{"id": "1606.08061", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "Exact gradient updates in time independent of output size for the spherical loss family", "abstract": "an immediately important class of computation problems involves for training exceptionally deep neural networks with persistent sparse prediction targets of a very very high dimension sized d. specifically these occur naturally late in e. po g. intensive neural language instruction models theoretically or particularly the learning of word - weight embeddings, initially often posed principally as problems predicting the sampling probability characteristic of next populated words along among which a scarce vocabulary of even size sense d ( in e. g. 200, 5000 000 ). computing sets the vocabulary equally large, but too typically non - sparse d - dimensional output vector from behind a last hidden layer of reasonable dimension depth d ( e. p g. 500 ) eventually incurs a prohibitive o ( typically dd ) computational cost error for measuring each example, as as gps does updating adding the $ 3 d \\ times d $ 360 output weight matrix indices and slowly computing efficiently the gradient needed for backpropagation additions to previous layers. while efficient handling of large sparse network inputs hardware is perhaps trivial, the case management of promising large hierarchical sparse domain targets is not, hence and has relatively thus and so long far been sidestepped even with typically approximate correlation alternatives such - as hierarchical discrete softmax update or sampling - based approximations performed during training. precisely in integrating this work we develop clearly an original algorithmic approach as which, for a single family of loss functions function that arguably includes objective squared error and posterior spherical softmax, can accurately compute hence the exact loss, mean gradient update for the output query weights, and correlation gradient for backpropagation, all in $ o ( if d ^ { 2 } ) $ per example weighed instead of $ 16 o ( ^ dd ) $, remarkably consistently without difficulties ever computing the d - dimensional output. the proposed algorithm algorithm yields a speedup of up compared to $ d / r 4d $ i. at e. two orders of magnitude for typical sizes, allowing for that critical part required of these the computations that relatively often dominates half the training time measured in this kind of network architecture.", "histories": [["v1", "Sun, 26 Jun 2016 17:57:36 GMT  (1642kb,D)", "http://arxiv.org/abs/1606.08061v1", "Expanded journal version of our NIPS-2015 conference paperarXiv:1412.7091with full algorithm generalized to the spherical family"]], "COMMENTS": "Expanded journal version of our NIPS-2015 conference paperarXiv:1412.7091with full algorithm generalized to the spherical family", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["pascal vincent", "alexandre de br\\'ebisson", "xavier bouthillier"], "accepted": false, "id": "1606.08061"}, "pdf": {"name": "1606.08061.pdf", "metadata": {"source": "CRF", "title": "Exact gradient updates in time independent of output size for the spherical loss family", "authors": ["Pascal Vincent", "Alexandre de Br\u00e9bisson", "Xavier Bouthillier"], "emails": [], "sections": [{"heading": null, "text": "4d , i.e. two orders of magnitude for typical sizes, for that critical part\nof the computations that often dominates the training time in this kind of network architecture.\n1 Introduction Many modern applications of neural networks have to deal with data represented, or representable, as very large sparse vectors. Such representations arise in natural language related tasks, where the dimension D of that vector is typically (a multiple of) the size of the vocabulary, but also in the sparse user-item matrices of collaborativefiltering applications. It is trivial to handle very large sparse inputs to a neural network in a computationally efficient manner: the forward propagation and update to the input weight matrix after backpropagation are correspondingly sparse. By contrast, training\nar X\niv :1\n60 6.\n08 06\n1v 1\n[ cs\n.N E\n] 2\nwith very large sparse prediction targets is problematic: even if the target is sparse, the computation of the equally large network output and the corresponding gradient update to the huge output weight matrix are not sparse and thus computationally prohibitive. This has been a practical problem ever since Bengio et al. [1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2]. Several approaches have been proposed to attempt to address this difficulty essentially by sidestepping it. They fall in two categories: \u2022 Sampling or selection based approximations consider and compute only a tiny frac-\ntion of the output\u2019s dimensions sampled at random or heuristically chosen. The reconstruction sampling of Dauphin et al. [3], the efficient use of biased importance sampling in Jean et al. [4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al. [7] all fall under this category. As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset. \u2022 Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class. Compared to the initial problem of considering all D output dimensions, both kinds of approaches are crude approximations. In the present work, we will instead investigate a way to actually perform the exact gradient update that corresponds to considering all D outputs, but do so implicitly, in a computationally efficient manner, without actually computing the D outputs. This approach works for a relatively restricted class of loss functions, that we call the spherical family, its simplest member being linear output with squared error (a natural choice for sparse real-valued regression targets). For simplicity and clarity we will begin with this squared error case, presenting the computational challenge that arises in the standard naive approach in Section 2 and deriving our algorithmic solution in Section 3. We will then extend our approach to the more general case of loss functions in the spherical family in Section 4. In Section 5 we will discuss numerical stability issues that may arise and detail our numerical stabilization strategy. Section 6 presents experimental validation focusing on timings obtained with our CPU and GPU implementations of our algorithm relative to the naive update algorithm.\n2 The problem\n2.1 Problem definition and setup We are concerned with gradient-descent based training of a deep feed-forward neural network with target vectors of very high dimension D (e.g. D = 200 000) but that are sparse, i.e. a comparatively small number, at most K D, of the elements of the target vector are non-zero. Such a K-sparse vector will typically be stored and represented compactly as 2K numbers corresponding to pairs (index, value). A network to be trained with such targets will naturally have an equally large output layer of dimension D. We can also optionally allow the input to the network to be a similarly\nhigh dimensional sparse vector of dimensionDin. Between the large sparse target, output, and (optionally large sparse) input, we suppose the network\u2019s intermediate hidden layers to be of smaller, more typically manageable, dimension d D (e.g. d = 500)1.\nMathematical notation:\n\u2022 Vectors are denoted using lower-case letters, e.g. h, and are considered columnvectors; corresponding row vectors are denoted with a transpose, e.g. hT . \u2022 Matrices are denoted using upper-case letters, e.g. W , withWT the transpose ofW . \u2022 The jth column of W is denoted Wj , and its ith row Wi\u2022 (both viewed as a column\nvector). \u2022 U\u2212T = ( U\u22121 )T denotes the transpose of the inverse of a square matrix. \u2022 1D denotes a D-dimensional column vector filled with ones. \u2022 1i\u2208A(y) denotes an indicator function whose value will be 1 if i \u2208 A(y) and 0\notherwise. \u2022 onehotD(j) = {1i=j}Di=1 is the D-dimensional column vector filled with zeros\nexcept at index j where its value is 1. \u2022 Id is the d\u00d7 d identity matrix.\nNetwork architecture\nWe consider a standard feed forward neural network architecture as depicted in Figure 1. An input vector x \u2208 RDin is linearly transformed into a linear activation a(1) = W (1)Tx + b(1) through a Din \u00d7 d input weight matrix W (1) (and an optional bias vector b(1) \u2208 Rd). This is typically followed by a non-linear transformation s to yield the representation of the first hidden layer h(1) = s(a(1)). This first hidden layer representation is then similarly transformed through a number of subsequent non-linear layers (that can be of any usual kind amenable to backpropagation) e.g. h(k) = s(a(k)) with a(k) = W (k)Th(k\u22121) + b(k) until we obtain last hidden layer representation h = h(m). We then obtain the final D-dimensional network output as o = Wh where W is a D \u00d7 d output weight matrix, which will be our main focus in this work. Finally, the network\u2019s D-dimensional output o is compared to the D-dimensional target vector y associated with input x using squared error, yielding loss L = \u2016o\u2212 y\u20162.\nTraining procedure\nThis architecture is a typical (possibly deep) multi-layer feed forward neural network architecture with a linear output layer and squared error loss. Its parameters (weight matrices and bias vectors) will be trained by gradient descent, using gradient backpropagation Rumelhart et al. [11], LeCun [12, 13] to efficiently compute the gradients. The procedure is shown in Figure 1. Given an example from the training set as an (input,target) pair (x, y), a pass of forward propagation proceeds as outlined above, computing the hidden representation of each hidden layer in turn based on the previous one, and finally the network\u2019s predicted output o and associated loss\n1Our approach does not impose any restriction on the architecture nor size of the hidden layers, as long as they are amenable to usual gradient backpropagation.\nEfficient Exact Gradient Update for Training Deep Networks with Very Large Sparse Targets Pascal Vincent * Alexandre de Br\u00e9bisson Xavier Bouthillier Abstract An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 500 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D ! d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. Training time is thus independent of output-layer size (or number of classes). Compared to naive backprop, the proposed algorithm is expected to yield an actual speedup of at least D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computation that often dominates the training time in this kind of network architecture.\nThe Problem \u2023 Training deep neural networks with very large sparse targets is an important problem \u2023 Arises e.g. in Neural Language Models [1] with large vocabulary size (e.g. D = 500 000 one-hot target). \u2023 Efficient handling of large sparse inputs is trivial. \u2023 But backprop training with large sparse targets is prohibitively expensive. \u2023 Focus on output layer: maps last hidden representation h of reasonable dimension d (e.g. 500)\nExperimental validation Timing of output layer computations, for CPU implementation on 2 GHz Intel Core i7. Minibatch size m =10. Both naive backprop version and the proposed factorised parameter version learn the same actual W.\nDetailed algorithm, benefits and limitations Accepted as a workshop contribution at ICLR 2015 3.5 PUTTING IT ALL TOGETHER: ALGORITHM FOR COMPUTING THE COST L, GRADIENT ON h, AND UPDATING U AND V Efficient computation of cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U\u2212T and Q. The following table describes the algorithmic steps that we put together from the equations derived above. Step # Operation Computational complexity Number of multiply-adds 1: h\u0302 = Qh O(d2) d2 2: y\u0302 = UT (V T y) O(Kd + d2) Kd + d2 3: z\u0302 = h\u0302\u2212 y\u0302 O(d) d 4: \u2207h = 2z\u0302 O(d) d 5: L = hT h\u0302\u2212 2hT y\u0302 + yT y O(2d + K) 2d + K + 1 6: Unew = U \u2212 2\u03b7(Uh)hT O(d2) 2d2 + d 7: U\u2212Tnew =\nU\u2212T + 2\u03b7 1\u22122\u03b7h2 (U\n\u2212T h)hT O(d2) 2d2 + 2d + 3\n8: Vnew = V + 2\u03b7y(U\u2212Tnewh) T O(d2 + Kd) d2 + K + Kd 9: Qnew = Q\u2212 2\u03b7  hz\u0302T + z\u0302hT  +\n(4\u03b72L)hhT\nO(d2) 4 + 2d + 3d2\n4 DISCUSSION: EXPECTED BENEFITS, EXTENSIONS AND LIMITATIONS\nHaving K  d  D we see that the proposed algorithm requires O(d2) operations whereas the standard approach required O(Dd) operations. If we take K \u2248 d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will requires roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm change corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the expected speedup is thus 100.\nNote that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and cha ge ll D \u00d7 d elements of matrix W , whereas the proposed approach only accesses the much smaller number K\u00d7d element of V as well as the three d\u00d7 d matrices U , U\u2212T , and Q. So overall we have a much faster algorithm, which while doing so implicitly, will however perform the exact same gradient update as the standard approach. We want to emphasize here that what we are doing is not at all the same as simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to Uand V would not be equivalent to the ordinary gradient update to W = V U .\nOur algorithm can be straightforwardly extended to the minibatch case, and is expected to yield the same speedup factor compared to the standard approach. But one needs to be careful in order to keep the computation of U\u2212T h reasonably efficient. Indeed, depending on the size of the minibatch m, it may be more efficient to resolve the correpsonding linear equation for each minibatch from scratch rather than updating U\u2212T with the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1). This approach that we detailed for linear output and squared error can easily be extended to slightly more exotic loss functions: basically any loss function that can be expressed using only the oc associated to non-zero yc and o2 =  j o 2 j the squared norm of the whole output vector, which we can compute cheaply. This family of loss functions does not include the standard softmax, but includes the so-called spherical softmax: log o 2 c\nj o 2 j (where c is the correct class label). It remains to be seen in practice how this approach performs computationally, and whether we lose something due to using this more limited family of loss functions.\n7\n* and CIFAR\nProposed approach We can do much better than O( Dd ). We can compute ! loss L ! gradient w.r.t. last hidden layer !h ! exact same gradient update to W all in O(d2) without ever computing full output o=Wh ! First trick: L and !h can be computed efficiently if we keep an up-to-date d x d matrix Q = WTW\nSecond trick: represent W implicitly as factorization and update U and V instead\n5.1 Computing the squared error loss efficiently Suppose we have, for a network input example x, computed last hidden representation h \u2208 Rd through forward propagation. The network\u2019s D dimensional output o = Wh is then in principle compared to high dimensional target y \u2208 RD. The corresponding squared error loss is L = Wh\u2212 y2. As we have seen in Section 3.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output Wh with a full D\u00d7d matrix W and a typically non-sparse h is O(Dd). Note however that we can rewrite this as: L = Wh\u2212 y2 = (Wh\u2212 y)T (Wh\u2212 y) = hT WT Wh\u2212 yT Wh\u2212 hT WT y + yT y = hT Qh\u2212 2hT (WT y) + yT y = hT Qh\u2212 2hT UT V T y + yT y = hT (Qh)\u2212 2hT (UT (V T y)) + yT y = hT ( Qh\nh\u0302\n\u22122(UT (V T y)   y\u0302 ) + yT y\nSHORT IDEA FORMULATION FOR SLIDES:\nL =  O(Dd) Wh \u2212y2\n= (Wh\u2212 y)T (Wh\u2212 y) = hT WT Wh\u2212 2hT (WT y) + yT y = hT ( Qh\nO(d2) \u22122(WT y)   O(Kd) ) + yT y O(K)\nwith Q = WT W Supposing we have maintained an up-to-date Q = WT W , which is a compact d\u00d7d matrix (we will see how we update Q cheaply in section ??????), computing h\u0302 = Qh has a complexity of O(d2). Thanks to the K\u2212sparsity and sparse representation of y, computing V T y is O(Kd) and results in a d\u2212dimensional vector, so that computing y\u0302 = UT (V T y) is O(Kd + d2) . The last term is O(K). So the overall computational complexity for computing L in this way is O(Kd+d2) = O((K +d)d). With K  D and d  D this can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.\nIf we define intermediate vectors h\u0302 = Qh and y\u0302 = WT y = UT (V T y) the computation of L can be rewritten a little more compactly as\nL = hT (h\u0302\u2212 2y\u0302) + y2\n5\nthis is O(Kd +d2 +K) = O(d2)\nComputing loss L\n5.2 Computing the gradient on h efficiently To backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last hidden layer representation h. This is \u2207h = \u2202L\u2202h = \u2202Wh\u2212y2 \u2202h = 2W T (Wh \u2212 y). Again, if we were to compute it directly in this manner the computational complexity would be a prohibitive O(Dd). But we can instea rewrite it as \u2207h = \u2202L \u2202h = \u2202 Wh\u2212 y2 \u2202h = 2WT (Wh\u2212 y) = 2  WT Wh\u2212WT y  = 2  Qh\u2212 UT V T y  = 2  Qh\u2212 UT (V T y)  = 2(h\u0302\u2212 y\u0302) Again, supposing we have maintained an up-to-date Q (we will see how we update Q cheaply in section ?????) computing \u2202L\u2202h this way is O(Kd + d 2) = O((K + d)d), much ch a er than the O(Dd) of the direct approach.\nHORT IDEA FORMULATION FOR SLIDES:\n\u2207h = \u2202L\n\u2202h = \u2202Wh\u2212 y2 \u2202h\n= 2WT (Wh\u2212 y) = 2( Qh\nO(d2) \u2212WT y   O(Kd) )\n5.3 Efficient gradient update of W The gradient of the squared error loss with respect to output layer weight matrix W is \u2202L\u2202W = \u2202Wh\u2212y2 \u2202W = 2(Wh\u2212y)hT . And the corresponding gradient descent update to W would be Wnew \u2190 W \u2212 2\u03b7(Wh \u2212 y)hT where \u03b7 is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residue Wh \u2212 y, and then to update all the Dd elements of W (since generally neither Wh\u2212 y nor h will be sparse). To overcome this difficulty let us first rewrite the update as\nWnew = W \u2212 2\u03b7(Wh\u2212 y)hT = W \u2212 2\u03b7WhhT + 2\u03b7yhT\nNote that we can decompose this update into two consecutive update steps:\n6\nthis is O(Kd +d2) = O(d2)\nProvided w maintain an up-to-date Q = WTW (achievable cheaply)\nComputing gradient !h w.r.t. last hidden layer\nW D\u00d7d = V D\u00d7d U d\u00d7d\n5.2 Computing the gradient n h efficiently To backpropagate the gradient through the network, we need to comput the gradient of loss L with respect to last hidden layer representation h. This is \u2207h = \u2202L\u2202h = \u2202Wh\u2212y2 \u2202h = 2W T (Wh \u2212 y). Again, if we were to compute it directly in this manner the computational complexity would be a prohibitive O(Dd). But we can instead rewrite it as \u2207h = \u2202L \u2202h = \u2202 Wh\u2212 y2 \u2202h = 2WT (Wh\u2212 y) = 2  WT Wh\u2212WT y  = 2  Qh\u2212 UT V T y  = 2  Qh\u2212 UT (V T y)  = 2(h\u0302\u2212 y\u0302)\nAgain, supposing we have maintained an up-to-date Q we will see how we update Q cheaply in section ?????) computing \u2202L\u2202h this way is O(Kd + d\n2) = O((K + d)d), much cheaper than the O(Dd) of the direct approach.\nSHORT IDEA FORMULATION FOR SLIDES:\n\u2207h = \u2202L\n\u2202h = \u2202Wh\u2212 y2 \u2202h\n= 2WT (Wh\u2212 y) = 2( Qh\nO(d2) \u2212WT y   O(Kd) )\n5.3 Efficient gradient update of W The gradient of the squared error loss with respect to output layer weight matrix W is \u2202L\u2202W = \u2202Wh\u2212y2 \u2202W = 2(Wh\u2212y)hT . And the corresponding gradient descent update to W would be Wnew \u2190 W \u2212 2\u03b7(Wh \u2212 y)hT where \u03b7 is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residue Wh \u2212 y, and then to update all the Dd elements of W (since generally neither Wh\u2212 y nor h will be sparse). To overcome this difficulty let us first rewrite the update as\nWnew = W \u2212 2\u03b7(Wh\u2212 y)hT = W \u2212 2\u03b7WhhT + 2\u03b7yhT\nNote that we can decompose this update into two consecutive update steps:\n6\nNaive gadient update is a rank-one update to W (all Dd elements of W modified!)\nEquivalently decomposed in 2 sequential steps: O( Dd )\na) W \u2190W \u2212 2\u03b7WhhT\nb) W \u2190W + 2\u03b7yhT\nWe will now see how we can perform each of these updates implicitly by updating only U and V respectively, as well as how we maintain correspondingly up-todate versio s of Q = V T V (needed to effic ently compute cost L and gradient on h in Equations ???? and ???? above) and U\u2212T = (U\u22121)T (that will be needed for update b) ).\nSolution:\na) Unew = U \u2212 2\u03b7(Uh)hT b) Vnew = V + 2\u03b7y(U\u2212Tnewh)T Proof:\nVnewUnew = (V + 2\u03b7y(U \u2212T newh) T ) Unew\n= V Unew + 2\u03b7y(U \u2212T newh) T Unew = V Unew + 2\u03b7yh T U\u22121newUnew = V (U \u2212 2\u03b7(Uh)hT ) + 2\u03b7yhT (U\u22121newUnew) = V U \u2212 2\u03b7V UhhT + 2\u03b7yhT = V U \u2212 2\u03b7(V Uh\u2212 y)hT = W \u2212 2\u03b7(Wh\u2212 y)T hT = Wnew\na) First update of the form W \u2190 W \u2212 2\u03b7WhhT This can be achieved implicitly by updating only U as follows:\nUnew = U \u2212 2\u03b7(Uh)hT\nProof:\nWnew = V Unew = V (U \u2212 2\u03b7(Uh)hT ) = V U \u2212 2\u03b7V UhhT = W \u2212 2\u03b7WhhT\nChanging U doesn\u2019t change Q = V T V . But we will need an up-to-date U\u2212T in the second update b).\nProvided we already have U\u2212T this can be achieved cheaply by using the Sherman-Morisson formula for the rank-one update to the inverse of U :\n(U + uvT )\u22121 = U\u22121 \u2212 1 1 + vT U\u22121u U\u22121uvT U\u22121\n7\na) W \u2190W \u2212 2\u03b7WhhT\nb) W \u2190W + 2\u03b7yhT\nWe will now see how we can perform each of these updates implicitly by updating only U and V respectively, as well as how we maintain correspondingly up-todate versions of Q = V T V (needed to efficiently compute cost L and gradient on h in Equations ???? and ???? above) and U\u2212T = (U\u22121)T (that will be needed for update b) ).\nSolution:\na) Unew = U \u2212 2\u03b7(Uh)hT b) Vnew = V + 2\u03b7y(U\u2212Tnewh)T Proof:\nVnewUnew = (V + 2\u03b7y(U \u2212T newh) T ) Unew\n= V Unew + 2\u03b7y(U \u2212T newh) T Unew = V Unew + 2\u03b7yh T U\u22121newUnew = V (U \u2212 2\u03b7(Uh)hT ) + 2\u03b7yhT (U\u22121newUnew) = V U \u2212 2\u03b7V UhhT + 2\u03b7yhT = V U \u2212 2\u03b7(V Uh\u2212 y)hT = W \u2212 2\u03b7(Wh\u2212 y)T hT = Wnew\na) First update of the form W \u2190 W \u2212 2\u03b7WhhT This can be achieved implicitly by updating only U as follows:\nUnew = U \u2212 2\u03b7(Uh)hT\nProof:\nWnew = V Unew = V (U \u2212 2\u03b7(Uh)hT ) = V U \u2212 2\u03b7V UhhT = W \u2212 2\u03b7WhhT\nChanging U doesn\u2019t change Q = V T V . But we will need an up-to-date U\u2212T in the second update b).\nProvided we already have U\u2212T this can be achieved cheaply by using the Sherman-Morisson formula for the rank-one update to the inverse of U :\n(U + uvT )\u22121 = U\u22121 \u2212 1 1 + vT U\u22121u U\u22121uvT U\u22121\n7\nThat can be performed implic ty through U and V:\nrank-1 update to U: O(d2)\nO(Kd) O(d2) provided we updated U-1 cheaply using Sherman-Morrison\nSparse update: only K rows of V instead of all D rows of W !\nO( Dd )\nProof:\nAccepted as a workshop contribution at ICLR 2015\na) W \u2190 W \u2212 2\u03b7WhhT b) W \u2190 W + 2\u03b7yhT\nNotice that we can perform each of these updates implicitly by updating only U and V respectively.:\na) Unew = U \u2212 2\u03b7(Uh)hT (4)\nb) Vnew = V + 2\u03b7y(U \u2212T newh) T (5)\nThis results in implicitly updating W as we did explicitly in the naive approach of Eq. 3.\nProof:\nVnewUnew = (V + 2\u03b7y(U \u2212T newh) T ) Unew\n= V Unew + 2\u03b7y(U \u2212T newh) T Unew = V Unew + 2\u03b7yh T U\u22121newUnew = V (U \u2212 2\u03b7(Uh)hT ) + 2\u03b7yhT ( \u22121newUnew) = V U \u2212 2\u03b7V UhhT + 2\u03b7yhT = V U \u2212 2\u03b7(V Uh\u2212 y)hT = W \u2212 2\u03b7(Wh\u2212 y)T hT = Wnew\nWe see that the update of U in Eq. 4 is a simple O(d2) operation. Following this simple rank-one update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U\u2212T which will also be O(d2):\nU\u2212Tnew = U \u2212T +\n2\u03b7\n1\u2212 2\u03b7 h2 (U\u2212T h)hT (6)\nIt is then easy to compute the U\u2212Tnewh, an O(d 2) operation needed in Eq. 5, and the ensuing rank-one update of V , thanks to the K-sparsity of y is only O(Kd). Thanks to the K\u2212sparsity and sparse representation of y, computing y\u0302 = V T y is O(Kd) and t2 is O(K). Computation of h\u0302 = U\u2212T h is O(d2). Given these, the update of Q is O(d2) and the rank-one update of V , thanks to the K-sparsity of y is O(Kd). So these operations together have computational complexity of O(Kd + d2) = O((K + d)d), which is much cheaper than the prohibitive O(Dd) of the direct approach.\n3.4 BOOKKEEPING: KEEPING AN UP-TO-DATE Q AND U\u2212T We have already seen, in Eq. 6, how we can cheaply maintain an up-to-date U\u2212T following our update of U . Similarly, following our updates to U and V , we need to keep an up-to-date Q = WT W which is needed to efficiently compute the loss L (Eq. 1) and gradient \u2207h (Eq. 2). The updates to and V in Equations 4 and 5 are equivalent to implicitly updating W as in Eq. 3, and this translates into the following update to Q = WT W :\nz\u0302 = Qh\u2212 UT (V T y) Qnew = Q\u2212 2\u03b7  hz\u0302T + z\u0302hT  + (4\u03b72L)hhT (7)\nProof is straightforward but not provided here due to space constraints.\n6\nBookkeeping operations as we update U and V: ! Using factored representation of W=VU does not change the complexity of the computation of L and !h .\n! Need to maintain an up-to-date U-1 following rank-1 update to U. \" achieved in O(d2) through Sherman-Morrison formula.\n! Need to maintain an up-to-date Q = WTW following updates to U and V. \" achieved in O(d2) as follows:\na) W \u2190W \u2212 2\u03b7WhhT\nb) W \u2190W + 2\u03b7yhT\nWe will now see how we can perform each of these updates implicitly by updating only U and V respectively, as well as how we maintain correspondingly up-todate versions of Q = V T V (needed to efficiently compute cost L and gradient on h in Equations ???? and ???? above) and U\u2212T = (U\u22121)T (that will be needed for update b) ).\nSolution:\na) Unew = U \u2212 2\u03b7(Uh)hT b) Vnew = V + 2\u03b7y(U\u2212Tnewh)T\nProof:\nVnewUnew = (V + 2\u03b7y(U \u2212T newh) T ) Unew\n= V Unew + 2\u03b7y(U \u2212T newh) T Unew = V Unew + 2\u03b7yh T U\u22121newUnew = V (U \u2212 2\u03b7(Uh)hT ) + 2\u03b7yhT (U\u22121newUnew) = V U \u2212 2\u03b7V UhhT + 2\u03b7yhT = V U \u2212 2\u03b7(V Uh\u2212 y)hT = W \u2212 2\u03b7(Wh\u2212 y)T hT = Wnew\nSHORT FORMULATION FOR SLIDES OF UPDATE OF Q IN ONLINE CASE:\nz\u0302 = Qh\u2212 UT (V T y) Qnew = Q\u2212 2\u03b7  hz\u0302T + z\u0302hT  + (4\u03b72L)hhT\na) First update of the form W \u2190 W \u2212 2\u03b7WhhT This can be achieved implicitly by updating only U as follows:\nUnew = U \u2212 2\u03b7(Uh)hT\nProof:\nWnew = V Unew = V (U \u2212 2\u03b7(Uh)hT ) = V U \u2212 2\u03b7V UhhT = W \u2212 2\u03b7WhhT\n7\nNote: this is NOT th same as a ordinary backprop update on two consecutive layers U and V which would still be O( Dd ). Altogether: O( d2 ) we suppose K << d << D\nwe suppose K << d << D\n\u2023 Sampling based approximations compute only a tiny fraction of the output\u2019s dimensions sampled at random.\nReconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.\n\u2023 Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.\n[1] Bengio, Y., Ducharme, R., and Vincent, P. (2001). A neural probabilistic language model. NIPS 2000.\n[2] Dauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with reconstruction sampling. ICML 2011.\n[5] Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation. NIPS 2013. [6] Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language model. AISTATS 2005. [3] Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. AISTATS 2010. [4] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. ICLR 2013 workshop track.\nwe suppose K << d << D Full algorithm (online version): ! Computation: O(12 d2) v.s. O(3 Dd) \" speedup of D/4d for typical sizes: between 50 and 300\n! Memory access: for each example access only Kd elements of V and d2 elements of U, U-1 and Q v.s. Dd elements of W.\nAnticipated benefits:\n! Approach limited to loss functions expressible using ||o||2 and the oc associated to non-zero yc only: \u2713 linear output + squared error # not regular log softmax \u2713 linear+spherical softmax:\n! Step 6 can lead over time to ill conditioning \" must periodically apply numerical stabilization strategy.\nLimitations\nExtension for minibatch of size m: ! Straightforward except for step 7: ! Update of U-T no longer with simple Sherman-Morrison. ! Several possibilities: Woodbury identity (must invert m x m matrix), or iterated\nSherman-Morrison, or solving UTx = h each time. Best choice will depends on m. ! \" complexity remains O(d2) per example.\nAccepted as a workshop contribution at ICLR 2015 3.5 PUTTING IT ALL TOGETHER: ALGORITHM FOR COMPUTING THE COST L, GRADIENT ON h, AND UPDATING U AND V Efficient computation of cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U\u2212T and Q. The following table describes the algorithmic steps that we put together from the equations derived above. Step # Operation Computational complexity Number of multiply-adds 1: h\u0302 = Qh O(d2) d2 2: y\u0302 = UT (V T y) O(Kd + d2) Kd + d2 3: z\u0302 = h\u0302\u2212 y\u0302 O(d) d 4: \u2207h = 2z\u0302 O(d) d 5: L = hT h\u0302\u2212 2hT y\u0302 + yT y O(2d + K) 2d + K + 1 6: Unew = U \u2212 2\u03b7(Uh)hT O(d2) 2d2 + d 7: U\u2212Tnew = U\u2212T + 2\u03b7 1\u22122\u03b7h2 (U \u2212T h)hT O(d2) 2d2 + 2d + 3 8: Vnew = V + 2\u03b7y(U\u2212Tnewh) T O(d2 + Kd) d2 + K + Kd 9: Qnew = Q\u2212 2\u03b7  hz\u0302T + z\u0302hT  + (4\u03b72L)hhT O(d2) 4 + 2d + 3d2 4 DISCUSSION: EXPECTED BENEFITS, EXTENSIONS AND LIMITATIONS Having K  d  D we see that the proposed algorithm requires O(d2) operations whereas the standard approach required O(Dd) operations. If we take K \u2248 d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will requires roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm change corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the ex ected speedup is thus 100. Note that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change all D \u00d7 d elements of matrix W , whereas the proposed approach only accesses the much smaller number K\u00d7d element of V as well as the three d\u00d7 d matrices U , U\u2212T , and Q. So overall we have a much faster algorithm, which while doing so implicitly, will however perform the exact same gradient update as the standard approach. We want to emphasize here that what we are doing is not at all the same as simply chaining 2 linear layers U and V and performing or inary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to Uand V would not be equivalent to the ordinary gradient update to W = V U . Our algorithm can be straightforwardly extended to the minibatch case, and is expected to yield the same speedup factor compared to the standard approach. But one needs to be careful in order to keep the computation of U\u2212T h reasonably efficient. Indeed, depending on the size of the minibatch m, it may be more efficient to resolve the correpsonding linear equation for each minibatch from scratch rather than updating U\u2212T with the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1). This approach that we detailed for linear output and squared error can easily be extended to slightly more exotic loss functions: basically any loss function that can be expressed using only the oc associated to non-zero yc and o2 =  j o 2 j the squared norm of the whole output vector, which we can compute cheaply. This family of loss functions does not include the standard softmax, but includes the so-called spherical softmax: log o 2 c\nj o 2 j (where c is the correct class label). It remains to be seen in practice how this approach performs computationally, and whether we lose something due to using this more limited family of loss functions.\n7\nTime taken by naive backprop (dotted lines) and the proposed factorised parameter version (full lines).\nSpeedup of factorised parameter version v.s. naive backprop (theoretical and experimentally measured).\nConclusion and future work \u2023 We developed an original algorithm that yields a huge speedup for performing a full exact gradient update in networks with very large\nsparse targets: remarkably time is independent of output size (number of classes).\n\u2023 Gain is from a fundamental algorithmic computational complexity improvement, not from low-level hardware-specific tricks or tuning. \u2023 Future: GPU implementation; spherical softmax cost; compare quality of word embeddings learned with these costs to standard softmax.\nReferences:\nL. A pass of gradient backpropagation then works in the opposite direction, starting from \u2207o = \u2202L\u2202o = 2(o \u2212 y) and propagating back the gradients \u2207h(k) = \u2202L\u2202h(k) and \u2207a(k) = \u2202L\u2202a(k) upstream through the network. The corresponding gradient contributions on parameters (weights and biases), collected along the way, are straightforward once we have the associated \u2207a(k) . Specifically they are \u2207b(k) = \u2207a(k) and \u2207W (k) = h(k\u22121)(\u2207a(k))T . Similarly for the input layer \u2207W (1) = x(\u2207a(1))T , and for the output layer \u2207W = (o \u2212 y)hT . Parameters are then updated through a gradient descent step W (k) \u2190 W (k) \u2212 \u03b7\u2207W (k) and b(k) \u2190 b(k) \u2212 \u03b7\u2207b(k) , where \u03b7 is a positive learning-rate. Similarly for the output layer which will be our main focus here: W \u2190W \u2212 \u03b7\u2207W .\n2.2 The easy part: input layer forward propagation and weight update\nIt is easy and straightforward to efficiently compute the forward propagation, and the backpropagation and weight update part for the input layer when we have a very large Din-dimensional but K\u2212sparse input vector x with appropriate sparse representation. Specifically we suppose that x is represented as a pair of vectors u, v of length (at most) K, where u contains integer indexes and v the associated real values of the elements of x such that xi = 0 if i /\u2208 u, and xuk = vk. \u2022 Forward propagation through the input layer: The sparse representation of x as\nthe positions of K elements together with their value makes it cheap to compute W (1)Tx. Even though W (1) may be a huge full Din \u00d7 d matrix, only K of its rows (those corresponding to the non-zero entries of x) need to be visited and summed to computeW (1)Tx. Precisely, with our (u, v) sparse representation of x this operation can be written asW (1)Tx = \u2211K k=1 vkW (1) :uk where each W (1) :uk is a d-dimensional\nvector, making this an O(Kd) operation rather than O(Dd). \u2022 Gradient and update through input layer: Let us for now suppose that we were\nable to get gradients (through backpropagation) up to the first hidden layer activations a(1) \u2208 Rd in the form of gradient vector \u2207a(1) = \u2202L\u2202a(1) . The corresponding gradient-based update to input layer weights W (1) is simply W (1) \u2190 W (1)\u2212\u03b7x(\u2207a(1))T . This is a rank-one update toW (1). Here again, we see that only the K rows of W (1) associated to the (at most) K non-zero entries of x need to be modified. Precisely this operation can be written as:W (1):uk \u2190W (1):uk\u2212\u03b7vk\u2207a(1) \u2200k \u2208 {1, . . . ,K} making this again a O(Kd) operation rather than O(Dd).\n2.3 The hard part: output layer propagation and weight update Given some network input x we suppose we can compute without difficulty through forward propagation the associated last hidden layer representation h \u2208 Rd. From then on: \u2022 Computing the final output o = Wh incurs a prohibitive computational cost of O(Dd) since W is a full D \u00d7 d matrix. Note that there is a-priori no reason for representation h to be sparse (e.g. with a sigmoid non-linearity) but even if it was, this would not fundamentally change the problem since it is D that is extremely large, and we supposed d reasonably sized already. Computing the residual (o\u2212 y) and associated squared error loss \u2016o\u2212 y\u20162 incurs an additional O(D) cost. \u2022 The gradient on h that we need to backpropagate to lower layers is \u2207h = \u2202L\u2202h = 2WT (o\u2212 y) which is another O(Dd) matrix-vector product. \u2022 Finally, when performing the corresponding output weight update W \u2190W \u2212\u03b7(o\u2212 y)hT we see that it is a rank-one update that updates allD\u00d7d elements ofW , which again incurs a prohibitive O(Dd) computational cost. For very large D, all these three O(Dd) operations are prohibitive, and the fact that y is sparse, seen from this perspective, doesn\u2019t help, since neither o nor o \u2212 y will be sparse.\n3 A computationally efficient algorithm for performing the exact online gradient update\nPreviously proposed workarounds are approximate or use stochastic sampling. We propose a different approach that results in the exact same, yet efficient gradient update, remarkably without ever having to compute large output o.\n3.1 Computing the squared error loss L and the gradient with respect to h efficiently\nSuppose that, we have, for a network input example x, computed the last hidden representation h \u2208 Rd through forward propagation. The network\u2019s D dimensional output o = Wh is then in principle compared to the high dimensional target y \u2208 RD. The corresponding squared error loss is L = \u2016Wh\u2212 y\u20162. As we saw in Section 2.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output Wh with a full D \u00d7 d matrix W and a typically non-sparse h is O(Dd). Similarly, to backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last hidden layer representation h. This is \u2207h = \u2202L\u2202h = \u2202\u2016Wh\u2212y\u20162 \u2202h = 2W\nT (Wh\u2212 y). So again, if we were to compute it directly in this manner, the computational complexity would be a prohibitive O(Dd). Provided we have maintained an up-to-date matrix Q = WTW , which is of reasonable size d \u00d7 d and can be cheaply maintained as we will see in Section 3.4, we can rewrite these two operations so as to perform them in O(d2):\nLoss computation:\nL = \u2016 O(Dd)\ufe37\ufe38\ufe38\ufe37 Wh \u2212y\u20162\n= (Wh\u2212 y)T (Wh\u2212 y) = hTWTWh\u2212 yTWh\u2212 hTWT y + yT y = hTQh\u2212 2hT (WT y) + yT y = hT ( Qh\ufe38\ufe37\ufe37\ufe38\nO(d2) \u22122 WT y\ufe38 \ufe37\ufe37 \ufe38 O(Kd) ) + yT y\ufe38\ufe37\ufe37\ufe38 O(K)\n(1)\nGradient on h:\n\u2207h = \u2202L\n\u2202h = \u2202\u2016Wh\u2212 y\u20162 \u2202h\n= 2WT (Wh\u2212 y) = 2 ( WTWh\u2212WT y )\n= 2( Qh\ufe38\ufe37\ufe37\ufe38 O(d2) \u2212WT y\ufe38 \ufe37\ufe37 \ufe38 O(Kd) ) (2)\nThe terms in O(Kd) and O(K) are due to leveraging the K-sparse representation of target vector y. WithK D and d D, we get altogether a computational cost of O(d2) which can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.\n3.2 Efficient gradient update of W The gradient of the squared error loss with respect to output layer weight matrix W is \u2202L \u2202W = \u2202\u2016Wh\u2212y\u20162 \u2202W = 2(Wh\u2212y)hT . And the corresponding gradient descent update to W would beWnew \u2190W \u22122\u03b7(Wh\u2212y)hT , where \u03b7 is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residual Wh \u2212 y, and then to update all the Dd elements of W (since generally neither Wh \u2212 y nor h will be sparse). All D \u00d7 d elements of W must be accessed during this update. On the surface this seems hopeless. But we will now see how we can achieve the exact same update on W in O(d2). The trick is to represent W implicitly as the factorization2 W\ufe38\ufe37\ufe37\ufe38\nD\u00d7d = V\ufe38\ufe37\ufe37\ufe38 D\u00d7d U\ufe38\ufe37\ufe37\ufe38 d\u00d7d and update U and V\ninstead:\na) Unew = U \u2212 2\u03b7(Uh)hT (3) b) Vnew = V + 2\u03b7y(U \u2212T newh) T (4)\nThis results in implicitly updating W as we did explicitly in the naive approach as we now prove:\nVnewUnew = (V + 2\u03b7y(U \u2212T newh) T )Unew\n= V Unew + 2\u03b7y(U \u2212T newh) TUnew = V Unew + 2\u03b7yh TU\u22121newUnew = V (U \u2212 2\u03b7(Uh)hT ) + 2\u03b7yhT (U\u22121newUnew) = V U \u2212 2\u03b7V UhhT + 2\u03b7yhT = V U \u2212 2\u03b7(V Uh\u2212 y)hT = W \u2212 2\u03b7(Wh\u2212 y)ThT = Wnew\nWe see that the update of U in Eq. 3 is a simple O(d2) operation. Following this simple rank-one update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U\u2212T which will also be O(d2):\nU\u2212Tnew = U \u2212T +\n2\u03b7\n1\u2212 2\u03b7 \u2016h\u20162 (U\u2212Th)hT (5)\nIt is then easy to compute the U\u2212Tnewh, anO(d 2) operation needed in Eq. 4. The ensuing rank-one update of V in Eq 4, thanks to the K-sparsity of y is only O(Kd): only theK rows V associated to non-zero elements in y are accessed and updated, sited of all D rows of W we had to modify in the naive update!\n2Note that we never factorize a pre-exisitng arbitrary W , which would be prohibitive as W is huge. We will no longer store a W nor work on it explicitly, but only matrices V and U which implicitly represent W .\n3.3 Adapting the computation of L and \u2207h to the factored representation of W\nWith the factored representation of W as V U , we only have W implicitly, so the WT y terms that entered in the computation of L and \u2207h in the previous section (Eq. 1 on page 6 and 2 on page 6) need to be adapted slightly as y\u0302 = WT y = UT (V T y), which becomesO(d2+Kd) rather thanO(Kd) in computational complexity. But this doesn\u2019t change the overall O(d2) complexity of these computations.\nThe adapted update computation of L and \u2207h can thus be expressed simply as:\n\u2207h = 2 (Qh\ufe38\ufe37\ufe37\ufe38 h\u0302 \u2212UT (V T y)\ufe38 \ufe37\ufe37 \ufe38 y\u0302 )\n\ufe38 \ufe37\ufe37 \ufe38 z\u0302\n(6)\nand L = hT (Qh\ufe38\ufe37\ufe37\ufe38\nh\u0302\n\u22122UT (V T y)\ufe38 \ufe37\ufe37 \ufe38 y\u0302 ) + yT y (7)\n3.4 Bookkeeping: keeping an up-to-date Q and U\u2212T\nWe have already seen, in Eq. 5, how we can cheaply maintain an up-to-date U\u2212T following our update of U . Similarly, following our updates to U and V , we need to keep an up-to-date Q = WTW which is needed to efficiently compute the loss L (Eq. 1) and gradient\u2207h (Eq. 2). We have shown that updates to U and V in equations 3 and 4 are equivalent to implicitly updating W as Wnew \u2190 W \u2212 2\u03b7(Wh\u2212 y)hT , and this translates into the following update to Q = WTW :\nQnew = Q\u2212 \u03b7 ( h\u2207Th +\u2207hhT ) + (4\u03b72L)hhT (8)\nOne can see that this last bookkeeping operation also has a O(d2) computational complexity.\nProof that this update to Q corresponds to the update Wnew \u2190 2(Wh\u2212 y)hT\nWTnewWnew = ( W \u2212 2\u03b7(Wh\u2212 y)hT )T ( W \u2212 2\u03b7(Wh\u2212 y)hT ) WTnewWnew = W TW \u2212 2\u03b7h(Wh\u2212 y)TW \u2212 2\u03b7WT (Wh\u2212 y)hT +4\u03b72h(Wh\u2212 y)T (Wh\u2212 y)hT WTnewWnew = Q\u2212 2\u03b7 ( hhTWTW \u2212 hyTW ) \u2212 2\u03b7 ( WTWhhT \u2212WT yhT ) +4\u03b72h(hTWTWh\u2212 hTWT y \u2212 yTWh+ yT y)hT WTnewWnew = Q\u2212 2\u03b7 ( hhTQ\u2212 h(WT y)T ) \u2212 2\u03b7 ( QhhT \u2212 (WT y)hT ) +4\u03b72h(hTQh\u2212 hT (WT y)\u2212 (WT y)Th+ yT y)hT WTnewWnew = Q\u2212 2\u03b7h ( hTQ\u2212 (WT y)T ) \u2212 2\u03b7 ( Qh\u2212WT y ) hT\n+4\u03b72h(hTQh\u2212 2hTWT y + yT y)hT\nWTnewWnew = Q\u2212 \u03b7h(2(Qh\u2212WT y)\ufe38 \ufe37\ufe37 \ufe38 \u2207h )T \u2212 \u03b7(2(Qh\u2212WT y)\ufe38 \ufe37\ufe37 \ufe38 \u2207h )hT\n+4\u03b72h (hT (Qh\u2212 2WT y) + yT y)\ufe38 \ufe37\ufe37 \ufe38 L hT\nwhere we see that the last term uses the expression of L from Eq. 1 on page 6 and the first two terms uses the expression of\u2207h from Eq. 6: \u2207h = 2(Qh\u2212UT (V T y)) = 2(Qh\u2212WT y). Thus we have shown that\nWTnewWnew = Q\u2212 \u03b7h\u2207Th \u2212 2\u03b7\u2207hhT + 4\u03b72hLhT = Q\u2212 \u03b7 ( h\u2207Th +\u2207hhT ) + (4\u03b72L)hhT\nwhich is the update Qnew that we gave in Eq. 8 above.\n3.5 Putting it all together: detailed online update algorithm and expected benefits\nWe have seen that we can efficiently compute cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping forU\u2212T andQ. Here we put everything together. The parameters of the output layer that we will learn are V,U and implicitly representW asW = V U . We first need to initialize these parameter matrices, as well as bookkeeping matrices Q and U\u2212T in a consistent way, as explained in Algo. 1. We then iterate over the following: \u2022 pick a next input,target example x, y (where y is K-sparse and uses an appropriate\nsparse representation) \u2022 perform forward propagation through all layers of the network up to the last hidden\nlayer, to compute last hidden layer representation h = h(x), that should include a constant 1 first element. \u2022 execute Algo. 2, that we put together from the equations derived above, and that will: compute the associated squared error lossL, perform an implicit gradient update step on W by correspondingly updating V and U in a computationally efficient manner, update bookkeeping matrices Q and U\u2212T accordingly, and compute and return the gradient of the loss with respect to the last hidden layer\u2207h \u2022 having \u2207h, further backpropagate the gradients upstream, and use them to update the parameters of all other layers Having K d D we see that the update algorithm we developed requires O(d2) operations, whereas the standard approach required O(Dd) operations. If we take K \u2248 d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will require roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm\nchange corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the expected speedup is thus 100. Note that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change allD\u00d7d elements of matrixW , whereas the proposed approach only accesses the much smaller number K \u00d7 d elements of V as well as the three d\u00d7 d matrices U , U\u2212T , and Q. So overall we have a substantially faster algorithm whose complexity is independent of D, which, while doing so implicitly, will nevertheless perform the exact same gradient update as the standard O(Dd) approach. We want to emphasize here that this approach is entirely different from simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to U and V would not be equivalent to the ordinary gradient update to W = V U .\nAlgorithm 1 Initialization of output layer parameters V,U and bookkeeping matrices Q,U\u2212T \u2022 we can initialize D \u00d7 d matrix V randomly as we would have initialized W so that we initially have V = W . Alternatively we can initialize V to 0 (there won\u2019t be symmetry breaking issues with having W initially be 0 provided the other layers are initialized randomly, since varying inputs and targets will naturally break symmetry for the output layer) \u2022 initialize Q \u2190 V TV (or more cheaply initialize Q \u2190 0 if we have initialized V to 0). \u2022 we initialize U to the identity: U \u2190 Id so that, trivially, we initially have V U = W . \u2022 initialize U\u2212T \u2190 Id\n3.6 Minibatch version of the algorithm for squared error The algorithm we derived for online gradient is relatively straightforward to extend to the case of minibatches containing m examples. We iniialize parameters as in the online case follpwing Algo. 1 and apply the same training procedure outlined in Section. 3.5, but now using minibatches containing m examples, rather than a single example vector. The corresponding update and gradient computation is given in Algorithm 3 which follows equivalent steps to the online version of Algorithm 2, but using matrices with m columns in place of single column vectors. For example step 3 which in the online algorithm was \u2207h = 2(h\u0302 \u2212 y\u0302) using d\u2212dimensional vectors becomes in the minibatch version\u2207H = 2(H\u0302 \u2212 Y\u0302 ) using d\u00d7m matrices instead.\nNote that in the minibatch version, in step 6, we update U\u2212T based on the Woodbury equation, which generalizes the Sheman-Morrison formula for m > 1 and involves inverting an m \u00d7 m matrix, an O(m3) operation. But depending on the size of the minibatch m, it may become more efficient to solve the corresponding linear equations for each minibatch from scratch every time, rather than inverting that m\u00d7m matrix. In which case we won\u2019t need to maintain an U\u2212T at all. Or in cases of minibatches containing more than d examples, it may even become more efficient to invert U from scratch every time.\nAlgorithm 2 Efficient computation of cost L, gradient \u2207h, and update to parameters U and V for squared error, in the online case Inputs (besides above parameters V,U,Q,U\u2212T ): \u2022 h \u2208 Rd hidden representation vector for one example h \u2208 Rd \u2022 y \u2208 RD associated K-sparse target vector stored using a sparse representation (in-\ndices and values of non-zero elements) \u2022 \u03b7 \u2208 R+learning rate for the update Outputs: \u2022 L \u2208 R the squared error loss for this example \u2022 updated parameters and bookkeeping matrices Unew, Vnew, Qnew, U\u2212Tnew \u2022 \u2207h \u2208 Rd the gradient of the loss with respect to h, to further backpropagate up-\nstream. Algorithm:\nStep # Operation Computational complexity Approximate number of elementary operations (multiplyadds) 1: h\u0302 = Qh O(d2) d2 2: y\u0302 = UT (V T y) O(Kd+ d2) Kd+ d2 3: \u2207h = 2(h\u0302\u2212 y\u0302) O(d) d 4: L = hT h\u0302\u2212 2hT y\u0302 + yT y O(2d+K) 2d+K + 1 5: U \u2190 U \u2212 2\u03b7(Uh)hT O(d2) 2d2 + d 6: U\u2212T \u2190 U\u2212T +\n2\u03b7 1\u22122\u03b7\u2016h\u20162 (U \u2212Th)hT [ from Sherman-Morrison formula ]\nO(d2) 2d2 + 2d+ 3\n7: V \u2190 V + 2\u03b7y(U\u2212Th)T where we must use the freshly updated U\u2212T\nresulting from step 6)\nO(d2 +Kd) d2+K+Kd\n8: Q\u2190 Q\u2212 \u03b7 ( h\u2207Th +\u2207hhT ) +\n(4\u03b72L)hhT\nO(d2) 4 + 2d+ 3d2\nAltogether: O(d2) provided K < d\nD\n\u2248 12d2 elementary operations\nIn step 9, the updateQnew forQ corresponds to the implicit weight updateWnew \u2190 W \u2212 2\u03b7(WH \u2212 Y )HT as we now prove:\nWe will use the following precomputed quantities: Q = WTW , H\u0302 = QH and Y\u0302 = WTY = UT (V TY ) and \u2207H = 2(H\u0302 \u2212 Y\u0302 ).\nQnew = W T newWnew\n= ( W \u2212 2\u03b7(WH \u2212 Y )HT )T ( W \u2212 2\u03b7(WH \u2212 Y )HT )\n= WTW \u2212 2\u03b7H(WH \u2212 Y )TW \u2212 2\u03b7WT (WH \u2212 Y )HT +4\u03b72H(WH \u2212 Y )T (WH \u2212 Y )HT = Q\u2212 2\u03b7 ( HHTWTW \u2212HY TW ) \u2212 2\u03b7 ( WTWHHT \u2212WTY HT )\n+4\u03b72H(HTWTWH \u2212HTWTY \u2212 Y TWH + Y TY )HT = Q\u2212 2\u03b7 ( HHTQ\u2212H(WTY )T ) \u2212 2\u03b7 ( QHHT \u2212 (WTY )HT )\n+4\u03b72H(HTQH \u2212HT (WTY )\u2212 (WTY )TH + Y TY )HT\n= Q\u2212 2\u03b7 ( HH\u0302T \u2212HY\u0302 T + H\u0302HT \u2212 Y\u0302 HT )\n+4\u03b72H(HT H\u0302 \u2212HT Y\u0302 \u2212 Y\u0302 TH + Y TY )HT\n= Q\u2212 2\u03b7 ( H(H\u0302 \u2212 Y\u0302 )T + (H\u0302 \u2212 Y\u0302 )HT ) + 4\u03b72H(HT (H\u0302 \u2212 Y\u0302 )\u2212 Y\u0302 TH + Y TY )HT = Q\u2212 \u03b7 ( H(2(H\u0302 \u2212 Y\u0302 ))T + (2(H\u0302 \u2212 Y\u0302 ))HT ) + 4\u03b72H(HT (H\u0302 \u2212 Y\u0302 )\u2212 Y\u0302 TH + Y TY )HT = Q\u2212 \u03b7 ( H\u2207TH +\u2207HHT ) + 4\u03b72H ( HT Z\u0302 \u2212 Y\u0302 TH + Y TY )\n\ufe38 \ufe37\ufe37 \ufe38 M\nHT\nwhich is the update of Q we use in in step 8 of Algorithm on the previous page.\nAlgorithm 3 Minibatch version of the update algorithm for squared error Inputs (besides above parameters V,U,Q,U\u2212T ): \u2022 parameters and bookkeeping matrices: U, V, Q, U\u2212T \u2022 H : a d \u00d7m matrix whose m columns contain the last hidden layer representation\nvectors form example (with an appended constant 1 element to account for an output bias). \u2022 Y : aD\u00d7m sparse target matrix. Each of itsm columns is theK-sparse target vector associated to one example of the minibatch, stored using a sparse representation (indices and values of non-zero elements). \u2022 \u03b7 \u2208 R+learning rate for the update Updates: \u2022 parameters and bookkeeping matrices: U, V, Q, U\u2212T Outputs: \u2022 L \u2208 R the sum of squared error losses for the m examples of the minibatch \u2022 \u2207H a d \u00d7m matrix whose m columns contain the gradient of the loss with respect\nto H , to further backpropagate upstream. Algorithm:\nStep # Operation Computation complexity Approximate number of elementary operations (multiply-adds)\n1: H\u0302 = QH O(md2) md2 2: Y\u0302 = UT (V TY ) O(mKd+ md2) mKd+md2 3: \u2207H = 2(H\u0302 \u2212 Y\u0302 ) O(md) md 4a: M =\nHT H\u0302 \u2212 (Y\u0302 TH +HT Y\u0302 ) + Y TY O(m2d+ m2K)\n2m2d+m2K\n4b: L = Tr(M) O(m) m 5: U \u2190 U \u2212 2\u03b7(UH)HT O(md2) 2md2 +md 6: U\u2212T \u2190 U\u2212T \u2212\n(U\u2212TH) ( (HTH \u2212 12\u03b7 Im)\u22121HT ) [ from Woodbury identity ] O(m2d+ m3 +md2) 2md2 +m+ 23m 3 +m2d (we count 23m 3\noperations for inversion of a m\u00d7m matrix)\n7: V \u2190 V + 2\u03b7Y (U\u2212TH)T where we must use the freshly updated U\u2212T resulting from step 6)\nO(md2 + mKd)\nmd2 +mK +mKd\n8: Q\u2190 Q\u2212 \u03b7 ( H\u2207TH +\u2207HHT ) +\n4\u03b72(HM)HT O(md2 + m2d)\nm2d+ 3md2 + 2d2\nAltogether: O(md2) provided K < m < d D.\n\u2248 10md2 + 3m2d+m3 elementary operations when K = 1\nNote that if we chose m > d we will not perform step 7 based on the Woodbury identity, which would be wasteful, but instead directly recompute the inverse of Unew in O(d3). The overall complexity remains O(md2) in this case also.\n4 Generalizing to a broader family of loss functions Let o = Wh the linear activations computed at the output layer. The approach that we detailed for linear output and squared error can be extended to a more general family of loss functions: basically any loss function ` that can be expressed using only the oc associated to non-zero yc together with q = \u2016o\u20162 = \u2211 j o 2 j the squared norm of the\nwhole output vector, and optionally s = sum(o) = \u2211 j oj which we will see that we can both compute cheaply. We call this family of loss functions the spherical family of loss functions or in short spherical losses, defined more formally as the family of losses that can be expressed as:\nL = `( \u2016o\u20162, sum(o), K, oK, yK)\nwhere K denotes the vector of indices of y of cardinality at most K D that is associated to non-zero elements of y in a sparse representation ofy; yK is the corresponding vector of values of y at positions K, i.e. yK = (y(K1), . . . , y(K|K|))T ; similarly oK is the vector of values of linear activation o at positions K, i.e. oK = (o(K1), . . . , o(K|K|))\nT . Note that the squared error loss belongs to this family as\n`squared =\nD\u2211\nj=1\n(oj \u2212 yj)2\n=\nD\u2211\nj=1\no2j \u2212 2ojyj + y2j\n=\n  D\u2211\nj=1\no2j\n \u2212 2   D\u2211\nj=1\nojyj\n +   D\u2211\nj=1\ny2j\n \n= \u2016o\u20162 \u2212 2\n \u2211\nj\u2208K ojyj\n +  \u2211\nj\u2208K y2j\n  since for j /\u2208 K we have yj = 0\n= \u2016o\u20162 \u2212 2oTKyK + \u2016yK\u20162 = `squared( \u2016o\u20162, sum(o), K, oK, yK)\nwhere `squared in particular doesn\u2019t use sum(o). The spherical family of loss functions does not include the standard log of softmax, but it includes possible alternatives, such as the spherical softmax and Taylor-softmax that we will introduce in a later section. Let us detail the steps for computing such a spherical loss from last hidden layer representation h: \u2022 o = Wh \u2022 q = \u2016o\u20162 = \u2211 o2i \u2022 s = sum(o) = \u2211 oi \u2022 L = `(q, s, K, oK, yK)\nThe gradient of the loss may be backpropagated and the parameters updated in the usual naive way with the following steps: \u2022 compute scalars \u2202`\u2202q (q, s, K, oK, yK) and \u2202`\u2202s (q, s, K, oK, yK) as well asK-dimensional gradient vector \u2202`\u2202oK (q, s, K, oK, yK)\u2022 clear D-dimensional gradient vector\u2207o \u2190 0 \u2022 update (\u2207o)K \u2190 \u2202`\u2202oK \u2022 update \u2207o \u2190 \u2207o + \u2202`\u2202q \u2202q\n\u2202o\ufe38\ufe37\ufe37\ufe38 2o\n\u2022 update \u2207o \u2190 \u2207o + \u2202`\u2202s \u2202s\n\u2202o\ufe38\ufe37\ufe37\ufe38 1D\n\u2022 backpropagate \u2207h = WT\u2207o \u2022 update W \u2190W \u2212 \u03b7\u2207ohT where \u03b7 is a scalar learning rate. Here again, as in the squared error case, we see that the computation of o in the forward pass and backpropagation of the gradient to \u2207h would both require multiplication by the D \u00d7 d matrix W , and that the update to W will generally be a non-sparse rank-1 update that requires modifying all itsDd elements. Each of these three operations have a O(Dd) complexity.\nWe will now follow the same logical steps as in the simpler squared error case to derive an efficient algorithm for the spherical loss family.\n4.1 Efficient computation of the loss Let us name the formal parameters of ` more clearly as follows:\n`(q, s,K,a, t) where q ands are scalars that will receive \u2016o\u20162 and sum(o) respectively; K is a vector that will contain the list of at mostK indices that correspond to non-zero elements of sparse y; a = oK and t = yK.\n4.1.1 Computing q = \u2016o\u20162\nq = \u2016o\u20162 = \u2016 O(Dd)\ufe37\ufe38\ufe38\ufe37 Wh \u20162\n= (Wh) T (Wh)\n= hTWTWh\n= hT ( Qh\ufe38\ufe37\ufe37\ufe38 O(d2) ) (9)\nsupposing we have maintained an up-to date Q = WTW . Derivative:\n\u2202q \u2202o = 2o\n4.1.2 Computing s = sum(o)\ns = sum(o) = sum( O(Dd)\ufe37\ufe38\ufe38\ufe37 Wh )\n=\nD\u2211\ni=1\n  d\u2211\nj=1\nhjWj   i\n=\nD\u2211\ni=1\nd\u2211\nj=1\nhjWij\n=\nd\u2211\nj=1\n( hj D\u2211\ni=1\nWij\n)\n=\nd\u2211\nj=1 hj sum(Wj)\ufe38 \ufe37\ufe37 \ufe38 w\u0304j\n= w\u0304Th\n= hT w\u0304 (10)\nThis is an O(d) operation, provided we have maintained an up-to-date vector w\u0304 = (sum(W1), . . . , sum(Wd)) = W T1D.\n\u2202s \u2202o = 1D\n4.1.3 Computing specific ok\nWe will also need to compute the specific ok for the few k \u2208 K.\nok = (Wh)k\n= hTWk\u2022\nwhich gives\na = oK = (o(K1), . . . , o(K|K|)) T\n= (hTWK1\u2022, . . . , h TWK|K|\u2022) T (11)\nwe then have all we need to pass to loss function ` to compute the associated loss\nL = `(q, s, K, oK, yK) = `(q, s, K, a, t) (12)\n4.1.4 Corresponding equations for the minibatch case\nIn the minibatch case, rather than having the hidden representation of a single example as a vector h we suppose we receive m hidden representations in the m columns of a d \u00d7m matrix H . The associated sparse target is D \u00d7m matrix Y whose m columns contain each at mostK non-zero elements. Y will be stored using sparse representation (K, T ) whereK is now a K\u00d7m matrix of indices and T is a K\u00d7m matrix containing the corresponding values of Y such that Tkj = YKkj ,j for k \u2208 {1, . . . ,K} and j \u2208 {1, . . . ,m}.\nThe above equations given for the online case, can easily be adapted to the minibatch case as follows:\nLet O = WH the D \u00d7m matrix of linear outputs whose jth column will contain the output vector of the jth example of the minibatch. The specific outputs associated to non-zero target values in Y (whose indexes are in K) will be collected in K \u00d7 m matrix A (the minibatch version of vector a of Equation 11 such that\nAkj = OKkj ,j = (Hj) TWKkj\u2022 (13)\nAdapting Equation 9 to the minibatch case, the squared norm of the m output vectors is obtained in m-dimensional vector q as\nq = diag(HT QH\ufe38\ufe37\ufe37\ufe38 H\u0302\ufe38 \ufe37\ufe37 \ufe38\nM\u0302\n) (14)\nAdapting Equation 10 to the minibatch case, the sum of each of the m output vectors is obtained in m-dimensional vector s as\ns = HT w\u0304 (15)\nAdapting Equation 12 the corresponding vector of m individual losses for the m examples of the minibatch is\n~L = [`(qj , sj ,Kj,, Aj , Tj)]j=1...m (16)\nand the total loss for the minibatch is\nL = sum(~L) (17)\n4.2 Gradient of loss L with respect to h Online case:\nTo backpropagate the gradients through the network, we first need the gradients with respect to linear activations o: \u2207o = \u2202L\u2202o .\nThere will be three types of contributions to this gradient: contribution due to q, contribution due to s, and contribution due to direct influence on the loss of the ok for k \u2208 K.\n\u2207o = \u2202L\n\u2202o =\n\u2202`\n\u2202q\n\u2202q \u2202o + \u2202` \u2202s \u2202s \u2202o +\nK\u2211\nk=1\n\u2202`\n\u2202ak\n\u2202ak \u2202o\nWe have \u2202q\u2202o = 2o, \u2202s \u2202o = 1D and \u2202ak \u2202o = onehotD(Kk) because ak = oKk so this\nbecomes\n\u2207o = 2o \u2202`\n\u2202q + 1D\n\u2202` \u2202s +\nK\u2211\nk=1\n\u2202`\n\u2202ak onehotD(Kk)\n= 2o \u2202`\n\u2202q + 1D\n\u2202` \u2202s + y\u030a (18)\nwhere we have defined vector y\u030a = \u2211K k=1 \u2202` \u2202ak\nonehotD(Kk) as a sparse vector, having value at position kj equal \u2202`\u2202aj . It will, like y, be stored in K \u2212 sparse representation, with the indexes given by k and the corresponding values in \u2202`\u2202aj .\nGradient with respect to h:\n\u2207h = \u2202o\n\u2202h\n\u2202L\n\u2202o\n= WT\u2207o = WT ( 2o \u2202`\n\u2202q + 1D\n\u2202` \u2202s + y\u030a\n)\n= 2WT o \u2202`\n\u2202q +WT1D\n\u2202` \u2202s +WT y\u030a\n= 2WTWh \u2202`\n\u2202q + w\u0304\n\u2202` \u2202s +WT y\u030a\n= 2Qh \u2202`\n\u2202q + w\u0304\n\u2202` \u2202s +WT\nK\u2211\nk=1\n\u2202`\n\u2202ak onehotD(Kk)\n= 2Qh \u2202`\n\u2202q + w\u0304\n\u2202` \u2202s +\nK\u2211\nk=1\n\u2202`\n\u2202ak WT onehotD(Kk)\n= 2Qh \u2202`\n\u2202q + w\u0304\n\u2202` \u2202s +\nK\u2211\nk=1\n\u2202`\n\u2202ak WKk\u2022\nMinibatch case:\nWe now consider a minibatch of m examples whose corresponding linear outputs are in a D \u00d7m matrix O = WH . Let us also denote the vectors of gradients of the loss with respect to q and s as:\n\u2207q = [ \u2202`\n\u2202q (qj , sj ,Kj,, Aj , Tj)\n]\nj=1...m\n\u2207s = [ \u2202`\n\u2202s (qj , sj ,Kj,, Aj , Tj)\n]\nj=1...m\nLet us also define\n\u2207A = [ \u2202`\n\u2202ak (qj , sj ,Kj,, Aj , Tj)\n]\nk=1...K, j=1...m\nand Y\u030a as the sparse D \u00d7m whose column j is defined as\nY\u030aj = K\u2211\nk=1\n\u2202`\n\u2202ak (qj , sj ,Kj,, Aj , Tj) onehotD(Kkj)\n=\nK\u2211\nk=1\n(\u2207A)kj onehotD(Kkj)\nwhich may be summarize asY\u030aKj = (\u2207A)j Equation 18 then becomes in the minibatch case:\n\u2207Oj = 2Oj (\u2207q)j + 1D (\u2207s)j + Y\u030aj or in matrix form\n\u2207O = 2O diag(\u2207q) + 1D\u2207Ts + Y\u030a (19) and the gradient with respect to H is:\n\u2207H = \u2202L\n\u2202H = \u2202O\n\u2202H\n\u2202L\n\u2202O\n= WT\u2207O (20) = WT ( 2O diag(\u2207q) + 1D\u2207Ts + Y\u030a )\n= 2WTO diag(\u2207q) +WT1D\u2207Ts +WT Y\u030a = 2WTWH diag(\u2207q) + w\u0304\u2207Ts +WT Y\u030a = 2QH diag(\u2207q) + w\u0304\u2207Ts +WT Y\u030a\ufe38 \ufe37\ufe37 \ufe38\nZ\u0302\n(21)\nwhere we define the d\u00d7m matrix Z\u0302 as Z\u0302 = w\u0304\u2207Ts +WT Y\u030a (22)\n4.3 Standard naive gradient update of parameters W The gradient of the loss with respect to output layer weight matrix W is\n\u2202L\n\u2202W =\n\u2202L\n\u2202O\n\u2202O\n\u2202W\n= \u2207OHT = ( 2O diag(\u2207q) + 1D\u2207Ts + Y\u030a ) HT = ( 2WH diag(\u2207q) + 1D\u2207Ts + Y\u030a ) HT\nAnd the corresponding gradient descent update to W would thus be\nWnew = W \u2212 \u03b7 ( 2WH diag(\u2207q) + 1D\u2207Ts + Y\u030a ) HT (23)\nwhere \u03b7 is a positive learning rate. Computed in this manner, this induces a prohibitive O(mDd) computational complexity, first to compute WH , and then to update all the Dd elements of W . Note that all D \u00d7 d elements of W must be accessed during this update. On the surface this seems hopeless. But we will see in the next section how we can achieve the exact same update of W in O(md2).\n4.4 Efficient gradient update of parameters using a factored representation of W\nFirst note that the update of W given in equation 23 can be decomposed in 3 consecutive updates:\na) W \u2190 W \u2212 2\u03b7(WH) diag(\u2207q)HT b) W \u2190 W \u2212 \u03b71D\u2207Ts HT c) W \u2190 W \u2212 \u03b7Y\u030a HT\nIn doing this we haven\u2019t yet changed anything to the O(mDd) complexity of this update. Note that update a) can also be seen as W \u2190W ( I\u2212 2\u03b7H diag(\u2207q)HT ) .\nThe trick now is to represent W implicitly as3:\nW\ufe38\ufe37\ufe37\ufe38 D\u00d7d = V\ufe38\ufe37\ufe37\ufe38 D\u00d7d U\ufe38\ufe37\ufe37\ufe38 d\u00d7d\n+1D\u03c9 T (24)\nwhere \u03c9 is a d-dimensional vector. In this case the following updates to V,U, \u03c9 respectively will implicitly update the implicit W in the exact same way as the above 3 updates:\n3Note that we never actually factorize an arbitrary pre-exisitng W , which would be prohibitive as W is huge. We will no longer store or update a W , but onlyV, U, \u03c9 which implicitly represent W .\na) Unew = U ( I\u2212 2\u03b7H diag(\u2207q)HT )\n= U \u2212 2\u03b7UH diag(\u2207q)HT (25) b) \u03c9new = ( I\u2212 2\u03b7H diag(\u2207q)HT )T \u03c9 \u2212 \u03b7H\u2207s\n= \u03c9 \u2212 2\u03b7H diag(\u2207q)HT\u03c9 \u2212 \u03b7H\u2207s = \u03c9 \u2212 \u03b7H ( 2 diag(\u2207q)HT\u03c9 +\u2207s ) (26)\nc) Vnew = V \u2212 \u03b7Y\u030a (U\u2212TnewH)T (27)\nBut, with this formulation, provided we keep an up-to-date U\u2212T (which we will see we can do cheaply using the Woodbury identity), the whole update to V,U, \u03c9 is now O(md2) rather than the equivalent naive O(mDd) update of Eq. 23 to an explicit W .\nIndeed, step a) and b) involve only multiplications between matrices of dimensions d\u00d7m and d\u00d7d (matricesH andU ). As for step c) it involves anO(md2) multiplication of U\u2212T by H , followed by a sparse update of V . Since Y\u030a is an extremely sparse D \u00d7m matrix whose m columns each contain at most K non-zero elements, update c) will touch at most Km rows of V , yielding an O(Kmd) operation. This is to be contrasted with the standard, equivalent but naive update of Eq. 23 to an explicit W , which requires accessing and modifying all D\u00d7d elements of W for every update and yields an overall O(mDd) computational complexity.\nProof that this sequence of updates yields the update of W given above:\nVnewUnew + 1D\u03c9 T new\n= ( V \u2212 \u03b7Y\u030a (U\u2212TnewH)T ) Unew + 1D ( \u03c9 \u2212 \u03b7H ( 2 diag(\u2207q)HT\u03c9 +\u2207s ))T = ( V \u2212 \u03b7Y\u030a HTU\u22121new ) Unew + 1D ( \u03c9T \u2212 \u03b7 ( 2 diag(\u2207q)HT\u03c9 +\u2207s )T HT ) = V Unew \u2212 \u03b7Y\u030a HTU\u22121newUnew + 1D\u03c9T \u2212 \u03b71D ( 2 diag(\u2207q)HT\u03c9 +\u2207s )T HT = V Unew \u2212 \u03b7Y\u030a HT + 1D\u03c9T \u2212 \u03b71D ( 2 diag(\u2207q)HT\u03c9 +\u2207s )T HT = V ( U \u2212 2\u03b7UH diag(\u2207q)HT ) \u2212 \u03b7Y\u030a HT + 1D\u03c9T \u2212 \u03b71D ( 2 diag(\u2207q)HT\u03c9 +\u2207s )T HT = V U \u2212 2\u03b7V UH diag(\u2207q)HT \u2212 \u03b7Y\u030a HT + 1D\u03c9T \u2212 \u03b71D ( 2 \u03c9TH diag(\u2207q) +\u2207Ts ) HT\n= (V U + 1D\u03c9 T )\u2212 2\u03b7V UH diag(\u2207q)HT \u2212 \u03b7Y\u030a HT \u2212 \u03b71D ( 2 \u03c9TH diag(\u2207q) +\u2207Ts ) HT\n= W \u2212 2\u03b7V UH diag(\u2207q)HT \u2212 \u03b7Y\u030a HT \u2212 2\u03b71D\u03c9TH diag(\u2207q)HT \u2212 \u03b71D\u2207Ts HT = W \u2212 2\u03b7V UH diag(\u2207q)HT \u2212 2\u03b71D\u03c9TH diag(\u2207q)HT \u2212 \u03b71D\u2207Ts HT \u2212 \u03b7Y\u030a HT = W \u2212 2\u03b7 ( V UH diag(\u2207q)HT + 1D\u03c9TH diag(\u2207q)HT ) \u2212 \u03b71D\u2207Ts HT \u2212 \u03b7Y\u030a HT = W \u2212 2\u03b7 ( V U + 1D\u03c9 T ) H diag(\u2207q)HT \u2212 \u03b71D\u2207Ts HT \u2212 \u03b7Y\u030a HT\n= W \u2212 2\u03b7WH diag(\u2207q)HT \u2212 \u03b71D\u2207Ts HT \u2212 \u03b7Y\u030a HT\n= W \u2212 \u03b7 ( 2WH diag(\u2207q) + 1D\u2207Ts + Y\u030a ) HT\n= Wnew\n4.5 Adapting the computation of loss L and gradient \u2207H to the factorized representation\nLet us now adapt the computation of loss L and gradient \u2207H now that we no longer have an explicit W but rather store it implicitly as W = V U + 1D\u03c9T .\n4.5.1 Loss L\nComputing the total loss L over a minibatch implies computing L = sum(~L) = sum ( [`(qj , sj ,Kj,, Aj , Tj)]j=1...m ) as previously seen in Eq. 16 and Eq. 17. Index matrix K and associated target matrix T are the same as before. Vectors q and s can be computed cheaply as previously using Eq. 14 and 15 provided we have kept an up-to-date Q and w\u0304 (we shall see how to update them effectively in the next section). So to be able to compute loss L using this factored representation of W it remains only to adapt the computation of K \u00d7 m matrix A. This matrix was defined in Eq. 13 as Akj = OKkj ,j = (Hj) TWKkj\u2022. Replacing W by its factored expression we can write\nAkj = (Hj) T ( V U + 1D\u03c9 T ) Kkj\u2022\n= (Hj) T (V U)Kkj\u2022 + (Hj)\nT ( 1D\u03c9 T ) Kkj\u2022\n= (Hj) T (V U)Kkj\u2022 + (Hj) T\u03c9 = (Hj) T ( (V U)T ) Kkj + (Hj) T\u03c9 = (Hj) T ( UTV T ) Kkj + (Hj) T\u03c9 = (Hj) TUT ( V T ) Kkj + (Hj) T\u03c9 = (UHj) T ( V T ) Kkj + (Hj) T\u03c9 = ((UH)j) TVKkj\u2022 + (Hj) T\u03c9\n= ((UH\ufe38\ufe37\ufe37\ufe38 H\u0303\n)j) TVKkj\u2022 + (H T\u03c9\ufe38 \ufe37\ufe37 \ufe38 h\u0303 )j\nIn summary, having computed\nH\u0303 = UH (28)\nand\nh\u0303 = HT\u03c9 (29)\nwe can efficiently compute the elements ofK\u00d7mmatrixA by accessing only the rows of V whose indexes are in Kas follows:\nAkj = (H\u0303j) TVKkj\u2022 + h\u0303j (30)\n4.5.2 Gradient \u2207H Let us now adapt the computation of the gradient with respect to H , starting from previous Eq. 21 i.e.\u2207H = 2QH diag(\u2207q) + Z\u0302 with Z\u0302 = w\u0304\u2207Ts +WT Y\u030a .\nSupposing we have kept an up-to-date Q and w\u0304 (we shall see how to update them effectively in the section 4.6), we are left with only adapting the computation of the WT Y\u030a term to use the factored representation of W :\nZ\u0302 = w\u0304\u2207Ts +WT Y\u030a = w\u0304\u2207Ts + ( V U + 1D\u03c9 T )T Y\u030a\n= w\u0304\u2207Ts + UTV T Y\u030a + \u03c91TDY\u030a = w\u0304\u2207Ts + UT (V T Y\u030a ) + \u03c9(Y\u030a T1D)T = w\u0304\u2207Ts + UT (V T Y\u030a ) + \u03c9y\u0304T (31)\nprovided we defined\ny\u0304 = Y\u030a T1D = rowsum(Y\u030a ) = rowsum(\u2207A) (32)\nWe see that computing d \u00d7 m matrix Z\u0302 in this manner can be achieved efficiently using our factored representation V,U and \u03c9. Note that computing V T Y\u030a is a multiplication by sparse matrix Y\u030a which will have a computational complexity of O(Kdm), and yield a d \u00d7 m matrix. The computation of Z\u0302 in this manner thus has aO(dm+ d2m+Kdm+ dm) complexity.\nWe can then proceed to computing\u2207H as in Eq. 21:\n\u2207H = 2QH\ufe38\ufe37\ufe37\ufe38 H\u0302 diag(\u2207q) + Z\u0302 (33)\n4.6 Bookkeeping operations: keeping up-to-date w\u0304 and Q We have shown in section 4.4 that our updates to V,U, \u03c9 (Eq. 27,25,26) achieve the same update on (an implicit)W as Eq. 23, i.e. Wnew = W\u2212\u03b7 ( 2WH diag(\u2207q) + 1D\u2207Ts + Y\u030a ) HT . The efficient computation of loss L and gradient\u2207H seen in Section 4.5 relies on having an up-to-dateQ = WTW and w\u0304 = rowsum(W ) = (sum(W1), . . . , sum(Wd)) = WT1D. In this section, we derive efficient updates to w\u0304 and Q that reflect the update to W .\n4.6.1 Update of w\u0304\nw\u0304new = W T new1D\n= ( W \u2212 \u03b7 ( 2WH diag(\u2207q) + 1D\u2207Ts + Y\u030a ) HT )T 1D = WT1D \u2212 \u03b7H ( 2WH diag(\u2207q) + 1D\u2207Ts + Y\u030a )T 1D\n= w\u0304 \u2212 \u03b7H ( 2diag(\u2207q)HTWT +\u2207s1TD + Y\u030a T ) 1D\n= w\u0304 \u2212 2\u03b7Hdiag(\u2207q)HTWT1D \u2212 \u03b7H\u2207s1TD1D \u2212 \u03b7H Y\u030a T1D\ufe38 \ufe37\ufe37 \ufe38 y\u0304\n= w\u0304 \u2212 2\u03b7Hdiag(\u2207q)HT w\u0304 \u2212 \u03b7DH\u2207s \u2212 \u03b7Hy\u0304 = w\u0304 \u2212 \u03b7H ( 2diag(\u2207q)HT w\u0304 \u2212 \u03b7D\u2207s \u2212 \u03b7y\u0304 ) (34)\n4.6.2 Update of Q\nQnew = W T newWnew\n= ( W \u2212 \u03b7\u2207OHT )T ( W \u2212 \u03b7\u2207OHT ) = WTW \u2212WT ( \u03b7\u2207OHT ) \u2212 ( \u03b7\u2207OHT )T W + \u03b72 ( \u2207OHT )T \u2207OHT\n= WTW\ufe38 \ufe37\ufe37 \ufe38 Q \u2212\u03b7WT\u2207O\ufe38 \ufe37\ufe37 \ufe38 \u2207H HT \u2212 \u03b7(WT\u2207O\ufe38 \ufe37\ufe37 \ufe38 \u2207H HT )T + \u03b72H\u2207TO\u2207OHT\nQnew = Q\u2212 \u03b7 ( \u2207HHT ) \u2212 \u03b7 ( \u2207HHT )T + \u03b72H(\u2207TO\u2207O\ufe38 \ufe37\ufe37 \ufe38\nM\n)HT (35)\nwhere we used the fact that WTW = Q and\u2207H = WT\u2207O. Note that while computing \u2207OHT would be a prohibitive O(mDd) computation (in addition to requiring to explicitly compute \u2207O in the first place), computing \u2207HHT is a comparatively cheap O(md2) operation.\nIt remains to derive a way to efficiently compute m \u00d7 m matrix M = \u2207TO\u2207O without explicitly computing O nor resorting to explicit W . Substituting \u2207O by its expression from Eq. 19 i.e. \u2207O = 2O diag(\u2207q) + 1D\u2207Ts + Y\u030a yields\nM = \u2207TO\u2207O M = ( 2O diag(\u2207q) + 1D\u2207Ts + Y\u030a )T ( 2O diag(\u2207q) + 1D\u2207Ts + Y\u030a )\nM = ( (2O diag(\u2207q))T + ( 1D\u2207Ts + Y\u030a )T)( (2O diag(\u2207q)) + ( 1D\u2207Ts + Y\u030a )) M = (2O diag(\u2207q))T (2O diag(\u2207q)) + ( 1D\u2207Ts + Y\u030a )T ( 1D\u2207Ts + Y\u030a )\n+ (2O diag(\u2207q))T ( 1D\u2207Ts + Y\u030a ) + ( 1D\u2207Ts + Y\u030a )T (2O diag(\u2207q))\nM = ( 4diag(\u2207q)OTO diag(\u2207q) ) + ( \u2207s1TD1D\u2207Ts + Y\u030a T Y\u030a +\u2207s1TDY\u030a + Y\u030a T1D\u2207Ts )\n+ (2O diag(\u2207q))T ( 1D\u2207Ts + Y\u030a ) + ( 1D\u2207Ts + Y\u030a )T (2O diag(\u2207q))\nM = 4diag(\u2207q)OTO diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+ (2O diag(\u2207q))T ( \u2207s1TD + Y\u030a T )T + ( \u2207s1TD + Y\u030a T ) (2O diag(\u2207q))\nM = 4diag(\u2207q)OTO diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+ (( \u2207s1TD + Y\u030a T ) (2O diag(\u2207q)) )T + (( \u2207s1TD + Y\u030a T ) (2O diag(\u2207q)) )\nM = 4diag(\u2207q)OTO diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+ ( 2\u2207s1TDO diag(\u2207q) + 2Y\u030a TO diag(\u2207q) )T + ( 2\u2207s1TDO diag(\u2207q) + 2Y\u030a TO diag(\u2207q) )\nSinceO = WH we haveOTO = HTWTWH = HTQH and 1TDO = 1 T DWH =\nw\u0304TH . Substituting these in the above expression of M we obtain\nM = 4diag(\u2207q) HTQH\ufe37 \ufe38\ufe38 \ufe37 OTO diag(\u2207q) + (D\u2207s\u2207Ts + M\u030a\ufe37 \ufe38\ufe38 \ufe37 Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+(2\u2207s 1TDO\ufe38 \ufe37\ufe37 \ufe38 w\u0304TH diag(\u2207q) + 2Y\u030a T O\ufe38\ufe37\ufe37\ufe38 WH diag(\u2207q))T + (2\u2207s 1TDO\ufe38 \ufe37\ufe37 \ufe38 w\u0304TH diag(\u2207q) + 2Y\u030a T O\ufe38\ufe37\ufe37\ufe38 WH diag(\u2207q))\nM = 4diag(\u2207q)HTQH diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+2 ( \u2207sw\u0304TH diag(\u2207q) + Y\u030a TWH diag(\u2207q) )T + 2 ( \u2207sw\u0304TH diag(\u2207q) + Y\u030a TWH diag(\u2207q) )\nM = 4diag(\u2207q)HTQH diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+2 (( \u2207sw\u0304T + Y\u030a TW ) H diag(\u2207q) )T + 2 (( \u2207sw\u0304T + Y\u030a TW ) H diag(\u2207q) )\nM = 4diag(\u2207q)HTQH diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+2(diag(\u2207q)HT (w\u0304\u2207Ts +WT Y\u030a )\ufe38 \ufe37\ufe37 \ufe38 Z\u0302 +2(diag(\u2207q)HT (w\u0304\u2207Ts +WT Y\u030a )\ufe38 \ufe37\ufe37 \ufe38 Z\u0302 )T .\nReusing previously defined Z\u0302 = w\u0304\u2207Ts +WT Y\u030a that were already part of the computation of \u2207H (see Eq. 33 in section 4.5.2), we can thus compute M efficiently as\nM = 4diag(\u2207q) M\u0302\ufe37 \ufe38\ufe38 \ufe37 HTQH diag(\u2207q) + ( D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts )\n+2 ( diag(\u2207q)HT Z\u0302 ) + 2 ( diag(\u2207q)HT Z\u0302 )T\n(36)\nNote that computing M requires computing Y\u030a T Y\u030a , a m\u00d7m matrix, each element of which is the dot product between two K \u2212 sparse columns of sparse matrix Y\u030a so that it can be computed in O(m2K).\nHaving M we can then update Q using Eq. 35.\n4.7 Bookkeeping operations: tracking U\u2212T\nWe can updateU\u2212T to reflect our rank-m update of U in step a), using the Woodbury identity.\n4.8 Putting it all together In this section, we put together all the operations that we have derived to write the minibatch version of the update algorithm for general spherical losses.\nThe parameters of the output layer that we will learn are V,U, \u03c9 and implicitly represent W as W = V U + 1D\u03c9T .\nThe algorithm will work for any spherical loss function ` in canonical form that computes `(q, s,K,,a, t) and for which we can compute gradients with respect to its parameters.\nInitialization \u2022 we can initialize D \u00d7 d matrix V randomly as we would have initialized W so that\nwe initially have V = W . Alternatively we can initialize V to 0 (there won\u2019t be symmetry breaking issues with having W initially be 0 provided the other layers are initialized randomly, since varying inputs and targets will naturally break symmetry for the output layer) \u2022 we initialize U to the identity: U \u2190 Id \u2022 and \u03c9 to zero \u03c9 \u2190 0d so that, trivially, we initially have V U + 1D\u03c9T = W . \u2022 initialize U\u2212T \u2190 Id \u2022 initialize Q \u2190 WTW = V TV (or more cheaply initialize Q \u2190 0 if we have\ninitialized V to 0). \u2022 initialize w\u0304 = WT1D = rowsum(W ) = rowsum(V ) (or more cheaply w\u0304 \u2190 0 if\nwe have initialized V to 0).\nMinibatch update algorithm for arbitrary spherical loss Inputs (besides above parameters V,U, \u03c9 and bookkeeping variables Q,U\u2212T , w\u0304): \u2022 H : a d \u00d7m matrix whose m columns contain the last hidden layer representation\nvectors form example (with an appended constant 1 element to account for an output bias). \u2022 Y : a D \u00d7 m sparse target matrix that uses sparse representation (K, T ) so that YKkj ,j = Tkj for k \u2208 {1, . . . ,K} and j \u2208 {1, . . . ,m}. Each of the m columns of Y is the K-sparse target vector associated to one example of the minibatch. \u2022 \u03b7 \u2208 R+learning rate for the update Updates: \u2022 parameters and bookkeeping matrices U, V, \u03c9,Q,U\u2212T , w\u0304 Returns: \u2022 L \u2208 R the sum of squared error losses for the m examples of the minibatch \u2022 \u2207H a d \u00d7m matrix whose m columns contain the gradient of the loss with respect\nto H , to further backpropagate upstream.\nThe detailed algorithm is given as Algorithm 4 Counting the total number of basic operations of the update algorithm yields roughly 8md2 +m3 + 7m2d+ 2mKd+ 3d2 \u2248 17md2 operations. Comparing this17md2 to the 3Dm of the naive update, the expected theoretical speedup is approximately 3D18d = 1 6 D d\nFor d = 512 and D = 793471 this yields a theoretical speedup of 258 Note that in the special cases where the specific loss function ` does not depend on the sum of outputs s (as is the case e.g. of the squared error) then we don\u2019t need to compute s, and can use a\u03c9 that is always 0 so there\u2019s a lot we don\u2019t need to compute and update.\n5 Controlling numerical stability The update of U may over time lead to U becoming ill-conditioned. Simultaneously, as we update U and U\u2212T (using Sherman-Morrison or Woodbury) our updated U\u2212Tmay over time start to diverge from the true U\u2212T due to numerical precision. It is thus important to prevent both of these form happening, i.e. make sure U stays well conditioned, to ensure the numerical stability of the algorithm. We present here progressively refined strategies for achieving this.\n5.1 Restoring the system in a pristine stable state One simple way to ensure numerical stability is to once in a while restore the system in its pristine state where V = W and U = Id = U\u2212T . This is easily achieved as follows:\nV \u2190 V U U \u2190 Id\nU\u2212T \u2190 Id.\nThis operation doesn\u2019t affects the product V U , so the implicit matrix W remains unchanged, nor does it affect Q = WTW . And it does restore U to a perfectly well conditioned identity matrix. But computing V U is an extremely costly O(Dd2) operation, so if possible we want to avoid it (except maybe once at the very end of training, if we want to compute the actual W ). In the next paragraphs we develop a more efficient strategy.\n5.2 Stabilizing only problematic singular values U becoming ill-conditioned is due to its singular values over time becoming too large and/or too small. Let use define \u03c31, . . . , \u03c3d as the singular values of U ordered in decreasing order. The conditioning number of U is defined as \u03c31\u03c3d and it can become overly large when \u03c31 becomes too large and/or when \u03c3d becomes too small. Restoring\nAlgorithm 4 Minibatch version of the update algorithm for general spherical loss FUNCTION spherical_minibatch_fbprop_update:\nInputs: hidden layer minibatch\ufe37\ufe38\ufe38\ufe37 H , sparse target\ufe37\ufe38\ufe38\ufe37 K, T , learning rate\ufe37\ufe38\ufe38\ufe37 \u03b7 , layer parameters\ufe37 \ufe38\ufe38 \ufe37 V,U, \u03c9 , bookkeeping variables\ufe37 \ufe38\ufe38 \ufe37 Q, w\u0304, U\u2212T Updates: V,U, \u03c9,Q, w\u0304, U\u2212T Returns: loss L, gradient\u2207H to backpropagate further upstream Operations main\ntext Eq. result dims\n# ops\nH\u0302 = QH Eq. 14 d\u00d7m md2 M\u0302 = HT H\u0302 Eq. 14 m\u00d7m q = diag(M\u0302 ) Eq. 14 m m\ns = HT w\u0304 Eq. 15 m md H\u0303 = UH Eq. 28 d\u00d7m md2 h\u0303 = HT\u03c9 Eq. 29 m md MatrixA: Akj = (H\u0303j)TVKkj\u2022 + h\u0303j Eq. 30 K \u00d7m mKd ~L = [`(qj , sj ,Kj,, Aj , Tj)]j=1...m Eq. 16 m typically\nO(Km)\nL = sum(~L) 1 m \u2207q = [ \u2202` \u2202q (qj , sj ,Kj,, Aj , Tj) ] j=1...m m\n\u2207s = [ \u2202` \u2202s (qj , sj ,Kj,, Aj , Tj) ] j=1...m\nm\n\u2207A =[ \u2202` \u2202ak (qj , sj ,Kj,, Aj , Tj) ] k=1...K, j=1...m\nK \u00d7m\nY\u030a = sparsematD,m(K,\u2207A) D \u00d7m (Ksparse) y\u0304 = Y\u030a T1D = rowsum(\u2207A) Eq. 32 m Km Z\u0302 = w\u0304\u2207Ts + UT (V T Y\u030a ) + \u03c9y\u0304T Eq. 31 d\u00d7m md \u2207H = 2H\u0302 diag(\u2207q) + Z\u0302 Eq. 33 d\u00d7m md U \u2190 U \u2212 2\u03b7(UH\ufe38\ufe37\ufe37\ufe38\nH\u0303\n) diag(\u2207q)HT Eq. 25 d\u00d7 d md2\nU\u2212T \u2190 ... use Woodbury Identity to update it. d\u00d7 d 2m2d+ m3 +\n2md2\n\u03c9 \u2190 \u03c9 \u2212 \u03b7H(2diag(\u2207q)HT\u03c9\ufe38 \ufe37\ufe37 \ufe38 h\u0303 +\u2207s) Eq. 26 d 2md+3d V \u2190 V \u2212 \u03b7Y\u030a (U\u2212TnewH)T Eq. 27 D \u00d7 d md2 + mKd\nw\u0304 \u2190 w\u0304 \u2212 \u03b7H ( 2diag(\u2207q)HT w\u0304 +D\u2207s + y\u0304 ) Eq. 34 d 2md+4d M = 4diag(\u2207q)M\u0302 diag(\u2207q) +D\u2207s\u2207Ts + Y\u030a T Y\u030a +\u2207sy\u0304T + y\u0304\u2207Ts +2 ( diag(\u2207q)HT Z\u0302 ) +2 ( diag(\u2207q)HT Z\u0302 )T Eq. 36 m\u00d7m 2m2d+ (5 + K)m2 +\nd2\nQ\u2190 Q\u2212 \u03b7\u2207HHT \u2212 \u03b7H\u2207TH + \u03b72(HM)HT Eq. 35 d\u00d7 d md2 + 2m2d+\n2d2\nRETURN L,\u2207H 28\nthe system in its pristine state, as shown in the previous paragraph, in effect brings back all singular values of U back to 1 (since it brings back U to being the identity). It is instead possible, and computationally far less costly, to correct when needed only for the singular values of U that fall outside a safe range. Most often we will only need to occasionally correct for one singular value (usually the smallest, and only when it becomes too small). Once we have determined the offending singular value and its corresponding singular vectors, correcting for that singular value, i.e. effectively bringing it back to 1, will be a O(Dd) operation. The point is to apply corrective steps only on the problematic singular values and only when needed, rather than blindly, needlessly and inefficiently correcting for all of them through the basic O(Dd2) full restoration explained in the previous paragraph.\nHere is the detailed algorithm that achieves this:\nAlgorithm 5 Numerical stabilization procedure for problematic singular values \u2022 The chosen safe range for singular values is [\u03c3low, \u03c3high] (ex: [0.001, 100] ) \u2022 The procedures given below act on output layer parameters U , U\u2212T and V . \u2022 For concision, we do not enlist these parameters explicitly in their parameter list. \u2022 Procedure SINGULAR-STABILIZE gets called after every ncheck gradient updates (ex: ncheck = 100). procedure SINGULAR-STABILIZE( )\nU\u0304, \u03c3, V\u0304 = SVD(U ) . Computes singular value decomposition of U as U = U\u0304 diag(\u03c3) V\u0304T\nfor all k \u2208 {1, . . . , d} do if \u03c3k < \u03c3low OR \u03c3k > \u03c3high then\nFIX-SINGULAR-VALUE(\u03c3k, U\u0304k, 1) end if\nend for end procedure\nThe following procedure will change singular value \u03c3 of U associated to singular vector u to become target singular value \u03c3\u2217 (typically 1). It doesn\u2019t change U \u2019s singular vectors, only that one singular value. It also changes V symetrically (with a rank-one update) in such a way that W = V U remains unchanged.\nprocedure FIX-SINGULAR-VALUE(\u03c3, u, \u03c3\u2217) \u03b1 = \u03c3\n\u2217\u2212\u03c3 \u03c3\n\u03b2 = \u2212 \u03b11+\u03b1 U \u2190 U + \u03b1u(UTu)T V \u2190 V + \u03b2(V u)uT U\u2212T \u2190 U\u2212T + \u03b2u(U\u22121u)T . Where U\u22121 is obtained as the transpose of U\u2212T . But we may instead of this prefer to recompute U\u2212T from scratch by inverting U to ensure it doesn\u2019t stray too much due to numerical imprecisions. end procedure\nProof that W = V U is left unchanged by FIX-SINGULAR-VALUE\nVnewUnew = (V + \u03b2(V u)u T ) (U + \u03b1u(UTu)T )\n= V (Id + \u03b2uu T ) (U + \u03b1uuTU) = V (Id + \u03b2uu T ) (Id + \u03b1uu T )U = V (I2d + \u03b2uu T + \u03b1uuT + \u03b2\u03b1uuTuuT )U = V (I2d + (\u03b1+ \u03b2)uu T + \u03b2\u03b1u(uTu)uT )U = V (Id + (\u03b1+ \u03b2)uu T + \u03b2\u03b1uuT )U = V (Id + (\u03b1\u2212 \u03b1\n1 + \u03b1 + \u03b1 \u2212\u03b1 1 + \u03b1 )uuT )U\n= V (Id + (\u03b1\u2212 \u03b1 1 + \u03b1 \u2212 \u03b1\n2\n1 + \u03b1 )uuT )U\n= V (Id + (\u03b1\u2212 \u03b1+ \u03b12\n1 + \u03b1 )uuT )U\n= V (Id + (\u03b1\u2212 \u03b1(1 + \u03b1)\n1 + \u03b1 )uuT )U\n= V (Id + (\u03b1\u2212 \u03b1)uuT )U = V IdU\n= V U\n5.3 Avoiding the cost of a full singular-value decomposition Computing the SVD of d\u00d7 d matrix U as required above, costs roughly 25d3 elementary operations (use the so-called R-SVD algorithm). But since the offending singular values will typically be only the smallest or the largest, it is wasteful to compute all d singular values every time. A possibly cheaper alternative is to use the power iteration method with U to find its largest singular value and associated singular vector, and similarly with U\u22121to obtain the smallest singular value of U (which corresponds to the inverse of the largest singular value of U\u22121). Each iteration of the power iteration method requires only O(d2) operations, and a few iterations may suffice. In our experiments we fixed it to 100 power iterations. Also it is probably not critical if the power iteration method is not run fully to convergence, as correcting along an approximate offending singular vector direction may be sufficient for the purpose of ensuring numerical stability.\nWith this refinement, we loop over finding the smallest singular value with the power iteration method, correcting for it to be 1 by calling FIX-SINGULAR-VALUE if it is too small, and we repeat this until we find the now smallest singular value to be inside the acceptable range. Similarly for the largest singular values.\nNote that while in principle we may not need to ever invert U from scratch (as we provided update formulas of U\u2212T with every change we make to U ), it nevertheless proved to be necessary to do so regularly to ensure U\u2212T doesn\u2019t stray too much from the correct value due to numerical imprecisions. Inverting U using Gaussianelimination costs roughly d3 operations, so it is very reasonable and won\u2019t affect the\ncomputational complexity if we do it no more often than every d training examples (which will typically correspond to less than 10 minibatches of size 128). In practice, we recompute U\u2212T from scratch every time before we run this check for singular value stabilization.\n6 Experimental validation We implemented both a CPU version using blas and a parallel GPU (Cuda) version using cublas of the proposed algorithm4. We evaluated the GPU and CPU implementations by training word embeddings with simple neural language models, in which a probability map of the next word given its preceding n-gram is learned by a neural network. We used a Nvidia Titan Black GPU and a i7-4820K @ 3.70GHz CPU and ran experiments on the one billion word dataset[? ], which is composed of 0.8 billions words belonging to a vocabulary of 0.8 millions words. We evaluated the resulting word embeddings with the recently introduced Simlex-999 score [? ], which measures the similarity between words. We also compared our approach to unfactorised versions and to a two-layer hierarchical softmax. Figure 2 and 3 (left) illustrate the practical speedup of our approach for the output layer only. Figure 3(right) shows that the LST (Large Sparse Target) models are much faster to train than the softmax models and converge to only slightly lower Simlex-999 scores. Table 1 summarizes the speedups for the different output layers we tried, both on CPU and GPU. We also emprically verified that our proposed factored algorithm learns the exact same model weights (V U) as the corresponding naive unfactored algorithm\u2019s W , as it theoretically should (up to negligible numerical precision differences), and followed the exact same learning curves (as a function of number of iterations, not time!).\n7 Conclusion and future work We introduced a new algorithmic approach to efficiently compute the exact gradient updates for training deep networks with very large sparse targets. Remarkably the complexity of the algorithm is independent of the target size, which allows tackling\n4Open source code will be released upon official publication of this research.\nvery large problems. Our CPU and GPU implementation yield similar speedups to the theoretical one and can thus be used in practical applications, which could be explored in further work. In particular, neural language models seem good candidates. But it remains unclear how using a loss function other than log-softmax may affect the quality of the resulting word embeddings and further research should be carried out in this direction. While restricted, the spherical family of loss functions, offers opportunities to explore alternatives to the ubiquitous softmax, that thanks to the algorithm presented here, could scale computationally to extremely large output spaces.\nAcknowledgements We would like to thank the developers of Theano [14, 15] and Blocks [16].\nThis research is supported by NSERC and Ubisoft.\nReferences [1] Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language\nmodel. In NIPS\u201900, pages 932\u2013938. MIT Press, 2001.\n[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12: 2493\u20132537, 2011.\n[3] Y. Dauphin, X. Glorot, and Y. Bengio. Large-scale learning of embeddings with reconstruction sampling. In Proceedings of the 28th International Conference on Machine learning, ICML \u201911, 2011.\n[4] S\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In ACL-IJCNLP\u20192015, 2015. arXiv:1412.2007.\n[5] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910), 2010.\n[6] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noisecontrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265\u2013 2273. Curran Associates, Inc., 2013.\n[7] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS\u20192013, pages 3111\u20133119. 2013.\n[8] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2321\u20132329. Curran Associates, Inc., 2014.\n[9] Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large output spaces. arxiv:1412.7479, 2014.\n[10] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 246\u2013252. Society for Artificial Intelligence and Statistics, 2005.\n[11] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning representations by backpropagating errors. Nature, 323:533\u2013536, 1986.\n[12] Yann LeCun. Une proc\u00e9dure d\u2019apprentissage pour R\u00e9seau \u00e0 seuil assym\u00e9trique. In Cognitiva 85: A la Fronti\u00e8re de l\u2019Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences, pages 599\u2013604, Paris 1985, 1985. CESTA, Paris.\n[13] Yann LeCun. Learning processes in an asymmetric threshold network. In E. Bienenstock, F. Fogelman-Souli\u00e9, and G. Weisbuch, editors, Disordered Systems and Biological Organization, pages 233\u2013240. Springer-Verlag, Berlin, Les Houches 1985, 1986.\n[14] James Bergstra, Olivier Breuleux, Fr\u00e9d\u00e9ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), 2010. Oral Presentation.\n[15] Fr\u00e9d\u00e9ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.\n[16] B. van Merri\u00ebnboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-Farley, J. Chorowski, and Y. Bengio. Blocks and Fuel: Frameworks for deep learning. ArXiv e-prints, jun 2015."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent"], "venue": "In NIPS\u201900,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Large-scale learning of embeddings with reconstruction sampling", "author": ["Y. Dauphin", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In ACL-IJCNLP\u20192015,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen"], "venue": "In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS\u20192013,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Deep networks with large output spaces", "author": ["Sudheendra Vijayanarasimhan", "Jonathon Shlens", "Rajat Monga", "Jay Yagnik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Learning representations by backpropagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Une proc\u00e9dure d\u2019apprentissage pour R\u00e9seau \u00e0 seuil assym\u00e9trique", "author": ["Yann LeCun"], "venue": "In Cognitiva 85: A la Frontie\u0300re de l\u2019Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1985}, {"title": "Learning processes in an asymmetric threshold network", "author": ["Yann LeCun"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2].", "startOffset": 264, "endOffset": 267}, {"referenceID": 2, "context": "[3], the efficient use of biased importance sampling in Jean et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "[7] all fall under this category.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.", "startOffset": 118, "endOffset": 124}, {"referenceID": 8, "context": "As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.", "startOffset": 118, "endOffset": 124}, {"referenceID": 9, "context": "\u2022 Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 30}, {"referenceID": 6, "context": "\u2022 Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 30}, {"referenceID": 10, "context": "[11], LeCun [12, 13] to efficiently compute the gradients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11], LeCun [12, 13] to efficiently compute the gradients.", "startOffset": 12, "endOffset": 20}, {"referenceID": 12, "context": "[11], LeCun [12, 13] to efficiently compute the gradients.", "startOffset": 12, "endOffset": 20}, {"referenceID": 0, "context": "in Neural Language Models [1] with large vocabulary size (e.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 79, "endOffset": 85}, {"referenceID": 4, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 79, "endOffset": 85}, {"referenceID": 5, "context": "\u2023 Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 29}, {"referenceID": 3, "context": "\u2023 Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "[1] Bengio, Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Dauphin, Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Mnih, A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Morin, F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Gutmann, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Mikolov, T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We would like to thank the developers of Theano [14, 15] and Blocks [16].", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "We would like to thank the developers of Theano [14, 15] and Blocks [16].", "startOffset": 48, "endOffset": 56}, {"referenceID": 15, "context": "We would like to thank the developers of Theano [14, 15] and Blocks [16].", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of sizeD (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating theD\u00d7 d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d) per example instead of O(Dd), remarkably without ever computing theD-dimensional output. The proposed algorithm yields a speedup of D 4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "creator": "LaTeX with hyperref package"}}}