{"id": "1606.08165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Supervised learning based on temporal coding in spiking neural networks", "abstract": "gradient descent training ensemble techniques are remarkably successful in explicitly training analog - valued unstable artificial neural tracking networks ( anns ). such training techniques, however, can don't adequately transfer easily to spiking nerve networks due actually to employing the spike generation hard non - linearity theorem and the discrete linear nature law of underlying spike communication. conversely we show that and in a partially feedforward independent spiking run network that quickly uses a temporal coding scheduling scheme where information is directly encoded in spike times instead of spike rates, the exact network dynamics input - relative output relation of is differentiable exponential almost everywhere. moreover, this relation is predominantly locally linear after adding a characteristic transformation graph of variables. methods described for tracing training naive anns automatically thus carry directly to manipulate the training curve of stationary such spiking networks almost as now we continually show when training operators on the characteristic permutation marko invariant finite mnist task. in contrast to average rate - mod based spiking networks tasks that are often used to approximate the behavior of nonlinear anns, besides the coupled networks above we present uniformly spike much more sparsely and that their behavior statistics can better not be directly well approximated by specific conventional anns. our results highlight a remarkable new approach asking for patiently controlling the behavior of spiking neurons networks with realistic temporal bandwidth dynamics, opening up the potential alternatives for using learning these networks to process future spike patterns with complex temporal lobe information.", "histories": [["v1", "Mon, 27 Jun 2016 08:58:29 GMT  (214kb,D)", "http://arxiv.org/abs/1606.08165v1", null], ["v2", "Wed, 16 Aug 2017 16:15:20 GMT  (321kb,D)", "http://arxiv.org/abs/1606.08165v2", "Extended the discussion and introduction. Clarified the training parameters"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["hesham mostafa"], "accepted": false, "id": "1606.08165"}, "pdf": {"name": "1606.08165.pdf", "metadata": {"source": "CRF", "title": "Supervised learning based on temporal coding in spiking neural networks", "authors": ["Hesham Mostafa"], "emails": ["hesham@ini.uzh.ch"], "sections": [{"heading": "1 Introduction", "text": "Artificial neural networks (ANNs) are enjoying great success as a means of learning complex nonlinear transformations by example [1]. The idea of a distributed network of simple neuron elements that adaptively adjusts its connection weights based on training examples is partially inspired by the operation of biological spiking networks [2]. ANNs, however, are fundamentally different from spiking networks. Unlike ANN neurons that are analog-valued, spiking neurons communicate using all-or-nothing discrete spikes. A spike triggers a trace of synaptic current in the target neuron. The target neuron integrates synaptic current over time until a threshold is reached, and then emits a spike and resets. Spiking networks are dynamical systems in which time plays a crucial role, while time is abstracted away in conventional feedforward ANNs.\nWhile there exist well-developed techniques for training feedforward ANNs (mainly based on gradient descent), there is no general technique for training feedforward spiking neural networks. An approach that bears some similarities to ours is the SpikeProp algorithm [3] that can be used to train spiking networks to produce output spikes at specific times. SpikeProp assumes a connection between two spiking neurons consists of a number of sub-connections, each with a different delay and a trainable weight. We use a more conventional network model that does not depend on combinations of pre-specified delay elements to transform input spike times to output spike times, and instead relies only on simple neural and synaptic dynamics. Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6]. The spiking networks obtained using these methods use rate coding where the spiking rate of a neuron encodes an analog quantity corresponding to the analog output of an ANN neuron.\nar X\niv :1\n60 6.\n08 16\n5v 1\n[ cs\n.N E\n] 2\n7 Ju\nn 20\nIn this paper, we develop a direct training approach that does not try to reduce spiking networks to conventional ANNs. Instead, we relate the time of any spike differentiably to the times of all spikes that had a causal influence on its generation. We can then impose any differentiable cost function on the spike times of the network and minimize this cost function directly through gradient descent. By using spike times as the information-carrying quantities, we avoid having to work with discrete spike counts or spike rates, and instead work with a continuous representation (spike times) that is amenable to gradient descent training. This training approach allows detailed control over the behavior of the network (at the level of single spike times) which would not be possible in training approaches based on rate-coding.\nSince we use a temporal spike code, neuron firing can be quite sparse as each spike carries significant information. Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9]. The behavior of the networks we present deviates quite significantly from that of conventional ANNs. They represent an alternative network paradigm where continuous time is explicitly modeled and used as a coding dimension, but which can still be directly and effectively trained using standard gradient descent methods."}, {"heading": "2 Network model", "text": "We use non-leaky integrate and fire neurons with exponentially decaying synaptic current kernels. The neuron\u2019s membrane dynamics are described by:\ndV jmem(t) dt = \u2211 i wji \u2211 r \u03ba(t\u2212 tri ) (1)\nwhere V jmem is the membrane potential of neuron j. The right hand side of the equation is the synaptic current. wji is the weight of the synaptic connection from neuron i to neuron j and tri is the time of the rth spike from neuron i. \u03ba is the synaptic current kernel given by:\n\u03ba(x) = \u0398(x)exp(\u2212 x \u03c4syn\n) where \u0398(x) = {\n1 if x \u2265 0 0 otherwise\n(2)\nSynaptic current thus jumps instantaneously on the arrival of an input spike, then decays exponentially with time constant \u03c4syn. Since \u03c4syn is the only time constant in the model, we set it to 1 in the rest of the paper, i.e, normalize all times with respect to it. The neuron spikes when its membrane potential crosses a firing threshold which we set to 1, i.e, all synaptic weights are normalized with respect to the firing threshold. The membrane potential is reset to 0 after a spike. We allow the membrane potential to go below zero if the integral of the synaptic current is negative.\nAssume a neuron receives N spikes at times {t1, .., tN} with weights {w1, .., wN} from N source neurons. Each weight can be positive or negative. Assume the neuron spikes in response at time tout. By integrating Eq. 1, the membrane potential for t < tout is given by:\nVmem(t) = N\u2211 i=1 \u0398(t\u2212 ti)wi(1\u2212 exp(\u2212(t\u2212 ti))) (3)\nAssume only a subset of these input spikes with indices in C \u2286 {1, .., N} had arrived before tout where C = {i : ti < tout}. It is only these input spikes that influence the time of the output neuron\u2019s first spike. We call this set of input spikes the causal set of input spikes. The sum of the weights of the causal input spikes has to be larger than 1, otherwise they could not have caused the neuron to fire. From Eq. 3, tout is then implicitly defined as:\n1 = \u2211 i\u2208C wi(1\u2212 exp(\u2212(tout \u2212 ti))) (4)\nwhere 1 is the firing threshold. Hence,\nexp(tout) =\n\u2211 i\u2208C\nwiexp(ti)\u2211 i\u2208C wi \u2212 1 (5)\nSpike times always appear exponentiated. Therefore, we do a transformation of variables exp(tx)\u2192 zx yielding an expression relating input spike times to the time of the first spike of the output neuron in the post-transformation domain (which we denote as the z-domain):\nzout =\n\u2211 i\u2208C\nwizi\u2211 i\u2208C wi \u2212 1 (6)\nNote that for the neuron to spike in the first place, we must have \u2211 i\u2208C wi > 1, so zout is always positive (one can show this is the case even if some of the weights are negative). It is also always larger than any element of {zi : i \u2208 C}, i.e, the output spike time is always larger than any input spike time in the causal set which follows from the definition of the causal set. We can obtain a similar expression relating the time of the Lth spike of the output neuron, zLout, to the input spike times in the z-domain:\nzLout =\n\u2211 i\u2208CL\nwizi\u2211 i\u2208CL wi \u2212 L (7)\nwhere CL is the set of indices of the input spikes that arrive before the Lth output spike. Equation 7 is only valid if the denominator is positive, i.e, there are sufficient input spikes with large enough total positive weight to push the neuron past the firing threshold L times. In the rest of the paper, we consider a neuron\u2019s output value to be the time of its first spike. During training, we use a weight cost term that insures the neuron receives sufficient input to spike as we describe in the next section.\nThe linear relation between input and output spike times in the z-domain is only valid in a local interval. A different linear relation holds when the set of causal input spikes changes. This is illustrated in Fig. 1, where the 4th input spike is part of the causal set in one case but not in the other. From Eq. 6, the effective weight of input zp in the linear input-output relation in the z-domain is wp/(\n\u2211 i\u2208C wi \u2212 1). This effective weight depends on the weights of the spikes in the causal set of\ninput spikes. As this causal set changes due to the changing spike times from the source neurons, the effective weight of the different input spikes that remain in the causal set changes (as is the case for the first three spikes in Figs. 1a and 1b whose effective weight changes as the causal set changes, even though their actual synaptic weights are the same ). For some input patterns, Some source neurons may spike late causing their spikes to leave the causal set of the output neuron and their effective spike weights to become zero. Other source neurons may spike early and influence the timing of the output neuron\u2019s spike and thus their spikes acquire a non-zero effective weight.\nThe causal set of input spikes is dynamically determined based on the input spike times and their weights. Many early spikes with strong positive weights will cause the output neuron to spike early, negating the effect of later spikes on the output neuron\u2019s first spike time regardless of the weights of these later spikes. The non-linear transformation from z = {z1, .., zN} to zout implemented by the spiking neuron is thus fundamentally different from the static non-linearities used in traditional ANNs where only the aggregate weighted input is considered.\nThe non-linear transformation implemented by the spiking neuron is continuous in most case, i.e, small perturbation in z will lead to proportionately small perturbations in zout. This is clear when the perturbations do not change the set of causal input spikes as the same linear relation continues to hold. Consider, however, the case of an input spike with weight wx that occurs just after the output spike at time zx = zout + . A small perturbation pushes this input spike to time zx = zout \u2212 adding it to the causal set. By applying Eq. 6, the perturbed output time is zperturbout = ( \u2211 i\u2208C wizi + wxzx)/( \u2211 i\u2208C wi + wx \u2212 1) where C is the causal set before the perturbation. Substituting for zx, z perturb out \u2212 zout = \u2212 wx/( \u2211 i\u2208C wi + wx \u2212 1). The output perturbation is thus\nproportional to the input perturbation but this is only the case when \u2211 i\u2208C wi + wx > 1, otherwise the perturbed input spike with negative weight at zout \u2212 would cancel the original output spike at zout. In summary, the input spike times to output spike time transformation of the spiking neuron is continuous except in situations where small perturbations affect whether a neuron spikes or not."}, {"heading": "3 Training", "text": "We consider feedforward neural networks where the neural and synaptic dynamics are described by Eqs. 1 and 2. The neurons are arranged in a layer-wise manner. Neurons in one layer project in an all-to-all fashion to neurons in the subsequent layer. A neuron\u2019s output is the time of its first spike and we work exclusively in the z-domain. The forward pass is described in Algorithm 1. All indices are 1-based, vectors are in boldface, and the ith entry of a vector a is a[i]. At each layer, the causal set for each neuron is obtained from the get causal set function. The neuron output is then evaluated according to Eq. 6. The get causal set function is shown in algorithm 2. It first sorts the input spike times, and then considers increasingly larger sets of the early input spikes until it finds a set of input spikes that causes the neuron to spike. The output neuron must spike at a time that is less than the time of any input spike not in the causal set, otherwise the causal set is incomplete. If no such set exists, i.e, the output neuron does not spike in response to the input spikes, get causal set returns the empty set \u03a6 and the neuron\u2019s output spike time is set to infinity (maximum positive value in implementation).\nFrom Eq. 6, the derivatives of a neuron\u2019s first spike time with respect to synaptic weights and input spike times are given by:\ndzout dwp =  zp\u2212zout\u2211 i\u2208C wi\u22121 if p \u2208 C 0 otherwise (8)\ndzout dzp =  wp\u2211 i\u2208C wi\u22121 if p \u2208 C 0 otherwise (9)\nUnlike the spiking neuron\u2019s input-output relation which can still be continuous at points where the causal set changes, the derivative of the neuron\u2019s output with respect to inputs and weights given by Eqs. 8 and 9 is discontinuous at such points. This is not a severe problem for gradient descent methods. Indeed, many feedforward ANNs use activation functions with a discontinuous first derivative such as rectified linear units (ReLUs) [10] while still being effectively trainable.\nA differentiable cost function can be imposed on the spike times generated anywhere in the network. The gradient of the cost function with respect to the weights in lower layers can be evaluated by backpropagating errors through the layers using Eqs. 8 and 9 through the standard backpropagation\nAlgorithm 1 Forward pass in a feedforward spiking network with L layers\n1: Input: z0: Vector of input spike times 2: Input: {N1, .., NL}: Number of neurons in the L layers 3: Input: {W 1, ..,WL}: Set of weight matrices. W l[i, j] is the weight from neuron j in layer l\u2212 1\nto neuron i in layer l 4: Output: zL: Vector of first spike times of neurons in the top layer 5: for r= 1 to L do 6: for i= 1 to Nr do 7: Cri \u2190 get causal set(zr\u22121,W r[i, :]) 8: if Cri 6= \u03a6 then\n9: zr[i]\u2190\n\u2211 k\u2208Cr\ni W r[i,k]zr\u22121[k]\u2211 k\u2208Cr\ni\nW r[i,k]\u22121\n10: else 11: zr[i]\u2190\u221e 12: end if 13: end for 14: end for\nAlgorithm 2 get causal set: Gets indices of input spikes influencing first spike time of output neuron 1: Input: z: Vector of input spike times of length N 2: Input: w: Weight vector of the input spikes 3: Output: C: Causal index set 4: sort indices\u2190 argsort(z) //Ascending order argsort 5: zsorted \u2190 z[sort indices] //sorted input vector 6: wsorted \u2190 w[sort indices] //weight vector rearranged to match sorted input vector 7: for i= 1 to N do 8: if i == N then 9: next input spike\u2190\u221e 10: else 11: next input spike\u2190 zsorted[i+ 1] 12: end if 13: if i\u2211\nk=1\nwsorted[k] > 1 \u2227\ni\u2211 k=1 wsorted[k]zsorted[k]\ni\u2211 k=1 wsorted[k]\u22121 < next input spike then\n14: return {sort indices[1], .., sort indices[i]} 15: end if 16: end for 17: return \u03a6\ntechnique. In the next section, we use the spiking network in a classification setting. In training the network, we had to use the following techniques to enable the networks to learn:\nConstraints on synaptic weights: We add a term to the cost function that heavily penalizes neurons\u2019 input weight vectors whose sum is less than 1. During training, this term pushes the sum of the weights in each neuron\u2019s input weight vector above 1 which ensures that a neuron spikes if all its input neurons spike. This term in the cost function is crucial, otherwise the network can become quiescent and stop spiking. We also use L2 weight regularization.\nGradient normalization: We observed that the gradients can become very large during training. This is due to the highly non-linear relation between the output spike time and the weights when the sum of the weights for the causal set of input spikes is close to 1. This can be seen from Eqs. 6, 8, and 9 where a small denominator can cause the output spike time and the derivatives to diverge. This hurts learning as it causes weights to make very large jumps. We use gradient normalization to counter that: if the Frobenius norm of the gradient of a weight matrix is above a threshold, we scale the matrix so that its Frobenius norm is equal to the threshold before doing the gradient descent step."}, {"heading": "4 Results", "text": "We trained the network to solve two classification tasks: an XOR task, and the permutation invariant MNIST task [11]. We used fully connected feedforward networks where the top layer had as many neurons as the number of classes (2 in the XOR task and 10 in the MNIST task). The goal is to train the network so that the neuron corresponding to the correct class fires first among the top layer neurons. We used the cross-entropy loss and interpreted the value of a top layer neuron as the negative of its spike time (in the z-domain). Thus, by maximizing the value of the correct class neuron value, training effectively pushes this neuron to fire earlier than the neurons representing the incorrect classes. For an output spike times vector zL and a target class index g, the loss function is given by\nL(g, zL) = \u2212ln exp(\u2212z L[g])\u2211\ni\nexp(\u2212zL[i]) (10)\nWe used standard gradient descent to minimize the loss function across the training examples. Training was done using Theano [12, 13]."}, {"heading": "4.1 XOR task", "text": "In the XOR task, two spike sources send a spike each to the network. Each of the two input spikes can occur at time 0.0 (early spike) or 2.0 (late spike). The two input spike sources project to a hidden layer of 4 neurons and the hidden neurons project to two output neurons. The first output neuron must spike before the second output neuron if exactly one input spike is an early spike. The network is shown in Fig. 2a together with the 4 input patterns.\nTo investigate whether the network can robustly learn to solve the XOR task, we repeated the training procedure 1000 times starting from random initial weights each time. In each of these 1000 training trials, we used as many training iterations as needed for training to converge. Each training iteration involved presenting the four input patterns 100 times. Across the 1000 trials, the maximum number of training iterations needed to converge was 61 while the average was 3.48. Figure 2b shows the\npost-training simulation results of the network when presented with each of the input patterns. The causal input sets of the different neurons change across the input patterns, allowing the network to implement the non-linearity needed to solve the XOR task."}, {"heading": "4.2 MNIST classification task", "text": "The MNIST database contains 70,000 28x28 grayscale images of handwritten digits. The training set of 60,000 labeled digits was used for training, and testing was done using the remaining 10,000. No validation set was used. All grayscale images were first binarized to two intensity values: high and low. Pixels with high intensity generate a spike at time 0, while pixels with low intensity generate a spike at time ln(6) = 1.79. All times are normalized with respect to the synaptic time constant (see Eq. 2). We investigated two feedforward network topologies with fully connected hidden layers: the first network has one hidden layer of 800 neurons (the 784-800-10 network), and the second has two hidden layers of 400 neurons each (the 784-400-400-10 network). We found that accuracy is slightly improved if we use an extra reference neuron that always spikes at time 0 and projects through trainable weights to all neurons in the network. We ran 100 epochs of training with an exponentially decaying learning rate and a mini-batch size of 10. Each of the two network topologies was trained twice, once with non-noisy input spike times and once with noise-corrupted input spike times. In the noisy input case, noise delays each spike with the absolute value of a random quantity drawn from a zero mean, unity variance Gaussian distribution. Noise was only used during training. Table 1 shows the performance results for the two networks after the noisy and non-noisy training regimes.\nThe small errors on the training set indicate the networks have enough representational power to solve this task, as well as being effectively trainable. The networks overfit the training set as indicated by the significantly higher test set errors. Noisy training input helps in regularizing the networks as it reduces test set error but further regularization is still needed. We experimented with dropout [14] where we randomly removed neurons from the network during training. However, dropout does not seem to be a suitable technique in our networks as it reduces the number of spikes received by the neurons, which would often prevent them from spiking. Effective techniques are still needed to combat overfitting and allow better generalization in these networks.\nFigures 3a and 3b show the distribution of spike times in the hidden layers and the distribution of the times of the earliest output layer spike in the two networks. The later are the times at which the networks made a decision for the 10,000 test examples. Both networks were first trained using noisy input. For both topologies, the network makes a decision after only a small fraction of the hidden layer neurons have spiked. For the 784-800-10 topology, an output neuron spikes (a class is selected) after only 3.0% of the hidden neurons have spiked (on average across the 10,000 test set images), while for the 784-400-400-10 topology, this number is 9.4%. The network is thus able to make very rapid decisions about the input class, after approximately 1-3 synaptic time constants from stimulus onset, based only on the spikes of a small subset of the hidden neurons. This is illustrated in Figs. 3c and 3d which show the membrane potentials of 10 hidden neurons and the 10 output neurons. The spikes of the 10 hidden neurons do not factor into the network decision in this case as they all spike after the earliest output spike, i.e, after the network has already selected a class."}, {"heading": "5 Discussion", "text": "We presented a form of spiking neural networks that can be effectively trained using gradient descent techniques. By using a temporal spike code, many difficulties involved in training spiking networks such as the discontinuous spike generation mechanism and the discrete nature of spike counts are avoided. The network input-output relation is locally linear after a transformation of the time variable.\nAs the input spike times change, the causal input sets of the neurons change, which in turn changes the form of the linear input-output relation (Fig. 1). This is analogous to the behavior of networks using rectified linear units (ReLUs) [10] where changes in the input change the set of ReLUs producing non-zero output, thus changing the linear transformation implemented by the network.\nRecordings from higher visual areas in the brain indicate these areas encode information about abstract features of visual stimuli as early as 125ms after stimulus onset [15]. Given the typical firing rate of cortical neurons and delays across synaptic stages, this indicates rapid visual processing is mostly a feedforward process where neurons get to spike at most once [16]. The presented networks follow a similar processing scheme and could thus be used as a trainable model to investigate the accuracy-response latency tradeoff in feedforward spiking networks. Output latency can be reduced by using a penalty term in the cost function that grows with the output spikes latency. Scaling this penalty term controls the tradeoff between minimizing latency and minimizing error during training.\nWe considered the case where each neuron in the network is allowed to spike once. The training scheme can be extended to the case where each neuron spikes multiple times. The time of later spikes can be differentiably related to the times of all causal input spikes (see Eq. 7).\nThe presented networks enable very rapid classification of input patterns. As shown in Fig. 3, the network selects a class before the majority of hidden layer neurons have spiked. This is expected as the only way the network can implement non-linear transformations (in the z-domain) is by changing the causal set of input spikes for each neuron, i.e, by making a neuron spike before a subset of its input neurons have spiked. This unique form of non-linearity not only results in rapid processing, but it enables the efficient implementation of these networks on neuromorphic hardware since processing can stop as soon as an output spike is generated. In the 784-800-10 MNIST network for example, the network classifies an input after only 25 spikes from the hidden layers (on average). Thus, only a small fraction of hidden layer spikes need to be dispatched and processed."}], "references": [{"title": "Parallel distributed processing: explorations in the microstructure of cognition. Volume 1. Foundations", "author": ["D.E. Rumelhart", "J.L. McClelland"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Error-backpropagation in temporally encoded networks of spiking", "author": ["S.M. Bohte", "J.N. Kok", "Han La-Poutre"], "venue": "neurons. Neurocomputing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Real-time classification and sensor fusion with a spiking deep belief network", "author": ["P. O\u2019Connor", "D. Neil", "S.-C. Liu", "T. Delbruck", "M. Pfeiffer"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing", "author": ["Peter U Diehl", "Daniel Neil", "Jonathan Binas", "Matthew Cook", "Shih-Chii Liu", "Michael Pfeiffer"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Spiking deep convolutional neural networks for energy-efficient object recognition", "author": ["Yongqiang Cao", "Yang Chen", "Deepak Khosla"], "venue": "International Journal of Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A re-configurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses", "author": ["Ning Qiao", "Hesham Mostafa", "Federico Corradi", "Marc Osswald", "Fabio Stefanini", "Dora Sumislawska", "Giacomo Indiveri"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Minitaur, an event-driven FPGA-based spiking network accelerator", "author": ["D. Neil", "S.-C. Liu"], "venue": "Very Large Scale Integration (VLSI) Systems, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations", "author": ["Ben Varkey Benjamin", "Peiran Gao", "Emmett McQuinn", "Swadesh Choudhary", "Anand R Chandrasekaran", "J Bussat", "R Alvarez-Icaza", "JV Arthur", "PA Merolla", "K Boahen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Le Cun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1929}, {"title": "Fast readout of object identity from macaque inferior temporal cortex", "author": ["C.P. Hung", "Gabriel Kreiman", "Tomaso Poggio", "J.J. DiCarlo"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Spike-based strategies for rapid processing", "author": ["S. Thorpe", "A. Delorme", "R. Van Rullen"], "venue": "Neural networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "The idea of a distributed network of simple neuron elements that adaptively adjusts its connection weights based on training examples is partially inspired by the operation of biological spiking networks [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "An approach that bears some similarities to ours is the SpikeProp algorithm [3] that can be used to train spiking networks to produce output spikes at specific times.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6].", "startOffset": 206, "endOffset": 215}, {"referenceID": 3, "context": "Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6].", "startOffset": 206, "endOffset": 215}, {"referenceID": 4, "context": "Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6].", "startOffset": 206, "endOffset": 215}, {"referenceID": 5, "context": "Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9].", "startOffset": 184, "endOffset": 193}, {"referenceID": 6, "context": "Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9].", "startOffset": 184, "endOffset": 193}, {"referenceID": 7, "context": "Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9].", "startOffset": 184, "endOffset": 193}, {"referenceID": 8, "context": "Indeed, many feedforward ANNs use activation functions with a discontinuous first derivative such as rectified linear units (ReLUs) [10] while still being effectively trainable.", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "We trained the network to solve two classification tasks: an XOR task, and the permutation invariant MNIST task [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Training was done using Theano [12, 13].", "startOffset": 31, "endOffset": 39}, {"referenceID": 11, "context": "Training was done using Theano [12, 13].", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "We experimented with dropout [14] where we randomly removed neurons from the network during training.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "This is analogous to the behavior of networks using rectified linear units (ReLUs) [10] where changes in the input change the set of ReLUs producing non-zero output, thus changing the linear transformation implemented by the network.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Recordings from higher visual areas in the brain indicate these areas encode information about abstract features of visual stimuli as early as 125ms after stimulus onset [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Given the typical firing rate of cortical neurons and delays across synaptic stages, this indicates rapid visual processing is mostly a feedforward process where neurons get to spike at most once [16].", "startOffset": 196, "endOffset": 200}], "year": 2016, "abstractText": "Gradient descent training techniques are remarkably successful in training analogvalued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard nonlinearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is locally linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior can not be directly approximated by conventional ANNs. Our results highlight a new approach for controlling the behavior of spiking networks with realistic temporal dynamics, opening up the potential for using these networks to process spike patterns with complex temporal information.", "creator": "LaTeX with hyperref package"}}}