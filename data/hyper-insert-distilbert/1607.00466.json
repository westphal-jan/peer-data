{"id": "1607.00466", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2016", "title": "Outlier absorbing based on a Bayesian approach", "abstract": "the actual presence of outliers is prevalent in machine learning applications projects and governments may thereby produce intentionally misleading lab results. explicitly in this validation paper setting a new finding method for frequently dealing with outliers and anomal samples is originally proposed. to overcome the generic outlier issue, developing the proposed method combines the global variables and local views presented of the samples. responding by combination amounts of conflicting these views, our query algorithm simultaneously performs in a robust manner. the experimental results show even the capabilities of investigating the proposed resulting method.", "histories": [["v1", "Sat, 2 Jul 2016 04:48:59 GMT  (651kb)", "http://arxiv.org/abs/1607.00466v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["parsa bagherzadeh", "hadi sadoghi yazdi"], "accepted": false, "id": "1607.00466"}, "pdf": {"name": "1607.00466.pdf", "metadata": {"source": "CRF", "title": "Outlier absorbing based on a Bayesian approach", "authors": ["Parsa Bagherzadeh", "Hadi Sadoghi Yazdi"], "emails": ["parsa.bagherzadeh@stu.um.ac.ir", "h-sadoghi@um.ac.ir"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 7.\n00 46\n6v 1\n[ cs\n.L G\n] 2\nJ ul\n2 01\n6\nI. INTRODUCTION\nData quality is one of greatest concerns in data mining and machine learning. Most of machine learning methods perform inaccurately or produce misleading results when data suffers from lack of quality. Limitation of measuring instruments, human error in the data equation process may lower data quality. In some cases, the value of a feature may be missing. In other cases, the data may be contaminated by external sources and not indicating their real value [2].\nOne of main issues is the context of data quality is the presence of outliers. Outliers are instances which have considerable difference with the majority of instances. Another outlier definition from [1] is: A sample (or subset of samples) which appears to be inconsistent with the rest of that data set. An outlier may also be surprising veridical data, a sample belonging to class \u03c91 but actually positioned inside class \u03c92 so the true (veridical) classification of the sample is surprising to the observer (this type of outlier is also called label noise).\nThe presence of outliers may cause potential problems in both supervised and unsupervised learning. The most significant consequence of label noise is degradation of classification performance [12], [13]. For example it is shown that only 5% of outliers can highly deviate the decision boundaries. In [3] SVMs, ridge regression, and logistic regressions are tested is the presence of outliers. The experiments show that the results are highly affected by outliers for all three methods.\nMoreover, outliers may cause over-fitting on training data. The presence of outliers also increases the required number of instances for learning, as well as the complexity of models [11]. In [14] it is shown that the removal of outliers reduces the number of support vectors. Non-robust classifier methods\nproduce models which are skewed when outliers are left in. An example of a data set suffering from outliers is illustrated in Fig. 1. The outliers are indicated by small red circles around.\nThere are various methods for detection of outliers [6]. In this paper we focus on proximity-based techniques including k-NN-based methods. These methods are simple to implement and make no prior assumptions about the data distribution model.\nRamaswamy et al. introduced an optimized k-NN to produce a ranked list of potential outliers [4]. A sample is an outlier if no more than n \u2212 1 other points in the data set have a higher Dm (distance to mth neighbor) where m is a user-specified parameter. Since most of k-NN based approaches are susceptible to the computational growth several techniques were proposed for speeding the k-NN algorithm such as partitioning the data into cells. If any cell and its adjacent cells contains more than k points, then the points in the cell are probably lied in a dense area of the distribution so the points contained are unlikely to be outliers.\nAnother proximity-based variant is the graph connectivity method. Shekhar et al. introduced an approach for traffic monitoring which views the outlier issue from a topologically perspective [5]. Shekhar detects traffic monitoring stations producing sensor values which are inconsistent with stations in the connected neighborhood. A station is an outlier if the\ndifference between its sensor value and the average sensor value of its topological neighbors differs significantly from the mean difference between all nodes and their topological neighbors.\nKnorr and Ng (1998) introduced an efficient type 1 k-NN approach. If m of the k nearest neighbors (where m < k) lie within a specific distance threshold d then the exemplar is deemed to lie in a sufficiently dense region of the data distribution to be classified as normal. However, if there are less than m neighbors inside the distance threshold then the exemplar is an outlier.\nSeveral problems are accompanying with k-NN based approaches. Most of k-NN based approaches only view data locally. This approach may fail when there are batches of outliers in data set. Another approach is to have a global view of samples. In global view, samples with large distance from the distribution of samples are detected as outlier. A potential problem however is the determination of a threshold. An inappropriate threshold may lead to detection of a correct sample as outlier.\nIn this paper a new kNN-based method for dealing with outliers is proposed. The proposed method solves the problems of kNN-based method by combining the local and global views of samples.\nThe rest of this paper is organized as follows: Section 2 presents our method for outlier absorbing. In Section 3 the experimental results are investigated and finaly section 4 give the concluding remarks."}, {"heading": "II. THE PROPOSED METHOD", "text": "In this section our proposed method for outlier absorbing is presented. The proposed method combines the local and global information of sample to achieve more robust results."}, {"heading": "A. Notations", "text": "Let \u2126x be the state space of traning samples, X . In other words\n\u2126x = {x1, x2, . . . , xn} (1)\nIf a sample x be noisy, it is desired to estimate x\u0302, as a new noise-free instance. Let \u2126x,k be the set of all k nearest neighborhoods and xkNNi be the nearest neighbors of instance xi. Also suppose \u2126\u2212ix be the set of all samples except the ith instance and \u2126\u2212ix,k be the set of all k nearest neighborhoods except the k nearest neighbors of ith instance."}, {"heading": "B. Markovian-like assumption", "text": "Markovian assumption holds for state spaces in which a sequence of states occurs temporally so that the probability of being in a sate at time t is only given it\u2019s previous state, not all of the previous states in the sequence. In other words\nf(xt|xt\u22121, xt\u22122, ...) = f(xt|xt\u22121) (2)\nThe main notion of Markovian property is that when a state xt\u22121 explicitly contains the information of other states {xt\u22122, xt\u22122, ...}, these states can be ignored. This property holds when the states have a temporal nature. We are looking for a same property when the states have a spatial nature. Similar to Markovian assumption, if a set of samples S1 contains the information of another set S2 it is reasonable to ignore set S2. Fore example, consider two sets \u2126 \u2212i x,k and xkNNi . Each instance in x kNN i can be represented by its k nearest neighbors in \u2126\u2212ix,k thus a Markovian-like property holds for these two sets. Along with our problem formulation, we will use this Markovian-like property."}, {"heading": "C. Problem formulation", "text": "Let f(x|\u2126x,k) be the probability density function of x given the set of all k nearest neighborhoods. A representation of f(x|\u2126x,k) is weighted perfect sampling.\nf(x|\u2126x,k) =\n\u2211 i\u2208\u2126x,k\nwi(x)\u03b4(x \u2212 xi)\u2211 i\u2208\u2126x,k wi(x) (3)\nwhere wi(x) is the weight of instance xi to be defined later. x\u0302 should be extracted from f(x|\u2126x,k). It can be defined as expected value of g(x) over f(x|\u2126x,k) where g(x) is an arbitrary loss function. In other words\nz\u0302 = E{g(x)} = \u222b g(x)f(x|\u2126x,k)dx. (4)\nIn the case of g(x) = x and using 3 we have:\nx\u0302 =\n\u2211 i\u2208\u2126x,k\nwi(x)xi\u2211 i\u2208\u2126x,k wi(x) (5)\nwhere xi is one of kNN samples (Look at the Appendix for details). The recent equation is the representation of an instance based on its k nearest neighbors which means a representation based on a local view to samples.\nLet f(\u2126x|\u2126x,k) be the PDF of \u2126x given the set of all k nearest neighborhoods and suppose that wi(x), the weight of instance xi be defined as:\nwi(x) = f(\u2126x|\u2126x,k)\nq(\u2126x|\u2126x,k) (6)\nWe can decompose f(\u2126x|\u2126x,k) as follow:\nf(\u2126x|\u2126x,k) = f(xi,\u2126\n\u2212i x ,\u2126 \u2212i x,k ,xkNN i ) f(\u2126x,k)\n= f(xkNNi |xi,\u2126 \u2212i x ,\u2126 \u2212i x,k)f(xi|\u2126 \u2212i x ,\u2126 \u2212i x,k)f(\u2126 \u2212i x |\u2126 \u2212i x,k)f(\u2126 \u2212i x,k)\nf(\u2126x,k) (7)\nIf the number of samples are sufficiently large, we can assume f(\u2126x,k) \u2243 f(\u2126 \u2212i x,k), thus\nf(\u2126x|\u2126x,k) \u221d f(x kNN i |xi,\u2126 \u2212i x ,\u2126 \u2212i x,k)f(xi|\u2126 \u2212i x ,\u2126 \u2212i x,k)f(\u2126 \u2212i x |\u2126 \u2212i x,k) (8) and by Markovian-like assumption\nf(\u2126x|\u2126x,k) \u221d f(x kNN i |\u2126 \u2212i x )f(xi|\u2126 \u2212i x )f(\u2126 \u2212i x |\u2126 \u2212i x,k) (9)\nUsing a slightly different decomposition for q(\u2126x|\u2126x,k) we can write:\nq(\u2126x|\u2126x,k) = q(\u2126\u2212ix ,xi,\u2126x,k)\nq(\u2126x,k)\n= q(xi|\u2126\n\u2212i x ,\u2126x,k)q(\u2126 \u2212i x |\u2126x,k)q(\u2126x,k)\nq(\u2126x,k)\n= q(xi|\u2126 \u2212i x ,\u2126x,k)q(\u2126 \u2212i x |\u2126x,k) (10)\nBy Markovian-like assumption:\nq(\u2126\u2212ix |\u2126x,k) \u2243 q(\u2126 \u2212i x |\u2126 \u2212i x,k)q(x kNN i |\u2126 \u2212i x,k) (11)\n(Look at the Appendix for details). Thus:\nwitni (x) = f(\u2126x|\u2126x,k) q(\u2126x|\u2126x,k)\n\u221d f(xkNNi |\u2126 \u2212i x )f(xi|\u2126 \u2212i x )f(\u2126 \u2212i x |\u2126 \u2212i x,k)\nq(xi|\u2126 \u2212i x ,\u2126x,k)q(\u2126 \u2212i x |\u2126 \u2212i x,k)q(x kNN i |\u2126 \u2212i x,k)\n(12)\ntaking\nwitn\u22121i (x) = f(\u2126\u2212ix |\u2126 \u2212i x,k)\nq(\u2126\u2212ix |\u2126 \u2212i x,k)\n(13)\nyields the following recursive update equation:\nwitni (x) = w itn\u22121 i (x)\nf(xkNNi |\u2126 \u2212i x )f(xi|\u2126 \u2212i x )\nq(xi|\u2126 \u2212i x ,\u2126x,k)q(xkNNi |\u2126 \u2212i x,k)\n(14)\nfor simplicity we assume\nf(xkNNi |\u2126 \u2212i x )\nq(xi|\u2126 \u2212i x ,\u2126x,k)q(xkNNi |\u2126 \u2212i x,k)\n= 1 (15)\nwhich yields an update equation for weights of samples\nwitni (x) = w itn\u22121 i f(xi|\u2126 \u2212i x ) (16)\nwhere f(xi|\u2126\u2212ix ) means the evaluated value of the PDF of all samples except xi at instance xi, which corresponds to a global view samples for estimation of x\u0302 using 5. A realization of f(zi|\u2126\u2212iz ) could be obtained using GMMs (Gaussian mixture models). The steps of the proposed algorithm for label denoising is presented as follows:\nOutlier absorbibg based on a Bayesian approach \u2022 Input: Data matrix X \u2208 \u211cn\u00d7d\n\u2022 Initialization: Set the weights of all samples to 1 n , where n is the number of samples. \u2022 Step 1: Update the weights using equation 16. \u2022 Step 2: For each instance z, estimate z\u0302 with respect to\nweights of its neighbors using equation 5 and update z with the estimation. \u2022 Step 3: If Div(f itn(\u2126z |\u2126z,k)||f itn\u22121(\u2126z |\u2126z,k)) < \u01eb or maximum numbers of iterations reached, then terminate, otherwise go to Step 1. \u2022 Output: Denoised data set"}, {"heading": "III. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Artificial Data sets", "text": "We have applied the proposed method to two artificial data sets. First, the method is applied to a gaussian distribution. Then the non-linear distribution case is considered.\n1) Gaussian distribution: In order to evaluate the proposed method, different portions of outliers are added to the artificial data set. 150 instances drawn from a Gaussian distribution (Fig. 2.a). These smaples are obtained by adding Gaussian noise to randomly selected samples. The evaluation of the proposed method is performed in the presence of outlier with different percentages including 5%, 10%, 15% and 20%. A demonstration of the case with 10% outliers is shown in Fig. 2.\n2) Non-linear distribution: A challenging case in outlier detection problem is the case of non-liner data. An example of a non-liner distribution is illustrated in Fig. 3. As our experiments show, out proposed method for outlier absorbing is also robust for these types of distributions. Fig. 3 and Fig. 4 show the performance of the proposed method for the case with 10% and 15% outlier respectively."}, {"heading": "B. Real world data set", "text": "In order to have a more realistic evaluation of the proposed methodm it should be tested on real world data set. Pen digit data set as of one UCI data sets is choosed for the evaluation [7].\n1) Evaluation metric: The goal of a denoising method is to recover the real distribution of data from the noisy one. Therefore the resulting distribution should close to the real distribution. In order to measure the diffrence between the resulting and real distributions, Divergence is employed. Divergence distance measures the similarity of two probability distributions [15].\nDpq = E{ln p(x)\nq(x) } =\n\u222b p(x)ln p(x)\nq(x) dx (17)\nsimilar discussion holds for class \u03c92\nDqp = E{ln q(x)\np(x) } =\n\u222b q(x)ln q(x)\np(x) dx (18)\nThe sum\nd = Dpq +Dqp (19)\nis called divergence and can be used as a discriminatory measure for the distributions p and q.\nAssuming that the density functions are Gaussians N (\u00b5p,\u03a3p) and N (\u00b5q,\u03a3q), the divergence can be computed as:\ndpq = 1 2 trace{\u03a3 \u22121 p \u03a3q +\u03a3 \u22121 q \u03a3p \u2212 2I}\n+ 1\n2 (\u00b5p \u2212 \u00b5q)\nT (\u03a3\u22121p +\u03a3 \u22121 q )(\u00b5p \u2212 \u00b5q) (20)\nBy the above definitions, the Divergence value for the real distribution and denoised distribution should be a minimum as possible for a good denoising method. The values of\nDivergence between the two distributions for different rates of outliers are summarized in Table I. As it can be seen, the proposed method yeilds distribution for which the Diverence value between them and the real distribution of the sample is small.\nIn Fig. 6 an illustration of the performance of the proposed method on Pen digits data set is provided. Fig. 6.a) illustrates the Pen digits data set in 3 dimension. Fig. 6.b) show the data set with 10% outliers and in Fig. 6.c) the data set is shown after outlier absorbing."}, {"heading": "IV. CONCLUSION", "text": "In this paper a new method for dealing with outliers was proposed. The poropsed method employs the local and global\ninformation of instance to overcome the outlier problem. As the experimental results showed, the combination leads to a more robust method for dealing with outliers. For future work we plan to extend our work to multi-class classification case."}], "references": [{"title": "Outliers in Statistical Data, 3rd edn", "author": ["V. Barnett", "T. Lewis"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Introduction to data mining", "author": ["T Pang-Ning", "M Steinbach", "V Kumar"], "venue": "Library of Congress,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Robustness of regularized linear classification methods in text categorization", "author": ["J Zhang", "Y Yang"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Efficient Algorithms for Mining Outliers from Large Data Sets", "author": ["S. Ramaswamy", "R. Rastogi", "K. Shim"], "venue": "Proceedings of the ACM SIGMOD Conference on Management of Data, Dallas,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Detecting Graph-Based Spatial Outliers: Algorithms and Applications", "author": ["S. Shekhar", "C. Lu", "P. Zhang"], "venue": "Proceedings of the Seventh ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "A Survey of Outlier Detection Methodoligies", "author": ["V. Hodge", "J. Austin"], "venue": "Artificial Intelligence Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "UCI Repository of machine learning databases", "author": ["C Blake", "CJ Merz"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Support Vector Method for Novelty Detection", "author": ["B Schlkopf", "RC Williamson", "AJ Smola", "J Shawe-Taylor", "JC Platt"], "venue": "In: NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Support vector novelty detection applied to jet engine vibration spectra", "author": ["P Hayton", "B Schlkopf", "L Tarassenko", "P Anuzis"], "venue": "In: NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Classification in the presence of label noise: a survey", "author": ["B Frnay", "M Verleysen"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on 25:845-869", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "The effect of errors in diagnosis and measurement on the estimation of the probability of an event Journal of the American Statistical Association", "author": ["JE Michalek", "RC Tripathi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1980}, {"title": "The efficiency of logistic regression compared to normal discriminant analysis under class-conditional classification noise Journal of Multivariate Analysis", "author": ["Y Bi", "DR Jeske"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Identifying mislabeled training data", "author": ["CE Brodley", "MA Friedl"], "venue": "Journal of Artificial Intelligence", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Pattern recognition.", "author": ["Theodoridis", "Sergios", "Konstantinos Koutroumbas"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "In other cases, the data may be contaminated by external sources and not indicating their real value [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Another outlier definition from [1] is: A sample (or subset of samples) which appears to be inconsistent with the rest of that data set.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "The most significant consequence of label noise is degradation of classification performance [12], [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "The most significant consequence of label noise is degradation of classification performance [12], [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "In [3] SVMs, ridge regression, and logistic regressions are tested is the presence of outliers.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "The presence of outliers also increases the required number of instances for learning, as well as the complexity of models [11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "In [14] it is shown that the removal of outliers reduces the number of support vectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "There are various methods for detection of outliers [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "introduced an optimized k-NN to produce a ranked list of potential outliers [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "introduced an approach for traffic monitoring which views the outlier issue from a topologically perspective [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Pen digit data set as of one UCI data sets is choosed for the evaluation [7].", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Divergence distance measures the similarity of two probability distributions [15].", "startOffset": 77, "endOffset": 81}], "year": 2016, "abstractText": "The presence of outliers is prevalent in machine learning applications and may produce misleading results. In this paper a new method for dealing with outliers and anomal samples is proposed. To overcome the outlier issue, the proposed method combines the global and local views of the samples. By combination of these views, our algorithm performs in a robust manner. The experimental results show the capabilities of the proposed method.", "creator": "LaTeX with hyperref package"}}}