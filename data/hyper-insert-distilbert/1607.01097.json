{"id": "1607.01097", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2016", "title": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks", "abstract": "moreover we present a new theoretical framework for continuous analyzing rapidly and actively learning artificial optical neural networks.'our engineering approach for simultaneously utilizes and adaptively directed learns both the biological structure of the network as well internationally as its perceived weights. the methodology is based upon and accom - panied independently by incorporating strong modeling data - dependent theoretical learning guarantees, so that the final engineered network enabling architecture provably adapts to understand the complexity model of any chosen given problem.", "histories": [["v1", "Tue, 5 Jul 2016 02:51:33 GMT  (65kb,D)", "http://arxiv.org/abs/1607.01097v1", null], ["v2", "Sat, 19 Nov 2016 00:46:26 GMT  (66kb,D)", "http://arxiv.org/abs/1607.01097v2", null], ["v3", "Tue, 28 Feb 2017 02:58:11 GMT  (239kb,D)", "http://arxiv.org/abs/1607.01097v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["corinna cortes", "xavier gonzalvo", "vitaly kuznetsov", "mehryar mohri", "scott yang"], "accepted": true, "id": "1607.01097"}, "pdf": {"name": "1607.01097.pdf", "metadata": {"source": "CRF", "title": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks", "authors": ["Corinna Cortes", "Xavi Gonzalvo"], "emails": ["corinna@google.com", "xavigonzalvo@google.com", "vitalyk@google.com", "mohri@cims.nyu.edu", "yangs@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep learning has become a very powerful framework for machine learning and has enjoyed strong success in many areas in recent years. In the supervised setting, mapping and representing an input through increasingly more abstract layers of feature representation has shown to be extremely effective in areas such as natural language processing, image captioning, and many others. The concept of multilayer feature representations and modeling machine learning problems using a network of neurons is also motivated and guided by studies of the brain, neurological behavior, and cognition.\nHowever, despite the compelling arguments for using neural networks as a general template for solving machine learning problems, the training of these models and design of the right network for a given task has been filled with many theoretical gaps and practical concerns. For training a network, one needs to specify an often large network architecture with several layers and nodes, and then solve a difficult non-convex optimization problem. Additionally, the pre-specified architecture is often treated as a hyperparameter which is tuned using a validation set. These spaces can become exorbitantly large (e.g. [Krizhevsky et al., 2012]). From an optimization perspective, there is no guarantee of stability of an output model or near optimality of the learning objective, and often, one needs to implement ad hoc methods (e.g. gradient clipping [Pascanu et al., 2013]) to produce coherent models. From the statistical standpoint, large-scale hyperparameter tuning for an effective network architecture is extremely wasteful of data (due to cross validation), and can also exhaust a lot of time and resources (e.g. grid search, random search [Bergstra et al., 2011]).\nIn this paper, we attempt to remedy some of these issues. Accepting the general structure of a neural network as an effective parametrized model for supervised learning, we provide a theoretical analysis of this model and proceed to derive an algorithm benefitting from that theory. In the process, we introduce a framework for training neural networks that:\n1. uses a stable and robust algorithm with a unique solution. 2. can produce much sparser and/or shallower networks compared to existing methods.\nar X\niv :1\n60 7.\n01 09\n7v 1\n[ cs\n.L G\n] 5\n3. adapts the structure and complexity of the network to the difficulty of the particular problem at hand, with no pre-defined architecture.\n4. is accompanied and in fact motivated by strong data-dependent generalization bounds, validating their adaptivity and statistical efficacy.\n5. is intuitive from the cognitive standpoint that originally motivated neural network architectures.\nNot all machine learning problems admit the same level of difficulty, and different tasks naturally require varying levels of complexity. The typical approach to training a neural network requires the model-builder to know and specify as an architecture the right level of complexity. This is often unreasonably hard and can lead to large amounts of hyperparameter tuning, a statistically wasteful task. Moreover, if a network architecture is specified a priori and trained using back-propagation, the model will always have as many layers as the one specified because there needs to be at least one path through the network in order for the hypothesis to be non-trivial. Single weights may be pruned [Han et al., 2015], a technique originally termed Optimal Brain Damage [LeCun et al., 1990], but the architecture itself is unchanged. This imposes a stringent lower bound on the complexity of the model and can make the model prone to overfitting when there is insufficient data.\nIn contrast to enforcing high complexity, we will attempt to learn the requisite model complexity for a machine learning problem in an adaptive way. Starting from a simple single layer neural network, we will add more neurons and additional layers as needed. From the cognitive perspective, we will adapt the neural complexity and architecture to the difficulty of the problem. The additional neurons that we add will be carefully selected and penalized according to rigorous estimates from the theory of statistical learning. This will serve as a catalyst for the sparsity of our model as well as the strong generalization bonds that we will be able to derive. Incredibly, our method will also turn out to be convex and hence more stable than the current methodologies employed."}, {"heading": "2 Related Work", "text": "There has been extensive work involving structure learning for neural networks (e.g. [Kwok and Yeung, 1997, Leung et al., 2003, Islam et al., 2003, Lehtokangas, 1999, Islam et al., 2009]). All these publications seek to grow and prune the neural network architecture using some heuristic (e.g. genetic, information theoretic, or correlation). The structure learning algorithm introduced in this paper is based directly on optimizing generalization performance, which is precisely the learning goal in the batch setting.\nFrom the theory perspective, there have been several major lines of research on the theoretical understanding of neural networks. The first deals with understanding properties of the objective function used when training neural networks. (e.g. [Choromanska et al., 2014, Sagun et al., 2014, Zhang et al., 2015, Livni et al., 2014]). The second involves studying the black-box optimization algorithms that are often used for training these networks (e.g. [Hardt et al., 2015, Lian et al., 2015]). The third analyzes the statistical and generalization properties of the neural networks that are created (e.g. [Bartlett, 1998]). The fourth takes the generative point of view (e.g. [Arora et al., 2014, 2015]), assuming that the data actually comes from a particular network and then attempting to recover it. The fifth investigates the expressive ability of neural networks and analyzing what types of mappings they can learn (e.g. [Cohen et al., 2015, Eldan and Shamir, 2015]).\nThis paper takes the discriminative approach to machine learning and incorporates the first three methodologies, starting with a theoretical analysis of neural networks, to deriving a computationally tractable objective function, and to finally describing a precise optimization method. [Janzamin et al., 2015] is another paper that touches on multiple theory components, analyzing the generalization and training of two-layer neural networks through tensor methods. Our work uses different methods, applies to arbitrary networks, and also learns a network structure from a single layer."}, {"heading": "3 Preliminaries", "text": "Let X denote the input space. We consider the standard supervised binary classification scenario and assume that training and test points are drawn i.i.d. according to some distribution D over\nX \u00d7 {\u22121,+1} and denote by S = ((x1, y1), . . . , (xm, ym)) a training sample of size m drawn according to Dm. Given any x \u2208 X, we denote by \u03a6(x) \u2208 Rn0 the feature representation of x. The standard description of a modern feedforward network is a network of layers of nodes, where each layer is mapped to the layer above it via a linear mapping composed with a component-wise nonlinear transformation. To make this precise, we define a neural network as follows.\nLet l denote the number of layers in the network. The networks we learn can be potentially very deep, that is l can be very large. For each k \u2208 [l], denote by nk the maximum number of nodes in layer k.\nLet 1 \u2264 p \u2264 \u221e and k \u2265 1. Then define the set H(p)k to be the family of functions at layer k of the network in the following way:\nH (p) 1 = { x 7\u2192 u \u00b7\u03a6(x) : u \u2208 Rn0 , \u2016u\u2016p \u2264 \u039b1 } (1)\nH (p) k =\n{ x 7\u2192 ( nk\u22121\u2211 j=1 uj(\u03d5k\u22121 \u25e6 hj)(x) ) : u \u2208 Rnk\u22121 , \u2016u\u2016p \u2264 \u039bk, hj \u2208 H(p)k\u22121 } , (\u2200k > 1) (2)\nwhere \u039bk > 0 is a hyperparameter and where \u03d5k is an activation function. Common activation functions include the Rectified Linear Unit (ReLU) \u03d5k(x) = max{0, x} and the sigmoid function \u03d5k(x) = 1 1+e\u2212x (see e.g. [Goodfellow et al., 2016]), although our work will allow for any 1-Lipschitz activation function. The choice of norm p here is left to the learner and will determine both the sparsity of the network and the accompanying learning guarantee of the resulting model.\nLet H denote the union of these families of functions and their reflections: H = \u22c3l k=1(H (p) k \u222a (\u2212H(p)k )). Any feedforward neural network, then, can be written as a composition of mappings f = fl \u25e6 fl\u22121 \u25e6 . . . \u25e6 f1, where fk \u2208 H(p)k . Intuitively, each transformation represents the encoding of the original data into an abstract layer of feature representation from which learnability is presumed to be \u201ceasier.\u201d\nNote that in our definition, the activation function is not directly included in the unit itself but instead only applied when feeding the neuron into the next layer. While this choice of notation ultimately represents the same family of models, it is a subtle but important distinction that will be crucial for our theory as well as algorithmic design, in particular for deriving Lemma 1 and Lemma 4."}, {"heading": "4 Theoretical properties of artificial neural networks", "text": "We measure the performance of a hypothesis f \u2208 H by its expected loss over the data\u2019s distribution D, also known as the generalization error: R(f) = E(x,y)\u223cD[1yf(x)\u22640]. We typically also want to measure the performance of the model on our training sample S. This will be done using the empirical margin loss: R\u0302S,\u03c1(f) = 1m \u2211m i=1 1yif(xi)\u2264\u03c1, where the margin refers to the \u03c1 term. Hypothesis functions that allow for large margin with small empirical margin loss intuitively represent classifiers with high confidence of accuracy.\nGiven a hypothesis set H of functions mapping from X to R, we denote by R\u0302S(H) the empirical Rademacher complexity of H for the sample S: R\u0302S(H) = 1m E\u03c3 [suph\u2208H \u2211m i=1 \u03c3ih(xi)] , and by Rm(H) the Rademacher complexity of H defined by Rm(H) = ES\u223cDm [R\u0302S(H)]. The empirical Rademacher complexity measures the correlation of a hypothesis set with random noise over the sample and is a problem-dependent measure of model complexity. It can also be shown to relate to other classical notions of model complexity, such as the VC-dimension and covering number (see e.g. [Vapnik, 1998, Mohri et al., 2012])."}, {"heading": "4.1 Learning bounds on the Rademacher complexity of network hypothesis sets", "text": "Since neural networks are built as compositions of layers, it is natural from the theoretical standpoint to first analyze the complexity of any layer in terms of the complexity of its previous layer. Our first result demonstrates that this can indeed be done, and that the empirical Rademacher complexity of\nany intermediate layer k in the network is bounded by the empirical Rademacher complexity of its input times a term that depends on a power of the size of the layer:\nLemma 1. Let 1p + 1 q = 1. Then for k \u2265 2, the empirical Rademacher complexity of H (p) k for a sample S of size m can be upper bounded as follows in terms of that of H(p)k\u22121:\nR\u0302S(H (p) k ) \u2264 2\u039bkn\n1 q\nk\u22121R\u0302S(H (p) k\u22121).\nThe proofs of this result, as well as all those that follow in this section, can be found in Appendix A.\nBy analyzing the complexity of the initial layer, we can derive a bound on the complexity of every layer in closed form without the need for any recurrence relation: Lemma 2. Let r\u221e = maxj\u2208[1,n1],i\u2208[1,m] |[\u03a6(xi)]j |, and 1p + 1 q = 1. Then for any k \u2265 1, the empirical Rademacher complexity of H(p)k for a sample S of size m can be upper bounded as follows:\nR\u0302S(H (p) k ) \u2264 2 k\u22121r\u221e ( k\u220f j=1 \u039bjn 1 q j\u22121 )\u221a 2 log(2n0) m .\nThe above two lemmas are instructive and intuitive in the sense that they convey the message that additional layers in a neural network contribute to increased complexity of a model. Because of this, while large models are more powerful, they also become increasingly more prone to overfitting. Moreover, the Rademacher complexity bounds also suggest that model complexity can increase much more due to a single additional layer as opposed to an additional node.\nGuided by this insight, we will seek to learn models that will be parsimonious in model complexity. Specifically, we will learn adaptive neural networks that consider all layers of feature representation simultaneously, emphasize shallower layers of representation more heavily, and only activate deeper ones when necessary. We will represent such models using the notation: f = \u2211l k=1 \u2211nk j=1 wk,jhk,j , where wk,j\u2019s are weights and hk,j \u2208 H(p)k . The motivation for this type of network is reinforced by the following learning guarantee: Theorem 3 (ADANET Generalization Bound). Let l\u0302 \u2208 [1, l] and for each k \u2208 [1, l\u0302], let n\u0302k \u2208 [1, nk]. For k \u2208 [1, l\u0302] and j \u2208 [1, n\u0302k], let wk,j \u2208 R and hk,j \u2208 H(p)k . Finally, let r\u221e = maxj\u2208[1,n1],i\u2208[1,m] |[\u03a6(xi)]j |, and 1p + 1 q = 1. Then for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4 over the sample S of size m drawn i.i.d. according to distribution Dm, the following inequality holds for any f = \u2211l\u0302 k=1 \u2211n\u0302k j=1 wk,jhk,j with \u2211l\u0302 k=1 \u2211n\u0302k j=1 wk,j = 1:\nR(f) \u2264 R\u0302S,\u03c1 + 4\n\u03c1 l\u0302\u2211 k=1 n\u0302k\u2211 j=1 wk,j4 k\u22121r\u221e ( k\u220f i=1 \u039bin 1 q i\u22121 )\u221a 2 log(2n0) m + C(\u03c1, l,m, \u03b4),\nwhere C(\u03c1, l,m, \u03b4) = 2\u03c1 \u221a log(l) m + \u221a d 4\u03c12 log( \u03c12m log(l) )e log(l) m + log( 2\u03b4 ) 2m .\nThe generalization bound above informs us that the complexity of the neural network returned is a weighted combination of the complexities of each node in the neural network, where the weights are precisely the ones that define our network. Specifically, this again agrees with our intuition that deeper networks are more complex and suggests that if we can find a model that has both small empirical error and most of its weight on the shallower nodes, then such a model will generalize well.\nToward this goal, we will design an algorithm that directly seeks to minimize upper bounds of this generalization bound. In the process, our algorithm will train neural networks that discriminate deeper networks from shallower ones. This is a novel property that existing regularization techniques in the deep learning toolbox do not enforce. Techniques such as l2 and l1 regularization and dropout (see e.g. Goodfellow et al. [2016]) are generally applied uniformly across all nodes in the network."}, {"heading": "5 Algorithm", "text": "This section describes our algorithm, ADANET, for adaptive deep learning. ADANET adaptively grows the structure of a neural network, balancing model complexity with margin maximization. We start with a high-level description of the algorithm before proceeding to a more detailed presentation."}, {"heading": "5.1 Overview", "text": "Our algorithm starts with the network reduced to the input layer, corresponding to the input feature vector, and an output unit, fully connected, and then augments or modifies the network over T rounds. At each round, it either augments the network with a new node or updates the weights defining the function h \u2208 H(p)k of an existing node of the network at layer k. A new node may be selected at any layer k \u2208 [1, l] already populated or start on a new layer but, in all cases, it is chosen with links only to existing nodes in the network in the layer below plus a connection to the output unit. Existing nodes are either those of the input layer or nodes previously added to the network by the algorithm. Figure 1(a) illustrates this design.\nThe choice of the node to construct or update at each round is a key aspect of our algorithm. This is done by iteratively optimizing an objective function that we describe in detail later. At each round, the choice of the best node minimizing the current objective is subject to the following trade-off: the best node selected from a lower layer may not help reduce the objective as much as one selected from a higher layer; on the other hand, nodes selected from higher layers augment the network with substantially more complex functions, thereby increasing the risk of overfitting. To resolve this tension quantitatively, our algorithm selects the best node at each round based on a combination of the amount by which it helps reduce the objective and the complexity of the family of hypotheses defined nodes at that layer.\nThe output node of our network is connected to all the nodes created during these T rounds, so that the hypothesis will directly use all nodes in the network. As our theory demonstrated in Theorem 3, this can significantly reduce the complexity of our model by assigning more weight to the shallower nodes. At the same time, it also provides us the flexibility to learn larger and more complex models. In fact, the family of neural networks that we search over is actually larger than the family of feedforward neural networks typically considered using back-propagation due to these additional connections.\nAn additional more sophisticated variant of our algorithm is depicted in Figure 1(b). In this design, the nodes created at each round can be connected not just to the nodes of the previous layer, but to those of any layer below. This allows for greater model flexibility, and by modifying the definitions of the hypotheses sets 1 and 2, we can adopt a principled complexity-sensitive way for learning these types of structures as well.\nIn the next section, we give a more formal description of our algorithm, including the exact optimization problem as well as a specific search process for new nodes."}, {"heading": "5.2 Objective function", "text": "Recall the definition of our hypothesis space conv(H) = \u22c3l k=1(H (p) k \u222a (\u2212H (p) k )), which is the convex hull of all neural networks up to depth l and naturally includes all neural networks of depth l \u2013 the common hypothesis space in deep learning. Note that the set of all functions in H is infinite, since the weights corresponding to any function can be any real value inside their respective lp balls.\nDespite this challenge, we will efficiently discover a finite subset of H, denoted by {h1, . . . , hN}, that will serve as the basis for our convex combination. Here, N will also represent the maximum number of nodes in our network. Thus, we have that N = \u2211l k=1 nk, and N will also generally be\nassumed as very large. Moreover, we will actually define and update our set {h1, . . . , hN} online, in a manner that will be made precise in Section 5.4. We will also rely on the natural bijection between the two enumerations {h1, . . . , hN} and {hk,j}k\u2208[l],j\u2208[nk], depending on which is more convenient. The latter is useful for saying that hk,j \u2208 H(p)k . Moreover, for any j \u2208 [N ], we will denote by kj \u2208 [l], the layer in which hypothesis hj lies. For simplicity, we will also write as rj the Rademacher complexity of the family of functions H (p) kj containing hj : rj = Rm(H (p) kj ).\nLet x 7\u2192 \u03a6(\u2212x) be a non-increasing convex function upper bounding the 0/1 loss, x 7\u2192 1x\u22640, with \u03a6 differentiable over R and \u03a6\u2032(x) 6= 0 for all x. \u03a6 may, for instance, be the exponential function, \u03a6(x) = ex as in the AdaBoost of Freund and Schapire [1997] or the logistic function, \u03a6(x) = log(1 + ex) as in logistic regression.\nAs in regularized boosting style methods (e.g. [R\u00e4tsch et al., 2001]), our algorithm will apply coordinate descent to the following objective function over RN :\nF (w) = 1\nm m\u2211 i=1 \u03a6 ( 1\u2212 yi N\u2211 j=1 wjhj(xi) ) + N\u2211 j=1 \u0393j |wj |, (3)\nwhere \u0393j = \u03bbrj + \u03b2 with \u03bb \u2265 0 and \u03b2 \u2265 0 hyperparameters. The objective function is the sum of the empirical error based on a convex surrogate loss function x 7\u2192 \u03a6(\u2212x) of the binary loss and a regularization term. The regularization term is a weighted-l1 penalty that contains two sub-terms: a standard norm-1 regularization which admits \u03b2 as a parameter, and a term that discriminates functions hj based on their complexity (i.e. rj) and which admits \u03bb as a parameter.\nOur algorithm can be viewed as an instance of the DeepBoost algorithm of Cortes et al. [2014]. However, unlike DeepBoost, which combines decision trees, ADANET algorithm learns a deep neural network, which requires both deep learning-specific theoretical analysis as well as an online method for constructing and searching new nodes. Both of these aspects differ significantly from the decision tree framework in DeepBoost, and the latter is particularly challenging due to the fact that our hypothesis space H is infinite."}, {"heading": "5.3 Coordinate descent", "text": "Let wt = (wt,1, . . . , wt,N )> denote the vector obtained after t \u2265 1 iterations and let w0 = 0. Let ek denote the kth unit vector in RN , k \u2208 [1, N ]. The direction ek and the step \u03b7 selected at the tth round are those minimizing F (wt\u22121 + \u03b7ek). Let ft\u22121 = \u2211N j=1 wt\u22121,jhj . Then we can write\nF (wt\u22121 + \u03b7ek)= 1\nm m\u2211 i=1 \u03a6 ( 1\u2212 yift\u22121(xi)\u2212\u03b7yihk(xi) ) + \u2211 j 6=k \u0393j |wt\u22121,j |+ \u0393k|wt\u22121,k + \u03b7|,\nFor any t \u2208 [1, T ], we will maintain the following distribution Dt over our sample: Dt(i) = \u03a6\u2032 ( 1\u2212yift\u22121(xi) )\nSt , where St is a normalization factor, St = \u2211m i=1 \u03a6\n\u2032(1\u2212 yift\u22121(xi)). Moreover, for any s \u2208 [1, T ] and j \u2208 [1, N ] and a given hypothesis hj bounded by C > 0, we will consider s,j , the weighted error of hypothesis hj over the distribution Ds: s,j = C2 [ 1\u2212 Ei\u223cDs [yihj(xi) C ]] . These weighted errors will be crucial for \u201cscoring\u201d the direction that the algorithm takes at each round."}, {"heading": "5.4 Search and active coordinates", "text": "As already mentioned, a key aspect of our AdaNet algorithm is the construction of new hypotheses at each round. We do not enumerate allN hypotheses at the beginning of the algorithm, because it would be extremely difficult to select good candidates before seeing the data. At the same time, searching through all node possible combinations using the data would be a computationally infeasible task.\nInstead, our search procedure will be online, building upon the nodes that we already have in our network and selecting at most a single at a time. Specifically, at each round, the algorithm selects a\nnode out of the following set of \u201cactive\u201d candidates: existing nodes in our network or new nodes with connections to existing nodes in some layer k.\nThere are many potential methods to construct new candidate nodes, and at first glance, scoring every possible new node with connections to existing nodes may seem a computational impediment. However, by using Banach space duality, we can compute directly and efficiently in closed form the new node that best optimizes the objective at each layer."}, {"heading": "5.4.1 New candidate nodes", "text": "Given a distribution D over the sample S and a tuple of hypotheses hk = (hk,1, . . . , hk,nk) \u2282 H (p) k , we denote by Margin(D, hk,j) the weighted margin of hypothesis hk,j composed with its activation on distribution D:\nMargin(D, hk,j) = Ei\u223cD[yi(\u03d5k \u25e6 hk,j)(xi)],\nand we denote by Margin(D,hk) the vector of weighted margins of all nodes in layer k:\nMargin(D,hk) = ( Ei\u223cD[yi(\u03d5k \u25e6 hk,1)(xi)], . . . ,Ei\u223cD[yi(\u03d5k \u25e6 hk,nk)(xi)] ) .\nFor any layer k in an existing neural network, a vector u \u2208 Rnk uniquely specifies a node that connects from nodes in the previous layer k \u2212 1. Let u\u0303k denote such a new node in layer k, and let l\u0302 be the number of layers with non-zero nodes. Then for layers 2 \u2264 k \u2264 l\u0302, if the number of nodes is less than the maximum allowed size nk, we will consider as candidates the nodes with the largest weighted margin. Remarkably, these nodes can be computed efficiently and in closed form:\nLemma 4 (Construction of new candidate nodes, see proof in Appendix B). Fix (hk\u22121,j) nk\u22121 j=1 \u2282 H (p) k\u22121. Then the solution u\u0303k to the optimization problem\nmax \u2016u\u2016p\u2264\u039bk\nEi\u223cD [ yi nk\u22121\u2211 j=1 uj(\u03d5k\u22121 \u25e6 hk\u22121,j)(xi) ] ,\ncan be computed coordinate-wise as:\n(u\u0303k)j = \u039bk\n\u2016Margin(D,hk\u22121)\u2016 q p q\n|Margin(D, hk\u22121,j)|q\u22121 sgn (Margin(D, hk\u22121,j)) ,\nand the solution has value: \u039bk \u2016Margin(D,hk\u22121)\u2016q ."}, {"heading": "5.5 Pseudocode", "text": "In this section, we present the pseudocode for our algorithm, ADANET, which applies the greedy coordinate-wise optimization procedure described in Section 5.3 on the objective presented in Section 5.2 with the search procedure described in Section 5.4.\nThe algorithm takes as input the sample S, the maximum number of nodes per layer (nk)lk=1, the complexity penalties (\u0393k)lk=1, the lp norms of the weights defining new nodes (\u039bk) l k=1, and upper bounds on the nodes in each layer (Ck)lk=1. ADANET then initializes all weights to zero, sets the distribution to be uniform, and considers the active set of coordinates to be the initial layer in the network. Then the algorithm repeats the following sequence of steps: it computes the scores of the existing nodes in the method EXISTINGNODES, of the new candidate nodes in NEWNODES, and finds the node with the best score (|dk,j | or |d\u0303k|) in BESTNODE. After finding this \u201ccoordinate\u201d, it updates the step size and distribution before proceeding to the next iteration (as described in Section 5.3). The precise pseudocode is provided in Figure 2, and details of its derivation are given in Section B."}, {"heading": "5.6 Convergence of ADANET", "text": "Remarkably, the neural network that ADANET outputs is competitive against the optimal weights for any sub-network that it sees during training. Moreover, it achieves this guarantee in linear time. The precise statement and proof are provided in Appendix D."}, {"heading": "5.7 Large-scale implementation of AdaNet", "text": "We describe a large-scale implementation of the ADANET optimization problem using state-of-the-art techniques from stochastic optimization in Appendix E."}, {"heading": "6 Conclusion", "text": "We presented a new framework for analyzing and learning artificial neural networks. Our method optimizes for generalization performance, and it explicitly and automatically addresses the trade-off between model architecture and empirical risk minimization, ideas that have been under-explored in deep learning. Our techniques are general and can be applied to other neural network architectures, including CNNs and LSTMs as well as to other learning settings such as multi-class classification and regression, all of which serve as interesting avenues for future work."}, {"heading": "A Proofs of theoretical guarantees", "text": "Lemma 1. Let 1p + 1 q = 1. Then for k \u2265 2, the empirical Rademacher complexity of H (p) k for a sample S of size m can be upper bounded as follows in terms of that of H(p)k\u22121:\nR\u0302S(H (p) k ) \u2264 2\u039bkn\n1 q\nk\u22121R\u0302S(H (p) k\u22121).\nProof.\nR\u0302S(H (p) k ) =\n1 m E \u03c3  sup hj\u2208H(p)k\u22121 \u2016u\u2016p\u2264\u039bk m\u2211 i=1 \u03c3i nk\u22121\u2211 j=1 uj(\u03d5k\u22121 \u25e6 hj)(xi) \n= 1\nm E \u03c3  sup hj\u2208H(p)k\u22121 \u2016u\u2016p\u2264\u039bk nk\u22121\u2211 j=1 uj m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 hj)(xi)  =\n\u039bk m E \u03c3  sup hj\u2208H(p)k\u22121 \u2225\u2225\u2225\u2225 m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 hj)(xi) \u2225\u2225\u2225\u2225 q  (def. of dual norm) = \u039bkn 1 q\nk\u22121 m E \u03c3  sup hj\u2208H(p)k\u22121 \u2225\u2225\u2225\u2225 m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 hj)(xi) \u2225\u2225\u2225\u2225 \u221e  (equiv. of lp norms and sup) = \u039bkn 1 q\nk\u22121 m E \u03c3  sup h\u2208H(p)k\u22121 \u2223\u2223\u2223\u2223 m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 h)(xi) \u2223\u2223\u2223\u2223 \n= \u039bkn\n1 q\nk\u22121 m E \u03c3  sup h\u2208H(p)k\u22121 s\u2208{\u22121,+1} s m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 h)(xi)  (def. of absolute value)\n\u2264 \u039bkn\n1 q\nk\u22121 m E \u03c3  sup h\u2208H(p)k\u22121 m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 h)(xi)  +\n\u039bk m E \u03c3  sup h\u2208H(p)k\u22121 m\u2211 i=1 \u2212\u03c3i(\u03d5k\u22121 \u25e6 h)(xi)  = 2\u039bkn 1 q\nk\u22121 m E \u03c3  sup h\u2208H(p)k\u22121 m\u2211 i=1 \u03c3i(\u03d5k\u22121 \u25e6 h)(xi)  \u2264 2\u039bkn 1 q\nk\u22121 m E \u03c3  sup h\u2208H(p)k\u22121 m\u2211 i=1 \u03c3ih(xi)  (Talagrand\u2019s inequality) \u2264 2\u039bkn 1 q k\u22121R\u0302S(H (p) k\u22121)\nLemma 2. Let r\u221e = maxj\u2208[1,n1],i\u2208[1,m] |[\u03a6(xi)]j |, and 1p + 1 q = 1. Then for any k \u2265 1, the empirical Rademacher complexity of H(p)k for a sample S of size m can be upper bounded as follows:\nR\u0302S(H (p) k ) \u2264 2 k\u22121r\u221e ( k\u220f j=1 \u039bjn 1 q j\u22121 )\u221a 2 log(2n0) m .\nProof.\nR\u0302S(H (p) 1 ) =\n1 m E \u03c3\n[ sup\n\u2016u\u2016p\u2264\u039b1 m\u2211 i=1 \u03c3iu \u00b7\u03a6(xi)\n]\n= 1\nm E \u03c3\n[ sup\n\u2016u\u2016p\u2264\u039b1 u \u00b7 m\u2211 i=1 \u03c3i\u03a6(xi)\n]\n= \u039b1 m E \u03c3 [\u2225\u2225\u2225\u2225 m\u2211 i=1 \u03c3i[\u03a6(xi)] \u2225\u2225\u2225\u2225 q ] (def. of dual norm)\n\u2264 \u039b1n 1 q 0\nm E \u03c3 [\u2225\u2225\u2225\u2225 m\u2211 i=1 \u03c3i[\u03a6(xi)] \u2225\u2225\u2225\u2225 \u221e ] (equivalence of lp norms)\n= \u039b1n\n1 q 0\nm E \u03c3 [ max j\u2208[1,n1] \u2223\u2223\u2223\u2223 m\u2211 i=1 \u03c3i[\u03a6(xi)]j \u2223\u2223\u2223\u2223 ]\n(def. of l\u221e norm)\n= \u039b1n\n1 q 0\nm E \u03c3  max j\u2208[1,n1]\ns\u2208{\u22121,+1}\nm\u2211 i=1 \u03c3is[\u03a6(xi)]j  (def. of absolute value) \u2264 \u039b1n 1 q 0 r\u221e \u221a m \u221a 2 log(2n0)\nm = r\u221e\u039b1n\n1 q 0\n\u221a 2 log(2n0)\nm . (Massart\u2019s lemma)\nThe result then follows by application of Lemma 1.\nTheorem 3 (AdaNet Generalization Bound). Let l\u0302 \u2208 [1, k] and for each k \u2208 [1, l\u0302], let n\u0302k \u2208 [1, nk]. For k \u2208 [1, l\u0302] and j \u2208 [1, n\u0302k], let wk,j \u2208 R and hk,j \u2208 H(p)k . Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 over the sample S of size m drawn i.i.d. according to distribution Dm, the following inequality holds for any f = \u2211l\u0302 k=1 \u2211n\u0302k j=1 wk,jhk,j with \u2211l\u0302 k=1 \u2211n\u0302k j=1 wk,j = 1:\nR(f) \u2264 R\u0302S,\u03c1 + 4\n\u03c1 l\u0302\u2211 k=1 n\u0302k\u2211 j=1 wk,j4 k\u22121r\u221e ( k\u220f i=1 \u039bin 1 q i\u22121 )\u221a 2 log(2n0) m + 2 \u03c1 \u221a log(l) m\n+\n\u221a\u2308 4\n\u03c12 log\n( \u03c12m\nlog(l)\n)\u2309 log(p)\nm +\nlog( 2\u03b4 )\n2m ,\nProof. By considering the symmetrized sets H = \u22c3l k=1(H (p) k \u222a (\u2212H (p) k )), we can assume without loss of generality that the weights wk,j are non-negative.\nWe will now use the following structural learning guarantee for ensembles of hypotheses:\nLemma 5 (DeepBoost Generalization Bound, Theorem 1 Cortes et al. [2014]). Let l > 1. Assume there exists some decomposition of the hypothesis spaceH = \u222ali=1Hi. Fix \u03c1 > 0. Given any h \u2208 H and a sample S, denote the empirical margin loss as R\u0302S,\u03c1. Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 over the sample S of size m drawn i.i.d. according to distribution Dm, for any \u03b1t \u2208 R+ such that \u2211T t=1 \u03b1t = 1, the following inequality holds for f = \u2211T t=1 \u03b1tht:\nR(f) \u2264 R\u0302S,\u03c1 + 4\n\u03c1 T\u2211 t=1 \u03b1tRm(Hkt) + 2 \u03c1\n\u221a log(l)\nm\n+\n\u221a\u2308 4\n\u03c12 log\n( \u03c12m\nlog(l)\n)\u2309 log(l)\nm +\nlog( 2\u03b4 )\n2m ,\nwhere for each ht \u2208 H, kt denotes the smallest k \u2208 [1, l] such that ht \u2208 Hkt .\nLemma 5 implies that\nR(f) \u2264 R\u0302S,\u03c1 + 4\n\u03c1 l\u0302\u2211 k=1 n\u0302k\u2211 j=1 wk,jRm(H (p) k \u222a (\u2212H (p) k )) + 2 \u03c1\n\u221a log(l)\nm\n+ \u221a d 4 \u03c12 log ( \u03c12m log(l) ) e log(p) m + log( 2\u03b4 ) 2m .\nBy using symmetrization in Lemma 2, l\u0302\u2211\nk=1 n\u0302k\u2211 j=1 wk,jRm(H (p) k \u222a (\u2212H (p) k )) \u2264 l\u0302\u2211 k=1 4k\u22121r\u221e ( k\u220f i=1 \u039bin 1 q i\u22121 )\u221a2 log(2n0) m n\u0302k\u2211 j=1 wk,j ."}, {"heading": "B Proofs for algorithmic design", "text": "B.1 New candidate nodes\nLemma 4 (Construction of new candidate nodes). Fix (hk\u22121,j) nk\u22121 j=1 \u2282 H (p) k\u22121. Then the solution u\u0303k to the optimization problem\nmax \u2016u\u2016p\u2264\u039bk Ei\u223cD[yi nk\u22121\u2211 j=1 uj(\u03d5k\u22121 \u25e6 hk\u22121,j)(xi)],\ncan be computed coordinate-wise as:\n(u\u0303k)j = \u039bk\n\u2016Margin(D,hk\u22121)\u2016 q p q\n|Margin(D, hk\u22121,j)|q\u22121 sgn (Margin(D, hk\u22121,j)) ,\nand the solution has value: \u039bk \u2016Margin(D,hk\u22121)\u2016q\nProof. By linearity of expectation,\nu\u0303k = argmax \u2016u\u2016p\u2264\u039bk Ei\u223cD[yi nk\u22121\u2211 j=1 uj(\u03d5k\u22121 \u25e6 hk\u22121,j)(xi)] = argmax \u2016u\u2016p\u2264\u039bk u \u00b7Margin(D,hk\u22121).\nWe claim that\n(u\u0303k)j = \u039bk\n\u2016Margin(D,hk\u22121)\u2016 q p q\n|Margin(D, hk\u22121,j)|q\u22121 sgn (Margin(D, hk\u22121,j)) .\nTo see this, note first that by Holder\u2019s inequality, u \u00b7Margin(D,hk\u22121) \u2264 \u2016u\u2016p\u2016Margin(D,hk\u22121)\u2016q \u2264 \u039bk\u2016Margin(D,hk\u22121)\u2016q,\nand the expression on the right-hand side is our proposed value. At the same time, our choice of u\u0303k also satisfies this upper bound:\nu\u0303k \u00b7Margin(D,hk\u22121) = nk\u22121\u2211 j=1 (u\u0303k)j Margin(D, hk\u22121,j)\n= nk\u22121\u2211 j=1\n\u039bk\n\u2016Margin(D,hk\u22121)\u2016 q p q\n|Margin(D, hk\u22121,j)|q\n= \u039bk\n\u2016Margin(D,hk\u22121)\u2016 q p q\n\u2016Margin(D, hk\u22121,j)\u2016qq\n= \u039bk\u2016Margin(D,hk\u22121)\u2016q. Thus, u\u0303k is a solution to the optimization problem and achieves the claimed value.\nB.2 Derivation of coordinate descent update\nRecall the form of our objective function:\nF (wt) = 1\nm m\u2211 i=1 \u03a6 ( 1\u2212 yi l\u2211 k=1 nk\u2211 j=1 wt,k,jhk,j(xi) ) + l\u2211 k=1 nk\u2211 j=1 \u0393k|wk,j |.\nWe want to find the directional derivative with largest magnitude as well as the optimal step-size in this coordinate direction.\nSince F is non-differentiable at 0 for each coordinate (due to the weighted l1 regularization), we must choose a representative of the subgradient. Since, F is convex, it admits both left and right directional derivatives, which we denote by\n\u2207+k,jF (w) = lim \u03b7\u21920+ F (w + \u03b7ek,j)\u2212 F (w) \u03b7 , \u2207\u2212k,jF (w) = lim \u03b7\u21920\u2212 F (w + \u03b7ek,j)\u2212 F (w) \u03b7 .\nMoreover, convexity ensures that \u2207\u2212k,jF \u2264 \u2207 + k,jF . Now, let \u03b4k,jF be the element of the subgradient that we will use to compare descent magnitudes, so that (kt, jt) = argmaxk\u2208[1,l],j\u2208[1,nk] \u2223\u2223\u03b4k,jF (wt)\u2223\u2223. This subgradient will always be chosen as the one closest to 0:\n\u03b4k,jF (w) = 0 if \u2207\u2212k,jF (w) \u2264 0 \u2264 \u2207 + k,jF (w)\n= \u2207+k,j(w) if \u2207 \u2212 k,jF (w) \u2264 \u2207 + k,jF (w) \u2264 0 = \u2207\u2212k,j(w) if 0 \u2264 \u2207 \u2212 k,jF (w) \u2264 \u2207 + k,jF (w).\nSuppose that wt,k,j 6= 0. Then by continuity, for \u03b7 sufficiently small, wt,k,j and wt,k,j + \u03b7ek,j have the same sign so that\nF (wt + \u03b7ek,j) = 1\nm m\u2211 i=1 \u03a6 ( 1\u2212 yi l\u2211 k=1 nk\u2211 j=1 wt,k,jhk,j(xi)\u2212 \u03b7yihk,j(xi) )\n+ \u2211\n(k\u0303,j\u0303) 6=(k,j)\n\u0393k|wt,k\u0303,j\u0303 |+ \u0393k sgn(wt,k,j)(wt,k,j + \u03b7).\nFurthermore, F is differentiable in the (k, j)-th at wt, which implies that\n\u2207k,jF (wt) = 1\nm m\u2211 i=1 \u2212yihk,j(xi)\u03a6\u2032 ( 1\u2212 yi l\u2211 k=1 nk\u2211 j=1 wt,k,jhk,j(xi) ) + sgn(wt,k,j)\u0393k\n= 1\nm m\u2211 i=1 yihk,j(xi)Dt(i)St + sgn(wt,k,j)\u0393k\n= (2 t,k,j \u2212 Ck) St m + sgn(wt,k,j)\u0393k\nWhen wt,k,j = 0, we can consider the left and right directional derivatives:\n\u2207+k,jF (wt) = (2 t,k,j \u2212 Ck) St m + \u0393k\n\u2207\u2212k,jF (wt) = (2 t,k,j \u2212 Ck) St m \u2212 \u0393k\nMoreover, \u2223\u2223\u2223\u2223 t,k,j \u2212 Ck2 \u2223\u2223\u2223\u2223 \u2264 \u0393km2St \u21d4 \u2207\u2212k,jF (w) \u2264 0 \u2264 \u2207+k,jF (w)\nt,k,j \u2212 Ck 2 \u2264 \u2212\u0393km 2St \u21d4 \u2207\u2212k,jF (w) \u2264 \u2207 + k,jF (w) \u2264 0\n\u0393km\n2St \u2264 t,k,j \u2212 Ck 2\n\u21d4 0 \u2264 \u2207\u2212k,jF (w) \u2264 \u2207 + k,jF (w),\nso that we have\n\u03b4k,jF (wt) = (2 t,k,j \u2212 Ck) St m + sgn(wt,k,j)\u0393k if \u2223\u2223\u2223\u2223 t,k,j \u2212 Ck2 \u2223\u2223\u2223\u2223 \u2264 \u0393km2St = 0 else if\n\u2223\u2223\u2223\u2223 t,k,j \u2212 Ck2 \u2223\u2223\u2223\u2223 \u2264 \u0393km2St\n= (2 t,k,j \u2212 Ck) St m \u2212 sgn\n( t,k,j \u2212\nCk 2\n) \u0393k otherwise ."}, {"heading": "C Other components of pseudocode", "text": "Figures 5, 6, 7, 8 present the remaining components of the pseudocode for ADANET with exponential loss.\nThe initial weight vector w0 is initialized to 0, and the initial weight distribution D1 is uniform over the coordinates.\nThe best node is simply the one with the highest score |dk,j | (or |d\u0303k|) among all existing nodes and the new candidate nodes.\nThe step-size taken at each round is the optimal step in the direction computed. For exponential loss functions, this can be computed exactly, and in general, it can be approximated numerically via line search methods (since the objective is convex).\nThe updated distribution at time t will be proportional to \u03a6\u2032(1 \u2212 yift\u22121(xi), as explained in Section 5.3."}, {"heading": "D Convergence of ADANET", "text": "Theorem 6. Let \u03a6 be a twice-continuously differentiable function with \u03a6\u2032\u2032 > 0, and suppose we terminate ADANET after O ( log(1/ ) ) iterations if it does not add a new node. Let Is \u2282 [1, N ] be the first j nodes added by ADANET, and let w\u2217Is = argminw\u2208PIs (RN+ ) F (w), where PIs denotes\nprojection onto RIs . Let N\u0302 = \u2211l\u0302 k=1 n\u0302k be the total number of nodes constructed by the ADANET algorithm at termination. Then ADANET will terminate after at most O(N\u0302 log(1/ )) iterations, producing a neural network and a set of weights such that\nF (wAdaNet)\u2212 min s\u2208[1,N\u0302 ] F (w\u2217Is) <\nProof. Recall that\nF (w) = 1\nm m\u2211 i=1 \u03a6 ( 1\u2212 yi N\u2211 j=1 wjhj(xi) ) + N\u2211 j=1 \u0393j |wj |.\nSince ADANET initializes all weights to 0 and grows the coordinate space in an online fashion, we may consider the algorithm in epochs, so that if the support of wt at any given t is Is, then\nF (w) = FIs(w) = 1\nm m\u2211 i=1 \u03a6 ( 1\u2212 yi \u2211 j\u2208Is wjhj(xi) ) + \u2211 j\u2208Is \u0393j |wj |.\nFix an epoch s \u2208 [1, N\u0302 ]. The optimal set of weights within the support Is is given by w\u2217Is , and this solution exists because F is a coercive convex function (due to the weighted l1 regularization). Let M \u2208 Rm\u00d7N be a matrix with elements given by Mi,j = hj(xi)yi, and let ei \u2208 Rm be a i-th elementary basis vector of Rm. Then for any vector w \u2208 RN , e>i Mw = \u2211N j=1 wjhj(xi)yi. Thus, if we define denote by GIs(z) = 1 m \u2211m i=1 \u03a6(1\u2212 e>i z), then the first component of FIs can be written as GIs(Mw).\nMoreover, since \u03a6 is twice continuously differentiable and \u03a6\u2032\u2032 > 0, it follows that GIs is twice continuously differentiable and strictly convex. We can also compute that for any w \u2208 RN ,\nm\u2211\nwhich is positive definite since \u03a6\u2032\u2032 > 0. Finally, since H is symmetric, we can, at the cost of flipping some hj \u2192 \u2212hj , equivalently write: FIs(w) = g(Mw) + \u2211 j\u2208Is \u0393jwj , subject to wj \u2265 0.\nThus, the problem of minimizing FIs is equivalent to the optimization problem studied in Luo and Tseng [1992], and we have verified that the conditions are satisfied as well. If the algorithm doesn\u2019t add another coordinate, then it performs the Gauss-Southwell method on the coordinates Is. By Theorem 2.1 in Luo and Tseng [1992], this method will converge to the optimal set of weights with support in Is linearly. This implies that if the algorithm terminates, it will maintain the error guarantee: FIs(wADANET)\u2212 FIs(w\u2217Is) < . If the algorithm does add a new coordinate before termination, then we can apply the same argument to FIs+1 .\nThus, when the algorithm does finally terminate, we maintain the error guarantee for every subset Is \u2282 [1, N ] of nodes that we create, and the total run time is at most O(N\u0302 log(1/ )) iterations."}, {"heading": "E Large-scale implementation: ADANET+", "text": "Our optimization problem is a regularized empirical risk minimization problem of the form: F (x) = G(x) + \u03c8(x) = 1m \u2211m i=1 fi(x) + \u2211N j=1 \u03c8j(xj), where each fi is smooth and convex and \u03c8, also convex, decomposes across the coordinates of x.\nFor any subset B \u2282 [1,m], let GB = 1m \u2211 i\u2208B fi denote the components of the ERM objective that correspond to that subset. For any j \u2208 [1, N ], let\u2207j denote the partial derivative in the coordinate j. We can leverage the Mini-Batch Randomized Block Coordinate Descent (MBRCD) technique introduced by Zhao et al. [2014]. Their work can be viewed as the randomized block coordinate descent variant of the Stochastic Variance Reduced Gradient (SVRG) family of algorithms (Johnson and Zhang [2013], Xiao and Zhang [2014]).\nOur algorithm divides the entire training period into K epochs. At every step, the algorithm samples two mini-batches from [1,m] uniformly. The first is used to approximate the ERM objective for the descent step, and the second is used to apply the NEWNODES subroutine in Figure 4 to generate new candidate coordinates. After generating these coordinates, the algorithm samples a coordinate from the set of new coordinates and the existing ones. Based on the coordinate chosen, it updates the active set. Then the algorithm computes a gradient descent step with the SVRG estimator in place of the gradient and with the sampled approximation of the true ERM objective. It then makes a proximal update with \u03c8 as the proximal function. Finally, the \u201ccheckpoint parameter\u201d of the SVRG estimator is updated at the end of every epoch.\nWhile our optimization problem itself is not strongly convex, we can apply the MBRCD indirectly by adding a strongly convex regularizer, solving: F\u0303 (x) = F (x) + \u03b3R(x), where R is a 1-strongly convex function, to still yield a competitive guarantee.\nMoreover, since our non-smooth component is a weighted-l1 term, the proximal update can be solved efficiently and in closed form in a manner that is similar to the Iterative Shrinkage-Thresholding Algorithm (ISTA) of Beck and Teboulle [2009].\nFigure 9 presents the pseudocode of the algorithm, ADANET+."}, {"heading": "Appendix References", "text": "A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183\u2013202, 2009.\nR. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.\nZ.-Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7 \u2013 35, 1992.\nL. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.\nT. Zhao, M. Yu, Y. Wang, R. Arora, and H. Liu. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems, pages 3329\u20133337, 2014."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.", "creator": "LaTeX with hyperref package"}}}