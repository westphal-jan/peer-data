{"id": "1607.02789", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2016", "title": "Charagram: Embedding Words and Sentences via Character n-grams", "abstract": "firstly we all present charagram embeddings, continuing a common simple performance approach for informally learning naive character - set based compositional problem models to embed large textual sequences. a word puzzle or sentence reference is only represented using a character - n - cube gram count vector, essentially followed successfully by typing a single nonlinear image transformation to yield a random low - dimensional embedding. although we just use relatively three particular tasks for memory evaluation : random word similarity, sentence similarity, and part - of - ear speech tagging. we demonstrate that multiple charagram dictionary embeddings outperform more complex architectures entirely based on diverse character - base level recurrent messages and multiple convolutional neural coding networks, achieving new visual state - of - the - art performance schemes on exactly several similarity tasks.", "histories": [["v1", "Sun, 10 Jul 2016 21:59:19 GMT  (36kb)", "http://arxiv.org/abs/1607.02789v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john wieting", "mohit bansal", "kevin gimpel", "karen livescu"], "accepted": true, "id": "1607.02789"}, "pdf": {"name": "1607.02789.pdf", "metadata": {"source": "CRF", "title": "CHARAGRAM: Embedding Words and Sentences via Character n-grams", "authors": ["John Wieting", "Mohit Bansal Kevin Gimpel", "Karen Livescu"], "emails": ["jwieting@ttic.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 7.\n02 78\n9v 1\n[ cs\n.C L\n] 1\n0 Ju"}, {"heading": "1 Introduction", "text": "Representing textual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015).\nSome prior work has found benefit from using character-based compositional models that\n1Trained models and code are available at http://ttic.uchicago.edu/\u02dcwieting.\nencode arbitrary character sequences into vectors. Examples include recurrent neural networks (RNNs) and convolutional neural networks (CNNs) on character sequences, showing improvements for several NLP tasks (Ling et al., 2015a; Kim et al., 2015; Ballesteros et al., 2015; dos Santos and Guimara\u0303es, 2015). By sharing subword information across words, character models have the potential to better represent rare words and morphological variants.\nOur approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded into a low-dimensional space using a single nonlinear transformation. This can be interpreted as learning embeddings of character n-grams, which are learned so as to produce effective sequence embeddings when a summation is performed over the character n-grams in the sequence.\nWe consider three evaluations: word similarity, sentence similarity, and part-of-speech tagging. On multiple word similarity datasets, CHARAGRAM outperforms RNNs and CNNs, achieving state-of-the-art performance on SimLex999 (Hill et al., 2015). When evaluated on a large suite of sentence-level semantic textual similarity tasks, CHARAGRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM-PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al. (2015a). The three architectures reach\nsimilar performance, though CHARAGRAM converges fastest to high accuracy.\nWe perform extensive analysis of our CHARAGRAM embeddings. We find large gains in performance on rare words, showing the empirical benefit of subword modeling. We also compare performance across different character n-gram vocabulary sizes, finding that the semantic tasks benefit far more from large vocabularies than the syntactic task. However, even for challenging semantic similarity tasks, we still see strong performance with only a few thousand character n-grams.\nNearest neighbors show that CHARAGRAM embeddings simultaneously address differences due to spelling variation, morphology, and word choice. Inspection of embeddings of particular character ngrams reveals etymological links; e.g., die is close to mort. We release our resources to the community in the hope that CHARAGRAM can provide a strong baseline for subword-aware text representation."}, {"heading": "2 Related Work", "text": "We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015).\nA recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long short-term memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Sev-\neral have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Manning, 2016), or for generating entire translations character-bycharacter (Chung et al., 2016).\nSutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupa\u0142a, 2013; Evang et al., 2013), and text normalization (Chrupa\u0142a, 2014).\nCNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimara\u0303es, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Jussa\u0300 and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (Jo\u0301zefowicz et al., 2016).\nMost closely-related to our approach is the DSSM (instantiated variously as \u201cdeep semantic similarity model\u201d or \u201cdeep structured semantic model\u201d) developed by Huang et al. (2013). For an information retrieval task, they represented words using feature vectors containing counts of character ngrams. Sperr et al. (2013) used a very similar technique to represent words in neural language models for machine translation. Our CHARAGRAM embeddings are based on this same idea. We show this strategy to be extremely effective when applied to both words and sentences, outperforming character LSTMs like those used by Ling et al. (2015a) and character CNNs like those from Kim et al. (2015)."}, {"heading": "3 Models", "text": "We now describe models that embed textual sequences using their characters, including our CHARAGRAM model and the baselines that we compare to. We denote a character-based textual sequence by x = \u3008x1, x2, ..., xm\u3009, which includes space characters between words as well as special start-of-sequence and end-of-sequence characters. We use xji to denote the subsequence of char-\nacters from position i to position j inclusive, i.e., x j i = \u3008xi, xi+1, ..., xj\u3009, and we define x i i = xi.\nOur CHARAGRAM model embeds a character sequence x by adding the vectors of its character ngrams followed by an elementwise nonlinearity:\ngCHAR(x) = h\n\nb+\nm+1 \u2211\ni=1\ni \u2211\nj=1+i\u2212k\nI [ xij \u2208 V ] W x i j\n\n\n(1) where h is a nonlinear function, b \u2208 Rd is a bias vector, k is the maximum length of any character ngram, I[p] is an indicator function that returns 1 if p is true and 0 otherwise, V is the set of character ngrams included in the model, and W x i j \u2208 Rd is the vector for character n-gram xij . The set V is used to restrict the model to a predetermined set (vocabulary) of character n-grams. Below, we compare several choices for defining this set. The number of parameters in the model is d + d|V |. This model is based on the letter n-gram hashing technique developed by Huang et al. (2013) for their DSSM approach. One can also view Eq. (1) (as they did) as first populating a vector of length |V | with counts of character n-grams followed by a nonlinear transformation.\nWe compare the CHARAGRAM model to two other models. First we consider LSTM architectures (Hochreiter and Schmidhuber, 1997) over the character sequence x, using the version from Gers et al. (2003). We use a forward LSTM over the characters in x, then take the final LSTM hidden vector as the representation of x. Below we refer to this model as \u201ccharLSTM.\u201d\nWe also compare to convolutional neural network (CNN) architectures, which we refer to below as \u201ccharCNN.\u201d We use the architecture from Kim (2014) with a single convolutional layer followed by an optional fully-connected layer. We use filters of varying lengths of character n-grams, using two primary configurations of filter sets, one of which is identical to that used by Kim et al. (2015). Each filter operates over the entire sequence of character n-grams in x and we use max pooling for each filter. We tune over the choice of nonlinearity for both the convolutional filters and for the optional fully-connected layer. We give more details below about filter sets, n-gram lengths, and nonlinearities.\nWe note that using character n-gram convolutional filters is similar to our use of character ngrams in the CHARAGRAM model. The difference is that, in the CHARAGRAM model, the n-gram must match exactly for its vector to affect the representation, while in the CNN each filter will affect the representation of all sequences (depending on the nonlinearity being used). So the CHARAGRAM model is able to learn precise vectors for particular character n-grams with specific meanings, while there is pressure for the CNN filters to capture multiple similar patterns that recur in the data. Our qualitative analysis shows the specificity of the learned character ngram vectors learned by the CHARAGRAM model."}, {"heading": "4 Experiments", "text": "We perform three sets of experiments. The goal of the first two (Section 4.1) is to produce embeddings for textual sequences such that the embeddings for paraphrases have high cosine similarity. Our third evaluation (Section 4.2) is a classification task, and follows the setup of the English part-of-speech tagging experiment from Ling et al. (2015a)."}, {"heading": "4.1 Word and Sentence Similarity", "text": "We compare the ability of our models to capture semantic similarity for both words and sentences. We train on noisy paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) with an L2 regularized contrastive loss objective function, following the training procedure of Wieting et al. (2015) and Wieting et al. (2016). Key details are provided here, but see Appendix A for a fuller description."}, {"heading": "4.1.1 Datasets", "text": "For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013).\nFor sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter\ntask (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent.\nEach STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015)."}, {"heading": "4.1.2 Preliminaries", "text": "For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases.\nBefore training the CHARAGRAM model, we need to populate V , the vocabulary of character n-grams included in the model. We obtain these from the training data used for the final models in each setting, which is either the lexical or phrasal section of PPDB XXL. We tune over whether to include the full sets of character n-grams in these datasets or only those that appear more than once.\nWhen extracting n-grams, we include spaces and add an extra space before and after each word or phrase in the training and evaluation data to ensure that the beginning and end of each word is represented. We note that strong performance can be obtained using far fewer character n-grams; we explore the effects of varying the number of n-grams and the n-gram orders in Section 4.4.\nWe used Adam (Kingma and Ba, 2014) with a learning rate of 0.001 to learn the parameters in the following experiments."}, {"heading": "4.1.3 Word Embedding Experiments", "text": "Training and Tuning For hyperparameter tuning, we used one epoch on the lexical section of PPDB XXL, which consists of 770,007 word pairs. We used either WS353 or SL999 for model selection (reported below). We then took the selected hyperparameters and trained for 50 epochs to ensure that all models had a chance to converge.\nFull details of our tuning procedure are provided in Appendix B. In short, we tuned all models thoroughly, tuning the activation functions for CHARAGRAM and charCNN, as well as the regularization strength, mini-batch size, and sampling type for all models. For charCNN, we experimented with two filter sets: one uses 175 filters for each n-gram size \u2208 {2, 3, 4}, and the other uses the set of filters from Kim et al. (2015), consisting of 25 filters of size 1, 50 of size 2, 75 of size 3, 100 of size 4, 125 of size 5, and 150 of size 6. We also experimented with using dropout (Srivastava et al., 2014) on the inputs of the last layer of the charCNN model in place of L2 regularization, as well as removing the last feedforward layer. Neither of these variations significantly improved performance on our suite of tasks for word or sentence similarity. However, using more filters does improve performance, seemingly linearly with the square of the number of filters.\nArchitecture Comparison The results are shown in Table 1. The CHARAGRAM model outperforms both the charLSTM and charCNN models, and also outperforms recent strong results on SL999.\nWe also found that the charCNN and charLSTM models take far more epochs to converge than the CHARAGRAM model. We noted this trend across experiments and explore it further in Section 4.3.\nComparison to Prior Work We found that performance of CHARAGRAM on word similarity tasks can be improved by using more character n-grams. This is explored in Section 4.4. Our best result from these experiments was obtained with the largest model we considered, which contains 173,881 ngram embeddings. When using WS353 for model selection and training for 25 epochs, this model achieves 70.6 on SL999. To our knowledge, this is the best result reported on SL999 in this setting; Table 2 shows comparable recent results. Note that a higher SL999 number is reported in (Mrks\u030cic\u0301 et al., 2016), but the setting is not comparable to ours as they started with embeddings tuned on SL999.\nLastly, we evaluated our model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013), using SL999 for model selection. We obtained a Spearman\u2019s \u03c1 of 47.1, which outperforms the 41.8 result from Soricut and Och (2015) and is competitive with the 47.8 reported in Pennington et al. (2014), despite only using PPDB for training."}, {"heading": "4.1.4 Sentence Embedding Experiments", "text": "Training and Tuning We did initial training of our models using one pass through PPDB XL, which consists of 3,033,753 unique phrase pairs. Following Wieting et al. (2016), we use the annotated phrase pairs developed by Pavlick et al. (2015) as our validation set, using Spearman\u2019s \u03c1 to rank the models. We then take the highest performing models and train on the 9,123,575 unique phrase pairs in the phrasal section of PPDB XXL for 10 epochs.\nFor all experiments, we fix the mini-batch size to 100, the margin \u03b4 to 0.4, and use MAX sampling (see Appendix A). For the CHARAGRAM model,\nV contains all 122,610 character n-grams (n \u2208 {2, 3, 4}) in the PPDB XXL phrasal section. The other tuning settings are the same as in Section 4.1.3.\nFor another baseline, we train the PARAGRAMPHRASE model of Wieting et al. (2016), tuning its regularization strength over {10\u22125, 10\u22126, 10\u22127, 10\u22128}. The PARAGRAMPHRASE model simply uses word averaging as its composition function, but outperforms many more complex models.\nIn this section, we refer to our model as CHARAGRAM-PHRASE because the input is a character sequence containing multiple words rather than only a single word as in Section 4.1.3. Since the vocabulary V is defined by the training data sequences, the CHARAGRAM-PHRASE model includes character n-grams that span multiple words, permitting it to capture some aspects of word order and word co-occurrence, which the PARAGRAMPHRASE model is unable to do.\nWe encountered difficulties training the charLSTM and charCNN models for this task. We tried several strategies to improve their chance at convergence, including clipping gradients, increasing training data, and experimenting with different optimizers and learning rates. We found success by using the original (confidence-based) ordering of the PPDB phrase pairs for the initial epoch of learning, then shuffling them for subsequent epochs. This is similar to curriculum learning (Bengio et al., 2009). The higher-confidence phrase pairs tend to be shorter and have many overlapping words, possibly making them easier to learn from.\nResults An abbreviated version of the sentence similarity results is shown in Table 3; Appendix C contains the full results. For comparison, we report performance for the median (50%), third quartile (75%), and top-performing (Max) systems from the shared tasks. We observe strong performance for the CHARAGRAM-PHRASE model. It always does better than the charCNN and charLSTM models, and outperforms the PARAGRAM-PHRASE model on 15 of the 22 tasks. Furthermore, CHARAGRAM-PHRASE matches or exceeds the top-performing task-tuned systems on 5 tasks, and is within 0.003 on 2 more. The charLSTM and charCNN models are signifi-\ncantly worse, with the charCNN being the better of the two and beating PARAGRAM-PHRASE on 4 of the tasks.\nWe emphasize that there are many other models that could be compared to, such as an LSTM over word embeddings. This and many other models were explored by Wieting et al. (2016). Their PARAGRAM-PHRASE model, which simply learns word embeddings within an averaging composition function, was among their best-performing models. We used this model in our experiments as a stronglyperforming representative of their results.\nLastly, we note other recent work that considers a similar transfer learning setting. The FastSent model (Hill et al., 2016) uses the 2014 STS task as part of its evaluation and reports an average Pearson\u2019s r of 61.3, much lower than the 74.7 achieved by CHARAGRAM-PHRASE on the same datasets."}, {"heading": "4.2 POS Tagging Experiments", "text": "We now consider part-of-speech (POS) tagging, since it has been used as a testbed for evaluating architectures for character-level word representations. It also differs from semantic similarity, allowing us to evaluate our architectures on a syntactic task. We replicate the POS tagging experimental setup of Ling et al. (2015a). Their model uses a bidirectional LSTM over character embeddings to represent words. They then use the resulting word representations in another bidirectional LSTM that predicts the tag for each word. We replace their character bidirectional LSTM with our three architectures: char-\nCNN, charLSTM, and CHARAGRAM. We use the Wall Street Journal portion of the Penn Treebank, using Sections 1-18 for training, 19-21 for tuning, and 22-24 for testing. We set the dimensionality of the character embeddings to 50 and that of the (induced) word representations to 150. For optimization, we use stochastic gradient descent with a mini-batch size of 100 sentences. The learning rate and momentum are set to 0.2 and 0.95 respectively. We train the models for 50 epochs, again to ensure that all models have an opportunity to converge.\nThe other settings for our models are mostly the same as for the word and sentence experiments (Section 4.1). We again use character n-grams with n \u2208 {2, 3, 4}, tuning over whether to include all 54,893 in the training data or only those that occur more than once. However, there are two minor differences from the previous sections. First, we add a single binary feature to indicate if the token contains a capital letter. Second, our tuning considers rectified linear units as the activation function for the CHARAGRAM and charCNN architectures.2\nThe results are shown in Table 4. Performance is similar across models. We found that adding a second fully-connected 150 dimensional layer to the CHARAGRAM model improved results slightly.3"}, {"heading": "4.3 Convergence", "text": "One observation we made during our experiments was that different models converged at significantly different rates. Figure 1 plots the performance of the word similarity and tagging tasks as a function of\n2We did not consider ReLU for the similarity experiments because the final embeddings are used directly to compute cosine similarities, which led to poor performance when restricting the embeddings to be non-negative.\n3We also tried adding a second (300 dimensional) layer for the word and sentence embedding models and found that it hurt performance.\nthe number of examples processed during training. For word similarity, we plot the oracle Spearman\u2019s \u03c1 on SL999, while for tagging we plot tagging accuracy on the validation set. We evaluate performance every quarter epoch (approximately every 194,252 word pairs) for word similarity and every epoch for tagging. We only show the first 10 epochs of training in the tagging plot.\nThe plots show that the CHARAGRAM model converges quickly to high performance. The charCNN and charLSTM models take many more epochs to converge. Even with tagging, which uses a very high learning rate, CHARAGRAM converges significantly faster than the others. For word similarity, it appears that charCNN and charLSTM are still slowly improving at the end of 50 epochs. This suggests that if training was done for a much longer period, and possibly on more data, the charLSTM or charCNN models could match and surpass the CHARAGRAM model. However, due to the large training sets available from PPDB and the computational requirements of these architectures, we were unable to explore the regime of training for many epochs. We conjecture that slow convergence could be the reason for the inferior performance of LSTMs for similarity tasks as reported by Wieting et al. (2016)."}, {"heading": "4.4 Model Size Experiments", "text": "The default setting for our CHARAGRAM and CHARAGRAM-PHRASE models is to use all character bigram, trigrams, and 4-grams that occur in the training data at least C times, tuning C over the set {1, 2}. This results in a large number of parameters, which could be seen as an unfair advantage over the comparatively smaller charCNN and charLSTM models, which have up to 881,025 and 763,200 parameters respectively in the similarity experiments.4\nOn the other hand, for a given training example, very few parameters in the CHARAGRAM model are actually used. For the charCNN and charLSTM models, by contrast, all parameters are used except the character embeddings for those characters that are not present in the example. For a sentence with 100 characters, and when using the 300-dimensional CHARAGRAM model with bigrams, trigrams, and 4- grams, there are approximately 90,000 parameters in use for this sentence, far fewer than those used by the charCNN and charLSTM for the same sentence.\nWe performed a series of experiments to investigate how the CHARAGRAM and CHARAGRAMPHRASE models perform with different numbers and lengths of character n-grams. For a given k, we took the top k most frequent character n-grams for each value of n in use. We experimented with k values in {100, 1000, 50000}. If there were fewer than k unique character n-grams for a given n, we used all of them. For these experiments, we did very little tuning, setting the regularization strength to 0 and only tuning over the activation function. We repeated this experiment for all three of our tasks. For word similarity, we report performance on SL999 after training for 5 epochs on the lexical section of PPDB XXL. For sentence similarity, we report the\n4This includes 134 character embeddings.\naverage Pearson\u2019s r over all 22 datasets after training for 5 epochs on the phrasal section of PPDB XL. For tagging, we report accuracy on the validation set after training for 50 epochs. The results are shown in Table 5.\nWhen using extremely small models with only 100 n-grams of each order, we still see relatively strong performance on POS tagging. However, the semantic similarity tasks require far more n-grams to yield strong performance. Using 1000 n-grams clearly outperforms 100, and 50,000 n-grams performs best."}, {"heading": "5 Analysis", "text": ""}, {"heading": "5.1 Quantitative Analysis", "text": "One of our primary motivations for character-based models is to address the issue of out-of-vocabulary (OOV) words, which were found to be one of the main sources of error for the PARAGRAM-PHRASE model from Wieting et al. (2016). They reported a negative correlation (Pearson\u2019s r of -0.45) between OOV rate and performance. We took the 12,108 sentence pairs in all 20 SemEval STS tasks and binned them by the total number of unknown words in the pairs.5 We computed Pearson\u2019s r over each bin. The results are shown in Table 6.\nThe CHARAGRAM-PHRASE model has better performance for each number of unknown words. The PARAGRAM-PHRASE model degrades when more unknown words are present, presumably because it is forced to use the same unknown word embedding for all unknown words. The CHARAGRAM-PHRASE\n5Unknown words were defined as those not present in the 1.7 million unique (case-insensitive) tokens that comprise the vocabulary for the GloVe embeddings available at http://nlp.stanford.edu/projects/glove/. The PARAGRAM-SL999 embeddings, used to initialize the PARAGRAM-PHRASE model, use this same vocabulary.\nmodel has no notion of unknown words, as it can embed any character sequence.\nWe next investigated the sensitivity of the two models to length, as measured by the maximum of the lengths of the two sentences in a pair. We binned all of the 12,108 sentence pairs in the 20 SemEval STS tasks by length and then again found the Pearson\u2019s r for both the PARAGRAM-PHRASE and CHARAGRAM-PHRASE models. The results are shown in Table 7.\nWe find that both models are robust to sentence length, achieving the highest correlations on the longest sentences. We also find that CHARAGRAMPHRASE outperforms PARAGRAM-PHRASE at all sentence lengths.\n5.2 Qualitative Analysis\nAside from OOVs, the PARAGRAM-PHRASE model lacks the ability to model word order or cooccurrence, since it simply averages the words in the sequence. We were interested to see whether CHARAGRAM-PHRASE could handle negation, since it does model limited information about word order (via character n-grams that span multiple words in the sequence). We made a list of \u201cnot\u201d bigrams that could be represented by a single word, then embedded each bigram using both models and did a\nnearest-neighbor search over a working vocabulary.6 The results, in Table 8, show how the CHARAGRAMPHRASE embeddings model negation. In all cases but one, the nearest neighbor is a paraphrase for the bigram and the next neighbors are mostly paraphrases as well. The PARAGRAM-PHRASE model, unsurprisingly, is incapable of modeling negation. In all cases, the nearest neighbor is not, as this word carries much more weight than the word it modifies. The remaining nearest neighbors are either the modified word or stalled.\nWe did two additional nearest neighbor explorations with our CHARAGRAM-PHRASE model. In the first, we collected the nearest neighbors for words that were not in the training data (i.e. PPDB XXL), but were in our working vocabulary. This consisted of 59,660 words. In the second, we collected nearest neighbors of words that were in our training data which consisted of 37,765 tokens.\nA sample of the nearest neighbors is shown in Table 9. Several kinds of similarity are being captured simultaneously by the model. One kind is similarity in terms of spelling variation, including misspellings (vehicals, vehicels, and vehicles) and repetition for emphasis (baby and babyyyyyyy). Another kind is similarity in terms of morphological variants of a shared root (e.g., journeying and journey). We also see that the model has learned many strong synonym relationships without significant amounts of over-\n6This contained all words in PPDB-XXL, our evaluations, and in two other datasets: the Stanford Sentiment task (Socher et al., 2013) and the SNLI dataset (Bowman et al., 2015), resulting in 93,217 unique (up-to-casing) tokens.\nlapping n-grams (e.g., vehicles, cars, and automobiles). We find these characteristics for words both in and out of the training data. Words in the training data, which tend to be more commonly used, do tend to have higher precision in their nearest neighbors (e.g., see neighbors for huge). We noted occasional mistakes for words that share a large number of ngrams but are not paraphrases (see nearest neighbors for litered which is likely a misspelling of littered).\nLastly, since our model learns embeddings for character n-grams, we include an analysis of character n-gram nearest neighbors in Table 10. These n-grams appear to be grouped into themes, such as death (first row), food (second row), and speed (third row), but have different granularities. The n-grams in the last row appear in paraphrases of 2, whereas the second-to-last row shows n-grams in words like french and vocabulary, which can broadly be classified as having to do with language."}, {"heading": "6 Conclusion", "text": "We performed a careful empirical comparison of character-based compositional architectures on three\nNLP tasks. While most prior work has considered machine translation, language modeling, and syntactic analysis, we showed how characterlevel modeling can improve semantic similarity tasks, both quantitatively and with extensive qualitative analysis. We found a consistent trend: the simplest architecture converges fastest to high performance. These results, coupled with those from Wieting et al. (2016), suggest that practitioners should begin with simple architectures rather than moving immediately to RNNs and CNNs. We release our code and trained models so they can be used by the NLP community for general-purpose, character-based text representation."}, {"heading": "Acknowledgments", "text": "We would like to thank the developers of Theano (Theano Development Team, 2016) and NVIDIA Corporation for donating GPUs used in this research."}, {"heading": "Appendix A Training", "text": "For word and sentence similarity, we follow the training procedure of Wieting et al. (2015) and Wieting et al. (2016), described below. For part-ofspeech tagging, we follow the English Penn Treebank training procedure of Ling et al. (2015a).\nFor the similarity tasks, the training data consists of a set X of phrase pairs \u3008x1, x2\u3009 from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), where x1 and x2 are assumed to be paraphrases. We optimize a margin-based loss:\nmin \u03b8\n1\n|X |\n(\n\u2211\n\u3008x1,x2\u3009\u2208X\nmax(0, \u03b4 \u2212 cos(g(x1), g(x2))\n+ cos(g(x1), g(t1))) + max(0, \u03b4 \u2212 cos(g(x1), g(x2))\n+ cos(g(x2), g(t2)))\n)\n+ \u03bb \u2016\u03b8\u20162\nwhere g is the embedding function in use, \u03b4 is the margin, the full set of parameters is contained in \u03b8 (e.g., for the CHARAGRAM model, \u03b8 = \u3008W, b\u3009), \u03bb is the L2 regularization coefficient, and t1 and t2 are carefully selected negative examples taken from a mini-batch during optimization (discussed below).\nIntuitively, we want the two phrases to be more similar to each other (cos(g(x1), g(x2))) than either is to their respective negative examples t1 and t2, by a margin of at least \u03b4.\nA.1 Selecting Negative Examples\nTo select t1 and t2 in Eq. 2, we tune the choice between two approaches. The first, MAX, simply chooses the most similar phrase in some set of phrases (other than those in the given phrase pair). For simplicity and to reduce the number of tunable parameters, we use the mini-batch for this set, but it could be a separate set. Formally, MAX corresponds to choosing t1 for a given \u3008x1, x2\u3009 as follows:\nt1 = argmax t:\u3008t,\u00b7\u3009\u2208Xb\\{\u3008x1,x2\u3009} cos(g(x1), g(t))\nwhere Xb \u2286 X is the current mini-batch. That is, we want to choose a negative example ti that is similar to xi according to the current model parameters. The downside of this approach is that we may occasionally choose a phrase ti that is actually a true paraphrase of xi.\nThe second strategy selects negative examples using MAX with probability 0.5 and selects them randomly from the mini-batch otherwise. We call this sampling strategy MIX. We tune over the choice of strategy in our experiments."}, {"heading": "Appendix B Tuning Word Similarity", "text": "Models\nFor all architectures, we tuned over the mini-batch size (25 or 50) and the type of sampling used (MIX or MAX). \u03b4 was set to 0.4 and the dimensionality d of each model was set to 300.\nFor the CHARAGRAM model, we tuned the activation function h (tanh or linear) and regularization coefficient \u03bb (over {10\u22124, 10\u22125, 10\u22126}). The n-gram vocabulary V contained all 100,283 character n-grams (n \u2208 {2, 3, 4}) in the lexical section of PPDB XXL.\nFor charCNN and charLSTM, we randomly initialized 300 dimensional character embeddings for all unique characters in the training data. For charLSTM, we tuned over whether to include an output gate. For charCNN, we tuned the filter activation function (rectified linear or tanh) and tuned the activation for the fully-connected layer (tanh or linear).\nFor both the charLSTM and charCNN models, we tuned \u03bb over {10\u22124, 10\u22125, 10\u22126}."}, {"heading": "Appendix C Full Sentence Similarity", "text": "Results\nTable 11 shows the full results of our sentence similarity experiments."}], "references": [{"title": "SemEval-2012 task 6: A pilot on semantic textual similarity", "author": ["Agirre et al.2012] Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume", "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "SEM 2013 shared task: Semantic textual similarity", "author": ["Agirre et al.2013] Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Agirre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "SemEval-2014 task 10: Multilingual semantic textual similarity", "author": ["Agirre et al.2014] Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Factored neural language models", "author": ["Alexandrescu", "Katrin Kirchhoff"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,", "citeRegEx": "Alexandrescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu et al\\.", "year": 2006}, {"title": "Improved transition-based parsing by modeling characters instead of words", "author": ["Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Bengio et al.2009] Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "venue": "Proceedings of ACL", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Joint learning of character and word embeddings", "author": ["Chen et al.2015] Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan"], "venue": "In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.06147", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Characterbased neural machine translation", "author": ["Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": "arXiv preprint arXiv:1603.00810", "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["dos Santos", "Guimar\u00e3es2015] Cicero dos Santos", "Victor Guimar\u00e3es"], "venue": "In Proceedings of the Fifth Named Entity Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Morpheme-based feature-rich language models using deep neural networks for lvcsr of egyptian arabic", "author": ["Hong-Kwang Jeff Kuo", "Lidia Mangu", "Hagen Soltau"], "venue": null, "citeRegEx": "Mousa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mousa et al\\.", "year": 2013}, {"title": "Elephant: Sequence labeling for word and sentence segmentation", "author": ["Evang et al.2013] Kilian Evang", "Valerio Basile", "Grzegorz Chrupa\u0142a", "Johan Bos"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Evang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Evang et al\\.", "year": 2013}, {"title": "Non-distributional word vector representations. arXiv preprint arXiv:1506.05230", "author": ["Faruqui", "Dyer2015] Manaal Faruqui", "Chris Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Ppdb: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In HLT-NAACL", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers et al.2003] Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He et al.2015] Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Embedding word similarity with neural machine translation", "author": ["Hill et al.2014] Felix Hill", "Kyunghyun Cho", "Sebastien Jean", "Coline Devin", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6448", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Hill et al.2016] Felix Hill", "Kyunghyun Cho", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1602.03483", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of the 22nd ACM international conference on Confer-", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling. CoRR, abs/1602.02410", "author": ["Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models. CoRR, abs/1508.06615", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Compositional-ly derived representations of morphologically complex words in distributional semantics", "author": ["Marco Marelli", "Roberto Zamparelli", "Marco Baroni"], "venue": "In Proceedings of the 51st Annual Meeting", "citeRegEx": "Lazaridou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015a] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015b] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid wordcharacter models. arXiv preprint arXiv:1604.00788", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness", "author": ["Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Counter-fitting word vectors to linguistic constraints", "author": ["Mrk\u0161i\u0107 et al.2016] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "TsungHsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Pushpendre Rastogi", "Juri Ganitkevich", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Ng."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Ng.,? 2011", "shortCiteRegEx": "Ng.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] Radu Soricut", "Franz Och"], "venue": "In Proc. NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Letter n-gram-based input encoding for continuous space language models", "author": ["Sperr et al.2013] Henning Sperr", "Jan Niehues", "Alex Waibel"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Sperr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sperr et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "From paraphrase database to compositional paraphrase model and back. Transactions of the ACL (TACL)", "author": ["Wieting et al.2015] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Dan Roth"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["Wieting et al.2016] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of International Conference on Learning Representations", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}, {"title": "SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT)", "author": ["Xu et al.2015] Wei Xu", "Chris Callison-Burch", "William B Dolan"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al.", "startOffset": 72, "endOffset": 119}, {"referenceID": 50, "context": ", 2015) to functions with rich recursive structure (Socher et al., 2011; Tai et al., 2015; Bowman et al., 2016).", "startOffset": 51, "endOffset": 111}, {"referenceID": 8, "context": ", 2015) to functions with rich recursive structure (Socher et al., 2011; Tai et al., 2015; Bowman et al., 2016).", "startOffset": 51, "endOffset": 111}, {"referenceID": 50, "context": "learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015).", "startOffset": 52, "endOffset": 87}, {"referenceID": 21, "context": "learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015).", "startOffset": 52, "endOffset": 87}, {"referenceID": 29, "context": "Examples include recurrent neural networks (RNNs) and convolutional neural networks (CNNs) on character sequences, showing improvements for several NLP tasks (Ling et al., 2015a; Kim et al., 2015; Ballesteros et al., 2015; dos Santos and Guimar\u00e3es, 2015).", "startOffset": 158, "endOffset": 254}, {"referenceID": 4, "context": "Examples include recurrent neural networks (RNNs) and convolutional neural networks (CNNs) on character sequences, showing improvements for several NLP tasks (Ling et al., 2015a; Kim et al., 2015; Ballesteros et al., 2015; dos Santos and Guimar\u00e3es, 2015).", "startOffset": 158, "endOffset": 254}, {"referenceID": 26, "context": "We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded into a low-dimensional space using a single nonlinear transformation.", "startOffset": 98, "endOffset": 118}, {"referenceID": 23, "context": "achieving state-of-the-art performance on SimLex999 (Hill et al., 2015).", "startOffset": 52, "endOffset": 71}, {"referenceID": 22, "context": "achieving state-of-the-art performance on SimLex999 (Hill et al., 2015). When evaluated on a large suite of sentence-level semantic textual similarity tasks, CHARAGRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM-PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al.", "startOffset": 53, "endOffset": 293}, {"referenceID": 22, "context": "achieving state-of-the-art performance on SimLex999 (Hill et al., 2015). When evaluated on a large suite of sentence-level semantic textual similarity tasks, CHARAGRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM-PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al. (2015a). The three architectures reach", "startOffset": 53, "endOffset": 425}, {"referenceID": 32, "context": "subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015).", "startOffset": 19, "endOffset": 105}, {"referenceID": 42, "context": "subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015).", "startOffset": 19, "endOffset": 105}, {"referenceID": 9, "context": "subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015).", "startOffset": 19, "endOffset": 105}, {"referenceID": 35, "context": "Luong et al. (2013) used recursive models to compose morphs into word", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": ", 2015b; Luong and Manning, 2016), or for generating entire translations character-bycharacter (Chung et al., 2016).", "startOffset": 95, "endOffset": 115}, {"referenceID": 31, "context": "Ling et al. (2015a) used a bidirectional long short-term memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Ballesteros et al. (2015) used this model to represent words for dependency parsing.", "startOffset": 0, "endOffset": 26}, {"referenceID": 15, "context": "Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupa\u0142a, 2013; Evang et al., 2013), and text normalization (Chrupa\u0142a, 2014).", "startOffset": 126, "endOffset": 162}, {"referenceID": 19, "context": "(2011) and Graves (2013) used character-level RNNs for language modeling.", "startOffset": 11, "endOffset": 25}, {"referenceID": 29, "context": "CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar\u00e3es, 2015), text classification (Zhang et al.", "startOffset": 127, "endOffset": 145}, {"referenceID": 54, "context": ", 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar\u00e3es, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss\u00e0 and Fonollosa, 2016).", "startOffset": 144, "endOffset": 164}, {"referenceID": 28, "context": "Combinations of CNNs and RNNs on characters have also been explored (J\u00f3zefowicz et al., 2016).", "startOffset": 68, "endOffset": 93}, {"referenceID": 26, "context": "Most closely-related to our approach is the DSSM (instantiated variously as \u201cdeep semantic similarity model\u201d or \u201cdeep structured semantic model\u201d) developed by Huang et al. (2013). For an information retrieval task, they represented words using feature vectors containing counts of character ngrams.", "startOffset": 159, "endOffset": 179}, {"referenceID": 26, "context": "Most closely-related to our approach is the DSSM (instantiated variously as \u201cdeep semantic similarity model\u201d or \u201cdeep structured semantic model\u201d) developed by Huang et al. (2013). For an information retrieval task, they represented words using feature vectors containing counts of character ngrams. Sperr et al. (2013) used a very similar technique to represent words in neural language models for machine translation.", "startOffset": 159, "endOffset": 319}, {"referenceID": 31, "context": "both words and sentences, outperforming character LSTMs like those used by Ling et al. (2015a) and character CNNs like those from Kim et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 29, "context": "(2015a) and character CNNs like those from Kim et al. (2015).", "startOffset": 43, "endOffset": 61}, {"referenceID": 26, "context": "This model is based on the letter n-gram hashing technique developed by Huang et al. (2013) for their DSSM approach.", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": "First we consider LSTM architectures (Hochreiter and Schmidhuber, 1997) over the character sequence x, using the version from Gers et al. (2003). We use a forward LSTM over the characters in x, then take the final LSTM hidden", "startOffset": 126, "endOffset": 145}, {"referenceID": 30, "context": "\u201d We use the architecture from Kim (2014) with a single convolutional layer followed by an optional fully-connected layer.", "startOffset": 31, "endOffset": 42}, {"referenceID": 29, "context": "which is identical to that used by Kim et al. (2015). Each filter operates over the entire sequence of character n-grams in x and we use max pooling for each filter.", "startOffset": 35, "endOffset": 53}, {"referenceID": 33, "context": "2) is a classification task, and follows the setup of the English part-of-speech tagging experiment from Ling et al. (2015a).", "startOffset": 105, "endOffset": 125}, {"referenceID": 18, "context": "We train on noisy paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) with an L2 regularized contrastive loss objective function, following the training procedure of Wieting et al.", "startOffset": 64, "endOffset": 97}, {"referenceID": 18, "context": "We train on noisy paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) with an L2 regularized contrastive loss objective function, following the training procedure of Wieting et al. (2015) and Wieting et al.", "startOffset": 71, "endOffset": 216}, {"referenceID": 18, "context": "We train on noisy paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) with an L2 regularized contrastive loss objective function, following the training procedure of Wieting et al. (2015) and Wieting et al. (2016). Key details are provided here, but see Appendix A for a", "startOffset": 71, "endOffset": 242}, {"referenceID": 17, "context": "For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999", "startOffset": 147, "endOffset": 173}, {"referenceID": 23, "context": "(SL999) (Hill et al., 2015).", "startOffset": 8, "endOffset": 27}, {"referenceID": 36, "context": "We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013).", "startOffset": 77, "endOffset": 97}, {"referenceID": 53, "context": "task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al.", "startOffset": 5, "endOffset": 22}, {"referenceID": 37, "context": ", 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014).", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015).", "startOffset": 63, "endOffset": 147}, {"referenceID": 1, "context": "Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015).", "startOffset": 63, "endOffset": 147}, {"referenceID": 2, "context": "Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015).", "startOffset": 63, "endOffset": 147}, {"referenceID": 48, "context": "We also experimented with using dropout (Srivastava et al., 2014) on the inputs of the last layer of the charCNN model in place of L2 regularization, as well as removing the last feedforward layer.", "startOffset": 40, "endOffset": 65}, {"referenceID": 29, "context": "For charCNN, we experimented with two filter sets: one uses 175 filters for each n-gram size \u2208 {2, 3, 4}, and the other uses the set of filters from Kim et al. (2015), consisting of 25 filters of size 1, 50 of size 2, 75 of size 3, 100 of size 4, 125 of size 5, and 150 of size 6.", "startOffset": 149, "endOffset": 167}, {"referenceID": 22, "context": "Model SL999 Hill et al. (2014) 52 Schwartz et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 22, "context": "Model SL999 Hill et al. (2014) 52 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al.", "startOffset": 12, "endOffset": 57}, {"referenceID": 22, "context": "Model SL999 Hill et al. (2014) 52 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al.", "startOffset": 12, "endOffset": 84}, {"referenceID": 22, "context": "Model SL999 Hill et al. (2014) 52 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al. (2015) 66.", "startOffset": 12, "endOffset": 109}, {"referenceID": 39, "context": "Note that a higher SL999 number is reported in (Mrk\u0161i\u0107 et al., 2016), but the setting is not comparable to ours as they started with embeddings tuned on SL999.", "startOffset": 47, "endOffset": 68}, {"referenceID": 36, "context": "Lastly, we evaluated our model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013), using SL999 for model selection.", "startOffset": 76, "endOffset": 96}, {"referenceID": 35, "context": "Lastly, we evaluated our model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013), using SL999 for model selection. We obtained a Spearman\u2019s \u03c1 of 47.1, which outperforms the 41.8 result from Soricut and Och (2015) and is competitive with the 47.", "startOffset": 77, "endOffset": 229}, {"referenceID": 35, "context": "Lastly, we evaluated our model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013), using SL999 for model selection. We obtained a Spearman\u2019s \u03c1 of 47.1, which outperforms the 41.8 result from Soricut and Och (2015) and is competitive with the 47.8 reported in Pennington et al. (2014), despite only using PPDB", "startOffset": 77, "endOffset": 299}, {"referenceID": 43, "context": "Training and Tuning We did initial training of our models using one pass through PPDB XL, which consists of 3,033,753 unique phrase pairs. Following Wieting et al. (2016), we use the annotated phrase pairs developed by Pavlick et al.", "startOffset": 6, "endOffset": 171}, {"referenceID": 40, "context": "(2016), we use the annotated phrase pairs developed by Pavlick et al. (2015) as our validation set, using Spearman\u2019s \u03c1 to rank the models.", "startOffset": 55, "endOffset": 77}, {"referenceID": 44, "context": "For another baseline, we train the PARAGRAMPHRASE model of Wieting et al. (2016), tuning its regularization strength over {10\u22125, 10\u22126, 10\u22127, 10\u22128}.", "startOffset": 64, "endOffset": 81}, {"referenceID": 5, "context": "This is similar to curriculum learning (Bengio et al., 2009).", "startOffset": 39, "endOffset": 60}, {"referenceID": 44, "context": "We emphasize that there are many other models that could be compared to, such as an LSTM over word embeddings. This and many other models were explored by Wieting et al. (2016). Their PARAGRAM-PHRASE model, which simply learns word embeddings within an averaging composition function, was among their best-performing models.", "startOffset": 106, "endOffset": 177}, {"referenceID": 24, "context": "The FastSent model (Hill et al., 2016) uses the 2014 STS task as part of its evaluation and reports an average Pearson\u2019s r of 61.", "startOffset": 19, "endOffset": 38}, {"referenceID": 33, "context": "We replicate the POS tagging experimental setup of Ling et al. (2015a). Their model uses a bidirectional LSTM over character embeddings to represent words.", "startOffset": 51, "endOffset": 71}, {"referenceID": 44, "context": "We conjecture that slow convergence could be the reason for the inferior performance of LSTMs for similarity tasks as reported by Wieting et al. (2016). Task # n-grams 2 2,3 2,3,4 2,3,4,5 2,3,4,5,6", "startOffset": 135, "endOffset": 152}, {"referenceID": 44, "context": "One of our primary motivations for character-based models is to address the issue of out-of-vocabulary (OOV) words, which were found to be one of the main sources of error for the PARAGRAM-PHRASE model from Wieting et al. (2016). They reported a negative correlation (Pearson\u2019s r of -0.", "startOffset": 212, "endOffset": 229}, {"referenceID": 45, "context": "This contained all words in PPDB-XXL, our evaluations, and in two other datasets: the Stanford Sentiment task (Socher et al., 2013) and the SNLI dataset (Bowman et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 7, "context": ", 2013) and the SNLI dataset (Bowman et al., 2015), resulting in 93,217 unique (up-to-casing) tokens.", "startOffset": 29, "endOffset": 50}, {"referenceID": 44, "context": "While most prior work has considered machine translation, language modeling, and syntactic analysis, we showed how characterlevel modeling can improve semantic similarity tasks, both quantitatively and with extensive qualitative analysis. We found a consistent trend: the simplest architecture converges fastest to high performance. These results, coupled with those from Wieting et al. (2016), suggest that practitioners should begin with simple architectures rather than moving immediately to RNNs and CNNs.", "startOffset": 60, "endOffset": 394}, {"referenceID": 42, "context": "For word and sentence similarity, we follow the training procedure of Wieting et al. (2015) and Wieting et al.", "startOffset": 54, "endOffset": 92}, {"referenceID": 42, "context": "For word and sentence similarity, we follow the training procedure of Wieting et al. (2015) and Wieting et al. (2016), described below.", "startOffset": 54, "endOffset": 118}, {"referenceID": 33, "context": "For part-ofspeech tagging, we follow the English Penn Treebank training procedure of Ling et al. (2015a).", "startOffset": 85, "endOffset": 105}, {"referenceID": 18, "context": "For the similarity tasks, the training data consists of a set X of phrase pairs \u3008x1, x2\u3009 from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), where x1 and x2 are assumed to be paraphrases.", "startOffset": 118, "endOffset": 151}], "year": 2016, "abstractText": "We present CHARAGRAM embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that CHARAGRAM embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.1", "creator": "LaTeX with hyperref package"}}}