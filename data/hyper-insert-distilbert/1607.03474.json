{"id": "1607.03474", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2016", "title": "Recurrent Highway Networks", "abstract": "many sequential digital processing tasks require complex generic nonlinear transition loop functions advancing from one step to the proceeding next. however, recurrent neural networks performed with such'deep'transition functions remain difficult to effectively train, ideally even when using alternatively long or short - stay term memory networks. we slowly introduce further a novel modular theoretical parameter analysis of constrained recurrent neurological networks based on ger \\ t v { s } le gorin's circle entropy theorem that illuminates both several formal modeling and optimization issues and additionally improves our understanding of the lstm learning cell. continuing based on this analysis field we essentially propose recurrent highway stream networks ( rhn ), proteins which generally are long not truly only exist in time regions but also in discrete space, together generalizing fast lstms to larger versus step - free to - step depths. experiments indicate that modelling the proposed architecture results in two complex but nevertheless efficient nonlinear models, beating existing previous models allowed for character prediction on entering the hutter \u2013 prize parameter dataset with far less than half of achieving the parameters.", "histories": [["v1", "Tue, 12 Jul 2016 19:36:50 GMT  (126kb,D)", "http://arxiv.org/abs/1607.03474v1", "9 pages, 5 figures. Submitted to NIPS conference 2016"], ["v2", "Thu, 11 Aug 2016 17:07:42 GMT  (136kb,D)", "http://arxiv.org/abs/1607.03474v2", "11 pages, 5 figures, 3 tables. Submitted to NIPS conference 2016"], ["v3", "Thu, 27 Oct 2016 19:39:22 GMT  (133kb,D)", "http://arxiv.org/abs/1607.03474v3", "13 pages, 6 figures, 2 tables"], ["v4", "Fri, 3 Mar 2017 21:10:42 GMT  (145kb,D)", "http://arxiv.org/abs/1607.03474v4", "12 pages, 6 figures, 3 tables"], ["v5", "Tue, 4 Jul 2017 19:29:23 GMT  (145kb,D)", "http://arxiv.org/abs/1607.03474v5", "12 pages, 6 figures, 3 tables"]], "COMMENTS": "9 pages, 5 figures. Submitted to NIPS conference 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["julian georg zilly", "rupesh kumar srivastava", "jan koutn\u00edk", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1607.03474"}, "pdf": {"name": "1607.03474.pdf", "metadata": {"source": "CRF", "title": "Recurrent Highway Networks", "authors": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "J\u00fcrgen Schmidhuber"], "emails": ["jzilly@ethz.ch", "juergen}@idsia.ch"], "sections": [{"heading": "1 Introduction & Previous Work", "text": "Network depth is of central importance in the resurgence of neural networks as a powerful machine learning paradigm [1]. Theoretical evidence indicates that deeper networks can be exponentially more efficient at representing certain function classes (see e.g. [2] and references therein). Due to their sequential nature, Recurrent Neural Networks (RNNs; 3\u20135) are deep in time. However, certain internal function mappings in RNNs usually do not take advantage of depth [6].\nUnfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139]. Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation. These training difficulties were first studied in the context of standard RNNs where the depth through time is proportional to the length of input sequence, which may have arbitrary size. The widely used Long Short-Term Memory (LSTM; 13, 14) architecture was introduced to specifically address the problem of vanishing/exploding gradients for recurrent networks.\nAs the amount of available computational resources grows and complex learning problems are attacked, the vanishing gradient problem also becomes a limitation when training very deep feedforward networks. The recently introduced Highway Layers [15] based on the LSTM cell address this limitation enabling the training of networks even with hundreds of stacked layers. These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].\n\u2217These authors contributed equally.\nar X\niv :1\n60 7.\n03 47\n4v 1\n[ cs\n.L G\n] 1\n2 Ju\nIn an attempt to add depth to the recurrent state transition in standard RNNs, Pascanu et al. [6] introduced Deep Transition RNNs (DT-RNNs) and Deep Transition RNNs with Skip connections (DT(S)-RNNs). While being powerful in principle, these architectures suffered from exacerbated gradient propagation issues. Due to this reason, deep state transitions for RNNs have not been widely adopted until now. A known way of creating large step-by-step depth is to let an RNN tick for several \"micro time steps\u201d per step of the sequence [19\u201321], where the RNN could be an LSTM with forget gates. However, then the LSTM has to learn by itself which parameters to use for memories of previous events and which for standard deep nonlinear processing.\nIn this paper we first provide a new mathematical analysis of standard RNNs which offers a deeper understanding of various recurrent network architectures. Based on these insights, we introduce LSTM networks that are not only long in time but also long in space (per time step), and call them Recurrent Highway Networks or RHN. They enable the use of substantially deeper transition architectures than have previously been trained while still maintaining successful backpropagation of the gradient. As a result, it becomes possible to construct powerful and trainable sequential models efficiently. Recurrent Highway Networks enable a powerful alternative method to make recurrent networks more expressive and complement other approaches such as stacked LSTM layers [22]."}, {"heading": "2 Revisiting Gradient Flow in Recurrent Networks", "text": "Let L denote the total loss for an input sequence of length T . Let x[t] \u2208 Rm and y[t] \u2208 Rn represent the output of a standard RNN at time t, W \u2208 Rn\u00d7m and R \u2208 Rn\u00d7n the input and recurrent weight matrices and b \u2208 Rn a bias vector. Then y[t] = f(Wx[t] +Ry[t\u22121] + b) describes the dynamics of a standard RNN. The derivative of the loss L with respect to parameters \u03b8 of a network can be expanded using the chain rule:\ndL d\u03b8\n= \u2211\n1\u2264t2\u2264T\ndL[t2]\nd\u03b8 = \u2211 1\u2264t2\u2264T \u2211 1\u2264t1\u2264t2 \u2202L[t2] \u2202y[t2] \u2202y[t2] \u2202y[t1] \u2202y[t1] \u2202\u03b8 . (1)\nThe Jacobian matrix \u2202y [t2]\n\u2202y[t1] , the key factor for the transport of the error from time step t2 to time step\nt1, is obtained by chaining the derivatives across all time steps:\n\u2202y[t2] \u2202y[t1] := \u220f t1<t\u2264t2 \u2202y[t] \u2202y[t\u22121] = \u220f t1<t\u2264t2 R>diag [ f \u2032(Ry[t\u22121]) ] , (2)\nwhere the input and bias have been omitted for simplicity. We can now obtain conditions for the gradients to vanish [10\u201312]. Let A := \u2202y [t]\n\u2202y[t\u22121] define the temporal Jacobian, \u03b3 be a maximal bound on\n\u03c3\u2032(Ry[t\u22121]) and \u03c3max be the largest singular value of R>. Then the norm of the Jacobian satisfies:\n\u2016A\u2016 \u2264 \u2225\u2225R>\u2225\u2225\u2225\u2225\u2225diag[f \u2032(Ry[t\u22121])]\u2225\u2225\u2225 \u2264 \u03b3\u03c3max, (3)\nwhich together with (2) provides the conditions for vanishing gradients (\u03b3\u03c3max < 1). Note that \u03b3 depends on the activation function f , e.g. |tanh\u2032(x)| \u2264 1, |\u03c3\u2032(x)| \u2264 14 ,\u2200x \u2208 R, where \u03c3 is a logistic sigmoid. Similarly, we can show that if the spectral radius \u03c1 of A is greater than 1, exploding gradients will emerge since \u2016A\u2016 \u2265 \u03c1. This description of the problem in terms of largest singular values or the spectral radius sheds light on boundary conditions for vanishing and exploding gradients yet does not illuminate how the eigenvalues are distributed overall. By applying the Ger\u0161gorin circle theorem we are able to provide further insight into the problem.\nGer\u0161gorin circle theorem (GCT) [23]: For any square matrix A \u2208 Rn\u00d7n,\nspec(A) \u2282 \u22c3\ni\u2208{1,...,n}\n\u03bb \u2208 C| \u2016\u03bb\u2212 aii\u2016C \u2264 n\u2211\nj=1,j 6=i\n|aij |  , (4)\ni.e., the eigenvalues of matrix A, comprising the spectrum of A, are located within the union of the complex circles centered around the diagonal values aii of A with radius \u2211n j=1,j 6=i |aij | equal to the sum of the absolute values of the non-diagonal entries in each row of A. Two example Ger\u0161gorin circles referring to differently initialized RNNs are depicted in Figure 1.\nUsing GCT we can understand the relationship between the entries of R and the possible locations of the eigenvalues of the Jacobian. Shifting the diagonal values aii shifts the possible locations of eigenvalues. Having large off-diagonal entries will allow for a large spread of eigenvalues. Small off-diagonal entries yield smaller radii and thus a more confined distribution of eigenvalues around the diagonal entries aii.\nLet us assume that matrix R is initialized with a zero-mean Gaussian distribution. We can then infer the following:\n\u2022 If the values of R are initialized with a standard deviation close to 0, then the spectrum of A, which is largely dependent on R, is also initially centered around 0. An example of a Ger\u0161gorin circle that could then be corresponding to a row of A is circle (1) in Figure 1. The magnitude of most of A\u2019s eigenvalues |\u03bbi| are initially likely to be substantially smaller than 1. Additionally, employing the commonly used L1/L2 weight regularization will also limit the magnitude of the eigenvalues.\n\u2022 Alternatively, if entries of R are initialized with a large standard deviation, the radii of the Ger\u0161gorin circles corresponding to A increase. Hence, A\u2019s spectrum may possess eigenvalues with norms greater 1 resulting in exploding gradients. As the radii are summed over the size of the matrix, larger matrices will have an associated larger circle radius. In consequence, larger matrices should be initialized with correspondingly smaller standard deviations to avoid exploding gradients.\nLe et al. [24] proposed to initialize R with identity and small random values on the off-diagonals. This changes the situation depicted by GCT \u2013 the result of the identity initialization is indicated by circle (2) in Figure 1. Initially, since aii = 1, the spectrum described in GCT is centered around 1, ensuring that gradients are less likely to vanish. However, this is not a flexible remedy. During training some eigenvalues can easily become larger than one, resulting in exploding gradients. We conjecture that due to this reason, extremely small learning rates were used by Le et al. [24]. Most importantly, unlike variants of LSTM, other RNNs have no direct mechanism to rapidly regulate their Jacobian eigenvalues across time steps, which can be efficient and necessary for complex sequence processing."}, {"heading": "3 Recurrent Highway Networks (RHN)", "text": "Highway layers [25] enable easy training of very deep feedforward networks through the use of adaptive computation. Let h = H(x,WH), t = T (x,WT ), c = C(x,WC) be outputs of nonlinear transforms H,T and C with associated weight matrices (including biases) WH,T,C . T and C typically utilize a sigmoid (\u03c3) nonlinearity and are referred to as the transform and the carry gates since they regulate the passing of the transformed input via H or the carrying over of the original input x. The Highway layer computation is defined as\ny = h \u00b7 t+ x \u00b7 c, (5)\nwhere \"\u00b7\" denotes element-wise multiplication.\nRecall that the recurrent state transition in a standard RNN is described by y[t] = f(Wx[t] + Ry[t\u22121] + b). We propose to construct a Recurrent Highway Network (RHN) layer with one or multiple Highway layers in the recurrent state transition. The number of layers in the recurrent state transition will in the following be referred to as recurrence depth. Formally, let WH,T,C \u2208 Rn\u00d7m and RH`,T`,C` \u2208 Rn\u00d7n represent the weights matrices of the H nonlinear transform and the T and C gates at layer ` \u2208 {1, . . . , L}. The biases are denoted by bH`,T`,C` \u2208 Rn and let s` denote the intermediate output at layer `. Then an RHN layer with a recurrence depth of L is described by\ns [t] ` = h [t] ` \u00b7 t [t] ` + s [t] `\u22121 \u00b7 c [t] ` , (6)\nwhere\nh [t] ` = tanh(WHx [t]I{`=1} +RH`s [t] `\u22121 + bH`), (7)\nt [t] ` = \u03c3(WTx [t]I{`=1} +RT`s [t] `\u22121 + bT`), (8)\nc [t] ` = \u03c3(WCx [t]I{`=1} +RC`s [t] `\u22121 + bC`), (9)\nand I{} is the indicator function. A schematic illustration of the RHN computation graph is shown in Figure 2. The output of the RHN layer is the output of the Lth Highway layer i.e. y[t] = s[t]L .\nNote that x[t] is directly transformed only by the first Highway layer (` = 1) in the recurrent transition1 and for this layer s[t]`\u22121 is the RHN layer\u2019s output of the previous time step, or concisely s [t] 0 = y\n[t\u22121]. Subsequent Highway layers only process the outputs of the previous ones. Dotted vertical lines in Figure 2 separate multiple Highway layers in the recurrent transition.\nFor conceptual clarity, it is important to observe that an RHN layer with L = 1 is a basic variant of an LSTM layer. Similar to other variants such as those studied by [26], it retains the essential components of the LSTM \u2013 multiplicative gating units controlling the flow of information through additive cells. However, an RHN layer naturally extends to L > 1, extending the LSTM to model far more complex state transitions. Like Highway and LSTM layers, other variants can also be constructed without changing the basic principles, for example by fixing one or both of the gates to always be open, or coupling the gates as done for the experiments in this paper.\nThe simpler formulation of RHN layers allows an analysis similar to standard RNNs based on GCT. Omitting the inputs and biases, the temporal Jacobian for an RHN layer with recurrence depth of 1 (such that y[t] = h[t] \u00b7 t[t] + y[t\u22121] \u00b7 c[t]) is\nA := \u2202y[t]\n\u2202y[t\u22121] = diag(c[t]) +H\u2032diag(t[t]) +C\u2032diag(y[t\u22121]) +T\u2032diag(h[t]), (10)\nwhere\nH\u2032 = R>Hdiag [ tanh\u2032(RHy [t\u22121]) ] , (11)\nT\u2032 = R>T diag [ \u03c3\u2032(RTy [t\u22121]) ] , (12)\nC\u2032 = R>Cdiag [ \u03c3\u2032(RCy [t\u22121]) ] , (13)\n1This is not strictly necessary, but simply a convenient choice.\nand has a spectrum of:\nspec(A) \u2282 \u22c3\ni\u2208{1,...,n}\n{ \u03bb \u2208 C \u2223\u2223 \u2225\u2225\u2225\u03bb\u2212 c[t]i \u2212H\u2032iit[t]i \u2212C\u2032iiy[t\u22121]i \u2212T\u2032iih[t]i \u2225\u2225\u2225C \u2264\nn\u2211 j=1,j 6=i \u2223\u2223t[t]i +C\u2032ijy[t\u22121]i +T\u2032ijh[t]i \u2223\u2223}. (14) Equation 14 captures the influence of the gates on the eigenvalues of A. Compared to the situation for standard RNN, it can be seen that an RHN layer has more flexibility in adjusting the centers and radii of the Ger\u0161gorin circles. In particular, two limiting cases can be noted. If all carry gates are fully open and transform gates are fully closed, we have c = 1n, t = 0n and T\u2032 = C\u2032 = 0n\u00d7n (since \u03c3 is saturated). This results in\nc = 1n, t = 0n \u21d2 \u03bbi = 1 \u2200i \u2208 {1, . . . , n}, (15)\ni.e. all eigenvalues are set to 1 since the Ger\u0161gorin circle radius is shrunk to 0 and each diagonal entry is set to ci = 1. In the other limiting case, if c = 0n and t = 1n then the eigenvalues are simply those of H\u2032. As the gates vary between 0 and 1, each of the eigenvalues of A can be dynamically adjusted to any combination of the above limiting behaviors.\nThe key takeaways from the above analysis are as follows. Firstly, GCT allows us to observe the behavior of the full spectrum of the temporal Jacobian, and the effect of gating units on it. We expect that for learning multiple temporal dependencies from real-world data efficiently, it is not sufficient to avoid vanishing and exploding gradients. The gates in RHN layers provide a more versatile setup for dynamically remembering, forgetting and transforming information compared to standard RNNs. Secondly, based on the above observations it becomes reasonable to expect that certain sequential problems require a capacity to express complicated gating functions to rapidly regulate information flow. Depth is a widely used method to add such expression to functions, motivating us to use multiple layers of H , T and C transformations. In this paper we opt for extending RHN layers to L > 1 using Highway layers in favor of simplicity and ease of training. However, we expect that in some cases stacking plain layers for these transformations can also be useful. Finally, the analysis of the RHN layer\u2019s flexibility in controlling its spectrum furthers our theoretical understanding of LSTM and Highway networks and their variants. For feedforward Highway networks, the Jacobian of the layer transformation (\u2202y/\u2202x) takes place of the temporal Jacobian in the above analysis. We can then observe that each Highway layer allows increased flexibility in controlling how various components of the input get transformed or carried. This flexibility is the likely reason behind the performance improvement from Highway layers even in cases where network depth is not high [17]."}, {"heading": "4 Experiments", "text": "In this work, the carry gate was coupled to the transform gate by setting C(\u00b7) = 1n \u2212 T (\u00b7) similar to the suggestion for Highway networks. This prevents an unbounded blow-up of state values leading to more stable training. An output non-linearity similar to LSTM networks could alternatively be used to combat this issue. Additionally, we set the initial bias of the gates to negative values at the start of training to facilitate information flow. All networks use a single hidden RHN layer since we are only interested in studying the effect of recurrence depth, and not of stacking multiple layers, which is already known to be useful. Detailed configurations for all experiments are included in the supplementary material. Code and results database for the experiments will be publicly released in the near future."}, {"heading": "4.1 Optimization", "text": "RHN is an architecture designed to enable the optimization of recurrent networks with deep transitions. Therefore, the primary experimental verification we seek is whether RHNs with higher recurrence depth are easier to optimize compared to other alternatives, preferably using simple gradient based methods.\nWe compare optimization of RHNs to DT-RNNs and DT(S)-RNNs [6]. Networks with recurrence depth of 1, 2, 4 and 6 are trained for next step prediction on the JSB Chorales polyphonic music prediction dataset [27]. Network sizes are chosen such that the total number of network parameters increases as the recurrence depth increases, but remains same across architectures. A hyperparameter search is then conducted for SGD-based optimization of each architecture and depth combination for fair comparisons. In the absence of optimization difficulties, larger networks should reach a similar or better loss value compared to smaller networks. However, the swarm plot in Figure 3 shows that both DT-RNN and DT(S)-RNN get considerably harder to optimize with increasing depth. Increasing the recurrence depth does not adversely affect optimization of RHNs. These results are similar to those obtained in an optimization study on feedforward Highway networks [25]."}, {"heading": "4.2 Sequence Modeling", "text": "Wikipedia: We train an RHN with recurrence depth of 5 and 864 units on the challenging Hutter Prize Wikipedia dataset (enwik8) [28]. The task is next symbol prediction with 205 unicode symbols in total. Training is performed using SGD with momentum and weight noise as used by Graves [29]. Due\n0 50 100 150 200 Epochs\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\nP er\npl ex\nity\nDepth\u00a01 Depth\u00a02 Depth\u00a03 Training Validation\nFigure 4: Training and Validation set perplexity over the course of training on Penn Treebank word-level language modeling using RHNs with fixed parameter budget and increasing recurrence depth.\nTable 1: Bits per character (BPC) on the Hutter Wikipedia dataset (without dynamic evaluation).\nArchitecture BPC # Param. Test Data\nStacked LSTM 1.67 27.0 M last 4 MB GF-RNN 1.58 20.0 M last 5 MB Grid-LSTM 1.47 16.8 M last 5 MB RHN 1.42 8.0 M last 5 MB\nto its size (100 M characters in total) and complexity (inclusion of Latin/non-Latin alphabets, XML markup and various special characters) this dataset allows us to stress the learning and generalization capacity of RHNs. The previous best result for this task was obtained using a 6-layer Grid-LSTM with tied weights. The RHN outperforms it using less than half the parameters, fewer units per layer and no weight-tying, demonstrating that adding depth to the recurrent transition efficiently utilizes parameters to improve model capacity. Table 1 compares our result to previous models.\nPenn Treebank: To examine the effect of recurrence depth we trained RHNs with one RHN layer and fixed total parameters (10 M) but with recurrence depths of 1, 2 and 3 for word level language modeling on the Penn TreeBank dataset [30]. Figure 4 shows the progress of training and validation set perplexity for each depth corresponding to the best learning rate for each depth selected based on validation set perplexity. Best scores obtained on validation set for each depth are shown in Table 2. The lowest value reached for both scores improves as the recurrence depth increases for a fixed parameter budget, demonstrating that deep transitions can be used to obtain improved performance even on small scale datasets."}, {"heading": "5 Analysis", "text": "We analyze the inner workings of RHNs through inspection of gate activations, and their effect on network performance. For the RHN with 6-layered recurrence optimized on the JSB Chorales dataset (subsection 4.1), Figure 5(a) shows the mean transform gate activity in each layer over time steps for 4 example sequences. We note that while the gates are biased towards zero (white) at initialization, all layers are utilized in the trained network. The gate activity in the first layer of the recurrent transition is typically high on average, indicating that at least one layer of recurrent transition is almost always utilized. Gates in other layers have varied behavior, dynamically switching their activity over time in a different way for each sequence.\nThe contributions of the layers towards network performance can be quantified through a lesioning experiment similar to that introduced by Srivastava et al. [25]. For one layer at a time, all the gates are pushed towards carry behavior by setting the bias to a large negative value, and the resulting loss is measured. Figure 5(b) shows the change in loss due to the biasing of each layer, and hence its contribution to the network performance. In this case, we find that the first layer contributes several times more to the overall performance compared to others. It is notable that removing any layer hurts the performance substantially due to the recurrent nature of the network."}, {"heading": "6 Conclusion", "text": "We developed a new analysis of the behavior of RNNs based on the Ger\u0161gorin Circle Theorem. The analysis provides insights into other recently proposed architectures such as IRNNs and Highway networks. A main result is the showcasing of inherent limitations of RNNs and the ability of gates to variably influence learning. Furthermore, we introduced Recurrent Highway Networks, a powerful new model designed to take advantage of increased depth in the recurrent transition, without incurring additional training problems. Experiments confirmed the theoretical optimization advantages as well as improved performance on sequence modeling tasks due to increased recurrence depth. Future work will compare RHN to a pure LSTM with several micro-ticks per time step.\nAcknowledgements: This research was partially supported by the H2020 project \u201cIntuitive Natural Prosthesis UTilization\u201d (INPUT; #687795) and SNSF grant \u201cAdvanced Reinforcement Learning\u201d (#156682). We thank Klaus Greff, Sjoerd van Steenkiste and Bas Steunebrink for many insightful discussions. The Brainstorm [31] library was used for all experiments excluding those on Penn Treebank, for which we modified the Torch7 [32] code provided by Zaremba et al. [33]."}, {"heading": "7 Supplementary Material", "text": ""}, {"heading": "7.1 Details of Experimental Setups", "text": "Optimization\nIn these experiments, we compare RHNs to Deep Transition RNNs (DT-RNNs) and Deep Transition RNNs with Skip connections (DT(S)-RNNs) introduced by [6]. A total number of 60 runs per architecture and depth is recorded. The number of units in each layer of the recurrence is fixed to {1.5\u00d7 105, 3\u00d7 105, 6\u00d7 105, 9\u00d7 105} for recurrence depths of 1, 2, 4 and 6, respectively. The batch size is set to 32. The tanh(\u00b7) is used as the activation function for the nonlinear layers. A maximum of 1000 epochs with stopping after 100 epochs without improvement are specified. Random search is conducted sampling the initial transform gate bias from {0,\u22121,\u22122,\u22123}. The initial learning rate is sampled uniformly (on logarithmic scale) between 100 and 10\u22124. Finally, all weights are initialized with a Gaussian distribution with standard deviation sampled uniformly (on logarithmic scale) from 10\u22122 to 10\u22128. For these experiments, optimization was performed using stochastic gradient descent with momentum, where momentum was set to 0.9.\nWikipedia\nThe Wikipedia dataset, created by Hutter [28], is split into sequences of length 50 as done by Graves in [29]. This dataset incorporates clear long-term dependencies introduced through the HTML-language the data is presented in.\nWe trained an RHN with 5 stacked layers in the recurrent state transition with 864 units, creating a network with a total of 8 million parameters to demonstrate the increased capacity the stacking of Highway layers in the recurrence of an RNN may offer. A constant learning rate of 0.001 with momentum of 0.9 and batch size 100 was used. A very weak L2-weight decay of 1e-6 was applied. Crucially for training, the activation of the previous sequence, each of length 50, was kept to enable learning of very long-term dependencies as done in [29]. Without this context, long-term dependencies are extremely hard to learn. Additive Gaussian weight noise [34] was added to prevent overfitting and iteratively increased to 0.03 in increments of 0.01. When training did not continue to improve the training loss, the learning rate was divided by a factor of 2.\nPenn Treebank\nThe Penn Treebank text corpus [30] is a comparatively small standard benchmark in language modeling. To showcase the influence of recurrence depth, we trained and compared RHNs with recurrence depth of {1, 2, 3} with a total budget of 10 M parameters. This leads to RHN sizes of 458, 441, and 427, respectively. A setup based on regularization of LSTMs using dropout by Zaremba et al. [33] was used with 1 RHN layer, batch size 20, sequence length 35, learning rates of {0.1, 0.05, 0.01}, learning rate decay of 1.01, dropout of 0.5 and a maximum gradient norm of 5. All weights were initialized from a uniform distribution between [\u22120.001, 0.001]."}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Technical Report CUED/F-INFENG/TR.1,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J Werbos"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "author": ["R.J. Williams"], "venue": "Technical Report NU-CCS-89-27,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors", "author": ["S. Linnainmaa"], "venue": "Master\u2019s thesis, Univ. Helsinki,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1970}, {"title": "Taylor expansion of the accumulated rounding error", "author": ["Seppo Linnainmaa"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1976}, {"title": "System Modeling and Optimization: Proceedings of the 10th IFIP Conference New York City, USA, August 31 \u2013 September 4, 1981, chapter Applications of advances in nonlinear sensitivity analysis, pages 762\u2013770", "author": ["Paul J. Werbos"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S Hochreiter"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Highway long short-term memory RNNS for distant speech recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yaco", "Sanjeev Khudanpur", "James Glass"], "venue": "In 2016 IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Adaptive Computation Time for Recurrent Neural Networks", "author": ["A. Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Reinforcement learning in markovian and non-markovian environments", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems 3. Morgan-Kaufmann,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1991}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["S. Fernandez", "A. Graves", "J. Schmidhuber"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "\u00dcber die Abgrenzung der Eigenwerte einer Matrix", "author": ["S. Ger\u0161gorin"], "venue": "Bulletin de l\u2019Acade\u0300mie des Sciences de l\u2019URSS. Classe des sciences mathe\u0300matiques,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1931}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "Juergen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "LSTM: A Search Space Odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u00edk", "B. R Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "The human knowledge compression", "author": ["M. Hutter"], "venue": "contest. http://prize.hutter1.net/,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Comput. Linguist.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}, {"title": "Brainstorm: Fast, Flexible and Fun Neural Networks, Version", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "ArXiv e-prints,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Network depth is of central importance in the resurgence of neural networks as a powerful machine learning paradigm [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "[2] and references therein).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "However, certain internal function mappings in RNNs usually do not take advantage of depth [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Unfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139].", "startOffset": 133, "endOffset": 138}, {"referenceID": 7, "context": "Unfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139].", "startOffset": 133, "endOffset": 138}, {"referenceID": 8, "context": "Unfortunately, increased depth represents a challenge when neural network parameters are optimized by means of error backpropagation [7\u20139].", "startOffset": 133, "endOffset": 138}, {"referenceID": 9, "context": "Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation.", "startOffset": 105, "endOffset": 112}, {"referenceID": 10, "context": "Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation.", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "Deep networks suffer from what are commonly referred to as the vanishing and exploding gradient problems [10\u201312], since the magnitude of the gradients may shrink or explode exponentially during backpropagation.", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "These layers have been used to improve performance in speech recognition [16] and language modeling [17], and a variant of Highway networks called Residual networks is currently the state-of-the-art model for image recognition [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 5, "context": "[6] introduced Deep Transition RNNs (DT-RNNs) and Deep Transition RNNs with Skip connections (DT(S)-RNNs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "A known way of creating large step-by-step depth is to let an RNN tick for several \"micro time steps\u201d per step of the sequence [19\u201321], where the RNN could be an LSTM with forget gates.", "startOffset": 127, "endOffset": 134}, {"referenceID": 18, "context": "A known way of creating large step-by-step depth is to let an RNN tick for several \"micro time steps\u201d per step of the sequence [19\u201321], where the RNN could be an LSTM with forget gates.", "startOffset": 127, "endOffset": 134}, {"referenceID": 19, "context": "Recurrent Highway Networks enable a powerful alternative method to make recurrent networks more expressive and complement other approaches such as stacked LSTM layers [22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 9, "context": "We can now obtain conditions for the gradients to vanish [10\u201312].", "startOffset": 57, "endOffset": 64}, {"referenceID": 10, "context": "We can now obtain conditions for the gradients to vanish [10\u201312].", "startOffset": 57, "endOffset": 64}, {"referenceID": 11, "context": "We can now obtain conditions for the gradients to vanish [10\u201312].", "startOffset": 57, "endOffset": 64}, {"referenceID": 20, "context": "Ger\u0161gorin circle theorem (GCT) [23]: For any square matrix A \u2208 Rn\u00d7n,", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "[24] proposed to initialize R with identity and small random values on the off-diagonals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Highway layers [25] enable easy training of very deep feedforward networks through the use of adaptive computation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "Similar to other variants such as those studied by [26], it retains the essential components of the LSTM \u2013 multiplicative gating units controlling the flow of information through additive cells.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "This flexibility is the likely reason behind the performance improvement from Highway layers even in cases where network depth is not high [17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "We compare optimization of RHNs to DT-RNNs and DT(S)-RNNs [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 24, "context": "Networks with recurrence depth of 1, 2, 4 and 6 are trained for next step prediction on the JSB Chorales polyphonic music prediction dataset [27].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "These results are similar to those obtained in an optimization study on feedforward Highway networks [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "Wikipedia: We train an RHN with recurrence depth of 5 and 864 units on the challenging Hutter Prize Wikipedia dataset (enwik8) [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "Training is performed using SGD with momentum and weight noise as used by Graves [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Penn Treebank: To examine the effect of recurrence depth we trained RHNs with one RHN layer and fixed total parameters (10 M) but with recurrence depths of 1, 2 and 3 for word level language modeling on the Penn TreeBank dataset [30].", "startOffset": 229, "endOffset": 233}, {"referenceID": 22, "context": "[25].", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such \u201cdeep\" transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Ger\u0161gorin\u2019s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks (RHN), which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize dataset with less than half of the parameters.", "creator": "LaTeX with hyperref package"}}}