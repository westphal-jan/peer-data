{"id": "1607.06153", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2016", "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "abstract": "in this short paper, we actively present the practical first experiments involved using neural network hybrid models applying for the repetitive task of error detection in autonomous learner numerical writing. currently we perform a systematic comparison trial of simpler alternative compositional architectures and increasingly propose using a standardized framework for error detection based on static bidirectional lstms. experiments on the earlier conll - s 14 shared resolution task dataset show ; the model is able to outperform other participants taking on - detecting errors frequently in learner writing. but finally, the model building is commercially integrated with a larger publicly domain deployed online self - assessment system, finally leading specifically to performance comparable to human query annotators.", "histories": [["v1", "Wed, 20 Jul 2016 23:26:33 GMT  (111kb,D)", "http://arxiv.org/abs/1607.06153v1", "Proceedings of ACL 2016"]], "COMMENTS": "Proceedings of ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["marek rei", "helen yannakoudakis"], "accepted": true, "id": "1607.06153"}, "pdf": {"name": "1607.06153.pdf", "metadata": {"source": "CRF", "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "authors": ["Marek Rei", "Helen Yannakoudakis"], "emails": ["marek.rei@cl.cam.ac.uk", "helen.yannakoudakis@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Automated systems for detecting errors in learner writing are valuable tools for second language learning and assessment. Most work in recent years has focussed on error correction, with error detection performance measured as a byproduct of the correction output (Ng et al., 2013; Ng et al., 2014). However, this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection. While closed-class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014). Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections (Bryant and Ng, 2015). Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct\nor incorrect in context. Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context patterns. Both of these methods can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns. In addition, they need to specify a fixed context size and are therefore often limited to using a small window near the target.\nNeural network models aim to address these weaknesses and have achieved success in various NLP tasks such as language modeling (Bengio et al., 2003) and speech recognition (Dahl et al., 2012). Recent developments in machine translation have also shown that text of varying length can be represented as a fixed-size vector using convolutional networks (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a) or recurrent neural networks (Cho et al., 2014b; Bahdanau et al., 2015).\nIn this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional structures for constructing informative context representations. Based on the findings, we propose a novel framework for performing error detection in learner writing, which achieves state-of-the-art results on two datasets of errorannotated learner essays. The sequence labeling model creates a single variable-size network over the whole sentence, conditions each label on all the words, and predicts all labels together. The effects of different datasets on the overall performance are investigated by incrementally providing additional training data to the model. Finally, we integrate the error detection framework with a publicly deployed self-assessment system, leading ar X iv :1\n60 7.\n06 15\n3v 1\n[ cs\n.C L\n] 2\n0 Ju\nl 2 01\n6\nto performance comparable to human annotators."}, {"heading": "2 Background and Related Work", "text": "The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014).\nHowever, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task.\nThe related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). The current state-of-the-art approaches can broadly be separated into two categories:\n1. Phrase-based statistical machine translation techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014)\n2. Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013).\nError correction systems require very specialised models, as they need to generate an improved version of the input text, whereas a wider range of\ntagging and classification models can be deployed on error detection. In addition, automated writing feedback systems that indicate the presence and location of errors may be better from a pedagogic point of view, rather than providing a panacea and correcting all errors in learner text. In Section 7 we evaluate a neural sequence tagging model on the latest shared task test data, and compare it to the top participating systems on the task of error detection."}, {"heading": "3 Sequence Labeling Architectures", "text": "We construct a neural network sequence labeling framework for the task of error detection in learner writing. The model receives only a series of tokens as input, and outputs the probability of each token in the sentence being correct or incorrect in a given context. The architectures start with the vector representations of individual words, [x1, ..., xT ], where T is the length of the sentence. Different composition functions are then used to calculate a hidden vector representation of each token in context, [h1, ..., hT ]. These representations are passed through a softmax layer, producing a probability distribution over the possible labels for every token in context:\npt = softmax(Woht) (1)\nwhereWo is the weight matrix between the hidden vector ht and the output layer.\nWe investigate six alternative neural network architectures for the task of error detection: convolutional, bidirectional recurrent, bidirectional LSTM, and multi-layer variants of each of them. In the convolutional neural network (CNN, Figure 1a) for token labeling, the hidden vector ht is calculated based on a fixed-size context window. The convolution acts as a feedforward network, using surrounding context words as input, and therefore it will learn to detect the presence of different types of n-grams. The assumption behind the convolutional architecture is that memorising erroneous token sequences from the training data is sufficient for performing error detection.\nThe convolution uses dw tokens on either side of the target token, and the vectors for these tokens are concatenated, preserving the ordering:\nct = xt\u2212dw : ... : xt+dw (2)\nwhere x1 : x2 is used as notation for vector concatenation of x1 and x2. The combined vector is\nthen passed through a non-linear layer to produce the hidden representation:\nht = tanh(Wcct) (3)\nThe deep convolutional network (Figure 1b) adds an extra convolutional layer to the architecture, using the first layer as input. It creates convolutions of convolutions, thereby capturing more complex higher-order features from the dataset.\nIn a recurrent neural network (RNN), each hidden representation is calculated based on the current token embedding and the hidden vector at the previous time step:\nht = f(Wxt + V ht\u22121) (4)\nwhere f(z) is a nonlinear function, such as the sigmoid function. Instead of a fixed context window, information is passed through the sentence using a recursive function and the network is able to learn which patterns to disregard or pass forward. This recurrent network structure is referred to as an Elman-type network, after Elman (1990).\nThe bidirectional RNN (Figure 1c) consists of two recurrent components, moving in opposite directions through the sentence. While the unidirectional version takes into account only context on the left of the target token, the bidirectional version recursively builds separate context representations from either side of the target token. The left and right context are then concatenated and used as the hidden representation:\nh\u2192t = f(Wrxt + Vrh \u2192 t\u22121) (5)\nh\u2190t = f(Wlxt + Vlh \u2190 t+1) (6)\nht = h \u2192 t : h \u2190 t (7)\nRecurrent networks have been shown to perform well on the task of language modeling (Mikolov et al., 2011; Chelba et al., 2013), where they learn an incremental composition function for predicting the next token in the sequence. However, while language models can estimate the probability of each token, they are unable to differentiate between infrequent and incorrect token sequences. For error detection, the composition function needs to learn to identify semantic anomalies or ungrammatical combinations, independent of their frequency. The bidirectional model provides extra information, as it allows the network to use context on both sides of the target token.\nIrsoy and Cardie (2014) created an extension of this architecture by connecting together multiple layers of bidirectional Elman-type recurrent network modules. This deep bidirectional RNN (Figure 1d) calculates a context-dependent representation for each token using a bidirectional RNN, and then uses this as input to another bidirectional RNN. The multi-layer structure allows the model to learn more complex higher-level features and effectively perform multiple recurrent passes through the sentence.\nThe long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is an advanced alternative to the Elman-type networks that has recently become increasingly popular. It uses\ntwo separate hidden vectors to pass information between different time steps, and includes gating mechanisms for modulating its own output. LSTMs have been successfully applied to various tasks, such as speech recognition (Graves et al., 2013), machine translation (Luong et al., 2015), and natural language generation (Wen et al., 2015).\nTwo sets of gating values (referred to as the input and forget gates) are first calculated based on the previous states of the network:\nit = \u03c3(Wixt + Uiht\u22121 + Vfct\u22121 + bi) (8)\nft = \u03c3(Wfxt + Ufht\u22121 + Vfct\u22121 + bf ) (9)\nwhere xt is the current input, ht\u22121 is the previous hidden state, bi and bf are biases, ct\u22121 is the previous internal state (referred to as the cell), and \u03c3 is the logistic function. The new internal state is calculated based on the current input and the previous hidden state, and then interpolated with the previous internal state using ft and it as weights:\nc\u0303t = tanh(Wcxt + Ucht\u22121 + bc) (10)\nct = ft ct\u22121 + it c\u0303t (11)\nwhere is element-wise multiplication. Finally, the hidden state is calculated by passing the internal state through a tanh nonlinearity, and weighting it with ot. The values of ot are conditioned on the new internal state (ct), as opposed to the previous one (ct\u22121):\not = \u03c3(Woxt + Uoht\u22121 + Voct + bo) (12)\nht = ot tanh(ct) (13)\nBecause of the linear combination in equation (11), the LSTM is less susceptible to vanishing gradients over time, thereby being able to make use of longer context when making predictions. In addition, the network learns to modulate itself, effectively using the gates to predict which operation is required at each time step, thereby incorporating higher-level features.\nIn order to use this architecture for error detection, we create a bidirectional LSTM, making use of the advanced features of LSTM and incorporating context on both sides of the target token. In addition, we experiment with a deep bidirectional LSTM, which includes two consecutive layers of bidirectional LSTMs, modeling even\nmore complex features and performing multiple passes through the sentence.\nFor comparison with non-neural models, we also report results using CRFs (Lafferty et al., 2001), which are a popular choice for sequence labeling tasks. We trained the CRF++ 1 implementation on the same dataset, using as features unigrams, bigrams and trigrams in a 7-word window surrouding the target word (3 words before and after). The predicted label is also conditioned on the previous label in the sequence."}, {"heading": "4 Experiments", "text": "We evaluate the alternative network structures on the publicly released First Certificate in English dataset (FCE-public, Yannakoudakis et al. (2011)). The dataset contains short texts, written by learners of English as an additional language in response to exam prompts eliciting freetext answers and assessing mastery of the upperintermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. We use the released test set for evaluation, containing 2,720 sentences, leaving 30,953 sentences for training. We further separate 2,222 sentences from the training set for development and hyper-parameter tuning.\nThe dataset contains manually annotated error spans of various types of errors, together with their suggested corrections. We convert this to a tokenlevel error detection task by labeling each token inside the error span as being incorrect. In order to capture errors involving missing words, the error label is assigned to the token immediately after the incorrect gap \u2013 this is motivated by the intuition that while this token is correct when considered in isolation, it is incorrect in the current context, as another token should have preceeded it.\nAs the main evaluation measure for error detection we use F0.5, which was also the measure adopted in the CoNLL-14 shared task on error correction (Ng et al., 2014). It combines both precision and recall, while assigning twice as much weight to precision, since accurate feedback is often more important than coverage in error detection applications (Nagata and Nakatani, 2010). Following Chodorow et al. (2012), we also report raw counts for predicted and correct tokens. Related evaluation measures, such as the M2-scorer (Ng et al., 2014) and the I-measure (Felice and\n1https://taku910.github.io/crfpp/\nBriscoe, 2015), require the system to propose a correction and are therefore not directly applicable on the task of error detection.\nDuring the experiments, the input text was lowercased and all tokens that occurred less than twice in the training data were represented as a single unk token. Word embeddings were set to size 300 and initialised using the publicly released pretrained Word2Vec vectors (Mikolov et al., 2013). The convolutional networks use window size 3 on either side of the target token and produce a 300-dimensional context-dependent vector. The recurrent networks use hidden layers of size 200 in either direction. We also added an extra hidden layer of size 50 between each of the composition functions and the output layer \u2013 this allows the network to learn a separate non-linear transformation and reduces the dimensionality of the compositional vectors. The parameters were optimised using gradient descent with initial learning rate 0.001, the ADAM algorithm (Kingma and Ba, 2015) for dynamically adapting the learning rate, and batch size of 64 sentences. F0.5 on the development set was evaluated at each epoch, and the best model was used for final evaluations."}, {"heading": "5 Results", "text": "Table 1 contains results for experiments comparing different composition architectures on the task of error detection. The CRF has the lowest F0.5 score compared to any of the neural models. It memorises frequent error sequences with high precision, but does not generalise sufficiently, resulting in low recall. The ability to condition on the previous label also does not provide much help on this task \u2013 there are only two possible labels and the errors are relatively sparse.\nThe architecture using convolutional networks performs well and achieves the second-highest result on the test set. It is designed to detect error patterns from a fixed window of 7 words, which is large enough to not require the use of more advanced composition functions. In contrast, the performance of the bidirectional recurrent network (Bi-RNN) is somewhat lower, especially on the test set. In Elman-type recurrent networks, the context signal from distant words decreases fairly rapidly due to the sigmoid activation function and diminishing gradients. This is likely why the BiRNN achieves the highest precision of all systems \u2013 the predicted label is mostly influenced by the target token and its immediate neighbours, allowing the network to only detect short highconfidence error patterns. The convolutional network, which uses 7 context words with equal attention, is able to outperform the Bi-RNN despite the fixed-size context window.\nThe best overall result and highest F0.5 is achieved by the bidirectional LSTM composition model (Bi-LSTM). This architecture makes use of the full sentence for building context vectors on both sides of the target token, but improves on BiRNN by utilising a more advanced composition function. Through the application of a linear update for the internal cell representation, the LSTM is able to capture dependencies over longer distances. In addition, the gating functions allow it to adaptively decide which information to include in the hidden representations or output for error detection.\nWe found that using multiple layers of compositional functions in a deeper network gave comparable or slightly lower results for all the composition architectures. This is in contrast to Ir-\nsoy and Cardie (2014), who experimented with Elman-type networks and found some improvements using multiple layers of Bi-RNNs. The differences can be explained by their task benefiting from alternative features: the evaluation was performed on opinion mining where most target sequences are longer phrases that need to be identified based on their semantics, whereas many errors in learner writing are short and can only be identified by a contextual mismatch. In addition, our networks contain an extra hidden layer before the output, which allows the models to learn higherlevel representations without adding complexity through an extra compositional layer."}, {"heading": "6 Additional Training Data", "text": "There are essentially infinitely many ways of committing errors in text and introducing additional training data should alleviate some of the problems with data sparsity. We experimented with incrementally adding different error-tagged corpora into the training set and measured the resulting performance. This allows us to provide some context to the results obtained by using each of the datasets, and gives us an estimate of how much annotated data is required for optimal performance on error detection. The datasets we consider are as follows:\n\u2022 FCE-public \u2013 the publicly released subset of FCE (Yannakoudakis et al., 2011), as described in Section 4.\n\u2022 NUCLE \u2013 the NUS Corpus of Learner English (Dahlmeier et al., 2013), used as the main training set for CoNLL shared tasks on error correction.\n\u2022 IELTS \u2013 a subset of the IELTS examination dataset extracted from the Cambridge\nLearner Corpus (CLC, Nicholls (2003)), containing 68,505 sentences from all proficiency levels, also used by Felice et al. (2014).\n\u2022 FCE \u2013 a larger selection of FCE texts from the CLC, containing 323,192 sentences.\n\u2022 CPE \u2013 essays from the proficient examination level in the CLC, containing 210,678 sentences.\n\u2022 CAE \u2013 essays from the advanced examination level in the CLC, containing 219,953 sentences.\nTable 2 contains results obtained by incrementally adding training data to the Bi-LSTM model. We found that incorporating the NUCLE dataset does not improve performance over using only the FCE-public dataset, which is likely due to the two corpora containing texts with different domains and writing styles. The texts in FCE are written by young intermediate students, in response to prompts eliciting letters, emails and reviews, whereas NUCLE contains mostly argumentative essays written by advanced adult learners. The differences in the datasets offset the benefits from additional training data, and the performance remains roughly the same.\nIn contrast, substantial improvements are obtained when introducing the IELTS and FCE datasets, with each of them increasing the F0.5 score by roughly 10%. The IELTS dataset contains essays from all proficiency levels, and FCE from mid-level English learners, which provides the model with a distribution of \u2018average\u2019 errors to learn from. Adding even more training data from\nhigh-proficiency essays in CPE and CAE only provides minor further improvements.\nFigure 2 also shows F0.5 on the FCE-public test set as a function of the total number of tokens in the training data. The optimal trade-off between performance and data size is obtained at around 8 million tokens, after introducing the FCE dataset."}, {"heading": "7 CoNLL-14 Shared Task", "text": "The CoNLL-14 shared task (Ng et al., 2014) focussed on automatically correcting errors in learner writing. The NUCLE dataset was provided as the main training dataset, but participants were allowed to include other annotated corpora and external resources. For evaluation, 25 students were recruited to each write two new essays, which were then annotated by two experts.\nWe used the same methods from Section 4 for converting the shared task annotation to a tokenlevel labeling task in order to evaluate the models on error detection. In addition, the correction outputs of all the participating systems were made available online, therefore we are able to report their performance on this task. In order to convert their output to error detection labels, the corrected sentences were aligned with the original input using Levenshtein distance, and any changes proposed by the system resulted in the corresponding source words being labeled as errors.\nThe results on the two annotations of the shared task test data can be seen in Table 3. We first evaluated each of the human annotators with respect to the other, in order to estimate the upper bound on this task. The average F0.5 of roughly 50% shows that the task is difficult and even human experts have a rather low agreement. It has been shown\nbefore that correcting grammatical errors is highly subjective (Bryant and Ng, 2015), but these results indicate that trained annotators can disagree even on the number and location of errors.\nIn the same table, we provide error detection results for the top 3 participants in the shared task: CAMB (Felice et al., 2014), CUUI (Rozovskaya et al., 2014), and AMU (Junczys-Dowmunt and Grundkiewicz, 2014). They each preserve their relative ranking also in the error detection evaluation. The CAMB system has a lower precision but the highest recall, also resulting in the highest F0.5. CUUI and AMU are close in performance, with AMU having slightly higher precision.\nAfter the official shared task, Susanto et al. (2014) published a system which combines several alternative models and outperforms the shared task participants when evaluated on error correction. However, on error detection it receives lower results, ranking 3rd and 4th when evaluated on F0.5 (P1+P2+S1+S2 in Table 3). The system has detected a small number of errors with high precision, and does not reach the highest F0.5.\nFinally, we present results for the Bi-LSTM sequence labeling system for error detection. Using only FCE-public for training, the overall performance is rather low as the training set is very small and contains texts from a different domain. However, these results show that the model behaves as expected \u2013 since it has not seen similar language during training, it labels a very large portion of tokens as errors. This indicates that the network is trying to learn correct language constructions from the limited data and classifies unseen structures as errors, as opposed to simply memorising error sequences from the training data.\nWhen trained on all the datasets from Section 6, the model achieves the highest F0.5 of all systems on both of the CoNLL-14 shared task test annotations, with an absolute improvement of 3% over the previous best result. It is worth noting that the full Bi-LSTM has been trained on more data than the other CoNLL contestants. However, as the shared task systems were not restricted to the NUCLE training set, all the submissions also used differing amounts of training data from various sources. In addition, the CoNLL systems are mostly combinations of many alternative models: the CAMB system is a hybrid of machine translation, a rule-based system, and a language model re-ranker; CUUI consists of different classifiers for each individual error type; and P1+P2+S1+S2 is a combination of four different error correction systems. In contrast, the Bi-LSTM is a single model for detecting all error types, and therefore represents a more scalable data-driven approach."}, {"heading": "8 Essay Scoring", "text": "In this section, we perform an extrinsic evaluation of the efficacy of the error detection system and examine the extent to which it generalises at higher levels of granularity on the task of automated essay scoring. More specifically, we replicate experiments using the text-level model described by Andersen et al. (2013), which is currently deployed in a self-assessment and tutoring system (SAT), an online automated writing feedback tool actively used by language learners.2\nThe SAT system predicts an overall score for a given text, which provides a holistic assessment of linguistic competence and language proficiency. The authors trained a supervised ranking perceptron model on the FCE-public dataset, using features such as error-rate estimates from a language model and various lexical and grammatical properties of text (e.g., word n-grams, part-of-speech n-grams and phrase-structure rules). We replicate this experiment and add the average probability of each token in the essay being correct, according to the error detection model, as an additional feature for the scoring framework. The system was then retrained on FCE-public and evaluated on correctly predicting the assigned essay score. Table 4 presents the experimental results.\nThe human performance on the test set is cal-\n2http://www.cambridgeenglish.org/learning-english/freeresources/write-and-improve/\nculated as the average inter-annotator correlation on the same data, and the existing SAT system has demonstrated levels of performance that are very close to that of human assessors. Nevertheless, the Bi-LSTM model trained only on FCE-public complements the existing features, and the combined model achieves an absolute improvement of around 1% percent, corresponding to 20-31% relative error reduction with respect to the human performance. Even though the Bi-LSTM is trained on the same dataset and the SAT system already includes various linguistic features for capturing errors, our error detection model manages to further improve its performance.\nWhen the Bi-LSTM is trained on all the available data from Section 6, the combination achieves further substantial improvements. The relative error reduction on Pearson\u2019s correlation is 64%, and the system actually outperforms human annotators on Spearman\u2019s correlation."}, {"heading": "9 Conclusions", "text": "In this paper, we presented the first experiments using neural network models for the task of error detection in learner writing. Six alternative compositional network architectures for modeling context were evaluated. Based on the findings, we propose a novel error detection framework using token-level embeddings, bidirectional LSTMs for context representation, and a multi-layer architecture for learning more complex features. This structure allows the model to classify each token as being correct or incorrect, using the full sentence as context. The self-modulation architecture of LSTMs was also shown to be beneficial, as it allows the network to learn more advanced composition rules and remember dependencies over longer distances.\nSubstantial performance improvements were achieved by training the best model on additional\ndatasets. We found that the largest benefit was obtained from training on 8 million tokens of text from learners with varying levels of language proficiency. In contrast, including even more data from higher-proficiency learners gave marginal further improvements. As part of future work, it would be beneficial to investigate the effect of automatically generated training data for error detection (e.g., Rozovskaya and Roth (2010)).\nWe evaluated the performance of existing error correction systems from CoNLL-14 on the task of error detection. The experiments showed that success on error correction does not necessarily mean success on error detection, as the current best correction system (P1+P2+S1+S2) is not the same as the best shared task detection system (CAMB). In addition, the neural sequence tagging model, specialised for error detection, was able to outperform all other participating systems.\nFinally, we performed an extrinsic evaluation by incorporating probabilities from the error detection system as features in an essay scoring model. Even without any additional data, the combination further improved performance which is already close to the results from human annotators. In addition, when the error detection model was trained on a larger training set, the essay scorer was able to exceed human-level performance."}, {"heading": "Acknowledgments", "text": "We would like to thank Prof Ted Briscoe and the reviewers for providing useful feedback."}], "references": [{"title": "Developing and testing a self-assessment and tutoring system", "author": ["\u00d8istein E. Andersen", "Helen Yannakoudakis", "Fiona Barker", "Tim Parish."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Andersen et al\\.,? 2013", "shortCiteRegEx": "Andersen et al\\.", "year": 2013}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model Yoshua", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "How Far are We from Fully Automatic High Quality Grammatical Error Correction", "author": ["Christopher Bryant", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Bryant and Ng.,? \\Q2015\\E", "shortCiteRegEx": "Bryant and Ng.", "year": 2015}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Ciprian Chelba", "Tom\u00e1\u0161 Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "arXiv preprint.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "On the Properties of Neural Machine Translation: EncoderDecoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Transla-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "An unsupervised method for detecting grammatical errors", "author": ["Martin Chodorow", "Claudia Leacock."], "venue": "Proceedings of the first conference on North American chapter of the Association for Computational Linguistics.", "citeRegEx": "Chodorow and Leacock.,? 2000", "shortCiteRegEx": "Chodorow and Leacock.", "year": 2000}, {"title": "Detection of grammatical errors involving prepositions", "author": ["Martin Chodorow", "Joel R. Tetreault", "Na-Rae Han."], "venue": "Proceedings of the 4th ACLSIGSEM Workshop on Prepositions.", "citeRegEx": "Chodorow et al\\.,? 2007", "shortCiteRegEx": "Chodorow et al\\.", "year": 2007}, {"title": "Problems in evaluating grammatical error detection systems", "author": ["Martin Chodorow", "Markus Dickinson", "Ross Israel", "Joel Tetreault."], "venue": "COLING 2012.", "citeRegEx": "Chodorow et al\\.,? 2012", "shortCiteRegEx": "Chodorow et al\\.", "year": 2012}, {"title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "author": ["George. E. Dahl", "Dong Yu", "Li Deng", "Alex Acero."], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 20.", "citeRegEx": "Dahl et al\\.,? 2012", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of learner English: The NUS corpus of learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "Helping Our Own: The HOO 2011 Pilot Shared Task", "author": ["Robert Dale", "Adam Kilgarriff."], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation.", "citeRegEx": "Dale and Kilgarriff.,? 2011", "shortCiteRegEx": "Dale and Kilgarriff.", "year": 2011}, {"title": "HOO 2012: A report on the Preposition and Determiner Error Correction Shared Task", "author": ["Robert Dale", "Ilya Anisimoff", "George Narroway."], "venue": "The Seventh Workshop on Building Educational Applications Using NLP.", "citeRegEx": "Dale et al\\.,? 2012", "shortCiteRegEx": "Dale et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive science, 14(2).", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Towards a standard evaluation method for grammatical error detection and correction", "author": ["Mariano Felice", "Ted Briscoe."], "venue": "The 2015 Annual Conference of the North American Chapter of the ACL.", "citeRegEx": "Felice and Briscoe.,? 2015", "shortCiteRegEx": "Felice and Briscoe.", "year": 2015}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."], "venue": "Conference on Computational Natural Language Learning: Shared Task (CoNLL-", "citeRegEx": "Felice et al\\.,? 2014", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "High-Order Sequence Modeling for Language Learner Error Detection", "author": ["Michael Gamon."], "venue": "Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Gamon.,? 2011", "shortCiteRegEx": "Gamon.", "year": 2011}, {"title": "Hybrid speech recognition with Deep Bidirectional LSTM", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel Rahman Mohamed."], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2013).", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Detecting Errors in English Article Usage with a Maximum Entropy Classifier Trained on a Large, Diverse Corpus", "author": ["Na-Rae Han", "Martin Chodorow", "Claudia Leacock."], "venue": "Proceedings of the 4th International Conference on Language Resources and", "citeRegEx": "Han et al\\.,? 2004", "shortCiteRegEx": "Han et al\\.", "year": 2004}, {"title": "Detecting errors in English article usage by non-native speakers", "author": ["Na-Rae Han", "Martin Chodorow", "Claudia Leacock."], "venue": "Natural Language Engineering, 12.", "citeRegEx": "Han et al\\.,? 2006", "shortCiteRegEx": "Han et al\\.", "year": 2006}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "EMNLP2014.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "The AMU System in the CoNLL-2014 Shared Task: Grammatical Error Correction by Data-Intensive and Feature-Rich Statistical Machine Translation", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the Eighteenth Confer-", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2014", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2014}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Detecting Learner Errors in the Choice of Content Words Using Compositional Distributional Semantics", "author": ["Ekaterina Kochmar", "Ted Briscoe."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguis-", "citeRegEx": "Kochmar and Briscoe.,? 2014", "shortCiteRegEx": "Kochmar and Briscoe.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of the 18th International Conference on Machine Learning. Citeseer.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Automated Grammatical Error Detection for Language Learners: Second Edition", "author": ["Claudia Leacock", "Martin Chodorow", "Michael Gamon", "Joel R. Tetreault"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 2014}, {"title": "Correcting misuse of verb forms", "author": ["John Lee", "Stephanie Seneff."], "venue": "Proceedings of the 46th Annual Meeting of the ACL.", "citeRegEx": "Lee and Seneff.,? 2008", "shortCiteRegEx": "Lee and Seneff.", "year": 2008}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Mnih-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "RNNLM-Recurrent neural network language modeling toolkit", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Anoop Deoras", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd."], "venue": "ASRU 2011 Demo Session.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tom\u00e1\u0161 Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2013).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluating performance of grammatical error detection to maximize learning effect", "author": ["Ryo Nagata", "Kazuhide Nakatani."], "venue": "Coling 2010: Poster Volume.", "citeRegEx": "Nagata and Nakatani.,? 2010", "shortCiteRegEx": "Nagata and Nakatani.", "year": 2010}, {"title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "author": ["Hwee Tou Ng", "Yuanbin Wu", "Christian Hadiwinoto."], "venue": "Computational Natural Language Learning (CoNLL), Shared Task.", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 Shared Task on Grammatical Error Correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of the Eighteenth Conference on Computational Natu-", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "The Cambridge Learner Corpus - error coding and analysis for lexicography and ELT", "author": ["Diane Nicholls."], "venue": "Proceedings of the Corpus Linguistics 2003 Conference.", "citeRegEx": "Nicholls.,? 2003", "shortCiteRegEx": "Nicholls.", "year": 2003}, {"title": "Training Paradigms for Correcting Errors in Grammar and Usage", "author": ["Alla Rozovskaya", "Dan Roth."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Rozovskaya and Roth.,? 2010", "shortCiteRegEx": "Rozovskaya and Roth.", "year": 2010}, {"title": "University of Illinois System in HOO Text Correction Shared Task", "author": ["Alla Rozovskaya", "Mark Sammons", "Joshua Gioja", "Dan Roth."], "venue": "Proceedings of the 13th European Workshop on Natural Language Generation (ENLG).", "citeRegEx": "Rozovskaya et al\\.,? 2011", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2011}, {"title": "The University of Illinois System in the CoNLL-2013 Shared Task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task.", "citeRegEx": "Rozovskaya et al\\.,? 2013", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2013}, {"title": "The IllinoisColumbia System in the CoNLL-2014 Shared Task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared", "citeRegEx": "Rozovskaya et al\\.,? 2014", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "System Combination for Grammatical Error Correction", "author": ["Raymond Hendy Susanto", "Peter Phandi", "Hwee Tou Ng."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP-2014).", "citeRegEx": "Susanto et al\\.,? 2014", "shortCiteRegEx": "Susanto et al\\.", "year": 2014}, {"title": "The Ups and Downs of Preposition Error Detection in ESL Writing", "author": ["Joel R. Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008).", "citeRegEx": "Tetreault and Chodorow.,? 2008", "shortCiteRegEx": "Tetreault and Chodorow.", "year": 2008}, {"title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "EMNLP 2015.", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A New Dataset and Method for Automatically Grading ESOL Texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 34, "context": "Most work in recent years has focussed on error correction, with error detection performance measured as a byproduct of the correction output (Ng et al., 2013; Ng et al., 2014).", "startOffset": 142, "endOffset": 176}, {"referenceID": 35, "context": "Most work in recent years has focussed on error correction, with error detection performance measured as a byproduct of the correction output (Ng et al., 2013; Ng et al., 2014).", "startOffset": 142, "endOffset": 176}, {"referenceID": 28, "context": "errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014).", "startOffset": 237, "endOffset": 286}, {"referenceID": 26, "context": "errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014).", "startOffset": 237, "endOffset": 286}, {"referenceID": 3, "context": "Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections (Bryant and Ng, 2015).", "startOffset": 133, "endOffset": 154}, {"referenceID": 10, "context": ", 2003) and speech recognition (Dahl et al., 2012).", "startOffset": 31, "endOffset": 50}, {"referenceID": 6, "context": ", 2014a) or recurrent neural networks (Cho et al., 2014b; Bahdanau et al., 2015).", "startOffset": 38, "endOffset": 80}, {"referenceID": 1, "context": ", 2014a) or recurrent neural networks (Cho et al., 2014b; Bahdanau et al., 2015).", "startOffset": 38, "endOffset": 80}, {"referenceID": 42, "context": "Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al.", "startOffset": 101, "endOffset": 154}, {"referenceID": 8, "context": "Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al.", "startOffset": 101, "endOffset": 154}, {"referenceID": 19, "context": ", 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014).", "startOffset": 18, "endOffset": 54}, {"referenceID": 20, "context": ", 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014).", "startOffset": 18, "endOffset": 54}, {"referenceID": 29, "context": ", 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014).", "startOffset": 20, "endOffset": 42}, {"referenceID": 26, "context": ", 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014).", "startOffset": 69, "endOffset": 96}, {"referenceID": 7, "context": "Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and func-", "startOffset": 0, "endOffset": 28}, {"referenceID": 17, "context": "Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser.", "startOffset": 0, "endOffset": 13}, {"referenceID": 12, "context": "Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al.", "startOffset": 16, "endOffset": 43}, {"referenceID": 38, "context": "Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011).", "startOffset": 250, "endOffset": 275}, {"referenceID": 12, "context": "past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 83, "endOffset": 163}, {"referenceID": 13, "context": "past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 83, "endOffset": 163}, {"referenceID": 34, "context": "past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 83, "endOffset": 163}, {"referenceID": 35, "context": "past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014).", "startOffset": 83, "endOffset": 163}, {"referenceID": 16, "context": "techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014)", "startOffset": 89, "endOffset": 150}, {"referenceID": 23, "context": "techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014)", "startOffset": 89, "endOffset": 150}, {"referenceID": 40, "context": "Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013).", "startOffset": 103, "endOffset": 153}, {"referenceID": 39, "context": "Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013).", "startOffset": 103, "endOffset": 153}, {"referenceID": 14, "context": "This recurrent network structure is referred to as an Elman-type network, after Elman (1990). The bidirectional RNN (Figure 1c) consists of two recurrent components, moving in opposite directions through the sentence.", "startOffset": 54, "endOffset": 93}, {"referenceID": 31, "context": "Recurrent networks have been shown to perform well on the task of language modeling (Mikolov et al., 2011; Chelba et al., 2013), where", "startOffset": 84, "endOffset": 127}, {"referenceID": 4, "context": "Recurrent networks have been shown to perform well on the task of language modeling (Mikolov et al., 2011; Chelba et al., 2013), where", "startOffset": 84, "endOffset": 127}, {"referenceID": 21, "context": "The long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is an advanced alternative to the Elman-type networks that has recently become increasingly popular.", "startOffset": 34, "endOffset": 68}, {"referenceID": 20, "context": "Irsoy and Cardie (2014) created an extension of this architecture by connecting together multiple layers of bidirectional Elman-type recurrent network modules.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "LSTMs have been successfully applied to various tasks, such as speech recognition (Graves et al., 2013), machine translation (Luong et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 30, "context": ", 2013), machine translation (Luong et al., 2015), and natural language generation (Wen et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 43, "context": ", 2015), and natural language generation (Wen et al., 2015).", "startOffset": 41, "endOffset": 59}, {"referenceID": 27, "context": "For comparison with non-neural models, we also report results using CRFs (Lafferty et al., 2001), which are a popular choice for sequence labeling tasks.", "startOffset": 73, "endOffset": 96}, {"referenceID": 44, "context": "We evaluate the alternative network structures on the publicly released First Certificate in English dataset (FCE-public, Yannakoudakis et al. (2011)).", "startOffset": 122, "endOffset": 150}, {"referenceID": 35, "context": "5, which was also the measure adopted in the CoNLL-14 shared task on error correction (Ng et al., 2014).", "startOffset": 86, "endOffset": 103}, {"referenceID": 33, "context": "It combines both precision and recall, while assigning twice as much weight to precision, since accurate feedback is often more important than coverage in error detection applications (Nagata and Nakatani, 2010).", "startOffset": 184, "endOffset": 211}, {"referenceID": 35, "context": "Related evaluation measures, such as the M2-scorer (Ng et al., 2014) and the I-measure (Felice and", "startOffset": 51, "endOffset": 68}, {"referenceID": 8, "context": "Following Chodorow et al. (2012), we also report raw counts for predicted and correct tokens.", "startOffset": 10, "endOffset": 33}, {"referenceID": 32, "context": "300 and initialised using the publicly released pretrained Word2Vec vectors (Mikolov et al., 2013).", "startOffset": 76, "endOffset": 98}, {"referenceID": 25, "context": "001, the ADAM algorithm (Kingma and Ba, 2015) for dynamically adapting the learning rate, and batch size of 64 sentences.", "startOffset": 24, "endOffset": 45}, {"referenceID": 44, "context": "\u2022 FCE-public \u2013 the publicly released subset of FCE (Yannakoudakis et al., 2011), as described in Section 4.", "startOffset": 51, "endOffset": 79}, {"referenceID": 11, "context": "\u2022 NUCLE \u2013 the NUS Corpus of Learner English (Dahlmeier et al., 2013), used as the main training set for CoNLL shared tasks on error correction.", "startOffset": 44, "endOffset": 68}, {"referenceID": 36, "context": "\u2022 IELTS \u2013 a subset of the IELTS examination dataset extracted from the Cambridge Learner Corpus (CLC, Nicholls (2003)), containing 68,505 sentences from all proficiency", "startOffset": 102, "endOffset": 118}, {"referenceID": 16, "context": "levels, also used by Felice et al. (2014).", "startOffset": 21, "endOffset": 42}, {"referenceID": 35, "context": "The CoNLL-14 shared task (Ng et al., 2014) focussed on automatically correcting errors in learner writing.", "startOffset": 25, "endOffset": 42}, {"referenceID": 3, "context": "It has been shown before that correcting grammatical errors is highly subjective (Bryant and Ng, 2015), but these results indicate that trained annotators can disagree even on the number and location of errors.", "startOffset": 81, "endOffset": 102}, {"referenceID": 16, "context": "CAMB (Felice et al., 2014), CUUI (Rozovskaya et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 40, "context": ", 2014), CUUI (Rozovskaya et al., 2014), and AMU (Junczys-Dowmunt and Grundkiewicz, 2014).", "startOffset": 14, "endOffset": 39}, {"referenceID": 23, "context": ", 2014), and AMU (Junczys-Dowmunt and Grundkiewicz, 2014).", "startOffset": 17, "endOffset": 57}, {"referenceID": 41, "context": "After the official shared task, Susanto et al. (2014) published a system which combines several alternative models and outperforms the shared task participants when evaluated on error correction.", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": "More specifically, we replicate experiments using the text-level model described by Andersen et al. (2013), which is currently deployed in a self-assessment and tutoring system (SAT), an online automated writing feedback tool actively used by language learners.", "startOffset": 84, "endOffset": 107}, {"referenceID": 37, "context": ", Rozovskaya and Roth (2010)).", "startOffset": 2, "endOffset": 29}], "year": 2016, "abstractText": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.", "creator": "TeX"}}}