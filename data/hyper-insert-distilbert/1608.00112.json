{"id": "1608.00112", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jul-2016", "title": "Supervised Attentions for Neural Machine Translation", "abstract": "moreover in developing this paper, we readily improve the mouse attention or alignment alignment encoding accuracy of classical neural machine translation instruction by gradually utilizing the alignments of matching training sentence pairs. we largely simply compute the computation distance between the machine string attentions pair and note the \" true \" string alignments, preserve and minimize this cost in skipping the training procedure. our large experiments on large - scale chinese - to - english task descriptions show that our model formulation improves both facial translation and alignment qualities significantly over the large - subunit vocabulary neural text machine translation system, tracking and even translating beats across a state - of - practically the - white art traditional digital syntax - based system.", "histories": [["v1", "Sat, 30 Jul 2016 12:39:19 GMT  (2108kb,D)", "http://arxiv.org/abs/1608.00112v1", "6 pages. In Proceedings of EMNLP 2016. arXiv admin note: text overlap witharXiv:1605.03148"]], "COMMENTS": "6 pages. In Proceedings of EMNLP 2016. arXiv admin note: text overlap witharXiv:1605.03148", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1608.00112"}, "pdf": {"name": "1608.00112.pdf", "metadata": {"source": "CRF", "title": "Supervised Attentions for Neural Machine Translation", "authors": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "emails": ["abei}@us.ibm.com"], "sections": [{"heading": null, "text": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \u201ctrue\u201d alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system."}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014).\nThe attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016).\nIn this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective function. Thus, we not only maximize the log translation probabilities, but also minimize the alignment distance cost. Large-scale experiments over Chineseto-English on various test sets show that our best method for a single system improves the translation quality significantly over the large vocabulary NMT system (Section 5) and beats the state-of-theart syntax-based system."}, {"heading": "2 Neural Machine Translation", "text": "As shown in Figure 1, attention-based NMT (Bahdanau et al., 2014) is an encoder-decoder network. the encoder employs a bi-directional recurrent neural network to encode the source sentence x = (x1, ..., xl), where l is the sentence length (including the end-of-sentence \u3008eos\u3009), into a sequence of hidden states h = (h1, ..., hl), each hi is a concatenation of a left-to-right \u2212\u2192 hi and a right-to-left \u2190\u2212 hi .\nGiven h, the decoder predicts the target translation by maximizing the conditional log-probability of the correct translation y\u2217 = (y\u22171, ...y \u2217 m), where m is the sentence length (including the end-ofsentence). At each time t, the probability of each word yt from a target vocabulary Vy is:\np(yt|h, y\u2217t\u22121..y\u22171) = g(st, y\u2217t\u22121), (1)\nwhere g is a two layer feed-forward neural network over the embedding of the previous word y\u2217t\u22121, and the hidden state st. The st is computed as:\nst = q(st\u22121, y \u2217 t\u22121, Ht) (2)\nHt =\n[\u2211l i=1 (\u03b1t,i \u00b7 \u2190\u2212 h i)\u2211l\ni=1 (\u03b1t,i \u00b7 \u2212\u2192 h i)\n] , (3)\nwhere q is a gated recurrent units, Ht is a weighted sum of h; the weights, \u03b1, are computed with a two layer feed-forward neural network r:\n\u03b1t,i = exp{r(st\u22121, hi, y\u2217t\u22121)}\u2211l k=1 exp{r(st\u22121, hk, y\u2217t\u22121)}\n(4)\nWe put all \u03b1t,i (t = 1...m, i = 1...l) into a matrix A\u2032, we have a matrix (alignment) like (c) in Figure 2, where each row (for each target word) is a probability distribution over the source sentence x.\nThe training objective is to maximize the conditional log-probability of the correct translation y\u2217\nar X\niv :1\n60 8.\n00 11\n2v 1\n[ cs\n.C L\n] 3\n0 Ju\nl 2 01\n6\ngiven x with respect to the parameters \u03b8\n\u03b8\u2217 = argmax \u03b8\nN\u2211\nn=1\nm\u2211\nt=1\nlog p(y\u2217nt |xn, y\u2217nt\u22121..y\u2217n1 ),\n(5) where n is the n-th sentence pair (xn,y\u2217n) in the training set, N is the total number of pairs."}, {"heading": "3 Alignment Component", "text": "The attentions, \u03b1t,1...\u03b1t,l, in each step t play an important role in NMT. However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score (Mi et al., 2016b; Tu et al., 2016). Thus, in this section, we explicitly add an alignment distance to the objective function in Eq. 5. The \u201ctruth\u201d alignments for each sentence pair can be from human annotated data, unsupervised or supervised alignments (e.g. GIZA++ (Och and Ney, 2000) or MaxEnt (Ittycheriah and Roukos, 2005)).\nGiven an alignment matrix A for a sentence pair (x,y) in Figure 2 (a), where we have an end-ofsource-sentence token \u3008eos\u3009 = xl, and we align all the unaligned target words (y\u22173 in this example) to \u3008eos\u3009, also we force y\u2217m (end-of-target-sentence) to be aligned to xl with probability one. Then we con-\nduct two transformations to get the probability distribution matrices ((b) and (c) in Figure 2)."}, {"heading": "3.1 Simple Transformation", "text": "The first transformation simply normalizes each row. Figure 2 (b) shows the result matrix A\u2217. The last column in red dashed lines shows the alignments of the special end-of-sentence token \u3008eos\u3009."}, {"heading": "3.2 Smoothed Transformation", "text": "Given the original alignment matrix A, we create a matrixA\u2217 with all points initialized with zero. Then, for each alignment point At,i = 1, we update A\u2217 by adding a Gaussian distribution, g(\u00b5, \u03c3), with a window sizew (t-w, ... t ... t+w). Take theA1,1 = 1 for example, we haveA\u22171,1 += 1,A\u22171,2 += 0.61, and A\u22171,3 += 0.14 with w=2, g(\u00b5, \u03c3)=g(0, 1). Then we normalize each row and get (c). In our experiments, we use a shape distribution, where \u03c3 = 0.5."}, {"heading": "3.3 Objectives", "text": "Alignment Objective: Given the \u201ctrue\u201d alignment A\u2217, and the machine attentions A\u2032 produced by NMT model, we compute the Euclidean distance\nbewteen A\u2217 and A\u2032.\nd(A\u2032,A\u2217) =\n\u221a\u221a\u221a\u221a m\u2211\nt=1\nl\u2211\ni=1\n(A\u2032t,i \u2212A\u2217t,i)2. (6)\nNMT Objective: We plug Eq. 6 to Eq. 5, we have\n\u03b8\u2217 = argmax \u03b8\nN\u2211\nn=1\n{ m\u2211\nt=1\nlog p(y\u2217nt |xn, y\u2217nt\u22121..y\u2217n1 )\n\u2212 d(A\u2032n,A\u2217n) } .\n(7)\nThere are two parts: translation and alignment, so we can optimize them jointly, or separately (e.g. we first optimize alignment only, then optimize translation). Thus, we divide the network in Figure 1 into alignment A and translation T parts:\n\u2022 A: all networks before the hidden state st,\n\u2022 T: the network g(st, y\u2217t\u22121).\nIf we only optimize A, we keep the parameters in T unchanged. We can also optimize them jointly J. In our experiments, we test different optimization strategies."}, {"heading": "4 Related Work", "text": "In order to improve the attention or alignment accuracy, Cheng et al. (2016) adapted the agreementbased learning (Liang et al., 2006; Liang et\nal., 2008), and introduced a combined objective that takes into account both translation directions (source-to-target and target-to-source) and an agreement term between the two alignment directions. By contrast, our approach directly uses and optimizes NMT parameters using the \u201csupervised\u201d alignments."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data Preparation", "text": "We run our experiments on Chinese to English task. The training corpus consists of approximately 5 million sentences available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, and webblog. We do not include HK Law, HK Hansard and UN data. The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). Our development set is the concatenation of several tuning sets (GALE Dev, P1R6 Dev, and Dev 12) initially released under the DARPA GALE program. The development set is 4491 sentences in total. Our test sets are NIST MT06 (1664 sentences) , MT08 news (691 sentences), and MT08 web (666 sentences).\nFor all NMT systems, the full vocabulary size of the training set is 300k. In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80. Following Mi et al. (2016a), the output vocabulary for each mini-batch or sentence is a sub-set of the full vo-\ncabulary. For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable.\nThe Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100.\nFollowing Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.\nOur traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013). We parse the Chinese side with Berkeley parser, and align the bilingual sentences with GIZA++ and MaxEnt. and extract Hiero and tree-to-string rules on the training set. Our language models are trained on the English side of the parallel corpus, and on monolingual corpora (around 10 billion words from Gigaword (LDC2011T07).We tune our system with PRO (Hopkins and May, 2011) to minimize (TER-\nBLEU)/2 1 on the development set."}, {"heading": "5.2 Translation Results", "text": "Table 1 shows the translation results of all systems. The syntax-based statistical machine translation model achieves an average (TER-BLEU)/2 of 13.36 on three test sets. The Cov. LVNMT system achieves an average (TER-BLEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system. Please note that all systems are single systems. It is highly possible that ensemble of NMT systems with different random seeds can lead to better results over SMT.\nWe test three different alignments:\n\u2022 Zh\u2192 En (one direction of GIZA++),\n\u2022 GDFA (the \u201cgrow-diag-final-and\u201d heuristic merge of both directions of GIZA++),\n\u2022 MaxEnt (trained on 67k hand-aligned sentences).\n1The metric used for optimization in this work is (TERBLEU)/2 to prevent the system from using sentence length alone to impact BLEU or TER. Typical SMT systems use target word count as a feature and it has been observed that BLEU can be optimized by tweaking the weighting of the target word count with no improvement in human assessments of translation quality. Conversely, in order to optimize TER shorter sentences can be produced. Optimizing the combination of metrics alleviates this effect (Arne Mauser and Ney, 2008).\nThe alignment quality improves from Zh \u2192 En to MaxEnt. We also test different optimization strategies: J (jointly), A (alignment only), and T (translation model only). A combination, A\u2192 T, shows that we optimize A only first, then we fix A and only update T part. Gau. denotes the smoothed transformation (Section 3.2). Only the last row uses the smoothed transformation, all others use the simple transformation.\nExperimental results in Table 1 show some interesting results. First, with the same alignment, J joint optimization works best than other optimization strategies (lines 3 to 6). Unfortunately, breaking down the network into two separate parts (A and T) and optimizing them separately do not help (lines 3 to 5). We have to conduct joint optimization J in order to get a comparable or better result (lines 3, 5 and 6) over the baseline system.\nSecond, when we change the training alignment seeds (Zh\u2192En, GDFA, and MaxEnt) NMT model does not yield significant different results (lines 6 to 8).\nThird, the smoothed transformation (J + Gau.) gives some improvements over the simple transformation (the last two lines), and achieves the best result (1.2 better than LVNMT, and 0.3 better than Tree-to-string). In terms of BLEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau. over LVNMT are significant on three test sets (p < 0.01).\nAt last, the brevity penalty (BP) consistently gets better after we add the alignment cost to NMT objective. Our alignment objective adjusts the translation length to be more in line with the human references accordingly."}, {"heading": "5.3 Alignment Results", "text": "Table 2 shows the alignment F1 scores on the alignment test set (447 hand aligned sentences). The MaxEnt model is trained on 67k hand-aligned sentences, and achieves an F1 score of 75.96. For NMT systems, we dump the alignment matrixes and convert them into alignments with following steps. For each target word, we sort the alphas and add the max probability link if it is higher than 0.2. If we only tune the alignment component (A in line 3), we improve the alignment F1 score from 45.76 to 47.87.\nAnd we further boost the score to 50.97 by tuning alignment and translation jointly (J in line 7). Interestingly, the system using MaxEnt produces more alignments in the output, and results in a higher recall. This suggests that using MaxEnt can lead to a sharper attention distribution, as we pick the alignment links based on the probabilities of attentions, the sharper the distribution is, more links we can pick. We believe that a sharp attention distribution is a great property of NMT.\nAgain, the best result is J + Gau. in the last row, which significantly improves the F1 by 5 points over the baseline Cov. LVNMT system. When we use MaxEnt alignments, J + Gau. smoothing gives us about 1.7 points gain over J system. So it looks interesting to run another J + Gau. over GDFA alignment.\nTogether with the results in Table 1, we conclude that adding the alignment cost to the training objective helps both translation and alignment significantly."}, {"heading": "6 Conclusion", "text": "In this paper, we utilize the \u201csupervised\u201d alignments, and put the alignment cost to the NMT objective function. In this way, we directly optimize the attention model in a supervised way. Experiments show significant improvements in both translation and alignment tasks over a very strong LVNMT system."}, {"heading": "Acknowledgment", "text": "We thank the anonymous reviewers for useful comments."}], "references": [{"title": "Automatic evaluation measures for statistical machine translation system optimization", "author": ["Arne Mauser", "Ney2008] Sasa Hasan Arne Mauser", "Hermann Ney"], "venue": "In Proceedings of LREC", "citeRegEx": "Mauser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mauser et al\\.", "year": 2008}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2016] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding", "author": ["Haitao Mi", "Bowen Zhou"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cmejrek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cmejrek et al\\.", "year": 2013}, {"title": "Clause restructuring for statistical machine translation", "author": ["Philipp Koehn", "Ivona Kucerova"], "venue": "In Proceedings of ACL,", "citeRegEx": "Collins et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "A maximum entropy word aligner for arabic-english machine translation", "author": ["Ittycheriah", "Roukos2005] Abraham Ittycheriah", "Salim Roukos"], "venue": "In HLT \u201905: Proceedings of the HLT and EMNLP,", "citeRegEx": "Ittycheriah et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ittycheriah et al\\.", "year": 2005}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] P. Liang", "B. Taskar", "D. Klein"], "venue": "In North American Association for Computational Linguistics (NAACL),", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Joint decoding with multiple translation models", "author": ["Liu et al.2009] Yang Liu", "Haitao Mi", "Yang Feng", "Qun Liu"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "2016a. A coverage embedding model for neural machine translation. ArXiv e-prints", "author": ["Mi et al.2016a] Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah"], "venue": null, "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "2016b. Vocabulary manipulation for neural machine translation", "author": ["Mi et al.2016b] Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "In Proceedings of ACL,", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Improved statistical alignment models", "author": ["Och", "Ney2000] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Och et al\\.", "year": 2000}, {"title": "Coverage-based Neural Machine Translation", "author": ["Tu et al.2016] Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method. CoRR", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Generalizing local and non-local word-reordering patterns for syntax-based machine translation", "author": ["Zhao", "Al-onaizan2008] Bing Zhao", "Yaser Alonaizan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 8, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 11, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al.", "startOffset": 75, "endOffset": 137}, {"referenceID": 15, "context": "However, the attention or alignment quality of NMT is still very low (Mi et al., 2016a; Tu et al., 2016).", "startOffset": 69, "endOffset": 104}, {"referenceID": 1, "context": "Neural machine translation (NMT) has gained popularity in recent two years (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015), especially for the attention-based models of Bahdanau et al. (2014). The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word.", "startOffset": 76, "endOffset": 207}, {"referenceID": 1, "context": "As shown in Figure 1, attention-based NMT (Bahdanau et al., 2014) is an encoder-decoder network.", "startOffset": 42, "endOffset": 65}, {"referenceID": 1, "context": "Figure 1: The architecture of attention-based NMT (Bahdanau et al., 2014).", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score (Mi et al., 2016b; Tu et al., 2016).", "startOffset": 112, "endOffset": 147}, {"referenceID": 9, "context": "(2016) adapted the agreementbased learning (Liang et al., 2006; Liang et al., 2008), and introduced a combined objective that takes into account both translation directions (source-to-target and target-to-source) and an agreement term between the two alignment directions.", "startOffset": 43, "endOffset": 83}, {"referenceID": 2, "context": "In order to improve the attention or alignment accuracy, Cheng et al. (2016) adapted the agreementbased learning (Liang et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 16, "context": "In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80.", "startOffset": 43, "endOffset": 57}, {"referenceID": 12, "context": "Following Mi et al. (2016a), the output vocabulary for each mini-batch or sentence is a sub-set of the full vo-", "startOffset": 10, "endOffset": 28}, {"referenceID": 5, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013).", "startOffset": 209, "endOffset": 228}, {"referenceID": 10, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 3, "context": "Our traditional SMT system is a hybrid syntaxbased tree-to-string model (Zhao and Al-onaizan, 2008), a simplified version of the joint decoding (Liu et al., 2009; Cmejrek et al., 2013).", "startOffset": 144, "endOffset": 184}, {"referenceID": 4, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al.", "startOffset": 210, "endOffset": 454}, {"referenceID": 4, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100.", "startOffset": 210, "endOffset": 573}, {"referenceID": 4, "context": "For each source sentence, the sentencelevel target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned from \u2018fast align\u2019 (Dyer et al., 2013). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable. The Cov. LVNMT system is a re-implementation of the enhanced NMT system of Mi et al. (2016a), which employs a coverage embedding model and achieves better performance over large vocabulary NMT Jean et al. (2015). The coverage embedding dimension of each source word is 100. Following Jean et al. (2015), we dump the alignments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.", "startOffset": 210, "endOffset": 664}, {"referenceID": 4, "context": "In terms of BLEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau.", "startOffset": 92, "endOffset": 114}], "year": 2016, "abstractText": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \u201ctrue\u201d alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "creator": "LaTeX with hyperref package"}}}