{"id": "1608.00778", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2016", "title": "Exponential Family Embeddings", "abstract": "word similarity embeddings are a powerful search approach for intimately capturing semantic similarity curves among terms specialized in locating a complex vocabulary. beginning in this paper, recently we develop generalized exponential family plot embeddings, a class of descriptive methods that extends down the idea of of combining word embeddings to introduce other types of high - dimensional data. utilizing as examples, we studied neural target data with real - valued observations, count experimental data resultant from observing a market basket error analysis, downloaded and analyzed ratings data originating from a movie recommendation detection system. the main idea is to model each variable observation conditioned on forming a probability set of seemingly other information observations. this set there is called just the truth context, and the way the classical context bias is defined performs is making a modeling option choice that depends on the problem. in short language syntax the word context is the surrounding words ; in animal neuroscience the context is close - guided by neurons ; also in market square basket data the conditional context response is also other items consumed in solving the cognitive shopping cart. each type of embedding trajectory model positively defines the context, defines the exponential family of conditional truth distributions, and influences how the latent embedding vectors are only shared across meaningful data. while we furthermore infer connected the embeddings functions with a scalable algorithm based on its stochastic fuzzy gradient kernel descent. on integrating all three potential applications - human neural activity typical of zebrafish, users'shopping network behavior, and adaptive movie statistical ratings - we found exponential harmonic family embedding distribution models prefer to be more effective again than other types of continuous dimension reduction. they better reconstruct held - afi out data and find interesting qualitative sample structure.", "histories": [["v1", "Tue, 2 Aug 2016 11:44:19 GMT  (703kb,D)", "http://arxiv.org/abs/1608.00778v1", null], ["v2", "Mon, 21 Nov 2016 15:12:54 GMT  (709kb,D)", "http://arxiv.org/abs/1608.00778v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["maja r rudolph", "francisco j r ruiz", "stephan mandt", "david m blei"], "accepted": true, "id": "1608.00778"}, "pdf": {"name": "1608.00778.pdf", "metadata": {"source": "CRF", "title": "Exponential Family Embeddings", "authors": ["Maja R. Rudolph", "Francisco J. R. Ruiz", "Stephan Mandt", "David M. Blei"], "emails": ["maja@cs.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "Word embeddings are a powerful approach for analyzing language (Bengio et al., 2006; Mikolov et al., 2013a,b; Pennington et al., 2014). A word embedding\n\u2217Corresponding author: maja@cs.columbia.edu\nar X\niv :1\n60 8.\n00 77\n8v 1\n[ st\nat .M\nL ]\nmethod discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c). Fitted word embeddings can help us understand the structure of language and are useful for downstream tasks based on text.\nThere are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas. Each term in a vocabulary is associated with two latent vectors, an embedding and a context vector. These two types of vectors govern conditional probabilities that relate each word to its context, i.e., the other words that appear around it. Specifically, the conditional probability of a word combines its embedding and the context vectors of its surrounding words. (Different methods combine them differently.) Given a corpus, we fit the embeddings and the context vectors by maximizing the conditional probabilities of the observed text.\nIn this paper we develop the exponential family embedding (EF-EMB), a class of models that generalizes the spirit of word embeddings to other types of highdimensional data. Our motivation is that other types of data can benefit from the same assumptions that underlie word embeddings, namely that a data point is governed by the other data in its context. In language, this is the foundational idea that words with similar meanings will appear in similar contexts (Harris, 1954). We use the tools of exponential families (Brown, 1986) and generalized linear models (GLMs) (McCullagh and Nelder, 1989) to adapt this idea beyond language.\nAs one example of a domain beyond language, we will study computational neuroscience. Neuroscientists measure sequential neural activity across many neurons in the brain. Their goal is to discover patterns in these data with the hope of better understanding the dynamics and connections among neurons. In this example, a context can be defined as the neural activities of other nearby neurons, or as neural activity in the past. Thus, it is plausible that the activity of each neuron depends on its context. We will use this idea to fit latent embeddings of neurons, representations of neurons that uncover hidden features which help suggest their roles in the brain.\nAnother example we study involves shoppers at the grocery store. Economists collect shopping data (called \u201cmarket basket data\u201d) and are interested in building models of purchase behavior for downstream econometric analysis, e.g., to predict demand and market changes. To build such models, they seek features of items that are predictive of when they are purchased and in what quantity. Similar to language, purchasing an item depends on its context, i.e., the other items in the shopping cart. In market basket data, Poisson embeddings can capture important econometric concepts, such as items that tend not to occur together but occur in the same contexts (substitutes) and items that co-occur,\nbut never one without the other (complements).\nWe define an EF-EMB, such as one for neuroscience or shopping data, with three ingredients. (1) We define the context, which specifies which other data points each observation depends on. (2) We define the conditional exponential family. This involves setting the appropriate distribution, such as a Gaussian for realvalued data or a Poisson for count data, and the way to combine embeddings and context vectors to form its natural parameter. (3) We define the embedding structure, how embeddings and context vectors are shared across the conditional distributions of each observation. These three ingredients enable us to build embedding models for many types of data.\nWe describe EF-EMB models and develop efficient algorithms for fitting them. We show how existing methods, such as continuous bag of words (CBOW) (Mikolov et al., 2013a) and negative sampling (Mikolov et al., 2013b), can each be viewed as an EF-EMB. We study our methods on three different types of data\u2014 neuroscience data, shopping data, and movie ratings data. Mirroring the success of word embeddings, EF-EMB models outperform traditional dimensionality reduction, such as exponential family principal component analysis (PCA) (Collins et al., 2001) and Poisson factorization (Gopalan et al., 2015), and find interpretable features of the data.\nRelated work. EF-EMB models generalize CBOW (Mikolov et al., 2013a) in the same way that exponential family PCA (Collins et al., 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al., 2015) generalize sigmoid belief networks (Neal, 1990). A linear EF-EMB (which we define precisely below) relates to contextwindow-based embedding methods such as CBOW or the vector log-bilinear language model (VLBL) (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013), which model a word given its context. The more general EF-EMB relates to embeddings with a nonlinear component, such as the skip-gram (Mikolov et al., 2013a) or the inverse vector log-bilinear language model (IVLBL) (Mnih and Kavukcuoglu, 2013). (These methods might appear linear but, when viewed as a conditional probabilistic model, the normalizing constant of each word induces a nonlinearity.)\nResearchers have developed different approximations of the word embedding objective to scale the procedure. These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al., 2013b), and negative sampling (Mikolov et al., 2013a). We show in Section 2.2 that negative sampling corresponds to biased stochastic gradients of an EF-EMB objective."}, {"heading": "2 Exponential Family Embeddings", "text": "We consider a matrix x= x1:I of I observations, where each x i is a D-vector. As one example, in language x i is an indicator vector for the word at position i and D is the size of the vocabulary. As another example, in neural data x i is the neural activity measured at index pair i = (n, t), where n indexes a neuron and t indexes a time point; each measurement is a scalar (D = 1).\nThe goal of an exponential family embedding (EF-EMB) is to derive useful features of the data. There are three ingredients: a context function, a conditional exponential family, and an embedding structure. These ingredients work together to form the objective. First, the EF-EMB models each data point conditional on its context; the context function determines which other data points are at play. Second, the conditional distribution is an appropriate exponential family, e.g., a Gaussian for real-valued data. Its parameter is a function of the embeddings of both the data point and its context. Finally, the embedding structure determines which embeddings are used when the ith point appears, either as data or in the context of another point. The objective is the sum of the log probabilities of each data point given its context.\nWe describe each ingredient, followed by the EF-EMB objective. Examples are in Section 2.1.\nContext. Each data point i has a context ci, which is a set of indices of other data points. The EF-EMB models the conditional distribution of x i given the data points in its context.\nThe context is a modeling choice; different applications will require different types of context. In language, the data point is a word and the context is the set of words in a window around it. In neural data, the data point is the activity of a neuron at a time point and the context is the activity of its surrounding neurons at the same time point. (It can also include neurons at future time or in the past.) In shopping data, the data point is a purchase and the context is the other items in the cart.\nConditional exponential family. An EF-EMB models each data point x i conditional on its context xci . The distribution is an appropriate exponential family,\nx i |xci \u223c ExpFam(\u03b7i(xci), t(x i)), (1)\nwhere \u03b7i(xci) is the natural parameter and t(x i) is the sufficient statistic. In language modeling, this family is usually a categorical distribution. Below, we will study Gaussian and Poisson.\nWe parameterize the conditional with two types of vectors, embeddings and context vectors. The embedding of the ith data point helps govern its distribution;\nwe denote it \u03c1[i] \u2208 RK\u00d7D. The context vector of the ith data point helps govern the distribution of data for which i appears in their context; we denote it \u03b1[i] \u2208 RK\u00d7D.\nHow to define the natural parameter as a function of these vectors is a modeling choice. It captures how the context interacts with an embedding to determine the conditional distribution of a data point. Here we focus on the linear embedding, where the natural parameter is a function of a linear combination of the latent vectors,\n\u03b7i(xci) = fi\n\n\u03c1[i]> \u2211\nj\u2208ci\n\u03b1[ j]x j\n\n . (2)\nFollowing the nomenclature of generalized linear models (GLMs), we call fi(\u00b7) the link function. We will see several examples of link functions in Section 2.1.\nThis is the setting of many existing word embedding models, though not all. Other models, such as the skip-gram, determine the probability through a \u201creverse\u201d distribution of context words given the data point. These amount to non-linear embeddings, though are still instances of an EF-EMB.\nEmbedding structure. The goal of an EF-EMB is to find embeddings and context vectors that describe features of the data. The embedding structure determines how an EF-EMB shares these vectors across the data. It is through sharing the vectors that we learn an embedding for the object of primary interest, such as a vocabulary term, a neuron, or a supermarket product.\nIn language the same parameters \u03c1[i] = \u03c1 and \u03b1[i] = \u03b1 are shared across all positions i. In neural data, observations share parameters when they describe the same neuron. Recall that the index connects to both a neuron and time point i = (n, t). We share parameters with \u03c1[i] = \u03c1n and \u03b1[i] = \u03b1n to find embeddings and context vectors that describe the neurons. Other variants might tie the embedding and context vectors to find a single set of latent variables, \u03c1[i] = \u03b1[i].\nThe objective function. The EF-EMB objective sums the log conditional probabilities of each data point, adding regularizers for the embeddings and context vectors.1 We use log probability functions as regularizers, e.g., a Gaussian probability leads to `2 regularization. We also use regularizers to constrain the embeddings,e.g., to be non-negative. Thus, the objective is\nL (\u03c1,\u03b1) = I \u2211\ni=1\n\u03b7>i t(x i)\u2212 a(\u03b7i) + log p(\u03c1) + log p(\u03b1). (3)\n1One might be tempted to see this as a probabilistic model that is conditionally specified. However, in general it does not have a consistent joint distribution (Arnold et al., 2001).\nWe maximize this objective with respect to the embeddings and context vectors. In Section 2.2 we show how to fit it with stochastic gradients.\nEquation (3) can be seen as a likelihood function for a bank of GLMs (McCullagh and Nelder, 1989). Each data point is modeled as a response conditional on its \u201ccovariates,\u201d which combine the context vectors and context, e.g., as in Equation (2); the coefficient for each response is the embedding itself. In Section 2.2, we will use properties of exponential families and results around GLMs to derive efficient algorithms for EF-EMB models."}, {"heading": "2.1 Examples", "text": "We highlight the versatility of EF-EMB models with three example models and their variations. We develop the Gaussian embedding (G-EMB) for analyzing real observations from a neuroscience application; we also introduce a nonnegative version, the nonnegative Gaussian embedding (NG-EMB). We develop two Poisson embedding models, Poisson embedding (P-EMB) and additive Poisson embedding (AP-EMB), for analyzing count data; these have different link functions. We present a categorical embedding model that corresponds to the continuous bag of words (CBOW) word embedding (Mikolov et al., 2013a). Finally, we present a Bernoulli embedding (B-EMB) for binary data. In Section 2.2 we show that negative sampling (Mikolov et al., 2013b) corresponds to biased stochastic gradients of the B-EMB objective.\nFor convenience, these acronyms are summarized in Table 1\nExample 1: Neural data and Gaussian observations. Consider the (calcium) expression of a large population of zebrafish neurons (Ahrens et al., 2013). The data are processed to extract the locations of the N neurons and the neural activity x i = x(n,t) across location n and time t. The goal is to model the similarity between neurons in terms of their behavior, to embed each neuron in a latent space such that neurons with similar behavior are close to each other.\nWe consider two neurons similar if they behave similarly in the context of the activity pattern of their surrounding neurons. Thus we define the context for data index i = (n, t) to be the indices of the activity of nearby neurons at the\nsame time. We find the K-nearest neighbors (KNN) of each neuron (using a Ball-tree algorithm) according to their spatial distance in the brain. We use this set to construct the context ci = c(n,t) = {(m, t)|m \u2208 KNN(n)}. This context varies with each neuron, but is constant over time.\nWith the context defined, each data point x i is modeled with a conditional Gaussian. The conditional mean is the inner product from Equation (2), where the context is the simultaneous activity of the nearest neurons. The link function is the identity.\nThe conditionals of two observations share parameters if they correspond to the same neuron. The embedding structure is thus \u03c1[i] = \u03c1n and \u03b1[i] = \u03b1n for all index pairs i = (n, t). Similar to word embeddings, each neuron has two distinct latent vectors: the neuron embedding \u03c1n \u2208 RK and the context vector \u03b1n \u2208 RK .\nThese ingredients, along with a regularizer, combine to form a neural embedding objective. G-EMB uses `2 regularization (i.e., a Gaussian prior); NG-EMB constrains the vectors to be nonnegative, placing an `2 regularization on the logarithm of the embeddings (i.e., a log-normal prior).\nExample 2: Shopping data and Poisson observations. We also study data about people shopping. The data contains the individual purchases of anonymous users in chain grocery and drug stores. There are N different items and T trips to the stores among all households. The data is a sparse N \u00d7 T matrix of purchase counts. The entry x i = x(n,t) indicates the number of units of item n that was purchased on trip t. Our goal is to learn a latent representation for each product that captures the similarity between them.\nWe consider items to be similar if they tend to be purchased in with similar groups of other items. The context for observation x i is thus the other items in the shopping basket on the same trip. For the purchase count at index i = (n, t), the context is ci = { j = (m, t)|m 6= n}.\nWe use Poisson distributions to model the conditional distributions of the count data. The sufficient statistic of the Poisson is t(x i) = x i, and its natural parameter is the logarithm of the rate (i.e., the mean). We set the natural parameter as in Equation (2), with the link function defined below. The embedding structure is the same as in G-EMB, resulting in an embedding and a context vector for each item.\nWe explore two choices for the link function. P-EMB uses an identity link function. Since the conditional mean is the exponentiated natural parameter, this implies that the context items contribute multiplicatively to the mean. (We use `2-regularization on the embeddings.) Alternatively, we can constrain the parameters to be nonnegative and set the link function f (\u00b7) = log(\u00b7). This is AP-EMB, a model with an additive mean parameterization. (We use `2-regularization on the logarithm of the embeddings.) Note AP-EMB only captures positive correlations\nbetween items.\nExample 3: Text modeling and categorical observations. EF-EMBs are inspired by word embeddings, such as CBOW (Mikolov et al., 2013a). CBOW is a special case of an EF-EMB; it is equivalent to a multivariate EF-EMB with categorical conditionals. In the notation here, each x i is an indicator vector of the ith word. Its dimension is the vocabulary size. The context of the ith word are the other words in a window around it (of size w), ci = { j 6= i|i\u2212w \u2264 j \u2264 i+w}.\nThe distribution of x i is categorical, conditioned on the surrounding words xci ; this is a softmax regression. It has natural parameter as in Equation (2) with an identity link function. The embedding structure imposes that parameters are shared across all observed words. The embeddings are shared across all positions in the text (\u03c1[i] = \u03c1, \u03b1[i] = \u03b1 \u2208 RN\u00d7K). The word and context embedding of the nth word is the nth row of \u03c1 and \u03b1 respectively. CBOW does not use any regularizer.\nExample 4: Text modeling and binary observations. One way to simplify the CBOW objective is with a model of each entry of the indicator vectors. The data are binary and indexed by i = (n, v), where n is the position in the text and v indexes the vocabulary; the variable xn,v is the indicator that word n is equal to term v. (This model relaxes the constraint that for any n only one xn,v will be on.) With this notation, the context is ci = {( j, v\u2032)|\u2200v\u2032, j 6= n, n\u2212w \u2264 j \u2264 n+w}; the embedding structure is \u03c1[i] = \u03c1[(n, v)] = \u03c1v and \u03b1[i] = \u03b1[(n, v)] = \u03b1v.\nWe can consider different conditional distributions in this setting. As one example, set the conditional distribution to be a Bernoulli with an identity link; we call this the B-EMB model for text. In Section 2.2 we show that biased stochastic gradients of the B-EMB objective recovers negative sampling (Mikolov et al., 2013b). As another example, set the conditional distribution to Poisson with link f (\u00b7) = log(\u00b7). The corresponding embedding model relates closely to Poisson approximations of distributed multinomial regression (Taddy et al., 2015)."}, {"heading": "2.2 Inference", "text": "We fit the embeddings \u03c1[i] and context vectors \u03b1[i] by maximizing the objective function in Equation (3). We use stochastic gradient descent (SGD).\nWe first calculate the gradient, using the identity for exponential family distributions that the derivative of the log-normalizer is equal to the expectation of the sufficient statistics, i.e., E[t(X )] =\u2207\u03b7a(\u03b7). With this result, the gradient with respect to the embedding \u03c1[ j] is\n\u2207\u03c1[ j]L = I \u2211\ni=1\nt(x i)\u2212E[t(x i)] \u2207\u03c1[ j]\u03b7i +\u2207\u03c1[ j] log p(\u03c1[ j]). (4)\nThe gradient with respect to \u03b1[ j] has the same form. In Appendix A, we detail this expression for the particular models that we study empirically (Section 3).\nThe gradient in Equation (4) can involve a sum of many terms and be computationally expensive to compute. To alleviate this, we follow noisy gradients using SGD. We form a subsample S of the I terms in the summation, i.e.,\nb\u2207\u03c1[ j]L = I\n|S |\n\u2211\ni\u2208S\nt(x i)\u2212E[t(x i)] \u2207\u03c1[ j]\u03b7i +\u2207\u03c1[ j] log p(\u03c1[ j]), (5)\nwhere |S | denotes the size of the subsample and where we scaled the summation to ensure an unbiased estimator of the gradient. Equation (5) reduces computational complexity when |S | is much smaller than the total number of terms. At each iteration of SGD we compute noisy gradients with respect to \u03c1[ j] and \u03b1[ j] (for each j) and take gradient steps according to a step-size schedule. We use Adagrad (Duchi et al., 2011) to set the step-size.\nRelation to negative sampling. In language, particularly when seen as a collection of binary variables, the data are sparse: each word is one of a large vocabulary. When modeling sparse data, we split the sum in Equation (4) into two contributions: those where x i > 0 and those where x i = 0. The gradient is\n\u2207\u03c1[ j]L = \u2211\ni:x i>0\nt(x i)\u2212E[t(x i)] \u2207\u03c1[ j]\u03b7i + \u2211\ni:x i=0\nt(0)\u2212E[t(x i)] \u2207\u03c1[ j]\u03b7i (6)\n+\u2207\u03c1[ j] log p(\u03c1[ j]).\nWe compute the gradient of the first term exactly\u2014when the data is sparse there are not many summations to make\u2014and we estimate the gradient of the second term with subsampling. Compared to computing the full gradient, this reduces the complexity when most of the entries x i are zero. But, it retains the strong information about the gradient that comes from the non-zero entries.\nThis relates to negative sampling, which is used to approximate the skip-gram objective (Mikolov et al., 2013b). Negative sampling re-defines the skip-gram objective to distinguish target (observed) words from randomly drawn words, using logistic regression. The gradient of the stochastic objective is identical to a noisy but biased estimate of the gradient in Equation (6) for a B-EMB model. To obtain the equivalence, preserve the terms for the non-zero data and subsample terms for the zero data. While an unbiased stochastic gradient would rescale the subsampled terms, negative sampling does not. It is thus a biased estimate, which down-weights the contribution of the zeros."}, {"heading": "3 Empirical Study", "text": "We study exponential family embedding (EF-EMB) models on real-valued and count-valued data, and in different application domains\u2014computational neuroscience, shopping behavior, and movie ratings. We present quantitative comparisons to other dimensionality reduction methods and illustrate how we can glean qualitative insights from the fitted embeddings."}, {"heading": "3.1 Real Valued Data: Neural Data Analysis", "text": "Data. We analyze the neural activity of a larval zebrafish, recorded at single cell resolution for 3000 time frames (Ahrens et al., 2013). Through genetic modification, individual neurons express a calcium indicator when they fire. The resulting calcium imaging data is preprocessed by a nonnegative matrix factorization to identify neurons, their locations, and the fluorescence activity x\u2217t \u2208 R\nN of the individual neurons over time (Friedrich et al., 2015). Using this method, our data contains 10,000 neurons (out of a total of 200,000).\nWe fit all models on the lagged data x t = x\u2217t \u2212 x \u2217 t\u22121 to filter out correlations based on calcium decay and preprocessing.2 The calcium levels can be measured with great spatial resolution but there is heavy aliasing in the temporal dimension; the firing rate is much higher than the sampling rate. Hence we ignore all \u201ctemporal structure\u201d in the data and model the simultaneous activity of the neurons. We use the Gaussian embedding (G-EMB) and nonnegative Gaussian embedding (NGEMB) from Section 2.1 to model the lagged activity of the neurons conditional on the lags of surrounding neurons. We study context sizes c \u2208 {10, 50} and latent dimension K \u2208 {10,100}.\nModels. We compare EF-EMB to probabilistic factor analysis (FA), fitting Kdimensional factors for each neuron and K-dimensional factor loadings for each time frame. In FA, each entry of the data matrix is a Gaussian with mean equal to the inner product of the corresponding factor and factor loading.\nEvaluation. We train each model on a random sample of 90% of the lagged time frames and hold out 5% each for validation and testing. With the test set, we use two types of evaluation. (1) Leave one out: For each neuron x i in the test set, we use the measurements of the other neurons to form predictions. For FA this means the other neurons are used to recover the factor loadings; for EF-EMB this means the other neurons are used to construct the context. (2) Leave 25% out: We randomly split the neurons into 4 folds. Each neuron is predicted using the three sets of neurons that are out of its fold. (This is a more difficult task.)\n2We also analyzed unlagged data but, as expected, all methods have better reconstruction performance on the lagged data.\nNote in EF-EMB, the missing data might change the size of the context of some neurons. See Table 5 in Appendix B for the choice of hyperparameters.\nResults. Table 2 reports both types of evaluation. The EF-EMB models significantly outperform FA in terms of mean squared error on the test set. G-EMB obtains the best results with 100 components and a context size of 50. Figure 1 illustrates how to use the learned embeddings to hypothesize connections between nearby neurons."}, {"heading": "3.2 Count data: Market Basket Analysis and Movie Ratings", "text": "We study the Poisson models Poisson embedding (P-EMB) and additive Poisson embedding (AP-EMB) on two applications: shopping and movies.\nMarket basket data. We analyze the IRI dataset3 (Bronnenberg et al., 2008),\n3We thank IRI for making the data available. All estimates and analysis in this paper, based\nwhich contains the purchases of anonymous households in chain grocery and drug stores. It contains 137,632 trips in 2012. We remove items that appear fewer than 10 times, leaving a dataset with 7,903 items. The context for each purchase is the other purchases from the same trip.\nMovieLens data. We also analyze the MovieLens-100K dataset (Harper and Konstan, 2015), which contains movie ratings on a scale from 1 to 5. We keep only positive ratings, defined to be ratings of 3 or more (we subtract 2 from all ratings and set the negative ones to 0). The context of each rating is the other movies rated by the same user. After removing users who rated fewer than 20 movies and movies that were rated fewer than 50 times, the dataset contains 777 users and 516 movies; the sparsity is about 5%.\nModels. We fit the P-EMB and the AP-EMB models using number of components K \u2208 {20,50,100}. For each K we select the Adagrad constant based on best predictive performance on the validation set. (The parameters we used are in Table 5.) In these datasets, the distribution of the context size is heavy tailed. To handle larger context sizes we pick a link function for the EF-EMB model which rescales the sum over the context in Equation (2) by the context size (the number of terms in the sum). We also fit a P-EMB model that artificially downweights the contribution of the zeros in the objective function by a factor of 0.1, as done by Hu et al. (2008) for matrix factorization. We denote it as \u201cP-EMB (dw).\u201d\non data provided by IRI, are by the authors and not by IRI.\nWe compare the predictive performance with HPF (Gopalan et al., 2015) and Poisson PCA (Collins et al., 2001). Both HPF and Poisson PCA factorize the data into K-dimensional positive vectors of user preferences, and K-dimensional positive vectors of item attributes. AP-EMB and HPF parameterize the mean additively; P-EMB and Poisson PCA parameterize it multiplicatively. For the EF-EMB models and Poisson PCA, we use stochastic optimization with `2 regularization. For HPF, we use variational inference. See Table 5 in Appendix B for details.\nEvaluation. For the market basket data we hold out 5% of the trips to form the test set, also removing trips with fewer than two purchased different items. In the MovieLens data we hold out 20% of the ratings and set aside an additional 5% of the non-zero entries from the test for validation. We report prediction performance based on the normalized log-likelihood on the test set. For P-EMB and AP-EMB, we compute the likelihood as the Poisson mean of each nonnegative count (be it a purchase quantity or a movie rating) divided by the sum of the Poisson means for all items, given the context. To evaluate HPF and Poisson PCA at a given test observation we recover the factor loadings using the other test entries we condition on, and we use the factor loading to form the prediction.\nPredictive performance. Table 3 summarizes the test log-likelihood of the four models, together with the standard errors across entries in the test set. In both applications the P-EMB model outperforms HPF and Poisson PCA. On shopping data P-EMB with K = 100 provides the best predictions; on MovieLens P-EMB with K = 20 is best. For P-EMB on shopping data, downweighting the contribution of the zeros gives more accurate estimates.\nItem similarity in the shopping data. Embedding models can capture qualitative aspects of the data as well. Table 4 shows four example products and their three most similar items, where similarity is calculated as the cosine distance between embedding vectors. (These vectors are from P-EMB with downweighted zeros and K = 100.) For example, the most similar items to a soda are other sodas; the most similar items to a yogurt are (mostly) other yogurts.\nThe P-EMB model can also identify complementary and substitutable products. To see this, we compute the inner products of the embedding and the context vectors for all item pairs. A high value of the inner product indicates that the probability of purchasing one item is increased if the second item is in the shopping basket (i.e., they are complements). A low value indicates the opposite effect and the items might be substitutes for each other.\nWe find that items that tend to be purchased together have high value of the inner product (e.g., potato chips and beer, potato chips and frozen pizza, or two different types of soda), while items that are substitutes have negative value (e.g., two different brands of pasta sauce, similar snacks, or soups from different brands). Other items with negative value of the inner product are not substitutes, but they are rarely purchased together (e.g., toast crunch and laundry\ndetergent, milk and a toothbrush). Appendix C gives examples of substitutes and complements.\nTopics in the movie embeddings. The embeddings from MovieLens data identify thematically similar movies. For each latent dimension k, we sort the context vectors by the magnitude of the kth component. This yields a ranking of movies for each component. In Appendix D we show two example rankings. (These are from the P-EMB model with K = 50.) The first one contains children\u2019s movies; the second contains science-fiction/action movies."}, {"heading": "4 Discussion", "text": "We described exponential family embeddings (EF-EMBs), conditionally specified latent variable models to extract distributed representations from high dimensional data. We showed that continuous bag of words (CBOW) (Mikolov et al., 2013b) is a special case of EF-EMB and we provided examples beyond text: the brain activity of zebrafish, shopping data, and movie ratings. We fit the EF-EMB objective usng stochastic gradients. Our empirical study demonstrates that an EF-EMB can better reconstruct data than existing dimensionality-reduction techniques based on matrix factorization. Further, the learned embeddings capture interesting semantic structure."}, {"heading": "A Stochastic Gradient Descent", "text": "To specify the gradients in Equation 4 for the stochastic gradient descent (SGD) procedure we need the sufficient statistic t(x), the expected sufficient statistic E[t(x)], the gradient of the natural parameter with respect to the embedding vectors and the gradient of the regularizer on the embedding vectors. In this appendix we specify these quantities for the models we study empirically in Section 3.\nA.1 Gradients for Gaussian embedding (G-EMB)\nUsing the notation i = (n, t) and reflecting the embedding structure \u03c1[i] = \u03c1n, \u03b1[i] = \u03b1n, the gradients with respect to each embedding and each context vector becomes\n\u2207\u03c1nL =\u2212\u03bb\u03c1n+ 1\n\u03c32\nT \u2211\nt=1\nx(n,t)\u2212\u03c1>n \u2211\nm\u2208cn\nx(m,t)\u03b1m\n\u2211\nm\u2208cn\nx(m,t)\u03b1m\n(7)\n\u2207\u03b1nL =\u2212\u03bb\u03b1n+ 1\n\u03c32\nT \u2211\nt=1\n\u2211\nm|n\u2208cm\nx(m,t)\u2212\u03c1>m \u2211\nr\u2208cm\nx(r,t)\u03b1r x(n,t)\u03c1m\n(8)\nA.2 Gradients for nonnegative Gaussian embedding (NG-EMB)\nBy restricting the parameters to be nonnegative we can learn nonnegative synaptic weights between neurons. For notational simplicity we write the parameters as exp(\u03c1) and exp(\u03b1) and update them in log-space. The operator \u25e6 stands for element wise multiplication. With this notation, the gradient for the NG-EMB can be easily obtained from Equations 7 and 8 by applying the chain rule.\n\u2207\u03c1nL =\u2212\u03bbexp(\u03c1n) \u25e6 exp(\u03c1n) (9)\n+ 1\n\u03c32\nT \u2211\nt=1\nx(n,t)\u2212 exp(\u03c1n)> \u2211\nm\u2208cn\nx(m,t) exp(\u03b1m)\n\u2211\nm\u2208cn\nx im exp(\u03c1n) \u25e6 exp(\u03b1m)\n\u2207\u03b1nL =\u2212\u03bbexp(\u03b1n) \u25e6 exp(\u03b1n) (10)\n+ 1\n\u03c32\nT \u2211\nt=1\n\u2211\nm|n\u2208cm\nx(m,t)\u2212 exp(\u03c1m)> \u2211\nr\u2208cm\nx(r,t) exp(\u03b1r) x(n,t) exp(\u03c1m) \u25e6 exp(\u03b1n)\nA.3 Gradients for Poisson embedding (P-EMB)\nWe proceed similarly as for the G-EMB model.\n\u2207\u03c1nL =\u2212\u03bb\u03c1n+ T \u2211\nt=1\nx(n,t)\u2212 exp \u03c1>n\n\u2211\nm\u2208cn\nx(m,t)\u03b1m\n!!\n\u2211\nm\u2208cn\nx(m,t)\u03b1m\n!\n(11)\n\u2207\u03b1nL =\u2212\u03bb\u03b1n+ T \u2211\nt=1\n\u2211\nm|n\u2208cm\nx(m,t)\u2212 exp \u03c1>m\n\u2211\nr\u2208cm\nx(r,t)\u03b1r\n!!\nx(n,t)\u03c1m\n(12)\nA.4 Gradients for additive Poisson embedding (AP-EMB)\nHere, we proceed in a similar manner as for the NG-EMB model.\n\u2207\u03c1nL =\u2212\u03bbexp(\u03c1n) \u25e6 exp(\u03c1n) + T \u2211\nt=1 x(n,t) \u03c1>n \u2211 m\u2208cn x(m,t)\u03b1m \u2212 1\n!\n\u2211\nm\u2208cn\nx(m,t)\u03b1m\n!\n(13)\n\u2207\u03b1nL =\u2212\u03bbexp(\u03b1n) \u25e6 exp(\u03b1n) + T \u2211\nt=1\n\u2211\nm|n\u2208cm\nx(m,t) \u03c1>m \u2211 r\u2208cm x(r,t)\u03b1r \u2212 1\n!\nx(n,t)\u03c1m\n(14)"}, {"heading": "B Algorithm Details", "text": "Model minibatch size regularization parameter number iterations negative samples\nneuro G-EMB 100 10 500 n/a neuro NG-EMB 100 0.1 500 n/a shopping all models n/a 1 3000 10 movies all models n/a 1 3000 10\nTable 5: Algorithm details for the models studied in Section 3."}, {"heading": "C Complements and Substitutes in the Shopping", "text": "Data\nTable 6 shows some pairs of items with high inner product of embedding vectors and context vector. The items in the first column have higher probability of being purchased if the item in the second column is in the shopping basket. We can observe that they correspond to items that are frequently purchased together (potato chips and beer, potato chips and frozen pizza, two different sodas).\nSimilarly, Table 7 shows some pairs of items with low inner product. The items in the first column have lower probability of being purchased if the item in the second column is in the shopping basket. We can observe that they correspond to items that are rarely purchased together (detergent and toast crunch, milk and toothbrush), or that are substitutes of each other (two different brands of snacks, soup, or pasta sauce)."}, {"heading": "D Movie Rating Results", "text": "Tables 8 and 9 show clusters of ranked movies that are learned by our P-EMB model. These rankings were generated as follows. For each latent dimension k \u2208 {1, \u00b7 \u00b7 \u00b7 , K} we sorted the context vectors according their value in this dimension. This gives us a ranking of context vectors for every k. Tables 8 and 9 show the 10 top items of the ranking for two different values of k. Similar as in topic modeling, the latent dimensions have the interpretation of topics. We see that sorting the context vectors this way reveals thematic structure in the collection of movies. While Table 8 gives a table of movies for children, Table 9 shows a cluster of science-fiction and action movies (with a few outliers)."}], "references": [{"title": "Whole-brain functional imaging at cellular resolution using light-sheet microscopy", "author": ["M.B. Ahrens", "M.B. Orger", "D.N. Robson", "J.M. Li", "P.J. Keller"], "venue": "Nature Methods, 10(5):413\u2013420.", "citeRegEx": "Ahrens et al\\.,? 2013", "shortCiteRegEx": "Ahrens et al\\.", "year": 2013}, {"title": "Conditionally specified distributions: an introduction (with comments and a rejoinder by the authors)", "author": ["B.C. Arnold", "E. Castillo", "Sarabia", "J. M"], "venue": null, "citeRegEx": "Arnold et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Arnold et al\\.", "year": 2001}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "Sen\u00e9cal", "J.-S.", "F. Morin", "Gauvain", "J.-L."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Database paper: The IRI marketing data set", "author": ["B.J. Bronnenberg", "M.W. Kruger", "C.F. Mela"], "venue": "Marketing Science, 27(4):745\u2013748.", "citeRegEx": "Bronnenberg et al\\.,? 2008", "shortCiteRegEx": "Bronnenberg et al\\.", "year": 2008}, {"title": "Fundamentals of statistical exponential families with applications in statistical decision theory", "author": ["L.D. Brown"], "venue": "Lecture Notes-Monograph Series, 9:i\u2013279.", "citeRegEx": "Brown,? 1986", "shortCiteRegEx": "Brown", "year": 1986}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R.E. Schapire"], "venue": "Neural Information Processing Systems, pages 617\u2013624.", "citeRegEx": "Collins et al\\.,? 2001", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, 12:2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Fast constrained non-negative matrix factorization for whole-brain calcium imaging data", "author": ["J. Friedrich", "D. Soudry", "L. Paninski", "Y. Mu", "J. Freeman", "M. Ahrens"], "venue": "NIPS workshop on Neural Systems.", "citeRegEx": "Friedrich et al\\.,? 2015", "shortCiteRegEx": "Friedrich et al\\.", "year": 2015}, {"title": "Scalable recommendation with hierarchical Poisson factorization", "author": ["P. Gopalan", "J. Hofman", "D.M. Blei"], "venue": "Uncertainty in Artificial Intelligence.", "citeRegEx": "Gopalan et al\\.,? 2015", "shortCiteRegEx": "Gopalan et al\\.", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2010}, {"title": "The MovieLens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19.", "citeRegEx": "Harper and Konstan,? 2015", "shortCiteRegEx": "Harper and Konstan", "year": 2015}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "Data Mining.", "citeRegEx": "Hu et al\\.,? 2008", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Neural Information Processing Systems, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg,? 2014", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Generalized linear models, volume 37", "author": ["P. McCullagh", "J.A. Nelder"], "venue": "CRC press.", "citeRegEx": "McCullagh and Nelder,? 1989", "shortCiteRegEx": "McCullagh and Nelder", "year": 1989}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop Proceedings. arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "Yih", "W.-T. a.", "G. Zweig"], "venue": "HLT-NAACL, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Neural Information Processing Systems, pages 2265\u20132273.", "citeRegEx": "Mnih and Kavukcuoglu,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "International Conference on Machine Learning, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh,? 2012", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Learning stochastic feedforward networks", "author": ["R.M. Neal"], "venue": "Department of Computer Science, University of Toronto.", "citeRegEx": "Neal,? 1990", "shortCiteRegEx": "Neal", "year": 1990}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Conference on Empirical Methods on Natural Language Processing, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "Artificial Intelligence and Statistics.", "citeRegEx": "Ranganath et al\\.,? 2015", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "Nature, 323:9.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Distributed multinomial regression", "author": ["M Taddy"], "venue": "The Annals of Applied Statistics, 9(3):1394\u20131414.", "citeRegEx": "Taddy,? 2015", "shortCiteRegEx": "Taddy", "year": 2015}, {"title": "Word representations via Gaussian embedding", "author": ["L. Vilnis", "A. McCallum"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Vilnis and McCallum,? 2015", "shortCiteRegEx": "Vilnis and McCallum", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Word embeddings are a powerful approach for analyzing language (Bengio et al., 2006; Mikolov et al., 2013a,b; Pennington et al., 2014).", "startOffset": 63, "endOffset": 134}, {"referenceID": 21, "context": "Word embeddings are a powerful approach for analyzing language (Bengio et al., 2006; Mikolov et al., 2013a,b; Pennington et al., 2014).", "startOffset": 63, "endOffset": 134}, {"referenceID": 23, "context": "method discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c).", "startOffset": 182, "endOffset": 250}, {"referenceID": 2, "context": "method discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c).", "startOffset": 182, "endOffset": 250}, {"referenceID": 17, "context": "method discovers distributed representations of words; these representations capture the semantic similarity between the words and reflect a variety of other linguistic regularities (Rumelhart et al., 1986; Bengio et al., 2006; Mikolov et al., 2013c).", "startOffset": 182, "endOffset": 250}, {"referenceID": 18, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 13, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 21, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 25, "context": "There are many variants, adaptations, and extensions of word embeddings (Mikolov et al., 2013a,b; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Vilnis and McCallum, 2015), but each reflects the same main ideas.", "startOffset": 72, "endOffset": 202}, {"referenceID": 11, "context": "In language, this is the foundational idea that words with similar meanings will appear in similar contexts (Harris, 1954).", "startOffset": 108, "endOffset": 122}, {"referenceID": 4, "context": "We use the tools of exponential families (Brown, 1986) and generalized linear models (GLMs) (McCullagh and Nelder, 1989) to adapt this idea beyond language.", "startOffset": 41, "endOffset": 54}, {"referenceID": 14, "context": "We use the tools of exponential families (Brown, 1986) and generalized linear models (GLMs) (McCullagh and Nelder, 1989) to adapt this idea beyond language.", "startOffset": 92, "endOffset": 120}, {"referenceID": 15, "context": "We show how existing methods, such as continuous bag of words (CBOW) (Mikolov et al., 2013a) and negative sampling (Mikolov et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": ", 2013a) and negative sampling (Mikolov et al., 2013b), can each be viewed as an EF-EMB.", "startOffset": 31, "endOffset": 54}, {"referenceID": 5, "context": "Mirroring the success of word embeddings, EF-EMB models outperform traditional dimensionality reduction, such as exponential family principal component analysis (PCA) (Collins et al., 2001) and Poisson factorization (Gopalan et al.", "startOffset": 167, "endOffset": 189}, {"referenceID": 8, "context": ", 2001) and Poisson factorization (Gopalan et al., 2015), and find interpretable features of the data.", "startOffset": 34, "endOffset": 56}, {"referenceID": 15, "context": "EF-EMB models generalize CBOW (Mikolov et al., 2013a) in the same way that exponential family PCA (Collins et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": ", 2013a) in the same way that exponential family PCA (Collins et al., 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 14, "context": ", 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 22, "context": ", 2001) generalizes PCA, GLMs (McCullagh and Nelder, 1989) generalize regression, and deep exponential families (Ranganath et al., 2015) generalize sigmoid belief networks (Neal, 1990).", "startOffset": 112, "endOffset": 136}, {"referenceID": 20, "context": ", 2015) generalize sigmoid belief networks (Neal, 1990).", "startOffset": 43, "endOffset": 55}, {"referenceID": 15, "context": "A linear EF-EMB (which we define precisely below) relates to contextwindow-based embedding methods such as CBOW or the vector log-bilinear language model (VLBL) (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013), which model a word given its context.", "startOffset": 161, "endOffset": 212}, {"referenceID": 18, "context": "A linear EF-EMB (which we define precisely below) relates to contextwindow-based embedding methods such as CBOW or the vector log-bilinear language model (VLBL) (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013), which model a word given its context.", "startOffset": 161, "endOffset": 212}, {"referenceID": 15, "context": "The more general EF-EMB relates to embeddings with a nonlinear component, such as the skip-gram (Mikolov et al., 2013a) or the inverse vector log-bilinear language model (IVLBL) (Mnih and Kavukcuoglu, 2013).", "startOffset": 96, "endOffset": 119}, {"referenceID": 18, "context": ", 2013a) or the inverse vector log-bilinear language model (IVLBL) (Mnih and Kavukcuoglu, 2013).", "startOffset": 67, "endOffset": 95}, {"referenceID": 9, "context": "These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al.", "startOffset": 43, "endOffset": 92}, {"referenceID": 19, "context": "These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al.", "startOffset": 43, "endOffset": 92}, {"referenceID": 16, "context": "These include noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010; Mnih and Teh, 2012), hierarchical softmax (Mikolov et al., 2013b), and negative sampling (Mikolov et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 15, "context": ", 2013b), and negative sampling (Mikolov et al., 2013a).", "startOffset": 32, "endOffset": 55}, {"referenceID": 1, "context": "However, in general it does not have a consistent joint distribution (Arnold et al., 2001).", "startOffset": 69, "endOffset": 90}, {"referenceID": 14, "context": "Equation (3) can be seen as a likelihood function for a bank of GLMs (McCullagh and Nelder, 1989).", "startOffset": 69, "endOffset": 97}, {"referenceID": 15, "context": "We present a categorical embedding model that corresponds to the continuous bag of words (CBOW) word embedding (Mikolov et al., 2013a).", "startOffset": 111, "endOffset": 134}, {"referenceID": 16, "context": "2 we show that negative sampling (Mikolov et al., 2013b) corresponds to biased stochastic gradients of the B-EMB objective.", "startOffset": 33, "endOffset": 56}, {"referenceID": 0, "context": "Consider the (calcium) expression of a large population of zebrafish neurons (Ahrens et al., 2013).", "startOffset": 77, "endOffset": 98}, {"referenceID": 15, "context": "EF-EMBs are inspired by word embeddings, such as CBOW (Mikolov et al., 2013a).", "startOffset": 54, "endOffset": 77}, {"referenceID": 16, "context": "2 we show that biased stochastic gradients of the B-EMB objective recovers negative sampling (Mikolov et al., 2013b).", "startOffset": 93, "endOffset": 116}, {"referenceID": 6, "context": "We use Adagrad (Duchi et al., 2011) to set the step-size.", "startOffset": 15, "endOffset": 35}, {"referenceID": 16, "context": "This relates to negative sampling, which is used to approximate the skip-gram objective (Mikolov et al., 2013b).", "startOffset": 88, "endOffset": 111}, {"referenceID": 0, "context": "We analyze the neural activity of a larval zebrafish, recorded at single cell resolution for 3000 time frames (Ahrens et al., 2013).", "startOffset": 110, "endOffset": 131}, {"referenceID": 7, "context": "The resulting calcium imaging data is preprocessed by a nonnegative matrix factorization to identify neurons, their locations, and the fluorescence activity x\u2217 t \u2208 R N of the individual neurons over time (Friedrich et al., 2015).", "startOffset": 204, "endOffset": 228}, {"referenceID": 3, "context": "We analyze the IRI dataset3 (Bronnenberg et al., 2008), 3We thank IRI for making the data available.", "startOffset": 28, "endOffset": 54}, {"referenceID": 8, "context": "Table 3: Comparison of predictive log-likelihood between P-EMB, AP-EMB, HPF (Gopalan et al., 2015), and Poisson PCA (Collins et al.", "startOffset": 76, "endOffset": 98}, {"referenceID": 5, "context": ", 2015), and Poisson PCA (Collins et al., 2001) on held out data.", "startOffset": 25, "endOffset": 47}, {"referenceID": 10, "context": "We also analyze the MovieLens-100K dataset (Harper and Konstan, 2015), which contains movie ratings on a scale from 1 to 5.", "startOffset": 43, "endOffset": 69}, {"referenceID": 12, "context": "1, as done by Hu et al. (2008) for matrix factorization.", "startOffset": 14, "endOffset": 31}, {"referenceID": 8, "context": "We compare the predictive performance with HPF (Gopalan et al., 2015) and Poisson PCA (Collins et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 5, "context": ", 2015) and Poisson PCA (Collins et al., 2001).", "startOffset": 24, "endOffset": 46}, {"referenceID": 16, "context": "We showed that continuous bag of words (CBOW) (Mikolov et al., 2013b) is a special case of EF-EMB and we provided examples beyond text: the brain activity of zebrafish, shopping data, and movie ratings.", "startOffset": 46, "endOffset": 69}], "year": 2016, "abstractText": "Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications\u2014neural activity of zebrafish, users\u2019 shopping behavior, and movie ratings\u2014we found exponential family embedding models to be more effective than other types of dimensionality reduction. They better reconstruct held-out data and find interesting qualitative structure.", "creator": "LaTeX with hyperref package"}}}