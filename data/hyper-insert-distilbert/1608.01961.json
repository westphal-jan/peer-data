{"id": "1608.01961", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2016", "title": "De-Conflated Semantic Representations", "abstract": "one major deficiency of most current semantic representation techniques is that they also usually model a weak word type as a single point in compiling the preceding semantic vector space, hence conflating all the overlapping meanings that describe the word form can naturally have. addressing this major issue repeatedly by learning distinct english representations for selecting individual adjective meanings of confusing words has however been becoming the highlighted subject of several research triangle studies established in the past few years. if however, the generated sense representations itself are arguably either not linked to exhibiting any linguistic sense inventory or are dramatically unreliable for infrequent word senses. we propose a modern technique that tackles deeper these problems faced by efficiently de - coded conflating using the representations of unfamiliar words based increasingly on the deep knowledge it now derives from a semantic network. nonetheless our approach provides multiple practical advantages in sharp comparison to the past work, including its high coverage and the ability to effortlessly generate geographically accurate thought representations even for employing infrequent terms word senses. we just carry carefully out evaluations on ~ six individual datasets across two semantic scale similarity modeling tasks and report similar state - of - the -! art results on most steps of examining them.", "histories": [["v1", "Fri, 5 Aug 2016 18:14:19 GMT  (500kb,D)", "http://arxiv.org/abs/1608.01961v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mohammad taher pilehvar", "nigel collier"], "accepted": true, "id": "1608.01961"}, "pdf": {"name": "1608.01961.pdf", "metadata": {"source": "CRF", "title": "De-Conflated Semantic Representations", "authors": ["Mohammad Taher Pilehvar", "Nigel Collier"], "emails": ["mp792@cam.ac.uk", "nhc30@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Modeling the meanings of linguistic items in a machine-interpretable form, i.e., semantic representation, is one of the oldest, yet most active, areas of research in Natural Language Processing (NLP). The field has recently experienced a resurgence of research interest with the new blood injected in its veins by neural network-based models that view the representation task as a language modeling problem and learn dense representations (usually referred to as embeddings) by efficiently processing massive\namounts of texts. However, either in its conventional count-based form (Turney and Pantel, 2010) or the recent predictive approach, the prevailing objective of representing each word type as a single point in the semantic space has a major limitation: it ignores the fact that words can have multiple meanings and conflates all these meanings into a single representation. This objective can have negative impacts on accurate semantic modeling, e.g., semantically unrelated words that are synonymous to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014).\nRecently, there has been a growing interest in addressing the meaning conflation deficiency of word representations. A series of techniques tend to associate a word to multiple points in the semantic space by clustering its contexts in a given text corpus and learning distinct representations for individual clusters (Reisinger and Mooney, 2010; Huang et al., 2012). Though, these techniques usually assume a fixed number of word senses per word type, disregarding the fact that the number of senses of a word can range from one (monosemy) to dozens. Neelakantan et al. (2014) tackled this issue by allowing the number to be dynamically adjusted for each word during training. However, the approach and all the other clustering-based techniques still suffer from the fact that the computed sense representations are not linked to any sense inventory, a linking which would require large amounts of senseannotated data (Agirre et al., 2006). In addition, because of their dependence on knowledge derived from a text corpus, these techniques are generally unable to learn accurate representations for word ar X iv :1 60 8.\n01 96\n1v 1\n[ cs\n.C L\n] 5\nA ug\nsenses that are infrequent in the underlying corpus. Knowledge-based techniques tackle these issues by deriving sense-specific knowledge from external sense inventories, such as WordNet (Fellbaum, 1998), and learning representations that are linked to the sense inventory. These approaches either use sense definitions and employ Word Sense Disambiguation (WSD) to gather sense-specific contexts (Chen et al., 2014; Iacobacci et al., 2015) or take advantage of the properties of WordNet, such as synonymy and direct semantic relations (Rothe and Schu\u0308tze, 2015). However, the non-optimal WSD techniques and the shallow utilization of knowledge from WordNet do not allow these techniques to learn accurate and high-coverage semantic representations for all senses in the inventory.\nWe propose a technique that de-conflates a given word representation into its constituent sense representations by exploiting deep knowledge from the semantic network of WordNet. Our approach provides the following three main advantages in comparison to the past work: (1) our representations are linked to the WordNet sense inventory and, accordingly, the number of senses for a word is a dynamic parameter which matches that defined by WordNet; (2) the deep exploitation of WordNet\u2019s semantic network allows us to obtain accurate semantic representations, even for word senses that are infrequent in generic text corpora; and (3) our methodology involves only minimal parameter tuning and can be effectively applied to any sense inventory that is viewable as a semantic network and to any word representation technique. We evaluate our sense representations in two tasks: word similarity (both incontext and in-isolation) and cross-level semantic similarity. Experimental results show that the proposed technique can provide consistently high performance across six datasets, outperforming the recent state of the art on most of them."}, {"heading": "2 De-Conflated Representations", "text": "Preliminaries. Our proposed approach takes a set of pre-trained word representations and uses the graph structure of a semantic lexical resource in order to de-conflate the representations into those of word senses. Therefore, our approach firstly requires a set of pre-trained word representations (e.g.,\nword embeddings). Any model that maps a given word to a fixed-size vector representation (i.e., vector space model) can be used by our approach. In our experiments, we opted for a set of publicly available word embeddings (cf. \u00a73.1).\nSecondly, we require a lexical resource whose semantic relations allow us to view it as a graph G = (V,E) where each vertex in the set of vertices V corresponds to a concept and edges in E denote lexicosemantic relationships among these vertices. Each concept c \u2208 V is mapped to a set of word senses by a mapping function \u00b5(c) : c\u2192 {s1, . . . , sl}. WordNet, the de facto community standard sense inventory, is a suitable resource that satisfies these properties. WordNet can be readily represented as a semantic graph in which vertices are synsets and edges are the semantic relations that connect these synsets (e.g., hypernymy and meronymy). The mapping function in WordNet maps each synset to the set of synonymous words it contains (i.e., word senses)."}, {"heading": "2.1 Overview of the approach", "text": "Our goal is to compute a semantic representation that places a given word sense in an existing semantic space of words. We achieve this by leveraging word representations as well as the knowledge derived from WordNet. The gist of our approach lies in its computation of a list of sense biasing words for a given word sense. To this end, we first analyze the semantic network of WordNet and extract a list of most representative words that can effectively pinpoint the semantics of individual synsets (\u00a72.2). We then leverage an effective technique which learns semantic representations for individual word senses by placing the senses in the proximity of their corresponding sense biasing words (\u00a72.3)."}, {"heading": "2.2 Determining sense biasing words", "text": "Algorithm 1 shows the procedure we use to extract from WordNet a list of sense biasing words for a given target synset yt. The algorithm receives as its inputs the semantic graph of WordNet and the mapping function \u00b5(\u00b7), and outputs an ordered list of biasing words Bt for yt. The list comprises the most semantically-related words to synset yt which can best represent and pinpoint its meaning. We leverage a graph-based algorithm for the computation of the sense biasing words.\nAlgorithm 1 Get sense biasing words for synset yt Require: Graph G = (V,E) of vertices V = {yi}mi=1\n(of m synsets) and edges E (semantic relationships between synsets) Require: Function \u00b5(yi) that returns for a given synset yi the words it contains Require: Target synset yt \u2208 V for which a sense biasing word sequence is required Ensure: The sequence Bt of sense biasing words for synset yt\n1: Bt\u2190 () 2: for all word w in \u00b5(yt) do 3: Bt\u2190 Bt \u222a (w) 4: for yi \u2208 V : yi 6= yt do 5: pi\u2190 PERSONALIZEDPAGERANK(yi, yt, G) 6: (y\u2217h) m\u22121 h=1 \u2190 SORT(V \\{yt}) according to scores pi 7: for h : 1 to m\u2212 1 do 8: for all word w in \u00b5(y\u2217h) do 9: if w /\u2208 Bt then\n10: Bt \u2190 Bt \u222a (w) 11: return sequence Bt\nSpecifically, we use the Personalized PageRank (Haveliwala, 2002, PPR) algorithm which has been extensively used by several NLP applications (Yeh et al., 2009; Niemann and Gurevych, 2011; Agirre et al., 2014). To this end, we first represent the semantic network of WordNet as a row-stochastic transition matrix M \u2208 Rm\u00d7m wherem is the number of synsets in WordNet (|V |). The cell Mij of M is set to the inverse of the degree of i if there is a semantic relationship between synsets i and j and to zero otherwise. We compute the PPR distribution for a target synset yt by using the power iteration method Pt+1 = (1\u2212 \u03c3)P0 + \u03c3MPt, where \u03c3 is the damping factor (usually set to 0.85) and P0 is a one-hot initialization vector with the corresponding dimension of yt being set to 1.0. The weight pi in line 5 is the value of the ith dimension of the PPR vector P computed for the synset yt. This weight can be seen as the importance of the corresponding synset of the ith dimension (i.e., yi) to yt. When applied to a semantic network, such as the WordNet graph, this importance can be interpreted as semantic relevance. Hence, the value of pi denotes the extent of semantic relatedness between yi and yt. We use this notion and retrieve a list of most semantically-related words to yt. To achieve this, we sort the synsets {y\u2217 \u2208 V : y\u2217 6= yt} according to their PPR values\n{pi}m\u22121i=1 (line 6). We then iterate (lines 7-10) the sorted list (y\u2217) and for each synset y\u2217h append the list Bt with all the words in y\u2217h (i.e., \u00b5(y\u2217h)). However, in order to ensure that the words in the target synset yt appear as the most representative words in Bt, we first assign these words to the list (line 3). Finally, the algorithm returns the ordered list Bt of sense biasing words for the target synset yt.\nTable 1 shows a sample of top biasing words extracted for the two senses of the noun digit: the numerical and the anatomical senses.1 We explain in \u00a72.3 how we use the sense biasing lists to learn sense-specific representations. Note that the size of the list is equal to the total number of strings in WordNet. However, we observed that taking a very small portion of the top-ranking elements in the lists is enough to generate representations that perform very similarly to those generated when using the full-sized lists (please see \u00a73.1)."}, {"heading": "2.3 Learning sense representations", "text": "Let V be the set of pre-trained d-dimensional word representations. Our objective here is to compute a set V\u2217 = {v\u2217s1 , . . . , v \u2217 sn} of representations for n word senses {s1, . . . , sn} in the same d-dimensional semantic space of words. We achieve this for each sense si by de-conflating the representation vsi of its corresponding lemma and biasing it towards the representations of the words in Bi. Specifically, we obtain a representation v\u2217si for a word sense si by solving:\nargmin v\u2217si \u03b1 d(v\u2217si , vsi) + \u2211 bij\u2208Bi \u03b4ij d(v \u2217 si , vbij ) (1)\nwhere vsi and vbij are the respective word representations (\u2208 V) of the lemma of si and the jth biasing word in the list of biasing words for si,\n1The first and third senses of the noun digit in WordNet 3.0.\ni.e, Bi. The distance d(v, v\u2032) between vectors v and v\u2032 is measured by squared Euclidean distance \u2016v \u2212 v\u2032\u20162= \u2211 k(vk \u2212 v\u2032k)2. The first term in Formula 1 requires the representation of the word sense si (i.e., v\u2217si) to be similar to that of its corresponding lemma, i.e., vsi , whereas the second term encourages v\u2217si to be in the proximity of its biasing words in the semantic space. The above criterion is similar to the frameworks of Das and Smith (2011) and Faruqui et al. (2015) which, though being convex, is usually solved for efficiency reasons by an iterative method proposed by Bengio et al. (2007). Following these works, we obtain the below equation for computing the representation of a word sense si:\nv\u2217si = \u03b1vsi +\n\u2211 bij\u2208Bi \u03b4ijvbij\n\u03b1+ \u2211 j \u03b4ij . (2)\nWe define \u03b4ij as e \u2212\u03bbr(i,j)\n|Bi| where r(i, j) denotes the rank of the word bij in the list Bi. This is essentially an exponential decay function that gives more importance to the top-ranking biasing words for si. The hyperparameter \u03b1 denotes the extent to which v\u2217si is kept close to its corresponding lemma representation vsi . Following Faruqui et al. (2015), we set \u03b1 to 1. The only parameter to be tuned in our experiments is \u03bb. We discuss the tuning of this parameter in \u00a73.1. The representation of a synset yi can be accordingly calculated as the centroid of the vectors of its associated word senses, i.e.,\n{ vyi \u2016vyi\u2016\n: vyi = \u2211\ns\u2208\u00b5(yi)\nv\u0302\u2217s , v\u0302 \u2217 s = v\u2217s \u2016v\u2217s\u2016 }. (3)\nAs a result of this procedure, we obtain the set V\u2217 of n sense representations in the same semantic space as word representations in V . In fact, we now have a unified semantic space which enables a direct\ncomparison of the two types of linguistic items. In \u00a73.3 we evaluate our approach in the word to sense similarity measurement framework.\nWe show in Table 2 the closest words to the word bass and two of its senses, music and fish,2 in our unified semantic space. We can see in row #1 a mixture of both meanings when the word representation is used whereas the closest words to the senses (rows #2 and #3) are mostly in-domain and specific to the corresponding sense.\nTo exhibit another interesting property of our sense representation approach, we depict in Figure 1 the word digit and its numerical and anatomical senses (from the example in Table 1) in a 2-d semantic space, along with a sample set of words in their proximity.3 We can see that the word digit is placed in the semantic space in the neighbourhood of words from the numerical domain (lower left of the figure), mainly due the dominance (Sanderson and Van Rijsbergen, 1999) of this sense in the general-domain corpus on which the word embeddings in our experiments were trained (cf. \u00a73.1). However, upon de-conflation, the emerging anatomical sense of the\n2The first and fourth senses in WordNet 3.0, respectively defined as \u201cthe lowest part of the musical range\u201d and \u201cthe lean flesh of a saltwater fish of the family Serranidae.\u201d\n3We used the t-SNE algorithm (van der Maaten and Hinton, 2008) for dimensionality reduction.\nword is shifted towards the region in the semantic space which is occupied by anatomical words (upper right of the figure). A clustering-based sense representation technique would have failed in accurately representing the infrequent anatomical meaning of digit by analyzing a general domain corpus (such as the one used here). But our sense representation technique, thanks to its proper usage of knowledge from a sense inventory, is effective in unveiling and accurately modeling less frequent or domain-specific senses of a given word.\nPlease note that any vector space model representation technique can be used for the pre-training of word representations in V . Also, the list of sense biasing words can be obtained for larger sense inventories, such as FreeBase (Bollacker et al., 2008) or BabelNet (Navigli and Ponzetto, 2012). We leave the exploration of other ways of computing sense biasing words to the future work."}, {"heading": "3 Experiments", "text": "We benchmarked our sense representation approach against several recent techniques on two standard tasks: word similarity (\u00a73.2), for which we evaluate on both in-isolation and in-context similarity datasets, and cross-level semantic similarity (\u00a73.3)."}, {"heading": "3.1 Experimental setup", "text": "Pre-trained word representations. As our word representations, we used the 300-d Word2vec (Mikolov et al., 2013) word embeddings trained on the Google News dataset4 mainly for their popularity across different NLP applications. However, our approach is equally applicable to any count-based representation technique (Baroni and Lenci, 2010; Turney and Pantel, 2010) or any other embedding approach (Pennington et al., 2014; LeCun et al., 2015). We leave the evaluation and comparison of various word representation techniques with different training approaches, objectives, and dimensionalities to the future work.\nParameter tuning. Recall from \u00a72.3 that our procedure for learning sense representations needs only one parameter to be tuned, i.e., \u03bb. We did not perform an extensive tuning on the value of this parameter and set its value to 1/5 after trying four differ-\n4https://code.google.com/archive/p/word2vec/\nent values (1, 1/2, 1/5, and 1/10) on a small validation dataset. We leave the more systematic tuning of the parameter and the choice of alternative decay functions (cf. \u00a72.3) to the future work.\nThe size of the sense biasing words lists. Also recall from \u00a72.2 that the extracted lists of sense biasing words were originally as large as the total number of unique strings in WordNet (around 150K in ver. 3.0). But, given that we use an exponential decay function in our learning algorithm (cf. \u00a72.3), the impact of the low-ranking words in the list is negligible. In fact, we observed that taking a very small portion of the top-ranking words, i.e., the top 25, produces similarity scores that are on par with those generated when the full lists were considered. Therefore, we experimented with the down-sized lists which enabled us to generate very quickly sense representations for all word senses in WordNet."}, {"heading": "3.2 Word similarity", "text": "Comparison systems. We compared our results against nine other sense representation techniques: the WordNet-based approaches of Pilehvar and Navigli (2015), Chen et al. (2014), Rothe and Schu\u0308tze (2015), Jauhar et al. (2015), and Iacobacci et al. (2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al. (2014), and Liu et al. (2015) (please see \u00a74 for more details). We also compared against the approach of Faruqui et al. (2015) which uses knowledge derived from WordNet for improving word representations. From the different configurations presented in (Faruqui et al., 2015) we chose the system that uses GloVe (Pennington et al., 2014) with all WordNet relations which is their best performing monolingual system. As for the approach of Jauhar et al. (2015), we show the results of the EM+RETERO system which performs most consistently across different datasets.\nBenchmarks. As our word similarity benchmark, we considered five datasets: RG-65 (Rubenstein and Goodenough, 1965), YP-130 (Yang and Powers, 2005), MEN-3K (Bruni et al., 2014), SimLex-999 (Hill et al., 2015, SL-999), and Stanford Contextual Word Similarity (Huang et al., 2012, SCWS). The latter benchmark provides for each word a context that triggers a specific meaning of the word, making\nit very suitable for the evaluation of sense representation techniques. For each of the datasets, we list the results that are reported by any of our comparison systems.\nSimilarity measurement. For the SCWS dataset, we follow the past works (Reisinger and Mooney, 2010; Huang et al., 2012) and report the results according to two system configurations: (1) AvgSim: where the similarity between two words is computed as the average of all the pairwise similarities between their senses, and (2) AvgSimC: where each pairwise sense similarity is weighted by the rele-\nvance of each sense to its corresponding context. For all the other datasets, since words are not provided with any context (they are in isolation), we measure the similarity between two words as that between their most similar senses. In all the experiments, we use the cosine distance as our similarity measure."}, {"heading": "3.2.1 Experimental results", "text": "Tables 4 and 3 show the results of our system, DECONF, and the comparison systems on the SCWS and the other four similarity datasets, respectively. In both tables we also report the word vectors baseline, whenever they are available, which is computed by directly comparing the corresponding word representations of the two words (\u2208 V). Please note that the word-based baseline does not apply to the approach of Pilehvar and Navigli (2015) as it is purely based on the semantic network of WordNet and does not use any pre-trained word embeddings.\nWe can see from the tables that our sense representations obtain considerable improvements over those of words across the five datasets. This highlights the fact that the de-conflation of word representations into those of their individual meanings has been highly beneficial. On the SCWS dataset, DECONF outperforms all the recent state-of-the-art sense representation techniques (in their best settings) which proves the effectiveness of our approach in capturing the semantics of specific mean-\nings of the words. The improvement is consistent across both system configurations (i.e., AvgSim and AvgSimC). Moreover, the state-of-the-art WordNetbased approach of Rothe and Schu\u0308tze (2015) uses the same initial word vectors as DECONF does (cf. \u00a73.1). Hence, the improvement we obtain indicates that our approach has made better use of the sensespecific knowledge encoded in WordNet.\nAs seen in Table 3 our approach shows competitve performance on the other four datasets. The YP-130 dataset focuses on verb similarity, whereas SimLex999 contains verbs and adjectives and MEN-3K has word pairs with different parts of speech (e.g., a noun compared to a verb). The results we obtain on these datasets exhibit the reliability of our approach in modeling non-nominal word senses."}, {"heading": "3.2.2 Discussion", "text": "The similarity scale of the SimLex-999 dataset is different from our other word similarity benchmarks in that it assigns relatively low scores to antonymous pairs. For instance, sunset-sunrise and man-woman in this dataset are assigned the respective similarities of 2.47 and 3.33 (in a [0, 10] similarity scale) which is in the same range as the similarity between word pairs with slight domain relatedness, such as head-nail (2.47), air-molecule (3.05), or succeedtry (3.98). In fact we observed that tweaking the similarity scale of our system in a way that it diminishes the similarity scores between antonyms can result in significant performance improvement on this dataset. To this end, we performed an experiment in which the similarity of a word pair was simply divided by five whenever the two words belonged to synsets that were linked by the antonymy relation. We observed that the performance on the SimLex999 dataset increased to 61.1 (from 54.2) and 59.0 (from 51.7) according to Pearson (r \u00d7 100) and Spearman (\u03c1\u00d7 100) correlation scores, respectively."}, {"heading": "3.3 Cross-Level semantic similarity", "text": "In addition to the word similarity benchmark, we evaluated the performance of our representations in the cross-level semantic similarity measurement framework. To this end, we opted for the SemEval2014 task on Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). The word to sense similarity subtask of this task, with 500 instances in its test\nset, provides a suitable benchmark for the evaluation of sense representation techniques.\nFor a word sense s and a word w, we compute the similarity score according to four different strategies: the similarity of s to the most similar sense of w (MaxSim), the average similarity of s to individual senses of w (AvgSim), the direct similarity of s to w when the latter is modeled as its word representation (Sense-to-Word or S2W) or as the centroid of its senses\u2019 representations (Sense to aggregated word senses or S2A). For this task, we can only compare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Schu\u0308tze (2015), Pilehvar and Navigli (2015) and Chen et al. (2014) which are linked to the WordNet sense inventory."}, {"heading": "3.3.1 Experimental results", "text": "Table 5 shows the results on the word to sense dataset of the SemEval-2014 CLSS task, according to Pearson (r) and Spearman (\u03c1) correlations and for the four strategies. As can be seen from the low overall performances, the task is a very challenging benchmark with many WordNet out-of-vocabulary or slang terms and rare usages. Despite this, DECONF provides consistent improvement over the comparison sense representation techniques according to both measures and for all the strategies.\nAcross the four strategies, S2A proves to be the most effective for DECONF and the representations of Rothe and Schu\u0308tze (2015). The representations of Chen et al. (2014) perform best with the S2W strategy whereas those of Iacobacci et al. (2015) do not show a consistent trend with relatively low performance across the four strategies. Also, a comparison of our results across the S2W and S2A strategies reveals that a word\u2019s aggregated representation, i.e., the centroid of the representations of its senses, is more accurate than its original word representation.\nOur analysis showed that the performances of the approaches of Rothe and Schu\u0308tze (2015) and Iacobacci et al. (2015) were hampered partly due to their limited coverage. In fact, the former was unable to model around 35% of the synsets in WordNet 1.7.1, mainly for its shallow exploitation of knowledge from WordNet, whereas the latter approach did not cover around 15% of synsets in WordNet 3.0. Chen et al. (2014) provide near-full coverage for\nword senses in WordNet. However, the relatively low performance of their system shows that the usage of glosses in WordNet and the automated disambiguation have not resulted in accurate sense representations. Thanks to its deep exploitation of the underlying resource, our approach provides full coverage over all word senses and synsets in WordNet.\nThe three best-performing systems in the task are Meerkat Mafia (Kashyap et al., 2014) (r = 37.5, \u03c1 = 39.3), SimCompass (Banea et al., 2014) (r = 35.4, \u03c1 = 34.9), and SemantiKLUE (Proisl et al., 2014) (r = 17.9, \u03c1 = 18.8). Please note that these systems are specifically designed for the cross-level similarity measurement task. For instance, the bestranking system in the task leverages a compilation of several dictionaries, including The American Heritage Dictionary, Wiktionary and WordNet, in order to handle slang terms and rare usages, which leads to its competitive performance (Kashyap et al., 2014)."}, {"heading": "4 Related Work", "text": "Learning semantic representations for individual senses of words has been an active area of research for the past few years. Based on the way they view the problem, the recent techniques can be classified into two main branches: (1) those that, similarly to our work, extract knowledge from external sense inventories for learning sense representations; and (2) those techniques that cluster the contexts in which a word appears in a given text corpus and learn distinct representations for individual clusters.\nExamples for the first branch include the ap-\nproaches of Chen et al. (2014), Jauhar et al. (2015) and Rothe and Schu\u0308tze (2015), all of which use WordNet as an external resource and obtain sense representations for this sense inventory. Chen et al. (2014) uses the content words in the definition of a word sense and WSD. However, the sole usage of glosses as sense-distinguishing contexts and the non-optimal WSD make the approach inaccurate, particularly for highly polysemous words with similar senses and for word senses with short definitions. Similarly, Rothe and Schu\u0308tze (2015) use only polysemy and synonymy properties of words in WordNet along with a small set of semantic relations. This significantly hampers the reliability of the technique in providing high coverage (discussed further in \u00a73.3.1). Our approach improves over these works by exploiting deep knowledge from the semantic network of WordNet, coupled with an effective training approach. ADW (Pilehvar and Navigli, 2015) is another WordNet-based approach which exploits only the semantic network of this resource an obtains interpretable sense representations. Other work in this branch include SensEmbed (Iacobacci et al., 2015) and Nasari (Camacho-Collados et al., 2015; Camacho-Collados et al., 2016) which are based on the BabelNet sense inventory (Navigli and Ponzetto, 2012). The former technique first disambiguates words in a given corpus with the help of a knowledge-based WSD system and then uses the generated sense-annotated corpus as training data for Word2vec. Nasari combines structural knowledge from the semantic network of BabelNet\nwith corpus statistics derived from Wikipedia for representing BabelNet synsets. However, the approach falls short of modeling non-nominal senses as Wikipedia, due to its very encyclopedic nature, does not cover verbs, adjectives, or adverbs.\nThe second branch, which is usually referred to as multi-prototype representation, is often associated with clustering. Reisinger and Mooney (2010) proposed one of the recent pioneering techniques in this branch. Other prominent work in the category include topical word embeddings (Liu et al., 2015) which use latent topic models for assigning topics to each word in a corpus and learn topicspecific word representations, and the technique proposed by Huang et al. (2012) which incorporates \u201cglobal document context.\u201d Tian et al. (2014) modified the Skip-gram model in order to learn multiple embeddings for each word type. Despite the fact that these techniques do not usually take advantage of the knowledge encoded in structured knowledge resource, they generally suffer from two disadvantages. The first limitation is that they usually make an assumption that a given word has a fixed number of senses, ignoring the fact that polysemy is highly dynamic across words that can range from monosemous to highly ambiguous with dozens of associated meanings (McCarthy et al., 2016). Neelakantan et al. (2014) tackled this issue by estimating the number of senses for a word type during the learning process. However, all techniques in the second branch suffer from another disadvantage that their computed sense representations are not linked to any sense inventory, a linking which itself would require the existence of high coverage sense-annotated data (Agirre et al., 2006).\nAnother notable line of research incorporates knowledge from external resources, such as PPDB (Ganitkevitch et al., 2013) and WordNet, to improve word embeddings (Yu and Dredze, 2014; Faruqui et al., 2015). Neither of the two techniques however provide representations for word senses."}, {"heading": "5 Conclusions", "text": "We put forward a sense representation technique, namely DECONF, that provides multiple advantages in comparison to the recent state of the art: (1) the number of word senses in our technique is flexi-\nble and the computed representations are linked to word senses in WordNet; (2) DECONF is effective in providing accurate representation of word senses, even for those senses that do not usually appear frequently in generic text corpora; and (3) our approach is general in that it can be readily applied to any set of word representations and any semantic network without the need for extensive parameter tuning. Our experimental results showed that DECONF can outperform recent state of the art on several datasets across two tasks. We release our computed representations for around 118K synsets and 205K word senses in WordNet 3.0 at https: //github.com/pilehvar/deconf. As future work, we plan to investigate the possibility of using larger semantic networks, such as FreeBase and BabelNet, which would also allow us to apply the technique to languages other than English. We also plan to evaluate the performance of our approach with other decay functions as well as with other initial word representations."}, {"heading": "Acknowledgments", "text": "The authors gratefully acknowledge the support of the MRC grant No. MR/M025160/1 for PheneBank."}], "references": [{"title": "Evaluating and optimizing the parameters of an unsupervised graph-based wsd algorithm", "author": ["Agirre et al.2006] Eneko Agirre", "David Mart\u0131\u0301nez", "Oier L\u00f3pez de Lacalle", "Aitor Soroa"], "venue": "In Proceedings of the First Workshop on Graph Based Methods for", "citeRegEx": "Agirre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2006}, {"title": "Random walks for knowledgebased Word Sense Disambiguation", "author": ["Agirre et al.2014] Eneko Agirre", "Oier L\u00f3pez de Lacalle", "Aitor Soroa"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Simcompass: Using deep learning word embeddings to assess cross-level similarity", "author": ["Banea et al.2014] Carmen Banea", "Di Chen", "Rada Mihalcea", "Claire Cardie", "Janyce Wiebe"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Banea et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Semi-Supervised Learning. MIT Press. chapter Label Propagation and Quadratic Criterion", "author": ["Bengio et al.2007] Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD Interna-", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "NASARI: a Novel Approach to a SemanticallyAware Representation of Items", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Camacho.Collados et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "NASARI: Integrating explicit knowledge and corpus statistics for amultilingual representation of concepts and entities", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "Artificial Intelligence", "citeRegEx": "Camacho.Collados et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2016}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["Das", "Smith2011] Dipanjan Das", "Noah A. Smith"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Associa-", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "PPDB: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Sensembed: Learning sense embeddings for word and relational similarity", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Iacobacci et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Ontologically grounded multisense representation learning for semantic vector space models", "author": ["Chris Dyer", "Eduard Hovy"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Associa-", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "SemEval-2014 task 3: Cross-level semantic similarity", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Jurgens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2014}, {"title": "Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity systems", "author": ["Lushan Han", "Roberto Yus", "Jennifer Sleeman", "Taneeya W. Satyapanich", "Sunil R Gandhi", "Tim Finin"], "venue": null, "citeRegEx": "Kashyap et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kashyap et al\\.", "year": 2014}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Word sense clustering and clusterability", "author": ["Marianna Apidianaki", "Katrin Erk"], "venue": "Computational Linguistics,", "citeRegEx": "McCarthy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["Navigli", "Ponzetto2012] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "Artificial Intelligence,", "citeRegEx": "Navigli et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2012}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "The people\u2019s Web meets linguistic knowledge: Automatic sense alignment of Wikipedia and Wordnet", "author": ["Niemann", "Gurevych2011] Elisabeth Niemann", "Iryna Gurevych"], "venue": "In Proceedings of the Ninth International Conference on Computational Seman-", "citeRegEx": "Niemann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niemann et al\\.", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "From senses to texts: An all-in-one graph-based approach for measuring semantic similarity", "author": ["Pilehvar", "Roberto Navigli"], "venue": "Artificial Intelligence,", "citeRegEx": "Pilehvar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pilehvar et al\\.", "year": 2015}, {"title": "Semantiklue: Robust semantic similarity at multiple levels using maximum weight matching", "author": ["Proisl et al.2014] Thomas Proisl", "Stefan Evert", "Paul Greiner", "Besim Kabashi"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (Se-", "citeRegEx": "Proisl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Proisl et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "AutoExtend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "The impact on", "author": ["Sanderson", "C.J. Van Rijsbergen"], "venue": null, "citeRegEx": "Sanderson et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sanderson et al\\.", "year": 1999}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["Tian et al.2014] Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["van der Maaten", "Hinton2008] L.J.P van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Measuring semantic similarity in the taxonomy of wordnet", "author": ["Yang", "Powers2005] Dongqiang Yang", "David M.W. Powers"], "venue": "In Proceedings of the Twenty-eighth Australasian Conference on Computer Science,", "citeRegEx": "Yang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2005}, {"title": "WikiWalk: Random walks on Wikipedia for semantic relatedness", "author": ["Yeh et al.2009] Eric Yeh", "Daniel Ramage", "Christopher D. Manning", "Eneko Agirre", "Aitor Soroa"], "venue": "In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language", "citeRegEx": "Yeh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yeh et al\\.", "year": 2009}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": ", semantically unrelated words that are synonymous to different senses of a word are pulled towards each other in the semantic space (Neelakantan et al., 2014).", "startOffset": 133, "endOffset": 159}, {"referenceID": 14, "context": "sociate a word to multiple points in the semantic space by clustering its contexts in a given text corpus and learning distinct representations for individual clusters (Reisinger and Mooney, 2010; Huang et al., 2012).", "startOffset": 168, "endOffset": 216}, {"referenceID": 23, "context": "Neelakantan et al. (2014) tackled this issue by allowing the number to be dynamically adjusted for each word during training.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "all the other clustering-based techniques still suffer from the fact that the computed sense representations are not linked to any sense inventory, a linking which would require large amounts of senseannotated data (Agirre et al., 2006).", "startOffset": 215, "endOffset": 236}, {"referenceID": 9, "context": "biguation (WSD) to gather sense-specific contexts (Chen et al., 2014; Iacobacci et al., 2015) or take advantage of the properties of WordNet, such as synonymy and direct semantic relations (Rothe and Sch\u00fctze, 2015).", "startOffset": 50, "endOffset": 93}, {"referenceID": 15, "context": "biguation (WSD) to gather sense-specific contexts (Chen et al., 2014; Iacobacci et al., 2015) or take advantage of the properties of WordNet, such as synonymy and direct semantic relations (Rothe and Sch\u00fctze, 2015).", "startOffset": 50, "endOffset": 93}, {"referenceID": 11, "context": "The above criterion is similar to the frameworks of Das and Smith (2011) and Faruqui et al. (2015) which, though being convex, is", "startOffset": 77, "endOffset": 99}, {"referenceID": 4, "context": "usually solved for efficiency reasons by an iterative method proposed by Bengio et al. (2007). Following these works, we obtain the below equation for computing the representation of a word sense si:", "startOffset": 73, "endOffset": 94}, {"referenceID": 11, "context": "Following Faruqui et al. (2015), we set \u03b1 to 1.", "startOffset": 10, "endOffset": 32}, {"referenceID": 5, "context": "biasing words can be obtained for larger sense inventories, such as FreeBase (Bollacker et al., 2008) or BabelNet (Navigli and Ponzetto, 2012).", "startOffset": 77, "endOffset": 101}, {"referenceID": 21, "context": "As our word representations, we used the 300-d Word2vec (Mikolov et al., 2013) word embeddings trained on the Google News dataset4 mainly for their popularity across different NLP applications.", "startOffset": 56, "endOffset": 78}, {"referenceID": 25, "context": "However, our approach is equally applicable to any count-based representation technique (Baroni and Lenci, 2010; Turney and Pantel, 2010) or any other embedding approach (Pennington et al., 2014; LeCun et al., 2015).", "startOffset": 170, "endOffset": 215}, {"referenceID": 9, "context": "We compared our results against nine other sense representation techniques: the WordNet-based approaches of Pilehvar and Navigli (2015), Chen et al. (2014), Rothe and Sch\u00fctze", "startOffset": 137, "endOffset": 156}, {"referenceID": 14, "context": "(2015), Jauhar et al. (2015), and Iacobacci et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 14, "context": "(2015), and Iacobacci et al. (2015) and the clustering-based approaches of Huang et al.", "startOffset": 12, "endOffset": 36}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al.", "startOffset": 46, "endOffset": 86}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al. (2014), and Liu et al.", "startOffset": 46, "endOffset": 113}, {"referenceID": 14, "context": "(2015) and the clustering-based approaches of Huang et al. (2012), Tian et al. (2014), Neelakantan et al. (2014), and Liu et al. (2015) (please see \u00a74 for more details).", "startOffset": 46, "endOffset": 136}, {"referenceID": 11, "context": "From the different configurations presented in (Faruqui et al., 2015) we chose the system that uses GloVe (Pennington et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 25, "context": ", 2015) we chose the system that uses GloVe (Pennington et al., 2014) with all WordNet relations which is their best performing monolingual system.", "startOffset": 44, "endOffset": 69}, {"referenceID": 11, "context": "approach of Faruqui et al. (2015) which uses knowledge derived from WordNet for improving word representations.", "startOffset": 12, "endOffset": 34}, {"referenceID": 11, "context": "approach of Faruqui et al. (2015) which uses knowledge derived from WordNet for improving word representations. From the different configurations presented in (Faruqui et al., 2015) we chose the system that uses GloVe (Pennington et al., 2014) with all WordNet relations which is their best performing monolingual system. As for the approach of Jauhar et al. (2015), we show the results of the EM+RETERO system which performs most consistently across different datasets.", "startOffset": 12, "endOffset": 366}, {"referenceID": 6, "context": "As our word similarity benchmark, we considered five datasets: RG-65 (Rubenstein and Goodenough, 1965), YP-130 (Yang and Powers, 2005), MEN-3K (Bruni et al., 2014), SimLex-999 (Hill et al.", "startOffset": 143, "endOffset": 163}, {"referenceID": 14, "context": "MEN-3K Iacobacci et al. (2015) \u2212 80.", "startOffset": 7, "endOffset": 31}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 75.", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 75.9 \u2212 73.7 Pilehvar and Navigli (2015) 61.", "startOffset": 2, "endOffset": 66}, {"referenceID": 14, "context": "1 Iacobacci et al. (2015) \u2212 87.", "startOffset": 2, "endOffset": 26}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 84.", "startOffset": 2, "endOffset": 24}, {"referenceID": 11, "context": "2 Faruqui et al. (2015) \u2212 84.2 \u2212 76.7 Pilehvar and Navigli (2015) 80.", "startOffset": 2, "endOffset": 66}, {"referenceID": 15, "context": "9 Iacobacci et al. (2015) \u2212 63.", "startOffset": 2, "endOffset": 26}, {"referenceID": 18, "context": "8 Neelakantan et al. (2014) (best) 67.", "startOffset": 2, "endOffset": 28}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.", "startOffset": 2, "endOffset": 49}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.", "startOffset": 2, "endOffset": 83}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.8 65.7 Tian et al. (2014) (best) \u2212 65.", "startOffset": 2, "endOffset": 112}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.8 65.7 Tian et al. (2014) (best) \u2212 65.7 Iacobacci et al. (2015) 62.", "startOffset": 2, "endOffset": 150}, {"referenceID": 9, "context": "3 Chen et al. (2014) 66.2 68.9 Liu et al. (2015) (best) \u2212 68.1 Huang et al. (2012) 62.8 65.7 Tian et al. (2014) (best) \u2212 65.7 Iacobacci et al. (2015) 62.4 \u2212 Jauhar et al. (2015) \u2212 58.", "startOffset": 2, "endOffset": 178}, {"referenceID": 14, "context": "For the SCWS dataset, we follow the past works (Reisinger and Mooney, 2010; Huang et al., 2012) and report the results according to two system configurations: (1) AvgSim: where the similarity between two words is computed as the average of all the pairwise similarities between their senses, and (2) AvgSimC: where each pairwise sense similarity is weighted by the relevance of each sense to its corresponding context.", "startOffset": 47, "endOffset": 95}, {"referenceID": 14, "context": "pare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 14, "context": "pare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al.", "startOffset": 61, "endOffset": 111}, {"referenceID": 14, "context": "pare against the publicly-available sense representations of Iacobacci et al. (2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al.", "startOffset": 61, "endOffset": 140}, {"referenceID": 9, "context": "(2015), Rothe and Sch\u00fctze (2015), Pilehvar and Navigli (2015) and Chen et al. (2014) which are linked to the WordNet sense inventory.", "startOffset": 66, "endOffset": 85}, {"referenceID": 9, "context": "The representations of Chen et al. (2014) perform best with the S2W strategy whereas those of Iacobacci et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "The representations of Chen et al. (2014) perform best with the S2W strategy whereas those of Iacobacci et al. (2015) do not show a consistent trend with relatively low performance across the four strategies.", "startOffset": 23, "endOffset": 118}, {"referenceID": 14, "context": "Our analysis showed that the performances of the approaches of Rothe and Sch\u00fctze (2015) and Iacobacci et al. (2015) were hampered partly due to their limited coverage.", "startOffset": 92, "endOffset": 116}, {"referenceID": 9, "context": "Chen et al. (2014) provide near-full coverage for", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "9 Iacobacci et al. (2015)\u2217 19.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "1 Chen et al. (2014)\u2217 17.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "0 \u2212 \u2212 \u2212 \u2212 Iacobacci et al. (2015) 19.", "startOffset": 10, "endOffset": 34}, {"referenceID": 9, "context": "and Sch\u00fctze (2015) and Chen et al. (2014) that use older versions of WordNet (1.", "startOffset": 23, "endOffset": 42}, {"referenceID": 18, "context": "The three best-performing systems in the task are Meerkat Mafia (Kashyap et al., 2014) (r = 37.", "startOffset": 64, "endOffset": 86}, {"referenceID": 2, "context": "3), SimCompass (Banea et al., 2014) (r = 35.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": "itage Dictionary, Wiktionary and WordNet, in order to handle slang terms and rare usages, which leads to its competitive performance (Kashyap et al., 2014).", "startOffset": 133, "endOffset": 155}, {"referenceID": 9, "context": "Examples for the first branch include the approaches of Chen et al. (2014), Jauhar et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 9, "context": "Examples for the first branch include the approaches of Chen et al. (2014), Jauhar et al. (2015) and Rothe and Sch\u00fctze (2015), all of which use WordNet as an external resource and obtain sense representations for this sense inventory.", "startOffset": 56, "endOffset": 97}, {"referenceID": 9, "context": "Examples for the first branch include the approaches of Chen et al. (2014), Jauhar et al. (2015) and Rothe and Sch\u00fctze (2015), all of which use WordNet as an external resource and obtain sense representations for this sense inventory.", "startOffset": 56, "endOffset": 126}, {"referenceID": 15, "context": "Other work in this branch include SensEmbed (Iacobacci et al., 2015) and Nasari (Camacho-Collados et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 7, "context": ", 2015) and Nasari (Camacho-Collados et al., 2015; Camacho-Collados et al., 2016) which are based on the BabelNet sense inventory (Navigli and Ponzetto, 2012).", "startOffset": 19, "endOffset": 81}, {"referenceID": 8, "context": ", 2015) and Nasari (Camacho-Collados et al., 2015; Camacho-Collados et al., 2016) which are based on the BabelNet sense inventory (Navigli and Ponzetto, 2012).", "startOffset": 19, "endOffset": 81}, {"referenceID": 19, "context": "Other prominent work in the category include topical word embeddings (Liu et al., 2015) which use latent topic models for assigning topics to each word in a corpus and learn topic-", "startOffset": 69, "endOffset": 87}, {"referenceID": 14, "context": "specific word representations, and the technique proposed by Huang et al. (2012) which incorporates \u201cglobal document context.", "startOffset": 61, "endOffset": 81}, {"referenceID": 14, "context": "specific word representations, and the technique proposed by Huang et al. (2012) which incorporates \u201cglobal document context.\u201d Tian et al. (2014) modified the Skip-gram model in order to learn multiple embeddings for each word type.", "startOffset": 61, "endOffset": 146}, {"referenceID": 20, "context": "of senses, ignoring the fact that polysemy is highly dynamic across words that can range from monosemous to highly ambiguous with dozens of associated meanings (McCarthy et al., 2016).", "startOffset": 160, "endOffset": 183}, {"referenceID": 20, "context": "of senses, ignoring the fact that polysemy is highly dynamic across words that can range from monosemous to highly ambiguous with dozens of associated meanings (McCarthy et al., 2016). Neelakantan et al. (2014) tackled this issue by estimating the", "startOffset": 161, "endOffset": 211}, {"referenceID": 0, "context": "the existence of high coverage sense-annotated data (Agirre et al., 2006).", "startOffset": 52, "endOffset": 73}, {"referenceID": 12, "context": "Another notable line of research incorporates knowledge from external resources, such as PPDB (Ganitkevitch et al., 2013) and WordNet, to improve word embeddings (Yu and Dredze, 2014; Faruqui et al.", "startOffset": 94, "endOffset": 121}, {"referenceID": 11, "context": ", 2013) and WordNet, to improve word embeddings (Yu and Dredze, 2014; Faruqui et al., 2015).", "startOffset": 48, "endOffset": 91}], "year": 2016, "abstractText": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.", "creator": "LaTeX with hyperref package"}}}