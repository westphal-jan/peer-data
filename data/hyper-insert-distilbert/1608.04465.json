{"id": "1608.04465", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees", "abstract": "remarkably efficient methods for storing and using querying routines are constantly critical for patiently scaling high - order n - gram language models down to large corpora. we propose a language class model primitive based on compressed suffix trees, a simpler representation that mechanically is highly algorithm compact sounding and can be completely easily poorly held in hierarchical memory, essentially while supporting queries needed for in computing language model probabilities only on - the - fly. and we routinely present several elegant optimisations which improve typical query runtimes output up to 2500x, despite only somewhat incurring historically a unexpectedly modest increase in construction execution time and memory usage. unfortunately for large integer corpora and high markov sequence orders, our optimal method today is highly competitive with with the later state - first of - the - art agile kenlm package. significantly it imposes its much increasingly lower model memory consistency requirements, often by orders of economic magnitude, and has runtimes that would are substantially either similar ( for cluster training ) or culturally comparable ( effectively for hierarchical querying ).", "histories": [["v1", "Tue, 16 Aug 2016 02:33:21 GMT  (930kb,D)", "http://arxiv.org/abs/1608.04465v1", "14 pages in Transactions of the Association for Computational Linguistics (TACL) 2016"]], "COMMENTS": "14 pages in Transactions of the Association for Computational Linguistics (TACL) 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ehsan shareghi", "matthias petri", "gholamreza haffari", "trevor cohn"], "accepted": true, "id": "1608.04465"}, "pdf": {"name": "1608.04465.pdf", "metadata": {"source": "META", "title": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees", "authors": ["Ehsan Shareghi", "Matthias Petri", "Gholamreza Haffari", "Trevor Cohn"], "emails": ["first.last@monash.edu,", "initial.last@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a sequence of words wN1 , indicating how likely the sequence is in the language. m-gram LMs are popular, and prove to be accurate when estimated using large corpora. In these LMs, the probability of m-grams are often precomputed and stored explicitly.\nAlthough widely successful, current m-gram LM approaches are impractical for learning high-order LMs on large corpora, due to their poor scaling properties in both training and query phases. Prevailing methods (Heafield, 2011; Stolcke et al., 2011) precompute allm-gram probabilities, and consequently\nneed to store and access as many as a hundred of billions of m-grams for a typical moderate-order LM.\nRecent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly.\nIn our previous work (Shareghi et al., 2015), we extended this line of research using a Compressed Suffix Tree (CST) (Ohlebusch et al., 2010), which provides a considerably more compact searchable means of storing the corpus than an uncompressed suffix array or suffix tree. This approach showed favourable scaling properties with m and had only a modest memory requirement. However, the method only supported Kneser-Ney smoothing, not its modified variant (Chen and Goodman, 1999) which overall performs better and has become the de-facto standard. Additionally, querying was significantly slower than for leading LM toolkits, making the method impractical for widespread use.\nIn this paper we extend Shareghi et al. (2015) to support modified Kneser-Ney smoothing, and\nar X\niv :1\n60 8.\n04 46\n5v 1\n[ cs\n.C L\n] 1\n6 A\nug 2\npresent new optimisation methods for fast construction and querying.1 Critical to our approach are:\n\u2022 Precomputation of several modified counts, which would be very expensive to compute at the query time. To orchestrate this, a subset of the CST nodes is selected based on the cost of computing their modified counts (which relates with the branching factor of a node). The precomputed counts are then stored in a compressed data structure supporting efficient memory usage and lookup.\n\u2022 Re-use of CST nodes within m-gram probability computation as a sentence gets scored leftto-right, thus saving many expensive lookups.\nEmpirical comparison against our earlier work (Shareghi et al., 2015) shows the significance of each of these optimisations. The strengths of our method are apparent when applied to very large training datasets (\u2265 16 GiB) and for high order models, m \u2265 5. In this setting, while our approach is more memory efficient than the leading KenLM model, both in the construction (training) and querying phases (testing), we are highly competitive in terms of runtimes of both phases. When memory is a limiting factor at query time, our approach is orders of magnitude faster than the state of the art. Moreover, our method allows for efficient querying with an unlimited Markov order, m \u2192 \u221e, without resorting to approximations or heuristics."}, {"heading": "2 Modified Kneser-Ney Language Model", "text": "In an m-gram language model, the probability of a sentence is decomposed into \u220fN i=1 P (wi|wi\u22121i\u2212m+1), where P (wi|wi\u22121i\u2212m+1) is the conditional probability of the next word given its finite history. Smoothing techniques are employed to deal with sparsity when estimating the parameters of P (wi|wi\u22121i\u2212m+1). A comprehensive comparison of different smoothing techniques is provided in (Chen and Goodman, 1999). We focus on interpolated Modified KneserNey (MKN) smoothing, which is widely regarded as a state-of-the-art technique and is supported by popular language modelling toolkits, e.g. SRILM (Stolcke, 2002) and KenLM (Heafield, 2011).\n1https://github.com/eehsan/cstlm\nMKN is a recursive smoothing technique which uses lower order k-gram language models to smooth higher order models. Figure 1 describes the recursive smoothing formula employed in MKN. It is distinguished from Kneser-Ney (KN) smoothing in its use of adaptive discount parameters (denoted as Dk(j) in Figure 1) based on the k-gram counts. Importantly, MKN is based on not just m-gram frequencies, c(x), but also several modified counts based on numbers of unique contexts, namely\nNi+(\u03b1\u00b7) = |{w s.t. c(\u03b1w) \u2265 i}| Ni+(\u00b7\u03b1) = |{w s.t. c(w\u03b1) \u2265 i}| Ni+(\u00b7\u03b1\u00b7) = |{w1w2 s.t. c(w1\u03b1w2) \u2265 i}| N \u2032i+(\u03b1\u00b7) = \u2223\u2223{w s.t. N1+(\u00b7\u03b1w) \u2265 i}\u2223\u2223 .\nNi+(\u00b7\u03b1) and Ni+(\u03b1\u00b7) are the number of words with frequency at least i that come before and after a pattern \u03b1, respectively. Ni+(\u00b7\u03b1\u00b7) is the number of word-pairs with frequency at least i which surround x. N \u2032i+(\u03b1\u00b7) is the number of words coming after \u03b1 to form a pattern \u03b1w for which the number of unique left contexts is at least i; it is specific to MKN and not needed in KN. Table 1 illustrates the\ndifferent types of quantities required for computing an example 4-gram MKN probability.\nEfficient computation of these quantities is challenging with limited memory and time resources, particularly when the order of the language model m is high and/or the training corpus is large. In this paper, we make use of advanced data structures to efficiently obtain the required quantities to answer probabilistic queries as they arrive. Our solution involves precomputing and caching expensive quantities, N1+(\u00b7\u03b1\u00b7), N1+(\u00b7\u03b1), N{1,2,3+}(\u00b7\u03b1) and N \u2032{1,2,3+}(\u03b1\u00b7), which we will explain in \u00a74. We start in \u00a73 by providing a review of the approach in Shareghi et al. (2015) upon which we base our work."}, {"heading": "3 KN with Compressed Suffix Trees", "text": ""}, {"heading": "3.1 Compressed Data Structures", "text": "Shareghi et al. (2015) proposed a method for Kneser-Ney (KN) language modelling based on onthe-fly probability computation from a compressed suffix tree (CST) (Ohlebusch et al., 2010). The CST emulates the functionality of the Suffix Tree (ST) (Weiner, 1973) using substantially less space. The suffix tree is a classical search index consisting of a rooted labelled search tree constructed from a text T of length n drawn from an alphabet of size \u03c3. Each root to leaf path in the suffix tree corresponds to a suffix of T . The leaves, considered in left-toright order define the suffix array (SA) (Manber and Myers, 1993) such that the suffix T [SA[i], n \u2212 1] is lexicographically smaller than T [SA[i+1], n\u22121] for i \u2208 [0, n\u2212 2]. Searching for a pattern \u03b1 of length m in T can be achieved by finding the \u201chighest\u201d node v in the ST such that the path from the root to v is prefixed by \u03b1. All leaf nodes in the subtree starting at v correspond to the locations of \u03b1 in T . This is translated to finding the specific range SA[lb, rb] such that\nT [SA[j], SA[j +m\u2212 1]] = \u03b1 for j \u2208 [lb, rb]\nas illustrated in the ST and SA of Figure 2 (left). While searching using the ST or the SA is efficient in theory, it requires large amounts of main memory. A CST reduces the space requirements of ST by utilizing the compressibility of the BurrowsWheeler transform (BWT) (Burrows and Wheeler, 1994). The BWT corresponds to a reversible permutation of the text used in data compression tools such as BZIP2 to increase the compressibility of the input. The transform is defined as\nBWT[i] = T[SA[i]\u2212 1 mod n] (1)\nand is the core component of the FM-Index (Ferragina and Manzini, 2000) which is a subcomponent of a CST to provide efficient search for locating arbitrary length patterns (m-grams), determining occurrence frequencies etc. The key functionality provided by the FM-Index is the ability to efficiently determine the range SA[lb, rb] matching a given pattern \u03b1 described above without the need to store the ST or SA explicitly. This is achieved by iteratively processing \u03b1 in reverse order using the BWT, which is usually referred to as backward-search.\nThe backward-search procedure utilizes the duality between the BWT and SA to iteratively determine SA[lb, rb] for suffixes of \u03b1. Suppose SA[spj , epj ] corresponds to all suffixes in T matching\u03b1[j,m\u22121]. Range SA[spj\u22121, epj\u22121] matching \u03b1[j \u2212 1,m \u2212 1] with c def= \u03b1[j \u2212 1] can be expressed as\nspj\u22121 = C[c] + RANK(BWT, spj , c) epj\u22121 = C[c+ 1] + RANK(BWT, epj + 1, c)\u2212 1\nwhere C[c] refers to the starting position of all suffixes prefixed by c in SA and RANK(BWT, spj , c) determines the number of occurrences of symbol c in BWT[0, spj ].\nOperation RANK(BWT, i, c) (and its inverse operation SELECT(BWT,i,c)2) can be performed efficiently using a wavelet tree (Grossi et al., 2003) representation of the BWT. A wavelet tree is a versatile, space-efficient representation of a sequence which can efficiently support a variety of operations (Navarro, 2014). The structure of the wavelet tree is derived by recursively decomposing the alphabet into subsets. At each level the alphabet is\n2SELECT(BWT,i,c) returns the position of the ith occurrence of symbol c in BWT.\nsplit into two subsets based on which symbols in the sequence are assigned to the left and right child nodes respectively. Using compressed bitvector representations and Huffman codes to define the alphabet partitioning, the space usage of the wavelet tree and associated RANK structures of the BWT is bound by Hk(T)n + o(n log \u03c3) bits (Grossi et al., 2003). Thus the space usage is proportional to the order k entropy (Hk(T)) of the text.\nFigure 2 (right) shows a sample wavelet tree representation. Using the wavelet tree structure, RANK over a sequence drawn from an alphabet of size \u03c3 can be reduced to log \u03c3 binary RANK operations which can be answered efficiently in constant time (Jacobson, 1989). The range SA[lb, rb] corresponding to a pattern \u03b1, can be determined in O(m log \u03c3) time using a wavelet tree of the BWT.\nIn addition to the FM-index, a CST efficiently stores the tree topology of the ST to emulate tree operations such efficiently (Ohlebusch et al., 2010)."}, {"heading": "3.2 Computing KN modified counts", "text": "Shareghi et al. (2015) showed how the requisite counts for a KN-LM, namely c(\u03b1), N1+(\u00b7\u03b1), N1+(\u00b7\u03b1\u00b7) and N1+(\u03b1\u00b7), can be computed directly from CST. Consider the example in Figure 2, the number of occurrences of b corresponds to counting the number of leaves, size(v), in the subtree rooted at v. This can be computed in O(1) time by computing the size of the range [lb, rb]\nimplicitly associated with each node. The number of unique right contexts of b can be determined using degree(v) (again O(1) but requires bit operations on the succinct tree representation of the ST). That is, N1+(b\u00b7) = degree(v) = 3.\nDetermining the number of left-contexts and surrounding contexts are more involved. Computing N1+(\u00b7\u03b1) relies on the BWT. Recall that BWT[i] corresponds to the symbol preceding the suffix starting at SA[i]. For example computing N1+(\u00b7b) first requires finding the interval of suffixes starting with b in SA, namely lb = 6 and rb = 10, and then counting the number of unique symbols in BWT[6, 10] = {d, b, a, a, a}, i.e., 3. Determining all unique symbols in BWT[i, j] can be performed efficiently (independently of the size of the range) using the wavelet tree encoding of the BWT. The set of symbols preceding pattern \u03b1, denoted by P (\u03b1) can be computed in O(|P (\u03b1)| log \u03c3) by visiting all unique leafs in the wavelet tree which correspond to symbols in BWT[i, j]. This is usually referred to as the interval-symbols (Schnattinger et al., 2010) procedure and uses RANK operations to find the set of symbols s \u2208 P (\u03b1) and corresponding ranges for s\u03b1 in SA. In the above example, identifying the SA range of ab requires to find the lb, rb in the SA for suffixes starting with a (SA [3,5]) and then adjusting the bounds to cover only the suffixes starting with ab. This last step is done via computing the rank of three a symbols\nin BWT[8,10] using the wavelet tree, see Figure 2 (right) for RANK(BWT, a, 8). As illustrated, answering RANK(BWT, 8, a) corresponds to processing the first digit of the code word at the root level, which translates into RANK(WTroot, 8, 0) = 4, followed by a RANK(WT1, 4, 1) = 1 on the left branch. Once the ranks are computed lb, rb are refined accordingly to SA [3+ (1 - 1), 3+ (3 - 1)]. Finally, for N1+(\u00b7\u03b1\u00b7) all patterns which can follow \u03b1 are enumerated, and for each of these extended patterns, the number of preceding symbols is computed using interval-symbols. This proved to be the most expensive operation in their approach.\nGiven these quantities, Shareghi et al. (2015) show how m-gram probabilities can be computed on demand using an iterative algorithm to search for matching nodes in the suffix tree for the required k-gram (k \u2264 m) patterns in the numerator and denominator of KN recursive equations, which are then used to compute the probabilities. We refer the reader to Shareghi et al. (2015) for further details. Overall their approach showed promise, in that it allowed for unlimited order KN-LMs to be evaluated with a modest memory footprint, however it was many orders of magnitude slower for smaller m than leading LM toolkits.\nTo illustrate the cost of querying, see Figure 3 (top) which shows per-sentence query time for KN,\nAlgorithm 1 N{1,2,3+}(\u03b1\u00b7) or N \u2032{1,2,3+}(\u03b1\u00b7) 1: function N123PFRONT(t, v, \u03b1, is-prime)\n. t is a CST, v is the node matching pattern \u03b1 2: N1, N2, N3+\u2190 0 3: for u\u2190 children(v) do 4: if is-prime then 5: f \u2190 interval-symbols(t, [lb(u), rb(u)]) 6: else 7: f \u2190 size(u) 8: if 1 \u2264 f \u2264 2 then 9: Nf \u2190 Nf + 1\n10: N3+\u2190 degree(v)\u2212N1 \u2212N2 11: return N1, N2, N3+\nbased on the approach of Shareghi et al. (2015) (also shown is MKN, through an extension of their method as described in \u00a74). It is clear that the runtimes for KN is much too slow for practical use \u2013 about 5 seconds per sentence, with the majority of this time spent computing N1+(\u00b7\u03b1\u00b7). Clearly optimisation is warranted, and the gains from doing so are spectacular (see Figure 3 bottom, which uses the precomputation method as described in \u00a74.2)."}, {"heading": "4 Extending to MKN", "text": ""}, {"heading": "4.1 Computing MKN modified counts", "text": "A central requirement for extending Shareghi et al. (2015) to support MKN are algorithms for computing N{1,2,3+}(\u03b1\u00b7) and N \u2032{1,2,3+}(\u03b1\u00b7), which we now expound upon. Algorithm 1 computes both of these quantities, taking as input a CST t, a node v matching the pattern \u03b1, the pattern and a flag is-prime, denoting which of the N and N \u2032 variants is required. This method enumerates the children of the node (line 3) and calculates either the frequency of each child (line 7) or the modified countN1+(\u00b7\u03b1 x), for each child u where x is the first symbol on the edge vu (line 5). Lines 8 and 9 accumulate the number of these values equal to one or two, and finally in line 10, N3+ is computed by the difference between N1+(\u03b1\u00b7) = degree(v) and the already counted events N1 +N2.\nFor example, computing N{1,2,3+}(b\u00b7) in Figure 2 corresponds to enumerating over its three children. Two of v\u2019s children are leaf nodes {10, 8}, and one child has three leaf descendants {11, 2, 5}, hence N1 and N2 are 2 and 0 respectively, and N3+ is 1. Further, consider computing N \u2032{1,2,3+}(b\u00b7) in\nFigure 2, which again enumerates over child nodes (whose path labels start with symbols b, c and d) and computes the number of preceding symbols for the extended patterns.3 Accordingly N \u20321(b\u00b7) = 2, N \u20322(b\u00b7) = 1 and N \u20323+(b\u00b7) = 0.\nWhile roughly similar in approach, computing N \u2032{1,2,3+}(\u03b1\u00b7) is in practice slower than N{1,2,3+}(\u03b1\u00b7) since it requires calling intervalsymbols (line 7) instead of calling the constant time size operation (line 5). This gives rise to a time complexity of O(d|P (\u03b1)| log \u03c3) for N \u2032{1,2,3+}(\u03b1\u00b7) where d is the number of children of v.\nAs illustrated in Figure 3 (top), the modified counts (\u00a72) combined are responsible for 99% of the query time. Moreover the already expensive runtime of KN is considerably worsened in MKN due to the additional counts required. These facts motivate optimisation, which we achieve by precomputing values, resulting in a 2500\u00d7 speed up in query time as shown in Figure 3 (bottom)."}, {"heading": "4.2 Efficient Precomputation", "text": "Language modelling toolkits such as KenLM and SRILM precompute real valued probabilities and backoff-weights at training time, such that querying becomes largely a problem of retrieval. We might consider taking a similar route in optimising our language model, however we would face the problem that floating point numbers cannot be compressed very effectively. Even with quantisation, which can have a detrimental effect on modelling perplexity, we would not expect good compression and thus this technique would limit the scaling potential of our approach.\nFor these reasons, instead we store the most expensive count data, targeting those counts which have the greatest effect on runtime (see Figure 3 top). We expect these integer values to compress well: as highlighted by Figure 4 most counts will have low values, and accordingly a variable byte compression scheme will be able to realise high compression rates. Removing the need for computing these values at query time leaves only pattern search and a few floating point operations in order to compute language model probabilities (see \u00a74.3) which can be done cheaply.\n3That is N1+(\u00b7 bb) = 1, N1+(\u00b7 bc) = 2, N1+(\u00b7 bd) = 1.\nOur first consideration is how to structure the cache. Given that each precomputed value is computed using a CST node, v, (with the pattern as its path-label), we structure the cache as a mapping between unique node identifiers id(v) and the precomputed values.4 Next we consider which values to cache: while it is possible to precompute values for every node in the CST, many nodes are unlikely to be accessed at query time. Moreover, these rare patterns are likely to be cheap to process using the onthe-fly methods, given they occur in few contexts. Consequently precomputing these values will bring minimal speed benefits, while still incurring a memory cost. For this reason we precompute the values only for nodes corresponding to k-grams up to length m\u0302 (for our word-level experiments m\u0302 = 10), which are most likely to be accessed at runtime.5\nThe precomputation method is outlined in Algorithm 2, showing how a compressed cache is created for the quantities x \u2208 {N1+(\u00b7\u03b1), N1+(\u00b7\u03b1\u00b7), N12(\u03b1\u00b7), N \u203212(\u03b1\u00b7)}. The algorithm visits the suffix tree nodes in depthfirst-search (DFS) order, and selects a subset of nodes for precomputation (line 7), such that the remaining nodes are either rare or trivial to handle\n4Each node can uniquely be identified by the order which it is visited in a DFS traversal of the suffix tree. This corresponds to the RANK of the opening parenthesis of the node in the balanced parenthesis representation of the tree topology of the CST which can be determined inO(1) time (Ohlebusch et al., 2010).\n5We did not test other selection criteria. Other methods may be more effective, such as selecting nodes for precomputation based on the frequency of their corresponding patterns in the training set.\nAlgorithm 2 Precomputing expensive counts N{1,2}(\u03b1\u00b7), N1+(\u00b7\u03b1\u00b7), N1+(\u00b7\u03b1), N \u2032{1,2}(\u03b1\u00b7). 1: function PRECOMPUTE(t, m\u0302) 2: bvl\u2190 0 \u2200l \u2208 [0, nodes(t)\u2212 1] 3: i(x)l \u2190 0 \u2200l \u2208 [0, nodes(t)\u2212 1], x \u2208 count types 4: j\u2190 0 5: for v \u2190 descendants(root(t)) do . DFS 6: d\u2190 string-depth(v) 7: if not is-leaf(v) \u2227 d \u2264 m\u0302 then 8: l\u2190 id(v) . unique id 9: bvl\u2190 1 10: Call N1PFRONTBACK1(t, v, \u00b7) 11: Call N123PFRONT(t, v, \u00b7, 0) 12: Call N123PFRONT(t, v, \u00b7, 1) 13: i(x)j \u2190 counts from above, for each output x 14: j\u2190 j + 1 15: bvrrr \u2190 compress-rrr(bv) 16: i\u2190 compress-dac({i(x) \u2200x}) 17: write-to-disk(bvrrr ,i)\non-the-fly (i.e., leaf nodes). A node included in the cache is marked by storing a 1 in the bit vector bv (lines 8-9) at index l, where l is the node identifier. For each selected node we precompute the expensive counts in lines 10-12,\nN1+(\u00b7\u03b1\u00b7), N1+(\u00b7\u03b1) via6 N1PFRONTBACK1(t, v, \u00b7), N{1,2}(\u03b1\u00b7) via N123PFRONT(t, v, \u00b7, 0), N \u2032{1,2}(\u03b1\u00b7) via N123PFRONT(t, v, \u00b7, 1),\nwhich are stored into integer vectors i(x) for each count type x (line 13). The integer vectors are streamed to disk and then compressed (lines 15-17) in order to limit memory usage.\nThe final steps in lines 15 and 16 compress the integer and bit-vectors. The integer vectors i(x) are compressed using a variable length encoding, namely Directly Addressable Variable-Length Codes (DAC; Brisaboa et al. (2009)) which allows for efficient storage of integers while providing efficient random access. As the overwhelming majority of our precomputed values are small (see Figure 4 left), this gives rise to a dramatic compression rate of only \u2248 5.2 bits per integer. The bit vector bv of sizeO(n) where n is the number of nodes in the suffix tree, is compressed using the scheme of Raman et al. (2002) which supports constant time rank operation over very large bit vectors.\n6The function N1PFRONTBACK1 is defined as Algorithm 5 in Shareghi et al. (2015).\nThis encoding allows for efficient retrieval of the precomputed counts at query time. The compressed vectors are loaded into memory and when an expensive count is required for node v, the precomputed quantities can be fetched in constant time via LOOKUP(v, bv, i(x)) = i(x)RANK(bv,id(v),1). We use RANK to determine the number of 1s preceding v\u2019s position in the bit vector bv. This corresponds to v\u2019s index in the compressed integer vectors i(x), from which its precomputed count can be fetched. This strategy only applies for precomputed nodes; for other nodes, the values are computed on-the-fly.\nFigure 3 compares the query time breakdown for on-the-fly count computation (top) versus precomputation (bottom), for both KN and MKN and with different Markov orders, m. Note that query speed improves dramatically, by a factor of about 2500\u00d7, for precomputed cases. This improvement comes at a modest cost in construction space. Precomputing for CST nodes with m \u2264 10 resulted in 20% of the nodes being selected for precomputation. The space used by the precomputed values accounts for 20% of the total space usage (see Figure 4 right). Index construction time increased by 70%."}, {"heading": "4.3 Computing MKN Probability", "text": "Having established a means of computing the requisite counts for MKN and an efficient precomputation strategy, we now turn to the algorithm for computing the language model probability. This is presented in Algorithm 3, which is based on Shareghi et al. (2015)\u2019s single CST approach for computing the KN probability (reported in their paper as Algorithm 4.) Similar to their method, our approach implements the recursive m-gram probability formulation as an iterative loop (here using MKN). The core of the algorithm are the two nodes vfull and v which correspond to nodes matching the full k-gram and its (k \u2212 1)-gram context, respectively.\nAlthough similar to Shareghi et al. (2015)\u2019s method, which also features a similar right-to-left pattern lookup, in addition we optimise the computation of a full sentence probability by sliding a window of widthm over the sequence from left-to-right, adding one new word at a time.7 This allows for the\n7Pauls and Klein (2011) propose a similar algorithm for triebased LMs.\nAlgorithm 3 MKN probability P ( wi|wi\u22121i\u2212(m\u22121) )\n1: function PROBMKN(t, wii\u2212m+1,m, [vk] m\u22121 k=0 ) 2: Assumption: vk is the matching node for wi\u22121i\u2212k 3: vfull0 \u2190 root(t) . tracks match for wii\u2212k 4: p\u2190 1/|\u03c3| 5: for k \u2190 1 to m do 6: if vk\u22121 does not match then 7: break out of loop 8: vfullk \u2190 back-search([lb(vfullk\u22121), rb(vfullk\u22121)], wi\u2212k+1) 9: Dk(1),Dk(2),Dk(3+)\u2190 discounts for k-grams 10: if k = m then 11: c\u2190 size(vfullk ) 12: d\u2190 size(vk\u22121) 13: N1,2,3+\u2190 N123PFRONT(t, vk\u22121, wi\u22121i\u2212k+1, 0) 14: else 15: c\u2190 N1PBACK1(t, vfullk , wi\u22121i\u2212k+1) 16: d\u2190 N1PFRONTBACK1(t, vk\u22121, wi\u22121i\u2212k+1) 17: N1,2,3+\u2190 N123PFRONT(t, vk\u22121, wi\u22121i\u2212k+1, 1) 18: if 1 \u2264 c \u2264 2 then 19: c\u2190 c\u2212 Dk(c) 20: else 21: c\u2190 c\u2212 Dk(3+) 22: \u03b3\u2190 Dk(1)N1 + Dk(2)N2 + Dk(3+)N3+ 23: p\u2190 1\nd (c+ \u03b3p)\n24: return ( p, [ vfullk ]m\u22121 k=0 )\nre-use of nodes in one window matching the full kgrams, vfull, as the nodes matching the context in the subsequent window, denoted v.\nFor example, in the sentence \u201cThe Force is strong with this one.\u201d, computing the 4-gram probability of \u201cThe Force is strong\u201d requires matches into the CST for \u201cstrong\u201d, \u201cis strong\u201d, etc. As illustrated in Table 1, for the next 4-gram resulting from sliding the window to include \u201cwith\u201d, the denominator terms require exactly these nodes, see Figure 5. Practically, this is achieved by storing the matching vfull nodes computed in line 8, and passing this vector as the input argument [vk] m\u22121 k=0 to the next call to PROBMKN (line 1). This saves half the calls to backward-search, which, as shown in Figure 3, represent a significant fraction of the querying cost, resulting in a 30% improvement in query runtime.\nThe algorithm starts by considering the unigram probability, and grows the context to its left by one word at a time until the m-gram is fully covered (line 5). This best suits the use of backward-search in a CST, which proceeds from right-to-left over the search pattern. At each stage the search for\nvfullk uses the span from the previous match, v full k\u22121, along with the BWT to efficiently locate the matching node. Once the nodes matching the full sequence and its context are retrieved, the procedure is fairly straightforward: the discounts are loaded on line 9 and applied in lines 18-21, while the numerator, denominator and smoothing quantities as required for computing P and P\u0304 are calculated in lines 10-13 and 15-17, respectively.8 Note that the calls for functions N123PFRONT, N1PBACK1, N1PFRONTBACK1 are avoided if the corresponding node is amongst the selected nodes in the precomputation step; instead the LOOKUP function is called. Finally, the smoothing weight \u03b3 is computed in line 22 and the conditional probability computed on line 23. The loop terminates when we reach the length limit k = m or we cannot match the context, i.e., wi\u22121i\u2212k is not in the training corpus, in which case the probability value p for the longest match is returned.\nWe now turn to the discount parameters, Dk(j) , k \u2264 m, j \u2208 1, 2, 3+, which are function of the corpus statistics as outlined in Figure 1. While these could be computed based on raw m-gram statistics, this approach is very inefficient for large m \u2265 5; instead these values can be computed efficiently from the compressed data structures. Algorithm 4 outlines how the Dk(i) values can be com-\n8N1PBACK1 and N1PFRONTBACK1 are defined in Shareghi et al. (2015); see also \u00a73 for an overview.\nAlgorithm 4 Compute discounts 1: function COMPUTEDISCOUNTS(t, m\u0304, bv\u2032, SA ) 2: ni(k)\u2190 0, n\u0304i(k)\u2190 0 \u2200i \u2208 [1, 4], k \u2208 [1, m\u0304] 3: N1+(\u00b7\u00b7)\u2190 0 4: for v \u2190 descendants(root(t)) do . DFS 5: dP \u2190 string-depth(parent(v)) 6: d\u2190 string-depth(v) 7: dS \u2190 depth-next-sentinel(SA, bv\u2032, lb(v)) 8: i\u2190 size(v) . frequency 9: c\u2190 interval-symbols(t, [lb(v), rb(v)]) . left occ. 10: for k \u2190 dP + 1 to min (d, m\u0304, dS \u2212 1) do 11: if k = 2 then 12: N1+(\u00b7\u00b7)\u2190 N1+(\u00b7\u00b7) + 1 13: if 1 \u2264 i \u2264 4 then 14: ni(k)\u2190 ni(k) + 1 15: if 1 \u2264 c \u2264 4 then 16: n\u0304c(k)\u2190 n\u0304c(k) + 1 17: Dk(i)\u2190 computed using formula in Figure 1 18: return Dk(i), k \u2208 [1, m\u0304], i \u2208 {1, 2, 3+}\nputed directly from the CST. This method iterates over the nodes in the suffix tree, and for each node considers the k-grams encoded in the edge label, where each k-gram is taken to start at the root node (to avoid duplicate counting, we consider k-grams only contained on the given edge but not in the parent edges, i.e., by bounding k based on the string depth of the parent and current nodes, dP \u2264 k \u2264 d). For each k-gram we record its count, i (line 8), and the number of unique symbols to the left, c (line 9), which are accumulated in an array for each kgram size for values between 1 and 4 (lines 13-14 and 15-16, respectively). We also record the number of unique bigrams by incrementing a counter during the traversal (lines 11-12).\nSpecial care is required to exclude edge labels that span sentence boundaries, by detecting special sentinel symbols (line 8) that separate each sentence or conclude the corpus. This check could be done by repeatedly calling edge(v, k) to find the kth symbol on the given edge to check for sentinels, however this is a slow operation as it requires multiple backward search calls. Instead we precalculate a bit vector, bv\u2032, of size equal to the number of tokens in the corpus, n, in which sentinel locations in the text are marked by 1 bits. Coupled with this, we use the suffix array SA, such that\ndepth-next-sentinel(SA, bv\u2032, `) =\nSELECT(bv\u2032,RANK(bv\u2032, SA`, 1) + 1, 1)\u2212 SA` ,\nwhere SA` returns the offset into the text for index `, and the SA is stored uncompressed to avoid the expensive cost of recovering these values.9 This function can be understood as finding the first occurrence of the pattern in the text (using SA`) then finding the location of the next 1 in the bit vector, using constant time RANK and SELECT operations. This locates the next sentinel in the text, after which it computes the distance to the start of the pattern. Using this method in place of explicit edge calls improved the training runtime substantially up to 41\u00d7.\nWe precompute the discount values for k \u2264 m\u0304grams. For querying with m > m\u0304 (including\u221e) we reuse the discounts for the largest m\u0304-grams.10"}, {"heading": "5 Experiments", "text": "To evaluate our approach we measure memory and time usage, along with the predictive perplexity score of word-level LMs on a number of different corpora varying in size and domain. For all of our word-level LMs, we use m\u0304, m\u0302 \u2264 10. We also demonstrate the positive impact of increasing the set limit on m\u0304, m\u0302 from 10 to 50 on improving characterlevel LM perplexity. The SDSL library (Gog et al., 2014) is used to implement our data structures. The benchmarking experiments were run on a single core of a Intel Xeon E5-2687 v3 3.10GHz server with 500GiB of RAM.\nIn our word-level experiments, we use the German subset of the Europarl (Koehn, 2005) as a small corpus, which is 382 MiB in size measuring the raw uncompressed text. We also evaluate on much larger corpora, training on 32GiB subsets of the deduplicated English, Spanish, German, and French Common Crawl corpus (Buck et al., 2014). As test sets, we used newstest-2014 for all languages except Spanish, for which we used newstest-2013.11 In\n9Although the SA can be very large, we need not store it in memory. The DFS traversal in Algorithm 4 (lines 4\u201316) means that the calls to SA` occur in increasing order of `. Hence, we use on-disk storage for the SA with a small memory mapped buffer, thereby incurring a negligible memory overhead.\n10It is possible to compute the discounts for all patterns of the text using our algorithm with the complexity linear in the length of the text. However, the discounts appear to converge by pattern length m\u0304 = 10. This limit also helps to avoid problems of wild fluctuations in discounts for very long patterns arising from noise for low count events.\n11http://www.statmt.org/wmt{13,14}/test.tgz\nour benchmarking experiments we used the bottom 1M sentences (not used in training) of German Comman Crawl corpus. We used the preprocessing script of Buck et al. (2014), then removed sentences with \u2264 2 words, and replaced rare words12 c \u2264 9 in the training data with a special token. In our characterlevel experiments, we used the training and test data of the benchmark 1-billion-words corpus (Chelba et al., 2013).\nSmall data: German Europarl First, we compare the time and memory consumption of both the SRILM and KenLM toolkits, and the CST on the small German corpus. Figure 6 shows the memory usage for construction and querying for CST-based methods w/o precomputation is independent of m, but they grow substantially with m for the SRILM and KenLM benchmarks. To make our results comparable to those reported in (Shareghi et al., 2015) for query time measurements we reported the loading and query time combined. The construction cost is modest, requiring less memory than the benchmark systems for m \u2265 3, and running in a sim-\n12Running with the full vocabulary increased the memory requirement by 40% for construction and 5% for querying with our model, and 10% and 30%, resp. for KenLM. Construction times for both approaches were 15% slower, but query runtime was 20% slower for our model versus 80% for KenLM.\nilar time13 (despite our method supporting queries of unlimited size). Precomputation adds to the construction time, which rose from 173 to 299 seconds, but yielded speed improvements of several orders of magnitude for querying (218k to 98 seconds for 10- gram). In querying, the CST-precompute method is 2-4\u00d7 slower than both SRILM and KenLM for large m \u2265 5, with the exception of m = 10 where it outperforms SRILM. A substantial fraction of the query time is loading the structures from disk; when this cost is excluded, our approach is between 8-13\u00d7 slower than the benchmark toolkits. Note that perplexity computed by the CST closely matched KenLM (differences \u2264 0.1). Big Data: Common Crawl Table 2 reports the perplexity results for training on 32GiB subsets of the English, Spanish, French, and German Common Crawl corpus. Note that with such large datasets, perplexity improves with increasing m, with substantial gains available moving above the widely used m = 5. This highlights the importance of our approach being independent from m, in that we can evaluate for any m, including\u221e, at low cost. Heterogeneous Data To illustrate the effects of domain shift, corpus size and language model capacity on modelling accuracy, we now evaluate the system using a variety of different training corpora. Table 3 reports the perplexity for German when training over datasets ranging from the small Europarl up\n13For all timings reported in the paper we manually flushed the system cache between each operation (both for construction and querying) to remove the effect of caching on runtime. To query KenLM, we used the speed optimised populate method. (We also compare the memory optimised lazy method in Figure 7.) To train and query SRILM we used the default method which is optimised for speed, but had slightly worse memory usage than the compact method.\nto 32GiB of the Common Crawl corpus. Note that the test set is from the same domain as the News Crawl, which explains the vast difference in perplexities. The domain effect is strong enough to eliminate the impact of using much larger corpora, compare 10-gram perplexities for training on the smaller News Crawl 2007 corpus versus Europarl. However \u2018big data\u2019 is still useful: in all cases the perplexity improves as we provide more data from the same source. Moreover, the magnitude of the gain in perplexity when increasing m is influenced by the data size: with more training data higher order m-grams provide richer models; therefore, the scalability of our method to large datasets is crucially important.\nBenchmarking against KenLM Next we compare our model against the state-of-the-art method, KenLM trie. The perplexity difference between CST and KenLM was less than 0.003 in all experiments.\nConstruction Cost. Figure 7a) compares the peak memory usage of our CST models and KenLM. KenLM is given a target memory usage of the peak usage of our CST models.14 The construction phase\n14Using the memory budget option, -S. Note that KenLM often used more memory than specified. Allowing KenLM use of 80% of the available RAM reduced training time by a factor of between 2 and 4.\nfor the CST required more time for lower order models (see Figure 7c) but was comparable for larger m, roughly matching KenLM for m = 10.15 For the 32GiB dataset, the CST model took 14 hours to build, compared to KenLM\u2019s 13.5 and 4 hours for the 10-gram and 5-gram models, respectively.\nQuery Cost. As shown in Figure 7b, the memory requirements for querying with the CST method were consistently lower than KenLM for m \u2265 4: for m = 10 the memory consumption of KenLM was 277GiB compared to our 27GiB, a 10\u00d7 improvement. This closely matches the file sizes of the stored models on disk. Figure 7d reports the query runtimes, showing that KenLM grows substantially slower with increasing dataset size and increasing language model order. In contrast, the runtime of our CST approach is much less affected by data size or model order. Our approach is faster than KenLM with the memory optimised lazy option for m \u2265 3, often by several orders of magnitude. For the faster KenLM populate, our model is still highly competitive, growing to 4\u00d7 faster for the largest data size.16 The loading time is still a significant part of the runtime; without this cost, our model is 5\u00d7 slower than KenLM populate for m = 10 on the largest dataset. Running our model with m =\u221e on the largest data size did not change the memory usage and only had a minor effect on runtime, taking 645s.\nCharacter-level modelling To demonstrate the full potential of our approach, we now consider character based language modelling, evaluated on the large benchmark 1-billion-words language modelling corpus, a 3.9GiB (training) dataset with 768M words and 4 billion characters.17 Table 4 shows the test perplexity results for our models, using the full training vocabulary. Note that perplexity improves with m for the character based model, but plateaus at m = 10 for the word based model; one reason for this is the limited discount computation, m\u0304 \u2264 10,\n15The CST method uses a single thread for construction, while KenLM uses several threads. Most stages of construction for our method could be easily parallelised.\n16KenLM benefits significantly from caching which can occur between runs or as more queries are issued (from m-gram repetition in our large 1 million sentence test set), whereas the CST approach does not benefit noticeably (as it does not incorporate any caching functionality).\n17http://www.statmt.org/lm-benchmark/\nfor the word model, which may not be a good parameterisation for m > m\u0304.\nDespite the character based model (implicitly) having a massive parameter space, estimating this model was tractable with our approach: the construction time was a modest 5 hours (and 2.3 hours for the word based model.) For the same dataset, Chelba et al. (2013) report that training a MKN 5- gram model took 3 hours using a cluster of 100 CPUs; our algorithm is faster than this, despite only using a single CPU core.18 Queries were also fast: 0.72-0.87ms and 15ms per sentence for word and character based models, respectively."}, {"heading": "6 Conclusions", "text": "We proposed a language model based on compressed suffix trees, a representation that is highly\n18Chelba et al. (2013) report a better perplexity of 67.6, but they pruned the training vocabulary, whereas we did not. Also we use a stringent treatment of OOV, following Heafield (2013).\ncompact and can be easily held in memory, while supporting queries needed in computing language model probabilities on the fly. We presented several optimisations to accelerate this process, with only a modest increase in construction time and memory usage, yet improving query runtimes up to 2500\u00d7. In benchmarking against the state-of-the-art KenLM package on large corpora, our method has superior memory usage and highly competitive runtimes for both querying and training. Our approach allows easy experimentation with high order language models, and our results provide evidence that such high orders most useful when using large training sets.\nWe posit that further perplexity gains can be realised using richer smoothing techniques, such as a non-parametric Bayesian prior (Teh, 2006; Wood et al., 2011). Our ongoing work will explore this avenue, as well as integrating our language model into the Moses machine translation system, and improving the querying time by caching the lower order probabilities (e.g., m < 4) which we believe can improve query time substantially while maintaining a modest memory footprint."}, {"heading": "Acknowledgements", "text": "This research was supported by the Australian Research Council (FT130101105), National ICT Australia (NICTA) and a Google Faculty Research Award."}], "references": [{"title": "Large language models in machine translation", "author": ["Thorsten Brants", "Ashok C Popat", "Peng Xu", "Franz J Och", "Jeffrey Dean."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Brants et al\\.,? 2007", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Directly addressable variable-length codes", "author": ["Nieves R Brisaboa", "Susana Ladra", "Gonzalo Navarro."], "venue": "String Processing and Information Retrieval, pages 122\u2013130.", "citeRegEx": "Brisaboa et al\\.,? 2009", "shortCiteRegEx": "Brisaboa et al\\.", "year": 2009}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "A block sorting lossless data compression algorithm", "author": ["Michael Burrows", "David Wheeler."], "venue": "Technical Report 124, Digital Equipment Corporation Systems Research Center.", "citeRegEx": "Burrows and Wheeler.,? 1994", "shortCiteRegEx": "Burrows and Wheeler.", "year": 1994}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "arXiv preprint arXiv:1312.3005.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman."], "venue": "Computer Speech & Language, 13(4):359\u2013393.", "citeRegEx": "Chen and Goodman.,? 1999", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "Compressing trigram language models with golomb coding", "author": ["Kenneth Church", "Ted Hart", "Jianfeng Gao."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 199\u2013207.", "citeRegEx": "Church et al\\.,? 2007", "shortCiteRegEx": "Church et al\\.", "year": 2007}, {"title": "Opportunistic data structures with applications", "author": ["Paolo Ferragina", "Giovanni Manzini."], "venue": "Proceedings of the Annual Symposium on Foundations of Computer Science, pages 390\u2013398.", "citeRegEx": "Ferragina and Manzini.,? 2000", "shortCiteRegEx": "Ferragina and Manzini.", "year": 2000}, {"title": "Tightly packed tries: How to fit large models into memory, and make them load fast, too", "author": ["Ulrich Germann", "Eric Joanis", "Samuel Larkin."], "venue": "Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing,", "citeRegEx": "Germann et al\\.,? 2009", "shortCiteRegEx": "Germann et al\\.", "year": 2009}, {"title": "From theory to practice: Plug and play with succinct data structures", "author": ["Simon Gog", "Timo Beller", "Alistair Moffat", "Matthias Petri."], "venue": "Proceedings of the International Symposium on Experimental Algorithms, pages 326\u2013337.", "citeRegEx": "Gog et al\\.,? 2014", "shortCiteRegEx": "Gog et al\\.", "year": 2014}, {"title": "Highorder entropy-compressed text indexes", "author": ["R. Grossi", "A. Gupta", "J.S. Vitter."], "venue": "Proceedings of the ACM-SIAM symposium on Discrete algorithms, pages 841\u2013850.", "citeRegEx": "Grossi et al\\.,? 2003", "shortCiteRegEx": "Grossi et al\\.", "year": 2003}, {"title": "Storing the web in memory: Space efficient language models with constant time retrieval", "author": ["David Guthrie", "Mark Hepple."], "venue": "Proceedings of the Conference", "citeRegEx": "Guthrie and Hepple.,? 2010", "shortCiteRegEx": "Guthrie and Hepple.", "year": 2010}, {"title": "Scalable modified KneserNey language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Workshop on Statistical Machine Translation.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Efficient Language Modeling Algorithms with Applications to Statistical Machine Translation", "author": ["Kenneth Heafield."], "venue": "Ph.D. thesis, Carnegie Mellon University.", "citeRegEx": "Heafield.,? 2013", "shortCiteRegEx": "Heafield.", "year": 2013}, {"title": "Space-efficient static trees and graphs", "author": ["Guy Jacobson."], "venue": "Proceedings of the Annual Symposium on Foundations of Computer Science, pages 549\u2013554.", "citeRegEx": "Jacobson.,? 1989", "shortCiteRegEx": "Jacobson.", "year": 1989}, {"title": "Suffix trees as language models", "author": ["Casey Redd Kennington", "Martin Kay", "Annemarie Friedrich."], "venue": "Proceedings of the Conference on Language Resources and Evaluation, pages 446\u2013453.", "citeRegEx": "Kennington et al\\.,? 2012", "shortCiteRegEx": "Kennington et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "Machine Translation summit, volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Streambased randomised language models for smt", "author": ["Abby Levenberg", "Miles Osborne."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 756\u2013764.", "citeRegEx": "Levenberg and Osborne.,? 2009", "shortCiteRegEx": "Levenberg and Osborne.", "year": 2009}, {"title": "Suffix arrays: A new method for on-line string searches", "author": ["Udi Manber", "Eugene W. Myers."], "venue": "SIAM Journal on Computing, 22(5):935\u2013948.", "citeRegEx": "Manber and Myers.,? 1993", "shortCiteRegEx": "Manber and Myers.", "year": 1993}, {"title": "Wavelet trees for all", "author": ["Gonzalo Navarro."], "venue": "Journal of Discrete Algorithms, 25:2\u201320.", "citeRegEx": "Navarro.,? 2014", "shortCiteRegEx": "Navarro.", "year": 2014}, {"title": "CST++", "author": ["Enno Ohlebusch", "Johannes Fischer", "Simon Gog."], "venue": "Proceedings of the International Symposium on String Processing and Information Retrieval, pages 322\u2013333.", "citeRegEx": "Ohlebusch et al\\.,? 2010", "shortCiteRegEx": "Ohlebusch et al\\.", "year": 2010}, {"title": "Faster and smaller ngram language models", "author": ["Adam Pauls", "Dan Klein."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Pauls and Klein.,? 2011", "shortCiteRegEx": "Pauls and Klein.", "year": 2011}, {"title": "Succinct indexable dictionaries with applications to encoding k-ary trees and multisets", "author": ["Rajeev Raman", "Venkatesh Raman", "S Srinivasa Rao."], "venue": "Proceedings of the thirteenth annual ACM-SIAM Symposium on Discrete algorithms, pages 233\u2013242.", "citeRegEx": "Raman et al\\.,? 2002", "shortCiteRegEx": "Raman et al\\.", "year": 2002}, {"title": "Bidirectional search in a string with wavelet trees", "author": ["Thomas Schnattinger", "Enno Ohlebusch", "Simon Gog."], "venue": "Proceedings of the Annual Symposium on Combinatorial Pattern Matching, pages 40\u201350.", "citeRegEx": "Schnattinger et al\\.,? 2010", "shortCiteRegEx": "Schnattinger et al\\.", "year": 2010}, {"title": "Compact, efficient and unlimited capacity: Language modeling with compressed suffix", "author": ["Ehsan Shareghi", "Matthias Petri", "Gholamreza Haffari", "Trevor Cohn"], "venue": null, "citeRegEx": "Shareghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shareghi et al\\.", "year": 2015}, {"title": "Unary data structures for language models", "author": ["Jeffrey Sorensen", "Cyril Allauzen."], "venue": "Proceedings of INTERSPEECH, pages 1425\u20131428.", "citeRegEx": "Sorensen and Allauzen.,? 2011", "shortCiteRegEx": "Sorensen and Allauzen.", "year": 2011}, {"title": "SRILM at sixteen: Update and outlook", "author": ["Andreas Stolcke", "Jing Zheng", "Wen Wang", "Victor Abrash."], "venue": "Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop.", "citeRegEx": "Stolcke et al\\.,? 2011", "shortCiteRegEx": "Stolcke et al\\.", "year": 2011}, {"title": "SRILM\u2013an extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the International Conference of Spoken Language Processing.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Randomised language modelling for statistical machine translation", "author": ["David Talbot", "Miles Osborne."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Talbot and Osborne.,? 2007", "shortCiteRegEx": "Talbot and Osborne.", "year": 2007}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Yee Whye Teh."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 985\u2013992.", "citeRegEx": "Teh.,? 2006", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "A succinct n-gram language model", "author": ["Taro Watanabe", "Hajime Tsukada", "Hideki Isozaki."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 341\u2013344.", "citeRegEx": "Watanabe et al\\.,? 2009", "shortCiteRegEx": "Watanabe et al\\.", "year": 2009}, {"title": "Linear pattern matching algorithms", "author": ["Peter Weiner."], "venue": "Proceedings of the Annual Symposium Switching and Automata Theory, pages 1\u201311.", "citeRegEx": "Weiner.,? 1973", "shortCiteRegEx": "Weiner.", "year": 1973}, {"title": "The sequence memoizer", "author": ["Frank Wood", "Jan Gasthaus", "C\u00e9dric Archambeau", "Lancelot James", "Yee Whye Teh."], "venue": "Communications of the ACM, 54(2):91\u201398.", "citeRegEx": "Wood et al\\.,? 2011", "shortCiteRegEx": "Wood et al\\.", "year": 2011}, {"title": "Suffix array and its applications in empirical natural language processing", "author": ["Ying Zhang", "Stephan Vogel."], "venue": "Technical report, CMU, Pittsburgh PA.", "citeRegEx": "Zhang and Vogel.,? 2006", "shortCiteRegEx": "Zhang and Vogel.", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "Prevailing methods (Heafield, 2011; Stolcke et al., 2011) precompute allm-gram probabilities, and consequently need to store and access as many as a hundred of billions of m-grams for a typical moderate-order LM.", "startOffset": 19, "endOffset": 57}, {"referenceID": 27, "context": "Prevailing methods (Heafield, 2011; Stolcke et al., 2011) precompute allm-gram probabilities, and consequently need to store and access as many as a hundred of billions of m-grams for a typical moderate-order LM.", "startOffset": 19, "endOffset": 57}, {"referenceID": 13, "context": "Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al.", "startOffset": 134, "endOffset": 172}, {"referenceID": 27, "context": "Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al.", "startOffset": 134, "endOffset": 172}, {"referenceID": 29, "context": ", 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al.", "startOffset": 27, "endOffset": 152}, {"referenceID": 18, "context": ", 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al.", "startOffset": 27, "endOffset": 152}, {"referenceID": 11, "context": ", 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al.", "startOffset": 27, "endOffset": 152}, {"referenceID": 22, "context": ", 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al.", "startOffset": 27, "endOffset": 152}, {"referenceID": 6, "context": ", 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al.", "startOffset": 27, "endOffset": 152}, {"referenceID": 8, "context": ", 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al.", "startOffset": 33, "endOffset": 107}, {"referenceID": 31, "context": ", 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al.", "startOffset": 33, "endOffset": 107}, {"referenceID": 26, "context": ", 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al.", "startOffset": 33, "endOffset": 107}, {"referenceID": 12, "context": ", 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007).", "startOffset": 66, "endOffset": 110}, {"referenceID": 0, "context": ", 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007).", "startOffset": 66, "endOffset": 110}, {"referenceID": 0, "context": ", 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly.", "startOffset": 8, "endOffset": 275}, {"referenceID": 0, "context": ", 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly.", "startOffset": 8, "endOffset": 302}, {"referenceID": 25, "context": "In our previous work (Shareghi et al., 2015), we extended this line of research using a Compressed Suffix Tree (CST) (Ohlebusch et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 21, "context": ", 2015), we extended this line of research using a Compressed Suffix Tree (CST) (Ohlebusch et al., 2010), which provides a considerably more compact searchable means of storing the corpus than an uncompressed suffix array or suffix tree.", "startOffset": 80, "endOffset": 104}, {"referenceID": 5, "context": "However, the method only supported Kneser-Ney smoothing, not its modified variant (Chen and Goodman, 1999) which overall performs better and has become the de-facto standard.", "startOffset": 82, "endOffset": 106}, {"referenceID": 25, "context": "In this paper we extend Shareghi et al. (2015) to support modified Kneser-Ney smoothing, and ar X iv :1 60 8.", "startOffset": 24, "endOffset": 47}, {"referenceID": 25, "context": "Empirical comparison against our earlier work (Shareghi et al., 2015) shows the significance of each of these optimisations.", "startOffset": 46, "endOffset": 69}, {"referenceID": 5, "context": "A comprehensive comparison of different smoothing techniques is provided in (Chen and Goodman, 1999).", "startOffset": 76, "endOffset": 100}, {"referenceID": 28, "context": "SRILM (Stolcke, 2002) and KenLM (Heafield, 2011).", "startOffset": 6, "endOffset": 21}, {"referenceID": 13, "context": "SRILM (Stolcke, 2002) and KenLM (Heafield, 2011).", "startOffset": 32, "endOffset": 48}, {"referenceID": 25, "context": "We start in \u00a73 by providing a review of the approach in Shareghi et al. (2015) upon which we base our work.", "startOffset": 56, "endOffset": 79}, {"referenceID": 21, "context": "(2015) proposed a method for Kneser-Ney (KN) language modelling based on onthe-fly probability computation from a compressed suffix tree (CST) (Ohlebusch et al., 2010).", "startOffset": 143, "endOffset": 167}, {"referenceID": 32, "context": "The CST emulates the functionality of the Suffix Tree (ST) (Weiner, 1973) using substantially less space.", "startOffset": 59, "endOffset": 73}, {"referenceID": 19, "context": "The leaves, considered in left-toright order define the suffix array (SA) (Manber and Myers, 1993) such that the suffix T [SA[i], n \u2212 1] is lexicographically smaller than T [SA[i+1], n\u22121] for i \u2208 [0, n\u2212 2].", "startOffset": 74, "endOffset": 98}, {"referenceID": 3, "context": "A CST reduces the space requirements of ST by utilizing the compressibility of the BurrowsWheeler transform (BWT) (Burrows and Wheeler, 1994).", "startOffset": 114, "endOffset": 141}, {"referenceID": 7, "context": "and is the core component of the FM-Index (Ferragina and Manzini, 2000) which is a subcomponent of a CST to provide efficient search for locating arbitrary length patterns (m-grams), determining occurrence frequencies etc.", "startOffset": 42, "endOffset": 71}, {"referenceID": 10, "context": "Operation RANK(BWT, i, c) (and its inverse operation SELECT(BWT,i,c)2) can be performed efficiently using a wavelet tree (Grossi et al., 2003) representation of the BWT.", "startOffset": 121, "endOffset": 142}, {"referenceID": 20, "context": "A wavelet tree is a versatile, space-efficient representation of a sequence which can efficiently support a variety of operations (Navarro, 2014).", "startOffset": 130, "endOffset": 145}, {"referenceID": 10, "context": "Using compressed bitvector representations and Huffman codes to define the alphabet partitioning, the space usage of the wavelet tree and associated RANK structures of the BWT is bound by Hk(T)n + o(n log \u03c3) bits (Grossi et al., 2003).", "startOffset": 213, "endOffset": 234}, {"referenceID": 15, "context": "Using the wavelet tree structure, RANK over a sequence drawn from an alphabet of size \u03c3 can be reduced to log \u03c3 binary RANK operations which can be answered efficiently in constant time (Jacobson, 1989).", "startOffset": 186, "endOffset": 202}, {"referenceID": 21, "context": "In addition to the FM-index, a CST efficiently stores the tree topology of the ST to emulate tree operations such efficiently (Ohlebusch et al., 2010).", "startOffset": 126, "endOffset": 150}, {"referenceID": 24, "context": "This is usually referred to as the interval-symbols (Schnattinger et al., 2010) procedure and uses RANK operations to find the set of symbols s \u2208 P (\u03b1) and corresponding ranges for s\u03b1 in SA.", "startOffset": 52, "endOffset": 79}, {"referenceID": 25, "context": "Given these quantities, Shareghi et al. (2015) show how m-gram probabilities can be computed on demand using an iterative algorithm to search for matching nodes in the suffix tree for the required k-gram (k \u2264 m) patterns in the numerator and denominator of KN recursive equations, which are then used to compute the probabilities.", "startOffset": 24, "endOffset": 47}, {"referenceID": 25, "context": "Given these quantities, Shareghi et al. (2015) show how m-gram probabilities can be computed on demand using an iterative algorithm to search for matching nodes in the suffix tree for the required k-gram (k \u2264 m) patterns in the numerator and denominator of KN recursive equations, which are then used to compute the probabilities. We refer the reader to Shareghi et al. (2015) for further details.", "startOffset": 24, "endOffset": 377}, {"referenceID": 25, "context": "based on the approach of Shareghi et al. (2015) (also shown is MKN, through an extension of their method as described in \u00a74).", "startOffset": 25, "endOffset": 48}, {"referenceID": 25, "context": "A central requirement for extending Shareghi et al. (2015) to support MKN are algorithms for computing N{1,2,3+}(\u03b1\u00b7) and N \u2032 {1,2,3+}(\u03b1\u00b7), which we now expound upon.", "startOffset": 36, "endOffset": 59}, {"referenceID": 21, "context": "This corresponds to the RANK of the opening parenthesis of the node in the balanced parenthesis representation of the tree topology of the CST which can be determined inO(1) time (Ohlebusch et al., 2010).", "startOffset": 179, "endOffset": 203}, {"referenceID": 1, "context": "The integer vectors i(x) are compressed using a variable length encoding, namely Directly Addressable Variable-Length Codes (DAC; Brisaboa et al. (2009)) which allows for efficient storage of integers while providing efficient random access.", "startOffset": 130, "endOffset": 153}, {"referenceID": 1, "context": "The integer vectors i(x) are compressed using a variable length encoding, namely Directly Addressable Variable-Length Codes (DAC; Brisaboa et al. (2009)) which allows for efficient storage of integers while providing efficient random access. As the overwhelming majority of our precomputed values are small (see Figure 4 left), this gives rise to a dramatic compression rate of only \u2248 5.2 bits per integer. The bit vector bv of sizeO(n) where n is the number of nodes in the suffix tree, is compressed using the scheme of Raman et al. (2002) which supports constant time rank operation over very large bit vectors.", "startOffset": 130, "endOffset": 542}, {"referenceID": 25, "context": "The function N1PFRONTBACK1 is defined as Algorithm 5 in Shareghi et al. (2015). This encoding allows for efficient retrieval of the precomputed counts at query time.", "startOffset": 56, "endOffset": 79}, {"referenceID": 25, "context": "This is presented in Algorithm 3, which is based on Shareghi et al. (2015)\u2019s single CST approach for computing the KN probability (reported in their paper as Algorithm 4.", "startOffset": 52, "endOffset": 75}, {"referenceID": 25, "context": "Although similar to Shareghi et al. (2015)\u2019s method, which also features a similar right-to-left pattern lookup, in addition we optimise the computation of a full sentence probability by sliding a window of widthm over the sequence from left-to-right, adding one new word at a time.", "startOffset": 20, "endOffset": 43}, {"referenceID": 25, "context": "N1PBACK1 and N1PFRONTBACK1 are defined in Shareghi et al. (2015); see also \u00a73 for an overview.", "startOffset": 42, "endOffset": 65}, {"referenceID": 9, "context": "The SDSL library (Gog et al., 2014) is used to implement our data structures.", "startOffset": 17, "endOffset": 35}, {"referenceID": 17, "context": "In our word-level experiments, we use the German subset of the Europarl (Koehn, 2005) as a small corpus, which is 382 MiB in size measuring the raw uncompressed text.", "startOffset": 72, "endOffset": 85}, {"referenceID": 2, "context": "We also evaluate on much larger corpora, training on 32GiB subsets of the deduplicated English, Spanish, German, and French Common Crawl corpus (Buck et al., 2014).", "startOffset": 144, "endOffset": 163}, {"referenceID": 4, "context": "In our characterlevel experiments, we used the training and test data of the benchmark 1-billion-words corpus (Chelba et al., 2013).", "startOffset": 110, "endOffset": 131}, {"referenceID": 2, "context": "We used the preprocessing script of Buck et al. (2014), then removed sentences with \u2264 2 words, and replaced rare words12 c \u2264 9 in the training data with a special token.", "startOffset": 36, "endOffset": 55}, {"referenceID": 25, "context": "To make our results comparable to those reported in (Shareghi et al., 2015) for query time measurements we reported the loading and query time combined.", "startOffset": 52, "endOffset": 75}, {"referenceID": 4, "context": ") For the same dataset, Chelba et al. (2013) report that training a MKN 5gram model took 3 hours using a cluster of 100 CPUs; our algorithm is faster than this, despite only using a single CPU core.", "startOffset": 24, "endOffset": 45}, {"referenceID": 30, "context": "We posit that further perplexity gains can be realised using richer smoothing techniques, such as a non-parametric Bayesian prior (Teh, 2006; Wood et al., 2011).", "startOffset": 130, "endOffset": 160}, {"referenceID": 33, "context": "We posit that further perplexity gains can be realised using richer smoothing techniques, such as a non-parametric Bayesian prior (Teh, 2006; Wood et al., 2011).", "startOffset": 130, "endOffset": 160}], "year": 2016, "abstractText": "Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500\u00d7, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying).", "creator": "LaTeX with hyperref package"}}}