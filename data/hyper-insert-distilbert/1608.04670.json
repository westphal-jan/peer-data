{"id": "1608.04670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Attribute Extraction from Product Titles in eCommerce", "abstract": "this 2008 paper fully presents a popularly named conceptual entity extraction system responsible for detecting attributes in product titles upstream of limited ecommerce specialist retailers countries like walmart. the absence of relevant syntactic structure in such specialized short language pieces of long text expressions makes extracting attribute values a challenging problem. we merely find aware that combining underlying sequence content labeling algorithms such categories as conditional random fields and structured perceptron optimization with a curated normalization removal scheme produces an effective system for solving the task of extracting product individual attribute values correctly from certain titles. continuing to keep the discussion together concrete,... we both will illustrate the conceptual mechanics framework of the system from the point of view of eliminating a buying particular attribute - setting brand. we please also discuss the importance of an xml attribute extraction system in showing the global context of retail websites bundled with particular large product catalogs, lets compare our approach to use other potential approaches to conquer this problem and they end begin the paper with a quick discussion of the performance approach of solving our system for extracting attributes.", "histories": [["v1", "Mon, 15 Aug 2016 03:34:13 GMT  (319kb)", "http://arxiv.org/abs/1608.04670v1", "Accepted at the Workshop on Enterprise Intelligence, KDD 2016"]], "COMMENTS": "Accepted at the Workshop on Enterprise Intelligence, KDD 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ajinkya more"], "accepted": false, "id": "1608.04670"}, "pdf": {"name": "1608.04670.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["@WalmartLabs", "amore@walmartlabs.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n04 67\n0v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 6"}, {"heading": "1. INTRODUCTION", "text": ""}, {"heading": "1.1 Vocabulary", "text": "Before beginning the discussion of the problem we will first define some terms that will be used in the remainder of the paper. A product is any commodity which may be sold by a retailer. An attribute is a feature that describes a specific property of a product or a product listing. Some examples of attributes include brand, color, gender, material, title, description, etc. An attribute value is a particular value assumed by the attribute. For example, for the product title\nApple iPad Mini 3 16GB Wi-Fi Refurbished, Gold\nthe brand attribute value is \u2018Apple\u2019 and the color attribute value is \u2018Gold\u2019. A product may alternatively be defined as a collection of such attribute-value pairs, where a value can potentially be empty. Formally, let p be a product with attributes \u03b11, \u03b12, ..., \u03b1m and values v1, v2, ..., vm respectively. Then, we represent p = {\u03b11 : v1, \u03b12 : v2, ..., \u03b1m : vm} and write p(\u03b1i) = vi for 1 \u2264 i \u2264 m to indicate an attribute-value relationship for p.\nIn the context of an eCommerce website, the attribute values may be used to filter search results based on the items with a matching attribute value. As is done on Walmart.com and several other retail websites, this may be accomplished by populating relevant attribute values in the left hand navigation pane. For any given search or browse session, the\nCopyright is held by the author/owner(s) Workshop on Enterprise Intelligence, Aug 14, KDD 2016\nattributes that are displayed for further filtering in the left hand navigation as described above will be called facets.\nFor the sake of brevity, we use the terms \u2018attribute value extraction\u2019 and \u2018attribute extraction\u2019 synonymously."}, {"heading": "1.2 Problem set up", "text": "We formalize attribute extraction as per the following definition.\nDefinition 1. Let x be a product title and let (x1, x2, .., xn) be a particular tokenization xt of x. Given an attribute \u03b1, attribute extraction is the process of discovering two functions E (raw extraction) and N (normalization) such that\n\u2022 E(xt) = E((x1, x2, ..., xn)) = (xi, xi+1, ..., xk) for 1 \u2264 i \u2264 k \u2264 n where av = (xi, xi+1, ..., xk) is a tokenization of a particular value of \u03b1, \u2022 N((xi, xi+1, ..., xk)) = as where as is the standardized representation of av.\nExample 1. Consider the product title\nx = Hewlett Packard B4L03A#B1H Officejet Pro Eaio\nso that whitespace tokenization yields\nxt = (x1, x2, ..., x6) = (Hewlett, Packard, ..., Eaio).\nLet \u03b1 be the attribute \u2018brand\u2019. Then we seek to find two functions E and N such that\nE((x1, x2, ..., x6)) = (x1, x2) = (Hewlett, Packard)\nand\nN((Hewlett, Packard)) = HP.\nThe goal of the attribute extraction system is to minimize the loss function 1\u2212 F1 where the F1 measure is as defined in section 9.1.\nAttribute value extraction is a particular instance of a named entity recognition problem. This paper explores the use of machine learning techniques, in particular, sequence labeling algorithms for the purposes of extracting attribute values from product titles. We will also illustrate a normalization scheme which provides the dual benefits of standardizing variations in the same attribute value and boosting precision.\nIn section 3, we outline some of the challenges associated with attribute extraction in general. We reference some of the previous work on the problem of entity extraction in section 4. Section 5 describes the sequence labeling algorithms used to build an attribute extraction system that works well\nwith eCommerce data. This system is described in detail in sections 6 - 9 with respect to a particular attribute - \u2018brand\u2019. We finish the paper by describing how similar approaches have been successful in extracting other attributes such as \u2019character\u2019, \u2018manufacturer part number\u2019, \u2018package quantity\u2019, etc and discussing the importance of attribute extraction in eCommerce."}, {"heading": "2. USE CASES", "text": "In this section, we elaborate on the importance of attribute extraction for retail websites."}, {"heading": "2.1 Discoverability", "text": "Using facets to filter search or browse results is a common way for consumers of eCommerce websites to navigate the site and search through the product space. In order to ensure a good faceted navigation experience, it is critical to associate attribute value metadata to products for the attributes that appear in the facets.\nFor example, let S be a search query entered by a user and R be the set of products returned as a result. Suppose product p \u2208 R and p has an attribute \u03b1 which also happens to be a facet. Suppose further that the value of \u03b1 applicable to p is v. When the user clicks on facet value v, the filtered result set is R \u2032 \u2286 R. Then p \u2208 R \u2032\nif and only if p(\u03b1) = v. More concretely, suppose a user enters the search query \u2018Tee shirt\u2019 and the result set contains the product titled\nHanes Mens NANO-T Dri T-Shirt S Deep Red\nHowever, if the attribute \u2018color\u2019 for the product is missing the value \u2018Red\u2019, then if the user clicks on the value \u2018Red\u2019 under facet \u2018color\u2019, this product will no longer show in the result set even though it has the attribute value of interest to the user.\nThus, absence of relevant attribute values in product metadata has a direct bearing on the discoverability of the product and ultimately funnels down to affect the sale of such products.\nIt is crucial for modern eCommerce sites to be able to add products to its catalog as quickly as possible. As such, in order to ensure that the item set up process has minimal requirements, providing attribute values may not be enforced for all attributes. Consequently, a large fraction of incoming product data may not have attribute values supplied. In the absence of a system to tag missing attribute values, the discoverability of such products will be diminished."}, {"heading": "2.2 Ad campaigns", "text": "Certain attribute values are also are required by ad publishers such as Google and Bing in order to launch ad campaigns for products. In the absence of the required attributes, the ad campaigns are rejected."}, {"heading": "2.3 Compliance", "text": "Some attribute values are necessary to satisfy government compliance requirements, e.g. unit price for food items or items sold in bulk."}, {"heading": "2.4 Knowledge discovery", "text": "The list of valid values for a particular attribute may not be fixed. This is true, for instance, with attributes like \u2018brand\u2019, \u2018character\u2019, \u2018model number\u2019, etc. It is desirable to build a solution that can discover attribute values not currently part of the knowledge base."}, {"heading": "3. CHALLENGES", "text": ""}, {"heading": "3.1 Extracting attributes from product titles", "text": "We will describe a sequence labeling based machine learning system for extracting values of certain product attributes from product titles. We will discuss the challenges associated with this task and how they were handled in the current solution. In the next few sections we will describe the attribute extraction system using the specific example of brand extraction.\nIn certain product categories, brand is an important attribute. Extracting brands from product titles presents several interesting challenges, some of which are outlined below. Whenever possible, we include examples of actual titles of products sold on Walmart.com. In later sections we discuss how our solution mitigates these issues.\n\u2022 Unlike English prose, product titles do not adhere to a syntactic structure. They may be a concatenation of several nouns and adjectives as well as product specific identifiers and acronyms. Verbs tend to be missing and there is no standardized way of handling letter case. For example, consider the following titles of actual Walmart products (the brand names are in bold).\n\u2013 Chihuahua Bella Decorative Pillow by Manual Woodworkers and Weavers - SLCBCH\n\u2013 Real Deal Memorabilia BCosbyAlbumMF Bill Cos \u2022 Due to the diversity of products sold in any leading\neCommerce site, product titles do not follow any specific composition. For instance, the location of brands within titles may vary. Additionally, the number of tokens that constitute a brand name is also highly variable.\n\u2013 Old World Prints OWP86575Z Romantic Jasmine Poster Print by Vision studio - 18 x 22 \u2013 Autograph Warehouse 84377 Jake Rodriguez Card Boxing 1996 Ringside No . 40\n\u2013 Straight Talk Samsung Galaxy S3 Prepaid Cell Phone, White \u2022 Further, different products may contain slightly vary-\ning spellings of the same brand. This may include presence or absence of spaces and hyphens, presence or absence of apostrophe, presence or absence of trailing words such as \u2018inc.\u2019 or \u2018ltd.\u2019, etc.\n\u2013 J&C Baseball Clubhouse JC000213 WWE John Cena Engraved Collector Plaque with 8x10 KNOCK OUT Photo \u2013 J & C Baseball Clubhouse JC000008 Pittsburgh Penguins All Time Greats 6 Card Collector Plaque \u2022 Some titles may contain abbreviations of brand names. \u2013 Kcl 2SL25WH Accessory LED Tape Power Supply Lead in White \u2013 Kichler Builder 5019NI 8 Light Bath Strip in Brushed Nickel \u2022 Brand names in titles may contain typographical errors.\n\u2013 Trademak Global AD-CLC4000-PITT Pittsburgh Panthers 40 inch Rectangular Stained Glass Billiard Light\n\u2013 Trademark Global 24\u201dCushioned Folding Stool \u2022 A case of particular interest is that of generic or un-\nbranded products. There is especially a preponderance of such products that fall under jewelry and clothing categories.\n\u2013 0.5Ctw Diamond Fashion Womens Fixed Ring Size - 7 \u2013 Women\u2019s Popcorn Stitch Infinity Scarf\n\u2022 There are categories of products for which brand name is not an important attribute. In such cases, the brand facet will not even be displayed for a search of items belonging to these categories. Examples of such categories include books, movie DVDs, posters, etc. \u2022 The list of brand names relevant to a given product catalog is constantly changing. Products with new\nbrand names may appear, while some old brands may no longer sell any products. Thus, it is desirable for the brand extraction algorithm to be able to discover new brand names as well as extract known brands with high precision. \u2022 Collecting expert feedback either for the purposes of generating training data or validating model generated labels is subject to inter-annotator disagreement. It is not ideal to show different facet values that correspond to the same brand as it diminishes the quality of user experience. This is one of the reasons that makes crowd sourcing an unsatisfactory solution to the problem of attribute extraction."}, {"heading": "3.2 Comparison with other approaches", "text": "3.2.1 Dictionary based lookup A simple method for attribute extraction is to prepare a\ncurated lexicon of attribute values and given a product title, scan it to find a value from the list. Some gazetteer based approaches are discussed in [16] in the context of named entity recognition problems. This approach suffers from numerous drawbacks. The curated list will need to be constantly updated in order to match to new attribute values in products. For certain attributes, the number of possible values can be of the order of the number of products themselves, e.g. \u2018manufacturer part number\u2019. For such attributes, lookup based approaches are completely ineffective. Further, as mentioned earlier, the same attribute value may occur in a variety of different forms in the title, so the curated list will need to discover and keep track of all variations. Finally, such a system will need to devise a mechanism to break ties in case of multiple matches.\n3.2.2 Crowd Sourcing Crowd sourcing as a solution for extracting attributes for\nproducts is rendered ineffective because of the scale of a major retail catalog. Given the large number of attributes that need to be extracted, potentially for millions of products, the time and cost of crowd sourcing make it prohibitive. In addition, since variations of attribute values may need to be standardized, resolving inter annotator disagreement can be challenging and may require expert intervention.\n3.2.3 Rule based extraction Rule based approaches have had success in certain named\nentity recognition tasks [5], [8], [15]. Such techniques typically leverage the grammatical structure of the language. However, as mentioned earlier in the section, product titles do not conform to a syntactical structure or grammar unlike news articles or prose. An alternative would be to use rule based approaches with product description, but descriptions may contain named entities unrelated to the product such as in comparisons to similar products.\nAnother disadvantage with rule based approaches stems from the number of different attributes that are used by a general retailer to describe products. Typically, a modern retailer deals with tens of thousands of attributes across thousands of product categories. Rule based approaches will need to be tailored for each attribute. Creating and maintaining rules for hundreds or thousands of attributes can be quite challenging. In contrast we were able to easily adapt\nour system with minimal changes to build models for a variety of attributes.\n3.2.4 Supervised text classification Yet another possibility is to use text classification algo-\nrithms such as logistic regression, naive bayes or support vector machines that do not leverage the sequential structure of the data. These algorithms can be suitable for certain attributes, where the number of classes is known and small. In this scenario, classification algorithms can provide great performance. In contrast, when the number of classes is in tens of thousands, we will need a lot labeled training data and the model footprint will also be large. However, the main drawback with these models for attributes like brand and manufacturer part number is that they can only predict classes on which they are trained. Thus, in order to predict new brand values, the training data will need to be constantly updated with labeled data corresponding to new brands. In the case of manufacturer part number, this approach is essentially worthless since every new product will likely have an unseen part number."}, {"heading": "4. PRIOR WORK", "text": "Tagging products with attributes falls under the umbrella of knowledge extraction from text. Such problems have been explored in a variety of application areas. We will present a brief survey of similar undertakings in other domains.\nBikel et.al. [2] discuss the named entity extraction problem to identify location names, person names, named organizations and a few other entities. They present a Hidden Markov model and show that it performs favorably on named entity recognition tasks on standard datasets like MUC-6 and MET-1. Ritter et. al. [19] built a system for mining named entities from tweets - which is another example of text that significantly deviates from fluent prose.\nPart of speech tagging is a quintessential example of an entity recognition task and a number of approaches have been investigated in literature. Eric Brill [3] introduced a simple rule based tagger for learning part of speech which was shown to have an error rate of 7.9% when trained on 90% of Brown corpus and tested on a held out 5% subset. Schmid [20] illustrates a probabilistic tagger in which transition probabilities are estimated using a decision tree and which achieves an accuracy of 96.36% on Penn-Treebank data. Ratnaparkhi describes a maximum entropy approach to this problem that achieves an accuracy of 96.6% on Wall Street Journal data in Penn-Treebank.\nAlani et. al. [1] present a system to extract knowledge about artists from web pages leveraging a curated ontology. Kazama and Torisawa [12] use features relying on Wikipedia information to generate a Conditional Random Fields (CRF) based model for named entity recognition. Etzioni et. al. [7] describe unsupervised techniques to extract relationships between entities from web documents.\nThe problems that are the subject of this paper are most similar to the following works. Ghani et. al. [9] discuss the representation of retail products as attribute-value pairs. They tackle the problem of extracting values for a predefined list of attributes like age group, degree of brand appeal and price point. After obtaining an initial expert labeled dataset, they augment it with unlabeled data using ExpectationMaximization. Popescu and Etzioni [17] extend their system described in [7] to the problem of mining product features\nfrom online reviews. Putthividhya and Hu [18] designed a system for extracting product attributes from short listing titles such as those found on eBay. They focus on specific categories - clothing and shoes and on the attributes brand, style, size and color. They compare performance of Hidden Markov models, Maximum Entropy models, Support Vector Machines and CRFs. Beginning with a seed data set of labeled attribute values, they generate additional training data using unlabeled examples. For normalizing variations in the extracted attribute values, they use n-gram substring matching."}, {"heading": "5. SEQUENCE LABELING APPROACHES", "text": "We propose a sequence labeling based approach for identifying brands from product titles. Given an input sequence x = x1, x2, ..., xm, a sequence labeling algorithm aims to unearth a label sequence y = y1, y2, ..., ym such that the element xj is labeled yj . The labels yj usually come from a finite set. A typical example of such an approach is part of speech tagging in natural language processing.\nIn this work we evaluated performance using two sequence labeling algorithms - Structured Perceptron and Conditional Random Fields. In section 9 we compare the performance of these models with respect to another sequence labeling algorithm - Hidden Markov model."}, {"heading": "5.1 Feature Functions", "text": "Let X be the set of all input sequences and let Y be the set of all label sequences of length m. Let I = {1, 2, ..., m}. A feature function for a sequence labeling algorithm is a function f : X \u00d7 Y \u00d7 I \u2192 R.\nConsider for example the problem of part of speech tagging. This involves assigning every word in a unit of text (e.g. a sentence) a tag/label corresponding to its part of speech. The tags that appear in Penn Treebank [14] include DT (determiner), JJ (adjective), NN (noun), VB (verb) and IN (preposition). Now let x = (The, quick, brown, fox, jumps, over, the, lazy, dog), y = (DT, JJ, JJ, NN, VB, IN, DT, JJ, NN). We may define a feature function as follows:\nf(x,y, i) =\n{\n1 if xi = the and yi = DT 0 otherwise .\nThen f(x,y, 2) = 0 andf(x,y, 7) = 1. We may also define feature functions that take into account contextual information of tokens in the input sequence. In fact, such features are critical to the success of a sequence labeling algorithm, for otherwise, we might as well build a traditional classifier ignoring the sequence structure. An example of such a feature function is given below:\nf(x,y, i) =\n \n\n1 if xi is capitalized and xi+1 is\nnot capitalized\n0 otherwise\n.\nNote that f(x,y, 1) = 1 and f(x,y, i) = 0 for i 6= 1. Suppose we construct d feature functions f1, f2, ..., fd. Let\nFi(x,y) = (f1(x,y, i), f2(x,y, i), ..., fd(x,y, i))\nbe the d-dimensional feature vector corresponding to the to the pair x,y and position i. We denote\nF (x,y) = \u03a3mi=1Fi(x,y)\nas the d-dimensional feature vector corresponding to the pair x,y. Note that although F depends on the length of the sequencem, for a given input sequence x, we evaluate F over pairs (x,y) where y varies over candidate label sequences which will all have the same length as the length of x.\nWe will discuss the feature functions used in building models for the brand extraction algorithms in section 7."}, {"heading": "5.2 Structured Perceptron", "text": "Michael Collins presented the Structured Perceptron learning algorithm in [6]. We use the refinement of the algorithm called averaged parameters in that paper. Parameter averaging reduces variance providing a regularization effect and improves performance ([6], [10]).\nStructured Perceptron is a supervised learning algorithm. The training set to the algorithm consists of labeled sequences {(xi,yi)} where i = 1, 2, ..., n. Each input xi is a sequence of the form (x1, x2, ..., xm)i with a corresponding sequence of labels (y1, y2, ..., ym)i such that the input sequence element xj has a corresponding label yj . The labels belong to a finite set YL. Let YS denote the set of all sequences of length m such that each entry in the sequence belongs to YL. Thus, |YS| = |YL|\nm. Let d is the number of feature functions as illustrated in section 5.1. We outline the Structured Perceptron with averaged parameters (SP) algorithm below.\n1. Initialize w = (0, 0, ...0) and wa = (0, 0, ...0) where w,wa are tuples of length d 2. for j = 1 to N 3. for i = 1 to n 4. y\u2217i = arg maxy\u2208YSw\nTF (xi,yi) 5. if yi 6= y \u2217 i , then 6. w = w + F (xi,yi)\u2212 F (xi,y \u2217 i ) 7. wa = wa +w 8. Return wa\nnN\nStep 4 of the algorithm is implemented using Viterbi decoding."}, {"heading": "5.3 Linear Chain Conditional Random Fields", "text": "Conditional Random Fields (CRF) is a probabilistic structured labeling algorithm introduced by Lafferty, McCallum and Pereira in [13]. In our system, we used a linear chain CRF (LCCRF). As before, let x,y denote input and label sequences and let YS represent the set of all label sequences. Let F (x,y) be the d-dimensional feature vector corresponding to the pair x,y as defined in section 5.1. Then, for a given weight vector w \u2208 R, according to the LCCRF model we have,\nPr(y = y\u2217|x;w) = exp(wTF (x,y\u2217))\n\u03a3y\u2208YS exp(w TF (x,y))\n.\nGiven a weight vector, we seek to find a label sequence that maximizes the above conditional probability. We may simplify this as follows\narg maxy\u2208YSPr(y|x;w)\n= arg maxy\u2208YS exp(wTF (x,y\u2217))\n\u03a3y\u2208YS exp(w TF (x,y))\n= arg maxy\u2208YSexp(w T F (x,y\u2217)) = arg maxy\u2208YSw T F (x,y\u2217)\nAs before we search for the sequence maximizing the above dot product using Viterbi decoding.\nFinally we need to estimate the weight vector w. Suppose we have a set of labeled sequences {(xi,yi)} where i = 1, 2, ..., n as our training set. We define the conditional log-likelihood of the data as\nL(w) = \u03a3ni=1logPr(yi|xi;w)\nand the L2\u2212regularized log likelihood as\nL2(w) = \u03a3 n i=1logPr(yi|xi;w)\u2212\n\u03bb 2 ||w||2.\nThe parameter estimation problem is now posed as an optimization problem as follows\nw\u2217 = arg maxw\u2208RdL2(w)."}, {"heading": "6. ANNOTATING PRODUCT TITLES", "text": ""}, {"heading": "6.1 Labeling scheme", "text": "As an input for both models, we used a product title decomposed into a sequence of tokens labeled with BIO encoding. The first step of the labeling process tokenizes the title into a sequence of tokens separated by white space and/or certain special characters. The BIO encoding scheme assigns one of the following three labels to each of the tokens obtained this way.\n1. B-brand: This label indicates that the token is the beginning token of a brand name. It will not be used if the title does not contain a brand name. 2. I-brand: This label indicates that the token is an intermediate (i.e. any token other than the first) token of a brand name. It will not be used if the title does not contain a brand name or if the brand name in the title has a length of 1 token. 3. O: Indicates that the token is not part of a brand name in the corresponding title. We illustrate this encoding on a product title with the brand name \u2018Manual Woodworkers and Weavers\u2019.\nManual \ufe38 \ufe37\ufe37 \ufe38 B-brand Woodworkers \ufe38 \ufe37\ufe37 \ufe38 I-brand and \ufe38\ufe37\ufe37\ufe38 I-brand Weavers \ufe38 \ufe37\ufe37 \ufe38 I-brand AIMFBQ \ufe38 \ufe37\ufe37 \ufe38\nO\nButterfly \ufe38 \ufe37\ufe37 \ufe38\nO\nQuilt \ufe38 \ufe37\ufe37 \ufe38\nO\n32X42 \ufe38 \ufe37\ufe37 \ufe38\nO\ninch \ufe38\ufe37\ufe37\ufe38\nO"}, {"heading": "6.2 Distantly supervised training data", "text": "In order to capture the variations in which an attribute name may appear in product titles, we needed a sizable training set. However, generating a reasonable sized training data set using manual labeling would require substantial tedious and time consuming effort. Alternatively, we could use the set of products which were already tagged with the corresponding attribute value. However, this approach is susceptible to any noise in the existing tags which was found to be significant.\nWe used a distant supervision approach to build our initial training data set. For each attribute we built regex based rules to programatically annotate product titles as per the labeling scheme in section 6.1. For example, in the case of brand, we only annotated product titles in which the brand name appears exactly as a substring, except possibly for a change in the case and/or the presence or absence of certain special characters. Note that not all brand values appear\nexactly in product titles owing to value normalization which we discuss in more detail in section 8.1. For example the following title\nKimberly-Clark Nitrile Xtra Exam Medium Gloves in Purple\nwith an associated brand attribute value of \u2018Kimberly-Clark\u2019 could be included in the training set. However, the title\nBarnett Single Friction Plate Fits 76-79 Yamaha RD400\nwhere the normalized Walmart brand value is \u2018Barnett Crossbows\u2019 would not be included since the word \u2018Crossbows\u2019 does not appear in the title. In particular, the initial training set did not contain any product titles where the product had a legitimate brand value but which was not contained in the title.\nThis restriction achieved the dual purpose of shielding the training set from noisy labels as well as allowing us to programmatically annotate product titles without the need for any manual labeling. For each such brand name, we included up to three annotated titles to the training set. The reason for this limit was to avoid having a heavily imbalanced dataset since the distribution of the number of items with a given brand name was observed to be heavy tailed.\nNote that although the initial training set did not contain any product titles for which the normalized brand value did not appear in the product title, we were subsequently able to increase the size of the training set with such product titles from the analyst feedback we received during the validation phase (see section 8.3).\nIn addition, we also wanted the algorithm to detect product titles which do not contain any brand name. A couple of examples of such titles were provided in Section 3. To this end, we added a small set of such product titles corresponding to unbranded products where every token was labeled \u2018O\u2019."}, {"heading": "6.3 Interpreting output labels", "text": "The output of the learning algorithm on a product title x is a sequence of labels - one label per token in the tokenization xt of x, according to the BIO-encoding scheme specified in section 6.1. This labeling is transformed into a candidate brand name using the following steps:\n1. If the label of all tokens of the product title is \u2018O\u2019, we posit that a brand name does not appear in the title and output value for the product title is \u2018unbranded\u2019. In the notation of section 1.2,\nE(xt) = \u2018\u2019 and N(E(xt)) = \u2018unbranded\u2019.\n2. Otherwise, there must be a token with the label \u2018Bbrand\u2019. Consider the contiguous subsequence of tokens xs satisfying: (a) The label of the first token is \u2018B-brand\u2019. (b) If the length of the subsequence is greater than\n1, then the label of each token except the first is \u2018I-brand\u2019. (c) The last token in the subsequence is either the last token of the product title or the succeeding token from the product title is labeled \u2018O\u2019.\nThe substring of the product title corresponding to xs (if applicable) is considered to be the predicted brand for x i.e. E(xt) = xs. We discuss normalization for this case in section 8.\nNote that each product title corresponds to a unique brand. Thus, every label sequence in the training data consists of at most one \u2018B-brand\u2019 label. In practice, label sequences produced by the algorithm also consist of a single \u2018B-brand\u2019 label. In the rare case that multiple such sequences exist in a given product title, we deem the predicted brand value to be the one corresponding to the first such subsequence. This heuristic works well with the datasets we analyzed. Depending on the attribute a different strategy for resolving ties may be necessary. Alternately, in the case of multi valued attributes, we may accept all possible subsequences satisfying the above criteria."}, {"heading": "7. FEATURES", "text": "Both SP and LCCRF models were trained using a similar set of features. The choice of feature functions was motivated from a variety of literature on this subject including [21] and [18]. We curated and adapted the feature functions to those that improved the performance of the attribute extraction system. Feature selection was done using ablative analysis. In this methodology, we start with a set of feature functions and measure the impact of iteratively turning off each feature on relevant metrics (e.g. F1 measure). The final set of feature functions used has the property that removing any feature function adversely impacts the metric of interest. Alternately, we can start with one feature function and iteratively keep adding feature functions that improve performance."}, {"heading": "7.1 Feature functions", "text": "Some examples of features used in the attribute extraction system are given below. In particular, unlike many other named entity recognition systems, we don\u2019t employ any features based on attribute specific lexicons.\n7.1.1 Characteristic features These features are derived from information about a given\ntoken alone. For example the identity of the token, the character composition of the token, letter case, token length in terms of number of characters.\n7.1.2 Locational features These features depend on the position of the token in the\ntoken sequence into which the title is decomposed. For instance, number of tokens in the title before the given token.\n7.1.3 Contextual features In order for sequence labeling algorithms to work well,\nwe need to capture information about tokens neighboring a given token. This may be achieved using features such as the identity of the preceding/succeeding token, whether the preceding/succeeding token is capitalized, the bigram consisting of the token and its predecessor/successor, whether the preceding token is a conjunction, part of speech tag of the token, etc."}, {"heading": "7.2 Features used in brand extraction", "text": "For the particular case of brand extraction, we show below the set of features that gave the best performance over a sample of product titles in selected departments. We progressively added features until there was an improvement\nin the average F1 measure 1 obtained from a 10-fold cross validation. We make use of the following notation. To determine feature functions corresponding to a position i in a title, let w0 denote the token under consideration. If applicable, let w\u22121, w\u22122, ... denote the preceding tokens and let w1, w2, ... denote the successive tokens. For a token w let w[j] denote the jth character of w.\nThe table below shows the drop in average F1 measure, \u2206F1 (in percentage), recorded by turning off a given feature at a time. All measurements are based on a 10-fold cross validation. A \u2018-\u2019 against a feature function indicates it was not used for that model.\nFeature function \u2206F1 (LCCRF) \u2206F1 (SP)\n(w\u22121, w0) 0.577 0.675\n(w0, w1) 0.423 0.443\nw0 0.220 0.181\nw\u22121 lemma 0.177 0.079\n(w\u22122, w\u22121) 0.207 0.272\nw1[0] is a digit 0.146 0.120\nw0 consists only of letters 0.142 0.132\nFirst token in title 0.134 0.049\nw\u22121 0.072 0.029\nw0 contains hyphen 0.071 -\nw0[0] is uppercase 0.066 0.027\nNumber of characters in w0 0.064 0.096\nw\u22121 = by 0.061 -\nw0 lemma 0.056 0.095\ni (token position) 0.037 -\nw0[0] and w1[0] both uppercase 0.036 0.078\nw0 is first token in title 0.032 -\nw\u22121 = and 0.021 -\nw\u22121[0] uppercase 0.014 - w0 consists only of digits - 0.035\nw0 is uppercase - 0.081"}, {"heading": "8. POST PROCESSING", "text": "An artifact of employing a sequence labeling algorithm for the task of extracting brands is that the extracted brand value for any product title is a substring (possibly empty) of the title. As a consequence, an incorrect brand name prediction is rarely itself also a brand name. For example, consider the product title\nSea Gull Lighting Parkfield 3 Light Bath Vanity Light\nwhere the brand name is \u2018Sea Gull Lighting\u2019. Assume that we are introducing products from \u2018Sea Gull Lighting\u2019 for the first time, and thus said brand is not present in the list of known brands. Further, any substring of this product title consisting of consecutive tokens is not a brand name. This motivated the following post processing scheme which significantly boosted precision of the accepted brand predictions."}, {"heading": "8.1 Normalization", "text": "As mentioned in section 3, a given brand name may appear in a variety of spellings (for instance missing spaces, presence/absence of special characters, use of acronyms, etc) in product titles. In order to be fit to display as a facet, any\n1We formally define the relevant metrics in the context of attribute extraction in section 9.1.\nvariations of a single brand name must be normalized to a unique value. We maintain a collection of key-value pairs where the key is a brand name variation and the value is the normalized brand name. The normalized values are curated by internal analysts. We will call this collection the \u2018normalization dictionary\u2019.\nFor example, consider the following titles corresponding to the brand \u2018Chenille Kraft\u2019. We highlight the relevant brand tokens in bold.\n\u2022 Chenille Kraft Wonderfoam Magnetic Alphabet Letters, Assorted Colors, 105pk \u2022 THE CHENILLE KRAFT COMPANY Regular Stems, 6\u201d X 4Mm, 100/Pack \u2022 Chenillekraft Round Wood Paint Brush Set - 24 Brush[es] - Nickel Plated Ferrule - Wood Handle (CKC5172) \u2022 Shoplet Best Value Kit - ChenilleKraft Assorted Brush Starter Set (CKC5180)\nThus, for the brand \u2018Chennile Kraft\u2019, we add the following entries to the normalization dictionary:\n1. {\u2018Chenille Kraft\u2019: \u2018Chenille Kraft\u2019} 2. {\u2018THE CHENILLE KRAFT COMPANY\u2019: \u2018Chenille\nKraft\u2019} 3. {\u2018Chenillekraft\u2019: \u2018Chenille Kraft\u2019} 4. {\u2018ChenilleKraft\u2019, \u2018Chenille Kraft\u2019}"}, {"heading": "8.2 Blacklist", "text": "In addition to the normalization dictionary, we also maintain a list of terms not known to be brand names. Typically, the list consists of terms from product titles where no brand name is present. This list is enlarged whenever the brand extraction process produces an unacceptable value. We will provide more details in the next section."}, {"heading": "8.3 Manual feedback", "text": "In this subsection, we consider the case where brand extraction is done in batch mode. During a single run, every product with a missing brand value within a category for which a brand facet is applicable is a candidate for extraction.\nWe follow the following procedure to obtain the final result set R. Suppose the brand extraction algorithm runs on n items indexed 1, 2, ..., n. Let the prediction for item i be denoted brand(i) and denote the normalization dictionary2 and blacklist as N and B respectively. For each predicted value not in N we maintain a dictionary F which tracks the count of a given value encountered thus far. We will ignore predicted values that are either not in the normalization dictionary or which do not meet a predetermined threshold frequency f in a given batch.\nfor i = 1 to n if the brand(i) \u2208 N\nadd (i,N(brand(i))) to R else\nF (brand(i)) = F (brand(i)) + 1 for brand in F\nif F (brand) > f Randomly sample m (product title, brand) pairs for manual validation of newly discovered brands\nWe used the values of f = 30 and m = 5.\n2We use the same symbol to denote both the normalization dictionary and the normalization function from section 1.2.\nThe sampled (product title, brand) pairs are sent to analysts for verifying newly discovered brand values. The analyst make the following decisions for each suggested brand:\n\u2022 If the predicted brand value vp is correct for each of the m items: 1. Suggest a normalized form vn for the brand name. 2. Add the pair (vp, vn) to N and for any index i with brand(i) = vp, add (i, vn) to R. \u2022 Else\n1. If the predicted value vp is deemed unlikely to be a brand name, vp is added to B (for instance in the case of unbranded items). 2. If the product title contains a legitimate brand name then the product title is annotated with this brand name and added to the training set to build a new model.\nIt should be noted here that a large number of predictions have a high item count, in some cases of the order of tens of thousands. Thus, the number of predictions needed to be validated by analysts is usually a very small fraction of the number of products with predictions."}, {"heading": "9. RESULTS", "text": ""}, {"heading": "9.1 Metrics", "text": "Now we define the metrics used to measure the performance of the brand extraction algorithm. Let n be the number of products on which we run the algorithm. Let nTB be the number of products which have a true brand value other than \u2018unbranded\u2019. Let nPB be the number of products which have a predicted brand value other than \u2018unbranded\u2019. Let c be the number of predictions that are correct and which have a brand value other than \u2018unbranded\u2019. Then we define\nPrecision = P = c\nnPB\nand\nRecall = R = c\nnTB .\nThe motivation for computing precision and recall over only items with brand value other than \u2018unbranded\u2019 stems from the fact that \u2018unbranded\u2019 value is not displayed in facets and thus these metrics more accurately represent the quality of the brand facet. Finally we define F1 measure as usual:\nF1 = 2PR\nP +R\nFor diagnostic purposes we also include the metric - \u2018Label Accuracy\u2019 which is the fraction of tokens in the data set that are assigned the correct BIO encoding labels i.e. {B-brand, I-brand, O}. Although by itself a high label accuracy does not imply a good sequence labeling, we can get meaningful information from this metric when used in combination with precision and recall.\nAdditionally, for the LCCRF model, we define Precision and Recall as functions of the threshold \u03b8 of the conditional probability of the predicted label sequence [11]. Let XT denote the set of all sequences in the test set. For x\u0303 \u2208 XT , suppose the LCCRF model predicts a label sequence y\u0303 and let the correct label sequence for x\u0303 be y\u0303t. Let ol denote a label sequence of length l in which every label is \u2018O\u2019 (note\nthat this translates to an output brand label of \u2018unbranded\u2019). Let I [\u00b7] denote the indicator function. Then we define\nPrecision(\u03b8) = P (\u03b8) =\n\u03a3x\u0303\u2208XT I [(Pr(y\u0303|x\u0303) \u2265 \u03b8) and y\u0303 6=o|y| and y\u0303 = y\u0303t]\n\u03a3x\u0303\u2208XT I [(Pr(y\u0303|x\u0303) \u2265 \u03b8) and y\u0303 6=o|y\u0303|]\nand\nRecall(\u03b8) = R(\u03b8) =\n\u03a3x\u0303\u2208XT I [(Pr(y\u0303|x\u0303) \u2265 \u03b8) and y\u0303 6=o|y| and y\u0303 = y\u0303t]\n\u03a3x\u0303\u2208XT I [y\u0303t 6=o|y\u0303t|] .\nNote that in particular, P (\u03b8 = 0) = P and R(\u03b8 = 0) = R."}, {"heading": "9.2 Model performance", "text": "We built a training set consisting of 61,374 product titles from a selected set of product categories, annotated with a brand attribute value. We performed a 10-fold cross validation for hyperparameter selection and computing model metrics.\nWe compare the results of SP and LCCRF models to a number of baseline approaches.\n9.2.1 Nearest Neighbors models The nearest neighbors model was chosen for comparison\nfor two main reasons: 1. The number of distinct brand labels in the training set\nis 25,851. 2. The number of times a particular brand label appears\nin the training set is between 1-9. The actual distribution of the frequency of the brand labels in the training set is shown below.\nGiven the large number of classes and since over 95% of labels appear 3 times or less in the training set, there is very little training data per class to train an effective model using text classification techniques.\nTo obtain the nearest neighbors benchmarks, we used a bag of words model where each token was encoded using its tf-idf score and the nearest neighbors were obtained using cosine similarity. Given the frequency of labels shown above,\nwe compare our results to those of k nearest neighbors models with k equal to 1 (1NN) and 3 (3NN). For 3NN, in case all three closest neighbors corresponded to distinct labels, the label with the highest cosine similarity was chosen i.e. the result would coincide with that of 1NN in this case. All results were obtained using a 10-fold cross validation.\nNote that since some labels appear only once, if such a label occurs in the test set during cross validation, both 1NN and 3NN would be incapable of producing the correct result. However, considering one of the goals of the attribute extraction system is to discover new attribute values, this situation closely models the reality of the data.\n9.2.2 Dictionary based approaches\nFor dictionary based benchmarks, we compile a lexicon of brands that appear in the training data with some preprocessing (lowercasing and tokenization). Given a test title, we perform the same preprocessing and we search for the presence of any brand from the lexicon in the title. We show the results from two approaches that were obtained from the heuristic used to resolve multiple matches:\n1. Dict-max: In case of multiple matches, we choose the match with the largest number of characters. 2. Dict-first: In case of multiple matches, we choose the match that appears earliest in the title.\n9.2.3 Hidden Markov Model\nFor an input sequence x = x1, x2, ..., xm, and a label sequence y = y1, y2, ..., ym, a second-order Hidden Markov Model (HMM) defines the joint probability of the input and label sequences as follows\nPr(x1, x2, ..., xm, y1, y2, ..., ym) = \u03a0m+1i=1 Pr(yi|yi\u22122, yi\u22121)\u03a0 m i=1Pr(xi|yi)\nwhere y\u22121, y0 = START and ym+1 = STOP are special labels. The probabilities on the right hand side of the above equation are parameters of the HMM and can be estimated from a training set of input and label sequences using maximum likelihood estimation. Given a test sequence, the label sequence that maximizes the above joint probability is chosen.\nIn estimating probabilities for unknown words, we mapped them to one of several morphological classes similar to [2].\n9.2.4 Model comparisons\nThe results for all models are shown below. The error margins indicate two standard errors.\nLCCRF and SP outperform the rest of the models and yield similar performance. The metrics for these models also meet business specifications at Walmart to power algorithmic solutions to extract product metdata.\nThe high label accuracy compared to the precision of the algorithm indicates a crucial fact that we exploited for post processing. In a large number of cases even when the predicted sequence labeling was incorrect, almost all the tokens still had the right label. What we observed was that the false positive brand value predictions typically were either subsequences or supersequences of the expected unnormalized brand. Due to this, we were able to employ the normalization scheme to improve both precision and recall further.\nWe will illustrate this with an example. Consider the following product title\nPlum Island Silver SC-007 Sterling Silver Fairy Piece Ear Cuff\nthat contains the brand name \u2018Plum Island Silver\u2019. Suppose the sequence labeling algorithm incorrectly extracts the brand value as \u2018Plum Island\u2019. In such a case, we add the pair (\u2018Plum Island\u2019, \u2018Plum Island Silver\u2019) to the normalization dictionary while adding the above title in the training set with the correctly annotated brand. In a subsequent run, it\u2019s highly likely that either the algorithm adapts and correctly predicts the brand or still returns the previous truncated value. However, even in the latter case, the normalization scheme will correctly pick the desired normalized value."}, {"heading": "10. OTHER ATTRIBUTES", "text": "The system is easily extended to a number of other attributes where we may not have a pre compiled list of possible attribute values or when such a list is large and may undergo frequent changes. Some examples include manufacturer specific model numbers and names, fictional characters that inspire children\u2019s toys, sports team and league names for sports branded apparel and memorabilia, product lines, etc. The approach presented above is readily amenable for the extraction of such attributes. Most of the methodology remains similar to that used for brands. We were able to use the same algorithms, similar set of features, distant supervision to produce annotated product titles for training and normalization schemes for standardization of variations of attributes values to build models with high precision and recall for such attributes."}, {"heading": "11. REAL WORLD PERFORMANCE", "text": "We used the approach presented above to extract brands for a subset of products in selected departments. Early results from algorithmic extraction were manually validated by analysts. The normalization dictionary was regularly updated based on analyst feedback. For the manually validated batches, post normalization precision varied between 92%-95% showing an improvement between 1%-3% over algorithmically extracted values. Sample validations of the products categorized as unbranded revealed that over 98% of those were indeed true negatives. The algorithm was able to \u2018discover\u2019 over a thousand brand values which were not part of the Walmart brand database at that point.\nWe estimated the impact of brand value extraction on product discoverability using the R package CausalImpact [4]. For the following analysis, the brand attribute value was extracted and used to augment product data for 272,697 products on the same day. We measured the impressions for this set of products during 44 days prior and 27 days following 3 this intervention conditioned on the brand facet being applied. The results of the analysis are shown below.\nThe plot consists of three panels. The solid line in the top panel shows the actual daily impressions for the set of products. The first vertical dotted line indicates the day of intervention (products augmented with brand metadata). The second vertical line marks the end of the measurement. The blue dotted time series in the first panel is the predicted counterfactual - how the impressions would have tracked in the absence of the intervention. The second panel shows the difference between the observed impressions and the counterfactual predictions while the final plot shows the cumulative difference. The synthetical control was constructed from products from similar categories for which no brand\n3these choices were motivated by the choice of series to create a synthetic control\nmetadata was augmented. The model predicts over 250K additional impressions during the test period for the set of items for which the brand metadata was supplied. Similar analysis for other attributes for which attribute extraction was employed typically suggests a positive impact on impressions and depending on the attribute also on other metrics of interest such as clicks, add to cart rate and orders.\nSo far, the attribute extraction system has been deployed for extracting over 20 attributes. We build a separate model for each attribute for several reasons. A given attribute is typically applicable to only a particular set of categories (for e.g. brand may not be an important attribute for books while sports team may not be applicable for electronic tablet devices). Building a separate model for each attribute allows us to independently train each model and tune the performance based on business needs. The size of the training set varies between a few hundred titles to tens of thousands depending on the attribute."}, {"heading": "12. CONCLUSION", "text": "In this paper, we introduced the problem of extracting attribute values from product titles in order to augment product metdata for eCommerce catalogs. We discussed the importance of attribute extraction for retail websites. We examined the challenges associated with this task especially when the product catalog is large. Attribute extraction from product titles was modeled as a sequence labeling problem. We also illustrated a method to leverage existing products tagged with attribute values to build the initial training data set without manual labeling. The experimental results show that SP or LCCRF models combined with a curated normalization scheme provide an efficacious mechanism for tagging products with certain attribute values with high precision and recall."}, {"heading": "13. REFERENCES", "text": "[1] Harith Alani, Sanghee Kim, David E. Millard, Mark J. Weal, Wendy Hall, Paul H. Lewis, and Nigel R. Shadbolt. Automatic ontology-based knowledge extraction from web documents. IEEE Intelligent Systems, 18(1):14\u201321, 2003. [2] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. An algorithm that learns what\u2019s in a name. Machine Learning, 34(1\u20133):211\u2013231, February 1999. [3] Eric Brill. A simple rule-based part of speech tagger. Proceedings of the Third Conference on Applied Natural Language Processing, pages 152\u2013155, 1992. [4] Kay H Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L Scott, et al. Inferring causal impact using bayesian structural time-series models. The Annals of Applied Statistics, 9(1):247\u2013274, 2015. [5] Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. Domain adaptation of rule-based annotators for namedentity recognition tasks. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1002\u20131012, 2010. [6] Michael Collins. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, 10:1\u20138, 2002.\n[7] Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91\u2013134, 2005. [8] Dimitra Farmakiotou, Vangelis Karkaletsis, John Koutsias, George Sigletos, Constantine D Spyropoulos, and Panagiotis Stamatopoulos. Rule-based named entity recognition for greek financial texts. In Proceedings of the Workshop on Computational lexicography and Multimedia Dictionaries (COMLEX 2000), pages 75\u201378, 2000. [9] R. Ghani, K. Probst, Y. Liu, M. Krema, and A. Fano. Text mining for product attribute extraction. SIGKDD, 8(1):41\u201348, 2006. [10] Yoav Goldberg and Michael Elhadad. Learning sparser perceptron models. Technical report, Ben Gurion University of the Negev, 2011. [11] Anitha Kannan, Patha Talukdar, Nikhil Rasiwasia, and Qifa Ke. Improving product classification using images. Data Mining (ICDM), 2011 IEEE 11th International Conference on, pages 310\u2013319, 2011. [12] Jun\u2019ichi Kazama and Kentaro Torisawa. Exploiting wikipedia as external knowledge for named entity recognition. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 698\u2013 707, 2007. [13] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: probabilistic models for segmenting and labeling sequence data. International Conference on Machine Learning, 2001. [14] Mitchell P Marcus and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2), 1993. [15] Andrei Mikheev, Marc Moens, and Clair Grover. Named entity recognition without gazetteers. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pages 1\u20138. Association for Computational Linguistics, 1999. [16] David Nadeau and Satoshi Sekine. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3\u201326, 2007. [17] Ana-Maria Popescu and Oren Etzioni. Extracting product features and opinions from reviews. Natural language processing and text mining, pages 9\u201328, 2007. [18] Duangmanee Putthividhya and Junling Hu. Bootstrapped named entity recognition for product attribute extraction. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1557\u20131567, 2011. [19] Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. Named entity recognition in tweets: An experimental study. Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1524\u2013 1534, 2011. [20] Helmut Schmid. Probabilistic part-of-speech tagging using decision trees. Proceedings of the international conference on new methods in language processing, 12:44\u201349, 1994. [21] Maksim Tkachenko and Andrey Simanovsky. Named entity recognition: Exploring features. In Proceedings of KONVENS, pages 118\u2013127, 2012."}], "references": [{"title": "Automatic ontology-based knowledge extraction from web documents", "author": ["Harith Alani", "Sanghee Kim", "David E. Millard", "Mark J. Weal", "Wendy Hall", "Paul H. Lewis", "Nigel R. Shadbolt"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "An algorithm that learns what\u2019s in a name", "author": ["Daniel M. Bikel", "Richard Schwartz", "Ralph M. Weischedel"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "A simple rule-based part of speech tagger", "author": ["Eric Brill"], "venue": "Proceedings of the Third Conference on Applied Natural Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Inferring causal impact using bayesian structural time-series models", "author": ["Kay H Brodersen", "Fabian Gallusser", "Jim Koehler", "Nicolas Remy", "Steven L Scott"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Domain adaptation of rule-based annotators for namedentity recognition tasks", "author": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Frederick Reiss", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Unsupervised named-entity extraction from the web: An experimental study", "author": ["Oren Etzioni", "Michael Cafarella", "Doug Downey", "Ana- Maria Popescu", "Tal Shaked", "Stephen Soderland", "Daniel S. Weld", "Alexander Yates"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Rule-based named entity recognition for greek financial texts", "author": ["Dimitra Farmakiotou", "Vangelis Karkaletsis", "John Koutsias", "George Sigletos", "Constantine D Spyropoulos", "Panagiotis Stamatopoulos"], "venue": "In Proceedings of the Workshop on Computational lexicography and Multimedia Dictionaries (COMLEX", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Text mining for product attribute extraction", "author": ["R. Ghani", "K. Probst", "Y. Liu", "M. Krema", "A. Fano"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning sparser perceptron models", "author": ["Yoav Goldberg", "Michael Elhadad"], "venue": "Technical report, Ben Gurion University of the Negev,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Improving product classification using images", "author": ["Anitha Kannan", "Patha Talukdar", "Nikhil Rasiwasia", "Qifa Ke"], "venue": "Data Mining (ICDM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Named entity recognition without gazetteers", "author": ["Andrei Mikheev", "Marc Moens", "Clair Grover"], "venue": "In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "A survey of named entity recognition and classification", "author": ["David Nadeau", "Satoshi Sekine"], "venue": "Lingvisticae Investigationes,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Extracting product features and opinions from reviews", "author": ["Ana-Maria Popescu", "Oren Etzioni"], "venue": "Natural language processing and text mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Bootstrapped named entity recognition for product attribute extraction", "author": ["Duangmanee Putthividhya", "Junling Hu"], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid"], "venue": "Proceedings of the international conference on new methods in language processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Named entity recognition: Exploring features", "author": ["Maksim Tkachenko", "Andrey Simanovsky"], "venue": "In Proceedings of KONVENS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "Some gazetteer based approaches are discussed in [16] in the context of named entity recognition problems.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "Rule based approaches have had success in certain named entity recognition tasks [5], [8], [15].", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "Rule based approaches have had success in certain named entity recognition tasks [5], [8], [15].", "startOffset": 86, "endOffset": 89}, {"referenceID": 14, "context": "Rule based approaches have had success in certain named entity recognition tasks [5], [8], [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "[2] discuss the named entity extraction problem to identify location names, person names, named organizations and a few other entities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] built a system for mining named entities from tweets - which is another example of text that significantly deviates from fluent prose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Eric Brill [3] introduced a simple rule based tagger for learning part of speech which was shown to have an error rate of 7.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Schmid [20] illustrates a probabilistic tagger in which transition probabilities are estimated using a decision tree and which achieves an accuracy of 96.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "[1] present a system to extract knowledge about artists from web pages leveraging a curated ontology.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Kazama and Torisawa [12] use features relying on Wikipedia information to generate a Conditional Random Fields (CRF) based model for named entity recognition.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "[7] describe unsupervised techniques to extract relationships between entities from web documents.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] discuss the representation of retail products as attribute-value pairs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Popescu and Etzioni [17] extend their system described in [7] to the problem of mining product features", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "Popescu and Etzioni [17] extend their system described in [7] to the problem of mining product features", "startOffset": 58, "endOffset": 61}, {"referenceID": 17, "context": "Putthividhya and Hu [18] designed a system for extracting product attributes from short listing titles such as those found on eBay.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "The tags that appear in Penn Treebank [14] include DT (determiner), JJ (adjective), NN (noun), VB (verb) and IN (preposition).", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "Michael Collins presented the Structured Perceptron learning algorithm in [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Parameter averaging reduces variance providing a regularization effect and improves performance ([6], [10]).", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "Parameter averaging reduces variance providing a regularization effect and improves performance ([6], [10]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Conditional Random Fields (CRF) is a probabilistic structured labeling algorithm introduced by Lafferty, McCallum and Pereira in [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "The choice of feature functions was motivated from a variety of literature on this subject including [21] and [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "The choice of feature functions was motivated from a variety of literature on this subject including [21] and [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Additionally, for the LCCRF model, we define Precision and Recall as functions of the threshold \u03b8 of the conditional probability of the predicted label sequence [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "In estimating probabilities for unknown words, we mapped them to one of several morphological classes similar to [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "We estimated the impact of brand value extraction on product discoverability using the R package CausalImpact [4].", "startOffset": 110, "endOffset": 113}], "year": 2016, "abstractText": "This paper presents a named entity extraction system for detecting attributes in product titles of eCommerce retailers like Walmart. The absence of syntactic structure in such short pieces of text makes extracting attribute values a challenging problem. We find that combining sequence labeling algorithms such as Conditional Random Fields and Structured Perceptron with a curated normalization scheme produces an effective system for the task of extracting product attribute values from titles. To keep the discussion concrete, we will illustrate the mechanics of the system from the point of view of a particular attribute brand. We also discuss the importance of an attribute extraction system in the context of retail websites with large product catalogs, compare our approach to other potential approaches to this problem and end the paper with a discussion of the performance of our system for extracting attributes.", "creator": "LaTeX with hyperref package"}}}