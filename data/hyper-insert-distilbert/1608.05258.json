{"id": "1608.05258", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "Parameter Learning for Log-supermodular Distributions", "abstract": "we strongly consider log - finite supermodular models on binary variables, which are independent probabilistic analytic models with additive negative log - densities coefficients which are submodular. basically these models provide complete probabilistic interpretations of common combinatorial optimization with tasks with such instance as image segment segmentation. in this paper, we focus primarily on parameter profile estimation in the analytic models from known upper - threshold bounds dependent on the intractable log - partition function. indeed we show evidence that the partial bound based on completely separable statistical optimization on the base kernel polytope subset of detecting the submodular function is always vastly inferior to constructing a corresponding bound calculation based on \" simultaneous perturb - proof and - diffusion map \" ideas. furthermore then, to learn model parameters, one given knowing that our approximation of the log - partition function theory is only an objective expectation ( over our own randomization ), we immediately use either a stochastic subgradient technique calculated to maximize providing a corresponding lower - marginal bound on the log - infinite likelihood. this scheme can also be simultaneously extended to construct conditional polynomial maximum functional likelihood. we further illustrate our impressive new mathematical results namely in obtaining a loose set of experiments results in binary structured image model denoising, and where we jointly highlight the resulting flexibility of a probabilistic stability model strategy to learn with missing query data.", "histories": [["v1", "Thu, 18 Aug 2016 13:55:41 GMT  (24kb)", "http://arxiv.org/abs/1608.05258v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tatiana shpakova", "francis r bach"], "accepted": true, "id": "1608.05258"}, "pdf": {"name": "1608.05258.pdf", "metadata": {"source": "CRF", "title": "Parameter Learning for Log-supermodular Distributions", "authors": ["Tatiana Shpakova"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 25\n8v 1\n[ st\nat .M\nL ]\n1 8"}, {"heading": "1 Introduction", "text": "Submodular functions provide efficient and flexible tools for learning on discrete data. Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14]. The key benefit of submodularity is the ability to model notions of diminishing returns, and the availability of exact minimization algorithms and approximate maximization algorithms with precise approximation guarantees [12].\nIn practice, it is not always straightforward to define an appropriate submodular function for a problem at hand. Given fully-labeled data, e.g., images and their foreground/background segmentations in image segmentation, structured-output prediction methods such as the structured-SVM may be used [18]. However, it is common (a) to have missing data, and (b) to embed submodular function minimization within a larger model. These are two situations well tackled by probabilistic modelling.\nLog-supermodular models, with negative log-densities equal to a submodular function, are a first important step toward probabilistic modelling on discrete data with submodular functions [5]. However, it is well known that the log-partition function is intractable in such models. Several bounds have been proposed, that are accompanied with variational approximate inference [6]. These bounds are based on the submodularity of the negative log-densities. However, the parameter learning (typically by maximum likelihood), which is a key feature of probabilistic modeling, has not been tackled yet. We make the following contributions:\n\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].\n\u2013 In Section 4.1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.\n\u2013 In Section 4.2, given that the bound based on \u201cperturb-and-MAP\u201d ideas is an expectation (over our own randomization), we propose to use a stochastic subgradient technique to maximize the lower-bound on the log-likelihood, which can also be extended to conditional maximum likelihood.\n\u2013 In Section 5, we illustrate our new results on a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model for learning with missing data."}, {"heading": "2 Submodular functions and log-supermodular models", "text": "In this section, we review the relevant theory of submodular functions and recall typical examples of log-supermodular distributions."}, {"heading": "2.1 Submodular functions", "text": "We consider submodular functions on the vertices of the hypercube {0, 1}D. This hypercube representation is equivalent to the power set of {1, . . . , D}. Indeed, we can go from a vertex of the hypercube to a set by looking at the indices of the components equal to one and from set to vertex by taking the indicator vector of the set.\nFor any two vertices of the hypercube, x, y \u2208 {0, 1}D, a function f : {0, 1}D \u2192 R is submodular if f(x)+f(y) > f(min{x, y})+f(max{x, y}), where the min and max operations are taken componentwise and correspond to the intersection and union of the associated sets. Equivalently, the function x 7\u2192 f(x + ei) \u2212 f(x), where ei \u2208 RD is the i-th canonical basis vector, is non-increasing. Hence, the notion of diminishing returns is often associated with submodular functions. Most widely used submodular functions are cuts, concave functions of subset cardinality, mutual information, set covers, and certain functions of eigenvalues of submatrices [1, 7]. Supermodular functions are simply negatives of submodular functions.\nIn this paper, we are going to use a few properties of such submodular functions (see [1, 7] and references therein). Any submodular function f can be extended from {0, 1}D to a convex function on RD, which is called the Lov\u00e1sz extension. This extension has the same value on {0, 1}D, hence we use the same notation f . Moreover, this function is convex and piecewise linear, which implies the existence of a polytope B(f) \u2282 RD, called the base polytope, such that for all x \u2208 RD, f(x) = maxs\u2208B(f) x\n\u22a4s, that is, f is the support function of B(f). The Lov\u00e1sz extension f and the base polytope B(f) have explicit expressions that are, however, not relevant to this paper. We will only use the fact that f can be efficiently minimized on {0, 1}D, by a variety of generic algorithms, or by more efficient dedicated ones for subclasses such as graph-cuts."}, {"heading": "2.2 Log-supermodular distributions", "text": "Log-supermodular models are introduced in [5] to model probability distributions on a hypercube, x \u2208 {0, 1}D, and are defined as\np(x) = 1\nZ(f) exp(\u2212f(x)),\nwhere f : {0, 1}D \u2192 R is a submodular function such that f(0) = 0 and the partition function is Z(f) = \u2211\nx\u2208{0,1}D exp(\u2212f(x)). It is more convenient to deal with the convex log-partition function\nA(f) = logZ(f) = log \u2211 x\u2208{0,1}D exp(\u2212f(x)). In general, calculation of the partition function Z(f) or the log-partition function A(f) is intractable, as it includes simple binary Markov random fields\u2014 the exact calculation is known to be #P -hard [10]. In Section 3, we review upper-bounds for the log-partition function."}, {"heading": "2.3 Examples", "text": "Essentially, all submodular functions used in the minimization context can be used as negative log-densities [5, 6]. In computer vision, the most common examples are graph-cuts, which are essentially binary Markov random fields with attractive potentials, but higher-order potentials have been considered as well [11]. In our experiments, we use graph-cuts, where submodular function minimization may be performed with max-flow techniques and is thus efficient [4]. Note that there are extensions of submodular functions to continuous domains that could be considered as well [2]."}, {"heading": "3 Upper-bounds on the log-partition function", "text": "In this section, we review the main existing upper-bounds on the log-partition function for logsupermodular densities. These upper-bounds use several properties of submodular functions, in particular, the Lov\u00e1sz extension and the base polytope. Note that lower bounds based on submodular maximization aspects and superdifferentials [5] can be used to highlight the tightness of various bounds, which we present in Figure 1."}, {"heading": "3.1 Base polytope relaxation with L-Field [5]", "text": "This method exploits the fact that any submodular function f(x) can be lower bounded by a modular function s(x), i.e., a linear function of x \u2208 {0, 1}D in the hypercube representation. The submodular function and its lower bound are related by f(x) = maxs\u2208B(f) s \u22a4x, leading to:\nA(f) = log \u2211\nx\u2208{0,1}D exp (\u2212f(x)) = log\n\u2211\nx\u2208{0,1}D min\ns\u2208B(f) exp (\u2212s\u22a4x),\nwhich, by swapping the sum and min, is less than\nmin s\u2208B(f)\nlog \u2211 x\u2208{0,1}D exp (\u2212s\u22a4x) = min s\u2208B(f)\nD \u2211\nd=1\nlog (1 + e\u2212sd) def = AL-field(f). (1)\nSince the polytope B(f) is tractable (through its membership oracle or by maximizing linear functions efficiently), the bound AL-field(f) is tractable, i.e., computable in polynomial time. Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u00b5d\u2208[0,1]\u2212\u00b5dsd \u2212 \u00b5d log\u00b5d \u2212 (1 \u2212 \u00b5d) log(1\u2212 \u00b5d), leading to:\nAL-field(f) = min s\u2208B(f) max \u00b5\u2208[0,1]D \u2212\u00b5\u22a4s+H(\u00b5) = max \u00b5\u2208[0,1]D H(\u00b5)\u2212 f(\u00b5),\nwhere H(\u00b5) = \u2212 \u2211D\nd=1\n{ \u00b5d log\u00b5d + (1 \u2212 \u00b5d) log(1 \u2212 \u00b5d) }\n. This shows in particular the convexity of f 7\u2192 AL-field(f). Finally, [6] shows the remarkable result that the minimizer s \u2208 B(f) may be obtained by minimizing a simpler function on B(f), namely the squared Euclidean norm, thus leading to algorithms such as the minimum-norm-point algorithm [7]."}, {"heading": "3.2 \u201cPertub-and-MAP\u201d with logistic distributions", "text": "Estimating the log-partition function can be done through optimization using \u201cpertub-and-MAP\u201d ideas. The main idea is to perturb the log-density, find the maximum a-posteriori configuration (i.e., perform optimization), and then average over several random perturbations [9, 17, 19].\nThe Gumbel distribution on R, whose cumulative distribution function is F (z) = exp(\u2212 exp(\u2212(z+ c))), where c is the Euler constant, is particularly useful. Indeed, if {g(y)}y\u2208{0,1}D is a collection of independent random variables g(y) indexed by y \u2208 {0, 1}D, each following the Gumbel distribution, then the random variable maxy\u2208{0,1}D g(y)\u2212 f(y) is such that we have logZ(f) = Eg [ maxy\u2208{0,1}D {g(y)\u2212 f(y)} ]\n[9, Lemma 1]. The main problem is that we need 2D such variables, and a key contribution of [9] is to show that if we consider a factored collection {gd(yd)}yd\u2208{0,1},d=1,...,D of i.i.d. Gumbel variables, then we get an upper-bound on the log partition-function, that is, logZ(f) \u2264 Egmaxy\u2208{0,1}D { \u2211D d=1 gd(yd)\u2212 f(y)}.\nWriting gd(yd) = [gd(1)\u2212 gd(0)]yd + gd(0) and using the fact that (a) gd(0) has zero expectation and (b) the difference between two independent Gumbel distributions has a logistic distribution (with cumulative distribution function z 7\u2192 (1 + e\u2212z)\u22121) [15], we get the following upper-bound:\nALogistic(f) = Ez1,...,zD\u223clogistic [\nmax y\u2208{0,1}D\n{z\u22a4y \u2212 f(y)} ] , (2)\nwhere the random vector z \u2208 RD consists of independent elements taken from the logistic distribution. This is always an upper-bound on A(f) and it uses only the fact that submodular functions are efficient to optimize. It is convex in f as an expectation of a maximum of affine functions of f ."}, {"heading": "3.3 Comparison of bounds", "text": "In this section, we show that AL-field(f) is always dominated by ALogistic(f). This is complemented by another result within the maximum likelihood framework in Section 4.\nProposition 1. For any submodular function f : {0, 1}D \u2192 R, we have: A(f) 6 ALogistic(f) 6 AL-field(f). (3)\nProof. The first inequality was shown by [9]. For the second inequality, we have:\nALogistic(f) = Ez [\nmax y\u2208{0,1}D\nz\u22a4y \u2212 f(y) ]\n= Ez [\nmax y\u2208{0,1}D z\u22a4y \u2212 min s\u2208B(f) s\u22a4y ] from properties of the base polytope B(f),\n= Ez [\nmax y\u2208{0,1}D min s\u2208B(f)\nz\u22a4y \u2212 s\u22a4y ] ,\n= Ez [\nmin s\u2208B(f) max y\u2208{0,1}D\nz\u22a4y \u2212 s\u22a4y ] by convex duality,\n6 min s\u2208B(f) Ez\n[\nmax y\u2208{0,1}D\n(z \u2212 s)\u22a4y ] by swapping expectation and minimization,\n= min s\u2208B(f) Ez\n[ \u2211D d=1(zd \u2212 sd)+ ] by explicit maximization,\n= min s\u2208B(f)\n[ \u2211D d=1 Ezd(zd \u2212 sd)+ ] by using linearity of expectation,\n= min s\u2208B(f)\n[ \u2211D\nd=1 \u222b +\u221e \u2212\u221e (zd \u2212 sd)+P (zd)dzd ] by definition of expectation,\n= min s\u2208B(f)\n[ \u2211D\nd=1 \u222b +\u221e sd (zd \u2212 sd) e \u2212z d (1+e\u2212zd )2 dzd ] by substituting the density function,\n= min s\u2208B(f)\n\u2211D d=1 log(1 + e \u2212sd) which leads to the desired result.\nIn the inequality above, since the logistic distribution has full support, there cannot be equality. However, if the base polytope is such that, with high probability \u2200d, |sd| \u2265 |zd|, then the two bounds are close. Since the logistic distribution is concentrated around zero, we have equality when |sd| is large for all d and s \u2208 B(f).\nTheoretical complexity of AL-field and Alogistic. The logistic bound Alogistic can be computed if there is efficient MAP-solver for submodular functions (plus a modular term). In this case, the divide-and-conquer algorithm can be applied for L-Field [5]. Thus, the complexity is dedicated to the minimization of O(|V |) problems. Meanwhile, for the method based on logistic samples, it is necessary to solve M optimization problems. In our empirical bound comparison (next paragraph), running time was the same for both methods. Note however that for parameter learning, we need a single SFM problem per gradient iteration (and not M).\nEmpirical comparison of AL-field and Alogistic. We compare the upper-bounds on the logpartition function AL-field and Alogistic, with the setup used by [5]. We thus consider data from a Gaussian mixture model with 2 clusters in R2. The centers are sampled from N([3, 3], I) and N([\u22123,\u22123], I), respectively. Then we sampled n = 50 points for each cluster. Further, these 2n points are used as nodes in a complete weighted graph, where the weight between points x and y is equal to e\u2212c||x\u2212y||.\nWe consider the graph cut function associated to this weighted graph, which defines a logsupermodular distribution. We then consider conditional distributions, one for each k = 1, . . . , n, on the events that at least k points from the first cluster lie on the one side of the cut and at least k points from the second cluster lie on the other side of the cut. For each conditional distribution, we evaluate and compare the two upper bounds.\nIn Figure 1, we show various bounds on A(f) as a function of the number on conditioned pairs. The logistic upper bound is obtained using 100 logistic samples: the logistic upper-bound Alogistic is close to the superdifferential lower bound from [5] and is indeed significantly lower than the bound AL-field."}, {"heading": "3.4 From bounds to approximate inference", "text": "Since linear functions are submodular functions, given any convex upper-bound on the log-partition function, we may derive an approximate marginal probability for each xd \u2208 {0, 1}. Indeed, following [9], we consider an exponential family model p(x|t) = exp(\u2212f(x) + s\u22a4x\u2212A(f \u2212 t)), where f \u2212 t\nis the function x 7\u2192 f(x)\u2212 t\u22a4x. When f is assumed to be fixed, this can be seen as an exponential family with the base measure exp(\u2212f(x)), sufficient statistics x, and A(f \u2212 t) is the log-partition function. It is known that the expectation of the sufficient statistics under the exponential family model Ep(x|t)x is the gradient of the log-partition function [23]. Hence, any approximation of this log-partition gives an approximation of this expectation, which in our situation is the vector of marginal probabilities that an element is equal to 1.\nFor the L-field bound, at t = 0, we have \u2202tdAL-field(f \u2212 t) = \u03c3(s\u2217d), where s\u2217 is the minimizer of \u2211D\nd=1 log(1 + e \u2212sd), thus recovering the interpretation of [6] from another point of view.\nFor the logistic bound, this is the inference mechanism from [9], with \u2202tdAlogistic(f \u2212 t) = Ezy \u2217(z), where y\u2217(z) is the maximizer of maxy\u2208{0,1}D z \u22a4y \u2212 f(y). In practice, in order to perform approximate inference, we only sample M logistic variables. We could do the same for parameter learning, but a much more efficient alternative, based on mixing sampling and convex optimization, is presented in the next section."}, {"heading": "4 Parameter learning through maximum likelihood", "text": "An advantage of log-supermodular probabilistic models is the opportunity to learn the model parameters from data using the maximum-likelihood principle. In this section, we consider that we are given N observations x1, . . . , xN \u2208 {0, 1}D, e.g., binary images such as shown in Figure 2.\nWe consider a submodular function f(x) represented as f(x) = \u2211K k=1 \u03b1kfk(x) \u2212 t\u22a4x. The modular term t\u22a4x is explicitly taken into account with t \u2208 RD, and K base submodular functions are assumed to be given with \u03b1 \u2208 RK+ so that the function f remains submodular. Assuming the data x1, . . . , xN are independent and identically (i.i.d.) distributed, then maximum likelihood is equivalent to minimizing:\nmin \u03b1\u2208RK\n+ , t\u2208RD\n\u2212 1 N\nN \u2211\nn=1\nlog p(xn|\u03b1, t) = min \u03b1\u2208RK\n+ , t\u2208RD\n1\nN\nN \u2211\nn=1\n{\nK \u2211\nk=1\n\u03b1kfk(xn)\u2212 t\u22a4xn +A(f) } ,\nwhich takes the particularly simple form\nmin \u03b1\u2208RK\n+ , t\u2208RD\nK \u2211\nk=1\n\u03b1k\n( 1\nN\nN \u2211\nn=1\nfk(xn) ) \u2212 t\u22a4 ( 1\nN\nN \u2211\nn=1\nxn\n)\n+A(\u03b1, t), (4)\nwhere we use the notation A(\u03b1, t) = A(f). We now consider replacing the intractable log-partition function by its approximations defined in Section 3."}, {"heading": "4.1 Learning with the L-field approximation", "text": "In this section, we show that if we replace A(f) by AL-field(f), we obtain a degenerate solution. Indeed, we have\nAL-field(\u03b1, t) = min s\u2208B(f)\nD \u2211\nd=1\nlog (1 + e\u2212sd) = min s\u2208B(\u2211K\nk=1 \u03b1KfK)\nD \u2211\nd=1\nlog (1 + e\u2212sd+td).\nThis implies that Eq. (4) becomes\nmin \u03b1\u2208RK\n+ , t\u2208RD\nmin s\u2208B( \u2211 K\nk=1 \u03b1KfK)\nK \u2211\nk=1\n\u03b1k\n( 1\nN\nN \u2211\nn=1\nfk(xn) ) \u2212 t\u22a4 ( 1\nN\nN \u2211\nn=1\nxn\n)\n+\nD \u2211\nd=1\nlog (1 + e\u2212sd+td).\nThe minimum with respect to td may be performed in closed form with td \u2212 sd = log \u3008x\u3009d1\u2212\u3008x\u3009d , where \u3008x\u3009 = 1\nN \u2211N n=1 xn. Putting this back into the equation above, we get the equivalent problem:\nmin \u03b1\u2208RK\n+\nmin s\u2208B( \u2211 K\nk=1 \u03b1KfK)\nK \u2211\nk=1\n\u03b1k\n( 1\nN\nN \u2211\nn=1\nfk(xn) ) \u2212 s\u22a4 ( 1\nN\nN \u2211\nn=1\nxn\n)\n+ const ,\nwhich is equivalent to, using the representation of f as the support function of B(f) for any submodular function:\nmin \u03b1\u2208RK\n+\nK \u2211\nk=1\n\u03b1k\n[\n1\nN\nN \u2211\nn=1\nfk(xn)\u2212 fk ( 1\nN\nN \u2211\nn=1\nxn )\n]\n.\nSince fk is convex, by Jensen\u2019s inequality, the linear term in \u03b1k is non-negative; thus maximum likelihood through L-field will lead to a degenerate solution where all \u03b1\u2019s are equal to zero."}, {"heading": "4.2 Learning with the logistic approximation with stochastic gradients", "text": "In this section we consider the problem (4) and replace A(f) by ALogistic(f):\nmin \u03b1\u2208RK\n+ , t\u2208RD\nK \u2211\nk=1\n\u03b1k\u3008fk(x)\u3009emp. \u2212 t\u22a4\u3008x\u3009emp. + Ez\u223clogistic [\nmax y\u2208{0,1}D\nz\u22a4y + t\u22a4y \u2212 K \u2211\nk=1\n\u03b1kf(y) ] , (5)\nwhere \u3008M(x)\u3009emp. denotes the empirical average of M(x) (over the data). Denoting by y\u2217(z, t, \u03b1) \u2208 {0, 1}D the maximizers of z\u22a4y + t\u22a4y \u2212 \u2211Kk=1 \u03b1kf(y), the objective function may be written:\nK \u2211\nk=1\n\u03b1k [ \u3008fk(x)\u3009emp. \u2212 \u3008fk(y\u2217(z, t, \u03b1))\u3009logistic ] \u2212 t\u22a4 [ \u3008x\u3009emp. \u2212 \u3008y\u2217(z, t, \u03b1)\u3009logistic] + \u3008z\u22a4y\u2217(z, t, \u03b1)\u3009logistic.\nThis implies that at optimum, for \u03b1k > 0, then \u3008fk(x)\u3009emp. = \u3008fk(y\u2217(z, t, \u03b1))\u3009logistic, while, \u3008x\u3009emp. = \u3008y\u2217(z, t, \u03b1)\u3009logistic, the expected values of the sufficient statistics match between the data and the optimizers used for the logistic approximation [9].\nIn order to minimize the expectation in Eq. (5), we propose to use the projected stochastic gradient method, not on the data as usually done, but on our own internal randomization. The algorithm then becomes, once we add weighted \u21132-regularization \u2126(t, \u03b1):\n\u2022 Input: functions fk, k = 1, . . . ,K, and expected sufficient statistics \u3008fk(x)\u3009emp. \u2208 R and \u3008x\u3009emp. \u2208 [0, 1]D, regularizer \u2126(t, \u03b1).\n\u2022 Initialization: \u03b1 = 0, t = 0\n\u2022 Iterations: for h from 1 to H\n\u2013 Sample z \u2208 RD as independent logistics \u2013 Compute y\u2217 = y\u2217(z, t, \u03b1) \u2208 arg max\ny\u2208{0,1}D z\u22a4y + t\u22a4y \u2212 \u2211K k=1 \u03b1kf(y)\n\u2013 Replace t by t\u2212 C\u221a h\n[ y\u2217 \u2212 \u3008x\u3009emp. + \u2202t\u2126(t, \u03b1) ]\n\u2013 Replace \u03b1d by (\n\u03b1d \u2212 C\u221a h\n[ \u3008fk(x)\u3009emp. \u2212 fk(y\u2217) + \u2202\u03b1d\u2126(t, \u03b1) ]\n)\n+ .\n\u2022 Output: (\u03b1, t).\nSince our cost function is convex and Lipschitz-continuous, the averaged iterates are converging to the global optimum [16] at rate 1/ \u221a H (for function values)."}, {"heading": "4.3 Extension to conditional maximum likelihood", "text": "In our experiments in Section 5, we consider a joint model over two binary vectors x, z \u2208 RD, as follows\np(x, z|\u03b1, t, \u03c0) = p(x|\u03b1, t)p(z|x, \u03c0) = exp(\u2212f(x)\u2212A(f)) D \u220f\nd=1\n\u03c0 \u03b4(zd 6=xd) d (1\u2212 \u03c0d)\u03b4(zd=xd), (6)\nwhich corresponds to sampling x from a log-supermodular model and considering z that switches the values of x with probability \u03c0d for each d, that is, a noisy observation of x. We have:\nlog p(x, z|\u03b1, t, \u03c0) = \u2212f(x)\u2212A(f) +\u2211Dd=1 { \u2212 log(1 + eud) + xdud + zdud \u2212 2xdzdud } ,\nwith ud = log \u03c0d 1\u2212\u03c0d which is equivalent to \u03c0d = (1 + e \u2212ud)\u22121.\nUsing Bayes rule, we have p(x|z, \u03b1, t, \u03c0) \u221d exp(\u2212f(x)\u2212A(f)+x\u22a4u\u2212 2x\u22a4(u \u25e6 z)), which leads to a log-supermodular model of the form p(x|z, \u03b1, t, \u03c0) = exp(\u2212f(x)+x\u22a4(u\u22122u\u25e6z)\u2212A(f\u2212u+2u\u25e6z)).\nThus, if we observe both z and x, we can consider a conditional maximization of the log-likelihood (still a convex optimization problem), which we do in our experiments for supervised image denoising, where we assume we know both noisy and original images at training time. Stochastic gradient on the logistic samples can then be used. Note that our conditional ML estimation can be seen as a form of approximate conditional random fields [13].\nWhile supervised learning can be achieved by other techniques such as structured-output-SVMs [18, 20, 22], our probabilistic approach also applies when we do not observe the original image, which we now consider."}, {"heading": "4.4 Missing data through maximum likelihood", "text": "In the model in Eq. (6), we now assume we only observed the noisy output z, and we want to perform parameter learning for \u03b1, t, \u03c0. This is a latent variable model for which ML can be readily applied. We have:\nlog p(z|\u03b1, t, \u03c0) = log\u2211x\u2208{0,1} p(z, x|\u03b1, t, \u03c0)\n= log \u2211 x\u2208{0,1}D exp(\u2212f(x)\u2212A(f)) \u220fD d=1 \u03c0 \u03b4(zd 6=xd) d (1 \u2212 \u03c0d)\u03b4(zd=xd) = A(f \u2212 u+ 2u \u25e6 z)\u2212A(f) + z\u22a4u\u2212 \u2211D\nd=1 log(1 + e ud).\nIn practice, we will assume that the noise probability \u03c0 (and hence u) is uniform across all elements. While we could use majorization-minization approaches such as the expectation-minimization algorithm (EM), we consider instead stochastic subgradient descent to learn the model parameters \u03b1, t and u (now a non-convex optimization problem, for which we still observed good convergence)."}, {"heading": "5 Experiments", "text": "The aim of our experiments is to demonstrate the ability of our approach to removing noise in binary images, following the experimental set-up of [9]. We consider the training sample of Ntrain = 100 images of size D = 50 \u00d7 50, and the test sample of Ntest = 100 binary images, containing a horse silhouette from the Weizmann horse database [3]. At first we add some noise by flipping pixels values independently with probability \u03c0. In Figure 2, we provide an example from the test sample: the original, the noisy and the denoised image (by our algorithm).\nWe consider the model from Section 4.3, with the two functions f1(x), f2(x) which are horizontal and vertical cut functions with binary weights respectively, together with a modular term of dimension D. To perform minimization we use graph-cuts [4] as we deal with positive or attractive potentials."}, {"heading": "5.1 Supervised image denoising", "text": "We assume that we observe N = 100 pairs (xi, zi) of original-noisy images, i = 1, . . . , N . We perform parameter inference by maximum likelihood using stochastic subgradient descent (over the logistic samples), with regularization by the squared \u21132-norm, one parameter for t, one for \u03b1, both learned by cross-validation. Given our estimates, we may denoise a new image by computing the \u201cmax-marginal\u201d, e.g., the maximum a posteriori maxx p(x|z, \u03b1, t) through a single graph-cut, or computing \u201cmean-marginals\u201d with 100 logistic samples. To calculate the error we use the normalized Hamming distance and 100 test images.\nResults are presented in Table 1, where we compare the two types of decoding, as well as a structured output SVM (SVM-Struct [22]) applied to the same problem. Results are reported in proportion of correct pixels. We see that the probabilistic models here outperform the max-margin formulation and that using mean-marginals (which is optimal given our loss measure) lead to slightly better performance."}, {"heading": "5.2 Unsupervised image denoising", "text": "We now only consider N = 100 noisy images z1, . . . , zN to learn the model, without the original images, and we use the latent model from Section 4.4. We apply stochastic subgradient descent for the difference of the two convex functions Alogistic to learn the model parameters and use fixed regularization parameters equal to 10\u22122."}, {"heading": "10% 1.9% 0.4% 2.1% 0.4% 6.8% 2.2% 7.0% 2.0%", "text": "We consider two situations, with a known noise-level \u03c0 or with learning it together with \u03b1 and t. The error was calculated using either max-marginals and mean-marginals. Note that here, structured-output SVMs cannot be used because there is no supervision. Results are reported in Table 2. One explanation for a better performance for max-marginals in this case is that the unsupervised approach tends to oversmooth the outcome and max-marginals correct this a bit.\nWhen the noise level is known, the performance compared to supervised learning is not degraded much, showing the ability of the probabilistic models to perform parameter estimation with missing data. When the noise level is unknown and learned as well, results are worse, still better than a trivial answer for moderate levels of noise (5% and 10%) but not better than outputting the noisy image for extreme levels (1% and 20%). In challenging fully unsupervised case the standard deviation is up to 2.2% (which shows that our results are statistically significant)."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented how approximate inference based on stochastic gradient and \u201cperturb-and-MAP\u201d ideas could be used to learn parameters of log-supermodular models, allowing us to benefit from the versatility of probabilistic modelling, in particular in terms of parameter estimation with missing data. While our experiments have focused on simple binary image denoising, exploring larger-scale applications in computer vision (such as done by [21, 24]) should also show the benefits of mixing probabilistic modelling and submodular functions."}, {"heading": "Acknowledgements", "text": "This work was funded by the MacSeNet Innovative Training Network. We would like to thank Sesh Kumar, Anastasia Podosinnikova and Anton Osokin for interesting discussions related to this work."}], "references": [{"title": "Learning with submodular functions: a convex optimization perspective", "author": ["F. Bach"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Submodular functions: from discrete to continuous domains", "author": ["F. Bach"], "venue": "Technical Report 1511.00394,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Combining Top-down and Bottom-up Segmentation", "author": ["E. Borenstein", "E. Sharon", "S. Ullman"], "venue": "In Proc. ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "From MAP to Marginals: Variational Inference in Bayesian Submodular Models", "author": ["J. Djolonga", "A. Krause"], "venue": "In Adv. NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Scalable Variational Inference in Log-supermodular Models", "author": ["J. Djolonga", "A. Krause"], "venue": "In Proc. ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Annals of discrete mathematics. Elsevier,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization", "author": ["D. Golovin", "A. Krause"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "In Proc. ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Polynomial-time approximation algorithms for the Ising model", "author": ["M. Jerrum", "A. Sinclair"], "venue": "SIAM Journal on Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P. Kohli", "L. Ladicky", "P.H.S. Torr"], "venue": "International Journal of Computer Vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems", "author": ["Andreas Krause", "Daniel Golovin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "In Proc. NAACL/HLT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A generalized logistic distribution", "author": ["S. Nadarajah", "S. Kotz"], "venue": "International Journal of Mathematics and Mathematical Sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Learning CRFs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "In Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Randomized optimum models for structured prediction", "author": ["D. Tarlow", "R.P. Adams", "R.S. Zemel"], "venue": "In Proc. AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Learning probabilistic submodular diversity models via noise contrastive estimation", "author": ["S. Tschiatschek", "J. Djolonga", "A. Krause"], "venue": "In Proc. AISTATS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Thomas Joachims", "Y. Altun", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Higher-order inference for multi-class log-supermodular models", "author": ["J. Zhang", "J. Djolonga", "A. Krause"], "venue": "In Proc. ICCV,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14].", "startOffset": 197, "endOffset": 207}, {"referenceID": 7, "context": "Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14].", "startOffset": 197, "endOffset": 207}, {"referenceID": 13, "context": "Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14].", "startOffset": 197, "endOffset": 207}, {"referenceID": 11, "context": "The key benefit of submodularity is the ability to model notions of diminishing returns, and the availability of exact minimization algorithms and approximate maximization algorithms with precise approximation guarantees [12].", "startOffset": 221, "endOffset": 225}, {"referenceID": 17, "context": ", images and their foreground/background segmentations in image segmentation, structured-output prediction methods such as the structured-SVM may be used [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "Log-supermodular models, with negative log-densities equal to a submodular function, are a first important step toward probabilistic modelling on discrete data with submodular functions [5].", "startOffset": 186, "endOffset": 189}, {"referenceID": 5, "context": "Several bounds have been proposed, that are accompanied with variational approximate inference [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].", "startOffset": 193, "endOffset": 199}, {"referenceID": 5, "context": "\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].", "startOffset": 193, "endOffset": 199}, {"referenceID": 4, "context": "1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.", "startOffset": 84, "endOffset": 90}, {"referenceID": 5, "context": "1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.", "startOffset": 84, "endOffset": 90}, {"referenceID": 8, "context": "1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "Most widely used submodular functions are cuts, concave functions of subset cardinality, mutual information, set covers, and certain functions of eigenvalues of submatrices [1, 7].", "startOffset": 173, "endOffset": 179}, {"referenceID": 6, "context": "Most widely used submodular functions are cuts, concave functions of subset cardinality, mutual information, set covers, and certain functions of eigenvalues of submatrices [1, 7].", "startOffset": 173, "endOffset": 179}, {"referenceID": 0, "context": "In this paper, we are going to use a few properties of such submodular functions (see [1, 7] and references therein).", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "In this paper, we are going to use a few properties of such submodular functions (see [1, 7] and references therein).", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "2 Log-supermodular distributions Log-supermodular models are introduced in [5] to model probability distributions on a hypercube, x \u2208 {0, 1}D, and are defined as p(x) = 1 Z(f) exp(\u2212f(x)), where f : {0, 1}D \u2192 R is a submodular function such that f(0) = 0 and the partition function is Z(f) = \u2211 x\u2208{0,1}D exp(\u2212f(x)).", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "In general, calculation of the partition function Z(f) or the log-partition function A(f) is intractable, as it includes simple binary Markov random fields\u2014 the exact calculation is known to be #P -hard [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 4, "context": "3 Examples Essentially, all submodular functions used in the minimization context can be used as negative log-densities [5, 6].", "startOffset": 120, "endOffset": 126}, {"referenceID": 5, "context": "3 Examples Essentially, all submodular functions used in the minimization context can be used as negative log-densities [5, 6].", "startOffset": 120, "endOffset": 126}, {"referenceID": 10, "context": "In computer vision, the most common examples are graph-cuts, which are essentially binary Markov random fields with attractive potentials, but higher-order potentials have been considered as well [11].", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "In our experiments, we use graph-cuts, where submodular function minimization may be performed with max-flow techniques and is thus efficient [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "Note that there are extensions of submodular functions to continuous domains that could be considered as well [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Note that lower bounds based on submodular maximization aspects and superdifferentials [5] can be used to highlight the tightness of various bounds, which we present in Figure 1.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "1 Base polytope relaxation with L-Field [5] This method exploits the fact that any submodular function f(x) can be lower bounded by a modular function s(x), i.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u03bcd\u2208[0,1]\u2212\u03bcdsd \u2212 \u03bcd log\u03bcd \u2212 (1 \u2212 \u03bcd) log(1\u2212 \u03bcd), leading to: AL-field(f) = min s\u2208B(f) max \u03bc\u2208[0,1]D \u2212\u03bc\u22a4s+H(\u03bc) = max \u03bc\u2208[0,1]D H(\u03bc)\u2212 f(\u03bc), where H(\u03bc) = \u2212 \u2211D d=1 { \u03bcd log\u03bcd + (1 \u2212 \u03bcd) log(1 \u2212 \u03bcd) }", "startOffset": 133, "endOffset": 138}, {"referenceID": 0, "context": "Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u03bcd\u2208[0,1]\u2212\u03bcdsd \u2212 \u03bcd log\u03bcd \u2212 (1 \u2212 \u03bcd) log(1\u2212 \u03bcd), leading to: AL-field(f) = min s\u2208B(f) max \u03bc\u2208[0,1]D \u2212\u03bc\u22a4s+H(\u03bc) = max \u03bc\u2208[0,1]D H(\u03bc)\u2212 f(\u03bc), where H(\u03bc) = \u2212 \u2211D d=1 { \u03bcd log\u03bcd + (1 \u2212 \u03bcd) log(1 \u2212 \u03bcd) }", "startOffset": 221, "endOffset": 226}, {"referenceID": 0, "context": "Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u03bcd\u2208[0,1]\u2212\u03bcdsd \u2212 \u03bcd log\u03bcd \u2212 (1 \u2212 \u03bcd) log(1\u2212 \u03bcd), leading to: AL-field(f) = min s\u2208B(f) max \u03bc\u2208[0,1]D \u2212\u03bc\u22a4s+H(\u03bc) = max \u03bc\u2208[0,1]D H(\u03bc)\u2212 f(\u03bc), where H(\u03bc) = \u2212 \u2211D d=1 { \u03bcd log\u03bcd + (1 \u2212 \u03bcd) log(1 \u2212 \u03bcd) }", "startOffset": 246, "endOffset": 251}, {"referenceID": 5, "context": "Finally, [6] shows the remarkable result that the minimizer s \u2208 B(f) may be obtained by minimizing a simpler function on B(f), namely the squared Euclidean norm, thus leading to algorithms such as the minimum-norm-point algorithm [7].", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "Finally, [6] shows the remarkable result that the minimizer s \u2208 B(f) may be obtained by minimizing a simpler function on B(f), namely the squared Euclidean norm, thus leading to algorithms such as the minimum-norm-point algorithm [7].", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": ", perform optimization), and then average over several random perturbations [9, 17, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 16, "context": ", perform optimization), and then average over several random perturbations [9, 17, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 18, "context": ", perform optimization), and then average over several random perturbations [9, 17, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 8, "context": "The main problem is that we need 2 such variables, and a key contribution of [9] is to show that if we consider a factored collection {gd(yd)}yd\u2208{0,1},d=1,.", "startOffset": 77, "endOffset": 80}, {"referenceID": 14, "context": "Writing gd(yd) = [gd(1)\u2212 gd(0)]yd + gd(0) and using the fact that (a) gd(0) has zero expectation and (b) the difference between two independent Gumbel distributions has a logistic distribution (with cumulative distribution function z 7\u2192 (1 + e\u2212z)\u22121) [15], we get the following upper-bound: ALogistic(f) = Ez1,.", "startOffset": 250, "endOffset": 254}, {"referenceID": 8, "context": "The first inequality was shown by [9].", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "In this case, the divide-and-conquer algorithm can be applied for L-Field [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "We compare the upper-bounds on the logpartition function AL-field and Alogistic, with the setup used by [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "The centers are sampled from N([3, 3], I) and N([\u22123,\u22123], I), respectively.", "startOffset": 31, "endOffset": 37}, {"referenceID": 2, "context": "The centers are sampled from N([3, 3], I) and N([\u22123,\u22123], I), respectively.", "startOffset": 31, "endOffset": 37}, {"referenceID": 4, "context": "The logistic upper bound is obtained using 100 logistic samples: the logistic upper-bound Alogistic is close to the superdifferential lower bound from [5] and is indeed significantly lower than the bound AL-field.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "Indeed, following [9], we consider an exponential family model p(x|t) = exp(\u2212f(x) + s\u22a4x\u2212A(f \u2212 t)), where f \u2212 t", "startOffset": 18, "endOffset": 21}, {"referenceID": 21, "context": "It is known that the expectation of the sufficient statistics under the exponential family model Ep(x|t)x is the gradient of the log-partition function [23].", "startOffset": 152, "endOffset": 156}, {"referenceID": 5, "context": "For the L-field bound, at t = 0, we have \u2202tdAL-field(f \u2212 t) = \u03c3(sd), where s\u2217 is the minimizer of \u2211D d=1 log(1 + e \u2212sd), thus recovering the interpretation of [6] from another point of view.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "For the logistic bound, this is the inference mechanism from [9], with \u2202tdAlogistic(f \u2212 t) = Ezy \u2217(z), where y\u2217(z) is the maximizer of maxy\u2208{0,1}D z \u22a4y \u2212 f(y).", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "= \u3008y\u2217(z, t, \u03b1)\u3009logistic, the expected values of the sufficient statistics match between the data and the optimizers used for the logistic approximation [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "\u2208 [0, 1], regularizer \u03a9(t, \u03b1).", "startOffset": 2, "endOffset": 8}, {"referenceID": 15, "context": "Since our cost function is convex and Lipschitz-continuous, the averaged iterates are converging to the global optimum [16] at rate 1/ \u221a H (for function values).", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Note that our conditional ML estimation can be seen as a form of approximate conditional random fields [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "While supervised learning can be achieved by other techniques such as structured-output-SVMs [18, 20, 22], our probabilistic approach also applies when we do not observe the original image, which we now consider.", "startOffset": 93, "endOffset": 105}, {"referenceID": 20, "context": "While supervised learning can be achieved by other techniques such as structured-output-SVMs [18, 20, 22], our probabilistic approach also applies when we do not observe the original image, which we now consider.", "startOffset": 93, "endOffset": 105}, {"referenceID": 8, "context": "5 Experiments The aim of our experiments is to demonstrate the ability of our approach to removing noise in binary images, following the experimental set-up of [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "We consider the training sample of Ntrain = 100 images of size D = 50 \u00d7 50, and the test sample of Ntest = 100 binary images, containing a horse silhouette from the Weizmann horse database [3].", "startOffset": 189, "endOffset": 192}, {"referenceID": 3, "context": "To perform minimization we use graph-cuts [4] as we deal with positive or attractive potentials.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "(a) original image (b) noisy image (c) denoised image Figure 2: Denoising of a horse image from the Weizmann horse database [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 20, "context": "Results are presented in Table 1, where we compare the two types of decoding, as well as a structured output SVM (SVM-Struct [22]) applied to the same problem.", "startOffset": 125, "endOffset": 129}], "year": 2016, "abstractText": "We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on \u201cperturb-and-MAP\u201d ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.", "creator": "LaTeX with hyperref package"}}}