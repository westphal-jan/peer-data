{"id": "1608.05604", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Modeling Human Reading with Neural Attention", "abstract": "when conditioned humans read normal text, they fixate some basic words and skip most others. \u2022 however, whilst there have been unexpectedly few attempts to successfully explain skipping what behavior model with formal computational models, as most largely existing model work has ever focused on blindly predicting reading times ( including e. g., ~ using surprisal ). in this latter paper, we ultimately propose a novel approach that neural models require both conditional skipping and reading, fundamentally using an unsupervised task architecture, that combines a conditioned neural motive attention with autoencoding, effectively trained on filtering raw text images using reinforcement learning. but our model explains human reading following behavior behaviors as a weak tradeoff between normal precision logic of language understanding ( encoding the perceived input signal accurately ) and task economy of attention ( fixating as few words as possible ). we evaluate the adaptive model on the dundee optimal eye - tracking hypothesis corpus, for showing strongly that reading it accurately constantly predicts skipping some behavior and reading times, detecting is competitive compliant with surprisal, and captures not known qualitative features missing of actual human text reading.", "histories": [["v1", "Fri, 19 Aug 2016 14:03:46 GMT  (40kb)", "http://arxiv.org/abs/1608.05604v1", "To appear in Proceedings of EMNLP 2016"], ["v2", "Mon, 24 Apr 2017 09:38:46 GMT  (40kb)", "http://arxiv.org/abs/1608.05604v2", "EMNLP 2016, pp. 85-95, Austin, TX"]], "COMMENTS": "To appear in Proceedings of EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael hahn 0001", "frank keller"], "accepted": true, "id": "1608.05604"}, "pdf": {"name": "1608.05604.pdf", "metadata": {"source": "CRF", "title": "Modeling Human Reading with Neural Attention", "authors": ["Michael Hahn", "Frank Keller"], "emails": ["s1582047@inf.ed.ac.uk", "keller@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 60\n4v 1\n[ cs\n.C L\n] 1\n9 A"}, {"heading": "1 Introduction", "text": "Humans read text by making a sequence of fixations and saccades. During a fixation, the eyes land on a word and remain fairly static for 200\u2013250 ms. Saccades are the rapid jumps that occur between fixations, typically lasting 20\u201340 ms and spanning 7\u2013 9 characters (Rayner, 1998). Readers, however, do not simply fixate one word after another; some saccades go in reverse direction, and some words are fixated more than once or skipped altogether.\nA range of computational models have been developed to account for human eye-movements in reading (Rayner and Reichle, 2010), including\nmodels of saccade generation in cognitive psychology, such as EZ-Reader (Reichle et al., 1998, 2003, 2009), SWIFT (Engbert et al., 2002, 2005), or the Bayesian Model of Bicknell and Levy (2010). More recent approaches use machine learning models trained on eye-tracking data to predict human reading patterns (Nilsson and Nivre, 2009, 2010; Hara et al., 2012; Matthies and S\u00f8gaard, 2013). Both types of models involve theoretical assumptions about human eye-movements, or at least require the selection of relevant eye-movement features. Model parameters have to be estimated in a supervised way from eye-tracking corpora.\nUnsupervised approaches, that do not involve training the model on eye-tracking data, have also been proposed. A key example is surprisal, which measures the predictability of a word in context, defined as the negative logarithm of the conditional probability of the current word given the preceding words (Hale, 2001; Levy, 2008). Surprisal is computed by a language model, which can take the form of a probabilistic grammar, an n-gram model, or a recurrent neural network. While surprisal has been shown to correlate with word-by-word reading times (McDonald and Shillcock, 2003a,b; Demberg and Keller, 2008; Frank and Bod, 2011; Smith and Levy, 2013), it cannot explain other aspects of human reading, such as reverse saccades, re-fixations, or skipping. Skipping is a particularly intriguing phenomenon: about 40% of all words are skipped (in the Dundee corpus, see below), without apparent detriment to text understanding.\nIn this paper, we propose a novel model architecture that is able to explain which words are skipped\nand which ones are fixated, while also predicting reading times for fixated words. Our approach is completely unsupervised and requires only unlabeled text for training.\nCompared to language as a whole, reading is a recent innovation in evolutionary terms, and people learning to read do not have access to competent readers\u2019 eye-movement patterns as training data. This suggests that human eye-movement patterns emerge from general principles of language processing that are independent of reading. Our starting point is the Tradeoff Hypothesis: Human reading optimizes a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). Based on the Tradeoff Hypothesis, we expect that humans only fixate words to the extent necessary for language understanding, while skipping words whose contribution to the overall meaning can be inferred from context.\nIn order to test these assumptions, this paper investigates the following questions:\n1. Can the Tradeoff Hypothesis be implemented in an unsupervised model that predicts skipping and reading times in quantitative terms? In particular, can we compute surprisal based only on the words that are actually fixated?\n2. Can the Tradeoff Hypothesis explain known qualitative features of human fixation patterns? These include dependence on word frequency, word length, predictability in context, a contrast between content and function words, and the statistical dependence of the current fixation on previous fixations.\nTo investigate these questions, we develop a generic architecture that combines neural language modeling with recent ideas on integrating recurrent neural networks with mechanisms of attention, which have shown promise both in NLP and in computer vision. We train our model end-to-end on a large text corpus to optimize a tradeoff between minimizing input reconstruction error and minimizing the number of words fixated. We evaluate the model\u2019s reading behavior against a corpus of human eye-tracking data. Apart from the unlabeled training corpus and the generic architecture, no further assumptions about\nlanguage structure are made \u2013 in particular, no lexicon or grammar or otherwise labeled data is required.\nOur unsupervised model is able to predict human skips and fixations with an accuracy of 63.7%. This compares to a baseline of 52.6% and a supervised accuracy of 69.9%. For fixated words, the model significantly predicts human reading times in a linear mixed effects analysis. The performance of our model is comparable to surprisal, even though it only fixates 60.4% of all input words. Furthermore, we show that known qualitative features of human fixation sequences emerge in our model without additional assumptions."}, {"heading": "2 Related Work", "text": "A range of attention-based neural network architectures have recently been proposed in the literature, showing promise in both NLP and computer vision (e.g., Mnih et al., 2014; Bahdanau et al., 2015). Such architectures incorporate a mechanism that allows the network to dynamically focus on a restricted part of the input. Attention is also a central concept in cognitive science, where it denotes the focus of cognitive processing. In both language processing and visual processing, attention is known to be limited to a restricted area of the visual field, and shifts rapidly through eye-movements (Henderson, 2003).\nAttention-based neural architectures either employ soft attention or hard attention. Soft attention distributes real-valued attention values over the input, making end-to-end training with gradient descent possible. Hard attention mechanisms make discrete choices about which parts of the input to focus on, and can be trained with reinforcement learning (Mnih et al., 2014). In NLP, soft attention can mitigate the difficulty of compressing long sequences into fixed-dimensional vectors, with applications in machine translation (Bahdanau et al., 2015) and question answering (Hermann et al., 2015). In computer vision, both types of attention can be used for selecting regions in an image (Ba et al., 2015; Xu et al., 2015)."}, {"heading": "3 The NEAT Reading Model", "text": "The point of departure for our model is the Tradeoff Hypothesis (see Section 1): Reading optimizes a tradeoff between precision of language understanding and economy of attention. We make this idea explicit by proposing NEAT (NEural Attention Tradeoff), a model that reads text and attempts to reconstruct it afterwards. While reading, the network chooses which words to process and which words to skip. The Tradeoff Hypothesis is formalized using a training objective that combines accuracy of reconstruction with economy of attention, encouraging the network to only look at words to the extent that is necessary for reconstructing the sentence."}, {"heading": "3.1 Architecture", "text": "We use a neural sequence-to-sequence architecture (Sutskever et al., 2014) with a hard attention mechanism. We illustrate the model in Figure 1, operating on a three-word sequence w. The most basic components are the reader, labeled R, and the decoder. Both of them are recurrent neural networks with Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) units. The recurrent reader network is expanded into time steps R0, . . . ,R3 in the figure. It goes over the input sequence, reading one word wi at a time, and converts the word sequence into a sequence of vectors h0, . . . ,h3. Each vector hi acts as a fixeddimensionality encoding of the word sequence w1, . . . ,wi that has been read so far. The last vector h3 (more generally hN for sequence length N), which encodes the entire input sequence, is then fed into the input layer of the decoder network, which attempts to reconstruct the input sequence w. It is also realized as a recurrent neural network, collapsed into a single box in the figure. It models a probability distribution over word sequences, outputting a probability distribution PDecoder(wi|w1,...,i\u22121,hN) over the vocabulary in the i-th step, as is common in neural language modeling (Mikolov et al., 2010). As the decoder has access to the vector representation created by the reader network, it ideally is able to assign the highest probability to the word sequence w that was actually read. Up to this point, the model is a standard sequence-to-sequence architecture reconstructing the input sequence, that is, performing\nautoencoding.\nAs a basic model of human processing, NEAT contains two further components. First, experimental evidence shows that during reading, humans constantly make predictions about the upcoming input (e.g., Van Gompel and Pickering, 2007). As a model of this behavior, the reader network at each time step outputs a probability distribution PR over the lexicon. This distribution describes which words are likely to come next (i.e., the reader network performs language modeling). Unlike the modeling performed by the decoder, PR, via its recurrent connections, has access to the previous context only.\nSecond, we model skipping by stipulating that only some of the input words wi are fed into the reader network R, while R receives a special vector representation, containing no information about the input word, in other cases. These are the words that are skipped. In NEAT, at each time step during reading, the attention module A decides whether the next word is shown to the reader network or not. When humans skip a word, they are able to identify it using parafoveal preview (Rayner, 2009). Thus, we can assume that the choice of which words to skip takes into account not only the prior context but also a preview of the word itself. We therefore allow the attention module to take the input word into account when making its decision. In addition, the attention module has access to the previous state hi\u22121 of the reader network, which summarizes what has been read so far. To allow for interaction between skipping and prediction, we also give the attention module access to the probability of the input word according to the prediction PR made at the last time step. If we write the decision made by A as \u03c9i \u2208 {0,1}, where \u03c9i = 1 means that word wi is shown to the reader and 0 means that it is not, we can write the probability of showing word wi as:\nP(\u03c9i = 1|\u03c91...i\u22121,w) = PA(wi,hi\u22121,PR(wi|w1...i\u22121,\u03c91...i\u22121))\n(1)\nWe implement A as a feed-forward network, followed by taking a binary sample \u03c9i.\nWe obtain the surprisal of an input word by taking the negative logarithm of the conditional probability\nof this word given the context words that precede it:\nSurp(wi|w1...i\u22121) =\u2212 logPR(wi|w1...i\u22121,\u03c91...i\u22121) (2)\nAs a consequence of skipping, not all input words are accessible to the reader network. Therefore, the probability and surprisal estimates it computes crucially only take into account the words that have actually been fixated. We will refer to this quantity as the restricted surprisal, as opposed to full surprisal, which is computed based on all prior context words.\nThe key quantities for predicting human reading are the fixation probabilities in equation (1), which model fixations and skips, and restricted surprisal in equation (2), which models the reading times of the words that are fixated."}, {"heading": "3.2 Model Objective", "text": "Given network parameters \u03b8 and a sequence w of words, the network stochastically chooses a sequence \u03c9 according to (1) and incurs a loss L(\u03c9|w,\u03b8) for language modeling and reconstruction:\nL(\u03c9|w,\u03b8) =\u2212\u2211 i log PR(wi|w1,...,i\u22121,\u03c91,...,i\u22121;\u03b8)\n\u2212\u2211 i logPDecoder(wi|w1,...,i\u22121;hN ;\u03b8)\n(3)\nwhere PR(wi, . . . ) denotes the output of the reader after reading wi\u22121, and PDecoder(wi| . . . ;hN) is the output of the decoder at time i\u2212 1, with hN being the vector representation created by the reader network for the entire input sequence.\nTo implement the Tradeoff Hypothesis, we train NEAT to solve language modeling and reconstruction with minimal attention, i.e., the network minimizes the expected loss:\nQ(\u03b8) := Ew,\u03c9 [L(\u03c9|w,\u03b8)+\u03b1 \u00b7 \u2016\u03c9\u2016\u21131 ] (4)\nwhere word sequences w are drawn from a corpus, and \u03c9 is distributed according to P(\u03c9|w,\u03b8) as defined in (1). In (4), \u2016\u03c9\u2016\u21131 is the number of words shown to the reader, and \u03b1 > 0 is a hyperparameter. The term \u03b1 \u00b7 \u2016\u03c9\u2016\u21131 encourages NEAT to attend to as few words as possible.\nNote that we make no assumption about linguistic structure \u2013 the only ingredients of NEAT are the\nneural architecture, the objective (4), and the corpus from which the sequences w are drawn."}, {"heading": "3.3 Training", "text": "We follow previous approaches to hard attention in using a combination of gradient descent and reinforcement learning, and separate the training of the recurrent networks from the training of A. To train the reader R and the decoder, we temporarily remove the attention network A, set \u03c9 \u223c Binom(n, p) (n sequence length, p a hyperparameter), and minimize E[L(w|\u03b8,\u03c9)] using stochastic gradient descent, sampling a sequence \u03c9 for each input sequence. In effect, NEAT is trained to perform reconstruction and language modeling when there is noise in the input. After R and the decoder have been trained, we fix their parameters and train A using the REINFORCE rule (Williams, 1992), which performs stochastic gradient descent using the estimate\n1 |B| \u2211w\u2208B;\u03c9 (L(\u03c9|w,\u03b8)+\u03b1 \u00b7 \u2016\u03c9\u2016\u21131)\u2202\u03b8A (logP(\u03c9|w,\u03b8)) (5) for the gradient \u2202\u03b8AQ. Here, B is a minibatch, \u03c9 is sampled from P(\u03c9|w,\u03b8), and \u03b8A \u2282 \u03b8 is the set of parameters of A. For reducing the variance of this estimator, we subtract in the i-th step an estimate of the expected loss:\nU(w,\u03c91...i\u22121) := E\u03c9i...N [L(\u03c91...i\u22121\u03c9i...N |w,\u03b8) + \u03b1 \u00b7 \u2016\u03c9\u2016\u21131 ] (6)\nWe compute the expected loss using an LSTM that we train simultaneously with A to predict L+ \u03b1\u2016\u03c9\u2016\u21131 based on w and \u03c91...i\u22121. To make learning more stable, we add an entropy term encouraging the distribution to be smooth, following Xu et al. (2015). The parameter updates to A are thus:\n\u2211 w,\u03c9 \u2211 i (L(\u03c9|w,\u03b8)+\u03b1\u2016\u03c9\u2016\u21131 \u2212U(w,\u03c91...i\u22121))\n\u00b7 \u2202\u03b8A (log P(\u03c9i|\u03c91...i\u22121,w,\u03b8))\n\u2212\u03b3 \u2202\u03b8A\n(\n\u2211 w,\u03c9 \u2211 i H[P(\u03c9i|\u03c91,...,i\u22121,w,\u03b8)]\n)\n(7)\nwhere \u03b3 is a hyperparameter, and H the entropy."}, {"heading": "4 Methods", "text": "Our aim is to evaluate how well NEAT predicts human fixation behavior and reading times. Furthermore, we want show that known qualitative properties emerge from the Tradeoff Hypothesis, even though no prior knowledge about useful features is hard-wired in NEAT."}, {"heading": "4.1 Training Setup", "text": "For both the reader and the decoder networks, we choose a one-layer LSTM network with 1,000 memory cells. The attention network is a one-layer feedforward network. For the loss estimator U , we use a bidirectional LSTM with 20 memory cells. Input data is split into sequences of 50 tokens, which are used as the input sequences for NEAT, disregarding sentence boundaries. Word embeddings have 100 dimensions, are shared between the reader and the attention network, and are only trained during the training of the reader. The vocabulary consists of the 10,000 most frequent words from the training corpus. We trained NEAT on the training set of the Daily Mail section of the corpus described by Hermann et al. (2015), which consists of 195,462 articles from the Daily Mail newspaper, containing approximately 200 million tokens. The recurrent networks and the attention network were each trained for one epoch. For initialization, weights are drawn from the uniform distribution. We set \u03b1= 5.0, \u03b3 = 5.0, and used a constant learning rate of 0.01 for A."}, {"heading": "4.2 Corpus", "text": "For evaluation, we use the English section of the Dundee corpus (Kennedy and Pynte, 2005), which consists of 20 texts from The Independent, annotated with eye-movement data from ten English na-\ntive speakers. Each native speakers read all 20 texts and answered a comprehension question after each text. We split the Dundee corpus into a development and a test set, with texts 1\u20133 constituting the development set. The development set consists of 78,300 tokens, and the test set of 281,911 tokens. For evaluation, we removed the datapoints removed by Demberg and Keller (2008), mainly consisting of words at the beginning or end of lines, outliers, and cases of track loss. Furthermore, we removed datapoints where the word was outside of the vocabulary of the model, and those datapoints mapped to positions 1\u20133 or 48\u201350 of a sequence when splitting the data. After preprocessing, 62.9% of the development tokens and 64.7% of the test tokens remained. To obtain the number of fixations on a token and reading times, we used the eye-tracking measures computed by Demberg and Keller (2008). The overall fixation rate was 62.1% on the development set, and 61.3% on the test set.\nThe development set was used to run preliminary versions of the human evaluation studies, and to determine the human skipping rate (see Section 5). All the results reported in this paper were computed on the test set, which remained unseen until the model was final."}, {"heading": "5 Results and Discussion", "text": "Throughout this section, we consider the following baselines for the attention network: random attention is defined by \u03c9 \u223c Binom(n, p), with p = 0.62, the human fixation rate in the development set. For full attention, we take \u03c9 = 1, i.e., all words are fixated. We also derive fixation predictions from full surprisal, word frequency, and word length by choosing a threshold such that the resulting fixation rate matches the human fixation rate on the develop-\nment set."}, {"heading": "5.1 Quantitative Properties", "text": "By averaging over all possible fixation sequences, NEAT defines for each word in a sequence a probability that it will be fixated. This probability is not efficiently computable, so we approximate it by sampling a sequence \u03c9 and taking the probabilities P(\u03c9i = 1|\u03c91...i\u22121,w) for i = 1, . . . ,50. These simulated fixation probabilities can be interpreted as defining a distribution of attention over the input sequence. Figure 2 shows heatmaps of the simulated and human fixation probabilities, respectively, for the beginning of a text from the Dundee corpus. While some differences between simulated and human fixation probabilities can be noticed, there are similarities in the general qualitative features of the two heatmaps. In particular, function words and short words are less likely to be fixated than content words and longer words in both the simulated and the human data.\nReconstruction and Language Modeling We first evaluate NEAT intrinsically by measuring how successful the network is at predicting the next word and reconstructing the input while minimizing the number of fixations. We compare perplexity on reconstruction and language modeling for \u03c9 \u223c P(\u03c9|w,\u03b8). In addition to the baselines, we run NEAT on the fixations generated by the human readers of the Dundee corpus, i.e., we use the human fixation sequence as \u03c9 instead of the fixation sequence generated by A to compute perplexity. This will tell us to what extent the human behavior minimizes the NEAT objective (4).\nThe results are given in Table 1. In all settings, the fixation rates are similar (60.4% to 62.1%) which makes the perplexity figures directly comparable. While NEAT has a higher perplexity on both tasks compared to full attention, it considerably outperforms random attention. It also outperforms the word length, word frequency, and full surprisal baselines. The perplexity on human fixation sequences is similar to that achieved using word frequency. Based on these results, we conclude that REINFORCE successfully optimizes the objective (4).\nLikelihood of Fixation Data Human reading behavior is stochastic in the sense that different runs of\neye-tracking experiments such as the ones recorded in the Dundee corpus yield different eye-movement sequences. NEAT is also stochastic, in the sense that, given a word sequence w, it defines a probability distribution over fixation sequences \u03c9. Ideally, this distribution should be close to the actual distribution of fixation sequences produced by humans reading the sequence, as measured by perplexity.\nWe find that the perplexity of the fixation sequences produced by the ten readers in the Dundee corpus under NEAT is 1.84. A perplexity of 2.0 corresponds to the random baseline Binom(n,0.5), and a perplexity of 1.96 to random attention Binom(n,0.62). As a lower bound on what can achieved with models disregarding the context, using the human fixation rates for each word as probabilities, we obtain a perplexity of 1.68.\nAccuracy of Fixation Sequences Previous work on supervised models for modeling fixations (Nilsson and Nivre, 2009; Matthies and S\u00f8gaard, 2013) has been evaluated by measuring the overlap of the fixation sequences produced by the models with those in the Dundee corpus. For NEAT, this method of evaluation is problematic as differences between model predictions and human data may be due to differences in the rate of skipping, and due to the inherently stochastic nature of fixations. We therefore derive model predictions by rescaling the simulated fixation probabilities so that their aver-\nage equals the fixation rate in the development set, and then greedily take the maximum-likelihood sequence. That is, we predict a fixation if the rescaled probability is greater than 0.5, and a skip otherwise. As in previous work, we report the accuracy of fixations and skips, and also separate F1 scores for fixations and skips. As lower and upper bounds, we use the random baseline \u03c9 \u223cBinom(n,0.62) and the agreement of the ten human readers, respectively. The results are shown in Table 2. NEAT clearly outperforms the random baseline and shows results close to full surprisal (where we apply the same rescaling and thresholding as for NEAT). This is remarkable given that NEAT has access to only 60.4% of the words in the corpus in order to predict skipping, while full surprisal has access to all the words.\nWord frequency and word length perform well, almost reaching the performance of supervised models. This shows that the bulk of skipping behavior is already explained by word frequency and word length effects. Note, however, that NEAT is completely unsupervised, and does not know that it has to pay attention to word frequency; this is something\nthe model is able to infer.\nRestricted Surprisal and Reading Times To evaluate the predictions NEAT makes for reading times, we use linear mixed-effects models containing restricted surprisal derived from NEAT for the Dundee test set. The mixed models also include a set of standard baseline predictors, viz., word length, log word frequency, log frequency of the previous word, launch distance, landing position, and the position of the word in the sentence. We treat participants and items as random factors. As the dependent variable, we take first pass duration, which is the sum of the durations of all fixations from first entering the word to first leaving it. We compare against full surprisal as an upper bound and against random surprisal as a lower bound. Random surprisal is surprisal computed by a model with random attention; this allows us to assess how much surprisal degrades when only 60.4% of all words are fixated, but no information is available as to which words should be fixated. The results in Table 3 show that restricted surprisal as computed by NEAT, full sur-\nprisal, and random surprisal are all significant predictors of reading time.\nIn order to compare the three surprisal estimates, we therefore need a measure of effect size. For this, we compare the model fit of the three mixed effects models using deviance, which is defined as the difference between the log likelihood of the model under consideration minus the log likelihood of the baseline model, multiplied by \u22122. Higher deviance indicates greater improvement in model fit over the baseline model. We find that the mixed model that includes restricted surprisal achieves a deviance of 867, compared to the model containing only the baseline features. With full surprisal, we obtain a deviance of 980. On the other hand, the model including random surprisal achieves a lower deviance of 832.\nThis shows that restricted surprisal as computed by NEAT not only significantly predicts reading times, it also provides an improvement in model fit compared to the baseline predictors. Such an improvement is also observed with random surprisal, but restricted surprisal achieves a greater improvement in model fit. Full surprisal achieves an even greater improvement, but this is not unexpected, as full surprisal has access to all words, unlike NEAT or random surprisal, which only have access to 60.4% of the words."}, {"heading": "5.2 Qualitative Properties", "text": "We now examine the second key question we defined in Section 1, investigating the qualitative features of the simulated fixation sequences. We will focus on comparing the predictions of NEAT with that of word frequency, which performs comparably at the task of predicting fixation sequences (see Section 5.1). We show NEAT nevertheless makes relevant predictions that go beyond frequency.\nFixations of Successive Words While predictors derived from word frequency treat the decision whether to fixate or skip words as independent, humans are more likely to fixate a word when the previous word was skipped (Rayner, 1998). This effect is also seen in NEAT. More precisely, both in the human data and in the simulated fixation data, the conditional fixation probability P(\u03c9i = 1|\u03c9i\u22121 = 1) is lower than the marginal probability P(\u03c9i = 1).\nThe ratio of these probabilities is 0.85 in the human data, and 0.81 in NEAT. The threshold predictor derived from word frequency also shows this effect (as the frequencies of successive words are not independent), but it is weaker (ratio 0.91).\nTo further test the context dependence of NEAT\u2019s fixation behavior, we ran a mixed model predicting the fixation probabilities simulated by NEAT, with items as random factor and the log frequency of word i as predictor. Adding \u03c9i\u22121 as a predictor results in a significant improvement in model fit (deviance = 4,798, t = 71.3). This shows that NEAT captures the context dependence of fixation sequences to an extend that goes beyond word frequency alone.\nParts of Speech Part of speech categories are known to be a predictor of fixation probabilities, with content words being more likely to be fixated than function words (Carpenter and Just, 1983). In Table 4, we give the simulated fixation probabilities and the human fixation probabilities estimated from the Dundee corpus for the tags of the Universal PoS tagset (Petrov et al., 2012), using the PoS annotation of Barrett et al. (2015). We again compare with the probabilities of a threshold predictor derived from\nword frequency.1 NEAT captures the differences between PoS categories well, as evidenced by the high correlation coefficients. The content word categories ADJ, ADV, NOUN, VERB and X consistently show higher probabilities than the function word categories. While the correlation coefficients for word frequency are very similar, the numerical values of the simulated probabilities are closer to the human ones than those derived from word frequency, which tend towards more extreme values. This difference can be seen clearly if we compare the mean squared error, rather than the correlation, with the human fixation probabilities (last row of Table 4).\nCorrelations with Known Predictors In the literature, it has been observed that skipping correlates with predictability (surprisal), word frequency, and word length (Rayner, 1998, p. 387). These correlations are also observed in the human skipping data derived from Dundee, as shown in Table 5. (Human fixation probabilities were obtained by averaging over the ten readers in Dundee.)\nComparing the known predictors of skipping with NEAT\u2019s simulated fixation probabilities, similar correlations as in the human data are observed. We observe that the correlations with surprisal are stronger\n1We omit the tag \u201c.\u201d for punctuation, as punctuation characters are not treated as separate tokens in Dundee.\nin NEAT, considering both restricted surprisal and full surprisal as measures of predictability."}, {"heading": "6 Conclusions", "text": "We investigated the hypothesis that human reading strategies optimize a tradeoff between precision of language understanding and economy of attention. We made this idea explicit in NEAT, a neural reading architecture with hard attention that can be trained end-to-end to optimize this tradeoff. Experiments on the Dundee corpus show that NEAT provides accurate predictions for human skipping behavior. It also predicts reading times, even though it only has access to 60.4% of the words in the corpus in order to estimate surprisal. Finally, we found that known qualitative properties of skipping emerge in our model, even though they were not explicitly included in the architecture, such as context dependence of fixations, differential skipping rates across parts of speech, and correlations with other known predictors of human reading behavior."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Ba", "Jimmy", "Ruslan R. Salakhutdinov", "Roger B. Grosse", "Brendan J. Frey."], "venue": "Advances in Neural Information Processing Systems. pages 2575\u20132583.", "citeRegEx": "Ba et al\\.,? 2015", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The Dundee treebank", "author": ["Barrett", "Maria", "\u017deljko Agi\u0107", "Anders S\u00f8gaard."], "venue": "Proceedings of the 14th International Workshop on Treebanks and Linguistic Theories. pages 242\u2013248.", "citeRegEx": "Barrett et al\\.,? 2015", "shortCiteRegEx": "Barrett et al\\.", "year": 2015}, {"title": "A rational model of eye movement control in reading", "author": ["Bicknell", "Klinton", "Roger Levy."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. pages 1168\u20131178.", "citeRegEx": "Bicknell et al\\.,? 2010", "shortCiteRegEx": "Bicknell et al\\.", "year": 2010}, {"title": "What your eyes do while your mind is reading", "author": ["P.A. Carpenter", "M.A. Just."], "venue": "K. Rayner, editor, Eye Movements in Reading, Academic Press, New York, pages 275\u2013307.", "citeRegEx": "Carpenter and Just.,? 1983", "shortCiteRegEx": "Carpenter and Just.", "year": 1983}, {"title": "Data from eye-tracking corpora as evidence for theories of syntactic processing complexity", "author": ["Demberg", "Vera", "Frank Keller."], "venue": "Cognition 109(2):193\u2013210.", "citeRegEx": "Demberg et al\\.,? 2008", "shortCiteRegEx": "Demberg et al\\.", "year": 2008}, {"title": "A dynamical model of saccade generation in reading based on spatially distributed lexical processing", "author": ["Engbert", "Ralf", "Andr\u00e9 Longtin", "Reinhold Kliegl."], "venue": "Vision Research 42(5):621\u2013636.", "citeRegEx": "Engbert et al\\.,? 2002", "shortCiteRegEx": "Engbert et al\\.", "year": 2002}, {"title": "SWIFT: A dynamical model of saccade generation during reading", "author": ["Engbert", "Ralf", "Antje Nuthmann", "Eike M. Richter", "Reinhold Kliegl."], "venue": "Psychological Review 112(4):777\u2013813.", "citeRegEx": "Engbert et al\\.,? 2005", "shortCiteRegEx": "Engbert et al\\.", "year": 2005}, {"title": "Insensitivity of the human sentence-processing system to hierarchical structure", "author": ["S.L. Frank", "R. Bod."], "venue": "Psychological Science 22:829\u2013834.", "citeRegEx": "Frank and Bod.,? 2011", "shortCiteRegEx": "Frank and Bod.", "year": 2011}, {"title": "A probabilistic Earley parser as a psycholinguistic model", "author": ["Hale", "John."], "venue": "Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics. volume 2, pages 159\u2013166.", "citeRegEx": "Hale and John.,? 2001", "shortCiteRegEx": "Hale and John.", "year": 2001}, {"title": "Predicting word fixations in text with a CRF model for capturing general reading strategies among readers", "author": ["Hara", "Tadayoshi", "Daichi Mochihashi Yoshinobu Kano", "Akiko Aizawa."], "venue": "Proceedings of the 1st Workshop on Eye-tracking and", "citeRegEx": "Hara et al\\.,? 2012", "shortCiteRegEx": "Hara et al\\.", "year": 2012}, {"title": "Human gaze control in realworld scene perception", "author": ["Henderson", "John."], "venue": "Trends in Cognitive Sciences 7:498\u2013504.", "citeRegEx": "Henderson and John.,? 2003", "shortCiteRegEx": "Henderson and John.", "year": 2003}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "ArXiv:1506.03340.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Parafoveal-onfoveal effects in normal reading", "author": ["Kennedy", "Alan", "Jo\u00ebl Pynte."], "venue": "Vision Research 45(2):153\u2013168.", "citeRegEx": "Kennedy et al\\.,? 2005", "shortCiteRegEx": "Kennedy et al\\.", "year": 2005}, {"title": "Expectation-based syntactic comprehension", "author": ["Levy", "Roger."], "venue": "Cognition 106(3):1126\u20131177.", "citeRegEx": "Levy and Roger.,? 2008", "shortCiteRegEx": "Levy and Roger.", "year": 2008}, {"title": "With blinkers on: Robust prediction of eye movements across readers", "author": ["Matthies", "Franz", "Anders S\u00f8gaard."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 803\u2013807.", "citeRegEx": "Matthies et al\\.,? 2013", "shortCiteRegEx": "Matthies et al\\.", "year": 2013}, {"title": "Eye movements reveal the on-line computation of lexical probabilities during reading", "author": ["McDonald", "Scott A.", "Richard C. Shillcock."], "venue": "Psychological Science 14(6):648\u2013652.", "citeRegEx": "McDonald et al\\.,? 2003a", "shortCiteRegEx": "McDonald et al\\.", "year": 2003}, {"title": "Low-level predictive inference in reading: the influence of transitional probabilities on eye movements", "author": ["McDonald", "Scott A.", "Richard C. Shillcock."], "venue": "Vision Research 43(16):1735\u20131751.", "citeRegEx": "McDonald et al\\.,? 2003b", "shortCiteRegEx": "McDonald et al\\.", "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of Interspeech. pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning where to look: Modeling eye movements in reading", "author": ["Nilsson", "Mattias", "Joakim Nivre."], "venue": "Proceedings of the 13th Conference on Computational Natural Language Learning. pages 93\u2013101.", "citeRegEx": "Nilsson et al\\.,? 2009", "shortCiteRegEx": "Nilsson et al\\.", "year": 2009}, {"title": "Towards a data-driven model of eye movement control in reading", "author": ["Nilsson", "Mattias", "Joakim Nivre."], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics. pages 63\u201371.", "citeRegEx": "Nilsson et al\\.,? 2010", "shortCiteRegEx": "Nilsson et al\\.", "year": 2010}, {"title": "A universal part-of-speech tagset", "author": ["Petrov", "Slav", "Dipanjan Das", "Ryan T. McDonald."], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation. pages 2089\u20132096.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Eye movements in reading and information processing: 20 years of research", "author": ["K. Rayner"], "venue": "Psychological Bulletin 124(3):372\u2013422.", "citeRegEx": "Rayner,? 1998", "shortCiteRegEx": "Rayner", "year": 1998}, {"title": "Eye movements in reading: Models and data", "author": ["Rayner", "Keith."], "venue": "Journal of Eye Movement Research 2(5):1\u201310.", "citeRegEx": "Rayner and Keith.,? 2009", "shortCiteRegEx": "Rayner and Keith.", "year": 2009}, {"title": "Models of the reading process", "author": ["Rayner", "Keith", "Erik D. Reichle."], "venue": "Wiley Interdisciplinary Reviews: Cognitive Science 1(6):787\u2013799.", "citeRegEx": "Rayner et al\\.,? 2010", "shortCiteRegEx": "Rayner et al\\.", "year": 2010}, {"title": "Toward a model of eye movement control in reading", "author": ["E.D. Reichle", "A. Pollatsek", "D.L. Fisher", "K. Rayner."], "venue": "Psychological Review 105(1):125\u2013157.", "citeRegEx": "Reichle et al\\.,? 1998", "shortCiteRegEx": "Reichle et al\\.", "year": 1998}, {"title": "Using EZ Reader to model the effects of higher level language processing on eye movements during reading", "author": ["E.D. Reichle", "T. Warren", "K. McConnell."], "venue": "Psychonomic Bulletin & Review 16(1):1\u201321.", "citeRegEx": "Reichle et al\\.,? 2009", "shortCiteRegEx": "Reichle et al\\.", "year": 2009}, {"title": "The EZ Reader model of eyemovement control in reading: Comparisons to other models", "author": ["Reichle", "Erik D.", "Keith Rayner", "Alexander Pollatsek."], "venue": "Behavioral and Brain Sciences 26(04):445\u2013476.", "citeRegEx": "Reichle et al\\.,? 2003", "shortCiteRegEx": "Reichle et al\\.", "year": 2003}, {"title": "The effect of word predictability on reading time is logarithmic", "author": ["Smith", "Nathaniel J.", "Roger Levy."], "venue": "Cognition 128(3):302\u2013319.", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Syntactic parsing", "author": ["Van Gompel", "Roger PG", "Martin J. Pickering."], "venue": "The Oxford Handbook of Psycholinguistics, Oxford University Press, pages 289\u2013307.", "citeRegEx": "Gompel et al\\.,? 2007", "shortCiteRegEx": "Gompel et al\\.", "year": 2007}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J."], "venue": "Machine Learning 8(34):229\u2013256.", "citeRegEx": "Williams and J.,? 1992", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "ArXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Saccades are the rapid jumps that occur between fixations, typically lasting 20\u201340 ms and spanning 7\u2013 9 characters (Rayner, 1998).", "startOffset": 115, "endOffset": 129}, {"referenceID": 10, "context": "More recent approaches use machine learning models trained on eye-tracking data to predict human reading patterns (Nilsson and Nivre, 2009, 2010; Hara et al., 2012; Matthies and S\u00f8gaard, 2013).", "startOffset": 114, "endOffset": 192}, {"referenceID": 6, "context": ", 1998, 2003, 2009), SWIFT (Engbert et al., 2002, 2005), or the Bayesian Model of Bicknell and Levy (2010). More recent approaches use machine learning models trained on eye-tracking data to predict human reading patterns (Nilsson and Nivre, 2009, 2010; Hara et al.", "startOffset": 28, "endOffset": 107}, {"referenceID": 8, "context": "While surprisal has been shown to correlate with word-by-word reading times (McDonald and Shillcock, 2003a,b; Demberg and Keller, 2008; Frank and Bod, 2011; Smith and Levy, 2013), it cannot explain other aspects of human reading, such as reverse saccades, re-fixations, or skipping.", "startOffset": 76, "endOffset": 178}, {"referenceID": 1, "context": "A range of attention-based neural network architectures have recently been proposed in the literature, showing promise in both NLP and computer vision (e.g., Mnih et al., 2014; Bahdanau et al., 2015).", "startOffset": 151, "endOffset": 199}, {"referenceID": 1, "context": "In NLP, soft attention can mitigate the difficulty of compressing long sequences into fixed-dimensional vectors, with applications in machine translation (Bahdanau et al., 2015) and question answering (Hermann et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 12, "context": ", 2015) and question answering (Hermann et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 0, "context": "In computer vision, both types of attention can be used for selecting regions in an image (Ba et al., 2015; Xu et al., 2015).", "startOffset": 90, "endOffset": 124}, {"referenceID": 32, "context": "In computer vision, both types of attention can be used for selecting regions in an image (Ba et al., 2015; Xu et al., 2015).", "startOffset": 90, "endOffset": 124}, {"referenceID": 29, "context": "We use a neural sequence-to-sequence architecture (Sutskever et al., 2014) with a hard attention mechanism.", "startOffset": 50, "endOffset": 74}, {"referenceID": 18, "context": ",i\u22121,hN) over the vocabulary in the i-th step, as is common in neural language modeling (Mikolov et al., 2010).", "startOffset": 88, "endOffset": 110}, {"referenceID": 32, "context": "To make learning more stable, we add an entropy term encouraging the distribution to be smooth, following Xu et al. (2015). The parameter updates to A are thus:", "startOffset": 106, "endOffset": 123}, {"referenceID": 12, "context": "We trained NEAT on the training set of the Daily Mail section of the corpus described by Hermann et al. (2015), which consists of 195,462 articles from the Daily Mail newspaper, containing approximately 200 million tokens.", "startOffset": 89, "endOffset": 111}, {"referenceID": 22, "context": "Fixations of Successive Words While predictors derived from word frequency treat the decision whether to fixate or skip words as independent, humans are more likely to fixate a word when the previous word was skipped (Rayner, 1998).", "startOffset": 217, "endOffset": 231}, {"referenceID": 4, "context": "Parts of Speech Part of speech categories are known to be a predictor of fixation probabilities, with content words being more likely to be fixated than function words (Carpenter and Just, 1983).", "startOffset": 168, "endOffset": 194}, {"referenceID": 21, "context": "In Table 4, we give the simulated fixation probabilities and the human fixation probabilities estimated from the Dundee corpus for the tags of the Universal PoS tagset (Petrov et al., 2012), using the PoS annotation of Barrett et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 2, "context": ", 2012), using the PoS annotation of Barrett et al. (2015). We again compare with the probabilities of a threshold predictor derived from", "startOffset": 37, "endOffset": 59}], "year": 2016, "abstractText": "When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g., using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.", "creator": "LaTeX with hyperref package"}}}