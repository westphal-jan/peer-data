{"id": "1608.06656", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Lexical Query Modeling in Session Search", "abstract": "computational lexical query composition modeling has been the international leading paradigm for session problem search. in compiling this paper, we analyze multi trec session query logs database and specifically compare the perceived performance of 6 different lexical matching approaches for session search. experimental naive methods based on extended term frequency weighing perform on the par with specialized structural session location models. in addition, we to investigate the expressive viability of lexical algorithm query models thus in building the recommendation setting guidelines of session search. we then give together important insights geared into managing the potential and limitations of lexical query retrieval modeling for functional session search and consequently propose future directions for the basic field of session search.", "histories": [["v1", "Tue, 23 Aug 2016 21:07:50 GMT  (1297kb,D)", "http://arxiv.org/abs/1608.06656v1", "ICTIR2016, Proceedings of the 2nd ACM International Conference on the Theory of Information Retrieval. 2016"]], "COMMENTS": "ICTIR2016, Proceedings of the 2nd ACM International Conference on the Theory of Information Retrieval. 2016", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["christophe van gysel", "evangelos kanoulas", "maarten de rijke"], "accepted": false, "id": "1608.06656"}, "pdf": {"name": "1608.06656.pdf", "metadata": {"source": "CRF", "title": "Lexical Query Modeling in Session Search", "authors": ["Christophe Van Gysel", "Evangelos Kanoulas", "Maarten de Rijke"], "emails": ["cvangysel@uva.nl", "e.kanoulas@uva.nl", "derijke@uva.nl", "permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "Many complex information seeking tasks, such as planning a trip or buying a car, cannot sufficiently be expressed in a single query [7]. These multi-faceted tasks are exploratory, comprehensive, survey-like or comparative in nature [14] and require multiple search iterations to be adequately answered [8]. Donato et al. [5] note that 10% of the user sessions (more than 25% of query volume) consists of such complex information needs.\nThe TREC Session Track [15] created an environment for researchers \u201cto test whether systems can improve their performance for a given query by using previous queries and user interactions with the retrieval system.\u201d The track\u2019s existence led to an increasing number of methods aimed at improving session search. Yang et al. [16] introduce the Query Change Model (QCM), which uses lexical editing changes between consecutive queries in addition to query terms occurring in previously retrieved documents, to improve session search. They heuristically construct a lexicon-based query model for every query in a session. Query models are then linearly combined for every document, based on query recency [16] or document satisfaction [3, 10], into a session-wide lexical query model. However, there has been a clear trend towards the use of supervised learning [3, 12, 16] and external data sources [6, 11]. Guan et al. [6] perform lexical query expansion by adding higherorder n-grams to queries by mining document snippets. In addition, they expand query representations by including anchor texts to previously top-ranked documents in the session. Carterette et al. [3] expand document representations by including incoming anchor\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nICTIR \u201916, September 12 - 16, 2016, Newark, DE, USA c\u00a9 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4497-5/16/09. . . $15.00\nDOI: http://dx.doi.org/10.1145/2970398.2970422\ntexts. Luo et al. [12] introduce a linear point-wise learning-to-rank model that predicts relevance given a document and query change features. They incorporate document-independent session features in their ranker.\nThe use of machine-learned ranking and the expansion of query and document representations is meant to address a specific instance of a wider problem in information retrieval, namely the query document mismatch [9]. In this paper, we analyze the session query logs made available by TREC and compare the performance of different lexical query modeling approaches for session search, taking into account session length.1 In addition, we investigate the viability of lexical query models in a session search setting.\nThe main purpose of this paper is to investigate the potential of lexical methods in session search and provide foundations for future research. We ask the following questions: (1) Increasingly complex methods for session search are being developed, but how do naive methods perform? (2) How well can lexical methods perform? (3) Can we solve the session search task using lexical matching only?"}, {"heading": "2. LEXICAL MATCHING FOR SESSIONS", "text": "We define a search session s as a sequence of n interactions (qi, ri) between user and search engine, where qi denotes a userissued query consisting of |qi| terms ti,1, . . . , ti,|qi| and ri denotes a result page consisting of |ri| documents ri,1, . . . , ri,|ri| returned by the search engine (also referred to as SERP). The goal, then, is to return a SERP rn+1 given a query qn+1 and the session history that maximizes the user\u2019s utility function.\nIn this work, we formalize session search by modeling an observed session s as a query model parameterized by \u03b8s = {\u03b8s1, . . . , \u03b8s|V |}, where \u03b8si denotes the weight associated with term ti \u2208 V (specified below). Documents dj are then ranked in decreasing order of\nlogP (dj | s) = |V |\u2211 k=1 \u03b8sk log \u03b8 dj k ,\nwhere \u03b8dj is a lexical model of document dj , which can be a language model (LM), a vector space model or a specialized model using hand-engineered features. Query model \u03b8s is a function of the query models of the interactions i in the session, \u03b8si (e.g., for a uniform aggregation scheme, \u03b8s = \u2211 i \u03b8\nsi ). Existing session search methods [6, 16] can be expressed in this formalism as follows: Term frequency (TF) Terms in a query are weighted according to their frequency in the query (i.e., \u03b8sik becomes the frequency 1An open-source implementation of our testbed for evaluating session search is available at https://github.com/cvangysel/sesh.\nar X\niv :1\n60 8.\n06 65\n6v 1\n[ cs\n.I R\n] 2\n3 A\nug 2\n01 6\nof term tk in qi). Queries qi that are part of the same session s are then aggregated uniformly for a subset of queries. In this work, we consider the following subsets: the first query, the last query and the concatenation of all queries in a session. Using the last query corresponds to the official baseline of the TREC Session track [3].\nNugget Nugget [6] is a method for effective structured query formulation for session search. Queries qi, part of session s, are expanded using higher order n-grams occurring in both qi and snippets of the top-k documents in the previous interaction, ri\u22121,1, . . . , ri\u22121,k. This effectively expands the vocabulary by additionally considering n-grams next to unigram terms. The query models of individual queries in the session are then aggregated using one of the aggregation schemes. Nugget is primarily targeted at resolving the query-document mismatch by incorporating structure and external data and does not model query transitions. The method can be extended to include external evidence by expanding \u03b8s to include anchor texts pointing to (clicked) documents in previous SERPs. Query Change Model (QCM) QCM [16] uses syntactic editing changes between consecutive queries in addition to query changes and previous SERPs to enhance session search. In QCM [16, Section 6.3], document model \u03b8d is provided by a language model with Dirichlet smoothing and the query model at interaction i, \u03b8si , in session s is given by\n\u03b8sik =  1 + \u03b1(1\u2212 P (tk | ri\u22121,1)), tk \u2208 qtheme 1\u2212 \u03b2P (tk | ri\u22121,1), tk \u2208 +\u2206q \u2227 tk \u2208 ri\u22121,1 1 + idf(tk), tk \u2208 +\u2206q \u2227 tk /\u2208 ri\u22121,1 \u2212\u03b4P (tk | ri\u22121,1), tk \u2208 \u2212\u2206q,\nwhere qtheme are the session\u2019s theme terms, +\u2206q (\u2212\u2206q, resp.) are the added (removed) terms, P (tk | ri\u22121,1) denotes the probability of tk occurring in SAT clicks, idf(tk) is the inverse document frequency of term tk and \u03b1, \u03b2, , \u03b4 are parameters. The \u03b8si are then aggregated into \u03b8s using one of the aggregation schemes, such as the uniform aggregation scheme (i.e., the sum of the \u03b8si ).\nIn \u00a74, we analyze the methods listed above in terms of their ability to handle sessions of different lengths and contextual history."}, {"heading": "3. EXPERIMENTS", "text": ""}, {"heading": "3.1 Benchmarks", "text": "We evaluate the lexical query modeling methods listed in \u00a72 on the session search task (G1) of the TREC Session track from 2011 to 2014 [15]. We report performance on each track edition independently and on the track aggregate. Given a query, the task is to improve retrieval performance by using previous queries and user interactions with the retrieval system. To accomplish this, we first retrieve the 2,000 most relevant documents for the given query and then re-rank these documents using the methods described in \u00a72. We use the \u201cCategory B\u201d subsets of ClueWeb09 (2011/2012) and ClueWeb12 (2013/2014) as document collections. Both collections consist of approximately 50 million documents. Spam documents are removed before indexing by filtering out documents with scores (GroupX and Fusion, respectively) below 70 [4]. Table 1 shows an overview of the benchmarks and document collections."}, {"heading": "3.2 Evaluation measures", "text": "To measure retrieval effectiveness, we report Normalized Discounted Cumulative Gain at rank 10 (NDCG@10) in addition to\nMean Reciprocal Rank (MRR). The relevance judgments of the tracks were converted from topic-centric to session-centric according to the mappings provided by the track organizers.2 Evaluation measures are then computed using TREC\u2019s official evaluation tool, trec_eval.3"}, {"heading": "3.3 Systems under comparison", "text": "We compare the lexical query model methods outlined in \u00a72. All methods compute weights for lexical entities (e.g., unigram terms) on a per-session basis, construct a structured Indri query [13] and query the document collection using pyndri.4 For fair comparison, we use Indri\u2019s default smoothing configuration (i.e., Dirichlet smoothing with \u00b5 = 2500) and uniform query aggregation for all methods (different from the smoothing used for QCM in [16]). This allows us to separate query aggregation techniques from query modeling approaches in the case of session search.\nFor Nugget, we use the default parameter configuration (ksnippet = 10, \u03b8 = 0.97, kanchor = 5 and \u03b2 = 0.1), using the strict expansion method. We report the performance of Nugget without the use of external resources (RL2), with anchor texts (RL3) and with click data (RL4). For QCM, we use the parameter configuration as described in [12, 16]: \u03b1 = 2.2, \u03b2 = 1.8, = 0.07 and \u03b4 = 0.4.\nIn addition to the methods above, we report the performance of an oracle that always ranks in decreasing order of ground-truth relevance. This oracle will give us an upper-bound on the achievable ranking performance."}, {"heading": "3.4 Ideal lexical term weighting", "text": "We investigate the maximally achievable performance by weighting query terms. Inspired by Bendersky et al. [1], we optimize NDCG@10 for every session using a grid search over the term weight space. We sweep the weight of every term between \u22121.0 and 1.0 (inclusive) with increments of 0.1, resulting in a total of 21 weight assignments per term. Due to the exponential time complexity of the grid search, we limit our analysis to the 230 sessions with 7 unique query terms or less (see Table 1). This experiment will tell us the maximally achievable retrieval performance in session search by the re-weighting lexical terms only."}, {"heading": "4. RESULTS & DISCUSSION", "text": "In this section, we report and discuss our experimental results. Of special interest to us are the methods that perform lexical matching based on a user\u2019s queries in a single session: QCM, Nugget (RL2) and the three variants of TF. Table 2 shows the methods\u2019 performance on the TREC Session track editions from 2011 to 2014. No single method consistently outperforms the other methods. Interestingly enough, the methods based on term frequency (TF) perform quite competitively compared to the specialized session search methods (Nugget and QCM). In addition, the TF variant using all queries in a session even outperforms Nugget (RL2) on the 2011 and 2014 editions and QCM on nearly all editions. Using the concatenation of all queries in a session, while being an obvious baseline, has not received much attention in recent literature or by TREC [15]. In addition, note that the best-performing (unsupervised) TF method achieves better results than the supervised method of Luo et al. [12] on the 2012 and 2013 tracks. Fig. 1 depicts the boxplot of the NDCG@10 distribution over all track editions (2011\u20132014). The term frequency approach using all queries\n2We take into account the mapping between judgments and actual relevance grades for the 2012 edition. 3https://github.com/usnistgov/trec_eval 4https://github.com/cvangysel/pyndri\nTable 1: Overview of 2011, 2012, 2013 and 2014 TREC session tracks. For the 2014 track, we report the total number of sessions in addition to those sessions with judgments. We report the mean and standard deviation where appropriate; M denotes the median.\n2011 2012 2013 2014\nSessions Sessions 76 98 87 100 (1,021 total) Queries per session 3.68 \u00b1 1.79; M=3.00 3.03 \u00b1 1.57; M=2.00 5.08 \u00b1 3.60; M=4.00 4.34 \u00b1 2.22; M=4.00 Unique terms per session 7.01 \u00b1 3.28; M=6.50 5.76 \u00b1 2.95; M=5.00 8.86 \u00b1 4.38; M=8.00 7.79 \u00b1 4.08; M=7.00 Topics Session per topic 1.23 \u00b1 0.46; M= 1.00 2.04 \u00b1 0.98; M= 2.00 2.18 \u00b1 0.93; M= 2.00 20.95 \u00b1 4.81; M= 21.00 Document judgments per topic 313.11 \u00b1 114.63; M=292.00 372.10 \u00b1 162.63; M=336.50 268.00 \u00b1 116.86; M=247.00 332.33 \u00b1 149.03; M=322.00 Collection Documents 21,258,800 15,702,181 Document length 1,096.18 \u00b1 1,502.45 649.07 \u00b1 1,635.29 Terms 3.40\u00d7 107 (2.33\u00d7 1010 total) 2.36\u00d7 107 (1.02\u00d7 1010 total) Spam scores GroupX Fusion\nTable 2: Overview of experimental results on 2011\u20132014 TREC Session tracks of the TF, Nugget and QCM methods (see \u00a72). The ground-truth oracle shows the ideal performance (\u00a73.3).\n2011 2012 2013 2014 NDCG@10 MRR NDCG@10 MRR NDCG@10 MRR NDCG@10 MRR\nGround-truth oracle 0.777 0.868 0.695 0.865 0.517 0.920 0.410 0.800 TF (first query) 0.371 0.568 0.302 0.523 0.121 0.379 0.120 0.336 TF (last query) 0.358 0.598 0.316 0.586 0.133 0.358 0.156 0.458 TF (all queries) 0.448 0.685 0.348 0.604 0.162 0.477 0.174 0.478 Nugget (RL2) 0.437 0.677 0.352 0.609 0.163 0.488 0.173 0.476 Nugget (RL3) 0.442 0.678 0.360 0.619 0.162 0.488 0.172 0.477 Nugget (RL4) 0.437 0.677 0.352 0.609 0.163 0.488 0.173 0.476 QCM 0.440 0.661 0.342 0.575 0.160 0.484 0.162 0.450\nachieves the highest mean/median overall. Given this peculiar finding, where a generic retrieval model performs better than specialized session search models, we continue with an analysis of the TREC Session search logs.\nIn Fig. 2 we investigate the effect of varying session lengths in the session logs. The distribution of session lengths is shown in the top row of Fig. 2. For the 2011\u20132013 track editions, most sessions consisted of only two queries. The mode of the 2014 edition lies at 5 queries per session. If we examine the performance of the methods on a per-session length basis, we observe that the TF methods perform well for short sessions. This does not come as a surprise,\nas for these sessions there is only a limited history that specialized methods can use. However, the TF method using the concatenation of all queries still performs competitively for longer sessions. This can be explained by the fact that as queries are aggregated over time, a better representation of the user\u2019s information need is created. This aggregated representation naturally emphasizes important theme terms of the session, which is a key component in the QCM [16].\nHow do these methods perform as the search session progresses? Fig. 3 shows the performance of sessions of length five after every user interaction, when using all queries in a session (Fig. 3a) and when using only the previous query (Fig. 3b). We can see that NDCG@10 increases as the session progresses for all methods. Beyond half of the session, the session search methods outperform retrieving according to the last query in the session. We see that, for longer sessions, specialized methods (Nugget, QCM) outperform generic term frequency models. This comes as no surprise. Bennett et al. [2] note that users tend to reformulate and adapt their information needs based on observed results and this is essentially the observation upon which QCM builds.\nFig. 1 and Table 2 reveal a large NDCG@10 gap between the compared methods and the ground-truth oracle. How can we bridge this gap? Table 3 shows a comparison between frequency-based term weighting, the ideal term weighting (\u00a73.4) and the groundtruth oracle (\u00a73.3) for all sessions consisting of 7 unique terms or less (\u00a73.4). Two important observations. There is still plenty of room for improvement using lexical query modeling only. Relatively speaking, around half of the gap between weighting according to term frequency and the ground-truth can be bridged by predicting better term weights. However, the other half of the performance gap cannot be bridged using lexical matching only, but instead requires a notion of semantic matching [9]."}, {"heading": "5. CONCLUSIONS", "text": "We have shown that naive frequency-based term weighting methods perform on par with specialized session search methods on the TREC Session track (2011\u20132014).5 This is due to the fact that shorter sessions are more prominent in the session query logs. On longer sessions, specialized models are able to exploit session history more effectively. Future work should focus on creating benchmarks consisting of longer sessions with complex information needs. Perhaps more importantly, we have looked at the viability of lexical query matching in session search. There is still much room for improvement by re-weighting query terms. How-\n5An open-source implementation of our testbed for evaluating session search is available at https://github.com/cvangysel/sesh.\never, the query/document mismatch is prevalent in session search and methods restricted to lexical query modeling face a very strict performance ceiling. Future work should focus on better lexical query models for session search, in addition to semantic matching and tracking the dynamics of contextualized semantics in search.\nAcknowledgments This work was supported by the Google Faculty Research Award and the Bloomberg Research Grant programs. Any opinions, findings and conclusions or recommendations expressed in this material are the authors\u2019 and do not necessarily reflect those of the sponsors. The authors would like to thank Daan Odijk, David Graus and the anonymous reviewers for their valuable comments and suggestions. REFERENCES [1] M. Bendersky, D. Metzler, and W. B. Croft. Effective query\nformulation with multiple information sources. In SIGIR, pages 443\u2013452. ACM, 2012.\n[2] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais, P. Bailey, F. Borisyuk, and X. Cui. Modeling the impact of short- and long-term behavior on search personalization. In SIGIR, pages 185\u2013194. ACM, 2012. [3] B. Carterette, E. Kanoulas, M. M. Hall, and P. D. Clough. Overview of the trec 2014 session track. In TREC, 2014. [4] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 14(5):441\u2013465, 2011. [5] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you want to take notes?: identifying research missions in yahoo! search pad. In WWW, pages 321\u2013330. ACM, 2010. [6] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. Techn. report, 2012. [7] A. Hassan, R. W. White, S. T. Dumais, and Y.-M. Wang. Struggling or exploring?: disambiguating long search sessions. In WSDM, pages 53\u201362. ACM, 2014. [8] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and J. Teevan. Modeling and analysis of cross-session search tasks. In SIGIR, pages 5\u201314. ACM, 2011. [9] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in Information Retrieval, 7(5):343\u2013469, June 2014.\n[10] J. Luo, X. Dong, and H. Yang. Modeling rich interactions in session search - georgetown university at trec 2014 session track. Techn. report, 2014. [11] J. Luo, S. Zhang, and H. Yang. Win-win search: Dual-agent stochastic game in session search. In SIGIR, pages 587\u2013596. ACM, 2014. [12] J. Luo, X. Dong, and H. Yang. Session search by direct policy learning. In ICTIR, pages 261\u2013270. ACM, 2015. [13] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. IPM, 40(5):735\u2013750, 2004. [14] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: exploring intrinsic diversity in web search. In SIGIR, pages 463\u2013472. ACM, 2013. [15] TREC. Session Track, 2009\u20132014. [16] H. Yang, D. Guan, and S. Zhang. The query change model:\nModeling session search as a markov decision process. TOIS, 33(4): 20:1\u201320:33, May 2015."}], "references": [{"title": "Effective query formulation with multiple information sources", "author": ["M. Bendersky", "D. Metzler", "W.B. Croft"], "venue": "In SIGIR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Modeling the impact of short- and long-term behavior on search personalization", "author": ["P.N. Bennett", "R.W. White", "W. Chu", "S.T. Dumais", "P. Bailey", "F. Borisyuk", "X. Cui"], "venue": "In SIGIR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Overview of the trec 2014 session track", "author": ["B. Carterette", "E. Kanoulas", "M.M. Hall", "P.D. Clough"], "venue": "In TREC,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Efficient and effective spam filtering and re-ranking for large web datasets", "author": ["G.V. Cormack", "M.D. Smucker", "C.L. Clarke"], "venue": "Information retrieval,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Do you want to take notes?: identifying research missions in yahoo! search pad", "author": ["D. Donato", "F. Bonchi", "T. Chi", "Y. Maarek"], "venue": "In WWW,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Effective structured query formulation for session search", "author": ["D. Guan", "H. Yang", "N. Goharian"], "venue": "Techn. report,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Struggling or exploring?: disambiguating long search sessions", "author": ["A. Hassan", "R.W. White", "S.T. Dumais", "Y.-M. Wang"], "venue": "In WSDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Modeling and analysis of cross-session search tasks", "author": ["A. Kotov", "P.N. Bennett", "R.W. White", "S.T. Dumais", "J. Teevan"], "venue": "In SIGIR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Semantic matching in search", "author": ["H. Li", "J. Xu"], "venue": "Found. & Tr. in Information Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Modeling rich interactions in session search - georgetown university at trec 2014 session track", "author": ["J. Luo", "X. Dong", "H. Yang"], "venue": "Techn. report,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Win-win search: Dual-agent stochastic game in session search", "author": ["J. Luo", "S. Zhang", "H. Yang"], "venue": "In SIGIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Session search by direct policy learning", "author": ["J. Luo", "X. Dong", "H. Yang"], "venue": "In ICTIR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Combining the language model and inference network approaches to retrieval", "author": ["D. Metzler", "W.B. Croft"], "venue": "IPM, 40(5):735\u2013750,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Toward whole-session relevance: exploring intrinsic diversity in web search", "author": ["K. Raman", "P.N. Bennett", "K. Collins-Thompson"], "venue": "In SIGIR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "The query change model: Modeling session search as a markov decision process", "author": ["H. Yang", "D. Guan", "S. Zhang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Many complex information seeking tasks, such as planning a trip or buying a car, cannot sufficiently be expressed in a single query [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 13, "context": "These multi-faceted tasks are exploratory, comprehensive, survey-like or comparative in nature [14] and require multiple search iterations to be adequately answered [8].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "These multi-faceted tasks are exploratory, comprehensive, survey-like or comparative in nature [14] and require multiple search iterations to be adequately answered [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 4, "context": "[5] note that 10% of the user sessions (more than 25% of query volume) consists of such complex information needs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[16] introduce the Query Change Model (QCM), which uses lexical editing changes between consecutive queries in addition to query terms occurring in previously retrieved documents, to improve session search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Query models are then linearly combined for every document, based on query recency [16] or document satisfaction [3, 10], into a session-wide lexical query", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "Query models are then linearly combined for every document, based on query recency [16] or document satisfaction [3, 10], into a session-wide lexical query", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "Query models are then linearly combined for every document, based on query recency [16] or document satisfaction [3, 10], into a session-wide lexical query", "startOffset": 113, "endOffset": 120}, {"referenceID": 2, "context": "However, there has been a clear trend towards the use of supervised learning [3, 12, 16] and external data sources [6, 11].", "startOffset": 77, "endOffset": 88}, {"referenceID": 11, "context": "However, there has been a clear trend towards the use of supervised learning [3, 12, 16] and external data sources [6, 11].", "startOffset": 77, "endOffset": 88}, {"referenceID": 14, "context": "However, there has been a clear trend towards the use of supervised learning [3, 12, 16] and external data sources [6, 11].", "startOffset": 77, "endOffset": 88}, {"referenceID": 5, "context": "However, there has been a clear trend towards the use of supervised learning [3, 12, 16] and external data sources [6, 11].", "startOffset": 115, "endOffset": 122}, {"referenceID": 10, "context": "However, there has been a clear trend towards the use of supervised learning [3, 12, 16] and external data sources [6, 11].", "startOffset": 115, "endOffset": 122}, {"referenceID": 5, "context": "[6] perform lexical query expansion by adding higherorder n-grams to queries by mining document snippets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] expand document representations by including incoming anchor", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] introduce a linear point-wise learning-to-rank model that predicts relevance given a document and query change features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The use of machine-learned ranking and the expansion of query and document representations is meant to address a specific instance of a wider problem in information retrieval, namely the query document mismatch [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "Existing session search methods [6, 16] can be expressed in this formalism as follows: Term frequency (TF) Terms in a query are weighted according to their frequency in the query (i.", "startOffset": 32, "endOffset": 39}, {"referenceID": 14, "context": "Existing session search methods [6, 16] can be expressed in this formalism as follows: Term frequency (TF) Terms in a query are weighted according to their frequency in the query (i.", "startOffset": 32, "endOffset": 39}, {"referenceID": 2, "context": "Using the last query corresponds to the official baseline of the TREC Session track [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "Nugget Nugget [6] is a method for effective structured query formulation for session search.", "startOffset": 14, "endOffset": 17}, {"referenceID": 14, "context": "Query Change Model (QCM) QCM [16] uses syntactic editing changes between consecutive queries in addition to query changes and previous SERPs to enhance session search.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "Spam documents are removed before indexing by filtering out documents with scores (GroupX and Fusion, respectively) below 70 [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 12, "context": ", unigram terms) on a per-session basis, construct a structured Indri query [13] and query the document collection using pyndri.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": ", Dirichlet smoothing with \u03bc = 2500) and uniform query aggregation for all methods (different from the smoothing used for QCM in [16]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "For QCM, we use the parameter configuration as described in [12, 16]: \u03b1 = 2.", "startOffset": 60, "endOffset": 68}, {"referenceID": 14, "context": "For QCM, we use the parameter configuration as described in [12, 16]: \u03b1 = 2.", "startOffset": 60, "endOffset": 68}, {"referenceID": 0, "context": "[1], we optimize NDCG@10 for every session using a grid search over the term weight space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] on the 2012 and 2013 tracks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "This aggregated representation naturally emphasizes important theme terms of the session, which is a key component in the QCM [16].", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "[2] note that users tend to reformulate and adapt their information needs based on observed results and this is essentially the observation upon which QCM builds.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "However, the other half of the performance gap cannot be bridged using lexical matching only, but instead requires a notion of semantic matching [9].", "startOffset": 145, "endOffset": 148}], "year": 2016, "abstractText": "Lexical query modeling has been the leading paradigm for session search. In this paper, we analyze TREC session query logs and compare the performance of different lexical matching approaches for session search. Naive methods based on term frequency weighing perform on par with specialized session models. In addition, we investigate the viability of lexical query models in the setting of session search. We give important insights into the potential and limitations of lexical query modeling for session search and propose future directions for the field of session search.", "creator": "LaTeX with hyperref package"}}}