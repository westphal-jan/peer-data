{"id": "1608.07630", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2016", "title": "Global Analysis of Expectation Maximization for Mixtures of Two Gaussians", "abstract": "expectation maximization ( also em ) is among the most popular algorithms for estimating parameters range of statistical models. here however, describing em, considering which is always an iterative algorithm based on the maximum likelihood principle, simply is generally essentially only guaranteed efforts to find those stationary points of the likelihood probability objective, measuring and these points may often be too far far from determining any maximizer. this article addresses this disconnect between the statistical analytical principles explained behind em and calculating its algorithmic properties. assuming specifically, effectively it provides a systematic global analysis of em methods for specific models in which the predicted observations comprise primarily an isolated i. + i. d. sample driven from a natural mixture of two gaussians. eventually this is achieved by ( i ) studying constructing the sequence of relative parameters from all idealized analytic execution of em obtained in both the simulated infinite duration sample limit, and allowing fully integration characterizing performing the fraction limit points \u03b1 of the sequence in equivalent terms of the posterior initial parameters ; producing and returning then ( module ii ) algorithms based on maintaining this convergence analysis, establishing a statistical consistency ( variance or lack thereof ) for the actual underlying sequence progression of parameters typically produced by em.", "histories": [["v1", "Fri, 26 Aug 2016 23:53:43 GMT  (111kb,D)", "http://arxiv.org/abs/1608.07630v1", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.CO stat.ML stat.TH", "authors": ["ji xu", "daniel j hsu", "arian maleki"], "accepted": true, "id": "1608.07630"}, "pdf": {"name": "1608.07630.pdf", "metadata": {"source": "CRF", "title": "Global Analysis of Expectation Maximization for Mixtures of Two Gaussians", "authors": ["Ji Xu", "Daniel Hsu", "Arian Maleki"], "emails": ["jixu@cs.columbia.edu,", "djhsu@cs.columbia.edu,", "arian@stat.columbia.edu"], "sections": [{"heading": null, "text": "1 Introduction\nSince Fisher\u2019s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering. The asymptotic consistency and optimality of MLEs have provided users with the confidence that, at least in some sense, there is no better way to estimate parameters for many standard statistical models. Despite its appealing properties, computing the MLE is often intractable. Indeed, this is the case for many latent variable models {f(Y, z;\u03b7)}, where the latent variables z are not observed. For each setting of the parameters \u03b7, the marginal distribution of the observed data Y is (for discrete z)\nf(Y;\u03b7) = \u2211 z f(Y, z;\u03b7) .\nIt is this marginalization over latent variables that typically causes the computational difficulty. Furthermore, many algorithms based on the MLE principle are only known to find stationary points of the likelihood objective (e.g., local maxima), and these points are not necessarily the MLE.\nE-mail: jixu@cs.columbia.edu, djhsu@cs.columbia.edu, arian@stat.columbia.edu\nar X\niv :1\n60 8.\n07 63\n0v 1\n[ m\nat h.\nST ]\n2 6\n1.1 Expectation Maximization\nAmong the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984). EM is an iterative algorithm for climbing the likelihood objective starting from an initial setting of the parameters \u03b7\u0302\u30080\u3009. In iteration t, EM performs the following steps:\nE-step: Q\u0302(\u03b7 | \u03b7\u0302\u3008t\u3009) , \u2211 z f(z | Y; \u03b7\u0302\u3008t\u3009) log f(Y, z;\u03b7) , (1)\nM-step: \u03b7\u0302\u3008t+1\u3009 , arg max \u03b7 Q\u0302(\u03b7 | \u03b7\u0302\u3008t\u3009) , (2)\nIn many applications, each step is intuitive and can be performed very efficiently. Despite the popularity of EM, as well as the numerous theoretical studies of its behavior, many important questions about its performance\u2014such as its convergence rate and accuracy\u2014have remained unanswered. The goal of this paper is to address these questions for specific models (described in Section 1.2) in which the observation Y is an i.i.d. sample from a mixture of two Gaussians.\nTowards this goal, we study an idealized execution of EM in the large sample limit, where the E-step is modified to be computed over an infinitely large i.i.d. sample from a Gaussian mixture distribution in the model. In effect, in the formula for Q\u0302(\u03b7 | \u03b7\u0302\u3008t\u3009), we replace the observed data Y with a random variable Y \u223c f(y;\u03b7?) for some Gaussian mixture parameters \u03b7? and then take its expectation. The resulting E- and M-steps in iteration t are\nE-step: Q(\u03b7 | \u03b7\u3008t\u3009) , EY [\u2211 z f(z | Y ;\u03b7\u3008t\u3009) log f(Y , z;\u03b7) ] , (3)\nM-step: \u03b7\u3008t+1\u3009 , arg max \u03b7 Q(\u03b7 | \u03b7\u3008t\u3009) . (4)\nThis sequence of parameters (\u03b7\u3008t\u3009)t\u22650 is fully determined by the initial setting \u03b7\u30080\u3009. We refer to this idealization as Population EM. Not only does Population EM shed light on the dynamics of EM in the large sample limit, but it can also reveal some of the fundamental limitations of EM. Indeed, if Population EM cannot provide an accurate estimate for the parameters \u03b7?, then intuitively, one would not expect the EM algorithm with a finite sample size to do so either. (To avoid confusion, we refer the original EM algorithm run with a finite sample as Sample-based EM.)\n1.2 Models and Main Contributions\nIn this paper, we study EM in the context of two simple yet popular and well-studied Gaussian mixture models. The two models, along with the corresponding Sample-based EM and Population EM updates, are as follows:\nModel 1. The observation Y is an i.i.d. sample from the mixture distribution 0.5N(\u2212\u03b8?,\u03a3) + 0.5N(\u03b8?,\u03a3); \u03a3 is a known covariance matrix in Rd, and \u03b8? is the unknown parameter of interest.\n1. Sample-based EM iteratively updates its estimate of \u03b8? according to the following equation:\n\u03b8\u0302 \u3008t+1\u3009 = 1\nn n\u2211 i=1 ( 2wd ( yi, \u03b8\u0302 \u3008t\u3009)\u2212 1)yi, (5)\nwhere y1, . . . ,yn are the independent draws that comprise Y,\nwd(y,\u03b8) , \u03c6d(y \u2212 \u03b8)\n\u03c6d(y \u2212 \u03b8) + \u03c6d(y + \u03b8) ,\nand \u03c6d is the density of a Gaussian random vector with mean 0 and covariance \u03a3.\n2. Population EM iteratively updates its estimate according to the following equation:\n\u03b8\u3008t+1\u3009 = E(2wd(Y ,\u03b8\u3008t\u3009)\u2212 1)Y , (6)\nwhere Y \u223c 0.5N(\u2212\u03b8?,\u03a3) + 0.5N(\u03b8?,\u03a3).\nModel 2. The observation Y is an i.i.d. sample from the mixture distribution 0.5N(\u00b5?1,\u03a3) + 0.5N(\u00b5?2,\u03a3). Again, \u03a3 is known, and (\u00b5?1,\u00b5?2) are the unknown parameters of interest.\n1. Sample-based EM iteratively updates its estimate of \u00b5?1 and \u00b5?2 at every iteration according to the following equations:\n\u00b5\u0302 \u3008t+1\u3009 1 =\n\u2211n i=1 vd(yi, \u00b5\u0302 \u3008t\u3009 1 , \u00b5\u0302\n\u3008t\u3009 2 )yi\u2211n\ni=1 vd(yi, \u00b5\u0302 \u3008t\u3009 1 , \u00b5\u0302 \u3008t\u3009 2 )\n, (7)\n\u00b5\u0302 \u3008t+1\u3009 2 =\n\u2211n i=1(1\u2212 vd(yi, \u00b5\u0302 \u3008t\u3009 1 , \u00b5\u0302\n\u3008t\u3009 2 ))yi\u2211n\ni=1(1\u2212 vd(yi, \u00b5\u0302 \u3008t\u3009 1 , \u00b5\u0302 \u3008t\u3009 2 ))\n, (8)\nwhere y1, . . . ,yn are the independent draws that comprise Y, and\nvd(y,\u00b51,\u00b52) , \u03c6d(y \u2212 \u00b51)\n\u03c6d(y \u2212 \u00b51) + \u03c6d(y \u2212 \u00b52) .\n2. Population EM iteratively updates its estimates according to the following equations:\n\u00b5 \u3008t+1\u3009 1 =\nEvd(Y ,\u00b5 \u3008t\u3009 1 ,\u00b5 \u3008t\u3009 2 )Y\nEvd(Y ,\u00b5 \u3008t\u3009 1 ,\u00b5 \u3008t\u3009 2 )\n, (9)\n\u00b5 \u3008t+1\u3009 2 =\nE(1\u2212 vd(Y ,\u00b5 \u3008t\u3009 1 ,\u00b5 \u3008t\u3009 2 ))Y\nE(1\u2212 vd(Y ,\u00b5 \u3008t\u3009 1 ,\u00b5 \u3008t\u3009 2 ))\n, (10)\nwhere Y \u223c 0.5N(\u00b5?1,\u03a3) + 0.5N(\u00b5?2,\u03a3).\nOur main contribution in this paper is a new characterization of the stationary points and dynamics of EM in both of the above models.\n1. We prove convergence for the sequence of iterates for Population EM from each model: the sequence (\u03b8\u3008t\u3009)t\u22650 converges to either \u03b8?, \u2212\u03b8?, or 0; the sequence ((\u00b5\u3008t\u30091 ,\u00b5 \u3008t\u3009 2 ))t\u22650 converges to\neither (\u00b5?1,\u00b5?2), (\u00b5?2,\u00b5?1), or ((\u00b5?1 + \u00b5?2)/2, (\u00b5?1 + \u00b5?2)/2). We also fully characterize the initial parameter settings that lead to each limit point.\n2. Using this convergence result for Population EM, we also prove that the limits of the Samplebased EM iterates converge in probability to the unknown parameters of interest, as long as Sample-based EM is initialized at points where Population EM would converge to these parameters as well.\nFormal statements of our results are given in Section 2.\n1.3 Background and Related Work\nThe EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method for computing parameter estimates from incomplete data. Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983). Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008). However, these analyses do not distinguish between global maximizers and other stationary points (except, e.g., when the likelihood function is unimodal). Thus, as an optimization algorithm for maximizing the log-likelihood objective, the \u201cworst-case\u201d performance of EM is somewhat discouraging.\nFor a more optimistic perspective on EM, one may consider a \u201cbest-case\u201d analysis, where (i) the data are an iid sample from a distribution in the given model, (ii) the sample size is sufficiently large, and (iii) the starting point for EM is sufficiently close to the parameters of the data generating distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses, and (iii) is a generous assumption that may be satisfied in certain cases. Redner and Walker (1984) show that in such a favorable scenario, EM converges to the MLE almost surely for a broad class of mixture models. Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM. Thus, EM may be used in a tractable two-stage estimation procedures given a first-stage pilot estimator that can be efficiently computed.\nIndeed, for the special case of Gaussian mixtures, researchers in theoretical computer science and machine learning have developed efficient algorithms that deliver the highly accurate parameter estimates under appropriate conditions. Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894).\nMost relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption. Lastly, for the specific case of our Model 1, Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant of EM) when started in a sufficiently small neighborhood around the true parameters; here, the size of the neighborhood grows with the separation between the two\nmixture components (which must be sufficiently large). Their analysis also proceeds by studying Population EM, and then relating Sample-based EM to it. Remarkably, by focusing attention on the local region around the true parameters, they obtain non-asymptotic bounds on the parameter estimation error. Our work is complementary to their result in that we focus on asymptotic limits rather than finite sample analysis. This allows us to provide a global analysis of EM, without any separation assumption; such an analysis cannot be deduced from the results of Balakrishnan et al. by taking limits.\n2 Analysis of EM for Mixtures of Two Gaussians\nIn this section, we present our results for Population EM and Sample-based EM under both Model 1 and Model 2, and also discuss further implications about the expected log-likelihood function. Without loss of generality, we may assume that the known covariance matrix \u03a3 is the identity matrix Id. Throughout, we denote the Euclidean norm by \u2016 \u00b7 \u2016, and the signum function by sgn(\u00b7) (where sgn(0) = 0, sgn(z) = 1 if z > 0, and sgn(z) = \u22121 if z < 0).\n2.1 Main Results for Population EM\nWe present results for Population EM for both models, starting with Model 1.\nTheorem 1. Assume \u03b8? \u2208 Rd \\ {0}. Let (\u03b8\u3008t\u3009)t\u22650 denote the Population EM iterates for Model 1, and suppose \u3008\u03b8\u30080\u3009,\u03b8?\u3009 6= 0. There exists \u03ba\u03b8 \u2208 (0, 1)\u2014depending only on \u03b8? and \u03b8\u30080\u3009\u2014such that\u2225\u2225\u2225\u03b8\u3008t+1\u3009 \u2212 sgn(\u3008\u03b8\u30080\u3009,\u03b8?\u3009)\u03b8?\u2225\u2225\u2225 \u2264 \u03ba\u03b8 \u00b7 \u2225\u2225\u2225\u03b8\u3008t\u3009 \u2212 sgn(\u3008\u03b8\u30080\u3009,\u03b8?\u3009)\u03b8?\u2225\u2225\u2225 .\nThe proof of Theorem 1, as well as all other omitted proofs, is given in Appendix A. Theorem 1 asserts that if \u03b8\u30080\u3009 is not on the hyperplane {x \u2208 Rd : \u3008x,\u03b8?\u3009 = 0}, then the sequence (\u03b8\u3008t\u3009)t\u22650 converges to either \u03b8? or \u2212\u03b8?.\nOur next result shows that if \u3008\u03b8\u30080\u3009,\u03b8?\u3009 = 0, then (\u03b8\u3008t\u3009)t\u22650 still converges, albeit to 0.\nTheorem 2. Let (\u03b8\u3008t\u3009)t\u22650 denote the Population EM iterates for Model 1. If \u3008\u03b8\u30080\u3009,\u03b8?\u3009 = 0, then\n\u03b8\u3008t\u3009 \u2192 0 as t\u2192\u221e .\nTheorems 1 and 2 together characterize the fixed points of Population EM for Model 1, and fully specify the conditions under which each fixed point is reached. The results are simply summarized in the following corollary.\nCorollary 1. If (\u03b8\u3008t\u3009)t\u22650 denote the Population EM iterates for Model 1, then\n\u03b8\u3008t\u3009 \u2192 sgn(\u3008\u03b8\u30080\u3009,\u03b8?\u3009)\u03b8? as t\u2192\u221e .\nWe now discuss Population EM with Model 2. To state our results more concisely, we use the following re-parameterization of the model parameters and Population EM iterates:\na\u3008t\u3009 , \u00b5 \u3008t\u3009 1 + \u00b5 \u3008t\u3009 2\n2 \u2212 \u00b5\n? 1 + \u00b5 ? 2\n2 , b\u3008t\u3009 ,\n\u00b5 \u3008t\u3009 2 \u2212 \u00b5 \u3008t\u3009 1\n2 , \u03b8? , \u00b5?2 \u2212 \u00b5?1 2 . (11)\nIf the sequence of Population EM iterates ((\u00b5\u3008t\u30091 ,\u00b5 \u3008t\u3009 2 ))t\u22650 converges to (\u00b5 ? 1,\u00b5 ? 2), then we expect b\u3008t\u3009 \u2192 \u03b8?. Hence, we also define \u03b2\u3008t\u3009 as the angle between b\u3008t\u3009 and \u03b8?, i.e.,\n\u03b2\u3008t\u3009 , arccos ( \u3008b\u3008t\u3009,\u03b8?\u3009 \u2016b\u3008t\u3009\u2016\u2016\u03b8?\u2016 ) \u2208 [0, \u03c0] .\n(This is well-defined as long as b\u3008t\u3009 6= 0 and \u03b8? 6= 0.) We first present results on Population EM with Model 2 under the initial condition \u3008b\u30080\u3009,\u03b8?\u3009 6= 0.\nTheorem 3. Assume \u03b8? \u2208 Rd \\ {0}. Let (a\u3008t\u3009, b\u3008t\u3009)t\u22650 denote the (re-parameterized) Population EM iterates for Model 2, and suppose \u3008b\u30080\u3009,\u03b8?\u3009 6= 0. Then b\u3008t\u3009 6= 0 for all t \u2265 0. Furthermore, there exist \u03baa \u2208 (0, 1)\u2014depending only on \u2016\u03b8?\u2016 and |\u3008b\u30080\u3009,\u03b8?\u3009/\u2016b\u30080\u3009\u2016|\u2014and \u03ba\u03b2 \u2208 (0, 1)\u2014depending only on \u2016\u03b8?\u2016, \u3008b\u30080\u3009,\u03b8?\u3009/\u2016b\u30080\u3009\u2016, \u2016a\u30080\u3009\u2016, and \u2016b\u30080\u3009\u2016\u2014such that\n\u2016a\u3008t+1\u3009\u20162 \u2264 \u03ba2a \u00b7 \u2016a\u3008t\u3009\u20162 + \u2016\u03b8?\u20162 sin2(\u03b2\u3008t\u3009)\n4 ,\nsin(\u03b2\u3008t+1\u3009) \u2264 \u03bat\u03b2 \u00b7 sin(\u03b2\u30080\u3009) .\nBy combining the two inequalities from Theorem 3, we conclude\n\u2016a\u3008t+1\u3009\u20162 = \u03ba2ta \u2016a\u30080\u3009\u20162 + \u2016\u03b8?\u20162\n4 t\u2211 \u03c4=0 \u03ba2\u03c4a \u00b7 sin2(\u03b2\u3008t\u2212\u03c4\u3009)\n\u2264 \u03ba2ta \u2016a\u30080\u3009\u20162 + \u2016\u03b8?\u20162\n4 t\u2211 \u03c4=0 \u03ba2\u03c4a \u03ba 2(t\u2212\u03c4) \u03b2 \u00b7 sin 2(\u03b2\u30080\u3009)\n\u2264 \u03ba2ta \u2016a\u30080\u3009\u20162 + \u2016\u03b8?\u20162 4 t ( max { \u03baa, \u03ba\u03b2 })t sin2(\u03b2\u30080\u3009) .\nTheorem 3 shows that the re-parameterized Population EM iterates converge, at a linear rate, to the average of the two means (\u00b5?1 +\u00b5?2)/2, as well as the line spanned by \u03b8\n?. The theorem, however, does not provide any information on the convergence of the magnitude of b\u3008t\u3009 to the magnitude of \u03b8?. This is given in the next theorem.\nTheorem 4. Assume \u03b8? \u2208 Rd \\{0}. Let (a\u3008t\u3009, b\u3008t\u3009)t\u22650 denote the (re-parameterized) Population EM iterates for Model 2, and suppose \u3008b\u30080\u3009,\u03b8?\u3009 6= 0. Then there exist T0 > 0, \u03bab \u2208 (0, 1), and cb > 0\u2014all depending only on \u2016\u03b8?\u2016, |\u3008b\u30080\u3009,\u03b8?\u3009/\u2016b\u30080\u3009\u2016|, \u2016a\u30080\u3009\u2016, and \u2016b\u30080\u3009\u2016\u2014such that\u2225\u2225\u2225b\u3008t+1\u3009 \u2212 sgn(\u3008b\u30080\u3009,\u03b8?\u3009)\u03b8?\u2225\u2225\u22252 \u2264 \u03ba2b \u00b7 \u2225\u2225\u2225b\u3008t\u3009 \u2212 sgn(\u3008b\u30080\u3009,\u03b8?\u3009)\u03b8?\u2225\u2225\u22252 + cb \u00b7 \u2016a\u3008t\u3009\u2016 \u2200t > T0 .\nIf \u3008b\u30080\u3009,\u03b8?\u3009 = 0, then we show convergence of the (re-parameterized) Population EM iterates to the degenerate solution (0,0).\nTheorem 5. Let (a\u3008t\u3009, b\u3008t\u3009)t\u22650 denote the (re-parameterized) Population EM iterates for Model 2. If \u3008b\u30080\u3009,\u03b8?\u3009 = 0, then\n(a\u3008t\u3009, b\u3008t\u3009) \u2192 (0,0) as t\u2192\u221e .\nTheorems 3, 4, and 5 together characterize the fixed points of Population EM for Model 2, and fully specify the conditions under which each fixed point is reached. The results are simply summarized in the following corollary.\nCorollary 2. If (a\u3008t\u3009, b\u3008t\u3009)t\u22650 denote the (re-parameterized) Population EM iterates for Model 2, then\na\u3008t\u3009 \u2192 \u00b5 ? 1 + \u00b5 ? 2\n2 as t\u2192\u221e ,\nb\u3008t\u3009 \u2192 sgn(\u3008b\u30080\u3009,\u00b5?2 \u2212 \u00b5?1\u3009) \u00b5?2 \u2212 \u00b5?1\n2 as t\u2192\u221e .\n2.2 Main Results for Sample-based EM\nUsing the results on Population EM presented in the above section, we can now establish consistency of (Sample-based) EM. We focus attention on Model 2, as the same results for Model 1 easily follow as a corollary. First, we state a simple connection between the Population EM and Sample-based EM iterates.\nTheorem 6. Suppose Population EM and Sample-based EM for Model 2 have the same initial parameters: \u00b5\u0302\u30080\u30091 = \u00b5 \u30080\u3009 1 and \u00b5\u0302 \u30080\u3009 2 = \u00b5 \u30080\u3009 2 . Then for each iteration t \u2265 0,\n\u00b5\u0302 \u3008t\u3009 1 \u2192 \u00b5 \u3008t\u3009 1 and \u00b5\u0302 \u3008t\u3009 2 \u2192 \u00b5 \u3008t\u3009 2 as n\u2192\u221e ,\nwhere convergence is in probability.\nNote that Theorem 6 does not necessarily imply that the fixed point of Sample-based EM (when initialized at (\u00b5\u0302\u30080\u30091 , \u00b5\u0302 \u30080\u3009 2 ) = (\u00b5 \u30080\u3009 1 ,\u00b5 \u30080\u3009 2 )) is the same as that of Population EM. It is conceivable that as t\u2192\u221e, the discrepancy between (the iterates of) Sample-based EM and Population EM increases. We show that this is not the case: the fixed points of Sample-based EM indeed converge to the fixed points of Population EM.\nTheorem 7. Suppose Population EM and Sample-based EM for Model 2 have the same initial parameters: \u00b5\u0302\u30080\u30091 = \u00b5 \u30080\u3009 1 and \u00b5\u0302 \u30080\u3009 2 = \u00b5 \u30080\u3009 2 . If \u3008\u00b5 \u30080\u3009 2 \u2212 \u00b5 \u30080\u3009 1 ,\u03b8 ?\u3009 6= 0, then\nlim sup t\u2192\u221e\n|\u00b5\u0302\u3008t\u30091 \u2212 \u00b5 \u3008t\u3009 1 | \u2192 0 and lim sup t\u2192\u221e |\u00b5\u0302\u3008t\u30092 \u2212 \u00b5 \u3008t\u3009 2 | \u2192 0 as n\u2192\u221e ,\nwhere convergence is in probability.\n2.3 Population EM and Expected Log-likelihood\nDo the results we derived in the last section regarding the performance of EM provide any information on the performance of other ascent algorithms, such as gradient ascent, that aim to maximize the loglikelihood function? To address this question, we show how our analysis can determine the stationary points of the expected log-likelihood and characterize the shape of the expected log-likelihood in a neighborhood of the stationary points. Let G(\u03b7) denote the expected log-likelihood, i.e.,\nG(\u03b7) , E(log f\u03b7(Y )) = \u222b f(y;\u03b7\u2217) log f(y;\u03b7) dy,\nwhere \u03b7\u2217 denotes the true parameter value. Also consider the following standard regularity conditions:\nR1 The family of probability density functions f(y;\u03b7) have common support. R2 \u2207\u03b7 \u222b f(y;\u03b7\u2217) log f(y;\u03b7) dy = \u222b f(y;\u03b7\u2217)\u2207\u03b7 log f(y;\u03b7) dy, where \u2207\u03b7 denotes the gradient with\nrespect to \u03b7. R3 \u2207\u03b7(E \u2211 z f(z | Y ;\u03b7\u3008t\u3009)) log f(Y , z; \u03b7) = E \u2211 z f(z | Y ;\u03b7\u3008t\u3009)\u2207\u03b7 log f(Y , z;\u03b7).\nThese conditions can be easily confirmed for many models including the Gaussian mixture models. The following theorem connects the fixed points of the Population EM and the stationary points of the expected log-likelihood.\nLemma 1. Let \u03b7\u0304 \u2208 Rd denote a stationary point of G(\u03b7). Also assume that Q(\u03b7 | \u03b7\u3008t\u3009) has a unique and finite stationary point in terms of \u03b7 for every \u03b7\u3008t\u3009, and this stationary point is its global maxima. Then, if the model satisfies conditions R1\u2013R3, and the Population EM algorithm is initialized at \u03b7\u0304, it will stay at \u03b7\u0304. Conversely, any fixed point of Population EM is a stationary point of G(\u03b7).\nProof. Let \u03b7\u0304 denote a stationary point of G(\u03b7). We first prove that \u03b7\u0304 is a stationary point of Q(\u03b7 | \u03b7\u0304).\n\u2207\u03b7Q(\u03b7 | \u03b7\u0304) \u2223\u2223 \u03b7=\u03b7\u0304 = \u222b \u2211 z f(z | y; \u03b7\u0304) \u2207\u03b7f(y, z;\u03b7) \u2223\u2223 \u03b7=\u03b7\u0304 f(y, z; \u03b7\u0304) f(y;\u03b7\u2217) dy\n= \u222b \u2211 z \u2207\u03b7f(y, z;\u03b7) \u2223\u2223 \u03b7=\u03b7\u0304 f(y; \u03b7\u0304) f(y;\u03b7\u2217) dy\n= \u222b \u2207\u03b7f(y,\u03b7)\u2223\u2223\u03b7=\u03b7\u0304 f(y; \u03b7\u0304) f(y;\u03b7\u2217) dy = 0 ,\nwhere the last equality is using the fact that \u03b7\u0304 is a stationary point of G(\u03b7). Since Q(\u03b7 | \u03b7\u0304) has a unique stationary point, and we have assumed that the unique stationary point is its global maxima, then Population EM will stay at that point. The proof of the other direction is similar.\nRemark 1. The fact that \u03b7\u2217 is the global maximizer of G(\u03b7) is well-known in the statistics and machine learning literature (e.g., Conniffe, 1987). Furthermore, the fact that \u03b7\u2217 is a global maximizer of Q(\u03b7 | \u03b7\u2217) is known as the self-consistency property (Balakrishnan et al., 2014).\nIt is straightforward to confirm the conditions of Lemma 1 for mixtures of Gaussians. This lemma confirms that Population EM may be trapped in every local maxima. However, less intuitively it may get stuck at local minima or saddle points as well. Our next result characterizes the stationary points of G(\u03b8) for Model 1.\nCorollary 3. G(\u03b8) has only three stationary points. If d = 1 (so \u03b8 = \u03b8 \u2208 R), then 0 is a local minima of G(\u03b8), while \u03b8\u2217 and \u2212\u03b8\u2217 are global maxima. If d > 1, then 0 is a saddle point, and \u03b8? and \u2212\u03b8? are global maxima.\nThe proof is a straightforward result of Lemma 1 and Corollary 1. The phenomenon that Population EM may stuck in local minima or saddle points also happens in Model 2. We can employ Corollary 2 and Lemma 1 to explain the shape of the expected log-likelihood function G. To simplify the notation, we consider the re-parametrization a , \u00b51+\u00b522 and b , \u00b52\u2212\u00b51 2 .\nCorollary 4. G(a, b) has three stationary points:( \u00b5?1 + \u00b5 ? 2\n2 , \u00b5?2 \u2212 \u00b5?1 2\n) ,\n( \u00b5?1 + \u00b5 ? 2\n2 , \u00b5?1 \u2212 \u00b5?2 2\n) , and\n( \u00b5?1 + \u00b5 ? 2\n2 , \u00b5?1 + \u00b5 ? 2 2\n) .\nThe first two points are global maxima. The third point is a saddle point.\n3 Concluding Remarks\nOur analysis of Population EM and Sample-based EM shows that the EM algorithm can, at least for the Gaussian mixture models studied in this work, compute statistically consistent parameter estimates. Previous analyses of EM only established such results for specific methods of initializing\nEM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit. However, in any real scenario, the large sample limit may not accurately characterize the behavior of EM. Therefore, these specific methods for initialization, as well as non-asymptotic analysis, are clearly still needed to understand and effectively apply EM.\nThere are several interesting directions concerning EM that we hope to pursue in follow-up work. The first considers the behavior of EM when the dimension d = dn may grow with the sample size n. Our proof of Theorem 7 reveals that the parameter error of the t-th iterate (in Euclidean norm) is of the order \u221a d/n as t \u2192 \u221e. Therefore, we conjecture that the theorem still holds as long as dn = o(n). This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn \u221d n as n\u2192\u221e (Barkai and Sompolinsky, 1994).\nAnother natural direction is to extend these results to more general Gaussian mixture models (e.g., with unequal mixing weights or unequal covariances) and other latent variable models.\nAcknowledgements. The second named author thanks Yash Deshpande and Sham Kakade for many helpful initial discussions. JX and AM were partially supported by NSF grant CCF-1420328. DH was partially supported by NSF grant DMREF-1534910 and a Sloan Fellowship.\nReferences\nD. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In Eighteenth Annual Conference on Learning Theory, pages 458\u2013469, 2005.\nS. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. The Annals of Applied Probability, 15(1A):69\u201392, 2005.\nS. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. ArXiv e-prints, August 2014.\nN. Barkai and H. Sompolinsky. Statistical mechanics of the maximum-likelihood density estimation. Physical Review E, 50(3):1766\u20131769, Sep 1994.\nM. Belkin and K. Sinha. Polynomial learning of distribution families. In Fifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 103\u2013112, 2010.\nS. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. In Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science, 2008.\nK. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and independence. In Twenty-First Annual Conference on Learning Theory, pages 9\u201320, 2008.\nK. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correlation analysis. In ICML, 2009a.\nK. Chaudhuri, S. Dasgupta, and A. Vattani. Learning mixtures of gaussians using the k-means algorithm. CoRR, abs/0912.0086, 2009b.\nS. Chr\u00e9tien and A. O. Hero. On EM algorithms and their proximal generalizations. ESAIM: Probability and Statistics, 12:308\u2013326, May 2008.\nD. Conniffe. Expected maximum log likelihood estimation. Journal of the Royal Statistical Society. Series D, 36(4):317\u2013329, 1987.\nS. Dasgupta. Learning mixutres of Gaussians. In Fortieth Annual IEEE Symposium on Foundations of Computer Science, pages 634\u2013644, 1999.\nS. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. Journal of Machine Learning Research, 8(Feb):203\u2013226, 2007.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1\u201338, 1977.\nR. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society, London, A., 222:309\u2013368, 1922.\nM. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 753\u2013760, 2015.\nD. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. In Fourth Innovations in Theoretical Computer Science, 2013.\nA. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In Forty-second ACM Symposium on Theory of Computing, pages 553\u2013562, 2010.\nR. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models. SIAM Journal on Computing, 38(3):1141\u20131156, 2008.\nV. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems. In \u00c9cole d\u2032\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour XXXVIII, 2011.\nS. P. Lloyd. Least squares quantization in PCM. IEEE Trans. Information Theory, 28(2):129\u2013137, 1982.\nJ. B. MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281\u2013297. University of California Press, 1967.\nA. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In Fifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 93\u2013102, 2010.\nK. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society, London, A., 185:71\u2013110, 1894.\nR. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM algorithm. SIAM Review, 26(2):195\u2013239, 1984.\nP. Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations Research, 29(1):27\u201344, Feb 2004.\nS. Vempala and G. Wang. A spectral algorithm for learning mixtures models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004.\nC. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of Statistics, 11(1): 95\u2013103, Mar 1983.\nL. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural Computation, 8:129\u2013151, 1996.\nA Proofs of the Main Results\nA.1 Organization\nThis appendix is devoted to the proofs of our main results, and is organized as follows.\n\u2022 Section A.2 establishes a connection between Model 1 and Model 2 for Population EM. This connection enables us to use the analysis of Population EM for Model 2 for the analysis of Population EM for Model 1.\n\u2022 Section A.3 presents several structural properties of Population EM. These properties will be used in the proofs of our main results.\n\u2022 Section A.4 introduces several notations that will be used in the proofs of our main results.\n\u2022 Section A.5 presents the proof of Theorem 3.\n\u2022 Section A.6 presents the proof of Theorem 4.\n\u2022 Section A.7 presents the proof of Theorem 1.\n\u2022 Section A.8 presents the proof of Theorem 5, which also implies Theorem 2.\n\u2022 Section A.9 presents the proof of Theorem 6.\n\u2022 Section A.10 presents the proof of Theorem 7.\n\u2022 Appendix B includes a few auxiliary results that are used in the proofs of our main results.\nA.2 Connection Between Models 1 and 2\nIn this section, we draw a connection between Model 1 and Model 2. This will enable us to conclude most of the results for Model 1 from the results we prove for Model 2. First, consider the re-parametrization introduced in (11). The iterations of Population EM can be written in terms of these new parameters a\u3008t\u3009, b\u3008t\u3009 as\na\u3008t+1\u3009 = \u03b3\u3008t+1\u3009(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) , (12) b\u3008t+1\u3009 = \u03b3\u3008t+1\u3009\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) , (13)\nwhere\n\u03b3\u3008t+1\u3009 = Ewd(Y \u2212 a\u3008t\u3009, b\u3008t\u3009)Y\n= \u222b wd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)y\u03c6+d (y,\u03b8 ?) dy , (14)\np\u3008t+1\u3009 = Ewd(Y \u2212 a\u3008t\u3009, b\u3008t\u3009)\n= \u222b wd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u03c6+d (y,\u03b8 ?) dy . (15)\nAbove, we use\n\u03c6+d (y,\u03b8 ?) ,\n1 2 (\u03c6d(y \u2212 \u03b8?) + \u03c6d(y + \u03b8?))\nas shorthand for the Gaussian mixture density 0.5N(\u2212\u03b8?, Id) + 0.5N(\u03b8?, Id). The following lemma establishes a connection between the iterations of Population EM for Model 1 and for Model 2.\nLemma 2. If a\u30080\u3009 = 0, then a\u3008t\u3009 = 0 for every t. Furthermore,\nb\u3008t+1\u3009 = 2Ewd(Y , b\u3008t\u3009)Y = E(2wd(Y , b\u3008t\u3009)\u2212 1)Y .\nObserve that the expression for b\u3008t+1\u3009 in Lemma 2 is the same as the Population EM update under Model 1, given in (6).\nLemma 2 tells us that Model 1 is a special case of Model 2 if we know the mean (\u00b5?1 + \u00b5?2)/2 is known. In this case, b\u3008t\u3009 is regarded as an estimate of (\u00b5?2 \u2212 \u00b5?1)/2, in the same way that \u03b8\u3008t\u3009 is an estimate of \u03b8? in Model 1. (This explains our choice of the notation \u03b8? , (\u00b5?2 \u2212 \u00b5?1)/2 in (11).)\nThe proof of Lemma 2 is a simple induction that exploits the fact that wd(Y , b\u3008t\u3009)+wd(\u2212Y , b\u3008t\u3009) = 1: if a\u3008t\u3009 = 0, then\np\u3008t+1\u3009 = Ewd(Y , b\u3008t\u3009) = 1 2 (Ewd(Y , b\u3008t\u3009) + Ewd(\u2212Y , b\u3008t\u3009)) = 1 2 .\nA.3 Some Structural Properties of Population EM\nAn important structural property of Population EM is that the updates are orthogonally invariant. This means that our analysis of Population EM can make use of any orthogonal basis as the coordinate system without affecting the conclusions. This is spelled out in the following lemma.\nLemma 3. Let A \u2208 Rd\u00d7d denote an orthogonal matrix. Define, a\u0303\u3008t\u3009 , Aa\u3008t\u3009, b\u0303\u3008t\u3009 , Ab\u3008t\u3009, \u03b8\u0303? , A\u03b8?, and \u03b3\u0303\u3008t\u3009 = A\u03b3\u3008t\u3009. Then\n\u03b3\u0303\u3008t+1\u3009 = \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y\u03c6+d (y, \u03b8\u0303 ?) dy,\np\u3008t+1\u3009 = \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )\u03c6+d (y, \u03b8\u0303 ?) dy.\nand\na\u0303\u3008t+1\u3009 = \u03b3\u0303\u3008t+1\u3009(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) ,\nb\u0303 \u3008t+1\u3009 = \u03b3\u0303\u3008t+1\u3009\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) . (16)\nProof. The proof is a simple change of integration variables from y to y\u0303 = A\u22121/2y.\nUsing the Lemma 3, we can establish some simple invariances about Population EM, which we state in the following lemmas.\nLemma 4. For all t \u2265 0,\nsgn ( \u3008a\u3008t\u3009, b\u3008t\u3009\u3009 ) = sgn ( \u3008a\u3008t+1\u3009, b\u3008t+1\u3009\u3009 ) = sgn ( 1\n2 \u2212 p\u3008t+1\u3009\n) ,\nsgn ( \u3008b\u3008t\u3009,\u03b8?\u3009 ) = sgn ( \u3008b\u3008t+1\u3009,\u03b8?\u3009 ) .\nLemma 5. The following holds for any two settings of (a\u30080\u3009(1), b \u30080\u3009 (1)) and (a \u30080\u3009 (2), b \u30080\u3009 (2)).\n1. If a\u30080\u3009(1) = \u2212a \u30080\u3009 (2) and b \u30080\u3009 (1) = b \u30080\u3009 (2), then\na \u3008t\u3009 (1) = \u2212a \u3008t\u3009 (2) and b \u3008t\u3009 (1) = b \u3008t\u3009 (2) , \u2200t \u2265 0 .\n2. If b\u30080\u3009(1) = \u2212b \u30080\u3009 (2) and a \u30080\u3009 (1) = a \u30080\u3009 (2), then\na \u3008t\u3009 (1) = a \u3008t\u3009 (2) and b \u3008t\u3009 (1) = \u2212b \u3008t\u3009 (2) , \u2200t \u2265 0 .\nThe proof of Lemma 4 is in Appendix B.1, and the proof of Lemma 5 is in Appendix B.2. These two lemmas imply that in our analysis of Population EM, we may assume without loss of generality that\n\u3008a\u30080\u3009, b\u30080\u3009\u3009 \u2265 0 and \u3008b\u30080\u3009,\u03b8?\u3009 \u2265 0 .\nNext, we show that effectively all of the action of Population EM takes place in a two-dimensional subspace.\nLemma 6. Consider any matrix U \u2208 Rd\u00d7k such that U>U = Ik and \u03b8? and d are in the range of U . For any matrix V \u2208 Rd\u00d7l such that V >U = 0, and any vector c \u2208 Rd, we have\nV >E [ wd(Y \u2212 c,d)Y ] = 0\nwhere Y \u223c 0.5N(\u2212\u03b8?, Id) + 0.5N(\u03b8?, Id).\nProof. It is easy to see that because \u03b8? is in the range of U , we have that U>Y is independent of V >Y . Moreover, because d is in the range of U , we have UU>d = d. Thus, for any y \u2208 Rd,\nwd(y \u2212 c,d) = wk(U>(y \u2212 c),U>d) .\nThis implies\nV >Ewd(Y \u2212 c,d)Y = Ewd(U>(Y \u2212 c),U>d)V >Y = E [ wd(U >(Y \u2212 c),U>d) ] \u00b7 E [ V >Y ] = 0\nwhere the last equality follows because Y has mean zero.\nLemma 7. Let M0 denote the span of b\u30080\u3009 6= 0 and \u03b8? 6= 0 for the model Y \u223c 0.5N(\u2212\u03b8?, Id) + 0.5N(\u03b8?, Id). Then b\u3008t\u3009 \u2208M0 for all t \u2265 0.\nProof. This follows from Lemma 6 and induction, by letting the columns of U be an orthonormal basis for M0, and letting the columns of V to be a basis for the orthogonal complement of M0.\nRecall that in Section 2.1, we defined the angle between b\u3008t\u3009 and \u03b8? as \u03b2\u3008t\u3009. For our analysis, it will turn out to be useful to consistently refer to the cosine and sine of this angle, and hence we should regard the angle as possibly being any value between 0 and 2\u03c0. To do this, we fix an orthogonal basis {e\u3008t\u30091 , \u00b7 \u00b7 \u00b7 , e \u3008t\u3009 d } such that\n\u3008b\u3008t\u3009, e\u3008t\u30091 \u3009 = \u2016b \u3008t\u3009\u2016 and \u03b8? = \u3008\u03b8?, e\u3008t\u30091 \u3009e \u3008t\u3009 1 + \u3008\u03b8 ?, e \u3008t\u3009 2 \u3009e \u3008t\u3009 2 , (17)\nand define \u03b2\u3008t\u3009 \u2208 [0, 2\u03c0] be the angle such that\ncos\u03b2\u3008t\u3009 = \u3008b\u3008t\u3009,\u03b8?\u3009 \u2016b\u3008t\u3009\u20162\u2016\u03b8?\u20162 , (18) sin\u03b2\u3008t\u3009 = \u3008\u03b8?, e\u3008t\u30092 \u3009 \u2016\u03b8?\u2016 . (19)\nUsing this definition of the angle \u03b2\u3008t\u3009, we can establish the following monotonicity property.\nLemma 8. If 0 \u2264 \u03b2\u30080\u3009 < \u03c0/2, then \u03b2\u30080\u3009 \u2265 \u03b2\u30081\u3009 \u2265 . . . \u03b2\u3008t\u3009 \u2265 . . . \u2265 0.\nProof. We assume \u03b2\u30080\u3009 > 0, the extension to \u03b2\u30080\u3009 = 0 is straightforward. Define \u03b1\u3008t\u3009 as the angle between b\u3008t\u3009 and b\u3008t+1\u3009 such that\ncos\u03b1\u3008t\u3009 = \u3008b\u3008t\u3009, b\u3008t+1\u3009\u3009 \u2016b\u3008t\u3009\u20162\u2016b\u3008t+1\u3009\u20162 , (20) sin\u03b1\u3008t\u3009 = \u3008b\u3008t+1\u3009, e\u3008t\u30092 \u3009 \u2016b\u3008t+1\u3009\u2016 . (21)\nThe strategy of the proof is to use induction to prove that the following three statements hold for \u2200t \u2265 0:\n(i) \u03b2\u3008t\u3009 \u2208 (0, \u03c02 ).\n(ii) \u03b1\u3008t\u3009 \u2208 (0, \u03b2\u3008t\u3009).\n(iii) \u03b2\u3008t+1\u3009 = \u03b2\u3008t\u3009 \u2212 \u03b1\u3008t\u3009 \u2208 (0, \u03b2\u3008t\u3009).\nIt is clear that the claim of the lemma holds if (iii) holds for all t \u2265 0. The inductive argument uses the following chain of arguments for step t:\nClaim 1 If (i) holds for t, then (ii) holds for t.\nClaim 2 If (i) and (ii) hold for t, then (iii) holds for t.\nClaim 3 If (i), (ii), and (iii) hold for t, then (i) holds for t+ 1.\nSince (i) holds for t = 0 by assumption, it suffices to prove Claims 1\u20133. Claim 3 is trivially true, and Claim 2 follows from the fact that \u03b8? and all b\u3008t\u3009 lie in a the same two-dimensional subspace. So we just have to prove Claim 1. For sake of clarity, we choose the orthogonal basis e\u3008t\u30091 , e \u3008t\u3009 2 , . . . , e \u3008t\u3009 d satisfying (17) to simplify the calculation. Let U t be the orthogonal matrix whose rows are e\u3008t\u30091 , e \u3008t\u3009 2 , . . . , e \u3008t\u3009 d , so\nU tb \u3008t\u3009 = (\u2016b\u3008t\u3009\u2016, 0, 0, . . . , 0)> , U t\u03b8? = (\u03b8?\u3008t\u3009,1, \u03b8 ? \u3008t\u3009,2, 0, . . . , 0) > .\nDefine\nb\u0303 \u3008t+1\u3009 \u3008t\u3009 , U tb \u3008t+1\u3009 , b\u0303 \u3008t\u3009 , U tb \u3008t\u3009 , \u03b8\u0303t , U t\u03b8 ? .\nAlso, let b\u0303\u3008t+1\u3009\u3008t\u3009,i denote the i th element of b\u0303 \u3008t+1\u3009 \u3008t\u3009 . We also define the same notations of \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009 , \u03b3\u0303 \u3008t\u3009, \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,i for \u03b3\u3008t\u3009 and a\u0303\u3008t+1\u3009\u3008t\u3009 , a\u0303 \u3008t\u3009, a\u0303 \u3008t+1\u3009 \u3008t\u3009,i for a\n\u3008t\u3009. Using this coordinate system and Lemma 7, Claim 1 is equivalent to proving that if \u03b8?\u3008t\u3009,2 > 0, then \u03b1\n\u3008t\u3009 > 0 and \u03b1\u3008t\u3009 < \u03b2\u3008t\u3009. Therefore, in the rest of the proof, we essentially do these two steps.\n1. \u03b1\u3008t\u3009 > 0: First note that b\u0303 \u3008t+1\u3009\nand \u03b3\u0303\u3008t+1\u3009 are in the same direction. Hence, to prove that \u03b1\u3008t\u3009 > 0 we should show that \u03b3\u0303\u3008t+1\u3009\u3008t\u3009,2 > 0. We have\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 = \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y2\u03c6 + d (y, \u03b8\u0303t) dy\n= \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)y2 1\n2 (\u03c6d(y \u2212 \u03b8\u0303t) + \u03c6d(y + \u03b8\u0303t)) dy\n= 1\n2\n\u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)\u03c6(y1 \u2212 \u03b8?\u3008t\u3009,1) dy1 \u222b y2\u03c6(y2 \u2212 \u03b8?\u3008t\u3009,2) dy2\n+ 1\n2\n\u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)\u03c6(y1 + \u03b8?\u3008t\u3009,1) dy1 \u222b y2\u03c6(y2 + \u03b8 ? \u3008t\u3009,2) dy2 (22)\n= \u03b8?\u3008t\u3009,2 \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b\n\u3008t\u3009\u2016)1 2 (\u03c6(y1 \u2212 \u03b8?\u3008t\u3009,1)\u2212 \u03c6(y1 + \u03b8 ? \u3008t\u3009,1)) dy1 (23)\n= \u03b8?\u3008t\u3009,2 \u222b \u221e 0\ne2y1\u2016b\u0303 \u3008t\u3009\u2016 \u2212 e\u22122y1\u2016b\u0303 \u3008t\u3009\u2016\ne2y1\u2016b\u0303 \u3008t\u3009\u2016 + e2a\u0303 \u3008t\u3009 1 \u2016b\u0303 \u3008t\u3009\u2016 + e\u22122y1\u2016b\u0303 \u3008t\u3009\u2016 + e\u22122a\u0303 \u3008t\u3009 1 \u2016b\u0303\n\u3008t\u3009\u2016 \u03c6\u2212(y1, \u03b8 ? \u3008t\u3009,1) dy1\n= \u03b8?\u3008t\u3009,2S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1), (24)\nwhere \u03c6\u2212(y, \u03b8) , 12(\u03c6(y\u2212\u03b8)\u2212\u03c6(y+\u03b8)) is shorthand for the difference of two Gaussian densities, and S : R3 \u2192 R is defined by\nS(xa, xb, x\u03b8) , \u222b \u221e\n0\ne2yxb \u2212 e\u22122yxb e2yxb + e\u22122xaxb + e\u22122yxb + e2xaxb \u00b7 1 2 \u221a 2\u03c0 (e\u2212(y\u2212x\u03b8) 2/2 \u2212 e\u2212(y+x\u03b8)2/2) dy\n= \u222b \u221e 0 w(y \u2212 xa, xb) 1 2 (\u03c6(y \u2212 x\u03b8)\u2212 \u03c6(y + x\u03b8)) dy .\nHence it is clear that \u03b8?\u3008t\u3009,2 > 0 implies \u03b1 \u3008t\u3009 > 0 since S(xa, xb, x\u03b8) > 0 for all xb > 0 and x\u03b8 > 0.\n2. \u03b1\u3008t\u3009 < \u03b2\u3008t\u3009: We just need to show \u03b1\u3008t\u3009 < \u03c0/2 and compare the co-tangent of \u03b1\u3008t\u3009 and \u03b2\u3008t\u3009.\nThis means that we have to show \u03b3\u0303\u3008t+1\u3009\u3008t\u3009,1 > 0 and compare \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2\nwith \u03b8?\u3008t\u3009,1 \u03b8?\u3008t\u3009,2 . We first calculate\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 .\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 = \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y1\u03c6 + d (y, \u03b8\u0303t) dy\n= \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)y1\u03c6+d (y, \u03b8\u0303t) dy (25)\n= \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)y1\u03c6+(y1, \u03b8?\u3008t\u3009,1) dy1 (26)\n= \u0393(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n= \u222b \u221e 0\ne2y1\u2016b\u0303 \u3008t\u3009\u2016 \u2212 e\u22122y1\u2016b\u0303 \u3008t\u3009\u2016\ne2y1\u2016b\u0303 \u3008t\u3009\u2016 + e2a\u0303 \u3008t\u3009 1 \u2016b\u0303 \u3008t\u3009\u2016 + e\u22122y1\u2016b\u0303 \u3008t\u3009\u2016 + e\u22122a\u0303 \u3008t\u3009 1 \u2016b\u0303\n\u3008t\u3009\u2016 y1\u03c6\n+(y1, \u03b8 ? \u3008t\u3009,1) dy1,\n(27)\nwhere \u0393 : R3 \u2192 R is defined as\n\u0393(xa, xb, x\u03b8) = \u222b w(y \u2212 xa, xb)y 1\n2 (\u03c6(y \u2212 x\u03b8) + \u03c6(y + x\u03b8)) dy .\nIt is clear that \u03b3\u0303\u3008t+1\u3009\u3008t\u3009,1 > 0. For comparing the co-tangent of two angles, we need to further simplify \u03b3\u0303\u3008t+1\u3009\u3008t\u3009,1 . We have,\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 = \u0393(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n= \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)y1 1\n2 (\u03c6(y1 \u2212 \u03b8?\u3008t\u3009,1) + \u03c6(y1 + \u03b8 ? \u3008t\u3009,1)) dy1 (28)\n= \u03b8?\u3008t\u3009,1 \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b\n\u3008t\u3009\u2016)1 2 (\u03c6(y1 \u2212 \u03b8?\u3008t\u3009,1)\u2212 \u03c6(y1 + \u03b8 ? \u3008t\u3009,1)) dy1\n+ \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , \u2016b\n\u3008t\u3009\u2016)1 2 {(y1 \u2212 \u03b8?\u3008t\u3009,1)\u03c6(y1 \u2212 \u03b8 ? \u3008t\u3009,1) + (y1 + \u03b8 ? \u3008t\u3009,1)\u03c6(y1 + \u03b8 ? \u3008t\u3009,1)} dy1\n= \u03b8?\u3008t\u3009,1S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) + \u222b \u221e \u2212\u221e w(y1 + \u03b8 ? \u3008t\u3009,1 \u2212 a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016)y1 1 2 \u03c6(y1) dy1\n+ \u222b \u221e \u2212\u221e w(y1 \u2212 \u03b8?\u3008t\u3009,1 \u2212 a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016)y1 1 2 \u03c6(y1) dy1 .\n= \u03b8?\u3008t\u3009,1S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) +R(\u2016b\u0303 \u3008t\u3009\u2016, a\u0303\u3008t\u30091 \u2212 \u03b8 ? \u3008t\u3009,1) +R(\u2016b \u3008t\u3009\u2016, a\u0303\u3008t\u30091 + \u03b8 ? \u3008t\u3009,1), (29)\nwhere R : R2 \u2192 R is defined as R(xb, x) , \u222b +\u221e\n0\ne2yxb \u2212 e\u22122yxb e2yxb + e2xxb + e\u22122yxb + e\u22122xxb 1 2 \u221a 2\u03c0 ye\u2212y 2/2 dy ,\n= \u222b \u221e \u2212\u221e w(y \u2212 x, xb)y 1 2 \u03c6(y) dy .\nEmploying (29) and (24) we have\ncot\u03b1\u3008t\u3009 = b\u0303 \u3008t+1\u3009 \u3008t\u3009,1\nb\u0303 \u3008t+1\u3009 \u3008t\u3009,2\n= \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2\n= \u03b8?\u3008t\u3009,1 \u03b8?\u3008t\u3009,2 + R(\u2016b\u0303\u3008t\u3009\u2016, a\u0303\u3008t\u30091 \u2212 \u03b8?\u3008t\u3009,1) +R(\u2016b \u3008t\u3009\u2016, a\u0303\u3008t\u30091 + \u03b8?\u3008t\u3009,1)\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2\n> \u03b8?\u3008t\u3009,1\n\u03b8?\u3008t\u3009,2 = cot\u03b2\u3008t\u3009 ,\nwhere the last inequality is due to the fact that R(xb, x) > 0 for all xb > 0.\nA.4 Notations for the Remaining Proofs\nIn this section we collect the main notations that will be used in the proofs of our results. For the basic notation, we let \u03c6d(x) and \u03a6d(x) denote the pdf and CDF for d-dimension standard Gaussian\ndistribution respectively. We use \u03c6(x) and \u03a6(x) as shorthand for one-dimension case. Let \u03c6+d (x, x\u03b8) denote the pdf for X \u223c 0.5N(\u2212x\u03b8, I) + 0.5N(x\u03b8, I), i.e.,\n\u03c6+d (x,x\u03b8) = 1\n2 (\u03c6d(x\u2212 x\u03b8) + \u03c6d(x+ x\u03b8)).\nWe shorthand \u03c6+1 (x, x\u03b8) as \u03c6 +(x, x\u03b8) if x \u2208 R. In addition, we let \u03c6\u2212(x, x\u03b8) denote the difference between these two pdf, i.e.,\n\u03c6\u2212(x,x\u03b8) = 1\n2 (\u03c6(x\u2212 x\u03b8)\u2212 \u03c6d(x+ x\u03b8)).\nNext, we introduce the notations for important functions. Similar to the proof of Lemma 8, in many other proofs we will use the rotation matrix U t that satisfies U tb\u3008t\u3009 = (\u2016b\u3008t\u3009\u20162, 0, 0, . . . , 0)>, and U t\u03b8? = (\u03b8?\u3008t\u3009,1, \u03b8 ? \u3008t\u3009,2, 0, . . . , 0) >. We also use the notations b\u0303 \u3008t+1\u3009 \u3008t\u3009 , U tb \u3008t+1\u3009, b\u0303 \u3008t\u3009 , U tb \u3008t\u3009 and \u03b8\u0303? , U t\u03b8 ?. Also, let b\u0303\u3008t+1\u3009\u3008t\u3009,i denote the i th element of b\u0303 \u3008t+1\u3009 \u3008t\u3009 . We also define the same notations of \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009 , \u03b3\u0303 \u3008t\u3009, \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,i for \u03b3 \u3008t\u3009 and a\u0303\u3008t+1\u3009\u3008t\u3009 , a\u0303 \u3008t\u3009, a\u0303 \u3008t+1\u3009 \u3008t\u3009,i for a \u3008t\u3009. By Lemma 4 and 5, we can assume that \u3008b\u3008t\u3009,\u03b8?\u3009 \u2265 0 and \u3008b\u3008t\u3009,a\u3008t\u3009\u3009 \u2265 0 for all t \u2265 0 without loss of generality. Hence we have\n\u03b8?\u3008t\u3009,1 \u2265 0, and a\u0303 \u3008t\u3009 1 \u2265 0.\nSince for any i \u2265 3, the ith coordinate is zero in the span of b\u0303\u3008t\u3009 and \u03b8\u0303?, we prove that b\u0303\u3008t+1\u3009\u3008t\u3009,i = 0 for i \u2265 3. Then according to (16) we have\na\u0303\u3008t+1\u3009 = \u03b3\u0303\u3008t+1\u3009(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) ,\nb\u0303 \u3008t+1\u3009 = \u03b3\u0303\u3008t+1\u3009\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) . (30)\nThe first notation we would like to introduce P : R3 \u2192 R P (xa, xb, x\u03b8) , \u222b eyxb\u2212xaxb\neyxb\u2212xaxb + e\u2212yxb+xaxb 1 2 \u221a 2\u03c0 (e\u2212(y\u2212x\u03b8) 2/2 + e\u2212(y+x\u03b8) 2/2) dy\n= \u222b w(y \u2212 xa, xb)\u03c6+(y, x\u03b8) dy . (31)\nThe importance of this function is clarified in the following calculations:\np\u3008t+1\u3009 = Ew(Y \u2212 a\u0303\u3008t\u3009, b\u0303\u3008t\u3009)\n= \u222b w(y1 \u2212 a\u0303\u3008t\u30091 , b\u0303 \u3008t\u3009 1 )\u03c6 +(y, \u03b8?\u3008t\u3009,1)\n=\n\u222b ey1\u2016b\u0303 \u3008t\u3009\u2016\u2212a\u0303\u3008t\u30091 \u2016b\u0303 \u3008t\u3009\u2016\ney1\u2016b\u0303 \u3008t\u3009\u2016\u2212a\u0303\u3008t\u30091 \u2016b\u0303 \u3008t\u3009\u2016 + e\u2212y1\u2016b\u0303 \u3008t\u3009\u2016+a\u0303\u3008t\u30091 \u2016b\u0303 \u3008t\u3009\u2016\n1 2 \u221a 2\u03c0 (e\u2212\n(y1\u2212\u03b8 ? \u3008t\u3009,1) 2\n2 + e\u2212 (y1+\u03b8\n? \u3008t\u3009,1) 2 2 ) dy1\n= P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1), (32)\nOur second notation is the \u0393 : R3 \u2192 R, where \u0393(xa, xb, x\u03b8) , \u222b eyxb\u2212xaxb\neyxb\u2212xaxb + e\u2212yxb+xaxb y\n1 2 \u221a 2\u03c0 (e\u2212(y\u2212x\u03b8) 2/2 + e\u2212(y+x\u03b8) 2/2) dy.\n= \u222b w(y \u2212 xa, xb)y\u03c6+(y, x\u03b8) dy. (33)\nNote that according to (27), we have\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 = \u0393(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1).\nThe next function we introduce here is S : R3 \u2192 R S(xa, xb, x\u03b8) , \u222b \u221e\n0\ne2yxb \u2212 e\u22122yxb e2yxb + e\u22122xaxb + e\u22122yxb + e2xaxb 1 2 \u221a 2\u03c0 (e\u2212(y\u2212x\u03b8) 2/2 \u2212 e\u2212(y+x\u03b8)2/2) dy\n= \u222b w(y \u2212 xa, xb)\u03c6\u2212(y, x\u03b8) dy.\nNote that according to (24),\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 = \u03b8 ? \u3008t\u3009,2S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1). (34)\nAnother notation we introduce here is R : R2 \u2192 R defined as R(xb, x) , \u222b +\u221e\n0\ne2yxb \u2212 e\u22122yxb e2yxb + e2xxb + e\u22122yxb + e\u22122xxb 1 2 \u221a 2\u03c0 ye\u2212y 2/2 dy\n= 1\n2\n\u222b w(y \u2212 x, xb)y\u03c6(y) dy.\nAccording to (29),\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 = \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n= \u03b8?\u3008t\u3009,1S(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) +R(\u2016b\u0303 \u3008t\u3009\u2016, a\u0303\u3008t\u30091 \u2212 \u03b8 ? \u3008t\u3009,1) +R(\u2016b\u0303 \u3008t\u3009\u2016, a\u0303\u3008t\u30091 + \u03b8 ? \u3008t\u3009,1). (35)\nThe final notation that we will be using in this paper is the function F : R2 \u2192 R:\nF (xb, x\u03b8) = \u222b e(y+x\u03b8)xb \u2212 e\u2212(y+x\u03b8)xb e(y+x\u03b8)xb + e\u2212(y+x\u03b8)xb (y + x\u03b8) 1\u221a 2\u03c0 e\u2212y 2/2 dy\n= \u222b (2w(y + x\u03b8, xb)\u2212 1)(y + x\u03b8)\u03c6(y) dy. (36)\nTo understand where this function may appear, note that\n\u0393(0, xb, x\u03b8) = \u222b w(y, xb)y 1\n2 \u03c6+(y, x\u03b8) dy\n= \u222b w(y, xb)y 1\n2 \u03c6(y \u2212 x\u03b8) dy +\n\u222b w(y, xb)y 1\n2 \u03c6(y + x\u03b8) dy\n= \u222b w(y + x\u03b8, xb)(y + x\u03b8) 1\n2 \u03c6(y) dy +\n\u222b w(y \u2212 x\u03b8, xb)(y \u2212 x\u03b8) 1\n2 \u03c6(y) dy\n= \u222b w(y + x\u03b8, xb)(y + x\u03b8) 1\n2 \u03c6(y) dy \u2212\n\u222b w(\u2212y \u2212 x\u03b8, xb)(y + x\u03b8) 1\n2 \u03c6(y) dy\n= 1\n2 \u222b e(y+x\u03b8)xb \u2212 e\u2212(y+x\u03b8)xb e(y+x\u03b8)xb + e\u2212(y+x\u03b8)xb (y + x\u03b8) 1\u221a 2\u03c0 e\u2212y 2/2 dy = 1 2 F (xb, x\u03b8). (37)\nA.5 Proof of Theorem 3\nWithout loss of generality, we assume that \u3008b\u30080\u3009,\u03b8?\u3009 > 0 and \u3008a\u30080\u3009, b\u30080\u3009\u3009 \u2265 0. We employ the notations and equations reviewed in Appendix A.4. For notational simplicity we also define Rs , R(\u2016b\u0303 \u3008t\u3009\u2016, a\u0303\u3008t\u30091 \u2212 \u03b8?\u3008t\u3009,1) + R(\u2016b\u0303 \u3008t\u3009\u2016, a\u0303\u3008t\u30091 + \u03b8?\u3008t\u3009,1) and Ss , S(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1). Since b \u3008t+1\u3009 and \u03b3\u3008t+1\u3009 are in the same direction, we have\ncos\u03b2\u3008t+1\u3009 = \u3008b\u0303\u3008t+1\u3009, \u03b8\u0303?\u3009 \u2016\u03b8\u0303?\u2016\u2016b\u0303\u3008t+1\u3009\u2016 = \u3008\u03b3\u0303\u3008t+1\u3009, \u03b8\u0303?\u3009 \u2016\u03b8\u0303?\u2016\u2016\u03b3\u0303\u3008t+1\u3009\u2016\n= \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 \u03b8 ? \u3008t\u3009,1 + \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 \u03b8 ? \u3008t\u3009,2 \u2016\u03b8\u0303?\u2016 \u221a\n(\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 ) 2 + (\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 ) 2\n(a) =\n(\u03b8?\u3008t\u3009,1) 2Ss + \u03b8 ? \u3008t\u3009,1Rs + (\u03b8 ? \u3008t\u3009,2) 2Ss \u2016\u03b8\u0303?\u2016 \u221a\n(\u03b8?\u3008t\u3009,1) 2S2s +R 2 s + 2Rs\u03b8 ? \u3008t\u3009,1Ss + (\u03b8 ? \u3008t\u3009,2) 2S2s\n= \u2016\u03b8\u0303?\u20162Ss + \u03b8?\u3008t\u3009,1Rs \u2016\u03b8\u0303?\u2016 \u221a \u2016\u03b8\u0303?\u20162S2s +R2s + 2Rs\u03b8?\u3008t\u3009,1Ss ,\nwhere Equality (a) is the result of (35) and (34). Hence it is straightforward to check that\nsin\u03b2t+1 = \u03b8?\u3008t\u3009,2Rs \u2016\u03b8\u0303?\u2016 \u221a (\u2016\u03b8\u0303?\u20162S2s +R2s + 2Rs\u03b8?\u3008t\u3009,1Ss) \u2264 \u03b8?\u3008t\u3009,2 \u2016\u03b8\u0303?\u2016 Rs Rs + \u03b8?\u3008t\u3009,1Ss .\nNote that since b\u0303\u3008t\u30092 = 0, we have \u03b8?\u3008t\u3009,2\n\u2016\u03b8\u0303?\u2016 = sin(\u03b2\u3008t\u3009).\nHence, we have\nsin\u03b2\u3008t+1\u3009 = Rs\nRs + \u03b8?\u3008t\u3009,1Ss sin\u03b2\u3008t\u3009. (38)\nOur goal is to prove that there exists 0 < \u03ba\u03b2 < 1, such that Rs/(Rs + \u03b8?\u3008t\u3009,1Ss) \u2264 \u03ba\u03b2 at every iteration. Toward this goal we will prove that \u03b8?\u3008t\u3009,1Ss > 0. First note that since according to Lemma 8 the angle \u03b2\u3008t\u3009 is decreasing \u03b8?\u3008t\u3009,1 is an increasing sequence. Hence, \u03b8 ? \u3008t\u3009,1Ss \u2265 \u03b8 ? \u30080\u3009,1Ss. Our goal is to show that Ss > 0. Note that Ss = S(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) is only zero if \u2016b \u3008t\u3009\u2016 = 0 and can only go to zero if a\u0303\u3008t\u30091 \u2192\u221e or \u2016b \u3008t\u3009\u2016 \u2192 \u221e. Hence, if we find a lower bound for inft \u2016b\u3008t\u3009\u2016 and prove that we have an upper bound for supt \u2016a\u3008t\u3009\u2016 and supt \u2016b\u3008t\u3009\u2016, then we obtain a non-zero lower bound for Ss. The following two lemmas prove our claims:\nLemma 9. For any initialization a\u30080\u3009, b\u30080\u3009 \u2208 Rd, we have\n\u2016a\u3008t\u3009\u20162 \u2264 max{\u2016a\u30080\u3009\u20162, 2 \u03c0 + \u2016\u03b8?\u20162 2 , 16 9 + 73 36 \u2016\u03b8?\u20162} , c2U,1,\u2200t \u2265 0, \u2016b\u3008t\u3009\u20162 \u2264 max{\u2016b\u30080\u3009\u20162, \u2016\u03b8?\u20162 + 1 4c2U,2(1\u2212 cU,2)2 (1 + \u2016\u03b8?\u20162)} , c2U,3,\u2200t \u2265 0,\nwhere cU,2 = 14(1\u2212 \u03a6(cU,1 + \u2016\u03b8 ?\u2016)). Hence, {\u2016a\u3008t\u3009\u2016, \u2016b\u3008t\u3009\u2016}t belong to a compact set.\nWe postpone the proof of this lemma until Appendix A.5.1, but the fact that the estimates remain bounded should not be surprising for the reader. Lemma 10. Let b\u3008t\u3009 and a\u3008t\u3009 denote the estimates of Population EM. There exists a value cl > 0 depending on \u2016\u03b8?\u2016, \u3008b\u30080\u3009,\u03b8?\u3009, \u2016a\u30080\u3009\u2016, and \u2016b\u30080\u3009\u2016 such that\n\u2016b\u3008t\u3009\u2016 \u2265 min(\u2016b\u30080\u3009\u2016, cl) , cL,1.\nWe postpone the proof of this claim to Appendix A.5.2. Note that according to Lemma 9 we know that supt \u2016a\u3008t\u3009\u2016 \u2264 cU,1 and supt \u2016b\u3008t\u3009\u2016 \u2264 cU,3. Hence, we define\n\u03ba\u03b2 = max cL,1\u2264\u2016b\u0303 \u3008t\u3009\u2016\u2264cU,3,\u2016a\u0303\u3008t\u3009\u2016\u2264cU,1\nRs\n\u03b8?\u30080\u3009,1S(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) +Rs) \u2208 (0, 1),\nthen (38) implies sin\u03b2\u3008t+1\u3009 \u2264 \u03ba\u03b2 sin\u03b2\u3008t\u3009. This proves the first claim in Theorem 3. Our next goal is to prove the second claim, i.e.,\n\u2016a\u3008t+1\u3009\u20162 \u2264 \u03ba2a\u2016a\u3008t\u3009\u20162 + \u2016\u03b8?\u20162 sin\u03b2\u3008t\u3009\n4 .\nAs before we write \u2016a\u3008t+1\u3009\u20162 = (a\u0303\u3008t+1\u3009\u3008t\u3009,1 ) 2 + (a\u0303 \u3008t+1\u3009 \u3008t\u3009,2 )\n2 and then bound each term separately. According to (30), we have\na\u0303 \u3008t+1\u3009 \u3008t\u3009,1 =\n\u0393(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 2P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) (?) \u2264 \u03baaa\u0303\u3008t\u30091 \u2264 \u03baa\u2016a\u0303 \u3008t\u3009\u2016. (39)\nwhere Inequality (?) is due to the following lemma:\nLemma 11. For any \u03b8 \u2265 0, there exists a constant \u03baa \u2208 (0, 1) only depending on \u03b8 and continuous for \u03b8 > 0 such that\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) \u2264 \u03baaxa, \u2200xa \u2265 0, xb > 0.\nThe proof of this Lemma is presented in Appendix A.5.3. Our next step is to establish the convergence of a\u0303t+1t,2 . From (30) and (34) we have:\na\u0303 \u3008t+1\u3009 \u3008t\u3009,2 =\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 (1\u2212 2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n2P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n= \u03b8?\u3008t\u3009,2 S(a\u0303\n\u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n2P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) .\nAnd according to (32), we have\nP (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) = \u222b w(y \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016)\u03c6+(y, \u03b8?\u3008t\u3009,1) dy\n= \u222b \u221e y=0 (w(y \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016) + w(\u2212y \u2212 a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016))\u03c6+(y, \u03b8?\u3008t\u3009,1) dy\n= \u222b \u221e y=0\ne2y\u2016b \u3008t\u3009\u2016 + 2e\u22122a\u0303 \u3008t\u3009 1 \u2016b \u3008t\u3009\u2016 + e\u22122y\u2016b \u3008t\u3009\u2016\ne2y\u2016b \u3008t\u3009\u2016 + e2a\u0303 \u3008t\u3009 1 \u2016b \u3008t\u3009\u2016 + e\u22122y\u2016b \u3008t\u3009\u2016 + e\u22122a\u0303 \u3008t\u3009 1 \u2016b\n\u3008t\u3009\u2016 \u03c6+(y, \u03b8?\u3008t\u3009,1) dy\n> S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) \u2265 0. (40)\nHence, we have\na\u0303 \u3008t+1\u3009 \u3008t\u3009,2 \u2264\n\u03b8?\u3008t\u3009,2 2 = \u2016\u03b8?\u2016 sin\u03b2\u3008t\u3009 2 . (41)\nCombining (39) and (41) establishes the second part of our main Theorem.\nA.5.1 Proof of Lemma 9\nIn this section we use the notations and equations that are summarize in Appendix A.4. Without loss of generality, we assume that \u3008b\u30080\u3009,\u03b8?\u3009 > 0 and \u3008a\u30080\u3009, b\u30080\u3009\u3009 \u2265 0 and it is straightforward to show that if \u2016b\u30080\u3009\u2016 = 0, then\na\u3008t\u3009 = b\u3008t\u3009 = 0, \u2200t \u2265 1.\nHence, we assume that \u2016b\u30080\u3009\u2016 > 0. We again rotate the coordinates with the U t matrix we introduced in Appendix A.4. Under this coordinate systems we know that a\u0303\u3008t+1\u3009\u3008t\u3009,i = b\u0303 \u3008t+1\u3009 \u3008t\u3009,i = 0 for every i \u2265 3. Hence, we will prove the boundedness of a\u0303\u3008t+1\u3009\u3008t\u3009,1 , a\u0303 \u3008t+1\u3009 \u3008t\u3009,2 , b\u0303 \u3008t+1\u3009 \u3008t\u3009,1 , and b\u0303 \u3008t+1\u3009 \u3008t\u3009,2 . We start with bounding a\u0303 \u3008t+1\u3009 \u3008t\u3009,2 and b\u0303 \u3008t+1\u3009 \u3008t\u3009,2 . According to (30) we have\na\u0303 \u3008t+1\u3009 \u3008t\u3009,2 =\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 (1\u2212 2p \u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) \u2264 \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2 2p\u3008t+1\u3009 (b) = \u03b8?\u3008t\u3009,2S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) 2p\u3008t+1\u3009 ,\nb\u0303 \u3008t+1\u3009 \u3008t\u3009,2 =\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) (c)\n\u2264 \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2\np\u3008t+1\u3009 (d) =\n\u03b8?\u3008t\u3009,2S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) p\u3008t+1\u3009 , (42)\nwhere Equalities (b) and (d) are due to (34). To obtain Inequality (c) we used the following chain of arguments: According to Lemma 4, p\u3008t\u3009 \u2264 0.5 for every t. Hence, 2(1\u2212 p\u3008t+1\u3009) \u2265 1.\nWith exactly same calculation showed in (40), we have\np\u3008t+1\u3009 = P (a\u0303 \u3008t\u3009 i , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) > S(a\u0303 \u3008t\u3009 i , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1).\nTogether with (42), we obtain\n|a\u0303\u3008t+1\u3009\u3008t\u3009,2 | \u2264 |\u03b8?\u3008t\u3009,2| 2 \u2264 \u2016\u03b8 ?\u2016 2 . |b\u0303\u3008t+1\u3009\u3008t\u3009,2 | \u2264 |\u03b8?\u3008t\u3009,2|\n2(1\u2212 p\u3008t+1\u3009) \u2264 \u2016\u03b8?\u2016. (43)\nHence the only remaining step is to bound a\u0303\u3008t+1\u3009\u3008t\u3009,1 and b\u0303 \u3008t+1\u3009 \u3008t\u3009,1 . To bound a\u0303 \u3008t+1\u3009 \u3008t\u3009,1 we consider two separate cases.\n1. a\u0303\u3008t\u30091 \u2265 \u03b8?\u3008t\u3009,1 \u2265 0, t \u2265 0: First note that according to (30), we have\n0 \u2264 a\u0303\u3008t+1\u3009\u3008t\u3009,1 = \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 (1\u2212 2p \u3008t+1\u3009)\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009)\n= \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n\u2264 \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) . (44)\nHence to bound a\u0303\u3008t+1\u3009\u3008t\u3009,1 we require a bound for\n\u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) .\nOur next lemma provides such a bound.\nLemma 12. If xa \u2265 x\u03b8 \u2265 0, xb \u2265 0, we have\n\u0393(xa, xb, x\u03b8)\n2P (xa, xb, x\u03b8) \u2264\nxa + \u221a 2 \u03c0\n2 .\nWe prove this lemma in the Appendix B.4. Combining (44) and Lemma 12 proves\n(a\u0303 \u3008t+1\u3009 \u3008t\u3009,1 )\n2 \u2264 ( a\u0303 \u3008t\u3009 1 +\n\u221a 2 \u03c0\n2 )2 \u2264 \u2016a\u3008t\u3009\u20162 + 2\u03c0 2 .\nTherefore, combined with (43), we have\n\u2016a\u3008t+1\u3009\u20162 = \u2016a\u0303\u3008t+1\u3009\u2016 = (a\u0303\u3008t+1\u3009\u3008t\u3009,1 ) 2 + (a\u0303 \u3008t+1\u3009 \u3008t\u3009,2 ) 2\n\u2264 \u2016a\u3008t\u3009\u20162 + 2\u03c0 2 + \u2016\u03b8?\u20162 4 \u2264 max{(\u2016a\u3008t\u3009\u2016)2, 2 \u03c0 + \u2016\u03b8?\u20162 2 }. (45)\n2. a\u0303\u3008t\u30091 < \u03b8 ? \u3008t\u3009,1: Again according to (30) we have\n0 \u2264 a\u0303\u3008t+1\u3009\u3008t\u3009,1 = \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 (1\u2212 2p \u3008t+1\u3009)\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009)\n= \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) .\nWe know from Lemma 4 that p\u3008t+1\u3009 \u2264 0.5. In the range 0 < p\u3008t+1\u3009 \u2264 0.5,\n(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009)\nis a positive decreasing function of p\u3008t+1\u3009. Hence, if we a lower bound for P may lead to an upper bound for a\u0303\u3008t+1\u3009\u3008t\u3009,1 . The following lemma provides such an upper bound.\nLemma 13. If xa \u2265 x\u03b8 \u2265 0, xb \u2265 0, we have\nP (xa, xb, x\u03b8) \u2265 1\n2 (1\u2212 \u03a6(xa \u2212 x\u03b8)) +\n1 2 (1\u2212 \u03a6(xa + x\u03b8)).\nIf 0 \u2264 xa < x\u03b8, xb \u2265 0, we have P (xa, xb, x\u03b8) \u2265 1\n4 .\nwhere \u03a6(x) is the CDF for a standard Gaussian distribution.\nThe proof of this lemma is presented in the Appendix B.3. By plugging P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) = 0.25 in (46), we have\n0 \u2264 a\u0303\u3008t+1\u3009\u3008t\u3009,1 = \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1 (1\u2212 2p \u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) \u2264 4 3 \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n\u2264 4 3\n\u222b |y|\u03c6+(y, \u03b8?\u3008t\u3009,1) dy \u2264 4\n3\n\u221a\u222b y2\u03c6+(y, \u03b8?\u3008t\u3009,1) dy\n= 4\n3\n\u221a 1 + (\u03b8?\u3008t\u3009,1) 2 \u2264 4 3 \u221a 1 + \u2016\u03b8?\u20162. (46)\nCombining this with (43), we obtain\n\u2016a\u3008t+1\u3009\u20162 \u2264 16 9 (1 + \u2016\u03b8?\u20162) + \u2016\u03b8 ?\u20162 4\n= 16\n9 +\n73 36 \u2016\u03b8?\u20162 (47)\nTherefore combining (45) and (47), we have\n\u2016a\u3008t\u3009\u20162 \u2264 max{\u2016a\u30080\u3009\u20162, 2 \u03c0 + \u2016\u03b8?\u20162 2 , 16 9 + 73 36 \u2016\u03b8?\u20162} = c2U,1 <\u221e, \u2200t \u2265 0\nSo far we have bounded {\u2016a\u3008t\u3009\u2016}t\u22650 by cU,1. Also, in (43) we obtained an upper bound for a\u0303\u3008t+1\u3009\u3008t\u3009,1 . Our next step is to obtain an upper bound for b\u0303\u3008t+1\u3009\u3008t\u3009,1 . First note that\nb\u0303 \u3008t+1\u3009 \u3008t\u3009,1 =\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) .\nHence, we have to find an upper bound for \u0393 and a lower bound for P . Note that\n\u2202P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n\u2202a\u0303 \u3008t\u3009 1\n= \u2212 \u222b\n2\u2016b\u3008t\u3009\u2016 (ey\u2016b \u3008t\u3009\u2016\u2212a\u0303\u3008t\u30091 \u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016+a\u0303\u3008t\u30091 \u2016b \u3008t\u3009\u2016)2 \u03c6+(y, \u03b8?\u3008t\u3009,1) dy \u2264 0.\nTherefore p\u3008t+1\u3009 = P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) is a decreasing function of a\u0303 \u3008t\u3009 1 . Since a\u0303 \u3008t\u3009 1 \u2264 \u2016a\u3008t\u3009\u2016 \u2264 cU,1, with Lemma 13, we have \u2200t \u2265 0\np\u3008t+1\u3009 = P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) \u2265 P (cU,1, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n\u2265 min{1 4 , 1 2 (1\u2212 \u03a6(cU,1 \u2212 \u03b8?\u3008t\u3009,1)) + 1 2 (1\u2212 \u03a6(cU,1 + \u03b8?\u3008t\u3009,1))} \u2265 1 4 (1\u2212 \u03a6(cU,1 + \u2016\u03b8?\u2016)) , cU,2 > 0.\nNote that in (46), we derived an upper bound for \u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1). Therefore, we have\n0 \u2264 b\u0303\u3008t+1\u3009\u3008t\u3009,1 = \u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,1\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009)\n\u2264 1 2cU,2(1\u2212 cU,2) \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) \u2264 1 2cU,2(1\u2212 cU,2) \u221a 1 + \u2016\u03b8?\u20162\nThus with (43), we have\n\u2016b\u3008t\u3009\u20162 \u2264 max{\u2016b\u30080\u3009\u20162, \u2016\u03b8?\u20162 + 1 4c2U,2(1\u2212 cU,2)2 (1 + \u2016\u03b8?\u20162)} = c2U,3 < \u221e,\u2200t \u2265 0.\nThis completes the proof of Lemma 9.\nA.5.2 Proof of Lemma 10\nWithout loss of generality we only consider the case \u3008b\u3008t\u3009,\u03b8?\u3009 > 0 and \u3008a\u3008t\u3009, b\u3008t\u3009\u3009 \u2265 0. Before we start the proof we remind the reader a couple of facts that we have proved in Lemma 8 and Lemma 9.\n1. supt \u2016a\u3008t\u3009\u2016 \u2264 cU,1 and supt \u2016b\u3008t\u3009\u2016 \u2264 cU,3.\n2. b\u30081\u3009, b\u30082\u3009, . . . , b\u3008t\u3009, . . . and \u03b8? are all on the same two-dimensional plane:\n3. The angle \u03b2\u3008t\u3009 , arccos ( \u3008b\u3008t\u3009,\u03b8?\u3009 \u2016b\u3008t\u3009\u2016\u2016\u03b8?\u2016 ) is non-increasing in terms of t.\n4. a\u3008t\u3009 is in the same direction as b\u3008t\u3009 for all t \u2265 1.\nWe use the notations we summarized in Appendix A.4. Similar to that section we consider the U t transformed vectors a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 . Note that we have\n\u2016b\u3008t+1\u3009\u2016 = \u2016b\u0303\u3008t+1\u3009\u2016 \u2265 b\u0303\u3008t+1\u3009\u3008t\u3009,1\n= \u0393(a\u0303 \u3008t\u3009 1 , \u2016b\u0303 \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n\u2265 2\u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1), (48)\nwhere \u0393 and P are defined in (33) and (31). Hence, the goal of the rest of the proof is to show that:\n2\u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) \u2265 min{\u2016b \u3008t\u3009\u2016, cl}.\nThe main idea of this part is as follows. First note that\n\u2202\u0393(xa, xb, x\u03b8)\n\u2202xb\n\u2223\u2223\u2223\u2223 xb=0 = 1 + |x\u03b8|2 2 .\nHence, intuitively speaking we can argue that there exists a neighborhood of xb = 0 on which the derivative is always larger than 0.5. Hence, when \u2016b\u3008t\u3009\u2016 belongs to this neighborhood, \u2016b\u3008t+1\u3009\u2016 is larger than \u2016b\u3008t\u3009\u2016 and cannot go to zero. Next lemma justifies this claim.\nLemma 14. For \u03b8?\u30080\u3009,1 > 0 there exists a value \u03b4b only depending on cU,1, \u03b8 ? \u30080\u3009,1 and \u2016\u03b8 ?\u2016 such that\ninf 0\u2264xa\u2264cU,1,0\u2264xb\u2264\u03b4b,\u03b8?\u30080\u3009,1\u2264x\u03b8\u2264\u2016\u03b8 ?\u2016\n\u2202\u0393(xa, xb, x\u03b8)\n\u2202xb \u2265 1/2.\nWe present the proof of this result in the Appendix B.6. We remind the reader that according to Lemma 9, \u2016a\u3008t\u3009\u2016 \u2264 cU,1. Furthermore, since the angle \u03b2\u3008t\u3009 is a non increasing sequence we have \u03b8?\u3008t\u3009,1 \u2265 \u03b8 ? \u30080\u3009,1. Suppose that \u2016b \u3008t\u3009\u2016 \u2264 \u03b4b. Then from (48) we know that \u2016b\u3008t+1\u3009\u2016 \u2265 2\u0393(a\u0303 \u3008t\u3009 1 , \u2016b\n\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1). Also from the mean value theorem we have:\n|\u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u0393(a\u0303 \u3008t\u3009 1 , 0, \u03b8 ? \u3008t\u3009,1)| =\n\u2202\u0393(a\u0303 \u3008t\u3009 1 , xb, \u03b8 ? \u3008t\u3009,1)\n\u2202xb |xb=\u03be\u2016b \u3008t\u3009\u2016 \u2265 1 2 \u2016b\u3008t\u3009\u2016,\nwhere \u03be \u2208 [0, \u2016b\u3008t\u3009\u2016] and to obtain the last inequality we used Lemma 14 and the fact that \u2016b\u3008t\u3009\u2016 \u2264 \u03b4b. So far we have proved that if \u2016b\u3008t\u3009\u2016 \u2264 \u03b4b, then \u2016b\u3008t+1\u3009\u2016 \u2265 \u2016b\u3008t\u3009\u2016. But, we have not ruled out the possibility of the situation in which \u2016b\u3008t\u3009\u2016 \u2265 \u03b4b, but \u2016b\u3008t+1\u3009\u2016 is close to zero. That requires a simple continuity argument. Note that since \u0393 is a continuous function of all its variables, its infimum over a compact set is achieved at certain point. Since, the value of \u0393(x1, xb, x\u03b8) is only zero when xb = 0, we conclude that the infimum is not zero. Hence, we conclude that\ncl , inf 0\u2264xa\u2264cU,1,\u03b4b\u2264xb\u2264cU,3,\u03b8?\u30080\u3009,1\u2264x\u03b8\u2264\u2016\u03b8\u2016 2\u0393(xa, xb, x\u03b8) > 0.\nHence, we have if \u2016b\u3008t\u3009\u2016 \u2265 \u03b4b, then\n\u2016b\u3008t+1\u3009\u2016 \u2265 2\u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) \u2265 cl.\nTherefore combining the result of \u2016b\u3008t\u3009\u2016 \u2264 \u03b4b and \u2016b\u3008t\u3009\u2016 \u2265 \u03b4b, we know Lemma 10 holds.\nA.5.3 Proof of Lemma 11\nWe consider three cases and deal with them separately: (i) 0 < xa < \u03b8, (ii) xa \u2265 \u03b8, (iii) xa = 0.\n(i) 0 < xa < \u03b8: Let x1 , \u03b8 + xa > \u03b8 \u2212 xa , x2 > 0. We first simplify the left hand side of the inequality. Our main goal in this section is to derive sharp upper bounds for \u0393(xa, xb, \u03b8) and 1\u2212 2P (xa, xb, \u03b8). We start with \u0393(xa, xb, \u03b8). Note that\n\u0393(xa, xb, \u03b8) = \u222b w(y \u2212 xa, xb)y\u03c6+(y, \u03b8) dy\n= xaP (xa, xb, \u03b8) + \u222b (w(y \u2212 xa, xb)\u2212 1\n2 +\n1 2 )(y \u2212 xa)\u03c6+(y, \u03b8) dy\n= xaP (xa, xb, \u03b8)\u2212 1\n2 xa +\n\u222b (w(y \u2212 xa, xb)\u2212 1\n2 )(y \u2212 xa)\u03c6+(y, \u03b8) dy\n= xaP (xa, xb, \u03b8)\u2212 1\n2 xa +\n1\n2\n\u222b (w(y \u2212 xa, xb)\u2212 1\n2 )(y \u2212 xa)(\u03c6(y \u2212 \u03b8) + \u03c6(y + \u03b8)) dy\n= xaP (xa, xb, \u03b8)\u2212 1\n2 xa +\n1 4 (F (xb, \u03b8 \u2212 xa) + F (xb, \u03b8 + xa))\n= xaP (xa, xb, \u03b8)\u2212 1\n2 xa +\n1 4 (F (xb, x1) + F (xb, x2)), (49)\nwhere F is defined in (36). Next, we find an upper bound for 12(F (xb, x1) + F (xb, x2)). Note\nthat \u2200xb \u2265 0, x\u03b8 \u2265 0, we have\nF (xb, x\u03b8) = \u222b eyxb \u2212 e\u2212yxb eyxb + e\u2212yxb y 1\u221a 2\u03c0 e\u2212(y\u2212x\u03b8) 2/2 dy\n\u2264 \u222b \u221e\n0 y 1\u221a 2\u03c0 e\u2212(y\u2212x\u03b8) 2/2 dy + \u222b \u221e 0 y 1\u221a 2\u03c0 e\u2212(y+x\u03b8) 2/2 dy\n= x\u03b8 \u222b x\u03b8 \u2212x\u03b8 1\u221a 2\u03c0 e\u2212y 2/2 dy + \u222b \u221e \u2212x\u03b8 y 1\u221a 2\u03c0 e\u2212y 2/2 dy + \u222b \u221e x\u03b8 y 1\u221a 2\u03c0 e\u2212y 2/2 dy \u2264 x\u03b8(1\u2212 2\u03a6(\u2212x\u03b8)) + 2\u03c6(x\u03b8) , l(x\u03b8).\nTherefore, if we plug in x\u03b8 = x1 and x\u03b8 = x2, we have\n1 2 (F (xb, x1) + F (xb, x2)) \u2264 1 2 (l(x1) + l(x2)) \u2264 l(x1), (50)\nwhere the last inequality holds since l(x) is an increasing function. This can be proved by taking the derivative of l(x):\ndl(x\u03b8)\ndx\u03b8 = 1\u2212 2\u03a6(\u2212x\u03b8) + 2x\u03b8\u03c6(x\u03b8)\u2212 2x\u03b8\u03c6(x\u03b8) = 1\u2212 2\u03a6(\u2212x\u03b8) \u2265 0.\nCombining (49) and (50) we obtain\n\u0393(xa, xb, \u03b8) \u2264 xaP (xa, xb, \u03b8)\u2212 1\n2 xa + 2l(x1). (51)\nNow we obtain an upper bound for 1\u2212 2P (xa, xb, \u03b8). Note that, 1\u2212 2P (xa, xb, \u03b8) = \u222b ( 1\n2 \u2212 e\nyxb\neyxb + e\u2212yxb ) 1\u221a 2\u03c0 (e\u2212(y+xa\u2212\u03b8) 2/2 + e\u2212(y+xa+\u03b8) 2/2) dy\n= \u222b ( 1\n2 \u2212 e\nyxb\neyxb + e\u2212yxb ) 1\u221a 2\u03c0 (e\u2212(y\u2212x2) 2/2 + e\u2212(y+x1) 2/2) dy\n=\n\u222b e\u2212yxb \u2212 eyxb\n2(eyxb + e\u2212yxb) 1\u221a 2\u03c0 (e\u2212(y\u2212x2) 2/2 + e\u2212(y+x1) 2/2) dy\n=\n\u222b eyxb \u2212 e\u2212yxb\n2(eyxb + e\u2212yxb) 1\u221a 2\u03c0 (e\u2212(y\u2212x1) 2/2 \u2212 e\u2212(y\u2212x2)2/2) dy\n= K(x1, xb)\u2212K(x2, xb), (52)\nwhere K(x, b) , \u222b\neyb\u2212e\u2212yb 2(eyb+e\u2212yb) 1\u221a 2\u03c0 e\u2212(y\u2212x) 2/2 dy. The following lemma proved in the Appendix B.7 summarizes some of the nice properties of this function, which will be used later in our proof.\nLemma 15. K(x, b) is a concave, strictly increasing function of x. Furthermore, K(0, xb) = 0.\nGiven (51) and (52) we can now prove the claimed upper bound in Lemma 11. We have\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8))\n= {xaP (xa, xb, \u03b8)\u2212 12xa + 1 4(F (xb, x1) + F (xb, x2))}(1\u2212 2P (xa, xb, \u03b8))\n2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8))\n= xa + 1 2(F (xb, x1) + F (xb, x2))(1\u2212 2P (xa, xb, \u03b8))\u2212 xa\n4P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8))\n= xa + 1 2(F (xb, x1) + F (xb, x2))(K(x1, xb)\u2212K(x2, xb))\u2212 x1\u2212x2 2\n4P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8))\n\u2264 xa + l(x1)(K(x1, xb)\u2212K(x2, xb))\u2212 12(x1 \u2212 x2)\n4P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) . (53)\nIt is straightforward to use the concavity of K(x, xb) in terms of x and prove that the function K(x1,xb)\u2212K(x2,xb) x1\u2212x2 is a decreasing function of x2. Hence, it is maximized at x2 = 0. Since K(0, xb) = 0, proved in Lemma 15, we have\nK(x1, xb)\u2212K(x2, xb) \u2264 K(x1, xb)\nx1 (x1 \u2212 x2). (54)\nCombining (53) and (54) implies:\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) \u2264 xa + (l(x1)\nK(x1,xb) x1 \u2212 12)(x1 \u2212 x2) 4P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8))\n= xa + (l(x1)\nK(x1,xb) x1 \u2212 12)xa 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8))\n= (1 + l(x1)\nK(x1,xb) x1 \u2212 12 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) )xa. (55)\nOur next step is to find an upper bound for (l(x1) K(x1,xb) x1 \u2212 12). Note that\nK(x1, xb) =\n\u222b eyxb \u2212 e\u2212yxb\n2(eyxb + e\u2212yxb) 1\u221a 2\u03c0 e\u2212(y\u2212x1) 2/2 dy\n= \u222b \u221e 0 1 2 1\u221a 2\u03c0 e\u2212(y\u2212x1) 2/2 dy \u2212 \u222b \u221e 0\ne\u2212yxb\n(eyxb + e\u2212yxb) 1\u221a 2\u03c0 e\u2212(y\u2212x1) 2/2 dy\n\u2212 \u222b \u221e\n0\n1\n2 1\u221a 2\u03c0 e\u2212(y+x1) 2/2 dy + \u222b \u221e 0\ne\u2212yxb\n(eyxb + e\u2212yxb) 1\u221a 2\u03c0 e\u2212(y+x1) 2/2 dy\n\u2264 \u222b x1\n0\n1\u221a 2\u03c0 e\u2212y 2/2 dy = 1 2 \u2212 \u03a6(\u2212x1).\nFinally to obtain an upper bound for 1x l(x)( 1 2 \u2212 \u03a6(\u2212x)) we use the following lemma:\nLemma 16. Define l(x) , x(1\u2212 2\u03a6(\u2212x)) + 2\u03c6(x), then for all x > 0, we have\nl(x)(12 \u2212 \u03a6(\u2212x)) x < 1 2 .\nThe proof of this lemma is presented in Appendix B.8. Using this lemma, we have\n2l(x1)( 1 2 \u2212 \u03a6(\u2212x1)) x1 < 1,\u2200x1 = xa + \u03b8 \u2208 [\u03b8, 2\u03b8].\nBy continuity of the function l(x)( 1 2 \u2212\u03a6(\u2212x)) x , we have\n\u03ba\u0304a(\u03b8) = sup x1\u2208[\u03b8,2\u03b8] 2l(x1)( 1 2 \u2212 \u03a6(\u2212x1)) x1 < 1.\nIt is straightforward to prove that \u03ba\u0304a(\u03b8) is a continuous function of \u03b8 \u2208 (0,\u221e). Since 4P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) \u2264 1, we can bound (55) in the following way:\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) \u2264 (1 + l(x1)\n1 2 \u2212\u03a6(\u2212x1) x1\n\u2212 12 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) )xa\n\u2264 (1 + \u03ba\u0304a(\u03b8)\u2212 1 4P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) )xa \u2264 \u03ba\u0304a(\u03b8)xa, \u22000 < xa < \u03b8, xb > 0. (56)\nThis completes the proof of part (i).\n(ii) xa \u2265 \u03b8: According to Lemma 4, 1\u2212 2P (xa, xb, \u03b8) \u2265 0. Together with Lemma 12 we have\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) \u2264 (xa +\n\u221a 2 \u03c0 )(1\u2212 2P (xa, xb, \u03b8))\n2(1\u2212 P (xa, xb, \u03b8))\n= xa +\n\u221a 2 \u03c0 (1\u2212 2P (xa, xb, \u03b8))\u2212 xa\n2(1\u2212 P (xa, xb, \u03b8)) . (57)\nFrom Lemma 13, we have\n1\u2212 2P (xa, xb, \u03b8) \u2264 \u03a6(xa + \u03b8) + \u03a6(xa \u2212 \u03b8)\u2212 1. (58)\nNote that\n\u2202(\u03a6(xa + \u03b8) + \u03a6(xa \u2212 \u03b8)\u2212 1) \u2202xa\n= \u03c6(xa + \u03b8) + \u03c6(xa \u2212 \u03b8) \u2264 \u221a 2\n\u03c0 , \u2200xa,\nand \u03a6(0 + \u03b8) + \u03a6(0\u2212 \u03b8)\u2212 1 = 0.\nTherefore, from (58) and mean value theorem, we have 1\u2212 2P (xa, xb, \u03b8) \u2264 \u221a 2\n\u03c0 xa.\nTogether with (57) and P (xa, xb, \u03b8)) \u2264 12 , we have\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) \u2264 xa +\n\u221a 2 \u03c0 (1\u2212 2P (xa, xb, \u03b8))\u2212 xa\n2(1\u2212 P (xa, xb, \u03b8))\n\u2264 xa \u2212 1\u2212 2\u03c0\n2(1\u2212 P (xa, xb, \u03b8)) xa\n\u2264 (1 2 + 1 \u03c0 )xa. (59)\n(iii) xa = 0: It is straightforward to prove that P (0, xb, \u03b8) = 12 . Hence,\n\u0393(xa, xb, \u03b8)(1\u2212 2P (xa, xb, \u03b8)) 2P (xa, xb, \u03b8)(1\u2212 P (xa, xb, \u03b8)) = 0.\nCombining Case (i), (ii), (iii), we conclude that if we define\n\u03baa(\u03b8) =  max(\u03ba\u0304a(\u03b8), 1 2 + 1 \u03c0 ), \u03b8 > 0 1\n2 +\n1 \u03c0 , \u03b8 = 0\n,\nthen the statement of Lemma A.5.3 holds.\nA.6 Proof of Theorem 4\nIt is straightforward to use Theorem 3 and show that for every \u03b4a > 0, there exists a value of T\u03b4a such that for every t > T\u03b4a , \u2016a\u3008t\u3009\u2016 \u2264 \u03b4a. For the moment suppose that the following claim is true: there exists \u03b4a > 0, \u03bab, cb only depending on \u2016\u03b8?\u2016, |\u03b8?\u30080\u3009,1| and the initialization {\u2016a\n\u30080\u3009\u2016, \u2016b\u30080\u3009\u2016}, such that if \u2016a\u3008t\u3009\u2016 \u2264 \u03b4a for some t, then the next iteration b\u3008t+1\u3009 satisfies the following equation:\n\u2016b\u3008t+1\u3009 \u2212 \u03b8?\u20162 \u2264 \u03ba2b\u2016b\u3008t\u3009 \u2212 \u03b8?\u20162 + cb\u2016a\u3008t\u3009\u2016.\nIf we combine this claim with the result of Theorem 3, we obtain Theorem 4. Hence, the problem reduces to proving the above claim.\nNote that in Lemma 9, Lemma 10 and Lemma 8, we have\n\u2016a\u3008t\u3009\u2016 \u2208 [0, cU,1], \u2016b\u3008t\u3009\u2016 \u2208 [cL,1, cU,3], \u03b8?\u3008t\u3009,1 \u2208 [\u03b8 ? \u30080\u3009,1, \u2016\u03b8 ?\u2016], \u2200t \u2265 0.\nTherefore, it is again straightforward to see that the following lemma implies our claim:\nLemma 17. For any a\u3008t\u3009, b\u3008t\u3009,\u03b8? \u2208 R2, if \u2016a\u3008t\u3009\u2016 \u2208 [0, Ua], \u2016b\u3008t\u3009\u2016 \u2208 [Lb, Ub], \u3008\u03b8 ?,b\u3008t\u3009\u3009 \u2016b\u3008t\u3009\u2016\n\u2208 [L\u03b8, \u2016\u03b8?\u2016],\u2200t \u2265 0, where Lb > 0, L\u03b8 > 0 then there exists \u03b4a \u2208 (0,min{L\u03b8, 1}];\u03bab \u2208 (0, 1); cb > 0 such that \u2200\u2016a\u3008t\u3009\u2016 \u2208 [0, \u03b4a], \u2016b\u3008t\u3009\u2016 \u2208 [Lb, Ub], \u3008\u03b8\n?,b\u3008t\u3009\u3009 \u2016b\u3008t\u3009\u2016 \u2208 [L\u03b8, \u2016\u03b8?\u2016], the next iteration b\u3008t+1\u3009 satisfying\n\u2016b\u3008t+1\u3009 \u2212 \u03b8?\u20162 \u2264 \u03ba2b\u2016b\u3008t\u3009 \u2212 \u03b8?\u20162 + cb\u2016a\u3008t\u3009\u2016,\nwhere \u03b4a, \u03bab and cb are functions of only Ua, Lb, Ub, L\u03b8, \u2016\u03b8?\u2016.\nTo prove this lemma, we use the notations and definition that are summarized in Appendix A.4. In particular, we use the rotation matrix U t introduced in that section and rotate all the vectors a, b, \u03b8, q with this matrix. Note that according to Lemma 7 we know that b\u3008t+1\u3009 lies in the span of \u03b8? and b\u3008t\u3009 and hence, b\u0303 \u3008\u3008t\u3009,i,t+1\u3009 = 0 for i \u2265 3. Therefore we only need to consider the first two coordinates. Our strategy of proving this lemma is to prove the following two claims for the first two coordinates:\n1. There exists \u03bas \u2208 (0, 1) such that |b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| \u2264 \u03bas|\u03b8 ? \u3008t\u3009,2|.\n2. There exists \u03ba\u2032b \u2208 (0, 1) and \u03b4a > 0 such that if \u2016a\u3008t\u3009\u2016 \u2264 \u03b4a, then\n|b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| \u2264 \u03ba \u2032 b|\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|+ (16\u2016\u03b8\u2016+ 6)\u2016a \u3008t\u3009\u2016.\nWe will then combine the above two claims to obtain Lemma 17.\n1. Proof of |b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| \u2264 \u03bas|\u03b8 ? \u3008t\u3009,2|: To prove our first claim, first note that according to (30)\nand (34) we have\nb\u0303 \u3008t+1\u3009 \u3008t\u3009,2 =\n\u03b3\u0303 \u3008t+1\u3009 \u3008t\u3009,2\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n= \u03b8?\u3008t\u3009,2 S(a\u0303\n\u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\nHence\n|b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| = |\u03b8 ? \u3008t\u3009,2|(1\u2212\nS(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) )\n\u2264 |\u03b8?\u3008t\u3009,2|(1\u2212 2S(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\nBy definition of function S, it is straightforward to conclude that S(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) is only zero if \u2016b\u3008t\u3009\u2016 = 0 and can only go to zero if a\u0303\u3008t\u30091 \u2192 \u221e or \u2016b \u3008t\u3009\u2016 \u2192 \u221e. Therefore, combined with the continuity of S, we conclude that\n\u03bas , sup a\u0303 \u3008t\u3009 1 \u2208[0,Ua],\u2016b \u3008t\u3009\u2016\u2208[Lb,Ub],\u03b8?\u3008t\u3009,1\u2208[L\u03b8,\u2016\u03b8 ?\u2016]\n1\u2212 2S(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) < 1, (60)\nwhere \u03bas only depends on Ua, Lb, Ub, L\u03b8 and \u2016\u03b8?\u2016. Hence,\n|b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| \u2264 \u03bas|\u03b8 ? \u3008t\u3009,2|. (61)\n2. Proof of |b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| \u2264 \u03ba \u2032 b|\u2016b \u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|+ (16\u2016\u03b8\u2016+ 6)\u2016a \u3008t\u3009\u2016: Note that\n|b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| = \u2223\u2223\u2223\u2223\u2223\u2223 \u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2212 \u03b8?\u3008t\u3009,1 \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 2\u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1 + \u03b8 ? \u3008t\u3009,1(2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1) 2\n4P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223 2\u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1\n4P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2223\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u2223\u2223\u2223\u2223 \u03b8?\u3008t\u3009,1(2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1) 2\n4P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n\u2223\u2223\u2223\u2223\u2223\u2223 (62)\nFurthermore in (49), we proved that\n\u0393(xa, xb, x\u03b8) = xaP (xa, xb, \u03b8)\u2212 1\n2 xa +\n1 4 (F (xb, xa + x\u03b8) + F (xb, x\u03b8 \u2212 xa)).\nTo see the definitions of P,\u0393, and F you may refer to Appendix A.4. Hence, we have\n|2\u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1|\n= |a\u0303\u3008t\u30091 (2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1) + 1 2 (F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1 \u2212 a\u0303 \u3008t\u3009 1 )\u2212 F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n+ 1\n2 (F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1 + a\u0303 \u3008t\u3009 1 )\u2212 F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) + (F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1)|. (63)\nCombining (62) and (63), we conclude that in order to obtain an upper bound for |b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212\u03b8 ? \u3008t\u3009,1| we have to find the following bounds:\n(a) Obtain an upper bound for |2P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1|. (b) Obtain an upper bound for |F (\u2016b\u3008t\u3009\u2016, x\u03b8)\u2212 F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)| for all \u03b8 ? \u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8\n?\u2016] and |x\u03b8 \u2212 \u03b8?\u3008t\u3009,1| \u2264 L\u03b8.\n(c) Obtain an upper bound for |F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1| for all \u03b8 ? \u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016] and \u2016b\u3008t\u3009\u2016 \u2208 [Lb, Ub] (d) Obtain a lower bound for 4P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)).\nWe summarize our strategy for bounding each of these terms below:\n(a) Upper bound for |2P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u22121|: It is straightforward to confirm 2P (0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) = 1 2 . Hence, we have to calculate |2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 2P (0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)|. According to\nmean value theorem\n|P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 P (0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)| = \u2223\u2223\u2223\u2223\u2223\u2223 \u2202P (xa, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) \u2202xa |xa=\u03be \u2223\u2223\u2223\u2223\u2223\u2223 (a\u0303\u3008t\u30091 ), (64) where \u03be \u2208 [0, a\u0303\u3008t\u30091 ]. Therefore we only need to bound | \u2202P (xa,\u2016b\u3008t\u3009\u2016,\u03b8?\u3008t\u3009,1) \u2202xa\n|. Note that\u2223\u2223\u2223\u2223\u2223\u2223 \u2202P (xa, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2202xa |xa=0 \u2223\u2223\u2223\u2223\u2223\u2223 = \u222b 2\u2016b\u3008t\u3009\u2016\n(ey\u2016b \u3008t\u3009\u2016\u2212xa\u2016b\u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016+xa\u2016b\u3008t\u3009\u2016)2 \u03c6+(y, \u03b8?\u3008t\u3009,1) dy|xa=0\n=\n\u222b 2\u2016b\u3008t\u3009\u2016\n(ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)2 \u03c6+(y, \u03b8?\u3008t\u3009,1) dy\n=\n\u222b 2\u2016b\u3008t\u3009\u2016\n(ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)2 \u03c6(y \u2212 \u03b8?\u3008t\u3009,1) dy. (65)\nNext we show that \u2223\u2223\u2223\u2223\u2202P (xa,\u2016b\u3008t\u3009\u2016,\u03b8?\u3008t\u3009,1))\u2202xa |xa=0 \u2223\u2223\u2223\u2223 is a decreasing function of \u03b8?\u3008t\u3009,1 and hence can be upper bounded by \u2223\u2223\u2223\u2223\u2202P (xa,\u2016b\u3008t\u3009\u2016,0))\u2202xa |xa=0 \u2223\u2223\u2223\u2223:\n\u2202 \u222b 2\u2016b\u3008t\u3009\u2016\n(ey\u2016b \u3008t\u3009\u2016+e\u2212y\u2016b \u3008t\u3009\u2016)2 \u03c6(y \u2212 \u03b8?\u3008t\u3009,1) dy\n\u2202\u03b8?\u3008t\u3009,1\n=\n\u222b 2\u2016b\u3008t\u3009\u2016\n(ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)2 (y \u2212 \u03b8?\u3008t\u3009,1)\u03c6(y \u2212 \u03b8 ? \u3008t\u3009,1) dy\n= \u2212 \u222b\n2\u2016b\u3008t\u3009\u2016 (ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)2 d\u03c6(y \u2212 \u03b8?\u3008t\u3009,1)\n= \u2212 \u222b\n4\u2016b\u3008t\u3009\u20162(ey\u2016b\u3008t\u3009\u2016 \u2212 e\u2212y\u2016b\u3008t\u3009\u2016) (ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)3 \u03c6(y \u2212 \u03b8?\u3008t\u3009,1) dy \u2264 0\nHence, \u2223\u2223\u2223\u2223\u2223\u2223 \u2202P (xa, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2202xa |xa=0 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u222b 2\u2016b\u3008t\u3009\u2016 (ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)2 \u03c6(y) dy\n\u2264 \u222b \u221e\n0 4\u2016b\u3008t\u3009\u2016e\u22122y\u2016b\n\u3008t\u3009\u2016\u03c6(y) dy \u2264 \u221a 2\n\u03c0 . (66)\nOur next goal is to show that there exists \u03b41 > 0 is a function of only Lb, Ub, L\u03b8, \u2016\u03b8?\u2016 such that\nsup a\u0303 \u3008t\u3009 1 \u2208[0,\u03b41],\u2016b \u3008t\u3009\u2016\u2208[Lb,Ub],\u03b8?\u3008t\u3009,1\u2208[L\u03b8,\u2016\u03b8 ?\u2016]\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2202P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n\u2202a\u0303 \u3008t\u3009 1 \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 1. (67) This is a simple proof by contradiction. Since we have already done similar arguments in the proof of Lemma 14, for the sake of brevity we skip this argument. By combining (64) and (67) we conclude:\n|1\u2212 2P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)| \u2264 2a\u0303 \u3008t\u3009 1 , \u2200a\u0303 \u3008t\u3009 1 \u2208 [0, \u03b41], \u2016b \u3008t\u3009\u2016 \u2208 [Lb, Ub], \u03b8?\u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016]. (68)\n(b) Upper bound for |F (\u2016b\u3008t\u3009\u2016, x\u03b8)\u2212 F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)|: Again by employing the mean value\ntheorem, we conclude that we have to bound \u2202F (\u2016b \u3008t\u3009\u2016,x\u03b8) \u2202x\u03b8 in a neighborhood of x\u03b8 = \u03b8?\u3008t\u3009,1\nfor all \u2016b\u3008t\u3009\u2016 \u2208 [Lb, Ub], \u03b8?\u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016]. Note that, \u2200x\u03b8 \u2265 0\u2223\u2223\u2223\u2223\u2223\u2202F (\u2016b\u3008t\u3009\u2016, x\u03b8)\u2202x\u03b8 \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222b (2w(y, \u2016b\u3008t\u3009\u2016)\u2212 1)y(y \u2212 x\u03b8)\u03c6(y \u2212 x\u03b8) dy\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u222b (2w(y, \u2016b\u3008t\u3009\u2016)\u2212 1){(y \u2212 x\u03b8)2 + x\u03b8(y \u2212 x\u03b8)}\u03c6(y \u2212 x\u03b8) dy\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223x\u03b8 \u222b ey\u2016b\u3008t\u3009\u2016 \u2212 e\u2212y\u2016b\u3008t\u3009\u2016 ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016 (y \u2212 x\u03b8)\u03c6(y \u2212 x\u03b8) dy\n+\n\u222b ey\u2016b \u3008t\u3009\u2016 \u2212 e\u2212y\u2016b\u3008t\u3009\u2016\ney\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b\n\u3008t\u3009\u2016 (y \u2212 x\u03b8)2\u03c6(y \u2212 x\u03b8) dy \u2223\u2223\u2223 (a) < \u2223\u2223\u2223\u2223\u2223x\u03b8 \u222b 4\u2016b\u3008t\u3009\u2016 (ey\u2016b \u3008t\u3009\u2016 + e\u2212y\u2016b \u3008t\u3009\u2016)2 \u03c6(y \u2212 x\u03b8) dy\n\u2223\u2223\u2223\u2223\u2223+ 1 (b) = 2x\u03b8 \u2223\u2223\u2223\u2223\u2223\u2202(P (xa, \u2016b\u3008t\u3009\u2016, x\u03b8))\u2202xa |xa=0 \u2223\u2223\u2223\u2223\u2223+ 1,\nwhere to obtain Inequality (a) we used integration by parts and also the fact that ey\u2016b \u3008t\u3009\u2016\u2212e\u2212y\u2016b\u3008t\u3009\u2016 ey\u2016b \u3008t\u3009\u2016+e\u2212y\u2016b \u3008t\u3009\u2016 < 1. To see why (b) holds, one may check (65). By employing (66), we then conclude that\n|\u2202F (\u2016b \u3008t\u3009\u2016, x\u03b8) \u2202x\u03b8 | < x\u03b8 4\u221a 2\u03c0 + 1 \u2264 4\u2016\u03b8?\u2016+ 1,\u2200x\u03b8 \u2208 [0, 2\u2016\u03b8?\u2016].\nTherefore, using mean value theorem, we have \u2200|x\u03b8 \u2212 \u03b8?\u3008t\u3009,1| \u2264 L\u03b8, \u03b8 ? \u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016],\n|F (\u2016b\u3008t\u3009\u2016, x\u03b8)\u2212 F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)| \u2264 (4\u2016\u03b8 ?\u2016+ 1)|x\u03b8 \u2212 \u03b8?\u3008t\u3009,1|.\nHence, we have \u2200a\u0303\u3008t\u30091 \u2208 [0, L\u03b8], \u03b8?\u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016],\n|1 2 (F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1 \u2212 a\u0303 \u3008t\u3009 1 )\u2212 F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) + 1 2 (F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1 + a\u0303 \u3008t\u3009 1 )\u2212 F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))| \u2264 (4\u2016\u03b8?\u2016+ 1)a\u0303\u3008t\u30091 . (69)\n(c) Upper bound for |F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1|:\nBecause the proof of this part has many algebraic steps we postpone it to the Appendix B.10.\nLemma 18. Given xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016] where 0 < Lb \u2264 L\u03b8 \u2264 \u2016\u03b8?\u2016 \u2264 Ub < \u221e, there exists \u03ba\u2032\u2032b \u2208 (0, 1) is a function of only Lb, Ub, L\u03b8, \u2016\u03b8 ?\u2016 such that\n|F (xb, x\u03b8)\u2212 x\u03b8| \u2264 \u03ba\u2032\u2032b |xb \u2212 x\u03b8|, \u2200xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016].\n(d) Lower bound for 4P (a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)): Note that\n1\n4P (0, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n\u2212 1 = 0.\nLet p = min{ 1\u2212\u03ba\u2032\u2032b 2\u03ba\u2032\u2032b\n, 1} (This choice will become clear later in the proof). Using contradiction arguments similar to the ones employed in the proof of Lemma 14, it is straight forward to see that there exists \u03b42 > 0 only depending on Lb, Ub, L\u03b8, \u2016\u03b8?\u2016 such that\nsup a\u0303 \u3008t\u3009 1 \u2208[0,\u03b42],\u2016b \u3008t\u3009\u2016\u2208[Lb,Ub],\u03b8?\u3008t\u3009,1\u2208[L\u03b8,\u2016\u03b8 ?\u2016]\n1\n4P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2212 1 \u2264 p.(70)\nNow combining (62), (63), (68), (69), (70) and Lemma 18 we conclude that for all a\u0303\u3008t\u30091 \u2208 [0,min{\u03b41, L\u03b8, 1}], \u2016b\u3008t\u3009\u2016 \u2208 [Lb, Ub], \u03b8?\u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016],\n|2\u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1|\n\u2264 |a\u0303\u3008t\u30091 (2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1)|+ | 1 2 (F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1 \u2212 a\u0303 \u3008t\u3009 1 )\u2212 F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))|\n+|1 2 (F (\u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1 + a\u0303 \u3008t\u3009 1 )\u2212 F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))|+ |F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1|\n\u2264 2(a\u0303\u3008t\u30091 ) 2 + (4\u2016\u03b8?\u2016+ 1)a\u0303\u3008t\u30091 + \u03ba \u2032\u2032 b |\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1| \u2264 (4\u2016\u03b8?\u2016+ 3)a\u0303\u3008t\u30091 + \u03ba \u2032\u2032 b |\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|.\nHence together with (68) again and (70) in (62), we have \u2200a\u0303\u3008t\u30091 \u2208 [0,min{\u03b41, L\u03b8, 1, \u03b42}], \u2016b \u3008t\u3009\u2016 \u2208 [Lb, Ub], \u03b8 ? \u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016]\n|b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| \u2264 \u2223\u2223\u2223\u2223\u2223\u2223 2\u0393(a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1\n4P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2223\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u2223\u2223\u2223\u2223 \u03b8?\u3008t\u3009,1(2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1) 2\n4P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 (1 + p) ( |2\u0393(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1|+ \u03b8 ? \u3008t\u3009,1 ( 2P (a\u0303 \u3008t\u3009 1 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1 )2)\n\u2264 (1 + p)((4\u2016\u03b8?\u2016+ 3)a\u0303\u3008t\u30091 + \u03ba \u2032\u2032 b |\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|+ 4\u2016\u03b8 ?\u2016a\u0303\u3008t\u30091 ) \u2264 2(8\u2016\u03b8?\u2016+ 3)a\u0303\u3008t\u30091 + ( 1\u2212 \u03ba\u2032\u2032b\n2\u03ba\u2032\u2032b + 1)\u03ba\u2032\u2032b |\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|\n\u2264 (16\u2016\u03b8?\u2016+ 6)\u2016a\u3008t\u3009\u2016+ 1 + \u03ba\u2032\u2032b\n2 |\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|.\nIn summary, if we set \u03b4a , min{\u03b41, L\u03b8, 1, \u03b42} > 0 and \u03ba\u2032b , 1+\u03ba\u2032\u2032b 2 < 1, we have \u2200a\u0303 \u3008t\u3009 1 \u2264 \u2016a\u3008t\u3009\u2016 \u2208 [0, \u03b4a], \u2016b\u3008t\u3009\u2016 \u2208 [Lb, Ub], \u03b8?\u3008t\u3009,1 \u2208 [L\u03b8, \u2016\u03b8 ?\u2016],\n|b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| \u2264 (16\u2016\u03b8 ?\u2016+ 6)\u2016a\u3008t\u3009\u2016+ \u03ba\u2032b|\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|.\nSo far we have proved in (61) and (71) and the following bounds:\n|b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| \u2264 (16\u2016\u03b8 ?\u2016+ 6)\u2016a\u3008t\u3009\u2016+ \u03ba\u2032b|\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1| |b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| \u2264 \u03bas|\u03b8 ? \u3008t\u3009,2|.\nLet \u03bab = max{\u03bas, \u03ba\u2032b} \u2208 (0, 1) and c\u2032b = 16\u2016\u03b8 ?\u2016+ 6. Then, we conclude that\n\u2016b\u3008t+1\u3009 \u2212 \u03b8?\u20162 = |b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| 2 + |b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| 2\n\u2264 ((16\u2016\u03b8?\u2016+ 6)\u2016a\u3008t\u3009\u2016+ \u03ba\u2032b|\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1|) 2 + (\u03bas\u03b8 ? \u3008t\u3009,2) 2 \u2264 \u03ba2b(|\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1| 2 + |\u03b8?\u3008t\u3009,2| 2) + (c\u2032b) 2\u2016a\u3008t\u3009\u20162 + 2c\u2032b\u2016a\u3008t\u3009\u2016\u03bab|\u2016b\u3008t\u3009\u2016 \u2212 \u03b8?\u3008t\u3009,1| \u2264 \u03ba2b\u2016b\u3008t\u3009 \u2212 \u03b8?\u20162 + ((c\u2032b)2 + 2c\u2032bUb + 2c\u2032b\u2016\u03b8?\u2016)\u2016a\u3008t\u3009\u2016.\nSetting cb = (c\u2032b) 2 + 2c\u2032bUb + 2c \u2032 b\u2016\u03b8 ?\u2016 completes the proof of Lemma 17.\nA.7 Proof of Theorem 1\nTo prove this result we will use some of the results we have proved in the last few sections. We summarize them here.\n(i) In Section 1.2 we showed that to study the dynamics of Population EM for Model 1, it is sufficient to study\n\u03b8\u3008t+1\u3009 = E((2wd(Y ,\u03b8\u3008t\u3009)\u2212 1)Y ), (71)\nwhere wd(y,\u03b8\u3008t\u3009) = \u03c6(y\u2212\u03b8\u3008t\u3009;I)\n\u03c6(y\u2212\u03b8\u3008t\u3009;I)+\u03c6(y+\u03b8\u3008t\u3009;I) and Y \u223c 12N(\u2212\u03b8 ?, Id) + 1 2N(\u03b8 ?, Id).\n(ii) In Section 1.2 and A.2 we showed that to study the dynamics of Population EM for Model 2, it is sufficient to study\na\u3008t+1\u3009 = \u03b3\u3008t+1\u3009(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) , b\u3008t+1\u3009 = \u03b3\u3008t+1\u3009\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) . (72)\nwhere\n\u03b3\u3008t+1\u3009 = Ewd(Y \u2212 a\u3008t\u3009, b\u3008t\u3009)Y , p\u3008t+1\u3009 = Ewd(Y \u2212 a\u3008t\u3009, b\u3008t\u3009),\nwhere Y \u223c 12N(\u2212\u03b8 ?, Id) + 1 2N(\u03b8 ?, Id).\n(iii) According to Lemma 2, (72) reduces to (71), if a\u30080\u3009 = 0. In other words, if we set a\u30080\u3009 = 0, then a\u3008t\u3009 = 0 and b\u3008t\u3009 = \u03b8\u3008t\u3009. This in turn implies that if we analyze the convergence dynamics of (72), we immediately obtain the convergence of (71) by setting a\u30080\u3009 = 0.\n(iv) If \u03b2\u3008t\u3009 denotes the angle between \u03b8? and b\u3008t\u3009, then we proved in Appendix A.5 that for (72) we have\n| sin\u03b2\u3008t+1\u3009| \u2264 \u03ba\u03b2| sin\u03b2\u3008t\u3009|.\nThe same is true for a\u30080\u3009 = 0 initialization.\n(v) According to Lemma 9 and Lemma 10, we have \u2016b\u3008t\u3009\u2016 \u2208 [cL,1, cU,3]. According to Lemma 8, we have \u03b8?\u3008t\u3009,1 \u2208 [\u03b8 ? \u30080\u3009,1, \u2016\u03b8 ?\u2016]. The same is true for a\u30080\u3009 = 0 initialization.\nNote that we can employ Theorem 4 to claim that (by setting a\u3008t\u3009 = 0) if \u3008b\u3008t\u3009,\u03b8?\u3009 > 0, then for the symmetric case, there exists T0 such that for every t > T0,\n|b\u3008t+1\u3009 \u2212 \u03b8?| \u2264 \u03ba2b |b\u3008t\u3009 \u2212 \u03b8?|.\nHowever, our claim in Theorem 1 is stronger. In fact we would like to show that for the symmetric case the geometric convergence starts at iteration 1. We use the notations and equations developed in Appendix A.4. In particular, we use the rotation matrix U t introduced there and rotate all the vectors b\u3008t\u3009 and \u03b3\u3008t\u3009 with U t. Note that according to Lemma 7 we know that b\u3008t+1\u3009 lies in the span of \u03b8? and b\u3008t\u3009 and hence, b\u0303\u3008t+1\u3009\u3008t\u3009,i = 0 for i \u2265 3. Therefore we only need to consider the first two coordinates. According to (30) and (34), we have\n|b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| =\n|\u03b3\u0303\u3008t+1\u3009\u3008t\u3009,2 |\n2P (0, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n= |\u03b8?\u3008t\u3009,2| \u2223\u2223\u2223\u2223\u2223\u2223 S(0, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\n2P (0, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)(1\u2212 P (0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1))\n\u2212 1 \u2223\u2223\u2223\u2223\u2223\u2223 = |\u03b8?\u3008t\u3009,2||2S(0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 1|, (73)\nwhere the last equality is due to the fact that P (0, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) = 1 2 . Also, according to (60) we have\n\u03bas , sup a\u0303 \u3008t\u3009 1 \u2208[0,Ua],\u2016b \u3008t\u3009\u2016\u2208[Lb,Ub],\u03b8?\u3008t\u3009,1\u2208[L\u03b8,\u2016\u03b8 ?\u2016]\n1\u2212 2S(a\u0303\u3008t\u30091 , \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) < 1.\nHence, let Ua = 0, Lb = cL,1, Ub = cL,3 and L\u03b8 = \u03b8?\u30080\u3009,1, we conclude that\n|b\u0303\u3008t+1\u3009\u3008t\u3009,2 \u2212 \u03b8 ? \u3008t\u3009,2| \u2264 \u03bas|\u03b8 ? \u3008t\u3009,2|. (74)\nSimilarly, employing (30) for the first coordinate and using the fact that P (0, \u2016b\u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1) = 1 2 , we obtain\n|b\u0303\u3008t+1\u3009\u3008t\u3009,1 \u2212 \u03b8 ? \u3008t\u3009,1| = |2\u0393(0, \u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1| = |F (\u2016b \u3008t\u3009\u2016, \u03b8?\u3008t\u3009,1)\u2212 \u03b8 ? \u3008t\u3009,1|. (75)\nNote that the last equality is due to (37). According to Lemma 18, we know that there exists \u03ba\u2032\u2032b \u2208 (0, 1) which is a function of only Lb, Ub, L\u03b8, \u2016\u03b8 ?\u2016 such that\n|F (xb, x\u03b8)\u2212 x\u03b8| \u2264 \u03ba\u2032\u2032b |xb \u2212 x\u03b8|,\u2200xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016]. (76)\nLet Lb = cL,1, Ub = cL,3 and L\u03b8 = \u03b8?\u30080\u3009,1. Combining (74), (75) and (76) completes the proof.\nA.8 Proof of Theorem 5\nA.8.1 Main Steps of the Proof\nThe proof for the case \u3008b\u30080\u3009,\u03b8?\u3009 = 0 is very different from the proof of the case \u3008b\u30080\u3009,\u03b8?\u3009 6= 0. It seems that the convergence of the algorithm to its stationary point may not be geometric and hence proof ideas we developed for the case \u3008b\u30080\u3009,\u03b8?\u3009 6= 0 are not applicable here. Hence, we prove Theorem 4 using the following strategy:\n(i) We first characterize all the stationary points of Population EM. Let (a, b) denote the stationary points and we show that a = 0 and b \u2208 {\u2212\u03b8?,0,\u03b8?}. This is discussed in Appendix A.8.2.\n(ii) We then show that any accumulation point of {(a\u3008t\u3009, b\u3008t\u3009)} is one of the stationary points. Let (a\u221e, b\u221e) denote any accumulation point. This is discussed in (i) in Appendix A.8.3.\n(iii) We show that if \u3008b\u30080\u3009,\u03b8?\u3009 = 0, b\u221e can not converge to \u2212\u03b8? or \u03b8?. Hence, the algorithm has to converge to 0. Since a\u221e = 0 for all stationary points, we have {a\u3008t\u3009, b\u3008t\u3009} converges to (0,0). This is discussed in (ii) in Appendix A.8.3\nA.8.2 Characterizing the Fixed Points of Population EM\nFirst note that if we write the iterations of Population EM in terms of a\u3008t\u3009 and b\u3008t\u3009 we obtain\na\u3008t+1\u3009 = \u03b3\u3008t+1\u3009(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) , b\u3008t+1\u3009 = \u03b3\u3008t+1\u3009\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) ,\nwhere\n\u03b3\u3008t+1\u3009 = \u222b wd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)y\u03c6+d (y,\u03b8 ?) dy,\np\u3008t+1\u3009 = \u222b wd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u03c6+d (y,\u03b8 ?) dy.\nIf (\u03b3\u3008t\u3009, p\u3008t\u3009,a\u3008t\u3009, b\u3008t\u3009) converges to (\u03b3, p,a, b), then it is straightforward to show that\na = \u03b3(1\u2212 2p) 2p(1\u2212 p) , (77) b = \u03b3\n2p(1\u2212 p) , (78)\n\u03b3 = \u222b wd(y \u2212 a, b)y\u03c6+d (y,\u03b8 ?) dy, (79)\np = \u222b wd(y \u2212 a, b)\u03c6+d (y,\u03b8 ?) dy. (80)\nHence, the main step of the proof is to characterize the solutions of these four equations. We first consider the one-dimensional setting in which Y \u2208 R and prove the following two facts:\n(i) The only feasible solution for a is zero.\n(ii) We then set a = 0 and show that the only possible solutions for b are \u2212\u03b8\u2217, 0, \u03b8\u2217.\nWe should prove the above two by considering the following four different cases: (1) a \u2265 0, b \u2265 0, (2) a \u2265 0, b \u2264 0, (3) a \u2264 0, b \u2265 0, (4) a \u2264 0, b \u2264 0. Since the four cases are similar we focus on the first case only, i.e., a \u2265 0, b \u2265 0. To prove that the only possible solution of a is zero, note that (77) can be written as\na = \u0393(a, b, \u03b8\u2217)(1\u2212 2P (a, b, \u03b8\u2217)) 2P (a, b, \u03b8\u2217)(1\u2212 P (a, b, \u03b8\u2217)) (1) \u2264 \u03baaa. (81)\nwhere \u03baa < 1. Note that Inequality (1) is a result of Lemma 11. Note that (81) implies that a must be zero.\nThe only remaining step is to examine the solutions for b. It is straightforward to prove that P (0, b, \u03b8\u2217) = 12 . Hence, we can simplify (78) to\nb = 2\u0393(0, b, \u03b8\u2217) = F (b, \u03b8\u2217), (82)\nwhere the last equality is due to (37). The following lemma enables us to characterize the solutions of (82).\nLemma 19. F (xb, x\u03b8) is a concave function of xb \u2265 0. Furthermore, we have the following: (i) F (0, x\u03b8) = 0, (ii) F (x\u03b8, x\u03b8) = x\u03b8.\nThe proof of this lemma is presented in the Appendix B.9. It is straightforward to use the above properties and show that F (xb, x\u03b8) has in fact the shape that is exhibited in Figure 1, which proves our claim in the one dimensional setting.\nTo extend the proof to higher dimensions, we rotate the coordinates. Suppose that the fixed point is a, b, p,\u03b3. Let M\u0303 denote a rotation for which the following two hold: (i) b\u0303 , M\u0303b = (\u2016b\u20162, 0, 0, . . . , 0) and (ii) \u03b8\u0303? , M\u0303\u03b8? = (\u03b8\u0303?1, \u03b8\u0303?2, 0, . . . , 0). Define \u03b3\u0303 = M\u0303\u03b3 and a\u0303 = M\u0303a. Lemma 3 shows that if we let b\u0303, a\u0303, \u03b3\u0303, p denote the corresponding fixed point in the new coordinates, then they satisfy the same equations, i.e.,\na\u0303 = \u03b3\u0303(1\u2212 2p) 2p(1\u2212 p) , (83) b\u0303 = \u03b3\u0303\n2p(1\u2212 p) , (84)\n\u03b3\u0303 = \u222b wd(y \u2212 a\u0303, b\u0303)y\u03c6+d (y,\u03b8 ?) dy, (85)\np = \u222b wd(y \u2212 a\u0303, b\u0303)\u03c6+d (y,\u03b8 ?) dy. (86)\nFirst, it is straightforward to employ (83) and (84) and confirm that \u2200i \u2265 2 we have\n\u03b3\u0303i = 2b\u0303ip(1\u2212 p) = 0, a\u0303i = b\u0303i(1\u2212 2p) = 0.\nHence, with \u03b8\u0303?i = 0,\u2200i \u2265 3, we only need to consider the first two coordinates. Our goal is to prove the following two statements:\n(i) If \u03b8\u0303?2 6= 0, then b\u03031 = 0 and a\u03031 = 0. In other words, both a and b are zero.\n(ii) If \u03b8\u0303?2 = 0, then the problem will be reduced to the one-dimensional problem that we have already discussed. Hence, it is straightforward to characterize the fixed points.\nHere we focus on case (i), i.e., \u03b8\u0303?2 6= 0. Since \u03b3\u03032 = 0, we have\n0 = \u03b3\u03032 = \u222b wd(y \u2212 a\u0303, b\u0303)y2\u03c6+d (y, \u03b8\u0303 ?) dy\n= 1\n2\n\u222b w(y1 \u2212 a\u03031, \u2016b\u2016)\u03c6(y1 \u2212 \u03b8\u0303?1) dy1 \u222b y2\u03c6(y2 \u2212 \u03b8\u0303?2) dy2\n+ 1\n2\n\u222b w(y1 \u2212 a\u03031, \u2016b\u2016)\u03c6(y1 + \u03b8\u0303?1) dy1 \u222b y2\u03c6(y2 + \u03b8\u0303?2) dy2\n= \u03b8\u0303?2 \u222b w(y1 \u2212 a\u03031, \u2016b\u2016)\u03c6\u2212(y1, \u03b8\u0303?1) dy1\n= \u03b8\u0303?2 \u222b +\u221e 0 (w(y1 \u2212 a\u03031, \u2016b\u2016)\u2212 w(\u2212y1 \u2212 a\u03031, \u2016b\u2016))\u03c6\u2212(y1, \u03b8\u0303?1) dy1\n= \u03b8\u0303?2 \u222b +\u221e 0\ne2y1\u2016b\u2016 \u2212 e\u22122y1\u2016b\u2016\ne2y1\u2016b\u2016 + e\u22122y1\u2016b\u2016 + e2\u2016a\u0303\u2016\u2016b\u2016 + e\u22122\u2016a\u0303\u2016\u2016b\u2016 1 2 \u221a 2\u03c0 (e\u2212(y1\u2212\u03b8\u0303 ? 1) 2/2 \u2212 e\u2212(y1+\u03b8\u0303?1)2/2) dy1.\nIt is straightforward to see that since \u03b8\u0303?2 6= 0, then \u03b8\u0303?1 = 0. Hence, from (83) and the definitions of \u0393 and P functions given in Appendix A.4 we have\n\u2016a\u2016 = \u2016a\u0303\u2016 = |a\u03031| = |b\u03031||1\u2212 2p| = \u0393(\u2016a\u0303\u2016, \u2016b\u2016, 0)(1\u2212 2P (\u2016a\u0303\u2016, \u2016b\u2016, 0)) 2P (\u2016a\u0303\u2016, \u2016b\u2016, 0)(1\u2212 P (\u2016a\u0303\u2016, \u2016b\u2016, 0)) \u2264 \u03baa\u2016a\u0303\u2016,\nwhere the last inequality is due to Lemma 11. We know that \u03baa < 1. Therefore we have a = 0 and\n\u2016b\u2016 = b\u03031 = \u03b3\u03031\n2p(1\u2212 p) = 2\u03b3\u03031\n= 2 \u222b w(y \u2212 a\u0303, b\u0303)y1\u03c6+d (y, \u03b8\u0303 ?) dy\n= 2\n\u222b ey1\u2016b\u2016\ney1\u2016b\u2016 + e\u2212y1\u2016b\u2016 y1 1\u221a 2\u03c0 e\u2212y 2 1/2 dy\n= 0.\nThus the only solution is (a, b) = (0,0).\nA.8.3 Proof of Convergence for Population EM\nWe can break the proof into the following steps. Let {a\u3008t\u3009, b\u3008t\u3009}\u221et=1 denote all the estimates of the Population EM algorithm.\n(i) We first prove that every accumulation point of {a\u3008t\u3009, b\u3008t\u3009}\u221et=1 satisfies the fixed point equations:\na = \u03b3(1\u2212 2p) 2p(1\u2212 p) , b = \u03b3\n2p(1\u2212 p) ,\n\u03b3 = Ewd(Y \u2212 a, b)Y p = Ewd(Y \u2212 a, b),\nwhere Y \u223c 12N(\u2212\u03b8 ?, I) + 12N(\u03b8 ?, I). This proof is presented in Appendix B.11. We have already proved that these fixed point equations only have the following solutions : a = 0 and b \u2208 {\u2212\u03b8?,0,\u03b8?}. We proved in Lemma 4 that sgn(\u3008b\u3008t\u3009,\u03b8?\u3009) = sgn(\u3008b\u3008t+1\u3009,\u03b8?\u3009). Hence, we conclude that \u3008b\u3008t\u3009,\u03b8?\u3009 = 0 for every t. The only possible fixed point is hence (0,0). In summary, in the first step we prove that it only has one accumulation point, that is (0,0).\n(ii) Next we prove that {a\u3008t\u3009, b\u3008t\u3009}\u221et=1 is a convergent sequence. Suppose that the sequence does not converge to (0,0), then there exists an such that for every T , there exists a t > T such that\n\u2016(a\u3008t\u3009, b\u3008t\u3009)\u20162 > . We construct a subsequence of our sequence in the following way: Set T = 1 and pick t1 > T such that \u2016(a\u3008t1\u3009, b\u3008t1\u3009)\u20162 > . Now, set T = t1 + 1, and pick t2 > T such that \u2016(a\u3008t2\u3009, b\u3008t2\u3009)\u20162 > . Continue the process until we construct a sequence {(a\u3008tn\u3009, b\u3008tn\u3009)}\u221en=1. According to Lemma 9 {(a\u3008tn\u3009, b\u3008tn\u3009)}\u221en=1 is in a compact set and has a convergent subsequence. But according to part (i) the converging subsequence of this sequence must converge to (0,0) which is in contradiction with the construction of the sequence {(a\u3008tn\u3009, b\u3008tn\u3009)}\u221en=1. Hence {a\u3008t\u3009, b\u3008t\u3009}\u221et=1 must be a convergent sequence and converges to (0,0)\nA.9 Proof of Theorem 6 Let a\u0302\u3008t\u3009 = \u00b5\u0302 \u3008t\u3009 1 +\u00b5\u0302 \u3008t\u3009 2\n2 and b\u0302 \u3008t\u3009 = \u00b5\u0302 \u3008t\u3009 2 \u2212\u00b5\u0302 \u3008t\u3009 1 2 . Then the iteration functions based on (a\u0302 \u3008t\u3009, b\u0302 \u3008t\u3009 ) are the\nfollowing:\na\u0302\u3008t+1\u3009 = q\u0302\u3008t+1\u3009(1\u2212 2p\u0302\u3008t+1\u3009) 2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) + y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) , (87)\nb\u0302 \u3008t+1\u3009 = q\u0302\u3008t+1\u3009\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2212 y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) . (88)\nwhere\ny\u0304 = 1\nn n\u2211 i=1 yi,\nq\u0302\u3008t+1\u3009 = 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 )yi, (89)\np\u0302\u3008t+1\u3009 = 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 ), (90)\nwhere\nwd(y \u2212 xa,xb) = \u03c6d(y \u2212 xa \u2212 xb)\n\u03c6d(y \u2212 xa + xb) + \u03c6d(y \u2212 xa \u2212 xb)\n= e\u3008y\u2212xa,xb\u3009\ne\u3008y\u2212xa,xb\u3009 + e\u2212\u3008y\u2212xa,xb\u3009 .\nTherefore q\u0302\u3008t\u3009 and p\u0302\u3008t\u3009 are the empirical versions of \u03b3\u3008t\u3009 and p\u3008t\u3009 respectively. Our first goal is, for each i \u2208 {1, 2}, to compare the Population EM sequence (\u00b5\u3008t\u3009i )t\u22650 to the Sample-based EM sequence (\u00b5\u0302 \u3008t\u3009 i )t\u22650, provided that the initial values \u00b5 \u30080\u3009 i and \u00b5\u0302 \u30080\u3009 i are the same. We prove that\na\u0302\u3008t\u3009\u2192a\u3008t\u3009 in probability, and b\u0302\u3008t\u3009\u2192b\u3008t\u3009 in probability, as n\u2192\u221e. (91)\nWe prove by induction. For t = 0, it is clear that (91) holds because both Population EM and Sample-based EM start with the same initialization. For t = 1, by Weak Large Law Numbers (WLLN), we have\nq\u0302\u30081\u3009 = 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u30080\u3009, b\u0302 \u30080\u3009 )yi p\u2192 Ewd(y \u2212 a\u0302\u30080\u3009, b\u0302 \u30080\u3009 )y = \u03b3\u30081\u3009,\np\u0302\u30081\u3009 = 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u30080\u3009, b\u0302 \u30080\u3009 ) p\u2192 Ewd(y \u2212 a\u0302\u30080\u3009, b\u0302 \u30080\u3009 ) = p\u30081\u3009,\ny\u0304 = 1\nn n\u2211 i=1 yi p\u2192 Ey = 0.\nSince p\u30081\u3009 \u2208 (0, 1), by employing the continuous mapping theorem, we have\na\u0302\u30081\u3009 = q\u0302\u30081\u3009(1\u2212 2p\u0302\u30081\u3009) 2p\u0302\u30081\u3009(1\u2212 p\u0302\u30081\u3009) + y\u0304 2(1\u2212 p\u0302\u30081\u3009) \u2192 \u03b3 \u30081\u3009(1\u2212 2p\u30081\u3009) 2p\u30081\u3009(1\u2212 p\u30081\u3009) = a\u30081\u3009 in probability,\nb\u0302 \u30081\u3009 = q\u0302\u30081\u3009\n2p\u0302\u30081\u3009(1\u2212 p\u0302\u30081\u3009) +\ny\u0304 2(1\u2212 p\u0302\u30081\u3009) \u2192 \u03b3\n\u30081\u3009\n2p\u30081\u3009(1\u2212 p\u30081\u3009) = b\u30081\u3009 in probability.\nTherefore (91) holds for t = 1. Now we assume that (91) holds for t \u2265 1, and our goal is to prove it for t+ 1. Note that\u2225\u2225\u2225\u2225\u2202wd(y \u2212 xa,xb)\u2202xa \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2212 2xb(e\u3008y,xb\u3009\u2212\u3008xa,xb\u3009 + e\u2212\u3008y,xb\u3009+\u3008xa,xb\u3009)2\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u2016xb\u2016\n2 ,\u2225\u2225\u2225\u2225\u2202wd(y \u2212 xa,xb)\u2202xb \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 2(y \u2212 xa)(e\u3008y\u2212xa,xb\u3009 + e\u2212\u3008y\u2212xa,xb\u3009)2 \u2225\u2225\u2225\u2225\u2225 \u2264 \u2016y \u2212 xa\n2 \u2016\n\u2264 \u2016y\u2016+ \u2016xa\u2016 2 .\nTherefore we have\n|p\u3008t+1\u3009 \u2212 p\u0302\u3008t+1\u3009| = |Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 )|\n\u2264 |Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)|+ \u2223\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)\u2212 1 n n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 ) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 |Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)|\n+ \u2223\u2223\u2223\u2223\u2223\u2223\u2223 1 n n\u2211 i=1 \u2016b\u3008t\u3009\u03be \u2016 2 \u2016a\u0302\u3008t\u3009 \u2212 a\u3008t\u3009\u2016+ \u2016yi\u2016+ \u2016a \u3008t\u3009 \u03be \u2016 2 \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016  \u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 |Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)|+ 1 2 \u2211n i=1 \u2016yi\u2016 n \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016\n+ \u2016b\u3008t\u3009\u03be \u2016 2 \u2016a\u0302\u3008t\u3009 \u2212 a\u3008t\u3009\u2016+ \u2016a\u3008t\u3009\u03be \u2016 2 \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016  , where\na \u3008t\u3009 \u03be = \u03bea \u3008t\u3009 + (1\u2212 \u03be)a\u0302\u3008t\u3009, and b\u3008t\u3009\u03be = \u03beb \u3008t\u3009 + (1\u2212 \u03be)b\u0302\u3008t\u3009, for some \u03be \u2208 [0, 1].\nBy WLLN, induction assumption and\n\u2016a\u3008t\u3009\u03be \u2016 \u2264 2\u2016a \u3008t\u3009\u2016+ \u2016a\u3008t\u3009 \u2212 a\u0302\u3008t\u3009\u2016, and \u2016b\u3008t\u3009\u03be \u2016 \u2264 2\u2016b \u3008t\u3009\u2016+ \u2016b\u3008t\u3009 \u2212 b\u0302\u3008t\u3009\u2016,\nwe have\n|p\u3008t+1\u3009 \u2212 p\u0302\u3008t+1\u3009| \u2264 |Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)\u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)|+ 2\u2016b\u3008t\u3009\u2016+ \u2016b\u3008t\u3009 \u2212 b\u0302\u3008t\u3009\u2016 2 \u2016a\u0302\u3008t\u3009 \u2212 a\u3008t\u3009\u2016\n+ 2\u2016a\u3008t\u3009\u2016+ \u2016a\u3008t\u3009 \u2212 a\u0302\u3008t\u3009\u2016\n2 \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016+ 1 2 \u2211n i=1 \u2016yi\u2016 n \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016 \u2192 0 in probability.\nSimilarly, we have\n\u2016\u03b3\u3008t+1\u3009 \u2212 q\u0302\u3008t+1\u3009\u2016 = \u2016Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)y \u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 )yi\u2016\n\u2264 \u2016Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)y \u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)yi\u2016\n+\u2016 1 n n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)yi \u2212 1 n n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 )yi\u2016\n\u2264 \u2016Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)y \u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)yi\u2016\n+ \u2225\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 ( \u2016b\u3008t\u3009\u03be \u2016 2 \u2016a\u0302\u3008t\u3009 \u2212 a\u3008t\u3009\u2016+ \u2016yi\u2016+ \u2016a \u3008t\u3009 \u03be \u2016 2 \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016)yi \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2016Ewd(y \u2212 a\u3008t\u3009, b\u3008t\u3009)y \u2212 1\nn n\u2211 i=1 wd(yi \u2212 a\u3008t\u3009, b\u3008t\u3009)yi\u2016+ \u2225\u2225\u2225\u2225\u2225\u2225 12n n\u2211 i=1 \u2016yi\u2016yi \u2225\u2225\u2225\u2225\u2225\u2225 \u2016b\u0302\u3008t\u3009 \u2212 b\u3008t\u3009\u2016 +(\u2016a\u3008t\u3009\u2016\u2016b\u3008t\u3009 \u2212 b\u0302\u3008t\u3009\u2016+ \u2016b\u3008t\u3009\u2016\u2016a\u3008t\u3009 \u2212 a\u0302\u3008t\u3009\u2016+ \u2016a\u3008t\u3009 \u2212 a\u0302\u3008t\u3009\u2016\u2016b\u3008t\u3009 \u2212 b\u0302\u3008t\u3009\u2016)\u2016 1\nn n\u2211 i=1 yi\u2016.\nBy WLLN and induction assumption, we have\n\u2016\u03b3\u3008t+1\u3009 \u2212 q\u0302\u3008t+1\u3009\u2016 \u2192 0 in probability.\nTherefore with p\u3008t+1\u3009 \u2208 (0, 1), we have\na\u0302\u3008t+1\u3009 = q\u0302\u3008t+1\u3009(1\u2212 2p\u0302\u3008t+1\u3009) 2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) + y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) \u2192 \u03b3 \u3008t+1\u3009(1\u2212 2p\u3008t+1\u3009) 2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) = a\u3008t+1\u3009 in probability,\nb\u0302 \u3008t+1\u3009 = q\u0302\u3008t+1\u3009\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) +\ny\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) \u2192 \u03b3\n\u3008t+1\u3009\n2p\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) = b\u3008t+1\u3009 in probability.\nHence (91) holds for t+ 1. With induction, we completes the proof of this lemma.\nA.10 Proof of Theorem 7\nA.10.1 Roadmap of the Proof\nThe main idea of the proof is simple. We first show that if we initialize Sample-based EM in a way that a\u0302\u30080\u3009 is small enough and b\u0302 \u30080\u3009 is in small neighborhood of \u03b8?, then the sampled based EM will\nconverge to a point whose distance from \u03b8? is O( \u221a d/n) with probability converging to 1 as n\u2192\u221e. Let\u2019s call this neighborhood of (a, b), N0,\u03b8? . According to Theorem 4 and 3 we know that Population EM converges to the true parameter under quite general initialization. Hence, there exists an iteration T0 at which the estimate of Population EM is in N0,\u03b8? . We know from Theorem 6 that at iteration T0, a\u0302\u3008T0\u3009 \u2192 a\u3008T0\u3009 and b\u0302 \u3008T0\u3009 \u2192 b\u3008T0\u3009 in probability. Hence, with probability converging to 1, (a\u0302\u3008T0\u3009, b\u0302\u3008T0\u3009) \u2208 N0,\u03b8? , and\nhence (a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 ) converge to a point that is at a distance O( \u221a d/n) from (0,\u03b8?). In other words, if a\u0302\u221e and b\u0302 \u221e the limiting estimates, then\n\u2016a\u0302\u221e\u2016 = O( \u221a d/n),\n\u2016b\u0302\u221e \u2212 \u03b8?\u2016 = O( \u221a d/n),\nwith probability converging to 1, which is equivalent to what we wanted to prove. As is clear from the above discussion, the only challenging part is to prove that if (a\u0302\u30080\u3009, b\u0302 \u30080\u3009 ) is in small neighborhood of (0,\u03b8?), then the sampled-based EM will converge to a point whose distance from (0,\u03b8?) is O( \u221a d/n). The proof of this fact is our main goal in the rest of this proof.\nWe remind the reader that according to Theorems 3 and 4 the estimates of Population EM satisfy the following equations (if initialized properly):\n\u2016a\u3008t\u3009\u2016 \u2192 0, \u2016b\u3008t\u3009 \u2212 \u03b8?\u2016 \u2192 0, (92)\nAlso, we know from the arguments provided in the proof of Theorem 6 that a\u0302\u3008t\u3009 and b\u0302 \u3008t\u3009\nconverge to a\u3008t\u3009 and b\u3008t\u3009 in probability. Hence, we expect to have a similar equations for a\u0302\u3008t\u3009 and b\u0302 \u3008t\u3009 , except for probably an error term that will vanish as n\u2192\u221e. The only issue that may happen is that the errors that are introduced in each iteration may accumulate and will let to a non-vanishing error for t\u2192\u221e. Our first lemma shows that this does not happen.\nLemma 20. Suppose that there exist \u03baa \u2208 (0, 1), \u03bab \u2208 (0, 1) and cb > 0 such that for all t\u2032 \u2265 1, we have\n\u2016a\u0302\u3008t\u2032\u3009\u2016 \u2264 \u03baa\u2016a\u0302\u3008t \u2032\u22121\u3009\u2016+ a, (93)\n\u2016b\u0302\u3008t \u2032\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u0302 \u3008t\u2032\u22121\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t \u2032\u22121\u3009\u2016+ b, (94)\nfor some a, b > 0. Then we have \u2200t \u2265 0,\n\u2016a\u0302\u3008t\u3009\u2016 \u2264 (\u03baa)t\u2016a\u0302\u30080\u3009\u2016+ 1\n1\u2212 \u03baa a, (95)\n\u2016b\u0302\u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 (\u03bab)t\u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016+ t \u221a cb\u2016a\u0302\u30080\u3009\u2016(max{ \u221a \u03baa, \u03bab})t +\n1\n1\u2212 \u03bab\n\u221a cb\n1\u2212 \u03baa a +\n1\n1\u2212 \u03bab b\n(96)\nThe proof of this lemma will be presented to in Appendix A.10.2. According to this lemma as long as the errors that are introduced in each iteration are bounded by a and b, the overall error will also remain bounded and are, in the worst case, proportional to \u221a a and b. Hence, if \u03b1 \u2192 0 and b \u2192 0 as n\u2192\u221e, the overall errors will go to zero too. Hence, proving that (93) and (94) hold for a \u2192 0 and \u2192 0 will complete the proof Theorem 7. Lemma 21. There exists constants \u03baa \u2208 ( \u221a 3 2 , 1), \u03bab \u2208 (0, 1); cb > 0 and\n\u03b4a \u2208 (0,min{1, \u221a 3\n2 \u2016\u03b8?\u2016, (1\u2212 \u03bab) 2(1\u2212 (\u03baa)2)\u2016\u03b8?\u20162 4cb })\nonly depending on \u03b8?, such that if the initialization (a\u0302\u30080\u3009, b\u0302 \u30080\u3009 ) satisfies\n\u2016a\u0302\u30080\u3009\u2016 \u2264 \u03b4a, and \u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016 \u2264 \u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016,\nthen \u2200t \u2265 0, we have\n\u2016a\u0302\u3008t+1\u3009\u2016 \u2264 \u03baa\u2016a\u0302\u3008t\u3009\u2016+ a,\n\u2016b\u0302\u3008t+1\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u3009\u2016+ b,\nwith probability at least 1\u2212 3\u03b4. The value of the other constants are the following c\u03b8 = 4(\u2016\u03b8?\u2016+ 2) \u221a 3d+ ln(1/\u03b4)\nn ,\nC\u03b8 = 3\u2016\u03b8?\u2016c\u03b8, a = b = 9C\u03b8 + c\u03b8 \u03c1(2\u2212 \u03c1) + 12C\u03b8 \u03c1(2\u2212 \u03c1) + c\u03b8 2\u2212 \u03c1 ,\n(97)\nwhere \u03c1 = sup\u2016xa\u2016\u22641,\u2016xb\u2016\u2264 32\u2016\u03b8?\u2016max{P (xa,xb,\u03b8 ?), 1 \u2212 P (xa,xb,\u03b8?)} \u2208 (0, 1). The function P is defined in Appendix A.4. In addition, assume n is large enough to satisfy the following conditions:\nC\u03b8 < \u03c1\n2 ,\na = b \u2264 min{(1\u2212 \u03baa)\u03b4a, 1\n2 (1\u2212 \u03bab)\n\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016}.\n(98)\nProof. Note that\nwd(y \u2212 xa,xb) , e\u3008y\u2212xa,xb\u3009\ne\u3008y\u2212xa,xb\u3009 + e\u2212\u3008y\u2212xa,xb\u3009 .\nWe showed the following equations in Appendix A.9:\na\u0302\u3008t+1\u3009 = q\u0302\u3008t+1\u3009(1\u2212 2p\u0302\u3008t+1\u3009) 2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) + y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) ,\nb\u0302 \u3008t+1\u3009 = q\u0302\u3008t+1\u3009\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2212 y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) .\nwhere\ny\u0304 = 1\nn n\u2211 i=1 yi,\nq\u0302\u3008t+1\u3009 = 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 )yi,\np\u0302\u3008t+1\u3009 = 1\nn n\u2211 i=1 wd(yi \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 ),\nWe will show in Appendix A.10.3 that with probability at least 1\u2212 3\u03b4, we have\n\u2016 1 n n\u2211 i=1 yi\u2016 \u2264 4(\u2016\u03b8?\u2016+ 2) \u221a 3d+ ln(1/\u03b4) n = c\u03b8, (99)\nsup \u2016xb\u2016\u2264 32\u2016\u03b8 ?\u2016,\u2016xa\u2016\u22641 | 1 n n\u2211 i=1 wd(yi \u2212 xa,xb)\u2212 EY wd(y \u2212 xa,xb)| \u2264 C\u03b8, (100)\nsup \u2016xb\u2016\u2264 32\u2016\u03b8 ?\u2016,\u2016xa\u2016\u22641 \u2016 1 n n\u2211 i=1 (wd(yi \u2212 xa,xb)\u2212 1 2 )yi \u2212 E(wd(y \u2212 xa,xb)\u2212 1 2 )y\u2016 \u2264 9 2 C\u03b8.\n(101)\nNote that by setting \u03b4 = 1n , we see that c\u03b8 \u2192 0, C\u03b8 \u2192 0, and \u03b4 \u2192 0 simultaneously. In the rest of the proof we assume that (99), (100) and (101) hold. Let\n\u03b3\u0304\u3008t+1\u3009 = Ewd(y \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 )y, p\u0304\u3008t+1\u3009 = Ewd(y \u2212 a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 ),\nand\na\u0304\u3008t+1\u3009 = \u03b3\u0304\u3008t+1\u3009(1\u2212 2p\u0304\u3008t+1\u3009) 2p\u0304\u3008t+1\u3009(1\u2212 p\u0304\u3008t+1\u3009) , b\u0304 \u3008t+1\u3009 = \u03b3\u0304\u3008t+1\u3009 2p\u0304\u3008t+1\u3009(1\u2212 p\u0304\u3008t+1\u3009) .\nThe following lemma that will be proved in Appendix A.10.4 is a key step in our analysis:\nLemma 22. There exists \u03baa \u2208 (0, 1) such that if \u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 min{ \u221a 1\u2212 (\u03baa)2, 12}\u2016\u03b8 ?\u2016, then\n\u2016a\u0304\u3008t+1\u3009\u2016 \u2264 \u03baa\u2016a\u0302\u3008t\u3009\u2016, (102)\nFurthermore, there exist \u03b4\u2032a \u2208 (0, 1), \u03bab \u2208 (0, 1) and cb > 0 such that if \u2016a\u0302\u3008t\u3009\u2016 \u2208 [0, \u03b4\u2032a], then\n\u2016b\u0304\u3008t+1\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u3009\u2016. (103)\nConstant \u03baa, \u03bab, \u03b4\u2032a and cb only depend on \u03b8 ?.\nThe above equations provide connections between (a\u0304\u3008t+1\u3009, b\u0304\u3008t+1\u3009) and (a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009\n). Next, we establish connection between (a\u0304\u3008t+1\u3009, b\u0304\u3008t+1\u3009) and (a\u0302\u3008t+1\u3009, b\u0302 \u3008t+1\u3009 ). In the rest of the proof we assume that \u03baa \u2208 ( \u221a 3/2, 1). If \u03baa is less than \u221a 3/2 we set it to \u221a\n3/2. This is just for making notations simpler and has no specific technical reason.\nNote that from (87), we have\n\u2016a\u0302\u3008t+1\u3009\u2016 = \u2016 q\u0302 \u3008t+1\u3009(1\u2212 2p\u0302\u3008t+1\u3009)\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) +\ny\u0304\n2(1\u2212 p\u0302\u3008t+1\u3009) \u2016\n\u2264 \u2016 q\u0302 \u3008t+1\u3009(1\u2212 2p\u0302\u3008t+1\u3009)\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2016+ \u2016 y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) \u2016\n\u2264 \u2223\u2223\u2223\u2223\u2223 (1\u2212 2p\u0302\u3008t+1\u3009)2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2223\u2223\u2223\u2223\u2223 \u2016q\u0302\u3008t+1\u3009 \u2212 \u03b3\u0304\u3008t+1\u3009\u2016+ \u2016 \u03b3\u0304\u3008t+1\u3009(1\u2212 2p\u0304\u3008t+1\u3009)2p\u0304\u3008t+1\u3009(1\u2212 p\u0304\u3008t+1\u3009) \u2016+ \u2016 y\u03042(1\u2212 p\u0302\u3008t+1\u3009)\u2016\n+ \u2225\u2225\u2225\u2225\u2225\u2225 b\u0304 \u3008t+1\u3009 ((p\u0304\u3008t+1\u3009)2 + (1\u2212 p\u0304\u3008t+1\u3009)2 \u2212 (1\u2212 2p\u0304\u3008t+1\u3009)|p\u0302\u3008t+1\u3009 \u2212 p\u0304\u3008t+1\u3009|) p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2225\u2225\u2225\u2225\u2225\u2225 |p\u0302\u3008t+1\u3009 \u2212 p\u0304\u3008t+1\u3009| \u2264 \u2223\u2223\u2223\u2223\u2223 12p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2223\u2223\u2223\u2223\u2223 \u2016q\u0302\u3008t+1\u3009 \u2212 \u03b3\u0304\u3008t+1\u3009\u2016+ \u2225\u2225\u2225\u2225\u2225\u2225 3b\u0304 \u3008t+1\u3009 p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009)\n\u2225\u2225\u2225\u2225\u2225\u2225 |p\u0302\u3008t+1\u3009 \u2212 p\u0304\u3008t+1\u3009| +\u2016a\u0304\u3008t+1\u3009\u2016+ \u2016 y\u0304\n2(1\u2212 p\u0302\u3008t+1\u3009) \u2016. (104)\nFurthermore, from (88) we have\n\u2016b\u0302\u3008t+1\u3009 \u2212 b\u0304\u3008t+1\u3009\u2016 = \u2016 q\u0302 \u3008t+1\u3009\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2212 y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) \u2212 \u03b3\u0304\n\u3008t+1\u3009\n2p\u0304\u3008t+1\u3009(1\u2212 p\u0304\u3008t+1\u3009) \u2016\n\u2264 \u2016 q\u0302 \u3008t+1\u3009\n2p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2212 \u03b3\u0304\n\u3008t+1\u3009\n2p\u0304\u3008t+1\u3009(1\u2212 p\u0304\u3008t+1\u3009) \u2016+ \u2016 y\u0304 2(1\u2212 p\u0302\u3008t+1\u3009) \u2016\n\u2264 \u2223\u2223\u2223\u2223\u2223 12p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2223\u2223\u2223\u2223\u2223 \u2016q\u0302\u3008t+1\u3009 \u2212 \u03b3\u0304\u3008t+1\u3009\u2016+ \u2016 y\u03042(1\u2212 p\u0302\u3008t+1\u3009)\u2016\n+ \u2225\u2225\u2225\u2225\u2225\u2225 b\u0304 \u3008t+1\u3009 (1\u2212 2p\u0304\u3008t+1\u3009 + |p\u0302\u3008t+1\u3009 \u2212 p\u0304\u3008t+1\u3009|) p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2225\u2225\u2225\u2225\u2225\u2225 |p\u0302\u3008t+1\u3009 \u2212 p\u0304\u3008t+1\u3009| \u2264 \u2223\u2223\u2223\u2223\u2223 12p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009) \u2223\u2223\u2223\u2223\u2223 \u2016q\u0302\u3008t+1\u3009 \u2212 \u03b3\u0304\u3008t+1\u3009\u2016+ \u2225\u2225\u2225\u2225\u2225\u2225 3b\u0304 \u3008t+1\u3009 p\u0302\u3008t+1\u3009(1\u2212 p\u0302\u3008t+1\u3009)\n\u2225\u2225\u2225\u2225\u2225\u2225 |p\u0302\u3008t+1\u3009 \u2212 p\u0304\u3008t+1\u3009|+ \u2016 y\u03042(1\u2212 p\u0302\u3008t+1\u3009)\u2016. (105)\nSuppose for the moment that \u2016a\u0302\u3008t\u3009\u2016 \u2208 [0, 1] and \u2016b\u0302\u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 12\u2016\u03b8 ?\u2016. It is straightforward to use (100) and (98) and the definition of \u03c1 in the statement of Lemma 21 to prove\np\u0302\u3008t+1\u3009 \u2208 (\u03c1 2 , 1\u2212 \u03c1 2 ). (106)\nBy combining (99)-(101), (104)), (105), and (106) we obtain\n\u2016a\u0302\u3008t+1\u3009\u2016 \u2264 \u2016a\u0304\u3008t+1\u3009\u2016+ 9C\u03b8 + c\u03b8 \u03c1(2\u2212 \u03c1) + 12C\u03b8 \u03c1(2\u2212 \u03c1) + c\u03b8 2\u2212 \u03c1 = \u2016a\u0304\u3008t+1\u3009\u2016+ a,\n\u2016b\u0302\u3008t+1\u3009 \u2212 b\u0304\u3008t+1\u3009\u2016 \u2264 9C\u03b8 + c\u03b8 \u03c1(2\u2212 \u03c1) + 12C\u03b8 \u03c1(2\u2212 \u03c1) + c\u03b8 2\u2212 \u03c1 = b, (107)\nand hence \u2016b\u0302\u3008t+1\u3009 \u2212 \u03b8?\u2016 \u2264 \u2016b\u0304\u3008t+1\u3009 \u2212 \u03b8?\u2016+ b. Now suppose that the assumptions of Lemma 22 hold, i.e., \u2016a\u0302\u3008t\u3009\u2016 \u2208 [0, \u03b4\u2032a] and \u2016b\u0302\n\u3008t\u3009 \u2212 \u03b8?\u2016 \u2264\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016. Then (107) implies that\n\u2016a\u0302\u3008t+1\u3009\u2016 \u2264 \u2016a\u0304\u3008t+1\u3009\u2016+ a \u2264 \u03baa\u2016a\u0302\u3008t\u3009\u2016+ a,\n\u2016b\u0302\u3008t+1\u3009 \u2212 \u03b8?\u2016 \u2264 \u2016b\u0304\u3008t+1\u3009 \u2212 \u03b8?\u2016+ b \u2264 \u03bab\u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u3009\u2016+ b. (108)\nNote that (108) is the result we claimed in Lemma 21. However, to obtain (103), which is one of the main steps in deriving (108) we have assumed that\n\u2016a\u0302\u3008t\u3009\u2016 \u2208 [0, \u03b4\u2032a] and \u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 \u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016.\nIn order to prove the above equation holds for every t, we will prove an even stronger statement:\n\u2016a\u0302\u3008t\u3009\u2016 \u2208 [0, \u03b4a] and \u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 \u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016, (109)\nwhere \u03b4a = min{\u03b4\u2032a, (1\u2212\u03bab)2(1\u2212(\u03baa)2)\u2016\u03b8?\u20162 4cb }. We use induction to prove that (109) holds \u2200t \u2265 0. By the assumptions of this Lemma, the initial estimates (a\u0302\u30080\u3009, b\u0302 \u30080\u3009 ) satisfy (109). Hence the base of the\ninduction is true. Suppose (109) holds for t \u2265 0, then for t+ 1 (108) holds. Hence all we need to prove is that\n\u03baa\u2016a\u0302\u3008t\u3009\u2016+ a \u2264 \u03b4a,\nand \u03bab\u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u3009\u2016+ b \u2264 \u221a 1\u2212 (\u03baa)2}\u2016\u03b8?\u2016. (110)\nFor the first inequality, since the condition on n in (98) ensure that a \u2264 (1\u2212 \u03baa)\u03b4a, together with induction assumption that \u2016a\u0302\u3008t\u3009\u2016 \u2264 \u03b4a, we have\n\u2016a\u0302\u3008t+1\u3009\u2016 \u2264 \u03baa\u2016a\u0302\u3008t\u3009\u2016+ a \u2264 \u03baa\u03b4a + (1\u2212 \u03baa)\u03b4a \u2264 \u03b4a.\nTo prove (110) note that the condition on n ensure that\nb \u2264 1\n2 (1\u2212 \u03bab)\n\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016.\nAlso the condition on \u03b4a and \u2016a\u0302\u3008t\u3009\u2016 \u2264 \u03b4a ensure that\u221a cb\u2016a\u0302\u3008t\u3009\u2016 \u2264 1\n2 (1\u2212 \u03bab)\n\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016.\nHence with induction assumption that \u2016b\u0302\u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 \u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016, we have\n\u2016b\u0302\u3008t+1\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u0302 \u3008t\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u3009\u2016+ b\n\u2264 \u03bab \u221a 1\u2212 (\u03baa)2\u2016\u03b8\u2016+ 1\n2 (1\u2212 \u03bab)\n\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016+ 1\n2 (1\u2212 \u03bab)\n\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016\n= \u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016.\nHence the second part of (109) holds for t+ 1. This completes the proof.\nA.10.2 Proof of Lemma 20\nWe first prove (95) for \u2016a\u0302\u3008t\u3009\u2016. Clearly the result holds for t = 0. For all t \u2265 1, using the condition (93) on \u2016a\u0302\u3008t\u2032\u3009\u2016 for all t\u2032 \u2264 t, we have\n\u2016a\u0302\u3008t\u3009\u2016 \u2264 \u03baa\u2016a\u0302\u3008t\u22121\u3009\u2016+ a \u2264 \u03baa(\u03baa\u2016a\u0302\u3008t\u22122\u3009\u2016+ a) + a\n\u2264 (\u03baa)t\u2016a\u0302\u30080\u3009\u2016+ a t\u22121\u2211 i=0 (\u03baa) i \u2264 (\u03baa)t\u2016a\u0302\u30080\u3009\u2016+ 1\n1\u2212 \u03baa a.\nHence (95) holds. Next, we prove (96) for \u2016b\u0302\u3008t\u3009\u2016. Clearly the result holds for t = 0. For all t \u2265 1, using the condition (94) on \u2016b\u0302\u3008t \u2032\u3009\u2016 for all t\u2032 \u2264 t, we have\n\u2016b\u0302\u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u0302 \u3008t\u22121\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u22121\u3009\u2016+ b\n\u2264 \u03bab(\u03bab\u2016b\u0302 \u3008t\u22122\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u3008t\u22122\u3009\u2016+ b) + b\n\u2264 (\u03bab)t\u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb t\u22121\u2211 i=0 (\u03bab) t\u22121\u2212i \u221a \u2016a\u0302\u3008i\u3009\u2016+ b t\u22121\u2211 i=0 (\u03bab) i\n\u2264 (\u03bab)t\u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb t\u22121\u2211 i=0 (\u03bab) t\u22121\u2212i \u221a \u2016a\u0302\u3008i\u3009\u2016+ 1 1\u2212 \u03bab b.\nFrom (95), we have \u2200t \u2265 0,\u221a \u2016a\u0302\u3008t\u3009\u2016 \u2264 \u221a (\u03baa)t\u2016a\u0302\u30080\u3009\u2016+\n1\n1\u2212 \u03baa a \u2264 (\u03baa)\nt 2 \u221a \u2016a\u0302\u30080\u3009\u2016+ \u221a 1\n1\u2212 \u03baa a.\nHence we have\n\u2016b\u0302\u3008t\u3009 \u2212 \u03b8?\u2016 \u2264 (\u03bab)t\u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb t\u22121\u2211 i=0 (\u03bab) t\u22121\u2212i \u221a \u2016a\u0302\u3008i\u3009\u2016+ 1 1\u2212 \u03bab b\n\u2264 (\u03bab)t\u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb t\u22121\u2211 i=0 (\u03bab) t\u22121\u2212i(( \u221a \u03baa) i \u221a \u2016a\u0302\u30080\u3009\u2016+ \u221a 1 1\u2212 \u03baa a) +\n1\n1\u2212 \u03bab b\n= (\u03bab) t\u2016b\u0302\u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u0302\u30080\u3009\u2016 t\u22121\u2211 i=0 (\u03bab) t\u22121\u2212i( \u221a \u03baa) i + \u221a cb 1\u2212 \u03baa a t\u22121\u2211 i=0 (\u03bab) i +\n1\n1\u2212 \u03bab b\n\u2264 (\u03bab)t\u2016b\u0302 \u30080\u3009 \u2212 \u03b8?\u2016+ t \u221a cb\u2016a\u0302\u30080\u3009\u2016(max{ \u221a \u03baa, \u03bab})t +\n1\n1\u2212 \u03bab\n\u221a cb\n1\u2212 \u03baa a +\n1\n1\u2212 \u03bab b.\nThis completes the proof of this lemma.\nA.10.3 Proof of (99)-(101)\nLemma 23. Let y1, \u00b7 \u00b7 \u00b7 , yn i.i.d.\u223c 12N(\u03b8 ?, Id) + 1 2N(\u2212\u03b8 ?, Id). Then, we have\n(1) \u2016 1n \u2211n i=1 yi\u2016 \u2264 4(\u2016\u03b8 ?\u2016+ 1) \u221a 2d+ln(1/\u03b4) n , with probability at least 1\u2212 \u03b4.\n(2) sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n \u2211n i=1 wd(yi\u2212xa,xb)\u2212Ewd(Y \u2212xa,xb)| \u2264 8c(\u2016\u03b8 ?\u2016+ 2) \u221a d+2+ln(1/\u03b4) n , with\nprobability at least 1\u2212 \u03b4.\n(3) sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 \u2016 1 n \u2211n i=1(wd(yi \u2212 xa,xb) \u2212 1 2)yi \u2212 E(wd(Y \u2212 xa,xb) \u2212 1 2)Y \u2016 \u2264 36c(\u2016\u03b8 ?\u2016 +\n2)\n\u221a d+2+ln(1/\u03b4)\nn , with probability at least 1\u2212 \u03b4.\nProof. We first prove the first claim (1). Note that yi can be expressed by yi = \u03b6i\u03b8 ? + \u03c9i, where \u03b6i are i.i.d sequence of Rademacher variables and \u03c9i are i.i.d N(0, Id) Gaussian random variables.\nTherefore we have,\n\u2016 1 n n\u2211 i=1 yi\u20162 = \u2016 1 n n\u2211 i=1 \u03b6i\u03b8 ? + 1 n n\u2211 i=1 \u03c9i\u20162\n= 1 n \u2016 1\u221a n n\u2211 i=1 \u03b6i\u03b8 ? + 1\u221a n n\u2211 i=1 \u03c9i\u20162.\nNote that \u2016 1\u221a n \u2211n i=1\u03c9i\u20162 dist. = \u03bd, where \u03bd \u223c \u03c72(d). Hence, using Cram\u00e9r-Chernoff inequality, we have probability at least 1\u2212 \u03b42 such that\u2223\u2223\u2223\u2223\u2223\u2223\u2016 1\u221an n\u2211 i=1 \u03c9i\u20162 \u2212 d\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2264\u221a8d ln(2/\u03b4) \u2264 d+ 2 ln(2/\u03b4) for sufficiently large n. Moreover, for Rademacher variables \u03b6i, using Hoeffding\u2019s inequality, we have with probability at least 1\u2212 \u03b42 such that\n| 1\u221a n n\u2211 i=1 \u03b6i| \u2264 \u221a 2 ln(2/\u03b4)\nTherefore, we have probability at least 1\u2212 \u03b4 such that\n\u2016 1 n n\u2211 i=1 yi\u2016 \u2264 1\u221a n \u2016 1\u221a n n\u2211 i=1 \u03b6i\u03b8 ? + 1\u221a n n\u2211 i=1 \u03c9i\u2016\n\u2264 1\u221a n \u221a\u221a\u221a\u221a2\u2016 1\u221a n n\u2211 i=1 \u03b6i\u03b8 ?\u20162 + 2\u2016 1\u221a n n\u2211 i=1 \u03c9i\u20162\n\u2264 1\u221a n\n\u221a 2(2 ln(2/\u03b4))\u2016\u03b8?\u20162 + 2(2d+ 2 ln 2/\u03b4)\n= 2\n\u221a ln(2/\u03b4)(\u2016\u03b8?\u20162 + 1) + d\nn \u2264 4(\u2016\u03b8?\u2016+ 1) \u221a 2d+ ln(1/\u03b4)\nn .\nFor the second claim, define\nZ+ , sup \u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n1\nn n\u2211 i=1 wd(yi \u2212 xa,xb)\u2212 Ewd(Y \u2212 xa,xb).\nThen we have \u2200\u2016xb\u2016 \u2264 c, \u2016xa\u2016 \u2264 1\nEe\u03bbZ+ (i)\n\u2264 EY ,Y \u2032e \u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n1 n \u2211n i=1(wd(yi\u2212xa,xb)\u2212wd(y\u2032i\u2212xa,xb))\n= EY ,Y \u2032,\u03bee \u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n1 n \u2211n i=1 \u03bei(wd(yi\u2212xa,xb)\u2212wd(y\u2032i\u2212xa,xb))\n\u2264 EY ,Y \u2032,\u03bee \u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n| 1 n \u2211n i=1 \u03bei(wd(yi\u2212xa,xb)\u2212wd(y\u2032i\u2212xa,xb))|\n\u2264 E\u03be{EY ( e\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n \u2211n i=1 \u03bei(wd(yi\u2212xa,xb)\u2212 1 2 )| )\n\u00d7EY \u2032 ( e\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n \u2211n i=1 \u03bei(wd(y \u2032 i\u2212xa,xb)\u2212 1 2 )| ) }\n\u2264 EY ,\u03bee2\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n\n\u2211n i=1 \u03bei(wd(yi\u2212xa,xb)\u2212 1 2\n)|,\nNote that to obtain Inequality (i) we have used Jensen\u2019s inequality. Also, \u03bei are i.i.d sequence of Rademacher variables. To simplify the final expression even further, we use the following lemma from Koltchinskii (2011)\nLemma 24. Let H \u2208 Rn and let \u03c8i : R 7\u2192 R, i = 1, \u00b7 \u00b7 \u00b7 , n be functions such that \u03c8i(0) = 0 and\n|\u03c8i(u)\u2212 \u03c8i(v)| \u2264 |u\u2212 v| \u2208 R.\nFor all convex nondecreasing functions \u03a8 : R+ 7\u2192 R+,\nE\u03a8( 1 2 sup h\u2208H | n\u2211 i=1 \u03c8i(hi) i|) \u2264 E\u03a8(sup h\u2208H | n\u2211 i=1 hi i|),\nwhere i are i.i.d. Rademacher random variables.\nSince wd(y \u2212 xa,xb) is a function of \u3008y \u2212 xa,xb\u3009 and\n|wd(y \u2212 xa,xb)\u2212 wd(y \u2212 x\u2032a,x\u2032b)| \u2264 1 2 |\u3008y \u2212 xa,xb\u3009 \u2212 \u3008y \u2212 x\u2032a,x\u2032b\u3009|,\nletting \u03a8(x) = e2\u03bbx and \u03c8i(x) = 2e x\nex+e\u2212x \u2212 1 with hi = \u3008yi \u2212 xa,xb\u3009 in Lemma 24, we have\nEe\u03bbZ+ \u2264 EY ,\u03bee2\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n\n\u2211n i=1 \u03bei(wd(yi,xa,xb)\u2212 1 2 )|\n\u2264 EY,\u03bee2\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n\n\u2211n i=1 \u03bei\u3008yi\u2212xa,xb\u3009|\n(ii) = EY ,\u03bee2\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 1 n\n\u2211n i=1 \u03bei\u3008yi\u2212xa,xb\u3009\n\u2264 EY ,\u03bee2\u03bb sup\u2016xb\u2016\u2264c 1 n \u2211n i=1 \u03bei\u3008yi,xb\u3009e2\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\u3008xa,xb\u3009 1 n \u2211n i=1 \u03bei \u2264 EY ,\u03bee2\u03bbc\u2016 1 n \u2211n i=1 \u03beiyi\u2016e2\u03bbc| 1 n \u2211n i=1 \u03bei| \u2264 (EY ,\u03bee4\u03bbc\u2016 1 n \u2211n i=1 \u03beiyi\u2016)1/2(E\u03bee4\u03bbc| 1 n \u2211n i=1 \u03bei|)1/2 \u2264 (EY e4\u03bbc\u2016 1 n \u2211n i=1 yi\u2016\ufe38 \ufe37\ufe37 \ufe38\npart 1\n)1/2(E\u03bee4\u03bbc| 1 n \u2211n i=1 \u03bei|\ufe38 \ufe37\ufe37 \ufe38\npart 2\n)1/2,\nwhere last equality holds for the fact that the distribution of yi is symmetric and equality (ii) holds for the fact that 1n \u2211n i=1 \u03bei\u3008yi \u2212 xa,xb\u3009 is symmetric in terms of xb and the constraints on xb is symmetric. For part 1, we use the notation {uj , j = 1, \u00b7 \u00b7 \u00b7 ,M} for a 1/2-covering of the d-dimensional sphere, Spd , {v \u2208 Rd, \u2016v\u2016 = 1}. Note that, for all v\u2032,v \u2208 Spd,\n| 1 n n\u2211 i=1 \u3008yi,v\u2032\u3009 \u2212 1 n n\u2211 i=1 \u3008yi,v\u3009| \u2264 \u2016v\u2032 \u2212 v\u2016 sup \u2016u\u2016=1 1 n n\u2211 i=1 \u3008yi,u\u3009,\ntherefore, we have for all u \u2208 Spd\n1\nn n\u2211 i=1 \u3008yi,u\u3009 \u2264 max j\u2208[M ] { 1 n n\u2211 i=1 \u3008yi,uj\u3009}+ \u2016uj \u2212 u\u2016 sup \u2016u\u2016=1 1 n n\u2211 i=1 \u3008yi,u\u3009,\nand hence\n1 n \u2016 n\u2211 i=1 yi\u2016 = sup \u2016u\u2016=1 1 n n\u2211 i=1 \u3008yi,u\u3009 \u2264 2 max j\u2208[M ] { 1 n n\u2211 i=1 \u3008yi,uj\u3009}. (111)\nrecall that yi = \u03b6i\u03b8 ? + \u03c9i. Hence, we have\nEY e\u3008yi,uj\u3009 = E\u03b6e\u03b6\u3008\u03b8 ?,uj\u3009E\u03c9e\u3008\u03c9i,uj\u3009\n= 1\n2 (e\u3008\u03b8 ?,uj\u3009 + e\u2212\u3008\u03b8 ?,uj\u3009)e 1 2 \u2264 e\n\u2016\u03b8?\u20162+1 2 , (112)\nwhere last inequality holds because of\n1 2 (e\u2016\u03b8 ?\u2016 + e\u2212\u2016\u03b8 ?\u2016) \u2264 e \u2016\u03b8?\u20162 2 .\nTherefore we have\nEY ,\u03bee4\u03bbc\u2016 1 n \u2211n i=1 yi\u2016 = EY ,\u03bee4\u03bbc sup\u2016u\u2016=1 1 n \u2211n i=1\u3008yi,u\u3009\n\u2264 EY ,\u03bee8\u03bbcmaxj\u2208[M ] 1 n\n\u2211n i=1\u3008yi,uj\u3009\n\u2264 M\u2211 j=1 EY ,\u03bee8\u03bbc 1 n \u2211n i=1\u3008yi,uj\u3009 \u2264 e32\u03bb2c2 \u2016\u03b8?\u20162+1 n +2d. (113)\nFor part 2, notice that 1n \u2211n i=1 \u03bei is symmetric, we have\nE\u03bee4\u03bbc| 1 n \u2211n i=1 \u03bei| \u2264 2E\u03bee4\u03bbc 1 n \u2211n i=1 \u03bei\n\u2264 2(E\u03bee 4\u03bbc n \u03be)n \u2264 e 8\u03bb2c2 n +1. (114)\nTherefore combining (113) and (114), we have\nEe\u03bbZ+ \u2264 e16\u03bb2c2 \u2016\u03b8?\u20162+1 n +d \u00d7 e 4\u03bb2c2 n + 1 2\n\u2264 e16\u03bb2c2 \u2016\u03b8?\u20162+2 n +d+ 1 2 .\nUsing Markov Inequality: P (Z+ > ) \u2264 Ee\u03bbZ+\u2212\u03bb ,\u2200 , \u03bb > 0,\nchoosing \u03bb = n 32c2(\u2016\u03b8?\u20162+2) , we have\nP (Z+ > ) \u2264 e 16c2\u03bb2(\u2016\u03b8?\u20162+2) n +d+ 1 2 \u2212\u03bb\n= e \u2212 n\n2\n64c2(\u2016\u03b8?\u20162+2) +d+ 1\n2 .\nTherefore\n| sup \u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n1\nn n\u2211 i=1 wd(yi \u2212 xa,xb)\u2212 Ewd(Y \u2212 xa,xb)| \u2264 8c(\u2016\u03b8?\u2016+ 2) \u221a d+ 2 + ln(1/\u03b4) n ,\nwith probability at least 1\u2212 \u03b4. For the last claim, we borrow a technique in the proof of corollary 2 in B.2 in Balakrishnan et al. (2014). Let\nZ = sup \u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n\u2016 1 n n\u2211 i=1 (wd(yi \u2212 xa,xb)\u2212 1 2 )yi \u2212 E(wd(Y \u2212 xa,xb)\u2212 1 2 )Y \u2016,\nand\nZu = sup \u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n| 1 n n\u2211 i=1 (wd(yi \u2212 xa,xb)\u2212 1 2 ))\u3008yi,u\u3009 \u2212 E(wd(Y \u2212 xa,xb)\u2212 1 2 )\u3008Y ,u\u3009|\nwe have\nEe\u03bbZ = EY e\u03bb sup\u2016u\u2016=1 Zu \u2264 Ee2\u03bbmaxj\u2208[M ] Zuj \u2264 M\u2211 j=1 Ee2\u03bbZuj\n\u2264 M\u2211 j=1 EY ,\u03bee4\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 1 n \u2211n i=1 \u03bei(wd(yi\u2212xa,xb)\u2212 1 2 )\u3008yi,uj\u3009,\nwhere \u03bei are i.i.d. sequence of Rademacher variables and the last inequality holds for standard symmetrization result for empirical process. Since\n|(2wd(yi\u2212xa,xb)\u22121)\u3008yi,uj\u3009\u2212(2wd(yi\u2212x\u2032a,x\u2032b)\u22121)\u3008yi,uj\u3009| \u2264 |\u3008yi\u2212xa,xb\u3009\u2212\u3008yi\u2212x\u2032a,x\u2032b\u3009|\u3008yi,uj\u3009,\nlet \u03a8(x) = e2\u03bbx and \u03c8i(x) = ( 2e x\nex\u2212e\u2212x \u2212 1)\u3008yi,uj\u3009 with hi = \u3008yi \u2212 xa,xb\u3009 in Lemma 24, we have\nEe\u03bbZ \u2264 M\u2211 j=1 EY ,\u03bee4\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 | 1 n \u2211n i=1 \u03bei\u3008yi\u2212xa,xb\u3009\u3008yi,uj\u3009|\niii = M\u2211 j=1 EY ,\u03bee4\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 1 n \u2211n i=1 \u03bei\u3008yi\u2212xa,xb\u3009\u3008yi,uj\u3009 \u2264 M\u2211 j=1 EY ,\u03bee4\u03bb sup\u2016xb\u2016\u2264c 1 n \u2211n i=1 \u03bei\u3008yi,xb\u3009\u3008yi,uj\u3009e4\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 1 n \u2211n i=1 \u03bei\u3008xa,xb\u3009\u3008yi,uj\u3009 \u2264 M\u2211 j=1 (EY ,\u03bee8\u03bb sup\u2016xb\u2016\u2264c 1 n \u2211n i=1 \u03bei\u3008yi,xb\u3009\u3008yi,uj\u3009) 1 2 (EY ,\u03bee8\u03bb sup\u2016xb\u2016\u2264c,\u2016xa\u2016\u22641 1 n \u2211n i=1 \u03bei\u3008xa,xb\u3009\u3008yi,uj\u3009) 1 2 \u2264 M\u2211 j=1 (EY ,\u03bee8\u03bbc\u2016 1 n \u2211n i=1 \u03beiyiy > i \u2016op\ufe38 \ufe37\ufe37 \ufe38\npart1\n) 1 2 (EY ,\u03bee8\u03bbc 1 n | \u2211n i=1 \u03bei\u3008yi,uj\u3009|\ufe38 \ufe37\ufe37 \ufe38\npart2\n) 1 2 ,\nwhere \u2016 \u00b7 \u2016op is l2-operator norm of a matrix(maximum singular value), equality (iii) holds for the fact that 1n \u2211n i=1 \u03bei\u3008yi \u2212 xa,xb\u3009\u3008yi,uj\u3009 is symmetric in terms of xb and constraints of xb is symmetric. The correctness of the last inequality is shown in B.2 of Balakrishnan et al. (2014). For part 1, as shown in B.2 of Balakrishnan et al. (2014) we have\nEY ,\u03bee8\u03bbc\u2016 1 n\n\u2211n i=1 \u03beiyiy\n> i \u2016op \u2264 EY ,\u03bee16\u03bbcmaxj\u2032\u2208[M ] 1 n\n\u2211n i=1 \u03bei\u3008yi,uj\u2032 \u30092\n(115)\nRecall that yi = \u03b6i\u03b8 ? + \u03c9i and (112), we have\nEY e\u3008yi,uj\u3009 = E\u03b6e\u03b6\u3008\u03b8 ?,uj\u3009E\u03c9e\u3008\u03c9i,uj\u3009 \u2264 e \u2016\u03b8?\u20162+1 2 .\nTherefore Ee\u03bb\u03be\u3008yi,uj\u3009 2 \u2264 e (\u2016\u03b8?\u20162+1)\u03bb2 2 , for small enough \u03bb.\nTherefore\nEY ,\u03bee16\u03bbcmaxj\u2032\u2208[M ] 1 n \u2211n i=1 \u03bei\u3008yi,uj\u2032 \u30092 \u2264 M\u2211 j\u2032=1 EY ,\u03bee16\u03bbc 1 n \u2211n i=1 \u03bei\u3008yi,uj\u2032 \u30092\n\u2264 e (\u2016\u03b8?\u20162+1)(16c\u03bb)2 2n +2d.\nFor part 2, since \u03bei\u3008yi,uj\u3009 dist. = \u3008yi,uj\u3009, using (112), we have\nEY ,\u03bee8\u03bbc 1 n | \u2211n i=1 \u03bei\u3008yi,uj\u3009| = EY e8\u03bbc 1 n | \u2211n i=1\u3008yi,uj\u3009|\niv \u2264 2EY e8\u03bbc 1 n\n\u2211n i=1\u3008yi,uj\u3009\n\u2264 e (\u2016\u03b8?\u20162+1)(8c\u03bb)2 2n +1,\nwhere inequality (iv) holds for the fact that the distribution of 1n \u2211n\ni=1\u3008yi,uj\u3009 is symmetric. Therefore, combining part 1 and part 2, we have\nEe\u03bbZ \u2264 M\u2211 j=1 e (\u2016\u03b8?\u20162+1)(16c\u03bb)2 4 +de (\u2016\u03b8?\u20162+1)(8c\u03bb)2 4 + 1 2\n\u2264 e 81(\u2016\u03b8?\u20162+1)c2\u03bb2 n +3d+ 1 2 .\nUsing Markov Inequality: P (Z > ) \u2264 EY e\u03bbZ\u2212\u03bb ,\u2200 , \u03bb > 0,\nchoosing \u03bb = n 32c2(\u2016\u03b8?\u20162+2) , we have\nP (Z > ) \u2264 e 81c2\u03bb2(\u2016\u03b8?\u20162+1) n +3d+ 1 2 \u2212\u03bb\n= e \u2212 n\n2\n324c2(\u2016\u03b8?\u20162+1) +3d+ 1\n2 .\nTherefore\nsup \u2016xb\u2016\u2264c,\u2016xa\u2016\u22641\n\u2016 1 n n\u2211 i=1 (wd(yi \u2212 xa,xb)\u2212 1 2 )yi \u2212 Ewd(Y \u2212 xa,xb)Y \u2016 \u2264 36c(\u2016\u03b8?\u2016+ 2)\n\u221a d+ 2 + ln(1/\u03b4)\nn ,\nwith probability at least 1\u2212 \u03b4.\nA.10.4 Proof of Lemma 22 Since a\u0304\u3008t+1\u3009 and b\u0304\u3008t+1\u3009 are the result of first iteration based on initialization (a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009\n) in Population EM model where initialization (a\u0302\u3008t\u3009, b\u0302 \u3008t\u3009 ) satisfying the corresponding condition mentioned in the lemma. Hence to prove the lemma holds for all t \u2265 0, it is sufficient to prove that for any initialization (a\u30080\u3009, b\u30080\u3009) satisfying the same condition, we have \u2016a\u30081\u3009\u2016 \u2264 \u03baa\u2016a\u30080\u3009\u2016 for (102) and \u2016b\u30081\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u30080\u3009\u2016 for (103). To achieve this goal, we use the notations and definition that are summarized in Appendix A.4. We first prove the first claim:\n\u2016a\u30081\u3009\u2016 \u2264 \u03baa\u2016a\u30080\u3009\u2016. (116)\nIf a\u30080\u3009 = 0, we immediately have (116) holds. If a\u30080\u3009 6= 0, because of Lemma 4 and Lemma 5, we assume \u3008a\u30080\u3009, b\u30080\u3009\u3009 > 0 without loss of generality, thus a\u0303\u30080\u30091 > 0. Since b\u0303 \u30081\u3009 (1 \u2212 2p\u30081\u3009) = a\u0303\u30081\u3009, we\nknow they are in the same direction, thus the angle between a\u30081\u3009 and \u03b8? is the same angle between b\u30081\u3009 and \u03b8?, i.e., \u03b2\u30081\u3009. Furthermore, according to Lemma 8, we have \u03b2\u30081\u3009 \u2264 \u03b2\u30080\u3009. Hence we have\n\u2016a\u30081\u3009\u2016 = a\u0303 \u30081\u3009 1\ncos\u03b2\u30081\u3009 \u2264 a\u0303\n\u30081\u3009 1\ncos\u03b2\u30080\u3009 . (117)\nTherefore, we need to bound a\u0303\u30081\u30091 and 1 cos\u03b2\u30080\u3009 . According to (39) and Lemma 11 we have,\na\u0303 \u30081\u3009 1 =\n\u0393(a\u0303 \u30080\u3009 1 , \u2016b \u30080\u3009\u2016, \u03b8?\u30080\u3009,1)(1\u2212 2P (a\u0303 \u30080\u3009 1 , \u2016b \u30080\u3009\u2016, \u03b8?\u30080\u3009,1)) P (a\u0303 \u30080\u3009 1 , \u2016b \u30080\u3009\u2016, \u03b8?\u30080\u3009,1)(1\u2212 P (a\u0303 \u30080\u3009 1 , \u2016b \u30080\u3009\u2016, \u03b8?\u30080\u3009,1)) \u2264 \u03ba\u2032aa\u0303 \u30080\u3009 1 \u2264 \u03ba \u2032 a\u2016a\u30080\u3009\u2016,\nwhere \u03ba\u2032a \u2208 (0, 1) is a continuous function of \u03b8?\u30080\u3009,1 > 0. Since the condition of \u2016b \u30080\u3009 \u2212 \u03b8?\u2016 \u2264 12\u2016\u03b8 ?\u2016 implies that \u03b8?\u30080\u3009,1 \u2265 \u221a 3 2 \u2016\u03b8 ?\u2016 > 0, we have\n\u03baa , sup \u03b8?\u30080\u3009,1\u2208[ \u221a 3 2 \u2016\u03b8?\u2016,\u2016\u03b8?\u2016]\n\u221a \u03ba\u2032a(\u03b8 ? \u30080\u3009,1) \u2208 (0, 1),\nand \u03baa only depends on \u03b8?. Now for 1cos\u03b2\u30080\u3009 , by the condition of \u2016b \u30080\u3009 \u2212 \u03b8?\u2016 \u2264\n\u221a 1\u2212 (\u03baa)2\u2016\u03b8?\u2016, we\nhave cos\u03b2\u30080\u3009 \u2265 \u03baa. Hence, combining the two parts in (117), we have\n\u2016a\u30081\u3009\u2016 \u2264 a\u0303 \u30081\u3009 1 cos\u03b2\u30080\u3009 \u2264 \u03ba\u2032a(\u03b8 ? \u30080\u3009,1) \u03baa \u2016a\u30080\u3009\u2016 \u2264 \u03baa\u2016a\u30080\u3009\u2016.\nHence (116) holds. Next we prove the second claim: \u2016b\u30081\u3009 \u2212 \u03b8?\u2016 \u2264 \u03bab\u2016b\u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u30080\u3009\u2016.\nAccording to Lemma 17 we can conclude there exists \u03b4\u2032a \u2208 (0, 1), \u03bab \u2208 (0, 1) and cb > 0 such that that if \u2016a\u30080\u3009\u2016 \u2264 \u03b4\u2032a, then\n\u2016b\u30081\u3009 \u2212 \u03b8?\u2016 \u2264 \u221a \u03ba2b\u2016b \u30080\u3009 \u2212 \u03b8?\u20162 + cb\u2016a\u30080\u3009\u2016 \u2264 \u03bab\u2016b\u30080\u3009 \u2212 \u03b8?\u2016+ \u221a cb\u2016a\u30080\u3009\u2016,\nwhere \u03b4\u2032a, \u03bab and cb only depend on Ua = 1, Lb = 1 2\u2016\u03b8 ?\u2016, Ub = 32\u2016\u03b8 ?\u2016, L\u03b8 =\n\u221a 3\n2 \u2016\u03b8 ?\u2016 and \u2016\u03b8?\u2016. Hence\n\u03b4\u2032a, \u03bab and cb only depend on \u03b8 ?. This completes the proof.\nB Proofs of Auxiliary Results\nB.1 Proof of Lemma 4\nFrom (12) and (12), we know that\n\u3008b\u3008t+1\u3009,\u03b8?\u3009 = \u3008\u03b3 \u3008t+1\u3009,\u03b8?\u3009\np\u3008t+1\u3009(1\u2212 p\u3008t+1\u3009) ,\n\u3008a\u3008t+1\u3009, b\u3008t+1\u3009\u3009 = \u2016b\u3008t+1\u3009\u20162(1\u2212 2p\u3008t+1\u3009).\nSince p\u3008t+1\u3009 \u2208 (0, 1) and \u2016\u03b8?\u2016 > 0, we have\nsgn(\u3008b\u3008t+1\u3009,\u03b8?\u3009) = sgn(\u3008\u03b3\u3008t+1\u3009,\u03b8?\u3009) sgn(\u3008a\u3008t+1\u3009, b\u3008t+1\u3009\u3009) = sgn(1\u2212 2p\u3008t+1\u3009) sgn(\u2016b\u3008t+1\u3009\u20162)\n= sgn(1\u2212 2p\u3008t+1\u3009)| sgn(\u3008b\u3008t+1\u3009,\u03b8?\u3009)|.\nHence if we have\nsgn(\u3008\u03b3\u3008t+1\u3009,\u03b8?\u3009) = sgn(\u3008b\u3008t\u3009,\u03b8?\u3009), (118)\nand\nsgn(1\u2212 2p\u3008t+1\u3009) = sgn(\u3008a\u3008t\u3009, b\u3008t\u3009\u3009), (119)\nthen immediately, we have\nsgn(\u3008b\u3008t+1\u3009,\u03b8?\u3009) = sgn(\u3008b\u3008t\u3009,\u03b8?\u3009),\nand\nsgn(\u3008a\u3008t+1\u3009, b\u3008t+1\u3009\u3009) = sgn(1\u2212 2p\u3008t+1\u3009)| sgn(\u3008b\u3008t+1\u3009,\u03b8?\u3009)| = sgn(\u3008a\u3008t\u3009, b\u3008t\u3009\u3009)| sgn(\u3008b\u3008t\u3009,\u03b8?\u3009)| = sgn(\u3008a\u3008t\u3009, b\u3008t\u3009\u3009).\nHence our next goal is to prove (118) and (119). Consider the rotation matrix O for which \u03b8\u0303? , O\u03b8? has all its coordinates except the first one equal to zero, i.e., \u03b8\u0303? = (\u2016\u03b8?\u2016, 0, . . . , 0)>. Also, let b\u0303 \u3008t\u3009 , Ob\u3008t\u3009, and a\u0303\u3008t\u3009 , Oa\u3008t\u3009. According to Lemma 3, we have\n\u3008\u03b3\u0303\u3008t+1\u3009, \u03b8\u0303?\u3009 = \u3008\u03b8\u0303?,Ewd(Y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )Y \u3009 = \u2016\u03b8\u0303?\u2016 \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y1\u03c6 + d (y, \u03b8\u0303 ?) dy\n= \u2016\u03b8\u0303?\u2016 \u222b e\u2212 \u2211d i=2 y 2 i /2\n\u221a 2\u03c0 d\u22121 (\n\u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y1\u03c6 +(y1, \u2016\u03b8\u0303?\u2016) dy1) dy2 \u00b7 \u00b7 \u00b7 dyd\n(120)\nHence, define y2...d = (y2, \u00b7 \u00b7 \u00b7 , yd) and B(y2...d) , \u2211d i=2 yib \u3008t\u3009 i , then to prove (118), it is sufficient to prove\nsgn( \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y1\u03c6 +(y1, \u2016\u03b8\u0303?\u2016) dy1) = sgn(\u3008b\u3008t\u3009,\u03b8?\u3009) = sgn(\u2016\u03b8?\u2016b\u0303\u3008t\u30091 )\n= sgn(b\u0303 \u3008t\u3009 1 ), \u2200y2...d, B(y2...d).\nNote that\u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y1\u03c6 +(y1, \u2016\u03b8\u0303?\u2016) dy1\n= \u222b +\u221e 0 (wd((y1,y2...d) > \u2212 a\u0303\u3008t\u3009, b\u0303\u3008t\u3009)\u2212 wd((\u2212y1,y2...d)> \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 ))y1\u03c6 +(y1, \u2016\u03b8\u0303?\u2016) dy1\n= \u222b +\u221e 0\ne2y1b\u0303 \u3008t\u3009 1 \u2212 e\u22122y1b\u0303 \u3008t\u3009 1\ne2y1b\u0303 \u3008t\u3009 1 + e\u22122y1b\u0303 \u3008t\u3009 1 + e2B(y2...d)\u22122\u3008a\u0303 \u3008t\u3009,b\u0303 \u3008t\u3009\u3009 + e\u22122B(y2...d)+2\u3008a\u0303 \u3008t\u3009,b\u0303 \u3008t\u3009\u3009\ny1\u03c6 +(y1, \u2016\u03b8\u0303?\u2016) dy1.\nHence, we have\nsgn( \u222b wd(y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )y1\u03c6 +(y1, \u2016\u03b8\u0303?\u2016) dy1) = sgn(b\u0303\u3008t\u30091 ), \u2200y2...d, B(y2...d).\nTo prove (119), according to Lemma 3, we have\n2p\u3008t+1\u3009 \u2212 1 = E(2wd(Y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )\u2212 1)\n= E(wd(Y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 ) + wd(\u2212Y \u2212 a\u0303\u3008t\u3009, b\u0303 \u3008t\u3009 )\u2212 1)\n= E( e2\u3008Y ,b\u0303\n\u3008t\u3009\u3009 + e\u22122\u3008Y ,b\u0303 \u3008t\u3009\u3009 + 2e\u22122\u3008a\u0303 \u3008t\u3009,b\u0303 \u3008t\u3009\u3009\ne2\u3008Y ,b\u0303 \u3008t\u3009\u3009 + e\u22122\u3008Y ,b\u0303 \u3008t\u3009\u3009 + e2\u3008a\u0303 \u3008t\u3009,b\u0303 \u3008t\u3009\u3009 + e\u22122\u3008a\u0303 \u3008t\u3009,b\u0303\n\u3008t\u3009\u3009 \u2212 1)\n= E e\u22122\u3008a\u0303\n\u3008t\u3009,b\u0303 \u3008t\u3009\u3009 \u2212 e2\u3008a\u0303\u3008t\u3009,b\u0303 \u3008t\u3009\u3009\ne2\u3008Y ,b\u0303 \u3008t\u3009\u3009 + e\u22122\u3008Y ,b\u0303 \u3008t\u3009\u3009 + e2\u3008a\u0303 \u3008t\u3009,b\u0303 \u3008t\u3009\u3009 + e\u22122\u3008a\u0303 \u3008t\u3009,b\u0303\n\u3008t\u3009\u3009 .\nHence, we have\nsgn(1\u2212 2p\u3008t+1\u3009) = sgn(\u3008a\u0303\u3008t\u3009, b\u0303\u3008t\u3009\u3009).\nThis completes the proof of this lemma.\nB.2 Proof of Lemma 5\nProof. We use induction to prove for claim (i) and the proof for claim (ii) is similar. Clearly (i) holds for t = 0. If (i) holds for t, then for t+ 1, note that \u2200a, b \u2208 Rd\nEwd(Y \u2212 a, b)Y = Ewd(Y + a, b)Y , Ewd(Y \u2212 a, b) = 1\u2212 Ewd(Y + a, b), Ewd(Y \u2212 a, b)Y = \u2212Ewd(Y \u2212 a,\u2212b)Y , Ewd(Y \u2212 a, b) = 1\u2212 Ewd(Y \u2212 a,\u2212b).\nHence by definition of a\u3008t\u3009 and b\u3008t\u3009, it is straight forward to see\na \u3008t+1\u3009 (1) = \u2212a \u3008t+1\u3009 (2) , b \u3008t+1\u3009 (1) = b \u3008t+1\u3009 (2) .\nHence claim (i) holds for t+ 1. By induction, we know claim (i) holds for all t \u2265 0.\nB.3 Proof of Lemma 13\nAccording to the definition of P (xa, xb, x\u03b8) presented in Appendix A.4 we have\nP (xa, xb, x\u03b8) = \u222b w(y \u2212 xa, xb)\u03c6+(y, x\u03b8) dy\n= \u222b w(y, xb)\u03c6 +(y + xa, x\u03b8) dy\n= \u222b y\u22650 \u03c6+(y + xa, x\u03b8) dy + \u222b y\u22650 w(\u2212y, xb)(\u03c6+(y \u2212 xa, x\u03b8)\u2212 \u03c6+(y + xa, x\u03b8)) dy.\n(121)\nwhere the last equality used the fact that w(y, xb) + w(\u2212y, xb) = 1. If xa \u2265 x\u03b8 \u2265 0, then\n2(\u03c6+(y \u2212 xa, x\u03b8)\u2212 \u03c6+(y + xa, x\u03b8)) = \u03c6(y \u2212 xa + x\u03b8) + \u03c6(y \u2212 xa \u2212 x\u03b8)\u2212 \u03c6(y + xa + x\u03b8) + \u03c6(y + xa \u2212 x\u03b8) = \u03c6(y \u2212 xa + x\u03b8)\u2212 \u03c6(y + xa \u2212 x\u03b8) + \u03c6(y \u2212 xa \u2212 x\u03b8)\u2212 \u03c6(y + xa + x\u03b8) \u2265 0, \u2200y \u2265 0.\nHence with (121), the above equation implies that if xa \u2265 x\u03b8 \u2265 0, we have P (xa, xb, x\u03b8) \u2265 \u222b y\u22650 \u03c6+(y + xa, x\u03b8) dy\n= 1\n2 (1\u2212 \u03a6(xa \u2212 x\u03b8)) +\n1 2 (1\u2212 \u03a6(xa + x\u03b8)). (122)\nThis completes the proof of the first part of Lemma 13. Now we discuss the second part, i.e., the case xa < x\u03b8. First note that P is a decreasing function of xa, since xb \u2265 0 and\n\u2202P (xa, xb, x\u03b8)\n\u2202xa = \u2212\n\u222b 2xb\n(eyxb\u2212xaxb + e\u2212yxb+xaxb)2 \u03c6+(y, x\u03b8) dy \u2264 0.\nTherefore, we have\nP (xa, xb, x\u03b8) \u2265 P (x\u03b8, xb, x\u03b8) \u2265 1\n4 +\n1 2 (1\u2212 \u03a6(2x\u03b8))\nwhere the last inequality holds because xa = x\u03b8 satisfies the condition of (122). Hence, it immediately gives us that if xa < x\u03b8, then\nP1(xa, xb, x\u03b8) \u2265 1\n4 .\nThis completes the proof.\nB.4 Proof of Lemma 12\nWe warn the reader that in this proof we use the proof of Lemma 13, presented in the last section. According to the definition of \u0393 presented in Appendix A.4, we have\n\u0393(xa, xb, x\u03b8) = \u222b w(y \u2212 xa, xb)y\u03c6+(y, x\u03b8) dy\n= xaP (xa, xb, x\u03b8) + \u222b w(y \u2212 xa, xb)(y \u2212 xa)\u03c6+(y, x\u03b8) dy\n= xaP (xa, xb, x\u03b8) + \u222b w(y, xb)y\u03c6 +(y + xa, x\u03b8) dy\n< xaP (xa, xb, x\u03b8) + \u222b y\u22650 y\u03c6+(y + xa, x\u03b8) dy = xaP (xa, xb, x\u03b8) + 1\n2 \u222b y\u22650 {(y + xa \u2212 x\u03b8)\u03c6(y + xa \u2212 x\u03b8) + (y + xa + x\u03b8)\u03c6(y + xa + x\u03b8)}dy\n\u22121 2\n{ (xa \u2212 x\u03b8) \u222b y\u22650 \u03c6(y + xa \u2212 x\u03b8) dy \u2212 (xa + x\u03b8) \u222b y\u22650 \u03c6(y + xa + x\u03b8) dy } = xaP (xa, xb, x\u03b8) + 1\n2\n{ \u03c6(x\u03b8 \u2212 xa)\u2212 (xa \u2212 x\u03b8)(1\u2212 \u03a6(xa \u2212 x\u03b8)) } + 1\n2\n{ \u03c6(x\u03b8 + xa)\u2212 (xa + x\u03b8)(1\u2212 \u03a6(xa + x\u03b8)) } = xaP (xa, xb, x\u03b8) + 1\n2 (W (xa + x\u03b8) +W (xa \u2212 x\u03b8)), (123)\nwhere W (x) = \u03c6(x)\u2212 x(1\u2212 \u03a6(x)). Therefore we should find an upper bound for W (x). Towards this goal we use the following lemma:\nLemma 25. Let \u03c6(x),\u03a6(x) denote the pdf and CDF of standard Gaussian respectively. Then we have\n\u03c6(x)\n1\u2212 \u03a6(x) < x+\n\u221a 2\n\u03c0 , \u2200x > 0.\nThe proof of this Lemma is presented in Appendix B.5. Therefore from this lemma, we have\nW (x) <\n\u221a 2\n\u03c0 (1\u2212 \u03a6(x)).\nHence we can upper bound (123) by the following inequality:\n\u0393(xa, xb, x\u03b8) \u2264 xaP (xa, xb, x\u03b8) + \u221a 2\n\u03c0 {1 2 (1\u2212 \u03a6(xa + x\u03b8)) + 1 2 (1\u2212 \u03a6(xa \u2212 x\u03b8))}.\nFrom Lemma 13, we have\nP (xa, xb, x\u03b8) \u2265 1\n2 (1\u2212 \u03a6(xa \u2212 x\u03b8)) +\n1 2 (1\u2212 \u03a6(xa + x\u03b8)), \u2200xa \u2265 x\u03b8.\nTherefore, if xa \u2265 x\u03b8, then we have \u0393(xa, xb, x\u03b8) \u2264 xaP (xa, xb, x\u03b8) + \u221a 2\n\u03c0 {1 2 (1\u2212 \u03a6(xa + x\u03b8)) + 1 2 (1\u2212 \u03a6(xa \u2212 x\u03b8))}\n\u2264 xaP (xa, xb, x\u03b8) + \u221a 2\n\u03c0 P (xa, xb, x\u03b8),\n(124)\nwhich completes the proof.\nB.5 Proof of Lemma 25\nIt is equivalent to show that\nr(x) , (x+\n\u221a 2\n\u03c0 )(1\u2212 \u03a6(x))\u2212 \u03c6(x) > 0, \u2200x > 0.\nTaking the first derivative of the left hand side, we have\ndr(x)\ndx = 1\u2212 \u03a6(x)\u2212 \u03c6(x)(x+\n\u221a 2\n\u03c0 ) + x\u03c6(x) = 1\u2212 \u03a6(x)\u2212\n\u221a 2\n\u03c0 \u03c6(x).\nTaking the second derivative, we have\nd2 r(x)\ndx2 = \u2212\u03c6(x) +\n\u221a 2\n\u03c0 x\u03c6(x) = (\n\u221a 2\n\u03c0 x\u2212 1)\u03c6(x).\nHence, we have r\u2032\u2032(x) < 0 if x < \u221a \u03c0/2 and r\u2032\u2032(x) > 0 if x > \u221a \u03c0/2. Therefore, r\u2032(x) is first strictly decreasing then strictly increasing function of x for x \u2265 0. Since r\u2032(0) = 1/2 \u2212 1/\u03c0 > 0, r\u2032( \u221a \u03c0/2) = \u22120.04008391 and\nlim x\u2192\u221e r\u2032(x) = lim x\u2192\u221e\n1\u2212 \u03a6(x)\u2212 \u221a 2\n\u03c0 \u03c6(x) = 0,\nwe know there exists x0 \u2208 (0, \u221a \u03c0/2) such that r\u2032(x) > 0 if x < x0 and r\u2032(x) < 0 if x > x0. Hence, r(x) is first strictly increasing and then strictly decreasing function of x \u2265 0. Since r(x) = 0 and\n| lim x\u2192\u221e r(x)| \u2264 lim x\u2192\u221e (x+\n\u221a 2\n\u03c0 )(1\u2212 \u03a6(x)) + \u03c6(x)\n\u2264 2 lim x\u2192\u221e \u222b +\u221e y=x x\u03c6(y) dy\n\u2264 2 lim x\u2192\u221e \u222b +\u221e y=x y\u03c6(y) dy = 2 lim x\u2192\u221e \u03c6(x) = 0.\nHence, we have r(x) > 0, \u2200x > 0. This completes the proof of this Lemma.\nB.6 Proof of Lemma 14\nWe first calculate the derivative \u2202\u0393(xa,xb,x\u03b8)\u2202xb at zero:\n\u2202\u0393(xa, xb, x\u03b8)\n\u2202xb |xb=0 =\n\u222b 2(y \u2212 xa)\n(eyxb\u2212xaxb + e\u2212yxb+xaxb)2 y\u03c6+(y, x\u03b8) dy|xb=0\n= 1\n2\n\u222b y2 \u2212 xay\n2 (\u03c6(y \u2212 x\u03b8) + \u03c6(y + x\u03b8)) dy\n= 1\n2 (1 + x2\u03b8).\nThis derivative is clearly larger than 0.5. Now we prove the main result by contradiction. Suppose that the claim of the lemma is not correct. Then, for any fixed {cU,1, |\u03b8?\u30080\u3009,1|, \u2016\u03b8\n?\u2016}, \u2200\u03b4 > 0, we have a\u03b4 \u2208 [0, cU,1], b\u03b4 \u2208 [0, \u03b4], \u03b8\u03b4 \u2208 [\u03b8?\u30080\u3009,1, \u2016\u03b8 ?\u2016] such that\n\u2202\u0393(xa, xb, x\u03b8)\n\u2202xb |xa=a\u03b4,xb=b\u03b4,x\u03b8=\u03b8\u03b4 <\n1 2 .\nTherefore, for any sequence {\u03b4i} such that \u03b4i \u2192 0, we have\na\u03b4i \u2208 [0, cU,1], b\u03b4i \u2208 [0, \u03b4i], \u03b8\u03b4i \u2208 [\u03b8 ? \u30080\u3009,1, \u2016\u03b8 ?\u2016].\nSince the sequence {a\u03b4i , b\u03b4i , \u03b8\u03b4i}\u221ei=1, belong to a compact set, there exists a subsequence \u03b4ij such that {(a\u03b4ij , b\u03b4ij , \u03b8\u03b4ij )} converges to a limit (a \u221e, b\u221e, \u03b8\u221e) satisfying\na\u221e \u2208 [0, cU,1], b\u221e \u2208 [0, lim j\u2192\u221e \u03b4ij = 0], \u03b8 \u221e \u2208 [\u03b8?\u30080\u3009,1, \u2016\u03b8 ?\u2016].\nBy continuity of \u2202\u0393(xa,xb,x\u03b8)\u2202xb , we have\n1 2 \u2265 lim j\u2192\u221e \u2202\u0393(xa, xb, x\u03b8) \u2202xb |xa=a\u03b4ij ,xb=b\u03b4ij ,x\u03b8=\u03b8\u03b4ij\n= \u2202\u0393(xa, xb, x\u03b8)\n\u2202xb |xa=a\u221e,xb=0,x\u03b8=\u03b8\u221e\n= 1\n2 (1 + (\u03b8\u221e)2) >\n1 2 .\nThis contradiction proves that Lemma 14 is correct.\nB.7 Proof of Lemma 15\nFirs note that if x = 0, then\nK(0, xb) =\n\u222b eyxb \u2212 e\u2212yxb\n2(eyxb + e\u2212yxb) 1\u221a 2\u03c0 e\u2212y 2/2 dy,\nwhich is the integral of an odd function and is hence equal to zero. To prove that the function is increasing and concave for x \u2265 0, we calculate its derivatives. It is straightforward to see that\n\u2202K(x, xb)\n\u2202x =\n\u222b eyxb \u2212 e\u2212yxb\n2(eyxb + e\u2212yxb) (y \u2212 x)\u03c6(y \u2212 x) dy = \u2212 \u222b\neyxb \u2212 e\u2212yxb 2(eyxb + e\u2212yxb) d\u03c6(y \u2212 x)\n= \u222b \u03c6(y \u2212 x) 2xb\n(eyxb + e\u2212yxb)2 dy > 0,\nwhere the last equality is the result of integration by parts. Similarly, \u2200x \u2265 0\n\u22022K(x, xb)\n\u2202x2 =\n\u222b (y \u2212 x)\u03c6(y \u2212 x) 2xb\n(eyxb + e\u2212yxb)2 dy = \u2212 \u222b\n2xb (eyxb + e\u2212yxb)2 d\u03c6(y \u2212 x)\n(a) = \u2212 \u222b \u03c6(y \u2212 x) 4x2b(e yxb \u2212 e\u2212yxb)\n(eyxb + e\u2212yxb)3 dy\n= \u222b \u221e 0 (\u03c6(y + x)\u2212 \u03c6(y \u2212 x)) 4x2b(e yxb \u2212 e\u2212yxb) (eyxb + e\u2212yxb)3 dy < 0,\nwhere equality (a) is an application of integration by parts.\nB.8 Proof of Lemma 16\nRecall the definition of l(x) in Appendix A.5.3:\nl(x) = x(1\u2212 2\u03a6(\u2212x)) + 2\u03c6(x).\nDefine\nJ(x) = 1\n2 (x\u2212 l(x)(1\u2212 2\u03a6(\u2212x))) = 2\u03c6(x)\u03a6(\u2212x) + 2x\u03a6(\u2212x)\u2212 \u03c6(x)\u2212 2x\u03a6(\u2212x)2.\nWe would like to show that J(x) \u2265 0. Hence, we analyze the shape of the function J(x) by taking the derivatives, for all x > 0\ndJ(x)\ndx = \u22122\u03c6(x)2 + 2\u03a6(\u2212x)\u2212 x\u03c6(x)\u2212 2\u03a6(\u2212x)2\nd2 J(x)\ndx2 = \u03c6(x)(4\u03c6(x)x\u2212 3 + x2 + 4\u03a6(\u2212x))\ndJ \u2032\u2032(x)/\u03c6(x)\ndx = 2x(1\u2212 2x\u03c6(x)) \u2265 2x(1\u2212\n\u221a 2\n\u03c0 ) > 0.\nTherefore J \u2032\u2032(x)/\u03c6(x) is an strictly increasing function of x. With J \u2032\u2032(0) < 0 and J \u2032\u2032(10) > 0, we have J \u2032(x) is first strictly decreasing then strictly increasing function of x. Since J \u2032(0) = 1/2\u2212 1/\u03c0 > 0\nand limx\u2192\u221e J \u2032(x) = 0, we know J(x) achieves its minimum at either 0 or \u221e. Since J(0) = 0 and limx\u2192\u221e J(x) = 0, we have J(x) > 0, \u2200x > 0. This completes the proof of this lemma.\nB.9 Proof of Lemma 19\nAccording to the definition of functions F (xb, x\u03b8), Q(xa, xb, \u03b8) and (37) in Appendix A.4 we have\nF (xb, x\u03b8) = 2\u0393(0, xb, x\u03b8)\n= \u222b e(y+x\u03b8)xb \u2212 e\u2212(y+x\u03b8)xb e(y+x\u03b8)xb + e\u2212(y+x\u03b8)xb (y + x\u03b8)\u03c6(y) dy. (125)\nTo prove the concavity of this function we show that\n\u22022F (xb, x\u03b8)\n\u2202x2b \u2264 0.\nWe have \u2202F (xb, x\u03b8)\n\u2202xb =\n\u222b 4\n(e(y+x\u03b8)xb + e\u2212(y+x\u03b8)xb)2 (y + x\u03b8)\n2\u03c6(y) dy \u2265 0,\n\u22022F (xb, x\u03b8)\n\u2202x2b = \u222b \u22128(e(y+x\u03b8)xb \u2212 e\u2212(y+x\u03b8)xb) (e(y+x\u03b8)xb + e\u2212(y+x\u03b8)xb)3 (y + x\u03b8) 3\u03c6(y) dy\n= \u2212 \u222b\n8(eyxb \u2212 e\u2212yxb) (eyxb + e\u2212yxb)3 y3\u03c6(y \u2212 x\u03b8) dy \u2264 0.\nTherefore F is increasing and strictly concave in xb > 0. Now we only need to calculate F (0, x\u03b8) and F (x\u03b8, x\u03b8). From (125), we have\nF (0, x\u03b8) =\n\u222b e0 \u2212 e\u22120\ne0 + e0 (y + x\u03b8)\u03c6(y) dy = 0.\nUsing definition of F , we have\nF (x\u03b8, x\u03b8) = 2Q(0, x\u03b8, x\u03b8)\n= 2\n\u222b eyx\u03b8\neyx\u03b8 + e\u2212yx\u03b8 y\n1 2 \u221a 2\u03c0 (e\u2212(y\u2212x\u03b8) 2/2 + e\u2212(y+x\u03b8) 2/2) dy\n= \u222b eyx\u03b8y\n1\u221a 2\u03c0 e\u2212(y 2+x2\u03b8)/2 dy\n= x\u03b8\nThis completes the proof of this lemma.\nB.10 Proof of Lemma 18\nAccording to the definition of function F , we have\n\u2202F (xb, x\u03b8)\u2212 x\u03b8 \u2202xb\n|xb=x\u03b8 = \u222b\n4y2\n(eyxb + e\u2212yxb)2 1\u221a 2\u03c0 e\u2212(y\u2212x\u03b8) 2/2 dy|xb=x\u03b8\n=\n\u222b 2y2\neyx\u03b8 + e\u2212yx\u03b8 1\u221a 2\u03c0 e\u2212(y 2+x2\u03b8)/2 dy\n\u2264 e\u2212 x2\u03b8 2 .\nWe claim there exists \u03b4 > 0 is a function of only Lb, Ub, L\u03b8, \u2016\u03b8?\u2016 such that \u2200|xb \u2212 x\u03b8| \u2208 [0, \u03b4], xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016],\n\u2202F (xb, x\u03b8)\u2212 x\u03b8 \u2202xb\n\u2264 1 + e \u2212L 2 \u03b8 2\n2 . (126)\nWe prove it by contradiction. If not, for all \u03b4 > 0, we have b\u03b4 \u2208 [Lb, Ub], \u03b8\u03b4 \u2208 [L\u03b8, \u2016\u03b8?\u2016], |b\u03b4\u2212\u03b8\u03b4| \u2208 [0, \u03b4] such that\n\u2202F (xb, x\u03b8)\u2212 x\u03b8 \u2202xb |xb=b\u03b4,x\u03b8=\u03b8\u03b4 > 1 + e\u2212\nL2\u03b8 2\n2 .\nFor any sequence {\u03b4i} such that \u03b4i \u2192 0, there exists subsequence \u03b4ij such that {(b\u03b4ij , \u03b8\u03b4ij )} converge to the limits (b\u221e, \u03b8\u221e). By compactness of the choice of xb, x\u03b8, we have\nb\u221e \u2208 [Lb, Ub], \u03b8\u221e \u2208 [L\u03b8, \u2016\u03b8?\u2016], |b\u221e \u2212 \u03b8\u221e| \u2208 [0, lim j\u2192\u221e \u03b4ij = 0].\nBy continuity of \u2202F (xb,x\u03b8)\u2212x\u03b8\u2202xb , we have\n1 + e\u2212 L2\u03b8 2\n2 \u2264 lim j\u2192\u221e \u2202F (xb, x\u03b8)\u2212 x\u03b8 \u2202xb |xb=b\u03b4ij ,x\u03b8=\u03b8\u03b4ij\n= \u2202F (xb, x\u03b8)\u2212 x\u03b8\n\u2202xb |xb=x\u03b8=\u03b8\u221e\n= e\u2212 (\u03b8\u221e)2 2 < 1 + e\u2212\nL2\u03b8 2\n2 .\nContradiction! Hence we have Eq.(126) holds and \u2200|xb \u2212 x\u03b8| \u2208 [0, \u03b4], xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016],\n|F (xb, x\u03b8)\u2212 x\u03b8| \u2264 |F (x\u03b8, x\u03b8)|+ | 1 + e\u2212\nL2\u03b8 2\n2 (xb \u2212 x\u03b8)| =\n1 + e\u2212 L2\u03b8 2\n2 |xb \u2212 x\u03b8|.\nFrom Lemma 19, we have\n|F (xb, x\u03b8)\u2212 x\u03b8| < |xb \u2212 x\u03b8|, \u2200|xb \u2212 x\u03b8| /\u2208 [0, \u03b4), xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016].\nLet\n\u03ba\u2032\u2032b = max{ 1 + e\u2212\nL2\u03b8 2\n2 , sup |xb\u2212x\u03b8|/\u2208[0,\u03b4),xb\u2208[Lb,Ub],x\u03b8\u2208[L\u03b8,\u2016\u03b8?\u2016] |F (xb, x\u03b8)\u2212 x\u03b8| |xb \u2212 x\u03b8| },\nby continuity of the function |F (xb,x\u03b8)\u2212x\u03b8||xb\u2212x\u03b8| , we have \u03ba \u2032\u2032 b \u2208 (0, 1) is a function of only Lb, Ub, L\u03b8, \u2016\u03b8 ?\u2016 and\n|F (xb, x\u03b8)\u2212 x\u03b8| \u2264 \u03ba\u2032\u2032b |xb \u2212 x\u03b8|, \u2200xb \u2208 [Lb, Ub], x\u03b8 \u2208 [L\u03b8, \u2016\u03b8?\u2016].\nThis completes the proof of this lemma.\nB.11 Cluster Points of Population EM\nLemma 26. Any clustering point (a, b) of the estimates of the Population EM {(a\u3008t\u3009, b\u3008t\u3009)}t satisfy the following equations:\na = \u03b3(1\u2212 2p) 2p(1\u2212 p) , b = \u03b3\n2p(1\u2212 p) ,\n\u03b3 = Ewd(Y \u2212 a, b)Y , p = Ewd(Y \u2212 a, b),\nwhere Y \u223c 0.5N(\u2212\u03b8?, I) + 0.5N(\u03b8?, I). Proof. Here is a summary of our strategy to prove this result. We first prove that \u2016a\u3008t+1\u3009\u2212a\u3008t\u3009\u2016 \u2192 0 and \u2016b\u3008t+1\u3009 \u2212 b\u3008t\u3009\u2016 \u2192 0 as t \u2192 \u221e. Then we use the following simple argument to prove that in fact the clustering points must satisfy the above fixed point equations. Suppose that (a, b) is an accumulation point. Then there is a subsequence {(a\u3008ti\u3009, b\u3008ti\u3009)}\u221ei=1 that converges to (a, b). Since we have \u2016a\u3008t+1\u3009 \u2212 a\u3008t\u3009\u2016 \u2192 0 and \u2016b\u3008t+1\u3009 \u2212 b\u3008t\u3009\u2016 \u2192 0, we can simply argue that {(a\u3008ti+1\u3009, b\u3008ti+1\u3009)}\u221ei=1 also converges to (a, b). We know that\na\u3008ti+1\u3009 = \u03b3\u3008ti\u3009(1\u2212 2p\u3008ti\u3009) 2p\u3008ti\u3009(1\u2212 p\u3008ti\u3009) , b\u3008ti+1\u3009 = \u03b3\u3008ti\u3009\n2p\u3008ti\u3009(1\u2212 p\u3008ti\u3009) ,\n\u03b3\u3008ti+1\u3009 = Ewd(Y \u2212 a\u3008t\u3009, b\u3008t\u3009)Y p\u3008ti+1\u3009 = Ewd(Y \u2212 a\u3008t\u3009, b\u3008t\u3009).\nBy taking the limit i\u2192\u221e from both sides of the above equations we obtain the fixed point equations. Hence, the rest of the section is devoted to the proof of \u2016a\u3008t+1\u3009 \u2212 a\u3008t\u3009\u2016 \u2192 0 and \u2016b\u3008t+1\u3009 \u2212 b\u3008t\u3009\u2016 \u2192 0. The technique we us to prove this claim was first developed in [2]. Since a\u3008t\u3009 = (\u00b5\u3008t\u30091 + \u00b5 \u3008t\u3009 2 )/2 and b\u3008t\u3009 = (\u00b5 \u3008t\u3009 2 \u2212 \u00b5 \u3008t\u3009 1 )/2, we only need to prove that \u2016\u00b5 \u3008t+1\u3009 1 \u2212 \u00b5 \u3008t\u3009 1 \u2016 \u2192 0 and \u2016\u00b5 \u3008t+1\u3009 2 \u2212 \u00b5 \u3008t\u3009 2 \u2016 \u2192 0.\nDefine the following notion of distance between two parameter vectors: D(\u03b7,\u03bd) = \u2212Ef(z|Y ;\u03bd) \u2211 z ln ( f(z|Y ;\u03b7) f(z|Y ;\u03bd) ),\nwhere f(\u00b7) indicates corresponding pdf. Let \u00b5\u3008t\u3009 is a shorthand for (\u00b5\u3008t\u30091 ,\u00b5 \u3008t\u3009 2 ). As the first step of our proof we would like to show that D(\u00b5\u3008t+1\u3009,\u00b5\u3008t\u3009)\u2192 0. From (3), we have\nQf (\u03b7|\u03bd) = E \u2211 z f(z|Y ;\u03bd) ln (f(z,Y ;\u03b7)) = E ln (f(Y ;\u03b7)) + E \u2211 z f(z|Y ;\u03bd) ln (f(z|Y ;\u03b7))\n= E ln (f(Y ;\u03b7))\u2212D(\u03b7,\u03bd) +H(\u03bd,\u03bd) = L(\u03b7)\u2212D(\u03b7,\u03bd) +H(\u03bd,\u03bd),\nwhere\nL(\u03b7) , E ln (f(Y |\u03b7)) = E ln (1 2 \u03c6d(Y \u2212 \u03b71) + 1 2 \u03c6d(Y \u2212 \u03b72))\n\u2264 \u2212d 2 ln 2\u03c0, (127)\nH(\u03b7,\u03bd) , E \u2211 z f(z|y;\u03bd) ln (f(z|y;\u03b7)). (128)\nHence, \u00b5\u3008t+1\u3009 = argmax\u00b5\u2032Qf (\u00b5 \u2032|\u00b5\u3008t\u3009) = argmax\u00b5\u2032{L(\u00b5\u2032)\u2212D(\u00b5\u2032,\u00b5\u3008t\u3009)}.\nNote that every estimate of Population EM is obtained in a trade-off between maximizing the expected log-likelihood and minimizing the distance between the two consecutive estimates. First note that\nL(\u00b5\u3008t+1\u3009)\u2212D(\u00b5\u3008t+1\u3009,\u00b5\u3008t\u3009) \u2265 L(\u00b5\u3008t\u3009)\u2212D(\u00b5\u3008t\u3009,\u00b5\u3008t\u3009) = L(\u00b5\u3008t\u3009) (129)\nHence, L(\u00b5\u3008t+1\u3009) \u2265 L(\u00b5\u3008t\u3009) + D(\u00b5\u3008t+1\u3009,\u00b5\u3008t\u3009). Therefore, {L(\u00b5\u3008t\u3009)} is a non-decreasing sequence. Since according to (127), L(\u00b5) is upper bounded, thus {L(\u00b5\u3008t\u3009)}t converges. Also, according to (129) we have\n0 \u2264 D(\u00b5\u3008t+1\u3009,\u00b5\u3008t\u3009) \u2264 L(\u00b5\u3008t+1\u3009)\u2212 L(\u00b5\u3008t\u3009)\u2192 0, as t\u2192\u221e.\nThis implies that {D(\u00b5\u3008t+1\u3009,\u00b5\u3008t\u3009)} \u2192 0, as t\u2192\u221e.\nNote that D(\u00b7, \u00b7) is a measure of discrepancy between its two arguments. However, our goal is to show that the Euclidean distance between \u00b5\u3008t\u3009 and \u00b5\u3008t+1\u3009 goes to zero. The rest of the proof is devoted to this claim. Since,\n\u00b5\u3008t\u3009 = Ef(z|Y ;\u00b5\u3008t\u22121\u3009)Y Ef(z|Y ;\u00b5\u3008t\u22121\u3009) ,\n\u00b5\u3008t+1\u3009 = Ef(z|Y ;\u00b5\u3008t\u3009)Y Ef(z|Y ;\u00b5\u3008t\u3009) ,\nin order to prove \u2016\u00b5\u3008t+1\u3009 \u2212 \u00b5\u3008t\u3009\u2016 \u2192 0, we should show that \u2200z \u2208 {1, 0}\n\u2016Ef(z|Y ;\u00b5\u3008t+1\u3009)Y \u2212 Ef(z|Y ;\u00b5\u3008t\u3009)Y \u2016 \u2192 0,\nand |Ef(z|Y ;\u00b5\u3008t+1\u3009)\u2212 Ef(z|Y ;\u00b5\u3008t\u3009)| \u2192 0.\nIf we define \u03c8(x) = \u2212 lnx+ x\u2212 1, then\nD(\u03b7,\u03bd) = E \u2211 z \u03c8( f(z|Y ;\u03b7) f(z|Y ;\u03bd) )f(z|Y ;\u03bd), (130)\nSince \u03c8(x) > 0 for every value of x > 0, the fact that D(\u00b5\u3008t+1\u3009,\u00b5\u3008t\u3009)\u2192 0 implies that \u2200z \u2208 {1, 0}\nE\u03c8( f(z|Y ;\u00b5\u3008t+1\u3009) f(z|Y ;\u00b5\u3008t\u3009) )f(z|Y ;\u00b5\u3008t\u3009) \u2192 0, as t\u2192\u221e. (131)\nHence, for all z \u2208 {1, 0}\nE\u03c8( f(z|Y ;\u00b5\u3008t+1\u3009) f(z|Y ;\u00b5\u3008t\u3009) )f(z|Y ;\u00b5\u3008t\u3009)\n= E { (f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))\u2212 ( ln f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 ln f(z|Y ;\u00b5\u3008t\u3009) ) f(z|Y ;\u00b5\u3008t\u3009) } (i) = E\nf(z|Y ;\u00b5\u3008t\u3009) 2\u03be2 (f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2\n(ii) \u2265 Ef(z|Y ;\u00b5\u3008t\u3009)(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2 \u2265 Ef(z|Y ;\u00b5\u3008t\u3009)(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2I(\u2016Y \u2016 < M), \u2200t,M > 0.\nwhere Equality (i) is the result of the Taylor expansion on lnX and \u03be is a number between f(z|Y ;\u00b5\u3008t+1\u3009) and f(z|Y ;\u00b5\u3008t\u3009) and Inequality (ii) holds for the fact that\nf(z|Y ;\u00b5\u3008t+1\u3009), f(z|Y ;\u00b5\u3008t\u3009) \u2208 (0, 1), \u2200z \u2208 {1, 0}.\nHence, with (131), we have\nEf(z|Y ;\u00b5\u3008t\u3009)(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2I(\u2016Y \u2016 < M)\u2192 0, as t\u2192\u221e, \u2200M > 0, z \u2208 {1, 0}.\nAccording to Lemma 9 {(a\u3008tn\u3009, b\u3008tn\u3009)}\u221en=1 is in a compact set and hence so is {\u00b5\u3008t\u3009}. Since f(z|y;\u00b5\u3008t\u3009) is a continuous function of y and \u00b5\u3008t\u3009 with f(z|y;\u00b5\u3008t\u3009) > 0 and compactness of {\u00b5\u3008t\u3009}, there exists a constant c only depending on M such that\nf(z|y;\u00b5\u3008t\u3009) > c, \u2200\u2016y\u2016 < M, z \u2208 {1, 0}, t > 0.\nTherefore, for all M > 0, z \u2208 {1, 0}, we have\nE(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2I(\u2016Y \u2016 < M)\u2192 0, as t\u2192\u221e. (132)\nAlso, for all t \u2265 0, z \u2208 {1, 0}, we have\nE(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2I(\u2016y\u2016 \u2265M) \u2264 EI(\u2016Y \u2016 \u2265M)\u2192 0, as M \u2192\u221e. (133)\nWith (132) and (133), we have\nE(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2 \u2192 0, as t\u2192\u221e, \u2200z \u2208 {1, 0}.\nTherefore for all z \u2208 {1, 0}, as t\u2192\u221e, we have,\n\u2016E(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))Y \u2016 \u2264 \u221a E(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2E\u2016Y \u20162 \u2192 0,\n|Ef(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009)| \u2264 \u221a E(f(z|Y ;\u00b5\u3008t+1\u3009)\u2212 f(z|Y ;\u00b5\u3008t\u3009))2 \u2192 0.\nHence with compactness on sequence {\u00b5\u3008t\u3009}, we have\n\u2016\u00b5\u3008t+2\u30091 \u2212 \u00b5 \u3008t+1\u3009 1 \u2016 = \u2225\u2225\u2225\u2225\u2225Ef(z = 0|Y ;\u00b5\u3008t+1\u3009)YEf(z = 0|Y ;\u00b5\u3008t+1\u3009) \u2212 Ef(z = 0|Y ;\u00b5\u3008t\u3009)YEf(z = 0|Y ;\u00b5\u3008t\u3009) \u2225\u2225\u2225\u2225\u2225\u2192 0, as t\u2192\u221e,\nand\n\u2016\u00b5\u3008t+2\u30092 \u2212 \u00b5 \u3008t+1\u3009 2 \u2016 = \u2225\u2225\u2225\u2225\u2225Ef(z = 1|Y ;\u00b5\u3008t+1\u3009)YEf(z = 1|Y ;\u00b5\u3008t+1\u3009) \u2212 Ef(z = 1|Y ;\u00b5\u3008t\u3009)YEf(z = 1|Y ;\u00b5\u3008t\u3009) \u2225\u2225\u2225\u2225\u2225\u2192 0, as t\u2192\u221e.\nThis completes the proof of this lemma."}], "references": [{"title": "On spectral learning of mixtures of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In Eighteenth Annual Conference on Learning Theory,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "Learning mixtures of separated nonspherical Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "The Annals of Applied Probability,", "citeRegEx": "Arora and Kannan.,? \\Q2005\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2005}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"], "venue": null, "citeRegEx": "Balakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2014}, {"title": "Statistical mechanics of the maximum-likelihood density estimation", "author": ["N. Barkai", "H. Sompolinsky"], "venue": "Physical Review E,", "citeRegEx": "Barkai and Sompolinsky.,? \\Q1994\\E", "shortCiteRegEx": "Barkai and Sompolinsky.", "year": 1994}, {"title": "Polynomial learning of distribution families", "author": ["M. Belkin", "K. Sinha"], "venue": "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S.C. Brubaker", "S. Vempala"], "venue": "In Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "In Twenty-First Annual Conference on Learning Theory,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Learning mixtures of gaussians using the k-means algorithm", "author": ["K. Chaudhuri", "S. Dasgupta", "A. Vattani"], "venue": "CoRR, abs/0912.0086,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "On EM algorithms and their proximal generalizations", "author": ["S. Chr\u00e9tien", "A.O. Hero"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Chr\u00e9tien and Hero.,? \\Q2008\\E", "shortCiteRegEx": "Chr\u00e9tien and Hero.", "year": 2008}, {"title": "Expected maximum log likelihood estimation", "author": ["D. Conniffe"], "venue": "Journal of the Royal Statistical Society. Series D,", "citeRegEx": "Conniffe.,? \\Q1987\\E", "shortCiteRegEx": "Conniffe.", "year": 1987}, {"title": "Learning mixutres of Gaussians", "author": ["S. Dasgupta"], "venue": "In Fortieth Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians", "author": ["S. Dasgupta", "L. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Maximum-likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Royal Statist. Soc. Ser. B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "On the mathematical foundations of theoretical statistics", "author": ["R.A. Fisher"], "venue": "Philosophical Transactions of the Royal Society,", "citeRegEx": "Fisher.,? \\Q1922\\E", "shortCiteRegEx": "Fisher.", "year": 1922}, {"title": "Tight bounds for learning a mixture of two gaussians", "author": ["M. Hardt", "E. Price"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Hardt and Price.,? \\Q2015\\E", "shortCiteRegEx": "Hardt and Price.", "year": 2015}, {"title": "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions", "author": ["D. Hsu", "S.M. Kakade"], "venue": "In Fourth Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Efficiently learning mixtures of two Gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "In Forty-second ACM Symposium on Theory of Computing,", "citeRegEx": "Kalai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Kannan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2008}, {"title": "Oracle inequalities in empirical risk minimization and sparse recovery problems", "author": ["V. Koltchinskii"], "venue": "In E\u0301cole d\u2032e\u0301te\u0301 de probabilite\u0301s de Saint-Flour XXXVIII,", "citeRegEx": "Koltchinskii.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2011}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "In Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "MacQueen.,? \\Q1967\\E", "shortCiteRegEx": "MacQueen.", "year": 1967}, {"title": "Settling the polynomial learnability of mixtures of Gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Mixture densities, maximum likelihood and the EM algorithm", "author": ["R.A. Redner", "H.F. Walker"], "venue": "SIAM Review,", "citeRegEx": "Redner and Walker.,? \\Q1984\\E", "shortCiteRegEx": "Redner and Walker.", "year": 1984}, {"title": "An analysis of the EM algorithm and entropy-like proximal point methods", "author": ["P. Tseng"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Tseng.,? \\Q2004\\E", "shortCiteRegEx": "Tseng.", "year": 2004}, {"title": "A spectral algorithm for learning mixtures models", "author": ["S. Vempala", "G. Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vempala and Wang.,? \\Q2004\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "On the convergence properties of the EM algorithm", "author": ["C.F.J. Wu"], "venue": "The Annals of Statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "On convergence properties of the EM algorithm for Gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Xu and Jordan.,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan.", "year": 1996}, {"title": "\u03b4. For the last claim, we borrow a technique in the proof of corollary 2 in B.2", "author": ["Balakrishnan"], "venue": null, "citeRegEx": "Balakrishnan,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan", "year": 2014}, {"title": "\u03bei\u3008yi \u2212 xa,xb\u3009\u3008yi,uj\u3009 is symmetric in terms of xb and constraints of xb is symmetric. The correctness of the last inequality is shown in B.2 of Balakrishnan et al", "author": ["Balakrishnan"], "venue": "EY ,\u03bee", "citeRegEx": "Balakrishnan,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "1 Introduction Since Fisher\u2019s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering.", "startOffset": 41, "endOffset": 55}, {"referenceID": 13, "context": "1 Expectation Maximization Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).", "startOffset": 199, "endOffset": 247}, {"referenceID": 23, "context": "1 Expectation Maximization Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).", "startOffset": 199, "endOffset": 247}, {"referenceID": 26, "context": "Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983).", "startOffset": 235, "endOffset": 245}, {"referenceID": 26, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008).", "startOffset": 140, "endOffset": 188}, {"referenceID": 24, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008).", "startOffset": 140, "endOffset": 188}, {"referenceID": 9, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008).", "startOffset": 140, "endOffset": 188}, {"referenceID": 11, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 1, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 12, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 25, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 18, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 0, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 6, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 5, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 17, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 4, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 22, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 16, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 15, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 21, "context": "(2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.", "startOffset": 75, "endOffset": 104}, {"referenceID": 20, "context": "(2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.", "startOffset": 75, "endOffset": 104}, {"referenceID": 2, "context": "3 Background and Related Work The EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method for computing parameter estimates from incomplete data.", "startOffset": 74, "endOffset": 97}, {"referenceID": 1, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008). However, these analyses do not distinguish between global maximizers and other stationary points (except, e.g., when the likelihood function is unimodal). Thus, as an optimization algorithm for maximizing the log-likelihood objective, the \u201cworst-case\u201d performance of EM is somewhat discouraging. For a more optimistic perspective on EM, one may consider a \u201cbest-case\u201d analysis, where (i) the data are an iid sample from a distribution in the given model, (ii) the sample size is sufficiently large, and (iii) the starting point for EM is sufficiently close to the parameters of the data generating distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses, and (iii) is a generous assumption that may be satisfied in certain cases. Redner and Walker (1984) show that in such a favorable scenario, EM converges to the MLE almost surely for a broad class of mixture models.", "startOffset": 164, "endOffset": 979}, {"referenceID": 0, "context": "Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM.", "startOffset": 25, "endOffset": 52}, {"referenceID": 0, "context": "Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM. Thus, EM may be used in a tractable two-stage estimation procedures given a first-stage pilot estimator that can be efficiently computed. Indeed, for the special case of Gaussian mixtures, researchers in theoretical computer science and machine learning have developed efficient algorithms that deliver the highly accurate parameter estimates under appropriate conditions. Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al.", "startOffset": 25, "endOffset": 678}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894).", "startOffset": 8, "endOffset": 447}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated.", "startOffset": 8, "endOffset": 640}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures.", "startOffset": 8, "endOffset": 839}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions.", "startOffset": 8, "endOffset": 1016}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.", "startOffset": 8, "endOffset": 1313}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption. Lastly, for the specific case of our Model 1, Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant of EM) when started in a sufficiently small neighborhood around the true parameters; here, the size of the neighborhood grows with the separation between the two", "startOffset": 8, "endOffset": 1660}, {"referenceID": 2, "context": "Furthermore, the fact that \u03b7\u2217 is a global maximizer of Q(\u03b7 | \u03b7\u2217) is known as the self-consistency property (Balakrishnan et al., 2014).", "startOffset": 107, "endOffset": 134}, {"referenceID": 2, "context": "EM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit.", "startOffset": 3, "endOffset": 65}, {"referenceID": 3, "context": "This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn \u221d n as n\u2192\u221e (Barkai and Sompolinsky, 1994).", "startOffset": 148, "endOffset": 178}, {"referenceID": 19, "context": "To simplify the final expression even further, we use the following lemma from Koltchinskii (2011) Lemma 24.", "startOffset": 79, "endOffset": 99}, {"referenceID": 2, "context": "2 in Balakrishnan et al. (2014). Let", "startOffset": 5, "endOffset": 32}, {"referenceID": 2, "context": "2 of Balakrishnan et al. (2014). For part 1, as shown in B.", "startOffset": 5, "endOffset": 32}, {"referenceID": 2, "context": "2 of Balakrishnan et al. (2014). For part 1, as shown in B.2 of Balakrishnan et al. (2014) we have EY ,\u03bee 1 n \u2211n i=1 \u03beiyiy > i \u2016op \u2264 EY ,\u03beej\u2208[M ] 1 n \u2211n i=1 \u03bei\u3008yi,uj\u2032 \u30092 (115) Recall that yi = \u03b6i\u03b8 ? + \u03c9i and (112), we have EY e\u3008yi,uj\u3009 = E\u03b6e ?jE\u03c9eij \u2264 e \u2016\u03b8?\u20162+1 2 .", "startOffset": 5, "endOffset": 91}], "year": 2016, "abstractText": "Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.", "creator": "LaTeX with hyperref package"}}}