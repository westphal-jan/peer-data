{"id": "1609.00288", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "A Unified View of Multi-Label Performance Measures", "abstract": "multi - label classification mainly deals with the systemic problem where creating each single instance is associated inherently with creating multiple class labels. because evaluation implemented in multi - label scale classification is more complicated tasks than single - label setting, far a number groups of performance optimization measures have quickly been proposed. it is noticed recently that requiring an evolved algorithm usually performs differently on entirely different measures. therefore, it is pretty important to understand which different algorithms continuously perform well on which measure ( s ) and why. in this paper, ideally we propose a unified margin effectiveness view to separately revisit multiple eleven collaborative performance measures in automated multi - item label classification. since in particular, we both define concurrent label - front wise margin and comparative instance - margin wise margin, and immediately prove ourselves that through maximizing these margins, different versus corresponding scale performance measures available will be optimized. based on the currently defined margins, performing a max - margin approach device called limo validation is consistently designed and ensure empirical reliability results completely verify our theoretical findings.", "histories": [["v1", "Thu, 1 Sep 2016 15:49:43 GMT  (24kb)", "http://arxiv.org/abs/1609.00288v1", null], ["v2", "Fri, 1 Sep 2017 08:18:33 GMT  (239kb,D)", "http://arxiv.org/abs/1609.00288v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xi-zhu wu", "zhi-hua zhou"], "accepted": true, "id": "1609.00288"}, "pdf": {"name": "1609.00288.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhi-Hua Zhou"], "emails": ["zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n00 28\n8v 1\n[ cs\n.L G\n] 1\nS ep\nMulti-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than single-label setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures will be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results verify our theoretical findings.\nKeywords: Multi-label classification, multi-label performance measures, margin, unified view"}, {"heading": "1. Introduction", "text": "Multi-label classification aims to build classification models for objects assigned with multiple labels simultaneously, which is a common learning paradigm in real-world applications. In text categorization, a document may be associated with a range of topics, such as science, entertainment, news and so on (Schapire and Singer, 2000); In image classification, an image can have both field and mountain tags (Boutell et al., 2004); In gene functional analysis, a gene belongs to the functions of protein synthesis, metabolism\n\u2217Email: zhouzh@lamda.nju.edu.cn\nPreprint submitted to Artificial Intelligence Journal September 2, 2016\nand transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008).\nIn the traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000).\nBecause different performance measures focus on different aspects, previous works often analyze few specific measures. Dembczynski et al. (2010) showed that hamming loss and subset 0/1 loss cannot be optimized at the same time. Gao and Zhou (2013) studied the consistency of two performance measures: ranking loss and hamming loss. They proved that none convex surrogate loss is Bayes consistent with the ranking loss, and gave a consistent surrogate loss function for hamming loss in deterministic case. Ye et al. (2012) pointed out that algorithms for learning to maximize F-measures follow two approaches: the decision-theoretic approach (DTA) and the empirical utility maximization (EUM) approach. Waegeman et al. (2014) studied the DTA consistent F-measure optimization in multi-label classification setting, and presented a Bayes-optimal algorithm via estimating parameters of the joint distribution. Koyejo et al. (2015) studied the EUM optimal multi-label classifier for F-measure, including instance-, micro- and macro-averages. As far as we know, although used to justify the superiority of multi-label classification algorithms (Zhang and Wu, 2015), some performance measures have not been carefully analyzed.\nInstead of detailedly analyzing one or two multi-label performance measures, we are trying to analyze whether there are shared common properties between different measures. Towards empirical utility maximization (EUM) and empirical loss minimization (ELM), we propose a margin view to revisit a large number of multi-label performance measures including hamming loss, ranking loss, one-error, coverage, average precision, macro-, micro- and instance-averaging F-measures and AUC s. Specifically, two new concepts\ncalled label-wise margin and instance-wise margin are proposed. Based on these margins, we define the effectiveness to describe the ability of a multilabel predictor F to seperate, and analyze the relationship between an effective predictor and performance measures. Our results show that by maximizing each margin, corresponding performance measures will be optimized. In addition, we design the LIMO approach to maximize these margins and conduct experiments on real-world benchmark datasets. Experiments not only support the theory predicts, but also demonstrate a flexible way to optimize multi-label performance measures we need through one approach by different parameter settings.\nThe rest of the manuscript is organized as follows. Section 2 introduces the notations and definitions of eleven multi-label performance measures. Section 3 proposes the label-wise and instance-wise margins, and analyzes how they are related to the performance measures. Section 4 presents the LIMO approach. Section 5 reports the results of experiments. Finally, section 6 concludes and indicates several future issues."}, {"heading": "2. Preliminaries", "text": "In this section, we define the notations used in this paper, and multi-label performance measures we will discuss in the next section."}, {"heading": "2.1. Notations", "text": "Assume xi \u2208 Rd\u00d71 is a real value instance vector, yi \u2208 {0, 1}l\u00d71 is a label vector for xi. m denotes the number of training samples. Therefore yij (i \u2208 {1, . . . , m}, j \u2208 {1, . . . , l}) means the jth label of the ith instance, and yij = 1 or 0 means the jth label is relevant or irrelevant. The instance matrix is X \u2208 Rm\u00d7d and the label matrix is Y \u2208 {0, 1}m\u00d7l. H : Rd \u2192 {0, 1}l is the multi-label classifier, and we consider separate classifier on each label, so H = {h1, . . . , hl} and hj(xi) denotes the prediction of yij. Moreover, F : Rd \u2192 Rl is the multi-label predictor and the predicted value can be regarded as the confidence of relevance. Similarly, F can be decomposed as {f1, . . . , fl} where fj(xi) denotes the predicted value of yij.\nH can be induced from F via thresholding functions. For example, hj(xi) = [[fj(xi) > t(xi)]] uses a thresholding function based on the instance xi and outputs 1 if predicted value is higher than the threshold. [[\u03c0]] returns 1 if predicate \u03c0 holds, and 0 otherwise.\nFor simplification, we use Y i\u00b7 to denote the ith row and Y \u00b7j to denote the jth column of the label matrix. Furthermore, Y +i\u00b7 (Y \u2212 i\u00b7 ) denotes the index set of relevant(irrelevant) labels of Y i\u00b7. Formally, Y + i\u00b7 = {j |yij = 1} and Y \u2212i\u00b7 = {j |yij = 0}. In terms of jth column of label matrix, Y +\u00b7j = {i |yij = 1} and Y \u2212\u00b7j = {i |yij = 0}. We use | \u00b7 | to denote the cardinality of a set, thus, the number of relevant labels xi has is simply |Y +i\u00b7 |."}, {"heading": "2.2. Multi-label Performance Measures", "text": "Table 1 summarizes the eleven multi-label performance measures we will study in this work. The first five measures (hamming loss, ranking loss, one-error, coverage, average precision) are considered in Schapire and Singer (2000) and a multitude of works, e.g., Zhang and Wu (2015) and Dembczynski et al. (2012). The next six measures are F-measure and AUC (the Area Under the ROC Curve) extensions in multi-label classification via different averaging strategies. These F-measures are popluar both in algorithm evaluation (Liu and Tsang, 2015) and theoretical analysis (Dembczynski et al., 2011) (Koyejo et al., 2015). AUC s are used for algorithm evaluation such as in Pham et al. (2015) Zhang and Wu (2015)."}, {"heading": "3. Label-wise Margin vs. Instance-wise Margin", "text": "Here we define two new concepts: label-wise margin and instance-wise margin.\nDefinition 1. Given a multi-label predictor F : Rd \u2192 Rl and F = {f1, . . . , fl}, a training set (X,Y ), the label-wise margin on instance xi is defined as:\n\u03b3 labeli = min u,v {fu(xi)\u2212 fv(xi) | (u, v) \u2208 Y +i\u00b7 \u00d7 Y \u2212i\u00b7 }. Y +i\u00b7 \u00d7 Y \u2212i\u00b7 is the set of all (relevant, irrelevant) label index pairs of instance i.\nDefinition 2. Given a multi-label predictor F : Rd \u2192 Rl and F = {f1, . . . , fl}, a training set (X,Y ), the instance-wise margin on label Y \u00b7j is defined as:\n\u03b3instj = min a,b {fj(xa)\u2212 fj(xb) | (a, b) \u2208 Y +\u00b7j \u00d7 Y \u2212\u00b7j }.\nY +\u00b7j \u00d7 Y \u2212\u00b7j is the set of all (positive, negative) instance index pairs of label j. Label-wise margin and instance-wise margin describe the ability of F to separate. The larger the label-wise margin, the easier to distinguish relevant\nand irrelevant labels of an instance. Meanwhile, the larger the instance-wise margin, the easier for F to distinguish positive and negative instances of a particular label. Therefore, we want to maximize label-wise/instance-wise margin to get better performance.\nAlthough we prefer maximizing these two margins, with respect to performance measures, the objective can be relaxed. We define three properties a predictor F can have: label-wise effective, instance-wise effective and double effective.\nDefinition 3. If all the label-wise margins of F on a dataset D = (X,Y ) are positive, this predictor F is label-wise effective on D.\nDefinition 4. If all the instance-wise margins of F on a dataset D = (X,Y ) are positive, this predictor F is instance-wise effective on D.\nDefinition 5. If all the label-wise margins and instance-wise margins of F on a dataset D = (X,Y ) are positive, this predictor F is double effective on D.\nRoughly speaking, label-wise effective means F can exactly distinguish relevant and irrelevant labels of each instance and instance-wise effective means F can exactly distinguish positive and negative instances of every label. Not surprisingly, double effective F has the strongest ability to seperate."}, {"heading": "3.1. F -based Performance Measures", "text": "Several multi-label performance measures can be empirically optimized according to the following theorems:\nTheorem 1. If a multi-label predictor F is label-wise effective on D, then ranking loss, one-error, coverage, average precision and instance-AUC are optimized on the dataset.\nProof: From the definition of label-wise effective, for every pair (u, v) \u2208 Y +i\u00b7 \u00d7 Y \u2212i\u00b7 , we have fu(xi) > fv(xi). Therefore, the reversed set SetRi (in Table 1 Ranking loss) is empty and the cardinality of the set is zero, which implies the cardinality sum of all reversed sets rloss(F ) = 0. Ranking loss is optimized.\nFor a label-wise effective F , because label-wise margin is positive on an instance xi, we have:\nmax u fu(xi) > max v\nfv(xi), \u2200u \u2208 Y +i\u00b7 , \u2200v \u2208 Y \u2212i\u00b7 .\nThen \u2200xi, argmaxF (xi) \u2208 Y +i\u00b7 .\nThus, [[argmaxF (xi) /\u2208 Y +i\u00b7 ]] = 0 for every instance xi, and one-error(F ) = 0. One-error is optimized.\nWhen F is label-wise effective, the maximum rank of a relevant label is less than the minimum rank of an irrelevant label, which means:\nmax u\u2208Y +i\u00b7 rankF (xi, u) < min v\u2208Y \u2212i\u00b7 rankF (xi, v), (1)\nmax u\u2208Y +i\u00b7\nrankF (xi, u) = |Y +i\u00b7 |.\nTherefore, coverage can be calculated as:\ncoverage(F ) = 1\nm\nm \u2211\ni=1\n[|Y +i\u00b7 | \u2212 1],\nwhich is the optimal value of coverage. Assume j is a relevant label of instance i, it follows from Eq.(1) that:\nrankF (xi, j) = |{k \u2208 Y +i\u00b7 |rankF (xi, k) \u2264 rankF (xi, j)}|.\nSince rankF (xi, j) is exactly the definition of SetPij , avgprec(F ) = 1, i.e, average precision is optimized.\nBecasuse of label-wise effectiveness, for an instance xi, we have:\nfu(xi) > fv(xi), \u2200(u, v) \u2208 Y +i\u00b7 \u00d7 Y \u2212i\u00b7 .\nTherefore, the size of the correct ordered prediction value pair on instance i is:\n|{(u, v) \u2208 Y +i\u00b7 \u00d7 Y \u2212i\u00b7 |fu(xi) \u2265 fv(xi)}| = |Y +i\u00b7 ||Y \u2212i\u00b7 |. So instance-AUC (F ) = 1 and instance-AUC is optimized.\nSimilar to the proof of instance-AUC, we can prove the result of macroAUC :\nTheorem 2. If a multi-label predictor F is instance-wise effective on D, then macro-AUC is optimized.\nProof: Becasuse of instance-wise effectiveness, for a label yj , we have:\nfj(xa) > fj(xb), \u2200(a, b) \u2208 Y +\u00b7j \u00d7 Y \u2212\u00b7j .\nTherefore, the the size of the correct ordered prediction value pair on label j is:\n|{(a, b) \u2208 Y +\u00b7j \u00d7 Y \u2212\u00b7j |fj(xa) \u2265 fj(xb)}| = |Y +\u00b7j ||Y \u2212\u00b7j |.\nSo marco-AUC (F ) = 1 and marco-AUC is optimized.\nmicro-AUC sees the label matrix as a whole and cannot be optimized by instance-wise effective F or label-wise effective F . In general case, it cannot be optimized by double effective F when the label matrix is very dense. Fortunately, in most practical multi-label problems, the label matrix is sparse. And we prove the Theorem 3 of micro-AUC under a simple sparse assumption.\nFor convenience, we redefine the micro-AUC in the following way:\nDefinition 6. Given a label matrix A \u2208 {0, 1}m\u00d7l and the corresponding prediction matrix B \u2208 Rm\u00d7l, Then all pair set Sall = {(i, j, u, v)|Aij = 1 \u2227 Auv = 0} and the inversed pair set Sinverse = {(i, j, u, v)|Bij < Buv \u2227 Aij = 1 \u2227Auv = 0}. Therefore, micro-AUC on prediction B is:\nmicro-AUC(A,B) = 1\u2212 |Sinverse||Sall| (2)\nThis definition is another form and the value is equal to the micro-AUC definition in Table 1. Based on this definition, we can prove the following lemma.\nLemma 1. Given a label matrix A \u2208 {0, 1}m\u00d7l, where m = kl(k \u2208 N+) and \u2211l\nj Aij = 1, \u2211n i Aij = k. The prediction matrix B \u2208 Rm\u00d7l, where:\nBij > Biu if Aij = 1 \u2227 Aiu = 0, Bij > Bvj if Aij = 1 \u2227 Avj = 0.\nThen, |Sinverse| |Sall| = O( 1 k ). (3)\nProof: Because for the given label matrix A, Sall is fixed. We construct the worst case B to maximize the size of Sinverse.\n1. Rearrange A to a block diagonal matrix where each block is a column vector e = [1, 1, \u00b7 \u00b7 \u00b7 , 1]\u22a4 filled with k ones. Rearrange the rows of B as the operations conducted on A.\nA =\n\n    e 0 \u00b7 \u00b7 \u00b7 0 0 e \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 e\n\n   \n2. Suppose the Bij where Aij = 1 is top-down sorted. This means inversed pairs (Bij, Bi\u2032j\u2032) can only appear when i\n\u2032 > i and j\u2032 > j. Therefore, the number of inversed pairs is:\n|Sinverse| = l\u22121 \u2211\ni=1\n(l \u2212 i)2k = 1 6 (l \u2212 1)l(2l \u2212 3)k. (4)\nAnd |Sall| = kl \u00d7 (l \u2212 1)kl = (l \u2212 1)l2k2. Thus,\n|Sinverse| |Sall| = O ((l \u2212 1)l(2l \u2212 3)k (l \u2212 1)l2k2 ) = O( 1 k ). (5)\nIt is noticed if Bij is not top-down sorted after rearranging, the size of Sinverse will decrease since some rows below will not be inversed.\nTheorem 3. If a multi-label predictor F is double effective on D, where each instance has one relevant label and each label has k positive instances, then 1\u2212micro-AUC(F )= O( 1\nk ).\nProof: If the prediction matrix of a double effective F on dataset D = (X, Y ) is F (X), then the relationship between F (X) and Y is the same as B and A in Lemma 1. Therefore 1\u2212micro-AUC (F )= O( 1\nk ).\nWith the above analysis, we can conclude that a label-wise effective F can optimize ranking loss, one-error, coverage, average precision, instance-AUC and an instance-wise effective F can optimize macro-AUC. For micro-AUC, a double effective F can control it to a certain extent.\n3.2. H-based Performance Measures\nAs mentioned in section 2.2, there are some measures evaluating classifier H instead of predictor F , leads to a discussion of thresholding.\nA label-wise effective F can be equipped with a thresholding function based on each instance such as t(xi) and construct the H by hj(xi) = [[fj(xi) > t(xi)]]. However, using t(xi) on an instance-wise effective F is unreasonable since the predicted values on different labels may not be comparable. In a word, we should use suitable threshold function on different effective F s, i.e., t(xi) on each instance for label-wise effective F , tj on each label for instance-wise effective F . It is reasonable to use either t(xi) or tj for double effective F .\nTo formally analyze the H-based performance measures, we define the threshold error:\nDefinition 7. Given a descending ordered real-value sequence x1, x2, . . . , xk with an optimal cut number c\u2217, where c\u2217 \u2208 N and 1 \u2264 c\u2217 \u2264 k. For a real value threshold t \u2208 (xk \u2212 1, x1 + 1), the threshold error \u01eb = | argmini(xi) \u2212 c\u2217| where xi > t.\nIntuitively, the threshold error \u01eb counts how many items are incorrectly classified on a descending ordered sequence where the correct answer is c\u2217. Based on the threshold error, we propose the following theorems about performance measures on H .\nTheorem 4. For a label-wise effective F , if the thresholding function makes at most \u01ebi error on each instance i, the instance-F1 and hamming loss can be bounded as follows:\ninstance-F1(H) \u2265 1 m\nm \u2211\ni=1\nmin {2(|Y +i\u00b7 | \u2212 \u01ebi)\n2|Y +i\u00b7 | \u2212 \u01ebi , 2|Y +i\u00b7 | 2|Y +i\u00b7 |+ \u01ebi } , (6)\nhloss(H) \u2264 1 ml\nm \u2211\ni=1\n\u01ebi. (7)\nProof: The result of hamming loss is trivial because hamming loss is counting all the flipped bits and averaging on the size of label matrix. On each instance i, the label-wise effective property ensures that at most \u01ebi bits are flipped, thus the Eq.(7) is proved.\nFor the result of F-measure, because F is label-wise effective, its order of prediction value on a specific instance xi is correct. Therefore, the threshold error \u01ebi can happen in either the two ways:\n1. \u01ebi positive labels are predicted as negative labels. In this case, the true positive number TPi on this instance becomes |Y +i\u00b7 | \u2212 \u01ebi, and the false positive number FPi is zero, and the false negative number FNi becomes \u01ebi. The precision value and the recall value will be:\nPreci = TPi\nTPi + FPi = 1, Reci = TPi TPi + FNi = |Y +i\u00b7 | \u2212 \u01ebi |Y +i\u00b7 |\nAnd the F -measurei is:\nF -measurei = 2Preci \u00d7 Reci Preci +Reci = 2(|Y +i\u00b7 | \u2212 \u01ebi) 2|Y +i\u00b7 | \u2212 \u01ebi\n2. \u01ebi negative labels are predicted as positive labels. In this case, the true positive number TPi on this instance is still |Y +i\u00b7 |, and the false positive number FPi = \u01ebi , and the false negative number FNi is zero. The precision value and the recall value will be:\nPreci = TPi\nTPi + FPi = |Y +i\u00b7 | |Y +i\u00b7 |+ \u01ebi , Reci = TPi TPi + FNi = 1\nAnd the F -measurei is:\nF -measurei = 2Preci \u00d7 Reci Preci +Reci = 2|Y +i\u00b7 | 2|Y +i\u00b7 |+ \u01ebi\nJointly considering the above two conditions, Eq.(6) is proved.\nSimilar to Theorem 4, we can prove the results for instance-wise effective F and double effective F :\nTheorem 5. For an instance-wise effective F , if the thresholding function makes at most \u01ebj error on each label j, then the macro-F1 and hamming loss can be bounded as follows:\nmacro-F1(H) \u2265 1 l\nl \u2211\nj=1\nmin {2(|Y +\u00b7j | \u2212 \u01ebj)\n2|Y +\u00b7j | \u2212 \u01ebj , 2|Y +\u00b7j | 2|Y +\u00b7j |+ \u01ebj } , hloss(H) \u2264 1 ml l \u2211\nj=1\n\u01ebj\nTheorem 6. For a double effective F , if each instance has one relevant label and each label contains k positive instances. When k is large enough and the thresholding function makes \u01eb error on the label matrix, then the micro-F1 and hamming loss can be bounded as follows:\nmicro-F1(H) \u2265 min {2(\n\u2211l j=1 |Y +\u00b7j | \u2212 \u01eb)\n2 \u2211l j=1 |Y +\u00b7j | \u2212 \u01eb ,\n2 \u2211l j=1 |Y +\u00b7j | 2 \u2211l\nj=1 |Y +\u00b7j |+ \u01eb\n} , hloss(H) \u2264 \u01eb ml\nThe proofs of Theorem 5 and Theorem 6 are similar to the proof of Theorem 4, and we omit them.\nWith the above analysis, we can conclude that a label-wise effective F can optimize instance-F1, an instance-wise effective F can optimize macroF1, and a double effective F can optimize micro-F1. All the three effective F s can optimize hamming loss."}, {"heading": "3.3. Summary of main results", "text": "According to the analysis in section 3.1 and section 3.2, the relationship between multi-label performance measures and proposed margins can be summarized in Table 2. Because double effective is a special case of label-wise effective and instance-wise effective, whenever a performance measure can be optimized by either of the two, double effective F can optimize it.\nIn the light of the analysis, the performance on different performance measures through optimizing margins can be expected. For example, if one only maximizes instance-wise margin on each label, s/he may suffer higher loss on ranking loss, coverage and some other measures where \u2018\u2717\u2019 marked in the instance-wise column; If one tries to maximize the label-wise margin but pay no attention to instance-wise margin, s/he may perform well on average precision but poor on macro-F1 (e.g., Elisseeff and Weston (2002)). Maximizing both the label-wise margin and instance-wise margins to get a double effective F is expected to be the best choice."}, {"heading": "4. The LIMO Approach", "text": "The above analysis reveals that maximizing different margins will optimize different measures, and if possible, double effective F is prefered since it enjoys the benefits of maximizing both the label-wise margin and the instancewise margin. Therefore, we propose the LIMO (Label-wise and Instance-wise\nMargins Optimization) approach. LIMO is a single approach which can optimize both the two margins, and it can also be degenerated to optimize either margin seperately via parameter setting."}, {"heading": "4.1. Formulation", "text": "Suppose F is a linear predictor, which means F (X) = W TX where W = [w1,w2, \u00b7 \u00b7 \u00b7 ,wl]. We propose the following formulation:\nargmin W ,\u03be\nl \u2211\ni=1\n||wi||2 + \u03bb1 m \u2211\ni=1\n\u2211\n(u,v)\n\u03beuvi + \u03bb2\nl \u2211\nj=1\n\u2211\n(a,b)\n\u03bejab\ns.t. w\u22a4uxi \u2212w\u22a4v xi > 1\u2212 \u03beuvi , \u03beuvi \u2265 0, for i = 1, \u00b7 \u00b7 \u00b7 , m and (u, v) \u2208 Y +i\u00b7 \u00d7 Y \u2212i\u00b7 , w\u22a4j xa \u2212w\u22a4j xb > 1\u2212 \u03bejab, \u03be j ab \u2265 0, for j = 1, \u00b7 \u00b7 \u00b7 , l and (a, b) \u2208 Y +\u00b7j \u00d7 Y \u2212\u00b7j .\n(8)\nHere \u03beuvi and \u03be j ab are the slack variables, and \u03bb1, \u03bb2 are the trade-off param-\neters. When both \u03bb1 and \u03bb2 are positive, both label-wise and instance-wise margins are considered. If we set \u03bb1 = 0 (or \u03bb2 = 0), then only the label-wise (or instance-wise) margin is considered."}, {"heading": "4.2. Algorithm", "text": "The objective Eq.(8) is difficult to solve directly because of the large number of constraints and slack variables. For a training set withm instances and l labels, the number of constraints will be O(m2l + ml2), which may exceed memory limit in real-world applicaitons.\nIn order to deal with the computational problem, we solve the Eq.(8) by stochastic gradient descent (SGD). The key point of SGD is to find out a random vector, whose expected value at each iteration equals the gradient direction. We weightedly random samples two kinds of triplets to satisfy this requirement. At each iteration t, we sample a triplet (xti, yiu, yiv) where yiu is relevant and yiv is irrelevant, and a triplet (j,x t a,x t b) where x t a is a positive instance and xtb is a negative instance both on label j. Then use the two triplets to compute the random vector for SGD. The detailed algorithm is presented in Algorithm 1 and the proof that the random vector is an unbiased estimation of the gradient direction is available in Theorem 7.\nTheorem 7. In each iteration (step 5 to step 15) of Algorithm 1, the updated direction of the model is an unbiased estimation of the gradient of Eq.(8).\nProof: Suppose the function in Eq.(8) is f(W ), because W can be decomposed into [w1,w2, \u00b7 \u00b7 \u00b7 ,wq], we consider the partial gradient of a particular wk:\n\u2202f(W )\n\u2202wk =2wk + \u03bb1\u03c61 + \u03bb2\u03c62 = 2wk\n+ \u03bb1\nm \u2211\ni=1\n{\n[[k \u2208 Y \u2212i\u00b7 ]]xi \u2211\nj\u2208Y +i\u00b7\n[[1 \u2212 (wj \u2212wk)\u22a4xi > 0]]\n\u2212 [[k \u2208 Y +i\u00b7 ]]xi \u2211\nj\u2208Y \u2212i\u00b7\n[[1 \u2212 (wk \u2212wj)\u22a4xi > 0]] }\n+ \u03bb2 \u2211\na\u2208Y + \u00b7k\n\u2211\nb\u2208Y \u2212 \u00b7k\n(xb \u2212 xa)[[1 \u2212w\u22a4k (xa \u2212 xb) > 0]]\n(9)\nThe second term \u03bb1\u03c61 is the gradient of label-wise margin on wk, and the third term \u03bb2\u03c62 is the gradient of the instance-wise margin on wk.\nAssume (xi, yik, yij) is picked in step 5 and 6, the direction will be computed in step 8 or 9 according to:\nglabel(xi, yik, yij) =[[k \u2208 Y \u2212i\u00b7 ]]\u03bb1xi[[1 \u2212 (wk \u2212 wj)\u22a4xi > 0]] \u2212 [[k \u2208 Y +i\u00b7 ]]\u03bb1xi[[1\u2212 (wj \u2212 wk)\u22a4xi > 0]] +wk\nAlgorithm 1 LIMO\nInput:\ntrain data matrix X \u2208 Rm\u00d7d, label matrix Y \u2208 {0, 1}m\u00d7l, step size \u03b7 \u2265 0, trade-off parameters \u03bb1 \u2265 0, \u03bb2 \u2265 0, the maximium iteration number T .\nOutput:\nmulti-label model W 1: Initialize W 0 with N(0, 1/ \u221a d) random values 2: Compute the weight vector cinst of each instance, cinsti = |Y +i\u00b7 ||Y \u2212i\u00b7 |/ \u2211m i=1 |Y +i\u00b7 ||Y \u2212i\u00b7 | 3: Compute the weight vector clabel of each label, clabelj = |Y +\u00b7j ||Y \u2212\u00b7j |/ \u2211l j=1 |Y +\u00b7j ||Y \u2212\u00b7j | 4: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 5: Random sample an instance xti using weight c inst 6: Random sample a positive label yiu and a negative label yiv of x t i 7: if 1\u2212w\u22a4uxti +w\u22a4v xti > 0 then 8: wtu = w t\u22121 u \u2212 \u03b7(\u2212\u03bb1xti +wt\u22121u ) 9: wtv = w t\u22121 v \u2212 \u03b7(\u03bb1xti +wt\u22121v )\n10: end if 11: Random sample a label using weight clabel, and the index of label is j 12: Random sample a positive instance xta and a negative instance x t b on label j 13: if 1\u2212w\u22a4j xta +w\u22a4j xtb > 0 then 14: wtj = w t\u22121 j \u2212 \u03b7(\u03bb2(xtb \u2212 xta) +wt\u22121j ) 15: end if\n16: end for 17: W = 1 T \u2211T t=1 W t\nThen do expectation:\nExi [ Eyij [g label(xi, yik, yij)] ]\n= 1\nC Exi\n[\n\u03bb1xi \u2211\nj\u2208Y +i\u00b7\n[[1 \u2212 (wk \u2212 wj)\u22a4xi > 0]]\n\u2212 \u03bb1xi \u2211\nj\u2208Y \u2212i\u00b7\n[[1 \u2212 (wj \u2212 wk)\u22a4xi > 0]] + 1\nD wk\n]\n= 1\nC \u2032 \u03bb1\u03c61 +\n1\nD\u2032 wk,\nwhere C \u2032 and D\u2032 are constants. Similarly, we can prove the expectation of the direction in step 11 to 15:\nExa,xb[g inst(yk,xa,xb)] =\n1\nC \u2032\u2032 \u03bb2\u03c62 +\n1\nD\u2032\u2032 wk\nBecause of the linearity of expectation, and absorbing the constants into \u03bb1 and \u03bb2, the gradient\n\u2202f(W ) \u2202wk\ncan be unbiased estimated. Namely, the updated direction of the algorithm is an unbiased estimation of the gradient of Eq.(8)."}, {"heading": "5. Experiments", "text": "We conduct experiments on eleven multi-label performance measures to show that optimizing the label-wise or the instance-wise margin can lead to different results.\nWe choose five benchmark multi-label datasets1 from different domains in our experiments: (i) A music dataset CAL500, (ii) an email dataset enron, (iii) a clinical text dataset medical, (iv) an image dataset corel5k, (v) a text dataset bibtex. The number of labels in these datasets varies from 45 to 374. We randomly split each dataset into two parts, i.e., 70% for training and 30% for testing. The experiments are repeated ten times, and the averaged results are reported.\nBinary Relevance (BR) (Zhang and Zhou, 2014), ML-kNN (Zhang and Zhou, 2007) and GFM (Waegeman et al., 2014) are provided for comparison. To demonstrate the relationship between margins and performance measures, we degenerate LIMO to only consider either margin by setting the trade-off parameter \u03bb1 or \u03bb2 to zero. LIMO-inst sets \u03bb1 = 0, \u03bb2 = 1 and LIMO-label sets \u03bb1 = 1, \u03bb2 = 0. Typical parameter configurations suggested in respective literatures are used for comparison methods. For BR, L2-regularized SVM (Chang and Lin, 2011) with C=1 is used as base learner. For ML-kNN and GFM, the number of nearest neighbors is 10. For LIMO-inst and LIMOlabel, the step size of SGD is set to 0.01 and suitable thresholds discussed in section 3.2 are used for H-based performance measures. The threshold values are computed by minimizing hamming loss or maximizing F-measure on the training set.\n1http://mulan.sourceforge.net/datasets-mlc.html\nAs summarized in Table 2, some performance measures can be optimized via maximizing label-wise margin, and some can be optimized via maximizing instance-wise margin. In order to show this phenomenon, we group the performance measures into label-wise margin related (hamming loss, ranking loss, average precision, one-error, coverage, instance-F1, instance-AUC ) and instance-wise margin related (hamming loss, macro-F1, macro-AUC ). Because some measures are better when higher, and some measures are better when lower, to demonstrate the results more clearly, we compute the average ranking of each approach on each dataset to denote the performance on either group. And we leave the detailed but hard to convey results in Appendix at the end. For example, on dataset CAL500, LIMO-inst ranks 3rd on hamming loss, ranks 1st on macro-F1 and ranks 1st on macro-AUC, then its average ranking on instance-wise margin related measures is (3+1+1)/3=1.67. Results of label-wise margin related measures are shown in Table 3, while instance-wise margin related measures are shown in Table 4.\nFinally, we set \u03bb1 = 1 and \u03bb2 = 1 (called LIMO) to jointly consider label-wise margin and instance-wise margin. And the ranking results are in Table 5. It is not surprising that LIMO beats LIMO-inst and LIMO-label on the average ranking over all the eleven performance measures. LIMO-label performs better than LIMO-inst mainly because more measures in eleven are label-wise margin related.\nThe experiment supports our theoretical analysis. Although different performance measures focus on different aspects, they share the common property which is formalized in our work as label-wise margin and instancewise margin. In practice, it is recommended to use higher weight (\u03bb1/\u03bb2) on\nspecific margin to optimize the required performance measure and use lower weight on the other to provide regularization.\nThough the performance of LIMO is highly competitive with state-ofthe-art multi-label classification algorithms, it is important to note that the emphasis of our work is to provide a unified understanding of the various performance measures, and LIMO is just an illustration that the unified understanding offers new insights for future algorithm design."}, {"heading": "6. Conclusion", "text": "In this work, we present the effort of analyzing multi-label performance measures in a unified margin view. Our main result is that by maximizing label-wise and instance-wise margins, corresponding performance measures will be optimized. We have also proposed a LIMO approach to maximize the margins, and experimental results verify our theoretical findings.\nOur work provides a new view of multi-label performance measures, disclosing that they share something in common. In the future, it is encouraging to study more effective ways to optimize these margins, which sheds a light on novel multi-label algorithms design."}, {"heading": "Appendix: Detailed Experimental Results", "text": "In this section, detailed experimental results are included. The ranking results in Table 3 is computed from Table 6. The ranking in Table 4 is computed from Table 7, and the ranking in Table 5 is computed from Table 8 and Table 9 . The full table of eleven multi-label performance measures is too wide. Therefore, we split the table into two tables. Table 8 includes first 6 measures and Table 9 includes the results of the last 5 measures."}], "references": [{"title": "Hierarchical multi-label prediction of gene function", "author": ["Z. Barut\u00e7uoglu", "R.E. Schapire", "O.G. Troyanskaya"], "venue": "Bioinformatics 22", "citeRegEx": "Barut\u00e7uoglu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barut\u00e7uoglu et al\\.", "year": 2006}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition", "citeRegEx": "Boutell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boutell et al\\.", "year": 2004}, {"title": "LIBSVM: A library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM TIST", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Consistent multilabel ranking through univariate losses", "author": ["K. Dembczynski", "W. Kotlowski", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "Dembczynski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2012}, {"title": "Regret analysis for performance metrics in multi-label classification: the case of hamming and subset zero-one loss. In: ECML/PKDD", "author": ["K. Dembczynski", "W. Waegeman", "W. Cheng", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "Dembczynski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2010}, {"title": "An exact algorithm for F-measure maximization", "author": ["K. Dembczynski", "W. Waegeman", "W. Cheng", "E. H\u00fcllermeier"], "venue": null, "citeRegEx": "Dembczynski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2011}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": null, "citeRegEx": "Elisseeff and Weston,? \\Q2002\\E", "shortCiteRegEx": "Elisseeff and Weston", "year": 2002}, {"title": "On the consistency of multi-label learning", "author": ["W. Gao", "Zhou", "Z.-H"], "venue": "Artificial Intelligence", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Consistent multilabel classification", "author": ["O. Koyejo", "N. Natarajan", "P. Ravikumar", "I.S. Dhillon"], "venue": null, "citeRegEx": "Koyejo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koyejo et al\\.", "year": 2015}, {"title": "On the optimality of classifier chain for multilabel classification", "author": ["W. Liu", "I.W. Tsang"], "venue": null, "citeRegEx": "Liu and Tsang,? \\Q2015\\E", "shortCiteRegEx": "Liu and Tsang", "year": 2015}, {"title": "Multi-instance multi-label learning in the presence of novel class instances", "author": ["A.T. Pham", "R. Raich", "X.Z. Fern", "J.P. Arriaga"], "venue": "ICML. pp. 2427\u20132435", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning", "citeRegEx": "Schapire and Singer,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer", "year": 2000}, {"title": "Random k-labelsets for multilabel classification", "author": ["G. Tsoumakas", "I. Katakis", "I.P. Vlahavas"], "venue": "IEEE TKDE", "citeRegEx": "Tsoumakas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2011}, {"title": "Semantic annotation and retrieval of music and sound effects", "author": ["D. Turnbull", "L. Barrington", "D.A. Torres", "G.R.G. Lanckriet"], "venue": "IEEE Transactions on Audio, Speech & Language Processing", "citeRegEx": "Turnbull et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Turnbull et al\\.", "year": 2008}, {"title": "On the bayes-optimality of F-measure maximizers", "author": ["W. Waegeman", "K. Dembczynski", "A. Jachnik", "W. Cheng", "E. H\u00fcllermeier"], "venue": "JMLR 15", "citeRegEx": "Waegeman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Waegeman et al\\.", "year": 2014}, {"title": "Optimizing F-measure: A tale of two approaches", "author": ["N. Ye", "K.M.A. Chai", "W.S. Lee", "H.L. Chieu"], "venue": null, "citeRegEx": "Ye et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2012}, {"title": "ML-KNN: A lazy learning approach to multi-label learning", "author": ["M. Zhang", "Z. Zhou"], "venue": "Pattern Recognition", "citeRegEx": "Zhang and Zhou,? \\Q2007\\E", "shortCiteRegEx": "Zhang and Zhou", "year": 2007}, {"title": "LIFT: Multi-label learning with label-specific features", "author": ["Zhang", "M.-L", "L. Wu"], "venue": "IEEE TPAMI", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A review on multi-label learning algorithms", "author": ["Zhang", "M.-L", "Zhou", "Z.-H"], "venue": "IEEE TKDE", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "In text categorization, a document may be associated with a range of topics, such as science, entertainment, news and so on (Schapire and Singer, 2000); In image classification, an image can have both field and mountain tags (Boutell et al.", "startOffset": 124, "endOffset": 151}, {"referenceID": 1, "context": "In text categorization, a document may be associated with a range of topics, such as science, entertainment, news and so on (Schapire and Singer, 2000); In image classification, an image can have both field and mountain tags (Boutell et al., 2004); In gene functional analysis, a gene belongs to the functions of protein synthesis, metabolism", "startOffset": 225, "endOffset": 247}, {"referenceID": 0, "context": "and transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al.", "startOffset": 18, "endOffset": 44}, {"referenceID": 13, "context": ", 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008).", "startOffset": 126, "endOffset": 149}, {"referenceID": 12, "context": "Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000).", "startOffset": 132, "endOffset": 156}, {"referenceID": 11, "context": ", 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000).", "startOffset": 79, "endOffset": 106}, {"referenceID": 0, "context": "and transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008). In the traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000). Because different performance measures focus on different aspects, previous works often analyze few specific measures. Dembczynski et al. (2010) showed that hamming loss and subset 0/1 loss cannot be optimized at the same time.", "startOffset": 19, "endOffset": 1123}, {"referenceID": 0, "context": "and transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008). In the traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000). Because different performance measures focus on different aspects, previous works often analyze few specific measures. Dembczynski et al. (2010) showed that hamming loss and subset 0/1 loss cannot be optimized at the same time. Gao and Zhou (2013) studied the consistency of two performance measures: ranking loss and hamming loss.", "startOffset": 19, "endOffset": 1226}, {"referenceID": 0, "context": "and transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008). In the traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000). Because different performance measures focus on different aspects, previous works often analyze few specific measures. Dembczynski et al. (2010) showed that hamming loss and subset 0/1 loss cannot be optimized at the same time. Gao and Zhou (2013) studied the consistency of two performance measures: ranking loss and hamming loss. They proved that none convex surrogate loss is Bayes consistent with the ranking loss, and gave a consistent surrogate loss function for hamming loss in deterministic case. Ye et al. (2012) pointed out that algorithms for learning to maximize F-measures follow two approaches: the decision-theoretic approach (DTA) and the empirical utility maximization (EUM) approach.", "startOffset": 19, "endOffset": 1500}, {"referenceID": 0, "context": "and transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008). In the traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000). Because different performance measures focus on different aspects, previous works often analyze few specific measures. Dembczynski et al. (2010) showed that hamming loss and subset 0/1 loss cannot be optimized at the same time. Gao and Zhou (2013) studied the consistency of two performance measures: ranking loss and hamming loss. They proved that none convex surrogate loss is Bayes consistent with the ranking loss, and gave a consistent surrogate loss function for hamming loss in deterministic case. Ye et al. (2012) pointed out that algorithms for learning to maximize F-measures follow two approaches: the decision-theoretic approach (DTA) and the empirical utility maximization (EUM) approach. Waegeman et al. (2014) studied the DTA consistent F-measure optimization in multi-label classification setting, and presented a Bayes-optimal algorithm via estimating parameters of the joint distribution.", "startOffset": 19, "endOffset": 1703}, {"referenceID": 0, "context": "and transcription (Barut\u00e7uoglu et al., 2006); In music information retrieval, a piece of music could convey various messages such as classic, piano and passionate (Turnbull et al., 2008). In the traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of multi-label performance measures focused on different aspects have been proposed, such as micro-F1, macro-F1 (Tsoumakas et al., 2011), hamming loss, ranking loss, one-error, average precision and coverage (Schapire and Singer, 2000). Because different performance measures focus on different aspects, previous works often analyze few specific measures. Dembczynski et al. (2010) showed that hamming loss and subset 0/1 loss cannot be optimized at the same time. Gao and Zhou (2013) studied the consistency of two performance measures: ranking loss and hamming loss. They proved that none convex surrogate loss is Bayes consistent with the ranking loss, and gave a consistent surrogate loss function for hamming loss in deterministic case. Ye et al. (2012) pointed out that algorithms for learning to maximize F-measures follow two approaches: the decision-theoretic approach (DTA) and the empirical utility maximization (EUM) approach. Waegeman et al. (2014) studied the DTA consistent F-measure optimization in multi-label classification setting, and presented a Bayes-optimal algorithm via estimating parameters of the joint distribution. Koyejo et al. (2015) studied the EUM optimal multi-label classifier for F-measure, including instance-, micro- and macro-averages.", "startOffset": 19, "endOffset": 1906}, {"referenceID": 9, "context": "These F-measures are popluar both in algorithm evaluation (Liu and Tsang, 2015) and theoretical analysis (Dembczynski et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 5, "context": "These F-measures are popluar both in algorithm evaluation (Liu and Tsang, 2015) and theoretical analysis (Dembczynski et al., 2011) (Koyejo et al.", "startOffset": 105, "endOffset": 131}, {"referenceID": 8, "context": ", 2011) (Koyejo et al., 2015).", "startOffset": 8, "endOffset": 29}, {"referenceID": 5, "context": "The first five measures (hamming loss, ranking loss, one-error, coverage, average precision) are considered in Schapire and Singer (2000) and a multitude of works, e.", "startOffset": 111, "endOffset": 138}, {"referenceID": 5, "context": "The first five measures (hamming loss, ranking loss, one-error, coverage, average precision) are considered in Schapire and Singer (2000) and a multitude of works, e.g., Zhang and Wu (2015) and Dembczynski et al.", "startOffset": 111, "endOffset": 190}, {"referenceID": 3, "context": ", Zhang and Wu (2015) and Dembczynski et al. (2012). The next six measures are F-measure and AUC (the Area Under the ROC Curve) extensions in multi-label classification via different averaging strategies.", "startOffset": 26, "endOffset": 52}, {"referenceID": 3, "context": ", Zhang and Wu (2015) and Dembczynski et al. (2012). The next six measures are F-measure and AUC (the Area Under the ROC Curve) extensions in multi-label classification via different averaging strategies. These F-measures are popluar both in algorithm evaluation (Liu and Tsang, 2015) and theoretical analysis (Dembczynski et al., 2011) (Koyejo et al., 2015). AUC s are used for algorithm evaluation such as in Pham et al. (2015) Zhang and Wu (2015).", "startOffset": 26, "endOffset": 430}, {"referenceID": 3, "context": ", Zhang and Wu (2015) and Dembczynski et al. (2012). The next six measures are F-measure and AUC (the Area Under the ROC Curve) extensions in multi-label classification via different averaging strategies. These F-measures are popluar both in algorithm evaluation (Liu and Tsang, 2015) and theoretical analysis (Dembczynski et al., 2011) (Koyejo et al., 2015). AUC s are used for algorithm evaluation such as in Pham et al. (2015) Zhang and Wu (2015).", "startOffset": 26, "endOffset": 450}, {"referenceID": 6, "context": ", Elisseeff and Weston (2002)).", "startOffset": 2, "endOffset": 30}, {"referenceID": 16, "context": "Binary Relevance (BR) (Zhang and Zhou, 2014), ML-kNN (Zhang and Zhou, 2007) and GFM (Waegeman et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 14, "context": "Binary Relevance (BR) (Zhang and Zhou, 2014), ML-kNN (Zhang and Zhou, 2007) and GFM (Waegeman et al., 2014) are provided for comparison.", "startOffset": 84, "endOffset": 107}, {"referenceID": 2, "context": "For BR, L2-regularized SVM (Chang and Lin, 2011) with C=1 is used as base learner.", "startOffset": 27, "endOffset": 48}], "year": 2016, "abstractText": "Multi-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than single-label setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures will be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results verify our theoretical findings.", "creator": "LaTeX with hyperref package"}}}