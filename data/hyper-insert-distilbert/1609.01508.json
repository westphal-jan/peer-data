{"id": "1609.01508", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Low-rank Bandits with Latent Mixtures", "abstract": "so we study the task domain of maximizing hidden rewards from recommending items ( critical actions ) loyal to users sequentially interacting successfully with at a recommender system. users strategies are automatically modeled as many latent reaction mixtures of c against many representative primary user feature classes, where each consumption class domain specifies a mean compensation reward profile across actions. both the user features ( standard mixture distribution over classes ) vectors and the matching item features ( probability mean reward preferences vector per value class ) are thus unknown a vis priori. the total user identity is the apparent only contextual preference information available assigned to the learner while clearly interacting. simply this induces therefore a low - rank structure dwelling on the matrix of expected rewards ratio r a, instance b from recommending item k a to user b. the problem reduces to proving the aforementioned well - generally known linear alternative bandit when either specified user attributes or variable item - side features are perfectly known. in the setting is where each conscious user, with and its appropriately stochastically sparse sampled taste profile, if interacts only only for a small number more of consumption sessions, we initially develop a bandit algorithm prepared for proving the two - opposing sided uncertainty.... it eventually combines the statistical robust tensor power suppression method of anandkumar babu et al. ( 2014b ) with, the results oful linear bandit algorithm of abbasi - yadkori et al. ( fischer 2011 ). \u2026 we provide the first rigorous dir regret distributed analysis of this explicit combination, showing improvement that its regret response after t user interactions is $ \\ quick tilde o ( instance c \\ weak sqrt { # bt } ) $, with loop b the number of beneficial users. whereas an ingredient strategy towards this singular result is such a locally novel attribute robustness confidence property r of oful, matrix of independent interest.", "histories": [["v1", "Tue, 6 Sep 2016 12:01:30 GMT  (74kb)", "http://arxiv.org/abs/1609.01508v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aditya gopalan", "odalric-ambrym maillard", "mohammadi zaki"], "accepted": false, "id": "1609.01508"}, "pdf": {"name": "1609.01508.pdf", "metadata": {"source": "CRF", "title": "Low-rank Bandits with Latent Mixtures", "authors": ["Aditya Gopalan"], "emails": ["ADITYA@ECE.IISC.ERNET.IN", "ODALRICAMBRYM.MAILLARD@INRIA.FR", "ZAKI@ECE.IISC.ERNET.IN"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n01 50\n8v 1\n[ cs\n.L G\n\u221a BT ), with B the number of users. An ingredient towards this result is a novel robustness\nproperty of OFUL, of independent interest.\nKeywords: Multi-armed bandits, online learning, low-rank matrices, recommender systems, reinforcement learning.\n1. Introduction\nRecommender systems aim to provide targeted, personalized content recommendations to users by learning their responses over time. The underlying goal is to be able to predict which items a user might prefer based on preferences expressed by other related users and items, also known as the principle of collaborative filtering.\nA popular approach to model preferences expressed by users in recommender systems is via probabilistic mixture models or latent class models (Hofmann and Puzicha, 1999; Kleinberg and Sandler, 2004). In such a mixture model, we have a set of A items (content) that can be recommended to B users (consumers). Whenever item a is recommended to user b, the system gains an expected reward of ra,b. The key structural assumption that captures the relationship between users\u2019 preferences is that there exists a set of latent set of C representative user types or typical taste profiles.\nFormally, each taste profile c is a unique vector uc \u2261 (ua,c)a of the expected rewards that every item a elicits under the taste profile. Each user b is assumed to sample one of the typical profiles randomly using an individual probability distribution vb \u2261 (vb,c)c; its reward distribution across the items subsequently becomes that induced by the assumed profile.\nOur focus is to address the sequential optimization of net reward gained by the recommender, without any prior knowledge of either the latent user classes or users\u2019 mixture distributions. Assuming that users arrive to the system repeatedly following an unknown stochastic process and re-sample their profiles over time, according to their respective unknown mixtures across latent classes, we seek online learning strategies that can achieve low regret relative to the best single item that can be recommended to each user. Note that this is qualitatively different than the task of estimating latent classes or user mixtures in a batch fashion, well-studied by now (Sutskever et al., 2009; Anandkumar et al., 2014a,b); the task of simultaneously optimizing net utility in a bandit fashion in complex expression models like these has received little or no analytical treatment. Our work takes a step towards filling this void.\nAn especially challenging aspect of online learning in recommender systems is the relatively meager number of available interactions with a same user, which is offset to an extent by the assumption that users can only have a limited number of taste profiles (classes). Indeed, if one can identify the class to which a certain user belongs and aggregate information from all other users in that class, then one can recommend to the user the best item for the class. In practice, classes are latent and not necessarily known in advance, and several works (Gentile et al., 2014; Lazaric et al., 2013; Maillard and Mannor, 2014) study the restricted situation when each user always belongs to one specific class (i.e., when all mixture distributions have support size 1). We go two steps further, since in many situations (a) users cannot be assumed to belong to one class only, such as when a user account is shared by several individuals (e.g. a smart-TV), and (b) the duration of a user-session, that is the number of consecutive recommendations to the same individual connected to a user-account, cannot assumed to be long1.\nThe key challenges that this work addresses are (1) the lack of knowledge of \u201cfeatures\u201d on both the user-side and item-side in a linear bandit problem (in this case, both the user mixture weights and the item class reward profiles) and (2) provable regret minimization with very few i.e. O(1) interactions with every user b having a specific taste profile, as opposed to a large number of interactions such as in transfer learning (Lazaric et al., 2013).\nContributions and overview of results. We consider a setting when users are assumed to come from arbitrary mixtures across classes (they are not assumed to fall perfectly in one class as was the assumption in works by Gentile et al. (2014); Maillard and Mannor (2014)). We develop a novel bandit algorithm (Algorithm 3) that combines (a) the Optimization in the Face of Uncertainty Linear bandit OFUL algorithm (Abbasi-Yadkori et al., 2011) for bandits with known action features, and (b) a variant of the Robust Tensor Power (RTP) algorithm (Anandkumar et al., 2014b) that uses only bandit (partial) estimates of latent user classes with observations coming from a mixture model. More specifically, we introduce a subroutine (Algorithm 1) that makes use of the RTP method to extract item-side attributes (U ) and, contributing to its theoretical analysis, show a recovery property (Theorem 1). Note that the RTP method ideally requires (unbiased) estimates of the 2nd and 3rd order moments of actions\u2019 rewards, but with bandit information the learner can access only partial reward information, i.e., a single reward sample from an action. To overcome this, we devise an importance sampling scheme across 3 successive time instants to build the 2nd and 3rd order moment tensor estimates that RTP uses. For the task of issuing recommendations, we develop an algorithm (section 4), essentially based on OFUL, instantiated per user, using for each a the estimated latent class vectors {ua,c}c (obtained via the RTP subroutine) as arm features, and uncertain parameter vector to be learned vb.\n1. It is also unlikely to be very short, say, less than 3.\nWe carry out a rigorous analysis of the algorithm and show that it achieves regret O\u0303(\u2113C \u221a BT ) in T rounds of interaction (Theorem 4), provided each arriving user interacts with the system for \u2113 > 3 rounds with the same profile. In comparison, the regret of the strategy that completely disregards the latent mixture structure of rewards and employs a standard bandit strategy (e.g. UCB (Auer et al., 2002)) per user, scales as O(B \u221a TA/B) = O( \u221a ABT ) after T rounds2 , which is considerably suboptimal in the practical case with a very large number of items but very few representative user classes (C \u226a A). It is also worth noting that the regret bound we achieve, order-wise, is what would result from applying the OFUL or any optimal linear bandit algorithm assuming a priori knowledge of all latent user classes {ua,c}a,c, that is O\u0303(\u2113C \u221a BT ). In this sense, our result shows that one can simultaneously estimate features on both sides of a bilinear reward model and achieve regret performance equivalent to that of a one-sided linear model, which is the first result of its kind to the best of our knowledge3. Our results are presented for finite time horizons with explicit details of the constants arising from the error analysis of RTP, which at this point are large but possibly improvable.\nEn route to deriving the regret for our algorithm, we also make a novel contribution that advances the theoretical understanding of OFUL, and which is of independent interest. We show that in the standard linear bandit setting, where the expected reward of an arm linearly depends\non d features, OFUL yields (sub-linear) O\u0303 ( \u03c1d \u221a T ) regret even when it makes decisions based on\nperturbed or inexact feature vectors (Theorem 3), where \u03c1 quantifies the distortion. This property holds whenever the perturbation error is small enough, and we explicitly give both (a) a sufficient condition on the size of the perturbation in terms of the set of actual features, and (b) a bound on the (multiplicative) distortion \u03c1 in the regret due to the perturbation (note that \u03c1 = 1 in the ideal linear case).\n2. Setup and notation\nFor any positive integer n, [n] denotes the set {1, 2, . . . , n}. At each n \u2208 N, nature selects a user bn \u2208 [B] according to the probability distribution \u03b2 over [B], independent of the past, and bn is revealed to the learner. A user class cn is subsequently sampled from the probability distribution vbn over [C], and cn (the assumed class of user bn) interacts with the learner for the next \u2113 > 3 consecutive steps. Such an interaction will often be termed a mini-session.\nIn each step l \u2208 [\u2113] of a mini-session, the learner plays an action (issues a recommendation) an,l \u2208 [A] and subsequently receives reward Yn,l = uan,l,cn + \u03b7n,l, where \u03b7n,l is a (centered) R-sub-Gaussian i.i.d. random variable independent from an,l, cn, representing the noise in the reward. We let ua \u2208 RC represent the vector (ua,c)c\u2208[C] of the mean rewards from action a in each class. Note that E[uan,l,cn |an,l] = E[u\u22a4an,lvbn |an,l]. For convenience, we use the index notation t \u2261 (n, l) and introduce T = N\u2113, where N is the total number of mini-sessions, and T the total number of interactions of the learner with the system. We denote likewise Yt, at, ct, \u03b7t for Yn,l, an,l, cn, \u03b7n,l, and let umax def = maxa\u2208[A],c\u2208[C] |ua,c|.\nWe are interested in designing an online recommendation strategy, i.e., one that plays actions depending on past observations, achieving low (cumulative) regret after T \u2261 (N, \u2113) mini-sessions, defined as RT def= \u2211 n\u2208[N ],l\u2208[\u2113] rn,l, where rn,l def = maxa\u2208[A] u \u22a4 a vbn \u2212 u\u22a4an,lvbn . In other words, we wish to compete against a strategy that plays for every user an action yielding the highest reward in expectation under its mixture distribution over user classes.\n2. Roughly, each UCB per-user plays from a pool of A actions for about T/B rounds, thus suffering regret O( \u221a\nA(T/B)). 3. An earlier result of Djolonga et al. (2013) gets O(T 4/5) regret while moreover assuming a perfect control of the\nsampling process (we can\u2019t assume this due to the user arrivals).\n3. Recovering latent user classes: The EstimateFeatures subroutine\nIn this section, we provide an estimation algorithm for the matrix U , using the RTP method.4\nEstimation of tensors. We assume that in mini-session n, when interacting with user bn, the triplet {an,l}l6\u2113 is chosen from a distribution pn(a, a\u2032, a\u2032\u2032|bn). Letting Xan,l,bn,n,l def = Yn,l = uan,l,cn + \u03b7n,l to explicitly indicate the active user and action chosen at (n, l), we form the importance-weighted estimates\nr\u0303a,a\u2032,n def =\n1\nn\nn\u2211\ni=1\nXai,1,bi,i,1Xai,2,bi,i,2\npi(a, a\u2032|bi) I{ai,1 = a, ai,2 = a\u2032},\nr\u0303a,a\u2032,a\u2032\u2032,n def =\n1\nn\nn\u2211\ni=1\nXai,1,bi,i,1Xai,2,bi,i,2Xai,3,bi,i,3\npi(a, a\u2032, a\u2032\u2032|bi) I{ai,1 = a, ai,2 = a\u2032, ai,3 = a\u2032\u2032} .\nfor the second and third-order tensors5. We introduce the matrices M\u0302n,2 \u2261 (r\u0303a,a\u2032,n)a,a\u2032\u2208[A] and M2 \u2261 (ma,a\u2032)a,a\u2032\u2208[A] with ma,a\u2032 def= E[r\u0303a,a\u2032,n], and the tensors M\u0302n,3 \u2261 (r\u0303a,a\u2032,a\u2032\u2032,n)a,a\u2032,a\u2032\u2032\u2208[A] and M3 \u2261 (ma,a\u2032,a\u2032\u2032)a,a\u2032,a\u2032\u2032\u2208[A] with ma,a\u2032,a\u2032\u2032 def = E[r\u0303a,a\u2032,a\u2032\u2032,n]. The following result decomposes the matrix M2 and tensor M3 as weighted sums of outer products.\nLemma 1 When the user arrivals are i.i.d. according to the law \u03b2, i.e., bi i.i.d\u223c \u03b2 \u2200i \u2208 [n], it holds that\nma,a\u2032,n = \u2211\nc\u2208[C] v\u03b2,cua,cua\u2032,c, and\nma,a\u2032,a\u2032\u2032,n = \u2211\nc\u2208[C] v\u03b2,cua,cua\u2032,cua\u2032\u2032,c .\nHaving shown the unbiasedness of the empirical 2nd and 3rd moment tensors M\u0302n,2 and M\u0302n,3, we next turn to showing concentration to their respective means.\nLemma 2 Assuming that pi(a, a\u2032|bi) > q2,i and pi(a, a\u2032, a\u2032\u2032|bi) > q3,i for deterministic q2,i, q3,i, for all i \u2208 N, a, a\u2032, a\u2032\u2032 \u2208 [A], then for all n 6 N , with probability higher than 1\u2212 \u03b4, it holds simultaneously for all a, a\u2032, a\u2032\u2032 that\n|r\u0303a,a\u2032,n \u2212ma,a\u2032,n| 6\n\u221a\u221a\u221a\u221a n\u2211\ni=1\nq\u221222,i log(4A2/\u03b4)\n2n2 ,\n|r\u0303a,a\u2032,n \u2212ma,a\u2032,a\u2032\u2032,n| 6\n\u221a\u221a\u221a\u221a n\u2211\ni=1\nq\u221223,i log(4A3/\u03b4)\n2n2 .\nAn immediate corollary is the following one:\nCorollary 1 Provided that q2,i = \u03b3i/A2 and q3,i = \u03b3i/A3 for some \u03b3i > 0, then on an event of probability higher than 1\u2212 \u03b4, the following hold simultaneously:\ne(2)n def = \u2016M\u0302n,2 \u2212M2\u2016 6 A3\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A2/\u03b4)\n2n2 ,\ne(3)n def = \u2016M\u0302n,3 \u2212M3\u2016 6 A9/2\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2 .\n4. We consider \u2113 = 3 to describe the algorithm; \u2113 > 3 is easily handled by repeating the 3-wise sampling p(a, a\u2032, a\u2032\u2032) for \u230a\u2113/3\u230b times and discarding the remaining (< 3) steps in the mini-session during exploration (leading to a negligible regret overhead). 5. An alternative is the implicit exploration method due to Koca\u0301k et al. (2014).\nAlgorithm 1 EstimateFeatures 1: Input: #sessions n; #mini-sessions \u2113; (user, action, reward) tuples\n(bi, ai,l,Xai,l ,bi,i,l)16i6n,16l6\u2113.\n2: Compute the A \u00d7 A matrix M\u0302n,2 = (r\u0303a,a\u2032,n)a,a\u2032\u2208[A] and the A \u00d7 A \u00d7 A tensor M\u0302n,3 = (r\u0302a,a\u2032,a\u2032\u2032,)a,a\u2032,a\u2032\u2032\u2208[A]. 3: Compute a A\u00d7 C whitening matrix W\u0302n of M\u0302n,2 {Take W\u0302n = U\u0302nD\u0302\u22121/2n where D\u0302n is the C \u00d7 C diagonal matrix with the top C eigenvalues of M\u0302n,2, and U\u0302n the A\u00d7 C matrix of corresponding eigenvectors.} 4: Form the C \u00d7 C \u00d7 C tensor T\u0302n = M\u0302n,3(W\u0302n, W\u0302n, W\u0302n). 5: Apply the RTP algorithm (Anandkumar et al., 2014b) to T\u0302n, and compute its robust eigenvalues\n(\u03bb\u0302n,c)c\u2208[C] with eigenvectors (\u03d5\u0302n,c)c\u2208[C]. {The paper of Anandkumar et al. (2014b, Sec. 4) defines eigenvalues/eigenvectors of tensors.}\n6: Compute for each c \u2208 [C], un,c = \u03bbn,c(W\u0302\u22a4n )\u2020\u03d5\u0302n,c and vn,c = \u03bb\u22122n,c. 7: Output: Estimate of latent classes U : The A\u00d7C matrix Un obtained by stacking the vectors\nun,c \u2208 RA side by side.\nReconstruction algorithm. The EstimateFeatures algorithm (Algorithm 1) employs a whitening matrix W\u0302n, of the empirical estimate of the matrix M2, to build the empirical tensor T\u0302n. This tensor is then used to recover the columns of the matrix U = (ua,c)a\u2208[A],c\u2208[C] via the RTP algorithm. For the sake of completeness, we also introduce W , a whitening matrix of M2 (i.e., WTM2W = I), the corresponding tensor T = M3(W,W,W ), and finally the estimation error en def = \u2016T\u0302n \u2212 T \u2016. Reconstruction guarantee. Our next result makes use of the following proposition from Anandkumar et al. (2014b, Theorem 5.1), restated here for completeness.\nProposition 1 (Theorem 5.1 of Anandkumar et al. (2014b)) Let T\u0302 = T + E \u2208 RC\u00d7C\u00d7C , where T is a symmetric tensor with orthogonal decomposition T = \u2211C c=1 \u03bbc\u03d5 \u22973 c , where each \u03bbc > 0, {\u03d5c}c\u2208[C] is an orthonormal basis, and E is a symmetric tensor with operator norm ||E|| 6 \u03b5. Let \u03bbmin = min{\u03bbc : c \u2208 [C]}, \u03bbmax = max{\u03bbc : c \u2208 [C]}. Run the RTP algorithm with input T\u0302 for C iterations. Let {(\u03bb\u0302c, \u03d5\u0302c)}c\u2208[C] be the corresponding sequence of estimated eigenvalue/eigenvector pairs returned. Then, there exist universal constants C1, C2 > 0 for which the following is true. Fix \u03b7 \u2208 (0, 1) and run RTP with parameters (i.e., number of iterations) L,N with L = poly(C) log(1/\u03b7), and N > C2 ( log(C) + log log ( \u03bbmax \u03b5 )) . If \u03b5 6 C1 \u03bbmin C , then with probability at least 1\u2212 \u03b7, there exists a permutation \u03c0 \u2208 SC such that\n\u2200c \u2208 [C] : |\u03bbc \u2212 \u03bb\u0302\u03c0(c)| 6 5\u03b5, ||\u03d5c \u2212 \u03d5\u0302\u03c0(c)|| 6 8\u03b5/\u03bbc,\nand ||T \u2212 C\u2211\nc=1\n\u03bb\u0302c\u03d5\u0302 \u22973 c || 6 55\u03b5 .\nLemma 1 gives a decomposition of the (symmetric) tensor M3, but it may be not orthogonal; standard transformation (Anandkumar et al., 2014b, Sec. 4.3) gives an orthogonal decomposition for the tensor6 M3(W,W,W ), with W a matrix that whitens M2. We can thus use Proposition 1\n6. For a 3rd order tensor A \u2208 Ra\u00d7a\u00d7a and 2nd order tensor or matrix B \u2208 Ra\u00d7b, A(B,B,B) \u2208 Rb\u00d7b\u00d7b is the 3rd order tensor defined by [A(B,B,B)]i1,i2,i3 def = \u2211\nj1,j2,j3\u2208[n] Aj1,j2,j3Bj1,i1Bj2,i2Bj3,i3 . See Anandkumar et al.\n(2014b) for more details on notation and results.\nwith T = M3(W,W,W ), T\u0302 = T\u0302n, \u03b5 = en and \u03b7 = \u03b4 in order to prove the following guarantee (Theorem 1) on the recovery error between columns of U and their estimate.\nWe now introduce mild separability conditions on the mixture weights vb and the spectrum of the 2nd moment matrix M2 needed for the reconstruction guarantee to hold, similar to those assumed for Lazaric et al. (2013, Theorem 2).\nAssumption 1 There exist positive constants vmin, \u03c3min, \u03c3max and \u0393 such that\nmin b\u2208[B],c\u2208[C] vb,c > vmin, \u2200c \u2208 [C], \u03c3c = \u221a \u03bbc(M2) \u2208 [\u03c3min, \u03c3max] and\nmin c 6=c\u2032\u2208[C]\u00d7[C]\n|\u03c3c \u2212 \u03c3c\u2032 | > \u0393 ,\nwhere \u03bbc(A) denotes the cth top eigenvalue of A.\nTheorem 1 (Recovery guarantee for online estimation of user classes U ) Let Assumption 1 hold, and let \u03b4 \u2208 (0, 1). If the number of mini-session satisfies\nn2\u2211n i=1 \u03b3 \u22122 i > max\n{ 2A6 log(4A2/\u03b4)\nmin{\u0393, \u03c3min}2 , A9(1 + 10( 1\u0393 + 1 \u03c3min )(1 + u3max)) 2C5 log(4A3/\u03b4) 2C21\u03c3 3 min\n} ,\nthen with probability at least 1 \u2212 2\u03b4, there exists some permutation \u03c0 \u2208 SC such that for all c \u2208 [C], the output U\u0304n of the EstimateFeatures algorithm satisfies\n||uc \u2212 un,\u03c0(c)|| 6 3A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 . (1)\nwhere uc = (ua,c)a\u2208[A]. Here, the constant (we use the \u201ddiamond\u201d symbol to denote it) is\n3 = ( CA \u03c3min\n)3/2 ( 13 \u221a \u03c3max + 4 \u221a 2min{\u0393, \u03c3min}\n+ 5 ( \u03c3max \u0393 + 1\n2\u03c3max\n) min{\u0393, \u03c3min} ) \u2135\n+\n( 2\u03c3max\n\u0393 +\n1\n\u03c3max\n) 1\nv2min\n+ 5 \u221a 3/8 (\u221a \u03c3max + \u221a min{\u0393, \u03c3min}/2 )(2CA \u03c3min )3 \u21352 min{\u0393, \u03c3min} ,\nwith the notation \u2135 = 1 + 10( 1\u0393 + 1\u03c3min )(1 + u 3 max).\nThe proof strategy follows that of Lazaric et al. (2013, Theorem 2) and is detailed in the appendix for clarity. It consists in relating, on the one hand, the estimation errors e(2)n of M2 and e (3) n of M3 from Corollary 1 to the condition \u03b5 6 C1 \u03bbminC , and, on the other hand, relating the reconstruction error on the columns of U to the control on the terms |\u03bbc \u2212 \u03bb\u0302\u03c0(c)| and ||\u03d5c \u2212 \u03d5\u0302\u03c0(c)|| coming from Proposition 1. We note that the bound appearing in the condition on the number of mini-sessions is potentially large (due to the terms A6, C5, etc.). This is due to the combination of the RTP method with the importance sampling scheme, and it remains unclear if the bound can be significantly improved within this framework.\n4. Recovering latent mixture distributions (vb): robustness of the OFUL algorithm\nIn order to recover the weights vectors vb \u2208 RC and thus the matrix V , it would be tempting to use again an instance of the RTP method but this time to aggregate across actions, i.e., by forming a B\u00d7B and B\u00d7B\u00d7B tensor. Unfortunately, aggregation of elements of U fails for two reasons: First, we do not have different views across users b, contrary to what we have for actions a. It is thus hopeless to be able to form an estimate of the 2nd and 3rd moment tensors as before. Second, and rather technically, convex combinations of the {ua,c}a\u2208[A] need not be positive. This prevents the application of the RTP method which requires positive weights to work.\nWe thus consider a different strategy that uses an algorithm designed for linear bandits. However since the feature matrix U is unknown a priori and can only be estimated, we need to work with perturbed features. A first solution is to propagate the additional error resulting from the error on the features in the standard proof of OFUL. However, this leads to a sub-optimal regret that is no longer scaling as O\u0303( \u221a T ) with the time horizon. We overcome this hurdle by showing in Theorem 3 a robustness property of OFUL of independent interest, which aids us in controlling the regret of the overall latent class algorithm (Algorithm 3).\nConsider OFUL run with perturbed (not necessarily linearly realizable) rewards. Formally, consider a finite action set A = {1, 2, . . . , A} and distinct feature vectors {u\u0304a \u2208 RC\u00d71}a\u2208A. Let U\u0304\u22a4 := [u\u03041 u\u03042 . . . u\u0304A] \u2208 RC\u00d7A. The expected reward when playing action At = a at time t is denoted by ma := E [ Yt \u2223\u2223 At = a ] , with m := (ma)a\u2208A. Let us assume that there exists a unique optimal action for the expected rewards m, i.e., argmaxa\u2208A ma = {a\u22c6}, with the regret at time n being Rn := \u2211n t=1 (ma\u22c6 \u2212mAt). The key point here is that m need not be linearly realizable w.r.t. the actions\u2019 features \u2013 we will not require that min v\u2208RC \u2225\u2225m\u2212 U\u0304v \u2225\u2225 be 0.\nAlgorithm 2 OFUL (Optimism in Face of Uncertainty for Linear bandits) (Abbasi-Yadkori et al., 2011)\nRequire: Arms\u2019 features U\u0304 , regularization parameter \u03bb, norm parameter R\u0398 for all times t > 1 do\n1. Form the C \u00d7 (t\u2212 1) matrix U\u03041:t\u22121 := [u\u0304A1 u\u0304A2 . . . u\u0304At\u22121 ] consisting of all arm features played up to time t\u2212 1, and Y1:t\u22121 := (Y1, . . . , Yt\u22121)\u22a4. Set Vt\u22121 := \u03bbI + \u2211t\u22121 s=1 u\u0304As u\u0304 \u22a4 As\n. 2. Choose the action\nAt \u2208 argmax a\u2208A max v\u2208Ct\u22121 u\u0304 \u22a4 a v, where\nCt\u22121 := {v \u2208 RC : \u2016v \u2212 v\u0302t\u22121\u2016Vt\u22121 6 Dt\u22121},\nDt\u22121 := R\n\u221a 2 log ( det(Vt\u22121)1/2\u03bb\u2212C/2\n\u03b4\n) +\u03bb1/2R\u0398\nv\u0302t\u22121 := V \u22121 t\u22121U\u03041:t\u22121Y1:t\u22121..\nend for\nOFUL Regret with linearly realizable rewards. The OFUL algorithm is stated for the sake of clarity as Algorithm 2. Before studying the linearly non-realizable case, we record the wellknown regret bound for it in the unperturbed case, that is when \u2200a \u2208 [A],ma = u\u0304\u22a4a v\u22c6 for some unknown v\u22c6.\nTheorem 2 (OFUL regret (Abbasi-Yadkori et al., 2011)) Assume that ||v\u22c6||2 6 R\u0398, and that for all a \u2208 A, ||u\u0304a||2 6 RX , |\u3008u\u0304a,v\u22c6\u3009| 6 1. Then with probability at least 1\u2212 \u03b4, the regret of OFUL satisfies: \u2200n > 0,\nRn 6 4 \u221a nC log(1 + nR2X /(\u03bbC) \u00d7\n( \u03bb1/2R\u0398 +R \u221a 2 log(1/\u03b4) + C log(1 + nR2X /(\u03bbC)) ,\nprovided that the regularization parameter \u03bb is chosen such that \u03bb > max { 1, R2X , 1/R 2 \u0398 } .\nRegret of OFUL with Perturbed Features. We make a structural definition to present the\nresult. Let \u03b1(U\u0304 ) := maxJ \u2225\u2225A\u22121J \u2225\u2225 2 , where A = [ U\u0304 IC ] \u2208 R(A+C)\u00d7C , AJ is the C \u00d7 C submatrix of A formed by picking rows J , and J ranges over all size-C subsets of full-rank rows of A. We will require for our purposes that \u03b1(U\u0304T) is not too large. For intuition regarding \u03b1, we refer to Forsgren (1996) (the final 3 paragraphs of p. 770, Corollary 5.4 and section 7). We remark that the condition that \u03b1(U\u0304T) be small is analogous to a \u03b3-incoherence type property commonly used in prior work (Bresler et al., 2014, Assumption A2), stating that two distinct feature vectors uc and uc\u2032 , c 6= c\u2032, must have a minimum angle separation.\nLet v\u25e6 \u2208 RC be arbitrary with \u21132 norm at most R\u0398 (it helps to think of U\u0304v\u25e6 as an approximation of m), \u03b5a := ma \u2212 u\u0304\u22a4a v\u25e6, \u03b5 := (\u03b5a)a\u2208A \u2208 RA. We now state a robustness result for OFUL potentially of independent interest.\nTheorem 3 (OFUL robustness property) Suppose ||v\u25e6||26R\u0398, \u03bb>max { 1, R2X , 1/4R 2 \u0398 } , \u2200a\u2208A, ||u\u0304a||2 6 RX and |ma| 6 1. If the deviation from linearity satisfies\n\u2016\u03b5\u20162 \u2261 \u2225\u2225m\u2212 U\u0304v\u25e6 \u2225\u2225 2 < min a 6=a\u22c6 u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 2\u03b1(U\u0304\u22a4) \u2016u\u0304a\u22c6 \u2212 u\u0304a\u20162 , (2)\nthen, with probability at least 1\u2212 \u03b4 for all T > 0,\nRT 6 8\u03c1 \u2032 \u221a TC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) ,\nwhere \u03c1\u2032 := max { 1,maxa 6=a\u22c6 ma\u22c6\u2212ma u\u0304 \u22a4\na\u22c6 v \u25e6\u2212u\u0304\u22a4a v\u25e6\n} .\nTheorem 3 essentially states that when the deviation of the actual mean reward vector from the subspace spanned by the feature vectors is small, the OFUL algorithm continues to enjoy a favorable O( \u221a T ) regret up to a factor \u03c1\u2032 > 1. The quantity \u03c1\u2032 in the result is a geometric measure of the distortion in the arms\u2019 actual rewards m with respect to the (linear) approximation U\u0304v\u25e6. We control this quantity in the next paragraph. (Note that \u03c1\u2032 = 1 in the perfectly linearly realizable case \u03b5 = 0, and this gives back the standard OFUL regret up to a universal multiplicative constant.)\nApplying the Robust analysis of OFUL to the Low-rank Bandit setup. In this paragraph, we translate Theorem 3 to our Low Rank Bandit (LRB) setting in which OFUL uses feature vectors with noisy perturbations (estimated by, say, a Robust Tensor Power (RTP) algorithm). Throughout this section, we fix a user b.\nWe can now translate Theorem 3 thanks to the correspondence with the perturbed OFUL setting: In our low-rank bandit setting, the matrix U\u0304 = U\u0304n depends on the reconstruction algorithm at mini-session n. Moreover, the optimal action a\u22c6 \u2261 a\u22c6b now depends on the user b. We denote for a user b \u2208 [B] the minimum gap across suboptimal actions to be gb def= mina 6=a\u22c6\nb (ua\u22c6 b \u2212 ua)\u22a4vb.\nLikewise, the error vector \u03b5 depends on b, n. Its norm ||\u03b5||2 appears in the condition (2) and the definition of \u03c1, and is controlled by the reconstruction error of Theorem 1. It decays with the number of mini-sessions n.\nWe define \u03b1n def = \u03b1(U\u0304n), \u03b1\u22c6 def = \u03b1(U) and use maxb ||vb|| for R\u0398, Using these notations, and adapting the proof of Theorem 3 to handle a variable Un, we can now translate the result of the perturbed OFUL to our LRB setting:\nLemma 3 Let 0 < \u03b4 6 1 and b \u2208 [B]. Provided that the number of mini-sessions n0 satisfies n 2 0\u2211n0\ni=1 \u03b3 \u22122 i\n>\n9b,\u03b4, where we introduced the notation\n9b,\u03b4 =max\n{ 2A6 log(4A2/\u03b4)\nmin{\u0393, \u03c3min}2 ,\nA9(1 + 10( 1\u0393 + 1 \u03c3min )(1 + u3max)) 2C5 log(4A3/\u03b4)\n2C21\u03c3 3 min\n\u00d7\n3 2A6C2 log(4A3/\u03b4) \u00d7\nmax { 2\u03b12\u22c6,\n8A||vb||22 g2b , 27\u03b12\u22c6Cu 2 max||vb||22 g2b + 1 2\n}} ,\nthen with probability at least 1 \u2212 2\u03b4, ||\u03b5||2 = ||(U \u2212 U\u0304n)vb||2 is small enough that for any n > n0, condition (2) is satisfied. Consequently, Theorem 3 applies with\nR\u0398 = max b ||vb||2, RX = max a\u2208A\n||ua||2 + \u221a A\n2\u03b1\u22c6 , and\n\u03c1\u2032 \u2261 \u03c1\u2032n,b 6 2.\nThus, provided that the total number of mini-sessions of interaction (not necessarily corresponding to interactions with user b) is large enough, then the OFUL algorithm run during interactions with user b will achieve a controlled regret. However, we want to warn that the 9b,\u03b4 resulting from the RTP method, especially the second term of the max, may be potentially large, although being a constant.\n5. Putting it together: Online Recommendation algorithm\nThis section details our main contributions for recommendations in the context of mini-sessions of interactions with unknown mixtures of latent profiles: first Algorithm 3 that combines RTP with OFUL, and then a regret analysis in Theorem 4.\nThe recommendation algorithm we propose (Algorithm 3) uses the RTP method to estimate the matrix U and then applies OFUL to determine an optimistic action. Importantly, it finally outputs a distribution that mixes the optimistic action with a uniform exploration. The mixture coefficient goes to 0 with the number of rounds, thus converging to playing OFUL only. It ensures that the importance sampling weights are bounded away from 0 in the beginning.\nMain analytical result: Regret bound\nTheorem 4 (Regret of Algorithm 3) With Assumption 1 holding, let \u03b4 \u2208 (0, 1), 9\u03b4 = maxb\u2208[B] 9b,\u03b4 (from Lemma 3), and let n0 be the first mini-session at which\nn20\u2211n0 i=1 \u03b3 \u22122 i > 9\u03b4. The regret of Algorithm 3 at time\nT = N\u2113 (acting for N mini-sessions of length \u2113) using internal instances of OFUL parameterized by \u03b4 > 0 satisfies\nE[RT ] 616\n\u221a BTC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n))\n+ \u2113(n0 \u2212 1 + N\u2211\nn=n0\n\u03b3n) + 3\u03b4T ,\nprovided that \u03bb > min{1, R2X , 1/R2\u0398}, with R\u0398 > maxb ||vb||2, RX > maxa\u2208A ||ua||2 + \u221a A 2\u03b1\u22c6 . Consequently, choosing \u03b4 = 1/T and \u03b3n = \u221a log(n+ 1)/n, n \u2208 N, say, yields the orderE[RT ] = O ( C \u221a BT logT ) .\nDiscussion. (1) The regret of Algorithm 3 scales with T similar to that of an OFUL algorithm run with perfect knowledge of the feature matrix U : O\u0303(C \u221a BT ). This is a non-trivial result as U is not assumed to be known a priori and is estimated by Algorithm 3 using tensor methods.\nAlgorithm 3 Per-user OFUL with exploration Require: Parameters \u03bb, R\u0398 for OFUL, exploration rate parameters \u03b3n, n > 1.\n1: for mini-session n = 1, . . . , N do 2: Get user bn. 3: Let pn \u223c Bernoulli(\u03b3n) 4: if pn = 0 then 5: {Carry out an ESTIMATE mini-session} 6: for step k = 1, 2, . . . , \u2113 do 7: Output an,k \u223c Uniform([A]). 8: end for 9: Let Un = EstimateFeatures (Algorithm 1) with input (bi, ai,l,Xai,l,bi,i,l)16i6n,16l6\u2113,pn=0\n{Update feature estimates using samples from previous ESTIMATE mini-sessions} 10: else 11: {Carry out an OFUL mini-session} 12: for step k = 1, 2, . . . , \u2113 do 13: Run one iteration of OFUL (Algorithm 2) with features Un, parameters \u03bb and R\u0398, and\npast actions and rewards (ai,l,Xai,l,bi,i,l), 1 6 i < n, 1 6 l 6 \u2113, for which pi = 1 and bi = bn {An instance of OFUL for each user using current feature estimates, and observed actions and rewards from previous OFUL mini-sessions}\n14: Output action an,k returned by OFUL 15: end for 16: end if 17: end for\n(2) One can also compare the result with the regret of ignoring the mixture (low-rank) structure and simply running an instance of UCB per user, which would scale as O( \u221a ABT ). This becomes highly suboptimal when the number of actions/items A is much larger than the number of user types C, demonstrating the gain from leveraging the mixed linear structure of the problem. Note\nalso that we do not need a specific user to interact for a long time but for as few as \u2113 > 3 consecutive steps, contrary for instance to the transfer method (Lazaric et al., 2013), where a large number of consecutive interaction steps with the same user is required.\n(3) It is worthwhile to contrast the result and approach with that in Djolonga et al. (2013) \u2013 the authors there incur an additional regret term due to the error in approximately estimating the low-rank matrix, which requires additional tuning ending up with a regret of O(T 4/5). On the other hand, we avoid this approximation error by showing and exploiting the robustness property of OFUL, which guarantees \u221a T regret as soon as the estimated features U\u0303 are within a small radius of the actual ones. The result (and analysis) does come with a caveat that the model-dependent term 9\u03b4 , although being independent on the time horizon T , is potentially large. With \u03b3n set as in Theorem 4, it appears as an additive exponential constant term in the regret7. This arises from the RTP method, and it is currently unclear if this term can be significantly reduced with the current line of analysis. Numerical evidence, however, indicates that no such large additive constant enters into the regret (Section 5). Also, on the bright side, note that 9\u03b4 does not need to be known by the algorithm.\nNumerical Results. The performance of the low-rank bandit strategy (Algorithm 3) is shown in Figure 1, simulated for 20 users arriving uniformly at random, 3 user classes and 200 actions. Both the latent class matrix U200\u00d73 the mixture matrix V20\u00d73 are random one-shot instantiations. The proposed algorithm (Algorithm 3), with two different exploration rate schedules O\u0303(n\u22121/2) and O\u0303(n\u22121/3) (\u2019RTP+OFUL(sqrt)\u2019 and \u2019RTP+OFUL(cuberoot)\u2019 in the figure), is compared with (a) basic UCB (\u2019UCB\u2019 in the figure) ignoring the linear structure of the problem (i.e., UCB per-user with 200 actions), (b) OFUL per-user with complete knowledge of the user classes and pn = 1 always, i.e., no exploration mini-sessions, and (c) An implementation of the Alternating Least Squares estimator (Taka\u0301cs and Tikk, 2012; Mary et al., 2014) for the matrix U along with OFUL per-user. The proposed algorithm, with the theoretically suggested exploration O\u0303(n\u22121/2), is observed to exploit the latent structure considerably better than simple UCB, and is not too far from the unrealistic OFUL strategy which enjoys the luxury of latent class information. It is also competitive with performing Alternating Least Squares, which does not come with analytically sound performance guarantees in the bandit learning setting. Also, the large additive constants in the theoretical bounds for Algorithm 3 do not manifest here.\nRelated work. The popular low-rank matrix completion problem studies the recovery U and V given a small number of entries sampled at random from UV T with both U and V being tall matrices, see for instance Jain et al. (2013) and citations therein. However, its setting is different than ours for several reasons. It typically deals with batch data arising from a sampling process that is not active but uniform across entries of UV T . Further, it requires sensing operators having strong properties (such as the RIP property), and most importantly, the performance metric is not regret but reconstruction error (Frobenius or 2-norm).\nIn the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori. In contrast, the problem of low regret recommendation across users with latent mixtures does not afford us the luxury of knowing either U or V , and so they must be learnt \u201con the fly\u201d. Another related work in the context of bandit type schemes for latent mixture model recommender systems is that of Bresler et al. (2014), in which, under the very specific uniform mixture model for all users, they exhibit strategies with good regret.\nNguyen et al. (2014) consider an alternating minimization type scheme in linear bandit models with two-sided uncertainty (an alternative model involving latent \u201cfactors\u201d). However no rigorous guarantees are given for the bandit schemes they present; moreover, it is not known if alternating minimization finds global minima in general. Another related work is in the transfer learning setting\n7. With additional prior knowledge of \u03b3n, the dependence of the additive term can be made polynomial in 9\u03b4: choosing \u03b3n = min{1, \u221a 9\u03b4/n}, it holds that \u2113(n0 \u2212 1 + \u2211N\nn=n0 \u03b3n) 6 2\n\u221a 9\u03b4\u2113T + \u2113 .\nfrom Lazaric et al. (2013): The method combines the RTP method (Anandkumar et al., 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d. As a result, the regret bound in this setting scales linearly with the number of rounds. Our result in this paper shows that with additional access to just user identifiers, we can reduce the regret rate to be sublinear in time.\nThe RTP method has been used as a processing step to the EM algorithm in crowdsourcing (Zhang et al., 2014), but only convergence properties are considered, which is not enough to provide regret guarantees.\nOn the theoretical side, our contribution generalizes the setting of clustered bandits (Maillard and Mannor, 2014; Gentile et al., 2014) in which a hard clustering model is assumed (one user is assigned to one class, or equivalently mixture distributions can only have support size 1). In particular, Maillard and Mannor (2014) specifically highlight the benefit of a collaborative gain across users against using a vanilla UCB for each user. However their setting is less general than assuming a soft clustering of users (one user corresponds to a mixture of classes) across various \u201crepresentative\u201d taste profiles as we study here.\nThe Alternating Least-Squares (ALS) method (Taka\u0301cs and Tikk, 2012; Mary et al., 2014) has been shown to yield promising experimental results in similar settings where both U and V are unknown. However, no theoretical guarantees are known for this algorithm that may converge to a local optimum in general.\nThe work of Valko et al. (2014) studies stochastic bandits with a linear model over a low-rank (graph Laplacian) structure. However, they assume complete knowledge of the graph and hence knowledge of the eigenvectors of the Laplacian, converting it into a bilinear problem with only one-sided uncertainty. This is in contrast to our setup where both U , V are completely uncertain.\nPerhaps the closest work to ours is that of Djolonga et al. (2013) where the authors develop a flexible approach for bandit problems in high dimension but with low-dimensional reward dependence. They use a two-phase algorithm: First a low-rank matrix completion technique (the Dantzig selector) estimates the feature-reward map, then a Gaussian Process-UCB (GP-UCB) bandit algorithm controls the regret, and show that if after n iterations the approximation error between the feature matrix and its estimate is less then \u03b7, the final regret is given by the sum of the regret of GP-UCB when given perfect knowledge of the features and of n + \u03b7(T \u2212 n) (due to the learning phase and approximation error). This results in an overall regret scaling with O(T 4/5). We depart from their results in two fundamental ways: Firstly, they have the possibility of uniformly sampling the entries (a common assumption in low-rank matrix completion techniques). We do not have this luxury in our setting as we do not control the process of user arrivals, that is not constrained to be uniform. Secondly, we prove and exploit a novel robustness property (see Theorem 8) of the bandit subroutine we use (OFUL in our case instead of GP-UCB), which allows us to effectively eliminate the approximation error in their work and obtain a O( \u221a T ) regret bound (see Theorem 4).\n6. Conclusion & Directions\nWe consider a full-blown latent class mixture model in which users are described by unknown mixtures across unknown user classes, more general and challenging than when users are assumed to fall perfectly in one class (Gentile et al., 2014; Maillard and Mannor, 2014).\nWe provide the first provable sublinear regret guarantees in this setting, when both the canonical classes and user mixture weights are completely unknown, which we believe is striking when compared to existing work in the setting, e.g., alternate minimization typically gets stuck in local minima. We currently use a combination of noisy tensor factorization and linear bandit techniques, and control the uncertainty in the estimates resulting from each one of these techniques. This enables us to effectively recover the latent class structure.\nFuture directions include reducing the numerical constant (e.g. using an alternative to RTP), and studying how to combine our work with the aggregation of user parameters suggested in Maillard and Mannor (2014).\nReferences\nYasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved Algorithms for Linear Stochastic Bandits. In Proc. NIPS, pages 2312\u20132320, 2011.\nAnimashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012.\nAnimashree Anandkumar, Rong Ge, Daniel Hsu, and Sham M Kakade. A tensor approach to learning mixed membership community models. Journal of Machine Learning Research, 15(1):2239\u20132312, 2014a.\nAnimashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. J. Mach. Learn. Res., 15(1):2773\u20132832, January 2014b.\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2):235\u2013256, 2002.\nGuy Bresler, George H Chen, and Devavrat Shah. A latent source model for online collaborative filtering. In Proc. NIPS 27, pages 3347\u20133355. Curran Associates, Inc., 2014.\nVarsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic Linear Optimization under Bandit Feedback. In Proc. COLT, 2008.\nJosip Djolonga, Andreas Krause, and Volkan Cevher. High-dimensional Gaussian process bandits. In Proc. NIPS, pages 1025\u20131033, 2013.\nAnders Forsgren. On linear least-squares problems with diagonally dominant weight matrices. SIAM Journal on Matrix Analysis and Applications, 17(4):763\u2013788, 1996.\nClaudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In Proc. ICML, pages 757\u2013765, 2014.\nMohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in multi-armed bandit with finite set of models. In Proc. NIPS, pages 2220\u20132228. Curran Associates, Inc., 2013.\nThomas Hofmann and Jan Puzicha. Latent class models for collaborative filtering. In IJCAI, volume 99, pages 688\u2013693, 1999.\nPrateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proc. ACM Symposium on Theory Of computing (STOC), pages 665\u2013674. ACM, 2013.\nJon Kleinberg and Mark Sandler. Using mixture models for collaborative filtering. In Proc. ACM Symposium on Theory Of Computing (STOC), pages 569\u2013578. ACM, 2004.\nToma\u0301s\u030c Koca\u0301k, Gergely Neu, Michal Valko, and Re\u0301mi Munos. Efficient learning by implicit exploration in bandit problems with side observations. In Proc. NIPS, pages 613\u2013621, 2014.\nAlessandro Lazaric, Emma Brunskill, et al. Sequential transfer in multi-armed bandit with finite set of models. In Proc. NIPS, pages 2220\u20132228, 2013.\nOdalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proc. ICML, pages 136\u2013144, 2014.\nJe\u0301re\u0301mie Mary, Romaric Gaudel, and Preux Philippe. Bandits warm-up cold recommender systems. arXiv preprint arXiv:1407.2806, 2014.\nHai Thanh Nguyen, Je\u0301re\u0301mie Mary, and Philippe Preux. Cold-start problems in recommendation systems via contextual-bandit algorithms. arXiv preprint, arXiv:1405.7544, 2014.\nPaat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395\u2013411, 2010.\nGilbert W Stewart, Ji-guang Sun, and Harcourt Brace Jovanovich. Matrix perturbation theory, volume 175. Academic press New York, 1990.\nIlya Sutskever, Joshua B. Tenenbaum, and Ruslan R Salakhutdinov. Modelling relational data using Bayesian clustered tensor factorization. In Proc. NIPS, pages 1821\u20131828. Curran Associates, Inc., 2009.\nGa\u0301bor Taka\u0301cs and Domonkos Tikk. Alternating least squares for personalized ranking. In ACM Conference on Recommender systems, 2012.\nMichal Valko, Re\u0301mi Munos, Branislav Kveton, and Toma\u0301s\u030c Koca\u0301k. Spectral Bandits for Smooth Graph Functions. In Proc. ICML, 2014.\nYuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In Proc. NIPS, pages 1260\u20131268, 2014.\nAppendix A. Proofs of Lemmas 1 and 2\nProof of Lemma 1 This result holds by construction of the estimates r\u0303a,a\u2032,n and r\u0303a,a\u2032,a\u2032\u2032,n. Note that\nE [ r\u0303a,a\u2032,n ] = 1\nn\nn\u2211\ni=1\n\u2211\nb\u2208[B] E\n[ Xai,1,bi,i,1Xai,2,bi,i,2\npi(a, a\u2032|bi) I{ai,1 = a, ai,2 = a\u2032}\n\u2223\u2223\u2223\u2223bi = b ] \u03b2(b)\n= 1\nn\nn\u2211\ni=1\n\u2211\nb\u2208[B]\n\u2211\nc\u2208[C] E\n[ Xai,1,b,i,1Xai,2,b,i,2\npi(a, a\u2032|b) I{ai,1 = a, ai,2 = a\u2032}\n\u2223\u2223\u2223\u2223bi = b, ci = c ] vb,c\u03b2(b)\n= 1\nn\nn\u2211\ni=1\n\u2211\nb\u2208[B]\n\u2211\nc\u2208[C] E\n[ Xa,b,i,1Xa\u2032,b,i,2 \u2223\u2223\u2223\u2223bi = b, ci = c ] vb,c\u03b2(b)\n(a) =\n1\nn\nn\u2211\ni=1\n\u2211\nb\u2208[B]\n\u2211\nc\u2208[C] E\n[ Xa,b,i,1 \u2223\u2223\u2223\u2223bi = b, ci = c ] E [ Xa\u2032,b,i,2 \u2223\u2223\u2223\u2223bi = b, ci = c ] vb,c\u03b2(b)\n= 1\nn\nn\u2211\ni=1\n\u2211\nb\u2208[B]\n\u2211\nc\u2208[C] ua,cua\u2032,cvb,c\u03b2(b)\n= \u2211 c\u2208[C] ( \u2211 b\u2208[B] vb,c\u03b2(b))ua,cua\u2032,c = \u2211\nc\u2208[C] v\u03b2,cua,cua\u2032,c ,\nwhere (a) holds by independence of the sample generated by user b when in the same class c. Note that ci is the same for all \u2113 = 1, 2, 3 interaction steps, that is ci = ci,1 = ci,2 = ci,3, where ci,\u2113 is the class corresponding to sample Xa,b,i,\u2113. This is the reason why we get ua,cua\u2032,cvb,c and not a product ua,cua\u2032,cv2b,c for instance.\nProof of Lemma 2 Since the rewards generated by each source a, b are i.i.d., the estimate r\u0303a,a\u2032,n is a sum of i.i.d. random variables bounded in [0, 1], re-weighted by the probability weights pi(a, a\n\u2032|bi), which are measurable functions of the past. Assuming that there exists some deterministic q2,i > 0 such that \u2200i \u2208 N, pi(a, a\u2032|bi) > q2,i, we can thus apply a version of Azuma-Hoeffding inequality for bounded martingale difference sequence. Let us recall that by this inequality, for a deterministic time s, and (Ym)m6s \u2208 [0, 1] being a bounded martingale difference sequence, then for all \u03b4 \u2208 (0, 1) it holds\nP(|1 s\ns\u2211\ni=1\nYi| > \u221a log(2/\u03b4)\n2s ) 6 \u03b4 .\nIn our case, Yi = Xai,1 ,bi,i,1Xai,2,bi,i,2 pi(a,a\u2032|bi) I{ai,1 = a, ai,2 = a \u2032} \u2212ma,a\u2032 , and we deduce that\nP(|r\u0303a,a\u2032,n \u2212ma,a\u2032 | >\n\u221a\u221a\u221a\u221a n\u2211\ni=1\nq\u221222,i log(2/\u03b4)\n2n2 ) 6 \u03b4 .\nLikewise, we get that\nP(|r\u0303a,a\u2032,a\u2032\u2032,n \u2212ma,a\u2032,a\u2032\u2032 | >\n\u221a\u221a\u221a\u221a n\u2211\ni=1\nq\u221223,i log(2/\u03b4)\n2n2 ) 6 \u03b4 .\nTaking a union bound over the actions in each case, and then over the two events concludes the proof.\nProof of Corollary 1 From Lemma 2, we deduce that on an event of probability higher than 1\u2212 \u03b4, it holds simultaneously that\ne(2)n def = \u2016M\u0302n,2 \u2212M2\u2016 6 A\n\u221a\u221a\u221a\u221a n\u2211\nm=1\nq\u221222,m log(4A2/\u03b4)\n2n2 ) and\ne(3)n def = \u2016M\u0302n,3 \u2212M3\u2016 6 A3/2\n\u221a\u221a\u221a\u221a n\u2211\nm=1\nq\u221223,m log(4A3/\u03b4)\n2n2 ) .\nThis indeed holds by relating the norm of the matrix (tensor) with each of the elements. We conclude by replacing the values of q2,i and q3,i.\nAppendix B. Proof of Theorem 1\nWe prove in this section a slightly more detailed result, namely, the following: Theorem 1. Assume that {\u03b3i}i>1 are chosen such that n\u22122 \u2211n i=1 \u03b3 \u22122 i\nn\u2192 0. Let \u03bbmin be the minimum robust eigenvalue of the tensor T = M3(W,W,W ). Let \u03b4 \u2208 (0, 1). Provided that\nn2\u2211n i=1 \u03b3 \u22122 i > max\n{ 2A6 log(4A2/\u03b4)\nmin{\u0393, \u03c3min}2 , A9(1 + 10( 1\u0393 + 1 \u03c3min )(1 + u3max)) 2C5 log(4A3/\u03b4) 2C21\u03bb 2 min\u03c3 3 min\n} ,\nwith probability higher than 1\u22122\u03b4, there exists some permutation \u03c0 \u2208 SC such that for all c \u2208 [C],\n||uc \u2212 un,\u03c0(c)|| 6 \u2206A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 + o(n\u22122\nn\u2211\ni=1\n\u03b3\u22122i ),\nwhere we introduced the problem-dependent constant\n\u2206 = 13 \u221a \u03c3max ( CA \u03c3min )3/2( 1 + 10( 1 \u0393 + 1 \u03c3min )(1 + u3max) ) + (2\u03c3max \u0393 + 1 \u03c3max ) 1 v2min .\nFor general {\u03b3i}i>1 (not necessarily such that n\u22122 \u2211n i=1 \u03b3 \u22122 i\nn\u2192 0), it holds with same probability that\n||uc \u2212 un,\u03c0(c)|| 6 3A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 ,\nwhere, using the notation \u2135 = 1 + 10( 1\u0393 + 1\u03c3min )(1 + u 3 max), we have introduced the constant\n3 = ( CA \u03c3min )3/2( 13 \u221a \u03c3max + 4 \u221a 2min{\u0393, \u03c3min}+ 5 (\u03c3max \u0393 + 1 2\u03c3max ) min{\u0393, \u03c3min} ) \u2135\n+ (2\u03c3max\n\u0393 +\n1\n\u03c3max ) 1 v2min + 5 \u221a 3/8 (\u221a \u03c3max + \u221a min{\u0393, \u03c3min}/2 )(2CA \u03c3min )3\u21352min{\u0393, \u03c3min} .\nProof The proof closely follows that of Gheshlaghi Azar et al. (2013). First, note that by property of the rank 1 decomposition ((Anandkumar et al., 2014b, Theorem 4.3)), it holds that \u03bbc = ( \u2211\nb\u2208[B] vb,c\u03b2(b)) \u22122 and thus v\u22122min > \u03bbmax > \u03bbmin > 1.\nWe first decompose the following term to make appear the terms from Proposition 1:\n||uc \u2212 un,\u03c0(c)|| 6 (3)\n|\u03bbc \u2212 \u03bb\u0302n,\u03c0(c)|\ufe38 \ufe37\ufe37 \ufe38 RTP.1 \u2016W\u22a4\u2020\u2016\ufe38 \ufe37\ufe37 \ufe38 b ||\u03d5c||\ufe38 \ufe37\ufe37 \ufe38 a + |\u03bb\u0302\u03c0(c)|\ufe38 \ufe37\ufe37 \ufe38 RTP.3 \u2016W\u22a4\u2020 \u2212 W\u0302\u22a4\u2020\u2016\ufe38 \ufe37\ufe37 \ufe38 d ||\u03d5c||\ufe38 \ufe37\ufe37 \ufe38 1 + |\u03bb\u0302\u03c0(c)|\ufe38 \ufe37\ufe37 \ufe38 RTP.3 \u2016W\u0302\u22a4\u2020\u2016\ufe38 \ufe37\ufe37 \ufe38 c ||\u03d5c \u2212 \u03d5\u0302n,\u03c0(c)||\ufe38 \ufe37\ufe37 \ufe38 RTP.2 .\nNote that \u03d5c, and \u03d5\u0302n,\u03c0(c) are both normalized vectors. Thus, (a) is bounded as ||\u03d5c|| 6 1. It holds for (b) that \u2016W\u22a4\u2020\u2016 6 \u221a C\u03c3max, and for (c), on the 1\u2212 \u03b4 event \u2126 from Corollary 1, that\n\u2016W\u0302\u22a4\u2020\u2016 6 \u221a C\u03c3\u0302max 6 \u221a C( \u221a \u03c3max + \u221a e (2) n ) . (4)\nThe term (d) requires a little more work. It holds that\n\u2016W\u22a4\u2020 \u2212 W\u0302\u22a4\u2020\u2016 = \u2016U\u0302D\u03021/2 \u2212 UD1/2\u2016 6 \u2016(U\u0302 \u2212 U)D1/2\u2016+ \u2016U\u0302(D\u03021/2 \u2212D1/2)\u2016 6 \u2016U\u0302 \u2212 U\u2016\ufe38 \ufe37\ufe37 \ufe38\ne\n\u03c3max + \u2016D\u03021/2 \u2212D1/2\u2016\ufe38 \ufe37\ufe37 \ufe38 f\n\u221a C .\nWe use the result of Lemma 5 from Gheshlaghi Azar et al. (2013) to control (e) and (f). If\ne (2) n 6 1 2\u0393, then it holds\n\u2016D\u03021/2 \u2212D1/2\u2016 6 e (2) n\n\u03c3max \u2016U\u0302 \u2212 U\u2016 6 2\n\u221a Ce (2) n\n\u0393 ,\nfrom which we deduce that\n\u2016W\u22a4\u2020 \u2212 W\u0302\u22a4\u2020\u2016 6 (2\u03c3max \u0393 + 1 \u03c3max ) \u221a Ce(2)n . (5)\nAt this point, (RTP.1), (RTP.2) and (RTP.3) are controlled by the perturbation method from Anandkumar et al. (2014b), under the condition that en = \u2016T \u2212 T\u0302\u2016 6 C1 \u03bbminC (where C1 is a universal constant). In this case, with probability 1 \u2212 \u03b4, the RTP algorithm with well-chosen parameters achieves\n|\u03bbc \u2212 \u03bb\u0302n,\u03c0(c)| 6 5\u2016T \u2212 T\u0302n\u2016\n\u2016\u03d5c \u2212 \u03d5\u0302n,\u03c0(c)\u2016 6 8 \u2016T \u2212 T\u0302n\u2016\n\u03bbc .\nIn order to make the condition explicit in our setting, we use the fact that by Lemma 6 from Gheshlaghi Azar et al. (2013), if e(2)n 6 12 min{\u0393, \u03c3min} then\nen 6 ( C \u03c3min )3/2( e(3)n + 2(1 + \u221a 2 + 2)e(2)n ( 1 \u0393\u03c3 + 1 \u03c3min )(e(3)n +maxc ||uc||3) ) . (6)\nThe condition e(2)n 6 12 min{\u0393, \u03c3min} holds if the number of sessions n is sufficiently large: Indeed on an event of probability higher than 1\u2212 \u03b4, then it is enough that\nA3\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A2/\u03b4)\n2n2 6\n1 2 min{\u0393, \u03c3min} ,\nthat is, reordering the terms, that\nn2\u2211n i=1 \u03b3 \u22122 i > 2A6 log(4A2/\u03b4) min{\u0393, \u03c3min}2 . (7)\nNow, in order to satisfy the condition en = \u2016T \u2212 T\u0302n\u2016 6 C1 \u03bbminC , it is enough that ( C \u03c3min )3/2( e(3)n + 2(1 + \u221a 2 + 2)e(2)n ( 1 \u0393 + 1 \u03c3min )(e(3)n +max c ||uc||3) ) 6 C1 \u03bbmin C .\nLet us decompose the left-hand-side term: After some simplifications usingmaxc ||uc||3 6 A3/2u3max and e(3)n 6 A3/2, the previous inequality happens when\ne(3)n +A 3/2 9e(2)n 6 C1 \u03bbmin\u03c3\n3/2 min\nC5/2 .\nwhere 9 = 2(1 + \u221a 2 + 2)( 1\u0393 + 1 \u03c3min )(1 + u3max). Using the definition of e (3) n and e (2) n then we deduce that it is enough that\n(1 + 9)A9/2\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2 6 C1\n\u03bbmin\u03c3 3/2 min\nC5/2 ,\nthat is, reordering the terms that\nn2\u2211n i=1 \u03b3 \u22122 i > A9(1 + 9)2C5 log(4A3/\u03b4) 2C21\u03bb 2 min\u03c3 3 min . (8)\nCombining the decomposition (3) with (4),(5), and using the fact that v\u22122min > \u03bbc > 1, we obtain\n||uc \u2212 u\u0304n,\u03c0(c)|| 6 5en \u221a C \u221a \u03c3max + (\u03bbc + 5en) \u221a C (2\u03c3max\n\u0393 +\n1\n\u03c3max\n) e(2)n\n+8 \u221a C(\u03bbc + 5en)( \u221a \u03c3max + \u221a e (2) n )\nen \u03bbc .\n6 \u221a C [ 13 \u221a \u03c3maxen +\n(2\u03c3max \u0393 + 1\n\u03c3max ) e(2)n v2min + 8\n\u221a e (2) n en\n+5 (2\u03c3max\n\u0393 +\n1\n\u03c3max\n) e(2)n en + 40( \u221a \u03c3max + \u221a e (2) n )e 2 n ] .\nNow, using (6) and unfolding the last inequality, it holds with probability higher than 1\u2212 2\u03b4 that ||uc \u2212 u\u0304n,\u03c0(c)||\n6 \u221a C [ 13 \u221a \u03c3max ( C \u03c3min )3/2 (e(3)n + e (2) n A 3/2 9) + (2\u03c3max \u0393\u03c3 + 1 \u03c3max ) e(2)n v2min\n+8 ( C \u03c3min )3/2\u221a e (2) n (e (3) n + e (2) n A 3/2 9) +5 ( C \u03c3min )3/2(2\u03c3max \u0393\u03c3 + 1 \u03c3max ) e(2)n (e (3) n + e (2) n A 3/2 9) + 40( \u221a \u03c3max + \u221a e (2) n )e 2 n ]\n6 [ 13 \u221a \u03c3max ( CA \u03c3min )3/2 (1 + 9) + (2\u03c3max \u0393\u03c3 + 1 \u03c3max ) 1 v2min ]\n\u00d7A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 + o(n\u22122\nn\u2211\ni=1\n\u03b3\u22122i ) ,\nwhich, after some cosmetic simplifications, concludes the first part of the proof of Theorem 1. Alternatively, when n\u22122 \u2211n i=1 \u03b3 \u22122 i 6\u2192 \u221e, we can always resort to the condition that e (2) n 6 1/2min{\u0393, \u03c3min} in order to simplify the previous derivation. We deduce, similarly, that\n||uc \u2212 u\u0304n,\u03c0(c)||\n6 \u221a C [ 13 \u221a \u03c3max ( C \u03c3min )3/2 (e(3)n + e (2) n A 3/2 9) + (2\u03c3max \u0393\u03c3 + 1 \u03c3max ) e(2)n v2min\n+8 ( C \u03c3min )3/2\u221a e (2) n (e (3) n + e (2) n A 3/2 9) +5 ( C \u03c3min )3/2(2\u03c3max \u0393\u03c3 + 1 \u03c3max ) e(2)n (e (3) n + e (2) n A 3/2 9) + 40( \u221a \u03c3max + \u221a e (2) n )e 2 n ]\n6 [( 13 \u221a \u03c3max ( CA \u03c3min )3/2 + 8 ( CA \u03c3min )3/2\u221a min{\u0393, \u03c3min}/2\n+5 ( CA \u03c3min )3/2(\u03c3max \u0393\u03c3 + 1 2\u03c3max ) min{\u0393, \u03c3min} ) (1 + 9) + (2\u03c3max \u0393\u03c3 + 1 \u03c3max ) 1 v2min +40 (\u221a \u03c3max + \u221a min{\u0393, \u03c3min}/2\n)( CA \u03c3min )3 (1 + 9)2 min{\u0393, \u03c3min} \u221a 3/8\n]\n\u00d7A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 ,\nwhere, in order to control the last term e2n, we used the property that\nen 6 ( CA \u03c3min )3/2( 1 + 9 ) min{e(2)n\n\u221a log(4A3/\u03b4)\nlog(4A2/\u03b4) , A\u22123/2e(3)n }\n6 ( CA \u03c3min )3/2( 1 + 9 ) min{ \u221a 3/2e(2)n , A \u22123/2e(3)n } .\nAppendix C. Proof of Theorem 3\nProof Let M1:t = (mA1 , . . . ,mAt) \u22a4. The argument used to prove Theorem 2 in Yadkori et al, 2011, can be used to show that\nv\u0302t\u22121 = V \u22121 t\u22121U\u03041:t\u22121\u03b71:t\u22121 + V \u22121 t\u22121U\u03041:t\u22121M1:t\u22121\nwhere \u03b71:t\u22121 := (\u03b71, . . . , \u03b7t\u22121) is the observed noise sequence. Let E1:t\u22121 := (\u03b5A1 , . . . , \u03b5At) \u22a4 = M1:t\u22121 \u2212 U\u03041:t\u22121v\u25e6. We then have\nv\u0302t\u22121 = V \u22121 t\u22121U\u03041:t\u22121\u03b71:t\u22121 + V \u22121 t\u22121U\u03041:t\u22121M1:t\u22121\n= V \u22121t\u22121U\u03041:t\u22121\u03b71:t\u22121 + V \u22121 t\u22121U\u03041:t\u22121\n( U\u0304\n\u22a4 1:t\u22121v \u25e6 +E1:t\u22121 )\n= V \u22121t\u22121U\u03041:t\u22121\u03b71:t\u22121 + v \u25e6 \u2212 \u03bbV \u22121t\u22121v\u25e6 + V \u22121t\u22121U\u03041:t\u22121E1:t\u22121.\nThus, letting v+t\u22121 := v \u25e6 + V \u22121t\u22121U\u03041:t\u22121E1:t\u22121 and using the above with techniques from\nYadkori et al together with \u2016v\u25e6\u20162 6 R\u0398, we have that\nv + t\u22121 \u2208 Ct\u22121\nwith probability at least 1\u2212 \u03b4. Now, let a+t\u22121 \u2208 argmaxa\u2208A u\u0304\u22a4a v+t\u22121 be an optimal action corresponding to the approximate parameter v+t\u22121, and define the instantaneous regret at time t with respect to the approximate parameter as\nr+t := u\u0304 \u22a4 a+t\u22121 v + t\u22121 \u2212 u\u0304\u22a4Atv + t\u22121 > 0.\nWe now bound this approximate regret using arguments along the lines of Yadkori et al, 2011. Consider\nr+t = u\u0304 \u22a4 a+t\u22121 v + t\u22121 \u2212 u\u0304\u22a4Atv + t\u22121\n6 u\u0304\u22a4At v\u0303t \u2212 u\u0304 \u22a4 Atv + t\u22121 (since (At, v\u0303t) is optimistic) = u\u0304\u22a4At ( v\u0303t \u2212 v+t\u22121 ) = u\u0304\u22a4At (v\u0303t \u2212 v\u0302t\u22121) + u\u0304 \u22a4 At ( v\u0302t\u22121 \u2212 v+t\u22121 ) 6 \u2016u\u0304At\u2016V \u22121t\u22121 \u2016v\u0303t \u2212 v\u0302t\u22121\u2016Vt\u22121 + \u2016u\u0304At\u2016V \u22121t\u22121 \u2225\u2225v\u0302t\u22121 \u2212 v+t\u22121 \u2225\u2225 Vt\u22121\n(Cauchy-Schwarz\u2019s inequality)\n6 2Dt\u22121 \u2016u\u0304At\u2016V \u22121t\u22121 . (9)\nNoting that ma \u2208 [\u22121, 1] \u2200a, the regret can be written as\nRT =\nT\u2211\nt=1\n(ma\u22c6 \u2212mAt) = T\u2211\nt=1\nmin{ma\u22c6 \u2212mAt , 2}\n= \u03c1\u2032 \u2211\na 6=a\u22c6\nT\u2211\nt=1\nmin\n{ ma\u22c6 \u2212ma\n\u03c1\u2032 , 2 \u03c1\u2032\n} I{At = a}\n6 \u03c1\u2032 \u2211\na 6=a\u22c6\nT\u2211\nt=1\nmin { u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6, 2\n\u03c1\u2032\n} I{At = a} (using the definition of \u03c1\u2032)\n(a) 6 \u03c1\u2032 T\u2211\nt=1\nmin { 2 ( u\u0304 \u22a4 a\u22c6v + t\u22121 \u2212 u\u0304\u22a4Atv + t\u22121 ) , 2\n\u03c1\u2032\n} (b) = 2\u03c1\u2032 T\u2211\nt=1\nmin { u\u0304 \u22a4 a+t\u22121 v + t\u22121 \u2212 u\u0304\u22a4Atv + t\u22121, 1 \u03c1\u2032 }\n= 2\u03c1\u2032 T\u2211\nt=1\nmin { r+t , 1\n\u03c1\u2032\n} = \u03c1\u2032 T\u2211\nt=1\n2 \u03c1\u2032 min\n{ \u03c1\u2032r+t , 1 } (c) 6 \u03c1\u2032 T\u2211\nt=1\n2 \u03c1\u2032 min { 2\u03c1\u2032Dt\u22121 \u2016u\u0304At\u2016V \u22121t\u22121 , 1 }\n(d) 6 \u03c1\u2032 T\u2211\nt=1\n4Dt\u22121min { \u2016u\u0304At\u2016V \u22121t\u22121 , 1 }\n6 \u03c1\u2032 \u221a\u221a\u221a\u221aT T\u2211\nt=1\n16DT 2 min { \u2016u\u0304At\u20162V \u22121t\u22121 , 1 } (by using Cauchy-Schwarz\u2019s inequality).\nIn the derivation above,\n\u2022 Steps (a) and (b) hold because of the following. By Lemma 4 (to follow below), \u2225\u2225v+t\u22121 \u2212 v\u25e6 \u2225\u2225 2 =\u2225\u2225V \u22121t\u22121U\u03041:t\u22121E1:t\u22121 \u2225\u2225 2 6 \u03b1(U\u0304 ) \u2016\u03b5\u20162. Since argmaxa\u2208A u\u0304\u22a4a v\u25e6 is uniquely a\u22c6 by hypothesis, we\nhave, thanks to Lemma 5 (to follow below), that u\u0304\u22a4a\u22c6v + t\u22121 \u2212 u\u0304\u22a4a v+t\u22121 >\nu\u0304 \u22a4 a\u22c6v \u25e6\u2212u\u0304\u22a4a v\u25e6 2 > 0 \u2200a 6= a\u22c6,\nestablishing (a). This in turn shows that the optimal action for v+t\u22121 is uniquely a \u22c6 at all times t, i.e., a+t\u22121 = argmaxa\u2208A u\u0304 \u22a4 a v + t\u22121 = a \u22c6, which is equality (b).\n\u2022 Inequality (c) holds by (9) and (d) holds because \u03c1\u2032 > 1 by definition, and Dt\u22121 > \u03bb1/2R\u0398 > 1/2 by hypothesis, implying that 2\u03c1\u2032Dt\u22121 > 1.\nThe argument from here can be continued in the same way as in Abbasi-Yadkori et al. (2011) to yield\nRT 6 8\u03c1 \u2032 \u221a TC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) .\nThis proves the theorem.\nLemma 4 (Analysis of the time-varying parameter error V \u22121t\u22121U\u03041:t\u22121E1:t\u22121) Let \u03b5a = ma \u2212 u\u0304\u22a4a v\u25e6 be the bias in arm a\u2019s reward due to model error, and let \u03b5 \u2261 (\u03b5a)a\u2208A be the |A| dimensional vector of arm reward biases. Then, \u2225\u2225V \u22121t\u22121U\u03041:t\u22121E1:t\u22121 \u2225\u2225 2 6 ( max J \u2225\u2225A\u22121J \u2225\u2225 2 ) \u2016\u03b5\u20162 ,\nwhere A(A+C)\u00d7C = [ U\u0304 Id ] , AJ is the C \u00d7C submatrix of A formed by picking rows J , and J ranges over all subsets of full-rank rows of A.\nProof [Proof of Lemma 4] Let zt\u22121 := V \u22121 t\u22121U\u03041:t\u22121E1:t\u22121 = vt\u22121\u2212v\u25e6 \u2208 RC , with \u2016E1:t\u22121\u2016\u221e 6\n\u2016\u03b5\u2016\u221e = \u2225\u2225m\u2212 U\u0304v\u25e6 \u2225\u2225 \u221e. We have\nzt\u22121 =\n( t\u22121\u2211\ns=1\nu\u0304As u\u0304 \u22a4 As + \u03bbI\n)\u22121 t\u22121\u2211\ns=1\n\u03b5As u\u0304As\n=\n( 1\nt\u2212 1\nt\u22121\u2211\ns=1\nu\u0304As u\u0304 \u22a4 As +\n\u03bb t\u2212 1I )\u22121 1 t\u2212 1 t\u22121\u2211\ns=1\n\u03b5As u\u0304As\n=\n(\u2211\na\u2208A u\u0304au\u0304\n\u22a4 a\n\u2211t\u22121 s=1 I{As = a}\nt\u2212 1 + \u03bb t\u2212 1I )\u22121 \u2211\na\u2208A \u03b5au\u0304a\n\u2211t\u22121 s=1 I{As = a}\nt\u2212 1\n=\n(\u2211\na\u2208A u\u0304au\u0304\n\u22a4 a fa(t\u2212 1) +\n\u03bb t\u2212 1I )\u22121 \u2211\na\u2208A \u03b5au\u0304afa(t\u2212 1),\nwhere fa(t\u2212 1) \u2261 fa represents the empirical frequency with which action a \u2208 A has been played up to and including time t \u2212 1. This allows us to equivalently interpret zt\u22121 as the solution of a weighted \u21132-regularized least squares regression problem with K = |A| observations (instead of the original interpretation with t\u2212 1 observations) as follows.\nLet F1/2 be the A \u00d7 A diagonal matrix with the values \u221a f1, . . . , \u221a fA on the diagonal (note:\u2211A\na=1 fa = 1). With this, we can express zt\u22121 as\nzt\u22121 = arg min z\u2208RC\n\u2225\u2225\u2225F1/2U\u0304z \u2212 F1/2\u03b5 \u2225\u2225\u2225 2\n2 +\n\u03bb\nt\u2212 1 \u2016z\u2016 2 2\n= arg min z\u2208RC\n\u2225\u2225\u2225F1/2 ( U\u0304z \u2212 \u03b5 )\u2225\u2225\u2225 2\n2 +\n\u03bb\nt\u2212 1 \u2016z\u2016 2 2\n= arg min z\u2208RC \u2225\u2225\u2225\u2225\u2225 [ F 1/2 0 0 \u221a\n\u03bb t\u22121IC\n]([ U\u0304 IC ] z \u2212 [ \u03b5 0 ])\u2225\u2225\u2225\u2225\u2225 2\n2\n\u2261 arg min z\u2208RC\n\u2225\u2225\u2225D1/2 (Az \u2212 b) \u2225\u2225\u2225 2\n2 = (A\u22a4DA)\u22121A\u22a4Db,\nwith D1/2 being a (A + C) \u00d7 (A + C) diagonal & positive semidefinite matrix, A\u22a4DA =\u2211 a\u2208A u\u0304au\u0304 \u22a4 a fa(t \u2212 1) + \u03bbt\u22121I positive definite, and A having full column rank C. A result of Forsgren (1996, Corollary 2.3) can now be applied to yield\n\u2225\u2225(A\u22a4DA)\u22121A\u22a4D \u2225\u2225 2 6 max\nJ\n\u2225\u2225A\u22121J \u2225\u2225 2\nwhere J ranges over all subsets of full-rank rows of A, and AJ is the C\u00d7C submatrix of A formed by picking rows J . Thus, \u2016zt\u22121\u20162 6 ( maxJ \u2225\u2225A\u22121J \u2225\u2225 2 ) \u2016\u03b5\u20162. This proves the lemma.\nLemma 5 (Critical radius) Let u\u0304\u22a4a\u22c6v \u25e6 > u\u0304\u22a4a v \u25e6 \u2200a 6= a\u22c6. Then, the following are equivalent:\n\u2016v \u2212 v\u25e6\u20162 6 \u03b1(U\u0304 ) \u2016\u03b5\u20162 \u21d2 u\u0304\u22a4a\u22c6v \u2212 u\u0304\u22a4a v > u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 2 \u2200a 6= a\u22c6, (10)\nand\n\u2016\u03b5\u20162 < mina 6=a\u22c6 u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 2\u03b1(U\u0304) \u2016u\u0304a\u22c6 \u2212 u\u0304a\u20162 . (11)\nProof [Proof of Lemma 5] Assuming (11), observe that when v lies in the interior of an \u03b1(U\u0304) \u2016\u03b5\u20162ball around v\u25e6, we have, for any a 6= a\u22c6,\n(u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v = (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v\u25e6 + (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 (v \u2212 v\u25e6) > (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v\u25e6 + min\n\u2016\u03c8\u201626\u03b1(U\u0304)\u2016\u03b5\u20162 (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 \u03c8\n= (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v\u25e6 \u2212 \u03b1(U\u0304) \u2016\u03b5\u20162\u2016u\u0304a\u22c6 \u2212 u\u0304a\u20162 > (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v\u25e6 \u2212 \u03b1(U\u0304)\u2016u\u0304a\u22c6 \u2212 u\u0304a\u20162 u\u0304 \u22a4 a\u22c6v\n\u25e6 \u2212 u\u0304\u22a4a v\u25e6 2\u03b1(U\u0304) \u2016u\u0304a\u22c6 \u2212 u\u0304a\u20162\n= u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 2 ,\nwhich proves one direction of the lemma. For the other direction, note that if \u2016\u03b5\u20162 > u\u0304\n\u22a4 a\u22c6v \u25e6\u2212u\u0304\u22a4a v\u25e6\n2\u03b1(U\u0304)\u2016u\u0304a\u22c6\u2212u\u0304a\u20162\nfor some a 6= a\u22c6, then by setting v = v\u25e6 \u2212 (u\u0304 \u22a4 a\u22c6v \u25e6\u2212u\u0304\u22a4a v\u25e6)(u\u0304a\u22c6\u2212u\u0304a) 2\u2016u\u0304a\u22c6\u2212u\u0304a\u201622 , we have both\n\u2016v \u2212 v\u25e6\u20162 = \u2225\u2225\u2225\u2225\u2225 ( u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 ) (u\u0304a\u22c6 \u2212 u\u0304a)\n2 \u2016u\u0304a\u22c6 \u2212 u\u0304a\u201622\n\u2225\u2225\u2225\u2225\u2225 2 = u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 2 \u2016u\u0304a\u22c6 \u2212 u\u0304a\u20162 6 \u03b1(U\u0304) \u2016\u03b5\u20162\nand\n(u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v = (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v\u25e6 \u2212 (u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 ( u\u0304 \u22a4 a\u22c6v \u25e6 \u2212 u\u0304\u22a4a v\u25e6 ) (u\u0304a\u22c6 \u2212 u\u0304a)\n2 \u2016u\u0304a\u22c6 \u2212 u\u0304a\u201622 =\n(u\u0304a\u22c6 \u2212 u\u0304a)\u22a4 v\u25e6 2\nwhich contradicts (10), and we are done.\nC.1 Proof of Lemma 3\nWe begin by establishing some auxiliary technical results, which together imply Lemma 3.\nLemma 6 (Controlling \u03b1n) If n is large enough so that (1) and\n3A3C\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2 6\n1\n2\u03b1\u22c6 , (12)\nhold, then with probability at least 1\u2212 \u03b4, \u03b1n 6 2\u03b1\u22c6. (13)\nProof [Proof of Lemma 6] The first step is to estimate the factor \u03b1 in the analysis of Perturbed OFUL. Towards this, note that the quantity \u03b1 \u2261 \u03b1(U\u0304) in our setting becomes\n\u03b1n \u2261 \u03b1n(U\u0304n) = max J \u2225\u2225(u\u22c4n)\u22121J \u2225\u2225 2 ,\nwhere u\u22c4n := [ U\u0304n IC ] has rank C, and J ranges over all combinations of its C full-rank rows. For\nany such subset of C linearly independent rows J , we have, after denoting u\u22c4 := [ U IC ] , that\n\u2225\u2225(u\u22c4n)\u22121J \u2225\u2225 2 6 \u2225\u2225(u\u22c4)\u22121J \u2225\u2225 2 + \u2225\u2225(u\u22c4n)\u22121J \u2212 (u\u22c4)\u22121J \u2225\u2225 2 .\nThe final term above can be bounded using Anandkumar et al. (2012, Lemma E.4) \u2013 a version of Theorem 2.5 in Stewart et al. (1990). Assuming (u\u22c4)J is invertible, and \u2225\u2225(u\u22c4)\u22121J ((u\u22c4n)J \u2212 (u\u22c4)J ) \u2225\u2225 2 < 1, then (u\u22c4n)J is invertible, and a resulting bound on the norm of its inverse lets us write\n\u2225\u2225(u\u22c4)\u22121J \u2225\u2225 2 + \u2225\u2225(u\u22c4n)\u22121J \u2212 (u\u22c4)\u22121J \u2225\u2225 2 6 \u2225\u2225(u\u22c4)\u22121J \u2225\u2225 2 +\n\u2016(u\u22c4n)J \u2212 (u\u22c4)J\u20162 \u2225\u2225(u\u22c4)\u22121J \u2225\u22252 2\n1\u2212 \u2225\u2225(u\u22c4)\u22121J ((u\u22c4n)J \u2212 (u\u22c4)J ) \u2225\u2225 2 .\nWriting J = Ju \u222a Jl (u and l stand for \u201cupper\u201d and \u201clower\u201d) with Jl representing the subset of rows taken from the bottom C rows of u\u22c4n (i.e., IC ), we have\n(u\u22c4n)J \u2212 (u\u22c4)J = [ (U\u0304n \u2212 U)Ju 0 ] .\nThus, with \u2016\u00b7\u2016F denoting the Frobenius norm, and using the dominance of the Frobenius norm over the matrix 2-norm, with probability at least 1\u2212 \u03b4,\n\u2016(u\u22c4n)J \u2212 (u\u22c4)J\u20162 6 \u2016(u\u22c4n)J \u2212 (u\u22c4)J\u2016F = \u2225\u2225(U\u0304n \u2212 U)Ju \u2225\u2225 F 6 \u2225\u2225U\u0304n \u2212 U \u2225\u2225 F\n=\n\u221a\u2211\nc\u2208[C]\n\u2225\u2225U\u0304n,c \u2212 Uc \u2225\u22252 2\n6 3A3C\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2 , (14)\nfrom the RTP error estimate (1). Now, letting \u03b1 \u2261 \u03b1(U) = maxJ \u2225\u2225\u2225(u\u22c4J) \u22121 \u2225\u2225\u2225 2 , the result above implies that for any suitable J ,\n\u2225\u2225(u\u22c4)\u22121J ((u\u22c4n)J \u2212 (u\u22c4)J) \u2225\u2225 2 6 \u2225\u2225(u\u22c4)\u22121J \u2225\u2225 2 \u2016(u\u22c4n)J \u2212 (u\u22c4)J\u20162\n6 \u03b1 \u2016(u\u22c4n)J \u2212 (u\u22c4)J\u20162\n6 \u03b13A3C\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2\n< 1/2\nwhenever n is large enough to satisfy (12). When the condition (12) above holds, we get, for any J at time n,\n\u2225\u2225(u\u22c4n)\u22121J \u2225\u2225 2 6 \u2225\u2225(u\u22c4)\u22121J \u2225\u2225 2 +\n\u2016(u\u22c4n)J \u2212 (u\u22c4)J\u20162 \u2225\u2225(u\u22c4)\u22121J \u2225\u22252 2\n1\u2212 \u2225\u2225(u\u22c4)\u22121J ((u\u22c4n)J \u2212 (u\u22c4)J ) \u2225\u2225 2\n6 \u03b1+ 2\u03b12 \u2016(u\u22c4n)J \u2212 (u\u22c4)J\u20162\n6 \u03b1+ 2\u03b123A3C\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2 [by (14)]\n6 \u03b1+ 2\u03b12 1\n2\u03b1 = 2\u03b1 .\nThis shows that \u03b1n = maxJ \u2225\u2225(u\u22c4n)\u22121J \u2225\u2225 2 6 2\u03b1.\nLemma 7 (Sufficient condition for (2)) If n is large enough so that (1), (12) and\n3A3C\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i log(4A3/\u03b4)\n2n2 6 min\n{ gb\n4 \u221a A \u2016vb\u20162\n, gb\n16\u03b1\u22c6 \u221a Cumax \u2016vb\u20162 + gb\n} (15)\nhold, then (2) is satisfied with probability at least 1\u2212 \u03b4.\nProof [Proof of Lemma 7] The term \u2016\u03b5\u20162 = \u2225\u2225(U \u2212 U\u0304n ) vb \u2225\u2225 2 is bounded from above by\n\u2225\u2225U \u2212 U\u0304n \u2225\u2225 2 \u2016vb\u20162 6 \u2225\u2225U \u2212 U\u0304n \u2225\u2225 F \u2016vb\u20162\n6 \u221a C \u2016vb\u20162 3A3\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 (by (14))\n\u2261 \u221a C \u2016vb\u20162 \u2135n, say. (16)\nFor any a 6= a\u22c6,\n(u\u0304n,a\u22c6 \u2212 u\u0304n,a)\u22a4 vb = (ua\u22c6 \u2212 ua)\u22a4 vb + \u2202avb > \u03b6a, (17)\nwith \u2202\u22a4a := (u\u0304n,a\u22c6 \u2212 ua\u22c6)\u2212 (u\u0304n,a \u2212 ua), and \u03b6a := inf\u2016\u03be\u201626\u2016\u2202a\u20162 (ua\u22c6 \u2212 ua) \u22a4 vb + \u03be \u22a4 vb.\nAlso, by (14), we have\nmax a\u2208[A]\n\u2016u\u0304n,a \u2212 ua\u20162 6 \u221a AC max\nc\u2208[C] \u2016u\u0304n,c \u2212 uc\u20162\n6 \u221a AC3A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2\n=: \u2135n \u221a AC.\nThus,\n\u03b6a > inf \u2016\u03be\u2016262\u2135n \u221a AC\n(ua\u22c6 \u2212 ua)\u22a4 vb + \u03be\u22a4vb = (ua\u22c6 \u2212 ua)\u22a4 vb \u2212 2\u2135n \u221a AC \u2016vb\u20162 . (18)\nBy (17) and (18), for any a 6= a\u22c6,\n(u\u0304n,a\u22c6 \u2212 u\u0304n,a)\u22a4 vb > (ua\u22c6 \u2212 ua)\u22a4 vb \u2212 2\u2135n \u221a AC \u2016vb\u20162 . (19)\nWe also have\n\u2016u\u0304n,a\u22c6 \u2212 u\u0304n,a\u20162 6 \u2016ua\u22c6 \u2212 ua\u20162 + \u2016u\u0304n,a\u22c6 \u2212 ua\u22c6\u20162 + \u2016u\u0304n,a \u2212 ua\u20162 6 \u2016ua\u22c6 \u2212 ua\u20162 + 2\u2135n \u221a AC (20)\nwhenever (12) holds. Putting (16), (19), (20) and the conclusion of Lemma 6 together, we have that condition (2) in our case, i.e,\n\u2016\u03b5\u20162 \u2261 \u2225\u2225(U \u2212 U\u0304n ) vb \u2225\u2225 2 6 min a 6=a\u22c6 (u\u0304n,a\u22c6 \u2212 u\u0304n,a)\u22a4 vb 2\u03b1n \u2016u\u0304n,a\u22c6 \u2212 u\u0304n,a\u20162\nis satisfied when\n\u221a C \u2016vb\u20162 \u2135n 6 min a 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb \u2212 2\u2135n\n\u221a AC \u2016vb\u20162\n4\u03b1\u22c6 \u2016ua\u22c6 \u2212 ua\u20162 + 2\u2135n \u221a AC .\nThis, in turn, is satisfied if\n2\u2135n \u221a AC \u2016vb\u20162 6 1\n2 min a 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb = gb 2 , and\n\u221a C \u2016vb\u20162 \u2135n 6\ngb/2\n8\u03b1\u22c6 \u221a Cumax + gb/(2 \u2016vb\u20162)\n\u21d4 \u2135n 6 gb\n16\u03b1\u22c6Cumax \u2016vb\u20162 + gb \u221a C .\nLemma 8 (Control of the distortion \u03c1 due to noisy feature estimates) Ifn is large enough so that (1), (12) and (15) hold, then \u03c1\u2032 6 2 with probability at least 1\u2212 \u03b4.\nProof [Proof of Lemma 8] We begin by considering\nmax a 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb (u\u0304n,a\u22c6 \u2212 u\u0304n,a)\u22a4 vb 6 max a 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb (ua\u22c6 \u2212 ua)\u22a4 vb + \u2202avb 6 max a 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb \u03b6a ,\nwith \u2202\u22a4a := (u\u0304n,a\u22c6 \u2212 ua\u22c6)\u2212 (u\u0304n,a \u2212 ua), and\n\u03b6a := inf \u2016\u03be\u201626\u2016\u2202a\u20162\n(ua\u22c6 \u2212 ua)\u22a4 vb + \u03be\u22a4vb\nas in the proof of Lemma 7. Also, by (14), we have that with probability at least 1\u2212 \u03b4,\nmax a\u2208[A]\n\u2016u\u0304n,a \u2212 ua\u20162 6 \u221a AC max\nc\u2208[C] \u2016u\u0304n,c \u2212 uc\u20162\n6 \u221a AC3A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2\n=: \u2135n \u221a AC, say.\nThus,\n\u03b6a > inf \u2016\u03be\u2016262\u2135n \u221a AC\n(ua\u22c6 \u2212 ua)\u22a4 vb + \u03be\u22a4vb\n= (ua\u22c6 \u2212 ua)\u22a4 vb \u2212 2\u2135n \u221a AC \u2016vb\u20162\n\u21d2 \u03b6a (ua\u22c6 \u2212 ua)\u22a4 vb\n> 1\u2212 2\u2135n \u221a AC \u2016vb\u20162\n(ua\u22c6 \u2212 ua)\u22a4 vb > 1\u2212 2\u2135n \u221a AC \u2016vb\u20162 gb ,\nwhere gb := mina 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb > 0 is the minimum gap for user b across suboptimal actions.\nProvided that (1), (12) and (15) hold, we get that with probability at least 1\u2212\u03b4, \u03b6a (ua\u22c6\u2212ua)\u22a4vb >\n1 2 for each a 6= a\u22c6. Also, by the definition of a\u22c6, the denominator is positive, i.e., (ua\u22c6 \u2212 ua) \u22a4 vb > 0. Hence,\nmax a 6=a\u22c6 (ua\u22c6 \u2212 ua)\u22a4 vb (u\u0304n,a\u22c6 \u2212 u\u0304n,a)\u22a4 vb 6 2,\ncompleting the proof of the result.\nLemma 9 (Bounding RX ) If n is large enough so that (1) and (12) hold, then\nRX 6\n\u221a A 2\u03b1\u22c6 +max a\u2208A \u2016ua\u20162 ,\nwith probability at least 1\u2212 \u03b4.\nProof [Proof of Lemma 9] Conditions (1) and (12), together with the estimate (20), imply that for any action a,\n\u2016u\u0304n,a\u20162 6 \u2016ua\u20162 + \u2016u\u0304n,a \u2212 ua\u20162 6 \u2016ua\u20162 + \u2135n \u221a AC 6 \u2016ua\u20162 + \u221a A/(2\u03b1\u22c6).\nwith probability at least 1\u2212 \u03b4.\nIn order to conclude the proof of Lemma 3, we gather the conditions from Lemma 6 and Lemma 7. After some simplifications, both conditions are satisfied as soon as\nn2\u2211n i=1 \u03b3 \u22122 i > 32A6C2 log(4A3/\u03b4)max\n{ 2\u03b12\u22c6,\n8A||vb||22 g2b , 27\u03b12\u22c6Cu 2 max||vb||22 g2b + 1/2\n} .\nAppendix D. Proof of Theorem 4\nProof Let n0 be the first mini-session such that both conditions in Lemma 3 are satisfied, that is such that\nn0\u2211n0 i=1 \u03b3 \u22122 i > 9\u03b4 .\nThe cumulative regret RT = \u2211T t=1 rt of Algorithm 3 satisfies\nRT =\nN\u2211\nn=1\n\u2113\u2211\nl=1\nrn,l\n6 (n0 \u2212 1)\u2113+ \u2211\nb\u2208[B]\nN\u2211\nn=n0\n\u2113\u2211\nl=1\nrn,lI{bn = b}\nwhere rt \u2261 rn,l def= u\u22a4a\u22c6bnvbn\u2212u \u22a4 an,lvbn is the instantaneous regret of Algorithm 3 at time t = \u2113n+k when the current user is bn = b. Using the notations of Algorithm 3, it holds that\nE[rt|bn = b] = E[rtI{pn = 1}|bn = b] + E[rtI{pn = 0}|bn = b] 6 E[u\u22a4a\u22c6bvb \u2212 u \u22a4 a\u0303n,k\nvb](1 \u2212 \u03b3n) + \u03b3n. 6 E[u\u22a4a\u22c6bvb \u2212 u \u22a4 a\u0303n,k ub] + \u03b3n,\nwhere a\u0303n,k is an action output by an instance of OFUL for user bn = b. Thus, we have\nE[RT |b1, . . . , bN ]\n6 (n0 \u2212 1)\u2113+ E [ \u2211\nb\u2208[B]\nN\u2211\nn=n0\n\u2113\u2211\nl=1\n( u \u22a4 a\u22c6b vb \u2212 u\u22a4a\u0303n,lvb ) I{bn = b} \u2223\u2223\u2223\u2223b1, . . . , bN ] + \u2113 N\u2211\nn=n0\n\u03b3n .\n= (n0 \u2212 1)\u2113+ \u2211\nb\u2208[B] E\n[ \u2211\nn06n6N, bn=b\n\u2113\u2211\nl=1\n( u \u22a4 a\u22c6b vb \u2212 u\u22a4a\u0303n,lvb )\n\ufe38 \ufe37\ufe37 \ufe38 (\u22c6)\n\u2223\u2223\u2223\u2223b1, . . . , bN ] + \u2113 N\u2211\nn=n0\n\u03b3n. (21)\nFor each user b \u2208 [B], the expectation in the right-hand side above corresponds to the cumulative regret of the OFUL strategy when interacting with user b in mini-sessions n0 through N , and when given at each mini-session n the set of perturbed feature vectors Un. Let Nb,n0 =\u2211N\nn=n0 I{bn = b} count the total number of mini-sessions from n0 in which user b is present (note\nthat \u2211 b\u2208[B]Nb,1 = N and \u2211 b\u2208[B] \u2113Nb,1 = T ). Let us denote the term (\u22c6) in the above explicitly using Rb,Nb,n0 ({Un}n\u2208[n0,N ],bn=b).\nWe can now use the OFUL robustness guarantee \u2013 a natural technical extension8 of Theorem 3 along with Lemma 3 \u2013 to obtain that, for a given user sequence b1, . . . , bN , with probability at least9 1\u2212 2\u03b4 \u2212 \u03b4 = 1\u2212 3\u03b4,\nRb,Nb,n0 ({Un}n\u2208[n0,N ],bn=b) 6\n16 \u221a \u2113Nb,n0 C log ( 1 + \u2113Nb,n0R 2 X\n\u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 + \u2113Nb,n0R 2 X\n\u03bbC\n)) .\n8. Although Theorem 3 holds only for a fixed perturbation \u03b5 and feature set u\u0304, it is not hard to see that a modification of it, with time-varying \u03b5t, u\u0304t and \u03c1\u2032 being the largest \u03c1\u2032t over all times t, yields the same conclusion (regret bound). We provide this extension in Theorem 5 in Appendix E below. 9. Although the time horizons played by each OFUL instance per user, Nb,n0 , are technically random and unknown to the instance at the start, conditioning on the sequence of users arriving at each time instant lets us use the conclusion of Lemma 3.\nThis in turn implies that\n\u2211 b\u2208B E [ Rb,Nb,n0 ({Un}n\u2208[n0,N ],bn=b) \u2223\u2223\u2223\u2223b1, . . . , bN ]\n(a) 6 16 \u2211\nb\u2208B\n\u221a \u2113Nb,n0 C log ( 1 + \u2113Nb,n0R 2 X\n\u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 + \u2113Nb,n0R 2 X\n\u03bbC\n))\n+ \u2211\nb\u2208B 3\u03b4\u2113Nb,n0\n(b) 6 16 \u2211\nb\u2208B\n\u221a \u2113Nb,n0 C log ( 1 + \u2113Nb,n0R 2 X\n\u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 + \u2113Nb,n0R 2 X\n\u03bbC\n))\n+ 3\u03b4T.\nThe last term on the right-hand side in (a) is due to the fact that with probability at most 3\u03b4, the per-user regret Rb,Nb,n0 ({Un}n\u2208[n0,N ],bn=b) can be as large as \u2113Nb,n0 (the total number of time slots for which user b interacts with the system). The corresponding term in (b) is by using\u2211\nb\u2208[B] \u2113Nb,1 = T . Further bounding using the Cauchy-Schwarz inequality \u2211 b\u2208B \u221a \u2113Nb,n0 6\u221a\nBT gives\n\u2211 b\u2208B E [ Rb,Nb,n0 ({Un}n\u2208[n0,N ],bn=b) \u2223\u2223\u2223\u2223b1, . . . , bN ]\n6 16 \u2211\nb\u2208B\n\u221a \u2113Nb,n0 C log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) + 3\u03b4T\n6 16 \u221a BTC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) + 3\u03b4T.\nPlugging this estimate into (21), we obtain that E[RT |b1, . . . bN ] 6 \u2113 ( n0 \u2212 1 + N\u2211\nn=n0\n\u03b3n\n)\n+ 16 \u221a BTC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) + 3\u03b4T.\nExpliciting n0 and tuning \u03b3n The next step is to control the term n0\u2212 1+ \u2211N\nn=n0 \u03b3n. To this\nend, we explicit n0 and optimize \u03b3n. We write 9 \u2261 9\u03b4 in the sequel for convenience. If \u03b3n = min{1,91/2n\u22121/2}, then\nn2\u2211n m=1 \u03b3 \u22122 m = n2 \u23089\u2309+ 19 \u2211n m>\u23089\u2309 m\n= 29n2\n\u23089\u230929+ n(n+ 1)\u2212 \u23089\u2309(\u23089\u2309 \u2212 1)\n> 29\n1 + 1/n+ (\u23089\u23099)/n2 .\nThus, this is higher than 9 if n2 \u2212 n \u2212 \u23089\u23099 > 0, that is if n > n0 def= \u23081/2 + \u221a \u23089\u23099 + 1/4\u2309. Since n0 > \u23089\u2309, we immediately get\nN\u2211\nn=n0\n\u03b3n 6 9 1/2n \u2212 12 0 + 29 1/2 ( N 1 2 \u2212 n 1 2 0 )\n6 1 + 291/2 ( N 1 2 \u2212 n 1 2 0 ) .\nThus, we obtain\nn0 \u2212 1 + N\u2211\nn=n0\n\u03b3n 6 29 1/2N 1 2 + n0 \u2212 291/2n1/20\n6 291/2N 1 2 + n0 \u2212 2 \u221a 9\u23089\u2309\nUsing the fact that 9 > 1, the bound simplifies to\nn0 \u2212 1 + N\u2211\nn=n0\n\u03b3n 6 2 \u221a 9N + 1 .\nIf, on the other hand, a bound on 9 is not readily available beforehand, then choosing \u03b3n =\u221a log(1 + n)/n, n > 1, gives, via a crude bound,\nn\u2211\nm=1\n\u03b3\u22122m = n\u2211\nm=1\nm/ log(1 +m) 6\n\u221a n\u2211\nm=1\nm/ log 2 + n\u2211\nm= \u221a n\nm/ log(1 + \u221a n)\n6 n/ log 2 + n2/ log \u221a n 6 2n2/ log \u221a n\n\u21d2 n 2 0\u2211n0\nm=1 \u03b3 \u22122 m\n> n20\n2n20/ log \u221a n0\n= log n0\n4 .\nThe bound above is at least 9 provided n0 > exp(49). Thus, we finally get that, upon setting \u03b4 = 1/ \u221a T , the total expected regret satisfies (as an order-wise function of T )\nE[RT ]\n6 \u2113 ( exp(49) + N\u2211\nn=1\n\u221a log(n+ 1)/n\n)\n+ 16 \u221a BTC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a log T + C log ( 1 +\nTR2X \u03bbC\n)) + 3 \u221a T\n= O ( C \u221a BT logT ) .\nAppendix E. Extension of Theorem 3: Robustness of OFUL\u2019s regret with time-varying features\nWe now control the robust regret for user bRb,Nb,n0 ({Un}n\u2208[n0,N ],bn=b) = \u2211\nn06n6N, bn=b\n\u2211\u2113 l=1 ( u \u22a4 a\u22c6b vb\u2212\nu \u22a4 a\u0303n,l vb ) , when OFUL is run with evolving feature matrices {Un}n\u2208[n0,N ],bn=b with decreasing feature error \u03b5n = (U \u2212 Un)vb, instead of a fixed U with fixed error \u03b5 = (U \u2212 U)vb. We reindex the n \u2208 [n0, N ], bn = b as t = 1, . . . , .. and prove the following result.\nTheorem 5 (OFUL robustness result, extension of Theorem 3 for time-varying features) Assume ||v\u25e6||2 6 R\u0398, \u03bb > max { 1, R2X , 1/4R 2 \u0398 } , \u2200a \u2208 A, t 6 T , ||u\u0304(t)a ||2 6 RX and |ma| 6 1, and that for all t 6 T , argmaxa\u2208A u\u0304 (t)\u22a4 a v\n\u25e6 = {a\u22c6} (i.e., the linearly realizable approximation with respect to the current features has a\u22c6 as its unique optimal action). If\n\u2225\u2225\u2225\u03b5(t) \u2225\u2225\u2225 2 \u2261 \u2225\u2225\u2225m\u2212 U\u0304 (t)v\u25e6 \u2225\u2225\u2225 2 < min a 6=a\u22c6 u\u0304 (t)\u22a4 a\u22c6 v \u25e6 \u2212 u\u0304(t)\u22a4a v\u25e6\n2\u03b1(U\u0304 (t)\u22a4) \u2225\u2225\u2225u\u0304(t)a\u22c6 \u2212 u\u0304(t)a \u2225\u2225\u2225 2 , (22)\nthen with probability at least 1\u2212 \u03b4, for all T > 0,\nRT 6 8\u03c1 \u2032 \u221a TC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) ,\nwhere \u03c1\u2032 := maxt max { 1,maxa 6=a\u22c6\nma\u22c6\u2212ma u\u0304 (t)\u22a4\na\u22c6 v \u25e6\u2212u\u0304(t)\u22a4a v\u25e6\n} .\nProof Let M1:t = (mA1 , . . . ,mAt) \u22a4. The argument used to prove Theorem 2 in Yadkori et al, 2011, shows that\nv\u0302t\u22121 = V \u22121 t\u22121U\u0304 (t) 1:t\u22121\u03b71:t\u22121 + V \u22121 t\u22121U\u0304 (t) 1:t\u22121M1:t\u22121\nwhere \u03b71:t\u22121 := (\u03b71, . . . , \u03b7t\u22121) is the observed noise sequence, and where U\u0304 (t) 1:t\u22121 is the matrix built from the time varying features at time t and the action sequence thus far. Let E(t)1:t\u22121 := (\u03b5 (t) A1 , . . . , \u03b5 (t) At )\u22a4 = M1:t\u22121 \u2212 U\u0304(t)1:t\u22121v\u25e6. We then have\nv\u0302t\u22121 = V \u22121 t\u22121U\u0304 (t) 1:t\u22121\u03b71:t\u22121 + V \u22121 t\u22121U\u0304 (t) 1:t\u22121M1:t\u22121\n= V \u22121t\u22121U\u0304 (t) 1:t\u22121\u03b71:t\u22121 + V \u22121 t\u22121U\u0304 (t) 1:t\u22121\n( U\u0304\n(t)\u22a4 1:t\u22121v \u25e6 +E(t)1:t\u22121\n)\n= V \u22121t\u22121U\u0304 (t) 1:t\u22121\u03b71:t\u22121 + v \u25e6 \u2212 \u03bbV \u22121t\u22121v\u25e6 + V \u22121t\u22121U\u0304 (t) 1:t\u22121E (t) 1:t\u22121.\nThus, letting v+t\u22121 := v \u25e6 + V \u22121t\u22121U\u0304 (t) 1:t\u22121E (t) 1:t\u22121, and using the above with techniques from\nYadkori et al together with \u2016v\u25e6\u20162 6 R\u0398, we have that\nv + t\u22121 \u2208 Ct\u22121\nwith probability at least 1\u2212 \u03b4. Now, let a+t\u22121 \u2208 argmaxa\u2208A u\u0304 (t)\u22a4 a v + t\u22121 be an optimal action corresponding to the approximate parameter v+t\u22121 and approximate feature u\u0304 (t)\u22a4 a , and define the instantaneous regret at time t with respect to the approximate parameter as\nr+t := u\u0304 (t)\u22a4 a+t\u22121 v + t\u22121 \u2212 u\u0304 (t)\u22a4 At v + t\u22121 > 0.\nWe now bound this approximate regret using arguments along the lines of Yadkori et al, 2011 as follows. Write\nr+t = u\u0304 (t)\u22a4 a+t\u22121 v + t\u22121 \u2212 u\u0304 (t)\u22a4 At v + t\u22121\n6 u\u0304 (t)\u22a4 At v\u0303t \u2212 u\u0304(t)\u22a4At v + t\u22121 (since (At, v\u0303t) is optimistic) = u\u0304 (t)\u22a4 At ( v\u0303t \u2212 v+t\u22121 ) = u\u0304 (t)\u22a4 At (v\u0303t \u2212 v\u0302t\u22121) + u\u0304(t)\u22a4At ( v\u0302t\u22121 \u2212 v+t\u22121 )\n6 \u2225\u2225\u2225u\u0304(t)At \u2225\u2225\u2225 V \u22121t\u22121 \u2016v\u0303t \u2212 v\u0302t\u22121\u2016Vt\u22121 + \u2225\u2225\u2225u\u0304(t)At \u2225\u2225\u2225 V \u22121t\u22121 \u2225\u2225v\u0302t\u22121 \u2212 v+t\u22121 \u2225\u2225 Vt\u22121\n(Cauchy-Schwarz\u2019s inequality)\n6 2Dt\u22121 \u2225\u2225\u2225u\u0304(t)At \u2225\u2225\u2225 V \u22121t\u22121 . (23)\nNoting that ma \u2208 [\u22121, 1] \u2200a, the regret can be written as\nRT =\nT\u2211\nt=1\n(ma\u22c6 \u2212mAt) = T\u2211\nt=1\nmin{ma\u22c6 \u2212mAt , 2}\n= \u03c1\u2032 \u2211\na 6=a\u22c6\nT\u2211\nt=1\nmin\n{ ma\u22c6 \u2212ma\n\u03c1\u2032 , 2 \u03c1\u2032\n} I{At = a}\n6 \u03c1\u2032 \u2211\na 6=a\u22c6\nT\u2211\nt=1\nmin { u\u0304 (t)\u22a4 a\u22c6 v \u25e6 \u2212 u\u0304(t)\u22a4a v\u25e6, 2\n\u03c1\u2032\n} I{At = a} (using the definition of \u03c1\u2032)\n(a) 6 \u03c1\u2032 T\u2211\nt=1\nmin { 2 ( u\u0304 (t)\u22a4 a\u22c6 v + t\u22121 \u2212 u\u0304 (t)\u22a4 At v + t\u22121 ) , 2\n\u03c1\u2032\n} (b) = 2\u03c1\u2032 T\u2211\nt=1\nmin { u\u0304 (t)\u22a4 a+t\u22121 v + t\u22121 \u2212 u\u0304 (t)\u22a4 At v + t\u22121, 1 \u03c1\u2032 }\n= 2\u03c1\u2032 T\u2211\nt=1\nmin { r+t , 1\n\u03c1\u2032\n} = \u03c1\u2032 T\u2211\nt=1\n2 \u03c1\u2032 min\n{ \u03c1\u2032r+t , 1 } (c) 6 \u03c1\u2032 T\u2211\nt=1\n2 \u03c1\u2032 min\n{ 2\u03c1\u2032Dt\u22121 \u2225\u2225\u2225u\u0304(t)At \u2225\u2225\u2225 V \u22121t\u22121 , 1 }\n(d) 6 \u03c1\u2032 T\u2211\nt=1\n4Dt\u22121min {\u2225\u2225\u2225u\u0304(t)At \u2225\u2225\u2225 V \u22121t\u22121 , 1 }\n6 \u03c1\u2032 \u221a\u221a\u221a\u221aT T\u2211\nt=1\n16DT 2 min {\u2225\u2225\u2225u\u0304(t)At \u2225\u2225\u2225 2\nV \u22121t\u22121\n, 1 } (by using Cauchy-Schwarz\u2019s inequality).\nIn the derivation above,\n\u2022 Steps (a) and (b) hold because of the following. By Lemma 10 (to follow below), \u2225\u2225v+t\u22121 \u2212 v\u25e6 \u2225\u2225 2 =\u2225\u2225\u2225V \u22121t\u22121U\u0304 (t) 1:t\u22121E (t) 1:t\u22121 \u2225\u2225\u2225 2 6 \u03b1(U\u0304t) \u2225\u2225\u03b5(t) \u2225\u2225 2 . Since argmaxa\u2208A u\u0304 (t)\u22a4 a v \u25e6 is uniquely a\u22c6 by hypothesis,\nwe have, thanks to Lemma 5, that u\u0304(t)\u22a4a\u22c6 v + t\u22121 \u2212 u\u0304 (t)\u22a4 a v + t\u22121 >\nu\u0304 (t)\u22a4 a\u22c6 v \u25e6\u2212u\u0304(t)\u22a4a v\u25e6 2 > 0 \u2200a 6= a\u22c6, es-\ntablishing (a). This in turn shows that the optimal action for v+t\u22121 is uniquely a \u22c6 at all times t, i.e., a+t\u22121 = argmaxa\u2208A u\u0304 (t)\u22a4 a v + t\u22121 = a \u22c6, which is precisely equality (b).\n\u2022 Remark. In the above, Lemma 5 is written for generic u\u0304a, \u03b5, so in particular applies to each time varying u\u0304(t)a , \u03b5(t). We also used an extended version of Lemma 4 to the case of varying u\u0304 (t) a , \u03b5(t),\nwhich we state and prove below as Lemma 10.\n\u2022 Inequality (c) holds by (23) and (d) holds because \u03c1\u2032 > 1 by definition, and Dt\u22121 > \u03bb1/2R\u0398 > 1/2 by hypothesis, implying that 2\u03c1\u2032Dt\u22121 > 1.\nThe argument from here can be continued in the same way as in Abbasi-Yadkori et al. (2011, proof of Theorem 3) to yield\nRT 6 8\u03c1 \u2032 \u221a TC log ( 1 +\nTR2X \u03bbC\n)( \u03bb1/2R\u0398 +R \u221a 2 log 1\n\u03b4 + C log\n( 1 +\nTR2X \u03bbC\n)) .\nThis proves the theorem.\nLemma 10 (Extension of Lemma 4 to time-varying feature sets) Let \u03b5(t)a = ma \u2212 u\u0304(t)\u22a4a v\u25e6 be the bias in arm a\u2019s reward due to model error, with respect to the features U\u0304t, and let \u03b5(t) \u2261 ( \u03b5 (t) a\n) a\u2208A . Then, we have\n\u2225\u2225\u2225V \u22121t\u22121U\u0304 (t) 1:t\u22121E (t) 1:t\u22121 \u2225\u2225\u2225 2 6 ( max J \u2225\u2225\u2225A(t)\u22121J \u2225\u2225\u2225 2 )\u2225\u2225\u2225\u03b5(t) \u2225\u2225\u2225 2 ,\nwhere A(t)(A+C)\u00d7C =\n[ U\u0304 (t)\nId\n] , A(t)J is the C \u00d7 C submatrix of A(t) consisting of rows in J , and J ranges\nover all subsets of full-rank rows of A(t).\nProof [Proof of Lemma 10] Let z(t)t\u22121 := V \u22121 t\u22121U\u0304 (t) 1:t\u22121E (t) 1:t\u22121 = v + t\u22121\u2212v\u25e6 \u2208 RC , thus \u2225\u2225\u2225E(t)1:t\u22121 \u2225\u2225\u2225 \u221e\n6 \u2225\u2225\u03b5(t)\n\u2225\u2225 \u221e = \u2225\u2225m\u2212 U\u0304 (t)v\u25e6 \u2225\u2225 \u221e. We now write\nz (t) t\u22121 =\n( t\u22121\u2211\ns=1\nu\u0304 (t) As u\u0304 (t)\u22a4 As + \u03bbI\n)\u22121 t\u22121\u2211\ns=1\n\u03b5 (t) As u\u0304 (t) As\n=\n( 1\nt\u2212 1\nt\u22121\u2211\ns=1\nu\u0304 (t) As u\u0304 (t)\u22a4 As + \u03bb t\u2212 1I )\u22121 1 t\u2212 1 t\u22121\u2211\ns=1\n\u03b5 (t) As u\u0304 (t) As\n=\n(\u2211\na\u2208A u\u0304 (t) a u\u0304 (t)\u22a4 a\n\u2211t\u22121 s=1 I{As = a}\nt\u2212 1 + \u03bb t\u2212 1I )\u22121 \u2211\na\u2208A \u03b5(t)a u\u0304 (t) a\n\u2211t\u22121 s=1 I{As = a}\nt\u2212 1\n=\n(\u2211\na\u2208A u\u0304 (t) a u\u0304 (t)\u22a4 a fa(t\u2212 1) +\n\u03bb t\u2212 1I )\u22121 \u2211\na\u2208A \u03b5(t)a u\u0304 (t) a fa(t\u2212 1),\nwhere fa(t \u2212 1) is the empirical frequency with which action a \u2208 A has been played up to and including time t \u2212 1. This allows us to equivalently interpret zt\u22121 as the solution of a weighted \u21132-regularized least squares regression problem with K = |A| observations (instead of the original interpretation with t\u2212 1 observations) as follows (we suppress the dependence of fa on t as per the context for clarity of notation).\nLet F1/2 be the A \u00d7 A diagonal matrix with the values \u221a f1, . . . , \u221a fA on the diagonal (note:\u2211A\na=1 fa = 1). With this, we can express zt\u22121 as\nz (t) t\u22121 = arg min\nz\u2208RC\n\u2225\u2225\u2225F1/2U\u0304 (t)z \u2212 F1/2\u03b5(t) \u2225\u2225\u2225 2\n2 +\n\u03bb\nt\u2212 1 \u2016z\u2016 2 2\n= arg min z\u2208RC\n\u2225\u2225\u2225F1/2 ( U\u0304 (t)z \u2212 \u03b5(t) )\u2225\u2225\u2225 2\n2 +\n\u03bb\nt\u2212 1 \u2016z\u2016 2 2\n= arg min z\u2208RC \u2225\u2225\u2225\u2225\u2225 [ F 1/2 0 0 \u221a\n\u03bb t\u22121IC\n]([ U\u0304 (t)\nIC\n] z \u2212 [ \u03b5(t)\n0 ])\u2225\u2225\u2225\u2225\u2225 2\n2\n\u2261 arg min z\u2208RC\n\u2225\u2225\u2225D1/2 (Az \u2212 b) \u2225\u2225\u2225 2\n2 = (A\u22a4DA)\u22121A\u22a4Db,\nwith D1/2 being a (A + C) \u00d7 (A + C) diagonal & positive semidefinite matrix, A\u22a4DA =\u2211 a\u2208A u\u0304 (t) a u\u0304 (t)\u22a4 a fa(t \u2212 1) + \u03bbt\u22121I being positive definite, and A having full column rank C. A result of Forsgren (1996, Corollary 2.3) now gives \u2225\u2225(A\u22a4DA)\u22121A\u22a4D\n\u2225\u2225 2 6 max\nJ\n\u2225\u2225A\u22121J \u2225\u2225 2\nwhere J ranges over all subsets of full-rank rows of A, and AJ is the C\u00d7C submatrix of A formed by picking rows J . Thus, \u2225\u2225\u2225z(t)t\u22121 \u2225\u2225\u2225 2 6 ( maxJ \u2225\u2225A\u22121J \u2225\u2225 2 ) \u2225\u2225\u03b5(t) \u2225\u2225 2 . This proves the lemma.\nAppendix F. Unregularized Least squares\nIn our setting where we consider finitely many arms, one way wonder whether it is possible to remove the regularization parameter \u03bb. Following Rusmevichientong and Tsitsiklis (2010), this is indeed possible under the assumption that the minimum eigenvalue of \u2211 a\u2208A uau \u22a4 a is away from 0. Then, we first play each arm once (once for all users B, not for each of them) before running\nAlgorithm 3, where OFUL is used with \u03bb = 0 and with Dt\u22121 redefined to be 4R2 ( A log(t) +\nlog(A/\u03b4) ) . This leads essentially to similar bounds, with \u03b1\u22c6 replaced by maxJ ||U\u22121J ||2, as we\nshow below. Let U \u2282 RC . We receive at time s, observation ys = u\u22a4s v\u22c6 + \u03b7s \u2208 R where v\u22c6 \u2208 RC and us \u2208 U . We make the following\nAssumption 2 There exists RX , R, \u03bb0 \u2208 R+\u22c6 such that 1. \u2200s, ||us|| 6 RX 2. \u2200\u03bb \u2208 R, logE exp(\u03bb\u03b7s) 6 \u03bb2R2/2. 3. \u03bbmin( \u2211t s=1 usu \u22a4 s ) > \u03bb0.\nAssumption 2.3 is satisfied for instance when there are C points (u0,i)i\u2208[C] in Rd such that\n\u03bbmin( \u2211C i=1 u0,iu \u22a4 0,i) = \u03bb0 > 0, and us = u0,s for s \u2208 [C]. We consider the least-squares estimate\nvt = ( t\u2211\ns=1\nusu \u22a4 s\n)\u22121 \u22a4\u2211\ns=1\nusys ,\nF.1 Preliminary\nIn case U is finite, one can get the following result Theorem 6 Let us introduce the confidence set\nCt = { w \u2208 RC : w\u22a4Gtw 6 Dt,\u03b4 } , where Gt = t\u2211\ns=1\nusu \u22a4 s\nand Dt,\u03b4 = 4R 2 ( |U| log(t) + log(|U|/\u03b4) ) .\nThen, under Assumption 2, it holds\nP ( vt \u2212 v\u22c6 \u2208 Ct ) > 1\u2212 \u03b4 .\nIn the general case, it holds\nTheorem 7 Let us introduce the confidence set\nCt = { w \u2208 RC : w\u22a4Gtw 6 Dt,\u03b4 } , where Gt = t\u2211\ns=1\nusu \u22a4 s\nand Dt,\u03b4 = 16R 2 [ 1 + log ( 1 +\n36R2X \u03bb0\n)][ C log ( 36R2X \u03bb0 t ) + log(1/\u03b4) ] log(t) .\nThen, under Assumption 2, and if t > \u03bb0 12R2\nX\nit holds\nP ( vt \u2212 v\u22c6 \u2208 Ct ) > 1\u2212 \u03b4 .\nProof: Indeed, let zt = \u2211\u22a4 s=1 us\u03b7s. Since Gt is invertible, it holds that vt = v\u22c6 + G \u22121 t zt,\nand thus\n(vt \u2212 v\u22c6)\u22a4Gt(vt \u2212 v\u22c6) = ztG\u22121t zt\nIn the case when U is finite, using the Proof of Theorem B.1 in Rusmevichientong and Tsitsiklis (2010) then we further get for all \u03b5 > 0,\nP ( ztG \u22121 t zt > \u03b5 2R2 ) 6 |U|t|U|e\u2212\u03b52/4 ,\nThus, choosing \u03b5 = 2 \u221a log(|U|t|U|/\u03b4), we obtain that\nP ( ztG \u22121 t zt > 4R 2 ( |U| log(t) + log(|U|/\u03b4) )) 6 \u03b4 ,\nwhich concludes the proof of Theorem 6. From the Proof of Theorem B.2 in Rusmevichientong and Tsitsiklis (2010), it holds that for all \u03b5 > 2,\nP ( ztG \u22121 t zt > \u03b5 2k20R 2 log(t) ) 6 ( 36R2X t/\u03bb0 )C e\u2212\u03b5 2/4 ,\nwhere k0 = 2 \u221a 1 + log(1 + 36R2X/\u03bb0), which leads to\nP ( ztG \u22121 t zt > 4 ( 1 + log(1 + 36R2X/\u03bb0))R 2 log(t)\u03b52 ) 6 ( 36R2X t/\u03bb0 )C e\u2212\u03b5 2/4 ,\nThus, let us use \u03b5 = 2 \u221a log (( 36R2X t/\u03bb0 )C /\u03b4 ) , which satisfies \u03b5 > 2 as soon as t > \u03bb0e 1/C\n36R2 X\n, thus\nin particular if t > \u03bb0 12R2\nX\n. Now, introducing the constant c = 36R2X/\u03bb0, we obtain\nP ( ztG \u22121 t zt > 16R 2(1 + log(1 + c)) log(t) ( C log(ct) + log(1/\u03b4) )) 6 \u03b4 ,\nwhich concludes the proof of theorem 7.\nF.2 Application to Low-Rank bandits\nIn order to apply this result to the low-rank bandit problem, we need to show that Gt is invertible. In our case, this matrix is at mini-session n M\u0303t = \u2211t s=1 u\u0303n,as u\u0303 \u22a4 n,as .\nLet us assume that all actions are sample at least once in the beginning. Thus, in this case \u03bbmin(M\u0303t) > \u03bbmin(A\u0303), where A\u0303 = \u2211 a\u2208[A] u\u0303n,au\u0303 \u22a4 n,a. For convenience, let us also introduce the C \u00d7 C matrix A =\u2211a\u2208[A] uau\u22a4a = U\u22a4U . In order to show that M\u0303t is invertible, it us enough to show that \u03bbmin(A\u0303) > 0. Now, by the result of reconstruction of the feature matrix M , we know that there exists with high probability a permutation \u03c0 such that the columns are well estimated:\n\u2200c, ||u\u03c0(c) \u2212 u\u0303n,c|| 6 3A3 \u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2 .\nThus, we study E = A\u0303\u2212A. Let \u03bb be any eigenvalue of E, then it holds\n\u03bb 6 trace(E) = \u2211\na\u2208[A] trace\n( u\u0303n,au\u0303 \u22a4 n,a \u2212 uau\u22a4a )\n6 \u2211\na\u2208[A] ||u\u0303n,a||2 \u2212 ||ua||2\n6 \u2211\na\u2208[A]\n\u2211\nc\u2208[C] u\u03032n,a,c \u2212 u2a,c\n6 \u2211\na\u2208[A]\n\u2211\nc\u2208[C] (u\u0303n,a,c \u2212 ua,c)2 + 2ua,c(u\u0303n,a,c \u2212 ua,c)\n6 \u2211\nc\u2208[C] ||u\u0303n,c \u2212 uc||2 + 2\n\u2211\nc\u2208[C]\n\u221a\u2211\na\u2208[A] u2a,c\n\u221a\u2211\na\u2208[A] (u\u0303n,a,c \u2212 ua,c)2\n6 \u2211\nc\u2208[C] ||u\u0303n,c \u2212 uc||2 + 2||uc||||u\u0303n,c \u2212 uc||\n6 (2umax + 1) \u2211\nc\u2208[C] ||u\u0303n,c \u2212 uc|| .\nThus, provided that n is large enough that\n\u03bbmin(A) > 2(2umax + 1) \u2211\nc\u2208[C] ||u\u0303n,c \u2212 uc|| ,\nwe deduce that M\u0303t is invertible. Using the fact that A = U\u22a4U , This translates to the condition\n\u03bbmin(U \u22a4U) > 23(2umax + 1)CA 3\n\u221a\u221a\u221a\u221a n\u2211\ni=1\n\u03b3\u22122i C log(4A3/\u03b4)\n2n2\nthat is\nn2\u2211n i=1 \u03b3 \u22122 i > 432(2umax + 1) 2C3A6 log(4A3/\u03b4) \u03bb2min(U \u22a4U) .\nThus, assuming that all actions are chosen at least once in the beginning, and that\nn2\u2211n m=1 \u03b3 \u22122 m > 432(2umax + 1) 2C3A6 log(4A3/\u03b4) \u03bb2min(U \u22a4U) ,\nthen \u03bbmin(M\u0303t) > \u03bbmin(U\u22a4U)/2 = \u03bb0/2 > 0 and Theorem 6 and Theorem 7 both apply. In order to control the regret of the unregularized version of OFUL, we now use the proof of Rusmevichientong and Tsitsiklis (2010, Theorem 4.1) combined with the fact that \u03bbmin(M\u0303t) > \u03bb0/2 to get\nn\u2211\nt=A+1\nmin{||u\u0304At ||2M\u0303\u22121t\u22121 , 1} 6 2max{1, 2R2X \u03bb0\n} ( C log(max{1, 2R 2 X\n\u03bb0 }) + (C + 1) log(n+ 1)\n) .\nA straightforward adaptation of the proof of Theorem 3 then gives\nRn 6 \u03c1 \u2032 \u221a\u221a\u221a\u221an(A+ 16D2n,\u03b4 n\u2211\nt=A+1\nmin{||u\u0304At ||2M\u0303\u22121t\u22121 , 1})\n6 16\u03c1\u2032R2 ( A log(n) + log(A/\u03b4) )\u221a n ( 2 +\n4R2X \u03bb0\n)( C log ( 1+\n2R2X \u03bb0\n) + (C+1) log(n+1)\n)\n+\u03c1\u2032 \u221a An .\nFollowing the same steps as for Lemma 3, we finally obtain the result:\nTheorem 8 (Unregularized OFUL robustness result) Assume ||v\u25e6||2 6 R\u0398, for all a \u2208 A, ||u\u0304a||2 6 RX and |ma| 6 1, and that argmaxa\u2208A u\u0304\u22a4a v\u25e6 = {a\u22c6} (i.e., the linearly realizable approximation has a\u22c6 as its unique optimal action). Assume that each action has been played once. Let 0 < \u03b4 6 1. Provided that the number of mini-sessions n0 is large enough to satisfy\nn20\u2211n0 i=1 \u03b3 \u22122 i > 9\u0303b,\u03b4\nwhere\n9\u0303b,\u03b4 = max\n{ 2A6 log(4A2/\u03b4)\nmin{\u0393, \u03c3min}2 , A9(1 + 10( 1\u0393 + 1 \u03c3min )(1 + u3max)) 2C5 log(4A3/\u03b4) 2C21\u03c3 3 min\n432(2umax + 1) 2C3A6 log(4A3/\u03b4)\n\u03bb2min(U \u22a4U)\n,\n3 2A6C2 log(4A3/\u03b4)max { 2\u03b12\u22c6,\n8A||vb||22 g2b , 27\u03b12\u22c6Cu 2 max||vb||22 g2b + 1/2\n}} ,\nthen with probability at least 1 \u2212 \u03b4 for all T > 0, the regret RA+1:n of the OFUL algorithm from decision A+ 1 to n satisfies\nRA+1:n 6 32R 2 [ A log(n) + log(A/\u03b4)\n]\u221a\nn ( 2 + 4R 2\nX \u03bb0\n)( C log ( 1+ 2R 2\nX \u03bb0\n) + (C+1) log(n+1) ) ,\nwhere we introduced\nRX = max a\u2208A\n||ua||2 + \u221a A\n2\u03b1\u22c6 and \u03b1\u22c6 = min J ||U\u22121J || .\nThis result enables to get the corresponding variant of Theorem 4 using an unregularized OFUL."}], "references": [{"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "In Proc. NIPS,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A latent source model for online collaborative filtering", "author": ["Guy Bresler", "George H Chen", "Devavrat Shah"], "venue": "In Proc. NIPS", "citeRegEx": "Bresler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bresler et al\\.", "year": 2014}, {"title": "Stochastic Linear Optimization under Bandit Feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In Proc. COLT,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Sequential transfer in multi-armed", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": null, "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "bandit with finite set of models", "author": ["Thomas Hofmann", "Jan Puzicha"], "venue": "In Proc. NIPS,", "citeRegEx": "Hofmann and Puzicha.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann and Puzicha.", "year": 2013}, {"title": "Using mixture models for collaborative filtering", "author": ["Jon Kleinberg", "Mark Sandler"], "venue": "Proc. ACM Symposium on Theory Of computing (STOC),", "citeRegEx": "Kleinberg and Sandler.,? \\Q2013\\E", "shortCiteRegEx": "Kleinberg and Sandler.", "year": 2013}, {"title": "Efficient learning by implicit exploration", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Gergely Neu", "Michal Valko", "R\u00e9mi Munos"], "venue": "Theory Of Computing (STOC),", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2004}, {"title": "bandit problems with side observations", "author": ["Alessandro Lazaric", "Emma Brunskill"], "venue": "In Proc. NIPS,", "citeRegEx": "Lazaric and Brunskill,? \\Q2014\\E", "shortCiteRegEx": "Lazaric and Brunskill", "year": 2014}, {"title": "Odalric-Ambrym Maillard and Shie Mannor", "author": ["2014. J\u00e9r\u00e9mie Mary", "Romaric Gaudel", "Preux Philippe"], "venue": "Proc. NIPS, pages 2220\u20132228,", "citeRegEx": "Mary et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mary et al\\.", "year": 2013}, {"title": "The final term above can be bounded using Anandkumar et al. (2012, Lemma E.4) \u2013 a version of Theorem", "author": ["Stewart"], "venue": null, "citeRegEx": "Stewart,? \\Q1990\\E", "shortCiteRegEx": "Stewart", "year": 1990}, {"title": "Unregularized Least squares In our setting where we consider finitely many arms, one way wonder whether it is possible to remove the regularization parameter \u03bb", "author": ["F. Appendix"], "venue": "Following Rusmevichientong and Tsitsiklis", "citeRegEx": "Appendix,? \\Q2010\\E", "shortCiteRegEx": "Appendix", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "It combines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al.", "startOffset": 46, "endOffset": 72}, {"referenceID": 0, "context": "(2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the first rigorous regret analysis of this combination, showing that its regret after T user interactions is \u00d5(C \u221a BT ), with B the number of users.", "startOffset": 49, "endOffset": 78}, {"referenceID": 0, "context": "We develop a novel bandit algorithm (Algorithm 3) that combines (a) the Optimization in the Face of Uncertainty Linear bandit OFUL algorithm (Abbasi-Yadkori et al., 2011) for bandits with known action features, and (b) a variant of the Robust Tensor Power (RTP) algorithm (Anandkumar et al.", "startOffset": 141, "endOffset": 170}, {"referenceID": 0, "context": ", 2009; Anandkumar et al., 2014a,b); the task of simultaneously optimizing net utility in a bandit fashion in complex expression models like these has received little or no analytical treatment. Our work takes a step towards filling this void. An especially challenging aspect of online learning in recommender systems is the relatively meager number of available interactions with a same user, which is offset to an extent by the assumption that users can only have a limited number of taste profiles (classes). Indeed, if one can identify the class to which a certain user belongs and aggregate information from all other users in that class, then one can recommend to the user the best item for the class. In practice, classes are latent and not necessarily known in advance, and several works (Gentile et al., 2014; Lazaric et al., 2013; Maillard and Mannor, 2014) study the restricted situation when each user always belongs to one specific class (i.e., when all mixture distributions have support size 1). We go two steps further, since in many situations (a) users cannot be assumed to belong to one class only, such as when a user account is shared by several individuals (e.g. a smart-TV), and (b) the duration of a user-session, that is the number of consecutive recommendations to the same individual connected to a user-account, cannot assumed to be long1. The key challenges that this work addresses are (1) the lack of knowledge of \u201cfeatures\u201d on both the user-side and item-side in a linear bandit problem (in this case, both the user mixture weights and the item class reward profiles) and (2) provable regret minimization with very few i.e. O(1) interactions with every user b having a specific taste profile, as opposed to a large number of interactions such as in transfer learning (Lazaric et al., 2013). Contributions and overview of results. We consider a setting when users are assumed to come from arbitrary mixtures across classes (they are not assumed to fall perfectly in one class as was the assumption in works by Gentile et al. (2014); Maillard and Mannor (2014)).", "startOffset": 8, "endOffset": 2064}, {"referenceID": 0, "context": ", 2009; Anandkumar et al., 2014a,b); the task of simultaneously optimizing net utility in a bandit fashion in complex expression models like these has received little or no analytical treatment. Our work takes a step towards filling this void. An especially challenging aspect of online learning in recommender systems is the relatively meager number of available interactions with a same user, which is offset to an extent by the assumption that users can only have a limited number of taste profiles (classes). Indeed, if one can identify the class to which a certain user belongs and aggregate information from all other users in that class, then one can recommend to the user the best item for the class. In practice, classes are latent and not necessarily known in advance, and several works (Gentile et al., 2014; Lazaric et al., 2013; Maillard and Mannor, 2014) study the restricted situation when each user always belongs to one specific class (i.e., when all mixture distributions have support size 1). We go two steps further, since in many situations (a) users cannot be assumed to belong to one class only, such as when a user account is shared by several individuals (e.g. a smart-TV), and (b) the duration of a user-session, that is the number of consecutive recommendations to the same individual connected to a user-account, cannot assumed to be long1. The key challenges that this work addresses are (1) the lack of knowledge of \u201cfeatures\u201d on both the user-side and item-side in a linear bandit problem (in this case, both the user mixture weights and the item class reward profiles) and (2) provable regret minimization with very few i.e. O(1) interactions with every user b having a specific taste profile, as opposed to a large number of interactions such as in transfer learning (Lazaric et al., 2013). Contributions and overview of results. We consider a setting when users are assumed to come from arbitrary mixtures across classes (they are not assumed to fall perfectly in one class as was the assumption in works by Gentile et al. (2014); Maillard and Mannor (2014)).", "startOffset": 8, "endOffset": 2092}, {"referenceID": 4, "context": "UCB (Auer et al., 2002)) per user, scales as O(B \u221a TA/B) = O( \u221a ABT ) after T rounds2 , which is considerably suboptimal in the practical case with a very large number of items but very few representative user classes (C \u226a A).", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": "An alternative is the implicit exploration method due to Koc\u00e1k et al. (2014).", "startOffset": 57, "endOffset": 77}, {"referenceID": 1, "context": "Our next result makes use of the following proposition from Anandkumar et al. (2014b, Theorem 5.1), restated here for completeness. Proposition 1 (Theorem 5.1 of Anandkumar et al. (2014b)) Let T\u0302 = T + E \u2208 RC\u00d7C\u00d7C , where T is a symmetric tensor with orthogonal decomposition T = \u2211C c=1 \u03bbc\u03c6 \u22973 c , where each \u03bbc > 0, {\u03c6c}c\u2208[C] is an orthonormal basis, and E is a symmetric tensor with operator norm ||E|| 6 \u03b5.", "startOffset": 60, "endOffset": 188}, {"referenceID": 1, "context": "See Anandkumar et al. (2014b) for more details on notation and results.", "startOffset": 4, "endOffset": 30}, {"referenceID": 0, "context": "Algorithm 2 OFUL (Optimism in Face of Uncertainty for Linear bandits) (Abbasi-Yadkori et al., 2011) Require: Arms\u2019 features \u016a , regularization parameter \u03bb, norm parameter R\u0398 for all times t > 1 do 1.", "startOffset": 70, "endOffset": 99}, {"referenceID": 0, "context": "Theorem 2 (OFUL regret (Abbasi-Yadkori et al., 2011)) Assume that ||v||2 6 R\u0398, and that for all a \u2208 A, ||\u016ba||2 6 RX , |\u3008\u016ba,v\u3009| 6 1.", "startOffset": 23, "endOffset": 52}, {"referenceID": 0, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori.", "startOffset": 32, "endOffset": 119}, {"referenceID": 6, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori.", "startOffset": 32, "endOffset": 119}, {"referenceID": 9, "context": ", no exploration mini-sessions, and (c) An implementation of the Alternating Least Squares estimator (Tak\u00e1cs and Tikk, 2012; Mary et al., 2014) for the matrix U along with OFUL per-user. The proposed algorithm, with the theoretically suggested exploration \u00d5(n\u22121/2), is observed to exploit the latent structure considerably better than simple UCB, and is not too far from the unrealistic OFUL strategy which enjoys the luxury of latent class information. It is also competitive with performing Alternating Least Squares, which does not come with analytically sound performance guarantees in the bandit learning setting. Also, the large additive constants in the theoretical bounds for Algorithm 3 do not manifest here. Related work. The popular low-rank matrix completion problem studies the recovery U and V given a small number of entries sampled at random from UV T with both U and V being tall matrices, see for instance Jain et al. (2013) and citations therein.", "startOffset": 125, "endOffset": 943}, {"referenceID": 0, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori. In contrast, the problem of low regret recommendation across users with latent mixtures does not afford us the luxury of knowing either U or V , and so they must be learnt \u201con the fly\u201d. Another related work in the context of bandit type schemes for latent mixture model recommender systems is that of Bresler et al. (2014), in which, under the very specific uniform mixture model for all users, they exhibit strategies with good regret.", "startOffset": 33, "endOffset": 578}, {"referenceID": 0, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori. In contrast, the problem of low regret recommendation across users with latent mixtures does not afford us the luxury of knowing either U or V , and so they must be learnt \u201con the fly\u201d. Another related work in the context of bandit type schemes for latent mixture model recommender systems is that of Bresler et al. (2014), in which, under the very specific uniform mixture model for all users, they exhibit strategies with good regret. Nguyen et al. (2014) consider an alternating minimization type scheme in linear bandit models with two-sided uncertainty (an alternative model involving latent \u201cfactors\u201d).", "startOffset": 33, "endOffset": 713}, {"referenceID": 4, "context": ", 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d.", "startOffset": 47, "endOffset": 66}, {"referenceID": 1, "context": "(2013): The method combines the RTP method (Anandkumar et al., 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d. As a result, the regret bound in this setting scales linearly with the number of rounds. Our result in this paper shows that with additional access to just user identifiers, we can reduce the regret rate to be sublinear in time. The RTP method has been used as a processing step to the EM algorithm in crowdsourcing (Zhang et al., 2014), but only convergence properties are considered, which is not enough to provide regret guarantees. On the theoretical side, our contribution generalizes the setting of clustered bandits (Maillard and Mannor, 2014; Gentile et al., 2014) in which a hard clustering model is assumed (one user is assigned to one class, or equivalently mixture distributions can only have support size 1). In particular, Maillard and Mannor (2014) specifically highlight the benefit of a collaborative gain across users against using a vanilla UCB for each user.", "startOffset": 44, "endOffset": 1014}, {"referenceID": 1, "context": "(2013): The method combines the RTP method (Anandkumar et al., 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d. As a result, the regret bound in this setting scales linearly with the number of rounds. Our result in this paper shows that with additional access to just user identifiers, we can reduce the regret rate to be sublinear in time. The RTP method has been used as a processing step to the EM algorithm in crowdsourcing (Zhang et al., 2014), but only convergence properties are considered, which is not enough to provide regret guarantees. On the theoretical side, our contribution generalizes the setting of clustered bandits (Maillard and Mannor, 2014; Gentile et al., 2014) in which a hard clustering model is assumed (one user is assigned to one class, or equivalently mixture distributions can only have support size 1). In particular, Maillard and Mannor (2014) specifically highlight the benefit of a collaborative gain across users against using a vanilla UCB for each user. However their setting is less general than assuming a soft clustering of users (one user corresponds to a mixture of classes) across various \u201crepresentative\u201d taste profiles as we study here. The Alternating Least-Squares (ALS) method (Tak\u00e1cs and Tikk, 2012; Mary et al., 2014) has been shown to yield promising experimental results in similar settings where both U and V are unknown. However, no theoretical guarantees are known for this algorithm that may converge to a local optimum in general. The work of Valko et al. (2014) studies stochastic bandits with a linear model over a low-rank (graph Laplacian) structure.", "startOffset": 44, "endOffset": 1658}, {"referenceID": 4, "context": "Proof The proof closely follows that of Gheshlaghi Azar et al. (2013). First, note that by property of the rank 1 decomposition ((Anandkumar et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 7, "context": "We use the result of Lemma 5 from Gheshlaghi Azar et al. (2013) to control (e) and (f).", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "3) are controlled by the perturbation method from Anandkumar et al. (2014b), under the condition that en = \u2016T \u2212 T\u0302\u2016 6 C1 \u03bbmin C (where C1 is a universal constant).", "startOffset": 50, "endOffset": 76}, {"referenceID": 7, "context": "In order to make the condition explicit in our setting, we use the fact that by Lemma 6 from Gheshlaghi Azar et al. (2013), if e n 6 12 min{\u0393, \u03c3min} then", "startOffset": 104, "endOffset": 123}, {"referenceID": 0, "context": "The argument from here can be continued in the same way as in Abbasi-Yadkori et al. (2011) to yield", "startOffset": 62, "endOffset": 91}, {"referenceID": 1, "context": "The final term above can be bounded using Anandkumar et al. (2012, Lemma E.4) \u2013 a version of Theorem 2.5 in Stewart et al. (1990). Assuming (u\u22c4)J is invertible, and \u2225(u\u22c4)\u22121 J ((un)J \u2212 (u\u22c4)J ) \u2225\u2225 2 < 1, then (un)J is invertible, and a resulting bound on the norm of its inverse lets us write \u2225(u\u22c4)\u22121 J \u2225\u2225 2 + \u2225(u\u22c4n)\u22121 J \u2212 (u\u22c4)\u22121 J \u2225\u2225 2 6 \u2225(u\u22c4)\u22121 J \u2225\u2225 2 + \u2016(un)J \u2212 (u)J\u20162 \u2225(u\u22c4)\u22121 J \u2225\u22252 2 1\u2212 \u2225(u\u22c4)\u22121 J ((un)J \u2212 (u\u22c4)J ) \u2225\u2225 2 .", "startOffset": 42, "endOffset": 130}], "year": 2016, "abstractText": "We study the task of maximizing rewards from recommending items (actions) to users sequentially interacting with a recommender system. Users are modeled as latent mixtures of C many representative user classes, where each class specifies a mean reward profile across actions. Both the user features (mixture distribution over classes) and the item features (mean reward vector per class) are unknown a priori. The user identity is the only contextual information available to the learner while interacting. This induces a low-rank structure on the matrix of expected rewards ra,b from recommending item a to user b. The problem reduces to the well-known linear bandit when either useror item-side features are perfectly known. In the setting where each user, with its stochastically sampled taste profile, interacts only for a small number of sessions, we develop a bandit algorithm for the two-sided uncertainty. It combines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the first rigorous regret analysis of this combination, showing that its regret after T user interactions is \u00d5(C \u221a BT ), with B the number of users. An ingredient towards this result is a novel robustness property of OFUL, of independent interest.", "creator": "LaTeX with hyperref package"}}}