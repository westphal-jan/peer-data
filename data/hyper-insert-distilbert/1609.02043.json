{"id": "1609.02043", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Feasibility of Post-Editing Speech Transcriptions with a Mismatched Crowd", "abstract": "manual correction of static speech transcription can probably involve manipulating a selection from plausible mental transcriptions. recent practical work progress has shown the engineering feasibility of employing a mismatched crowd construct for logical speech transcription. however, it is somewhat yet emerging to wholly be readily established whether today a mismatched worker has sufficiently fine - granular mental speech perception to choose options among their the easiest phonetically graded proximate options that consistently are infinitely likely to be generated from the sophisticated trellis logic of an automated asru. for hence, theoretically we consider evaluating five basic languages, arabic, german, hindi, russian and absolute spanish. then for each we generate synthetic, presumably phonetically proximate, imperative options which generally emulate post - editing procedural scenarios of varying mathematical difficulty. we consistently too observe non - obvious trivial gesture crowd ability to comfortably choose behaviors among fine - granular options.", "histories": [["v1", "Wed, 7 Sep 2016 16:05:20 GMT  (253kb)", "http://arxiv.org/abs/1609.02043v1", "HCOMP 2016 Works-in-Progress"]], "COMMENTS": "HCOMP 2016 Works-in-Progress", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["purushotam radadia", "shirish karande"], "accepted": false, "id": "1609.02043"}, "pdf": {"name": "1609.02043.pdf", "metadata": {"source": "CRF", "title": "Feasibility of Post-Editing Speech Transcriptions with a Mismatched Crowd", "authors": ["Purushotam Radadia", "Shirish Karande"], "emails": ["purushotam.radadia@tcs.com,", "shirish.karande@tcs.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n02 04\n3v 1\n[ cs\n.A I]\n7 S\nep 2\n01 6"}, {"heading": "Introduction", "text": "The ASR errors are unavoidable, especially in case of low-resourced languages. Human intervention is required to correct such errors. Crowds can be utilized for post-editing ASR transcripts (McGraw et al. 2010; Wald 2011). The Scribe system proposed in (Lasecki et al. 2012) allows non-experts to caption the speech in near real time (less than 5 sec latency). The study reported in (Gaur 2015) analyzes latency of transcriptions achieved from crowd based on the errors in an ASR output. In above mentioned studies, a worker must be familiar with the spoken language. This can limit the crowd base for under-resourced languages due to the significant mismatch between population of native speakers of languages in the world and the crowd workers who speak that language (Pavlick et al. 2014). Therefore, in (Jyothi and Hasegawa-Johnson 2015; Radadia et al. 2016) the use of a mismatched crowd, which is unfamiliar with the source language has been explored for transcription. Nevertheless, to the best of our knowledge, the utility of mismatched crowd is not explored for transcription post editing. Our contribution in this paper is to employ a mismatched crowd that performs transcription voting tasks for a given utterance. We consider five different languages in our study: Arabic, German, Hindi, Russian, Spanish languages.\nCopyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."}, {"heading": "Corpus Creation", "text": "Word Selection: We select 500 words each from, viz., Arabic, German, Hindi, Russian and Spanish. The words are sampled from publically available pronunciation dictionaries that are being used in ASR systems1. The pronunciation dictionary contains words and their phonetic decompositions in terms of arpabets. The words are selected in such a way that the overall corpus becomes phonetically rich for each language. In addition, the length of selected words is limited to 4-8 arpabets.\nSynthetic Utterances: Since, the word utterances in natural speech can be dependent on the speaker as well as the acoustic conditions of the recording environment, we use synthetic utterances in our experiments. We synthesize language specific word utterances using Google speech synthesis interface via SoundOfText2 utility.\nGeneration of Synthetic Options: Since, we are interested in studying the ability of the crowd to capture the fine grained phonetic information, the valid dictionary word may not provide the proper set of options which are phonetically proximate. Hence, we aim to generate them synthetically. We introduced phonetic substitution errors to generate the perturbed sequence. Further, we only allow the vowel to vowel and consonant to consonant substitutions and not cross substitutions. This helps in maintaining the pronunciability of the word. We present a worker with a set of 5 options (4 strings and a \"none of the above\"). The 4 sequences are generated from the reference sequence to form an option set for a given word. We considered 6 such option sets for each word in the corpus. The sets are, S0124, S0112, S0111, S1234, S1124 and S1111. This results in 24 different sequences for each reference sequence. The set S0124 indicates that the alternate arpabet sequences are 0, 1, 2 and 4 substitutions away from reference, respectively.\nPhoneme to Grapheme Modeling: Since the crowd worker may neither be familiar with the phone set nor the script of a spoken language, we convert all the phoneme sequences to the valid script that workers are familiar with. We chose Roman(English) script for this purpose. To transform the phoneme sequence to the Roman letters (Graphemes), Phoneme to Grapheme models (P2G) models are used. We\n1https://sourceforge.net/projects/cmusphinx/ 2http://soundoftext.com/\nused Carmel Finite State Toolkit (Graehl ) to build P2G model for each language. Consequently, the trained P2G models enable us to create the Roman letter sequences of the perturbed arpabet sequences of the alternate list.\nTask Allocation and Crowd Work: We used the CrowdFlower platform for our experiments. We created 6 tasks for each of the 500 words from the considered 5 languages. Each task was repeated thrice to study vote aggregation, as well as remove spam bias. Each worker was allocated 15 randomly selected tasks and thus we employed 3000 workers to complete 15000 voting tasks."}, {"heading": "Experimental Results", "text": "Crowd Bias against Rejection: We observed that crowd distribution is skewed towards the quadrant where accuracy if true world listed is greater than 50%, while the accuracy if word is not-listed is less than 50%. This indicates that majority of the crowd fails to exercise the Reject option when similar sounding options are available in the list. Hence, in the remainder of the paper, we focus on the cases where the correct word is always present in the list shown to the worker.\nPerformance of Crowd workers: Table 1 shows that as the task difficulty increases (i.e., the options becomes more phonetically similar), the performance is degraded. We highlight that even an accuracy of 45%, as observed for Arabic over the hardest options, is significantly better than that of random guess accuracy of 20% (or 25%). Meanwhile, for the Indo-European languages the average accuracy, especially for the relatively easy tasks (S0124) is greater than 75%.\nLanguage Tree Analysis: The native language of a transcriber can significantly impact his/her perception of the mismatched speech. Table 2 shows the performance of workers, belonging to certain languages/language-families. The language families considered in the Table 2 are represented as: Indo-Aryan={Bengali, Bihari, Urdu, Sinhalese, Gujarati, Marathi, Nepali}, Slavic={Polish, Czech, Croatian, Ukranian, Bulgerian, Serbian, Slovenian, Slovak }, Romance={Italian, Portugese, French} and Tonal={Chinese, Vietnamese}. It is evident that for each of the five languages the best accuracies were exhibited by the natives. The fact that natives are unable to provide near 100% accuracy can be attributed to spam, but, also to the fact that task involves utterances from multiple languages, thus, denying the listener of a context. In addition, one can observe some correlation across related languages, for example, the second best performance for Hindi was by the Indo-Aryan group, for Russian was by the Slavic group and for Spanish was by the Romance group. We could not establish such a pattern for German (where English should have been expected to be the second best.) The correlations in speech perceptions\nmay indeed be influenced by modern cultural influences and shared words, rather than just linguistic connections.\nFeasibility of Aggregating Labels: We consider two very simple vote aggregation strategies. The first one is the traditional majority vote decoding, where, the aggregated label is the majority label if a simple majority exists, else if all three labels are distinct, we simply declare an error. The second algorithm, seeks to break such three way ties, by building worker reputations on the tasks where a majority does exist. The reputation is described by the count of correct votes in simple pure majorities. In a three-way tie, the worker with the highest such repute is declared a winner. Table 3 shows the performance of the Marjority Vote (MV) as well as the Majority Vote with Tie-Breaks (TB). When compared to Table 1, MV provides a gain in accuracy of (1-2)% for Arabic, (2-4)% for German, (3-6)% for Hindi, (3-4)% for Russian and (2-5)% for Spanish. Additionally, it can be observed that reputation based TB provides an additional gain over MV of (4-5)% for Arabic, (4-5)% for German, (2-5)% for Hindi, (3)% for Russian and (3-4)% for Spanish."}, {"heading": "Conclusion", "text": "It was observed that a mismatched crowd is reluctant to use a Reject Option, in future, we may have to provide improved interaction to un-bias the crowd. However, in the cases where the true value is a part of the option list it was observed that a mismatched crowd can indeed identify the correct option even among fine-granular (phonetically proximate) choices. There is early evidence that some language pairs (or demographies) though non-native may be better matches, similarly there is evidence that the voting errors are not correlated as shown by the gain in accuracy through even majority voting based strategies."}], "references": [{"title": "Acquiring speech transcriptions using mismatched crowdsourcing", "author": ["Jyothi", "P. Hasegawa-Johnson 2015] Jyothi", "M. Hasegawa-Johnson"], "venue": "In AAAI,", "citeRegEx": "Jyothi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jyothi et al\\.", "year": 2015}, {"title": "Real-time captioning by groups of non-experts", "author": ["Lasecki"], "venue": "In Proceedings of the 25th annual ACM symposium on User interface software and technology,", "citeRegEx": "Lasecki,? \\Q2012\\E", "shortCiteRegEx": "Lasecki", "year": 2012}, {"title": "Collecting voices from the cloud", "author": ["McGraw"], "venue": null, "citeRegEx": "McGraw,? \\Q2010\\E", "shortCiteRegEx": "McGraw", "year": 2010}, {"title": "The language demographics of amazon mechanical turk. Transactions of the Association for Computational Linguistics 2:79\u201392", "author": ["Pavlick"], "venue": null, "citeRegEx": "Pavlick,? \\Q2014\\E", "shortCiteRegEx": "Pavlick", "year": 2014}, {"title": "On employing a highly mismatched crowd for speech transcription", "author": ["Radadia"], "venue": null, "citeRegEx": "Radadia,? \\Q2016\\E", "shortCiteRegEx": "Radadia", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Manual correction of speech transcription can involve a selection from plausible transcriptions. Recent work has shown the feasibility of employing a mismatched crowd for speech transcription. However, it is yet to be established whether a mismatched worker has sufficiently fine-granular speech perception to choose among the phonetically proximate options that are likely to be generated from the trellis of an ASRU. Hence, we consider five languages, Arabic, German, Hindi, Russian and Spanish. For each we generate synthetic, phonetically proximate, options which emulate post-editing scenarios of varying difficulty. We consistently observe non-trivial crowd ability to choose among fine-granular options.", "creator": "LaTeX with hyperref package"}}}