{"id": "1609.02906", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "Robust Spectral Detection of Global Structures in the Data by Learning a Regularization", "abstract": "western spectral classify methods are historically popular models in detecting global structures in the given data that randomly can be represented initially as a singular matrix. however when reading the typical data matrix sample is sparse or quite noisy, other classic spectral classification methods usually fail to work, due way to localization of eigenvectors ( or normal singular vectors ) versus induced interference by mixing the sparsity operators or diffuse noise. then in this method work, nowadays we propose thus a general method to extensively solve the noisy localization problem above by generally learning a linear regularization matrix distinct from the localized eigenvectors. using kernel matrix perturbation algorithm analysis, we demonstrate that unfortunately the naive learned function regularizations suppress down the stable eigenvalues associated previously with localized eigenvectors and effectively enable us to recover mainly the partially informative eigenvectors representing the resulting global identity structure. we show applications of our method primarily in several inference training problems : community detection in exponential networks, quantum clustering resulting from spatial pairwise intrinsic similarities, rank noise estimation and matrix completion problems. namely using highly extensive coding experiments, separately we illustrate precisely that however our method easily solves across the robust localization problem separately and works down to the theoretical detectability limits in different working kinds of synthetic correlation data. this is especially in contrast with generally existing spectral algorithms extensively based on linear data matrix, orthogonal non - backtracking matrix, laplacians and those with rank - one regularizations, communities which often perform poorly in clearing the sparse case induced with noise.", "histories": [["v1", "Fri, 9 Sep 2016 19:48:29 GMT  (5430kb,D)", "http://arxiv.org/abs/1609.02906v1", "13 pages, 9 figures, Neural Information Processing Systems 2016"]], "COMMENTS": "13 pages, 9 figures, Neural Information Processing Systems 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.SI physics.soc-ph", "authors": ["pan zhang"], "accepted": true, "id": "1609.02906"}, "pdf": {"name": "1609.02906.pdf", "metadata": {"source": "CRF", "title": "Robust Spectral Detection of Global Structures in the Data by Learning a Regularization", "authors": ["Pan Zhang"], "emails": ["panzhang@itp.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "In many statistical inference problems, the task is to detect, from given data, a global structure such as low-rank structure or clustering. The task is usually hard to solve since modern datasets usually have a large dimensionality. When the dataset can be represented as a matrix, spectral methods are popular as it gives a natural way to reduce the dimensionality of data using eigenvectors or singular vectors. In the point-of-view of inference, data can be seen as measurements to the underlying structure. Thus more data gives more precise information about the underlying structure.\nHowever in many situations when we do not have enough measurements, i.e. the data matrix is sparse, standard spectral methods usually have localization problems thus do not work well. One example is the community detection in sparse networks, where the task is to partition nodes into groups such that there are many edges connecting nodes within the same group and comparatively few edges connecting nodes in different groups. It is well known that when the graph has a large connectivity c, simply using the first few eigenvectors of the adjacency matrix A \u2208 {0, 1}n\u00d7n (with Aij = 1 denoting an edge between node i and node j,and Aij = 0 otherwise) gives a good result. In this case, like that of a sufficiently dense Erdo\u030bs-Re\u0301nyi (ER) random graph with average degree c, the spectral density follows Wigner\u2019s semicircle rule, P (\u03bb) = \u221a 4c\u2212 \u03bb2/2\u03c0c, and there is a gap between the edge of bulk of eigenvalues and the informative eigenvalue that represents the underlying community structure. However when the network is large and sparse, the spectral density of the adjacency matrix deviates from the semicircle, the informative eigenvalue is hidden in the bulk of eigenvalues, as displayed in Fig. 1 left. Its eigenvectors associated with largest eigenvalues (which are roughly proportional to log n/ log log n for ER random graphs) are localized on the large-\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 9.\n02 90\n6v 1\n[ st\nat .M\nL ]\n9 S\nep 2\ndegree nodes, thus reveal only local structures about large degrees rather than the underlying global structure. Other standard matrices for spectral clustering [19, 22], e.g. Laplacian, random walk matrix, normalized Laplacian, all have localization problems but on different local structures such as dangling trees.\nAnother example is the matrix completion problem which asks to infer missing entries of matrix A \u2208 Rm\u00d7n with rank r \u221a mn from only few observed entries. A popular method for this problem is based on the singular value decomposition (SVD) of the data matrix. However it is well known that when the matrix is sparse, SVD-based method performs very poorly, because the singular vectors corresponding to the largest singular values are localized, i.e. highly concentrated on high-weight column or row indices.\nA simple way to ease the pain of localization induced by high degree or weight is trimming [6, 13] which sets to zero columns or rows with a large degree or weight. However trimming throws away part of the information, thus does not work all the way down to the theoretical limit in the community detection problem [6, 15]. It also performs worse than other methods in matrix completion problem [25].\nIn recent years, many methods have been proposed for the sparsity-problem. One kind of methods use new linear operators related to the belief propagation and Bethe free energy, such as the nonbacktracking matrix [15] and Bethe Hessian [24]. Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23]. These methods are quite successful in some inference problems in the sparse regime. However in our understanding none of them works in a general way to solve the localization problem. For instance, the non-backtracking matrix and the Bethe Hessian work very well when the graph has a locally-tree-like structure, but they have again the localization problems when the system has short loops or sub-structures like triangles and cliques. Moreover its performance is sensitive to the noise in the data [10]. Rank-one regularizations have been used for a long time in practice, the most famous example is the \u201cteleportation\u201d term in the Google matrix. However there is no satisfactory way to determine the optimal amount of regularization in general. Moreover, analogous to the non-backtracking matrix and Bethe Hessian, the rank-one regularization approach is also sensitive to the noise, as we will show in the paper.\nThe main contribution of this paper is to illustrate how to solve the localization problem of spectral methods for general inference problems in sparse regime and with noise, by learning a proper regularization that is specific for the given data matrix from its localized eigenvectors. In the following text we will first discuss in Sec. 2 that all three methods for community detection in sparse graphs can be put into the framework of regularization. Thus the drawbacks of existing methods can be seen as improper choices of regularizations. In Sec. 3 we investigate how to choose a good regularization that is dedicated for the given data, rather than taking a fixed-form regularization as in the existing approaches. We use matrix perturbation analysis to illustrate how the regularization works in penalizing the localized eigenvectors, and making the informative eigenvectors that correlate with the global structure float to the top positions in spectrum. In Sec. 4 we use extensive numerical experiments to validate our approach on several well-studied inference problems, including the community detection in sparse graphs, clustering from sparse pairwise entries, rank estimation and matrix completion from few entries."}, {"heading": "2 Regularization as a unified framework", "text": "We see that the above three methods for the community detection problem in sparse graphs, i.e. trimming, non-backtracking/Bethe Hessian, and rank-one regularizations, can be understood as doing different ways of regularizations. In this framework, we consider a regularized matrix\nL = A\u0302+ R\u0302. (1)\nHere matrix A\u0302 is the data matrix or its (symmetric) variance, such as A\u0303 = D\u22121/2AD\u22121/2 with D denoting the diagonal matrix of degrees, and matrix R\u0302 is a regularization matrix. The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611T , with \u03b6 being a tunable parameter controlling strength of regularizations. It is also easy to see that in the trimming, A\u0302 is set to be the adjacency matrix and R\u0302 contains entries to remove columns or rows with high degrees from A.\nFor spectral algorithms using the non-backtracking matrix, its relation to form Eq. (1) is not straightforward. However we can link them using the theory of graph zeta function [8] which says that an eigenvalue \u00b5 of the non-backtracking operator satisfies the following quadratic eigenvalue equation,\ndet[\u00b52I \u2212 \u00b5A+ (D \u2212 I)] = 0,\nwhere I is the identity matrix. It indicates that a particular vector v that is related to the eigenvector of the non-backtracking matrix satisfies (A \u2212 D\u2212I\u00b5 )v = \u00b5v. Thus spectral clustering algorithm using the non-backtracking matrix is equivalent to the spectral clustering algorithm using matrix with form in Eq. (1), while A\u0302 = A, R\u0302 = D\u2212I\u00b5 , and \u00b5 acting as a parameter. We note here that the parameter does not necessarily be an eigenevalue of the non-backtracking matrix. Actually a range of parameters work well in practice, like those estimated from the spin-glass transition of the system [24]. So we have related different approaches of resolving localizations of spectral algorithm in sparse graphs into the framework of regularization. Although this relation is in the context of community detection in networks, we think it is a general point-of-view, when the data matrix has a general form rather than a {0, 1} matrix. As we have argued in the introduction, above three ways of regularization work from case to case and have different problems, especially when system has noise. It means that in the framework of regularizations, the effective regularization matrix R\u0302 added by these methods do not work in a general way and is not robust. In our understanding, the problem arises from the fact that in all these methods, the form of regularization is fixed for all kinds of data, regardless of different reasons for the localization. Thus one way to solve the problem would be looking for the regularizations that are specific for the given data, as a feature. In the following section we will introduce our method explicitly addressing how to learn such regularizations from localized eigenvectors of the data matrix."}, {"heading": "3 Learning regularizations from localized eigenvectors", "text": "The reason that the informative eigenvectors are hidden in the bulk is that some random eigenvectors have large eigenvalues, due to the localization which represent the local structures of the system. In the complementary side, if these eigenvectors are not localized, they are supposed to have smaller eigenvalues than the informative ones which reveal the global structures of the graph. This is the main assumption that our idea is based on.\nIn this work we use the Inverse Participation Ratio (IPR), I(v) = \u2211n i=1 v 4 i , to quantify the amount of localization of a (normalized) eigenvector v. IPR has been used frequently in physics, for example for distinguishing the extended state from the localized state when applied on the wave function [3]. It is easy to check that I(v) ranges from 1n for vector { 1\u221a n , 1\u221a n , ..., 1\u221a n } to 1 for vector {0, ..., 0, 1, 0, ..., 0}. That is, a larger I(v) indicates more localization in vector v.\nOur idea is to create a matrix LX with similar structures to A, but with non-localized leading eigenvectors. We call the resulting matrix X-Laplacian, and define it as LX = A+X , where matrix A is the data matrix (or its variant), and X is learned using the procedure detailed below:\nAlgorithm 1: Regularization Learning Input: Real symmetric matrix A, number of eigenvectors q, learning rate \u03b7 = O(1), threshold \u2206. Output: X-Laplacian, LX , whose leading eigenvectors reveal the global structures in A.\n1. Set X to be all-zero matrix. 2. Find set of eigenvectors U = {u1, u2, ..., uq} associated with the first q largest\neigenvalues (in algebra) of LX . 3. Identify the eigenvector v that has the largest inverse participation ratio among the q\neigenvectors in U . That is, find v = argmaxu\u2208U I(u). 4. if I(v) < \u2206, return LX = A+X; Otherwise, \u2200i,Xii \u2190 Xii \u2212 \u03b7v2i , then go to step 2.\nWe can see that the regularization matrix X is a diagonal matrix, its diagonal entries are learned gradually from the most localized vector among the first several eigenvectors. The effect of X is to penalize the localized eigenvectors, by suppressing down the eigenvalues associated with the localized eigenvectors. The learning will continue until all q leading eigenvectors are delocalized, thus are supposed to correlate with the global structure rather than the local structures. As an example, we show the effect of X to the spectrum in Fig. 1. In the left panel, we plot the spectrum of the adjacency matrix (i.e. before learning X) and the X-Laplacian (i.e. after learning X) of a sparse network generated by the stochastic block model with q = 2 groups. For the adjacency matrix in the left panel, localized eigenvectors have large eigenvalues and contribute a tail to the semicircle, covering the informative eigenvalue, leaving only one eigenvalue, which corresponds to the eigenvector that essentially sorts vertices according to their degree, out of the bulk. The spectral density of X-Laplacian is shown in the right panel of Fig. 1. We can see that the right corner of the continues part of the spectral density appearing in the spectrum of the adjacency matrix , is missing here. This is because due to the effect of X , the eigenvalues that are associated with localized eigenvectors in the adjacency matrix are pushed into the bulk, maintaining a gap between the edge of bulk and the informative eigenvalue (being pointed by the left red arrow in the figure).\nThe key procedure of the algorithm is the learning part in step 4, which updates diagonal terms of matrix X using the most localized eigenvector v. Throughout the paper, by default we use learning rate \u03b7 = 10 and threshold \u2206 = 5/n. As \u03b7 = O(1) and v2i = O(1/n), we can treat the learned entries in each step, L\u0302, as a perturbation to matrix LX . After applying this perturbation, we anticipate that an eigenvalue of L changes from \u03bbi to \u03bbi+ \u03bb\u0302i, and an eigenvector changes from ui to ui+ u\u0302i. If we assume that matrix LX is not ill-conditioned, and the first few eigenvectors that we care about are distinct, then we have \u03bb\u0302i = uTi L\u0302ui. Derivation of the above expression is straightforward, but for the completeness we put the derivations in the appendices. In our algorithm, L\u0302 is a diagonal matrix with entries L\u0302ii = \u2212\u03b7v2i with v denoting the identified eigenvector who has the largest inverse participation ratio, so last equation can be written as \u03bb\u0302i = \u2212\u03b7 \u2211 k v 2 ku 2 ik. For the identified vector v, we further have \u03bb\u0302v = \u2212\u03b7\n\u2211 i v4i = \u2212\u03b7I(v). (2)\nIt means the eigenvalue of the identified eigenvector with inverse participation ratio I(v) is decreased by amount \u03b7I(v). That is, the more localized the eigenvector is, the larger penalty on its eigenvalue.\nIn addition to the penalty to the localized eigenvalues, We see that the leading eigenvectors are delocalizing during learning. We have analyzed the change of eigenvectors after the perturbation given by the identified vector v, and obtained (see appendices for the derivations) the change of an eigenvector u\u0302i as a function of all the other eigenvalues and eigenvectors, u\u0302i = \u2211 j 6=i \u2211 k ujkv 2 kuik\n\u03bbi\u2212\u03bbj uj . Then the inverse participation ratio of the new vector ui + u\u0302i can be written as\nI(ui + u\u0302i) = I(ui)\u2212 4\u03b7 n\u2211 l=1 \u2211 j 6=i u2jlv 2 l u 4 il \u03bbi \u2212 \u03bbj \u2212 4\u03b7 n\u2211 l=1 \u2211 j 6=i \u2211 k 6=l u3ilv 2 kujkuikujl \u03bbi \u2212 \u03bbj . (3)\nAs eigenvectors ui and uj are orthogonal to each other, the term 4\u03b7 \u2211n l=1 \u2211 j 6=i u2jlv 2 l u 4 il\n\u03bbi\u2212\u03bbj can be seen as a signal term and the last term can be seen as a cross-talk noise with zero mean. We see that the cross-talk noise has a small variance, and empirically its effect can be neglected. For the leading eigenvector corresponding to the largest eigenvalue \u03bbi = \u03bb1, it is straightforward to see that the signal term is strictly positive. Thus if the learning is slow enough, the perturbation will always decrease the inverse participation ratio of the leading eigenvector. This is essentially an argument for convergence of the algorithm. For other top eigenvectors, i.e. the second and third eigenvectors and so on, though \u03bbi \u2212 \u03bbj is not strictly positive, there are much more positive terms than negative terms in the sum, thus the signal should be positive with a high probability. Thus one can conclude that the process of learning X makes first few eigenvectors de-localizing.\nAn example illustrating the process of the learning is shown in Fig. 2 where we plot the second eigenvector vs. the third eigenvector, at several times steps during the learning, for a network generated by the stochastic block model with q = 3 groups. We see that at t = 0, i.e. without learning, both eigenvectors are localized, with a large range of distribution in entries. The color of eigenvectors encodes the group membership in the planted partition. We see that at t = 0 three colors are mixed together indicating that two eigenvectors are not correlated with the planted partition. At t = 4 three colors begin to separate, and range of entry distribution become smaller, indicating that the localization is lighter. At t = 25, three colors are more separated, the partition obtained by applying k-means algorithm using these vectors successfully recovers 70% of the group memberships. Moreover we can see that the range of entries of eigenvectors shrink to [\u22120.06, 0.06], giving a small inverse participation ratio."}, {"heading": "4 Numerical evaluations", "text": "In this section we validate our approach with experiments on several inference problems, i.e. community detection problems, clustering from sparse pairwise entries, rank estimation and matrix completion from a few entries. We will compare performance of spectral algorithms using the X-Laplacian with recently proposed state-of-the-art spectral methods in the sparse regime."}, {"heading": "4.1 Community Detection", "text": "First we use synthetic networks generated by the stochastic block model [9], and its variant with noise [10]. The standard Stochastic Block Model (SBM), also called the planted partition model, is a popular model to generate ensemble of networks with community structure. There are q groups of nodes and a planted partition {t\u2217i } \u2208 {1, ..., q}. Edges are generated independently according to a q \u00d7 q matrix {pab}. Without loss of generality here we discuss the commonly studied case where the q groups have equal size and where {pab} has only two distinct entries, pab = cin/n if a = b and cout/n if a 6= b. Given the average degree of the graph, there is a so-called detectability transition \u2217 = cout/cin = ( \u221a c \u2212 1)/( \u221a c \u2212 1 + q) [7] , beyond which point it is not possible to obtain any information about the planted partition. It is also known spectral algorithms based on the non-backtracking matrix succeed all the way down to the transition [15]. This transition was recently established rigorously in the case of q = 2 [20, 21]. Comparisons of spectral methods using different matrices are shown in Fig. 7 left. From the figure we see that the X-Laplacian works as\nwell as the non-backtracking matrix, down to the detectability transition. While the direct use of the adjacency matrix, i.e. LX before learning, does not work well when exceeds about 0.1.\nIn the right panel of Fig. 7, each network is generated by the stochastic block model with the same parameter as in the left panel, but with 10 extra cliques, each of which contains 10 randomly selected nodes. Theses cliques do not carry information about the planted partition, hence act as noise to the system. In addition to the non-backtracking matrix, X-Laplacian, and the adjacency matrix, we put into comparison the results obtained using other classic and newly proposed matrices, including Bethe Hessian [24], Normalized Laplacian (N. Laplacian) Lsym = I \u2212 A\u0303, and regularized and normalized Laplacian (R.N. Laplacian) LA = A\u0303 \u2212 \u03b611T, with a optimized regularization \u03b6 (we have scanned the whole range of \u03b6, and chosen an optimal one that gives the largest overlap, i.e. fraction of correctly reconstructed labels, in most of cases). From the figure we see that with the noise added, only X-Laplacian works down to the original transition (of SBM without cliques). All other matrices fail in detecting the community structure with > 0.15.\nWe have tested other kinds of noisy models, including the noisy stochastic block model, as proposed in [10]. Our results show that the X-Laplacian works well (see appendices) while all other spectral methods do not work at all on this dataset [10]. Moreover, in addition to the classic stochastic block model, we have extensively evaluated our method on networks generated by the degree-corrected stochastic block model [12], and the stochastic block model with extensive triangles. We basically obtained qualitatively results as in Fig. 7 that the X-Laplacian works as well as the state-of-the-art spectral methods for the dataset. The figures and detailed results can be found at the appendices.\nWe have also tested real-world networks with an expert division, and found that although the expert division is usually easy to detect by directly using the adjacency matrix, the X-Laplacian significantly improves the accuracy of detection. For example on the political blogs network [1], spectral clustering using the adjacency matrix gives 83 mis-classified labels among totally 1222 labels, while the X-Laplacian gives only 50 mis-classified labels."}, {"heading": "4.2 Clustering from sparse pairwise measurements", "text": "Consider the problem of grouping n items into clusters based on the similarity matrix S \u2208 Rn\u00d7n, where Sij is the pairwise similarity between items i and j. Here we consider not using all pairwise similarities, but only O(n) random samples of them. In other words, the similarity graph which encodes the information of the global clustering structure is sparse, rather than the complete graph. There are many motivations for choosing such sparse observations, for example in some cases all measurements are simply not available or even can not be stored.\nIn this section we use the generative model recently proposed in [26], since there is a theoretical limit that can be used to evaluate algorithms. Without loss of generality, we consider the problem with only q = 2 clusters. The model in [26] first assigns items hidden clusters {ti} \u2208 {1, 2}n, then generates similarity between a randomly sampled pairs of items according to probability distribution,\npin and pout, associated with membership of two items. There is a theoretical limit c\u0302 satisfying 1 c\u0302 = 1 q \u222b ds (pin(s)\u2212pout(s)) 2\npin(s)+(q\u22121)pout(s) , that with c < c\u0302 no algorithm could obtain any partial information of the planted clusters; while with c > c\u0302 some algorithms, e.g. spectral clustering using the Bethe Hessian [26], achieve partial recovery of the planted clusters.\nSimilar to the community detection in sparse graphs, spectral algorithms directly using the eigenvectors of a similarity matrix S does not work well, due to the localization of eigenvectors induced by the sparsity. To evaluate whether our method, the X-Laplacian, solves the localization problem, and how it works compared with the Bethe Hessian, in Fig. 9 we plot the performance (in overlap, the fraction of correctly reconstructed group labels) of three algorithms on the same set of similarity matrices. For all the datasets there are two groups with distributions pin and pout being Gaussian with unit variance and mean 0.75 and \u22120.75 respectively. In the left panel of Fig. 9 the topology of pairwise entries is random graph, Bethe Hessian works down to the theoretical limit, while directly using of the measurement matrix gives a poor performance. We can also see that X-Laplacian has fixed the localization problem of directly using of the measurement matrix, and works almost as good as the Bethe-Hessian. We note that the Bethe Hessian needs to know the parameters (i.e. parameters of distributions pin and pout), while the X-Laplacian does not use them at all.\nIn the right panel of Fig. 9, on top of the ER random graph topology, we add some noisy local structures by randomly selecting 20 nodes and connecting neighbors of each selected node to each other. The weights for the local pairwise were set to 1, so that the noisy structures do not contain information about the underlying clustering. We can see that Bethe Hessian is influenced by noisy local structures and fails to work, while X-Laplacian solves the localization problems induced by sparsity, and is robust to the noise. We have also tested other kinds of noise by adding cliques, or hubs, and obtained similar results (see appendices)."}, {"heading": "4.3 Rank estimation and Matrix Completion", "text": "The last problem we consider in this paper for evaluating the X-Laplacian is completion of a low rank matrix from few entries. This problem has many applications including the famous collaborative filtering. A problem that is closely related to it is the rank estimation from revealed entries. Indeed estimating rank of the matrix is usually the first step before actually doing the matrix completion. The problem is defined as follows: let Atrue = UV T , where U \u2208 Rn\u00d7r and V \u2208 Rm\u00d7r are chosen uniformly at random and r \u221a nm is the ground-true rank. Only few, say c \u221a mn, entries of matrix Atrue are revealed. That is we are given a matrix A \u2208 Rn\u00d7m who contains only subset of Atrue, with other elements being zero. Many algorithms have been proposed for matrix completion, including nuclear norm minimization [5] and methods based on the singular value decomposition [4] etc. Trimming which sets to zero all rows and columns with a large revealed entries, is usually introduced to control the localizations of singular vectors and to estimate the rank using the gap of\nsingular values [14]. Analogous to the community detection problem, trimming is not supposed to work optimally when matrix A is sparse. Indeed in [25] authors reported that their approach based on the Bethe Hessian outperforms trimming+SVD when the topology of revealed entries is a sparse random graph. Moreover, authors in [25] show that the number of negative eigenvalues of the Bethe Hessian gives a more accurate estimate of the rank of A than that based on trimming+SVD.\nHowever, we see that if the topology is not locally-tree-like but with some noise, for example with some additional cliques, both trimming of the data matrix and Bethe Hessian perform much worse, reporting a wrong rank, and giving a large reconstruction error, as illustrated in Fig. 5. In the left panel of the figure we plot the eigenvalues of the Bethe Hessian, and singular values of trimmed matrix A with true rank rtrue = 2. We can see that both of them are continuously distributed: there is no clear gap in singular values of trimmed A, and Bethe Hessian has lots of negative eigenvalues. In this case since matrix A could be a non-squared matrix, we need to define the X-Laplacian as\nLX = ( 0 A A 0 ) \u2212X . The eigenvalues of LX are also plotted in Fig. 5 where one can see clearly\nthat there is a gap between the second largest eigenvalue and the third one. Thus the correct rank can be estimated using the value minimizing consecutive eigenvalues, as suggested in [14].\nAfter estimating the rank of the matrix, matrix completion is done by using a local optimization algorithm [27] starting from initial matrices, that obtained using first r singular vectors of trimming+SVD, first r eigenvectors of Bethe Hessian and X-Laplacian with estimated rank r respectively. The results are shown in Fig. 5 right where we plot the probability that obtained root mean square error (RMSE) is smaller than 10\u22127 as a function of average number of revealed entries per row c, for the ER random-graph topology plus noise represented by several cliques. We can see that X-Laplacian outperforms Bethe Hessian and Trimming+SVD with c \u2265 13. Moreover, when c \u2265 18, for all instances, only X-Laplacian gives an accurate completion for all instances."}, {"heading": "5 Conclusion and discussion", "text": "We have presented the X-Laplacian, a general approach for detecting latent global structure in a given data matrix. It is completely a data-driven approach that learns different forms of regularization for different data, to solve the problem of localization of eigenvectors or singular vectors. The mechanics for de-localizing of eigenvectors during learning of regularizations has been illustrated using the matrix perturbation analysis. We have validated our method using extensive numerical experiments, and shown that it outperforms state-of-the-art algorithms on various inference problems in the sparse regime and with noise.\nIn this paper we discuss the X-Laplacian using directly the data matrix A, but the this is not the only choice. Actually we have tested approaches using various variants of A, such as A\u0303, and found they\nwork as well. We have also tried learning regularizations for the Bethe Hessian, and found it succeeds in repairing Bethe Hessian when Bethe Hessian has localization problem. These indicate that our scheme of regularization-learning is a general spectral approach for hard inference problems."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Perturbation analysis", "text": "After applying the perturbation, We anticipate that an eigenvalue of LX changes from \u03bbi to \u03bbi + \u03bb\u0302i, and an eigenvector changes from ui to ui + u\u0302i. If we assume that matrix LX is not ill-conditioned, and the first few eigenvectors that we care about are distinct, then we have\n(LX + L\u0302)(ui + u\u0302i) = (\u03bbi + \u03bb\u0302i)(ui + u\u0302i).\nBy making use of LXui = \u03bbiui,\nand keeping only first order terms, we have\nL\u0302ui + LX u\u0302i = \u03bbiu\u0302i + \u03bb\u0302iui. (4)\nSince LX is a real symmetric matrix, we can represent u\u0302i as a weighted sum of eigenvectors of LX , as\nu\u0302i = n\u2211 j=1 \u03c9juj , (5)\nwhere \u03c9j is the coefficient and uj is j\u2019th eigenvector of LX . Insert last equation into Eq. (4), we have\nL\u0302ui + LX n\u2211 j=1 \u03c9juj = \u03bbi n\u2211 j=1 \u03c9juj + \u03bb\u0302iui, (6)\nwhich evaluates to\nL\u0302ui + n\u2211 j=1 \u03c9j\u03bbjuj = \u03bbi n\u2211 j=1 \u03c9juj + \u03bb\u0302iui. (7)\nMultiplying uTi to both sides of last equation results to\nuTi L\u0302ui + n\u2211 j=1 \u03c9j\u03bbju T i uj = \u03bbi n\u2211 j=1 uTi \u03c9juj + \u03bb\u0302iu T i ui. (8)\nNotice that in the last equation uTi ui = 1 and the second term in the left hand side and the first term in the right hand side cancel each other, thus we have\n\u03bb\u0302i = u T i L\u0302ui. (9)\nIn our algorithm, L\u0302 is a diagonal matrix with entries L\u0302ii = \u2212\u03b7v2i where vi denotes the i\u2019th element of the selected eigenvector v who has the largest inverse participation ratio. Thus the shift of an eigenvalue \u03bbj associated with eigenvector uj (which is different from v) is then\n\u03bb\u0302j = \u2212\u03b7 n\u2211\ni=1\nv2i u 2 ji. (10)\nFor the selected vector v, the change of its eigenvalue is\n\u03bb\u0302v = \u2212\u03b7 n\u2211\ni=1\nv4i = \u2212\u03b7I(v). (11)\nThat is, the amount of decreasing of eigenvalue associated with the selected vector is proportional to its inverse participation ratio.\nIn addition to the shift of eigenvalues, we can also derive the change of eigenvectors after perturbation. Multiplying transpose of an eigenvector uj to both sides of Eq.(7) results to\nuTj L\u0302ui + n\u2211 k=1 \u03c9k\u03bbku T j uk = \u03bbi n\u2211 k=1 uTj \u03c9kuk, (12)\nwhich evaluates to uTj L\u0302ui + \u03c9j\u03bbj = \u03bbi\u03c9j , (13)\nwhere we can find that\n\u03c9j = uTj L\u0302ui\n\u03bbi \u2212 \u03bbj . (14)\nGiven that the perturbation is L\u0302ii = \u2212\u03b7v2i , we have an expression for the change of an eigenvector\nu\u0302i = \u2211 j 6=i uTj L\u0302ui \u03bbi \u2212 \u03bbj uj\n= \u2212\u03b7 \u2211 j 6=i \u2211 k ujkv 2 kuik \u03bbi \u2212 \u03bbj uj . (15)\nNotice that the inverse participate ratio of the new vector ui + u\u0302i is\nI(ui + u\u0302i) = n\u2211 l=1 (uil + u\u0302il) 4,\n(16)\nExpand above equation to the first order of u\u0302il, we have\nI(ui + u\u0302i) \u2248 I(ui) + 4 n\u2211\nl=1\nu3ilu\u0302il\n= I(ui)\u2212 4\u03b7 n\u2211\nl=1 u3il \u2211 j 6=i \u2211 k ujkv 2 kuik \u03bbi \u2212 \u03bbj ujl\n= I(ui)\u2212 4\u03b7 n\u2211\nl=1 \u2211 j 6=i u2jlv 2 l u 4 il \u03bbi \u2212 \u03bbj \u2212 4\u03b7 n\u2211 l=1 \u2211 j 6=i \u2211 k 6=l u3ilv 2 kujkuikujl \u03bbi \u2212 \u03bbj\n(17)"}, {"heading": "6.2 Detailed process of learning a regularization", "text": "In Fig. 6.2 we plot the evolution of eigenvalues, overlap and the Inverse Participation Ratio (IPR) for the second, third and forth eigenvectors during learning of the X-Laplacian for a network generated by the stochastic block model. The network has a community structure with 3 groups, however the first three eigenvectors of the adjacency matrix are localized (see left panel at t = 0) and do not reveal the underlying community structure (see the right panel at t small. We can also see from the left panel that the IPR of them are decreasing as t increases during learning. From the middle panel of the figure, we see that all the 3 eigenvalues are decreasing, while the spectral gap D3 \u2212 D4 is increasing during learning. It is interesting to see that at t = 4, there is a exchange of positions of the third eigenvector and the forth eigenvector. This gives a bump of the IPR, as well as an increase of accuracy of detection (characterized by overlap) at t = 4."}, {"heading": "6.3 Additional numerical evaluations on community detections", "text": "Here we compare the performance of the X-Laplacian with other state-of-art spectral algorithms on variants of the stochastic block model, namely the degree-corrected stochastic block model [12] and the triangular stochastic block model [28] which is the stochastic block model with triangles. It is known that in the stochastic block model, there is a detectability transition at\n\u2217 = ( \u221a c\u0302\u2212 1)/( \u221a c\u0302\u2212 1 + q),\nwhere c\u0302 is the excess average degree\nc\u0302 =\n\u2329 k2 \u232a\n\u3008k\u3009 \u2212 1,\nand the spectral clustering algorithm based on the non-backtracking matrix achieves this threshold. In the left panel of Fig.7 we compare the performance (evaluated using the overlap, fraction of correctly reconstructed labels) of spectral algorithms using the adjacency matrix, the non-backtracking matrix and the X-Laplacian on networks generated by the degree corrected stochastic block model with a power-law degree distribution with exponent \u22122.5. As the figure shows, our approach works even better than the algorithm using the nonbacktracking matrix, this is because when the networks size (104) is not large enough, the long tails of degree distribution creates short loops in the network, downgrading the performance of the algorithm using the nonbacktracking matrix which is supposed to work optimally in the locally-tree like networks.\nFor the triangular stochastic block model, due to the presence of triangles, the non-backtracking matrix suffers from short loops and does not work well. In this case the generalized non-backtracking matrix, which runs on a factor graph with both edges and triangles treated as function nodes, works down to the transition [28]. In the right panel of Fig. 7 we compare the performance of the spectral algorithm using the adjacency matrix, generalized non-backtracking matrix and the X-Laplacian , and we can see that X-Laplacian works as well as the generalized non-backtracking matrix and down to the transition.\nIt has been reported in [10] that on the perturbed stochastic block model, spectral algorithms including the one using Bethe Hessian do fail in detecting the community structures, while other method, e.g. semi-definite programming, works well. In the perturbed stochastic block model, after a network is generated by the stochastic block model, neighbors of some randomly selected nodes are connected to each other acting as noise to the underlying community structure, In Fig. 8 we numerically examined the performance of X-Laplacian using A\u0303, i.e. LX = A\u0303 + X , on networks generated by perturbed stochastic block model [10], with exactly the same network size and parameters as in [10] (see Fig. 5 in their appendices), with parameter a and b denoting expected number of edges per node connecting nodes in the same group and in different groups, respectively. By comparing Fig. 8 with Fig. 5 in appendices of [10] we can see from X-Laplacian works similarly to the semi-definite programming while Bethe-Hessian based method does not work at all."}, {"heading": "6.4 Additional numerical evaluations on spectral clustering using pairwise similarity measurements", "text": "In this section we compare the performance of spectral algorithms using the data matrix, the Bethe Hessian and the X-Laplacian, on the model recently proposed in [25], which generates pairwise measurements between\ntwo groups of nodes from different probability distributions. Two distributions pin and pout are chosen to be Gaussian with unit variance and mean 0.75 and\u22120.75 respectively. On top of the network we add two different kinds of noise, i.e. cliques and hubs to the random graph topology. And from figures we can see that the results are qualitatively similar to right panel of Fig. 4 in the main text where X-Laplacian outperforms both Bethe Hessian and X-Laplacian in reconstructing the planted partition."}], "references": [{"title": "The political blogosphere and the 2004 us election: divided they blog", "author": ["L.A. Adamic", "N. Glance"], "venue": "Proceedings of the 3rd international workshop on Link discovery, pages 36\u201343. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "et al", "author": ["A.A. Amini", "A. Chen", "P.J. Bickel", "E. Levina"], "venue": "Pseudo-likelihood methods for community detection in large sparse networks. The Annals of Statistics, 41(4):2097\u20132122", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Atomic vibrations in vitreous silica", "author": ["R. Bell", "P. Dean"], "venue": "Discussions of the Faraday society, 50:55\u201361", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1970}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization, 20(4):1956\u20131982", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, 9(6):717\u2013772", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["A. COJA-OGHLAN"], "venue": "Combinatorics, Probability and Computing, 19:227\u2013284,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Phys. Rev. E,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Zeta functions of finite graphs and representations of p-adic groups", "author": ["K.-i. Hashimoto"], "venue": "Advanced Studies in Pure Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Stochastic blockmodels: First steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social networks, 5(2):109\u2013137", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Phase transitions in semidefinite relaxations", "author": ["A. Javanmard", "A. Montanari", "F. Ricci-Tersenghi"], "venue": "Proceedings of the National Academy of Sciences, 113(16):E2218", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Impact of regularization on spectral clustering", "author": ["A. Joseph", "B. Yu"], "venue": "arXiv preprint arXiv:1312.1733", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic blockmodels and community structure in networks", "author": ["B. Karrer", "M.E.J. Newman"], "venue": "Phys. Rev. E,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Low-rank matrix completion with noisy observations: a quantitative comparison", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on, pages 1216\u20131222. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "S. Oh", "A. Montanari"], "venue": "Information Theory, 2009. ISIT 2009. IEEE International Symposium on, pages 324\u2013328. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral redemption in clustering sparse networks", "author": ["F. Krzakala", "C. Moore", "E. Mossel", "J. Neeman", "A. Sly", "L. Zdeborov\u00e1", "P. Zhang"], "venue": "Proc. Natl. Acad. Sci. USA, 110(52):20935\u201320940", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse random graphs: regularization and concentration of the laplacian", "author": ["C.M. Le", "E. Levina", "R. Vershynin"], "venue": "arXiv preprint arXiv:1502.03049", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Concentration and regularization of random graphs", "author": ["C.M. Le", "R. Vershynin"], "venue": "arXiv preprint arXiv:1506.00669", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["J. Lei", "A. Rinaldo"], "venue": "Consistency of spectral clustering in stochastic block models. The Annals of Statistics, 43(1):215\u2013237", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "and Pertinence", "author": ["U.V. Luxburg", "M. Belkin", "O. Bousquet"], "venue": "A tutorial on spectral clustering. Stat. Comput", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Community detection thresholds and the weak ramanujan property", "author": ["L. Massouli\u00e9"], "venue": "Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 694\u2013703. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic block models and reconstruction", "author": ["E. Mossel", "J. Neeman", "A. Sly"], "venue": "arXiv preprint arXiv:1202.1499", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849\u2013856", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Regularized spectral clustering under the degree-corrected stochastic blockmodel", "author": ["T. Qin", "K. Rohe"], "venue": "Advances in Neural Information Processing Systems, pages 3120\u20133128", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Spectral clustering of graphs with the bethe hessian", "author": ["A. Saade", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "Advances in Neural Information Processing Systems, pages 406\u2013414", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix completion from fewer entries: Spectral detectability and rank estimation", "author": ["A. Saade", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1261\u20131269. Curran Associates, Inc.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering from sparse pairwise measurements", "author": ["A. Saade", "M. Lelarge", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "arXiv preprint arXiv:1601.06683", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Triangular stochastic block model", "author": ["P. Zhang"], "venue": "unpublished", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Other standard matrices for spectral clustering [19, 22], e.", "startOffset": 48, "endOffset": 56}, {"referenceID": 21, "context": "Other standard matrices for spectral clustering [19, 22], e.", "startOffset": 48, "endOffset": 56}, {"referenceID": 5, "context": "A simple way to ease the pain of localization induced by high degree or weight is trimming [6, 13] which sets to zero columns or rows with a large degree or weight.", "startOffset": 91, "endOffset": 98}, {"referenceID": 12, "context": "A simple way to ease the pain of localization induced by high degree or weight is trimming [6, 13] which sets to zero columns or rows with a large degree or weight.", "startOffset": 91, "endOffset": 98}, {"referenceID": 5, "context": "However trimming throws away part of the information, thus does not work all the way down to the theoretical limit in the community detection problem [6, 15].", "startOffset": 150, "endOffset": 157}, {"referenceID": 14, "context": "However trimming throws away part of the information, thus does not work all the way down to the theoretical limit in the community detection problem [6, 15].", "startOffset": 150, "endOffset": 157}, {"referenceID": 24, "context": "It also performs worse than other methods in matrix completion problem [25].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "One kind of methods use new linear operators related to the belief propagation and Bethe free energy, such as the nonbacktracking matrix [15] and Bethe Hessian [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 23, "context": "One kind of methods use new linear operators related to the belief propagation and Bethe free energy, such as the nonbacktracking matrix [15] and Bethe Hessian [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 1, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 10, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 15, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 16, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 17, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 22, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 9, "context": "Moreover its performance is sensitive to the noise in the data [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 10, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 15, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 16, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 17, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 22, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 7, "context": "However we can link them using the theory of graph zeta function [8] which says that an eigenvalue \u03bc of the non-backtracking operator satisfies the following quadratic eigenvalue equation,", "startOffset": 65, "endOffset": 68}, {"referenceID": 23, "context": "Actually a range of parameters work well in practice, like those estimated from the spin-glass transition of the system [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "IPR has been used frequently in physics, for example for distinguishing the extended state from the localized state when applied on the wave function [3].", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "First we use synthetic networks generated by the stochastic block model [9], and its variant with noise [10].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "First we use synthetic networks generated by the stochastic block model [9], and its variant with noise [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "Given the average degree of the graph, there is a so-called detectability transition \u2217 = cout/cin = ( \u221a c \u2212 1)/( \u221a c \u2212 1 + q) [7] , beyond which point it is not possible to obtain any information about the planted partition.", "startOffset": 126, "endOffset": 129}, {"referenceID": 14, "context": "It is also known spectral algorithms based on the non-backtracking matrix succeed all the way down to the transition [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "This transition was recently established rigorously in the case of q = 2 [20, 21].", "startOffset": 73, "endOffset": 81}, {"referenceID": 20, "context": "This transition was recently established rigorously in the case of q = 2 [20, 21].", "startOffset": 73, "endOffset": 81}, {"referenceID": 23, "context": "In addition to the non-backtracking matrix, X-Laplacian, and the adjacency matrix, we put into comparison the results obtained using other classic and newly proposed matrices, including Bethe Hessian [24], Normalized Laplacian (N.", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "We have tested other kinds of noisy models, including the noisy stochastic block model, as proposed in [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "Our results show that the X-Laplacian works well (see appendices) while all other spectral methods do not work at all on this dataset [10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "Moreover, in addition to the classic stochastic block model, we have extensively evaluated our method on networks generated by the degree-corrected stochastic block model [12], and the stochastic block model with extensive triangles.", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "For example on the political blogs network [1], spectral clustering using the adjacency matrix gives 83 mis-classified labels among totally 1222 labels, while the X-Laplacian gives only 50 mis-classified labels.", "startOffset": 43, "endOffset": 46}, {"referenceID": 25, "context": "In this section we use the generative model recently proposed in [26], since there is a theoretical limit that can be used to evaluate algorithms.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "The model in [26] first assigns items hidden clusters {ti} \u2208 {1, 2}, then generates similarity between a randomly sampled pairs of items according to probability distribution,", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "spectral clustering using the Bethe Hessian [26], achieve partial recovery of the planted clusters.", "startOffset": 44, "endOffset": 48}, {"referenceID": 25, "context": "The model used to generate pairwise measurements is proposed in [26], see text for detailed descriptions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Many algorithms have been proposed for matrix completion, including nuclear norm minimization [5] and methods based on the singular value decomposition [4] etc.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Many algorithms have been proposed for matrix completion, including nuclear norm minimization [5] and methods based on the singular value decomposition [4] etc.", "startOffset": 152, "endOffset": 155}, {"referenceID": 13, "context": "singular values [14].", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Indeed in [25] authors reported that their approach based on the Bethe Hessian outperforms trimming+SVD when the topology of revealed entries is a sparse random graph.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Moreover, authors in [25] show that the number of negative eigenvalues of the Bethe Hessian gives a more accurate estimate of the rank of A than that based on trimming+SVD.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Thus the correct rank can be estimated using the value minimizing consecutive eigenvalues, as suggested in [14].", "startOffset": 107, "endOffset": 111}], "year": 2016, "abstractText": "Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.", "creator": "LaTeX with hyperref package"}}}