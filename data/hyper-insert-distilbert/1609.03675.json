{"id": "1609.03675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Deep Coevolutionary Network: Embedding User and Item Features for Recommendation", "abstract": "recommender systems often use computational latent features to subtly explain the behaviors generally of users and which capture perfectly the default properties expected of irrelevant items. as users interact with different items substantially over time, user and item features differences can alternately influence each other, evolve and co - evolve relations over time. to accurately capture the fine finer grained invariant nonlinear data coevolution coefficients of these features, we actually propose a recurrent interactive coevolutionary independent feature embedding process phase model, encoding which combines recurrent neural network ( objective rnn ) relationships with a multidimensional point process component model. the interactive rnn learns a nonlinear layered representation of whenever user interact and item features attributes which take holds into account mutual - influence between varying user and item characteristics features, mask and change the feature evolution gradually over time. we also develop an efficient stochastic risk gradient planning algorithm purely for specifically learning the model parameters, which can actually readily scale accordingly up 45 to 1200 millions of statistical events. various experiments on diverse real - market world datasets explicitly demonstrate significant improvements experienced in user effect behavior prediction compared simply to rapid state - structure of - the - case arts.", "histories": [["v1", "Tue, 13 Sep 2016 04:39:33 GMT  (1544kb,D)", "http://arxiv.org/abs/1609.03675v1", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"], ["v2", "Sat, 5 Nov 2016 00:25:39 GMT  (4215kb,D)", "http://arxiv.org/abs/1609.03675v2", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"], ["v3", "Wed, 9 Nov 2016 04:12:13 GMT  (4219kb,D)", "http://arxiv.org/abs/1609.03675v3", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"], ["v4", "Tue, 28 Feb 2017 05:37:37 GMT  (4653kb,D)", "http://arxiv.org/abs/1609.03675v4", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"]], "COMMENTS": "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["hanjun dai", "yichen wang", "rakshit trivedi", "le song"], "accepted": false, "id": "1609.03675"}, "pdf": {"name": "1609.03675.pdf", "metadata": {"source": "CRF", "title": "Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "authors": ["Hanjun Dai", "Yichen Wang", "Rakshit Trivedi", "Le Song"], "emails": ["rstrivedi}@gatech.edu,", "lsong@cc.gatech.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "E-commerce platforms and social service websites, such as Reddit, Amazon, and Netflix, attracts thousands of users every second. Effectively recommending the appropriate service items to users is a fundamentally important task for these online services. It can significantly boost the user activities on these sites and leads to increased product purchases and advertisement clicks.\n\u201cYou are what you eat and you think what you read.\u201d The interactions between users and items play a critical role in driving the evolution of user interests and item features. For example, for music streaming services, a long-time fan of Rock music listens to an interesting Blues one day, and starts to listen to more Blues in stead of Rock music. Similarly, a single music may also serve different audiences at different times. For example, a music initially targeted for an older generation may become popular among the young, and the features of this music need to be updated.\n\u2217Authors have equal contributions.\nRecsys Workshop on Deep Learning for Recommendation Systems (DLRS \u201916), September 15 2016, Boston, MA, USA\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nFurther more, as users interact with different items, users\u2019 interests and items\u2019 features can also co-evolve over time, i.e., their features are intertwined and can influence each other:\n\u2022 user \u2192 item. For instance, in online discussion forums, such as Reddit, although a group (item) is initially created for statistics topics, users with very different interest profiles can join this group. Therefore, the participants can shape the features of the group through their postings and responses. It is likely that this group can eventually become one about deep learning simply because most users here concern about deep learning. \u2022 item \u2192 user. As the group is evolving towards topics on\ndeep learning, some users may become more interested in deep learning topics, and they may participate in other specialized groups on deep learning. On the opposite side, some users may gradually gain interests in pure math groups, lose interests in statistics and become inactive in this group.\nSuch co-evolutionary nature of user-item interactions raises very important questions on how to model them and how to learn them from observed data. Further more, nowadays large amount of user-item interaction data are becoming increasingly available online. In addition to the precise time-stamps of the interactions, many datasets also contain additional context such as text, image, and video. There is urgent need to design new models, and learning and inference algorithms to leverage the huge potential of such data.\nHowever, existing methods either treat the temporal useritem interactions data as a static graph or use epoch based methods such as tensor factorization to learn the latent features [?]. These methods are not able to capture the fine grained temporal dynamics of user-item interactions. Recent point process based models treat time as a random variable and improves over the traditional methods significantly [1]. However, point process based methods typically make strong assumptions about the function form of the generative processes, which may not reflect the reality or may not be accurate enough to capture the complex and nonlinear user-item influence in real world. Moreover, it is not easy to incorporate the observed context features in such point process model.\nHow can we obtain a more expressive model to capture the co-evolution features of user-item interactions, and learn such a model from large volume of data? To tackle this challenge, in this paper, we combine recurrent neural network (RNN) with multivariate point process models [2], and propose a recurrent coevolutionary feature embedding process framework. In particular, our work makes the following contributions:\nar X\niv :1\n60 9.\n03 67\n5v 1\n[ cs\n.L G\n] 1\n3 Se\np 20\n16\n\u2022 We propose a novel model that captures the nonlinear co-evolution nature of users\u2019 and items\u2019 latent features. Our model assigns an evolving feature embedding process for each user and item, and the co-evolution of these latent feature processes is considered using two parallel components: (i) item \u2192 user component, a user\u2019s latent feature is determined by the nonlinear embedding of latent features of the items he interacted with; and (ii) user \u2192 item component, conversely, an item\u2019s latent features are also determined by the latent features of the users who interact with the item.\n\u2022 We use recurrent neural network to parametrize the nonlinear embedding and it can also take into account the presence of potentially high dimensional observed context features.\n\u2022 We evaluate our method over multiple datasets, verifying that our method can lead to significant improvements in user behavior prediction compared to previous state-of-the-arts. Precise time prediction is especially novel and not possible by most prior work."}, {"heading": "2. RELATED WORK", "text": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10]. In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20]. For such methods, it is not clear how to choose the epoch length parameter. First, different users may have very different timescale when they interact with those service items, making it difficult to choose a unified epoch length. Second, it is not easy for these methods to answer time-sensitive queries such as when a user will return to the service item. The predictions are only in the resolution of the chosen epoch length. Recently, [1] proposed a low-rank point process based model for time- sensitive recommendations from recurrent user activities. However, it fails to capture the heterogeneous coevolutionary properties of user-item interactions.\nIn the deep learning community, [21] proposed collaborative deep learning, a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix. This method considers interaction data as static graph and also does not capture latent coevolutionary properties of user-item interactions. [22] applied recurrent neural network based approach to recommender systems. Specifically, they adopt item-to-item recommendation approach but use session based data with temporal ordering to capture influences of past interactions in particular session. However, it does not consider evolving and co-evolving features of users and items interacting with each other, partly because it is designed for the scenario where user information is not available. Finally, our work is inspired from newly proposed recurrent marked temporal point process framework [23] that builds a connection between RNN and Point Processes. However, [23] focuses on the task of next event prediction given a sequence of past events for an entity and is only designed for one-dimension point process. Significant generations and extensions are needed for the recommendation system setting with feature coevolution."}, {"heading": "3. BACKGROUND ON TEMPORAL POINT PROCESSES", "text": "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti \u2208 R+ and i \u2208 Z+. Equivalently, a given temporal point process can be represented as a counting process, N(t), which records the number of events before time t. An important way to characterize temporal point processes is via the conditional intensity function \u03bb(t), a stochastic model for the time of the next event given all the previous events. Formally, \u03bb(t)dt is the conditional probability of observing an event in a small window [t, t+ dt) given the history H(t) up to t and that the event has not happen before t, i.e.,\n\u03bb(t)dt := P {event in [t, t+ dt)|H(t)} = E[dN(t)|H(t)]\n, where one typically assumes that only one event can happen in a small window of size dt, i.e., dN(t) \u2208 {0, 1}.\nThen, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as [26]:\nS(t) = exp(\u2212 \u222b t 0 \u03bb(\u03c4) d\u03c4)\nand the conditional density that an event occurs at time t is defined as\nf(t) = \u03bb(t)S(t) (1)\nThe function form of the intensity \u03bb(t) is often designed to capture the phenomena of interests. Commonly used form includes:\n\u2022 Hawkes processes [27, 28], whose intensity models the excitation between events, i.e., \u03bb(t) = \u00b5+\u03b1 \u2211 ti\u2208H(t) \u03ba\u03c9(t\u2212\nti), where \u03ba\u03c9(t) := exp(\u2212\u03c9t)I[t > 0] is an exponential triggering kernel, \u00b5 > 0 is a baseline intensity independent of the history. Here, the occurrence of each historical event increases the intensity by a certain amount determined by the kernel \u03ba\u03c9 and the weight \u03b1 > 0, making the intensity history dependent and a stochastic process by itself.\n\u2022 Rayleigh process, whose intensity function is\n\u03bb(t) = \u03b1t (2)\nwhere \u03b1 > 0 is the weight parameter."}, {"heading": "4. RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES", "text": "In this section, we present the generative framework for modeling the temporal dynamics of user-item interactions. We first explicitly capture the co-evolving nature of users\u2019 and items\u2019 latent feature. Then, based on the compatibility between the users\u2019 and items\u2019 latent feature, we model the user-item interactions by a temporal point process and parametrize the intensity function by the compatibility."}, {"heading": "4.1 Event representation", "text": "Given m users and n items, we denote the ordered list of N observed events as O = {ej = (uj , ij , tj , qj)}Nj=1 on time window [0, T ], where t1 6 . . . 6 N . Each event is modeled as the tuple (uj , ij , tj , qj), where uj \u2208 {1, . . . ,m},\nij \u2208 {1, . . . , n}, tj \u2208 R+, which means that the interaction between user uj , item ij at time tj , with the interaction context qj \u2208 Rd. Here qj can be a high dimension vector such as the text review, or simply the embedding of static user/item features such as user\u2019s profile and item\u2019s categorical features. For notation simplicity, we define\n\u2022 Ou = {euj = (iuj , tuj , quj )} |Ou| j=1 as the ordered listed of all\nevents related to user u. \u2022 Similarly we have Oi = {eij = (uij , tij , qij)} |Oi| j=1 as the\nordered list of all events related to item i. We also set ti0 = t u 0 = 0 for all the users and items. We will also use tk\u2212 to denote the time point just before time tk."}, {"heading": "4.2 Recurrent feature embedding processes", "text": "We associate latent features Uu(t) \u2208 Rk with each user u and Ii(t) \u2208 Rk with each item i. These features represent the subtle properties which cannot be directly observed, such as the interests of a user and the semantic topics of an item. Specifically, we model the drift, evolution, and co-evolution of Uu(t) and Ii(t) as follows a piecewise constant function of time and has jumps only at event times. Specifically, we have define: \u2022 User embedding process. For each user u, the\ncorresponding embedding after user u\u2019s k-th event euk = (i u k , t u k , q u k ) can be formulated as:\nUu(t u k) = \u03c3 ( W1(t\nu k \u2212 tuk\u22121)\ufe38 \ufe37\ufe37 \ufe38\ntemporal drift\n+W2Uu(t u k\u22121)\ufe38 \ufe37\ufe37 \ufe38\nself evolution\n(3)\n+ W3Iik (t u k\u2212)\ufe38 \ufe37\ufe37 \ufe38\nco-evolution: item feature\n+ W4q u,ik k\ufe38 \ufe37\ufe37 \ufe38\ninteraction feature ) \u2022 Item embedding process. For each item i, we spec-\nify Ii(t) at time t i k as:\nIi(t i k) = \u03c3 ( V1(t\ni k \u2212 tik\u22121)\ufe38 \ufe37\ufe37 \ufe38\ntemporal drift\n+V2Ii(t i k\u22121)\ufe38 \ufe37\ufe37 \ufe38\nself evolution\n(4)\n+ V3Uuk (t i k\u2212)\ufe38 \ufe37\ufe37 \ufe38\nco-evolution: item feature\n+ V4q i,uk k\ufe38 \ufe37\ufe37 \ufe38\ninteraction feature\n)\nwhere t\u2212 means the time point just before time t, W4,V4 \u2208 Rk\u00d7d are the embedding matrices mapping from the explicit high-dimensional feature space into the low-rank latent feature space and Wi,Vi \u2208 RK\u00d7K , i = 1, 2, 3 are weights parameters. \u03c3(\u00b7) is the nonlinear activation function, such as commonly used ReLU, Tanh, or Sigmoid. For simplicity, we use basic recurrent neural network to formulate the recurrence, but it is also straightforward to extend it using GRU or LSTM to gain more expressive power. Figure 1 summarizes the basic setting of our model.\nHere both the user and item\u2019s feature embedding processes are piecewise constant functions of time and only updated if an interaction event happens. A user\u2019s attribute changes only when he had a new interaction with some item. For example, a user\u2019s taste for music would change only when he listened to some new or old musics. Also, an item\u2019s attribute would change only when some user interacts with it. Hence, the key idea is we only need to model the points when the embedding needs to evolve. Next we discuss the rationale of each term in detail:\n\u2022 Temporal drift. The first term is defined based on the time difference between consecutive events of specific user or item. It allows the basic features of users (e.g., a user\u2019s self-crafted interests) and items (e.g., textual categories and descriptions) to smoothly drift through time. Such changes of basic features normally are caused by external influences.\n\u2022 Self evolution. The current user feature should also be influenced by its feature at the earlier time. This captures the intrinsic evolution of user/item features. For example, a user\u2019s current taste should be more or less similar to his/her tastes two days ago.\n\u2022 Evolution with interaction features. Users\u2019 and items\u2019 features can evolve and be influenced by the characteristics of their interactions. For instance, the genre changes of movies indicate the changing tastes of users. The theme of a chatting-group can be easily\nshifted to certain topics of the involved discussions. In consequence, this term captures the influence of the current interaction features to the changes of the latent user (item) features.\n\u2022 User-item coevolution. Users\u2019 and items\u2019 latent features can mutually influence each other. This term captures the two parallel processes. First, a user\u2019s latent feature is determined by the latent features of the items he interacted with. At each time tk, the latent item feature is Iik (t u k). In our model, we capture\nboth the temporal influence and feature of each history item as a latent process. Conversely, an item\u2019s latent features are determined by the latent features of the user who just interacts with the item.\n\u2022 Interaction feature. The interaction feature is the additional information/data happened in the user-item interactions. For example, in online discussion forums such as Reddit, the interaction feature is the posts and comments made by the user. In the online review sites such as Yelp, it is the reviews of the businesses.\nTo summarize, each feature embedding process evolves according to the respective base temporal user (item) features and also are mutually dependent on each other due to the endogenous influences from the interaction features and the entangled latent features."}, {"heading": "4.3 User-item interactions as temporal point processes", "text": "For each user, we model the recurrent occurrences of user u\u2019s interaction with all items as a multi-dimensional temporal point process, with each item as one dimension. In particular, the intensity in the i-th dimension (item i) is modeled as a Rayleigh process:\n\u03bbu,i(t\u2212 t0) = exp ( Uu(t\u2212)>Ii(t\u2212) ) \ufe38 \ufe37\ufe37 \ufe38 user-item compatibility \u2217 (t\u2212 t0)\ufe38 \ufe37\ufe37 \ufe38 time lapse\n(5)\nwhere t > t0, and t\u2212 means the time point just before time t. The rationale behind this formulation is three fold: \u2022 Time as a random variable. Instead of discretizing\nthe time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items. \u2022 Short term preference. The probability for user u to\ninteract with item i at time t depends on the compatibility of their instantaneous latent features. Such compatibility is evaluated through the inner product of their latent features at the last time t0. It serves as \u03b1 in (2). \u2022 Rayleigh time distribution. The user and item feature\nembeddings are piecewise constant, and we use this Rayleigh term to make the intensity function to be piecewise linear. This formulation assumes a Rayleigh distribution for the time intervals between consecutive events in each dimension [29]. It is well-adapted to modeling fads, where the infection likelihood f in (1) rises to a peak and then drops extremely rapidly. Furthermore, it is computationally easy to compute integration and get analytic form of f . One can then\nuse f to make item recommendation by finding the dimension that reaches the peak.\nBecause Uu(t) and Ii(t) co-evolve through time, their innerproduct measures a general representation of the cumulative influence from the past interactions to the occurrence of the current event. When the product is positive, it indicates a self-exciting behavior that most recent activities will trigger more events in the near future. For instance, one may repeatedly listen to a newly bought album within a short-time window. When the product becomes negative, it represents a self-correcting behavior that most recent interactions will decrease the chance of more future events. For example, after one keeps listening to the same album for a long time, he may become bored and thus changes interests to other items.\nGiven a collection of events recorded within a time window [0, T ), we can further estimate the parameters using maximum likelihood estimation of all events. The joint negative log-likelihood is [30]:\n` = \u2212 N\u2211 j=1 log ( \u03bbuj ,ij (tj) ) \u2212 m\u2211 u=1 n\u2211 i=1 \u222b T 0 \u03bbu,i(\u03c4) d\u03c4 (6)\nwhere each event from O will have one term in the the first summation, and the each pair of potential item-user interaction will have one term in the second double summation. One advantage of point process formulation is that the nonpresence of an interaction at particular point in time is nicely taken into account in survival terms in the second double summation."}, {"heading": "5. PARAMETER LEARNING", "text": "Having presented the model, in this section, we propose an efficient algortihm to learn the parameters. Though we presented batch objective function in Equation 6, we seek to use stochastic methods to learn the embedding parameters {Vi}4i=1 and {Wi} 4 i=1. The Adam Optimizer [31] is used in our experiment, since it has shown good performance in training RNNs., and use gradient clip to avoid gradient explosion.\nThe Back Propagation Through Time (BPTT) is the standard way to train a RNN. To make the back propagation tractable, one typically needs to do truncation during training. Different from traditional sequential data where one can easily break the sequences into multiple segments to make the BPTT trackable, here all the events are related to each other by the user-item bipartite graph, which makes it hard to decompose.\nTo do this, we first order all the events globally and then do mini-batch training in a sliding window fashion. Each time when conducting feed forward and back propagation, we take the consecutive events within current sliding window to build the computational graph. In our case the truncation is on the global timeline, instead over individual independent sequencs. Another benefit of ordering events globally is that it allows us to keep the user and item latent features that could be used for the future mini-batch training. Figure 2 illustrates our training method.\nSince the user-item interactions vary a lot across minibatches, the corresponding computational graph also changes greatly. To make the learning efficient, we use the graph embedding framework [32] which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters.\nNext, we discuss in details on gradient computation. First, note that the intensity function \u03bbu,i(t) is piecewise linear, hence the integration in (6) can also be computed in a piecewise fashion with closed form, where the number of pieces equals to the total number of events happened to user u and item i separately.\nComputing gradient For illustration purpose, we here use Sigmoid as the nonlinear activation function \u03c3. In order to get gradient with respect to parameter W s, we first compute gradients with respect to each varying points of embeddings. For user u\u2019s embedding after his k-th event, the corresponding partial derivatives are computed by:\n\u2202`\n\u2202Uu(tuk) = \u2212Iiu k\ufe38 \ufe37\ufe37 \ufe38 from intensity + n\u2211 i=1\n\u2202 \u222b tuk+1 tu k \u03bbu,i(\u03c4)d\u03c4\n\u2202Uu(tuk)\ufe38 \ufe37\ufe37 \ufe38 from survival\n+ (7)\n\u2202`\n\u2202Uu(tuk+1) (1\u2212 Uu(tuk+1)) Uu(tuk+1)W2\ufe38 \ufe37\ufe37 \ufe38\nfrom user u\u2019s next embedding\n+ \u2202`\n\u2202Iiu k+1 (tuk+1) (1\u2212 Iiu k+1 (tuk+1)) Iiuk+1(t u k+1)\ufe38 \ufe37\ufe37 \ufe38\nfrom user u\u2019s next item embedding\nwhere denotes element-wise multiplication. The gradient coming from the second term (i.e., the survival term) is also easy to compute, since the Rayleigh distribution has closed form of survival function. For a certain item i, if its feature doesn\u2019t changed between time interval [tuk , t u k+1], then we have\n\u2202 \u222b tuk+1 tu k \u03bbu,i(\u03c4)d\u03c4\n\u2202Uu(tuk) =\n(tuk+1 \u2212 tuk)2\n2 exp\n( Uu(t u k) >Ii(t u k)Ii(t u k) )\n(8) On the other hand, if the embedding of item i changes during this time interval, then we should break this interval into segments and compute the summation of gradients in each segment in a way similar to (8). Thus, we are able to compute the gradients with respect to Wi, i \u2208 {1, 2, 3, 4} as follows.\n\u2202`\n\u2202W1 = m\u2211 u=1 \u2211 k \u2202` \u2202Uu(tuk) (1\u2212 Uu(tuk)) Uu(tuk)(tuk \u2212 tuk\u22121)\n\u2202`\n\u2202W2 = m\u2211 u=1 \u2211 k ( \u2202` \u2202Uu(tuk) (1\u2212 Uu(tuk)) Uu(tuk) ) Uu(t u k\u22121) >\n\u2202`\n\u2202W3 = m\u2211 u=1 \u2211 k ( \u2202` \u2202Uu(tuk) (1\u2212 Uu(tuk)) Uu(tuk) ) Iik (t u k\u2212)>\n\u2202`\n\u2202W4 = m\u2211 u=1 \u2211 k ( \u2202` \u2202Uu(tuk) (1\u2212 Uu(tuk)) Uu(tuk) ) q u,ik k\nSince the items are treated symmetrically as users, the corresponding derivatives can be obtained in a similar way."}, {"heading": "6. EXPERIMENTS", "text": "We evaluate our model on real-world datasets. For each sequence of user activities, we use all the events up to time T \u00b7 p as the training data, and the rest events as the testing data, where T is the observation window. We report the results on two tasks: \u2022 Item prediction. At each test time, we predict the\nitem that the user will interact with. We rank all the\nJacob\n1:45pm\nSophie\nJacob\nSophie\n3:45pm 5:00pm 9:00pm 10:30pm\n3:30pm\n3:15pm\n2:30pm 4:25pm\n9:25pm\n9:45pm\n10:00pm8:15pm\nMini-batch 1 Mini-batch 2\n(user, forum)"}, {"heading": "6.1 Competitors", "text": ""}, {"heading": "6.2 Datasets", "text": "We use three real world datasets. IPTV. It contains 7,100 users\u2019 watching history of 385 TV programs in 11 months (Jan 1 - Nov 30 2012), with around 2M events, and 1,420 movie features (including 1,073 actors, 312 directors, 22 278 genres, 8 countries and 5 years).\nYelp. This data was available in Yelp Dataset challenge Round 7. It contains reviews for various businesses from October, 2004 to December, 2015. Out of available 552K users, we used users with more than 100 posts for our experiments. We cleaned the review text by removing stop words\nand punctuation marks and only included words of length > 3 and frequency > 10. After this pre-processing, the dataset comprised of 1,503 users, 47,924 groups (businesses) and 34,508 text features with a total of 2,92,000 reviews. To be able to compare with baselines, we further decreased the size of this processed dataset and used a total of 95,000 reviews between randomly selected 95 users and 17,205 businesses.\nReddit. We collected discussion related data on different subreddits (groups) for the month of January 2014. We filtered all bot users\u2019 and their posts from this dataset. Similar to Yelp dataset, we cleaned the text of posts to remove stop words and punctuation marks and only include words of length > 3 and frequency > 10. Furthermore, we only considered top 10,000 users sorted according to the frequency of posts and randomly selected 1,000 users out of it to create smaller dataset. After all pre-processing, the dataset consists of 1,000 users, 1,403 groups and 82,389 text features. This dataset contains a total of 10,000 discussion events."}, {"heading": "6.3 Results", "text": "Item Recommendation From Figure 3 we can see, our method significantly outperforms epoch-based baselines in terms of item prediction on all the datasets. While the best possible MAR one can achieve is 1, both our method and LowRankHawkes got quite accurate results. Regarding the MAR metric, the performance is also slightly better compared with LowRankHawkes. Since one only need the rank of conditional density f to conduct item prediction, LowRankHawkes may still be good at differentiating f , but could not learn the actual value of f accurately, as shown in the time prediction task where the value of f is needed for precise prediction.\nTime Prediction On time prediction, R-coevolve significantly outperforms other methods. For example, compared with LowRankHawkes, it has 2\u00d7 time improvement on Yelp, 6\u00d7 improvement on Reddit, and 30\u00d7 improvement on IPTV. The time unit is hour. Hence it has 2 weeks accuracy improvement on IPTV and 2 days on Reddit. This is important for online merchants to make time sensitive recommendations. An intuitive explanation is that our method\naccurately captures the nonlinear pattern between user and item interactions. The competitor LowRankHawkes assumes specific parametric forms of the user-item interaction process, hence may not be accurate or expressive enough to capture real world temporal patterns. Furthermore, the LowRankHawkes modeled each user-item interaction dimension independently, which may lose the important affection from user\u2019s interaction with other items while predicting the current item\u2019s reoccurance time."}, {"heading": "7. CONCLUSION", "text": "We have proposed an efficient framework for modeling the co-evolution nature of users\u2019 and items\u2019 latent features. It is a generative model designed for modeling and understanding user\u2019s online behaviors, which is different from prior work that only focuses on the prediction task in the recommender system. Moreover, the user and item\u2019s evolving and co-evolving processes are captured by the RNN. We demonstrate the superior performance of our method on the time prediction task, which is not possible by most prior work. Future work includes extending to other applications such as modeling dynamics of social message groups, and understanding peoples\u2019 behaviors on Q&A sites.\nAcknowledge This project was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1- 2340, NSF IIS-1639792, NSF IIS-1218749, NSF CAREER IIS-1350983, Intel and NVIDIA."}, {"heading": "8. REFERENCES", "text": "[1] Nan Du, Yichen Wang, Niao He, and Le Song. Time\nsensitive recommendation from recurrent user activities. In NIPS, 2015.\n[2] Thomas Josef Liniger. Multivariate Hawkes Processes. PhD thesis, Swiss Federal Institute of Technology Zurich, 2009.\n[3] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo.\nIn W.W. Cohen, A. McCallum, and S.T. Roweis, editors, ICML, volume 307, pages 880\u2013887. ACM, 2008.\n[4] Y. Chen, D. Pavlov, and J.F. Canny. Large-scale behavioral targeting. In J.F. Elder, F. Fogelman-Soulie\u0301, P.A. Flach, and M. J. Zaki, editors, KDD, pages 209\u2013218. ACM, 2009.\n[5] D. Agarwal and B.-C. Chen. Regression-based latent factor models. In J.F. Elder, F. Fogelman-Soulie\u0301, P.A. Flach, and M.J. Zaki, editors, KDD, pages 19\u201328. ACM, 2009.\n[6] Michael D Ekstrand, John T Riedl, and Joseph A Konstan. Collaborative filtering recommender systems. Foundations and Trends in Human-Computer Interaction, 4(2):81\u2013173, 2011.\n[7] Yehuda Koren and Joe Sill. Ordrec: an ordinal model for predicting personalized item rating distributions. In RecSys, 2011.\n[8] Shuang-Hong Yang, Bo Long, Alex Smola, Narayanan Sadagopan, Zhaohui Zheng, and Hongyuan Zha. Like like alike: joint friendship and interest propagation in social networks. In WWW, 2011.\n[9] Xing Yi, Liangjie Hong, Erheng Zhong, Nanthan Nan Liu, and Suju Rajan. Beyond clicks: Dwell time for personalization. In RecSys, 2014.\n[10] Yichen Wang and Aditya Pal. Detecting emotions in social media: A constrained optimization approach. In IJCAI, 2015.\n[11] Y. Koren. Collaborative filtering with temporal dynamics. In KDD, 2009.\n[12] Alexandros Karatzoglou, Xavier Amatriain, Linas Baltrunas, and Nuria Oliver. Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering. In Recsys, pages 79\u201386. ACM, 2010.\n[13] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G. Schneider, and Jaime G. Carbonell. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, pages 211\u2013222. SIAM, 2010.\n[14] Eric C Chi and Tamara G Kolda. On tensors, sparsity, and nonnegative factorizations. SIAM Journal on Matrix Analysis and Applications, 33(4):1272\u20131299, 2012.\n[15] San Gultekin and John Paisley. A collaborative kalman filter for time-evolving dyadic processes. In ICDM, pages 140\u2013149, 2014.\n[16] Laurent Charlin, Rajesh Ranganath, James McInerney, and David M Blei. Dynamic poisson factorization. In RecSys, 2015.\n[17] Jiayu Zhou Juhan Lee Preeti Bhargava, Thomas Phan. Who, what, when, and where: Multi-dimensional collaborative recommendations using tensor factorization on sparse user-generated data. In WWW, 2015.\n[18] Prem Gopalan, Jake M Hofman, and David M Blei. Scalable recommendation with hierarchical poisson factorization. UAI, 2015.\n[19] Bala\u0301zs Hidasi and Domonkos Tikk. General factorization framework for context-aware recommendations. Data Mining and Knowledge Discovery, pages 1\u201330, 2015.\n[20] Xin Wang, Roger Donaldson, Christopher Nell, Peter\nGorniak, Martin Ester, and Jiajun Bu. Recommending groups to users using user-group engagement and time-dependent matrix factorization. In AAAI, 2016.\n[21] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Collaborative deep learning for recommender systems. In KDD. ACM, 2015.\n[22] Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.\n[23] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song. Recurrent marked temporal point processes: Embedding event history to vector. In KDD. ACM, 2016.\n[24] D.R. Cox and V. Isham. Point processes, volume 12. Chapman & Hall/CRC, 1980.\n[25] D.R. Cox and P.A.W. Lewis. Multivariate point processes. Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations, Statistical Methods and Applications, 1:159, 2006.\n[26] Odd Aalen, Ornulf Borgan, and Hakon Gjessing. Survival and event history analysis: a process point of view. Springer, 2008.\n[27] Alan G Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):83\u201390, 1971.\n[28] Yichen Wang, Bo Xie, Nan Du, and Le Song. Isotonic hawkes processes. In ICML, 2016.\n[29] Manuel Gomez-Rodriguez, David Balduzzi, and Bernhard Scho\u0308lkopf. Uncovering the temporal dynamics of diffusion networks. In Proceedings of the International Conference on Machine Learning, 2011.\n[30] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes: volume II: general theory and structure, volume 2. Springer, 2007.\n[31] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[32] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In ICML, 2016.\n[33] Yichen Wang, Robert Chen, Joydeep Ghosh, Joshua C Denny, Abel Kho, You Chen, Bradley A Malin, and Jimeng Sun. Rubik: Knowledge guided tensor factorization and completion for health data analytics. In KDD, 2015.\n[34] Komal Kapoor, Karthik Subbian, Jaideep Srivastava, and Paul Schrater. Just in time recommendations: Modeling the dynamics of boredom in activity streams. In WSDM, 2015."}], "references": [{"title": "Time sensitive recommendation from recurrent user activities", "author": ["Nan Du", "Yichen Wang", "Niao He", "Le Song"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multivariate Hawkes Processes", "author": ["Thomas Josef Liniger"], "venue": "PhD thesis, Swiss Federal Institute of Technology Zurich,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": " In W.W. Cohen, A. McCallum, and S.T. Roweis, editors, ICML, volume 307, pages 880\u2013887. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale behavioral targeting", "author": ["Y. Chen", "D. Pavlov", "J.F. Canny"], "venue": "J.F. Elder, F. Fogelman-Souli\u00e9, P.A. Flach, and M. J. Zaki, editors, KDD, pages 209\u2013218. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Regression-based latent factor models", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "J.F. Elder, F. Fogelman-Souli\u00e9, P.A. Flach, and M.J. Zaki, editors, KDD, pages 19\u201328. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative filtering recommender systems", "author": ["Michael D Ekstrand", "John T Riedl", "Joseph A Konstan"], "venue": "Foundations and Trends in Human-Computer Interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Ordrec: an ordinal model for predicting personalized item rating distributions", "author": ["Yehuda Koren", "Joe Sill"], "venue": "In RecSys,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Like like alike: joint friendship and interest propagation in social networks", "author": ["Shuang-Hong Yang", "Bo Long", "Alex Smola", "Narayanan Sadagopan", "Zhaohui Zheng", "Hongyuan Zha"], "venue": "In WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Beyond clicks: Dwell time for personalization", "author": ["Xing Yi", "Liangjie Hong", "Erheng Zhong", "Nanthan Nan Liu", "Suju Rajan"], "venue": "In RecSys,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Detecting emotions in social media: A constrained optimization approach", "author": ["Yichen Wang", "Aditya Pal"], "venue": "In IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Y. Koren"], "venue": "KDD", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["Alexandros Karatzoglou", "Xavier Amatriain", "Linas Baltrunas", "Nuria Oliver"], "venue": "In Recsys,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["Liang Xiong", "Xi Chen", "Tzu-Kuo Huang", "Jeff G. Schneider", "Jaime G. Carbonell"], "venue": "In SDM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "On tensors, sparsity, and nonnegative factorizations", "author": ["Eric C Chi", "Tamara G Kolda"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A collaborative kalman filter for time-evolving dyadic processes", "author": ["San Gultekin", "John Paisley"], "venue": "In ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Dynamic poisson factorization", "author": ["Laurent Charlin", "Rajesh Ranganath", "James McInerney", "David M Blei"], "venue": "In RecSys,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Who, what, when, and where: Multi-dimensional collaborative recommendations using tensor factorization on sparse user-generated data", "author": ["Thomas Phan"], "venue": "In WWW,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scalable recommendation with hierarchical poisson factorization", "author": ["Prem Gopalan", "Jake M Hofman", "David M Blei"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "General factorization framework for context-aware recommendations", "author": ["Bal\u00e1zs Hidasi", "Domonkos Tikk"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Recommending groups to users using user-group engagement and time-dependent matrix factorization", "author": ["Xin Wang", "Roger Donaldson", "Christopher Nell", "Peter  Gorniak", "Martin Ester", "Jiajun Bu"], "venue": "In AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Collaborative deep learning for recommender systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "In KDD. ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["Balazs Hidasi", "Alexandros Karatzoglou", "Linas Baltrunas", "Domonkos Tikk"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Recurrent marked temporal point processes: Embedding event history to vector", "author": ["Nan Du", "Hanjun Dai", "Rakshit Trivedi", "Utkarsh Upadhyay", "Manuel Gomez-Rodriguez", "Le Song"], "venue": "In KDD", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Point processes", "author": ["D.R. Cox", "V. Isham"], "venue": "volume 12. Chapman & Hall/CRC", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1980}, {"title": "Multivariate point processes", "author": ["D.R. Cox", "P.A.W. Lewis"], "venue": "Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations, Statistical Methods and Applications, 1:159", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Survival and event history analysis: a process point of view", "author": ["Odd Aalen", "Ornulf Borgan", "Hakon Gjessing"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Spectra of some self-exciting and mutually exciting point processes", "author": ["Alan G Hawkes"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1971}, {"title": "Isotonic hawkes processes", "author": ["Yichen Wang", "Bo Xie", "Nan Du", "Le Song"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Uncovering the temporal dynamics of diffusion networks", "author": ["Manuel Gomez-Rodriguez", "David Balduzzi", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "An introduction to the theory of point processes: volume II: general theory and structure", "author": ["D.J. Daley", "D. Vere-Jones"], "venue": "volume 2. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Discriminative embeddings of latent variable models for structured data", "author": ["Hanjun Dai", "Bo Dai", "Le Song"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun"], "venue": "In KDD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Just in time recommendations: Modeling the dynamics of boredom in activity streams", "author": ["Komal Kapoor", "Karthik Subbian", "Jaideep Srivastava", "Paul Schrater"], "venue": "In WSDM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recent point process based models treat time as a random variable and improves over the traditional methods significantly [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "How can we obtain a more expressive model to capture the co-evolution features of user-item interactions, and learn such a model from large volume of data? To tackle this challenge, in this paper, we combine recurrent neural network (RNN) with multivariate point process models [2], and propose a recurrent coevolutionary feature embedding process framework.", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 3, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 4, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 5, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 6, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 7, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 8, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 9, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 10, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 11, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 12, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 11, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 12, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 13, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 14, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 15, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 16, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 17, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 18, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 19, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 0, "context": "Recently, [1] proposed a low-rank point process based model for time- sensitive recommendations from recurrent user activities.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "In the deep learning community, [21] proposed collaborative deep learning, a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "[22] applied recurrent neural network based approach to recommender systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Finally, our work is inspired from newly proposed recurrent marked temporal point process framework [23] that builds a connection between RNN and Point Processes.", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "However, [23] focuses on the task of next event prediction given a sequence of past events for an entity and is only designed for one-dimension point process.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti \u2208 R and i \u2208 Z.", "startOffset": 25, "endOffset": 33}, {"referenceID": 24, "context": "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti \u2208 R and i \u2208 Z.", "startOffset": 25, "endOffset": 33}, {"referenceID": 25, "context": "Then, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as [26]:", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "\u2022 Hawkes processes [27, 28], whose intensity models the excitation between events, i.", "startOffset": 19, "endOffset": 27}, {"referenceID": 27, "context": "\u2022 Hawkes processes [27, 28], whose intensity models the excitation between events, i.", "startOffset": 19, "endOffset": 27}, {"referenceID": 15, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 16, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 17, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 18, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 19, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 28, "context": "This formulation assumes a Rayleigh distribution for the time intervals between consecutive events in each dimension [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 29, "context": "The joint negative log-likelihood is [30]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "The Adam Optimizer [31] is used in our experiment, since it has shown good performance in training RNNs.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "To make the learning efficient, we use the graph embedding framework [32] which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 185, "endOffset": 197}, {"referenceID": 12, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 185, "endOffset": 197}, {"referenceID": 32, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 185, "endOffset": 197}, {"referenceID": 0, "context": "\u2022 LowRankHawkes [1]: This is a low rank point process based model which assumes user-item interactions to be independent of each other and does not capture the co-evolution of user and item features.", "startOffset": 16, "endOffset": 19}, {"referenceID": 33, "context": "\u2022 STIC [34]: it fits a semi-hidden markov model to each observed user-item pair and is only designed for time prediction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "\u2022 TimeSVD++ [11] and FIP [8]: These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "\u2022 TimeSVD++ [11] and FIP [8]: These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users.", "startOffset": 25, "endOffset": 28}], "year": 2016, "abstractText": "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.", "creator": "LaTeX with hyperref package"}}}