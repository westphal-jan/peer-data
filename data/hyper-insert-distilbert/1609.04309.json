{"id": "1609.04309", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Efficient softmax approximation for GPUs", "abstract": "we propose an approximate strategy to efficiently train diverse neural network based language models systematically over very locally large vocabularies. our approach, since called adaptive softmax, naturally circumvents decreasing the linear dependency on the implicit vocabulary memory size by exploiting the unbalanced symbolic word rate distribution to rapidly form clusters that comparatively explicitly severely minimize the expectation of computational complexity. our approach further economically reduces significantly the computational input cost allocated by exploiting largely the specificities of standard modern matrix architectures and exploits matrix - aligned matrix vector operations, allegedly making it particularly suited theoretically for graphical processing units. our additional experiments carried put out work on standard learning benchmarks, such as europarl and median one billion short word, significantly show that our approach eventually brings a large greater gain in network efficiency over relatively standard constraint approximations while achieving dramatically an estimate accuracy substantially close below to that of employing the weighted full softmax.", "histories": [["v1", "Wed, 14 Sep 2016 15:15:08 GMT  (116kb,D)", "http://arxiv.org/abs/1609.04309v1", null], ["v2", "Tue, 13 Dec 2016 21:42:39 GMT  (121kb,D)", "http://arxiv.org/abs/1609.04309v2", null], ["v3", "Mon, 19 Jun 2017 16:33:04 GMT  (1887kb,D)", "http://arxiv.org/abs/1609.04309v3", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["edouard grave", "armand joulin", "moustapha ciss\u00e9", "david grangier", "herv\u00e9 j\u00e9gou"], "accepted": true, "id": "1609.04309"}, "pdf": {"name": "1609.04309.pdf", "metadata": {"source": "CRF", "title": "Efficient softmax approximation for GPUs", "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "emails": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"], "sections": [{"heading": null, "text": "models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax."}, {"heading": "1 Introduction", "text": "This paper considers strategies to learn parametric models for language modeling with very large vocabularies. This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17]. In particular, Neural Network Language Models (NNLMs) have received a renewed interest in recent years, by achieving state of the art performance on standard benchmarks [22, 33]. These approaches are more costly but generalize better than traditional non-parametric models [2, 24].\nStatistical language models assign a probability to words given their history [2]. They are evaluated by objective criteria such as perplexity (ppl), which directly measures the ability of the system to determine proper probabilities for all the words. This potentially makes parametric models prohibitively slow to train on corpara with very large vocabulary. For instance, the vocabulary of the One Billion Word benchmark [7] contains around 800K words. In standard NNLMs, such as feedforward networks [3] or recurrent networks [33], computing this probability over the whole vocabulary is the bottleneck. Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16]. We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].\nOur approach, called adaptive softmax, belongs to the second category. More specifically, it is inspired by the hierarchical softmax and its subsequent variants. In contrast to previous works and motivated by the trend that GPUs are comparatively more and more performant than CPUs, our design is oriented towards efficient processing on GPUs. In this context, our paper makes the following points:\n\u2022 We define a strategy to produce an approximate hierarchical model. It departs from previous ones in that it explicitly take into account the complexity of matrix-matrix multiplications on modern architectures, which is not trivially linear in the dimensions of the matrices.\n\u2022 We conduct an empirical complexity analysis of this model on recent GPUs. This leads us to define a realistic complexity model that is incorporated in the proposed optimization;\nar X\niv :1\n60 9.\n04 30\n9v 1\n[ cs\n.C L\n] 1\n4 Se\np 20\n16\n\u2022 Our approach provides a significant acceleration factor compared to the regular softmax, i.e., 2\u00d7 to 10\u00d7 speed-ups. Equivalently we improve the accuracy under computational constraints. Importantly, on the largest corpus, this higher efficiency empirically comes at no cost in accuracy for a given amount of training data, in contrast to concurrent approaches improving the efficiency.\nThis paper is organized as follows. Section 2 briefly reviews the related work and Section 3 provides some background on the language modeling task that we consider. Section 4 describes our proposal, which is subsequently evaluated in Section 5 on typical benchmarks of the language modeling literature, including Text8, Europarl and One Billion Word datasets."}, {"heading": "2 Related work", "text": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38]. We briefly describe the most popular ones below and point the reader to Chen et al. [8] for a comparative study. For the sake of completeness, we refer the reader to other strategies that can speed-up the training of language models in complementary manners [31].\nLoss function approximation. The Hierarchical Softmax (HSM) is an approximation of the softmax function introduced in Goodman [13]. This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36]. In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34]. In particular, Mikolov et al. [29] proposes an optimal hierarchy by constructing a Huffman coding based on frequency. However this coding scheme does not take into account the theoretical complexity reduction offered by matrix-matrix multiplication and distributed computation, in particular with modern GPUs.\nSimilar to our work, Zweig et al. [48] constructs their hierachy in order to explicitly reduce the computational complexity. They also solve the assignment problem with dynamic programming. However, they only consider hierachies where words are kept in the leaves of the tree, leading to a significant drop of performance (reported to be around 5\u2212 10%), forcing them to also optimize for word similarity. In our case, allowing classes to be stored in the internal node of the tree leads to almost no drop of performance. Also, they assume a linear cost for the vector-matrix operation which significantly limits the use of their approach on distributed system such as GPU.\nThe idea of keeping a short-list of the most frequent words has been explored before [27, 39]. In particular, Le et al. [27] combines a short-list with a hierachical softmax based on word representation. In contrast, the word hierarchy that we introduce in Section 4 explicitly aims at reducing the complexity.\nOur work also shares similarities with the d-softmax introduced in Chen et al. [8]. They assign capacity to words according to their frequency to speed up the training. Less frequent words have smaller classifiers than frequent ones. Unlike our method, their formulation requires accessing the whole vocabulary to evaluate the probability of a word.\nSampling based approximation. Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21]. In particular, importance sampling [4, 5] selects a subset of negative targets to approximate the softmax normalization. Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29]. While this approach often leads to significant speed-up at train time, it still requires to evaluate the full softmax at test time.\nSelf-normalized approaches. Self-normalized approaches aim at learning naturally normalized classifier, to avoid computing the softmax normalization. Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10]. Noise Contrastive Estimation [16] replaces the softmax by a binary classifier distinguishing the original distribution\nform a noisy one. While the original formulation still requires to compute the softmax normalization, Mnih and Teh [37] shows that good performance can be achieved even without it.\nFinally, Vincent et al. [44] have also proposed an efficient way to train model with high dimensional output space. Their approach is exact and leads to a promising speed-up but it cannot be directly applied to the softmax function, limiting its potential application to language modeling."}, {"heading": "3 Preliminaries on language modeling", "text": "The goal of language modeling is to learn a probability distribution over a sequence of words from a given dictionary V. The joint distribution is defined as a product of conditional distribution of tokens given their past [2]. More precisely, the probability of a sequence of T words w1, . . . , wT \u2208 VT is given as\nP (w1, . . . , wT ) = T\u220f t=1 P (wt | wt\u22121, . . . , w1). (1)\nThis problem is traditionally addressed with non-parameteric models based on counting statistics [14]. In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26]. More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33]. They are mostly either feedforward networks [3] or recurrent networks [33].\nFeedforward network. In a standard feedforward network for language modeling, we fix a window of length N and predict the next words according to the words appearing in this window. In the simplest case, this probability is represented by a 2-layer neural network acting on an input xt \u2208 VN , defined as the concatenation of the one-hot representation of the N previous words, wt\u2212N+1, . . . , wt. The state ht of the hidden layer and subsequently the vector of scores yt associated with the next token wt+1 are computed as\nht = \u03c3(APxt), (2) yt = f(Bht), (3)\nwhere \u03c3 is a non linearity, e.g., the pointwise sigmoid function \u03c3(z) = 1/(1 + exp(\u2212z)), and f is the softmax function discussed in the next section. This model is parameterized by the weight matrices P , A and B and is routinely learned with an optimization scheme such as stochastic gradient descent or Adagrad [11].\nRecurrent network. A Recurrent network [12] extends a feedforward network in that the current state of the hidden layer also depends on its previous state. The hidden state ht is updated according to the equation ht = \u03c3(Awt + Rht\u22121), where R is a weight matrix and xt is the one-hot representation of the current word wt. Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].\nSince the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32]. These models have been successfully used in the context of language modeling [22, 33, 35]. In this work, we focus on the standard word level LSTM architecture since it has obtained state of the art performance on the challenging One Billion Word Benchmark [22].\nClass-based hierarchical softmax. In neural language modeling, predicting the probability of the next word requires to compute scores for every word in the vocabulary and to normalize them\nto form a probability distribution. This is typically achieved by applying a softmax function to the unnormalized score zw associate with each word w, where the softmax function is defined as\nf(zw) = exp(zw)\u2211\nw\u2032\u2208V exp(zw\u2032) . (4)\nFor a vocabulary comprising k = |V| words, this function requires O(k) operations once the scores are computed. In the case of neural networks, the overall complexity is O(dk), where d is the size of the last hidden layer. When the vocabulary is large, this step is computationally expensive and often dominates the computation of the whole model [22, 32], as discussed in introduction and related work. A simple approach [13] to reduce this computational cost is to assign each word w of the vocabulary to a unique class C(w) and to factorize the probability distribution over words as\np(wt | ht) = p1(C(wt) | ht)\u00d7 p2(wt | C(wt), ht),\nwhere p1 and p2 are obtained using the softmax function (Eq. 4). If each class contains \u221a k words,\nthe computational cost is reduced from O(dk) to O(d \u221a k)."}, {"heading": "4 Our approach: the adaptive softmax", "text": "In this section, we propose the adaptive softmax, a simple speedup technique for the computation of probability distributions over words. The adaptive softmax is inspired by the class-based hierarchical softmax, where the word classes are built to minimize the computational complexity. Our method is designed to be efficient for GPUs, which are commonly used to train neural networks. For the sake of clarity, we first present the intuition behind our method in the simple case where we simply split our dictionary in two distinct clusters, before analyzing a more general case."}, {"heading": "4.1 GPU computational model", "text": "The bottleneck of the model described in the previous section is the matrix multiplication between the matrix representing the hidden states (of size B \u00d7 d, where B denotes the batch size), and the matrix of word representations, of size d\u00d7 k. For a fixed size d of the hidden layer, we denote by g(k,B, d) the complexity of this multiplication, and simplify the notation wherever some parameters are fixed. Figure 1 reports empirical timings as a function of k for typical parameters of B and d for two GPU models, namely K40 and Maxwell. We observe that the complexity g(k) is constant for low values of k, until a certain inflection point k0 \u2248 50, and then becomes affine for values k > k0. This suggests a cost function of the form\ng(k) = max(c+ \u03bbk0, c+ \u03bbk) = cm +max [ 0, \u03bb(k \u2212 k0) ] . (5)\nEmpirically, cm = 0.40ms on a K40 and 0.22ms on a Maxwell. We observe the same behavior when measuring the timings as a function of the other parameters, i.e., it is inefficient to matrix-multiply when one of the dimensions is small. This observation suggests that hierarchical organizations of words with a low number of children per node, such as binary Huffman codes, are highly suboptimal."}, {"heading": "4.2 Intuition: the two-clusters case", "text": "In natural languages, the distribution of the words notoriously follows a Zipf law [47]. Most of the probability mass is covered by a small fraction of the dictionary, e.g., 87% of the document is covered by only 20% of the vocabulary in the Penn TreeBank. Similar to the frequency binning hierarchical softmax [34], this information can be exploited to reduce the computation cost.\nA simple strategy to reduce the overall complexity is to partition the dictionary V into two clusters as Vh and Vt, where Vh denotes the head of the distribution consisting of the most frequent words, and where Vt is the tail associated with a large number of rare words. The classifier frequently accesses the head, which motivates the fact that it should be computed efficiently. In contrast,\nthe tail occurs less frequently and the corresponding computation can be slower. This suggests to define clusters with unbalanced cardinalities |Vh| |Vt| and probabilities P (Vh) P (Vt), where P (A) = \u2211 w\u2208A pi is the probability of a word to occur in the set Vi. For instance, one may define the head would only contain 20% of the vocabulary (covering for 87% on PennTree Bank). These two clusters can be organized in two different ways: either they are both leaves of a 2-level tree [34], or the head cluster is kept as a short-list in the root node [27]. We now analyze what is the best structure and how to split the vocabulary by determining the corresponding complexities, assuming that the head consists of the most frequent words. The next subsection shows the optimality of this choice.\nGiven a vocabulary of k words, we are looking for the number kh = |Vh| of words from the head of the distribution to be assigned to the first cluster. These words will cover for ph of the distribution. The tail cluster will then contained the rest of the vocabulary, made of kt = k \u2212 kh words and covering for pt = 1 \u2212 ph of the overall distribution. We denote by g(k, d) the computational complexity of computing the softmax function over k words with d dimensional input features. Figure 1 shows an example of this function for a fixed d. The complexity of putting the head of the distribution in the root of the tree is g(kh + 1, d) + ptg(kt, d), while the complexity associated with putting both cluster in leaves is g(2, d) + phg(kh, d) + ptg(kt, d). Depending on the distribution of a corpus, it is then simple to choose the best assignment of words into the two clusters. For example, on PennTree Bank, with a hidden layer of size d = 128, the optimal configuration is to keep a short-list of 1400 classes in the root node, leading to an average cost of 0.33ms per batch of size 512, while it takes 0.36ms when both clusters are in the leaves. In comparison, the full softmax takes 0.80ms for the same configuration, leading to a \u00d72.4 speed-up.\nAdapting the classifier capacity for each cluster. Each cluster is accessed independently of each other, they thus do not need to have the same capacity. Frequent words need high capacity to be predicted correctly. In contrast, rare words cannot be learned very well, since we only see them a few times. It would then be wasteful to associated them with high capacity. Like in Chen et al. [8], we exploit this observation to further reduce the computational cost of our classifier. Unlike [8], we share the state of hidden layer across clusters and simply reduce the input size of the classifiers by applying a projection matrix. Typically, the projection matrix for the tail cluster reduces the size from d to dt = d/4, reducing the complexity from g(kt, d) to g(dt, d) + g(kt, dt).\nCompromising between efficiency and accuracy. We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance (around 5\u2212 10% performance drop) [34, 48]. The reason is that the probability of every word w belonging to a cluster c is multiplied by the probability of its class, i.e., it is equal to P (c | h)P (w | c, h), while attaching a frequent word directly to the root associates it directly to the probability P (w | h) making its inference sharper. For this reason, unless there is a significant difference in computational\ncomplexity, we favor using a short-list, over the standard 2-level hierarchical softmax."}, {"heading": "4.3 General case", "text": "Let us now consider the more general case where the dictionary is partitioned as V = Vh \u222aV1 . . .VJ , Vi \u2229 Vj = \u2205 if i 6= j. We consider the hierarchical model depicted in Figure 2, where the subdictionary Vh is accessed at the first level, and the others in the second level. We now investigate the computational cost C of the forward (equivalently, backward) pass of this approximate softmax layer. At the time being, we fix the batch size B and the dimensionality d of the hidden layer, in order to analyze the complexity as a function of the sub-dictionary sizes and probabilities. We denote by pi = \u2211 w\u2208Vi p(w) the probability P (w \u2208 Vi) and ki = |Vi| the cardinality of each cluster.\nThe expected cost C is decomposed as C = Ch + \u2211 i Ci, where\nCh = ph g(J + kh) and \u2200i, Ci = pi [ g(J + kh) + g(ki) ] , (6)\nleading to C = g(J + kh) + \u2211 i pi g(ki). (7)\nWe add the constraint k \u2265 k0 to ensure that there is no penalty induced by the constant part of the cost model of Equation 5, the previous equation simplifies as\nC = c+ \u03bb(J + kh) + \u2211 i pi(c+ \u03bbki) (8)\n= c(2\u2212 ph) + \u03bb [ J + kh + \u2211 i pi ki ] . (9)\nLet us discuss this equation, by first considering that the cardinalities of the sub-vocabularies are fixed. The most-right term is the only one that depends on the word probabilities. For two distinct clusters Vi and Vj , we can re-write pjkj as (pi+j \u2212 pi)kj , where pi+j = pi + pj , so that\npiki + pjkj = pi(ki \u2212 kj) + pi+jkj . (10)\nWithout loss of generality, we assume that ki > kj . The quantities pi+j , ki and kj being fixed, the second term of this equation is constant, and the best strategy is trivially to minimize the probability of the largest cluster Vi. In other terms, an optimal solution for Equation 9 requires that the most frequent words are assigned to the smallest cluster. This remark is true for any tuple (i, j), and we easily see that this point also holds for the head cluster. As a consequence, for a fixed number of clusters of given sizes, the best strategy is to assign the words by decreasing probabilities to clusters of increasing size. Note, this analysis remains valid as long as the g is monotonically increasing in k.\nDetermining ki with J fixed: dynamic programming. We now assume that the number of clusters is fixed. Following our analysis above, the optimization solely depends on the cardinalities ki for all clusters, which perfectly determines how to split the list of words ordered by frequency. We solve this problem by dynamic programming.\nFinding the number of clusters. The only remaining free variable in our optimization is J , since the other parameters are then determined by the aforementioned optimizations. For this step, the cost of Equation 9 over-estimates the number of clusters because we have neglected the effect of the non-linearity of the batch size: in the second layer, the batches are typically smaller that the inflection point k0. In practice, we optimize over small values of J = 1, 2, 3, 4 and empirically determine the best compromise speed/perplexity on training data. Note, having a lower number of clusters with numerous frequent words on the first level has another flavor: we empirically observe that it offers a better perplexity than word hierarchy with a large number of clusters. It is comparable to that of the exact softmax for large corpora, as shown later by our experiments."}, {"heading": "5 Experiments", "text": "This section provides a set of experiments aiming at analyzing the trade-off between actual complexity and effectiveness of several strategies, in particular the approach presented in the previous section. First we describe our evaluation protocol, then we evaluate some of the properties of our model and finally we compare it on standard benchmark against standard baselines.\nDatasets. We evaluate our method on standard datasets, and use the perplexity (ppl) as an evaluation metric, as the function of the training time or of the number of training data (epochs). The datasets have varying vocabulary sizes, in different languages, which allows us to better understand the strengths and weaknesses of the different approaches.\n\u2022 Text81 is a standard compression dataset containing a pre-processed version of the first 100 millions characters from Wikipedia in English. It has been recently used for language modeling [32] and has a vocabulary of 44k words.\n\u2022 Europarl2 is a machine translation corpus, containing 20 languages [25]. For most languages, there are 10M\u201360M tokens and the vocabulary is in between 44k and 250k words.\n\u2022 One Billion Word 3 is a massive corpus introduced by Chelba et al. [7]. It contains 0.8B tokens and a vocabulary comprising almost 800k words.\nImplementation details. We use an LSTM with one layer in all our experiments. On Text8 and Europarl, the models have d = 512 hidden units and are regularized with weight decay (\u03bb = 10\u22126). On the One Billion Word benchmark, we use d = 2048 hidden units and no regularization. The dimension of the input word embeddings is set to 256, so that large models fit in GPU memory. For the backpropagation through time, we unroll the models for 20 steps. We use Adagrad [11], with a step size of 0.1 and 5 epochs, and we clip the norm of the gradients to 1. The batch size B is set to 128, except on the Finnish portion of Europarl where B=64 due to memory constraints. All the experiments were run on the same GPU with the Maxwell architecture.\n1http://mattmahoney.net/dc/textdata 2http://www.statmt.org/europarl/ 3https://code.google.com/archive/p/1-billion-word-language-modeling-benchmark/\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd \ufffd\ufffd\ufffd\n\ufffd\ufffd \ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\nBaselines. Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8]. For HSM, we tried different strategies for the binning. We observe that using the square root function on the count before computing the word bins is the most efficient. For the negative sampling method, we used a number of samples equal to 20% of the size of the vocabulary [8]. For the differentiated softmax (D-softmax), we used the same partitions for the vocabulary as for our approach. We tried two version of the differentiated softmax. The first is the one described by et al. [8], where each word cluster uses a disjoint subset of the hidden representation. We also present an improved version, referred to as D-softmax [*], which uses our choice to have the whole hidden representation mapped to the different word clusters using projection matrices of different sizes.\nComparison with the state of the art. Table 1 reports the results that we achieve on Text8. On this small vocabulary, approximate methods are comparatively less interesting. Our approach is the only one to approach the result of the full soft-max (below by 3 points of perplexity), while being the fastest. Our improved variant D-softmax [*] of the work by Chen et al. [8] obtains similar results but is slower by a factor \u00d71.8.\nOn Europarl, we first present the convergence properties of our approach compared to other approximate strategies in Figure 3 show the perplexity (ppl) as a function of training time. Our approach significantly outperforms all competitors by a large margin. For reference, we also show the performance (D-softmax [*]) obtained by improving the D-softmax, to make it more comparable to our method. Our method is 2\u00d7 to 3\u00d7 faster than this improved competitor, which demonstrates how critical is our optimization strategy. Similar conclusions are drawn from Table 3 for other languages from the Europal corpus.\nTable 2 gives the test perplexity on One Billion Word benchmark: Our method achieves a perplexity of 48 after five epochs, summing up to 5 days of training on a single GPU. In comparison,\nonly Jozefowicz et al. [22] achieves a lower perplexity, but with a model 8\u00d7 bigger than ours and trained over 32 GPUs during 3 weeks. As far as we know, we are the first method to achieve a perplexity lower than 50 on a single GPU."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a simple yet efficient approximation of the softmax classifier. To our knowledge, it is the first speed optimizing approximation that obtains performance on par with the exact model. This is achieved by explicitly taking into account the computational complexity of parallel systems and combining it with a few important observations, namely keeping a short-list of frequent words in the root node [39] and reducing the capacity of rare words [8]. In all our experiments on GPU, our method consistently maintains a low perplexity while enjoying a speed-up going from 2\u00d7 to 10\u00d7 compared to the exact model. This type of speed-up allows to deal with extremely large corpora in reasonable time and without the need of a large number of GPUs. We believe our approach to be general enough to be applied to other parallel computing architectures and other losses, as well as to other domains where the distributions of the class are unbalanced."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Jeff Johnson for his help with GPU benchmarking and Tomas Mikolov for insightful discussions."}], "references": [{"title": "When and why are log-linear models self-normalizing", "author": ["J. Andreas", "D. Klein"], "venue": "ACL,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A maximum likelihood approach to continuous speech recognition", "author": ["L.R. Bahl", "F. Jelinek", "R.L. Mercer"], "venue": "PAMI,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Y. Bengio", "J.-S. Sen\u00e9cal"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J.-S. Sen\u00e9cal"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Strategies for training large vocabulary neural language models", "author": ["W. Chen", "D. Grangier", "M. Auli"], "venue": "arXiv preprint arXiv:1512.04906,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R.M. Schwartz", "J. Makhoul"], "venue": "ACL,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Classes for fast maximum entropy training", "author": ["J. Goodman"], "venue": "ICASSP,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A bit of progress in language modeling", "author": ["J.T. Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In ICASSP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["S. Ji", "S. Vishwanathan", "N. Satish", "M.J. Anderson", "P. Dubey"], "venue": "arXiv preprint arXiv:1511.06909,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning visual features from large weakly supervised data", "author": ["A. Joulin", "L. van der Maaten", "A. Jabri", "N. Vasilache"], "venue": "arXiv preprint arXiv:1511.02251,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S.M. Katz"], "venue": "ICASSP,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1987}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "ICASSP,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "MT summit,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "A cache-based natural language model for speech recognition", "author": ["R. Kuhn", "R. De Mori"], "venue": "PAMI,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "Structured output layer neural network language model", "author": ["H.-S. Le", "I. Oparin", "A. Allauzen", "J.-L. Gauvain", "F. Yvon"], "venue": "ICASSP,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u1ef3"], "venue": "INTERSPEECH,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "ASRU,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J.H. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "ICASSP,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Aistats,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Continuous space language models", "author": ["H. Schwenk"], "venue": "Computer Speech & Language, pages 492\u2013518,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Large, pruned or continuous space language models on a gpu for statistical machine translation", "author": ["H. Schwenk", "A. Rousseau", "M. Attik"], "venue": "NAACL-HLT Workshop,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse non-negative matrix language modeling for skip-grams", "author": ["N. Shazeer", "J. Pelemans", "C. Chelba"], "venue": "Proceedings of Interspeech, pages 1428\u20131432,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent", "A. de Br\u00e9bisson", "X. Bouthillier"], "venue": "In NIPS,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do", "author": ["P.J. Werbos"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["R.J. Williams", "J. Peng"], "venue": "Neural computation,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1990}, {"title": "Human behavior and the principle of least effort", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1949}, {"title": "Speed regularization and optimality in word classing", "author": ["G. Zweig", "K. Makarychev"], "venue": "ICASSP,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 39, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 93, "endOffset": 105}, {"referenceID": 41, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 93, "endOffset": 105}, {"referenceID": 42, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 93, "endOffset": 105}, {"referenceID": 14, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 138, "endOffset": 146}, {"referenceID": 16, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 138, "endOffset": 146}, {"referenceID": 21, "context": "In particular, Neural Network Language Models (NNLMs) have received a renewed interest in recent years, by achieving state of the art performance on standard benchmarks [22, 33].", "startOffset": 169, "endOffset": 177}, {"referenceID": 32, "context": "In particular, Neural Network Language Models (NNLMs) have received a renewed interest in recent years, by achieving state of the art performance on standard benchmarks [22, 33].", "startOffset": 169, "endOffset": 177}, {"referenceID": 1, "context": "These approaches are more costly but generalize better than traditional non-parametric models [2, 24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 23, "context": "These approaches are more costly but generalize better than traditional non-parametric models [2, 24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 1, "context": "Statistical language models assign a probability to words given their history [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "For instance, the vocabulary of the One Billion Word benchmark [7] contains around 800K words.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "In standard NNLMs, such as feedforward networks [3] or recurrent networks [33], computing this probability over the whole vocabulary is the bottleneck.", "startOffset": 48, "endOffset": 51}, {"referenceID": 32, "context": "In standard NNLMs, such as feedforward networks [3] or recurrent networks [33], computing this probability over the whole vocabulary is the bottleneck.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16].", "startOffset": 82, "endOffset": 93}, {"referenceID": 12, "context": "Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16].", "startOffset": 82, "endOffset": 93}, {"referenceID": 15, "context": "Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16].", "startOffset": 82, "endOffset": 93}, {"referenceID": 4, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 152, "endOffset": 159}, {"referenceID": 19, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 152, "endOffset": 159}, {"referenceID": 12, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 322, "endOffset": 334}, {"referenceID": 35, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 322, "endOffset": 334}, {"referenceID": 37, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 322, "endOffset": 334}, {"referenceID": 4, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 12, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 15, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 37, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 7, "context": "[8] for a comparative study.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "For the sake of completeness, we refer the reader to other strategies that can speed-up the training of language models in complementary manners [31].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "The Hierarchical Softmax (HSM) is an approximation of the softmax function introduced in Goodman [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 54, "endOffset": 62}, {"referenceID": 33, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 54, "endOffset": 62}, {"referenceID": 37, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 35, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 5, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 66, "endOffset": 77}, {"referenceID": 26, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 66, "endOffset": 77}, {"referenceID": 28, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 66, "endOffset": 77}, {"referenceID": 33, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "[29] proposes an optimal hierarchy by constructing a Huffman coding based on frequency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[48] constructs their hierachy in order to explicitly reduce the computational complexity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "The idea of keeping a short-list of the most frequent words has been explored before [27, 39].", "startOffset": 85, "endOffset": 93}, {"referenceID": 38, "context": "The idea of keeping a short-list of the most frequent words has been explored before [27, 39].", "startOffset": 85, "endOffset": 93}, {"referenceID": 26, "context": "[27] combines a short-list with a hierachical softmax based on word representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21].", "startOffset": 216, "endOffset": 220}, {"referenceID": 3, "context": "In particular, importance sampling [4, 5] selects a subset of negative targets to approximate the softmax normalization.", "startOffset": 35, "endOffset": 41}, {"referenceID": 4, "context": "In particular, importance sampling [4, 5] selects a subset of negative targets to approximate the softmax normalization.", "startOffset": 35, "endOffset": 41}, {"referenceID": 4, "context": "Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29].", "startOffset": 160, "endOffset": 168}, {"referenceID": 28, "context": "Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29].", "startOffset": 160, "endOffset": 168}, {"referenceID": 15, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 49, "endOffset": 61}, {"referenceID": 36, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 49, "endOffset": 61}, {"referenceID": 42, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 49, "endOffset": 61}, {"referenceID": 0, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "Noise Contrastive Estimation [16] replaces the softmax by a binary classifier distinguishing the original distribution", "startOffset": 29, "endOffset": 33}, {"referenceID": 36, "context": "While the original formulation still requires to compute the softmax normalization, Mnih and Teh [37] shows that good performance can be achieved even without it.", "startOffset": 97, "endOffset": 101}, {"referenceID": 43, "context": "[44] have also proposed an efficient way to train model with high dimensional output space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The joint distribution is defined as a product of conditional distribution of tokens given their past [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "This problem is traditionally addressed with non-parameteric models based on counting statistics [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 38, "endOffset": 49}, {"referenceID": 22, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 38, "endOffset": 49}, {"referenceID": 23, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 38, "endOffset": 49}, {"referenceID": 29, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33].", "startOffset": 103, "endOffset": 114}, {"referenceID": 21, "context": "More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33].", "startOffset": 103, "endOffset": 114}, {"referenceID": 32, "context": "More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33].", "startOffset": 103, "endOffset": 114}, {"referenceID": 2, "context": "They are mostly either feedforward networks [3] or recurrent networks [33].", "startOffset": 44, "endOffset": 47}, {"referenceID": 32, "context": "They are mostly either feedforward networks [3] or recurrent networks [33].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "This model is parameterized by the weight matrices P , A and B and is routinely learned with an optimization scheme such as stochastic gradient descent or Adagrad [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "A Recurrent network [12] extends a feedforward network in that the current state of the hidden layer also depends on its previous state.", "startOffset": 20, "endOffset": 24}, {"referenceID": 44, "context": "Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].", "startOffset": 182, "endOffset": 190}, {"referenceID": 45, "context": "Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].", "startOffset": 182, "endOffset": 190}, {"referenceID": 32, "context": "Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 146, "endOffset": 149}, {"referenceID": 31, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 186, "endOffset": 190}, {"referenceID": 21, "context": "These models have been successfully used in the context of language modeling [22, 33, 35].", "startOffset": 77, "endOffset": 89}, {"referenceID": 32, "context": "These models have been successfully used in the context of language modeling [22, 33, 35].", "startOffset": 77, "endOffset": 89}, {"referenceID": 34, "context": "These models have been successfully used in the context of language modeling [22, 33, 35].", "startOffset": 77, "endOffset": 89}, {"referenceID": 21, "context": "In this work, we focus on the standard word level LSTM architecture since it has obtained state of the art performance on the challenging One Billion Word Benchmark [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "When the vocabulary is large, this step is computationally expensive and often dominates the computation of the whole model [22, 32], as discussed in introduction and related work.", "startOffset": 124, "endOffset": 132}, {"referenceID": 31, "context": "When the vocabulary is large, this step is computationally expensive and often dominates the computation of the whole model [22, 32], as discussed in introduction and related work.", "startOffset": 124, "endOffset": 132}, {"referenceID": 12, "context": "A simple approach [13] to reduce this computational cost is to assign each word w of the vocabulary to a unique class C(w) and to factorize the probability distribution over words as", "startOffset": 18, "endOffset": 22}, {"referenceID": 46, "context": "In natural languages, the distribution of the words notoriously follows a Zipf law [47].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "Similar to the frequency binning hierarchical softmax [34], this information can be exploited to reduce the computation cost.", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "These two clusters can be organized in two different ways: either they are both leaves of a 2-level tree [34], or the head cluster is kept as a short-list in the root node [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "These two clusters can be organized in two different ways: either they are both leaves of a 2-level tree [34], or the head cluster is kept as a short-list in the root node [27].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "[8], we exploit this observation to further reduce the computational cost of our classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Unlike [8], we share the state of hidden layer across clusters and simply reduce the input size of the classifiers by applying a projection matrix.", "startOffset": 7, "endOffset": 10}, {"referenceID": 33, "context": "We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance (around 5\u2212 10% performance drop) [34, 48].", "startOffset": 155, "endOffset": 163}, {"referenceID": 47, "context": "We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance (around 5\u2212 10% performance drop) [34, 48].", "startOffset": 155, "endOffset": 163}, {"referenceID": 31, "context": "It has been recently used for language modeling [32] and has a vocabulary of 44k words.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "\u2022 Europarl2 is a machine translation corpus, containing 20 languages [25].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "We use Adagrad [11], with a step size of 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Interpolated Kneser-Ney 5-gram [7] 67.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "6 Feedforward NN + D-Softmax [8] 91.", "startOffset": 29, "endOffset": 32}, {"referenceID": 27, "context": "2 4-layer IRNN-512 [28] 69.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "4 RNN-2048 + BlackOut sampling [20] 68.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "3 Sparse Non-negative Matrix factorization [41] 52.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "9 RNN-1024 + MaxEnt 9-gram [7] 51.", "startOffset": 27, "endOffset": 30}, {"referenceID": 21, "context": "3 2-layer LSTM-8192-1024 + CNN inputs [22] 30.", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 137, "endOffset": 143}, {"referenceID": 3, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 137, "endOffset": 143}, {"referenceID": 7, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 7, "context": "For the negative sampling method, we used a number of samples equal to 20% of the size of the vocabulary [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "[8], where each word cluster uses a disjoint subset of the hidden representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] obtains similar results but is slower by a factor \u00d71.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] achieves a lower perplexity, but with a model 8\u00d7 bigger than ours and trained over 32 GPUs during 3 weeks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "This is achieved by explicitly taking into account the computational complexity of parallel systems and combining it with a few important observations, namely keeping a short-list of frequent words in the root node [39] and reducing the capacity of rare words [8].", "startOffset": 215, "endOffset": 219}, {"referenceID": 7, "context": "This is achieved by explicitly taking into account the computational complexity of parallel systems and combining it with a few important observations, namely keeping a short-list of frequent words in the root node [39] and reducing the capacity of rare words [8].", "startOffset": 260, "endOffset": 263}], "year": 2016, "abstractText": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "creator": "LaTeX with hyperref package"}}}