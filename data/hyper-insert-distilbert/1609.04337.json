{"id": "1609.04337", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Quick and energy-efficient Bayesian computing of binocular disparity using stochastic digital signals", "abstract": "reconstruction of the usual tridimensional geometry of evaluating a visual graphic scene using the binocular disparity information is an inherent important challenging issue in computer vision and mobile robotics, which can be formulated as only a bayesian inference matching problem. arguably however, effective computation of remembering the approximate full disparity of distribution problems with an advanced sequential bayesian model interpreter is usually an often intractable analytic problem, nowadays and only proves computationally challenging even that with a completely simple operational model. only in applying this paper, we briefly show roughly how probabilistic controller hardware interfaces using parallel distributed memory and alternate representation of data encoding as stochastic bitstreams normally can solve that problem with low high performance efficiency and energy requirement efficiency. ideally we naturally put forward a different way to express discrete probability distributions using stochastic configuration data representations and perform bayesian virtual fusion demonstrations using those associated representations, and show me how that experimental approach can be physically applied to neural diparity computation. theoretically we best evaluate the target system using a simulated stochastic implementation tree and discuss possible sophisticated hardware implementations indicative of commonly such analog architectures and their potential for combine sensorimotor processing and acoustic robotics.", "histories": [["v1", "Wed, 14 Sep 2016 16:41:31 GMT  (3301kb,D)", "https://arxiv.org/abs/1609.04337v1", "Preprint of article submitted for publication in International Journal of Approximate Reasoning and accepted pending minor revisions"], ["v2", "Mon, 31 Oct 2016 15:36:01 GMT  (5037kb,D)", "http://arxiv.org/abs/1609.04337v2", "Preprint of article submitted for publication in International Journal of Approximate Reasoning and accepted pending minor revisions"]], "COMMENTS": "Preprint of article submitted for publication in International Journal of Approximate Reasoning and accepted pending minor revisions", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["alexandre coninx", "pierre bessi\\`ere", "jacques droulez"], "accepted": false, "id": "1609.04337"}, "pdf": {"name": "1609.04337.pdf", "metadata": {"source": "CRF", "title": "Quick and energy-efficient Bayesian computing of binocular disparity using stochastic digital signals", "authors": ["Alexandre Coninxa", "Pierre Bessi\u00e8re", "Jacques Droulez"], "emails": ["alexandre.coninx@isir.upmc.fr"], "sections": [{"heading": null, "text": "Reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics, which can be formulated as a Bayesian inference problem. However, computation of the full disparity distribution with an advanced Bayesian model is usually an intractable problem, and proves computationally challenging even with a simple model. In this paper, we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency. We put forward a way to express discrete probability distributions using stochastic data representations and perform Bayesian fusion using those representations, and show how that approach can be applied to diparity computation. We evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics.\nKeywords: Bayesian inference, stochastic computing, sensory processing, energy efficiency, hardware implementation, binocular disparity"}, {"heading": "1. Introduction", "text": "Using two cameras in a stereoscopic setup to reconstruct the tridimensional geometry of a visual scene, in a way similar to that performed by human stereopsis, is an important issue in computer vision, with major applications to autonomous robotics (and more specifically autonomous driving [1]). That issue has been an active research topic since at least 40 years, and a wide range of methods and algorithms have been proposed [2, 3] and evaluated on standardized benchmarks [4, 5].\nSeveral works have shown that the binocular disparity computation can efficiently be formulated as a Bayesian inference problem [6, 7]. The disparity value for each pixel is then expressed as a discrete probability distribution, which can\n\u2217Corresponding author Email address: alexandre.coninx@isir.upmc.fr (Alexandre Coninx)\nPreprint submitted to International Journal of Approximate Reasoning November 1, 2016\nar X\niv :1\n60 9.\n04 33\n7v 2\n[ cs\n.C V\n] 3\n1 O\nct 2\nbe computed through a probabilistic model using likelihood values specified from the image data. However, computing the full disparity distribution on whole images proves challenging and compuationally demanding. That\u2019s why most works on binocular disparity using Bayesian models instead reduce the output to a single disparity value per pixel (often using the maximium a-posteriori likelihood estimator). That approach simplifies the computation and allows to reformulate it as an energy minimization problem that can be solved efficiently by classic optimization techniques such as dynamic programming [see 6, for an exemple].\nHowever, it means that although the computation is based on a probabilistic formalism, it yields deterministic disparity values and not disparity distributions, despite the latter representation being richer and offering many benefits, especially for robotics and sensorimotor systems. Full disparity distributions can accurately represent cases where stereopsis is not sufficient to completely determinate the world geometry, such as ambiguous pixels with multiple matches, or pixels with no matches (e.g. due to occlusions). Such probabilistic representations can also directly be used by Bayesian mapping and navigation methods such as the Bayesian occupation filter [8], and more generally by probabilistic and Bayesian robotics techniques [9, 10, 11]. Bayesian inference also provides a powerful framework to express assumptions and prior knowledge about the structure of the world (for example the location of the ground or other known objects) as prior probability distributions.\nStochastic computing is a field dedicated to designing and using computing devices that are intentionally stochastic to perform probabilistic reasoning, using non-Von Neumann architectures, distributed memory and specific data representations. More specifically, the BAMBI project is a research effort to develop stochastic machines implementing Bayesian inference (Bayesian machines) [12]. In this paper, we show how those Bayesian machines can be used to efficiently compute full binocular disparity distribution, paving the way towards fully stochastic autonomous robots and other sensorimotor systems.\nIn the remainder of this article, we will first give an overview of the related work in section 2, both about stochastic computing and fast binocular disparity computation. We will then describe our Bayesian binocular disparity computation model in section 3. Section 4 will be dedicated to the description of the stochastic computer implementing that model, focusing first on the general principles of computation using stochastic bitstream and second to their application to the Bayesian disparity computation. The evaluation of that system and its results will be presented in section 5 and further discussed in section 6. We will then conclude in section 7 by summing up the implications of that work for the design of Bayesian robotic systems using stochastic components and discussing the future prospects of that topic."}, {"heading": "2. Previous work", "text": ""}, {"heading": "2.1. Hardware stochastic computing", "text": "The general idea of stochastic computations with temporal coding can be traced back to the seminal works of Von Neumann [13] and Gaines [14] who highlighted the interest of such data representations, but their approaches were not widely pursued due to the rapid development of more efficient deterministic computers. The topic has recently received a renewed attention due to the development of probabilistic and Bayesian models in computer science and engineering \u2013 and more specifically for sensorimotor and cognitive systems \u2013 and the limitations of classic computers to implement those models.\nThe idea of developing hardware dedicated to bayesian reasoning has recently been pursued by several teams [15, 16, 17], exploring different computational paradigms to perform probabilistic inference. To address the problem of approximate inference Mansinghka [16] uses sampling methods for approximate inference and in a similar way, Jonas designed Markov Chain Monte Carlo based algorithms to provide a representation of probability distributions as sets of samplers [17]. To compute exact inference, a number of different frameworks and toolsets have been put forward. Vigoda [15] designed architectures based on probabilities represented by analog signals, and used the message passing algorithm to compute exact inference. More recently, a research project conducted at the Nanoscale Computing Fabrics Laboratory has led to the design of an unconventional hardware architecture based on electro-magnetic computations to perform inference on Bayesian Network models [18]. Ferreira et al. [19] also showed that exact inference can be efficiently computed using GPU hardware for some high-dimensional problems. Finally, the approach taken by Thakur et al. [20] is quite similar to ours: they use stochastic bitstreams and target special inference problems. They have proposed two frameworks, BEAST (Bayesian Estimation And Stochastic Tracker) and BIND (Bayesian INference in DAG), to perform inference using stochastic electronics on two types of Bayesian models, Hidden Markov Models and Direct Acyclic Graphs (DAG) respectively.\nIn the framework of the BAMBI project, another stochastic architecture has been proposed to perform naive Bayesian fusion using Muller C-Elements [21], which achieves exact inference with normalization for binary random variables, but create harmful correlations in the stochastic signals and can\u2019t be easily extended to non-binary discrete distributions. Other recent work conducted within the BAMBI project have proposed using digital signals with temporal coding to perform Bayesian inference, and a proof-of-concept to solve a simple sensorimotor problem has been put forward [22]. In this paper, we apply the same principles to a more computationally challenging Bayesian model to highlight their benefits."}, {"heading": "2.2. Disparity computation", "text": "As it provides a way to estimate the depth information using data from standard digital cameras, the binocular disparity problem has received a wide attention since the beginnings of computer vision. Existing approaches have\nbeen summarized in reviews [2, 3], which show that most methods follow the same general structure which can be divided in three steps:\n1. Computing a matching cost, which is a positive value associated to each possible pair of matching pixels1. The matching cost is a dissimilarity measure: the least likely the pixels are to match, the higher it is. The cost is computed locally, typically by comparing the luminance or color of individual pixels. The most common matching cost is the squared difference of pixel values [2], but some other techniques preprocess the image with operators such as the gradient [23] or use banks of linear spatial filters [24].\n2. Applying an optional cost aggregation, which performs spatial integration of the pixel-wise information provided by cost values. The main goal of that step is to take into account the fact that most points of the disparity map are locally smooth and therefore neighbouring pixels have correlated disparity values. The simplest form of cost aggregation relies on averaging cost values for a given disparity across a given neighborhood.\n3. An optimization step, which uses the (aggregated) cost to compute the final disparity image. This step can be limited to simply selecting the disparity value associated to the lowest cost in a winner-takes-all way. But it can also involve global computations to optimize the disparity map with regard to a given world model (e.g. smoothness, plane surfaces, etc. [6]), using techniques such as dynamic programming, in which cases it can complement or replace cost aggregation."}, {"heading": "2.2.1. Bayesian disparity computation", "text": "Several of the existing works [6, 7] use the Bayesian inference framework to describe this process. For example, Belhumeur [6] proposes to reconstruct the scene geometry S from the left and right images Il and Ir using a Bayesian model:\nP (S|Il, Ir) \u221d P (S) \u00b7 P (Il, Ir|S) (1)\nwith P (S) being a prior specifying the expected shape (smooth, etc.) of the world and P (Il, Ir|S) a data term computed from the matching cost. Computing P (Il, Ir|S) therefore corresponds to the matching cost computation step, there is no cost aggregation step, and computing and integrating the prior constitutes the optimization step. Belhumeur uses squared difference to compute the cost and proposes three increasingly complex world models to define the prior, but the computation of the full posterior probability distribution \u2013 which has cardinality (Dmax + 1) w\u00d7h \u2013 is intractable.\n1Most algorithms use rectified image pairs, which allows to only consider pixels on corresponding rows for matching, and limit the disparity to a maximum value Dmax corresponding to a minimum distance. Dmax depends on image resolution, camera focal length and visual environment; typical values are 50 to 100 pixels.\nHe therefore uses an energy formalism and defines E[S] = \u2212 log(P (S) \u00b7 P (Il, Ir|S)), which allows to compute S\u0302 = arg max\nS P (S|Il, Ir) by minimizing\nE[S], and shows that a simplified form of this optimization problem can be solved by dynamic programming. As mentioned in section 1, despite that algorithm being based on Bayesian inference, it only yields a single disparity value for each pixel."}, {"heading": "2.2.2. Supervised techniques", "text": "The development of public image pairs datasets provided with a disparity baseline such as the KITTI dataset [4] or the Middlebury dataset [5] have made it possible to treat disparity computation as a supervised machine learning problem. Some algorithms use deep convolutional networks to learn the matching cost [25, 26], and perform cost aggregation and optimization using other techniques.\nThose techniques currently populate the top of the KITTI leaderboard2. Although they are extremely accurate on benchmarks, their efficiency depend on the existence of a relevant supervised training dataset. Besides, they are computationally very intensive, using high-end CPUs and GPUs and sometimes requiring a computing time of several minutes per frame. Those features would make applying those techniques in a mobile robotics context challenging."}, {"heading": "2.2.3. Sampling approach", "text": "An approach that is directly relevant to our positioning is the method proposed by Jonas et al. [17] as an application of his aforementioned hardware architecture for approximate inference. In that work, they model the disparity distribution using a Markov random field, and use a hardware architecture using Gibbs sampling to sample the posterior distribution. Although this approach is efficient and allows to use a computationally intensive Bayesian disparity model with global optimization, it uses a unique, centralized pseudo-random number generator as source of entropy and lacks some of the features of our system, such as the high parallelism and the robust computation of the full disparity with a very low number of clock cycle."}, {"heading": "3. Bayesian disparity computation model", "text": ""}, {"heading": "3.1. Overview", "text": "The goal of the disparity computation is to estimate the tridimensional geometry of a visual scene from two rectified images taken from two identical cameras with focal length f distant from a known baseline distance B. If an object projects into the left camera\u2019s image plane Il at position x and in the right camera\u2019s image plane Ir at position x \u2212 d, its depth Z from the cameras\n2http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo, consulted 24/03/2016\ncan be computed by Z = B\u00b7fd (see fig. 1). The goal of a disparity algorithm is therefore to identify matching pixels in the two images to compute the disparity."}, {"heading": "3.2. Model description", "text": "As mentioned in section 2.2.1, the main obstacle to compute full disparity distributions is the very high cardinality of the considered distribution: integrating smoothness constraints in the probabilistic model requires to perform inference on distributions of size (Dmax + 1)\nNpixels , where Npixels is the number of pixels in the domain on which the optimization is performed. If the optimization is performed on the whole image or on entire rows or columns (as is the case in [6]) the problem becomes completely intractable, but even smaller integration neighborhoods are problematic. In order to avoid that issue, we will perform all of the spatial information integration as image preprocessing operations, and then only perform pixelwise Bayesian operations using the preprocessed data.\nOur stereo matching method therefore relies on the preprocessing of images using linear convolution filters to extract relevant features. Other algorithms have used such convolution filters for disparity computation [24], although they process the feature information from those filter in a different way. The relevance of using such linear spatial filters as a preprocessing step is also highlighted by recent works using deep neural networks to compute disparity [25, 26], which use a convolutional layer (with filters trained through supervised learning) as their input.\nThe feature maps output by the filters are then used to compute feature matching costs for pixel pairs corresponding to the possible disparities. Those costs are used to compute probabilistic likelihood functions similar to those used by Belhumeur [6], and those likelihood terms are then combined using naive Bayesian fusion.\nIn our method, we only use three simple square spatial filters of size 5 pixels to process images of width W and height H:\n\u2022 One simple luminance linear averaging filter m;\n\u2022 One linear horizontal luminance gradient filter gH ;\n\u2022 One linear vertical luminance gradient filter gV .\nFor each of the three filters f \u2208 {m, gH , gV }, we compute the left and right feature maps by applying the filter to the left and right images: f l = Il \u2217 f and fr = Ir \u2217 f . Due to the size of the convolution filters, those feature maps have width Wf = W \u2212 4 and height Hf = H \u2212 4.\nFor each pair of feature maps and each possible disparity value we compute a matching cost, using the simple squared difference:\nCf (x, y, d) = (f l(x, y)\u2212 fr(x\u2212 d, y))2 (2)\nfor d \u2208 J0;DmaxK and x \u2265 Dmax. The cost shown by Eq. 2 measures the dissimilarity between the pixels at coordinate (x, y) in the left image and (x \u2212 d, y) in the right image for the feature f . In order to use a Bayesian inference framework, we use these costs to compute likelihood probability values:\np(fr(x\u2212 d, y)|f l(x, y), [D(x, y) = d]) = p0 + (1\u2212 p0)e \u2212 Cf (x,y,d) 2\u03c32 f (3)\nEquation 3 expresses the likelihood of observing the value fr(x\u2212d, y) in the right feature map if the value f l(x, y) is observed in the left feature map and the disparity at coordinates (x, y) is d. That probabilistic formulation allows us to specify a base probability p0 of the features matching even if the cost is high (which can happen when the two images locally differ for reasons unrelated to the problem, for example because of specular reflections), and a parameter \u03c3f representing the expected inaccuracy of the cost measurement (a small value of \u03c3f results in a null or very small cost being required to give a high likelihood value).\nIn the following, we will drop the (x, y) and (x\u2212 d, y) spatial coordinates in equations for better readability. Assuming conditional independance between the likelihood terms, we can compute the disparity distribution using naive Bayesian fusion:\np([D = d]|Il, Ir) \u221d p([D = d]) \u220f\nf\u2208{m,gH ,gV }\np(fr|f l, [D = d]) (4)\nwhere p(fr|f l, [D = d]) are the the likelihoods computed by eq. 3 and p(D) is a prior on the disparity distribution, which can either be set to uniform or be used to represent prior information about the world (for example, if we know the world contains a flat floor with no holes, the prior probability of disparities corresponding to objects under the floor can be set to zero).\np(D|Il, Ir), is the posterior disparity distribution, which can be used in further probabilistic computation \u2013 for example as input of a probabilistic occupancy model \u2013 or estimated using the maximum a-posteriori (MAP) estimator: d\u2217 = arg max\nd\u2208J0;DmaxK p([D = d]|Il, Ir)\nWe described the algorithm with three simple filters operating on luminance data, but the same method can easily be extended to color processing (by applying each filter to each of the three color layers), or to using a higher number of filters using various convolution kernels."}, {"heading": "4. Stochastic implementation of the model", "text": "Previous work [22] has shown that naive Bayesian fusion could be performed by stochastic machines. In this section, we will describe that structure of a Bayesian machine architectured as a matrix of stochastic operators and explain how it can be used to implement the probabilistic binocular disparity computation detailed in section 3.2."}, {"heading": "4.1. Stochastic Bayesian fusion", "text": ""}, {"heading": "4.1.1. Probabilities as stochastic bitstreams", "text": "Our stochastic computational architecture represents data using stochastic bitstreams. Stochastic bitstreams are random digital binary signals that express a probability value (p-value)) by the proportion of bits set to 1 in a given signal (Fig. 2a). Generating a stochastic bitstream b encoding probability p is therefore done by using a random number generator outputing random bits set to 1 with a probability p. Conversely, extracting the value of p from b and storing it as a floating point or fixed point number requires to integrate information from b on an extended duration to count the proportion of bits set to 1, the precision of the recovered p value increasing with the integration time.\nIf two probability values p1 and p2 are encoded by two uncorrelated stochastic bitstreams b1 and b2 and those two signals are input to a logic AND gate, the probability pout of the output signal sout to be in state 1 at a given time is given by :\npout = P ([sout = 1])\n= P ([s1 = 1] \u2227 [s2 = 1]) = P ([s1 = 1]) \u00b7 P ([s2 = 1]|[s1 = 1]) = P ([s1 = 1]) \u00b7 P ([s2 = 1]) = p1 \u00b7 p2\nThe stochastic signal data representation allows to perform probability product with a simple logic circuit."}, {"heading": "4.1.2. Representation of discrete random variables: the stochastic bus", "text": "A discrete random variable V with cardinality M can be represented by a set of M stochastic bitstreams b1, . . . , bM , which we will name a stochastic bus of width M . The j-th bitstream bj encodes a probability pj = C \u00b7 P ([V = Vj ]). C is a bus normalization constant chosen to facilitate data encoding and processing : since\n\u2211 j P ([V = Vj ]) = 1, we have \u2211 j pj = C. A useful choice is\nCmax = 1\nmax j\nP ([V=Vj ]) , which allows to represent the most probable value V maxj\nby pmaxj = 1 and maximizes the p-values of other signals on the bus. Stochastic buses can be instanciated by a set of M random number generators outputting the individual bitstreams. Similarly, a set of M counters can be used to recover the unnormalized probability distribution C \u00b7 P (V ).\nThat data representation implies that the average number of bits before observing a \u201d1\u201d on the j-th signal of the bus is Tavg =\n1 C\u00b7P ([V=Vj ]) . That number,\nwhich directly determines the number of bits necessary to get an accurate reconstruction of the distribution using counters, depends on the shape of the distribution and on the value of C, which is modified by the computations done on the bus and can often not be easily controlled or computed. This creates two problems. First, the number of bits necessary to reconstruct the distribution with a given desired precision can\u2019t be easily anticipated. Second, in some cases \u2013 especially if C is low \u2013 that number may be very high, which leads to poor performance of the stochastic machine (which we call the time dilution problem).\nThe first problem can be adressed by integrating the data until a given number of \u201d1\u201d bits have been observed on a signal, instead of during a fixed number of bits. This can easily be achieved using counters overflow. If a stochastic bitstream of width M is connected to counters with a maximum value nmax, we can run the signals until one of the M counters (with index (jmax) overflows. If the computation is stopped at that moment, the counter with index jmax stores the value nmax corresponding to the p-value p max j = 1, and the other counters store values nj corresponding to p-values pj = nj\nnmax .\nThat process allows to renormalize the distribution with regard to the maximum probability value pmaxj , and to read it as a set of fixed-point numbers with precision depending on nmax. Furthermore, the index of the overflowing counter immediately gives the index of the most probable value, which implements the maximum a-posteriori estimator."}, {"heading": "4.1.3. Bayesian inference with stochastic bitstreams: the Bayesian machine", "text": "One of the most common Bayesian computing techniques is naive Bayesian fusion [27] : computing the posterior probability distribution on a searched variable S, knowing a prior distribution P (S) and the conditional distributions P (Ki|S) on some known variables K1, . . . ,KN . If the Ki variables are condi-\ntionally independant given S, the inference is computed by :\nP (S|K1, . . . ,KN ) = 1\nZ P (S) N\u220f i=1 P (Ki|S) (5)\nwhere Z is a normalization constant. This distribution can be computed using stochastic bitstreams by representing both the prior P (S) and the data terms P (Ki|S) with stochastic buses of width M , corresponding to the cardinality of S. After the bitstreams bj,0 (j \u2208 {1, . . . ,M}) encoding the prior values P (S = Sj) (with a bus normalization constant C0) are generated, the data terms can be integrated using simple computational modules comprised of a memory, a random generator and a logic AND gate as described in fig. 2b. For each line j \u2208 {1, . . . ,M} in the stochastic bus and for each data term i \u2208 {1, . . . , N}, the memory stores the value pi,j = Ci \u00b7 P (Ki|S = Sj) (where Ci is the bus normalization constant associated with data term i), the random generator generates a stochastic bitstream encoding probability pi,j , and the AND gate perform the probability product between that signal and the signal bj,i\u22121 from the previous element, outputting signal bj,i.\nThe resulting architecture performs Bayesian inference using a matrix of stochastic operators, with a number of rows equal to the cardinality M of variable S and a number of columns equal to the number of data terms N (see fig. 3). The output stochastic bus, comprised of the signals bj,N for j \u2208 {1, . . . ,M}, encodes the posterior probability distribution P (S|K1, . . . ,Kn), with a bus nor-\nmalization constant Cout = N\u220f i=0 Ci."}, {"heading": "4.2. Stochastic disparity computation", "text": ""}, {"heading": "4.2.1. General description", "text": "The architecture described in section 4.1 can be used to implement the disparity computation model described in section 3. The search variable is the disparity D, which takes values in J0;DmaxK and therefore has cardinality Dmax+1, and the data terms are the three likelihood values computed from the luminance features3 through equation 3.\nWe therefore use such a matrix of stochastic operators with N = 3 and M = Dmax + 1 to compute a stochastic bus representation of the posterior disparity representation. In the following, we will use a uniform disparity prior (P ([D = d]) = 1Dmax+1\u2200i \u2208 J0;DmaxK), which can efficiently be represented by a stochastic bus with all signals constantly equal to 1 (C0 = Dmax + 1). Each of the data terms are integrated as described above in section 4.1, and the full disparity distribution for a pixel can be estimated using counters (see fig. 4). If the posterior disparity distribution is unimodal and clearly indicates a disparity value, that value can be estimated by the maximum a-posteriori estimator by simply getting the index of the first overflowing counter, as suggested in section 4.1.2."}, {"heading": "4.2.2. Processing of occlusions and low-contrast areas", "text": "Although the previous architecture allows for efficient computation when the output distribution is unimodal and indicates a clear disparity value or a\n3Color processing, with each of the three convolution filters being applied to each color layer, was also considered and experimented, but did not show significant improvement over luminance processing in the present case.\nsmall range of values, we must adapt it to take into account some issues that arise from the fact that disparity values cannot always be computed. We will describe those problems and their consequence on the architecture, and then put forward a solution.\nIn some cases such as occlusion (see fig. 5), some pixels in the left image have no matching pixel in the right one and the matching costs will therefore be high for every possible disparity value. In our Bayesian model, it means the values computed by equation 3 will be small for all d \u2208 J0;DmaxK, which in our stochastic architecture translates to very low p-values for all output signals. For example, in the limit case of a pixel (x, y) where the matching cost Cf (x, y, d) is infinite for all disparity values d \u2208 J0;DmaxK and for each feature f \u2208 {m, gH , gV } in equation 3, we have p(fr(x\u2212 d, y)|f l(x, y), [D(x, y) = d]) = p0, \u2200d \u2208 J0;DmaxK, \u2200f \u2208 {m, gH , gV }. In our stochastic computation, all the signals in the output stochastic bus is have a p-value of p30. This corresponds to a uniform distribution \u2013 which is correct since no information could be inferred about the disparity value from the data \u2013 but that distribution is encoded with a very low bus normalization constant C = (Dmax + 1) \u00b7 p30, which is problematic because of the time dilution problem mentioned in section 4.1.2. For p0 = 0.02, for example, it means that an average of one every 125000 bits will be set to 1, and the machine has to be run for an average of one million cycles simply to fill a 8-bits counter, which is very inefficient.\nIn some other cases, such as large uniform areas with no texture or distinctive features, the opposite problem arises: many (or possibly all) disparity values are possible match and therefore have low matching costs. The likelihood values p(fr(x \u2212 d, y)|f l(x, y), [D(x, y) = d]) then have values close to 1 for all disparity values d \u2208 J0;DmaxK, and all the the signals in the output stochastic bus will have a p-value close to 1, which encodes a high-entropy, close to uniform distribution with a high bus normalization constant. Again, this is a correct result and the high bus normalization constant means the time dilution problem does not arises; that output can efficiently be converted to numerical values or used in further stochastic computations. However, such high-entropy distributions are ill-suited to the use of the maximum a-posteriori estimator, which will return a random result among the possible disparity values.\nA way to solve both those problems is to explicitly model the case where\na pixel can\u2019t satisfyingly be matched, either because of occlusions or because of a lack of contrast, with an extra signal on the stochastic bus encoding a probability pnomatch:\nP (nomatch(x, y)) = pnm0 + (1\u2212 pnm0)e \u2212 (g\nl V (x,y)) 2 2\u03c32nm (6)\nThe first term in the equation is a probability pnm0 p30 that determines the time until which an occluded pixel is detected as not matching. It should be low enough that if the pixel can be correctly matched, the stochastic signal of the corresponding disparity value j has a p-value pj > pnm0, but high enough that if, as described above, no match is possible because of an occlusion, the \u201cno match\u201d signal fills its counter and stops the computation in a reasonable time, while detecting an absence of match.\nThe second term of equation 6 handles the poorly contrasted areas, which have been found to be characterized by low values of the vertical gradient4 glV (x, y). Weakly contrasted areas will therefore have a P (nomatch(x, y)) value very close to 1, and the corresponding stochastic signal will very quickly fill the counter and detect an absence of match before a spurious match attributed to the behavior of the MAP estimator can be detected.\nThe final architecture for our disparity computation stochastic machine is shown in fig. 6. With the extra \u201cno match\u201d signal, it has a dimension N = 3 and M = Dmax + 2.\n4Note that the square of the gradient value of the left image itself is used, and not a matching cost associated to the gradient as in equation 3. P (nomatch(x, y)) is therefore high if the gradient is close to zero, that is in weakly contrasted areas."}, {"heading": "5. Stochastic model evaluation", "text": ""}, {"heading": "5.1. Model implementation", "text": "In order to evaluate the benefits of using a stochastic disparity computation system, we will compare two implemetations of the same Bayesian disparity algorithm described in section 3.2\n\u2022 A reference implementation performing the computation as floating point operations.\n\u2022 A simulated stochastic implementation, using pseudo-random number generators (PRNG) and bitwise boolean logic operations to simulate the behaviour of the Bayesian machine described in section 4.2.\nBoth implementations are programs written in C++ and run on a desktop computer equipped with an Intel Xeon E3-1271 v3 64-bit CPU. The reference implementation use FPU computations using 64 bit floating point numbers. The simulated stochastic implementation uses the Mersenne twister 19937 PRNG provided by the GNU implementation of C++11 to generate stochastic bitstreams, and the 64-bit bitwise boolean AND operation to perform probability product."}, {"heading": "5.2. Results", "text": "The reference implementation ran in about 1.25 seconds per frame, which is the order of magnitude of the \u201cfast\u201d disparity algorithms from the state of the art. The simulated stochastic implementation ran in 25 to 110 seconds per frame (depending on the frame and on the size of the output counters). That low performance is due to the overhead of simulating stochastic machines using non-stochastic hardware; the performance of the stochastic system is better estimated by the number of simulated clock cycles used to compute a frame (see below)."}, {"heading": "5.2.1. Dataset and model parameters", "text": "We collected stereo image pairs using a PointGrey BumbleBee2 BB2-03S2C25 wide-angle color stereoscopic camera, with focal length f = 2.5mm, baseline distance B = 120mm and resolution 640 \u00d7 480 at 25 frames per second. The images were rectified using the Triclops proprietary PointGrey middleware. The camera was mounted on a TurtleBot 2 mobile robot base, which was manually controlled in an office environment to collect data. A total of 6301 frames were captured, corresponding to 4 minutes and 10 seconds of video.\nThe 24 bits color images captured were converted to 8 bits luminance images, with pixel values in J0; 255K. The preprocessing described in section 3.2 therefore generate feature maps with pixel values in J0; 255K for the averaging filter and J\u2212127; 127K for the gradients. The Dmax value was set to 80, which corresponds to a minimum distance of 42 centimeters and was found to be adequate to the size and mobility of our robot (shorter minimum distances can easily be achieved by increasing Dmax, at the price of a higher computational cost). A simple\ngrid search performed during preliminary experiments allowed us to select good values of the other parameters, summarized in table 1.\nThe feature maps were used to compute the likelihoods as described in equation 3, and those likelihoods were used both in the reference implementation and in the simulated stochastic implementation to compute the disparity distribution."}, {"heading": "5.2.2. Disparity computation accuracy", "text": "A feature of stochastic computing using stochastic bitstreams is progressive precision: the longer the information from a bitstream is integrated, the more precisely the corresponding p-value can be estimated. In the context of the stochastic bus framework described in section 4.1.2, it means that precision increases with the size of the counters used to estimate the distribution: the higher the counters\u2019 maximum value, the closer to the reference implementation the resulting distribution is expected to be. We therefore used the simulated stochastic implementation with variable counter sizes to quantify that phenomenon.\nThe stochastic disparity processor described in section 4.2 performs two functions: detecting the \u201cno match\u201d pixels, and computing the disparity distribution on matched pixels. The performance of the \u201cno match\u201d pixels discrimination task can be assessed by computing the F1-score between the set of pixels identified as \u201cno match \u201d by the reference implementation and the simulated stochastic implementation. The performance of the disparity distribution computation task can be assessed by measuring the RMS error between the distributions estimated from the simulated stochastic implementation and the reference implementation5.\nFigure 7a shows that indeed, the F1-score exponentially grows closer to 1 and the RMS error exponentially decreases with the counter max value. For example, with 16-bits counters the RMS error is below 0.05 and the F1-score above 80%.\n5Using the KL-divergence has also been considered, but proved problematic because of the frequent occurence of 0 as a p-value in the distributions estimated from the simulated stochastic implementation.\nFigure 7b shows the relationship between the counter max value and the average time (in number of clock cycles) the simulated stochastic machine has to run before filling a counter. As expected, it grows linearly with maximum value of the counter: as all signals are uncorrelated, each extra \u201c1\u201d required to fill the counter generates a constant overhead.\nFig. 8 shows an example of reconstructed disparity image with the reference implementation, and with the simulated stochastic implementation using two maximum counter values, 1 and 16. The disparity image from the stochastic system with the larger counters is visually close to the reference. The image obtained using 1-bit counters, while clearly noisier and lower quality, still correctly describes the general tridimensional structure of the scene and could possibly be used to drive a robust robot control system. According to the data from fig. 7b, the stochastic computation of the disparity distribution requires 2.21 \u00b1 0.09 clock cycles per pixel for 1-bit counters and 27.97 \u00b1 4.58 clock cycles per pixel for counters with a maximum value of 16. Both those values compare favorably to the floating point computations performing to the same operations, which requires at least 81\u00d7 3 floating point number products and a maximum search on a 81-value vector."}, {"heading": "6. Discussion: speed, energy and hardware implementation considerations", "text": "The above results show that our stochastic computational system can successfully implement a Bayesian binocular disparity algorithm and compute full\ndisparity distribution with good accuracy, using stochastic bitstreams and a reduced number of computation cycles.\nHowever, the stochastic bitstream-based computational system described in section 4.1 supposes the use of fast, efficient sources of stochastic signals, that could be integrated at a large scale in a hardware component, jointly with AND gates, memories and counters, to implement the architecture seen in section 4.1.3. In this paper, we used a simulated implementation using Mersenne twister PRNGs to evaluate the potential of this approach in the absence of such components. But recent advances in new nanodevices based on spintronics, such as the superparamagnetic tunnel junction (SMTJ) [28, 29], bear the promise that such generators could be available in the short or medium term. Experimental SMTJ devices have been shown to be able to generate high-quality stochastic bitstreams at a frequency of 500MHz with a very low power consumption of 50 \u00b5W. Those components can be built with CMOS technology using an area equivalent to 12 bytes of SRAM [30], making them suitable to large scale integration with the other components needed to build the stochastic machines described above.\nUsing those figures as guidelines, we can compute the order of magnitude of the speed and power consumption of the disparity computation systen described in section 4.2 and evaluated in simulation in section 5. The system requires 246 random signal generators, which would have a total power consumption of 12.3 mW. Using counters with a maximum value of 16, which has been shown in section 5.2.2 to be an adequate tradeoff between speed and accuracy, we need an average of 27.97 clock cycles per pixel, with (640\u22124\u221280)\u00d7(480\u22124)6, which represents an average total of 7402428.32 clock cycles per image. At a frequency of 500MHz, the architecture would therefore be able to process about 67.5 image pairs per second. As our system processes data for each pixel independantly, computation time and power consumption are expected to grow linearly with image width and height.\nThose computations are only rough estimations; more specifically the energy consumption computation ignores the energy cost of the AND gates, memories and counters also necessary to implement the circuit, and the performance does not take into account the overhead induced by reinitializing the machine between each pixel (resetting the counters and loading the data memories). Our Bayesian algorithm also makes use of preprocessed images using spatial filters; the cost (both computational and energetic) of that preprocessing should be taken into account into any global evaluation of the system. But many methods exist to perform such spatial filtering (using general-purpose CPUs, GPUs, FPGAs, dedicated hardware, etc.) with various cost, performance and energy-efficiency characteristics; further work will explore ways through which such filtering could be done using stochastic computations. Similarly, the cost computation step is\n6Each dimension of the original 640\u00d7 480 images is reduced by 4 pixels by the prefiltering as seen in section 3.2, and the horizontal dimension is further reduced by Dmax since the distribution can\u2019t be computed for the Dmax first pixels of each row as shown by equation 2.\ncurrently performed using classic floating-point computation, the opportunity to use stochastic computations instead is currently being studied.\nOn the other hand, those computations are assumed to be performed sequentially for each pixel on a unique instance of the systems described in section 4.2, using only 246 of the computing modules described in fig. 2b. But our Bayesian machine architecture is parallel by design, and a higher number of those modules would allow for parallel processing of many pixels and increased performance, at the cost of higher circuit size and energy consumption."}, {"heading": "7. Conclusion", "text": "We have put forward an architecture to compute a class of Bayesian inference problems with probabilistic hardware using stochastic bitstreams, and evaluated that system in simulation on the example of binocular disparity computation, demonstrating high performance and energy-efficiency. Although the work described in this paper uses simulations of hypothetical stochastic machines using experimental hardware devices and can therefore only offer rough estimations of the performance of those systems, it is our belief that those results clearly highlight the potential of Bayesian computation using stochastic bitstreams for sensorimotor processing, especially in applications with tight constraints on computational and energy resources such as mobile robotics, embedded systems or distributed sensors.\nThe proposed architecture allows to solve many sensory fusion and processing problems, yielding full distributions expressed as bus of stochastic bitstreams, with a low power consumption and reduced computational resources. The parallel and distributed nature of our architecture could allow to easily address a variety of different problems using the same arrays of generic components, in a way similar to FPGAs. Furthermore, the progressive precision of the stochastic bitstream data representation allows to easily adjust the speed/accuracy or power/accuracy tradeoffs by changing the signal integration time (determined by the counters maximum values), making it possible, for example, to maintain degraded operation with lower accuracy in low energy conditions.\nFuture work will entail continued collaboration with projects partner to physically instantiate the system described and simulated inm the present work. A partial implementation of our Bayesian Machine infrastructure using FPGA systems has been demonstrated [31], and further research will also integrate the technology developed by teams working on stochastic signal generator devices. Efforts will also be dedicated to extending the breadth of the computations implemented by stochastic bitstream based systems, combining the disparity computation to other sensory computations (such as optical flow) to create an occupancy map, using and extending existing Bayesian spatial cognition algorithms [32, 33, 8] which could then be used for obstacle avoidance and robot navigation, paving the way to a completely stochastic robot sensorimotor controller."}, {"heading": "Acknowledgements", "text": "This work was performed within the EU Future and Emerging Technologies BAMBI project [FP7-ICT-2013- C, project number 618024]. It was also partly supported by ANR Labex SMART [ANR-11-LABX-65]."}], "references": [{"title": "Are we ready for Autonomous Driving? The \\textsc{KITTI} Vision Benchmark Suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "Computer Vision and Pattern Recognition) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "author": ["D. Scharstein", "R. Szeliski"], "venue": "International Journal of Computer Vision 47 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Review of Stereo Vision Algorithms: From Software to Hardware", "author": ["N. Lazaros", "G.C. Sirakoulis", "A. Gasteratos"], "venue": "International Journal of Optomechatronics 2 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Vision meets robotics: The KITTI dataset", "author": ["a. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "The International Journal of Robotics Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "High-resolution stereo datasets with subpixelaccurate ground truth", "author": ["D. Scharstein", "H. Hirschm\u00fcller", "Y. Kitajima", "G. Krathwohl", "N. Ne\u0161i\u0107", "X. Wang", "P. Westling"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8753 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A Bayesian Approach to Binocular Stereopsis", "author": ["P.N. Belhumeur"], "venue": "International Journal of Computer Vision 19 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical model of color and disparity with application to Bayesian stereopsis", "author": ["C.C. Su", "A.C. Bovik", "L.K. Cormack"], "venue": "Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian Occupancy Filtering for Multitarget Tracking: An Automotive Application", "author": ["C. Coue", "C. Pradalier", "C. Laugier", "T. Fraichard", "P. Bessiere"], "venue": "The International Journal of Robotics Research 25 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic Robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "MIT Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Programmation Bay\u00e9sienne des Robots", "author": ["O. Lebeltel"], "venue": "Ph.D. thesis, Universit\u00e9 de Grenoble", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic Reasoning and Decision Making in Sensory-Motor Systems", "author": ["P. Bessi\u00e8re", "C. Laugier", "R. Siegwart"], "venue": "volume 46 of Springer Tracts in Advanced Robotics, Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Brief Survey on Computational Solutions for Bayesian Inference", "author": ["J.D. Alves", "J.F. Ferreira", "J. Lobo", "J. Dias"], "venue": "in: Workshop on Unconventional computing for Bayesian inference at IROS2015, Hamburg", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic logics and the synthesis of reliable organisms from unreliable components, 1956", "author": ["J. Von Neumann"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1956}, {"title": "Stochastic computing systems", "author": ["B. Gaines"], "venue": "Advances in information systems science ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1969}, {"title": "Analog Logic : Continuous-Time Analog Circuits for Statistical Signal Processing", "author": ["B. Vigoda"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Natively Probabilistic Computation", "author": ["V. Mansinghka"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "M", "author": ["E. Jonas", "J.B. Tenenbaum"], "venue": "a. Wilson, Stochastic Architectures for Probabilistic Computation by, Ph.D. thesis, Massachssets Institute of Technology", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-Similar Magneto-Electric Nanocircuit Technology for Probabilistic Inference Engines", "author": ["S. Khasanvis", "M. Li", "M. Rahman", "M. Salehi-Fashami", "A.K. Biswas", "J. Atulasimha", "S. Bandyopadhyay", "C.A. Moritz"], "venue": "IEEE Transactions on Nanotechnology 14 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast Exact Bayesian Inference for High-Dimensional Models", "author": ["J.F. Ferreira", "P. Lanillos", "J. Dias"], "venue": "in: Workshop on Unconventional computing for Bayesian inference (UCBI), IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["C.S. Thakur", "S. Afshar", "R.M. Wang", "T.J. Hamilton", "J. Tapson"], "venue": "van Schaik, Bayesian Estimation and Inference using Stochastic Hardware, Frontiers in Neuroscience 10 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bayesian Inference With Muller C-Elements", "author": ["J.S. Friedman", "L.E. Calvet", "P. Bessiere", "J. Droulez", "D. Querlioz"], "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers In Press ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic Bayesian Computation for Autonomous Robot Sensorimotor Systems", "author": ["M. Faix", "J. Lobo", "R. Laurent", "D. Vaufreydaz", "E. Mazer"], "venue": "in: Proceedings of the IROS2015 workshop on Unconventional computing for Bayesian inference", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Matching images by comparing their gradient fields", "author": ["D. Scharstein"], "venue": "Proceedings of 12th International Conference on Pattern Recognition 1 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "A Computational framework for determining stereo correspondence from a set of linear spatial filters", "author": ["D.G. Jones", "J. Malik"], "venue": "Image and Vision Computing 10 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1992}, {"title": "Computing the Stereo Matching Cost with a Convolutional Neural Network", "author": ["J. \u017dbontar", "Y. LeCun"], "venue": "arXiv preprint arXiv:1409.4326 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A Large Dataset to Train Convolutional Networks for Disparity", "author": ["N. Mayer", "E. Ilg", "P. H\u00e4usser", "P. Fischer", "D. Cremers", "A. Dosovitskiy", "T. Brox"], "venue": "Optical Flow, and Scene Flow Estimation, Technical Report, arXiv preprint 1512.02134", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian Programming", "author": ["P. Bessi\u00e8re", "J.-M. Ahuactzin", "K. Mekhnacha", "E. Mazer"], "venue": "Chapman and Hall/CRC", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Spin torque nanodevices for bio-inspired computing", "author": ["N. Locatelli", "A. Mizrahi", "A. Accioly", "D. Querlioz", "J.-V. Kim", "V. Cros", "J. Grollier"], "venue": "in: 2014 14th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA), volume 1, IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Spintronic Devices as Key Elements for Energy-Efficient Neuroinspired Architectures", "author": ["N. Locatelli", "A.F. Vincent", "A. Mizrahi", "J.S. Friedman", "D. Vodenicarevic", "J.-V. Kim", "J.-O. Klein", "W. Zhao", "J. Grollier", "D. Querlioz"], "venue": "Proceedings of the 2015 Design, Automation & Test in Europe Conference & Exhibition 1 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Review of IEF\u2019s work - Modelling of superparamagnetic MTJs", "author": ["D. Querlioz"], "venue": "in: BAMBI-FET second year annual meeting, Paris", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Bayesian Sensor Fusion with Fast and Low Power Stochastic 23  Circuits", "author": ["A. Coninx", "R. Laurent", "M.A. Aslam", "P. Bessi\u00e8re", "J. Lobo", "E. Mazer", "J. Droulez"], "venue": "in: Proceedings of the first IEEE International Conference on Rebooting Computing (ICRC) [In press], IEEE Computer Society, San Diego, CA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Using Occupancy Grids for Mobile Robot Perception and Navigation", "author": ["A. Elfes"], "venue": "IEEE Computer 22 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning occupancy grid maps with forward sensor models", "author": ["S. Thrun"], "venue": "Autonomous Robots 15 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Using two cameras in a stereoscopic setup to reconstruct the tridimensional geometry of a visual scene, in a way similar to that performed by human stereopsis, is an important issue in computer vision, with major applications to autonomous robotics (and more specifically autonomous driving [1]).", "startOffset": 291, "endOffset": 294}, {"referenceID": 1, "context": "That issue has been an active research topic since at least 40 years, and a wide range of methods and algorithms have been proposed [2, 3] and evaluated on standardized benchmarks [4, 5].", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "That issue has been an active research topic since at least 40 years, and a wide range of methods and algorithms have been proposed [2, 3] and evaluated on standardized benchmarks [4, 5].", "startOffset": 132, "endOffset": 138}, {"referenceID": 3, "context": "That issue has been an active research topic since at least 40 years, and a wide range of methods and algorithms have been proposed [2, 3] and evaluated on standardized benchmarks [4, 5].", "startOffset": 180, "endOffset": 186}, {"referenceID": 4, "context": "That issue has been an active research topic since at least 40 years, and a wide range of methods and algorithms have been proposed [2, 3] and evaluated on standardized benchmarks [4, 5].", "startOffset": 180, "endOffset": 186}, {"referenceID": 5, "context": "Several works have shown that the binocular disparity computation can efficiently be formulated as a Bayesian inference problem [6, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 6, "context": "Several works have shown that the binocular disparity computation can efficiently be formulated as a Bayesian inference problem [6, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 7, "context": "Such probabilistic representations can also directly be used by Bayesian mapping and navigation methods such as the Bayesian occupation filter [8], and more generally by probabilistic and Bayesian robotics techniques [9, 10, 11].", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "Such probabilistic representations can also directly be used by Bayesian mapping and navigation methods such as the Bayesian occupation filter [8], and more generally by probabilistic and Bayesian robotics techniques [9, 10, 11].", "startOffset": 217, "endOffset": 228}, {"referenceID": 9, "context": "Such probabilistic representations can also directly be used by Bayesian mapping and navigation methods such as the Bayesian occupation filter [8], and more generally by probabilistic and Bayesian robotics techniques [9, 10, 11].", "startOffset": 217, "endOffset": 228}, {"referenceID": 10, "context": "Such probabilistic representations can also directly be used by Bayesian mapping and navigation methods such as the Bayesian occupation filter [8], and more generally by probabilistic and Bayesian robotics techniques [9, 10, 11].", "startOffset": 217, "endOffset": 228}, {"referenceID": 11, "context": "More specifically, the BAMBI project is a research effort to develop stochastic machines implementing Bayesian inference (Bayesian machines) [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 12, "context": "The general idea of stochastic computations with temporal coding can be traced back to the seminal works of Von Neumann [13] and Gaines [14] who highlighted the interest of such data representations, but their approaches were not widely pursued due to the rapid development of more efficient deterministic computers.", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "The general idea of stochastic computations with temporal coding can be traced back to the seminal works of Von Neumann [13] and Gaines [14] who highlighted the interest of such data representations, but their approaches were not widely pursued due to the rapid development of more efficient deterministic computers.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "The idea of developing hardware dedicated to bayesian reasoning has recently been pursued by several teams [15, 16, 17], exploring different computational paradigms to perform probabilistic inference.", "startOffset": 107, "endOffset": 119}, {"referenceID": 15, "context": "The idea of developing hardware dedicated to bayesian reasoning has recently been pursued by several teams [15, 16, 17], exploring different computational paradigms to perform probabilistic inference.", "startOffset": 107, "endOffset": 119}, {"referenceID": 16, "context": "The idea of developing hardware dedicated to bayesian reasoning has recently been pursued by several teams [15, 16, 17], exploring different computational paradigms to perform probabilistic inference.", "startOffset": 107, "endOffset": 119}, {"referenceID": 15, "context": "To address the problem of approximate inference Mansinghka [16] uses sampling methods for approximate inference and in a similar way, Jonas designed Markov Chain Monte Carlo based algorithms to provide a representation of probability distributions as sets of samplers [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "To address the problem of approximate inference Mansinghka [16] uses sampling methods for approximate inference and in a similar way, Jonas designed Markov Chain Monte Carlo based algorithms to provide a representation of probability distributions as sets of samplers [17].", "startOffset": 268, "endOffset": 272}, {"referenceID": 14, "context": "Vigoda [15] designed architectures based on probabilities represented by analog signals, and used the message passing algorithm to compute exact inference.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "More recently, a research project conducted at the Nanoscale Computing Fabrics Laboratory has led to the design of an unconventional hardware architecture based on electro-magnetic computations to perform inference on Bayesian Network models [18].", "startOffset": 242, "endOffset": 246}, {"referenceID": 18, "context": "[19] also showed that exact inference can be efficiently computed using GPU hardware for some high-dimensional problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] is quite similar to ours: they use stochastic bitstreams and target special inference problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In the framework of the BAMBI project, another stochastic architecture has been proposed to perform naive Bayesian fusion using Muller C-Elements [21], which achieves exact inference with normalization for binary random variables, but create harmful correlations in the stochastic signals and can\u2019t be easily extended to non-binary discrete distributions.", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "Other recent work conducted within the BAMBI project have proposed using digital signals with temporal coding to perform Bayesian inference, and a proof-of-concept to solve a simple sensorimotor problem has been put forward [22].", "startOffset": 224, "endOffset": 228}, {"referenceID": 1, "context": "been summarized in reviews [2, 3], which show that most methods follow the same general structure which can be divided in three steps:", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "been summarized in reviews [2, 3], which show that most methods follow the same general structure which can be divided in three steps:", "startOffset": 27, "endOffset": 33}, {"referenceID": 1, "context": "The most common matching cost is the squared difference of pixel values [2], but some other techniques preprocess the image with operators such as the gradient [23] or use banks of linear spatial filters [24].", "startOffset": 72, "endOffset": 75}, {"referenceID": 22, "context": "The most common matching cost is the squared difference of pixel values [2], but some other techniques preprocess the image with operators such as the gradient [23] or use banks of linear spatial filters [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "The most common matching cost is the squared difference of pixel values [2], but some other techniques preprocess the image with operators such as the gradient [23] or use banks of linear spatial filters [24].", "startOffset": 204, "endOffset": 208}, {"referenceID": 5, "context": "[6]), using techniques such as dynamic programming, in which cases it can complement or replace cost aggregation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Several of the existing works [6, 7] use the Bayesian inference framework to describe this process.", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "Several of the existing works [6, 7] use the Bayesian inference framework to describe this process.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "For example, Belhumeur [6] proposes to reconstruct the scene geometry S from the left and right images Il and Ir using a Bayesian model:", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "The development of public image pairs datasets provided with a disparity baseline such as the KITTI dataset [4] or the Middlebury dataset [5] have made it possible to treat disparity computation as a supervised machine learning problem.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "The development of public image pairs datasets provided with a disparity baseline such as the KITTI dataset [4] or the Middlebury dataset [5] have made it possible to treat disparity computation as a supervised machine learning problem.", "startOffset": 138, "endOffset": 141}, {"referenceID": 24, "context": "Some algorithms use deep convolutional networks to learn the matching cost [25, 26], and perform cost aggregation and optimization using other techniques.", "startOffset": 75, "endOffset": 83}, {"referenceID": 25, "context": "Some algorithms use deep convolutional networks to learn the matching cost [25, 26], and perform cost aggregation and optimization using other techniques.", "startOffset": 75, "endOffset": 83}, {"referenceID": 16, "context": "[17] as an application of his aforementioned hardware architecture for approximate inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "If the optimization is performed on the whole image or on entire rows or columns (as is the case in [6]) the problem becomes completely intractable, but even smaller integration neighborhoods are problematic.", "startOffset": 100, "endOffset": 103}, {"referenceID": 23, "context": "Other algorithms have used such convolution filters for disparity computation [24], although they process the feature information from those filter in a different way.", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "The relevance of using such linear spatial filters as a preprocessing step is also highlighted by recent works using deep neural networks to compute disparity [25, 26], which use a convolutional layer (with filters trained through supervised learning) as their input.", "startOffset": 159, "endOffset": 167}, {"referenceID": 25, "context": "The relevance of using such linear spatial filters as a preprocessing step is also highlighted by recent works using deep neural networks to compute disparity [25, 26], which use a convolutional layer (with filters trained through supervised learning) as their input.", "startOffset": 159, "endOffset": 167}, {"referenceID": 5, "context": "Those costs are used to compute probabilistic likelihood functions similar to those used by Belhumeur [6], and those likelihood terms are then combined using naive Bayesian fusion.", "startOffset": 102, "endOffset": 105}, {"referenceID": 21, "context": "Previous work [22] has shown that naive Bayesian fusion could be performed by stochastic machines.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "One of the most common Bayesian computing techniques is naive Bayesian fusion [27] : computing the posterior probability distribution on a searched variable S, knowing a prior distribution P (S) and the conditional distributions P (Ki|S) on some known variables K1, .", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "But recent advances in new nanodevices based on spintronics, such as the superparamagnetic tunnel junction (SMTJ) [28, 29], bear the promise that such generators could be available in the short or medium term.", "startOffset": 114, "endOffset": 122}, {"referenceID": 28, "context": "But recent advances in new nanodevices based on spintronics, such as the superparamagnetic tunnel junction (SMTJ) [28, 29], bear the promise that such generators could be available in the short or medium term.", "startOffset": 114, "endOffset": 122}, {"referenceID": 29, "context": "Those components can be built with CMOS technology using an area equivalent to 12 bytes of SRAM [30], making them suitable to large scale integration with the other components needed to build the stochastic machines described above.", "startOffset": 96, "endOffset": 100}, {"referenceID": 30, "context": "A partial implementation of our Bayesian Machine infrastructure using FPGA systems has been demonstrated [31], and further research will also integrate the technology developed by teams working on stochastic signal generator devices.", "startOffset": 105, "endOffset": 109}, {"referenceID": 31, "context": "Efforts will also be dedicated to extending the breadth of the computations implemented by stochastic bitstream based systems, combining the disparity computation to other sensory computations (such as optical flow) to create an occupancy map, using and extending existing Bayesian spatial cognition algorithms [32, 33, 8] which could then be used for obstacle avoidance and robot navigation, paving the way to a completely stochastic robot sensorimotor controller.", "startOffset": 311, "endOffset": 322}, {"referenceID": 32, "context": "Efforts will also be dedicated to extending the breadth of the computations implemented by stochastic bitstream based systems, combining the disparity computation to other sensory computations (such as optical flow) to create an occupancy map, using and extending existing Bayesian spatial cognition algorithms [32, 33, 8] which could then be used for obstacle avoidance and robot navigation, paving the way to a completely stochastic robot sensorimotor controller.", "startOffset": 311, "endOffset": 322}, {"referenceID": 7, "context": "Efforts will also be dedicated to extending the breadth of the computations implemented by stochastic bitstream based systems, combining the disparity computation to other sensory computations (such as optical flow) to create an occupancy map, using and extending existing Bayesian spatial cognition algorithms [32, 33, 8] which could then be used for obstacle avoidance and robot navigation, paving the way to a completely stochastic robot sensorimotor controller.", "startOffset": 311, "endOffset": 322}], "year": 2016, "abstractText": "Reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics, which can be formulated as a Bayesian inference problem. However, computation of the full disparity distribution with an advanced Bayesian model is usually an intractable problem, and proves computationally challenging even with a simple model. In this paper, we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency. We put forward a way to express discrete probability distributions using stochastic data representations and perform Bayesian fusion using those representations, and show how that approach can be applied to diparity computation. We evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics.", "creator": "LaTeX with hyperref package"}}}