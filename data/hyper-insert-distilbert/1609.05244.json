{"id": "1609.05244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis", "abstract": "multimodal trait sentiment relationship analysis is seen drawing an allegedly increasing impressive amount of attention these few days. it enables mining identification of opinions in video reviews and surveys which inevitably are now available somewhat aplenty on online entertainment platforms including like @ youtube. however, reflecting the limited number of surprisingly high - quality multimodal sentiment data samples may accidentally introduce the problem consequences of the sentiment being readily dependent on indicating the principal individual characteristic specific features in the dataset. this results seem in reflecting a lack standard of credible generalizability checks of the trained models made for classification modeling on larger online platforms. further in researching this paper, we carefully first mainly examine the conditioned data distribution and afterwards verify the existence mechanism of this dependence classification problem. then we propose a select - additive feedback learning ( rb sal ) analytic procedure that improves understanding the generalizability of trained discriminative structured neural networks. selecting sal is a symmetric two - phase factor learning encoding method. in successful selection strategy phase, alternately it selects correctly the trait confounding learned representation. ultimately in addition to phase, it forces the causal classifier participant to slightly discard confounded representations by perhaps adding another gaussian correlation noise. in our experiments, lastly we show how constructing sal continuously improves significantly the generalizability effectiveness of appropriate state - of - the - art models. later we actually increase predict prediction accuracy or significantly in all three modalities ( text, audio, video ), as trans well as assisting in their fusion. we show how identifying sal, even useful when trained data on one sampled dataset, achieves good data accuracy across test objective datasets.", "histories": [["v1", "Fri, 16 Sep 2016 21:33:42 GMT  (583kb,D)", "http://arxiv.org/abs/1609.05244v1", "13 pages"], ["v2", "Wed, 12 Apr 2017 21:38:40 GMT  (836kb,D)", "http://arxiv.org/abs/1609.05244v2", "Supplementary files at:this http URL"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["haohan wang", "aaksha meghawat", "louis-philippe morency", "eric p xing"], "accepted": false, "id": "1609.05244"}, "pdf": {"name": "1609.05244.pdf", "metadata": {"source": "CRF", "title": "Select-Additive Learning: Improving Cross-individual Generalization in Multimodal Sentiment Analysis", "authors": ["Haohan Wang", "Eric P. Xing"], "emails": ["epxing}@cs.cmu.edu"], "sections": [{"heading": null, "text": "Sentiment analysis is the automatic identification of the private state of a human mind with a focus on determining whether this state is positive, negative or neutral [16]. It has been extensively studied in the last few decades [18]. However, previous studies have been primarily based on textual data. With the recent proliferation of online avenues for expressing and sharing opinions, there is plenty of visual, textual and audio data of people expressing their opinions. This availability of data grants us the option of making use of all three modalities together. Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31]. Multimodal sentiment analysis has garnered considerable attention in both industry and academia. To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc.\nA typical data collection procedure is as follows: researchers identify videos of individuals reviewing products from large scale corpora like YouTube. They select highquality videos while maintaining the diversity of data w.r.t. ethnicity, gender, subject\nar X\niv :1\n60 9.\n05 24\n4v 1\n[ cs\n.C L\n] 1\n6 Se\nmatter etc. Then they split the videos into utterances (a contiguous block of speech between two pauses) to create an utterance-level multimodal sentiment analysis data set. In a typical YouTube product review, one usually consistently compliments or discredits the product. Therefore for each individual, the samples are largely positive or negative. An ideal data set collection procedure would be in a lab where individuals would be requested to contribute both positive and negative utterances.\nSentiment is correlated with individual style of expression. Different individuals can express the same sentiment very differently which is why sentiment can be dependent on identity. This dependence between sentiment and identity is statistically confirmed with \u03c72 independence test in one of our data sets [32]. The p-value obtained is 1.202\u00d710\u221219, which strongly suggests its existence. In statistics, identity in this setting is known as a confounding factor [8, 27]. A lot of effort has to be put into annotating these datasets which is why they are usually small in size. This further exacerbates the problem of lack of generalizability of the trained models.\nNeural network approaches have achieved state of the art performance in sentiment detection over many datasets. The core idea of these approaches is to learn a representation that better encodes the information content of the data for sentiment classification. In some models, these representations serve as input to traditional classifiers (Figure 1b). Confounding factors can reduce the quality of the representations and consequently the classification accuracy. This idea is further explained in Figure 1. Figure1(a) visualizes utterance level datasets. Most individuals are primarily positive or primarily neg-\native in contrast to an ideal balanced data set. Figure 1(b) shows what happens when a Convolutional Neural Network is trained on these data. To simplify the illustration, we separate a typical CNN into two parts: 1) bottom part (convolutional layers, sampling layers, MLP) learns a representation of data. 2) upper part (Logistic Regression) uses the representation to make prediction. Figure 1(c) shows a possible representation learned from CNN on this data. As we can see, some (blue ones) are generalizable representation that are related to sentiment, while others (red ones) are user identity specific (denoted as identity-related confounding dimensions in this paper). Figure 1(d) shows that the rules learned from Logistic Regression layer. As we can see, one rule (in red) is confoundingly learned.\nInspired by how traditional statistics correct confounding factors [14], we propose a Select-Additive Learning (SAL) procedure that builds on a special architecture as an extension to any discriminative neural network to improve performance. SAL is a two-phased (Selection Phase and Addition Phase) procedure. In Selection Phase, SAL identifies the latent representation of features learned by neural networks from confounding factors. In Addition Phase, SAL forces the original model to discard the latent representation of confounding features by adding Gaussian noises to these representations. Figure 1(e) illustrates the how CNN will be like after SAL is applied to improve generalizability. We perform extensive experiments to test the performance of the stateof-art model enhanced by SAL. Our baseline is the performance of the state-of-the-art in a real-world setting, i.e. we ensure that there is no common individual in the test data set and training/validation data set. The experiment results indicate that the SAL procedure works significantly better than the original model for each of the modalities separately as well as in the case of their fusion.\nIn the next section, we first introduce works related to multimodal sentiment analysis. Then we describe the SAL procedure which overcomes the issue of confounding factors, followed by a discussion of our experimental results and conclusions."}, {"heading": "1 Related Work", "text": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively. Recently, deep neural networks have also been used [25, 28].\nMany works focus on the detection of certain emotional states such as anger or sadness [1, 5] for the audio modality. Sentiment level classification is still performed primarily based on text features and audio signals are converted to text using speech to text methods [10, 11]. The facial expression coding system [7] laid the groundwork for analyzing emotions and sentiments in the visual modality. [24, 26]\nStarting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention. A variety of methods have been proposed and extensively discussed in recent years. [6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].\nThe novel contributions we make in this paper are the Select-Additive Learning (SAL) procedure that can improve the generalizability of neural networks and improve the prediction accuracy upon state-of-the-art on three different data sets for every\nmodality (audio, video, text), as well as for multimodal fusion, in the real-world setting. Now, we introduce our proposed Select-Additive Learning procedure."}, {"heading": "2 Select-Additive Learning", "text": "The main goal of our work is to increase the generalizability of models by encouraging the model to consider sentiment-associated features (i.e. individuals\u2019 facial expression) more than the identity-related features (i.e. individuals\u2019 appearances or voices). Refer to Figure 1(c), SAL aims to first identify the red dimensions (denoted as identity-related confounding dimensions, as previously mentioned) and then force the model to ignore them.\nWe use X to denote a matrix of size n \u00d7 p that encodes features, for n utterances with p features each, use Z to denote a matrix of size n\u00d7m that encodesm individuals\u2019 identities, and use y to denote a vector of size n\u00d7 1 representing annotated sentiments."}, {"heading": "2.1 Select-Additive Learning Architecture", "text": "To successfully select the identity-related confounding dimensions and remove them, SAL needs a special network architecture to support its algorithm. This architecture can be extended from any discriminative neural network (e.g. CNN) that can be split into a\nrepresentation learner component and a classification component. To simplify the notation, we use g(\u00b7; \u03b8) to denote representation learner component and \u03b8 stands for its parameters. Similarly, we use f(\u00b7;\u03c6) to denote the classification component and \u03c6 denotes the parameters.\nTo select identity-related confounding dimensions, SAL introduces a simple neural network (denoted as h(\u00b7; \u03b4), and \u03b4 stands for its parameters) to predict identity-related confounding dimensions from individual identities Z, so that by minimizing the different between h(Z; \u03b4) and g(X; \u03b8), h(Z; \u03b4) will effectively pinpoint the identity-related confounding dimensions in g(X; \u03b4).\nTo force the model to discard identity-related confounding dimensions, SAL introduces Gaussian noise to these dimensions while minimizing prediction error, so that f(\u00b7;\u03c6) learns to neglect noised representation. The noises are added through a Gaussian Sampling Layer, which is fully introduced in [3, 12].\nFigure 2 shows how SAL assembles g(\u00b7; \u03b8), f(\u00b7;\u03c6) and h(\u00b7; \u03b4) together via Gaussian Sampling Layer.\nIn Figure 2, the left architecture (purple one) stands for a deep learning model that is trained to make prediction y\u0302 based on input features X . Similarly to Figure 1(b), we split it into representation learner component g(X; \u03b8) and classification component y\u0302 = f(g(X; \u03b8);\u03c6). This purple model could be any discriminative classifier like CNN, MLP, LSTM. In our experiment, we use a state-of-the-art CNN as described in [20]. h(\u00b7; \u03b4) is attached to the classifier through a Gaussian Sampling Layer. The h(\u00b7; \u03b4) could also be any representation learning network architecture as its goal is to predict identityrelated confounding dimensions from individual identities. In our experiment, we use a one layer traditional neural network."}, {"heading": "2.2 Select-Additive Learning Algorithm", "text": "Select-Additive Learning (SAL) consists of two phases: 1) In Selection Phase: it forces h(\u00b7; \u03b4) to identify the identity-related confounding dimensions learned. 2) In Addition Phase: SAL adds Gaussian noises to these dimensions and forces the model to shift focus towards other dimensions that are associated with sentiment.\nBefore applying SAL, a model needs to be sufficiently trained to achieve:\nargmin \u03c6,\u03b8\n1 2 (y \u2212 f(g(X; \u03b8);\u03c6))2\nThis is the same as a standard deep learning training.\nSelection Phase This phase, as illustrated in Figure 3 (a), is used to tune \u03b4 by solving:\nargmin \u03b4\n1 2 (g(X; \u03b8)\u2212 h(Z; \u03b4))2 + \u03bb||\u03b4||1 (1)\nwhere \u03bb is a scalar that controls the weight of the sparsity regularizer. In this phase, both X and Z are available, but only \u03b4 is tuned.\nThe goal of this phase is to select identity-related confounding dimensions learned originally. To achieve this, we tune \u03b4 to minimize the difference between g(X; \u03b8) and\nh(Z; \u03b4). As Z only encodes identity information, the minimum of difference will be achieved when h(Z; \u03b4) is matched to the identity-related confounding dimensions of g(X; \u03b8). L1 regularization of \u03b4 is necessary to avoid overfitting as output dimension of h(\u00b7; \u03b4) is typically significantly higher than input dimension.\nThe result of this selection phase is shown on the right-hand-side of Figure 3(a). All the weights of original model (purple circle) are active and connected to every dimension while only some weights of h(\u00b7, \u03b4) (red circle) are active and connected to the identity-related confounding dimensions.\nAddition Phase Now h(\u00b7, \u03b4) can precisely select identity-related confounding dimensions. What remains is to force the model to ignore these dimensions. We achieve this by confusing the model with Gaussian noises. We aim to solve the following equation:\nargmin \u03c6\n1 2 (y \u2212 f(g(X; \u03b8) + h(Z; \u03b4) \u25e6 ;\u03c6))2 (2)\nwhere \u223c N(0, \u03c3I) and \u25e6 stands for element-wise product. This phase is illustrated in Figure 3(b). Only Parameter \u03c6 is tuned. The input representation to f(\u00b7;\u03c6) consists of representation learned from g(X; \u03b8) and the h(Z; \u03b4)selected identity-related confounding dimensions with Gaussian noises added. The noise ensures that identity-related confounding dimensions is no longer informative so that f(\u00b7;\u03c6) can be trained to ignore them.\nAs illustrated on the right side of Figure 3(b), identity-related confounding dimensions are contaminated with addition of noises. Therefore, the model learns to discard these non-informative dimensions, and its weights get optimized to focus on the rest dimensions, which are sentiment associated dimensions.\nOne may suggest that in Addition Phase, we can simply discard the weights connecting to identity-related confounding dimensions, rather than tuning the network with noises. Compared to this suggestion, this phase of our algorithm has a distinct advantage as it tunes \u03c6 continuously because the selection previous phase performed is continuous. This advantage will be further illustrated in Discussion Section.\nSAL introduces one more set of parameters to train (\u03b4) and two more hyper-parameters to select (\u03bb in Equation 1 and \u03c3 in Equation 2 via ). Strategies to select \u03bb and \u03c3 to tune \u03b4 will be discussed in the next section with the context of differently confounded data."}, {"heading": "3 Experiments", "text": "In this section, we perform experiments extensively on three different data sets to see whether SAL can help improve the generalizability of previous models. We use the state-of-art CNN as the baseline and improve upon it with SAL. We ensure that there is no common individual between the testing set and the training/validation sets. We also test the across-dataset generalizability by using two of these datasets exclusively for testing. Our results show that SAL can significantly improve upon the state-of-art."}, {"heading": "3.1 Models", "text": "We compare the following models :\n\u2013 CNN: We replicate the state-of-the-art CNN architecture described in [20] as the baseline. We use a similar architecture for video and audio modality. \u2013 SAL-CNN: After the state-of-the-art CNN is fully trained, we use SAL to increase its generalizability and predict sentiment."}, {"heading": "3.2 Data Set", "text": "We performed our experiment on three multimodal sentiment analysis data set:\n\u2013 MOSI: This dataset consists of 93 videos obtained from YouTube channels. The dataset has 2199 utterances which were created manually from the videos for single topic expressions of opinion. [32] \u2013 YouTube: This dataset consists of 47 opinion videos (280 utterances). [16] \u2013 MOUD: This dataset consists of 498 Spanish opinion utterances across 55 individ-\nuals. [23]"}, {"heading": "3.3 Feature Design", "text": "Text Features We extracted an embedding for each word in the text sentence of the utterance using a word2vec dictionary pre-trained on a Google News corpus [15]. The text input of each utterance was formed by concatenating the word embeddings for all the words in the sentence and padding them with the appropriate zeros to have the same dimension. We set the maximum length as 60 and discard additional words (only around 0.5% utterances in our datasets have more than 60 words). For YouTube dataset, we extracted the transcripts using the IBM Bluemix\u2019s speech2text API1. For MOUD dataset, we translated Spanish transcripts into English transcripts.\n1 https://www.ibm.com/watson/developercloud/speech-to-text.html\nAudio Features We used the openSMILE [9] to extract the low-level audio descriptors of the utterances. These audio descriptors included the Mel-frequency cepstral coefficients, pitch, voice quality etc (39 features). We split each utterance into 50 trunks and took the average of features within each trunk and resulted in a set of 1950 dimensional vectors for each utterance.\nVisual Features Each utterance video was split into still frames. We used the CLMZ library [2] for extracting facial characteristic points (415 features). We split each utterance into 5 trunks and took the average of features within each trunk and resulted in a set of 2075 dimensional vectors for each utterance."}, {"heading": "3.4 Experiment Setup", "text": "We simulated a real-world setting by applying the constraint that training and validation set had no individuals in common with the test set. We generate our training set and testing set as following:\n\u2013 Training/Validation Set: The first 62 individuals in the MOSI data set are selected as training/validation set. There are around 1250 utterances in total. These utterances are shuffled and then 80% are used for training and 20% used for validation. \u2013 Testing Set: We have three testing data set. ? MOSI: 775 utterances from the remaining 31 individuals in MOSI data set. ? YouTube: 195 utterances of 47 individuals from the YouTube data set, after\nremoving neutral utterance. ? MOUD: 450 utterances of 55 individuals from MOUD data set, after removing\nneutral utterance.\nWe first sufficiently trained the CNN model and saved the one with the maximum validation accuracy. Then we tested it on our three test datasets. Then we used SAL to improve the performance of the pre-trained CNN and saved the SAL-CNN with the minimum validation accuracy. We tested this saved SAL-CNN on the three test datasets. We trained classifiers for three different modalities, and then multimodal classifier is achieved by integrating the classifiers of single modality together."}, {"heading": "3.5 Experiment Results", "text": "The results are shown in Table 1. First, it is noteworthy that in some cases the performance of the CNN is worse than mere chance. This inferior performance substantiates the existence of the problems we are targeting because these models are selected as the ones that achieve minimum error rate in validation sets and they can barely perform well when tested across data sets. The results also indicate that SAL could help to increase the generalizability of the trained model. On all these three data sets, SAL corrects for confounding factors and raises the test accuracies significantly higher than previous state-of-the-art CNN performance.\nAll the three datasets originate from the same web platform but they differ in recording quality and the processing done after curation. These differences show up in the\naccuracies for the YouTube and MOUD datasets which are much lower than those of MOSI. Text features again are obtained from different ASR tools for all the three datasets. Moreover, the text features in the MOUD dataset need one extra step of translation from Spanish to English. These differences are also reflected in accuracies for the text modality. Despite these obstacles and differences, Select-Additive Learning increases the robustness and performance of the previous models consistently (except only two cases: Video modality in MOUD and fusion of audio & video in MOSI).\nParameter Tuning and Hyperparameter Selection Strategy For brevity, we focus the discussion on the parameters that SAL introduces. A successful training of \u03b4 depends highly on a successful selection of \u03bb. Fortunately, selection of \u03bb can be guided by a prior understanding of the data. Stronger confounding factors indicate smaller \u03bb. Extra inspections are recommended to check that \u03bb is neither too small (\u03b4 is not sparse) nor too large (\u03b4 will be all zeros). Selection of \u03c3 can be done in the same way. The Larger confounding effect should lead to larger \u03c3.\nAs SAL is designed to improve the generalizability of models that are suffering from confounding factors, we notice that for the modality that is heavily confounded (video modality), almost no extra effort of hyperparameter selection is required. SAL can easily increase the performance of CNN with a variety of choices of \u03c3. However, for the modality that is less confounded (text modality), extra effort is needed.\nWe report the parameters we use in the Table 2 as an example to guide hyperparameter selection. lr\u00b7 stands for learning rate. \u03bb\u03c6 is weight of L1 sparsity regularizer of \u03c6."}, {"heading": "4 Discussion", "text": "Figure 4 shows a plot of h(Z, \u03b4) during the Selection phase. It is a zoomed in figure for the first 50 utterances (rows) and first 100 values of the representation vector (columns). Blue indicates lowest values and red indicates highest values and other colors are linearly interpolated in between.\nThe representation of utterances forms clear clusters and each cluster belongs to one individual. This figure suggests that confounding representation might be different across individuals, which fosters the argument in favor of Addition phase of SelectAdditive Learning as opposed to dropping weights.\nDespite each individual having their own pattern, there are dimensions that have generalized well across individuals, e.g. the blue vertical line at about index 100 or\ngreen vertical line at about index 20. Our model will learn to assign more weights to these dimensions after noise is introduced."}, {"heading": "5 Conclusion", "text": "In this paper, we first presented the existence of a problem for multimodal sentiment analysis. The sentiment is not independent of individuals and a model could be confounded by individual-sensitive features such as appearance. Therefore, a model trained on a group of individuals does not generalize well to other individuals.\nAfter verifying the existence of the problem, we proposed a Select-Additive Learning procedure to solve it. SAL is a two-phase learning method. In Selection phase, it selects the identity-related confounding dimensions. In Addition phase, it forces the classifier to discard these dimensions by adding Gaussian noise. In our experiments, we showed how SAL improves the generalizability of state-of-the-art models. We increased prediction accuracy significantly in all three modalities (text, audio, video), as well as in their fusion. We also showed that SAL could achieve good prediction accuracy even when tested across data sets."}], "references": [{"title": "A speaker independent approach to the classification of emotional vocal expressions", "author": ["H. Atassi", "A. Esposito"], "venue": "2008 20th IEEE International Conference on Tools with Artificial Intelligence. vol. 2, pp. 147\u2013152. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "3d constrained local model for rigid and nonrigid facial tracking", "author": ["T. Baltru\u0161aitis", "P. Robinson", "L.P. Morency"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 2610\u20132617. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1509.00519", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Senticnet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis", "author": ["E. Cambria", "D. Olsher", "D. Rajagopal"], "venue": "Proceedings of the twenty-eighth AAAI conference on artificial intelligence. pp. 1515\u20131521. AAAI Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal emotion recognition from expressive faces, body gestures and speech", "author": ["G. Caridakis", "G. Castellano", "L. Kessous", "A. Raouzaiou", "L. Malatesta", "S. Asteriadis", "K. Karpouzis"], "venue": "IFIP International Conference on Artificial Intelligence Applications and Innovations. pp. 375\u2013388. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "magic mirror in my hand, what is the sentiment in the lens?: An action unit based approach for mining sentiments from multimedia contents", "author": ["L. Casaburi", "F. Colace", "M. De Santo", "L. Greco"], "venue": "Journal of Visual Languages & Computing 27, 19\u201328", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Facial action coding system", "author": ["P. Ekman", "W.V. Friesen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Confounding factors in the detection of species responses to habitat fragmentation", "author": ["R.M. Ewers", "R.K. Didham"], "venue": "Biological Reviews 81(01), 117\u2013142", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Opensmile: the munich versatile and fast open-source audio feature extractor", "author": ["F. Eyben", "M. W\u00f6llmer", "B. Schuller"], "venue": "Proceedings of the 18th ACM international conference on Multimedia. pp. 1459\u20131462. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentiment analysis of call centre audio conversations using text classification", "author": ["S. Ezzat", "N. El Gayar", "M. Ghanem"], "venue": "Int. J. Comput. Inf. Syst. Ind. Manag. Appl 4(1), 619\u2013627", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sentiment extraction from natural audio streams", "author": ["L. Kaushik", "A. Sangwan", "J.H. Hansen"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 8485\u20138489. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Sentiment analysis: A perspective on its past, present and future", "author": ["A. Kumar", "M.S. Teeja"], "venue": "International Journal of Intelligent Systems and Applications 4(10), 1", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalized linear mixed models", "author": ["C.E. McCulloch", "J.M. Neuhaus"], "venue": "Wiley Online Library", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["L.P. Morency", "R. Mihalcea", "P. Doshi"], "venue": "Proceedings of the 13th international conference on multimodal interfaces. pp. 169\u2013176. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics. p. 271. Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and trends in information retrieval 2(1-2), 1\u2013135", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Utterance-level multimodal sentiment analysis", "author": ["V. P\u00e9rez-Rosas", "R. Mihalcea", "L.P. Morency"], "venue": "ACL (1). pp. 973\u2013982", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis", "author": ["S. Poria", "E. Cambria", "A. Gelbukh"], "venue": "Proceedings of EMNLP. pp. 2539\u20132544", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content", "author": ["S. Poria", "E. Cambria", "N. Howard", "G.B. Huang", "A. Hussain"], "venue": "Neurocomputing 174, 50\u201359", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning extraction patterns for subjective expressions", "author": ["E. Riloff", "J. Wiebe"], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing. pp. 105\u2013112. Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Multimodal sentiment analysis of spanish online videos", "author": ["V.P. Rosas", "R. Mihalcea", "L.P. Morency"], "venue": "IEEE Intelligent Systems (3), 38\u201345", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On the relevance of sequence information for decoding facial expressions of pain and disgust?: An avatar study", "author": ["M. Siebers", "T. Engelbrecht", "U. Schmid"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the conference on empirical methods in natural language processing (EMNLP). vol. 1631, p. 1642. Citeseer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Expression analysis/synthesis system based on emotion space constructed by multilayered neural network", "author": ["N. Ueki", "S. Morishima", "H. Yamada", "H. Harashima"], "venue": "Systems and Computers in Japan 25(13), 95\u2013107", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "On the definition of a confounder", "author": ["T.J. VanderWeele", "I. Shpitser"], "venue": "Annals of statistics 41(1), 196", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Dimensional sentiment analysis using a regional cnn-lstm model", "author": ["J. Wang", "L.C. Yu", "K.R. Lai", "X. Zhang"], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. p. 225", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning subjective adjectives from corpora", "author": ["J. Wiebe"], "venue": "AAAI/IAAI. pp. 735\u2013740", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing. pp. 347\u2013354. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Youtube movie reviews: Sentiment analysis in an audio-visual context", "author": ["M. Wollmer", "F. Weninger", "T. Knaup", "B. Schuller", "C. Sun", "K. Sagae", "L.P. Morency"], "venue": "Intelligent Systems, IEEE 28(3), 46\u201353", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Micro-opinion sentiment intensity analysis and summarization in online videos", "author": ["A. Zadeh"], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. pp. 587\u2013591. ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Sentiment analysis is the automatic identification of the private state of a human mind with a focus on determining whether this state is positive, negative or neutral [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "It has been extensively studied in the last few decades [18].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31].", "startOffset": 158, "endOffset": 165}, {"referenceID": 30, "context": "Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31].", "startOffset": 158, "endOffset": 165}, {"referenceID": 22, "context": "To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc.", "startOffset": 123, "endOffset": 127}, {"referenceID": 31, "context": "This dependence between sentiment and identity is statistically confirmed with \u03c7 independence test in one of our data sets [32].", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "In statistics, identity in this setting is known as a confounding factor [8, 27].", "startOffset": 73, "endOffset": 80}, {"referenceID": 26, "context": "In statistics, identity in this setting is known as a confounding factor [8, 27].", "startOffset": 73, "endOffset": 80}, {"referenceID": 13, "context": "Inspired by how traditional statistics correct confounding factors [14], we propose a Select-Additive Learning (SAL) procedure that builds on a special architecture as an extension to any discriminative neural network to improve performance.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 81, "endOffset": 88}, {"referenceID": 28, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 81, "endOffset": 88}, {"referenceID": 29, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "Recently, deep neural networks have also been used [25, 28].", "startOffset": 51, "endOffset": 59}, {"referenceID": 27, "context": "Recently, deep neural networks have also been used [25, 28].", "startOffset": 51, "endOffset": 59}, {"referenceID": 0, "context": "Many works focus on the detection of certain emotional states such as anger or sadness [1, 5] for the audio modality.", "startOffset": 87, "endOffset": 93}, {"referenceID": 4, "context": "Many works focus on the detection of certain emotional states such as anger or sadness [1, 5] for the audio modality.", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "Sentiment level classification is still performed primarily based on text features and audio signals are converted to text using speech to text methods [10, 11].", "startOffset": 152, "endOffset": 160}, {"referenceID": 10, "context": "Sentiment level classification is still performed primarily based on text features and audio signals are converted to text using speech to text methods [10, 11].", "startOffset": 152, "endOffset": 160}, {"referenceID": 6, "context": "The facial expression coding system [7] laid the groundwork for analyzing emotions and sentiments in the visual modality.", "startOffset": 36, "endOffset": 39}, {"referenceID": 23, "context": "[24, 26] Starting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[24, 26] Starting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[24, 26] Starting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 18, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 20, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The noises are added through a Gaussian Sampling Layer, which is fully introduced in [3, 12].", "startOffset": 85, "endOffset": 92}, {"referenceID": 11, "context": "The noises are added through a Gaussian Sampling Layer, which is fully introduced in [3, 12].", "startOffset": 85, "endOffset": 92}, {"referenceID": 19, "context": "In our experiment, we use a state-of-the-art CNN as described in [20].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "\u2013 CNN: We replicate the state-of-the-art CNN architecture described in [20] as the baseline.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "[32] \u2013 YouTube: This dataset consists of 47 opinion videos (280 utterances).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] \u2013 MOUD: This dataset consists of 498 Spanish opinion utterances across 55 individuals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23]", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Text Features We extracted an embedding for each word in the text sentence of the utterance using a word2vec dictionary pre-trained on a Google News corpus [15].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "Audio Features We used the openSMILE [9] to extract the low-level audio descriptors of the utterances.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "We used the CLMZ library [2] for extracting facial characteristic points (415 features).", "startOffset": 25, "endOffset": 28}], "year": 2016, "abstractText": "Multimodal sentiment analysis is drawing an increasing amount of attention these days. It enables mining of opinions in video reviews and surveys which are now available aplenty on online platforms like YouTube. However, the limited number of high-quality multimodal sentiment data samples may introduce the problem of the sentiment being dependent on the individual specific features in the dataset. This results in a lack of generalizability of the trained models for classification on larger online platforms. In this paper, we first examine the data and verify the existence of this dependence problem. Then we propose a SelectAdditive Learning (SAL) procedure that improves the generalizability of trained discriminative neural networks. SAL is a two-phase learning method. In Selection phase, it selects the confounding learned representation. In Addition phase, it forces the classifier to discard confounded representations by adding Gaussian noise. In our experiments, we show how SAL improves the generalizability of state-of-the-art models. We increase prediction accuracy significantly in all three modalities (text, audio, video), as well as in their fusion. We show how SAL, even when trained on one dataset, achieves good accuracy across test datasets. Sentiment analysis is the automatic identification of the private state of a human mind with a focus on determining whether this state is positive, negative or neutral [16]. It has been extensively studied in the last few decades [18]. However, previous studies have been primarily based on textual data. With the recent proliferation of online avenues for expressing and sharing opinions, there is plenty of visual, textual and audio data of people expressing their opinions. This availability of data grants us the option of making use of all three modalities together. Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31]. Multimodal sentiment analysis has garnered considerable attention in both industry and academia. To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc. A typical data collection procedure is as follows: researchers identify videos of individuals reviewing products from large scale corpora like YouTube. They select highquality videos while maintaining the diversity of data w.r.t. ethnicity, gender, subject ar X iv :1 60 9. 05 24 4v 1 [ cs .C L ] 1 6 Se p 20 16", "creator": "LaTeX with hyperref package"}}}