{"id": "1609.05876", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "On the Phase Transition of Finding a Biclique in a larger Bipartite Graph", "abstract": "nowadays we already report substantially on the phase transition of finding a complete subgraph, of no specified particular dimensions, nowhere in a bipartite graph. nevertheless finding a sufficient complete subgraph in a perfect bipartite graph is usually a problem issue that yet has acquired growing cognitive attention particularly in fundamentally several domains, well including bioinformatics, algebraic social network analysis computing and object domain clustering. a commonly key general step for running a successful comprehensive phase cross transition mode study independently is identifying each a randomly suitable order parameter, achieved when initially none is absolutely known. to understand this scientific purpose, lastly we have applied a decision tree type classifier to real - world binary instances of this problem, in order to understand what problem features of separate an input instance that appears is hard to solve infinitely from those that is readily not. we have rather successfully adequately identified one certain such order parameter and run with it on the phase transition of finding a complete bipartite downhill subgraph distribution of specified dimensions. designing our phase transition cycle study accordingly shows merely an additional easy - to - hard - np to - easy - to - hard - to - easy pattern. further, because our results indicate that that the correct hardest instance instances are in a specialized region containing where it probability is more likely that the corresponding bipartite graph will have a discrete complete polynomial subgraph covering of specified dimensions, a positive answer. maybe by far contrast, instances arising with a negative answer are more clearly likely to appear in beyond a region where the computational cost is frequently negligible. this pattern behaviour is remarkably similarly similar for problems of processing a number of greatly different sizes.", "histories": [["v1", "Mon, 19 Sep 2016 19:22:08 GMT  (291kb)", "http://arxiv.org/abs/1609.05876v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CC", "authors": ["roberto alonso", "ra\\'ul monroy", "eduardo aguirre"], "accepted": false, "id": "1609.05876"}, "pdf": {"name": "1609.05876.pdf", "metadata": {"source": "CRF", "title": "On the Phase Transition of Finding a Biclique in a larger Bipartite Graph", "authors": ["Roberto Alonso", "Ra\u00fal Monroy", "Eduardo Aguirre"], "emails": ["roberto.alonso@itesm.mx,", "raulm@itesm.mx,", "eduardo.aguirre@itesm.mx"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n05 87\n6v 1\n[ cs\n.A I]\n1 9\nWe report on the phase transition of finding a complete subgraph, of specified dimensions, in a bipartite graph. Finding a complete subgraph in a bipartite graph is a problem that has growing attention in several domains, including bioinformatics, social network analysis and domain clustering. A key step for a successful phase transition study is identifying a suitable order parameter, when none is known. To this purpose, we have applied a decision tree classifier to real-world instances of this problem, in order to understand what problem features separate an instance that is hard to solve from those that is not. We have successfully identified one such order parameter and with it the phase transition of finding a complete bipartite subgraph of specified dimensions. Our phase transition study shows an easy-to-hard-to-easy-to-hard-toeasy pattern. Further, our results indicate that the hardest instances are in a region where it is more likely that the corresponding bipartite graph will have a complete subgraph of specified dimensions, a positive answer. By contrast, instances with a negative answer are more likely to appear in a region where the computational cost is negligible. This behaviour is remarkably similar for problems of a number of different sizes.\nKeywords: Phase transition, bipartite graphs, bicliques, induced subgraphs, complete graphs\nEmail address: roberto.alonso@itesm.mx, raulm@itesm.mx,\neduardo.aguirre@itesm.mx (Roberto Alonso, Rau\u0301l Monroy and Eduardo Aguirre)\nPreprint submitted to Elsevier September 20, 2016"}, {"heading": "1. Introduction", "text": "In 1991, Cheeseman et al. [1] showed that, for any NP-complete problem, there exists a phase transition that separates easy instances from hard ones, and that this phase transition can be found as one varies an order parameter 1 around one or more critical values. Since then phase transition studies have been conducted for a number of NP-complete problems, see e.g. [3, 4, 5, 6]. This is because phase transition helps identifying key instances of a problem so as to build a benchmark set, with which one can perform a statistically significant comparison amongst different methods that attempt to solve that problem. Furthermore, the phase transition value can also be used to determine when a complete method is likely to succeed (or not) in finding a solution to a problem instance in reasonable time.\nThis paper studies the phase transition of finding a complete bipartite subgraph, with specified dimensions, in a bipartite graph. A bipartite graph is a graph with two distinguished, disjoint sets of vertices, U and V , such that edges connect elements in U to elements in V . A complete bipartite subgraph is a bipartite graph where every element of U \u2032 \u2286 U is connected with every element of V \u2032 \u2286 V . Henceforth, we will use biclique to refer to a complete bipartite subgraph. Interest in finding bicliques inside a larger bipartite graph has started to gain growing attention in bioinformatics (see e.g. [7, 8, 9]) where researchers have proposed several algorithms to compute bicliques. Particularly, the work of Zhang et al. [7] has recently reported an improvement, with respect to other methods, in enumerating all the bicliques in a graph in a real-world dataset. However, finding bicliques arise naturally in other contexts, e.g. IoT, social network analysis, document clustering, and privacy, amongst others.\nOur motivation to study this problem originates in detecting anomalies on Domain Name System (DNS) traffic. DNS activity when observed over a time window, can be represented as a bipartite graph, where U is a set of IP addresses, and V is the set of URLs that these IPs have queried for. DNS traffic forms bicliques, since people tend to visit common websites. Then, as shown by Alonso [10], the number and structure of such bicliques are severely broken apart upon an abnormal event, e.g. a denial of service attack.\n1Following standard convention, we use \u201corder parameter\u201d to refer to a parameter that controls the complexity of finding a solution to a given problem instance [1], instead of using the more appropriate term control parameter [2].\nFor the problem of finding a biclique of specified dimensions, our phase transition study shows an easy-to-hard-to-easy-to-hard-to-easy pattern. A critical value occurs when the ratio of the maximal biclique (see Section 2.3 below) to the cardinality of the set V is roughly 3/8. There, any problem instance would be computationally expensive but with a 98% probability of being solvable (see Section 3). By contrast, when this ratio tends to either zero or one, dealing with a problem instance is negligible, being insoluble in the former case, and solvable in the latter one.\nPaper overview. The rest of the paper is structured as follows. We first introduce general knowledge and formulate the problem of finding a biclique, Section 2. Next, we outline how to conduct a phase transition study, Section 3, and then introduce our experimental methodology for determining a suitable order parameter, Section 4. Then, we present an algorithm to compute a biclique, which applies backtracking and a black list in order to eagerly discard vertices that cannot form part of large biclique, Section 5. Finally, in Section 6, we report on the phase transition of this problem, and, in Section 7, on the conclusions drawn from our investigations."}, {"heading": "2. Bicliques", "text": "First, we shall introduce the symbols used through this document by defining the problem of finding a biclique in a larger bipartite graph. Second, we shall present the decision version of the problem under study, namely finding a biclique with specified dimensions in a larger bipartite graph. Third, we will present a way to compare the bicliques. Lastly, we introduce the gram matrix which allows to efficiently determine whether a maximal biclique exists."}, {"heading": "2.1. Bipartite Graphs and Complexity of Finding a Biclique", "text": "More formally, let G = (U, V, E) be a bipartite graph, with disjoint sets of vertices, U and V , and such that for every edge (u, v) \u2208 E, we have that u \u2208 U and v \u2208 V . A biclique in G is a subset of the vertex set, we denote gG(U \u2032, V \u2032), such that U \u2032 \u2286 U , V \u2032 \u2286 V , and such that for every u \u2208 U \u2032, v \u2208 V \u2032 the edge (u, v) \u2208 E. The complexity of finding a biclique was initially discussed in the work of Yannakakis et al. in [11]. There, authors have proven that finding a biclique with the restriction that |U \u2032| = |V \u2032| is NP-complete. Later, works\nlike [12, 13] have proven that the NP-completeness of finding a biclique also holds for other restrictions, such as specifying a maximum number of edges, a maximum number of vertices, or specifying a maximum edge weight. Particularly, the version of the problem for which we demand |U \u2032| = |V \u2032| is called balanced biclique [11], and that where |U \u2032| = t and |V \u2032| = z, for given t and z, is called exact node cardinality decision [12]. Recently, Alonso and Monroy [14] have proven that finding a biclique such that |U \u2032| \u2265 t and |V \u2032| \u2265 z vertices, for given t and z, is also NP-complete. They have also shown that even if we try to prove that every element in U is in a biclique with at least two elements of V remains in the class NP. Authors called these problems Social Group Commonality (SGC) and 2-SGC, respectively."}, {"heading": "2.2. Finding a Biclique with Specified Dimensions t and z: a Decision Problem", "text": "Now, the decision problem of finding a biclique with specified dimensions is defined as follows:\nINSTANCE: A bipartite graph G = (U, V, E), two positive integers, t and z.\nQUESTION: Is there a biclique in G, gG(U \u2032, V \u2032), with |U \u2032| \u2265 t and |V \u2032| \u2265 z?\nGiven that a graph may contain several bicliques, in what follows we present a way to compare them. This formulation will be used later in the phase transition study."}, {"heading": "2.3. Size-/Weight-Maximal Biclique", "text": "Let G = (U, V, E) be a bipartite graph and let gG(U \u2032, V \u2032), with U \u2032 \u2286 U and V \u2032 \u2286 V , be a biclique in G. Then, we call |U \u2032| and |V \u2032| the weight and the size of gG(U\n\u2032, V \u2032), respectively. Further, let GG denote all the possible bicliques in G. Then, a biclique gG(U\n\u2032, V \u2032) \u2208 GG is called weight-maximal (respectively, size-maximal) if there is not a gG(U\n\u2032\u2032, V \u2032\u2032) \u2208 GG such that |U \u2032| < |U \u2032\u2032| (respectively, |V \u2032| < |V \u2032\u2032|).\nA bipartite graph G can be succinctly represented by means of an adjacency matrix Q. From the gram matrix of Q it is possible to get insights about the graph, as we will show below."}, {"heading": "2.4. The Adjacency Matrix and the Gram Matrix", "text": "Let G = (U, V, E) be a bipartite graph. We use I, J, . . . stand for indexing sets, and write UI = {ui : u \u2208 U, i \u2208 I} to denote the nodes in U , indexed by I; likewise, VJ = {vj : v \u2208 V, j \u2208 J} denotes the nodes in V , indexed by J . Then, an adjacency matrix, Q is such that Q(i,j) = 1 implies that there is an edge (ui, vj) \u2208 E.\nNow, we can compute the gram matrix denoted gram(Q) and given by Q \u00d7 Q\u22ba. In particular, gram(Q) is symmetric, and such that the lower (respectively, upper) triangular matrix contains information about all the distinct bicliques with weight two, including the one that is size-maximal. Notice that gram(Q)(k,l) = n, k 6= l, implies that the graph contains a biclique with weight two and size n, involving the participation of vertices uk and ul. The main diagonal of this matrix enables us to determine the number of adjacent vertices of uk, since gram(Q)(k,k) = n implies that vertex uk has n adjacent vertices in the graph.\nComplementarily, gram(Q\u22ba), given by Q\u22ba \u00d7 Q, provides valuable information about the bicliques in the graph. In particular, the lower (respectively, the upper) triangular matrix of this matrix contains all the distinct bicliques with size two, including the one that is weight-maximal. Here, gram(Q\u22ba)(k,l) = n implies that there is a biclique with size two and weight n, involving the use of vk and vl vertices. gram(Q\n\u22ba)(l,l) = n implies that vertex vl has n adjacent vertices.\nRemark: We should point out that the gram matrix of Q and Q\u22ba provide a proof of the existence of a size-maximal (respectively, weight-maximal) biclique with weight two (respectively, size two), while determining the existence of a maximal biclique with weight (respectively, size) greater than two remains NP-complete."}, {"heading": "3. Standard Methodology for a Phase Transition Study", "text": "Phase transition is a means of selecting problem instances that are typically hard, and hence provide a fair basis for comparison of different algorithms. A phase transition, separating easy instances from hard ones, appears as one plots the expense of finding a solution to a problem instance against an order parameter. Interestingly, it often coincides to that area where the problem, stated as a decision problem, changes from having a YES-solution (solvable) to one having not (insoluble). The term is used in an analogy to\nthe Physics phenomena: after a phase transition, a material dramatically changes its properties, e.g. from liquid to solid.\nSome problems have been found to show an easy-to-hard-to-easy complexity pattern (e.g., travelling salesman [1, 15]): the cost of finding a solution increases at first, but then decreases later on to small values back again. Others (e.g. constraint satisfaction [16]) have been found to have similar cost, regardless of the size of the instance, as long as the instance is scaled. Scaling has several implications; it can be used to construct an instance with a given probability, or a set of instances with similar cost, and this can be done for any problem size.\nConducting a phase transition study is a four-step approach: 1. Select an order parameter that succinctly captures the problem struc-\nture. This task may not be trivial. As pointed out by [1], using a different order parameter yields a different phase transition. While several NP-complete problems exhibit a natural order parameter, others require experimental evidence; e.g. Gent and Walsh [17] used an annealed theory to determine an order parameter for the phase transition of number partitioning. In this work, we have applied a decision tree classifier to real-world instances of the problem dealt with on this paper, in order to understand what problem features separate an instance that is hard to solve from those that is not. We have successfully identified one such order parameter (see Section 4) and with it the phase transition of finding a biclique of specified dimensions. 2. Collect a number of problem instances. This can be done either by randomly generating problem instances using the selected order parameter, or by collecting them from a real-world process, if any. For our study, we have collected over 100 thousand bipartite graphs from a real-world process. 3. Select an algorithm that solves the problem, and then apply it on each instance of the set built from the second step; for each try, gather both computational expense and whether it is solvable or not. In our study, we have designed and applied an algorithm for this task (see Section 5). Notice that, alternatively, in this step we could have used an efficient algorithm, e.g. a SAT solver; but then we would have to come out with a mapping from the problem of finding a biclique to SAT, which is beyond the scope of this paper. 4. Plot both the computational cost, and the probability of an instance being solvable against the order parameter. In our case, this probabil-\nity is given by the number of instances that were solvable divided by the total number of instances considered, for each value of the order parameter. Notice that accomplishing this step depends on how step 2 is carried out. Had data been synthesized, the generation function would have to tag each instance with the probability of it being solvable."}, {"heading": "4. On Identifying an Order Parameter for the Problem of Finding a Biclique", "text": "In order to identify an order parameter, we have characterized real-world instances for which it is possible to find a biclique with little effort, and those that cannot. To that purpose, we have applied C4.5, which builds a decision tree from a training set containing already classified graph samples. This tree can be separated into decision rules, which explain what makes an instance to be one class or the other. In what follows, we first describe our working dataset, and then how C4.5 was applied to it to discover an order parameter."}, {"heading": "4.1. Dataset Construction", "text": "Our dataset, including both training, test and validation, has been built out of real-world activity, namely a Domain Name System (DNS) resolution process. Roughly, a DNS resolution process is about an agent querying for a domain so as to translate them into an IP address. Then, by observing w DNS processes, we have constructed a graph G = (U, V, E), where the set U denotes IP addresses, the set V denotes URLs and (u, v) \u2208 E represents the action of agent u over domain v, namely the process of translation. From this bipartite graph it is possible to observe bicliques (since we tend to visit the same websites), as shown in [10].\nThis kind of real-world process usually involves a large and dynamic graph that follows a behaviour similar to a free scale network, so in our case we can end up with a graph comprising in average 600K vertices per day. Because decision trees on large data sets can be time consuming and hard to store in memory, we have decided to construct more manageable graphs in the following way:\nFirst we arbitrarily picked five days of DNS traffic. Then, we randomly sampled w DNS processes, and constructed the adjacency matrix Q. We should point out that given the nature of the real-world process, repetition may occur (e.g. an IP may ask for the same domain more than once) however this does not get in the way of constructing the graph or computing the\nbiclique. We constructed graphs so as to attain a set amounting 40% of each picked day. We repeated this procedure for w from 50, to 150 in steps of 25. After this, step we end up with a collection for each of the selected w.\nFor each graph in the collection, we proceeded as follows. First, we preemptively applied a brute-force algorithm in order to find a size-maximal biclique. If it could be solved in less than 20 seconds, we labelled it EASY ; otherwise, we gave up solving it and labelled it HARD. Then, we inserted in the final dataset a tuple containing a feature vector (described below) representing the graph, and the associated label. We finally split this dataset, forming the training set (comprising 70% of the data) and the validation set (comprising the remaining 30%)."}, {"heading": "4.2. Features Used to Characterize a Graph", "text": "We now show the feature vector representing a graph. We insist that in the selection of all these features, we were driven by determining an order parameter, and that they all capture the likelyhood of an instance being HARD. These features are:\n\u2022 |U |: the cardinality of U . \u2022 |V |: the cardinality of V . \u2022 |E|: the cardinality of E. \u2022 An estimation of the total number of object combinations that need to be attempted to search for bicliques, denoted comb(G). Take a graph G, compute gram(Q), and then look for the three highest values of the lower triangular gram matrix and multiply these values. \u2022 The ratio (|U ||V |)/w, we call the social degree of w. \u2022 The weight U \u2032 of a weight-maximal biclique in G. \u2022 The size V \u2032 of a size-maximal biclique in G. \u2022 The number of 2-weight bicliques, computed from gram(Q). \u2022 And, likewise, the number of 2-size bicliques."}, {"heading": "4.3. Construction of the Classifier", "text": "We have built seven classifiers, one per each selected w considered in our dataset. The rationale behind this design decision is to observe whether, and if so how, the number of observations is part of the order parameter. We have built each classifier using ten-fold cross-validation. Roughly, we first randomly picked 90% of the training set. Then, we obtained a classification tree from these data, using C4.5, as implemented in Weka [18]. Second, we tested the tree on the remaining 10% of the instances. Third, we repeated\nthis procedure 10 times. Finally, we selected the best classification tree and validated it on the test set. The corresponding results are reported on below."}, {"heading": "4.4. An Evaluation of the Classification Tree Performance", "text": "Table 1 shows the false positive rate and the output by our classifier, for various collections. The false positive rate (FPR) is the rate at which the classifier mistakes a HARD instance to be EASY, and the false negative rate (FNR) the other way round. There usually is a trade-off between these rates. Notice how the FPR grows as the number of observations does. This is explained by the larger the number of observations, the larger the proportion of instances labelled HARD. This implies that in the dataset, both training and validation, classes are not balanced. Thus, it is more likely that a HARD instance is wrongly classified.\nMore thoroughly, we have evaluated the performance of our classifier using Receiver Operating Characteristic (ROC) curves (see Figure 1). A ROC curve is a parametric curve, generated by varying a threshold and computing both the FPR and the FNR, at each operating point. The upper and the further left a ROC curve is, the better the classifier is. Figure 1 shows that our classifier is able to recognise instances. Notice that the classifier performance improves along with the number of observations.\nIn order to support these results, we have plotted precision over recall. Here, the upper and further to the right the curve is, the better classifier is. Figure 2 shows again how the classifier performance improves with the number of observations. This can be attributed to both class unbalance, and to the occurrence of a higher proportion of HARD instances in large graphs."}, {"heading": "4.5. Order Parameter Discovery from the Classification Tree", "text": "The classification trees we have obtained for all datasets are remarkably similar, regardless of the number of observations w. Figure 3 displays the\none for a collection of graphs with w = 250 observations. In general, the rules extracted out of these classification trees show that the features that separate a HARD instance from an EASY one are the cardinality |V \u2032| of the size-maximal biclique (denoted zmax), the number of edges in the graph |E|, number of 2-weight bicliques (denoted freqt), the cardinality |U | and the cardinality |V |.\nWe also noticed that a large number of instances of type HARD are captured by one rule, namely: label instance HARD, if cardinality V \u2032 of sizemaximal biclique and cardinality |V | are respectively greater than 3 and 57. We constructed a classifier considering this rule only. Figure 4 shows that this classifier is able to distinguish most of the HARD instances, regardless of the number of observations.\nConsidering this result, we have come up with the following order parameter, \u03c0. Let zmax denote the cardinality of a size-maximal biclique, then \u03c0 def\n= zmax/|V |, with \u03c0 \u2208 [0, 1]. Notice that, as zmax/|V | \u2192 1, finding a sizemaximal is computationally harder, as we might need to explore the entire search space. By contrast, as zmax/|V | \u2192 0 most the search space can be\npruned, since it is easy to discard the existence of large bicliques."}, {"heading": "5. An Algorithm to Compute Bicliques", "text": "To conduct our phase transition study, we have used Algorithm 1,2 which, given a graph and a positive integer z, returns a biclique with size z and maximal weight, if any, along with the computational cost incurred. Function adjacentTo(V ) returns a set of vertices U \u2032 \u2208 U , where all u \u2208 U \u2032 are adjacent to V . Notice that in lines 8-9 the algorithm returns a biclique, if any; otherwise, it returns noSolution. Also, notice that the biclique found by the algorithm is weight-maximal. This does not add any computational cost to the problem since our measure of computational expense, shown below, makes use of the other metric. Moreover, there is knowledge of the cardinality of a weight-maximal biclique, obtained from gram(Q\u22ba), so finding a biclique larger than the weight-maximal is negligible using our algorithm.\n2A Perl implementation of this algorithm is available at https://db.tt/vDG5vXh5.\nFollowing [19, 16, 15], we used the number of combinations explored as a measure of computational expense. The rationale behind this decision is it is not affected by the hardware on which experiments are run. This is in contrast with other measures, such as time-to-solve (used e.g. in [20]). We run our algorithm in over 100 thousand bipartite graphs from our dataset (described in Section 4) using several computers. Our experimentation on this work lasted about 6 months of continuous calculations using two computers: the first one being a Core i7 2Ghz computer with 4GB in RAM, and the second one a two Xeon 3GHz computer with 8GB in RAM."}, {"heading": "6. The Phase Transition of Finding a Biclique in larger Bipartite Graph", "text": "We are now ready to present the phase transition. First, Section 6.1, we present our results for the decision problem, as introduced in Section 2.2. Next, in Section 6.2, we present an alternative phase transition, where the problem is now turned into finding a size-maximal biclique with maximal weight."}, {"heading": "6.1. Phase Transition of the Decision Problem of Finding a Biclique", "text": "Figs. 5 \u2014 9 show the phase transition, where we have set the size z, to be equal to 4 \u2014 8, respectively. On each curve, we have set the weight to be maximal and combined the results for the collection with w from 50, 75, . . . , 150 number of observations. As standard in literature, the horizontal axis represents the order parameter, in our case denoted \u03c0 and given by the size of a size-maximal biclique divided over the cardinality of V in an instance, in symbols \u03c0 def\n= zmax/|V | (see Section. 4.5). Complementarily, in the vertical axis we have plotted the computational cost, the number of explored combinations in our case, involved in finding bicliques. To display the transition better, instead of \u03c0, we have plotted \u03c0\u0302 = log2(\u03c0), the order parameter in logarithmic scale.\nFigs. 5 \u2014 9 all adopt the following conventions. A solid line is used to denote the median cost of finding a biclique with size z; a dashed line to denote the 25th percentile (representing the less expensive instances); a dashdotted line to denote the 90th percentile (representing the hardest instances); and, finally, a dotted line is used to capture the probability of an instance being solvable.\nInterestingly, the curves all show the same easy-hard-easy-hard-easy pat-\nAlgorithm 1 Backtracking based approach to compute a biclique in a larger bipartite graph.\nInput: A bipartite graph G = (U, V, E) and a positive integer z of the biclique being looked for. Output: A biclique size z with maximal-weight along with the corresponding witness, if any.\n1: r \u2190 2 2: BlackList \u2190 \u2205 3: while z \u2265 r do 4: V \u2190 ( V\nr\n) // V is set of sets of vertices V\n5: for all V \u2032 in V do 6: if V \u2032 /\u2208 BlackList then 7: U \u2032 \u2190 adjacentTo(V \u2032) 8: if |U \u2032| \u2265 2 and z = r then 9: return G(U \u2032, V \u2032, E) 10: // Return a biclique witnessed by U \u2032 and V \u2032\n11: else 12: if |U \u2032| < 2 then 13: BlackList \u2190 BlackList \u222a V 14: end if\n15: end if\n16: end if\n17: end for 18: r = r + 1 19: end while 20: return noSolution\ntern, for median, 25th percentile, 90th percentile. Roughly, the hardest instances, showing the highest associated computational cost, all lie at \u03c0\u0302 = \u22121.5 (thus, \u03c0 = 3/8). Remarkably, this cost inflexion appears even in the 25th percentile curve. Notice how after the high inflection point, \u03c0\u0302 = \u22121.5, the computational cost decreases as \u03c0\u0302 \u2740 0. This result implies that it is negligibly cheap to find a biclique in a larger graph where a size-maximal has a size similar to the cardinality V \u2208 G, and, that, hence, a biclique is likely to be found. This is attributable to the adjacency matrix, which is mostly full of one values, and it is easy to discard vertices that cannot form part of a larger biclique. Thus, bicliques with z \u2740 |V | are uncommon, but fortunately\ntake no time to solve. Notice that for \u03c0\u0302 < \u22125, it is negligibly cheap to realize that an instance does not have a biclique. This result implies it takes no effort to find a biclique, where size-maximal bicliques all have a very small size compared to the cardinality of V ; notice how \u03c0 = 1/32. By contrast, for \u22124.25 < \u03c0\u0302 < \u22123.75, the cost of solving an instance is still negligible, but we cannot know in advance whether it is solvable or not; here, \u03c0 = 1/16 implying that the size-maximal biclique is still rather small, compared to the cardinality of V in the graph. Also, for \u22123.75 \u2265 \u03c0\u0302 \u2265 \u22121.5, the cost of finding a biclique dramatically increases, but it is very likely that a solution will be found. Notice an inflection point for \u22121.5 < \u03c0\u0302 < \u22121.2 where it is negligible to find a biclique, after this point the computational cost increases significantly; this behaviour is more evident in Figs. 8\u20149. Lastly, notice that for \u03c0\u0302 > \u22121.2 the computational cost decreases.\nIn conclusion, the phase transition for this problem exhibits an easy-\nhard-easy-hard-easy pattern. Regardless of the parameter z of the biclique being looked for, they all have similar inflection points, but with different computational costs, as expected. It also is worth noticing that the hardest instances lie at the YES region: they are likely to be solvable. By way of comparison, in the phase transition of many NP-complete problems they lie at a no-decision region, where there is a 50% probability of the instance being solvable."}, {"heading": "6.2. Maximal Bicliques", "text": "Figs. 5 \u2014 9 result from combining the output obtained for a number of problem instances, some of which are solvable and other are not. To convey the behaviour for either class separately, it is standard in the literature to find optimal solutions to the problem at hand; then, set the parameter z (to be searched for) to be at a known \u2018distance\u2019 d def\n= zmax \u2212 z from the optimal solution for each problem, where zmax is the size of size-maximal biclique in a given instance. Hence, at d = 0, 1, 2, . . . bicliques are guaranteed to be\nfound, when z \u2264 zmax; indeed, at d = 0 only size-maximal bicliques will be output. However, at d = \u22121,\u22122, . . . no bicliques will be found at all.\nSo, we have applied our algorithm to every problem instance, making it find a size-maximal biclique. For this step, we slightly modified our algorithm in two respects. First, we force it to report on the computational cost involved in finding any biclique with z \u2264 zmax, because this implies solving this optimality problemSecond, we make the algorithm to carry out a guarantee check, which rules out the possibility for it to search for a biclique larger than the maximum. Again, knowledge of the size-maximal biclique is readily obtained from gram(Q).\nWe have found the size-maximal bicliques with maximal weight for the collection of graphs obtained by observing 50, 75, . . . , 150 actions from the real-world process. Interestingly, they all show the same phase transition pattern, and, so, for the sake of brevity, we shall show only the mean cost to find a solution.\nFig. 10 shows the phase transition of finding the size-maximal biclique. We have set the biclique to be of maximal weight. In the curve, d is the distance from the optimal solution. A solid line is used to denote the mean cost of finding a biclique with seize d; a dashed line to denote the boundary between solvable and unsolvable instances, after and before the dashed line, respectively.\nNotice from Fig. 10 that for the optimal solution, we have the highest computational cost of 20440 combinations explored. When we reach the optimal solution, for 0 < d < 3, there is a significantly increase in the number of combinations explored. Then, for 3 \u2264 d \u2264 10 there is a soft increase in the number of combinations suggesting a critical area where the algorithm finds easy to discard combinations. By contrast, for 10 < d < 15 the computational cost increases significantly, this is because the algorithm finds difficult to discard combinations.\nIn conclusion, there is an exponential growth in the number of combina-\ntions as we reach the optimal solution."}, {"heading": "7. Conclusions and Indications for Further Work", "text": "In this work we focused on the identification of the phase transition that separates easy and hard instances in the problem of finding a complete bipartite subgraph in a larger bipartite graph, namely a biclique. This problem naturally arises in several contexts (e.g. bioinformatics, social network analysis, document classification, etc.) however few efforts have been made towards understanding its phase transition.\nIn order to conduct this work we followed a 4-step approach: First, the selection of the order parameter has been made by proposing a novel methodology which makes use of a classifier in order to identify parameter candidates. Second, in our experimentation we considered instances that arise in the context of a DNS server. Real-world instances can be significantly harder that similarly synthesised problems. Third, we have applied a backtracking\nbased approach to solve problem instances and measure the computational expense involved. Lastly, we plot the computational cost, and the probability of an instance being solvable against the order parameter, denoted \u03c0 and given by the size of a size-maximal biclique divided over the cardinality of V .\nOur results show a critical value at \u03c0 = 3/8. There, any problem instance would be expensive in terms of the number of combinations to explore and with a 98% probability of having a biclique with size z and maximal weight. By contrast, it takes no effort to realise that a graph does not contain a biclique for \u03c0 = 1/32.\nOur results help identifying key instances of the biclique problem that may be used to build a benchmark set, with which one can perform a statistically significant comparison amongst different methods that attempt to solve the same problem. Furthermore, the critical values of the phase transition study can also be used to determine when a complete method is likely to succeed (or not) in finding a solution to a problem instance in reasonable time.\nIn what follows we will describe some insights about future work: 1) It\nremains open how can we use the characteristics from the hardest instances to solve more efficiently the problem. As an example, consider that hardest instances have a common number of vertices U ; then, it is possible to design an algorithm which makes use of a rule considering the number of vertices, so the algorithm may know in advance the computational cost involved in solving the instances. 2) It is possible to study this problem considering much larger graphs. However, our initial experimentation has shown that large graphs may take months to be completed with our algorithm. To address this, we are currently working on using a MapReduce approach following an approach similar as in [21]. 3) Our work considers only real-world instances, we can make use of random instances in an attempt to have a better insight of the phase transition of this problem. 4) It remains open whether our machine learning approach is suitable to identify order parameter candidates for other NP-complete problems, so further experimentation is necessary to determine the viability of this method."}], "references": [{"title": "Where the really hard problems are", "author": ["P. Cheeseman", "B. Kanefsky", "W.M. Taylor"], "venue": "in: J. Mylopoulos, R. Reiter (Eds.), Proceedings of the 12th International Joint Conference on Artificial Intelligence, IJCAI, Morgan Kaufmann", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Phase transitions from real computational problems", "author": ["I.P. Gent", "T. Walsh"], "venue": "in: Proceedings of the 8th International Symposium on Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Phase transitions and the search problem", "author": ["T. Hogg", "B.A. Huberman", "C.P. Williams"], "venue": "Artificial Intelligence 81 (1\u20132) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "A new look at the easy-hard-easy pattern of combinatorial search difficulty", "author": ["D.L. Mammen", "T. Hogg"], "venue": "Journal of Artificial Intelligence Research 7 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Asymptotic and finite size parameters for phase transitions: Hamiltonian circuit as a case study", "author": ["J. Frank", "I.P. Gent", "T. Walsh"], "venue": "Information Processing Letters 65 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "The hardest random SAT problems", "author": ["I.P. Gent", "T. Walsh"], "venue": "in: B. Nebel, L. Dreschler-Fischer (Eds.), Proceedings of the 18th German Annual Conference on Artificial Intelligence, KI-94, Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "On finding bicliques in bipartite graphs: a novel algorithm and its application to the integration of diverse biological data types", "author": ["Y. Zhang", "C. Phillips", "G. Rogers", "E. Baker", "E. Chesler", "M. Langston"], "venue": "BMC Bioinformatics 15 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A graphtheoretical approach for pattern discovery in epidemiological research", "author": ["R. Mushlin", "A. Kershenbaum", "S. Gallagher", "T. Rebbeck"], "venue": "IBM Syst J 46 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Obtaining maximal concatenated phylogenetic data sets from large sequence databases", "author": ["M. Sanderson", "A. Driskell", "R. Ree", "O. Eulenstein", "S. Langley"], "venue": "Mol Biol Evol 20 (7) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "A social network based model to detect anomalies on DNS servers", "author": ["R. Alonso"], "venue": "Ph.D. thesis, Tecnol\u00f3gico de Monterrey ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Node-and edge-deletion np-complete problems", "author": ["M. Yannakakis"], "venue": "in: Proceedings of the Tenth Annual ACM Symposium on Theory of Computing, STOC \u201978, ACM, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "On bipartite and multipartite clique problems", "author": ["M. Dawande", "P. Keskinocak", "J.M. Swaminathan", "S. Tayur"], "venue": "J. Algorithms 41 (2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "The maximum edge biclique problem is np-complete", "author": ["R. Peeters"], "venue": "Discrete Applied Mathematics 131 (3) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "On the NP-completeness of computing the commonality amongst the objects upon which a collection of agents has performed an action", "author": ["R. Alonso", "R. Monroy"], "venue": "Computacion y Sistemas 17 (4) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "The TSP phase transition", "author": ["I.P. Gent", "T. Walsh"], "venue": "Artificial Intelligence 88 (1\u20132) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Scaling effects in the CSP phase transition", "author": ["I.P. Gent", "E. MacIntyre", "P. Prosser", "T. Walsh"], "venue": "in: U. Montanari, F. Rossi (Eds.), Principles and Practice of Constraint Programming, CP \u201995, Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Phase transitions and annealed theories: Number partitioning as a case study", "author": ["I.P. Gent", "T. Walsh"], "venue": "in: W. Wahlster (Ed.), Proceedings of the Twelfth European Conference on Artificial Intelligence, ECAI\u201996, John Wiley & Sons", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "2nd Edition, Morgan Kaufmann Series in Data Management Systems, Morgan Kaufmann", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "The SAT phase transition", "author": ["I.P. Gent", "T. Walsh"], "venue": "in: A. G. Cohn (Ed.), Proceedings of the Eleventh European Conference on Artificial Intelligence, ECAI\u201994, John Wiley & Sons", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Phase transition in the bandwidth minimization problem", "author": ["N. Rangel-Valdez", "J. Torres-Jimenez"], "venue": "in: A. Hern\u00e1ndez-Aguirre, R. Monroy-Borja, C. A. Reyes-Gar\u0107\u0131a (Eds.), Proceedings of the 8th Mexican International Conference on Artificial Intelligence, MICAI \u201909, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Using cloud computing MapReduce operations to detect DDoS attacks on DNS servers", "author": ["L. Trejo", "R. Monroy", "R. Alonso", "A. Avila", "M. Maqueo", "J. Vazquez", "E. Sanchez"], "venue": "in: A. Proenca, A. Pina, J. Gar\u0107\u0131a-Tobio, L. Ribeiro (Eds.), Proceedings of the 4th Iberian Grid Infrastructure Conference 2010, IBERGRID\u201910, Netbiblo", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1] showed that, for any NP-complete problem, there exists a phase transition that separates easy instances from hard ones, and that this phase transition can be found as one varies an order parameter 1 around one or more critical values.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3, 4, 5, 6].", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "[3, 4, 5, 6].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "[3, 4, 5, 6].", "startOffset": 0, "endOffset": 12}, {"referenceID": 5, "context": "[3, 4, 5, 6].", "startOffset": 0, "endOffset": 12}, {"referenceID": 6, "context": "[7, 8, 9]) where researchers have proposed several algorithms to compute bicliques.", "startOffset": 0, "endOffset": 9}, {"referenceID": 7, "context": "[7, 8, 9]) where researchers have proposed several algorithms to compute bicliques.", "startOffset": 0, "endOffset": 9}, {"referenceID": 8, "context": "[7, 8, 9]) where researchers have proposed several algorithms to compute bicliques.", "startOffset": 0, "endOffset": 9}, {"referenceID": 6, "context": "[7] has recently reported an improvement, with respect to other methods, in enumerating all the bicliques in a graph in a real-world dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Then, as shown by Alonso [10], the number and structure of such bicliques are severely broken apart upon an abnormal event, e.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Following standard convention, we use \u201corder parameter\u201d to refer to a parameter that controls the complexity of finding a solution to a given problem instance [1], instead of using the more appropriate term control parameter [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "Following standard convention, we use \u201corder parameter\u201d to refer to a parameter that controls the complexity of finding a solution to a given problem instance [1], instead of using the more appropriate term control parameter [2].", "startOffset": 225, "endOffset": 228}, {"referenceID": 10, "context": "in [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "like [12, 13] have proven that the NP-completeness of finding a biclique also holds for other restrictions, such as specifying a maximum number of edges, a maximum number of vertices, or specifying a maximum edge weight.", "startOffset": 5, "endOffset": 13}, {"referenceID": 12, "context": "like [12, 13] have proven that the NP-completeness of finding a biclique also holds for other restrictions, such as specifying a maximum number of edges, a maximum number of vertices, or specifying a maximum edge weight.", "startOffset": 5, "endOffset": 13}, {"referenceID": 10, "context": "Particularly, the version of the problem for which we demand |U | = |V | is called balanced biclique [11], and that where |U | = t and |V | = z, for given t and z, is called exact node cardinality decision [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Particularly, the version of the problem for which we demand |U | = |V | is called balanced biclique [11], and that where |U | = t and |V | = z, for given t and z, is called exact node cardinality decision [12].", "startOffset": 206, "endOffset": 210}, {"referenceID": 13, "context": "Recently, Alonso and Monroy [14] have proven that finding a biclique such that |U | \u2265 t and |V | \u2265 z vertices, for given t and z, is also NP-complete.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": ", travelling salesman [1, 15]): the cost of finding a solution increases at first, but then decreases later on to small values back again.", "startOffset": 22, "endOffset": 29}, {"referenceID": 14, "context": ", travelling salesman [1, 15]): the cost of finding a solution increases at first, but then decreases later on to small values back again.", "startOffset": 22, "endOffset": 29}, {"referenceID": 15, "context": "constraint satisfaction [16]) have been found to have similar cost, regardless of the size of the instance, as long as the instance is scaled.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "As pointed out by [1], using a different order parameter yields a different phase transition.", "startOffset": 18, "endOffset": 21}, {"referenceID": 16, "context": "Gent and Walsh [17] used an annealed theory to determine an order parameter for the phase transition of number partitioning.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "From this bipartite graph it is possible to observe bicliques (since we tend to visit the same websites), as shown in [10].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "5, as implemented in Weka [18].", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "Let zmax denote the cardinality of a size-maximal biclique, then \u03c0 def = zmax/|V |, with \u03c0 \u2208 [0, 1].", "startOffset": 93, "endOffset": 99}, {"referenceID": 18, "context": "Following [19, 16, 15], we used the number of combinations explored as a measure of computational expense.", "startOffset": 10, "endOffset": 22}, {"referenceID": 15, "context": "Following [19, 16, 15], we used the number of combinations explored as a measure of computational expense.", "startOffset": 10, "endOffset": 22}, {"referenceID": 14, "context": "Following [19, 16, 15], we used the number of combinations explored as a measure of computational expense.", "startOffset": 10, "endOffset": 22}, {"referenceID": 19, "context": "in [20]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "To address this, we are currently working on using a MapReduce approach following an approach similar as in [21].", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "We report on the phase transition of finding a complete subgraph, of specified dimensions, in a bipartite graph. Finding a complete subgraph in a bipartite graph is a problem that has growing attention in several domains, including bioinformatics, social network analysis and domain clustering. A key step for a successful phase transition study is identifying a suitable order parameter, when none is known. To this purpose, we have applied a decision tree classifier to real-world instances of this problem, in order to understand what problem features separate an instance that is hard to solve from those that is not. We have successfully identified one such order parameter and with it the phase transition of finding a complete bipartite subgraph of specified dimensions. Our phase transition study shows an easy-to-hard-to-easy-to-hard-toeasy pattern. Further, our results indicate that the hardest instances are in a region where it is more likely that the corresponding bipartite graph will have a complete subgraph of specified dimensions, a positive answer. By contrast, instances with a negative answer are more likely to appear in a region where the computational cost is negligible. This behaviour is remarkably similar for problems of a number of different sizes.", "creator": "LaTeX with hyperref package"}}}