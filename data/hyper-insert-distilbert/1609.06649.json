{"id": "1609.06649", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Minimally Supervised Written-to-Spoken Text Normalization", "abstract": "in basic speech - tracing applications such them as text - to - sms speech ( tts ) or automatic domain speech recognition ( gps asr ), \\ @ emph { text query normalization } refers to the task of converting from a \\ emph { \u2022 written } pictorial representation into a spatial representation of { how the text segment is designed to be \\ @ emph { & spoken }. yet in all real - world speech applications, the appropriate text gen normalization engine is likewise developed - - - in twelve large sections part - - - performed by fourth hand. for making example, evaluating a hand - finger built grammar may variously be used to enumerate precisely the possible ways of actually saying eliminating a possibly given token in applying a given complex language, and including a statistical model used to select the most appropriate pronunciation possibilities in context. and in confronting this overall study potentially we examine the uncertain tradeoffs is associated reasonably with using successively more or - less language - specific domain knowledge in a text global normalization engine. in exploring the most data - rich computational scenario, we have access mapped to a customized carefully constructed hand - crank built normalization grammar that for any exceptionally given input token configuration will confidently produce surely a set of basically all possible distinct verbalizations for improving that token. secondly we also assume interpreting a corpus of aligned written - spoken utterances, from which situation we all can train within a flexible ranking scale model that freely selects the appropriate optimal verbalization language for replacing the given context. alternatively as choosing a substitute for the carefully randomly constructed grammar, we also strongly consider a scenario supplied with a matching language - universal normalization \\ mobile emph { covering grammar }, ideally where the platform developer merely suddenly needs to provide specifically a set of associated lexical items particular to the language. as using a substitute for replacing the already aligned functional corpus, we also consider defining a scenario where one only construct has the spoken discourse side, otherwise and the clearly corresponding similarly written side typically is \" profoundly hallucinated \" by considering composing the spoken side with rejecting the intended inverted normalization responsive grammar. while we investigate specifically the accuracy risks of preparing a text normalization engine under each of these scenarios. we report mostly the results critically of experiments on english and russian.", "histories": [["v1", "Wed, 21 Sep 2016 17:51:11 GMT  (1355kb)", "http://arxiv.org/abs/1609.06649v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ke wu", "kyle gorman", "richard sproat"], "accepted": false, "id": "1609.06649"}, "pdf": {"name": "1609.06649.pdf", "metadata": {"source": "CRF", "title": "Minimally Supervised Written-to-Spoken Text Normalization", "authors": ["Ke Wu"], "emails": ["wuke@google.com", "kbg@google.com", "rws@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n06 64\n9v 1\n[ cs\n.C L\n] 2\n1 Se\np 20\n16"}, {"heading": "1 Introduction", "text": "Over the past 15 years there has been substantial progress on machine-learning approaches to text normalization (Sproat et al., 2001; Schwarm and Ostendorf, 2002; Han and Baldwin, 2011; Liu et al., 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013). Nonetheless, for real-world speech applications, such as text-to-speech (TTS) systems or automatic speech recognition systems (ASR), text normalization engines require a substantial amount of manual grammar development. Despite powerful machine-learning methods, it is still not possible to learn accurate end-to-end text normalization engines for speech applications using only a large amount of annotated data.\nOne reason for this is a lack of appropriate annotated data. Constructing hand-built grammars is a labor intensive and time consuming process, but so is constructing appropriate corpora consisting of raw text and the corresponding normalization.1 Furthermore training corpora for text normalization must be constructed for the particular application they are intended for. There are, for example, several training corpora for social media text normalization (Han and Baldwin, 2011; Liu et al., 2011; Yang and Eisenstein, 2013), but these are of little use\n1Minimally supervised approaches that attempt to construct normalizers using unannotated text data based on matching unnormalized forms with possible expansions given some corpus of \u2018clean\u2019 text (Sproat et al., 2001; Yang and Eisenstein, 2013; Roark and Sproat, 2014) are relevant, but are only applicable to certain classes of tokens, such as abbreviations.\nfor a TTS system where the normal input rarely includes examples like cu l8tr lv u ;-) but where, on the other hand, numbers need to be expanded into words (e.g., twenty-three thousand), something that is generally of no interest when normalizing social media text.\nIn this paper we present a collection of methods to learn a mapping from written text to its spoken form, i.e., the mapping needed for a TTS system, or for producing language model training data from raw text. For our data we use 2.24 million words of English transcribed speech and 1.95 million words of Russian speech, both collected from a voice search application. This spoken form data was converted to plausible written forms by a semiautomatic process, then corrected by a team of native-speaker annotators. This results in 1.75 million written tokens for English, and 1.59 million written tokens for Russian.2\nIn some of our studies we also assume a hand-built language-specific normalization grammar, henceforth referred to as the language-specific (normalization) grammar. In this study, we use pre-existing grammars developed as part of a system for training language models for ASR. The normalization grammars are represented as weighted finite-state transducers (WFST) compiled using the Thrax grammar development system (Roark et al., 2012). This grammar is designed to overgenerate; that is, it produces, for any input token, a (potentially weighted) lattice of possible spoken strings \u2014 see (Gorman and Sproat, 2016). Thus for example 123 might produce one hundred twenty three, one two three, one twenty three, among possibly others. In Section 3 we describe how we combine this grammar with a reranker that selects the appropriate choice in context. These language-specific grammars are written by native speaker linguists and designed to cover a range of types of normalization phenomena, including numbers, some mathematical expressions, some abbreviations, times and currency amounts. Following Taylor (2009), we refer to these categories targeted by normalization as semiotic classes.\nSuch grammars, needless to say, take a fair amount of effort and knowledge to develop for a new language. But in fact, there is a lot of information that can be shared across languages if the grammars are appropriately parameterized. To take a simple example, a time such as 3:30 PM might be read as the equivalent of three thirty PM as in English, or as PM three thirty, with the period expression before the time, as in Chinese, Japanese or Korean. It is thus possible to construct what we shall henceforth refer to as a covering grammar, which sets out the universally available options. For 3:30 PM example, the covering grammar would allow verbalizations in both orders. All that is required to specialize the covering grammar for a specific language is to provide a small lexicon providing verbalizations for individual terms (e.g., how PM is read): such a lexicon could be as small as a few hundred items, and can be developed by a native speaker who is not a trained linguist. Something more than this is required for readings of numbers, which are considerably less constrained: for that we make use of the algorithm described in (Gorman and Sproat, 2016), which can learn verbalizations for number names in a language from about 300 labeled examples. Given the covering grammar and the language-specific lexicon, we leave it up to the ranker to learn whether, e.g., one reads PM before or after the 3:30. In Section 3, we describe the use of a covering grammar in place of the more-carefully engineered language-specific grammar considered, and consider the degradation resulting from replacing the language-specific grammar with the covering grammar. Thus we have two kinds of normalizers: one derived from a hand-constructed language-specific grammar, and the other from a permissive language-universal covering grammar composed with a language-specific lexicon.\nFinally, we consider one other scenario, one where we have only the spoken data, bu have not gone to the effort to create the corresponding written forms at all. This scenario may obtain when one has transcriptions of speech in some language, or a corpus of fairly clean written text where even things like numbers are written out in words. This is a situation that is likely to obtain in low-resource languages where one might not have the resources to create large amounts of parallel data. Our approach in this scenario is to \u201challucinate\u201d the written-side text by inverting\n2Any personally identifiable information was removed from this data before analysis.\nthe normalizer to turn it into a denormalizer (Shugrina, 2010; Vasserman et al., 2015), and then composing the resulting lattice of putative written forms with the normalizer.3 We discuss this process in Section 3. Note that this final scenario has much in common with prior work by Sagae et al. 2012 and Smith and Eisner 2005.\nThe above scenarios yield four training conditions, as follows: 1. language-specific grammar with real written data 2. language-specific grammar with hallucinated written data 3. covering grammar with real written data 4. covering grammar with hallucinated written data\nAs one might expect, the first condition yields the best results and, usually the fourth condition the worst results. But our primary question is how much of a loss in accuracy results from the lower-resource conditions, and we report in detail on this below. We perform experiments both on English and Russian. In general the English results are overall much better than those of Russian \u2014not surprisingly given the complex morphology of the latter\u2014but the results for these two languages are otherwise comparable.\nThis work thus takes a somewhat different approach than many of the recent machinelearning approaches to text normalization, work best exemplified by research on social media normalization (e.g., Yang and Eisenstein 2013). In particular we are prepared to assume some amount of linguistic resource development, including the development of languagespecific lexical information, the possible existence of aligned written-spoken data, and for numbers, a minimal set of digit-sequence-to-number-name mappings as required by the method in (Gorman and Sproat, 2016). For readings of numbers, methods such as those of Yang and Eisenstein 2013 are in any case completely inapplicable. Instead we focus on the relationship between the amount of language-specific effort required and accuracy of the resulting system."}, {"heading": "2 Data and Conventions", "text": "As noted above, our data consists English and Russian transcribed speech, both collected from a voice search application. This written form data has been converted to plausible spoken forms by a semiautomatic process, then corrected by native-speaker annotators. Each sentence in both corpora contains at least one token that requires normalization. Disjoint subsets are held-out from the corpora as development and test sets. Table 2 lists some basic statistics of the English and the Russian corpora. As one can see, the majority of the tokens remain unchanged after normalization.\n3 Inversion of the verbalizer is trivial if one represents the grammatical knowledge in terms of a WFST\u2014but much harder if it is represented, for example, as a neural network.\nThe parallel data is assumed to be aligned at the word level, so that each written token aligns to one or more spoken tokens. Therefore we represent a training sentence as a sequence of pairs (x1, z1), . . . , (xn, zn), where xi is a written token and zi is the sequence of spoken tokens it aligns to. Note that here and below, we use lowercase letters (e.g., x) to represent words and scalars and bold lowercase letters (e.g., y) for sequences of words and for vectors. For any operation that takes a string operand (e.g., composition), we consider a word sequence y to be synonymous to the string constructed by joining the elements of y with space. Figure 1 is an example of an annotated parallel sentence.\nAs discussed above, we assume two types of grammar, namely a language-specific normalization grammar and a covering grammar. The language-specific normalization grammar for English consists of about 2,500 lines of rules and lexical specifications written using the Thrax finite-state grammar development toolkit (Roark et al., 2012), and the equivalent Russian grammar has about 4,100 lines.\nThe covering grammar is fundamentally language-independent and consists of about 700 lines of Thrax code. In addition we have 75 lexical specifications for English and 220 for Russian, with the bulk of the difference being due to the need to list various potential inflected forms in Russian. This does not however, include the verbalizations for cardinal and ordinal numbers, which, as we have already noted, in the covering grammar setting are learned from a small set of aligned examples using the method described in (Gorman and Sproat, 2016). As described above, the covering grammar is intended to represent all possible ways in which a language might express particular semiotic classes. By way of illustration, consider the grammar fragment below as it is written in Thrax:\nperiod = @@TIME_AM@@ | @@TIME_PM@@; space = \" \" | (\"\" : \" \"); time = Optimize[(period space)? time_variants |\ntime_variants (space period)?];\nThe first expression defines period to be either @@TIME_AM@@ or @@TIME_PM@@, which will be verbalized with appropriate lexical items in the target language. The third expression then defines a time to be any of a number of ways to verbalize core time values\u2014e.g., three thirty or half past three, termed time_variants here\u2014either preceded or followed by an optional period and an intervening space. For any target language, this will overgenerate, so we require a means of selecting the appropriate form for the language; see Section 3. A fragment of the lexical map for English is given in Figure 2, where variables marked with @@ are the abstract elements of the grammar that need to be specified.\nIn developing the covering grammar we endeavored to cover the same phenomena as covered in the language-specific verbalization grammars. For example the English verbalization grammar handles various ways of reading digit sequences, so that 990 might be read as nine hundred ninety, or nine ninety or nine nine oh, all of which are possible in various contexts. In a similar fashion we also allowed digit sequences in the covering grammars to be read as cardinal (or ordinal numbers), or as various combinations of these two categories. Obviously a fair amount of genre-specific knowledge goes into the design of the covering grammar, and without this knowledge, this strategy would likely fail. On the other hand, once such a grammar is created,\ndeveloping a system for a new language merely requires one to specify a lexical map, which requires far less work and expertise than developing a new grammar.4"}, {"heading": "3 Ranking models", "text": "The discussion in this section is applicable to both the language specific normalizer grammar and the covering grammar, and so we refer to both as simply the grammar.\nFor a written token xi in sentence x, the set of possible outputs Yi consists of both the result of composition of xi with the grammar as well as xi itself (which corresponds to passing the token through, unmodified). When |Yi| > 1\u2014i.e., when there are multiple normalization options for xi\u2014we need some way to choose the contextually appropriate output y \u2217\ni \u2208 Yi which is closest to the reference verbalization zi. The simplest such model is an n-gram language model (LM) built from the spoken side of the parallel training data.5 We refer to this as the baseline system. However, there are two limitations with this approach. First, the vast majority of the LM\u2019s parameters are otiose as they pertain to n-grams unaffected by the normalization process. Secondly, the LM scoring makes no use of knowledge about the written inputs or about the grammar.\nWe therefore cast the choice of yi,j as a ranking problem. For each Yi, let Gi = {y | d(y, zi) = miny\u2032 d(y\n\u2032, zi)} be the subset of good candidates from Yi, according to some distance metric d(\u00b7) such as label edit distance. Given a feature function \u03a6(yi,j, . . .) that combines the candidate verbalization and properties of the context into a feature vector, we train a maximum entropy ranker (Sproat and Hall, 2014) by choosing w that maximizes\nL(w) = \u2211\ni\nlog\n\u2211 yi,j\u2208Gi\nexp\u3008w,\u03a6(yi,j , . . .)\u3009 \u2211\nyi,j\u2208Yi exp\u3008w,\u03a6(yi,j , . . .)\u3009\nThe exact training and inference algorithms depend on the choice of feature function. In this paper, we experiment with two classes of feature functions."}, {"heading": "3.1 Local ranking", "text": "We can use a local feature function \u03a6I(X, i,y), which sees the whole input and the current verbalization, but not the verbalization of any other written tokens. Such feature functions allow independent inference for each written token. For training, we simply generate one training example for each xi for which |Yi| > 1. We use the following features in our experiments: Local output n-grams: n-grams (with n = 1, 2, 3) within output yi; Boundary trigrams: two written words on the left (xi\u22122, xi\u22121) and the first word of yi; two written words on the right (xi+1, xi+2) and the last word of yi; Written/spoken skip-grams: pairs of one written word on either the left or the right within a 4-word window and an output word in yi; Bias: 1xi=yi , i.e., whether xi is passed through.\n4 In the hopes that they will be more widely useful, we will release the covering grammars and lexical maps for English and Russian under the Apache 2.0 license.\n5This is essentially the approach taken by Sproat et al. 2001."}, {"heading": "3.2 Discriminative language model", "text": "In normalization, the majority of the written tokens are actually passed through, therefore a feature function \u03a6O(y1,j1 , . . . ,yi,ji ,1xi=yi,ji ) that sees the verbalization history but nothing much about the written tokens actually has a lot of overlapping information with \u03a6I(\u00b7). We therefore limit the features to spoken token n-gram suffixes ending at yi,j and the bias 1xi=yi,j . Further, we fix the weight of the bias feature to a constant negative number because passing through should be discouraged, and leave the n-gram weights as the only tunable parameters. Then, the trained model can be encoded as a WFST (Wu et al., 2014) for efficient inference.\nThis feature parameterization also allows us to train a model from spoken tokens without any information about the written tokens. In effect we are \u201challucinating\u201d the written side in a way similar to Sagae et al. 2012. Consider the following simple training procedure with \u03a6O(\u00b7): for each Yi, we extract feature vectors by assuming all the previous verbalizations are correct, i.e., \u03a6O(z1, . . . , zi\u22121,yi,j ,1xi=yi,j), and train w to assign higher scores to those yi,j \u2208 Gi. If we only have spoken tokens Y but not the corresponding written tokens, we can approximate the above example generation for any sub-sequence yi in Y by approximating Yi by Y \u2032 i = \u03c0o(yi \u25e6V \u22121 \u25e6V ), where V is the WFST representing the grammar, and Gi by G \u2032\ni = {yi}. The output of the composition yi \u25e6 V\n\u22121 is the set of written tokens from which the grammar may produce yi. Then, the cascaded composition gives all the spoken tokens the grammar may produce from a written token that produces yi. If V has perfect coverage over the written tokens, then Y \u2032\ni is a superset of Yi and G \u2032\ni is a subset of Gi. To control the amount data generated, we only allow yi to be at most 5 words, and limit the size of Y \u2032\ni to be the 10,000 shortest paths through the cascade. For example, the spoken sequence one twenty might be mapped via V \u22121 to various written forms such as 120, 1:20, one20, inter alia. These in turn composed with V would yield various potential spoken forms, including one twenty, one hundred twenty, twenty past one, inter alia."}, {"heading": "3.3 Candidate pruning", "text": "Although the number of normalization candidates for a single written token almost never exceeds 100 for English, the number of candidates in Russian can be prohibitively large. This is because the Russian number grammar permits all possible morphological variants for each output word, even though most combinations are ill-formed, leading to a combinatorial explosion. We therefore bias the output by composing the number grammar with a local language model over the spoken (output) side of the number grammar. The language model was trained using Witten-Bell smoothing and a held-out corpus of approximately 10,000 randomly-generated numbers. We note it is also possible to mine this data from the web by identifying strings which match the output projection of the unbiased grammar (Sproat, 2010)."}, {"heading": "3.4 Discriminative LM vs local ranking", "text": "As we will see, there is a large gap in error rates between discriminative LMs trained on real data and local ranking, especially with language-specific grammars. This is due to the lack of two pieces of information not easily available for hallucinated data, the more interesting use case of the discriminative LM method:\nTuned pass-through bias We fix the bias because we cannot tune it with hallucinated data. However, the grammar sometimes produces incorrect verbalizations for common words that should really be passed through, often because of pecularities in the data annotation. When training on real data, we can easily fix this issue by tuning the bias along with n-gram weights. Spoken phrase boundary Consider the input 1911 9mm, whose correct verbalization is nineteen eleven nine millimeter. The discriminative LM prefers one nine one one nine millimeter, because the hallucinated written form includes 19119 mm, and there is a strong bias in these data to reading five-digit numbers as digit sequences. With real data, we know where\nthe spoken phrase for a single written word begins or ends, and can train discriminative LMs on data with a special marker \u00a1p\u00bf inserted at such boundaries. This way, the LM can distinguish between one nine one one \u00a1p\u00bf nine and one nine one one nine, and know that the former is less likely than nineteen eleven \u00a1p\u00bf nine."}, {"heading": "4 Results", "text": "We evaluate the text normalization systems using the following two metrics, Word Error Rate (WER) The edit distance between the system output and the reference\noutput, divided by the number of spoken tokens in the reference that are a result of normalization;\nSentence Error Rate (SER) The proportion of test sentences with at least one error in the output.\nFor the baseline systems, we use unpruned trigram6 language models trained on the spoken side of the training data, with Katz smoothing. We also experiment with the variants of discriminative LM discussed in Section 3.4, by adding tuned bias and spoken phrase boundary when training on real data. For all the systems, we set hyperparameters so as to minimize WER over the development set.\nResults for English and Russian are shown in Table 2.7 We observe that all of the rankingbased systems outperform the baseline system that uses the same grammar. Indeed in Russian, all ranking-based systems outperform both baselines regardless of the grammar used. The cost of using the covering grammar as opposed to the language-specific grammar is about a 25% relative increase in WER for English, but only about a 6% increase for Russian, suggesting that the hand-built Russian grammar has been less carefully curated than the English gram-\n6Using higher order n-grams does not lower WER on the development set. 7While the error rates of even the best systems in Table 2 may seem high, in early experiments with these same data we found that the methods here actually outperformed those of a commercial TTS normalization system (Ebden and Sproat, 2014). This is largely due to the fact that the TTS normalization system was tuned for different data from the voice search domain so that, for example a digit sequence like 920 has a default reading in the TTS system as nine hundred twenty, whereas in the voice search domain it is much more likely to be nine twenty.\nmar. Local ranking with all the features discussed in Section 3.1 yields the best results, with a penalty for the discriminative LM (Section 3.2) with a language-specific grammar of between 14% relative (English) to 25% relative (Russian) WER. When using a covering grammar, the discriminative LM and local ranking are much closer to each other in performance. Finally, without tuned bias or spoken phrase boundary, the difference between the discriminative LM trained on real written-spoken correspondences, and that trained on hallucinated data is usually quite small (<1% absolute), except in the case of the Russian covering grammar. However, with the language-specific grammar, the hallucinated training scenario actually performs slightly better than training with real written-spoken correspondences. Note also that with real data the covering grammar outperforms the language-specific grammar for Russian with the discriminative language model.\nAs is obvious from Table 2, Russian has much higher error rates overall than English, reflecting the greater morphological complexity of this language, as well as other things such as the fact that in these data, tokens written as digit sequences could be read as either cardinal or ordinal numbers in Russian (but only rarely as ordinals in English), and thus the system must resolve that ambiguity as well as handling case, gender and number morphology.\nIn summary, local ranking solutions outperform the baseline spoken-side trigram LM as a method to rank the candidates generated by the grammar. Local ranking outperforms discriminative language model ranking without tuned bias or phrase boundary information, but controlling for the same setup in discriminative language model experiments, hallucinated data does not hurt performance by very much. These results suggest that one can develop an initial system with reasonable performance for a new language with very little work if one just has a source of \u201cspoken\u201d text, and is willing to invest a small amount of effort in developing a list of lexical entries, as well as some example number names \u2014 cf. (Gorman and Sproat, 2016) \u2014 for the language. Hallucinated data already approximates the ambiguity a ranking model needs to resolve very well and future effort should focus on approximating pass-through bias and phrase boundary in discriminative LM training. Naturally one can do better if one develops more language-specific grammars, as well as aligned written and spoken data. But the lowestresource scenarios may be sufficient for an initial system, and other\u2014higher-resource\u2014 scenarios we propose suggest a path one might take to improve such a system."}, {"heading": "5 Qualitative error analysis", "text": "We performed a qualitative error analysis in order to see if there were any broad generalizations about the kinds of errors that could be attributed to using the covering grammar versus the language specific grammar, and hallucinated versus real training data. We present these results below."}, {"heading": "5.1 English errors", "text": "For the covering grammar (vs language-specific grammar), the main error categories were the following:\n\u2022 Derived numerical expressions like 49ers, which the covering grammar does not support. \u2022 Occasional readings of words letter-by-letter: s o m e t h i n g. \u2022 Explicit o\u2019clock in times (e.g. three o\u2019clock p m versus three p m). \u2022 Digit readings of some numbers, such as 6308 as six three oh eight instead of sixty three oh eight. \u2022 Hundreds readings in cases like 2200\u2014two thousand two hundred versus twenty two hundred.\nMany of these categories of errors can be attributed to language-particular constructions that are simply not handled by the covering grammars, e.g., the case of 49ers.\nFor the hallucinated versus real data the main categories were, again, digit-by-digit readings of some numbers; and reading of + as and rather than plus. This second error is presumably due to the fact that + may be read as and, and in the construction V \u22121 \u25e6 V , we map from and\nto + and back to and and plus as candidates. As and is far more common that plus, the system learns to prefer and as a reading of +."}, {"heading": "5.2 Russian errors", "text": "The Russian errors for covering grammars vs. language-specific grammars fall mostly into the following categories:\n\u2022 Failure to read \u2018dot\u2019 in URLs. \u2022 Use of cardinal numbers when ordinals would be more appropriate: \u2018August 15\u2019 rather than . \u2022 A small number of instances of the special case of using for \u2018one\u2019 in counting ( \u2014 1 2 3), which the number grammar does not permit. The case of \u2018dot\u2019 is apparently due to an omission in the covering grammar, which does not handle the ru top-level domain. For hallucinated versus real data, the main errors were:\n\u2022 Reading words as letter sequences or vice versa: \u2018AK-47\u2019 read as (i.e. \u2018ak forty seven\u2019 rather than \u2018a k forty seven\u2019) \u2022 Digit-by digit readings versus grouped readings: 4400 as (\u2018four four zero zero\u2019) versus (\u2018forty four zero zero\u2019)."}, {"heading": "6 Discussion and Future Work", "text": "We have explored the relative contribution of different hand-built data to the performance of text-normalization systems trained using ranking. Specifically:\n\u2022 language-specific normalizer grammars versus language-independent covering grammars with a small amount of language-specific lexical knowledge; and\n\u2022 aligned written-spoken data versus only spoken data \u201challucinating\u201d the written side. We have shown that the performance degradation for using a covering grammar over a languagespecific grammar need not be large\u2014and in one configuration for Russian, the former actually outperformed the latter. In a similar fashion, the degradation caused by hallucinated data need also not be large; again in Russian, the hallucinated scenario slightly outperforms the discriminative LM trained on real data.\nThe choice of English and Russian in these experiments was motivated by the fact that we already had text normalization systems for these languages (see Footnote 7), and we could thus compare directly with existing approaches. But of course the real interest in these methods is in developing systems for new languages, and in particular low-resource languages where we can get raw data, and some linguistic knowledge, but do not have the resources to invest in large-scale grammar development. Therefore we plan to apply the methods reported here to low-resource languages in future work."}, {"heading": "Acknowledgments", "text": "We thank Michael Riley for much discussion of various versions of this work."}], "references": [{"title": "The Kestrel TTS text normalization system", "author": ["Ebden", "Sproat2014] Peter Ebden", "Richard Sproat"], "venue": "Natural Language Engineering,", "citeRegEx": "Ebden et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ebden et al\\.", "year": 2014}, {"title": "Minimally supervised models for number normalization. Transactions of the Association for Computational Linguistics", "author": ["Gorman", "Sproat2016] Kyle Gorman", "Richard Sproat"], "venue": null, "citeRegEx": "Gorman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorman et al\\.", "year": 2016}, {"title": "Lexical normalization of short text messages: Makn sens a #twitter", "author": ["Han", "Baldwin2011] Bo Han", "Timothy Baldwin"], "venue": "In ACL,", "citeRegEx": "Han et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Han et al\\.", "year": 2011}, {"title": "Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision", "author": ["Liu et al.2011] Fei Liu", "Fuliang Weng", "Bingqing Wang", "Yang Liu"], "venue": "In NAACL,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Toward text message normalization: Modeling abbreviation generation", "author": ["Pennell", "Liu2011] Deana Pennell", "Yang Liu"], "venue": "In ICASSP,", "citeRegEx": "Pennell et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pennell et al\\.", "year": 2011}, {"title": "Hippocratic abbreviation expansion", "author": ["Roark", "Sproat2014] Brian Roark", "Richard Sproat"], "venue": "In ACL,", "citeRegEx": "Roark et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2014}, {"title": "The OpenGrm open-source finite-state grammar software libraries", "author": ["Roark et al.2012] Brian Roark", "Richard Sproat", "Cyril Allauzen", "Michael Riley", "Jeffrey Sorensen", "Terry Tai"], "venue": "In ACL,", "citeRegEx": "Roark et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2012}, {"title": "Hallucinated N-best lists for discriminative language modeling", "author": ["K. Sagae", "M. Lehr", "E. Prud\u2019hommeaux", "P. Xu", "N. Glenn", "D. Karakosc", "S. Khudanpur", "B. Roark", "M. Saralar", "I. Shafran", "D. Bikel", "C. Callison-Burch", "Y. Cao", "K. Hall", "E. Hasler", "P. Koehn", "A. Lopez", "M. Post", "D. Riley"], "venue": null, "citeRegEx": "Sagae et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2012}, {"title": "Text normalization with varied data sources for conversational speech language modeling", "author": ["Schwarm", "Ostendorf2002] Sarah Schwarm", "Mari Ostendorf"], "venue": "In ICASSP,", "citeRegEx": "Schwarm et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schwarm et al\\.", "year": 2002}, {"title": "Formatting time-aligned ASR transcripts for readability", "author": ["Maria Shugrina"], "venue": "In NAACL,", "citeRegEx": "Shugrina.,? \\Q2010\\E", "shortCiteRegEx": "Shugrina.", "year": 2010}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Smith", "Eisner2005] Noah Smith", "Jason Eisner"], "venue": "In ACL,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Applications of maximum entropy rankers to problems in spoken language processing", "author": ["Sproat", "Hall2014] Richard Sproat", "Keith B. Hall"], "venue": "In INTERSPEECH,", "citeRegEx": "Sproat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sproat et al\\.", "year": 2014}, {"title": "Normalization of non-standard words", "author": ["Alan W. Black", "Stanley Chen", "Shankar Kumar", "Mari Ostendorf", "Christopher Richards"], "venue": "Computer Speech and Language,", "citeRegEx": "Sproat et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sproat et al\\.", "year": 2001}, {"title": "Lightly supervised learning of text normalization: Russian number names", "author": ["Richard Sproat"], "venue": "In IEEE Workshop on Speech and Language Technology,", "citeRegEx": "Sproat.,? \\Q2010\\E", "shortCiteRegEx": "Sproat.", "year": 2010}, {"title": "Text to speech synthesis", "author": ["Paul Taylor"], "venue": null, "citeRegEx": "Taylor.,? \\Q2009\\E", "shortCiteRegEx": "Taylor.", "year": 2009}, {"title": "Sequence-based class tagging for robust transcription in ASR", "author": ["Vlad Schogol", "Keith Hall"], "venue": "In INTERSPEECH,", "citeRegEx": "Vasserman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vasserman et al\\.", "year": 2015}, {"title": "Encoding linear models as weighted finite-state transducers", "author": ["Wu et al.2014] Ke Wu", "Cyril Allauzen", "Keith B. Hall", "Michael Riley", "Brian Roark"], "venue": "In INTERSPEECH,", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "A log-linear model for unsupervised text normalization", "author": ["Yang", "Eisenstein2013] Yi Yang", "Jacob Eisenstein"], "venue": "In EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Over the past 15 years there has been substantial progress on machine-learning approaches to text normalization (Sproat et al., 2001; Schwarm and Ostendorf, 2002; Han and Baldwin, 2011; Liu et al., 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013).", "startOffset": 112, "endOffset": 253}, {"referenceID": 3, "context": "Over the past 15 years there has been substantial progress on machine-learning approaches to text normalization (Sproat et al., 2001; Schwarm and Ostendorf, 2002; Han and Baldwin, 2011; Liu et al., 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013).", "startOffset": 112, "endOffset": 253}, {"referenceID": 3, "context": "There are, for example, several training corpora for social media text normalization (Han and Baldwin, 2011; Liu et al., 2011; Yang and Eisenstein, 2013), but these are of little use", "startOffset": 85, "endOffset": 153}, {"referenceID": 12, "context": "Minimally supervised approaches that attempt to construct normalizers using unannotated text data based on matching unnormalized forms with possible expansions given some corpus of \u2018clean\u2019 text (Sproat et al., 2001; Yang and Eisenstein, 2013; Roark and Sproat, 2014) are relevant, but are only applicable to certain classes of tokens, such as abbreviations.", "startOffset": 194, "endOffset": 266}, {"referenceID": 6, "context": "The normalization grammars are represented as weighted finite-state transducers (WFST) compiled using the Thrax grammar development system (Roark et al., 2012).", "startOffset": 139, "endOffset": 159}, {"referenceID": 5, "context": "The normalization grammars are represented as weighted finite-state transducers (WFST) compiled using the Thrax grammar development system (Roark et al., 2012). This grammar is designed to overgenerate; that is, it produces, for any input token, a (potentially weighted) lattice of possible spoken strings \u2014 see (Gorman and Sproat, 2016). Thus for example 123 might produce one hundred twenty three, one two three, one twenty three, among possibly others. In Section 3 we describe how we combine this grammar with a reranker that selects the appropriate choice in context. These language-specific grammars are written by native speaker linguists and designed to cover a range of types of normalization phenomena, including numbers, some mathematical expressions, some abbreviations, times and currency amounts. Following Taylor (2009), we refer to these categories targeted by normalization as semiotic classes.", "startOffset": 140, "endOffset": 835}, {"referenceID": 9, "context": "the normalizer to turn it into a denormalizer (Shugrina, 2010; Vasserman et al., 2015), and then composing the resulting lattice of putative written forms with the normalizer.", "startOffset": 46, "endOffset": 86}, {"referenceID": 15, "context": "the normalizer to turn it into a denormalizer (Shugrina, 2010; Vasserman et al., 2015), and then composing the resulting lattice of putative written forms with the normalizer.", "startOffset": 46, "endOffset": 86}, {"referenceID": 6, "context": "The language-specific normalization grammar for English consists of about 2,500 lines of rules and lexical specifications written using the Thrax finite-state grammar development toolkit (Roark et al., 2012), and the equivalent Russian grammar has about 4,100 lines.", "startOffset": 187, "endOffset": 207}, {"referenceID": 16, "context": "Then, the trained model can be encoded as a WFST (Wu et al., 2014) for efficient inference.", "startOffset": 49, "endOffset": 66}, {"referenceID": 13, "context": "We note it is also possible to mine this data from the web by identifying strings which match the output projection of the unbiased grammar (Sproat, 2010).", "startOffset": 140, "endOffset": 154}], "year": 2016, "abstractText": "In speech-applications such as text-to-speech (TTS) or automatic speech recognition (ASR), text normalization refers to the task of converting from a written representation into a representation of how the text is to be spoken. In all real-world speech applications, the text normalization engine is developed\u2014in large part\u2014by hand. For example, a handbuilt grammar may be used to enumerate the possible ways of saying a given token in a given language, and a statistical model used to select the most appropriate pronunciation in context. In this study we examine the tradeoffs associated with using more or less language-specific domain knowledge in a text normalization engine. In the most datarich scenario, we have access to a carefully constructed hand-built normalization grammar that for any given token will produce a set of all possible verbalizations for that token. We also assume a corpus of aligned written-spoken utterances, from which we can train a ranking model that selects the appropriate verbalization for the given context. As a substitute for the carefully constructed grammar, we also consider a scenario with a language-universal normalization covering grammar, where the developer merely needs to provide a set of lexical items particular to the language. As a substitute for the aligned corpus, we also consider a scenario where one only has the spoken side, and the corresponding written side is \u201challucinated\u201d by composing the spoken side with the inverted normalization grammar. We investigate the accuracy of a text normalization engine under each of these scenarios. We report the results of experiments on English and Russian.", "creator": "LaTeX with hyperref package"}}}