{"id": "1609.07561", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser", "abstract": "we introduce two first - order objective graph - based likelihood dependency parsers achieving having a new state of the art. initially the first is a feasible consensus parser built collectively from already an ensemble family of independently trained greedy lstm containing transition - node based risk parsers agreeing with different random order initializations. we cast increasingly this approach critically as minimum expense bayes at risk risk decoding ( notably under increasing the hamming transformation cost ) test and argue that recognizing weaker expectation consensus within the ensemble structures is a minimal useful psychological signal of difficulty or resource ambiguity. the conventional second constraint parser is a \" distillation \" decomposition of the ensemble diagram into a compatible single model. we thereby train the distillation parser reasonably using a structured / hinge power loss processing objective with varying a small novel uncertainty cost that adequately incorporates ensemble uncertainty estimates for each such possible target attachment, thereby avoiding the intractable cross - entropy computations than required by applying the standard sequential distillation objectives to problems with accurately structured outputs. while the first - order distillation parser matches dominates or surpasses the state effects of the digital art on english, cantonese chinese, ukrainian and southwestern german.", "histories": [["v1", "Sat, 24 Sep 2016 02:58:26 GMT  (44kb,D)", "http://arxiv.org/abs/1609.07561v1", "10 pages. To appear at EMNLP 2016"]], "COMMENTS": "10 pages. To appear at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adhiguna kuncoro", "miguel ballesteros", "lingpeng kong", "chris dyer", "noah a smith"], "accepted": true, "id": "1609.07561"}, "pdf": {"name": "1609.07561.pdf", "metadata": {"source": "CRF", "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser", "authors": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith"], "emails": ["akuncoro@cs.cmu.edu", "cdyer@cs.cmu.edu", "lingpenk@cs.cmu.edu", "miguel.ballesteros@upf.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In \u00a73, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae\nand Lavie, 2006) that seeks consensus among 20 randomly-initialized stack LSTM parsers (Dyer et al., 2015), achieving nearly the best-reported performance on the standard Penn Treebank Stanford dependencies task (94.51 UAS, 92.70 LAS).\nWe give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble\u2019s members may be taken as a signal that an attachment decision is difficult or ambiguous.\nEnsemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding consensus. We address this issue in \u00a75 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function, inspired by the notion of \u201csoft targets\u201d (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble\u2019s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions.\nThe distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and\nar X\niv :1\n60 9.\n07 56\n1v 1\n[ cs\n.C L\n] 2\n4 Se\np 20\nGerman. The code to reproduce our results is publicly available.1"}, {"heading": "2 Notation and Definitions", "text": "Let x = \u3008x1, . . . , xn\u3009 denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h,m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or relation type). Most dependency parsers are constrained to return y that form a directed tree.\nA first-order graph-based (FOG; also known as \u201carc-factored\u201d) dependency parser exactly solves\ny\u0302(x) = arg max y\u2208T (x) \u2211 (h,m)\u2208y s(h,m,x)\n\ufe38 \ufe37\ufe37 \ufe38 S(y,x)\n, (1)\nwhere T (x) is the set of directed trees over x, and s is a local scoring function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity.\nThe exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2) or O(n3) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing.\nAn alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the time of this writing employ neural networks (Andor et al., 2016).\n1https://github.com/adhigunasurya/ distillation_parser.git\nLet hy(m) denote the parent of xm in y (using a special null symbol when m is the root of the tree), and hy\u2032(m) denotes the parent of xm in the predicted tree y\u2032. Given two dependency parses of the same sentence, y and y\u2032, the Hamming cost is\nCH(y,y \u2032) = n\u2211 m=1 { 0 if hy(m) = hy\u2032(m) 1 otherwise\nThis cost underlies the standard dependency parsing evaluation scores (unlabeled and labeled attachment scores, henceforth UAS and LAS). More generally, a cost functionC maps pairs of parses for the same sentence to non-negative values interpreted as the cost of mistaking one for the other, and a firstorder cost function (FOC) is one that decomposes by attachments, like the Hamming cost.\nGiven a cost function C and a probabilistic model that defines p(y | x), minimum Bayes risk (MBR) decoding is defined by\ny\u0302MBR(x) = arg min y\u2208T (x) \u2211 y\u2032\u2208T (x) p(y\u2032 | x) \u00b7 C(y,y\u2032)\n= arg min y\u2208T (x) Ep(Y |x)[C(y,Y )]. (2)\nUnder the Hamming cost, MBR parsing equates algorithmically to FOG parsing with s(h,m,x) = p((h,m) \u2208 Y | x), the posterior marginal of the attachment under p. This is shown by linearity of expectation; see also Titov and Henderson (2006).\nApart from MBR decoding, cost functions are also used for discriminative training of a parser. For example, suppose we seek to estimate the parameters \u03b8 of scoring function S\u03b8. One approach is to minimize the structured hinge loss of a training dataset D with respect to \u03b8:\nmin \u03b8 \u2211 (x,y)\u2208D [\u2212 S\u03b8(y,x)\n+ max y\u2032\u2208T (x)\n( S\u03b8(y \u2032,x) + C(y\u2032,y) ) ]\n(3)\nIntuitively, this amounts to finding parameters that separate the model score of the correct parse from any wrong parse by a distance proportional to the cost of the wrong parse. With regularization, this is equivalent to the structured support vector machine\n(Taskar et al., 2005; Tsochantaridis et al., 2005), and if S\u03b8 is (sub)differentiable, many algorithms are available. Variants have been used extensively in training graph-based parsers (McDonald et al., 2005b; Martins et al., 2009), which typically make use of Hamming cost, so that the inner max can be solved efficiently using FOG parsing with a slightly revised local scoring function:\ns\u2032(h,m,x) = s(h,m,x) + { 0 if (h,m) \u2208 y 1 otherwise\n(4) Plugging this into Eq. 1 is known as costaugmented parsing."}, {"heading": "3 Consensus and Minimum Bayes Risk", "text": "Despite the recent success of neural network dependency parsers, most prior works exclusively report single-model performance. Ensembling neural network models trained from different random starting points is a standard technique in a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al., 2015). We aim to investigate the benefit of ensembling independently trained neural network dependency parsers by applying the parser ensembling method of Sagae and Lavie (2006) to a collection of N strong neural network base parsers.\nHere, each base parser is an instance of the greedy, transition-based parser of Dyer et al. (2015), known as the stack LSTM parser, trained from a different random initial estimate. Given a sentence x, the consensus FOG parser (Eq. 1) defines score s(h,m,x) as the number of base parsers that include the attachment (h,m), which we denote votes(h,m).2 An example of this scoring function with an ensemble of 20 models is shown in Figure 1 We assign to dependency (h,m) the label most frequently selected by the base parsers that attach m to h.\nNext, note that if we let s(h,m,x) = votes(h,m)/N , this has no effect on the parser (we have only scaled by a constant factor). We can therefore view s as a posterior marginal, and the ensemble parser as an MBR parser (Eq. 2).\n2An alternative to building an ensemble of stack LSTM parsers in this way would be to average the softmax decisions at each timestep (transition), similar to Vinyals et al. (2015).\nExperiment. We consider this approach on the Stanford dependencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015).3\nTable 1 shows that ensembles, even with small N , strongly outperform a single stack LSTM parser. Our ensembles of greedy, locally normalized parsers perform comparably to the best previously reported, due to Andor et al. (2016), which uses a beam (width 32) for training and decoding."}, {"heading": "4 What is Ensemble Uncertainty?", "text": "While previous works have already demonstrated the merit of ensembling in dependency parsing (Sagae and Lavie, 2006; Surdeanu and Manning, 2010), usually with diverse base parsers, we consider whether the posterior marginals estimated by p\u0302((h,m) \u2208 Y | x) = votes(h,m)/N can be interpreted. We conjecture that disagreement among base parsers about where to attach xm (i.e., uncertainty in the posterior) is a sign of difficulty or am-\n3We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.\nSentence: It will go for work ranging from refinery modification to changes in the distribution system, including the way service stations pump fuel into cars.\nbiguity. If this is true, then the ensemble provides information about which confusions are more or less reasonable\u2014information we will exploit in our distilled parser (\u00a75).\nA complete linguistic study is out of scope here; instead, we provide a motivating example before empirically validating our conjecture. Table 2 shows an example where there is considerable disagreement among base parsers over the attachment of a word (including). We invite the reader to attempt to select the correct attachment and gauge the difficulty of doing so, before reading on.\nRegardless of whether our intuition that this is an inherently difficult and perhaps ambiguous case is correct, it is uncontroversial to say that the words in the sentence not listed, which received zero votes (e.g., both instances of the), are obviously implausible attachments.\nOur next idea is to transform ensemble uncertainty into a new estimate of cost\u2014a replacement\n4In \u00a73, we used 20 models. Since those 20 models were trained on the whole training set, they cannot be used to obtain the uncertainty estimates on the training set, where the example sentence in Table 2 comes from. Therefore we trained a new ensemble of 21 models from scratch with five-way jackknifing. The same jackknifing setting is used in the distillation parser (\u00a76).\nfor the Hamming cost\u2014and use it in discriminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model."}, {"heading": "5 Distilling the Ensemble", "text": "Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for \u201cdistilling\u201d the ensemble\u2019s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea.\nThe idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike model combination techniques like stacking and beam search.\nDistilling an ensemble of classifiers into one simpler classifer that behaves similarly is due to Bucila\u030c et al. (2006) and Hinton et al. (2015); they were likewise motivated by a desire to create a simpler model that was cheaper to run at test time. In their work, the ensemble provides a probability distribution over labels for each input, and this predicted distribution serves as the training target for the distilled model (a sum of two cross entropies objective is used, one targeting the empirical training distribution and the other targeting the ensemble\u2019s posterior distribution). This can be contrasted with the supervision provided by the training data alone, which conventionally provides a single correct label for each instance. These are respectively called \u201csoft\u201d and \u201chard\u201d targets.\nWe propose a novel adaptation of the soft target idea to the structured output case. Since a sentence\nhas an exponential (in its length) number of parses, representing the posterior distribution over parses predicted by the ensemble is nontrivial. We solve this problem by taking a single parse from each model, representing the N -sized ensemble\u2019s parse distribution using N samples.\nSecond, rather than considering uncertainty at the level of complete parse trees (which would be analogous to the classification case) or larger structures, we instead consider uncertainty about individual attachments, and seek to \u201csoften\u201d the attachment targets used in training the parser. An illustration for the prepositional phrase attachment ambiguity in Fig. 1, taken from the ensemble output for the sentence, is shown in Table 3. Soft targets allow us to encode the notion that mistaking woman as the parent of with is less bad than attaching with to John or telescope. Hard targets alone do not capture this information."}, {"heading": "5.1 Distillation Cost Function", "text": "The natural place to exploit this additional information when training a parser is in the cost function. When incorporated into discriminative training, the Hamming cost encodes hard targets: the correct attachment should receive a higher score than all incorrect ones, with the same margin. Our distillation cost function aims to reduce the cost of decisions that\u2014based on the ensemble uncertainty\u2014appear to\nbe more difficult, or where there may be multiple plausible attachments.\nLet \u03c0(h,m) =\n1\u2212 p\u0302((h,m) \u2208 Y | x) = N \u2212 votes(h,m) N .\nOur new cost function is defined by CD(y,y\u2032) =\u2211n m=1max { 0, \u03c0(hy\u2032(m),m)\u2212 \u03c0(hy(m),m) } = \u2211n m=1max { 0, p\u0302(hy(m),m)\u2212 p\u0302(hy\u2032(m),m) } .\n(5)\nRecall that y denotes the correct parse, according to the training data, and y\u2032 is a candidate parse.\nThis function has several attractive properties:\n1. When a word xm has more than one plausible (according to the ensemble) but incorrect (according to the annotations) attachment, each one has a diminished cost (relative to Hamming cost and all implausible attachments). 2. The correct attachment (according to the goldstandard training data) always has zero cost since hy(m) = hy\u2032(m) and Eq. 5 cancels out. 3. When the ensemble is confident, cost for its choice(s) is lower than it would be under Hamming cost\u2014even when the ensemble is wrong. This means that we are largely training the distilled parser to simulate the ensemble, including mistakes and correct predictions. This encourages the model to replicate the state of the art ensemble performance. 4. Further, when the ensemble is perfectly confident and correct, every incorrect attachment has a cost of 1, just as in Hamming cost. 5. The cost of any attachment is bounded above by the proportion of votes assigned to the correct attachment.\nOne way to understand this cost function is to imagine that it gives the parser more ways to achieve a zero-cost5 attachment. The first is to correctly attach a word to its correct parent. The second is to predict a parent that the ensemble prefers to the correct parent, i.e., \u03c0(hy\u2032(m),m) < \u03c0(hy(m),m). Any other decision will incur a non-zero cost that is\n5It is important to note the difference between cost (Eq. 5) and loss (Eq. 3).\nproportional to the implausibility of the attachment, according to the ensemble. Hence the model is supervised both by the hard targets in the training data annotations and the soft targets from the ensemble.\nWhile it may seem counter-intuitive to place zero cost on an incorrect attachment, recall that the cost is merely a margin that must separate the scores of parses containing correct and incorrect arcs. In contrast, the loss (in our case, the structured hinge loss) is the \u201cpenalty\u201d the learner tries to minimize while training the graph-based parser, which depends on both the cost and model score as defined in Equation 3. When an incorrect arc is preferred by the ensemble over the gold arc (hence assigned a cost/margin of 0), the model will still incur a loss if s(hy(m),m,x) < s(hy\u2032(m),m,x). In other words, the score of any incorrect arc (including those strongly preferred by the ensemble) cannot be higher than the score of the gold arc.\nThe learner only incurs 0 loss if s(hy(m),m,x) \u2265 s(hy\u2032(m),m,x). This means that the gold score and the predicted score can have a margin of 0 (i.e., have the same score and incur no loss) when the ensemble is highly confident of that prediction, but the score of the correct parse cannot be lower regardless of how confident the ensemble is (hence the objective does not encourage incorrect trees at the expense of gold ones).\nIn the example in Table 2, we show the (additive) contribution to the distillation cost by each attachment decision (column labeled \u201cnew cost\u201d). Note that more plausible attachments according to the ensemble have a lower cost than less plausible ones (e.g., the cost for modification is less than system, though both are incorrect). While in the last line stations received no votes in the ensemble (implausible attachment), its contribution to the cost is bounded by the proportion of votes for correct attachment. The intuition is that, when the ensemble is not certain of the correct answer, it should not assign a large cost to implausible attachments. In contrast, Hamming cost would assign a cost of 1 (column labeled \u201cHamming\u201d) in all incorrect cases."}, {"heading": "5.2 Distilled Parser", "text": "Our distilled parser is trained discriminatively with the structured hinge loss (Eq. 3). This is a natural choice because it makes the cost function explicit\nand central to learning.6 Further, because our ensemble\u2019s posterior gives us information about each attachment individually, the cost function we construct can be first-order, which simplifies training with exact inference.\nThis approach to training a model is wellstudied for a FOG parser, but not for a transitionbased parser, which is comprised of a collection of classifiers trained to choose good sequences of transitions\u2014not to score whole trees for good attachment accuracy. Transition-based approaches are therefore unsuitable for our proposed distillation cost function, even though they are asymptotically faster. We proceed with a FOG parser (with Eisner\u2019s algorithm for English and Chinese, and MST for German since it contains a considerable number of non-projective trees) as the distilled model.\nConcretely, we use a bidirectional LSTM followed by a hidden layer of non-linearity to calculate the scoring function s(h,m,x), following Kiperwasser and Goldberg (2016) with minor modifications. The bidirectional LSTM maps each word xi to a vector x\u0304i that embeds the word in context (i.e., x1:i\u22121 and xi+1:n). Local attachment scores are given by:\ns(h,m,x) = v> tanh (W[x\u0304h; x\u0304m] + b) (6)\nwhere the model parameters are v, W, and b, plus the bidirectional LSTM parameters. We will refer to this parsing model as neural FOG.\nOur model architecture is nearly identical to that of Kiperwasser and Goldberg (2016), with two primary differences. The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second,\n6Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010).\n7To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training.\nwe apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages."}, {"heading": "6 Experiments", "text": "We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajic\u030c et al., 2009) tasks.\nExperimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64.\nHyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncertainty under the ensemble. To obtain the ensemble uncertainty on each language, we use 21 base models for English (see footnote 4), 17 for Chinese, and 11 for German.\nSpeed. One potential drawback of using a quadratic or cubic time parser to distill an ensemble of linear-time transition-based models is speed. Our FOG model is implemented using the same CNN library as the stack LSTM transition-based parser. On the same single-thread CPU hardware, the distilled MST parser9 parses 20 sentences per second without any pruning, while a single stack LSTM model\n8https://github.com/clab/cnn.git 9The runtime of the Hamming-cost bidirectional LSTM\nFOG parser is the same as the distilled parser.\nis only three times faster at 60 sentences per second. Running an ensemble of 20 stack LSTMs is at least 20 times slower (without multi-threading), not including consensus parsing. In the end, the distilled parser is more than ten times faster than the ensemble pipeline.\nAccuracy. All scores are shown in Table 5. First, consider the neural FOG parser trained with Hamming cost (CH in the second-to-last row). This is a very strong benchmark, outperforming many higherorder graph-based and neural network models on all three datasets. Nonetheless, training the same model with distillation cost gives consistent improvements for all languages. For English, we see that this model comes close to the slower ensemble it was trained to simulate. For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre (2012) for LAS.\nEffects of Pre-trained Word Embedding. As an ablation study, we ran experiments on English without pre-trained word embedding, both with the Hamming and distillation costs. The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS, compared to 93.6 UAS and 91.1 LAS for the model with distillation cost. This result further showcases the consistent improvements from using the distillation cost across different settings and languages.\nWe conclude that \u201csoft targets\u201d derived from ensemble uncertainty offer useful guidance, through the distillation cost function and discriminative training of a graph-based parser. Here we consid-\nered a FOG parser, though future work might investigate any parser amenable to training to minimize a cost-aware loss like the structured hinge."}, {"heading": "7 Related Work", "text": "Our work on ensembling dependency parsers is based on Sagae and Lavie (2006) and Surdeanu and Manning (2010); an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing. Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals. The local optima in his base model\u2019s training objective arise from latent variables instead of neural networks (in our case).\nModel distillation was proposed by Bucila\u030c et al. (2006), who used a single neural network to simulate a large ensemble of classifiers. More recently, Ba and Caruana (2014) showed that a single shal-\nlow neural network can closely replicate the performance of an ensemble of deep neural networks in phoneme recognition and object detection. Our work is closer to Hinton et al. (2015), in the sense that we do not simply compress the ensemble and hit the \u201csoft target,\u201d but also the \u201chard target\u201d at the same time10. These previous works only used model compression and distillation for classification; we extend the work to a structured prediction problem (dependency parsing).\nTa\u0308ckstro\u0308m et al. (2013) similarly used an ensemble of other parsers to guide the prediction of a seed model, though in a different context of \u201cambiguityaware\u201d ensemble training to re-lexicalize a transfer model for a target language. We similarly use an ensemble of models as a supervision for a sin-\n10Our cost is zero when the correct arc is predicted, regardless of what the soft target thinks, something a compression model without gold supervision cannot do.\ngle model. By incorporating the ensemble uncertainty estimates in the cost function, our approach is cheaper, not requiring any marginalization during training. An additional difference is that we learn from the gold labels (\u201chard targets\u201d) rather than only ensemble estimates on unlabeled data.\nKim and Rush (2016) proposed a distillation model at the sequence level, with application in sequence-to-sequence neural machine translation. There are two primary differences with this work. First, we use a global model to distill the ensemble, instead of a sequential one. Second, Kim and Rush (2016) aim to distill a larger model into a smaller one, while we propose to distill an ensemble instead of a single model."}, {"heading": "8 Conclusions", "text": "We demonstrate that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state of the art accuracy on English dependency parsing. This approach corresponds to minimum Bayes risk decoding, and we conjecture that the arc attachment posterior marginals quantify a notion of uncertainty that may indicate difficulty or ambiguity. Since running an ensemble is computationally expensive, we proposed discriminative training of a graph-based model with a novel cost function that distills the ensemble uncertainty. Deriving a cost function from a statistical model and extending distillation to structured prediction are new contributions. This distilled model, trained to simulate the slower ensemble parser, improves over the state of the art on Chinese and German."}, {"heading": "Acknowledgments", "text": "We thank Swabha Swayamdipta, Sam Thomson, Jesse Dodge, Dallas Card, Yuichiro Sawai, Graham Neubig, and the anonymous reviewers for useful feedback. We also thank Juntao Yu and Bernd Bohnet for re-running the parser of Bohnet and Nivre (2012) on Chinese with gold tags. This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR001115-C-0114; it was also supported in part by Contract\nNo. W911NF-15-1-0543 with the DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA)."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Andor et al.2016] Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Proc. of ACL", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Do deep nets really need to be deep", "author": ["Ba", "Caruana2014] Jimmy Ba", "Rich Caruana"], "venue": "In Proc. of NIPS", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Yoav Goldberg", "Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "A transition-based system for joint part-ofspeech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proc. of EMNLP-CoNLL", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Stanford typed dependencies manual", "author": ["De Marneffe", "Christopher D. Manning"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M. Eisner"], "venue": "In Proc. of COLING", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Softmax-margin CRFs: Training loglinear models with cost functions", "author": ["Gimpel", "Smith2010] Kevin Gimpel", "Noah A Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Gimpel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2010}, {"title": "Distilling the knowledge in a neural network. CoRR, abs/1503.02531", "author": ["Oriol Vinyals", "Jeffrey Dean"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Sequence-level knowledge distillation", "author": ["Kim", "Rush2016] Yoon Kim", "Alexander M. Rush"], "venue": "In Proc. of EMNLP", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In Proc. of ACL", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "The inside-outside recursive neural network model for dependency parsing", "author": ["Le", "Zuidema2014] Phong Le", "Willem Zuidema"], "venue": "In Proc. of EMNLP", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proc. of NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Fourth-order dependency parsing", "author": ["Ma", "Zhao2012] Xuezhe Ma", "Hai Zhao"], "venue": "In Proc. of COLING", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Stacking dependency parsers", "author": ["Dipanjan Das", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of EMNLP", "citeRegEx": "Martins et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2008}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of ICML", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Mariana S.C. Almeida", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proc. of ACL", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proc. of EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Integrating graph-based and transitionbased dependency parsers", "author": ["Nivre", "McDonald2008] Joakim Nivre", "Ryan McDonald"], "venue": "In Proc. of ACL", "citeRegEx": "Nivre et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2008}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proc. of IWPT", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Products of random latent variable grammars", "author": ["Slav Petrov"], "venue": "In Proc.of NAACL", "citeRegEx": "Petrov.,? \\Q2010\\E", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Parser combination by reparsing", "author": ["Sagae", "Lavie2006] Kenji Sagae", "Alon Lavie"], "venue": "In Proc. of NAACL", "citeRegEx": "Sagae et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2006}, {"title": "Minimum risk annealing for training log-linear models", "author": ["Smith", "Eisner2006] David A. Smith", "Jason Eisner"], "venue": "In Proc. of ACL", "citeRegEx": "Smith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2006}, {"title": "Ensemble models for dependency parsing: Cheap and good", "author": ["Surdeanu", "Manning2010] Mihai Surdeanu", "Christopher D. Manning"], "venue": "In Proc. of NAACL", "citeRegEx": "Surdeanu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Target language adaptation of discriminative transfer parsers", "author": ["Ryan T. McDonald", "Joakim Nivre"], "venue": "In Proc. of NAACL", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar et al.2005] Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proc. of ICML", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Bayes risk minimization in natural language parsing", "author": ["Titov", "Henderson2006] Ivan Titov", "James Henderson"], "venue": "Technical report, University of Geneva", "citeRegEx": "Titov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2006}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proc. of NAACL", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables. JMLR", "author": ["Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": null, "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proc. of NIPS", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wang", "Chang2016] Wenhui Wang", "Baobao Chang"], "venue": "In Proc. of ACL", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proc. of ACL", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Building a large-scale annotated chinese corpus", "author": ["Xue et al.2002] Nianwen Xue", "Fu-Dong Chiou", "Martha Palmer"], "venue": "In Proc. of COLING", "citeRegEx": "Xue et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2002}, {"title": "Incremental recurrent neural network dependency parser with search-based discriminative training", "author": ["Yazdani", "Henderson2015] Majid Yazdani", "James Henderson"], "venue": "In Proc. of CoNLL", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing using beam-search", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proc. of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "A re-ranking model for dependency parser with recursive convolutional neural network", "author": ["Zhu et al.2015] Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang"], "venue": "In Proc. of ACL-IJCNLP", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "startOffset": 71, "endOffset": 130}, {"referenceID": 36, "context": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "startOffset": 71, "endOffset": 130}, {"referenceID": 0, "context": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "startOffset": 71, "endOffset": 130}, {"referenceID": 7, "context": "In \u00a73, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae and Lavie, 2006) that seeks consensus among 20 randomly-initialized stack LSTM parsers (Dyer et al., 2015), achieving nearly the best-reported performance on the standard Penn Treebank Stanford", "startOffset": 176, "endOffset": 195}, {"referenceID": 10, "context": "We address this issue in \u00a75 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function, inspired by the notion of \u201csoft targets\u201d (Hinton et al., 2015).", "startOffset": 183, "endOffset": 204}, {"referenceID": 19, "context": "The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014).", "startOffset": 326, "endOffset": 393}, {"referenceID": 20, "context": ") To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hid-", "startOffset": 15, "endOffset": 39}, {"referenceID": 20, "context": ") To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hid-", "startOffset": 15, "endOffset": 172}, {"referenceID": 8, "context": ", 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2) or O(n3) runtime, respectively.", "startOffset": 78, "endOffset": 92}, {"referenceID": 23, "context": "An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003).", "startOffset": 194, "endOffset": 207}, {"referenceID": 0, "context": "Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the time of this writing employ neural networks (Andor et al., 2016).", "startOffset": 242, "endOffset": 262}, {"referenceID": 30, "context": "(Taskar et al., 2005; Tsochantaridis et al., 2005), and if S\u03b8 is (sub)differentiable, many algorithms are available.", "startOffset": 0, "endOffset": 50}, {"referenceID": 33, "context": "(Taskar et al., 2005; Tsochantaridis et al., 2005), and if S\u03b8 is (sub)differentiable, many algorithms are available.", "startOffset": 0, "endOffset": 50}, {"referenceID": 18, "context": "Variants have been used extensively in training graph-based parsers (McDonald et al., 2005b; Martins et al., 2009), which typically make use of Hamming cost, so that the inner max can be solved efficiently using FOG parsing with a slightly revised local scoring function:", "startOffset": 68, "endOffset": 114}, {"referenceID": 28, "context": "work models trained from different random starting points is a standard technique in a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al.", "startOffset": 136, "endOffset": 160}, {"referenceID": 34, "context": ", 2014) and constituency parsing (Vinyals et al., 2015).", "startOffset": 33, "endOffset": 55}, {"referenceID": 7, "context": "greedy, transition-based parser of Dyer et al. (2015), known as the stack LSTM parser, trained from a different random initial estimate.", "startOffset": 35, "endOffset": 54}, {"referenceID": 34, "context": "An alternative to building an ensemble of stack LSTM parsers in this way would be to average the softmax decisions at each timestep (transition), similar to Vinyals et al. (2015). John saw the woman with a telescope 19", "startOffset": 157, "endOffset": 179}, {"referenceID": 0, "context": "Model UAS LAS UEM Andor et al. (2016) 94.", "startOffset": 18, "endOffset": 38}, {"referenceID": 7, "context": "the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015).", "startOffset": 58, "endOffset": 77}, {"referenceID": 0, "context": "perform comparably to the best previously reported, due to Andor et al. (2016), which uses a beam (width 32) for training and decoding.", "startOffset": 59, "endOffset": 79}, {"referenceID": 7, "context": "We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.", "startOffset": 164, "endOffset": 183}, {"referenceID": 10, "context": "(2006) and Hinton et al. (2015); they were likewise motivated by a desire to create a simpler model that was cheaper to run at test time.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.", "startOffset": 131, "endOffset": 150}, {"referenceID": 7, "context": "The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors.", "startOffset": 132, "endOffset": 298}, {"referenceID": 0, "context": "Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010).", "startOffset": 185, "endOffset": 205}, {"referenceID": 0, "context": "Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training.", "startOffset": 185, "endOffset": 405}, {"referenceID": 37, "context": "0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Haji\u010d et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 32, "context": "(2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese.", "startOffset": 55, "endOffset": 79}, {"referenceID": 7, "context": "Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 15, "context": "All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gi-", "startOffset": 62, "endOffset": 81}, {"referenceID": 23, "context": "For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre (2012)", "startOffset": 120, "endOffset": 133}, {"referenceID": 19, "context": "Zhang and Nivre (2011) Transition (beam) - - 86.", "startOffset": 10, "endOffset": 23}, {"referenceID": 19, "context": "Zhang and Nivre (2011) Transition (beam) - - 86.0 84.4 - Bohnet and Nivre (2012)\u2020 Transition (beam) - - 87.", "startOffset": 10, "endOffset": 81}, {"referenceID": 19, "context": "Zhang and Nivre (2011) Transition (beam) - - 86.0 84.4 - Bohnet and Nivre (2012)\u2020 Transition (beam) - - 87.3 85.9 91.37 89.38 Chen and Manning (2014) Transition (greedy) X 91.", "startOffset": 10, "endOffset": 150}, {"referenceID": 4, "context": "4 - Dyer et al. (2015) Transition (greedy) X 93.", "startOffset": 4, "endOffset": 23}, {"referenceID": 4, "context": "4 - Dyer et al. (2015) Transition (greedy) X 93.1 90.9 87.2 85.7 - Weiss et al. (2015) Transition (beam) X 94.", "startOffset": 4, "endOffset": 87}, {"referenceID": 4, "context": "4 - Dyer et al. (2015) Transition (greedy) X 93.1 90.9 87.2 85.7 - Weiss et al. (2015) Transition (beam) X 94.0 92.0 - - - Yazdani and Henderson (2015) Transition (beam) - - - - 89.", "startOffset": 4, "endOffset": 152}, {"referenceID": 1, "context": "0 Ballesteros et al. (2015) Transition (greedy) 91.", "startOffset": 2, "endOffset": 28}, {"referenceID": 1, "context": "0 Ballesteros et al. (2015) Transition (greedy) 91.63 89.44 85.30 83.72 88.83 86.10 Ballesteros et al. (2016) Transition (greedy) X 93.", "startOffset": 2, "endOffset": 110}, {"referenceID": 1, "context": "0 Ballesteros et al. (2015) Transition (greedy) 91.63 89.44 85.30 83.72 88.83 86.10 Ballesteros et al. (2016) Transition (greedy) X 93.56 91.42 87.65 86.21 - Kiperwasser and Goldberg (2016) Transition (greedy) X 93.", "startOffset": 2, "endOffset": 190}, {"referenceID": 0, "context": "1 - Andor et al. (2016) Transition (beam) X 94.", "startOffset": 4, "endOffset": 24}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.", "startOffset": 7, "endOffset": 29}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.", "startOffset": 7, "endOffset": 82}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - Zhu et al. (2015) Reranking/blend X - - 85.", "startOffset": 7, "endOffset": 134}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - Zhu et al. (2015) Reranking/blend X - - 85.7 - - Kiperwasser and Goldberg (2016) Graph (1st order) 93.", "startOffset": 7, "endOffset": 197}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - Zhu et al. (2015) Reranking/blend X - - 85.7 - - Kiperwasser and Goldberg (2016) Graph (1st order) 93.1 91.0 86.6 85.1 - Wang and Chang (2016) Graph (1st order) X 94.", "startOffset": 7, "endOffset": 259}, {"referenceID": 24, "context": "Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals.", "startOffset": 0, "endOffset": 14}, {"referenceID": 10, "context": "Our work is closer to Hinton et al. (2015), in the sense that we do not simply compress the ensemble and hit the \u201csoft target,\u201d but also the \u201chard target\u201d at the same time10.", "startOffset": 22, "endOffset": 43}, {"referenceID": 7, "context": "We demonstrate that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state of the art accuracy on English dependency parsing.", "startOffset": 57, "endOffset": 76}, {"referenceID": 23, "context": "We also thank Juntao Yu and Bernd Bohnet for re-running the parser of Bohnet and Nivre (2012) on Chinese with gold tags.", "startOffset": 81, "endOffset": 94}], "year": 2016, "abstractText": "We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a \u201cdistillation\u201d of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable crossentropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.", "creator": "LaTeX with hyperref package"}}}