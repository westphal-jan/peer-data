{"id": "1609.08789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition", "abstract": "numerical recurrent signaling neural networks ( rnns ) have previously shown seemingly clear conceptual superiority in multimedia sequence modeling, particularly the simple ones with gated signaling units, as such as long short - term memory ( lstm ) and gated transfer recurrent unit ( gru ). conversely however, the dynamic consistency properties behind the remarkable encoding performance also remain as unclear. in implementing many applications, just e. g., with automatic graded speech coding recognition ( asr ). this analytical paper especially employs visualization therapy techniques to study individually the repetitive behavior of common lstm and gru when primarily performing speech recognition tasks. our experiments nevertheless show today some slightly interesting patterns in evaluating the gated link memory, and like some of those them have only inspired simple yet surprisingly effective patch modifications on the network edge structure. we report two amounts of such such modifications : ( 1 ) straightforward lazy cell graphs update topology in standard lstm, and ( 2 ) simple shortcut graph connections for residual learning. both modifications lead to yielding more tightly comprehensible and powerful nonlinear networks.", "histories": [["v1", "Wed, 28 Sep 2016 06:26:16 GMT  (995kb,D)", "http://arxiv.org/abs/1609.08789v1", "Submitted to ICASSP 2017"], ["v2", "Mon, 26 Dec 2016 09:25:14 GMT  (995kb,D)", "http://arxiv.org/abs/1609.08789v2", "ICASSP 2017"], ["v3", "Mon, 27 Feb 2017 02:07:34 GMT  (2135kb,D)", "http://arxiv.org/abs/1609.08789v3", "ICASSP 2017"]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["zhiyuan tang", "ying shi", "dong wang", "yang feng", "shiyue zhang"], "accepted": false, "id": "1609.08789"}, "pdf": {"name": "1609.08789.pdf", "metadata": {"source": "CRF", "title": "MEMORY VISUALIZATION FOR GATED RECURRENT NEURAL NETWORKS IN SPEECH RECOGNITION", "authors": ["Zhiyuan Tang", "Ying Shi", "Dong Wang", "Yang Feng", "Shiyue Zhang"], "emails": ["tangzy@cslt.riit.tsinghua.edu.cn", "shiying@cslt.riit.tsinghua.edu.cn", "fengyang@cslt.riit.tsinghua.edu.cn", "zhangsy@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "Index Terms\u2014 long short-term memory, gated recurrent unit, visualization, residual learning, speech recognition"}, {"heading": "1. INTRODUCTION", "text": "Deep learning has gained brilliant success in a wide spectrum of research areas including automatic speech recognition (ASR) [1]. Among various deep models, recurrent neural network (RNN) is in particular interesting for ASR, partly due to its capability of modeling the complex temporal dynamics in speech signals as a continuous state trajectory, which essentially overturns the long-standing hidden Markove model (HMM) that describes the dynamic properties of speech signals as discrete state transition. Promising results have been reported for the RNN-based ASR [2\u20134]. A known issue of the vanilla RNN model is that training the network is generally difficult, largely attributed to the gradient vanishing and explosion problem. Additionally, the vanilla RNN model tends to forget things quickly. To solve these problems, a gated memory mechanism was proposed by researchers, leading to gated RNNs that rely on a few trainable gates to select the most important information to receive, memorize and propagate. Two widely used gated RNN structures are the long short-term memory (LSTM), proposed by Hochreiter [5], and the gated recurrent unit (GRU), proposed recently by Cho et al. [6]. Both of the two structures have delivered promising performance in ASR [4, 7].\nDespite the success of gated RNNs, what has happened in the gated memory at run-time remains unclear in speech\nrecognition. This prevents us from a deep understanding of the gating mechanism, and the relative advantage of different gated units can be understood neither intuitively nor systematically. In this paper, we utilize the visualization technique to study the behavior of gated RNNs when performing ASR. The focus is on the evolution of the gated memory. We are more interested in the difference of the two popular gated RNN units, LSTM and GRU, in terms of duration of memorization and quality of activation patterns. With visualization, the behavior of a gated RNN can be better understood, which in return may inspire ideas for more effective structures. This paper reports two simple modifications inspired by the visualization results, and the experiments demonstrate that they do result in models that are not only more powerful but also more comprehensible.\nThe rest of the paper is organized as follows: Section 2 describes some related work, and Section 3 presents the experimental settings. The visualization results are shown in Section 4, and two modifications inspired by the visualization results are presented in Section 5. The entire paper is concluded by Section 6."}, {"heading": "2. RELATED WORK", "text": "Visualization has been used in several research areas to study the behavior of neural models. For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310]. Natural language processing (NLP) is another area where visualization has been widely utilized. Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].\nIn speech recognition (and other speech processing tasks), visualization has not been employed as much as in CV and NLP, partly because displaying speech signals as visual patterns is not as straightforward as for images and text. The only work we know for RNN visualization in ASR was conducted by Miao et al. [15], which studied the input and forget gates of an LSTM, and found they are correlated. The visualization analysis presented in this paper differs from Miao\u2019s work in that our analysis is based on comparative study, which identifies the most important mechanism for good ASR per-\nar X\niv :1\n60 9.\n08 78\n9v 1\n[ cs\n.L G\n] 2\n8 Se\np 20\n16\nformance by comparing the behavior of different gated RNN structures (LSTM and GRU), in terms of activation patterns and temporal memory traces.\nComparative analysis for LSTM and GRU has been conducted by Chung et al. [16]. This paper is different from Chung\u2019s work in that we compare the two structures by visualization rather than by reasoning. Moreover, our analysis focuses on group behavior of individual units (activation pattern), rather than an all-in-one performance."}, {"heading": "3. EXPERIMENTAL SETUP", "text": "We first describe the LSTM and GRU structures whose behaviors will be visualized in the following sections, and then describe the settings of the ASR system that the visualization is based on."}, {"heading": "3.1. LSTM and GRU", "text": "We choose the LSTM structure described by Chung in [16], as it has shown good performance for ASR. The computation is as follows:\nit = \u03c3(Wixxt +Wimmt\u22121 + Vicct\u22121)\nft = \u03c3(Wfxxt +Wfmmt\u22121 + Vfcct\u22121)\nct = ft ct\u22121 + it g(Wcxxt +Wcmmt\u22121) ot = \u03c3(Woxxt +Wommt\u22121 + Vocct)\nmt = ot h(ct).\nIn the above equations, the W and V terms denote weight matrices, where V \u2019s are diagonal. xt is the input symbol; it, ft, ot represent respectively the input, forget and output gates; ct is the cell and mt is the unit output. \u03c3(\u00b7) is the logistic sigmoid function, and g(\u00b7) and h(\u00b7) are hyperbolic activation functions. denotes element-wise multiplication. We ignore bias vectors in the formula for simplification.\nGRU was introduced by Cho in [6]. It follows the same idea of information gating as LSTM, but uses a simpler structure. The computation is as follows:\nit = \u03c3(Wixxt +Wicct\u22121)\nft = 1\u2212 it ot = \u03c3(Woxxt +Wocct\u22121)\nmt = ot ct\u22121 ct = ft ct\u22121 + it g(Wcxxt +Wcmmt). (1)"}, {"heading": "3.2. Speech recognition task", "text": "Our experiments are conducted on the WSJ database whose profile is largely standard: 37, 318 utterances for model training and 1, 049 utterances (involving dev93, eval92 and eval93) for testing. The input feature is 40-dimensional Fbanks, with a symmetric 2-frame window to splice neighboring frames. The number of recurrent layers varies from 1 to 6, and the number of units in each hidden layer is set\nto 512. The units may be LSTM or GRU. The output layer consists of 3, 377 units, equal to the total number of Gaussian components in the conventional GMM system used to bootstrap the RNN model.\nThe Kaldi toolkit [17] is used to conduct the model training and performance evaluation, and the training process largely follows the WSJ s5 nnet3 recipe. The natural stochastic gradient descent (NSGD) algorithm [18] is used to train the model. The results in terms of word error rate (WER) are reported in Table 1, where \u2018LSTM\u2019 denotes the system with LSTMs as the recurrent units, and \u2018GRU\u2019 denotes the system with GRUs as the recurrent units. We can observe that the RNNs based on GRU units perform slightly better than the one based on LSTM units."}, {"heading": "4. VISUALIZATION", "text": "This section presents some visualization results. Due to the limited space, our emphasis is put on the comparison between LSTM and GRU. More detailed results and analysis can be found in the associated technical report [19]."}, {"heading": "4.1. Activation patterns", "text": "The first experiment investigates how different gated RNNs encode information in different ways. For both LSTM and GRU RNNs, 50 units are randomly selected from each hidden layer, and for each unit, the distribution of the cell values ct on 500 utterances is computed. The results are shown in Fig. 1 for the LSTM and GRU RNNs respectively. Due to the limited space, only the first and fourth layers are presented. For LSTM, we reset irregular values (smaller than \u221210 or bigger than 10) to \u221210 or 10, for better visualization. It can be observed that most of the cell values in LSTM concentrate on zero values, and the concentration decreases in the higher-level layer. This pattern suggests that LSTM relies on great positive or negative cell values of some units to represent information. In contrast, most of the cells in GRU concentrate on \u22121 or +1, and this pattern is more clear for the higher-level layer. This suggests that GRU relies on the contrast among cell values of different units to encode information. This difference in activation patterns suggests that information in GRU is more distributed than in LSTM. We conjecture that this may lead to a more compact model with a better parameter sharing.\nA related observation is that the activations of LSTM cells\nare unlimited, and the absolute values of some cells are rather large. For GRU, the cell values are strictly constrained with in (\u22121,+1). This can be also derived from Eq. (1): since ft and it are both positive and less than 1, g(\u00b7) is between (\u22121,+1), if a cell is initialized by a value between (\u22121,+1), the cell will remain in this range. The constrained range of values is an advantage for model training, as it may partly avoid abnormal gradients that often hinder RNN training."}, {"heading": "4.2. Temporal trace", "text": "The second experiment investigates the evolution of the cell activations when performing recognition. This is achieved by drawing the cell vectors of all the frames using the t-SNE tool [20] when decoding an utterance. The results are shown in Fig. 2, where the temporal traces for the four layers are drawn in the plots from top to bottom. An interesting observation is that the traces are much more smooth with LSTM than with GRU. This indicates that LSTM tends to remember more than GRU: with a long-term memory, the novelty of the current time is largely averaged out by the past memory, leading to a smooth temporal trace. For GRU, new experience is quickly adopted and so the memory tends to change drastically. When comparing the memory traces at different layers, it can be seen that for GRU, the traces become more smooth at higher-level layers, whereas this trend is not clear for LSTM. This suggests that GRU can trade off innovation and memorization at different layers: at low-level layers, it concentrates on innovation, while at high-level layers, memorization becomes more important. This is perhaps an advantage and is analog to our human brain where the low-level features change abruptly while the high-level information keeps evolving gradually."}, {"heading": "4.3. Memory robustness", "text": "The third experiment tests the robustness of LSTM and GRU with noise interruptions. Specially, during the recognition,\na noise segment is inserted into the speech stream, and we observe the influence of this noise segment by visualizing the difference in cell values caused by the noise insertion. The results are shown in Fig. 3.\nIt can be seen that both units accumulate longer memory at higher-level layers, and GRU is more robust than LSTM in noisy conditions. With LSTM, the impact of the noise lasts almost till the end on some cells, even at the final layer for which the units are supposed to be noise robust. With GRU, the impact lasts just a few frames. This demonstrates a big advantage of GRU, and double confirms the observation in the second experiment that GRU remembers less than LSTM."}, {"heading": "5. APPLICATION TO STRUCTURE DESIGN", "text": "The visualization results shown in the previous section demonstrate that LSTM and GRU possess different properties in both information encoding and temporal evolution. By these differences, it is not easy to tell which model is better in a particular task. In speech recognition, the experimental results in Section 3.2 seemingly demonstrate that GRU is more suitable. This can be explained by the fact that speech signals are pseudo-stationary and typical durations of phones are not longer than 50 frames. This means that shorter\nmemory is likely an advantage, particularly when the noise robustness is considered. Inspired by these observations, we introduce some modifications to LSTM and/or GRU, both resulting in performance gains."}, {"heading": "5.1. Lazy cell update", "text": "A difference between LSTM and GRU, as shown in Section 3.1, is that GRU updates cells as the final step, while LSTM updates cells before computing output gates. To study the impact of the lazy update with GRU, we reorder the computation in LSTM as shown in Fig. 4 (a). The recognition results are presented in Table 2, and the temporal trace with lazy update is shown in Fig. 5 (a). Note that only the final LSTM layer has been modified.\nFrom the results, it can be seen that the lazy update does improve performance of LSTM. From the temporal trace, it seems that the modified LSTM behaves more like a GRU: the trace is less smooth, allowing quicker adoption of new input. This demonstrates the short-memory behavior of GRU is possibly an important factor for the good performance, and this behavior is closely related to the lazy cell update."}, {"heading": "5.2. Shortcut connections for residual learning", "text": "Another modification is inspired by the visualization result that the gates at high-level layers show a similar pattern [12]. This implies that the cells in high-level layers are mostly\nlearned by residual. This is also confirmed by recent research on residual net [21]. We borrow this idea and add explicit shortcut connections alongside the gated units, so that the main path is enforced to learn residual. This is shown in Fig. 4 (b).\nThe results with the residual learning are shown in Table 3, and the temporal traces are shown in Fig 5 (b)(c). These results show that adding shortcut connections indeed introduces consistent performance gains with both LSTM and GRU. The temporal traces at different layers seem more consistent (note that for t-SNE, only the topological relations are important). This is particularly evident for GRU, where the third layer now can remember some short-time events as well. This is expected, as the information flow is quicker and easier with the shortcut connections."}, {"heading": "6. CONCLUSION", "text": "This paper presented some visualization results for gated RNNs, and in particular focused on comparison between LSTM and GRU. The results show that the two gated RNNs use different ways to encode information and the information in GRU is more distributed. Moreover, LSTM possesses a long-term memory but it is also noise sensitive. Inspired by these observations, we introduced two modifications to enhance gated RNNs: lazy cell update and short connections for residual learning, and both provide interesting performance improvement. Future work will compare neural models in different categories, e.g., TDNN and RNN."}, {"heading": "7. REFERENCES", "text": "[1] Li Deng and Dong Yu, \u201cDeep learning: Methods and applications,\u201d Foundations and Trends in Signal Processing, vol. 7, no. 3-4, pp. 197\u2013387, 2013.\n[2] Alex Graves, A-R Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.\n[3] Alex Graves and Navdeep Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in Proceedings of the 31st International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.\n[4] Hasim Sak, Andrew Senior, and Franc\u0327oise Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.\n[5] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[6] Kyunghyun Cho, Bart Van Merrie\u0308nboer, Dzmitry Bahdanau, and Yoshua Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014.\n[7] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al., \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d arXiv preprint arXiv:1512.02595, 2015.\n[8] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent, \u201cVisualizing higher-layer features of a deep network,\u201d University of Montreal, vol. 1341, 2009.\n[9] Matthew D Zeiler and Rob Fergus, \u201cVisualizing and understanding convolutional networks,\u201d in European Conference on Computer Vision. Springer, 2014, pp. 818\u2013 833.\n[10] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman, \u201cDeep inside convolutional networks: Visualising image classification models and saliency maps,\u201d arXiv preprint arXiv:1312.6034, 2013.\n[11] Michiel Hermans and Benjamin Schrauwen, \u201cTraining and analysing deep recurrent neural networks,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.\n[12] Andrej Karpathy, Justin Johnson, and Li Fei-Fei, \u201cVisualizing and understanding recurrent networks,\u201d arXiv preprint arXiv:1506.02078, 2015.\n[13] A\u0301kos Ka\u0301da\u0301r, Grzegorz Chrupa\u0142a, and Afra Alishahi, \u201cRepresentation of linguistic form and function in recurrent neural networks,\u201d arXiv preprint arXiv:1602.08952, 2016.\n[14] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky, \u201cVisualizing and understanding neural models in nlp,\u201d arXiv preprint arXiv:1506.01066, 2015.\n[15] Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong Zhang, and Yifan Gong, \u201cSimplifying long short-term memory acoustic models for fast training and decoding,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2284\u20132288.\n[16] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[17] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number EPFL-CONF-192584.\n[18] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur, \u201cParallel training of dnns with natural gradient and parameter averaging,\u201d arXiv preprint arXiv:1410.7455, 2014.\n[19] Zhiyuan Tang, Ying Shi, Dong Wang, Yang Feng, and Shiyue Zhang, \u201cVisualization analysis for recurrent networks,\u201d Tech. Rep., 2016, http://cslt.riit.tsinghua.edu.cn/mediawiki/images/6/6a/Visual.pdf.\n[20] Laurens van der Maaten and Geoffrey Hinton, \u201cVisualizing data using t-sne,\u201d Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015."}], "references": [{"title": "Deep learning: Methods and applications", "author": ["Li Deng", "Dong Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3-4, pp. 197\u2013387, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "A-R Mohamed", "Geoffrey Hinton"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "University of Montreal, vol. 1341, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 818\u2013 833.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1506.02078, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi"], "venue": "arXiv preprint arXiv:1602.08952, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1506.01066, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Simplifying long short-term memory acoustic models for fast training and decoding", "author": ["Yajie Miao", "Jinyu Li", "Yongqiang Wang", "Shi-Xiong Zhang", "Yifan Gong"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2284\u20132288.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number EPFL-CONF-192584.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualization analysis for recurrent networks", "author": ["Zhiyuan Tang", "Ying Shi", "Dong Wang", "Yang Feng", "Shiyue Zhang"], "venue": "Tech. Rep., 2016, http://cslt.riit.tsinghua.edu.cn/mediawiki/images/6/6a/Visual.pdf.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning has gained brilliant success in a wide spectrum of research areas including automatic speech recognition (ASR) [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Promising results have been reported for the RNN-based ASR [2\u20134].", "startOffset": 59, "endOffset": 64}, {"referenceID": 2, "context": "Promising results have been reported for the RNN-based ASR [2\u20134].", "startOffset": 59, "endOffset": 64}, {"referenceID": 3, "context": "Promising results have been reported for the RNN-based ASR [2\u20134].", "startOffset": 59, "endOffset": 64}, {"referenceID": 4, "context": "Two widely used gated RNN structures are the long short-term memory (LSTM), proposed by Hochreiter [5], and the gated recurrent unit (GRU), proposed recently by Cho et al.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Both of the two structures have delivered promising performance in ASR [4, 7].", "startOffset": 71, "endOffset": 77}, {"referenceID": 6, "context": "Both of the two structures have delivered promising performance in ASR [4, 7].", "startOffset": 71, "endOffset": 77}, {"referenceID": 7, "context": "For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310].", "startOffset": 231, "endOffset": 237}, {"referenceID": 8, "context": "For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310].", "startOffset": 231, "endOffset": 237}, {"referenceID": 9, "context": "For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310].", "startOffset": 231, "endOffset": 237}, {"referenceID": 10, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 11, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 12, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 13, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 14, "context": "[15], which studied the input and forget gates of an LSTM, and found they are correlated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We choose the LSTM structure described by Chung in [16], as it has shown good performance for ASR.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "GRU was introduced by Cho in [6].", "startOffset": 29, "endOffset": 32}, {"referenceID": 16, "context": "The Kaldi toolkit [17] is used to conduct the model training and performance evaluation, and the training process largely follows the WSJ s5 nnet3 recipe.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "The natural stochastic gradient descent (NSGD) algorithm [18] is used to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "More detailed results and analysis can be found in the associated technical report [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "This is achieved by drawing the cell vectors of all the frames using the t-SNE tool [20] when decoding an utterance.", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Another modification is inspired by the visualization result that the gates at high-level layers show a similar pattern [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "This is also confirmed by recent research on residual net [21].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.", "creator": "LaTeX with hyperref package"}}}