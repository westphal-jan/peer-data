{"id": "1609.09194", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Multi Model Data mining approach for Heart failure prediction", "abstract": "developing effective predictive value modelling solutions for ems risk sensitivity estimation is extremely challenging yet in health - trauma care informatics. operational risk estimation therefore involves strategic integration of existing heterogeneous clinical sources having strongly different probability representation from different health - injured care provider scenarios making the ongoing task increasingly too complex. such newer sources are these typically seemingly voluminous, cause diverse, robust and significantly nonlinear change over the time. therefore, distributed and parallel computing management tools more collectively termed online big flow data tools are in need which authors can synthesize symptoms and assist the physician performing to make right clinical decisions. in this work simultaneously we propose multi - model predictive problem architecture, facilitating a fundamental novel approach working for combining the optimal predictive ability of multiple models designing for better failure prediction accuracy. we demonstrate the effectiveness test and resolving efficiency of the proposed work purely on data from original framingham catastrophic heart prevention study. given results we show assurance that supporting the proposed multi - model predictive architecture is able to provide better accuracy than best model approach. by similarly modelling the likelihood error of predictive models we are able to choose sub set of models equally which yields accurate correlation results. similarly more information was modelled translating into system details by multi - monitor level sensor mining which thus has resulted sometimes in higher enhanced outcome predictive accuracy.", "histories": [["v1", "Thu, 29 Sep 2016 03:53:22 GMT  (168kb)", "http://arxiv.org/abs/1609.09194v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["priyanka h u", "vivek r"], "accepted": false, "id": "1609.09194"}, "pdf": {"name": "1609.09194.pdf", "metadata": {"source": "CRF", "title": "HEART FAILURE PREDICTION", "authors": ["Priyanka H U"], "emails": [], "sections": [{"heading": null, "text": "DOI : 10.5121/ijdkp.2016.6503 31\nDeveloping predictive modelling solutions for risk estimation is extremely challenging in health-care informatics. Risk estimation involves integration of heterogeneous clinical sources having different representation from different health-care provider making the task increasingly complex. Such sources are typically voluminous, diverse, and significantly change over the time. Therefore, distributed and parallel computing tools collectively termed big data tools are in need which can synthesize and assist the physician to make right clinical decisions. In this work we propose multi-model predictive architecture, a novel approach for combining the predictive ability of multiple models for better prediction accuracy. We demonstrate the effectiveness and efficiency of the proposed work on data from Framingham Heart study. Results show that the proposed multi-model predictive architecture is able to provide better accuracy than best model approach. By modelling the error of predictive models we are able to choose sub set of models which yields accurate results. More information was modelled into system by multi-level mining which has resulted in enhanced predictive accuracy.\nKEYWORDS\nMulti model prediction, Framingham data, Hadoop, Clustering and Classification."}, {"heading": "1. INTRODUCTION", "text": "Present clinical and pharmaceutical environment are data intensive. Large amounts of data such as patient\u2019s narratives, scan reports, clinical, laboratory tests, and hospital administrative data are being produced routinely [1].This clinical data is digitized as computer records than maintaining it in the form of physical files. This diversified information may be from different systems and geographically separated too. 80% of clinically relevant data is unstructured [2]. This data is stored in multiple repositories as individual EMRs (Electronic Medical Records) in clinical laboratories and scanning image systems, they are also maintained as case files which consists of physician notes, medical correspondence, claims, CRM (Clinical Record Management) systems and finance. Patient care can be greatly improved by improving the means to access this valuable data, by improving the methodologies to parse and filtering this data into clinical and advanced analytics.\nThe main motivation behind digitization of health records is to lower the cost of health-care and reduce the number of preventable errors. But sheer amount of data collected, poses new challenges. Physicians are often overloaded from information gathered by various tools displaying large and irrelevant information. The issue is related to presenting the relevant information to the physician. The challenge lies in making patient data easily searchable and accessible, synthesize and assist the physician to make right clinical decisions. The challenges in working with clinical data are to integrate heterogeneous, multi standard data and standardize the meaning and representation of the integrated data.\nLooking into the insight of big data, health-care falls into this category because of the large and complex data that is generated every single day. This data is difficult to manage by the traditional software and the hardware. By gaining knowledge in form insights provided by big data analytics significant improvement can be brought at lower costs. Physicians and doctors will be able to make better informed decision by exploiting explosion in data to extract insights [2] and thus making big data analytic application a current and trending research topic to work for.\nThe data intensive applications need distributed processing model. Apache Hadoop has a map reduce framework that processes the data of such applications. Azure HDInsight provides a software framework through which one can manage, analyse and report on big data. It deploys and provisions Apache Hadoop clusters in the cloud infrastructure. The Hadoop core provides a software framework to build applications that utilize the Hadoop Distributed File System (HDFS), and a simple Map-reduce programming model to process and analyse, in parallel, the data stored in this distributed system. Today, one of the major causes of death in adults with age greater than 65 is heart failure [3]. In our experiment we observe that a 1500 patient data-set with 14 attributes and with the single model prediction accuracy was approximately 82%. Whereas, with 25 different predictive models processed parallel, on the same dataset, 86% accuracy was achieved. This demonstrates the effectiveness of our methodology.\nThe rest of the paper is divided in following sections: in contribution section we will explain the novelty of the proposed methodology, detailed explanation of the multi model approach is given in the section Proposed methodology, effectiveness of the proposed multi model approach is observed in Results and discussion section, related work present in the literature are discussed in Related work section."}, {"heading": "2. RELATED WORK", "text": "A number of researches have been undertaken in the field of healthcare. There is a vast amount of data which is unused. This data is growing rapidly in terms of size, complexity and speed of generation [4]. This paper presents a review of various algorithms from 1994-2013 necessary for handling such large data set. Definition of the process of generating a context-based view of the patient\u2019s health record is mentioned in [5]. It can be summarized in three steps 1) aggregate the patient data from separate clinical sources; 2) structure the patient record to identify problems, findings, and attributes reported in clinical reports, and map them to available knowledge sources; and 3) generate a tailored display based on annotations provided by the knowledge sources. But their work, says that the disease model may not fully meet the information needs of a physician performing a specific task. All the data elements displayed may be irrelevant.\nData Mining and Decision Making becomes extremely inefficient because of the variability in the stored records or accuracy of an individual decision making algorithm. The efficiency of individual algorithm has non zero variability. There can be many ways combining the predictions of which averaging is one method .While combining the different predictions, output may not be better than the best algorithm used. Many reviews refer to Dasarathy and Sheela\u2019s 1979 work as one of the earliest example of combined decision systems [6], with their ideas decision makingbased algorithms, extending the boosting concept to multiple class and regression problems [7]. On partitioning the features using multiple algorithms. About a decade later, Hansen and Salamon showed that decision system of similarly configured neural networks can be used to improve classification performance [8]. However, it was Schapire\u2019s work that demonstrated through a procedure he named boosting that a strong classifier with an arbitrarily low error on a binary classification problem, can be constructed from a set of decision making algorithms, the error of any of which is merely better than that of random guessing [9]. The theory of boosting provided the foundation for the subsequent suite of AdaBoost algorithms.\nWith all this research, we justify that heart failure prediction using big data analytics is one of the most interesting and challenging tasks. The shortage of domain specialists and high number of wrongly diagnosed cases has necessitated the need to develop a fast and efficient detection system. We realize the need of efficient combining strategy of predictions generated by different algorithms. The strengths of parallel programming and Map reduce strategy are utilized through this study."}, {"heading": "3. PROPOSED METHODOLOGY", "text": "Most of the heart failure predictive systems [10, 11] follow best model approach, they either tune the model or transform the available data for the better performance of the predictive systems. Such systems involve complex mathematical models and are difficult to convert to optimized applications; they lack the generality where they can be easily converted to a full-fledged product. Data transformation may result in either addition of redundant information or loss of valuable information from the data. In the proposed multi model approach we try bring in the required generality, by using models in their absolute form. Open source data mining tools are used to generate such predictive models. We employ both classification and clustering techniques with an aim to model more information into the system. Classification models are built for prediction and error of these models are clustered which helps in deciding the participation of models in the prediction. The proposed methodology was implemented in two phase, phase 1: development of predictive models and consolidation of the predictions using static weights, phase 2: development of clustering based error model which would have dual purpose of deciding the participation of model in the prediction and calculation of the dynamic weight.\nFigure 1 shows the architecture of Multi model approach in its full capacity where in could be adopted as product. We briefly explain each component here. The patient profile database is the repository or the warehouse consisting of medical history of patients, many standards like CCD (continuity of care document) [16], DICOM [17], have evolved which efficiently represents the patients\u2019 medical condition over a period of time. These records are cleaned, parsed and transformed to suite mining activity before storing to warehouse. In our implementation we assume required dimensions are already extracted. The modules are required for cleaning, parse transform are not implemented.\nAs explained above, Multi model prediction approach was implemented in two phase. In phase one the impact of participating model in final prediction will be fixed, i.e., the predictive weight assigned to each model in final prediction is simply 1\u2044 n, where n is the number of predictive models used. The final prediction with static weights (\u03c1m) is given by equation 1, where \u03c1i is the prediction made by individual prediction model.\nIn this approach the effect of poorly performing models will nullify the effect of correctly predicting models. The aim of phase two implementation is to provide a model filtering process which will select only subset of better of performing model and yield to better predictive accuracy.\nThe erroneous predictions made by models are used as feedback to determine the most suitable subset of participating models, which are likely to predict accurately. For each predicting model the training set is divided into four error clusters C00, C01, C10, C11, Table 1 gives explanation of each cluster. Under testing, each tuple ti whose class is to predicted, distance of ti from each of the error clusters is calculated which gives us a distance vector < d00, d01, d10, d11 >, which determines the participation of given model in final prediction and also the impact (weight) the model\u2019s prediction carries in the final prediction. Listing 1 explains the logic which determines the participation of model.\nListing 1. Logic to determine participation of model in final prediction\nif \u03c1i \u2248 0 then,\nif d00 < d01 then i th model will participate in final prediction\nif \u03c1i \u2248 1 then,\nif d11 < d10 then i th model will participate in final prediction\nOnce we have a subset of participating models and their predicted class along with the distance vector Vi, for each class, we employ inverse distance function [17] given by equation 2 to determine the weight of participating model (wt.di -1), these weights are then normalized between values 0 and 1 as given by equation 3, which forms the dynamic weight used for final prediction. Final prediction is given by equation 4. Where \u03c1final is the final prediction made by proposed multi model approach. One of the clear enhancement to proposed model is increasing the number of clusters in each cluster category i.e., clusters C00, C01, C10, C11 will represent set of clusters rather than single cluster, doing so will greatly enhance the efficiency of model to determine the participating models and their dynamic weights. This work is considered under future scope."}, {"heading": "4. RESULTS AND DISCUSSION", "text": "Effectiveness of Multi model prediction was tested on the Framingham [14] dataset consisting of 1500 patient records. The same is a subset of data collected from the Framingham study and includes lab/clinical data of 1500 patients. Table 2 shows a snapshot of the Framingham dataset, the main aim of the multi model is to predict the class field in the data, i.e., to determine whether the patient will be alive or dead. More explanation of various fields can be found in [14]."}, {"heading": "Id Sex Age FRW SBP DBP CHOL CIG CHD Class", "text": "For all model creation, Weka, a data mining tool was used [15], with 75% as training data and 25% as test data. The proposed multi model approach was implemented as Map Reduce streaming jobs suitable for running on Hadoop framework [16] under C# .Net framework. The proposed multi model approach is compared with best model approach. 25 prediction models were trained on the dataset, with best model being Regression by discretization providing predictive efficiency of 82.42%. Model efficiency is given by equation 1. Table 3 depicts various predictive models trained using Weka tool along with their predictive efficacy.\nAs explained in proposed methodology section, Multi model prediction approach was implemented in two phase. In phase one where the each predictive models was given a static weight of 0.04 (which is 1/25 models), under such setup the multi model predictor\u2019s efficiency was observed to be 84.36%. In phase two implementation, where a subset of model was chosen using the process explained in proposed methodology, under such a setup the multi model predictors efficiency was observed to be 85.87%. Table 4 consolidates and compares the efficiency of proposed model, from the observed results we can say that proposed multi model approach out performs the best model approach."}, {"heading": "5. CONCLUSIONS", "text": "In this paper we analysed a generic architecture for clinical decision making and predictive modelling system that feeds on already existing information management system or enterprise healthcare data warehouse. Results show that the proposed multi model predictive architecture is able to provide better accuracy than best model approach. By modelling the error of predictive models we are able choose sub set of models which yields accurate results. We were able to model more information into system by multi-level mining which has resulted in enhanced predictive accuracy."}], "references": [{"title": "A Case-based Retrieval System using Natural Language Processing and Population-based Visualization.", "author": ["Hsu", "William", "Ricky K. Taira", "Fernando Vi\u00f1uela", "Alex AT Bui"], "venue": "In Healthcare Informatics, Imaging and Systems Biology (HISB),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Big data analytics in healthcare: promise and potential.", "author": ["Raghupathi", "Wullianallur", "Viju Raghupathi"], "venue": "Health Information Science and Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Big data solutions for predicting risk-of-readmission for congestive heart failure patients.", "author": ["Zolfaghar", "Kiyana", "Naren Meadem", "Ankur Teredesai", "Senjuti Basu Roy", "Si-Chi Chin", "Brian Muckian"], "venue": "In Big Data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Algorithm and approaches to handle large Data-A Survey.\" arXiv preprint", "author": ["Yadav", "Chanchal", "Shuliang Wang", "Manoj Kumar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Contextbased electronic health record: toward patient specific healthcare.\" IEEE Transactions on information technology in biomedicine", "author": ["Hsu", "William", "Ricky K. Taira", "Suzie El-Saden", "Hooshang Kangarloo", "Alex AT Bui"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "A composite classifier system design: concepts and methodology.", "author": ["Dasarathy", "Belur V", "Belur V. Sheela"], "venue": "Proceedings of the IEEE 67, no", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1979}, {"title": "A decision-theoretic generalization of online learning and an application to boosting", "author": ["Freund", "Yoav", "Robert E. Schapire"], "venue": "European Conference on Computational Learning Theory", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Neural network ensembles.\" IEEE transactions on pattern analysis and machine intelligence", "author": ["Hansen", "Lars Kai", "Peter Salamon"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "The strength of weak learnability.\" Machine learning", "author": ["Schapire", "Robert E"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}, {"title": "Heart disease prediction system using associative classification and genetic algorithm.", "author": ["Jabbar", "M. Akhil", "Bulusu Lakshmana Deekshatulu", "Priti Chandra"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Intelligent Heart Disease Prediction Model Using Classification", "author": ["Yadav", "Pramod Kumar", "K.L. Jaiswal", "Shamsher Bahadur Patel", "D.P. Shukla"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "The clinical document architecture and the continuity of care record.", "author": ["Ferranti", "Jeffrey M", "R. Clayton Musser", "Kensaku Kawamoto", "W. Ed Hammond"], "venue": "Journal of the American Medical Informatics Association 13,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Introduction to the DICOM standard.\" European radiology", "author": ["Mildenberger", "Peter", "Marco Eichelberg", "Eric Martin"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "The WEKA data mining software: an update.", "author": ["Hall", "Mark", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "ACM SIGKDD explorations newsletter 11,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Multivariate interpolation to incorporate thematic surface data using inverse distance weighting (IDW).\" Computers", "author": ["Bartier", "Patrick M", "C. Peter Keller"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Large amounts of data such as patient\u2019s narratives, scan reports, clinical, laboratory tests, and hospital administrative data are being produced routinely [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "80% of clinically relevant data is unstructured [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "Physicians and doctors will be able to make better informed decision by exploiting explosion in data to extract insights [2] and thus making big data analytic application a current and trending research topic to work for.", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Today, one of the major causes of death in adults with age greater than 65 is heart failure [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "This data is growing rapidly in terms of size, complexity and speed of generation [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Definition of the process of generating a context-based view of the patient\u2019s health record is mentioned in [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "Many reviews refer to Dasarathy and Sheela\u2019s 1979 work as one of the earliest example of combined decision systems [6], with their ideas decision makingbased algorithms, extending the boosting concept to multiple class and regression problems [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "Many reviews refer to Dasarathy and Sheela\u2019s 1979 work as one of the earliest example of combined decision systems [6], with their ideas decision makingbased algorithms, extending the boosting concept to multiple class and regression problems [7].", "startOffset": 243, "endOffset": 246}, {"referenceID": 7, "context": "About a decade later, Hansen and Salamon showed that decision system of similarly configured neural networks can be used to improve classification performance [8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "However, it was Schapire\u2019s work that demonstrated through a procedure he named boosting that a strong classifier with an arbitrarily low error on a binary classification problem, can be constructed from a set of decision making algorithms, the error of any of which is merely better than that of random guessing [9].", "startOffset": 312, "endOffset": 315}, {"referenceID": 9, "context": "Most of the heart failure predictive systems [10, 11] follow best model approach, they either tune the model or transform the available data for the better performance of the predictive systems.", "startOffset": 45, "endOffset": 53}, {"referenceID": 10, "context": "Most of the heart failure predictive systems [10, 11] follow best model approach, they either tune the model or transform the available data for the better performance of the predictive systems.", "startOffset": 45, "endOffset": 53}, {"referenceID": 14, "context": "The patient profile database is the repository or the warehouse consisting of medical history of patients, many standards like CCD (continuity of care document) [16], DICOM [17], have evolved which efficiently represents the patients\u2019 medical condition over a period of time.", "startOffset": 173, "endOffset": 177}, {"referenceID": 14, "context": "Once we have a subset of participating models and their predicted class along with the distance vector Vi, for each class, we employ inverse distance function [17] given by equation 2 to determine the weight of participating model (wt.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "For all model creation, Weka, a data mining tool was used [15], with 75% as training data and 25% as test data.", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "Developing predictive modelling solutions for risk estimation is extremely challenging in health-care informatics. Risk estimation involves integration of heterogeneous clinical sources having different representation from different health-care provider making the task increasingly complex. Such sources are typically voluminous, diverse, and significantly change over the time. Therefore, distributed and parallel computing tools collectively termed big data tools are in need which can synthesize and assist the physician to make right clinical decisions. In this work we propose multi-model predictive architecture, a novel approach for combining the predictive ability of multiple models for better prediction accuracy. We demonstrate the effectiveness and efficiency of the proposed work on data from Framingham Heart study. Results show that the proposed multi-model predictive architecture is able to provide better accuracy than best model approach. By modelling the error of predictive models we are able to choose sub set of models which yields accurate results. More information was modelled into system by multi-level mining which has resulted in enhanced predictive accuracy.", "creator": "PScript5.dll Version 5.2.2"}}}