{"id": "1609.09481", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Fast learning rates with heavy-tailed losses", "abstract": "we study robust fast natural learning convergence rates when discovering the losses are inevitably not necessarily too bounded and it may merely have a frequency distribution consistent with more heavy empirical tails. for to further enable such analyses, assume we introduce merely two typical new conditions : ( i ) the envelope support function $ \\ sup _ { f \\ in \\ ~ mathcal { f } } | \\ ~ ell \\ @ circ f | $, where $ \\ % ell $ is ultimately the intrinsic loss zeta function and $ \\ mathcal { f } $ - is the hypothesis support class, still exists and md is $ o l ^ r $ - integrable, occurs and ( l ii ) $ \\ < ell $ j satisfies the complete multi - octave scale bernstein's vanishing condition property on $ \\ mathcal { 3 f } $. under these behavioral assumptions, we formally prove only that polynomial learning output rate faster variable than $ o ( n ^ { - t 1 / 2 2 } ) $ can almost be consistently obtained and, continuing depending on $ r $ and the simple multi - scale bernstein'theorem s threshold powers, we can somehow be arbitrarily close exponential to $ o ( true n ^ { - 1 } ) $. finally we then routinely verify by these assumptions and derive fast learning rates for the problem of autonomous vector structure quantization justified by $'k $ - means harmonic clustering with heavy - tailed distributed distributions. the robust analyses enable us to then obtain novel rapid learning rates that extend smoothly and reasonably complement existing operational results in the analytic literature from examining both theoretical and simulated practical simulations viewpoints.", "histories": [["v1", "Thu, 29 Sep 2016 19:46:13 GMT  (18kb)", "http://arxiv.org/abs/1609.09481v1", "Advances in Neural Information Processing Systems (NIPS 2016): 11 pages"]], "COMMENTS": "Advances in Neural Information Processing Systems (NIPS 2016): 11 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["vu c dinh", "lam s ho", "binh t nguyen", "duy m h nguyen"], "accepted": true, "id": "1609.09481"}, "pdf": {"name": "1609.09481.pdf", "metadata": {"source": "CRF", "title": "Fast learning rates with heavy-tailed losses", "authors": ["Vu Dinh", "Duy Nguyen", "Binh T. Nguyen"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n09 48\n1v 1\n[ st\nat .M\nL ]\n2 9\nSe p"}, {"heading": "1 Introduction", "text": "The rate with which a learning algorithm converges as more data comes in play a central role in machine learning. Recent progress has refined our theoretical understanding about setting under which fast learning rates are possible, leading to the development of robust algorithms that can automatically adapt to data with hidden structures and achieve faster rates whenever possible. The literature, however, has mainly focused on bounded losses and little has been known about rates of learning in the unbounded cases, especially in cases when the distribution of the loss has heavy tails (van Erven et al., 2015).\nMost of previous work about learning rate for unbounded losses are done in the context of density estimation (van Erven et al., 2015; Zhang, 2006a,b), of which the proofs of fast rates implicitly employ the central condition (Gru\u0308nwald, 2012) and cannot be extended to address losses with polynomial tails (van Erven et al., 2015). Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that Lr-diameter of the hypothesis class is bounded. However, results about fast learning rates were not obtained in both approaches. Fast learning rates are derived in Lecue\u0301 and Mendelson (2013) for sub-Gaussian losses and in Lecue\u0301 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions. To the best of our knowledge, no previous work about fast learning rates for heavy-tailed losses has been done in the literature.\nThe goal of this research is to study fast learning rates for the empirical risk minimizer when the losses are not necessarily bounded and may have a distribution with heavy tails.\n\u2217V Dinh was supported by DMS-1223057 and CISE-1564137 from the National Science Foundation and U54GM111274 from the National Institutes of Health. LST Ho was supported by NSF grant IIS 1251151.\nWe recall that heavy-tailed distributions are probability distributions whose tails are not exponentially bounded: that is, they have heavier tails than the exponential distribution. To enable the analyses of fast rates with heavy-tailed losses, two new assumptions are introduced. First, we assume the existence and the Lr-integrability of the envelope function F = supf\u2208F |f | of the hypothesis class F for some value of r \u2265 2, which enables us to use the results of Lederer and van de Geer (2014) on concentration inequalities for suprema of empirical unbounded processes. Second, we assume that the loss function satisfies the multi-scale Bernstein\u2019s condition, a generalization of the standard Bernstein\u2019s condition for unbounded losses, which enables derivation of fast learning rates.\nBuilding upon this framework, we prove that if the loss has finite moments up to order r large enough and if the hypothesis class satisfies the regularity conditions described above, then learning rate faster than O(n\u22121/2) can be obtained. Moreover, depending on r and the multi-scale Bernstein\u2019s powers, the learning rate can be arbitrarily close to the optimal rate O(n\u22121). We then verify these assumptions and derive fast learning rates for the k-mean clustering algorithm and prove that if the distribution of observations has finite moments up to order r and satisfies the Pollard\u2019s regularity conditions, then fast learning rate can be derived. The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings."}, {"heading": "2 Mathematical framework", "text": "Let the hypothesis class F be a class of functions defined on some measurable space X with values in R. Let Z = (X,Y ) be a random variable taking values in Z = X \u00d7 Y with probability distribution P where Y \u2282 R. The loss \u2113 : Z \u00d7 F \u2192 R+ is a non-negative function. For a hypothesis f \u2208 F and n iid samples {Z1, Z2, . . . , Zn} of Z, we define\nP\u2113(f) = EZ\u223cP [\u2113(Z, f)] and Pn\u2113(f) = 1\nn\nn \u2211\ni=1\n\u2113(Zi, f).\nFor unsupervised learning frameworks, there is no output (Y = \u2205) and the loss has the form \u2113(X, f) depending on applications. Nevertheless, P\u2113(f) and Pn\u2113(f) can be defined in a similar manner. We will abuse the notation to denote the losses \u2113(Z, f) by \u2113(f). We also denote the optimal hypothesis f\u2217 be any function for which P\u2113(f\u2217) = inff\u2208F P\u2113(f) := P \u2217\nand consider the empirical risk minimizer (ERM) estimator f\u0302n = argminf\u2208F Pn\u2113(f).\nWe recall that heavy-tailed distributions are probability distributions whose tails are not exponentially bounded. Rigorously, the distribution of a random variable V is said to have a heavy right tail if limv\u2192\u221e e\u03bbvP[V > v] = \u221e for all \u03bb > 0 and the definition is similar for heavy left tail. A learning problem is said to be with heavy-tailed loss if the distribution of \u2113(f) has heavy tails from some or all hypotheses f \u2208 F . For a pseudo-metric space (G, d) and \u01eb > 0, we denote by N (\u01eb,G, d) the covering number of (G, d); that is, N (\u01eb,G, d) is the minimal number of balls of radius \u01eb needed to cover G. The universal metric entropy of G is defined by H(\u01eb,G) = supQ logN (\u01eb,G, L2(Q)), where the supremum is taken over the set of all probability measures Q concentrated on some finite subset of G. For convenience, we define G = \u2113 \u25e6 F the class of all functions g such that g = \u2113(f) for some f \u2208 F and denote by F\u01eb a finite subset of F such that G is contained in the union of balls of radius \u01eb with centers in G\u01eb = \u2113 \u25e6 F\u01eb. We refer to F\u01eb and G\u01eb as an \u01eb-net of F and G, respectively. To enable the analyses of fast rates for learning problems with heavy-tailed losses, throughout the paper, we impose the following regularity conditions on F and \u2113. Assumption 2.1 (Multi-scale Bernstein\u2019s condition). Define F\u2217 = argminF P\u2113(f). There exist a finite partition of F = \u222ai\u2208IFi, positive constants B = {Bi}i\u2208I , constants \u03b3 = {\u03b3i}i\u2208I in (0, 1], and f\u2217 = {f\u2217i }i\u2208I \u2282 F\u2217 such that E[(\u2113(f) \u2212 \u2113(f\u2217i ))2] \u2264 Bi (E[\u2113(f)\u2212 \u2113(f\u2217i )])\u03b3i for all i \u2208 I and f \u2208 Fi.\nAssumption 2.2 (Entropy bounds). The hypothesis class F is separable and there exist C \u2265 1, K \u2265 1 such that \u2200\u01eb \u2208 (0,K], the L2(P )-covering numbers and the universal metric entropies of G are bounded as logN (\u01eb,G, L2(P )) \u2264 C log(K/\u01eb) and H(\u01eb,G) \u2264 C log(K/\u01eb). Assumption 2.3 (Integrability of the envelope function). There exists W > 0, r \u2265 C + 1 such that (\nE supg\u2208G |g|r )1/r \u2264 W .\nThe multi-scale Bernstein\u2019s condition is more general than the Bernstein\u2019s condition. This entails that the multi-scale Bernstein\u2019s condition holds whenever the Bernstein\u2019s condition does, thus allows us to consider a larger class of problems. In other words, our results are also valid with the Bernstein\u2019s condition. The multi-scale Bernstein\u2019s condition is more proper to study unbounded losses since it is able to separately consider the behaviors of the risk function on microscopic and macroscopic scales, for which the distinction can only be observed in an unbounded setting.\nWe also recall that if G has finite VC-dimension, then Assumption 2.2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition. Fast learning rate for hypothesis classes with envelope functions were studied in Lecue\u0301 and Mendelson (2012), but under a much stronger assumption that the envelope function is sub-exponential.\nUnder these assumptions, we illustrate that fast rates for heavy-tailed losses can be obtained. Throughout the analyses, two recurrent analytical techniques are worth mentioning. The first comes from the simple observation that in the standard derivation of fast learning rates for bounded losses, the boundedness assumption is used in multiple places only to provide reverse-Holder-type inequalities, where the L2-norm are upper bounded by the L1-norm. This use of the boundedness assumption can be simply relieved by the assumption that the Lr-norm of the loss is bounded, which implies\n\u2016u\u2016L2 \u2264 \u2016u\u2016(r\u22122)/(2r\u22122)L1 \u2016u\u2016 r/(2r\u22122) Lr .\nThe second technique relies on the following results of Lederer and van de Geer (2014) on concentration inequalities for suprema of empirical unbounded processes.\nLemma 2.1. If {Vk : k \u2208 K} is a countable family of non-negative functions such that E sup\nk\u2208K |Vk|r \u2264 M r \u03c32 = sup k\u2208K EV 2k and V := sup k\u2208K PnVk,\nthen for all \u03b6, x > 0, we have\nP[V \u2265 (1 + \u03b6)EV + x] \u2264 min 1\u2264l\u2264r\n(1/x)l [ ( 64/\u03b6 + \u03b6 + 7) (l/n) 1\u2212l/r M + 4\u03c3 \u221a l/n )l ] .\nAn important notice from this result is that the failure probability is a polynomial in the deviation x. As we will see later, for a given level of confidence \u03b4, this makes the constant in the convergence rate a polynomial function of (1/\u03b4) instead of log(1/\u03b4) as in sub-exponential cases. Thus, more careful examinations of the order of the failure probability are required for the derivation of any generalization bound with heavy-tailed losses."}, {"heading": "3 Fast learning rates with heavy-tailed losses", "text": "The derivation of fast learning rate with heavy tailed losses proceeds as follows. First, we will use the assumption of integrable envelope function to prove a localization-based result that allows us to reduce the analyses from the separable parametric classes F to its finite \u01eb-net F\u01eb. The multi-scale Bernstein\u2019s condition is then employed to derive a fast-rate inequality that helps distinguish the optimal hypothesis from alternative hypotheses in F\u01eb. The two results are then combined to obtain fast learning rates."}, {"heading": "3.1 Preliminaries", "text": "Throughout this section, let G\u01eb be an \u01eb-net for G in the L2(P )-norm, with \u01eb = n\u2212\u03b2 for some 1 \u2265 \u03b2 > 0. Denote by \u03c0 : G \u2192 G\u01eb an L2(P )-metric projection from G to G\u01eb. For any g0 \u2208 G\u01eb, we denote K(g0) = {|g0 \u2212 g| : g \u2208 \u03c0\u22121(g0)}. We have\n(i) the constant zero function is an element of K(g0), (ii) E[supu\u2208K(g0) |u|r] \u2264 (2W )r; and supu\u2208K(g0) \u2016u\u2016L2(P ) \u2264 \u01eb, (iii) N (t,K(g0), L2(P )) \u2264 (K/t)C for all t > 0.\nGiven a sample Z = (Z1, . . . , Zn), we denote by KZ the projection of K(g0) onto the sample Z and by D(KZ) half of the radius of (KZ , \u2016 \u00b7\u20162), that is D(KZ) = supu,v\u2208KZ \u2016u\u2212v\u2016/4. We have the following preliminary lemma, for which the proofs are provided in the Appendix.\nLemma 3.1. 2\u221a n ED(KZ) \u2264\n( \u01eb+ E supu\u2208K(g0) (Pn \u2212 P )u )\nr\u22122 2(r\u22121)\n(2W ) r 2(r\u22121) .\nLemma 3.2. Given 0 < \u03bd < 1, there exist constant C1, C2 > 0 depending only on \u03bd such that for all x > 0, if x \u2264 ax\u03bd + b then x \u2264 C1a1/(1\u2212\u03bd) + C2b. Lemma 3.3. Define\nA(l, r, \u03b2, C, \u03b1) = max { l2/r \u2212 (1\u2212 \u03b2)l + \u03b2C, [\u03b2 (1\u2212 \u03b1/2)\u2212 1/2] l + \u03b2C } . (3.1)\nAssuming that r \u2265 4C and \u03b1 \u2264 1, if we choose l = r (1\u2212 \u03b2) /2 and 0 < \u03b2 < (1 \u2212 2 \u221a\nC/r)/(2\u2212 \u03b1), (3.2) then 1 \u2264 l \u2264 r and A(l, r, \u03b2, C, \u03b1) < 0. This also holds if \u03b1 \u2265 1 and 0 < \u03b2 < 1\u2212 2 \u221a C/r."}, {"heading": "3.2 Local analysis of the empirical loss", "text": "The preliminary lemmas enable us to locally bound E supu\u2208K(g0) (Pn \u2212 P )u as follows: Lemma 3.4. If \u03b2 < (r\u22121)/r, there exists c1 > 0 such that E supu\u2208K(g0) (Pn \u2212 P )u \u2264 c1n\u2212\u03b2 for all n.\nProof. Without loss of generality, we assume that K(g0) is countable. The arguments to extend the bound from countable classes to separable classes are standard (see, for example, Lemma 12 of Mehta and Williamson (2014)). Denote Z\u0304 = supu\u2208K(g0) (Pn \u2212 P )u and let \u01eb = 1/n\u03b2, R = (R1, R2, . . . , Rn) be iid Rademacher random variables, using standard results about symmetrization and chaining of Rademacher process (see, for example, Corollary 13.2 in Boucheron et al. (2013)), we have\nnE sup u\u2208K(g0)\n(Pn \u2212 P )g \u2264 2E\n\nER sup u\u2208K(g0)\nn \u2211\nj=1\nRju(Xj)\n\n\n\u2264 24E \u222b D(KX)\u2228\u01eb\n0\n\u221a logN (t,KX , \u2016 \u00b7 \u20162)dt \u2264 24E \u222b D(KX)\u2228\u01eb\n0\n\u221a\nH ( t/ \u221a n,K(g0) ) dt,\nwhere ER denotes the expected value with respect to the random variables R1, R2, . . . , Rn. By Assumption 2.2, we deduce that\nnEZ\u0304 \u2264 C0(K,n, \u03c3, C)(\u01eb+ ED(KX)) where C0 = O( \u221a logn).\nIf we define\nx = \u01eb + EZ\u0304, b = C0\u01eb/n = O( \u221a log n/n\u03b2+1), a = C0n \u22121/2(2W ) r 2(r\u22121) /2 = O( \u221a logn/ \u221a n),\nthen by Lemma 3.1, we have x \u2264 ax(r\u22122)/(2r\u22122) + b+ \u01eb. Using Lemma 3.2, we have x \u2264 C1a2(r\u22121)/r + C2(b+ \u01eb) \u2264 C3n\u2212\u03b2,\nwhich completes the proof.\nLemma 3.5. Assuming that r \u2265 4C, if \u03b2 < 1\u2212 2 \u221a C/r, there exist c1, c2 > 0 such that for all n and \u03b4 > 0\nsup u\u2208K(g0)\nPnu \u2264 ( 9c1 + (c2/\u03b4) 1/[r(1\u2212\u03b2)] ) n\u2212\u03b2 \u2200g0 \u2208 G\u01eb\nwith probability at least 1\u2212 \u03b4.\nProof. Denote Z = supu\u2208K(g0) Pnu and Z\u0304 = supu\u2208K(g0) (Pn \u2212 P )u. We have Z = sup\nu\u2208K(g0) Pnu \u2264 Z\u0304 + sup u\u2208K(g0) Pu \u2264 Z\u0304 + sup u\u2208K(g0) \u2016u\u2016L2(P ) = Z\u0304 + \u01eb.\nApplying Lemma 2.1 for \u03b6 = 8 and x = y/n\u03b2 for Z\u0304, using the facts that\n\u03c3 = sup u\u2208Kg0\n\u221a\nE[u(X)]2 \u2264 \u01eb = 1/n\u03b2, and E[ sup u\u2208Kg0 |u|r] \u2264 (2W )r,\nwe have\nP [ Z\u0304 \u2265 9EZ\u0304 + y/n\u03b2 ] \u2264 min 1\u2264l\u2264r\ny\u2212l [ ( 46 (l/n) 1\u2212l/r n\u03b2W + 4 \u221a l/n )l ] := \u03c6(y, n).\nTo provide a union bound for all g0 \u2208 G\u01eb, we want the total failure probability \u03c6(y, n)(n\u03b2K)C \u2264 \u03b4. This failure probability, as a function of n, is of order A(l, r, \u03b2, C, \u03b1) (as define in Lemma 3.3) with \u03b1 = 2 . By choosing l = r(1 \u2212 \u03b2)/2 and \u03b2 < 1 \u2212 2 \u221a\nC/r, we deduce that there exist c2, c3 > 0 such that \u03c6(y, n)(n\n\u03b2K)C \u2264 c2/(nc3yl) \u2264 c2/yr(1\u2212\u03b2)/2. The proof is completed by choosing y = (c2/\u03b4) 2/[r(1\u2212\u03b2)] and using the fact that EZ\u0304 \u2264 c1/n\u03b2 (note that 1\u2212 2 \u221a C/r \u2264 (r \u2212 1)/r and we can apply Lemma 3.4 to get the bound).\nA direct consequence of this Lemma is the following localization-based result.\nTheorem 3.1 (Local analysis). Under Assumptions 2.1, 2.2 and 2.3, let G\u01eb be a minimal \u01eb-net for G in the L2(P )-norm, with \u01eb = n\u2212\u03b2 where \u03b2 < 1 \u2212 2 \u221a\nC/r. Then there exist c1, c2 > 0 such that for all \u03b4 > 0,\nPng \u2265 Pn(\u03c0(g)) \u2212 ( 9c1 + (c2/\u03b4) 2/[r(1\u2212\u03b2)] ) n\u2212\u03b2 \u2200g \u2208 G\nwith probability at least 1\u2212 \u03b4."}, {"heading": "3.3 Fast learning rates with heavy-tailed losses", "text": "Theorem 3.2. Given a0, \u03b4 > 0. Under the multi-scale (B, \u03b3, I)-Bernstein\u2019s condition and the assumption that r \u2265 4C, consider\n0 < \u03b2 < (1\u2212 2 \u221a C/r)/(2\u2212 \u03b3i) \u2200i \u2208 I. (3.3) Then there exist Na0,\u03b4,r,B,\u03b3 > 0 such that \u2200f \u2208 F\u01eb and n \u2265 Na0,\u03b4,r,B,\u03b3, we have P\u2113(f)\u2212 P \u2217 \u2265 a0/n\u03b2 implies \u2203f\u2217 \u2208 F\u2217 : Pn\u2113(f)\u2212 Pn\u2113(f\u2217) \u2265 a0/(4n\u03b2) with probability at least 1\u2212 \u03b4.\nProof. Define a = [P\u2113(f)\u2212 P \u2217]n\u03b2 . Assuming that f \u2208 Fi, applying Lemma 2.1 for \u03b6 = 1/2 and x = a/4n\u03b2 for a single hypothesis f , we have\nP [Pn\u2113(f)\u2212 Pn\u2113(f\u2217i ) \u2264 (P\u2113(f)\u2212 P\u2113(f\u2217i ))/4] \u2264 h(a, n) where\nh(a, n, i) = min 1\u2264l\u2264r\n(4/a)l ( 50n\u03b2 (l/n) 1\u2212l/r W + 4n\u03b2Bia \u03b3i/2/n\u03b2\u03b3i/2 \u221a l/n )l\nusing the fact that \u03c32 = E[\u2113(f) \u2212 \u2113(f\u2217i )]2 \u2264 Bi [E(\u2113(f)\u2212 \u2113(f\u2217i ))]\u03b3i = Bia\u03b3i/n\u03b2\u03b3i if f \u2208 Fi. Since \u03b3i \u2264 1, h(a, n, i) is a non-increasing function in a. Thus,\nP [Pn\u2113(f)\u2212 Pn\u2113(f\u2217i ) \u2264 (P\u2113(f)\u2212 P\u2113(f\u2217i ))/4] \u2264 h(a0, n, i).\nTo provide a union bound for all f \u2208 F\u01eb such that P\u2113(f) \u2212 P\u2113(f\u2217i ) \u2265 a0/n\u03b2, we want the total failure probability to be small. This is guaranteed if h(a0, n, i)(n\n\u03b2K)C \u2264 \u03b4. This failure probability, as a function of n, is of order A(l, r, \u03b2, C, \u03b3i) as defined in equation (3.1). By choosing r, l as in Lemma 3.3 and \u03b2 as in equation (3.3), we have 1 \u2264 l \u2264 r and A(l, r, \u03b2, C, \u03b3i) < 0 for all i. Thus, there exists c4, c5, c6 > 0 such that\nh(a0, n, i)(n \u03b2K)C \u2264 c6a\u2212c5(1\u2212\u03b3i/2)0 n\u2212c4 \u2200n, i.\nHence, when n \u2265 Na,\u03b4,r,B,\u03b3 = ( c6\u03b4a \u2212c5(1\u2212\u03b3\u0303/2) 0 )1/c4 where \u03b3\u0303 = max{\u03b3}1{a0\u22651} + min{\u03b3}1{a0<1}, we have: \u2200f \u2208 F\u01eb, P \u2113(f) \u2212 P \u2217 \u2265 a0/n\u03b2 implies \u2203f\u2217 \u2208 F\u2217, Pn\u2113(f) \u2212 Pn\u2113(f \u2217) \u2265 a0/(4n\u03b2) with probability at least 1\u2212 \u03b4.\nTheorem 3.3. Under Assumptions 2.1, 2.2 and 2.3, consider \u03b2 as in equation (3.3) and c1, c2 as in previous theorems. For all \u03b4 > 0, there exists N\u03b4,r,B,\u03b3 such that if n \u2265 N\u03b4,r,B,\u03b3, then\nP\u2113(f\u0302z) \u2264 P\u2113(f\u2217) + ( 36c1 + 1 + 4 (2c2/\u03b4) 2/[r(1\u2212\u03b2)] ) n\u2212\u03b2\nwith probability at least 1\u2212 \u03b4.\nProof of Theorem 3.3. Let F\u01eb by an \u01eb-net of F with \u01eb = 1/n\u03b2 such that f\u2217 \u2208 F\u01eb. We denote the projection of f\u0302z to F\u01eb by f1 = \u03c0(f\u0302z). For a given \u03b4 > 0, define\nA1 = { \u2203f \u2208 F : Pnf \u2264 Pn(\u03c0(f)) \u2212 ( 9c1 + (c3/\u03b4) 2/[r(1\u2212\u03b2)] ) n\u2212\u03b2 } ,\nA2 = { \u2203f \u2208 F\u01eb : Pn\u2113(\u03c0(f))\u2212 Pn\u2113(f\u2217) \u2264 a0/(4n\u03b2) and P\u2113(\u03c0(f))\u2212 P\u2113(f\u2217) \u2265 a0/n\u03b2 } ,\nwhere c1, c2 is defined as in previous theorem, a0/4 = 9c1+(c3/\u03b4) 2/[r(1\u2212\u03b2)] and n \u2265 Na0,\u03b4,r,\u03b3 . We deduce that A1 and A2 happen with probability at most \u03b4. On the other hand, under the event that A1 and A2 do not happen, we have\nPn\u2113(f1) \u2264 Pn\u2113(f\u0302z) + ( 9c1 + (c3/\u03b4) 2/[r(1\u2212\u03b2)] ) n\u2212\u03b2 \u2264 Pn\u2113(f\u2217) + a0/(4n\u03b2).\nBy definition of F\u01eb, we have P\u2113(f\u0302z) \u2264 P\u2113(f1) + \u01eb \u2264 P\u2113(f\u2217) + (a0 + 1)/n\u03b2."}, {"heading": "3.4 Verifying the multi-scale Bernstein\u2019s condition", "text": "In practice, the most difficult condition to verify for fast learning rates is the multi-scale Bernstein\u2019s condition. We derive in this section some approaches to verify the condition. We first extend the result of Mendelson (2008) to prove that the (standard) Bernstein\u2019s condition is automatically satisfied for functions that are relatively far way from f\u2217 under the integrability condition of the envelope function (proof in the Appendix). We recall that R(f) = E\u2113(f) is referred to as the risk function.\nLemma 3.6. Under Assumption 2.3, we define M = W r/(r\u22122) and \u03b3 = (r \u2212 2)/(r \u2212 1). Then, if \u03b1 > M and R(f) \u2265 \u03b1/(\u03b1\u2212M)R(f\u2217), then E(\u2113(f)\u2212\u2113(f\u2217))2 \u2264 2\u03b1\u03b3E(\u2113(f)\u2212\u2113(f\u2217))\u03b3 .\nThis allows us to derive the following result, for which the proof is provided in the Appendix.\nLemma 3.7. If F is a subset of a vector space with metric d and the risk function R(f) = E\u2113(f) has a unique minimizer on F at f\u2217 in the interior of F and\n(i) There exists L > 0 such that E(\u2113(f)\u2212 \u2113(g))2 \u2264 Ld(f, g)2 for all f, g \u2208 F .\n(ii) There exists m \u2265 2, c > 0 and a neighborhood U around f\u2217 such that R(f)\u2212R(f\u2217) \u2265 cd(f, f\u2217)m for all f \u2208 U .\nThen the multi-scale Bernstein\u2019s condition holds for \u03b3 = ((r \u2212 2)/(r \u2212 1), 2/m).\nCorollary 3.1. Suppose that (F , d) is a pseudo-metric space, \u2113 satisfies condition (i) in Lemma 3.7 and the risk function is strongly convex with respect to d, then the Bernstein\u2019s condition holds with \u03b3 = 1. Remark 3.1. If the risk function is analytic at f\u2217, then condition (ii) in Lemma 3.7 holds. Similarly, if the risk function is continuously differentiable up to order 2 and the Hessian of R(f) is positive definite at f\u2217, then condition (ii) is valid with m = 2.\nCorollary 3.2. If the risk function R(f) = E\u2113(f) has a finite number of global minimizers f1, f2, . . . , fk, \u2113 satisfies condition (i) in Lemma 3.7 and there exists mi \u2265 2, ci > 0 and neighborhoods Ui around fi such that R(f)\u2212R(fi) \u2265 cid(f, fi)mi for all f \u2208 Ui, i = 1, . . . , k, then the multi-scale Bernstein\u2019s condition holds for \u03b3 = ((r \u2212 2)/(r \u2212 1), 2/m1, . . . , 2/mk)."}, {"heading": "3.5 Comparison to related work", "text": "Theorem 3.3 dictates that under our settings, the problem of learning with heavy-tailed losses can obtain convergence rates up to order\nO ( n\u2212(1\u22122 \u221a C/r)/(2\u2212min{\u03b3}) )\n(3.4)\nwhere \u03b3 is the multi-scale Bernstein\u2019s order and r is the degree of integrability of the loss. We recall that convergence rate of O(n\u22121/(2\u2212\u03b3)) is obtained in Mehta and Williamson (2014) under the same setting but for bounded losses. (The analysis there was done under the \u03b3weakly stochastic mixability condition, which is equivalent with the standard \u03b3-Bernstein\u2019s condition for bounded losses (van Erven et al., 2015)). We note that if the loss is bounded, r = \u221e and (3.4) reduces to the convergence rate obtained in Mehta and Williamson (2014). Fast learning rates for unbounded loses are previously derived in Lecue\u0301 and Mendelson (2013) for sub-Gaussian losses and in Lecue\u0301 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions. In Lecue\u0301 and Mendelson (2013), the Bernstein\u2019s condition is not directly imposed, but is replaced by condition (ii) of Lemma 3.7 with m = 2 on the whole hypothesis class, while the assumption of sub-Gaussian hypothesis class validates condition (i). This implies the standard Bernstein\u2019s condition with \u03b3 = 1 and makes the convergence rate O(n\u22121) consistent with our result (note that for sub-Gaussian losses, r can be chosen arbitrary large). The analysis of Lecue\u0301 and Mendelson (2012) concerns about non-exact oracle inequalities (rather than the sharp oracle inequalities we investigate in this paper) and can not be directly compared with our results.\n4 Application: k-means clustering with heavy-tailed source\ndistributions\nk-means clustering is a method of vector quantization aiming to partition n observations into k \u2265 2 clusters in which each observation belongs to the cluster with the nearest mean. Formally, let X be a random vector taking values in Rd with distribution P . Given a codebook (set of k cluster centers) C = {yi} \u2208 (Rd)k, the distortion (loss) on an instant x is defined as \u2113(C, x) = minyi\u2208C \u2016x\u2212 yi\u20162 and k-means clustering method aims at finding a minimizer C\u2217 of R(\u2113(C)) = P\u2113(C) via minimizing the empirical distortion Pn\u2113(C).\nThe rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007). Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217. Little is known about the finite-sample performance of empirically designed quantizers under possibly heavy-tailed distributions. In Telgarsky and Dasgupta (2013), a convergence rate ofO(n\u22121/2+2/r) are derived, where r is the number of moments of X that are assumed to be finite. Brownlees et al. (2015) uses some robust mean estimators to replace empirical means and derives a convergence rate of O(n\u22121/2) assuming only that the variance of X is finite.\nThe results from previous sections enable us to prove that with proper setting, the convergence rate of k-means clustering for heavy-tailed source distributions can be arbitrarily close to O(1/n). Following the framework of Brownlees et al. (2015), we consider\nG = {\u2113(C, x) = min yi\u2208C \u2016x\u2212 yi\u20162, C \u2208 F = (\u2212\u03c1, \u03c1)d\u00d7k}\nfor some \u03c1 > 0 with the regular Euclidean metric. We let C\u2217, C\u0302n be defined as in the previous sections. Theorem 4.1. If X has finite moments up to order r \u2265 4k(d + 1), P has a continuous density with respect to the Lebesgue measure, the risk function has a finite number of global minimizers and the Hessian matrix of C \u2192 R(C) is positive definite at the every optimal C\u2217 in the interior of F , then for all \u03b2 that satisfies\n0 < \u03b2 < r \u2212 1 r (1\u2212 2 \u221a k(d+ 1)/r),\nthere exists c1, c2 > 0 such that for all \u03b4 > 0, with probability at least 1\u2212 \u03b4, we have R(C\u0302n)\u2212R(C\u2217) \u2264 ( c1 + 4 (c2/\u03b4) 2/r ) n\u2212\u03b2\nMoreover, when r \u2192 \u221e, \u03b2 can be chosen arbitrarily close to 1.\nProof. We have (\nE sup C\u2208F\n\u2113(C,X)r )1/r \u2264 ( 1\n2r E[\u2016X\u20162 + \u03c12]r\n)1/r\n\u2264 ( 1 2 E\u2016X\u20162r + 1 2 \u03c12r\n)1/r\n\u2264 W < \u221e,\nwhile standard results about VC-dimension of k-means clustering hypothesis class guarantees that C \u2264 k(d+ 1) (Linder et al., 1994). On the other hand, we can verify that\nE[\u2113(C,X)\u2212 \u2113(C\u2032, X)]2 \u2264 L\u03c1\u2016C \u2212 C\u2032\u201622, which validates condition (i) in Lemma 3.7. The fact that the Hessian matrix of C \u2192 R(C) is positive definite at C\u2217 prompts R(C\u0302n) \u2212 R(C\u2217) \u2265 c\u2016C\u0302n \u2212 C\u2217\u20162 for some c > 0 in a neighborhood U around any optimal codebook C\u2217. Thus, Lemma 3.6 confirms the multiscale Bernstein\u2019s condition with \u03b3 = ((r \u2212 2)/(r \u2212 1), 1, . . . , 1). The inequality is then obtained from Theorem 3.3."}, {"heading": "5 Discussion and future work", "text": "We have shown that fast learning rates for heavy-tailed losses can be obtained for hypothesis classes with an integrable envelope when the loss satisfies the multi-scale Bernstein\u2019s condition. We then verify those conditions and obtain new convergence rates for k-means clustering with heavy-tailed losses. The analyses extend and complement existing results in the literature from both theoretical and practical points of view. We also introduce a new fast-rate assumption, the multi-scale Bernstein\u2019s condition, and provide a clear path to verify the assumption in practice. We believe that the multi-scale Bernstein\u2019s condition is the proper assumption to study fast rates for unbounded losses, for its ability to separate the behaviors of the risk function on microscopic and macroscopic scales, for which the distinction can only be observed in an unbounded setting.\nThere are several avenues for improvement. First, we would like to consider hypothesis class with polynomial entropy bounds. Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015). While the condition of integrable envelope is an improvement from the condition of sub-exponential envelope previously investigated in the literature, it would be interesting to see if the rates retain under weaker conditions, for example, the assumption that the Lr-diameter of the hypothesis class is bounded (Cortes et al., 2013). Finally, the recent work of Brownlees et al. (2015); Hsu and Sabato (2016) about robust estimators as alternatives of ERM to study heavy-tailed losses has yielded more favorable learning rates under weaker conditions, and we would like to extend the result in this paper to study such estimators."}], "references": [{"title": "Individual convergence rates in empirical vector quantizer design", "author": ["Andr\u00e1s Antos", "L\u00e1szl\u00f3 Gy\u00f6rfi", "Andr\u00e1s Gy\u00f6rgy"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Antos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2005}, {"title": "The minimax distortion redundancy in empirical quantizer design", "author": ["Peter L Bartlett", "Tam\u00e1s Linder", "G\u00e1bor Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Bartlett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 1998}, {"title": "A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering", "author": ["Shai Ben-David"], "venue": "Machine Learning,", "citeRegEx": "Ben.David.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David.", "year": 2007}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "OUP Oxford,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Introduction to statistical learning theory", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "G\u00e1bor Lugosi"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Empirical risk minimization for heavy-tailed losses", "author": ["Christian Brownlees", "Emilien Joly", "G\u00e1bor Lugosi"], "venue": "The Annals of Statistics,", "citeRegEx": "Brownlees et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Brownlees et al\\.", "year": 2015}, {"title": "Relative deviation learning bounds and generalization with unbounded loss functions", "author": ["Corinna Cortes", "Spencer Greenberg", "Mehryar Mohri"], "venue": null, "citeRegEx": "Cortes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2013}, {"title": "Learning from non-iid data: Fast rates for the one-vs-all multiclass plug-in classifiers", "author": ["Vu Dinh", "Lam Si Tung Ho", "Nguyen Viet Cuong", "Duy Nguyen", "Binh T Nguyen"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "Dinh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2015}, {"title": "The safe Bayesian: learning the learning rate via the mixability gap", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings of the 23rd international conference on Algorithmic Learning Theory,", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "Fast learning from \u03b1-mixing observations", "author": ["Hanyuan Hang", "Ingo Steinwart"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Hang and Steinwart.,? \\Q2014\\E", "shortCiteRegEx": "Hang and Steinwart.", "year": 2014}, {"title": "Loss minimization and parameter estimation with heavy tails", "author": ["Daniel Hsu", "Sivan Sabato"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hsu and Sabato.,? \\Q2016\\E", "shortCiteRegEx": "Hsu and Sabato.", "year": 2016}, {"title": "General nonexact oracle inequalities for classes with a sub-exponential envelope", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2012\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2012}, {"title": "Learning sub-Gaussian classes: Upper and minimax bounds", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": null, "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2013\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2013}, {"title": "New concentration inequalities for suprema of empirical processes", "author": ["Johannes Lederer", "Sara van de Geer"], "venue": null, "citeRegEx": "Lederer and Geer.,? \\Q2020\\E", "shortCiteRegEx": "Lederer and Geer.", "year": 2020}, {"title": "Fast rates for empirical vector quantization", "author": ["Cl\u00e9ment Levrard"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Levrard.,? \\Q2013\\E", "shortCiteRegEx": "Levrard.", "year": 2013}, {"title": "Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding", "author": ["Tam\u00e1s Linder", "G\u00e1bor Lugosi", "Kenneth Zeger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Linder et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Linder et al\\.", "year": 1994}, {"title": "From stochastic mixability to fast rates", "author": ["Nishant A Mehta", "Robert C Williamson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mehta and Williamson.,? \\Q2014\\E", "shortCiteRegEx": "Mehta and Williamson.", "year": 2014}, {"title": "Obtaining fast error rates in nonconvex situations", "author": ["Shahar Mendelson"], "venue": "Journal of Complexity,", "citeRegEx": "Mendelson.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson.", "year": 2008}, {"title": "A central limit theorem for k-means clustering", "author": ["David Pollard"], "venue": "The Annals of Probability,", "citeRegEx": "Pollard.,? \\Q1982\\E", "shortCiteRegEx": "Pollard.", "year": 1982}, {"title": "Fast learning from non-iid observations", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Steinwart and Christmann.,? \\Q2009\\E", "shortCiteRegEx": "Steinwart and Christmann.", "year": 2009}, {"title": "Moment-based uniform deviation bounds for k-means and friends", "author": ["Matus J Telgarsky", "Sanjoy Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Telgarsky and Dasgupta.,? \\Q2013\\E", "shortCiteRegEx": "Telgarsky and Dasgupta.", "year": 2013}, {"title": "Fast rates in statistical and online learning", "author": ["Tim van Erven", "Peter D Gr\u00fcnwald", "Nishant A Mehta", "Mark D Reid", "Robert C Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erven et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2015}, {"title": "From \u01eb-entropy to KL-entropy: Analysis of minimum information complexity density estimation", "author": ["Tong Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}, {"title": "Information-theoretic upper and lower bounds for statistical estimation", "author": ["Tong Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}], "referenceMentions": [{"referenceID": 8, "context": ", 2015; Zhang, 2006a,b), of which the proofs of fast rates implicitly employ the central condition (Gr\u00fcnwald, 2012) and cannot be extended to address losses with polynomial tails (van Erven et al.", "startOffset": 99, "endOffset": 115}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that L-diameter of the hypothesis class is bounded.", "startOffset": 38, "endOffset": 165}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that L-diameter of the hypothesis class is bounded. However, results about fast learning rates were not obtained in both approaches. Fast learning rates are derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 38, "endOffset": 461}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that L-diameter of the hypothesis class is bounded. However, results about fast learning rates were not obtained in both approaches. Fast learning rates are derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 38, "endOffset": 519}, {"referenceID": 0, "context": "The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings.", "startOffset": 60, "endOffset": 80}, {"referenceID": 0, "context": "The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings.", "startOffset": 60, "endOffset": 99}, {"referenceID": 0, "context": "The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings.", "startOffset": 60, "endOffset": 250}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004).", "startOffset": 15, "endOffset": 62}, {"referenceID": 4, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004).", "startOffset": 15, "endOffset": 62}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.", "startOffset": 16, "endOffset": 328}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition.", "startOffset": 16, "endOffset": 478}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition. Fast learning rate for hypothesis classes with envelope functions were studied in Lecu\u00e9 and Mendelson (2012), but under a much stronger assumption that the envelope function is sub-exponential.", "startOffset": 16, "endOffset": 630}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition. Fast learning rate for hypothesis classes with envelope functions were studied in Lecu\u00e9 and Mendelson (2012), but under a much stronger assumption that the envelope function is sub-exponential. Under these assumptions, we illustrate that fast rates for heavy-tailed losses can be obtained. Throughout the analyses, two recurrent analytical techniques are worth mentioning. The first comes from the simple observation that in the standard derivation of fast learning rates for bounded losses, the boundedness assumption is used in multiple places only to provide reverse-Holder-type inequalities, where the L2-norm are upper bounded by the L1-norm. This use of the boundedness assumption can be simply relieved by the assumption that the Lr-norm of the loss is bounded, which implies \u2016u\u2016L2 \u2264 \u2016u\u2016(r\u22122)/(2r\u22122) L1 \u2016u\u2016 r/(2r\u22122) Lr . The second technique relies on the following results of Lederer and van de Geer (2014) on concentration inequalities for suprema of empirical unbounded processes.", "startOffset": 16, "endOffset": 1436}, {"referenceID": 15, "context": "The arguments to extend the bound from countable classes to separable classes are standard (see, for example, Lemma 12 of Mehta and Williamson (2014)).", "startOffset": 122, "endOffset": 150}, {"referenceID": 3, "context": "2 in Boucheron et al. (2013)), we have", "startOffset": 5, "endOffset": 29}, {"referenceID": 17, "context": "We first extend the result of Mendelson (2008) to prove that the (standard) Bernstein\u2019s condition is automatically satisfied for functions that are relatively far way from f\u2217 under the integrability condition of the envelope function (proof in the Appendix).", "startOffset": 30, "endOffset": 47}, {"referenceID": 14, "context": "We recall that convergence rate of O(n\u22121/(2\u2212\u03b3)) is obtained in Mehta and Williamson (2014) under the same setting but for bounded losses.", "startOffset": 63, "endOffset": 91}, {"referenceID": 14, "context": "We recall that convergence rate of O(n\u22121/(2\u2212\u03b3)) is obtained in Mehta and Williamson (2014) under the same setting but for bounded losses. (The analysis there was done under the \u03b3weakly stochastic mixability condition, which is equivalent with the standard \u03b3-Bernstein\u2019s condition for bounded losses (van Erven et al., 2015)). We note that if the loss is bounded, r = \u221e and (3.4) reduces to the convergence rate obtained in Mehta and Williamson (2014). Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 63, "endOffset": 451}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 66, "endOffset": 93}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 66, "endOffset": 151}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions. In Lecu\u00e9 and Mendelson (2013), the Bernstein\u2019s condition is not directly imposed, but is replaced by condition (ii) of Lemma 3.", "startOffset": 66, "endOffset": 250}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions. In Lecu\u00e9 and Mendelson (2013), the Bernstein\u2019s condition is not directly imposed, but is replaced by condition (ii) of Lemma 3.7 with m = 2 on the whole hypothesis class, while the assumption of sub-Gaussian hypothesis class validates condition (i). This implies the standard Bernstein\u2019s condition with \u03b3 = 1 and makes the convergence rate O(n\u22121) consistent with our result (note that for sub-Gaussian losses, r can be chosen arbitrary large). The analysis of Lecu\u00e9 and Mendelson (2012) concerns about non-exact oracle inequalities (rather than the sharp oracle inequalities we investigate in this paper) and can not be directly compared with our results.", "startOffset": 66, "endOffset": 707}, {"referenceID": 18, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 1, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 15, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 2, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217.", "startOffset": 78, "endOffset": 98}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217.", "startOffset": 78, "endOffset": 201}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217. Little is known about the finite-sample performance of empirically designed quantizers under possibly heavy-tailed distributions. In Telgarsky and Dasgupta (2013), a convergence rate ofO(n\u22121/2+2/r) are derived, where r is the number of moments of X that are assumed to be finite.", "startOffset": 78, "endOffset": 654}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217. Little is known about the finite-sample performance of empirically designed quantizers under possibly heavy-tailed distributions. In Telgarsky and Dasgupta (2013), a convergence rate ofO(n\u22121/2+2/r) are derived, where r is the number of moments of X that are assumed to be finite. Brownlees et al. (2015) uses some robust mean estimators to replace empirical means and derives a convergence rate of O(n\u22121/2) assuming only that the variance of X is finite.", "startOffset": 78, "endOffset": 795}, {"referenceID": 5, "context": "Following the framework of Brownlees et al. (2015), we consider G = {l(C, x) = min yi\u2208C \u2016x\u2212 yi\u2016, C \u2208 F = (\u2212\u03c1, \u03c1)d\u00d7k} for some \u03c1 > 0 with the regular Euclidean metric.", "startOffset": 27, "endOffset": 51}, {"referenceID": 15, "context": "while standard results about VC-dimension of k-means clustering hypothesis class guarantees that C \u2264 k(d+ 1) (Linder et al., 1994).", "startOffset": 109, "endOffset": 130}, {"referenceID": 19, "context": "Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015).", "startOffset": 120, "endOffset": 197}, {"referenceID": 9, "context": "Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015).", "startOffset": 120, "endOffset": 197}, {"referenceID": 7, "context": "Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015).", "startOffset": 120, "endOffset": 197}, {"referenceID": 6, "context": "While the condition of integrable envelope is an improvement from the condition of sub-exponential envelope previously investigated in the literature, it would be interesting to see if the rates retain under weaker conditions, for example, the assumption that the L-diameter of the hypothesis class is bounded (Cortes et al., 2013).", "startOffset": 310, "endOffset": 331}, {"referenceID": 5, "context": "Finally, the recent work of Brownlees et al. (2015); Hsu and Sabato (2016) about robust estimators as alternatives of ERM to study heavy-tailed losses has yielded more favorable learning rates under weaker conditions, and we would like to extend the result in this paper to study such estimators.", "startOffset": 28, "endOffset": 52}, {"referenceID": 5, "context": "Finally, the recent work of Brownlees et al. (2015); Hsu and Sabato (2016) about robust estimators as alternatives of ERM to study heavy-tailed losses has yielded more favorable learning rates under weaker conditions, and we would like to extend the result in this paper to study such estimators.", "startOffset": 28, "endOffset": 75}], "year": 2016, "abstractText": "We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function supf\u2208F |l \u25e6 f |, where l is the loss function and F is the hypothesis class, exists and is Lintegrable, and (ii) l satisfies the multi-scale Bernstein\u2019s condition on F . Under these assumptions, we prove that learning rate faster than O(n\u22121/2) can be obtained and, depending on r and the multi-scale Bernstein\u2019s powers, can be arbitrarily close to O(n\u22121). We then verify these assumptions and derive fast learning rates for the problem of vector quantization by k-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.", "creator": "LaTeX with hyperref package"}}}