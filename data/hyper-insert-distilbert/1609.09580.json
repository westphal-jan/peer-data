{"id": "1609.09580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Referential Uncertainty and Word Learning in High-dimensional, Continuous Meaning Spaces", "abstract": "this paper discusses lexicon word threshold learning in high - backward dimensional construct meaning spaces directly from the viewpoint of referential uncertainty. while we investigate evaluating various possible state - benefits of - the - kind art machine learning statistical algorithms methods and discuss examine the impact of scaling, representation composition and meaning space structure. hence we demonstrate that \u2026 current scientific machine learning optimization techniques combined successfully together deal with high - middle dimensional abstract meaning spaces. in particular, we all show that exponentially increasing dimensions frequently linearly impact learner performance and that resulting referential modeling uncertainty departing from decreasing word sensitivity has no impact.", "histories": [["v1", "Fri, 30 Sep 2016 03:20:52 GMT  (604kb,D)", "http://arxiv.org/abs/1609.09580v1", "Published as Spranger, M. and Beuls, K. (2016). Referential uncertainty and word learning in high-dimensional, continuous meaning spaces. In Hafner, V. and Pitti, A., editors, Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2016 Joint IEEE International Conferences on, 2016. IEEE"]], "COMMENTS": "Published as Spranger, M. and Beuls, K. (2016). Referential uncertainty and word learning in high-dimensional, continuous meaning spaces. In Hafner, V. and Pitti, A., editors, Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2016 Joint IEEE International Conferences on, 2016. IEEE", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael spranger", "katrien beuls"], "accepted": false, "id": "1609.09580"}, "pdf": {"name": "1609.09580.pdf", "metadata": {"source": "CRF", "title": "Referential Uncertainty and Word Learning in High-dimensional, Continuous Meaning Spaces", "authors": ["Michael Spranger", "Katrien Beuls"], "emails": ["michael.spranger@gmail.com", "katrien@ai.vub.ac.be"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe most important aspect of word learning is often thought to be referential uncertainty. Quine [1] famously framed referential uncertainty as a general problem everybody faces when trying to learn an unknown language. Suppose an anthropologist studies an isolated tribe. When members of the tribe see a rabbit, they shout \u201cgavagai\u201d. The anthropologist hears the word for the first time and has no idea what the word means. In principle the meaning of the word could be anything from perceptual features of objects present (or not present!), features of the environment, social and historical facts etc. The space of possible meanings is essentially infinite and the question is how the anthropologist can solve this puzzle.\nMany researchers claim that children face the same problem when trying to learn language. Models that deal with word learning try to capture referential uncertainty in various ways. Schematically, we can distinguish between the following approaches to meaning and corresponding models (see Fig. 1).\na) Word-Object Mapping Models (WOM): Some models approach word learning as a discrete mapping problem from words to object, e.g. [2], [3]. How difficult the mapping problem is depends on factors such as the number of objects in the context of an interaction or the feedback given by the caregiver. If there is only one object, then the mapping can be learned instantaneously and no problem exists. If there are multiple objects, then referential uncertainty does exist if there is ambiguous, unreliable or no feedback from the caregiver.\nb) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features. The features themselves are known prior to word learning. In CFM the meaning space is as large as all possible combinations of features. The meaning space grows combinatorially with the number of features. There are fundamental differences between WOM and CFM.\nIn CFM, a word can be linked to multiple features. The learner upon hearing a word and seeing a single object cannot know which features of the object the word refers to. Consequently, referential uncertainty can occur in single object contexts This is different from WOM, where referential uncertainty is exclusively related to the number of objects in the context.\nc) Continuous Meaning Space Models (CMS): Few models address the learning of words related to representations in continuous vector spaces [7], [8]. The problem of referential uncertainty in continuous meaning spaces is large and depends chiefly on the number of objects in a learning context, the number of dimensions of meaning vectors and whether or not objects can refer to subspaces of the meaning space (color space etc). In that sense CMS behave similar to CFM. The difference to CFM is that the meaning space is an infinite continuous vector space by default.\nThe few available CMS models are often shown to work in low-dimensions and with few words. In this paper we survey the state of the art in word learning algorithms for high-dimensional continuous meaning spaces with referential uncertainty caused by words referring to features of objects (rather than caused by multiple objects in the context). As it turns out, this problem has been dealt with in the Machine Learning (ML) community. We first analyze the problem of word learning from the viewpoint of machine learning in ndimensional spaces. We then test various state-of-the-art ML methods on simulated and grounded data sets, analyze dynamics of online learning and the impact of various referential uncertainty. Lastly, we discuss how all this fits in the general landscape of language learning models.\nar X\niv :1\n60 9.\n09 58\n0v 1\n[ cs\n.C L\n] 3\n0 Se\np 20\n16"}, {"heading": "II. DESCRIPTION GAMES", "text": "One interaction pattern (game) often used in models of word learning is the description game (DG). The basic structure of DG is the following. The learner observes a particular situation (context) of l objects (l = 1 for this paper) and also observes k words uttered by the tutor (k = 5 for this paper). So for instance, both learner and tutor observe one object. The tutor says \u201cblock, bright, red,..,..\u201d (order does not matter). The learner then has to learn the meaning of these words by integrating information over various trials. The success of the learner is measured by testing which words the learner produces for objects and how this overlaps with the production of words of the tutor.\nAn important aspect of these games is the tutor strategy - the representation and algorithm the tutor uses for producing k words for an object. In this paper, objects are represented by n dimensional feature vectors o \u2208 [0, 1]n. The tutor represents each word using a prototype p \u2208 [0, 1]n and a weight vector w \u2208 [0, 1]n. Prototypes for tutors are randomly drawn from a uniform distribution U(0, 1)n. Weights are drawn from a binomial distribution B(1, 0.5)n. If a weight vector is all 0 we randomly set one of the weights to 1.\nFor an object o \u2208 [0, 1]n the tutor first computes a weighted Euclidean distance wd\nwdw,p(o) = \u221a\u221a\u221a\u221a n\u2211 i=1 wi(oi \u2212 pi)2\nThe tutor then chooses the k closest words for the description of an object, i.e.\nargmin w,p\u2208P (wdw,p(o))\nThis representation and word production strategy has the following desirable properties. The strategy models feature dimension sensitivity of words/prototypes. For instance, there could be a word that is sensitive only to the brightness dimension. Other words could be sensitive to blue and red channels. Secondly, words are not produced uniformly - similar to human language. Some words are used often, others only occur few times in the training set. Lastly, the tutor produces k words for any object. Together with the object distribution in [0, 1]n this leads to interesting, non-linear interactions between words and objects.\nLet us briefly examine the difficulty of the learning task. The learner has to learn to produce (predict) the same words as the tutor based on examples provided by the tutor. Suppose that m = |W | denotes the number of words the tutor knows. In principle, chance performance is equal to choosing k out of m words without repetition and order does not matter. For experiments with k = 5 and m = 100 this amounts to ( 100 5 ) = 75, 287, 520 assuming a uniform distribution of words."}, {"heading": "III. DESCRIPTION GAMES AND MACHINE LEARNING", "text": "In machine learning terms, DG is a supervised, multiclass (multiple words), multi-label (multiple words per object),\nonline classification problem. There is an immense literature and an abundance of algorithms that can be tested on this problem [9], [10].\nFor this paper, we test roughly a dozen different learning methods from linear models, to ensemble classifiers, Bayesian learning and neural networks that solve the description game learning problem. The following paragraphs give a (brief) overview of the various methods we tested.\nd) Nearest neighbor (NN): One of the simplest and often best performing methods is nearest neighbor - also called KNN or in this paper KNeighbors [11] . KNeighbors is a nonparametric method that stores all samples ever encountered. New samples are classified based on the class of its k nearest neighbor (stored examples). The algorithms simplicity and the (often out of the box) success of this method have led to its widespread adoption.\nWe also use a related algorithm from the same family: Nearest Centroid (NC). NC represents classes using centroids of corresponding samples. New samples are classified based on the shortest distance to centroids. This method is among the most widely used in word learning because it corresponds nicely with ideas in psychology [12].\ne) Generalized Linear Models (GLM): GLM describe a family of algorithms that all model predictions as linear combinations of input variables. All models furthermore assume that predictions (dependent variables) are generated from exponential probability distributions (Gaussian, binomial, gamma etc). Learners also differ in terms of regularization and learning regime (closed-form, stochastic etc). We are using various classifiers: Logistic Regression, Online Passive Aggressive (PA) [13] and Stochastic Gradient Descent (SGD) [14].\nf) Ensemble methods (ENM): ENM are meta algorithms that try to improve classification results by combining results of sets (ensembles) of classifiers (NN, linear or others). There are basically two types of ensemble methods. The first relies on weak, underfitting classifiers each often only slightly better than random choice. There are two main methods in this group: AdaBoost [15] and Gradient Boosting [16]. AdaBoost works by fitting a series of weak learners. Each step (boosting iteration), training data is weighted to focus on samples that are not correctly classified in the previous step. Successive classifiers therefore essentially encode classification for various aspects of the data. New samples are classified by computing the majority estimates of classifiers. Gradient Boosting is an extension of boosting for optimizing any differentiable loss function.\nAnother class of ensemble methods takes the opposite approach and relies on sets of classifiers that are complex, over-fitting classifiers. Here, we use Random Forest [17] and Extra Trees [18], which use ensembles of decision trees. Decision trees are a non-parametric method that learns binary decisions (nodes) and arranges them in a binary tree [19]. Both RandomForest and ExtraTrees build multiple overfitting classifiers on random subsets of samples and features.\ng) Bayesian Methods (BM): BM rely on Bayes theorem to transform the classification problem into one of estimating probability distributions. New samples are classified based on prior probabilities of classes, as well as posterior estimates of sample probabilities and the probability of observing samples given classes. Bayesian classifiers primarily differ in the assumptions they make for the probability distributions that need to be estimated. We use Gaussian Naive Bayes classifiers [20] (normal distributions, independent features) and Multinomial Naive Bayes (multinomial distributions, independent features). Parameters for probability distributions are estimated using expectation-maximization.\nh) Neural networks (NN): Recently neural networks have pushed the state-of-the-art in many classification problems such as face recognition, image labeling etc. The current trend is to stack multiple layers of neurons (mostly nonlinear functions) and train them (often one layer at a time) using variants of the backpropagation algorithm. NN can take various forms in terms of network topologies, transfer functions, training regimes and learning rules. For the purpose of this paper, we used a multi-layer perceptron (MLP) - (2 layers, rectified linear units, with a final sigmoid layer for classification)."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": ""}, {"heading": "A. Datasets", "text": "We compare different learners on simulated and robot data sets. The robot data sets consists of 20 different objects (see Figure 2) of various color, shape, size. We recorded approximately 1000 scenes in which two robots observe objects from different perspectives. Scenes differ in the positions of objects, which also alters objects\u2019 perceived shape and color features. For each object n = 17 feature dimensions are extracted: x, y, z position, YUV color values (mean min, max), width, height, length. Data is scaled between 0 and 1 using a linear scaler and for some classifiers to zero mean, unit variance.\ni) GRO1: This dataset consists of all object observations ever made by the two robots in one matrix (4532\u00d717). GRO1 is used to experiment, with tutor and learner having the same object perception, in particular, the same feature estimation for objects.\nj) GRO2: This dataset is a grounded robot data set. It consists of two matrices. One for each robot (two matrices of size 4532 \u00d7 17). Each row corresponds to the perception of the same physical object from the viewpoint of the two\ndifferent robots. This data is used in the following way. The tutor produces words from the perspective of robot 1 (matrix 1). The learner learns from the observation of the same object but from perspective of the other robot (matrix 2). GRO2 is used to evaluate what happens if there is perceptual deviation [21]. That is tutor and learner see the scene from different viewpoints and therefore have different feature estimations for objects. For instance, the tutor robot might observe different x, y positions for an object, since he sees the object from a different perspective. But also color and shape features will be slightly different.\nk) SIM: This dataset is simulated and consists of 4532 object observations of n = 17 feature dimensions drawn from a uniform distribution U(0, 1)."}, {"heading": "B. Methods", "text": "Learners are trained on samples of objects and one-hot vector encoded words produced by the tutor. Each classifier has to predict (produce) the correct set of k words given (new) vectors of objects by predicting a one-hot vector encoding of word activations. We then measure the difference between the production of the tutor and the prediction of the learner. For learning algorithms that predict probabilities p for words (e.g. MLP), if p > 0.5 then the word is counted as a prediction.\nFor all experiments here, we draw |W | = 100 prototypes and weights for the tutor (according to the description in Section II) and perform 4-fold validation on data sets each consisting of 4532 samples. This means that training happens on roughly 3400 samples and testing on 1100 new samples. In summary, the standard parameters for our evaluations are n = 17 (number of features), |W | = 100 (number of words and prototypes tutor), k = 5 (number of words uttered by the tutor) and p = 0.5 (for the binomial distribution of tutor weights).\nClassifiers not supporting multi-class, multi-label by default were trained using one-vs-rest [22]. The exception are RandomForest, KNeighbors, NearestCentroid and MLP. Most classifiers rely on various hyper-parameters. We optimized hyper-parameters using parameter grid searches on a separate simulated dataset SIM-DEVELOP (same characteristics as SIM). Hyper-parameters were optimized once on SIMDEVELOP and then fixed for all results reported here."}, {"heading": "C. Measures", "text": "In this paper we use a single performance measure: f-score [23]. F-score is defined as the harmonic mean of precision and recall. There are various definitions of precision and recall depending on the classification problem (binary, multiclass, multi-label). Generally speaking, precision measures the amount of wrong words per sample found by the learner. Recall measures how many of the correct words were predicted by the learner. We use a particular f-score measure which is called example-based (or sample) that does not take into account unbalanced word distributions. An f-score of 100 means that all words and only those words uttered by the tutor are uttered by the learner."}, {"heading": "V. RESULTS", "text": "Table I shows the performance of various classifiers on grounded and simulated data. Many learners perform well on the task with respect to task complexity. In particular, simple algorithms such as GaussianNB or linear models perform well. More complex methods such as ensemble methods generally are top performers. The best performing method on SIM and GRO2 is the multi-layer perceptron MLP. On GRO1 GradientBoosting is the front runner. Although ensemble methods generally perform quite similar. None of the methods fail catastrophically, which is mostly due to hyper-parameter optimization.\nInterestingly all methods improve on grounded data (GRO1) some by as much as 20 points (e.g. PassiveAgressive). This suggests that methods are able to take advantage of structure available in grounded data. However, all methods perform worse on GRO2 than on GRO1. In some cases performance differs by almost 30 points between GRO2 and GRO1.\nA word on how to understand these results. This study focusses on understanding the baseline for word learning in high-dimensional meaning spaces. These results are existence proofs (lower bounds) showing that learners can solve this problem with relatively high f-score. These results do not show limits of individual learning algorithms, rather results show general trends that hint at the scope of the learning problem."}, {"heading": "A. Scaling Object Feature Dimensions", "text": "We are interested in understanding referential uncertainty in high-dimensional meaning spaces. Consequently, we manipulated the number of dimensions of object features n. The number of dimensions in the grounded data sets is fixed by the vision system, so we increased the number of dimensions for simulated data. All other parameters are kept the same. Figure 3 shows how classifiers do for n \u2208 {10, 100, 1000, 10000}.\nAverage performance across all classifiers degrades linearly with orders of magnitude of difference in n for the best\nperforming classifiers (MLP, AdaBoost). These results suggest that classifiers optimized for various n and/or increasing the number of training samples, could actually deal with even higher n-dimensional data (remember that all classifiers were optimized for n = 17).\nThere is one classifier that performs poorly (MultinomialNB) all along, while others (e.g. ExtraTrees, RandomForest) degrade much more rapidly with number of dimensions than the best performing ones. Others perform best for certain n (e.g. PassiveAggressive). All of these classifiers perform reasonably well or very well on the hyper-parameter optimized n = 17. Consequently, these classifiers are sensitive to hyperparameters with respect to number of dimensions."}, {"heading": "B. Scaling Word Sensitivity", "text": "Another dimension of scaling is the sensitivity of words. Prototype weights for all experiments reported so far were drawn from a binomial distribution B(1, p = 0.5). This means that words are on average sensitive to half of the dimensions. In our experiments, referential uncertainty is tied to the fact that words can refer to aspects of objects. To test learners, we ran experiments for various p \u2208 {0.1, 0.25, 0.5, 0.75, 1.0} and n = 100. All other parameters stay the same.\nFigure 4 shows that quite a few learners (AdaBoost, ExtraTrees, MLP etc) have absolutely no problem dealing with various p. We can conclude that these learners will perform well on mixed languages where some words encompass all features of an object and some are more specific and only refer to certain features. Other learners such as KNeighbors become better with larger p. This is no surprise since KNeighbors stores full examples."}, {"heading": "C. Online Learning", "text": "One important aspect in language learning from a developmental point of view is online learning. We tested how well the algorithms perform over time. For this we incrementally train classifiers on the training set. For instance, we train on the first m object observations and evaluate the f-score on the test data set. Figure 5 shows the performance of classifiers over time. Pretty much all classifiers learn very fast with most of the gains happening on the first 500 training samples. In other words, the learner becomes quite sufficient after 500 interactions with\nthe tutor. After 1000 training samples all classifiers are within 5 points of their final f-score.\nThese results for online learning are quite remarkable given that there are 100 words that need to be learned. Certainly, what helps here is that certain words will be used frequently and others less frequently. We analyzed results with respect to frequency of words and it becomes evident that this is indeed one big driver of the speed of learning."}, {"heading": "D. Discussion of Results", "text": "Our results show that many off-the-shelf machine learning algorithms can deal with high-dimensional meaning spaces. We only optimized classifiers for n = 17 and we get linear drop in performance for exponential increase in number of dimensions. This suggest that we might be able to get on par performance even for very high n - if we optimize classifiers for each n and if we logarithmically increase training set size. Our results also show that many classifiers have no problem dealing with feature sensitivity related to referential uncertainty.\nOur experiments suggest that referential uncertainty in highdimensional meaning spaces is NOT an exponentially growing problem. In other words adding dimensions has at most a linear effect on learners in the paradigm discussed here. This is an interesting result because it suggests that referential uncertainty although often thought of as an exponentially growing problem with the number of dimensions or the degree with which words can associate to aspects of objects is much less of an issue than one might expect.\nl) Meaning Space Structure: There is a difference in performance between SIM and GRO1 data sets. The reason is that there is much more structure in the real world than in the simulated world. The real world data sets consist of limited sets of objects that make up very defined spaces in the perceptual data space. For instance, there are certain clearly separable color regions based on the objects in GRO1. This structure in the environment helps all learning algorithms in becoming more successful.\nm) Tutoring: Tutoring strategies are often thought to be about social feedback (e.g. pointing or agreement). But tutoring can also mean that the tutor is structuring the environment (and possibly also the language) for the learner. This can include taking perspective or conceptualizing the world from the viewpoint of the learner. Generally speaking it has been found that tutoring strategies help learners [8], [24]. The delta in performance between GRO1 and GRO2 confirms these ideas. In GRO1 the tutor will utter words based on what the learner sees. In GRO2 that is not the case. All classifiers perform less well on GRO2.\nn) Unbalanced data: Another aspect that affects performance is the fact that training of words is unbalanced. There are some words that occur often and others that don\u2019t (in fact some words do not even appear in the training set and only in the test set). The classifiers have difficulties with sparsely used words. Something that becomes apparent when examining macro-averaged f-scores (not reported here). This is often much lower than micro-averaged f-scores. This split suggests that (generally speaking) learners are good in learning frequent words but less good in learning less frequent words.\no) Representation: It is interesting to analyze various learning algorithms with respect to whether they actually build representations similar to that of the tutor. We deliberately chose various algorithms none of which directly tried to replicate the tutor behavior in the learner by learning the same representation. The tutor operates using weighted feature distances to prototypes. Words are only sensitive to a particular feature channel (e.g. the brightness). Algorithms such as KNeighbors do not explicitly represent information like that. They just collect samples. Others such as RandomForrest do actually learn how to distinguish different words based on explicitly learning which features matter with respect to the word. An interesting result of this study is that both of these algorithms perform comparably well in terms of replicating the tutors behavior. But if we look at different p value experiments, we can see that discriminative feature learners such as AdaBoost outperform KNeighbors."}, {"heading": "VI. RELATED WORK", "text": "p) Referential uncertainty: There is an important difference between the setup here and other studies. Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among others). In this paper, we use referential uncertainty closer to Quine\u2019s formulation and early studies by Siskind [4]. Quine focusses on aspects of a situation and not on enumerable\nobjects as the source of referential uncertainty. In that sense the problem in Quine is larger. Even if you know the referent, the learner still knows nothing about the aspect of the referent the word refers to (color, shape etc). The question remains which referential uncertainty problem is solved by children (possibly both).\nq) Description vs Discrimination: An important distinction between various models is the tutoring strategy - representation and algorithm for word production in the tutor. In description games, the speaker minimize distances between the topic object and words (here weighted Euclidean distances). In other types of interaction (called guessing games), the function being maximized for each word is the difference between the topic object t and all the other objects (or features) in the context. An interesting question is whether different production strategies require different learning algorithms or not. Often the learner is modeled after the tutor and both use the same production and interpretation algorithms (see [8], [6] for some recent examples). This obviously biases the system and the question is whether this is necessary. What we can say for the models described in this paper is that nowhere did we bias the learners explicitly towards a particular production algorithm. Rather all that happens is that the learner is trying to replicate the tutor behavior.\nr) Child Learning Strategies: Researchers in child language acquisition have provided many ideas about strategies that children use to learn the meaning of words. Some of them such as perceptual biases [26] could potentially be exploited by learning algorithms - if the language affords it. That is, if the language to be learned is based on salient perceptual distinctions then algorithms that learn discriminative features (e.g. decision trees, ensemble methods) can take advantage of that. Other child language learning strategies based on linguistic constraints [27] are not by definition part of the learning paradigm discussed in this paper. We only focus on unordered sets of words uttered by the tutor. The impact of strategies such as mutual exclusivity [28] and contrast [29] on the learning problem defined in this paper is subject to future work."}, {"heading": "VII. CONCLUSION", "text": "Abstract models can be used to answer questions about what algorithms children use to learn language. This is often done by trying to replicate empirical data. Another goal of models is to grasp the essence of the learning problem, characterize how hard it is and how the best known algorithms perform. This paper addresses aspects of the second goal. We defined an abstract version of the word learning problem, translated it to Machine Learning and compared state of the art methods on the problem. This establishes a baseline that can be used to understand word learning from the viewpoint of complexity.\nSource code as well as data sets are published online at https://github.com/mspranger/icdl2016language."}], "references": [{"title": "Word and object", "author": ["W.V.O. Quine"], "venue": "MIT press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "and L", "author": ["J. Fontanari", "V. Tikhanoff", "A. Cangelosi", "R. Ilin"], "venue": "Perlovsky, \u201cCross-situational learning of object\u2013word mapping using neural modeling fields,\u201d Neural Networks, vol. 22, no. 5, pp. 579\u2013585", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "and R", "author": ["G. Kachergis", "C. Yu"], "venue": "M. Shiffrin, \u201cAn associative model of adaptive inference for learning word\u2013referent mappings,\u201d Psychonomic bulletin & review, vol. 19, no. 2, pp. 317\u2013324", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A computational study of cross-situational techniques for learning word-to-meaning mappings,", "author": ["J.M. Siskind"], "venue": "Cognition, pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "and T", "author": ["J. De Beule", "B. De Vylder"], "venue": "Belpaeme, \u201cA cross-situational learning algorithm for damping homonymy in the guessing game,\u201d in Artificial Life X. MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Coping with combinatorial uncertainty in word learning: A flexible usage-based model,\u201d in The Evolution of Language", "author": ["P. Wellens"], "venue": "Proceedings of the 7Th International Conference. World Scientific,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Word and category learning in a continuous semantic domain: comparing cross-situational and interactive learning,", "author": ["T. Belpaeme", "A. Morse"], "venue": "Advances in Complex Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Grounded lexicon acquisition - case studies in spatial language,\u201d in Development and Learning and Epigenetic Robotics (ICDL- Epirob)", "author": ["M. Spranger"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A review on multi-label learning algorithms,", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "and I", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Vlahavas, \u201cMining multi-label data,\u201d in Data mining and knowledge discovery handbook. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Nearest neighbor pattern classification,", "author": ["T.M. Cover", "P.E. Hart"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1967}, {"title": "Cognitive representations of semantic categories,", "author": ["E. Rosch"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1975}, {"title": "and Y", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz"], "venue": "Singer, \u201cOnline passive-aggressive algorithms,\u201d The Journal of Machine Learning Research, vol. 7, pp. 551\u2013585", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Large-scale machine learning with stochastic gradient descent,", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A desicion-theoretic generalization of online learning and an application to boosting,", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory. Springer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Greedy function approximation: a gradient boosting machine,", "author": ["J.H. Friedman"], "venue": "Annals of statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Random forests,", "author": ["L. Breiman"], "venue": "Machine learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "and L", "author": ["P. Geurts", "D. Ernst"], "venue": "Wehenkel, \u201cExtremely randomized trees,\u201d Machine learning, vol. 63, no. 1, pp. 3\u201342", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Data mining with decision trees: theory and applications", "author": ["L. Rokach", "O. Maimon"], "venue": "World scientific", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "On discriminative vs", "author": ["A. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes,\u201d Advances in neural information processing systems, vol. 14, p. 841", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Dealing with Perceptual Deviation - Vague Semantics for Spatial Language and Quantification,", "author": ["M. Spranger", "S. Pauw"], "venue": "in Language Grounding in Robots. Springer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "and H", "author": ["N. Spola\u00f4r", "M.C. Monard", "G. Tsoumakas"], "venue": "D. Lee, \u201cA systematic review of multi-label feature selection and a new method based on label construction,\u201d Neurocomputing", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A systematic analysis of performance measures for classification tasks,", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing & Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Incremental grounded language learning in robot-robot interactions - examples from spatial language,\u201d in Development and Learning and Epigenetic Robotics (ICDL-Epirob)", "author": ["M. Spranger"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "and J", "author": ["M. Frank", "N.D. Goodman"], "venue": "B. Tenenbaum, \u201cA bayesian framework for cross-situational word-learning.\u201d in Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "and E", "author": ["S.M. Pruden", "K. Hirsh-Pasek", "R.M. Golinkoff"], "venue": "A. Hennon, \u201cThe birth of words: Ten-month-olds learn words through perceptual salience,\u201d Child development, vol. 77, no. 2, pp. 266\u2013280", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "The structural sources of verb meanings,", "author": ["L. Gleitman"], "venue": "Language acquisition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "Children\u2019s use of mutual exclusivity to constrain the meanings of words,", "author": ["E.M. Markman", "G.F. Wachtel"], "venue": "Cognitive psychology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "The principle of contrast: A constraint on language acquisition,", "author": ["E.V. Clark"], "venue": "Mechanisms of language acquisition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1987}], "referenceMentions": [{"referenceID": 0, "context": "Quine [1] famously framed referential uncertainty as a general problem everybody faces when trying to learn an unknown language.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "[2], [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2], [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "b) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "b) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "b) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "c) Continuous Meaning Space Models (CMS): Few models address the learning of words related to representations in continuous vector spaces [7], [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 7, "context": "c) Continuous Meaning Space Models (CMS): Few models address the learning of words related to representations in continuous vector spaces [7], [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "In this paper, objects are represented by n dimensional feature vectors o \u2208 [0, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "The tutor represents each word using a prototype p \u2208 [0, 1] and a weight vector w \u2208 [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "The tutor represents each word using a prototype p \u2208 [0, 1] and a weight vector w \u2208 [0, 1].", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "For an object o \u2208 [0, 1] the tutor first computes a weighted Euclidean distance wd wdw,p(o) = \u221a\u221a\u221a\u221a n \u2211", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "Together with the object distribution in [0, 1] this leads to interesting, non-linear interactions between words and objects.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "problem [9], [10].", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "problem [9], [10].", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "d) Nearest neighbor (NN): One of the simplest and often best performing methods is nearest neighbor - also called KNN or in this paper KNeighbors [11] .", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "This method is among the most widely used in word learning because it corresponds nicely with ideas in psychology [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "We are using various classifiers: Logistic Regression, Online Passive Aggressive (PA) [13] and Stochastic Gradient Descent (SGD) [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "We are using various classifiers: Logistic Regression, Online Passive Aggressive (PA) [13] and Stochastic Gradient Descent (SGD) [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "There are two main methods in this group: AdaBoost [15] and Gradient Boosting [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "There are two main methods in this group: AdaBoost [15] and Gradient Boosting [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "Here, we use Random Forest [17] and Extra Trees [18], which use ensembles of decision trees.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "Here, we use Random Forest [17] and Extra Trees [18], which use ensembles of decision trees.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "Decision trees are a non-parametric method that learns binary decisions (nodes) and arranges them in a binary tree [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "We use Gaussian Naive Bayes classifiers [20] (normal distributions, independent features) and Multinomial Naive Bayes (multinomial distributions, independent features).", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "GRO2 is used to evaluate what happens if there is perceptual deviation [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "Classifiers not supporting multi-class, multi-label by default were trained using one-vs-rest [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "In this paper we use a single performance measure: f-score [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 7, "context": "Generally speaking it has been found that tutoring strategies help learners [8], [24].", "startOffset": 76, "endOffset": 79}, {"referenceID": 23, "context": "Generally speaking it has been found that tutoring strategies help learners [8], [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "In this paper, we use referential uncertainty closer to Quine\u2019s formulation and early studies by Siskind [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Often the learner is modeled after the tutor and both use the same production and interpretation algorithms (see [8], [6] for some recent examples).", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Often the learner is modeled after the tutor and both use the same production and interpretation algorithms (see [8], [6] for some recent examples).", "startOffset": 118, "endOffset": 121}, {"referenceID": 25, "context": "Some of them such as perceptual biases [26] could potentially be exploited by learning algorithms - if the language affords it.", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "Other child language learning strategies based on linguistic constraints [27] are not by definition part of the learning paradigm discussed in this paper.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "The impact of strategies such as mutual exclusivity [28] and contrast [29] on the learning problem defined in this paper is subject to future work.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "The impact of strategies such as mutual exclusivity [28] and contrast [29] on the learning problem defined in this paper is subject to future work.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "This paper discusses lexicon word learning in highdimensional meaning spaces from the viewpoint of referential uncertainty. We investigate various state-of-the-art Machine Learning algorithms and discuss the impact of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact.", "creator": "LaTeX with hyperref package"}}}