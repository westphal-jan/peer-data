{"id": "1610.01108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "abstract": "in this published paper we have provide the largest formally published comparison of translation quality for phrase - based smt computational and neural machine processing translation across below 30 regional translation directions. for past ten successive directions downstream we also additionally include systematic hierarchical phrase - based mt. experiments and are performed for the now recently published european united nations parallel corpus v1. 8 0 dictionary and its large six - way sentence - alignment aligned subcorpus. for in the main second part point of explaining the paper we personally investigate aspects consisting of translation speed, introducing amunmt, our efficient neural matrix machine translation decoder. clearly we demonstrate that current analog neural and machine translation could already be regularly used automatically for comparing in - production systems generally when physically comparing normal words - based per - second ratios.", "histories": [["v1", "Tue, 4 Oct 2016 17:52:42 GMT  (137kb,D)", "http://arxiv.org/abs/1610.01108v1", null], ["v2", "Tue, 11 Oct 2016 07:35:47 GMT  (146kb,D)", "http://arxiv.org/abs/1610.01108v2", null], ["v3", "Wed, 30 Nov 2016 18:37:18 GMT  (147kb,D)", "http://arxiv.org/abs/1610.01108v3", "Accepted for presentation at IWSLT 2016, Seattle"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "tomasz dwojak", "hieu hoang"], "accepted": false, "id": "1610.01108"}, "pdf": {"name": "1610.01108.pdf", "metadata": {"source": "META", "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "authors": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang"], "emails": ["junczys@amu.edu.pl", "t.dwojak@amu.edu.pl", "hieu@hoang.co.uk"], "sections": [{"heading": "1 Introduction", "text": "We compare the performance of phrase-based SMT, hierarchical phrase-based, and neural machine translation (NMT) across fifteen language pairs and thirty translation directions. Ziemski et al. (2016) recently published the United Nations Parallel Corpus v1.0. It contains a subcorpus of ca. 11M sentences fully aligned across six languages (Arabic, Chinese, English, French, Russian, and Spanish) and official development and test sets, which makes this an ideal resource for experiments across multiple language pairs. It is also a compelling use case for in-domain translation with large bilingual in-house resources. We provide BLEU scores for the entire translation matrix for all official languages from the fully aligned subcorpus.\nWe also introduce AmuNMT1, our efficient neural machine translation decoder and demon-\n1https://github.com/emjotde/amunmt\nstrate that the current set-up could already be used instead of Moses in terms of translation speed when a single GPU per machine is available. Multiple GPUs would surpass the speed of the proposed in-production Moses configuration by far."}, {"heading": "2 Training data", "text": ""}, {"heading": "2.1 The UN corpus", "text": "The United Nations Parallel Corpus v1.0 (Ziemski et al., 2016) consists of human translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish. Apart from the pairwise aligned documents, a fully aligned subcorpus for the six official UN languages is distributed. This subcorpus consists of sentences that are consistently aligned across all languages with the English primary documents. Statistics for the data are provided below:\nDocuments released in 2015 (excluded from the main corpus) were used to create official development and test sets for machine translation tasks. Development data was randomly selected from documents that were released in the first quarter of 2015 and test data was selected from the second quarter. Both sets comprise 4,000 sentences that are 1-1 alignments across all official languages. As in the case of the fully aligned subcorpus, any translation direction can be evaluated."}, {"heading": "2.2 Preprocessing", "text": "Sentences longer than 100 words were discarded. We lowercased the training data as it was done in Ziemski et al. (2016); the data was tokenized\nar X\niv :1\n61 0.\n01 10\n8v 1\n[ cs\n.C L\n] 4\nO ct\n2 01\n6\nwith the Moses tokenizer. For Chinese segmentation we used Jieba2 before applying the Moses tokenizer."}, {"heading": "2.3 Subword units", "text": "To avoid the large-vocabulary problem in NMT models (Luong et al., 2015), we use byte-pairencoding (BPE) to achieve open-vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2015). For all languages we set the number of subword units to 30,000. Segmentation into subword units is applied after any other preprocessing step. During evaluation, subwords are concatenated to form full tokens."}, {"heading": "3 Phrase-based SMT baselines", "text": "In Ziemski et al. (2016), we provided baseline BLEU scores for Moses (Koehn et al., 2007) configurations that were trained on the 6-way subcorpus. We repeat the system description here:\nTo speed up the word alignment procedure, we split the training corpora into four equally sized parts that are aligned with MGIZA++ (Gao and Vogel, 2008), running 5 iterations of Model 1 and the HMM model on each part.3 We use a 5-gram language model trained from the target parallel data, with 3-grams or higher order being pruned if they occur only once. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al., 2013) (3-grams and 4-grams are pruned if they occur only once, 5-grams and 6-grams if they occur only twice, etc.), both trained using KenLM (Heafield et al., 2013). To reduce the phrasetable size, we apply significance pruning (Johnson et al., 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012). During decoding, we use the cube-pruning algorithm with stack size and cube-pruning pop limits of 1,000. This configuration is the actual setting used at the United Nations for their inhouse translation system.\n2https://github.com/fxsjy/jieba 3We confirmed that there seemed to be no quality loss due to splitting and limiting the iterations to simpler alignment models."}, {"heading": "4 Neural translation systems", "text": "The neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2014), which has been trained with Nematus. We used mini-batches of size 40, a maximum sentence length of 100, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 (Pascanu et al., 2013). Models were trained with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. The models have been trained model for 1.2M iterations (one iteration corresponds to one mini-batch), saving every 30, 000 iterations. On our NVidia GTX 1080 this corresponds to roughly 4 epochs and 8 days of training time. Models with English as there source or target data were trained for another 1.2M iterations (another 2 epochs, 8 days) to test the influence of increased training time. For ensembling/averaging, we chose the last four model checkpoints."}, {"heading": "5 Phrase-based vs. NMT \u2013 full matrix", "text": "In Figure 1 we present the results for all thirty language pairs in the United Nations parallel corpus for the officially included test set. Here we compare with NMT models that were trained for 4 epochs or 1.2M iterations. With the exception of fr-es and ru-en the neural system is always comparable or better than the phrase-based system. The differences where NMT is worse are very small. Especially in cases where Chinese is one of the languages in a language pair, the improvement of NMT over PB-SMT is dramatic with between 7 and 9 BLEU points. This seems to confirm the large gains reported for Chinese-English by Wu et al. (2016). However, it also seems to suggest that this is a property of the language pair, not so much of their set-up. We also see large improvements for translations out of and into Arabic. It should be noted here, that no special preprocessing has been applied for Arabic. It is interesting to observe that improvements are present also in the case of the highest scoring translation directions, en-es and es-en."}, {"heading": "6 Phrase-based vs. Hiero vs. NMT \u2013 language pairs with English", "text": "The particularly impressive results for any translation direction involving Chinese motived us to experiment with hierarchical phrase-based machine\ntranslation (Hiero) as well. Hiero has been confirmed to outperform phrase-based SMT for the Chinese-English language pair. We further decided to expand our experiment with all language pairs that include English, as these are the main translation directions the United Nations are working with. For these ten translation directions we created a hierarchical PB-SMT system with the same preprocessing settings as the PB-SMT system. We also continued training of our neural systems for another four epochs or 1.2M iterations (2.4M in total) which increased training time to 16 days in total per neural system.\nFigure 2 summarizes the results. As expected, Hiero outperforms PB-SMT by a significant mar-\ngin for Chinese-English and English-Chinese, but does not even reach half the improvement of the NMT systems. For the other languages pairs we see mixed results; for French-English and Russian-English, where results for PB-SMT and NMT were very close, Hiero is actually the best system (TODO: Missing results here for the 2.4M iterations).\nTraining the NMT system for another eight days always improves the performance of the NMT system, but gains are rather small between 0.4 and 0.7 BLEU. We did not observe any improvements beyond 2M iterations. It seems that stopping training after 8-10 days might be a viable heuristic with little loss in terms of BLEU.\nTo summarize the first part of this work: choosing NMT over PB-SMT seem like a good bet with results that reach from comparable to far superior in terms of BLEU. In the future we would like to verify these results with human evaluation."}, {"heading": "7 Efficient decoding with AmuNMT", "text": "AmuNMT4 is a ground-up neural MT toolkit implementation, developed in C++. It currently consist of an efficient beam-search inference engine for models trained with Nematus (Sennrich et al., 2016; Bahdanau et al., 2014). We focused mainly on efficiency and usability. Features of the AmuNMT decoder include:\n\u2022 Low-latency CPU-based decoding with intra-sentence multi-threading (one sentence makes use of multiple threads during matrix operations) and sentence-wise threads (different sentences are decoded in different threads); \u2022 Multi-GPU support for sentence-wise translation per GPU; \u2022 Full compatibility with NMT models trained with Nematus (Sennrich et al., 2016); \u2022 Ensembling of similar models; \u2022 Ensembling of models with the same out-\nput vocabulary but different inputs. A\n4http://github.com/emjotde/amunmt\nsimilar configuration was used in the winning automatic post-editing shared task at WMT2016 (Junczys-Dowmunt and Grundkiewicz, 2016). \u2022 YAML vocabularies and configuration files; \u2022 Integrated segmentation into subword units\nand de-segmentation (Sennrich et al., 2015); \u2022 A clean and documented C++ code-base."}, {"heading": "7.1 Beam size vs. Speed and Quality", "text": "Beam size has a large impact on decoding speed. In Figure 3 we plot the influence of beam size on decoding speed (as words per second) and translation quality (in BLEU) for the English-French model. The English part of the UN test set consists or ca. 120.000 tokens, the whole test sets was translated for each experiment. As can be seen, beam sizes beyond 5-7 do not result in significant improvements as the quality for a beam size of 5 is only 0.1 BLEU below the maximum. However, decoding speed is significantly slower. We therefore choose a beam-size of 5 for our next experiments."}, {"heading": "7.2 Comparison of AmunNMT to Moses and Nematus", "text": "In Figure 4a we report speed in terms of words per second as provided by Wu et al. (2016). Although the models of Wu et al. (2016) are more\ncomplex that our own, we treat the reported figures as a reference of deployment-ready performance for NMT.\nWe ran our experiments on an Intel Xeon E52620 2.40GHz server with four NVIDIA GeForce GTX 1080 GPUs. The phrase-based parameters are described in Section 3 which is guided by best practises to achieving reasonable speed vs. quality trade-off (Pouliquen et al., 2013). The neural MT models are as described in the previous section.\nWe present the words-per-second ratio for our NMT models using AmunNMT and Nematus, executed on the CPU and GPU, Figure 4b. For the CPU version we use 16 threads, translating one sentence per thread. We restrict the number of OpenBLAS threads to 1 per main Nematus thread, restricting the total number of threads to 16. For the GPU version of Nematus we use 5 processes to maximize GPU saturation. As a baseline, the phrase-based model reaches 455 words per second using 16 threads.\nThe CPU-bound execution of Nematus reaches 47 words per second while the GPU-bound achieved 270 words per second.\nIn similar settings, the CPU-bound AmuNMT is 2.5 times faster than Nematus, but still nearly four times slower than Moses. However, the GPUexecuted version 1.5 time faster than Nematus and is almost as fast as Moses, achieving 409 words per second. The similar translation speed will allow us to replace a Moses-based SMT system with an AmunNMT-based NMT system in a production environment without severely affecting translation throughput.\nAmuNMT can parallelize to multiple GPUs present in one machine by processing one sentence per GPU. Translation speed increases almost linearly to 1,200 words per second with four GPUs. AmuNMT has a start-up time of less than 10 seconds, while Nematus may need several minutes until the first translation can be produced. Nevertheless, the model used in AmuNMT is an exact implementation of the Nematus model that produces identical results.\nThe size of the NMT model with the chosen parameters is approximately 300 MB, which means about 24 models could be loaded onto a single GPU with 8GB of RAM. Hardly any overhead is\nrequired during translation. With multiple GPUs, access could be parallelized and optimally scheduled in a query-based server setting."}, {"heading": "7.3 Low-latency translation", "text": "Until now we evaluated bulk-throughput which corresponds to a high translation load in settings with long documents that need to be translated continuously. Translation speed was averaged across whole documents and multiple cores. However, there are settings where the latency for a single sentence may be more important, one example would be predictive translation (Knowles and Koehn, 2016). Many other settings will be located between these extremes \u2014 low-latency translation and bulk-translation \u2014 as for example MT usage in CAT systems, another instance of interactive translation. To compare per-sentence latency we translate our test set with all tools in a serial fashion, using at most one CPU thread or process. We do not aim at full GPU saturation as this would not improve latency. We then average time over the number of sentences and report seconds per sentence in Figure 5, lower values are better. Here AmuNMT GPU compares very favourably against all other solutions with a 5 times smaller latency than Moses and a more than 4 times smaller latency that Nematus. Latency between the CPU-only variants shows the\nsame ratios as bulk-translation in the previous section. (TODO: evaluate latency with multiple CPUthreads per sentence.)"}, {"heading": "8 Conclusions", "text": "We evaluated the performance of neural machine translation on all thirty translation directions of the United Nations Parallel Corpus v1.0. We showed that for nearly all translation directions NMT is either on par with or surpasses phrase-based SMT. There seems to be no risk in potential quality loss due to switching to NMT. For some language pairs, the gains are substantial, as measured by BLEU. These include all pairs that have Chinese as a source or target language. Very respectable gains can also be observed for Arabic. For other language pairs there is generally at least some improvement.\nAlthough NMT systems are known to generalize better than phrase-based systems for out-ofdomain data, it was unclear how they perform in a purely in-domain setting which is of interest for any larger organization with significant resources of their own data, such as the UN or other governmental bodies. This work currently lacks human evaluation which we would like to supply in future versions.\nWe introduced our efficient neural machine translation beam-search decoder, AmuNMT, and demonstrated that high-quality and highperformance neural machine translation can be achieved on commodity hardware; the GPUs we tested on are widely available to the general public for gaming PCs and graphics workstations. A single GPU matches the performance of 16 CPU threads on server-grade Intel Xeon CPUs. We can take advantage of multiple GPUs to increase translation speed even further. Access to specialized hardware like Google\u2019s Tensor Processing Units seems unnecessary when planning to switch to neural machine translation using lower-parametrized models.\nEven the performance of the CPU-only version of AmuNMT allows to set-up demo systems and can be a viable solution for low-throughput settings. Training, however, requires a GPU. Still, one might start with one GPU for training and reuse the CPU machines on which Moses has been running for first deployment. For future work, we plan to further improve the performance of AmuNMT, especially for CPU-only settings.\nDisclaimer\nThis is a draft version describing on-going work. Missing figures will be supplied at a later point. Content, results, and conclusions may change."}, {"heading": "Acknowledgments", "text": "This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 688139 (SUMMA)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Can Markov models over minimal translation units help phrasebased SMT? In ACL, pages 399\u2013405", "author": ["Nadir Durrani", "Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn."], "venue": "The Association for Computer Linguistics.", "citeRegEx": "Durrani et al\\.,? 2013", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Parallel implementations of word alignment tool", "author": ["Qin Gao", "Stephan Vogel."], "venue": "Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49\u201357. ACL.", "citeRegEx": "Gao and Vogel.,? 2008", "shortCiteRegEx": "Gao and Vogel.", "year": 2008}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the ACL, pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Improving translation quality by discarding most of the phrasetable", "author": ["J Howard Johnson", "Joel Martin", "George Forst", "Roland Kuhn."], "venue": "Proceedings of EMNLP-CoNLL\u201907, pages 967\u2013975.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "CoRR, abs/1605.04800.", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Phrasal RankEncoding: Exploiting phrase redundancy and translational relations for phrase table compression", "author": ["Marcin Junczys-Dowmunt."], "venue": "Prague Bull. Math. Linguistics, 98:63\u201374.", "citeRegEx": "Junczys.Dowmunt.,? 2012", "shortCiteRegEx": "Junczys.Dowmunt.", "year": 2012}, {"title": "Neural interactive translation prediction", "author": ["Rebecca Knowles", "Philipp Koehn."], "venue": "Proceedings of AMTA 2016, Austin, USA, October.", "citeRegEx": "Knowles and Koehn.,? 2016", "shortCiteRegEx": "Knowles and Koehn.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, pages 1310\u20131318, , Atlanta, GA, USA.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Large-scale multiple language translation accelerator at the United Nations", "author": ["Bruno Pouliquen", "Cecilia Elizalde", "Marcin JunczysDowmunt", "Christophe Mazenc", "Jos\u00e9 Garc\u0131\u0301aVerdugo"], "venue": "In MTSummit XIV,", "citeRegEx": "Pouliquen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pouliquen et al\\.", "year": 2013}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR, abs/1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Ger-", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "The united nations parallel corpus v1.0", "author": ["Micha\u0142 Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen"], "venue": null, "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Ziemski et al. (2016) recently published the United Nations Parallel Corpus v1.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "0 (Ziemski et al., 2016) consists of human translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.", "startOffset": 2, "endOffset": 24}, {"referenceID": 15, "context": "We lowercased the training data as it was done in Ziemski et al. (2016); the data was tokenized ar X iv :1 61 0.", "startOffset": 50, "endOffset": 72}, {"referenceID": 8, "context": "To avoid the large-vocabulary problem in NMT models (Luong et al., 2015), we use byte-pairencoding (BPE) to achieve open-vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 12, "context": ", 2015), we use byte-pairencoding (BPE) to achieve open-vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2015).", "startOffset": 122, "endOffset": 145}, {"referenceID": 2, "context": "We repeat the system description here: To speed up the word alignment procedure, we split the training corpora into four equally sized parts that are aligned with MGIZA++ (Gao and Vogel, 2008), running 5 iterations of Model 1 and the HMM model on each part.", "startOffset": 171, "endOffset": 192}, {"referenceID": 1, "context": "Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 9, "context": ", 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al., 2013) (3-grams and 4-grams are pruned if they occur only once, 5-grams and 6-grams if they occur only twice, etc.", "startOffset": 131, "endOffset": 153}, {"referenceID": 3, "context": "), both trained using KenLM (Heafield et al., 2013).", "startOffset": 28, "endOffset": 51}, {"referenceID": 4, "context": "To reduce the phrasetable size, we apply significance pruning (Johnson et al., 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012).", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": ", 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012).", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "In Ziemski et al. (2016), we provided baseline BLEU scores for Moses (Koehn et al.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "The neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2014), which has been trained with Nematus.", "startOffset": 72, "endOffset": 95}, {"referenceID": 10, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 14, "context": "Models were trained with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs.", "startOffset": 34, "endOffset": 48}, {"referenceID": 13, "context": "It currently consist of an efficient beam-search inference engine for models trained with Nematus (Sennrich et al., 2016; Bahdanau et al., 2014).", "startOffset": 98, "endOffset": 144}, {"referenceID": 0, "context": "It currently consist of an efficient beam-search inference engine for models trained with Nematus (Sennrich et al., 2016; Bahdanau et al., 2014).", "startOffset": 98, "endOffset": 144}, {"referenceID": 13, "context": "\u2022 Low-latency CPU-based decoding with intra-sentence multi-threading (one sentence makes use of multiple threads during matrix operations) and sentence-wise threads (different sentences are decoded in different threads); \u2022 Multi-GPU support for sentence-wise translation per GPU; \u2022 Full compatibility with NMT models trained with Nematus (Sennrich et al., 2016); \u2022 Ensembling of similar models; \u2022 Ensembling of models with the same output vocabulary but different inputs.", "startOffset": 338, "endOffset": 361}, {"referenceID": 5, "context": "com/emjotde/amunmt similar configuration was used in the winning automatic post-editing shared task at WMT2016 (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 111, "endOffset": 151}, {"referenceID": 12, "context": "\u2022 YAML vocabularies and configuration files; \u2022 Integrated segmentation into subword units and de-segmentation (Sennrich et al., 2015); \u2022 A clean and documented C++ code-base.", "startOffset": 110, "endOffset": 133}, {"referenceID": 11, "context": "quality trade-off (Pouliquen et al., 2013).", "startOffset": 18, "endOffset": 42}, {"referenceID": 7, "context": "However, there are settings where the latency for a single sentence may be more important, one example would be predictive translation (Knowles and Koehn, 2016).", "startOffset": 135, "endOffset": 160}], "year": 2016, "abstractText": "In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentencealigned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.", "creator": "LaTeX with hyperref package"}}}