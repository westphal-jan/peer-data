{"id": "1610.01367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech Processing Models", "abstract": "a basic pascal innovation challenge entitled monaural multi - talker speech recognition algorithms was simultaneously developed, targeting the problem of robust automatic speech recognition against speech like noises impairment which significantly degrades the speech performance of automatic speech accent recognition systems. in this challenge, two competing speakers say a simple command simultaneously speak and lastly the objective is determined to correctly recognize robust speech content of unknown the target speaker. surprisingly during the 2010 challenge, a team from ibm transportation research, qatar could achieve similar a performance better than engaged human listeners on this task. the latter proposed specific method of examining the two ibm team, consist of an individual intermediate human speech separation and repeated then achieving a single - talker speech recognition. this case paper reconsiders the limited task motivation of proving this challenge based more on gain adapted factorial and speech processing models. above it develops currently a joint - sense token competitive passing judgment algorithm proposing for direct utterance decoding of similarly both target and masker speakers, simultaneously. comparing combined it to the successful challenge winner, it uses maximum performance uncertainty technique during considering the decoding performance which result cannot certainly be initially used in the more past purely two - phased method. combined it clearly provides remarkably detailed iso derivation conclusions of competitive inference on these coding models based on relatively general cognitive inference fitting procedures of equivalent probabilistic graphical models. as therefore another evolutionary improvement, it uses deep neural networks for creating joint - speaker identification and gain estimation which probably makes these evaluation two steps easier choices than before producing desired competitive matching results for fulfilling these diagnostic steps. twice the proposed mathematical method of this technique work outperforms previously past super - human results equally and even the eventual results were achieved recently again by microsoft research, using deep fuzzy neural networks. it nonetheless achieved - 5. 5 % absolute task frequency performance - improvement achievement compared to the applications first super - human system and 2. 7 % absolute task learning performance improvement compared similarly to its main recent turing competitor.", "histories": [["v1", "Wed, 5 Oct 2016 11:34:36 GMT  (1171kb)", "http://arxiv.org/abs/1610.01367v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["mahdi khademian", "mohammad mehdi homayounpour"], "accepted": false, "id": "1610.01367"}, "pdf": {"name": "1610.01367.pdf", "metadata": {"source": "CRF", "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech Processing Models", "authors": ["Mahdi Khademian", "Mohammad Mehdi Homayounpour"], "emails": ["homayoun@aut.ac.ir"], "sections": [{"heading": null, "text": "A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past superhuman results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first superhuman system and 2.7% absolute task performance improvement compared to its recent competitor.\nKeywords: factorial hidden Markov model, vector Taylor series, monaural mixed-speech recognition, joint-decoding, two-dimensional Viterbi, joint-speaker identification\n1 * Corresponding author: Mohammad Mehdi Homayounpour\nEmail: homayoun@aut.ac.ir Tel: +98 21 64542722"}, {"heading": "1. Introduction", "text": "Robustness of automatic speech recognition systems (ASR) against diverse speech processing environments and adverse disturbing noises still remains as one of the important research areas in speech recognition systems (Baker et al., 2009; Li et al., 2014). Among all diversities and conditions in everyday environments which ASR systems must manage, dealing with the Babble noise and presence of competing speakers is one of the challenging problems of these systems. This problem is known as the cocktail party problem (Haykin and Chen, 2005) in which a person (or a system) wants to focus and follow the conversation of a speaker in a place where some people talk simultaneously. Roughly speaking, two groups of approaches are developed for addressing this problem. Approaches in the first group incorporate signals captured from several microphones or capturing channels and perform low level signal processing techniques such as the beam forming and blind source separation which reduce footprints of the competing audio sources. These approaches accomplish their speech processing tasks using the improved captured and processed signals. Approaches in the second group use only one recording channel and perform high level speech processing and machine learning techniques and accomplish their tasks in the presence of the competing audio sources which seems to be more challenging.\nAn interesting competition entitled \u201cMonaural speech separation and recognition challenge\u201d, addressing the challenges related to the second group of approaches was developed in 2006 (Cooke et al., 2010). In this challenge, two competing speakers are simultaneously issuing a command and the objective is to recognize the command of the target speaker. The task uses a simple grammar for commands and it has a small vocabulary.\nThe problem of monaural multi-talker speech recognition becomes more difficult when speech of the masker speaker has higher energy than the target. The problem becomes worse when the masker speaker voice is similar to the target voice; i.e. when the two speakers have the same gender or two speakers are the same. Several teams attended this competition with different techniques for handling the problem (Cooke et al., 2010). Among the competitors, surprisingly, a team from IBM research presented a technique that outperforms the other techniques and even human listeners (Hershey et al., 2010). In their work, the task was accomplished in three main steps. First, the identity of speakers and gain is estimated using high resolution Gaussian Mixture Models (GMM) as speaker models. In the main step, speech of both speakers are separated from the mixed-\nspeech signal using factorial speech processing models. In this step, expected value of source features given the observed feature and joint acoustic states are considered for source estimation, then source separation is performed. In the third step, two separated speeches are decoded using a single-talker recognition system (Hershey et al., 2010). This team further developed their system to support mixture of speeches of more than two speakers and separate their voices only by one recording channel! Later researches continue to work on this dataset. To the best of our knowledge, only one work outperforms IBM\u2019s super-human results from the Microsoft research. This work incorporates a pair of Deep Neural Networks for acoustic inference over semi-joint hidden Markov models (HMM) (Weng et al., 2015); a network for the generation of senone posteriors of high energy utterances (or instantaneous high energy frames) and a network for low energy utterances. Then they perform decoding over these senone posteriors.\nThe method presented in this paper is a model based approach based on factorial speech processing models for recognizing monaural mixed-speech signals which is applied for the \u201cMonaural speech separation and recognition challenge\u201d. It directly performs joint-decoding over the model to decode both utterances of the target and masker speaker, simultaneously. Direct speech recognition of this work is done by a joint-decoder which is developed by extending the token passing algorithm to support inference over factorial speech processing models constructed with grammar and dictionary. While joint-decoding over the models with grammar and dictionary increases complexity of decoder, at the same time it provides significant performance improvement over the past developed systems. Additionally, this work uses Deep Neural Networks for speaker identification and gain estimation as one important step for determining audio sources of the factorial model. Moreover, this paper presents a detailed inference procedure over the factorial models using the general inference procedures of probabilistic graphical models.\nAfter this introduction, the next section briefly describes the challenge. It also presents a detailed description of steps for applying factorial speech processing models to this challenge, first by deriving inference procedures for the factorial speech processing models, then developing the joint-token passing algorithm for performing the joint-decoding. Section 3 describes the methods for determining and adapting source models. Section 4 presents experiments, scoring procedure and results. Section 5 will conclude the paper."}, {"heading": "2. Factorial speech processing models for single channel speech recognition", "text": "The objective of the monaural speech separation and recognition challenge is to recognize some keywords of a target speaker from a mixed-speech of the target and a masker speaker (Cooke et al., 2010). Mixed-speech signals of this task are artificially created from speech materials of the Grid corpus (Cooke et al., 2006). This corpus contains simple six-word slot commands from 34 different speakers. Each command is a sequence of command words, color, preposition, a letter, a digit and an adverb depicted in Fig. 1.\nMixed-speech signals are created by selecting two utterances from the Grid corpus, one for the target speaker and the other for the masker. The target speaker always uses \u201cwhite\u201d as the command color and the masker does not. This is the clue for discrimination of the target and masker voices. Two speech signals are mixed by the following time domain relation:\n\ud835\udc66 = \ud835\udc65\ud835\udc4e + \ud835\udc54\ud835\udc65\ud835\udc4f (1)\nin which \ud835\udc65\ud835\udc4e is the speech signal for the target and \ud835\udc65\ud835\udc4f is for the masker. The challenge is designed for different signal energy ratios of the target and masker which is called Target to Masker Ratio (TMR). This is adjusted by the gain coefficient, \ud835\udc54, in (1). Objective keywords of the task are the letter and digit of the target speaker which is used for scoring. The next sub-section provides an overview of factorial models of speech processing, which are used in this work for performing the task. Then the inference and decoding procedures of these models are presented."}, {"heading": "2.1. Factorial speech processing models", "text": "Factorial models of speech processing are generative models for modeling the combination of multiple audio sources into one (or even multiple) observable mixed-audio signals. It is applicable for robust-ASR systems (Hershey et al., 2012) and is tailored exactly for the challenge of this paper. Fig. 2 shows the graphical model of a factorial speech processing model. This model is based on factorial hidden Markov models which are used for modeling processes with multiple independent underlying Markov chains (Koller and Friedman, 2009). In this figure, the two source Markov processes are the speech process of speaker \ud835\udc4e and \ud835\udc4f. Conditional probability distribution (CPD) of the Markov chain of the audio sources are \ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4e |\ud835\udc60\ud835\udc61 \ud835\udc4e) and \ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4f |\ud835\udc60\ud835\udc61 \ud835\udc4f) which are modeled parametrically by stochastic matrices. Each chain of a factorial speech processing model contains an HMM for modeling its audio source which is known as acoustic modeling (Young, 1996) in conventional speech recognition applications. These are shown in Fig. 2, by the dashed boxes around each audio source (source \ud835\udc4e and \ud835\udc4f).\nConventionally, in speech processing applications, Gaussian mixture models are used for observation distributions of HMM and left-to-right topology is used for modeling the Markov\nchain of that HMM. The observation probability distribution of HMMs is modeled by the following CPD:\n\ud835\udc5d(\ud835\udc99\ud835\udc61 \ud835\udc4e|\ud835\udc60\ud835\udc61 \ud835\udc4e) = \u2211 \ud835\udc5d(\ud835\udc99\ud835\udc61 \ud835\udc4e|\ud835\udc5a\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61 \ud835\udc4e)\ud835\udc5a\ud835\udc61\ud835\udc4e \ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4e|\ud835\udc60\ud835\udc61 \ud835\udc4e) (2)\nwhere \ud835\udc5d(\ud835\udc99\ud835\udc61 \ud835\udc4e|\ud835\udc5a\ud835\udc61 \ud835\udc4e = \ud835\udc56\ud835\udc5a, \ud835\udc60\ud835\udc61 \ud835\udc4e = \ud835\udc56\ud835\udc60) = \ud835\udca9(\ud835\udc99\ud835\udc61 \ud835\udc4e; \ud835\udf41(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a), \ud835\udeba(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a)) is the \ud835\udc56\ud835\udc5a Gaussian component of the GMM of state \ud835\udc56\ud835\udc60, and \ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4e|\ud835\udc60\ud835\udc61 \ud835\udc4e) models the component weights by a stochastic matrix. The observation model of the second chain is similar to the first one.\nFactorial speech processing models have additional CPD for combining source features comparing them to factorial hidden Markov models which is called the acoustic interaction function. This CPD provides a probabilistic relationship between the source and the combined features, i.e. \ud835\udc5d(\ud835\udc9a\ud835\udc61|\ud835\udc99\ud835\udc61 \ud835\udc4e, \ud835\udc99\ud835\udc61 \ud835\udc4f). In these models, source features are not observable but the combined feature is. By the acoustic interaction function we can infer the posterior distribution of source features and then the posterior distribution of source states in an \u201cevidential reasoning\u201d pattern (Koller and Friedman, 2009). The number of Markov chains in factorial models of speech processing depends on the number of audio sources in the mixed-signal.\nFor the recognition task of monaural speech separation and recognition challenge, we can consider one audio source of the factorial model as the clean source model of the target speaker and the other as the model of masker. Then based on mixed-signal feature vectors, we can infer acoustic states of the audio sources which are used for decoding. The next sub-section describes the way that inference is done over the factorial models of speech processing."}, {"heading": "2.2. Inference", "text": "The objective of inference in models in the form of Fig. 2 is to find the most probable states of\nthe sources in a period of time given the observed feature vectors of that period:\n\ud835\udc601:\ud835\udc47 \u2217\ud835\udc4e,\ud835\udc4f = argmax\n\ud835\udc601:\ud835\udc47 \ud835\udc4e,\ud835\udc4f\n\ud835\udc5d(\ud835\udc601:\ud835\udc47 \ud835\udc4e,\ud835\udc4f|\ud835\udc9a1:\ud835\udc47) (3)\nBut the main objective of inference in our task is to find the sequence of spoken words of each speaker; more specifically, the mentioned letter and digit of the target speaker. Decoding the most probable acoustic states into the sequence of spoken words is done by a joint-decoder. In this subsection, acoustic and temporal inference over the factorial model is described and in the next subsection the decoding procedure is discussed."}, {"heading": "2.2.1. Acoustic inference", "text": "The graphical model of Fig. 2 is not used directly in the applications. In fact, an intermediate step during the inference is marginalizing-out hidden feature vectors of audio sources (\ud835\udc99\ud835\udc4e and \ud835\udc99\ud835\udc4f). This step will merge three CPDs including \ud835\udc5d(\ud835\udc99\ud835\udc4e|\ud835\udc60\ud835\udc4e, \ud835\udc5a\ud835\udc4e), \ud835\udc5d(\ud835\udc99\ud835\udc4f|\ud835\udc60\ud835\udc4f, \ud835\udc5a\ud835\udc4f) and \ud835\udc5d(\ud835\udc9a|\ud835\udc99\ud835\udc4e, \ud835\udc99\ud835\udc4f) into state conditional likelihood, \ud835\udc5d(\ud835\udc9a|\ud835\udc60\ud835\udc4e, \ud835\udc5a\ud835\udc4e, \ud835\udc60\ud835\udc4f , \ud835\udc5a\ud835\udc4f). Probabilistically we have:\n\ud835\udc5d(\ud835\udc9a|\ud835\udc60\ud835\udc4e, \ud835\udc5a\ud835\udc4e, \ud835\udc60\ud835\udc4f , \ud835\udc5a\ud835\udc4f) = \u222c \ud835\udc5d(\ud835\udc9a|\ud835\udc99\ud835\udc4e, \ud835\udc99\ud835\udc4f)\ud835\udc5d(\ud835\udc99\ud835\udc4e|\ud835\udc60\ud835\udc4e, \ud835\udc5a\ud835\udc4e)\ud835\udc5d(\ud835\udc99\ud835\udc4f|\ud835\udc60\ud835\udc4f, \ud835\udc5a\ud835\udc4f)\ud835\udc51\ud835\udc99\ud835\udc4e\ud835\udc51\ud835\udc99\ud835\udc4f (4)\nAs a result, the model of Fig. 2 will be simplified to the graphical model of Fig. 3. This form of marginalization is seen before in noise-robust automatic speech recognition when one audio source is clean speech and the other is the disturbing noise (Hershey et al., 2012). Depending on the feature space and source models, several approaches are suggested for the calculation of (4). For example, for high resolution power spectral features, the max-model (Roweis, 2003) is used for marginalizing-out source feature vectors. This feature space is usually used for enhancement applications. The objective of our task is to recognize utterances of the audio sources (speakers) where the audio sources are modeled by GMMs. Usually for this application, MFCC features are used for source modeling. By using an appropriate mismatch function (Gales and Young, 1996) which combines source features into the observed feature vector, we can approximate each state conditional distribution by one Gaussian. This technique is known as approximation by the vector Taylor series (Moreno et al., 1996), VTS, which will be discussed later."}, {"heading": "2.2.2. VTS based acoustic inference", "text": "Returning to the time domain expression (1) for creating the mixed-speech signal, here the expressions for combining clean source features and then state-conditional likelihoods of mixedspeech features are developed. The gain coefficient is omitted temporarily and will be reconsidered in section 3.2, in details. After framing, windowing and by use of short-term discrete Fourier transform we have the following relation between the frames\u2019 complex feature vectors:\n\ud835\udc9a\ud835\udc61 = \ud835\udc99\ud835\udc61 \ud835\udc4e + \ud835\udc99\ud835\udc61 \ud835\udc4f (5)\nwhere vector elements contain different frequency bins. In the power spectrum we have:\n|\ud835\udc9a\ud835\udc61| 2 = |\ud835\udc99\ud835\udc61 \ud835\udc4e|2 + |\ud835\udc99\ud835\udc61 \ud835\udc4f| 2 + 2|\ud835\udc99\ud835\udc61 \ud835\udc4e||\ud835\udc99\ud835\udc61 \ud835\udc4f| cos \ud835\udf3d (6)\nin which \ud835\udf3d is the phase difference between \ud835\udc99\ud835\udc61 \ud835\udc4e and \ud835\udc99\ud835\udc61 \ud835\udc4f complex vectors at different frequency bins (\ud835\udc61 subscript is removed in subsequent expression and provided as needed for notational brevity). In the terminology of noise-robust ASR, this expression is called \u201cmismatch function\u201d when one of the source signals is the disturbing noise (Gales and Young, 1996). Then, by using Mel scale averaging filters we have:\n\ud835\u0303\udc9a = \ud835\udc16|\ud835\udc9a | 2, \ud835\u0303\udc99 \ud835\udc4e = \ud835\udc16|\ud835\udc99 \ud835\udc4e|2, \ud835\u0303\udc99 \ud835\udc4f = \ud835\udc16|\ud835\udc99 \ud835\udc4f|2\nin which \ud835\udc16 is the weighing matrix for Mel filters (each row contains weighting elements for each Mel filter). Now for the Mel filter bank features, equation (6) is transformed to:\n\ud835\u0303\udc9a = \ud835\u0303\udc99 \ud835\udc4e + \ud835\u0303\udc99 \ud835\udc4f + 2\ud835\udf36\u221a\ud835\u0303\udc99 \ud835\udc4e\ud835\u0303\udc99 \ud835\udc4f (7)\nwhere \ud835\udf36 is called the phase factor and is equal to:\n\ud835\udf36 = \ud835\udc16|\ud835\udc99\n\ud835\udc4e||\ud835\udc99 \ud835\udc4f| cos \ud835\udf3d \u221a\ud835\u0303\udc99 \ud835\udc4e\ud835\u0303\udc99 \ud835\udc4f (8)\nConsidering random values for \ud835\udf3d in [\u2212\ud835\udf0b, \ud835\udf0b], alpha becomes stochastic in the range of [\u22121,1] (Van Dalen, 2011). Moreover \ud835\udf36 can be considered independent from source feature vectors by an additional simplifying assumption (Leutnant and Haeb-Umbach, 2009). Thus, it can be considered as an internal independent stochastic variable in the mismatch function. It is traditionally ignored in many applications (Van Dalen, 2011) or considered as a constant for all frequency bins (Li et al., 2009). By taking the logarithm and left multiplying the [truncated] DCT matrix, we have the following relation between the feature vectors in the Cepstrum domain:\n\ud835\udc9a c = \ud835\udc02 \ud835\udc25\ud835\udc28\ud835\udc20 (\ud835\udc1e\ud835\udc31\ud835\udc29(\ud835\udc02\u22121 \ud835\udc99 \ud835\udc4e c ) + \ud835\udc1e\ud835\udc31\ud835\udc29(\ud835\udc02\u22121 \ud835\udc99 \ud835\udc4f c ) + 2\ud835\udf36 \ud835\udc1e\ud835\udc31\ud835\udc29 (\n1 2 \ud835\udc02\u22121( \ud835\udc99 \ud835\udc4e c + \ud835\udc99 \ud835\udc4f c ))) (9)\nwhere c in \ud835\udc9a c denotes Cepstral features ( c will be removed in subsequent expressions, from now on, all feature vectors are considered as MFCC features). Equation (9) is known as the most applicable mismatch function in noise-robust ASR where one of the audio sources is considered as the disturbing noise; i.e. \ud835\udc9a = \ud835\udc1f(\ud835\udc99 \ud835\udc4e, \ud835\udc99 \ud835\udc4f , \ud835\udf36). This equation constructs a non-linear relationship between feature vectors of the audio sources and observable features. It can be considered as a deterministic CPD of \ud835\udc5d(\ud835\udc9a |\ud835\udc99 \ud835\udc4e, \ud835\udc99 \ud835\udc4f) in Fig. 2. A linear approximation of (9) could be established by first-order vector Taylor series expansion around \ud835\udc990 \ud835\udc4e, \ud835\udc990 \ud835\udc4f and considering a constant alpha as:\n\ud835\udc9a = \ud835\udc1f(\ud835\udc99 \ud835\udc4e, \ud835\udc99 \ud835\udc4f , \ud835\udefc) \u2245 \ud835\udc870 + \ud835\udc06(\ud835\udc99 \ud835\udc4e \u2212 \ud835\udc990 \ud835\udc4e) + \ud835\udc07(\ud835\udc99 \ud835\udc4f \u2212 \ud835\udc990 \ud835\udc4f) (10)\nin which \ud835\udc870 = \ud835\udc87(\ud835\udc990 \ud835\udc4e, \ud835\udc990\n\ud835\udc4f , \ud835\udefc), \ud835\udc06 = \ud835\udf4f\ud835\udc87\n\ud835\udf4f\ud835\udc99\ud835\udc4e |\n\ud835\udc990 \ud835\udc4e,\ud835\udc990\n\ud835\udc4f,\ud835\udefc and \ud835\udc07 =\n\ud835\udf4f\ud835\udc87\n\ud835\udf4f\ud835\udc99\ud835\udc4f |\n\ud835\udc990 \ud835\udc4e,\ud835\udc990\n\ud835\udc4f,\ud835\udefc .\nExpression (10) can be viewed as the sum of two transformed vector random variables. By assuming that \ud835\udc9a has Gaussian distribution and considering the expansion point as the source variable means, the Gaussian parameters become:\n\ud835\udf41\ud835\udc9a = \ud835\udc87(\ud835\udf41\ud835\udc99\ud835\udc4e , \ud835\udf41\ud835\udc99\ud835\udc4f , \ud835\udefc) (11)\n\ud835\udeba\ud835\udc32 = \ud835\udc06\ud835\udf41\ud835\udc99\ud835\udc4e\ud835\udc06 \ud835\udc47 + \ud835\udc07\ud835\udf41\ud835\udc99\ud835\udc4f\ud835\udc07 \ud835\udc47 (12)\nAt this point, only inference over the static part of feature vectors is done. For performing inference over the dynamic coefficients, consider the following expression for discrete differentiation of feature vectors:\n\ud835\udeab\ud835\udc9a\ud835\udc61 = \u2211 \ud835\udc64\ud835\udc56\ud835\udc9a\ud835\udc61\u2212\ud835\udc56\ud835\udc8a , \u2211 \ud835\udc64\ud835\udc56 = 0 (13)\nin which \ud835\udc64\ud835\udc56s are weights for differentiation. For example, for a context window of size 5, \ud835\udc64\ud835\udc56s are {\u2212 1 4\u2044 , \u2212 1 2\u2044 , 0, 1 2\u2044 , 1 4\u2044 }. Considering expression (13), assuming that dynamic coefficients can also be modeled by a Gaussian and by one practical approximation, Gaussian parameters for delta and acceleration coefficients can be extracted similar to static parts as follows:\n\ud835\udf41\ud835\udeab\ud835\udc9a = \ud835\udc06\ud835\udf41\ud835\udeab\ud835\udc99\ud835\udc4e + \ud835\udc07\ud835\udf41\ud835\udeab\ud835\udc99\ud835\udc4f (14)\n\ud835\udeba\ud835\udeab\ud835\udc32 = \ud835\udc06\ud835\udf41\ud835\udeab\ud835\udc99\ud835\udc4e\ud835\udc06 \ud835\udc47 + \ud835\udc07\ud835\udf41\ud835\udeab\ud835\udc99\ud835\udc4f\ud835\udc07 \ud835\udc47 (15)\n\ud835\udf41\ud835\udeab\ud835\udeab\ud835\udc9a = \ud835\udc06\ud835\udf41\ud835\udeab\ud835\udeab\ud835\udc99\ud835\udc4e + \ud835\udc07\ud835\udf41\ud835\udeab\ud835\udeab\ud835\udc99\ud835\udc4f (16)\n\ud835\udeba\ud835\udeab\ud835\udeab\ud835\udc32 = \ud835\udc06\ud835\udf41\ud835\udeab\ud835\udeab\ud835\udc99\ud835\udc4e\ud835\udc06 \ud835\udc47 + \ud835\udc07\ud835\udf41\ud835\udeab\ud835\udeab\ud835\udc99\ud835\udc4f\ud835\udc07 \ud835\udc47 (17)\nFor a detailed derivation, the reader is referred to the appendix of (Li et al., 2009). Now the\nacoustic inference can be done by the following expression:\n\ud835\udc5d(\ud835\udc9a|\ud835\udc60\ud835\udc4e = \ud835\udc56\ud835\udc60, \ud835\udc5a \ud835\udc4e = \ud835\udc56\ud835\udc5a, \ud835\udc60 \ud835\udc4f = \ud835\udc57\ud835\udc60, \ud835\udc5a \ud835\udc4f = \ud835\udc57\ud835\udc5a) = \ud835\udca9(\ud835\udc9a\ud835\udc94; \ud835\udf41\ud835\udc94\ud835\udc9a(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a), \ud835\udeba\ud835\udc94\ud835\udc9a(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a)) \u00d7\n\ud835\udca9(\ud835\udc9a\ud835\udeab; \ud835\udf41\ud835\udeab\ud835\udc32(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a), \ud835\udeba\ud835\udeab\ud835\udc32(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a)) \u00d7 \ud835\udca9(\ud835\udc9a\ud835\udeab\ud835\udeab; \ud835\udf41\ud835\udeab\ud835\udeab\ud835\udc32(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a), \ud835\udeba\ud835\udeab\ud835\udeab\ud835\udc32(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a)) (18) in which \ud835\udc9a = [\ud835\udc9a\ud835\udc94, \ud835\udc9a\ud835\udeab, \ud835\udc9a\ud835\udeab\ud835\udeab] \u2032. The expansion point for calculating means and covariance matrices are determined by the source states and GMM components. For example, \ud835\udf41\ud835\udeab\ud835\udc32(\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a,\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a) = \ud835\udc06\ud835\udf41\ud835\udeab\ud835\udc9a\ud835\udc56\ud835\udc60,\ud835\udc56\ud835\udc5a \ud835\udc06 \ud835\udc47 + \ud835\udc07\ud835\udf41\ud835\udeab\ud835\udc9a\ud835\udc57\ud835\udc60,\ud835\udc57\ud835\udc5a \ud835\udc07 \ud835\udc47. Now, state conditional likelihoods for different joint-source states and GMM components can be calculated by (18). At this moment, we are prepared to perform temporal inference in the graphical model of Fig. 3 which is covered in the next sub-section."}, {"heading": "2.2.3. Temporal inference", "text": "For extracting exact temporal inference expressions, at first, a clique tree is constructed from the simplified factorial model of Fig. 3. This clique tree is depicted in Fig. 4 which is an arbitrary tree constructed by first eliminating the state variable of source b and then a.\nBased on the constructed clique tree, the following recursions are extracted to do temporal\ninference:\n\ud835\udf0f\ud835\udc61 \ud835\udc4f(\ud835\udc60\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61+1 \ud835\udc4f ) = max\n\ud835\udc60\ud835\udc61 \ud835\udc4f,\ud835\udc5a\ud835\udc61 \ud835\udc4e,\ud835\udc5a\ud835\udc61 \ud835\udc4f\n\ud835\udf0f\ud835\udc61\u22121 \ud835\udc4e (\ud835\udc60\ud835\udc61 \ud835\udc4e , \ud835\udc60\ud835\udc61 \ud835\udc4f)\ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4f |\ud835\udc60\ud835\udc61 \ud835\udc4f)\ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4e|\ud835\udc60\ud835\udc61 \ud835\udc4e)\ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4f|\ud835\udc60\ud835\udc61 \ud835\udc4f)\ud835\udc5d(\ud835\udc9a\ud835\udc61|\ud835\udc60\ud835\udc61 \ud835\udc4e, \ud835\udc5a\ud835\udc61 \ud835\udc4e , \ud835\udc60\ud835\udc61 \ud835\udc4f , \ud835\udc5a\ud835\udc61 \ud835\udc4f) (19)\n\ud835\udf0f\ud835\udc61 \ud835\udc4e(\ud835\udc60\ud835\udc61+1 \ud835\udc4e , \ud835\udc60\ud835\udc61+1 \ud835\udc4f ) = max\n\ud835\udc60\ud835\udc61 \ud835\udc4e\n\ud835\udf0f\ud835\udc61 \ud835\udc4f(\ud835\udc60\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61+1 \ud835\udc4f )\ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4e |\ud835\udc60\ud835\udc61 \ud835\udc4e) (20)\nwhere in these two recursions, \ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4e |\ud835\udc60\ud835\udc61 \ud835\udc4e) and \ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4f |\ud835\udc60\ud835\udc61 \ud835\udc4f) are state transition matrices of two underlying Markov chains, \ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4e|\ud835\udc60\ud835\udc61 \ud835\udc4e) and \ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4f|\ud835\udc60\ud835\udc61 \ud835\udc4f) are component weights of GMM observation models of source\nHMMs and \ud835\udc5d(\ud835\udc9a\ud835\udc61|\ud835\udc60\ud835\udc61 \ud835\udc4e , \ud835\udc5a\ud835\udc61 \ud835\udc4e , \ud835\udc60\ud835\udc61 \ud835\udc4f , \ud835\udc5a\ud835\udc61 \ud835\udc4f) is the result of acoustic inference. The importance of breaking this joint-state maximization problem into two single-variable problems is the reduction of operations by a factor of number of chain states. In other words, instead of jointly doing maximization over \u2329\ud835\udc60\ud835\udc61 \ud835\udc4f, \ud835\udc60\ud835\udc61 \ud835\udc4e\u232a, at each step one source state variable is optimized in a dynamic programming manner (the presence of \ud835\udc5a\ud835\udc61 \ud835\udc4e, \ud835\udc5a\ud835\udc61 \ud835\udc4f is not important in the first recursion since the number of GMM components are not significant relative to the number of states). The initial factor \ud835\udf0f0 \ud835\udc4e(\ud835\udc600 \ud835\udc4e, \ud835\udc600 \ud835\udc4f) is defined as:\n\ud835\udf0f0 \ud835\udc4e(\ud835\udc601 \ud835\udc4e , \ud835\udc601 \ud835\udc4f) = \ud835\udc5d(\ud835\udc601 \ud835\udc4e)\ud835\udc5d(\ud835\udc601 \ud835\udc4f) (21)\nin which \ud835\udc5d(\ud835\udc601 \ud835\udc4e) and \ud835\udc5d(\ud835\udc601 \ud835\udc4f) are state priors of two underlying Markov chains. At each step, a back pointer to the previous maximized state is used for the final backtracking:\n\ud835\udf13\ud835\udc61 \ud835\udc4f(\ud835\udc60\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61+1 \ud835\udc4f ) = argmax\n\ud835\udc60\ud835\udc61 \ud835\udc4f\n(\ud835\udf0f\ud835\udc61\u22121 \ud835\udc4e (\ud835\udc60\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61 \ud835\udc4f)\ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4f |\ud835\udc60\ud835\udc61 \ud835\udc4f) max\n\ud835\udc5a\ud835\udc61 \ud835\udc4e,\ud835\udc5a\ud835\udc61\n\ud835\udc4f \ud835\udc5d(\ud835\udc5a\ud835\udc61\n\ud835\udc4e|\ud835\udc60\ud835\udc61 \ud835\udc4e)\ud835\udc5d(\ud835\udc5a\ud835\udc61 \ud835\udc4f|\ud835\udc60\ud835\udc61 \ud835\udc4f)\ud835\udc5d(\ud835\udc9a\ud835\udc61|\ud835\udc60\ud835\udc61 \ud835\udc4e , \ud835\udc5a\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61 \ud835\udc4f, \ud835\udc5a\ud835\udc61 \ud835\udc4f)) (22)\n\ud835\udf13\ud835\udc61 \ud835\udc4e(\ud835\udc60\ud835\udc61+1 \ud835\udc4e , \ud835\udc60\ud835\udc61+1 \ud835\udc4f ) = argmax\n\ud835\udc60\ud835\udc61 \ud835\udc4e\n\ud835\udf0f\ud835\udc61 \ud835\udc4f(\ud835\udc60\ud835\udc61 \ud835\udc4e, \ud835\udc60\ud835\udc61+1 \ud835\udc4f )\ud835\udc5d(\ud835\udc60\ud835\udc61+1 \ud835\udc4e |\ud835\udc60\ud835\udc61 \ud835\udc4e) (23)\nThis algorithm which is derived here using general inference procedures of probabilistic graphical models is called the two-dimensional Viterbi algorithm (Hershey et al., 2010). For more details about clique tree construction and general inference over the graphical models, the reader can refer to (Koller and Friedman, 2009; Murphy, 2002)."}, {"heading": "2.3. Joint-decoding", "text": "For the recognition task of the challenge, different decoding methods can be used. In the simplest method, for speech recognition in a single chain, whole word acoustic models can be used for the creation of a composite HMM. This can be done since the challenge has a small vocabulary. The composite HMM is created by concatenation of states of the whole word HMMs, allowing transitions between the different words in the adjacent word-slots. Single chain composite HMM models can now be considered in a factorial model with multiple-chains as source models for performing inference. This method of inference has some limitations. First, this method cannot be used in the tasks with medium and large vocabulary size since considering whole word acoustic models is not applicable in these scenarios. Second, explicit construction of the composite HMM in the tasks with complex grammar and language-models is very difficult. Third, acoustic states in whole word HMMs have overlap with each other which makes acoustic inference on factorial models, inefficient.\nThe more efficient method is to use sub-word acoustic units. The past proposed method for decoding single chain HMMs with sub-word acoustic units is called token passing algorithm. It is a conceptual framework for decoding in large vocabulary continuous speech recognition tasks (Young, 1996; Young et al., 2009).\nIn the token passing algorithm, a word lattice network based on the task grammar is created which models allowable transitions between words of the task. Then a lexicon is used for phonetic transcription of words appearing in the network into a sequence of phonemes. Now based on the network and lexicon, a set of hypotheses of phoneme sequences is used for decoding. These hypotheses and their scores are represented by tokens. Each token preserves a history of states using a pointer to its previous token and it has a score which is the acoustic score of the whole state sequence; the states of different hypotheses of phoneme sequences. At the final time-frame, the best sequence is selected for backtracking by considering its score. Therefore, for each timeframe, the acoustic state of phoneme in the phoneme sequence, the phoneme and the active word are determined and feature vectors of the command are decoded into the command words. However, this method is applicable for single chain models.\nThe token passing framework can be considered for performing the decoding in the factorial models of speech processing, which is called joint-decoding. For the joint-decoding, a new notion of state is constructed to perform the inference. In a single chain phoneme based decoding, word_id in the word lattice network, phoneme_id in the phonetic transcription of the word and acoustic state of phoneme, construct a rich state which can be used in decoding. In multiple chain models, the Cartesian product of this rich state can be considered as a joint-state. In Fig. 5 we can see jointstates of joint-tokens which are represented by the split ovals. Each part of this oval represents a token with a rich state where the token\u2019s corresponding word and phoneme are written in the oval instead of their ids. Additionally, phoneme state is denoted in the parenthesis in front of the phoneme name.\nThe joint-state of each token is used for doing the acoustic and temporal inference. The jointstate conditional likelihood of each frame is calculated using (18). This likelihood is calculated by considering the relation (18) conditioned on the joint-state of the joint-tokens in the active token list. Then the joint-tokens are propagated through the word lattice network, within a word through the phonemes and states of a phoneme to update the list and moving to the next frame. These steps\nare depicted by their corresponding examples in Fig. 5. The joint-token passing algorithm for phoneme based joint-decoding is simply stated in Fig. 6 to provide a clear insight into the problem.\nAs we can see, steps 1-3 in the algorithm are related to the first chain expansion in (19) and steps 4-5 are related to the second chain expansion in (20). The best joint-tokens in each joint-state are selected according to their scores. This is equivalent to the maximization steps of (19) and (20) which is depicted also in Fig. 5 by steps 3 and 5. In this figure, we can see the joint-token <red d(3), by ay(3) -243.36> is selected due to its score which is greater than <red d(3), by ay(3) - 283.72>. Moreover after each iteration of algorithm the weak tokens, the tokens with low scores, can be removed. This can be done by selecting some threshold or maintaining active token list size by some limit.\nToken scores are mainly affected by the joint-state likelihoods rather than state transition probabilities in each acoustic model. Moreover since in the challenge appearance of words in each word slot are equally likely, no other score updating due to this is involved in the steps of algorithm which can be considered for more general tasks.\nThe algorithm can support multiple phonetic transcription of a word. Also we can consider multiple chains in the factorial models of speech processing. The extension is straightforward by considering that going forward in the time must be performed chain-by-chain and then returning to the first chain; similar to the two-dimensional Viterbi algorithm. In fact, in the two-dimensional Viterbi algorithm, the dynamic programming is run within a time-frame in addition to running in time, which reduces the computational complexity of inference."}, {"heading": "3. Determining and adapting source models", "text": "Using speaker adapted source models in the factorial models for the mixed-speech recognition task of \u201cmonaural speech separation and recognition challenge\u201d, significantly improves recognition performance. This happens since discriminative features of speaker voices are also clues for improving acoustic inference within time-slices rather than dynamic constraints which applies to the whole utterance. Additionally, any adaptation related to the gain effect of the expression (1) for synthetizing the mixed-speech signal also significantly improves recognition performance. In the mixed-speech recognition task of the challenge, utterances of two speakers among 34 speakers of the GRID dataset are randomly selected for synthetizing each mixed-speech signal. During the test phase, using speaker labels of the test files is prohibited and therefore the identity of speakers for each utterance is unknown to the recognizer. On the other hand, TMR is also unknown to the recognizer. Identifying speakers and gain estimation is needed for determining audio source models of factorial models and adapting model parameters for improving inference and recognition performance which are described in the next two sub-sections."}, {"heading": "3.1. Speaker identification", "text": "In the challenge, model based methods such as the IBM system use speaker conditional likelihoods of each frame for the calculation of speaker id posteriors over the frames of the input mixed-speech signal. Then the speaker identification is done by voting high confidence identified frames which yields relatively precise speaker identification. This method is applied based on an underlying assumption which states that for the high resolution spectral features we can assume that one source is dominant in each time-frequency cell after the short-term discrete Fourier transform analysis. In the proposed method, we simply use a deep neural network for joint-speaker identification which also yields competitive results with relative simplicity and lower test-time computations compared to the previous methods.\nA deep feed forward architecture with 34 sigmoid output neurons are selected for construction of the network. In addition, high resolution Mel-scale filterbank energies in a context-window including 2\ud835\udf0f + 1 frames are selected as the network input (see Fig. 7). High resolution features of the synthetized mixed-speech signals with speaker identities are provided for the training phase. For each mixed-speech signal, two output neurons are designated as the desired output value\nexcept for the case where one speaker is presented as both target and masker in the signal. The network architecture and its input and target values are illustrated in Fig. 7. All neuron activation functions have logistic sigmoid shape.\nThe joint-speaker identification network is pre-trained layer-wise by restricted Boltzmann machines (RBM) generatively, which is necessary before the training phase. Then the network is trained by error back-propagation using speaker labels.\nIn the test-phase, for each context-window of frames of each utterance, network output is extracted. At first, uncertain neurons (neurons with low activation values) are eliminated by a threshold as follows:\n\ud835\udc67\ud835\udc61 \ud835\udc56 = {\n0 \ud835\udc67\ud835\udc61 \ud835\udc56 < \ud835\udf06 \ud835\udc67\ud835\udc61 \ud835\udc56 \ud835\udc67\ud835\udc61 \ud835\udc56 \u2265 \ud835\udf06 , \u2200 \ud835\udc56 \u2208 {1,2,3, \u2026 ,34}, \ud835\udc61 \u2208 {1,2,3, \u2026 , \ud835\udc47} (24)\nThen for these context-windows, two best speaker ids are selected for final voting as follows:\n\ud835\udc60\ud835\udc61 \ud835\udc57\n= {\ud835\udc67\ud835\udc61 \ud835\udc57 \ud835\udc57 \u2208 {\ud835\udc501, \ud835\udc502}\n0 \ud835\udc5c\ud835\udc64 (25)\nin which \ud835\udc50\ud835\udc56s are the indices of sorted activation values within a context-window; i.e. \ud835\udc2c\ud835\udc28\ud835\udc2b\ud835\udc2d((\ud835\udc67\ud835\udc61 \ud835\udc56)\ud835\udc56=1 34 ) = (\ud835\udc67\ud835\udc61 \ud835\udc50\ud835\udc56)\ud835\udc56=1 34 . Now speaker voting is done by the following summation:\n\ud835\udc60\ud835\udc56 = \u2211 \ud835\udc60\ud835\udc61 \ud835\udc56\ud835\udc47 \ud835\udc61=1 (26)\nAt this step speaker scores are sorted and speaker indices are stored in \ud835\udc50\ud835\udc56s; i.e. \ud835\udc2c\ud835\udc28\ud835\udc2b\ud835\udc2d((\ud835\udc60 \ud835\udc56)\ud835\udc56=1 34 ) = (\ud835\udc60\ud835\udc50\ud835\udc56)\ud835\udc56=1 34 . Then, recognized speakers are selected using a second threshold, \ud835\udf03, and the limit of maximum two speakers. The \ud835\udf03 threshold is used to eliminate the second speaker in the same speaker test utterance cases.\n{\ud835\udc50\ud835\udc56|\ud835\udc60 \ud835\udc50\ud835\udc56 > \ud835\udf03, \u2200\ud835\udc56 \u2208 {1,2}} (27)\nTwo thresholds, \ud835\udf06 and \ud835\udf03 can be tuned by the validation set which will be discussed in the\nexperiments."}, {"heading": "3.2. Gain estimation and model adaptation", "text": "Recalling the expression (1) for the generation of mixed-speech signal, relative speech signal energy of the speaker a and b can be modeled by a gain coefficient (\ud835\udc54). Here, at first, by considering one audio source, the effect of gain coefficient in each step of feature extraction is investigated. Starting from the power spectrum of speech frames (|\ud835\udc99|2), for extracting MFCC features we have:\n\ud835\udc99\u2032 c = \ud835\udc02 \ud835\udc25\ud835\udc28\ud835\udc20(\ud835\udc16|\ud835\udc54\ud835\udc99|2) (28)\nagain, \ud835\udc54 is the gain, \ud835\udc16 is the matrix of Mel shaped averaging filters, and \ud835\udc02 is the [truncated] DCT matrix. Then we have:\n\ud835\udc99\u2032 c = \ud835\udc02(\ud835\udc25\ud835\udc28\ud835\udc20(\ud835\udc16|\ud835\udc99|2) + 2 \ud835\udc25\ud835\udc28\ud835\udc20(\ud835\udc54\ud835\udfcf)) = \ud835\udc99 c + \ud835\udc990 (29)\nwhere \ud835\udfcf is column vector of one and \ud835\udc99 c is the original MFCC features. Then, \ud835\udc990 is the gain compensated vector as:\n\ud835\udc990 = 2 log(\ud835\udc54) \ud835\udc02 \u00d7 \ud835\udfcf = [\ud835\udc540, 0,0, \u2026 ,0] \ud835\udc47 (30)\nwhere the \ud835\udc540 is defined as:\n\ud835\udc540 = 2 log(\ud835\udc54)\u221a\ud835\udc5a (31)\nin which \ud835\udc5a is the number of Mel filters in the filterbank (the number of columns of \ud835\udc16).\nAs we can see in (30), gain factor only affects the first MFCC coefficient (MFCC\u2019s 0th order coefficient). For the delta and acceleration features based on (13), we can observe that the gain has no effect on delta coefficients and consequently on acceleration coefficients. Therefore, only the first element of each feature vector is affected by the gain coefficient and we must adjust the related parameters of the source models for the adaptation. Equation (29) shows that a constant vector is added to the original MFCC features. Therefore, only Gaussian mean vectors must be updated and covariance matrices remain unaltered. In fact, only the first element of Gaussian means in the source models must be adjusted.\nIn the test phase, the gain coefficient and therefore \ud835\udc540 is unknown and it must be estimated. The general method for this estimation is to use maximum likelihood estimators where for the appropriate gain coefficient, the likelihood of test utterance when decoded in the adapted model is maximized. i.e.:\n\ud835\u0302\udc54\u2217 = argmax \ud835\udc54\ud835\udc56 \ud835\udcdb\u2217(\ud835\udc9a1:\ud835\udc47|\ud835\udc54\ud835\udc56) (32)\nThe method is applicable in this way, because factorial speech processing models are generative models which model the way clean audio sources are mixed for creating output signals. Therefore, a more matched parameter set yields to a greater decoding likelihood for the test utterance. During the challenge, model based methods have used this technique in their gain estimation step.\nIn our work, we use deep architectures for this step once again. In fact, similar architecture to joint-speaker identification is used here for gain estimation except that a linear neuron is used for estimating the gain coefficient. During the training phase, mixed-speech training utterances with various gains are synthetized for the network with its gain as the desired output. Feature extraction and framing is similar to joint-speaker identification by deep networks. This method for estimating the masker gain is straightforward compared to the past MLE methods. However, it is applicable to the task of this paper, since only the gain of the masker utterance is adjusted in this task. At the final step, based on the estimated gain, model parameters are updated and inference is done for decoding."}, {"heading": "4. Experiments", "text": "The monaural speech separation and recognition challenge is designed based on the GRID dataset (Cooke et al., 2006). Speech material of this task consists of 1000 utterances of 34 speakers which are partitioned evenly for the training and test phase. Synthetized mixed-speech of randomly selected joint-speakers are extracted from 500 utterances of the test part for each speaker in 6 different TMRs including 6, 3, 0, -3, -6 and -9 dBs. For each TMR, 600 and 300 mixed-speech signals are synthetized for the test and development set respectively. While identification of jointspeakers and the mixed-speech TMR can be understood by the filenames and folders, it cannot be used in the test phase. In this task, recognition of the letter and digit of the target speaker is the objective and the scoring is done by the scoring scripts which are provided by the organizers. The\ntarget speaker always uses the \u201cwhite\u201d color and the masker does not and we can use this clue to identify the target speaker. For this challenge, recognition performance of human listeners is evaluated for comparing automated systems with humans as the gold standard (Fig. 11).\nIn this section, source modeling, feature extraction, grammar definition and extraction of the task dictionary are explained. Then based on experiments conducted on the development set, selection of feature space and tuning speaker identification hyperparameters are done and the results are provided. Finally, the main experiments are conducted and the results are compared to the past super-human results."}, {"heading": "4.1. Source Models, Grammar and Lexicon", "text": "Three state HMM monophones are selected as source acoustic models which are trained by the HTK tool (Young et al., 2009). The models are first initialized by the TIMIT dataset (Garofolo et al., 1993), then re-estimated by training speech utterances from 34 speakers of the challenge corpus. The number of mixture components is increased from 8 to 32 for each monophone state. Then models are adapted for each speaker and the number of mixture components is decreased to 8 and 4 components which are used in the experiments. The four component speaker adapted models are used for feature selection and phase factor adjustments and the eight component models are used for the main experiments. MFCC features are used for acoustic modeling and feature extraction is done by the Voicebox toolbox (Brookes, 1997). For feature extraction, 27 Mal-scale filters are used and framing is done by 10ms frame shift and 25ms frame length. Number of 13 through 23 MFCC coefficients (excluding delta and acceleration coefficients) are extracted using the truncated DCT matrix (its effect will be discussed further in the experiments).\nHMM initialization by the TIMIT dataset requires no Grammar and Dictionary and 40 monophones (including silence model) are trained by the TIMIT dataset. For HMM re-estimation, due to the word level transcription of training speech materials, the definition of lexicon and Grammar is required. We use the BEEP dictionary (Robinson, 1997) for the phonetic transcription of 51 different words of the challenge (see Fig. 1). Additionally, the word lattice network is created based on the task grammar which is already provided in (Cooke et al., 2010) and also presented in Fig. 1."}, {"heading": "4.2. Feature selection and phase factor determination", "text": "Before initiating the main experiments, selection of appropriate features for this task and adjusting the phase factor in the mismatch function of (9) for performing the acoustic inference must be done. At this step, MFCC features and their first and second order derivatives are selected as the feature type. Now, various static phase factors are selected for the test on the development set. Configuration of feature extraction is similar to the task baseline system except that MFCC 0th\u2019s coefficient is used in features instead of the logarithm of frame energy and 27 filters are used in the Mel-scale filter bank.\nAt this step, no speaker identification and gain adaptation is carried out and identity of the speakers are considered to be known during the tests. Since at this step, gain adaptation is not performed yet, only performance of the system in near zero TMRs are considered for phase factor (alpha) selection; i.e. 3, 0 and -3 dB TMRs. Fig. 8 shows the average performance of these three TMRs against different alpha values. The best alpha value is selected for the next experiments. Alpha candidates are selected almost similar to the work of (Li et al., 2009) and the selected alpha is 2. This alpha value is not valid regarding its support set (the reader is referred to (Li et al., 2009) or (Van Dalen, 2011) for explanations of this theoretical contradiction).\nFor selecting the appropriate feature type, a different combination of MFCC features are selected for evaluation on the development set which their results are listed in Table 1. These feature types are used for source modeling and acoustic inference with the selected alpha value. Again here, only system performance is considered for near zero TMRs.\nMFCC0DA(39) and the second is MFCC0D(38)."}, {"heading": "4.3. Speaker identification results", "text": "Joint-speaker identification of this task is done by a deep neural network. The network contains 5 layers, including 4 hidden layers (each layer has 2500 logistic sigmoid neurons) and one output layer which consists of 34 neurons for joint-speaker identification. The network is fed with concatenation of 21 high resolution power spectral features (\ud835\udf0f = 10) which are extracted by installing a dense Mel-scale filterbank on the frame power spectrum (110 filters are included in the filterbank). Pre-training of the network is done layer-wise by RBMs and then it is fined-tuned with the speaker labels by the DeeBNet toolbox (Keyvanrad and Homayounpour, 2014). About 7000 training utterances are synthetized by mixing target and masker speaker utterances extracted from the training data in different TMR values similar to the test data. The related speaker labels are attached to the training data for the network fine-tuning.\nSpeaker adaptation hyperparameters , \ud835\udf06 = 0.99 and \ud835\udf03 = 0.1, are adjusted by the development set according to (27). Based on the selected thresholds, speaker identification results are summarized in Table 2 and compared to the IBM system speaker identification results.\nThe IBM system, uses GMM speaker models and speaker likelihoods for speaker identification in two steps. At first, a list of speaker candidates (called finalists) are selected based on speaker id posteriors. In the second phase, different combinations of joint-speakers are selected from the speaker finalists to then performing a search for finding the best joint-speakers beside their associated gain. Joint-speaker ids which their gain adapted models maximize utterance likelihood are selected as the identified speakers. Comparing our system to IBM\u2019s super-human system, it can be seen that competitive results are achieved in a single phase identification. As another observation, we can see that in near zero TMRs both systems perform better than the cases when one speaker dominates the other (speech of one speaker masks the other speaker and prevents correct identification)."}, {"heading": "4.4. Joint-decoding by gain adapted models, pushing forward past super-human results", "text": "For the main experiments, speaker identification and TMR information cannot be used during the decoding. Joint-identification of speakers and gain estimation is done by the trained deep neural networks and the rest of the recognition procedure is carried as mentioned before which is illustrated in Fig. 9.\nThe role of the two target identifiers shown in this figure are explained as follows. The jointspeaker identification network can identify those speakers which have some speech footprints in the mixed-speech signal, even in extreme TMRs; i.e. 6, -6 and -9 dBs. But it cannot determine the masker speaker which is needed for further model adaptation. For resolving this ambiguity, the gain adapted model which maximizes the likelihood of the test utterance is selected as the masker model. After that, joint-decoding is performed based on the proposed algorithm (see Fig. 6). At this step, another check for the possible target-masker swapping is done for reporting the\nrecognized letter and digit of the target speaker. This step provides no change on extreme TMRs, since the adapted masker model can distinguish between the target and masker utterances during the joint-decoding.\nRecognition results of the selected feature types based on the proposed procedure are provided in Table 3. Additionally, recognition results when we select and adapt source models based on test file information (oracle speaker ids) are also provided in this table which removes the effect of the speaker identification phase. As it was expected based on the results of Table 1, the MFCC-delta features perform better than the MFCC-delta-acceleration features. This can be explained by the approximation involved during the acoustic inference of the dynamic coefficients. The approximation assumes that source states are not changed during the extraction of dynamic coefficients of each frame. Moreover, by comparing the results to the oracle system, we can see more performance upgrade for this task is possible when we use a more accurate speaker identifier.\nDetailed recognition accuracy for MFCC-delta features for different TMRs over three sets of test utterances is also provided in Table 4. As it can be seen, speaker difference clues can improve recognition accuracy which is apparent when the results of the \u201cSame Talker\u201d condition are compared to the other cases. Additionally, on the other axis of the \u201cSame Talker\u201d condition we can see that the only distinguishing factor is the relative energy of voices of target and masker speakers. In this case, recognition performance is degraded in near zero conditions (see Fig. 10 for the gain adapted case). On the other hand, it was expected that the best results can be achieved in \u201cDifferent Gender\u201d test utterances for the best TMRs, i.e. 3 and 6 dBs.\nA further comparison for decoding results of gain adapted source (masker) models and unaltered models can be done by investigating Fig. 10. It can be seen that gain adaptation is substantially beneficial in the same talker condition. This is due to the fact that both source models are the same and no other discriminative clue is available in this case except their first dimension parameters which is changed in the gain adaptation phase. Moreover, it can be seen that gain adaptation is more beneficial in extreme TMR conditions in all test sets and has no benefit in zero TMR. In the zero TMR case, some minor adaptation may occur due to the variation of the volume of speaker voice in different recording trials.\nFor the final comparison, the past best-achieved results for this challenge, to the best of our knowledge, are provided in Fig. 11. The first super-human system was the IBM system (Hershey et al., 2010; Rennie et al., 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al., 2015). The IBM super-human system has two phases for performing recognition. The first phase is to separate speech of the target and masker speakers, which its results were amazing at that time, especially their extension to the task which supports up to five interfering speakers. Then based on the separated speech it performs decoding. The speech separation phase of the IBM system was done based on factorial models. In their system, the feature vector state-conditional posteriors are calculated in the inference phase for the speech reconstruction. In their work, various configurations are considered for this task, but their best performance is provided in Table 5 and also plotted in Fig. 11.\nRecently, a DNN based recognizer from Microsoft research outperforms the IBM system. The system was not based on speaker adapted models but it uses energy difference clues by training a set of two DNNs for senone posterior extraction. One DNN is trained for high energy frames and the other for low energy frames. It can be seen that this system performs well in extreme TMRs where it can incorporate more energy difference clues in the mixed-speech signal. Since it uses instantaneous frame energy and it controls energy switching time-stamps, near zero TMR results\nare also surprising. Comparing the results to human listeners, it can be seen that human listeners cannot perform well in near zero TMRs while highly specialized joint-decoding systems can perform far better than humans in these conditions."}, {"heading": "5. Conclusion", "text": "In this paper, factorial speech processing models were presented by the language of probabilistic graphical models. Conditional probability distributions of these models are described; especially detailed derivation of their centric CPD, the CPD which combines source audio features. Moreover, the inference algorithm over these models is derived using the factor graphs. The idea of token passing for large vocabulary continuous speech recognition is extended in this work to support decoding of the task of monaural speech separation and recognition challenge. Therefore, a joint-token passing algorithm based on the idea of token passing is developed to perform jointdecoding on factorial speech processing models by the two-dimensional Viterbi algorithm. Moreover, a set of two task specific deep neural networks is suggested and used in this work for joint-speaker identification and gain estimation. Based on these networks, source speaker models are first selected and then adapted for compensating gain mismatch for the masker speaker in the test phase of the challenge. We can see that the developed single phase joint-speaker identification network makes the joint-speaker identification comparable to the past best system in the challenge. Additionally, we can see that gain adaptation is very effective in improving the system performance simply by adjusting one dimension of masker source models.\nThe proposed system of this work outperforms two past super-human systems for this challenge. Comparing it to the IBM factorial system, this superiority is due to performing direct inference over the mixed-speech signal which incorporates all uncertainty during the inference. Moreover, source modeling of low resolution feature spaces in our work is more accurate and convenient rather than facing with high resolution spectral features which was necessitated by the separation phase of the IBM system. Comparing it to the Microsoft DNN based recognizer, since\nwe used speaker adapted models rather than only high and low energy DNNs, the proposed system performs better than the DNNs by using discriminating information of the speakers. In a fair comparison, it can be mentioned that the Microsoft system uses only two main DNN models which is a more generic solution than our task specific speaker adapted models. As a future work, it is expected that by performing DNN based acoustic inference using speaker adapted DNNs, even the best-achieved results of this task can be more improved. The problem is to develop a mechanism for training jointly speaker adapted DNNs for performing acoustic inference."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Mr. Mohammad Ali Keyvanrad for his valuable arguments and support of DeeBNet toolbox in this work."}], "references": [{"title": "Developments and directions in speech recognition and understanding, Part 1 [DSP Education", "author": ["J. Baker", "L. Deng", "J. Glass", "S. Khudanpur", "C. Lee", "N. Morgan", "D. O\u2019Shaughnessy"], "venue": "IEEE Signal Process. Mag", "citeRegEx": "Baker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2009}, {"title": "Voicebox: Speech Processing Toolbox for MATLAB. Imperial College, Exhibition Road, London", "author": ["M. Brookes"], "venue": null, "citeRegEx": "Brookes,? \\Q1997\\E", "shortCiteRegEx": "Brookes", "year": 1997}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "J. Acoust. Soc. Am", "citeRegEx": "Cooke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cooke et al\\.", "year": 2006}, {"title": "Monaural speech separation and recognition challenge", "author": ["M. Cooke", "J.R. Hershey", "S.J. Rennie"], "venue": "Comput. Speech Lang", "citeRegEx": "Cooke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cooke et al\\.", "year": 2010}, {"title": "Robust continuous speech recognition using parallel model combination", "author": ["M.J.F. Gales", "S.J. Young"], "venue": "IEEE Trans. Speech Audio Process", "citeRegEx": "Gales and Young,? \\Q1996\\E", "shortCiteRegEx": "Gales and Young", "year": 1996}, {"title": "The Cocktail Party Problem", "author": ["S. Haykin", "Z. Chen"], "venue": "Neural Comput", "citeRegEx": "Haykin and Chen,? \\Q2005\\E", "shortCiteRegEx": "Haykin and Chen", "year": 2005}, {"title": "Factorial Models for Noise", "author": ["J.R. Hershey", "S.J. Rennie", "J. Le Roux"], "venue": "Robust Speech Recognition,", "citeRegEx": "Hershey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2012}, {"title": "Super-human multi-talker speech recognition: A graphical modeling approach", "author": ["J.R. Hershey", "S.J. Rennie", "P.A. Olsen", "T.T. Kristjansson"], "venue": "Comput. Speech Lang", "citeRegEx": "Hershey et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2010}, {"title": "A brief survey on deep belief networks and introducing a new object oriented toolbox (DeeBNet)", "author": ["M.A. Keyvanrad", "M.M. Homayounpour"], "venue": null, "citeRegEx": "Keyvanrad and Homayounpour,? \\Q2014\\E", "shortCiteRegEx": "Keyvanrad and Homayounpour", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "An analytic derivation of a phase-sensitive observation model for noise robust speech recognition, in: Interspeech", "author": ["V. Leutnant", "R. Haeb-Umbach"], "venue": null, "citeRegEx": "Leutnant and Haeb.Umbach,? \\Q2009\\E", "shortCiteRegEx": "Leutnant and Haeb.Umbach", "year": 2009}, {"title": "An Overview of Noise-Robust Automatic Speech Recognition", "author": ["J. Li", "L. Deng", "Y. Gong", "R. Haeb-Umbach"], "venue": "IEEEACM Trans. Audio Speech Lang. Process", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions", "author": ["J. Li", "L. Deng", "D. Yu", "Y. Gong", "A. Acero"], "venue": "Comput. Speech Lang", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "A vector Taylor series approach for environmentindependent speech recognition", "author": ["P.J. Moreno", "B. Raj", "R.M. Stern"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Moreno et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Moreno et al\\.", "year": 1996}, {"title": "Dynamic bayesian networks: representation, inference and learning (Ph.D. Thesis)", "author": ["K.P. Murphy"], "venue": "University of California", "citeRegEx": "Murphy,? \\Q2002\\E", "shortCiteRegEx": "Murphy", "year": 2002}, {"title": "Single-Channel Multitalker Speech Recognition", "author": ["S.J. Rennie", "J.R. Hershey", "P.A. Olsen"], "venue": "IEEE Signal Process. Mag. 27,", "citeRegEx": "Rennie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rennie et al\\.", "year": 2010}, {"title": "BEEP dictionary [WWW Document", "author": ["T. Robinson"], "venue": null, "citeRegEx": "Robinson,? \\Q1997\\E", "shortCiteRegEx": "Robinson", "year": 1997}, {"title": "Factorial models and refiltering for speech separation and denoising", "author": ["S.T. Roweis"], "venue": "in: Eighth European Conference on Speech Communication and Technology", "citeRegEx": "Roweis,? \\Q2003\\E", "shortCiteRegEx": "Roweis", "year": 2003}, {"title": "Statistical models for noise-robust speech recognition (Ph.D. Thesis)", "author": ["R.C. Van Dalen"], "venue": null, "citeRegEx": "Dalen,? \\Q2011\\E", "shortCiteRegEx": "Dalen", "year": 2011}, {"title": "Deep Neural Networks for Single-Channel Multi-Talker Speech Recognition", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEEACM Trans. Audio Speech Lang. Process", "citeRegEx": "Weng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weng et al\\.", "year": 2015}, {"title": "A review of large-vocabulary continuous-speech", "author": ["S. Young"], "venue": "IEEE Signal Process. Mag. 13,", "citeRegEx": "Young,? \\Q1996\\E", "shortCiteRegEx": "Young", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Robustness of automatic speech recognition systems (ASR) against diverse speech processing environments and adverse disturbing noises still remains as one of the important research areas in speech recognition systems (Baker et al., 2009; Li et al., 2014).", "startOffset": 217, "endOffset": 254}, {"referenceID": 11, "context": "Robustness of automatic speech recognition systems (ASR) against diverse speech processing environments and adverse disturbing noises still remains as one of the important research areas in speech recognition systems (Baker et al., 2009; Li et al., 2014).", "startOffset": 217, "endOffset": 254}, {"referenceID": 5, "context": "This problem is known as the cocktail party problem (Haykin and Chen, 2005) in which a person (or a system) wants to focus and follow the conversation of a speaker in a place where some people talk simultaneously.", "startOffset": 52, "endOffset": 75}, {"referenceID": 3, "context": "An interesting competition entitled \u201cMonaural speech separation and recognition challenge\u201d, addressing the challenges related to the second group of approaches was developed in 2006 (Cooke et al., 2010).", "startOffset": 182, "endOffset": 202}, {"referenceID": 3, "context": "Several teams attended this competition with different techniques for handling the problem (Cooke et al., 2010).", "startOffset": 91, "endOffset": 111}, {"referenceID": 7, "context": "Among the competitors, surprisingly, a team from IBM research presented a technique that outperforms the other techniques and even human listeners (Hershey et al., 2010).", "startOffset": 147, "endOffset": 169}, {"referenceID": 7, "context": "In the third step, two separated speeches are decoded using a single-talker recognition system (Hershey et al., 2010).", "startOffset": 95, "endOffset": 117}, {"referenceID": 19, "context": "This work incorporates a pair of Deep Neural Networks for acoustic inference over semi-joint hidden Markov models (HMM) (Weng et al., 2015); a network for the generation of senone posteriors of high energy utterances (or instantaneous high energy frames) and a network for low energy utterances.", "startOffset": 120, "endOffset": 139}, {"referenceID": 3, "context": "The objective of the monaural speech separation and recognition challenge is to recognize some keywords of a target speaker from a mixed-speech of the target and a masker speaker (Cooke et al., 2010).", "startOffset": 179, "endOffset": 199}, {"referenceID": 2, "context": "Mixed-speech signals of this task are artificially created from speech materials of the Grid corpus (Cooke et al., 2006).", "startOffset": 100, "endOffset": 120}, {"referenceID": 6, "context": "It is applicable for robust-ASR systems (Hershey et al., 2012) and is tailored exactly for the challenge of this paper.", "startOffset": 40, "endOffset": 62}, {"referenceID": 9, "context": "This model is based on factorial hidden Markov models which are used for modeling processes with multiple independent underlying Markov chains (Koller and Friedman, 2009).", "startOffset": 143, "endOffset": 170}, {"referenceID": 20, "context": "Each chain of a factorial speech processing model contains an HMM for modeling its audio source which is known as acoustic modeling (Young, 1996) in conventional speech recognition applications.", "startOffset": 132, "endOffset": 145}, {"referenceID": 9, "context": "By the acoustic interaction function we can infer the posterior distribution of source features and then the posterior distribution of source states in an \u201cevidential reasoning\u201d pattern (Koller and Friedman, 2009).", "startOffset": 186, "endOffset": 213}, {"referenceID": 6, "context": "This form of marginalization is seen before in noise-robust automatic speech recognition when one audio source is clean speech and the other is the disturbing noise (Hershey et al., 2012).", "startOffset": 165, "endOffset": 187}, {"referenceID": 17, "context": "For example, for high resolution power spectral features, the max-model (Roweis, 2003) is used for marginalizing-out source feature vectors.", "startOffset": 72, "endOffset": 86}, {"referenceID": 4, "context": "By using an appropriate mismatch function (Gales and Young, 1996) which combines source features into the observed feature vector, we can approximate each state conditional distribution by one Gaussian.", "startOffset": 42, "endOffset": 65}, {"referenceID": 13, "context": "This technique is known as approximation by the vector Taylor series (Moreno et al., 1996), VTS, which will be discussed later.", "startOffset": 69, "endOffset": 90}, {"referenceID": 4, "context": "In the terminology of noise-robust ASR, this expression is called \u201cmismatch function\u201d when one of the source signals is the disturbing noise (Gales and Young, 1996).", "startOffset": 141, "endOffset": 164}, {"referenceID": 10, "context": "Moreover \u03b1 can be considered independent from source feature vectors by an additional simplifying assumption (Leutnant and Haeb-Umbach, 2009).", "startOffset": 109, "endOffset": 141}, {"referenceID": 12, "context": "It is traditionally ignored in many applications (Van Dalen, 2011) or considered as a constant for all frequency bins (Li et al., 2009).", "startOffset": 118, "endOffset": 135}, {"referenceID": 12, "context": "10 For a detailed derivation, the reader is referred to the appendix of (Li et al., 2009).", "startOffset": 72, "endOffset": 89}, {"referenceID": 7, "context": "This algorithm which is derived here using general inference procedures of probabilistic graphical models is called the two-dimensional Viterbi algorithm (Hershey et al., 2010).", "startOffset": 154, "endOffset": 176}, {"referenceID": 9, "context": "For more details about clique tree construction and general inference over the graphical models, the reader can refer to (Koller and Friedman, 2009; Murphy, 2002).", "startOffset": 121, "endOffset": 162}, {"referenceID": 14, "context": "For more details about clique tree construction and general inference over the graphical models, the reader can refer to (Koller and Friedman, 2009; Murphy, 2002).", "startOffset": 121, "endOffset": 162}, {"referenceID": 20, "context": "It is a conceptual framework for decoding in large vocabulary continuous speech recognition tasks (Young, 1996; Young et al., 2009).", "startOffset": 98, "endOffset": 131}, {"referenceID": 2, "context": "The monaural speech separation and recognition challenge is designed based on the GRID dataset (Cooke et al., 2006).", "startOffset": 95, "endOffset": 115}, {"referenceID": 1, "context": "MFCC features are used for acoustic modeling and feature extraction is done by the Voicebox toolbox (Brookes, 1997).", "startOffset": 100, "endOffset": 115}, {"referenceID": 16, "context": "We use the BEEP dictionary (Robinson, 1997) for the phonetic transcription of 51 different words of the challenge (see Fig.", "startOffset": 27, "endOffset": 43}, {"referenceID": 3, "context": "Additionally, the word lattice network is created based on the task grammar which is already provided in (Cooke et al., 2010) and also presented in Fig.", "startOffset": 105, "endOffset": 125}, {"referenceID": 12, "context": "Alpha candidates are selected almost similar to the work of (Li et al., 2009) and the selected alpha is 2.", "startOffset": 60, "endOffset": 77}, {"referenceID": 12, "context": "This alpha value is not valid regarding its support set (the reader is referred to (Li et al., 2009) or (Van Dalen, 2011) for explanations of this theoretical contradiction).", "startOffset": 83, "endOffset": 100}, {"referenceID": 8, "context": "Pre-training of the network is done layer-wise by RBMs and then it is fined-tuned with the speaker labels by the DeeBNet toolbox (Keyvanrad and Homayounpour, 2014).", "startOffset": 129, "endOffset": 163}, {"referenceID": 7, "context": "The first super-human system was the IBM system (Hershey et al., 2010; Rennie et al., 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 15, "context": "The first super-human system was the IBM system (Hershey et al., 2010; Rennie et al., 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 19, "context": ", 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al., 2015).", "startOffset": 73, "endOffset": 92}], "year": 2016, "abstractText": "A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past superhuman results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first superhuman system and 2.7% absolute task performance improvement compared to its recent competitor.", "creator": "Microsoft\u00ae Word 2013"}}}