{"id": "1610.02391", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization", "abstract": "importantly we propose suggesting a technique based for models making convolutional active neural network ( f cnn ) - correlation based models easily more transparent by visualizing the regions of their input that are \" simultaneously important \" for manipulating predictions from these models - or visual communication explanations.", "histories": [["v1", "Fri, 7 Oct 2016 19:54:24 GMT  (8245kb,D)", "http://arxiv.org/abs/1610.02391v1", "17 pages, 16 figures"], ["v2", "Fri, 30 Dec 2016 07:19:35 GMT  (8596kb,D)", "http://arxiv.org/abs/1610.02391v2", "22 pages, 19 figures"], ["v3", "Tue, 21 Mar 2017 23:48:00 GMT  (9133kb,D)", "http://arxiv.org/abs/1610.02391v3", "24 pages, 22 figures. Adds bias experiments, and robustness to adversarial noise"]], "COMMENTS": "17 pages, 16 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ramprasaath r selvaraju", "michael cogswell", "abhishek das", "ramakrishna vedantam", "devi parikh", "dhruv batra"], "accepted": false, "id": "1610.02391"}, "pdf": {"name": "1610.02391.pdf", "metadata": {"source": "CRF", "title": "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization", "authors": ["Ramprasaath R. Selvaraju", "Abhishek Das", "Ramakrishna Vedantam", "Michael Cogswell", "Devi Parikh", "Dhruv Batra", "Virginia Tech"], "emails": ["dbatra}@vt.edu"], "sections": [{"heading": null, "text": "Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping (CAM) [43]. While CAM is limited to a narrow class of CNN models, Grad-CAM is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations (such as Guided Backpropagation [38]) to create a high-resolution class-discriminative visualization\n(Guided Grad-CAM).\nWe generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + Long Short Term Memory (LSTM) models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs.\nFinally, we design and conduct human studies to measure if\nar X\niv :1\nGuided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both networks make identical predictions, simply on the basis of their different explanations. Our code is available at https://github.com/ramprs/ grad-cam/ and a demo is available on CloudCV [2]1."}, {"heading": "1. Introduction", "text": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33]. While these deep neural networks enable superior performance, their lack of decomposability into intuitive and understandable components makes them hard to interpret [25]. Consequently, when today\u2019s intelligent systems fail, they fail spectacularly disgracefully, without warning or explanation, leaving a user staring at incoherent output, wondering why the system did what it did. Interpretability Matters. In order to build trust in intellegent systems and move towards their meaningful integration into our everyday lives, it is clear that we must build \u2018transparent\u2019 models that explain why they predict what they do. Broadly speaking, this transparency is useful at three different stages of Artificial Intelligence (AI) evolution. First, when AI is significantly weaker than humans and not yet reliably \u2018deployable\u2019 (e.g. visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions. Second, when AI is on par with humans and reliably \u2018deployable\u2019 (e.g., image classification [20] on a set of categories with enough training data), the goal is to establish trust with users. Third, when AI is significantly stronger than humans (e.g. chess or Go playing bots [35]), the goal of transparency & explanations is machine teaching [18] \u2013 i.e., a machine teaching a human on how to make accurate predictions. There typically exists a trade-off between accuracy and simplicity/interpretability. Classical rule-based or expert systems [16] were highly interpretable but not very accurate (or robust). Decomposable pipelines where each stage is handdesigned are thought to be more interpretable as each individual component assumes a natural intuitive explanation. By using deep models, we sacrifice interpretable modules for uninterpretable ones that achieve greater performance through greater abstraction (more layers) and tighter integration (end-to-end training).\n1http://gradcam.cloudcv.org\nDeep models are beginning to explore the spectrum between interpretability and accuracy. Zhou et al. [43] recently proposed a technique called Class Activation Mapping (CAM) for identifying discriminative regions used by a particular class of image classification CNNs (not containing any fullyconnected layers). In essence, this work trades off model complexity for more transparency into the working of the model. In contrast, we make existing state-of-the-art deep models interpretable without altering the architecture, thus avoiding a tradeoff between interpretability and accuracy. Our approach is a generalization of CAM [43] to any CNNbased architecture (CNNs with fully-connected layers, CNNs stacked with Recurrent Neural Networks (RNNs), etc.). What makes a good visual explanation? Consider image classification [9] \u2013 a \u2018good\u2019 visual explanation from the model justifying a predicted class should be (a) classdiscriminative (i.e. localize the category in the image) and (b) high-resolution (i.e. capture fine-grained detail). Fig. 1 shows outputs from a number of visualizations for the \u2018tiger cat\u2019 class (top) and \u2018boxer\u2019 (dog) class (bottom). Pixel-space gradient visualizations such as Guided Backpropagation [38] and Deconvolution [41] are high-resolution and highlight fine-grained details in the image, but are not class-discriminative (for example, the visualization for both \u2018cat\u2019 and \u2018dog\u2019 in Figures 1b and 1g are very similar). In contrast, our approach (Grad-CAM) shown in Figures 1c and 1h, is highly class-discriminative (i.e. the \u2018cat\u2019 explanation exclusively highlights the \u2018cat\u2019 regions, and not the \u2018dog\u2019 regions and vice versa). Note that these very closely match with the occlusion maps generated through multiple forward passes (Figures 1e and 1j). The spatial resolution of the most class-discriminative Grad-CAM maps is the size of the last convolution layer in the CNN, which is typically small (e.g. 14 \u00d7 14 in VGGNet [37]) and hence does not show fine-grained details. In order to combine the best of both worlds, we show that it is possible to fuse existing pixel-space gradient visualizations with Grad-CAM to create Guided Grad-CAM visualizations that are both high-resolution and class-discriminative. As a result, important regions of the image which correspond to a class of interest are visualized in high-resolution detail even if the image contains multiple classes, as shown in Figures 1d and 1i. When visualized for \u2018tiger cat\u2019, Guided GradCAM not only highlights the cat regions, but also highlights the stripes on the cat which is important for predicting that particular variety of cat. To summarize, our contributions are as follows: (1) We propose a class-discriminative localization technique called Gradient-weighted Class Activation Mapping (GradCAM) that can be used to generate visual explanations from any CNN-based network without requiring architectural changes. We evaluate Grad-CAM for weakly-supervised image localization on ImageNet where it outperforms pixel-\nspace gradients. (2) To illustrate the broad applicability of our technique across tasks, we apply Grad-CAM to state-of-the-art classification, image captioning and visual question answering models, effectively visualizing the image support for predictions from such networks. For image classification, our visualizations lend insight into failure modes of current generation CNNs, showing that seemingly unreasonable predictions have reasonable explanations. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + Long Short Term Memory (LSTM) models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. (3) We design and conduct human studies to show that Guided Grad-CAM explanations are class-discriminative and help humans not only establish trust, but also helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both networks make identical predictions, simply on the basis of their different explanations."}, {"heading": "2. Related Work", "text": "Our work draws on recent work in CNN visualizations, model trust assessment, and weakly-supervised localization. Visualizing CNNs. A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.e. change in intensities of these pixels have the most impact on the prediction\u2019s score). Specifically, Simonyan et al. [36] visualize partial derivatives of predicted class scores w.r.t. pixel intensities, while Guided Backpropagation [38] and Deconvolution [41] make modifications to \u2018raw\u2019\ngradients that result in qualitative improvements. Despite producing fine-grained visualizations, these methods are not class-discriminative. Visualizations with respect to different classes are nearly identical (see Figures 1b and 1g). Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10]. Although these can be high-resolution and class-discriminative, they visualize a model overall and not predictions for specific input images. Assessing Model Trust. Motivated by notions of interpretability [25] and assessing trust in models [34], we evaluate Grad-CAM visualizations in a manner similar to Ribeiro et al. [34] via human studies to show that they can be important tools for users to evaluate and place trust in automated systems. Weakly supervised localization. Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43]. Most relevant to our approach is the Class Activation Mapping (CAM) approach to localization [43]. This approach modifies image classification CNN architectures replacing fully-connected layers with convolutional layers and global average pooling [23], thus achieving class-specific feature maps. Others have investigated similar methods using global max pooling [31] and log-sum-exp pooling [32]. A drawback of CAM is that it requires feature maps to directly precede softmax layers, so it is only applicable to a particular kind of CNN architectures performing global average pooling over convolutional maps immediately prior to prediction (i.e. conv feature maps\u2192 global average pooling \u2192 softmax layer). Such architecures may achieve inferior\naccuracies compared to general networks on some tasks (e.g. image classification) or may simply be inapplicable to other tasks (e.g. image captioning or VQA). We introduce a new way of combining feature maps using the gradient signal that does not require any modification in the network architecture. This allows our approach to be applied to any CNN-based architecture, including those for image captioning and visual question answering. For a fully-convolutional architecture, Grad-CAM reduces to CAM (ignoring rectification/normalization for visualization purposes). Thus, Grad-CAM is a strict generalization to CAM. Other methods approach localization by classifying perturbations of the input image. Zeiler and Fergus [41] perturb inputs by occluding small patches and classifying the occluded image, typically resulting in lower classification scores for relevant objects when those objects are occluded. This principle is applied for localization in [4]. Oquab et al. [30] classify many patches containing a pixel then average these patch class-wise scores to provide the pixel\u2019s class-wise score. Unlike these methods, our approach achieves localization in one shot; it only requires a single forward and a partial backward pass per image and thus is typically an order of magnitude more efficient. In concurrent work Zhang et al. [42] introduce a probabilistic Winner-Take-All formulation for modelling the top-down attention for neural classification models."}, {"heading": "3. Approach", "text": "We briefly recap the localization approach presented in CAM [43], and then describe our generalization, Grad-CAM. Then we describe how this class-discriminative but coarse localization technique can be combined with high-resolution visualizations obtained using Deconvolution and Guided Backpropagation to obtain both desirable properties (highresolution and class-discrimination). Class Activation Mapping (CAM). Recall that CAM [43] produces a localization map for an image classification CNN with a specific kind of architecture where global average pooled convolutional feature maps are fed directly into a softmax. Specifically, let the penultimate layer produce K feature maps Ak \u2208 Ru\u00d7v of width u and height v. These feature maps are then spatially pooled using Global Average Pooling (GAP) and linearly transformed to produce a score yc for each class c\nyc = \u2211 k\nwck\ufe38\ufe37\ufe37\ufe38 class feature weights\nglobal average pooling\ufe37 \ufe38\ufe38 \ufe37 1\nZ \u2211 i \u2211 j\nAkij\ufe38\ufe37\ufe37\ufe38 feature map\n(1)\nTo produce a localization map LcCAM \u2208 Ru\u00d7v for class c, CAM computes the linear combination of the final feature maps using the learned weights of the final layer:\nLcCAM = \u2211 k wckA k\n\ufe38 \ufe37\ufe37 \ufe38 linear combination\n. (2)\nThis is normalized to lie between 0 and 1 for visualization purposes. To apply CAM to a network which uses multiple fully-connected layers before the final layer, the fully connected layers are replaced with convolutional ones and the network is re-trained. Gradient-weighted Class Activation Mapping. In order to obtain the class-discriminative localization map GradCAM LcGrad-CAM \u2208 Ru\u00d7v in general architectures, we first compute the gradient of yc with respect to feature maps A of a convolutional layer, i.e. \u2202y c\n\u2202Akij . These gradients flowing\nback are global-average-pooled to obtain weights \u03b1ck:\n\u03b1ck =\nglobal average pooling\ufe37 \ufe38\ufe38 \ufe37 1\nZ \u2211 i \u2211 j\n\u2202yc\n\u2202Akij\ufe38 \ufe37\ufe37 \ufe38 gradients via backprop\n(3)\nThis weight \u03b1ck represents a partial linearization of the deep network downstream from A, and captures the \u2018importance\u2019 of feature map k for a target class c2. Empirically, using the averaged gradient through Global Average Pooling (GAP) is more robust to noise in the gradients and thus leads to better localizations than other choices like taking the Global Max Pooling, as shown in the supplementary. In general, yc need not be the class score produced by an image classification CNN, and could be any differentiable activation. As in CAM, our Grad-CAM heat-map is a weighted combination of feature maps, but we follow this by a ReLU:\nLcGrad-CAM = ReLU (\u2211 k \u03b1ckA k ) \ufe38 \ufe37\ufe37 \ufe38 linear combination\n(4)\nNotice that this results in a coarse heat-map of the same size as the convolutional feature maps (14\u00d7 14 in the case of last convolutional layers of VGG [37] and AlexNet [22] networks). For visualization as in Figure 1c, 1h, and others, LcGrad-CAM is normalized so the values lie between 0 and 1. For architectures where CAM is applicable \u2013 i.e., fullyconvolutional CNNs with A being the output of the final conv layer, followed by global average pooling and softmax \u2013 the weights used in CAM wck are precisely \u03b1 c k. Other than the ReLU in (4), this makes Grad-CAM a generalization of CAM. The motivation for the ReLU is the following \u2013 we are only interested in the features that have a positive influence on the class of interest, i.e. pixels whose intensity should be\n2 We performed some initial experiments using variants of gradients used in the computation of \u03b1ck . The \u201cgradients\u201d computed using Deconvolution led to non-discriminative localizations while Guided Backpropagation led to somewhat discriminative localizations, though much less so than normal unmodified gradients.\nincreased in order to increase yc. Since the feature maps A are already non-negative, we rectify the heat-map obtained in (4) to highlight such pixels. And as expected, without this ReLU, localization maps sometimes highlight more than just the desired class and achieve lower localization performance (see supplementary). Intuitively, negative values indicate pixels that are likely to belong to other categories in the image, and the application of ReLU excludes them. Figures 1c and 1h show Grad-CAM visualizations for \u2018tiger cat\u2019 and \u2018boxer (dog)\u2019 respectively. More Grad-CAM visual explanations can be found in the supplementary.\nNote that the above generalization also allows us to generate visual explanations from CNN-based models that cascade convolutional layers with more complex interactions. Indeed, we apply Grad-CAM to \u201cbeyond classification\u201d tasks and models that utilize CNNs for image captioning and Visual Question Answering (VQA) (Sec. 7).\nGuided Grad-CAM. While Grad-CAM visualizations are class-discriminative and localize relevant image regions well, they lack the ability to show fine-grained importance like pixel-space gradient visualization methods (Guided Backpropagation and Deconvolution). For example in Figure 1c, Grad-CAM can easily localize the cat region; however, it is unclear from the low-resolutions of the heat-map why the network predicts this particular instance is \u2018tiger cat\u2019. In order to combine the best aspects of both, we fuse Guided Backpropagation and Grad-CAM visualizations via pointwise multiplication (LcCAM is first up-sampled to the input image resolution using bi-linear interpolation). Fig. 2 bottomleft illustrates this fusion. This visualization is both highresolution (when the class of interest is \u2018tiger cat\u2019, it identifies important \u2018tiger cat\u2019 features like stripes, pointy ears and eyes) and class-discriminative (it shows the \u2018tiger cat\u2019 but not the \u2018boxer (dog)\u2019). Replacing Guided Backpropagation with Deconvolution in the above gives similar results, but we found Deconvolution to have artifacts (and Guided Backpropagation visualizations were generally noise-free), so we chose Guided Backpropagation over Deconvolution.\nA number of works have asserted that as the depth of a CNN increases, higher-level visual constructs are captured [5, 28]. Furthermore, convolutional layers naturally retain spatial information which is lost in fully-connected layers, so we\nexpect the last convolutional layers to have the best compromise between high-level semantics and detailed spatial information. We provide Grad-CAM visualizations in Fig. 3 computed at various convolutional layers by replacing A with feature maps of indicated layer to demonstrate this effect."}, {"heading": "4. Weakly-supervised Localization", "text": "In this section, we evaluate the localization capability of Grad-CAM in the context of image classification. The ImageNet localization challenge [9] requires competing approaches to provide bounding boxes in addition to classification labels. Similar to classification, evaluation is performed for both the top-1 and top-5 predicted categories. Similar to Zhou et al. [43], given an image, we first obtain class predictions from our network. Next, we generate Grad-CAM localization maps for each of the predicted classes and binarize with threshold of 15% of the max intensity. This results in connected segments of pixels and we draw our bounding box around the single largest segment. We evaluate the pretrained off-the-shelf VGG-16 [37] model from the Caffe [17] Model Zoo. Following ILSVRC evaluation, we report both top-1 and top-5 localization error on ILSVRC-15 validation set in Table. 1. Grad-CAM localization errors are lower than those achieved by Simonyan et al. [36] for the VGG-16 model, which uses grabcut to postprocess image space gradients into heat maps. We also see that CAM achieves a slighly better localization, but requires a change in the VGG architecture, necessitates re-training, and achieves worse top-1 val classification error (2.76% increase) [43], whereas our model makes no compromise on classification accuracy."}, {"heading": "5. Evaluating Visualizations", "text": "Our first human study evaluates the main premise of our approach: are Grad-CAM visualizations more class-\ndiscriminative than previous techniques? Moreover, we want to understand if our class-discriminative interpretations can lead an end user to trust the visualized models. For these experiments, we use VGG and AlexNet CNNs finetuned on PASCAL VOC 2007 train set, and the validation set is used to generate visualizations."}, {"heading": "5.1. Evaluating Class Discrimination", "text": "We select images from PASCAL VOC 2007 val set that contain exactly two annotated categories, and create visualizations for one of the classes. For both VGG-16 and AlexNet CNNs, we obtain visualizations using four techniques: Deconvolution, Guided Backpropagation, and Grad-CAM versions of each these methods (Deconvolution Grad-CAM and Guided Grad-CAM). We show visualizations to workers on Amazon Mechanical Turk (AMT) and ask them \u201cWhich of the two object categories is depicted in the image?\u201d, as shown in Fig. 4a. The two PASCAL categories present in the original image are shown as options. This task measures if people can tell from the visualization which class is being visualized.\nthe category being visualized in 61.23% of cases (compared to 44.44% for Guided Backpropagation; thus, Grad-CAM improves human perfomance by 16.79%). Similarly, we find that Grad-CAM helps make both Deconvolution and Guided Backpropagation more class-discriminative. Guided Grad-CAM performs the best among all the methods. Interestingly, our results seem to indicate that Deconvolution is more class discriminative than Guided Backpropagation, although Guided Backpropagation is more aesthetically pleasing than Deconvolution. To the best of our knowledge, our evaluations are the first to quantify these subtle differences."}, {"heading": "5.2. Evaluating Trust", "text": "Given two prediction explanations, we want to evaluate which of them seems more trustworthy. We use AlexNet and VGG-16 to compare Guided Backpropagation and Guided Grad-CAM visualizations, noting that VGG-16 is known to be more reliable than AlexNet with an accuracy of 79.09 mAP vs. 69.20 mAP for AlexNet. In order to tease apart the efficacy of the visualization from the accuracy of the model being visualized, we consider only those instances where both models made the same prediction as ground truth.Given a visualization from AlexNet and one from VGG-16, and the name of object predicted by both the networks, workers are instructed to rate the reliability of the models relative to each other on a scale of clearly more reliable (+/-2), slightly more reliable (+/-1), and equally reliable (0). This interface is shown in Fig. 4c. To eliminate any biases in the study, VGG and AlexNet were assigned to be model1 with approximately equal probability. Remarkably, we find that human subjects are able to identify the more accurate classifier (VGG over AlexNet) despite viewing identical predictions from the two, simply from the different explanations generated from the two. With Guided Backpropagation, humans assign VGG an average score of 1.00 which means that it is slightly more reliable than AlexNet, while Guided Grad-CAM achieves\na higher score of 1.27 which is closer to the option saying that VGG is clearly more reliable. Thus our Guided GradCAM visualization can help users place trust in a model that can generalize better, based on individual prediction explanations.\n5.3. Faithfulness vs. Interpretability\nFaithfulness of a visualization to a model is its ability to accurately explain the function learned by the model. Naturally, there exists a tradeoff between the interpretability and faithfulness of a visualization: a more faithful visualization is typically less interpretable and vice versa. In fact, one could argue that a fully faithful explanation is the entire description of the model, which in the case of deep models is not interpretable/easy to visualize. We have verified in previous sections that our visualizations are reasonably interpretable. We now evaluate how faithful they are to the underlying model. One expectation is that our explanations should be locally accurate or faithful. That is, in the vicinity of the input data point, our explanation should be faithful to the model [34]. For comparison, we need a reference explanation with high local-faithfulness. One obvious choice for such a visualization is image occlusion [41], where we mask out different regions of the input image, and perform multiple CNN evaluations to observe how the resulting score for the class of interest changes. If masking out a region causes the score for the particular category of interest to decrease, that region\nis considered to be important for predicting that class. We compare Guided Backpropagation vs. Guided Grad-CAM on their correlation with occluded scores. We divide the input image into an 8\u00d78 grid, occlude each region, and perform a forward pass through the VGG-16 CNN. At each location on the grid we store the rectified value of the difference in scores between the masked version and the original score for the class of interest. The regions with highest rectified difference in scores are the most discriminative for a given class. Next, we compute the energy of a test visualization method (Guided Backpropagation and Guided Grad-CAM) in each region of the grid by taking the L-1 norm of the intensities. We concatenate values from every grid to compute the rankcorrelation between the visualization method and the occlusion map, averaged over the 2510 images from PASCAL VOC 2007 validation set. We find that Guided Grad-CAM has a higher mean rank-correlation with occlusion maps than Guided Backpropagation (0.261 vs. 0.168). This shows that Guided Grad-CAM is more faithful to the original model than Guided Backpropagation."}, {"heading": "6. Analyzing Failure Modes for Image Classification", "text": "We use Guided Grad-CAM to analyze failure modes of the VGG-16 CNN on ImageNet classification [9]. In order to see what mistakes a network is making we first get a list of examples that the network (VGG-16) fails to\nclassify correctly. For the misclassified examples, we use Guided Grad-CAM to visualize both the correct class and the predicted class. A major advantage of Guided Grad-CAM over other methods is its ability to more usefully investigate and explain classification mistakes, since our visualizations are high-resolution and more class-discriminative. As seen in Fig. 5, some failures are due to ambiguities inherent in ImageNet classification. We can also see that seemingly unreasonable predictions have reasonable explanations, which is a similar observation to HOGgles [40]."}, {"heading": "7. Image Captioning and VQA", "text": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks. We find that Grad-CAM leads to interpretable visual explanations for these tasks as compared to baseline visualizations which do not change noticeably across different predictions."}, {"heading": "7.1. Image Captioning", "text": "In this section, we visualize spatial support for a simple image captioning model (without attention) using Grad-CAM visualizations. More specifically, we build on top of the publicly available \u2018neuraltalk2\u20193 implementation [21] that makes use of a finetuned VGG-16 CNN for images and an LSTMbased language model. Given a caption, we compute the gradient of its log probability w.r.t. units in the last convolutional layer of the CNN (conv5_3 for VGG-16) and generate Grad-CAM visualizations as described in Section 3. Results are shown in Fig. 6a. For first example, the Grad-CAM maps for the generated caption localizes every occurrence of both the kites and people inspite of their relatively small size. In the top right example, see how Grad-CAM correctly highlights the pizza and the man, but ignores the woman nearby, since \u2018woman\u2019 is not mentioned in the caption. More\n3https://github.com/karpathy/neuraltalk2\nqualitative examples can be found in the supplementary. Comparison to dense captioning. Johnson et al. [19] recently introduced the Dense Captioning (DenseCap) task that requires a system to jointly localize and caption salient regions in a given image. The model proposed in [19] consists of a Fully Convolutional Localization Network (FCLN) and an LSTM-based language model that produces both bounding boxes for regions of interest and associated captions in a single forward pass. Using the DenseCap model, we generate region-specific captions. Next, we visualize GradCAM localizations for these region-specific captions using the simple captioning model described earlier (neuraltalk2). Interestingly, we observe that Grad-CAM localizations correspond to regions in the image that the DenseCap model described, even though the holistic captioning model was not trained with any region or bounding-box level annotations. Results are shown in Fig. 6b."}, {"heading": "7.2. Visual Question Answering", "text": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions. The image and the question representations are fused to predict the answer (typically a distribution of the top-1000 frequent answers in the VQA-train set). Since this is a classification problem, the visualization is similar to the image classification case: given an image and a question, pick an answer and use Grad-CAM to show image evidence that supports the answer. Qualitative Examples. We explain the answers of a VQA model [27] that combines a CNN and an LSTM representation through a pointwise multiplication. Despite the complexity of the task, involving both visual and language components, the evidence found by Grad-CAM in suprisingly intuitive and informative. For instance, consider the image in Fig. 7, paired with the question \u201cWhat color is the firehydrant\u201d. Creating Grad-CAM visualizations w.r.t. the predicted answer \u201cred\u201d shows the lower red regions of the\nfirehydrant. When asked to provide support for \u201cyellow\u201d (or \u201cyellow and red\u201d), just the top (or the entirety) of the hydrant is highlighted. Additional qualitative results including Grad-CAM and Guided Grad-CAM visualizations for VQA models with different CNNs can be found in the supplementary material. Comparison to Human Attention. Das et al. [8] collected human attention maps for a subset of the VQA dataset [3]. These maps have high intensity where humans looked in order to answer a visual question. Human attention maps are compared to Grad-CAM visualizations of the simple VQA model introduced above [27] on 1374 question-image (QI) pairs from the validation set of the VQA dataset [3]. We use the rank correlation evaluation protocal developed in [8]. Grad-CAM and human attention maps have a correlation of 0.136, which is statistically higher than chance or random attention maps (zero correlation). This shows that despite not being trained on grounded image-text pairs, CNN+LSTM based VQA models are surprisingly good at localizing discriminative regions required to output a particular answer."}, {"heading": "8. Conclusion", "text": "In this work, we proposed a novel class-discriminative localization technique \u2013 Gradient-weighted Class Activation Mapping (Grad-CAM) \u2013 for making CNN-based models more transparent by producing visual explanations. Our technique achieved better localization accuracy on ILSVRC val than Simonyan et al. [36]. Further, we combined our Grad-CAM localizations with existing high-resolution visualizations having poor localization ability to obtain highresolution class-discriminative visualizations, Guided GradCAM. Extensive human studies with visualizations reveal that our localization-augmented visualizations can discriminate between classes more accurately and better reveal the\ntrustworthiness of a classifier. Finally, we provide some quantitative and qualitative results on interpreting predictions from image classification, visual question answering and image captioning. We believe that a true AI system should not only be intelligent, but also be able to reason about its beliefs and actions for humans to trust it. Future work includes explaining the decisions made by deep networks in domains such as reinforcement learning and natural language processing. Our code is publicly available at https://github.com/ramprs/grad-cam/ and an online demo is available on CloudCV [2]4."}, {"heading": "B. Experimental Results", "text": "In this section we provide more qualitative results for Grad-CAM and Guided Grad-CAM applied to the task of image classification, image captioning and VQA.\nB.1. Image Classification\nFigure A1: Visualizations for randomly sampled images from the COCO validation dataset. Predicted classes are mentioned at the top of each column.\nWe use Grad-CAM and Guided Grad-CAM to visualize the regions of the image that provide support for a particular prediction. The results reported below correspond to the VGG-16 [37] network trained on ImageNet. Fig. A1 shows randomly sampled examples from COCO [24] validation set. COCO images typically have multiple objects per image and Grad-CAM visualizations show precise localization to support the model\u2019s prediction. Guided Grad-CAM can even localize tiny objects. For example our approach correctly localizes the predicted class \u201ctorch\u201d (Fig. A1.a) inspite of its size and odd location in the image. Our method is also class-discriminative \u2013 it places attention only on the \u201ctoilet seat\u201d even when a popular ImageNet category \u201cdog\u201d exists in the image (Fig. A1.e).\nFigure A2: Guided Backpropagation, Grad-CAM and Guided Grad-CAM visualizations for the captions produced by the Neuraltalk2 image captioning model.\nWe also visualized Grad-CAM, Guided Backpropagation (GB), Deconvolution (DC), GB + Grad-CAM (Guided Grad-CAM), DC + Grad-CAM (Deconvolution Grad-CAM) for images from the ILSVRC-15 detection val set that have at least 2 unique object categories each. The visualizations for the mentioned class can be found in the following links.\n\u201ccomputer keyboard, keypad\u201d class: http://i.imgur.com/QMhsRzf.jpg\n\u201csunglasses, dark glasses, shades\u201d class: http://i.imgur.com/a1C7DGh.jpg\nFigure A3: Guided Backpropagation, Grad-CAM and Guided Grad-CAM visualizations for the answers from a VQA model. For each image-question pair, we show visualizations for AlexNet, VGG-16 and VGG-19. Notice how the attention changes in row 3, as we change the answer from Yellow to Green.\nB.2. Image Captioning\nWe use the publicly available Neuraltalk2 code and model5 for our image captioning experiments. The model uses VGG-16 to encode the image. The image representation is passed as input at the first time step to an LSTM that generates a caption for the image. The model is trained end-to-end along with CNN finetuning using the COCO [24] Captioning dataset. We feedforward the image to the image captioning model to obtain a caption. We use Grad-CAM to get a coarse localization and combine it with Guided Backpropagation to get a high-resolution visualization that highlights regions in the image that provide support for the generated caption.\nB.3. Visual Question Answering (VQA)\nWe use Grad-CAM and Guided Grad-CAM to explain why a publicly available VQA model [27] answered what it answered. The VQA model by Lu et al. uses a standard CNN followed by a fully connected layer to transform the image to 1024-dim to match the LSTM embeddings of the question. Then the transformed image and LSTM embeddings are pointwise multiplied to get a combined representation of the image and question and a multi-layer perceptron is trained on top to predict one among 1000 answers. We show visualizations for the VQA model trained with 3 different CNNs - AlexNet [22], VGG-16 and VGG-19 [37]. Even though the CNNs were not finetuned for the task of VQA, it is interesting to see how our approach can serve as a tool to understand these networks better by providing a localized high-resolution visualization of the regions the model is looking at. Note that these networks were trained with no explicit attention mechanism enforced. Notice in the first row of Fig. A3, for the question, \u201cIs the person riding the waves?\u201d, the VQA model with AlexNet and VGG-16 answered \u201cNo\u201d, as they concentrated on the person mainly, and not the waves. On the other hand, VGG-19 correctly answered \u201cYes\u201d, and it looked at the regions around the man in order to answer the question. In the second row, for the question, \u201cWhat is the person hitting?\u201d, the VQA model trained with AlexNet answered \u201cTennis ball\u201d just based on context without looking at the ball. Such a model might be risky when employed in real-life scenarios. It is difficult to determine the trustworthiness of a model just based on the predicted answer. Our visualizations provide an accurate way to explain the model\u2019s predictions and help in determining which model to trust, without making any architectural changes or sacrificing accuracy. Notice in the last row of Fig. A3, for the question, \u201cIs this a whole orange?\u201d, the model looks for regions around the orange to answer \u201cNo\u201d."}, {"heading": "C. Ablation studies", "text": "In this section we provide details of the ablation studies we performed.\nC.1. Varying mask size for occlusion\n5https://github.com/karpathy/neuraltalk2\nC.2. Grad-CAM on different layers\nFigure A5: Grad-CAM localizations for \u201ctiger cat\u201d category for different rectified convolutional layer feature maps for AlexNet.\nFigure A6: Grad-CAM localizations for \u201ctiger cat\u201d category for different rectified convolutional layer feature maps for VGG-16.\nWe show results of applying Grad-CAM for the \u201cTiger-cat\u201d category on different convolutional layers in AlexNet and VGG-16 CNN. As expected, the results from Fig. A6 show that localization becomes progressively worse as we move to shallower convolutional layers. This is because the later convolutional layers capture high-level semantic information and at the same time retain spatial information, while the shallower layers have smaller receptive fields and only concentrate on local features that are important for the next layers.\nC.3. Design choices\nMethod Top-1 error\nGrad-CAM 59.65\nGrad-CAM without ReLU in (4) 74.98 Grad-CAM with Absolute gradients 58.19\nGrad-CAM with GMP gradients 59.96 Grad-CAM with Deconv ReLU 83.95 Grad-CAM with Guided ReLU 59.14\nTable A1: Localization results on ILSVRC-15 val for the ablation studies.\nWe evaluate design choices via top-1 localization error on the ILSVRC-15 val set [9], with each image resized to 224\u00d7 224.\nC.3.1 Importance of ReLU in (4) in main paper\nRemoving ReLU ((4) in main paper) increases error by 15.3%. See Table. A1. Negative values in Grad-CAM indicate confusion between multiple occurring classes. Thus, localization improves when we suppress them (see Fig. A7).\nC.3.2 Absolute value of each derivative in (3) in main paper\nTaking the absolute value of each derivative in (3) in main paper decreases the error by 1.5% (see Table. A1). But qualitatively maps look a bit worse (see Fig. A7), and this evaluation does not capture class discriminability (most ImageNet images have only 1 class).\nFigure A7: Grad-CAM visualizations for \u201ctiger cat\u201d category stating the importance of ReLU and effect of using absolute gradients in (4) of main paper.\nC.3.3 Global Average Pooling vs. Global Max Pooling\nInstead of Global Average Pooling (GAP) the incoming gradients to the convolutional layer, we tried Global Max Pooling (GMP) the gradients. We observe that using GMP lowers the localization ability of our Grad-CAM technique. An example can be found in Fig. A8 below. This observation is also summarized in Table. A1. This may be due to the fact max is statistically less robust to noise compared to the averaged gradient.\nFigure A8: Grad-CAM visualizations for \u201ctiger cat\u201d category with Global Average Pooling and Global Max Pooling.\nC.3.4 Effect of different ReLU on Grad-CAM\nWe experiment with different modifications to the backward pass of ReLU, namely, using Guided-ReLU [38] and Deconv-ReLU [41]. Effect of Guided-ReLU: Springenberg et al. [38] introduced Guided Backprop, where they modified the backward pass of ReLU to pass only positive gradients to regions with positive activations. Applying this change to the computation of our Grad-CAM maps introduces a drop in the class-discriminative ability of Grad-CAM as can be seen in Fig. A9, but it gives a slight improvement in the localization ability on ILSVRC-15 localization challenge (see Table. A1). Effect of Deconv-ReLU: Zeiler and Fergus [41] in their Deconvolution work introduced a slight modification to the backward pass of ReLU, to pass only the positive gradients from higher layers. Applying this modification to the computation of our Grad-CAM gives worse qualitative results as shown in Fig. A9 and Table. A1.\nFigure A9: Grad-CAM visualizations for \u201ctiger cat\u201d category for different modifications to the ReLU backward pass. The best results are obtained when we use the actual gradients during the computation of Grad-CAM."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "CloudCV: Large Scale Distributed Computer Vision as a Cloud Service", "author": ["H. Agrawal", "C.S. Mathialagan", "Y. Goyal", "N. Chavali", "P. Banik", "A. Mohapatra", "A. Osman", "D. Batra"], "venue": "Mobile Cloud Visual Media Computing, pages 265\u2013290. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-taught object localization with deep networks", "author": ["L. Bazzani", "A. Bergamo", "D. Anguelov", "L. Torresani"], "venue": "WACV", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft COCO captions: Data Collection and Evaluation Server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly supervised object localization with multi-fold multiple instance learning", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "IEEE transactions on pattern analysis and machine intelligence", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "and D", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh"], "venue": "Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In EMNLP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Inverting Convolutional Networks with Convolutional Networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing Higherlayer Features of a Deep Network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal, 1341", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From Captions to Visual Concepts and Back. In CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Diagnosing Error in Object Detectors", "author": ["D. Hoiem", "Y. Chodpathumwan", "Q. Dai"], "venue": "ECCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to Expert Systems", "author": ["P. Jackson"], "venue": "Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 3rd edition", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACM MM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Becoming the Expert - Interactive Multi-Class Machine Teaching", "author": ["E. Johns", "O. Mac Aodha", "G.J. Brostow"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "What I learned from competing against a ConvNet on ImageNet", "author": ["A. Karpathy"], "venue": "http://karpathy.github.io/2014/09/02/what-i-learned-fromcompeting-against-a-convnet-on-imagenet/", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "The Mythos of Model Interpretability", "author": ["Z.C. Lipton"], "venue": "ArXiv e-prints, June 2016", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github. com/VT-vision-lab/VQA_LSTM_CNN", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing deep convolutional neural networks using natural pre-images", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "International Journal of Computer Vision, pages 1\u201323", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Is object localization for free? \u2013 weakly-supervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "From image-level to pixel-level labeling with convolutional networks", "author": ["P.O. Pinheiro", "R. Collobert"], "venue": "CVPR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "author": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "venue": "SIGKDD", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "CoRR, abs/1312.6034", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"], "venue": "CoRR, abs/1412.6806", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "HOGgles: Visualizing Object Detection Features", "author": ["C. Vondrick", "A. Khosla", "T. Malisiewicz", "A. Torralba"], "venue": "ICCV", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Top-down Neural Attention by Excitation Backprop", "author": ["J. Zhang", "Z. Lin", "J. Brandt", "X. Shen", "S. Sclaroff"], "venue": "ECCV", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "(b) Guided Backpropagation [38]:", "startOffset": 27, "endOffset": 31}, {"referenceID": 37, "context": "We also show how Grad-CAM may be combined with existing pixel-space visualizations (such as Guided Backpropagation [38]) to create a high-resolution class-discriminative visualization (Guided Grad-CAM).", "startOffset": 115, "endOffset": 119}, {"referenceID": 37, "context": "In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task.", "startOffset": 277, "endOffset": 281}, {"referenceID": 40, "context": "In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task.", "startOffset": 300, "endOffset": 304}, {"referenceID": 1, "context": "com/ramprs/ grad-cam/ and a demo is available on CloudCV [2]1.", "startOffset": 57, "endOffset": 60}, {"referenceID": 21, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 192, "endOffset": 196}, {"referenceID": 25, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 5, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 243, "endOffset": 254}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 243, "endOffset": 254}, {"referenceID": 18, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 243, "endOffset": 254}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 12, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 28, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 32, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 24, "context": "While these deep neural networks enable superior performance, their lack of decomposability into intuitive and understandable components makes them hard to interpret [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions.", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions.", "startOffset": 105, "endOffset": 112}, {"referenceID": 19, "context": ", image classification [20] on a set of categories with enough training data), the goal is to establish trust with users.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "chess or Go playing bots [35]), the goal of transparency & explanations is machine teaching [18] \u2013 i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "chess or Go playing bots [35]), the goal of transparency & explanations is machine teaching [18] \u2013 i.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "Classical rule-based or expert systems [16] were highly interpretable but not very accurate (or robust).", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "What makes a good visual explanation? Consider image classification [9] \u2013 a \u2018good\u2019 visual explanation from the model justifying a predicted class should be (a) classdiscriminative (i.", "startOffset": 68, "endOffset": 71}, {"referenceID": 37, "context": "Pixel-space gradient visualizations such as Guided Backpropagation [38] and Deconvolution [41] are high-resolution and highlight fine-grained details in the image, but are not class-discriminative (for example, the visualization for both \u2018cat\u2019 and \u2018dog\u2019 in Figures 1b and 1g are very similar).", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "Pixel-space gradient visualizations such as Guided Backpropagation [38] and Deconvolution [41] are high-resolution and highlight fine-grained details in the image, but are not class-discriminative (for example, the visualization for both \u2018cat\u2019 and \u2018dog\u2019 in Figures 1b and 1g are very similar).", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "14 \u00d7 14 in VGGNet [37]) and hence does not show fine-grained details.", "startOffset": 18, "endOffset": 22}, {"referenceID": 35, "context": "A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.", "startOffset": 27, "endOffset": 39}, {"referenceID": 37, "context": "A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.", "startOffset": 27, "endOffset": 39}, {"referenceID": 40, "context": "A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.", "startOffset": 27, "endOffset": 39}, {"referenceID": 35, "context": "[36] visualize partial derivatives of predicted class scores w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "pixel intensities, while Guided Backpropagation [38] and Deconvolution [41] make modifications to \u2018raw\u2019 gradients that result in qualitative improvements.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "pixel intensities, while Guided Backpropagation [38] and Deconvolution [41] make modifications to \u2018raw\u2019 gradients that result in qualitative improvements.", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 83, "endOffset": 91}, {"referenceID": 10, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 83, "endOffset": 91}, {"referenceID": 27, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 126, "endOffset": 134}, {"referenceID": 9, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 126, "endOffset": 134}, {"referenceID": 24, "context": "Motivated by notions of interpretability [25] and assessing trust in models [34], we evaluate Grad-CAM visualizations in a manner similar to Ribeiro et al.", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "Motivated by notions of interpretability [25] and assessing trust in models [34], we evaluate Grad-CAM visualizations in a manner similar to Ribeiro et al.", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "[34] via human studies to show that they can be important tools for users to evaluate and place trust in automated systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43].", "startOffset": 172, "endOffset": 187}, {"referenceID": 29, "context": "Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43].", "startOffset": 172, "endOffset": 187}, {"referenceID": 30, "context": "Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43].", "startOffset": 172, "endOffset": 187}, {"referenceID": 22, "context": "This approach modifies image classification CNN architectures replacing fully-connected layers with convolutional layers and global average pooling [23], thus achieving class-specific feature maps.", "startOffset": 148, "endOffset": 152}, {"referenceID": 30, "context": "Others have investigated similar methods using global max pooling [31] and log-sum-exp pooling [32].", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "Others have investigated similar methods using global max pooling [31] and log-sum-exp pooling [32].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "Zeiler and Fergus [41] perturb inputs by occluding small patches and classifying the occluded image, typically resulting in lower classification scores for relevant objects when those objects are occluded.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "This principle is applied for localization in [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 29, "context": "[30] classify many patches containing a pixel then average these patch class-wise scores to provide the pixel\u2019s class-wise score.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] introduce a probabilistic Winner-Take-All formulation for modelling the top-down attention for neural classification models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Notice that this results in a coarse heat-map of the same size as the convolutional feature maps (14\u00d7 14 in the case of last convolutional layers of VGG [37] and AlexNet [22] networks).", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "Notice that this results in a coarse heat-map of the same size as the convolutional feature maps (14\u00d7 14 in the case of last convolutional layers of VGG [37] and AlexNet [22] networks).", "startOffset": 170, "endOffset": 174}, {"referenceID": 36, "context": "This figure analyzes how localizations change qualitatively as we perform Grad-CAM with respect to different feature maps in a CNN (VGG16 [37]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "A number of works have asserted that as the depth of a CNN increases, higher-level visual constructs are captured [5, 28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 27, "context": "A number of works have asserted that as the depth of a CNN increases, higher-level visual constructs are captured [5, 28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 8, "context": "The ImageNet localization challenge [9] requires competing approaches to provide bounding boxes in addition to classification labels.", "startOffset": 36, "endOffset": 39}, {"referenceID": 36, "context": "We evaluate the pretrained off-the-shelf VGG-16 [37] model from the Caffe [17] Model Zoo.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "We evaluate the pretrained off-the-shelf VGG-16 [37] model from the Caffe [17] Model Zoo.", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "[36] for the VGG-16 model, which uses grabcut to postprocess image space gradients into heat maps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Backprop on VGG-16 [36] 61.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "Grad-CAM outperforms [36] which uses backpropagation to pixel-space for localization.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "That is, in the vicinity of the input data point, our explanation should be faithful to the model [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 40, "context": "One obvious choice for such a visualization is image occlusion [41], where we mask out different regions of the input image, and perform multiple CNN evaluations to observe how the resulting score for the class of interest changes.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "We use Guided Grad-CAM to analyze failure modes of the VGG-16 CNN on ImageNet classification [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 20, "context": "6a Visual explanations from image captioning model [21] highlighting image regions considered to be important for producing the captions.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "6b Grad-CAM localizations of a global or holistic captioning model for captions generated by a dense captioning model [19] for the three bounding box proposals marked on the left.", "startOffset": 118, "endOffset": 122}, {"referenceID": 39, "context": "We can also see that seemingly unreasonable predictions have reasonable explanations, which is a similar observation to HOGgles [40].", "startOffset": 128, "endOffset": 132}, {"referenceID": 5, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 75, "endOffset": 86}, {"referenceID": 18, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 75, "endOffset": 86}, {"referenceID": 38, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 75, "endOffset": 86}, {"referenceID": 2, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 12, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 28, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 32, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 20, "context": "More specifically, we build on top of the publicly available \u2018neuraltalk2\u20193 implementation [21] that makes use of a finetuned VGG-16 CNN for images and an LSTMbased language model.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "[19] recently introduced the Dense Captioning (DenseCap) task that requires a system to jointly localize and caption salient regions in a given image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The model proposed in [19] consists of a Fully Convolutional Localization Network (FCLN) and an LSTM-based language model that produces both bounding boxes for regions of interest and associated captions in a single forward pass.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 12, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 28, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 32, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 26, "context": "We explain the answers of a VQA model [27] that combines a CNN and an LSTM representation through a pointwise multiplication.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "[8] collected human attention maps for a subset of the VQA dataset [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[8] collected human attention maps for a subset of the VQA dataset [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 26, "context": "Human attention maps are compared to Grad-CAM visualizations of the simple VQA model introduced above [27] on 1374 question-image (QI) pairs from the validation set of the VQA dataset [3].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Human attention maps are compared to Grad-CAM visualizations of the simple VQA model introduced above [27] on 1374 question-image (QI) pairs from the validation set of the VQA dataset [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "We use the rank correlation evaluation protocal developed in [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "com/ramprs/grad-cam/ and an online demo is available on CloudCV [2]4.", "startOffset": 64, "endOffset": 67}], "year": 2016, "abstractText": "We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are \u2018important\u2019 for predictions from these models \u2013 or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping (CAM) [43]. While CAM is limited to a narrow class of CNN models, Grad-CAM is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations (such as Guided Backpropagation [38]) to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + Long Short Term Memory (LSTM) models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if 1 ar X iv :1 61 0. 02 39 1v 1 [ cs .C V ] 7 O ct 2 01 6 Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both networks make identical predictions, simply on the basis of their different explanations. Our code is available at https://github.com/ramprs/ grad-cam/ and a demo is available on CloudCV [2]1.", "creator": "LaTeX with hyperref package"}}}