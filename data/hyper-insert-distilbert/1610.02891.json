{"id": "1610.02891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning", "abstract": "it there is locally difficult tasks to genetically train and a sufficiently personalized task - oriented dialogue system strictly because the data collected from participating each unknown individual is reasonably often locally insufficient. existing personalized expert dialogue systems loosely trained on selecting a small isolated dataset can routinely overfit and make it difficult to systematically adapt attention to different differentiated user needs. one way to actually solve up this problem is to consider a persistent collection of multiple identity users'data as a source / domain boundary and an individual given user's data identifiable as a target domain, and to perform a transfer system learning from the source to deliver the targeted target domain. starting by the following this this idea, someday we propose \" petal \" ( personalized independent task dialogue ), a personality transfer - learning framework effectively based on pomdp to consciously learn a personalized dialogue classification system. the system first learns common dialogue knowledge from serving the source domain context and then adapts this spoken knowledge readily to reaching the target user. this messaging framework can avoid the robust negative responsive transfer problem by considering personal differences visibly between source inhabitants and target users. the privacy policy leaders in the classroom personalized pomdp can dramatically learn to confidently choose different actions differently appropriately for another different primary users. experimental results on a possible real - world coffee - shopping data and simulation output data show that our personalized autonomous dialogue identification system can choose to different optimal actions for different users, personally and and thus hopefully effectively could improve the mutual dialogue quality under the personalized setting.", "histories": [["v1", "Mon, 10 Oct 2016 12:51:05 GMT  (139kb)", "http://arxiv.org/abs/1610.02891v1", null], ["v2", "Mon, 17 Oct 2016 14:08:42 GMT  (140kb)", "http://arxiv.org/abs/1610.02891v2", null], ["v3", "Fri, 26 May 2017 14:05:07 GMT  (91kb,D)", "http://arxiv.org/abs/1610.02891v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["kaixiang mo", "shuangyin li", "yu zhang", "jiajun li", "qiang yang"], "accepted": false, "id": "1610.02891"}, "pdf": {"name": "1610.02891.pdf", "metadata": {"source": "CRF", "title": "Personalizing a Dialogue System with Transfer Learning", "authors": ["Kaixiang Mo", "Shuangyin Li", "Yu Zhang", "Jiajun Li", "Qiang Yang"], "emails": ["qyang}@cse.ust.hk", "jiajun.li@alumni.ust.hk"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n02 89\n1v 1\n[ cs\n.A I]\n1 0\nO ct\n2 01"}, {"heading": "Introduction", "text": "Dialogue systems can be classified into two classes: open domain dialogue systems (Ritter et al. 2011; Galley et al. 2015; Serban et al. 2015; Li et al. 2016; Mou et al. 2016) and task-oriented dialogue systems (Levin et al. 1997; Young et al. 2013; Wen et al. 2015; Wen et al. 2016; Williams and Zweig 2016). Open domain dialogue system do not limit the dialogue topic to a specific domain, and typically do not have a clear dialogue goal. Task-oriented dialogue system aims to solve a domain specific task with dialogue. We focus on task-oriented dialogue system.\nPersonalized task-oriented dialogue systems aim to help the target user finish the dialogue task better and faster than non-personalized dialogue system. Personalized dialogue systems can learn about the target user\u2019s preference and habit during multiple interactions with the target user, and then utilize these personalized\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ninformation to speed up the conversation process. Personalized dialogue systems could be categorized into rule-based dialogue systems (Thompson et al. 2004; Kim et al. 2014; Bang et al. 2015) and learningbased dialogue systems (Casanueva et al. 2015; Genevay and Laroche 2016). In rule-based personalization systems, the dialogue state, system speech act and user speech act are predefined by developers, so it is difficult for us to use this system in some situation when the dialogue state and the speech act are hard to define manually. Learning-based dialogue systems could learn states and actions from training data, without requiring explicit rules provided by human.\nHowever, it is difficult to train a personalized taskoriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit, making it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users\u2019 data as a source domain and an individual user\u2019s data as a target domain, and perform transfer learning from the source domain to the target domain. Can we learn common dialogue knowledge from the source domain and then adapts this knowledge to the target user?\nWhen transferring dialogue knowledge, the challenge lies in the difference between the source and target domains. Dialogue system trained directly with data from the source domain could not provide personalized service. Some works (Casanueva et al. 2015; Genevay and Laroche 2016) proposed to transfer dialogue knowledge among similar users, but they did not model the difference between different users and might harm personalization performance in the target domain.\nIn this paper, we propose the \u201cPETAL\u201d (PErsonalized Task-oriented diALogue), which is a transfer learning framework based on the POMDP for learning a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. We propose to model personalized policy with personalized Q-function defined as the expected cumulative general reward plus expected cumulative personal reward. The personalized Q-function can model differences between the source and target users, and thus can avoid the negative transfer problem brought by dif-\nferences between source and target users. Experimental results on a real-world coffee-shopping dataset and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus the proposed personalized POMDP framework effectively improves the dialogue quality under a personalized setting.\nOur contributions are three-fold: Firstly, we tackle the problem of learning common dialogue knowledge from the source domain and adapting to the target user in a personalized dialogue system. Secondly, we propose a transferlearning framework on the POMDP capable user personalization. To the best of our knowledge, this is the first framework considering user personalization on POMDP. Finally, we demonstrate the effectiveness of the proposed personalized dialogue system framework on a real-world dialogue dataset as well as on simulation data."}, {"heading": "Related Works", "text": "Personalized dialogue systems could be categorized into rule-based dialogue systems and learning-based dialogue systems. For rule-based systems, Thompson et al. (Thompson et al. 2004) proposed an interactive system, the Adaptive Place Advisor system. In this system, users can choose a place via an interactive conversational process, the system could learn user preference and use it to improve future conversation. Kim et al. (Kim et al. 2014; Bang et al. 2015) proposed a personalization framework for dialogue systems, which extract and utilize user related facts(triples), and then generate responses by applying userrelated facts to the templates.\nDifferent from rule-based systems, learning-based personalized dialogue systems can learn states and actions from training data without requiring explicit rules. Casanueva et al. (Casanueva et al. 2015) proposed to initialize personalized dialogue system for a target speaker with data from similar speakers, in order to improve the performance for the target speaker. However, this work requires a predefined user similarity metric to select similar users, and the selected similar user could still be somehow different from the target user, which affect the personalization performance of the target user. Genevay et al. (Genevay and Laroche 2016) proposed to select and transfer optimized policy from source users to a target user by using a multi-armed stochastic bandit algorithm to select an optimal source policy, which do not require a predefined user similarity measure. However, this algorithm has high complexity. For each target user, this algorithm requires n2 bandit selection operations where n is the number of source users. In addition, the selected source user could still be somehow different from the target user and affect the personalization performance in the target domain.\nTransfer learning (Pan and Yang 2010; Taylor and Stone 2009; Tan et al. 2015; Tan et al. 2014; Wei et al. 2016) technique has been applied to other tasks in dialogue system. Gavsic et al. (Gas\u030cic et al. 2013) proposed to use transfer learning to extend a dialogue system to include a previously unseen concept. Gavsic et al. (Gasic et al. 2014) proposed an incremental scheme to adapt an existing dialogue management system to extended\ndomain. These two works proposed to transfer policy parameters in the source domain as a prior to the target domain. However, these two models did not deal with multiple-source domains, and they do not have explicit personalized mechanisms for different users. As a result, personalization performance in the target domains might be affected by source users.\nProblem"}, {"heading": "Notations", "text": "In this paper, matrices are denoted in bold capital case, row vectors are in bold lower case, scalars are in lower case. The text in the dialogues, denoted in curlicue, is represented by the the bag-of-words assumption. Each of the involved bagof-words representations is a vector, each element of which has a binary value. v is the size of the dialogue vocabulary. d is the dimension of belief vectors. M is a state projection matrix that maps bag-of-words representations to belief vectors. O is a user utterance in the bag-of-words representation and o = OM is the belief state vector of one of the user\u2019s utterances. A is the reply of an agent in the bag-of-words representation and a is the belief state vector of one of the agents\u2019 replies. r is the reward. b is a belief state vector.\nWe formulate the dialogue as a POMDP, which is defined as {S,A,O, P,R, Z, \u03b3}. S are the hidden unobservable states, A are the replies of the agent, O are users\u2019 utterances, P is the state transition probability function, R is the reward function, Z is the observation function, \u03b3 \u2208 [0, 1] is the discounted factor. In time step i, Si is the hidden conversation state, Oi is the user utterance, Ai is the reply of the agent and ri is the reward. Moreover, in time step i, we can only observe Oi,Ai and ri. We define bi as the belief state vector, which represents the probability distribution of unobservable Si.\nThe real-world dataset consists of a collection of dialogues from each user u: D = {{Oui ,Aui , ri}Ti=0}, where ri is defined by the reward function.\nThe simulator could generate an user utterance and a reward by {Oi, ri} = G({Ok,Ak}i\u22121k=0)."}, {"heading": "Problem Settings", "text": "Our goal is to collect a set of choices about the final order from a target user. The j-th choice we want to collect is denoted as Cj , and the exact choice in Cj is denoted by cj . For example, Latte is a choice c1 in a coffee-type C1. In order to get the information needed to complete the order, we need to ask the user a question at each round, to guide the user to complete the order. We assume all possible choices could be recognized with keyword-matching methods. The inputs for this problem include\n1. Dialogue Data {{Ousi ,A us i } T i=0} of source customers us.\n2. Dialogue history of the target user ut on or before current time step i, {Oi, {Ok,Ak} i\u22121 k=0}.\nThe expected output is\n1. A reply Auti for the target user at time i.\nIn order to solve the problem, we aim to find a policy \u03c0ut , which could choose an appropriate action Ai at each time step i based on current dialogue history Hi, to maximize the cumulated reward of the whole process as follow: \u03c0ut = argmax\u03c0 E[ \u2211\u221e k=0 \u03b3krt+k+1]."}, {"heading": "PETAL: A Framework for Personalized Dialogue Management", "text": "In this section, we introduce our transfer-learning framework for personalized dialogue management. In this paper, we use PETAL to denote both the proposed framework and the proposed algorithm."}, {"heading": "The Framework", "text": "To model belief states, we introduce a state projection matrix M to map dialogue history Hi to belief state bi,\nbi = f(Hi|M).\nThe Q-function is defined as the expected cumulative reward according to policy \u03c0 by starting from belief state bi and taking action Ai:\nQ \u03c0(Hi,Ai) = E\u03c0\n[\n\u221e \u2211\nk=0\n\u03b3 k rt+k+1|Hi,Ai\n]\n.\nIn order to build a personalized dialogue system for the target user, we need to find a personalized Q-function Q\u03c0ut for this user. However, our training data {{Outi ,A ut i } T i } for the target user ut is very limited, and we can hardly estimate the personalized Q-function Q\u03c0ut with the limited data.\nIn order to model Q\u03c0ut , we have to transfer common dialogue knowledge from the source domain, which has the dialogue data from many other users {{Ousi ,A us i } T i }. However, different users may have different preferences, so directly using the data from source users would bring negative effects.\nWe propose to model the personalized policy \u03c0u with the personalized Q-function defined as the expected cumulative general reward plus expected cumulative personal reward:\nQ\u03c0u(Hi,Ai)\n=Qg(Hi,Ai|\u0398 g) +Qp(Hi,Ai|\u0398 p u)\n=E\u03c0\n[\n\u221e \u2211\nk=0\n\u03b3kr g t+k+1|Hi,Ai\n]\n+ E\u03c0\n[\n\u221e \u2211\nk=0\n\u03b3kr p t+k+1|Hi,Ai\n]\n,\nwhere Qg(Hi,Ai|\u0398g) captures expected reward related to the general dialogue policy for all users, \u0398g contains a large amount of parameters and requires a lot of training data to learn, and Qp(Hi,Ai|\u0398pu) captures the expected reward related to the preference of the target user. Because the vocabulary of choices is much smaller than the whole vocabulary, we can estimate the personal preference parameters \u0398pu with a few dialogue data {{Outi ,A ut i } T i } from the target user.\nThe proposed framework is based on transfer learning. M and \u0398g are shared across different users, which could be trained on the source domains and transferred to the target domain. These parameters contain the common dialogue knowledge, which is independent of the users\u2019 preferences."}, {"heading": "Personalized Dialogue Management", "text": "In this section, we introduce the parametric form for bi = f(Hi|M), Q(Hi,Ai|\u0398g) and Qp(Hi,Ai|\u0398pu).\nAll utterances and replies will be projected into state vectors with a state projection matrix M, where M is initialized with the word2vec and will be updated in the learning process. \u03be is the memory factor, by which the historical state vectors will be discounted at each time step. The state projection matrix M is the parameter we want to learn. bi = f(Hi|M) maps the dialogue history, Hi = {{Ok} i k=0, {Ak} i\u22121 k=0}, to a belief state vector. The corresponding belief state vector bi is\nbi = [ ohi\u22121,oi, a h i\u22122, ai\u22121 ] ,\nwhere the variables involved are defined as ohi = \u2211i\nk=0 \u03bei\u2212kok, oi = OiM, a h i = \u2211i k=0 \u03bei\u2212kak, ai\u22121 =\nAi\u22121M. Based on these definitions, we can see that ohi represents all previous user utterances, oi represents the current user utterance, ahi represents all previous agent replies, and ai\u22121 represents the last agent reply.\nIn order to model the correlations between dimensions in ai and bi, we define Qg(Hi,Ai|\u0398g) as\nQg(Hi,Ai|\u0398 g) = aiWb T i ,\nwhere W \u2208 Rd\u00d74d is a parameter matrix to be learned. Based on the properties of the Kronecker product and operator vec(\u00b7) which transforms a matrix to a vector in a columnwise manner, we can rewrite Qg(Hi,Ai|\u0398g) as a linear function on w = vec(W)T \u2208 R4d 2\n: Qg(Hi,Ai|\u0398g) = (bi \u2297 ai)w\nT , where bi \u2297 ai is the Kronecker product of bi and ai.\nWe denote by c = {cj}mj=1 the user-dependent choices in Ai, where m is the total number of order choices. For example, in coffee ordering where m = 4, c1 could be \u201clatte\u201d, and c2 could be \u201ciced\u201d. We assume that different choices are independent of each other. Specifically, for choice j, p(cj), the probability of a user to choose cj , follows a categorical distribution C(puj), where pujk is the kth entry in puj and p(cj = Ck) = pujk. The expected reward function related to users\u2019 preference is formulated as\nQp(Hi,Ai|\u0398 p u) = wp\nm \u2211\nj=1\nC(cj |puj)\u03b4(Cj |Hi),\nwhere \u03b4(Cj |Hi) = 0 if \u2203Cj \u2208 Cj and Cj \u2208 Hi and otherwise \u03b4(Cj |Hi) = 1. \u03b4(Cj |Hi) = 0 means the user has already chosen a choice cj in Cj , and \u03b4(Cj |Hi) = 1 means the user and agent have never talked about Cj . The parameters to be learned are puj and wp. Here the personal weight parameter, wp, controls the importance of the personalized reward. When wp is close to zero the reward function will depend on the general dialogue policy. Note that \u2211m\nj=1 C(cj|puj)\u03b4(Cj |Hi) is 0 if we know nothing about the user, or Ai do not show any personal preference.\nThe Q-function can finally be defined as\nQ \u03c0u(Hi,Ai) = (bi \u2297 ai)w T + wp\nm \u2211\nj=1\nC(cj |puj)\u03b4(Cj |Hi)."}, {"heading": "Reward", "text": "The reward consists of the general reward and personal reward, and the total reward is the sum of general reward and personal reward. The general and personal rewards can be defined as follows:\n1. Personal rewards of positive values will be received when the user confirms the suggestion of the agent. This is related to the personal information of the user. For example, the user could confirm the address suggested by the agent.\n2. General rewards of positive values will be received when the user provides the information about each cj .\n3. General rewards of positive values will be received when the user proceeds with payment.\n4. General reward of negative values will be received by the agent when the dialogue has more turns or the user rejects to pay if the system is generating non-logical responses.\nParameter Learning We use the value iteration method (Bellman 1957) to learn both the general and personal Q-function. There are in total four sets of parameters to be learned. The first parameter is the state projection matrix M, which is responsible for mapping the dialogue history to a belief state vector. The second parameter is the weight vector w defined in the general Q-function, which is responsible for the common dialogue knowledge. The third parameter is wp defined in the personal Q-function, which is responsible for the personalized dialogue control. The final parameter is personal preference vector puj for each user, which is responsible for modelling personal preference. We denote all these parameter by \u0398 = {M,w, wp, {pu}}.\nWe adopt an online stochastic gradient descent algorithm (Bottou 2010) to optimize our model. Specifically, we use the State-Action-Reward-State-Action (SARSA) algorithm. In the on-policy training with simulation, the model has probability \u03b7 = 0.2 of choosing a random reply in the candidate set so as to ensure sufficient exploration."}, {"heading": "Loss Function", "text": "When dealing with real-world data, the training set consists of (Hi,Ai, ri), which records optimal actions provided by human, and hence the loss function is defined as follows:\nL(\u0398) = E[(ri + \u03bbQ(Hi+1,Ai+1|\u0398)\u2212Q(Hi,Ai|\u0398)) 2].\nIn the on-policy training with user simulator, the loss function is defined as\nL(\u0398) = E[(ri + maxA\u2032 i+1\n\u03bbQ(Hi+1,A \u2032 i+1|\u0398)\u2212Q(Hi,Ai|\u0398)) 2],\nwhere ri is the reward obtained at time step i, Oi+1 is the user response at time i+1, and Hi+1 is the update dialogue history at time step i+ 1. Note that source domain users are disjoint with target domain users, so pu learned in \u2018Train Source Model\u2019 could not be applied in target domain.\nSource Model Training We train our model for each user in the source domain. M, w and wp are shared by all users and there is a separatepu for each user in the source domain.\nTransfer to Target Domain We transfer M, w and wp to the target domain by using them to initialize the corresponding variables in the target domain, and then we train them as well as pu for each target user with limited training data."}, {"heading": "Complexity", "text": "The detailed algorithm is shown in Algorithm 1. The number of parameters in our model is around d2+dv, where v is the total vocabulary size and d is the dimension of the state vector. In our experiment where v = 1, 500 and d = 50, the number of general parameters is 85k. The number of personal parameter is under 100 for each user, so personalized parameters could be learned with a few data in the target domain. On the real-world dataset with 2,185 complete dialogues, each epoch requires 1.5 hour on a server with 20 CPUs and the proposed model converges at around 3 epochs.\nAlgorithm 1 PETAL: Transfer Learning with Personalized POMDP 1: Input:Ds,Dt 2: Output: {M,w, wp {pu}} 3: Let \u0398 = {M,w, wp, {pu}} 4: procedure TRANSFER ALGORITHM(Ds ,Dt) 5: {M,w, wp} \u2190 TRAIN SOURCE MODEL(Ds ) 6: {M,w, wp {pu}} \u2190 TRANSFER MODEL(Dt ,M, w, wp)\nreturn {M,w, wp {pu}}\n7: function TRAIN SOURCE MODEL(Ds ) 8: for {{Oui ,A u i } T i } inD\ns do 9: if pu exist then 10: load pu 11: else pu \u2190 0 12: for (Hi,Ai, ri,Hi+1,Ai+1) in {{Oui ,A u i } T i } do 13: \u0398t+1 \u2190 \u0398t + \u03b1\u2206\u0398L(\u0398t) return {M,w, wp} 14: function TRANSFER MODEL(Dt , M,w, wp ) 15: for {{Oui ,Aui }Ti } inDt do 16: if pu exist then 17: load pu 18: else pu \u2190 0 19: for (Hi,Ai, ri,Hi+1,Ai+1) in {{Oui ,A u i } T i } do 20: \u0398t+1 \u2190 \u0398t + \u03b1\u2206\u0398L(\u0398t) return {M,w, wp {pu}}"}, {"heading": "Experiments", "text": "In this section, we experimentally verify the effectiveness of the personalized POMDP model by conducting experiments on a real-world dataset and a simulation dataset."}, {"heading": "Baselines", "text": "We compare our personalized model, denoted by \u201cPETAL\u201d, with six baseline algorithms which are listed as follows:\n1. NoneTL: Dialogue system is trained only with the data from target users.\n2. Sim (Casanueva et al. 2015): Dialogue system is trained with the data from both target user and the most similar user in the source domain.\n3. Bandit (Genevay and Laroche 2016): For each target user, we will find the most useful source user by a bandit algorithm.\n4. PriorSim (Gas\u030cic et al. 2013): For each target user, we use the policy from the most similar user in the source domain as a prior.\n5. PriorAll (Gas\u030cic et al. 2013): For each target user, we use the dialogue policy trained on all the users in the source domain as a prior.\n6. All: The policy is trained on all source users\u2019 data."}, {"heading": "Experiments on Real-World Data", "text": "In this section, we evaluate our model on a real-world dataset. This real-world dataset contains 2,185 coffee dialogues from 72 users, which are collected between July 2015 and April 2016 from a O2O coffee ordering service. We select 52 users with more than 23 dialogues as the source domain. Each of the remaining 20 users is used separately as a target domain. There are 1,859 coffee dialogues in the source domain and 329 coffee dialogues in the target domain. 221 dialogues in the target domain are used as the training set\nand the remaining 108 dialogues are used as the test set. The statistics of this dataset is shown in Table 2.\nEvaluation Metrics For each round of the testing conversation, Hi,Ai, the model will rank the ground truth reply Ai among 10 randomly chosen agent replies. The label assigned to Ai is 1 and those for randomly chosen agent replies are 0. By following (Williams and Zweig 2016), we calculate the AUC score for each turn in a conversation and the performance of an algorithm is measured by the average AUC score of each dialogue for every user in the test set.\nResults The results are shown in Figure 2 and Table 3. \u201cNoneTL\u201d, \u201cPriorSim\u201d and \u201cPriorAll\u201d are worse than directly transferring training data, because fitting only target domain data can cause the overfitting. Transferring data from similar users (i.e., \u201cSim\u201d) is not as good as transferring data from all source users (i.e., \u201cAll\u201d), because common knowledge has to be learn from more data. The \u201cPETAL\u201d method performs the best because it learns common knowledge from all users and avoid the negative transfer caused by different preferences among source and target users and this indicates that our model fits personalized dialogues better and demonstrates the effectiveness of PETAL on this realworld dataset."}, {"heading": "Experiments on Simulation Data", "text": "In this section, we compare our model with baseline models on the simulation data.\nSettings We have 11 simulated users in the source domain, in which 10 users have their own special preferences while the rest one has no preference. The target domain has 5 users, which have different preferences from the users in the source domain. A simulator is designed based on the realworld dataset used in the previous section. The simulator will give a specific order with probability 0.8 and otherwise\nthe simulator will order coffee randomly. The training set of each user in the target domain has 20 dialogues and the test set has 300 dialogues.\nEvaluation Metrics Each model will choose a reply from a set of candidates at each turn, and the simulator will react to the reply accordingly. For each model, we report the averaged reward (Genevay and Laroche 2016), averaged success rate (Casanueva et al. 2015) and averaged dialogue length over all possible target users.\nResults The results are shown in Figure 3 and Table 2. PETAL outperforms all baselines and obtains the highest average reward, the highest success rate and the lowest dialogue length, which implies that PETAL has found a better dialogue policy which can adapt its behaviour according the preference of target users and again demonstrates the effectiveness of PETAL in a live environment.\nCase Study We show a typical case for the simulation data in Table 4, which are translated from Chinese. The nonpersonalized dialogue system corresponding to the \u201cAll\u201d model has to ask users all the choices, because there is no universal recommendation for all the users with different preferences, while PETAL has learned the target users\u2019 preferences. As shown in Table 4, the agent can respond to the user as \u201cStill in an old way, tall hot macchiato, and deliver to your company?\u201d, which is specially tailed for the target user and this personalized question can guide the user to complete the task quickly. This case shows that PETAL can choose different optimal actions for different users and effectively shorten the dialogue process.\nIn this paper, we tackle the problem of learning common dialogue knowledge from the source domain and transferring this knowledge to the target user in a personalized dialogue system. We propose the \u201cPETAL\u201d, a transfer learning framework based on the POMDP for learning a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. We propose to model personalized policy in the POMDP with a personalized Q-function. This framework can avoid the negative transfer problem brought by differences between the source users and the target user. Experimental results on the real-world coffeeordering data and the simulation data show that PETAL can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting. In the future, we will investigate to transfer knowledge from heterogeneous domains such as knowledge graphs and images."}], "references": [{"title": "In Proceedings of International Conference on Big Data and Smart Computing", "author": ["Jeesoo Bang", "Hyungjong Noh", "Yonghee Kim", "Gary Geunbae Lee. Example-based chat-oriented dialogue system with personalized long-term memory"], "venue": "pages 238\u2013243,", "citeRegEx": "Bang et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Technical report", "author": ["Richard Bellman. A Markovian decision process"], "venue": "DTIC Document,", "citeRegEx": "Bellman 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "In Proceedings of COMPSTAT\u20192010", "author": ["L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent"], "venue": "pages 177\u2013186. Springer,", "citeRegEx": "Bottou 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge transfer between speakers for personalised dialogue management", "author": ["Inigo Casanueva", "Thomas Hain", "Heidi Christensen", "Ricard Marxer", "Phil Green"], "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Casanueva et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06863,", "citeRegEx": "Galley et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "POMDP-based dialogue manager adaptation to extended domains", "author": ["Ga\u0161ic"], "venue": "In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Ga\u0161ic,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161ic", "year": 2013}, {"title": "Incremental on-line adaptation of POMDP-based dialogue managers to extended domains", "author": ["Gasic"], "venue": "In Proceedings of the 15th Annual Conference of the International Speech", "citeRegEx": "Gasic,? \\Q2014\\E", "shortCiteRegEx": "Gasic", "year": 2014}, {"title": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems", "author": ["Aude Genevay", "Romain Laroche. Transfer learning for user adaptation in spoken dialogue systems"], "venue": "pages 975\u2013983,", "citeRegEx": "Genevay and Laroche 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Acquisition and use of long-term memory for personalized dialog systems", "author": ["Kim"], "venue": "In Proceedings of International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction,", "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert. Learning dialogue strategies within the Markov decision process framework"], "venue": "pages 72\u201379,", "citeRegEx": "Levin et al. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1606.01541,", "citeRegEx": "Li et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1607.00970,", "citeRegEx": "Mou et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "IEEE Transactions on Knowledge and Data Engineering", "author": ["Sinno Jialin Pan", "Qiang Yang. A survey on transfer learning"], "venue": "22(10):1345\u20131359,", "citeRegEx": "Pan and Yang 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan. Data-driven response generation in social media"], "venue": "pages 583\u2013593,", "citeRegEx": "Ritter et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "Serban et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-transfer: Transfer learning with multiple views and multiple sources", "author": ["Ben Tan", "Erheng Zhong", "Evan Wei Xiang", "Qiang Yang"], "venue": "Statistical Analysis and Data Mining, 7(4):282\u2013293,", "citeRegEx": "Tan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "author": ["Ben Tan", "Yangqiu Song", "Erheng Zhong", "Qiang Yang. Transitive transfer learning"], "venue": "pages 1155\u20131164,", "citeRegEx": "Tan et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research, 10:1633\u20131685,", "citeRegEx": "Taylor and Stone 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Journal of Artificial Intelligence Research", "author": ["Cynthia A Thompson", "Mehmet H Goker", "Pat Langley. A personalized system for conversational recommendations"], "venue": "21:393\u2013 428,", "citeRegEx": "Thompson et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "author": ["Ying Wei", "Yu Zheng", "Qiang Yang. Transfer knowledge between cities"], "venue": "pages 1905\u20131914,", "citeRegEx": "Wei et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1508.01745,", "citeRegEx": "Wen et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A network-based end-to-end trainable taskoriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning", "author": ["Jason D Williams", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1606.01269,", "citeRegEx": "Williams and Zweig 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "Proceedings of the IEEE, 101(5):1160\u20131179,", "citeRegEx": "Young et al. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users\u2019 data as a source domain and an individual user\u2019s data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose the \u201cPETAL\u201d (PErsonalized Task-oriented diALogue), a transfer learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.", "creator": "LaTeX with hyperref package"}}}