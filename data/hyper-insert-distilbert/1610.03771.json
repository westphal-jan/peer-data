{"id": "1610.03771", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods", "abstract": "in this paper,, we introduce demonstrating the diverse task mechanics of targeted subjective aspect - domain based sentiment structure analysis. namely the goal is structured to extract complex fine - grained information with fundamental respect respects to entities mentioned in user comments. this fundamental work which extends both advanced aspect - domain based sentiment analysis forms that principally assumes reviewing a reasonably single entity per document and targeted sentiment analysis methods that assumes observing a hypothetical single group sentiment towards such a target neutral entity. in particular, we identify the sentiment towards each aspect of choosing one or increasingly more entities. as a testbed software for refining this analytic task, we introduce below the detailed sentihood sampling dataset, extracted from a question answering ( qa ) input platform where actual urban neighbourhoods are discussed broadly by users. implicit in testing this context units of text assessment often mention several aspects of one village or more neighbourhoods. this accomplishment is the first time that a generic unified social media platform acquired in this precise case a qa document platform, historically is used for explicitly fine - grained or opinion mining. across text coming from particular qa platforms there is far less constrained compared to text sources from literature review specific platforms which current consensus datasets are fundamentally based on. second we develop against several strong relevance baselines, relying particularly on formal logistic regression measures and state - of - the - place art recurrent neural networks.", "histories": [["v1", "Wed, 12 Oct 2016 16:23:11 GMT  (559kb,D)", "http://arxiv.org/abs/1610.03771v1", "Accepted at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marzieh saeidi", "guillaume bouchard", "maria liakata", "sebastian riedel"], "accepted": false, "id": "1610.03771"}, "pdf": {"name": "1610.03771.pdf", "metadata": {"source": "CRF", "title": "SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods", "authors": ["Marzieh Saeidi", "Guillaume Bouchard", "Maria Liakata"], "emails": ["msaeidi@cs.ucl.ac.uk", "guillaume@bloomsbury.ai", "m.liakata@warwick.ac.uk", "sriedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Sentiment analysis is an important task in natural language processing. It has received not only a lot of interest in academia but also in industry, in particular for identifying customer satisfaction on products and services. Early research in the field (Das and Chen, 2001; Morinaga et al., 2002) of sentiment analysis only focused on identifying the overall sentiment or polarity of a given text. The underlying assumption of this work was that there is one overall polarity in the whole text.\nAspect-based sentiment analysis (ABSA) (Jo and Oh, 2011; Pontiki et al., 2015; Pontiki et al., 2016) relates to the task of extracting fine-grained information by identifying the polarity towards different aspects of an entity in the same unit of text, and recognizing the polarity associated with each aspect separately. The datasets for this task were mostly based on specialized review platforms such as Yelp where it is assumed that only one entity is discussed in one review snippet, but the opinion on multiple aspects can be expressed. This task is particularly useful because a user can assess the aggregated sentiment for each individual aspect of a given product or service and get a more fine-grained understanding of its quality.\nAnother line of research in this field is targeted (a.k.a. target-dependent) sentiment analysis (Jiang et al., 2011; Vo and Zhang, 2015). Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences (often a tweet). For instance in the sentence \u201cPeople everywhere love Windows & vista. Bill Gates\u201d, polarity towards Bill Gates is \u201cNeutral\u201d but the positive sentiment towards Windows & vista will interfere with identifying it if the usual methods for sentiment analysis task are employed. However this task assumes only the overall sentiment for each entity. Moreover, the existing corpora for this task so far has contained only a single target entity per unit of text.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 0.\n03 77\n1v 1\n[ cs\n.C L\n] 1\n2 O\nct 2\n01 6\nBoth settings are obviously limited, and there exists many scenarios in which sentiments towards different aspects of several entities are discussed in the same unit of text. As a running example, we use urban areas: choosing which area to live or to visit is an important task when moving or visiting a new city. Currently there are no dedicated platforms for reviewing and rating aspects of neighbourhoods of a city. However we can find many discussions and threads on several blogs and question answering platforms that discuss aspects of areas in many cities around the world. In general, these conversations are very comprehensible: they often contain specific information about several aspects of several neighbourhoods. One example is the following (area names are highlighted in bold and aspect related terms are underlined):\n\u201cOther places to look at in South London are Streatham (good range of shops and restaurants, maybe a bit far out of central London but you get more for your money) Brixton (good transport links, trendy, can be a bit edgy) Clapham (good transport, good restaurants/pubs, can feel a bit dull, expensive) ...\u201d\nThe example above does not perfectly fit into the existing tasks in sentiment analysis mentioned earlier. In this work, we introduce a new task that not only subsumes the existing sub-fields of targeted and aspect-based sentiment analysis but it also makes less assumptions on the number of entities that can be discussed in the unit of text.\nTo compare with the existing aspect-based sentiment analysis task, take the following example from the restaurant dataset used by SemEval shared ABSA (Pontiki et al., 2016) task. \u201cThe design of the space is good but the service is horrid!\u201d. The ABSA task aims to identify that a positive sentiment towards the ambiance aspect is expressed (opinion target expression is \u201cspace\u201d). Moreover, a negative sentiment is expressed towards the service aspect (opinion target expression is \u201cservice\u201d). In this example, it is assumed that both of these opinions are expressed about a single restaurant which is not mentioned explicitly. However, take the following synthetic example that ABSA is not addressing:\n\u201cThe design of the space is good in Boqueria but the service is horrid, on the other hand, the staff in Gremio are very friendly and the food is always delicious.\u201d\nIn this example, more than one restaurant are discussed and restaurants for which opinions are expressed, are explicitly mentioned. We call these target entities. Current ABSA task can only recognise that positive and negative opinions towards aspect \u201cservice\u201d are expressed. But it can not identify the target entity for each of these opinions (i.e. Germio and Boqueria respectively). Targeted aspect-based sentiment analysis handles extracting the target entities as well as different aspects and their relevant sentiments.\nIn the following, we argue that this task is both very relevant in practice, and raises interesting modelling questions. To facilitate research on this task we introduce the SentiHood dataset. SentiHood is based on the text from a QA platform in the domain of neighbourhoods of a city. Table 2 shows examples of input sentences and annotations provided.\nOur contributions in this paper can be summarised as follows:\n\u2022 We introduce the task of targeted aspect-based sentiment analysis as a further step towards extracting more fine-grained information from more complex text in the field of sentiment analysis.\n\u2022 We use the text from social media platforms, in particular QA, for fine-grained opinion mining. So far, all datasets in this field have utilised text from review specific platforms where certain assumptions can be made and data is more constrained and less noisy.\n\u2022 We propose SentiHood, a benchmark dataset that is annotated for the task of targeted aspect-based sentiment analysis in the domain of urban neighbourhoods.\n\u2022 We show that despite the fact that the texts in QA were not written with the goal of writing a review in mind, question answering platforms and online forums are in general rich in information.\n\u2022 We provide strong baselines for the task using both logistic regression and Long Short Term Memory (LSTM) networks and analysis of the results."}, {"heading": "2 SentiHood", "text": "SentiHood is a dataset for the task of targeted aspect-based sentiment analysis. It is based on the text taken from question answering platform of Yahoo! Answers that is filtered for questions relating to neighbourhoods of the city of London. In this section we explain the data collection and annotation process and summarise properties of the dataset."}, {"heading": "2.1 Data Collection Process", "text": "Entities in the dataset are locations or neighbourhoods. Yahoo! Answers was queried using the name of each neighbourhood of the city of London. Location (entity) names were taken from the gazetteer GeoNames1 and restricted to those within the boundaries of London. This list includes names of areas and boroughs and therefore entities are not always geographically exclusive (a borough contains several areas or neighbourhoods). The content of each question-answer pairs was aggregated and split into sentences. We keep only sentences that have a mention of a location entity name and discard other sentences."}, {"heading": "2.2 Categories", "text": "The Number of location mentions in a single sentence in our dataset varies from one to over 50. To simplify the task, we only annotate sentences that contain one or two location mentions. These sentences were divided into two groups: sentences containing one location mention \u2014 Single, and sentences containing two location mentions \u2014 Multi. This is to observe the difficulty of annotating two groups by human annotators and by the models."}, {"heading": "2.3 Aspects", "text": "Like existing work in the aspect-based sentiment analysis task (Brychc\u0131n et al., 2014), a pre-defined list of aspects is provided for annotators to choose from. These aspects are: live, safety, price, quiet, dining, nightlife, transit-location, touristy, shopping, green-culture and multicultural. Adding an additional aspect of misc was considered. However in the initial round of annotations, we realised that it had a negative effect on the decisiveness of annotators and it led to a lower overall agreement. Aspect general refers to a generic opinion about a location, e.g. \u201cI love Camden Town\u201d."}, {"heading": "2.4 Sentiment", "text": "For each selected aspect, annotators were required to select a polarity or sentiment. Most work in this area considers three sentiment categories of \u201cPositive\u201d. \u201cNegative\u201d and \u201cNeutral\u201d. In our annotation however, we only provided \u201cPositive\u201d and \u201cNegative\u201d sentiment labels. This is because in our data we rarely come across cases where aspects are discussed without a polarity.\n1http://www.geonames.org/"}, {"heading": "2.5 Target Entity", "text": "Target entity is a location entity in which an opinion (aspect and sentiment) is expressed for. We also refer to target entity as target location."}, {"heading": "2.6 Out of scope", "text": "For the sentences that do not comply with our schema, we define the two following special labels. Sentences marked with one of the these labels are removed from the dataset.\n1. Irrelevant: When the identified name does not refer to a location entity: for example in the sentence \u201cNotting Hill (1999) stars Julia Roberts and Hugh Grant use the characteristic features of the area as a backdrop to the action\u201d, \u201cNotting Hill\u201d refers to the movie and not the area.\n2. Uncertain: When two contradicting sentiments are expressed for the same location and aspect, e.g. \u201cLike any other area, Camden Town has good and bad parts\u201d. Moreover, when the opinion is expressed for an area without a direct mention in the sentences, e.g. \u201cIt\u2019s a very trendy area and not too far from King\u2019s Cross\u201d."}, {"heading": "2.7 Procedure:", "text": "We use the BRAT annotation tool (Stenetorp et al., 2012) to simplify the annotation task. Three annotators were initially selected for the task. None of the annotators are experts in linguistics. Annotators began by reading the guidelines and examples. Each annotator was then required to annotate a small subset of the data. After each round of annotation, agreements between annotators were calculated and discussed and this procedure continued until they reached a reasonable agreement. 10% of the whole dataset was randomly selected and annotated by all the three annotators. The annotator with the highest inter-annotator agreement was selected to annotate all the dataset.\nAgreements: Cohen\u2019s Kappa coefficient(K) (COHEN, 1960) is often used for measuring the pairwise agreement between each two annotators for the task of aspect-based sentiment analysis (Gamon et al., 2005; Ganu et al., 2009) and other tasks (Liakata et al., 2010). The Kappa Coefficient is calculated over aspect-sentiment pairs per each location. Pairwise inter-annotator agreement for aspect categories measured using Cohen\u2019s Kappa is 0.73, 0.78 and 0.70, which is deemed of sufficient quality. It is worth mentioning that agreements on different aspect categories varied, with some aspects having a higher agreement rate. Agreements for aspect expressions are 0.93, 0.94, 0.93. These agreements indicate reasonably high inter-annotator agreements (Pavlopoulos, 2014).\nDisagreements: Main disagreements between annotators occurred in detecting the aspect rather than detecting the sentiment, aspect expression or the target location. For instance, some annotators associated the expression \u201cresidential area\u201d with a \u201cPositive\u201d sentiment for aspect \u201cquiet\u201d or \u201clive\u201d and others did not agree that \u201cresidential\u201d implies quietness or desirable for living. In the case of disagreements, the vote of the majority was considered as the correct annotation.\nSome ambiguity was also observed with respect to detecting the target location. This occurred mainly when a location is confined in another location. For instance the sentence \u201cAngel in Inslington has many great restaurants for eating out\u201d expresses a \u201cPositive\u201d sentiment for the aspect \u201cdining\u201d of area Angel which is within the borough of Islington. Some annotators suggested that the sentence also implies the same opinion for Islington. However at the end all annotators agreed that in such cases no implicit assumptions should be made and only confined area should be labeled."}, {"heading": "2.8 Dataset", "text": "SentiHood currently contains annotated sentences containing one or two location entity mentions.2 SentiHood contains 5215 sentences with 3862 sentences containing a single location and 1353 sentences containing multiple (two) locations. Figure 1 shows the number of sentences that are labeled with each aspect, breaking down on the sentiment \u201cPositive\u201d or \u201cNegative\u201d. \u201cPositive\u201d sentiment is dominant for\n2SentiHood data can be obtained at http://annotate-neighborhood.com/download/download.html\naspects such as dining and shopping. This shows that for some aspects, people usually talk about areas that are good for it as oppose to areas that are not. The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences. Notice that since each sentence can contain one or more opinions, the total number of opinions (5920) in the dataset is higher than the number of sentences.\nLocation entity names are masked by location1 and location2 in the whole dataset, so the task does not involve identification and segmentation of the named entities. We also provide the dataset with the original location entity names."}, {"heading": "3 Task", "text": "We define the task of targeted aspect-based sentiment analysis as follows: given a unit of text s (for example, a sentence), provide a list of tuples (labels) {(l, a, p)}Tt=0, where p is the polarity expressed for the aspect a of entity l. Each sentence can have zero to T number of labels associated with it.\nWithin the current aspect-based sentiment analysis work, three tasks are defined (Brychc\u0131n et al., 2014): detecting the aspect, detecting the opinion target expression and detecting the sentiment, with detecting the opinion target expression being an intermediary task for identifying the sentiment of the aspect.\nHere we focus on identifying only the aspect and sentiment for each entity. We identify each aspect, its relevant sentiment and the target location entity jointly by introducing a new polarity class called \u201cNone\u201d. \u201cNone\u201d indicated that a sentence does not contain an opinion for the aspect a of location l. Therefore the overall task can be defined as a three-class classification task for each (l, a) pair with labels \u201cPositive\u201d, \u201cNegative\u201d, \u201cNone\u201d. Table 2 shows an example of the input sentence and output labels."}, {"heading": "4 Evaluation", "text": "Most existing work in aspect-based sentiment analysis field, report F1 measure for aspect detection task, and accuracy for sentiment classification. The scores can be calculated over 2-class or 3-class sentiments (Pontiki et al., 2015). In our results, F1 score is calculated with a threshold that is optimized on validation set.\nWe also propose the AUC (area under the ROC curve) metric for both aspect and sentiment detection tasks. AUC captures the quality of the ranking of output scores and does not rely on a threshold."}, {"heading": "5 Baseline", "text": "Here we propose baselines for the task. In all our methods, we treat the task as a three-class classification for each aspect and use a softmax function as follows:\np(yl,a = c) = softmax(c) = exp(wc.el + bc)\u2211C\nc\u2032=1 exp(wc\u2032 .el + bc\u2032) (1)\nwhere yl,a is the sentiment label of aspect a for location l. wc and bC are the weights and the bias specific to each sentiment class c, respectively. el is a representation of location l. This representation can be a BoW or a distributional representation. Each method that we propose here define their own specific representation for el."}, {"heading": "5.1 Logistic Regression", "text": "Many existing works in the aspect-based sentiment analysis task,3 use a classifier, such as logistic regression or SVM, based on linguistic features such as n-grams, POS information or more hand-engineered features. We can think of these features as a sparse representation el that enter the softmax in equation 1. More concretely, we define the following sparse representations of locations:\nMask target entity n-grams: For each location, we define an n-gram representation over the sentence and mask the target location using a special token. This can help to differentiate between representations of two locations present in the same sentence.\nLeft-right n-grams: we create an n-gram representation for both the right and the left context around each location mention. We then concatenate these two representations to obtain one single feature vector.\nLeft right pooling: Previously embedding representations over the left and right context have been used for automatic feature detection in the targeted sentiment analysis task (Vo and Zhang, 2015). Inspired by this approach, we obtain max, min, average and standard deviation pooling over all the word embeddings for left and right context separately. We then combine the pooled embeddings of the left and right context to obtain a single feature vector. Word embeddings are obtained by running word2vec tool on a combination of our Yahoo! Answers corpus and a substantially big corpus from the web.4"}, {"heading": "5.2 Long Short-Term Memory (LSTM)", "text": "Inspired by the recent success of applying deep neural networks on language tasks, we use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to learn a classifier for each of the aspects. Representations for a location (el) are obtained using one of the following two approaches:\nFinal output state (LSTM - Final): el is the output embedding of the bidirectional LSTM.\nLocation output state (LSTM - Location): el is the output representation at the index corresponding to the location entity as illustrated in Figure 2.\n3including participants of SemEval ABSA tasks 4http://ebiquity.umbc.edu/redirect/to/resource/id/351/UMBC-webbase-corpus"}, {"heading": "6 Experiments", "text": "In this paper, we select the four most frequent aspects from the dataset which are: \u201cprice\u201d, \u201csafety\u201d, \u201ctransit-location\u201d and \u201cgeneral\u201d but the same approach can be applied to the remaining aspects. We divide each collection of single and multiple location mentions into train, dev and test set, with each having 70%, 10% and 20% of data respectively. We choose the best model with respect to the dev set.\nIn the case of the LSTM, we evaluate the loss on both training set and dev set after each iteration. We save the best model which has the lowest loss on the dev set over all the iterations. We then run this model on the test set and report the results. We report results separately on both categories of single location sentences and sentences with two locations and over all the data in the test set. Results on single location sentences mainly show the ability of the model to detect the correct sentiment for an aspect. On the other hand, results on two location sentences demonstrate the ability of the system not only on detecting the relevant sentiment of an aspect but also on recognising the target entity of the opinion.\nTraining LSTMs We implement our LSTM models using tensorflow (ten, 2015). To tackle the problem of having an unbalanced dataset (i.e. too many \u201cNone\u201d instances), we train the LSTM model in batches with every batch having the same number of sentences selected randomly from each sentiment class. We tune the hyper parameters of the model on the dev set. The best model uses hidden units of size 50 and batch sizes of size 150. The Adam optimizer is used for optimization with a starting learning rate of 0.01 which is tuned to be the best performing on the dev set. Dropout is used both on initial word embeddings and on LSTM cells with the probability of 0.001. Tensorflow (ten, 2015) is used for the implementation of LSTM.\nTraining Logistic Regression Logistic regression models were based on implementations from scikitlearn.5 Since we have an unbalanced dataset, we use a weighted logistic regression. To obtain the best weights, we cross-validate them on the development set. Weights inversely proportional to the size of each class result in the best performance.\n5http://scikit-learn.org/"}, {"heading": "7 Results", "text": "Table 3 shows the results (averaged over all selected aspects) in terms of both F1/accuracy and AUCs. It also shows the results of logistic regression based models versus LSTM models.\nAs we can see, the n-gram representation with location masking achieves slightly better results over the left-right context. N-grams include unigrams and bigrams. Also, by adding POS information, we gain an increase in the performance. We also experimented with adding tri-grams but it did not have a positive effect on the overall scores. Separating the left and the right context (LR-Left-Right) for BoW representation, does not improve the performance. Left-right pooling of dense embeddings performed weakly in comparison with other representations and therefore their results were omitted.\nAmongst the two variations of LSTM, the model with final state embeddings does slightly better than the model where we use the embeddings at the location index, however they are not significantly different (with a p value less than 0.01). It is interesting to note that the best LSTM model is not superior to logistic regression model, especially in terms of AUC. This can be due to the fact that the amount of training data is not sufficient for LSTM to perform well. Moreover, while we provide some grammar information to logistic regression model through POS tags, such information is not incorporated into LSTM models. Another interesting observation is that the F1 measure for logistic regression model with n-grams and POS information is very low while this model\u2019s performance is superior to other models in terms of AUC. This is because in general, it is easier to rank prediction scores than to assign predicted labels to instances by choosing a hard threshold.\nTable 4 shows the average AUC (over aspect and sentimentclassification tasks) for two categories of data: Single \u2014 sentences that contain one location entity and Multi \u2014 sentences that contain two location entities. While logistic regression can perform slightly better on son Single location sentences, LSTM performs slightly better on Multi location sentences.\nTable 5 shows the break down of average AUC scores for each aspect. We can see that aspects such as \u201csafety\u201d can be predicted with a better AUC score than aspect \u201cgeneral\u201d.\nTable 6 shows examples of correct and incorrect predictions using the best logistic regression model. The top part of the table contains examples that each contain a single location entity. At the bottom of the table, a sentence with two location entities is provided. The system correctly identifies that a \u201cPositive\u201d sentiment is expressed for the general aspect about location2. However, no sentiment is expressed for this aspect for location1."}, {"heading": "8 Related Work", "text": "The term sentiment analysis was first used in (et al, 2003). Since then, the field has received much attention from both research and industry. Sentiment analysis has applications in almost in every domain and it raised many interesting research questions. Furthermore, the availability of a huge volume of opinionated data on social media platforms has accelerated the development in this area.\nIn the beginning work on sentiment analysis mainly focused on identifying the overall sentiment of a unit of text. The unit of text varied from document (Pang et al., 2002; Turney, 2002), paragraph or sentences (Hu and Liu, 2004). However, only considering the overall sentiment fails to capture the sentiments over the aspects on which an entity can be reviewed or sentiment expressed toward different entities. Two remedy this, two new tasks have been introduced: aspect-based sentiment analysis and targeted sentiment analysis.\nAspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010). This task however does not consider more than one entity in the given text.\nTargeted (target dependent) sentiment analysis is another task that identifies polarity towards a target entity (as opposed to over entire unit of text) (Mitchell et al., 2013; Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015; Zhang et al., 2016). (Jiang et al., 2011) was the first to propose targeted sentiment analysis on Twitter and demonstrates the importance of targets by showing that 40% of sentiment errors are due to not considering them in classification. However this task only identifies the overall sentiment and the existing corpora for the task consist only of text with one single entity per unit of analysis.\nThe task of targeted aspect-based sentiment analysis caters for more generic text by making fewer assumptions while extracting fine-grained information."}, {"heading": "9 Conclusion", "text": "In this paper, we introduced the task of targeted aspect-based sentiment analysis and a new dataset. We also provide two strong baselines using logistic regression and LSTM. Ways to improve the baselines can involve using parse trees for identifying the context of each location. Data augmentation can be used\nto make the models and especially LSTM more robust to variations in the data. We also like to provide more detailed analysis of what each system can achieve."}], "references": [{"title": "A Vector Space Approach for Aspect-Based Sentiment Analysis", "author": ["Abdulaziz Alghunaim."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Alghunaim.,? 2015", "shortCiteRegEx": "Alghunaim.", "year": 2015}, {"title": "Care more about customers: unsupervised domain-independent aspect detection for sentiment analysis of customer reviews", "author": ["Ayoub Bagheri", "Mohamad Saraee", "Franciska De Jong."], "venue": "Knowledge-Based Systems, 52:201\u2013213.", "citeRegEx": "Bagheri et al\\.,? 2013", "shortCiteRegEx": "Bagheri et al\\.", "year": 2013}, {"title": "An unsupervised aspect-sentiment model for online reviews", "author": ["Samuel Brody", "Noemie Elhadad."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 804\u2013812. Association for Computational Linguistics.", "citeRegEx": "Brody and Elhadad.,? 2010", "shortCiteRegEx": "Brody and Elhadad.", "year": 2010}, {"title": "Uwb: Machine learning approach to aspect-based sentiment analysis", "author": ["Tom\u00e1\u0161 Brychc\u0131n", "Michal Konkol", "Josef Steinberger."], "venue": "SemEval 2014, page 817.", "citeRegEx": "Brychc\u0131n et al\\.,? 2014", "shortCiteRegEx": "Brychc\u0131n et al\\.", "year": 2014}, {"title": "A coefficient of agreement for nominal scales", "author": ["JACOB COHEN."], "venue": "Educational and Psychological Measurement, 20(1):37\u201346.", "citeRegEx": "COHEN.,? 1960", "shortCiteRegEx": "COHEN.", "year": 1960}, {"title": "Yahoo! for amazon: Extracting market sentiment from stock message boards", "author": ["Sanjiv Das", "Mike Chen."], "venue": "Proceedings of the Asia Pacific finance association annual conference (APFA), volume 35, page 43. Bangkok, Thailand.", "citeRegEx": "Das and Chen.,? 2001", "shortCiteRegEx": "Das and Chen.", "year": 2001}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu."], "venue": "ACL (2), pages 49\u201354.", "citeRegEx": "Dong et al\\.,? 2014", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Extracting sentiments about a given topic using natural language processing techniques; jeonghee yi et al; ibm", "author": ["Jeonghee Yi"], "venue": "Proceedings of the Third IEEE International Conference on Data Mining (ICDM\u201903) 0-7695-1978-4/03, volume 17.", "citeRegEx": "Yi,? 2003", "shortCiteRegEx": "Yi", "year": 2003}, {"title": "Pulse: Mining customer opinions from free text", "author": ["Michael Gamon", "Anthony Aue", "Simon Corston-Oliver", "Eric Ringger."], "venue": "Advances in Intelligent Data Analysis VI, pages 121\u2013132. Springer.", "citeRegEx": "Gamon et al\\.,? 2005", "shortCiteRegEx": "Gamon et al\\.", "year": 2005}, {"title": "Beyond the stars: Improving rating predictions using review text content", "author": ["Gayatree Ganu", "Noemie Elhadad", "Am\u00e9lie Marian."], "venue": "WebDB, volume 9, pages 1\u20136. Citeseer.", "citeRegEx": "Ganu et al\\.,? 2009", "shortCiteRegEx": "Ganu et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177. ACM.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Target-dependent twitter sentiment classification", "author": ["Long Jiang", "Mo Yu", "Ming Zhou", "Xiaohua Liu", "Tiejun Zhao."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 151\u2013160. Association for Computational Linguistics.", "citeRegEx": "Jiang et al\\.,? 2011", "shortCiteRegEx": "Jiang et al\\.", "year": 2011}, {"title": "Aspect and sentiment unification model for online review analysis", "author": ["Yohan Jo", "Alice H Oh."], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, pages 815\u2013824. ACM.", "citeRegEx": "Jo and Oh.,? 2011", "shortCiteRegEx": "Jo and Oh.", "year": 2011}, {"title": "Aspect specific sentiment analysis using hierarchical deep learning", "author": ["Himabindu Lakkaraju", "Richard Socher", "Chris Manning"], "venue": null, "citeRegEx": "Lakkaraju et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2014}, {"title": "Corpora for the conceptualisation and zoning of scientific papers", "author": ["Maria Liakata", "Simone Teufel", "Advaith Siddharthan", "Colin R Batchelor"], "venue": "In LREC. Citeseer", "citeRegEx": "Liakata et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liakata et al\\.", "year": 2010}, {"title": "Multi-aspect sentiment analysis with topic models", "author": ["Bin Lu", "Myle Ott", "Claire Cardie", "Benjamin K Tsou."], "venue": "Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on, pages 81\u201388. IEEE.", "citeRegEx": "Lu et al\\.,? 2011", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "Open domain targeted sentiment", "author": ["Margaret Mitchell", "Jacqueline Aguilar", "Theresa Wilson", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2013}, {"title": "Mining product reputations on the web", "author": ["Satoshi Morinaga", "Kenji Yamanishi", "Kenji Tateishi", "Toshikazu Fukushima."], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 341\u2013349. ACM.", "citeRegEx": "Morinaga et al\\.,? 2002", "shortCiteRegEx": "Morinaga et al\\.", "year": 2002}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79\u201386. Association for Computational Linguistics.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Aspect based sentiment analysis", "author": ["Ioannis Pavlopoulos."], "venue": "Athens University of Economics and Business.", "citeRegEx": "Pavlopoulos.,? 2014", "shortCiteRegEx": "Pavlopoulos.", "year": 2014}, {"title": "Semeval-2015 task 12: Aspect based sentiment analysis", "author": ["Maria Pontiki", "Dimitrios Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Association for Computational Linguistics, Denver, Colorado, pages 486\u2013495.", "citeRegEx": "Pontiki et al\\.,? 2015", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "Automatic product feature extraction from online product reviews using maximum entropy with lexical and syntactic features", "author": ["Gamgarn Somprasertsri", "Pattarachai Lalitrojwong."], "venue": "Information Reuse and Integration, 2008. IRI 2008. IEEE International Conference on, pages 250\u2013255. IEEE.", "citeRegEx": "Somprasertsri and Lalitrojwong.,? 2008", "shortCiteRegEx": "Somprasertsri and Lalitrojwong.", "year": 2008}, {"title": "Brat: a web-based tool for nlp-assisted text annotation", "author": ["Pontus Stenetorp", "Sampo Pyysalo", "Goran Topi\u0107", "Tomoko Ohta", "Sophia Ananiadou", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Stenetorp et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stenetorp et al\\.", "year": 2012}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["Ivan Titov", "Ryan McDonald."], "venue": "Proceedings of the 17th international conference on World Wide Web, pages 111\u2013120. ACM.", "citeRegEx": "Titov and McDonald.,? 2008", "shortCiteRegEx": "Titov and McDonald.", "year": 2008}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["Peter D Turney."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 417\u2013424. Association for Computational Linguistics.", "citeRegEx": "Turney.,? 2002", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "Target-dependent twitter sentiment classification with rich automatic features", "author": ["Duy-Tin Vo", "Yue Zhang."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015), pages 1347\u20131353.", "citeRegEx": "Vo and Zhang.,? 2015", "shortCiteRegEx": "Vo and Zhang.", "year": 2015}, {"title": "Gated neural networks for targeted sentiment analysis", "author": ["Meishan Zhang", "Yue Zhang", "Duy-Tin Vo."], "venue": "Thirtieth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Early research in the field (Das and Chen, 2001; Morinaga et al., 2002) of sentiment analysis only focused on identifying the overall sentiment or polarity of a given text.", "startOffset": 28, "endOffset": 71}, {"referenceID": 18, "context": "Early research in the field (Das and Chen, 2001; Morinaga et al., 2002) of sentiment analysis only focused on identifying the overall sentiment or polarity of a given text.", "startOffset": 28, "endOffset": 71}, {"referenceID": 13, "context": "Aspect-based sentiment analysis (ABSA) (Jo and Oh, 2011; Pontiki et al., 2015; Pontiki et al., 2016) relates to the task of extracting fine-grained information by identifying the polarity towards different aspects of an entity in the same unit of text, and recognizing the polarity associated with each aspect separately.", "startOffset": 39, "endOffset": 100}, {"referenceID": 21, "context": "Aspect-based sentiment analysis (ABSA) (Jo and Oh, 2011; Pontiki et al., 2015; Pontiki et al., 2016) relates to the task of extracting fine-grained information by identifying the polarity towards different aspects of an entity in the same unit of text, and recognizing the polarity associated with each aspect separately.", "startOffset": 39, "endOffset": 100}, {"referenceID": 12, "context": "target-dependent) sentiment analysis (Jiang et al., 2011; Vo and Zhang, 2015).", "startOffset": 37, "endOffset": 77}, {"referenceID": 26, "context": "target-dependent) sentiment analysis (Jiang et al., 2011; Vo and Zhang, 2015).", "startOffset": 37, "endOffset": 77}, {"referenceID": 3, "context": "3 Aspects Like existing work in the aspect-based sentiment analysis task (Brychc\u0131n et al., 2014), a pre-defined list of aspects is provided for annotators to choose from.", "startOffset": 73, "endOffset": 96}, {"referenceID": 23, "context": "7 Procedure: We use the BRAT annotation tool (Stenetorp et al., 2012) to simplify the annotation task.", "startOffset": 45, "endOffset": 69}, {"referenceID": 4, "context": "Agreements: Cohen\u2019s Kappa coefficient(K) (COHEN, 1960) is often used for measuring the pairwise agreement between each two annotators for the task of aspect-based sentiment analysis (Gamon et al.", "startOffset": 41, "endOffset": 54}, {"referenceID": 8, "context": "Agreements: Cohen\u2019s Kappa coefficient(K) (COHEN, 1960) is often used for measuring the pairwise agreement between each two annotators for the task of aspect-based sentiment analysis (Gamon et al., 2005; Ganu et al., 2009) and other tasks (Liakata et al.", "startOffset": 182, "endOffset": 221}, {"referenceID": 9, "context": "Agreements: Cohen\u2019s Kappa coefficient(K) (COHEN, 1960) is often used for measuring the pairwise agreement between each two annotators for the task of aspect-based sentiment analysis (Gamon et al., 2005; Ganu et al., 2009) and other tasks (Liakata et al.", "startOffset": 182, "endOffset": 221}, {"referenceID": 15, "context": ", 2009) and other tasks (Liakata et al., 2010).", "startOffset": 24, "endOffset": 46}, {"referenceID": 20, "context": "These agreements indicate reasonably high inter-annotator agreements (Pavlopoulos, 2014).", "startOffset": 69, "endOffset": 88}, {"referenceID": 3, "context": "Within the current aspect-based sentiment analysis work, three tasks are defined (Brychc\u0131n et al., 2014): detecting the aspect, detecting the opinion target expression and detecting the sentiment, with detecting the opinion target expression being an intermediary task for identifying the sentiment of the aspect.", "startOffset": 81, "endOffset": 104}, {"referenceID": 21, "context": "The scores can be calculated over 2-class or 3-class sentiments (Pontiki et al., 2015).", "startOffset": 64, "endOffset": 86}, {"referenceID": 26, "context": "Left right pooling: Previously embedding representations over the left and right context have been used for automatic feature detection in the targeted sentiment analysis task (Vo and Zhang, 2015).", "startOffset": 176, "endOffset": 196}, {"referenceID": 10, "context": "2 Long Short-Term Memory (LSTM) Inspired by the recent success of applying deep neural networks on language tasks, we use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to learn a classifier for each of the aspects.", "startOffset": 143, "endOffset": 177}, {"referenceID": 19, "context": "The unit of text varied from document (Pang et al., 2002; Turney, 2002), paragraph or sentences (Hu and Liu, 2004).", "startOffset": 38, "endOffset": 71}, {"referenceID": 25, "context": "The unit of text varied from document (Pang et al., 2002; Turney, 2002), paragraph or sentences (Hu and Liu, 2004).", "startOffset": 38, "endOffset": 71}, {"referenceID": 11, "context": ", 2002; Turney, 2002), paragraph or sentences (Hu and Liu, 2004).", "startOffset": 46, "endOffset": 64}, {"referenceID": 16, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 14, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 0, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 1, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 22, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 0, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 16, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 24, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 2, "context": "Aspect based sentiment analysis assumes a single entity per a unit of analysis and tries to identify sentiments towards different aspects of the entity (Lu et al., 2011; Lakkaraju et al., 2014; Alghunaim, 2015; Bagheri et al., 2013; Somprasertsri and Lalitrojwong, 2008; Alghunaim, 2015; Lu et al., 2011; Titov and McDonald, 2008; Brody and Elhadad, 2010).", "startOffset": 152, "endOffset": 355}, {"referenceID": 17, "context": "Targeted (target dependent) sentiment analysis is another task that identifies polarity towards a target entity (as opposed to over entire unit of text) (Mitchell et al., 2013; Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015; Zhang et al., 2016).", "startOffset": 153, "endOffset": 255}, {"referenceID": 12, "context": "Targeted (target dependent) sentiment analysis is another task that identifies polarity towards a target entity (as opposed to over entire unit of text) (Mitchell et al., 2013; Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015; Zhang et al., 2016).", "startOffset": 153, "endOffset": 255}, {"referenceID": 6, "context": "Targeted (target dependent) sentiment analysis is another task that identifies polarity towards a target entity (as opposed to over entire unit of text) (Mitchell et al., 2013; Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015; Zhang et al., 2016).", "startOffset": 153, "endOffset": 255}, {"referenceID": 26, "context": "Targeted (target dependent) sentiment analysis is another task that identifies polarity towards a target entity (as opposed to over entire unit of text) (Mitchell et al., 2013; Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015; Zhang et al., 2016).", "startOffset": 153, "endOffset": 255}, {"referenceID": 27, "context": "Targeted (target dependent) sentiment analysis is another task that identifies polarity towards a target entity (as opposed to over entire unit of text) (Mitchell et al., 2013; Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015; Zhang et al., 2016).", "startOffset": 153, "endOffset": 255}, {"referenceID": 12, "context": "(Jiang et al., 2011) was the first to propose targeted sentiment analysis on Twitter and demonstrates the importance of targets by showing that 40% of sentiment errors are due to not considering them in classification.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks.", "creator": "TeX"}}}