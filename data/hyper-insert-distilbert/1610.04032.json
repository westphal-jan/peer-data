{"id": "1610.04032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Predicting the dynamics of 2d objects with a deep residual network", "abstract": "fundamentally we investigate how a continuous residual network can learn to predict the static dynamics responses of interacting shapes purely indirectly as an experimental image - to - objects image regression problem.", "histories": [["v1", "Thu, 13 Oct 2016 11:27:07 GMT  (1168kb,D)", "https://arxiv.org/abs/1610.04032v1", null], ["v2", "Thu, 24 Nov 2016 11:12:52 GMT  (1217kb,D)", "http://arxiv.org/abs/1610.04032v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["fran\\c{c}ois fleuret"], "accepted": false, "id": "1610.04032"}, "pdf": {"name": "1610.04032.pdf", "metadata": {"source": "CRF", "title": "Predicting the dynamics of 2d objects with a deep residual network", "authors": ["Fran\u00e7ois Fleuret"], "emails": ["francois.fleuret@idiap.ch"], "sections": [{"heading": null, "text": "We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression task.\nWith a simple 2d physics simulator, we generate short sequences composed of rectangles put in motion by applying a pulling force at a point picked at random. The network is trained with a quadratic loss to predict the image of the resulting configuration, given the image of the starting configuration and an image indicating the point of grasping.\nExperiments show that the network learns to predict accurately the resulting image, which implies in particular that (1) it segments rectangles as distinct components, (2) it infers which one contains the grasping point, (3) it models properly the dynamic of a single rectangle, including the torque, (4) it detects and handles collisions to some extent, and (5) it re-synthesizes properly the entire scene with displaced rectangles."}, {"heading": "1 Problem definition", "text": "We implemented a simple 2d physics simulator to generate short sequences of interacting shapes. The simulation is quite crude but still includes an elastic collision model, a proper torque model, and (strong) fluid frictions.\nAs illustrated with a few examples on Figure 1, each sequence is composed of grayscale images of resolution 64\u00d764, and is created as follows: We dispatch 10 rectangles\n\u2217francois.fleuret@idiap.ch\nar X\niv :1\n61 0.\n04 03\n2v 2\n[ cs\n.C V\n] 2\nof fixed size at random in the unit square, so that they do not overlap. Then we pick at random a point uniformly in the union of the rectangle interiors, and we apply there a constant force pulling upward for a constant time delay. This moves the grasped rectangle upward and may induce collisions with other rectangles, and make them move. The borders of the square area are impenetrable, hence rectangles grabbed near the top may have their motion constrained accordingly.\nWhile the grasping point location is randomized for every sequence, the characteristics of the force and its duration are common to all the sequences.\nAs illustrated on Figure 1, from each generated sequence we produce three images: Gn, Sn, Rn which correspond, respectively, to the grasping point image (all white, with a dot at the location of the grasp, as shown in the leftmost column of Figure 1), the starting configuration, which is the first image of the sequence, and the resulting configuration, which is the last image of the sequence."}, {"heading": "2 Network and training", "text": "We train a residual network (He et al., 2015) with 18 layer and 16 channels to predict Rn, given Gn and Sn as input.\nTo ease the reading of long compositions of mappings, given two mappings f and g, let f B g stand for g \u25e6 f ."}, {"heading": "2.1 Structure of the network", "text": "Our network follows the classical structure of the residual networks, and chains several identical modules of two convolutional layers. We define\n\u2022 Cfc,d a standard convolution layer (LeCun et al., 1998) with filters of size f \u00d7 f , padding of (f \u2212 1)/2 to maintain the map size, c channels as input and d channels as output,\n\u2022 R a ReLU rectifier layer (Glorot et al., 2011),\n\u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015),\n\u2022 I the identity layer, and \u2022 M = ( Cfq,q B B B R B Cfq,q + I ) B B B R a two-layer resnet module (He\net al., 2015) with the second batch normalization and non-linearity applied after summing the identity.\nThe structure of the full network is\n\u03a8 = Cf2,q B B B R B M B . . . B M\ufe38 \ufe37\ufe37 \ufe38 \u00d7D B Cfq,1 (1)\nwith convolution filters of size f \u00d7 f with f = 5, q = 16 channels in the internal encoding, and D = 8 resnet modules, each with two layers. It has a total of 104, 417 parameters, which is roughly f 2 \u00d7 q2 \u00d7 2D."}, {"heading": "2.2 Loss, initialization and training", "text": "We minimize the quadratic loss between the predicted and target training images, L = \u2211 n \u2016\u03a8(Sn, Gn)\u2212Rn\u201622 (2)\nand train with 32, 768 samples. We use a standard stochastic gradient descent, randomizing the training set ordering for every epoch, using mini-batches of size 128, and a constant learning rate of 0.1.\nThe initialization of the weights is the standard Torch rule, which for the convolution layers is a centered uniform distribution of width twice the inverse of the square root of the number of weights (i.e. total number of filter coefficients), and for the batch normalization picks the target standard deviation uniformly in [0, 1] and sets the target mean to zero.\nWe did not tune the network structure, all the results obtained here are with the first attempt. A run with half the channels (i.e. q = 8) shows that it degrades noticeably the performance.\n3 Results\nWe implemented the simulator in C++ and the network processing and performance evaluation in the Torch framework (Collobert et al., 2011). The network implementation is given in appendix A.\nThe source code of the simulator and the residual network to replicate the experiments is available under the GPL-3.0 open-source license at\nhttps://gitlab.idiap.ch/francois.fleuret/dyncnn/\nAs shown on Figure 2, the loss decreases regularly, with no over-fitting. It is still going down after 2, 000 epochs, which takes slightly less than 30 hours on a NVIDIA GTX 1080 graphic card, using cuda toolkit 8.0, and cudnn 5.1."}, {"heading": "3.1 Prediction", "text": "The resulting network makes an accurate prediction of the final configuration. We provide on Figure 3 five examples selected to illustrate the strengths and weaknesses of the prediction, and on Figure 4 some examples taken according to the ranks of their individual losses to get a better intuition of the overall performance.\nWe observe that the network:\n\u2022 detects the grasped rectangle, and moves it while keeping the other ones undisturbed if there is no collision.\n\u2022 models translation and torque.\n\u2022 propagates to some extent the dynamics when collisions occur (Figure 3(d)).\n\u2022 models the hard borders around the area, although with some deformations (Figure 3(b)).\n\u2022 implements the synthesis of the perturbed scene, which involves in particular the segmentation of the moving vs. non-moving parts, and synthesis of edges at multiple orientations.\nFigure 4 gives a better understanding of the overall performance of the network on 1, 024 test sequences. The six examples (a)-(f) shown in the top half are those with the highest losses, hence are the worst mistakes of the network, and the six bottom examples (g)-(l) correspond to the ones ranked in the middle.\nIt is remarkable that the worst mistakes correspond to complicated cases even for a human, where the predicted motion would have been correct for a minimal variation of the starting conditions (e.g. Figure 4(a), and (b)), or involves a chain of collisions (e.g. Figure 4(c), (d), (e), and (f))"}, {"heading": "3.2 Inner representation", "text": "To shade a light on the processing occurring in the network, we represent on Figure 5 for two examples the processing from top to bottom as the activations of the input layer, ReLU layers after each resnet module, and output layer.\nThe top row contains two activation maps corresponding to the two input channels, respectively the starting configuration Sn and the grasping point Gn, the bottom row contains a single map, corresponding to the network\u2019s output \u03a8(Sn, Gn). The 9 other rows correspond to the ReLU layer situated after the initial convolution layer Cf2,q that converts the 2 input channels to the q internal channels, followed by the ReLU layers placed at the output of each of the D = 8 resnet modules M. The 16 columns in this 9 rows correspond to the q = 16 channels for the internal coding.\nWe observe a homogeneity \u201cper channel\u201d, which translates here to \u201cper column\u201d. This is probably because the resnet architecture favors processing near the identity, which results in gradual changes through layers, and discourages the shuffling of information across channels.\nAs we can see, an important part of the computation aims at segmenting the grasped rectangle (channels 11 and 15), segmenting the moving rectangles (channels 4 and 5), and removing the moving parts (channels 1, 10, and 16)."}, {"heading": "509/1024 510/1024 511/1024 512/1024 513/1024 514/1024", "text": ""}, {"heading": "1/1024 2/1024 3/1024 4/1024 5/1024 6/1024", "text": ""}, {"heading": "A Torch network structure", "text": "nn.Sequential {\n[input -> (1) -> (2) -> (3) -> (4) -> (5) -> output] (1): nn.SpatialConvolution(2 -> 16, 5x5, 1,1, 2,2) (2): nn.SpatialBatchNormalization (3): nn.ReLU (4): nn.Sequential {\n[input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8)\n-> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> (18) -> (19) -> (20) -> (21) -> (22) -> (23) -> (24) -> (25) -> (26) -> (27) -> (28) -> (29) -> (30) -> (31) -> (32) -> output]\n(1): nn.ConcatTable {\ninput\n|\u2018-> (1): nn.Sequential { | [input -> (1) -> (2) -> (3) -> (4) -> output] | (1): nn.SpatialConvolution(16 -> 16, 5x5, 1,1, 2,2) | (2): nn.SpatialBatchNormalization | (3): nn.ReLU | (4): nn.SpatialConvolution(16 -> 16, 5x5, 1,1, 2,2) | }\n\u2018-> (2): nn.Identity ... -> output\n} (2): nn.CAddTable (3): nn.SpatialBatchNormalization (4): nn.ReLU\n/... repeated 7 more times .../\n} (5): nn.SpatialConvolution(16 -> 1, 5x5, 1,1, 2,2)\n}"}], "references": [{"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In Proceedings of the BigLearn NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "We train a residual network (He et al., 2015) with 18 layer and 16 channels to predict Rn, given Gn and Sn as input.", "startOffset": 28, "endOffset": 45}, {"referenceID": 4, "context": "\u2022 C c,d a standard convolution layer (LeCun et al., 1998) with filters of size f \u00d7 f , padding of (f \u2212 1)/2 to maintain the map size, c channels as input and d channels as output, \u2022 R a ReLU rectifier layer (Glorot et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 1, "context": ", 1998) with filters of size f \u00d7 f , padding of (f \u2212 1)/2 to maintain the map size, c channels as input and d channels as output, \u2022 R a ReLU rectifier layer (Glorot et al., 2011), \u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer, and \u2022 M = ( C q,q B B B R B C q,q + I ) B B B R a two-layer resnet module (He et al.", "startOffset": 157, "endOffset": 178}, {"referenceID": 3, "context": ", 2011), \u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer, and \u2022 M = ( C q,q B B B R B C q,q + I ) B B B R a two-layer resnet module (He et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 2, "context": ", 2011), \u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer, and \u2022 M = ( C q,q B B B R B C q,q + I ) B B B R a two-layer resnet module (He et al., 2015) with the second batch normalization and non-linearity applied after summing the identity.", "startOffset": 166, "endOffset": 183}, {"referenceID": 0, "context": "We implemented the simulator in C++ and the network processing and performance evaluation in the Torch framework (Collobert et al., 2011).", "startOffset": 113, "endOffset": 137}], "year": 2016, "abstractText": "We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression task. With a simple 2d physics simulator, we generate short sequences composed of rectangles put in motion by applying a pulling force at a point picked at random. The network is trained with a quadratic loss to predict the image of the resulting configuration, given the image of the starting configuration and an image indicating the point of grasping. Experiments show that the network learns to predict accurately the resulting image, which implies in particular that (1) it segments rectangles as distinct components, (2) it infers which one contains the grasping point, (3) it models properly the dynamic of a single rectangle, including the torque, (4) it detects and handles collisions to some extent, and (5) it re-synthesizes properly the entire scene with displaced rectangles. 1 Problem definition We implemented a simple 2d physics simulator to generate short sequences of interacting shapes. The simulation is quite crude but still includes an elastic collision model, a proper torque model, and (strong) fluid frictions. As illustrated with a few examples on Figure 1, each sequence is composed of grayscale images of resolution 64\u00d764, and is created as follows: We dispatch 10 rectangles \u2217francois.fleuret@idiap.ch 1 ar X iv :1 61 0. 04 03 2v 2 [ cs .C V ] 2 4 N ov 2 01 6 of fixed size at random in the unit square, so that they do not overlap. Then we pick at random a point uniformly in the union of the rectangle interiors, and we apply there a constant force pulling upward for a constant time delay. This moves the grasped rectangle upward and may induce collisions with other rectangles, and make them move. The borders of the square area are impenetrable, hence rectangles grabbed near the top may have their motion constrained accordingly. While the grasping point location is randomized for every sequence, the characteristics of the force and its duration are common to all the sequences. Gn Sn Rn Figure 1: Each row corresponds to one sequence of our data-set. It is composed of six gray-scale images of size 64 \u00d7 64: the \u201cgrasping point image\u201d, followed by five frames. In each sequence, the rectangle originally containing the grasping point is pulled upward and moves accordingly. It may collide with and push other rectangles. We show several frames of each sequence for clarity here, but use only the two leftmost images (Sn, Gn) and the rightmost one Rn from each sequence in our experiments, in which we try to predict the latter from the former. As illustrated on Figure 1, from each generated sequence we produce three images: Gn, Sn, Rn which correspond, respectively, to the grasping point image (all white, with a dot at the location of the grasp, as shown in the leftmost column of Figure 1), the starting configuration, which is the first image of the sequence, and the resulting configuration, which is the last image of the sequence.", "creator": "LaTeX with hyperref package"}}}