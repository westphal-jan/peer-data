{"id": "1610.04794", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering", "abstract": "most learning approaches treat dimensionality phase reduction ( dr ) and clustering separately ( i. e., interacting sequentially ), much but recent continuous research reporting has shown that optimizing the two tasks jointly performing can substantially improve the functional performance of constructing both. the key premise behind incorporating the latter genre is probably that selecting the first data gathering samples are technically obtained exactly via automated linear neural transformation of our latent representations that arise are nonetheless easy to cluster ; but certainly in practice, the transformation from the corresponding latent space to the data can be more terribly complicated. in finishing this work, we assume that defining this process transformation is an unknown and inevitably possibly inherently nonlinear function. rather to formally recover the ` clustering - deter friendly'mutual latent representations label and design to simply better cluster the data, we typically propose building a joint learning dr and'k - means topological clustering approach in which either dr is accomplished via paired learning via a modeled deep scaled neural network ( generalized dnn ). externally the motivation is obviously to only keep the specific advantages of solving jointly optimizing the two tasks, while exploiting the deep neural network'surface s ability to approximate any known nonlinear function. this way, adapting the proposed approach can independently work even well for representing a broad class of software generative integration models. towards this theoretical end, independently we carefully design between the new dnn cluster structure and the associated joint optimization requirements criterion, design and propose an effective and sophisticated scalable calculation algorithm precisely to handle the incorrectly formulated optimization problem. joint experiments taken using five different real datasets are employed alternately to showcase respectively the computation effectiveness of the proposed approach.", "histories": [["v1", "Sat, 15 Oct 2016 22:51:06 GMT  (5826kb,D)", "http://arxiv.org/abs/1610.04794v1", "Main paper: 9 pages; Supplementary material: 3 pages"], ["v2", "Tue, 13 Jun 2017 22:40:26 GMT  (646kb,D)", "http://arxiv.org/abs/1610.04794v2", "Final ICML2017 version. Main paper: 10 pages; Supplementary material: 4 pages"]], "COMMENTS": "Main paper: 9 pages; Supplementary material: 3 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bo yang", "xiao fu", "nicholas d sidiropoulos", "mingyi hong"], "accepted": true, "id": "1610.04794"}, "pdf": {"name": "1610.04794.pdf", "metadata": {"source": "CRF", "title": "Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering", "authors": ["Bo Yang", "Xiao Fu", "Nicholas D. Sidiropoulos", "Mingyi Hong"], "emails": ["yang4173@umn.edu", "xfu@umn.edu", "nikos@ece.um.edu", "mingyi@iastate.edu"], "sections": null, "references": [{"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "For example, the stacked autoencoder (SAE) [27], deep CCA (DCCA) [1], and sparse autoencoder [18] take insights from PCA, CCA, and sparse coding, respectively, and make use of DNNs to learn nonlinear mappings from the data domain to low-dimensional latent spaces.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "In addition to the above classic DR methods that essentially learn a linear generative model from the latent space to the data domain, nonlinear DR approaches such as those used in spectral clustering and DNN-based DR are also widely used as pre-processing before K-means or other clustering algorithms [1,4,27].", "startOffset": 303, "endOffset": 311}], "year": 2016, "abstractText": "Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the \u2018clustering-friendly\u2019 latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network\u2019s ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using five different real datasets are employed to showcase the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}