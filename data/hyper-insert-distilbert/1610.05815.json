{"id": "1610.05815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Statistical Learning Theory Approach for Data Classification with l-diversity", "abstract": "corporations itself are also retaining ever - larger corpuses production of online personal household data ; noting the frequency or breaches and corresponding possible privacy violations impact have also been addressed rising slightly accordingly. one way to mitigate this risk is through use assurance of more anonymized data, limiting the minimum exposure of individual customer data solely to only professionals where it does is often absolutely needed. this process would seem particularly appropriate financially for data mining, though where the goal target is generalizable in knowledge rather than data bestowed on specific individuals. typically in research practice, corporate hybrid data miners often eagerly insist on original data, for increasing fear that they might \" miss noticing something \" with anonymized or differentially valued private approaches. this paper provides consumers a theoretical justification precisely for the increasing use probability of utilizing anonymized data. specifically, because we show indication that a support vector domain classifier presently trained on measuring anatomized lab data systematically satisfying lacking l - biological diversity should clearly be well expected to do as systematically well thus as on the original design data. anatomy preserves all significant data values, overlapping but introduces extreme uncertainty in the mapping between users identifying and managing sensitive values, thus satisfying l - diversity. the theoretical computational effectiveness effectiveness of modeling the proposed hierarchical approach presently is validated using several privately publicly released available datasets, showing indicators that we outperform the state of the art for interactive support scheme vector classification structures using training data explicitly protected initially by k - - anonymity, and are comparable to continuous learning effects on the original data.", "histories": [["v1", "Tue, 18 Oct 2016 22:14:27 GMT  (2170kb,D)", "http://arxiv.org/abs/1610.05815v1", "Technical Report"]], "COMMENTS": "Technical Report", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB", "authors": ["koray mancuhan", "chris clifton"], "accepted": false, "id": "1610.05815"}, "pdf": {"name": "1610.05815.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Koray Mancuhan", "Chris Clifton"], "emails": ["kmancuha@cs.purdue.edu", "clifton@cs.purdue.edu"], "sections": [{"heading": null, "text": "Corporations are retaining ever-larger corpuses of personal data; the frequency or breaches and corresponding privacy impact have been rising accordingly. One way to mitigate this risk is through use of anonymized data, limiting the exposure of individual data to only where it is absolutely needed. This would seem particularly appropriate for data mining, where the goal is generalizable knowledge rather than data on specific individuals. In practice, corporate data miners often insist on original data, for fear that they might \u201dmiss something\u201d with anonymized or differentially private approaches. This paper provides a theoretical justification for the use of anonymized data. Specifically, we show that a support vector classifier trained on anatomized data satisfying `-diversity should be expected to do as well as on the original data. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values, thus satisfying `-diversity. The theoretical effectiveness of the proposed approach is validated using several publicly available datasets, showing that we outperform the state of the art for support vector classification using training data protected by k-anonymity, and are comparable to learning on the original data."}, {"heading": "1 Introduction", "text": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]). Other alternatives include value swapping [24], distortion [2], randomization [12], and noise addition (e.g., differential privacy [11]). Generalization consists of replacing identifying attribute values with a less specific version [28]. Suppression can be viewed as the ultimate generalization, replacing the identifying value with an \u201cany\u201d value [28]. Generalization has the advantage of preserving truth, but a less specific truth that reduces utility of the published data.\nXiao and Tao proposed anatomization as a method\n\u2217Supported by the Northrop Grumman Cybersecurity Research Consortium \u2020Purdue University-West Lafayette; Department of Computer Science and CERIAS \u2021{kmancuha,clifton}@cs.purdue.edu\nto enforce `-diversity while preserving specific data values [33]. Anatomization splits instances across two tables, one containing identifying information and the other containing private information. The more general approach of fragmentation [7] divides a given dataset\u2019s attributes into two sets of attributes (2 partitions) such that an encryption mechanism avoids associations between two different small partitions. Vimercati et al. extend fragmentation to multiple partitions [9], and Tamas et al. propose an extension that deals with multiple sensitive attributes [13]. The main advantage of anatomization/fragmentation is that it preserves the original values of data; the uncertainty is only in the mapping between individuals and sensitive values.\nWe show that this additional information has real value. First, we demonstrate that in theory, learning from anatomized data can be as good as learning from the raw data. We then demonstrate empirically that learning from anatomized data beats learning from generalization-based anonymization.\nThis paper looks at linear support vector (SVC) and support vector machine (SVM) classifiers. This focus was chosen because these classifiers have a wide range of successful applications, and also have some solid theoretical basis their generalization properties. We propose a simple heuristic to preprocess the anatomized data such that SVC and SVM generalize well with sufficiently large training data.\nThere is concern that anatomization is vulnerable to several attacks [18, 14, 21]. While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [20, 11]. Introducing uncertainty into the anonymization process reduces the risk of many attacks, e.g., minimality [31, 8]. Our theoretical analysis holds for any assignment of items to anatomy groups, including a random assignment, which provides a high degree of robustness against minimality and correlationbased attacks. While this does not eliminate privacy risk, if the alternative is to use the original data, we show that anatomy provides comparable utility while reducing the privacy risk. This paper has the following key contributions: ar X\niv :1\n61 0.\n05 81\n5v 1\n[ cs\n.L G\n] 1\n8 O\nct 2\n01 6\n1. We define a classification task on anatomized data without violating the random worlds assumption. A violating classification task would be the prediction of sensitive attribute, a task that was found to be #P-complete by Kifer [18].\n2. We propose a heuristic algorithm to train SVC and SVM when the test data is neither anonymized nor anatomized. Inan et al. already gives a practical applications of such a learning scenario [15].\n3. We study the effect of our heuristic algorithm on the generalization error. To our best knowledge, this is the first paper in the privacy community that does such analysis for `-diversity\n4. In empirical analysis, our algorithm will be compared with SVM and SVC that are trained on either unprotected data or generalized data (under k-anonymity [15]). The analysis will be justified with the statistical learning theory [29, 5]\nWe next summarize related work and define the problem statement. We then give necessary definitions and notations. Section 4 proposes the heuristic algorithm and gives theoretical analysis. Empirical analysis is presented in section 5. Section 6 summarizes the work and gives future directions."}, {"heading": "2 Related Work and Problem Statement", "text": "There have been studies of linear classification for anonymized data. Agrawal et al. proposed an iterative distribution reconstruction algorithm for distorted training data from which a C4.5 decision tree classifier was trained [1]. Iyengar suggested using a classification metric so as to find the optimum generalization. Then, a C4.5 decision tree classifier was trained from the optimally generalized training data [16]. Dowd et al. studied C4.5 decision tree learning from training data perturbed by random substitutions. A matrix based distribution reconstruction algorithm was applied on the perturbed training data from which an accurate C4.5 decision tree classifier was learned [10]. Inan et al. proposed support vector machine classifiers using anonymized training data that satisfy k-anonymity. Taylor approximation was used to estimate the linear and RBF kernel computation from generalized data[15]. Rubinstein et al. studies the kernels of support vector machine in the differential privacy and show the tradeoff between privacy level and the data utility. They analyze finite and infinite dimensional kernels in function of the approximation error under differential privacy [26]. Lin at al. studies training support vector classification for outsourced data. Random transformation is applied\non the training set so that the cloud server computes the accurate model without knowing what the actual values are [22]. Jain et al. studies the support vector machine kernels in the differential privacy setting. They propose differentially private mechanisms to train support vector machines for interactive, semi-interactive and non-interactive learning scenarios, providing theoretical analysis of the proposed approaches [17].\nNone of the earlier work has provided a linear classifier directly applicable to anatomized training data. Such a classifier requires specific theoretical and experimental analysis, because anatomized training data provides additional detail that has the potential to improve learning; but also additional uncertainty that must be dealt with. Furthermore, most of the previous work didn\u2019t justify theoretically why the proposed heuristics let classifiers generalize well. Therefore, this paper studies the following problem: Define a heuristic to train SVCs and SVMs on anatomized data without violating `-diversity while using the sensitive information, with a theoretical guarantee of good generalization under reasonable assumptions."}, {"heading": "3 Definitions and Notations", "text": "The first four definitions restate standard definitions of unprotected data and attribute types.\nDefinition 1. A dataset D is called a person specific dataset for population P if each instance Xi \u2208 D belongs to a unique individual p \u2208 P .\nThe person specific dataset will be called the original training data in this paper. Next, we will give the first type of attributes.\nDefinition 2. A set of attributes are called direct identifying attributes if they let an adversary associate an instance Xi \u2208 D to a unique individual p \u2208 P without any background knowledge.\nDefinition 3. A set of attributes are called quasiidentifying attributes if there is background knowledge available to the adversary that associates the quasiidentifying attributes with a unique individual p \u2208 P .\nWe include both direct and quasi-identifying attributes under the name identifying attribute. First name, last name and social security number (SSN) are common examples of direct identifying attributes. Some common examples of quasi-identifying attributes are age, postal code, and occupation. Next, we will give the second type of attribute.\nDefinition 4. An attribute of instance Xi \u2208 D is called a sensitive attribute if we should protect against\nadversaries correctly inferring the value for an individual.\nPatient disease and individual income are common examples of sensitive attributes. Unique individuals p \u2208 P typically don\u2019t want these sensitive information to be revealed to individuals without a direct need to know that information. Provided an instance Xi \u2208 D, the class label is denoted by Xi.C. We don\u2019t consider the case where C is sensitive, as this would make the purpose of classification to violate privacy. C is neither sensitive nor identifying in this paper, although our analysis holds for C being an identifying attribute.\nGiven the former definitions, we will next define the anonymized training data following the definition of kanonymity [28].\nDefinition 5. A training dataset D that satisfies the following conditions is said to be anonymized training data Dk [28]:\n1. The training data Dk does not contain any unique identifying attributes.\n2. Every instance Xi \u2208 Dk is indistinguishable from at least (k \u2212 1) other instances in Dk with respect to its quasi-identifying attributes.\nAnatomy satisfies a slightly weaker definition; the indistinguishability applies only to sensitive data. This will be captured in Definitions 8-15.\nIn this paper, we assume that the anonymized training data Dk is created according to a generalization based data publishing method. We next define the comparison classifiers.\nDefinition 6. A linear support vector classifier (SVC) that is trained on the anonymized training data Dk is called the anonymized SVC. Similarly, a support vector machine (SVM) that is trained on the anonymized training data Dk is called the anonymized SVM.\nDefinition 7. A linear support vector classifier (SVC) that is trained on the original training data D is called the original SVC. Similarly, a support vector machine (SVM) that is trained on the original training data D is called the original SVM.\nThe theoretical aspects of comparison classifiers are out of the scope of this paper. We will remind the theoretical analysis of the original SVC and SVM classifiers in the end of this section [29].\nWe go further from Definition 5, requiring that there must be multiple possible sensitive values that could be linked to an individual. The proposed\nalgorithms will be centered around the following definitions. This new requirement uses the definition of groups [23].\nDefinition 8. A group Gj is a subset of instances in original training data D such that D = \u222amj=1Gj, and for any pair (Gj1 , Gj2) where 1 \u2264 j1 6= j2 \u2264 m, Gj1 \u2229Gj2 = \u2205.\nNext, we define the concept of `-diversity or `- diverse (multiple possible sensitive values) for all the groups in the original training data D.\nDefinition 9. A set of groups is said to be `-diverse if and only if for all groups Gj \u2200v \u2208 \u03a0As(Gj), freq(v,Gj)|Gj | \u2264 1 ` where As is the sensitive attribute in D, \u03a0As(\u2217) is the database As projection operation on original training data \u2217 (or on data table in the database community), freq(v,Gj) is the frequency of v in Gj and |Gj | is the number of instances in Gj.\nWe extend the data publishing method anatomization that is originally based on `-diverse groups by Xiao et al. [33].\nDefinition 10. Given an original training data D partitioned in m `-diverse groups according to Definition 9, anatomization produces an identifying table IT and a sensitive table ST as follows. IT has schema\n(C,A1, ..., Ad, GID)\nincluding the class attribute, the quasi-identifying attributes Ai \u2208 IT for 1 \u2264 i \u2264 d, and the group id GID of the group Gj. For each group Gj \u2208 D and each instance Xi \u2208 Gj, IT has an instance Xi of the form:\n(Xi.C,Xi.A1, ..., Xi.Ad, j)\nST has schema (GID,As)\nwhere As is the sensitive attribute in D and GID is the group id of the group Gj. For each group Gj \u2208 D and each instance Xi \u2208 Gj, ST has an instance of the form:\n(j,Xi.As)\nThe IT table includes only the quasi-identifying and class attributes. We assume that direct identifying attributes are removed before creating the IT and ST tables. We have the following observation from Definition 10 to train a classifier: every instance Xi \u2208 IT can be matched to ` instances Xj \u2208 ST using the common attribute GID in both data table. This observation yields the anatomized training data.\nDefinition 11. Given two data tables IT and ST resulting from the anatomization on original training data D, the anatomized training data DA is\nDA = \u03a0IT.A1,\u00b7\u00b7\u00b7IT.Ad,ST.As( IT 1 ST )\nwhere 1 is the database inner join operation with respect to the condition IT.GID = ST.GID and \u03a0(\u2217) is the database projection operation on training data *.\nAnatomized training data shows one of the most na\u0308\u0131ve data preprocessing approaches. Another one is ignoring the sensitive attribute in ST table.\nDefinition 12. Given two data tables IT and ST resulting from the anatomization on original training data D, the identifying training data Did is\nDid = \u03a0IT.A1,\u00b7\u00b7\u00b7IT.Ad( IT )\nwhere \u03a0(\u2217) is the database projection operation on training data *.\nThe na\u0308\u0131ve training method of Defintion 11 is both costly (a factor of ` increase in size) and noisy: for every true instance, there are ` \u2212 1 incorrect instances that may not be linearly separable. Ignoring the sensitive data, on the other hand, does not use all the information available in the published data (and would likely lead to users insisting on having the original data.) A smarter preprocessing algorithm would eliminate ` \u2212 1 instances within each group such that the training data becomes separable having good generalization (with or without soft margin). This gives the definition of our proposition: pruned training data.\nDefinition 13. Given two data tables IT and ST resulting from anatomization of the original training data D, the pruned training data DP is\nDP = \u03a0IT.A1,\u00b7\u00b7\u00b7IT.Ad,ST.As(\u03c3G( IT, ST ))\nwhere \u03c3G(IT, ST ) is a pruning mechanism eliminating `\u2212 1 instances for all groups G in the IT/ST pair that are unlikely to be separable (cf. Section 4), and \u03a0(\u2217) is the database projection operation on training data *.\nDefinition 14. A linear support vector classifier (SVC) that is trained on the identifying training data Did is called the identifying SVC. Similarly, a support vector machine (SVM) that is trained on the identifying training data Did is called the identifying SVM.\nDefinition 15. A linear support vector classifier (SVC) that is trained on the pruned training data DP is called the pruned SVC. Similarly, a support vector machine (SVM) that is trained on the pruned training data DP is called the pruned SVM.\nNow, we are giving the notations of this paper. Xi will denote a training instance in the original training data D and pruned training data DP interchangeably. N will be the total number of instances in D and DP . X will be a random variable vector in D and DP interchangeably. D \u2282 Rd+1 and DP \u2282 Rd+1 will hold in Euclidean space (see Appendix for practical issues). y will be the binary class label with values {\u22121, 1}. f(X) = wX + b will be a linear classifier such that w \u2208 Rd+1 and b \u2208 R. F is the functional space (3.1) {f : Rd+1 \u2192 {0, 1} : f(X) = wX + b, b \u2208 R, w \u2208 Rd+1}.\nWe will use f instead of f(X) for shorthand in subsequent parts of this paper. The risk of a linear classifier f is R(f) in (3.2).\n(3.2) R(f) = \u222b |(y \u2212 f(X))|p(X, y)dXdy\nIn (3.2), p(X, y) is the joint probability density of training instances X with class label y. The empirical risk of classifier f is R\u0302N (f) in (3.3).\n(3.3) R\u0302N (f) = 1\nN\nN\u2211\ni=1\nI(y 6= f(Xi))\nIn (3.3), N is the number of training instances and I(\u2217) is the indicator function. The linear classifier f is an empirical risk minimizer such that f\u0302N = argmin\nf\u2208F R\u0302N (f).\nGiven the empirical risk minimizer f\u0302N is the SVC with the largest margin, bound (3.4) holds\n(3.4) E[R(f\u0302N )] \u2264 E[(R||w||)2]\nN\nwhen the training data is linearly separable [5]. In (3.4), R stands for the radius of the sphere that the shatterable instances lie on and w stands for the weight vector of hyperplane f(X) in (3.1). For the same SVC, the generalization ability is defined in (3.5) according to VC theory [29, 5]. (3.5)\nE[R(f\u0302N )]\u2212 inf f\u2208F\nR(f) \u2264 4 \u221a (d+ 2)log(N + 1) + log2\nN\nIn (3.5), inf f\u2208F R(f) is the minimum possible risk for the SVC f . Next, we define our pruning mechanism for the anatomization."}, {"heading": "4 Pruning Mechanism for Anatomization", "text": "4.1 Algorithm We will explain our algorithm (\u03c3G in Definition 13) through the example in Figure 1. The\ncurious reader should visit Figures B.1 and B.2 in the appendix to see the pseudo code and the complexity. Although the example is for any linear classifier (hyperplane), the pruning mechanism is valid for SVC and SVM. We later define the generalization ability of pruned SVC/SVM (cf. Definition 15).\nFigure 1a shows the original training data with six instances: two instances of a blue class (on the left side) and four of a red class (on the right side), with two attributes A1 and A2. Here, every instance has a different shape and filling combination since they are unique. Figure 1b shows the anatomized training data with 12 instances created from pairs IT (A1, GID) and ST (GID,A2) when ` = 2 (cf. Definitions 10 and 11).\nA typical training procedure would be the subtraction of mean from attributes A1 and A2 in the original training data, and solving an objective function of a perceptron or SVC (cf. Figure 2). In Figure 2, the original training data is linearly separable and the instances which are closest to the separating hyperplane lie on the surface of the circle 1. This circle is the key point of linear classification, because the original training data is guaranteed to be linearly separable if the instances that\n1The discussion can be generalized to sphere for 3 or larger dimensions. See Burges [5] and Vapnik [29] for general discussion.\nare closest to the decision boundary lie on the surface of a circle [5]. This observation let us define two steps of the pruning mechanism algorithm:\n1. Prerequisite Step: Estimate the circle of shatterable instances from the anatomized training data (Algorithm in Figure B.1).\n2. Pruning Step: For every group in the anatomized training data, pick an instance that is closest to the surface of the estimated circle of shatterable instances (Algorithm in Figure B.2).\nFigure 3 show the range of radiuses for all possible circles of shatterable instances in the prerequisite step. The radius of the original training data must be between the norms of the pair of instances that are closest to (rAmin in Figure 3) and farthest from (rAmax in Figure 3), the origin. Under the random worlds assumption [33], the prerequisite step assumes that (rAmin, rAmax) has uniform distribution and therefore estimates the expected radius E[r] with rAmin+rAmax2 (dashed green line in Figure 3).\nUsing the estimated radius from the prerequisite step, the pruning step creates the pruned training data in Figure 4. Figure 4 also has the hyperplane that is\ntrained from the pruned training data. Although the shatterable instances of the pruned training data (cf. Figure 4) are the same as the shatterable instances of the original training data (cf. Figure 2), other instances are different. The purpose of the pruning step is to find a linearly separable case instead of distribution reconstruction.\nThere are two remaining issues to address. First is the application of the pruning algorithm even if the anatomized training data is linearly separable (cf. Figure 1b). Even though the anatomized training data is linearly separable in this case, it is not always guaranteed. The instances within each group are not linearly independent from the other `\u2212 1 instances and the shattering property is damaged [5]. The second issue is non-separable original and anatomized training data. If the training data is not linearly separable in the original (d + 1) dimensional space, the right approach would be projecting it into higher dimensional space, apply the pruning algorithm in the projected space and hope for the best with a soft margin classifier.\n4.2 Privacy Preservation The preprocessing and pruning steps preserve the `-diversity condition of anatomization. The algorithm doesn\u2019t estimate the correct matchings between the identifying and the sensitive tables. Instead, it makes a random guess within each group which is expected to give some linearly separable training data. It is possible that the original training data isn\u2019t linearly separable or even is a random set of instances without any pattern (see Section 5).\n4.3 Generalization Error of Pruned SVC We will now give the upper bound on the generalization error of the pruned SVC (cf. Definition 15).\nTheorem 4.1. Let N be the number of instances, d be the number of identifying attributes and d + 1 be the total number of attributes in the original and the pruned training data. Let R be the radius of sphere containing\nthe shatterable instances of the original training data D and w be the weights of the linear hyperplane resulting from linear SV classifier trained on the original training data D. Let Rp and wp be the symmetric notations for a linear SV classifier trained on the pruned training data Dp. Assume that all the training instances are located in an Euclidean space Rd+1. Let || \u2217 || be the Euclidean norm of vector \u2217. Let r2 be (R||w||)2, r2p be (RP ||wp||)2, [r2p]min be min{r2p} > 0 and [r2p]max be max{r2p} < \u221e. Let R\u0302N (f) be the empirical risk of on the original training data D and R\u0302Np(f) be the empirical risk on the pruned training data. Let F be the functional space defining the set of possible linear SV classifiers on the original training data D and Fp be the functional space of possible linear SV classifiers on the pruned training data DP . Let f\u0302N be the empirical risk minimizer such that f\u0302N = argmin\nf\u2208F R\u0302N (f) and f\u0302Np be the empirical risk\nminimizer such that f\u0302Np = argmin f\u2208Fp R\u0302Np(f) Last, let inf f\u2208F R(f) be the lowest value of the risk of the linear SV classifier f that could be analytically calculated. Then, the expected risk E[R(f\u0302Np)] of f\u0302Np converges to inf\nf\u2208F R(f)\nunder the upper bound\nE[R(f\u0302Np)]\u2212 inf f\u2208F\nR(f) \u2264 4 \u221a (d+ 2)log(N + 1) + log2\nN\n+ [r2p]max \u2212 [r2p]min\nN\n(4.6)\nusing only DP .\nThe proof of Theorem 4.1 is provided in Appendix Section A. The upper bound (4.6) is defined as the function of two terms where the second term is the result of using pruned training data. The former upper bound shows that pruned SVC can be as accurate as the original SVC under two conditions: 1) Very large training data size (N \u2192 \u221e) 2) Small size of sensitive attribute domain or low ` value or both ([r2p]max \u2212 [r2p]min \u2192 0).\nTheorem 4.1 holds when the pruned training data is mapped into a higher dimensional space d\u2032 using kernel trick. Although the generalization ability of SVMs with RBF kernel is not formally defined (invalid Theorem 4.1), SVMs with RBF kernel are expected to work under the conditions of Theorem 4.1 in the infinite dimensional space [29, 5]."}, {"heading": "5 Experiments", "text": "5.1 Prerequisites\n5.1.1 Datasets We tested our algorithm on the adult, IPUMS and marketing datasets of the UCI data repository [4] and the fatality dataset of Keel data repository [3]:\n1. Adult: Adult dataset is drawn from 1994 census data of the United States [4]. It is composed of 45222 instances after the removal of instances with missing values. The binary classification task is to predict whether a person\u2019s adjusted gross income is \u2264 50K or > 50K. The attribute \u201cfinal weight\u201d is ignored. Last, education was treated as sensitive attribute in the experiments.\n2. IPUMS: This data is drawn from the 1970, 1980 and 1990 census data of the Los Angeles and Long Beach areas [4]. It has 233584 instances in total. We picked the 10 attributes that are included in the adult data. The binary classification task is to predict whether a person\u2019s total income is \u2264 50K or > 50K. The classifiers are expected to show a different behavior from the former adult data since the population (and to some extent, classification task, as it is total income rather than adjusted gross income) are different. Last, education was treated as sensitive attribute in the experiments.\nThe additional information is provided in the appendix for marketing and fatality datasets. Weka was used for attribute selection and discretization if needed [30].\n5.1.2 Privacy Setup The anatomization was done according to Xiao et al.\u2019s bucketization algorithm [33]. When `-diversity condition is not satisfied, the instances were divided into groups of size ` according to the original bucketization algorithm. Leftover instances were suppressed (not used in training models).\nAnonymized training data was created for the adult dataset. We used Inan et al.\u2019s value generalization hierarchies in the experiments. The privacy parameters were k = ` for k-anonymity and `-diversity to compare the classifiers using same group sizes in training data.\nAnonymized and anatomized training data had the same identifying and sensitive attributes. The sensitive attributes were chosen such that the `-diversity is satisfied for at least ` = 2.\n5.1.3 Model Evaluation Setup LibSVM version 3.21 was used for the support vector classification [6]. We will train the support vector machine with linear (SVC) and RBF kernels (SVM).\n10-fold cross validation was used for evaluation. The comparison includes pruned SVC/SVM, original SVC/SVM and identifying SVC/SVM. The comparison on adult dataset also include anonymized SVC/SVM. The anonymized SVC/SVM are not included for other datasets since Inan et al. provided generalization hierarchies only in the adult dataset [15]. Last, the error rates of pruned and original SVC/SVM are compared using the Student t-test (See Appendix). Other models are not included, because Theorem 4.1 covers only pruned and original SVC/SVM.\n5.2 Analysis of Results Figures 5, 6 (see above) and C.1 through C.8 (see Appendix) show the boxplots of error rates for SVC and SVM. In all Figures, \u201cOrg.\u201d and \u201cId.\u201d labels will stand for original SVC/SVM and identifying SVC/SVM respectively. The pruned and anonymized SVC/SVM will be represented by their respective privacy parameters (L for ` and k for k.) This section will include the discussion of results in Figures 5 and 6. (See Appendix for analysis of other results.) The analysis have three observation aspects.\nThe first is the comparison between the pruned and the original SVC/SVM. From Theorem 4.1, we expect that the average error rate of the pruned SVC/SVM will be higher than the average error rate of the original SVC/SVM because bound (4.6) has the additional second term at the right-hand side (cf. Section 4.3). Increasing the ` parameter would result in the increase of the average error rate if the training data size remains same between multiple ` values (no suppression). The rate of the error rate increase in function of ` is theoretically hard to estimate since the assignments of sensitive attributes to each group will be random throughout the bucketization algorithm [33]. Figure 6 show the expected behaviour for the pruned SVC. The average error rates in Figure 5 show a surprising result. We observe that the pruned SVC outperforms the original SVC for multiple ` values. Moreover, the average error rate decreases when ` is increased. Here, the assumption of Theorem 4.1 is violated because of suppression for ` \u2265 4. The bound 4.6 thus does not hold and the result is statistically insignificant (see Appendix.)\nThe second aspect is the comparison between pruned and identifying SVC/SVM. In the first case, the identifying SVC/SVM are likely to outperform the pruned SVC/SVM if the sensitive attribute is a bad predictor of the class attribute in the original training data. The sensitive attribute damages the shattering property of the instances in the original training data and the instances near the decision boundary are not on the surface of a sphere. The pruned SVC/SVM estimates a model of the original training data that is not likely to generalize well. Figure 5 shows a bad predictor case, because the average error rates of the identifying SVC is less than the average error rates of the pruned SVC when ` is 2 to 4. ` = 5 is a special case where the pruning algorithm and `-diversity show the regularization effect to reduce the bias of the underfitting original SVC. In the second case, the pruned SVC/SVM are likely to outperform the identifying SVC/SVM if the sensitive attribute is a good predictor of the class attribute in the original training data. The symmetric shattering argument implicitly holds here. Figure 6 shows a special case of a good predictor. The pruned SVC outperforms the identifying SVC only for ` = 2. For ` values 3 to 5, the pruning algorithm and `-diversity act like a poorly tuned regularizer that cause overfitting. Unfortunately, we cannot know whether the sensitive attribute is a good or bad predictor. Knowing such a behavior would indicate the prediction of the sensitive attribute, a defacto violation of `-diversity.\nThe third aspect is the comparison between the pruned and the anonymized SVC/SVM. Figure 5\nshow the anonymized SVC/SVM in addition to the anatomized and original SVC/SVM. The anatomized SVC/SVM are expected to outperform the anonymized SVC/SVM because anatomization preserves the original values for all the attributes. The generalization based k-anonymity, on the other hand, distorts most of the original attribute values [15]. In Figure 5, the average error rate of the pruned SVC is less than the anonymized SVC\u2019s when ` is 3 to 5. These results show the advantage of anatomization versus generalizationbased k-anonymity. Anatomization has high data utility while the sensitive attribute has a strong privacy guarantee, unlike generalization-based k-anonymity."}, {"heading": "6 Conclusion and Future Directions", "text": "We proposed a preprocessing algorithm for anatomization. Our algorithm estimates a linearly separable training data from the anatomized training data. We defined the generalization ability of support vector classifiers when they are trained on the former preprocessed data. The key point to remember is that our algorithm gives good generalization guarantees to support vector classifiers. The proposed mechanism is evaluated on multiple publicly available datasets and accurate models were observed in most cases while `-diversity is preserved.\nThere are multiple future directions for this work. First is the development of other classification or clustering algorithms for anatomization. Second is the extension of current work to the k-anonymity or generalization based `-diversity. Considering multiple sensitive attributes is also another direction."}, {"heading": "A Proof of Generalization Theorem", "text": "Theorem A.1. (4.1 in the main paper) Let N be the number of instances, d be the number of identifying attributes and d + 1 be the total number of attributes in the original and pruned training data. Let R be the radius of sphere containing the shatterable instances of the original training data D and w be the weights of the linear hyperplane resulting from linear SV classifier trained on the original training data D. Let Rp and wp be the symmetric notations for a linear SV classifier trained on the pruned training data Dp. Assume that all the training instances are located in an Euclidean space Rd+1. Let || \u2217 || be the Euclidean norm of vector \u2217. Let r2 be (R||w||)2, r2p be (RP ||wp||)2, [r2p]min be min{r2p} > 0 and [r2p]max be max{r2p} <\u221e. Let R\u0302N (f) be the empirical risk of on the original training data D and R\u0302Np(f) be the empirical risk on the pruned training data. Let F be the functional space defining the set of possible linear SV classifiers on the original training data D and Fp be the functional space of possible linear SV classifiers on the pruned training data DP . Let f\u0302N be the empirical risk minimizer such that f\u0302N = argmin\nf\u2208F R\u0302N (f) and f\u0302Np be the empirical risk minimizer\nsuch that f\u0302Np = argmin f\u2208Fp R\u0302Np(f) Last, let inf f\u2208F R(f) be the lowest value of the risk of the linear SV classifier f that could be analytically calculated. Then, the expected risk E[R(f\u0302Np)] of f\u0302Np converges to inf\nf\u2208F R(f) under the\nupper bound\nE[R(f\u0302Np)]\u2212 inf f\u2208F\nR(f) \u2264 4 \u221a (d+ 2)log(N + 1) + log2\nN\n+ [r2p]max \u2212 [r2p]min\nN\n(A.1)\nusing only DP .\nProof. From 3.4, we have\n(A.2) E[R(f\u0302Np)] \u2264 E[r2p]\nN\nLet > 0 be the small change on r2 caused by DP such\nthat r2p = r 2 \u00b1 holds. Using A.2, we have\nE[R(f\u0302Np)] \u2264 E[r2p]\nN\n= E[r2]\nN \u00b1 N\n(A.3)\nFrom 3.4, we also have R(f\u0302N ) \u2264 E[r 2]\nN . Using this in the second line of A.3 results in\n(A.4) E[R(f\u0302Np)] \u2264 max{R(f\u0302N )} \u00b1\nN\nSubtracting inf f\u2208F R(f) from both sides of A.4 gives A.5. (A.5) E[R(f\u0302Np)]\u2212 inf f\u2208F R(f) \u2264 max{R(f\u0302N )} \u2212 inf f\u2208F R(f)\u00b1 N\nUsing 3.5 in the right-hand side of A.5 and considering the worst case of r2p = r 2 + result in A.6\nE[R(f\u0302Np)]\u2212 inf f\u2208F\nR(f) \u2264 4 \u221a (d+ 2)log(N + 1) + log2\nN\n+\nN\n(A.6)\nSince both r2 and r2p is expected to exist in the interval ([r2p]min, [r 2 p]max) according to the algorithm in Figure B.2 and the definition of maximum margin in the linear SV classifier [5], 0 \u2264 \u2264 [r2p]max \u2212 [r2p]min holds. Using \u2264 [r2p]max\u2212 [r2p]min in the right-hand side of A.6 gives A.7.\nE[R(f\u0302Np)]\u2212 inf f\u2208F\nR(f) \u2264 4 \u221a (d+ 2)log(N + 1) + log2\nN\n+ [r2p]max \u2212 [r2p]min\nN\n(A.7)\nThis concludes the proof of Theorem A.1."}, {"heading": "B Pruning Mechanism Algorithm", "text": "Figures B.1 and B.2 give the pseudocodes of two steps that are mentioned in Section 4.1.\npruneTrainingData (\ud835\udc37\ud835\udc34):\nIn the pseudocodes, the paramater DA signifies the augmented anatomized training data. The augmentation here includes three points:\n1. After the inner join operation between the IT and ST tables (cf. Definition 11), the instances are sorted with respect to the attribute GID; and then the attributes IT.GID and ST.GID are dropped.\n2. The mean of every numeric and non-numeric ordinal attribute is substracted in the augmented anatomized training data. If the attribute Ai is nonnumeric ordinal, we replaced the non-numeric values with integer values 1 to |dom(Ai)| according\nto domain-wise order and set mode of the discrete values to be mean.\n3. If the attribute Ai is non-numeric nominal, we replaced the mode of Ai with integer 0 and the rest of Ai values with integer 1.\nNote that the pseudocodes use the squared norm instead of the norm itself, because Theorem A.1 defines the generalization error upper bound with the squared radius of sphere containing the shatterable instances of the original training data D.\nThe complexity of the algorithm in Figure B.2 is O(N(d+2)). Note that although there are 2 while loops in the algorithm, every instance is visited once and the algorithm doesn\u2019t go through d + 1 attributes due to listsqNorm which makes the execution time of pruning O(N). The prerequisites algorithm need to visit every instance and dimension which makes the execution time O(N(d+1)). So the total execution time is O(N(d+2)). All the groups (G) and the total number of instances within each group (Gj) of the anatomized training data are assumed to be known. In case it is not known, the grouping information can be computed using an inner join operation and group by query on IT and ST tables. Such a nested operation is easily implemented in O(N logN) execution time."}, {"heading": "C Analysis of Additional Results", "text": "C.1 Additional Datasets There are 2 more datasets to describe.\nMarketing Data: This data is drawn from a phone based marketing campaign of a Portuguese banking institution for long term deposits [4]. We created the following binary classification task which is linearly separable under a soft margin SVC: \u201camong all the people who didn\u2019t submit a long term deposit, predict whether a person has a housing loan or not\u201d. We performed the following preprocessing using Weka filters [30]: 1) pick 39922 instances who didn\u2019t make a long term deposit 2) choose four attributes job, day, month and age using the correlation with the class attribute \u201chousing\u201d. Discretized age is treated as sensitive attribute.\nFatality Data: This data is a U.S. National Center for Statistics and Analysis compilation of 2001 car accidents. The original class attribute has eight labels indicating the level of injury suffered [3]. We created the binary \u201cInjured\u201d and \u201cNo Injury\u201d in the following way: 1) remove the instances with labels \u201cInjured Severity - Unknown\u201d, \u201cDied Prior to Accident\u201d, \u201cUnknown\u201d and \u201cPossible Injury\u2019 from the original data. This results in 91085 instances 2) label \u201cInjured\u201d the instances with labels \u201cNonincapaciting Evident Injury\u201d, \u201cIncapaciting - Injury\u201d and \u201cFatal Injury\u201d. No feature selection is\napplied on this dataset. \u201cPOLICE REPORTED ALCOHOL INVOLVEMENT\u201d was treated as sensitive attribute.\nC.2 Analysis of Additional Results We will analyze more results in this section. In Section 5, three observation aspects were discussed for evaluation.\nTo recall, the first aspect was the comparison between the pruned and original SVC/SVM. From Theorem A.1, we expect that the average error rates of pruned SVC/SVM will be greater then the original SVC/SVM\u2019s if there is no suppression due to `-diversity constraint. One exceptional case would be the regularization effect where `-diversity and pruning algorithm reduces either the bias of underfitting SVC/SVM or the variance of overfitting SVC/SVM. Another exceptional case would be the suppression of many instances of the original training data due to `-diversity constraint. This violates the assumption of Theorem A.1. The second aspect is the comparison between pruned and identify-\ning SVC/SVM. From the shattering properties of the statistical learning theory, the pruned SVC/SVM are expected to outperfom the identifying SVC/SVM if the sensitive attribute is a good predictor of the class attribute. If sensitive attribute is, on the other hand, a bad predictor of the class attribute; the opposite of the former behaviour is expected to occur. Last, the third aspect is the comparison between the pruned and anonymized SVC/SVM. The pruned SVC/SVM are expected to outperform the anonymized SVC/SVM because anatomization preserves the original values for all the attributes. Figures C.1 to C.8 show the results of all the experiments. In all Figures, \u201cOrg.\u201d and \u201cId.\u201d labels will stand for the original SVC/SVM and the identifying SVC/SVM respectively. The pruned and anonymized SVC/SVM will be represented by their respective privacy parameters (L for ` and k for k.)\nFigure C.1 shows a surprising result of pruned SVC in the first and second aspect. In the first aspect, increasing ` reduces the average error rate of the pruned SVC so `-diversity and pruning algorithm regularize the underfitting original SVC. Theorem A.1 does not hold as well for ` \u2265 4, because some original training data instances are suppressed. In the second aspect, the sensitive attribute is a bad predictor of the class attribute. Identifying SVC performs better than many pruned SVCs. In the third aspect, the average error rate of pruned SVC is less than the average error rate of anonymized SVC for all ` = k values (See Section 5.2).\nFigure C.2 shows the expected results of pruned SVM in all three aspects. Increasing ` result in the increase of average error rate for pruned SVM and original SVM outperforms the pruned SVM. The expectation from Theorem A.1 occurs here despite suppression (violation of assumption). We believe that `-diversity and pruning algorithm act as regularizer for SVMs with RBF kernel which tend to overfit to the training data. Notice that the sensitive attribute is a good predictor of the class attribute in the infinite dimensional space since average error rate of pruned SVM is less than the average error rate of identifying SVM. Last, the average error rate of the pruned SVM is less than the anonymized SVM\u2019s one by 0.1.\nFigure C.3 shows in general the expected result of pruned SVC in the first aspect. ` = 4 is a special case where its average error rate is greater than the pruned SVC\u2019s that is trained on 5-diverse data. Theorem A.1\u2019s assumption is violated again when ` = 5 because some original training data instances are suppressed. In the second aspect, the pruned SVC cannot capture the good shattering property that the sensitive attribute provide in the original dimensional space.\nFigure C.4 shows in general the expected result of pruned SVM for first and second aspects. In the first aspect, the pruned SVM outperforms the original SVM when ` = 2. This shows that the pruning algorithm and `-diversity has the regularization effect even if the sensitive attribute is a good predictor according to the second aspect. The regularization case could occur in general, because it is statistically significant for confidence interval 0.95 (See Table 2).\nFigure C.5 show the expected behaviour of pruned SVC in the first aspect. One thing to emphasize is the surprising spike in the error rate distribution when ` = 5. The reason is that the original training data satisfies the `-diversity condition when ` = 2 and ` = 3. When ` = 5, almost half of the training instances are suppressed. This strongly violates the assumption of Theorem A.1 and the result is also not statistically significant (cf. Table 1). We should note that the sensitive attribute is a bad predictor since the average error rates of pruned SVC are greater than the identifying SVC\u2019s.\nFigure C.6 show the expected behaviour of pruned SVC in the first aspect. The pruned SVC also show the expected result in the second aspect. The sensitive attribute is not a good predictor in the infinite dimensional space.\nIn Figure C.7, the pruned SVC gives an interesting and surprising result in the first aspect. The average error rate of pruned SVC is approximately same as the average error rate of original SVC for all ` values. We believe that this would only occur in this dataset because the results are not statistically significant (cf. Table 1.) When ` = 3 and ` = 4, the assumption in Theorem A.1 is violated because most of the training instances are suppressed. In the second aspect, sensitive attribute is bad (insignificant) predictor since the pruned SVC thus does not reduce the error rate of the identifying SVC.\nFigure C.8 show the expected results of pruned SVM in the first two aspects (despite violating the assumption of Theorem A.1). In the second aspect, note that the sensitive attribute is a bad predictor in\nthe inifinite dimensional space. The average error rate of pruned SVM is greater than the identifying SVM\u2019s.\nC.3 Student t-test for Pruned SVC/SVM versus Original SVC/SVM Tables 1 and 2 give the statistical test results for confidence interval 0.95. In all Tables, \u201cP\u201d stands for pass while \u201cF\u201d stands for fail. \u201cN/A\u201d stands for not applicable in cases where the domain size of sensitive attribute is less than the ` value. \u201cOrg.\u201d stand for the original SVC/SVM whereas \u201c`\u201d stand for the pruned SVC/SVM. Note that we do the test for original SVC/SVM vs pruned SVC/SVM, because the Theorem A.1\u2019s scope covers this analysis.\nIn Section C.2, we saw the theoretically expected results for pruned SVC vs original SVC when they are trained on IPUMS dataset (cf. Figure C.3). Table 1 shows that the difference between the pruned and original SVC is statistically significant for almost all ` values. We saw, in contrary, theoretically unexpected results in case of pruned SVC vs. original SVC on adult and fatality datasets. Table 1 shows that the difference between\nthe pruned and original SVC are statistically insignificant in adult and fatality datasets. So, the theoretically unexpected results are likely to occur by just random chance and it doesn\u2019t make Theorem A.1 invalid. In marketing dataset, the difference between the pruned and the original SVC is statistically insignificant for ` values 3-to-5, because most of the training instances were suppressed. Note that Theorem A.1 is valid if and only if both the pruned and the original training dataset have the same number of instances (no or negligeable suppression.) (See Theorem A.1)\nTable 2 shows that the difference between pruned SVM and original SVM are statistically significant in almost all datasets for multiple ` values. In Section C.2, the expectation from Theorem A.1 occured in all the datasets. (cf. Figures C.2, C.4, C.6 and C.8) This good result are therefore very unlikely to occur by random chance and Theorem A.1 is valid in infinite dimensions. This result is in parallel to Vapnik and Burges\u2019 claim for the original SVM when there is no suppression [29, 5]. Last, we observe surprisingly significant results when the assumption of Theorem A.1 is violated. We believe that the `-diversity and pruning acts as a regularizer since SVMs with RBF kernel tend to overfit to the training data.\nIn summary, we measured the statistically significant error rates when the pruned SVC/SVM show the expectation from Theorem A.1. The error rates were statistically insignificant when they don\u2019t respect the expected result of Theorem A.1 or when the pruned training data violates the assumption of Theorem A.1."}], "references": [{"title": "On the design and quantification of privacy preserving data mining algorithms", "author": ["D. Agrawal", "C.C. Aggarwal"], "venue": "Proceedings of the Twentieth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, Santa Barbara,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Privacy-preserving data mining", "author": ["R. Agrawal", "R. Srikant"], "venue": "Proceedings of the 2000 ACM SIG- MOD Conference on Management of Data, Dallas,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Keel datamining software tool: Data set repository", "author": ["J. Alcal\u00e1", "A. Fern\u00e1ndez", "J. Luengo", "J. Derrac", "S. Gar\u0107\u0131a", "L. S\u00e1nchez", "F. Herrera"], "venue": "integration of algorithms and experimental analysis framework, Journal of Multiple-Valued Logic and Soft Computing, 17 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining fragmentation and encryption to protect privacy in data storage", "author": ["V. Ciriani", "S.D.C.D. Vimercati", "S. Foresti", "S. Jajodia", "S. Paraboschi", "P. Samarati"], "venue": "ACM Trans. Inf. Syst. Secur., 13 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimizing minimality and maximizing utility: Analyzing method-based attacks on anonymized data", "author": ["G. Cormode", "N. Li", "T. Li", "D. Srivastava"], "venue": "Proceedings of the VLDB Endowment, vol. 3", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Extending loose associations to multiple fragments, in DBSec\u201913", "author": ["S.D.C. di Vimercati", "S. Foresti", "S. Jajodia", "G. Livraga", "S. Paraboschi", "P. Samarati"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Privacy-preserving decision tree mining based on random substitutions", "author": ["J. Dowd", "S. Xu", "W. Zhang"], "venue": "tech. report, In International Conference on Emerging Trends in Information and Communication Security", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "33rd International Colloquium on Automata, Languages and Programming ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Limiting privacy breaches in privacy preserving data mining", "author": ["A. Evfimievski", "J. Gehrke", "R. Srikant"], "venue": "Proceedings of the 22nd ACM SIGACT-SIGMOD- SIGART Symposium on Principles of Database Systems ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "A privacy protection model for patient data with multiple sensitive attributes", "author": ["T. Gal", "Z. Chen", "A. Gangopadhyay"], "venue": "International Journal of Information Security and Privacy, IGI Global, Hershey, PA, 2 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "and B", "author": ["X. He", "Y. Xiao", "Y. Li", "Q. Wang", "W. Wang"], "venue": "Shi, Permutation anonymization: Improving anatomy for privacy preservation in data publication., in PAKDD Workshops, L. Cao, J. Z. Huang, J. Bailey, Y. S. Koh, and J. Luo, eds., vol. 7104 of Lecture Notes in Computer Science, Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Using anonymized data for classification", "author": ["A. Inan", "M. Kantarcioglu", "E. Bertino"], "venue": "Proceedings of the 2009 IEEE International Conference on Data Engineering, ICDE \u201909, Washington, DC, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Transforming data to satisfy privacy constraints", "author": ["V. Iyengar"], "venue": "Proc., the Eigth ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Differentially private learning with kernels", "author": ["P. Jain", "A. Thakurta"], "venue": "ICML (3),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Attacks on privacy and de finettis theorem", "author": ["D. Kifer"], "venue": "In SIGMOD", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "t-closeness: Privacy beyond k-  anonymity and l-diversity", "author": ["N. Li", "T. Li"], "venue": "Proceedings of the 23nd International Conference on Data Engineering (ICDE \u201907),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "On the tradeoff between privacy and utility in data publishing", "author": ["T. Li", "N. Li"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Slicing: A new approach for privacy preserving data publishing", "author": ["T. Li", "N. Li", "J. Zhang", "I. Molloy"], "venue": "IEEE Trans. Knowl. Data Eng., 24 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Privacy-preserving outsourcing support vector machines with random transformation", "author": ["K.-P. Lin", "M.-S. Chen"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201910, New York, NY, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "l-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "J. Gehrke", "D. Kifer", "M. Venkitasubramaniam"], "venue": "Proceedings of the 22nd IEEE International Conference on Data Engineering ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Controlled data-swapping techniques for masking public use microdata sets, Statistical Research Division Report Series RR 96-04", "author": ["R.A. Moore", "Jr."], "venue": "U.S. Bureau of the Census,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "\u03b4-presence without complete world knowledge", "author": ["M.E. Nergiz", "C. Clifton"], "venue": "22 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for svm learning", "author": ["B.I. Rubinstein", "P.L. Bartlett", "L. Huang", "N. Taft"], "venue": "arXiv preprint arXiv:0911.5708, ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Protecting respondent\u2019s privacy in microdata release", "author": ["P. Samarati"], "venue": "13 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "k-anonymity: a model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal on Uncertainty, Fuzziness and Knowledge-based Systems, ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik", "V. Vapnik"], "venue": "vol. 1, Wiley New York", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "Minimality attack in privacy preserving data publishing", "author": ["R.C.-W. Wong", "A.W.-C. Fu", "K. Wang", "J. Pei"], "venue": "VLDB", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "\u03b1", "author": ["R.C.-W. Wong", "J. Li", "A.W.-C. Fu", "K. Wang"], "venue": "k)-anonymity: An enhanced k-anonymity model for privacy preserving data publishing, in Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, New York, NY, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Anatomy: Simple and effective privacy preservation", "author": ["X. Xiao", "Y. Tao"], "venue": "Proceedings of 32nd International Conference on Very Large Data Bases ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 21, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]).", "startOffset": 112, "endOffset": 120}, {"referenceID": 26, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]).", "startOffset": 112, "endOffset": 120}, {"referenceID": 17, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 30, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (`-diversity[23], kanonymity [27, 28], t-closeness [19], \u03b4-presence [25], (\u03b1,k)-anonymity [32]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 22, "context": "Other alternatives include value swapping [24], distortion [2], randomization [12], and noise addition (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Other alternatives include value swapping [24], distortion [2], randomization [12], and noise addition (e.", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "Other alternatives include value swapping [24], distortion [2], randomization [12], and noise addition (e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": ", differential privacy [11]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "Generalization consists of replacing identifying attribute values with a less specific version [28].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "Suppression can be viewed as the ultimate generalization, replacing the identifying value with an \u201cany\u201d value [28].", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "edu to enforce `-diversity while preserving specific data values [33].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "The more general approach of fragmentation [7] divides a given dataset\u2019s attributes into two sets of attributes (2 partitions) such that an encryption mechanism avoids associations between two different small partitions.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "extend fragmentation to multiple partitions [9], and Tamas et al.", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "propose an extension that deals with multiple sensitive attributes [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "There is concern that anatomization is vulnerable to several attacks [18, 14, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 12, "context": "There is concern that anatomization is vulnerable to several attacks [18, 14, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 19, "context": "There is concern that anatomization is vulnerable to several attacks [18, 14, 21].", "startOffset": 69, "endOffset": 81}, {"referenceID": 18, "context": "While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [20, 11].", "startOffset": 145, "endOffset": 153}, {"referenceID": 9, "context": "While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [20, 11].", "startOffset": 145, "endOffset": 153}, {"referenceID": 29, "context": ", minimality [31, 8].", "startOffset": 13, "endOffset": 20}, {"referenceID": 6, "context": ", minimality [31, 8].", "startOffset": 13, "endOffset": 20}, {"referenceID": 16, "context": "A violating classification task would be the prediction of sensitive attribute, a task that was found to be #P-complete by Kifer [18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "already gives a practical applications of such a learning scenario [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "In empirical analysis, our algorithm will be compared with SVM and SVC that are trained on either unprotected data or generalized data (under k-anonymity [15]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 27, "context": "The analysis will be justified with the statistical learning theory [29, 5]", "startOffset": 68, "endOffset": 75}, {"referenceID": 3, "context": "The analysis will be justified with the statistical learning theory [29, 5]", "startOffset": 68, "endOffset": 75}, {"referenceID": 0, "context": "5 decision tree classifier was trained [1].", "startOffset": 39, "endOffset": 42}, {"referenceID": 14, "context": "5 decision tree classifier was trained from the optimally generalized training data [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "5 decision tree classifier was learned [10].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Taylor approximation was used to estimate the linear and RBF kernel computation from generalized data[15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "They analyze finite and infinite dimensional kernels in function of the approximation error under differential privacy [26].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Random transformation is applied on the training set so that the cloud server computes the accurate model without knowing what the actual values are [22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "They propose differentially private mechanisms to train support vector machines for interactive, semi-interactive and non-interactive learning scenarios, providing theoretical analysis of the proposed approaches [17].", "startOffset": 212, "endOffset": 216}, {"referenceID": 26, "context": "Given the former definitions, we will next define the anonymized training data following the definition of kanonymity [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "A training dataset D that satisfies the following conditions is said to be anonymized training data Dk [28]:", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "We will remind the theoretical analysis of the original SVC and SVM classifiers in the end of this section [29].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "This new requirement uses the definition of groups [23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 31, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "when the training data is linearly separable [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 27, "context": "5) according to VC theory [29, 5].", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": "5) according to VC theory [29, 5].", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": "See Burges [5] and Vapnik [29] for general discussion.", "startOffset": 11, "endOffset": 14}, {"referenceID": 27, "context": "See Burges [5] and Vapnik [29] for general discussion.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "are closest to the decision boundary lie on the surface of a circle [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 31, "context": "Under the random worlds assumption [33], the prerequisite step assumes that (rAmin, rAmax) has uniform distribution and therefore estimates the expected radius E[r] with rAmin+rAmax 2 (dashed green line in Figure 3).", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "The instances within each group are not linearly independent from the other `\u2212 1 instances and the shattering property is damaged [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 27, "context": "1 in the infinite dimensional space [29, 5].", "startOffset": 36, "endOffset": 43}, {"referenceID": 3, "context": "1 in the infinite dimensional space [29, 5].", "startOffset": 36, "endOffset": 43}, {"referenceID": 2, "context": "1 Datasets We tested our algorithm on the adult, IPUMS and marketing datasets of the UCI data repository [4] and the fatality dataset of Keel data repository [3]:", "startOffset": 158, "endOffset": 161}, {"referenceID": 28, "context": "Weka was used for attribute selection and discretization if needed [30].", "startOffset": 67, "endOffset": 71}, {"referenceID": 31, "context": "\u2019s bucketization algorithm [33].", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "21 was used for the support vector classification [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 13, "context": "provided generalization hierarchies only in the adult dataset [15].", "startOffset": 62, "endOffset": 66}, {"referenceID": 31, "context": "The rate of the error rate increase in function of ` is theoretically hard to estimate since the assignments of sensitive attributes to each group will be random throughout the bucketization algorithm [33].", "startOffset": 201, "endOffset": 205}, {"referenceID": 13, "context": "The generalization based k-anonymity, on the other hand, distorts most of the original attribute values [15].", "startOffset": 104, "endOffset": 108}], "year": 2016, "abstractText": "Corporations are retaining ever-larger corpuses of personal data; the frequency or breaches and corresponding privacy impact have been rising accordingly. One way to mitigate this risk is through use of anonymized data, limiting the exposure of individual data to only where it is absolutely needed. This would seem particularly appropriate for data mining, where the goal is generalizable knowledge rather than data on specific individuals. In practice, corporate data miners often insist on original data, for fear that they might \u201dmiss something\u201d with anonymized or differentially private approaches. This paper provides a theoretical justification for the use of anonymized data. Specifically, we show that a support vector classifier trained on anatomized data satisfying `-diversity should be expected to do as well as on the original data. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values, thus satisfying `-diversity. The theoretical effectiveness of the proposed approach is validated using several publicly available datasets, showing that we outperform the state of the art for support vector classification using training data protected by k-anonymity, and are comparable to learning on the original data.", "creator": "TeX"}}}