{"id": "1610.06106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Efficiency of active learning for the allocation of workers on crowdsourced classification tasks", "abstract": "crowdsourcing has only been successfully employed locally in practically the past as an effective and cheap way to execute classification aggregation tasks and has therefore attracted by the attention of the research related community. however, we still often lack a theoretical understanding of how to collect simply the observed labels from the crowd in an considered optimal way. in this current paper together we focus on the problem of worker allocation and compare two sufficiently active learning evaluation policies when proposed in shaping the empirical experimental literature with then a uniform allocation ranking of the available social budget. to do this end we only make a thorough mathematical analysis judgment of the problem carefully and will derive a new bound dependence on slowing the performance of the system. furthermore again we help run very extensive physical simulations placed in a more realistic model scenario and show observation that \u2026 our fundamental theoretical results therefore hold sway in practice.", "histories": [["v1", "Wed, 19 Oct 2016 17:03:27 GMT  (181kb,D)", "http://arxiv.org/abs/1610.06106v1", "paper accepted in the CrowdML workshop at NIPS 2016"]], "COMMENTS": "paper accepted in the CrowdML workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["edoardo manino", "long tran-thanh", "nicholas r jennings"], "accepted": false, "id": "1610.06106"}, "pdf": {"name": "1610.06106.pdf", "metadata": {"source": "CRF", "title": "Efficiency of active learning for the allocation of workers on crowdsourced classification tasks", "authors": ["Edoardo Manino", "Long Tran-Thanh"], "emails": ["em4e15@soton.ac.uk", "ltt08r@ecs.soton.ac.uk", "n.jennings@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Crowdsourcing has emerged as an effective way of hiring a temporary workforce from an online community and execute a large number of simple and repetitive data-processing tasks. Despite the unreliability of the single members, as a whole these crowds have been proven capable of producing valuable solutions that would not have been achievable with traditional methods. Our focus in particular is on classification projects, where the employer presents the workers with a series of multiple-choice questions and use their answers to infer the correct label of the tasks. This model has been successfully employed for various purposes including image labeling [Sorokin and Forsyth, 2008], ballot voting in complex workflows and filtering [Parameswaran et al., 2012].\nAs the market and the demand for these kind of solutions expand, there is an increasing interest in improving the efficiency of the crowdsourcing process. In the case where the workers are rewarded with small payments, as in the famous Amazon Mechanical Turk1 platform, this goal is particularly important as it could make crowdsourcing even more attractive as a cheap alternative. One of the main obstacles however is the heterogeneity in the quality of the answers collected from the crowd [Ipeirotis et al., 2010]. As a large number of them may be incorrect, the usual strategy consists in letting several workers execute the same task and then average their answers to come up with a reliable conclusion. For this reason much of the existing research in machine learning techniques for crowdsourcing has thus focussed on how to optimally aggregate the set of labels collected from the crowd and reduce the number of misclassified tasks.\nOur research adresses a related problem. During the crowdsourcing process itself in fact, we need to allocate the available workforce on the individual tasks. However, the decisions we take are likely to influence the final performance of the aggregation method in place. The research question is thus finding which allocation policies guarantee the best results at the end of the crowdsourcing effort. Perhaps surprisingly, this problem has received less attention in the existing literature and while there are some empirical results, little is known about their theoretical properties [Slivkins and Vaughan,\n1mturk.com\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n06 10\n6v 1\n[ cs\n.H C\n] 1\n9 O\nct 2\n01 6\n2013]. In this paper we investigate those properties, though in a simplified setting, and show that they hold even in a more realistic scenario.\nIn next section we introduce our crowdsourcing model and the related works of the existing literature. Then we present our theoretical analysis of active learning policies in an ideal scenario where the quality of each individual worker is known in advance. Finally we show that the empirical performance of active learning does not degrade when this piece of information is not accessible and an inference algorithms is used to estimate it."}, {"heading": "2 Background", "text": "We analyse here a setup where we have to discover the true label `\u02dai of a set of independent tasks i P r1,M s. These labels are binary and we conventionally restrict their values in the set L \u201c t`1,\u00b41u. We further assume that we have a maximum budget B of votes that we can gather from the crowd, and the individual vote `ij of worker j on task i has probability Pp`ij \u201c `\u02dai q \u201c pj of being correct. In other terms we model the accuracy of each worker as a single parameter that is neither affected by the true label of the task she is working on, nor by other factors such fatigue effect or tasks with different degree of difficulty. In this model, experts will have a pj close to one, whereas spammers are associated to a pj \u00ab 1{2. On the other side of the spectrum, we call adversaries the workers with pj \u0103 1.2 as their answers are likely to be incorrect. When the parameters pj of the workers are known, we can optimally aggregate the workers\u2019 labels according to the following rule [Nitzan and Paroush, 1982]:\n\u02c6\u0300 i \u201c sign\n\"\n\u00ff\njPNi\n`ij log \u00b4 pj 1\u00b4 pj \u00af\n*\n(1)\nwhere Ni is the subset of workers who executed task i. Notably, these rule follows directly from the log-ratio of the likelihood of each class and allows us to compute the confidence in our prediction as Pp\u02c6\u0300i \u201c `\u02dai q \u201c expp|zi|q{p1` expp|zi|qq where zi is the argument of the sign function in Equation 1. In a more realistic scenario though, the level of expertise of each individual worker is unknown and thus we need to estimate it. A common idea is to use some machine learning algorithm to infer both the parameters pj and the task classification at the same time in an unsupervised fashion. The first application of this approach dates back to the expectation-maximisation algorithm of Dawid and Skene [1979] but in more recent years many alternative techniques have been proposed [Karger et al., 2011, Zhou et al., 2012]. In this paper we will use the approximate mean-field algorithm proposed by Liu et al. [2012] which consists in iterating between the following two steps until convergence:\nE-step: \u00b5ip`q9 \u017a\njPNi\np\u0302 \u03b4ij j p1\u00b4 p\u0302jq 1\u00b4\u03b4ij (2)\nM-step: p\u0302j \u201c \u0159 iPNj \u00b5ip`ijq ` \u03b1 |Nj | ` \u03b1` \u03b2\n(3)\nwhere \u03b4ij \u201c 1 if `ij \u201c ` and zero otherwise, while \u03b1 and \u03b2 are the parameters of the Beta distribution which is used as the prior on the workers\u2019 skill.\nThese inference techniques are designed to minimise the number of misclassification errors given a set of labels, but they give no indication on how to conduct the label collection process itself. The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process.\nIn contrast, a number of empirical-oriented works on crowdsourcing advocate the use of other policies that adapt the allocation depending on the data collected during the crowdsourcing process. Specifically Welinder and Perona [2010] propose an algorithm that computes the confidence in the classifications after the acquisition of every new label, and allocates more workers on the most uncertain tasks. Simpson and Roberts [2014] address the same problem from a different perspective and introduce a greedy algorithm to maximise the amount of information collected from the crowd.\nNotably these two approaches belong to the family of active learning policies [Settles, 2010], but the authors provide no theoretical guarantee on their performances."}, {"heading": "3 A theoretical analysis of allocation policies", "text": "In this section we analyse the impact that different worker allocation strategies have on the performance of the system. Specifically we are interested in computing the probability Ppp\u0302`qi \u201c `\u02dai q of classifying task i correctly under each of the following policies:\n\u2022 the uniform allocation policy which allocates the same number of workers to each task [Karger et al., 2011];\n\u2022 a greedy policy that maximises the expected information gain of each new incoming label [Simpson and Roberts, 2014];\n\u2022 a simple uncertainty sampling rule that assigns new workers on the task with the least confident label [Lewis and Gale, 1994, Welinder and Perona, 2010].\nTo keep our discussion simple, we model our interaction with the crowd as a sequence of B distinct rounds. In each round a new worker j becomes available and we have to assign her to a task i of our choice. Once the label `ij is acquired, the system moves on to the next round j ` 1. Furthermore we assume the presence of an oracle that at each round reveals the parameter pj of the incoming worker. We will relax this assumption in Section 4."}, {"heading": "3.1 Equivalence between different active learning policies", "text": "We can now adapt the worker allocation policy of Simpson and Roberts [2014] to our setup by choosing the task i that maximises the amount of information collected at each round. To do so we interpret the effect of the next incoming label as a random variable xj \u201c \u02d8 logppj{p1` pjqq which modifies the current posterior probability Pp`\u02dai \u201c `1q \u201c exppziq{pexppziq ` 1q, and measure its information gain as the Kullback-Leibler divergence between the two distributions:\nIpi, xjq \u201c xj exppzi ` xjq exppzi ` xjq ` 1 ` log \u02c6 exppziq ` 1 exppzi ` xjq ` 1 \u02d9\n(4)\nAs the sign of xj is unknown a priori, we need to choose the task i that maximises Equation 4 in expectation, i.e. i\u02da \u201c argmaxipExj pIpi, xjqqq. Fortunately it is possible to perform such computation explicitly by taking into account the current posterior on i and the probability pj of observing the correct label. Moreover it can be shown that the expected information gain is a symmetric function around its maximum in zi \u201c 0 for any logppj{p1 \u00b4 pjqq \u2030 0 (we omit the derivation here for lack of space). As a consequence, we can make the optimal greedy decision using the following rule:\ni\u02da \u201c argmin i p|zi|q (5)\nInterestingly, Equation 5 corresponds to the uncertainty sampling rule, as i\u02da is the task with the least confident label. We have therefore proven that the two active learning policies are actually the same, and as a result we will consider them as a single one in the following discussion."}, {"heading": "3.2 A random walk interpretation of the allocation policies", "text": "From the point of view of a single task i, the active learning policy keeps allocating new workers in short bursts of activity, until i is no more the least confident. In this respect, the evolution of the log-likelihood ratio zi can be interpreted as a one-dimensional random walk with starting point zi \u201c 0. Furthermore, at the end of the crowdsourcing process we can identify a confidence threshold zB P R such that zB \u201c maxxt|zi| \u011b xu for all the task i P r1,M s. The random walk has thus the property of being bounded in that whenever |zi| reaches zB , task i does not receive any additional worker.\nEvery new label collected from the crowd corresponds to step in the random walk. Given the probability distribution fP ppq \u201c Pppj \u201c pq of the workers\u2019 skill, we can derive the distribution of the weights wj \u201c logppj{p1\u00b4 pjqq associated to the vote of each worker as follows:\nfW pzq \u201c exppzq p1` exppzqq2 fP \u00b4 exppzq 1` exppzq \u00af\n(6)\nand then compute the p.d.f. of the labels xj \u201c \u02d8wj received from the crowd by assuming that `\u02dai \u201c `1 and taking into account that Pp`ij \u201c `\u02dai q \u201c exppwjq{p1` exppwjqq:\nfV pzq \u201c exppzq\n1` exppzq pfW pzq ` fW p\u00b4zqq (7)\nan example of which can be found in Figure 1.\nWith a closed form or a numerical representation of fV we can iteratively compute the expected number of steps the random walk needs to reach one of the two boundaries \u02d8zB . Specifically we have to perform the following computation:\nf1Apzq \u201c fV pzq (8) fkBpzq \u201c fkApzqIpz P p\u00b4zB , zBqq (9) fk`1A pzq \u201c pf k B \u02da fV q (10)\nEpraq \u201c 8 \u00ff\nk\u201c1 k \u00b4\n\u017c \u00b4zB\n\u00b48 fkApzqdz `\n\u017c 8\n`zB fkApzqdz\n\u00af\n(11)\nwhere Ip\u201aq is the indicator function and \u02da the convolution operator. In contrast, we can interpret the uniform allocation case as an unbounded random walk where we fix the number of steps ru and compute the probability of a correct classification as follows:\nPp\u02c6\u0300ui \u201c `\u02dai q \u201c \u017c 8\n0` p\u02darufvpzqqdz (12)\nA comparison between Equation 11 and 12 is possible by finding a value of zB such that Epraq \u201c ru. As can be seen in Figure 3 for a population fP \u201e Betap4, 2q, the active learning policy provides greater accuracy except for small values of ru."}, {"heading": "3.2.1 The homogeneous crowd case", "text": "An interesting scenario arises when all the workers have the same parameter pj \u201c p, i.e. the distribution of the crowd is a Dirac delta fP \u201c \u03b4ppq. In this case the labels collected from the crowd have the same weight and thus our aggregation technique simplifies to a majority voting rule. Moreover we can compute a closed form of both Equations 11 and 12, with the only assumption that ru is an odd integer:\nEpraq \u201c\n` 2 exppzBq1`exppzBq \u00b4 1 \u02d8 zB\np2p\u00b4 1q log ` p 1\u00b4p\n\u02d8 (13)\nPp\u02c6\u0300ui \u201c `\u02dai q \u201c ru \u00ff\nr\u201crru{2s prp1\u00b4 pqru\u00b4r (14)\nInterestingly, for any value of ru \u0105 1 and p P p0, 1{2q Y p1{2, 1q, the active learning policy achieves better performances than uniform allocation as is shown in Figure 2."}, {"heading": "3.2.2 Probability bounds on the heterogeneous case", "text": "More in general, we may have only access to the first moments of the distribution fV . If this is the case we can still provide some closed-form bounds on Equation 11 and 12. First of all we need to assume that the distribution fV is bounded in r\u00b4\u03b3, \u03b3s with \u03b3 \u0103 8 or, in other terms, that there are no perfect workers with pj \u201c 1 or pj \u201c 0 in our population. We can then apply the results in Ethier and Khoshnevisan [2002] and derive an upper bound on the expected number of labels required by the active learning policy to reach zB :\nEpraq \u010f 1\nEpfV q\n\u00b4 p2zB ` \u03b3q \u03c1zB`\u03b30 \u00b4 1 \u03c12zB`\u03b30 \u00b4 1 \u00b4 zB \u00af\n(15)\nwhere \u03c10 \u2030 1 is the solution of the equation Ep\u03c1fV q \u201c 1 and can be approximated as follows:\n\u03c10 \u00ab 2EpfV q VarpfV q\n(16)\nwhich does hold as long as fV is close to a normal distribution (see Figure 1 for an example) [Kozec, 1995, Canjar, 2007].\nIn the case of uniform allocation instead, we can observe that zi is the sum of ru identically-distributed independent variables and apply Chernoff\u2019s inequality to bound Equation 12:\nPp\u02c6\u0300ui \u201c `\u02dai q \u010f 1\n1` rupEpfV qq2VarpfV q (17)\nIt must be noted that Equation 17 is not asymptotically tight, but nevertheless provides a better bound for \u201csmall\u201d values of ru compared to other concentration inequalities [Hoeffding, 1963]. A comparison between the bounds of the two policies is shown in Figure 3 where we assumed \u03b3 \u201c 5 a\nVarpfV q: as long as ru \u0105 \u03b3{EpfV q the bound on the active learning policy is quite tight, but some work is still necessary to improve it in the other case."}, {"heading": "4 An empirical comparison between policies", "text": "In this section we study a more realistic scenario where the parameters pj of the workers are unknown and the workers are allowed to execute multiple tasks. These two differences in the setting have the following consequences:\n\u2022 in order to aggregate the set of labels collected from the crowd we need to rely on the output of some inference method, which is inherently less reliable than the prediction of the optimal rule by Nitzan and Paroush [1982];\n\u2022 in general, having the workers execute multiple tasks is beneficial as it improves the accuracy of the estimation of their skills, however it can also interfere with the allocation policy as we cannot assign the same worker j on a particular task i more than once.\nIn order to test the performances of the different policies we had to make a couple of changes in our system with respect to the previous section. First, we selected the approximate mean-field algorithm by Liu et al. [2012] to learn the workers\u2019 parameter pj and predict the classification of the tasks. As this algorithm needs a prior on the distribution fP , we decided to test the most favourable scenario where the prior matches the real distribution. Second, in order to avoid any repetition of a worker-task pair pj, iq we modified the allocation policies as follows:\nUniform allocation: i\u02da \u201c min i:jRNi t|Ni|u (18)\nActive learning: i\u02da \u201c min i:jRNi t|zi|u (19)\nwhere zi is estimated by running one iteration of an online version of the approximate mean-field algorithm after the arrival of each new label.\nIn our experiments we explored the impact of three parameters on the performance of our crowdsourcing system: the number of tasks M , the total budget B and the average number of labels |Nj | provided by each worker. All the results we provide are generated by extracting the workers\u2019 skills from a distribution fP \u201e Betap4, 2q and simulating B rounds of the crowdsourcing process. For each datapoint we show the estimated mean and standard error after 1000 simulations.\nAs far as the number of tasks is concerned, we tested a wide range of values from M \u201c 20 to M \u201c 10000. For this set of experiments the budget was set to B \u201c 10M and the number of labels per worker to |Nj | \u201c 10. As the data in Figure 4 show, the performances of the two allocation policies are similar for all the values of M .\nIn contrast the amount of budget B has a strong impact on the accuracy of the system as a larger number of labels improves the probability of a correct classification. Figure 5 shows the results with M \u201c 1000, |Nj | \u201c 10 and values of B{M ranging from 2 to 20. It is interesting to notice that while the active learning policy achieves a better performance than uniform allocation, the gap between the two is greater than what the theoretical analysis of the previous section may suggests (see Figures 2 and 3).\nFinally, the number of labels provided by each worker has an impact on the performance of the system. Even with a fixed number of tasks M \u201c 1000 and budget B \u201c 10M , the accuracy of the predictions improves as |Nj | increases from 2 to 20 as shown in Figure 6. This phenomenon is most likely due to the fact that the inference algorithm is able to exploit the larger amount of data available per each worker, and produce better estimates of the parameters pj ."}, {"heading": "5 Conclusions and future works", "text": "In this paper we addressed the problem of worker allocation on crowdsourced classification tasks from a theoretical perspective. First we studied the properties of three allocation policies proposed by the existing literature. During our analysis we were able to prove the equivalence between two different active learning strategies and provide a new bound on their performance. Second, we compared these adaptive active learning policies with a uniform allocation one and showed that the latter is less efficient in terms of classification accuracy. Finally we validated our theoretical results through an extensive empirical analysis. Future works include improving our bounds when the available budget is small, and extending our theoretical analysis to the more realistic case where the workers\u2019 skill is unknown a priori."}, {"heading": "Acknowledgments", "text": "This research is funded by the UK Research Council project \u201cORCHID\u201d, grant EP/I011587/1"}], "references": [{"title": "Gambler\u2019s Ruin Revisited: The Effects of Skew and Large Jackpots", "author": ["R. Michael Canjar"], "venue": "In Optimal Play: Mathematical Studies of Games and Gambling,", "citeRegEx": "Canjar.,? \\Q2007\\E", "shortCiteRegEx": "Canjar.", "year": 2007}, {"title": "Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society Series C Applied Statistics,", "citeRegEx": "Dawid and Skene.,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "Bounds on Gambler\u2019s Ruin Probabilities in Terms of Moments", "author": ["S.N. Ethier", "Davar Khoshnevisan"], "venue": "Methodology And Computing In Applied Probability,", "citeRegEx": "Ethier and Khoshnevisan.,? \\Q2002\\E", "shortCiteRegEx": "Ethier and Khoshnevisan.", "year": 2002}, {"title": "Probability Inequalities for Sums of Bounded Random Variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Quality management on Amazon Mechanical Turk", "author": ["Panagiotis G. Ipeirotis", "Foster Provost", "Jing Wang"], "venue": "In Proceedings of the ACM SIGKDD Workshop on Human Computation,", "citeRegEx": "Ipeirotis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ipeirotis et al\\.", "year": 2010}, {"title": "Budget-Optimal Crowdsourcing using Lowrank Matrix Approximations", "author": ["David R. Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "In Proceedings of the 49th Annual Conference on Communication, Control, and Computing (Allerton),", "citeRegEx": "Karger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2011}, {"title": "Kozec. A Rule of Thumb (not Only) for Gamblers", "author": ["S. Andrzej"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Andrzej,? \\Q1995\\E", "shortCiteRegEx": "Andrzej", "year": 1995}, {"title": "A Sequential Algorithm for Training Text Classifiers", "author": ["David D Lewis", "William A Gale"], "venue": "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Lewis and Gale.,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Gale.", "year": 1994}, {"title": "Variational Inference for Crowdsourcing", "author": ["Qiang Liu", "Jian Peng", "Alexander T Ihler"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Optimal Decision Rules in Uncertain Dichotomous Choice Situations", "author": ["Shmuel Nitzan", "Jacob Paroush"], "venue": "International Economic Review,", "citeRegEx": "Nitzan and Paroush.,? \\Q1982\\E", "shortCiteRegEx": "Nitzan and Paroush.", "year": 1982}, {"title": "CrowdScreen: Algorithms for Filtering Data with Humans", "author": ["Aditya G. Parameswaran", "Hector Garcia-Molina", "Hyunjung Park", "Neoklis Polyzotis", "Aditya Ramesh", "Jennifer Widom"], "venue": "In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Parameswaran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parameswaran et al\\.", "year": 2012}, {"title": "Active Learning Literature Survey", "author": ["Burr Settles"], "venue": "Machine Learning,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "Bayesian Methods for Intelligent Task Assignment in Crowdsourcing Systems. In Scalable Decision Making: Uncertainty, Imperfection, Deliberation", "author": ["Edwin Simpson", "Stephen Roberts"], "venue": null, "citeRegEx": "Simpson and Roberts.,? \\Q2014\\E", "shortCiteRegEx": "Simpson and Roberts.", "year": 2014}, {"title": "Online Decision Making in Crowdsourcing Markets: Theoretical Challenges", "author": ["Aleksandrs Slivkins", "Jennifer Wortman Vaughan"], "venue": "ACM SIGecom Exchanges,,", "citeRegEx": "Slivkins and Vaughan.,? \\Q2013\\E", "shortCiteRegEx": "Slivkins and Vaughan.", "year": 2013}, {"title": "Utility Data Annotation with Amazon", "author": ["Alexander Sorokin", "David Forsyth"], "venue": "Mechanical Turk. Urbana,", "citeRegEx": "Sorokin and Forsyth.,? \\Q2008\\E", "shortCiteRegEx": "Sorokin and Forsyth.", "year": 2008}, {"title": "Online Crowdsourcing: Rating Annotators and Obtaining CostEffective Labels", "author": ["Peter Welinder", "Pietro Perona"], "venue": "In Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Welinder and Perona.,? \\Q2010\\E", "shortCiteRegEx": "Welinder and Perona.", "year": 2010}, {"title": "Learning from the Wisdom of Crowds by Minimax Entropy", "author": ["Dengyong Zhou", "John Platt", "Sumit Basu", "Yi Mao"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "This model has been successfully employed for various purposes including image labeling [Sorokin and Forsyth, 2008], ballot voting in complex workflows and filtering [Parameswaran et al.", "startOffset": 88, "endOffset": 115}, {"referenceID": 10, "context": "This model has been successfully employed for various purposes including image labeling [Sorokin and Forsyth, 2008], ballot voting in complex workflows and filtering [Parameswaran et al., 2012].", "startOffset": 166, "endOffset": 193}, {"referenceID": 4, "context": "One of the main obstacles however is the heterogeneity in the quality of the answers collected from the crowd [Ipeirotis et al., 2010].", "startOffset": 110, "endOffset": 134}, {"referenceID": 9, "context": "When the parameters pj of the workers are known, we can optimally aggregate the workers\u2019 labels according to the following rule [Nitzan and Paroush, 1982]:", "startOffset": 128, "endOffset": 154}, {"referenceID": 1, "context": "The first application of this approach dates back to the expectation-maximisation algorithm of Dawid and Skene [1979] but in more recent years many alternative techniques have been proposed [Karger et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 1, "context": "The first application of this approach dates back to the expectation-maximisation algorithm of Dawid and Skene [1979] but in more recent years many alternative techniques have been proposed [Karger et al., 2011, Zhou et al., 2012]. In this paper we will use the approximate mean-field algorithm proposed by Liu et al. [2012] which consists in iterating between the following two steps until convergence:", "startOffset": 95, "endOffset": 325}, {"referenceID": 5, "context": "The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process.", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process. In contrast, a number of empirical-oriented works on crowdsourcing advocate the use of other policies that adapt the allocation depending on the data collected during the crowdsourcing process. Specifically Welinder and Perona [2010] propose an algorithm that computes the confidence in the classifications after the acquisition of every new label, and allocates more workers on the most uncertain tasks.", "startOffset": 34, "endOffset": 523}, {"referenceID": 5, "context": "The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process. In contrast, a number of empirical-oriented works on crowdsourcing advocate the use of other policies that adapt the allocation depending on the data collected during the crowdsourcing process. Specifically Welinder and Perona [2010] propose an algorithm that computes the confidence in the classifications after the acquisition of every new label, and allocates more workers on the most uncertain tasks. Simpson and Roberts [2014] address the same problem from a different perspective and introduce a greedy algorithm to maximise the amount of information collected from the crowd.", "startOffset": 34, "endOffset": 721}, {"referenceID": 11, "context": "Notably these two approaches belong to the family of active learning policies [Settles, 2010], but the authors provide no theoretical guarantee on their performances.", "startOffset": 78, "endOffset": 93}, {"referenceID": 5, "context": "Specifically we are interested in computing the probability Ppp\u0302`qi \u201c `i q of classifying task i correctly under each of the following policies: \u2022 the uniform allocation policy which allocates the same number of workers to each task [Karger et al., 2011]; \u2022 a greedy policy that maximises the expected information gain of each new incoming label [Simpson and Roberts, 2014]; \u2022 a simple uncertainty sampling rule that assigns new workers on the task with the least confident label [Lewis and Gale, 1994, Welinder and Perona, 2010].", "startOffset": 233, "endOffset": 254}, {"referenceID": 12, "context": ", 2011]; \u2022 a greedy policy that maximises the expected information gain of each new incoming label [Simpson and Roberts, 2014]; \u2022 a simple uncertainty sampling rule that assigns new workers on the task with the least confident label [Lewis and Gale, 1994, Welinder and Perona, 2010].", "startOffset": 99, "endOffset": 126}, {"referenceID": 12, "context": "1 Equivalence between different active learning policies We can now adapt the worker allocation policy of Simpson and Roberts [2014] to our setup by choosing the task i that maximises the amount of information collected at each round.", "startOffset": 106, "endOffset": 133}, {"referenceID": 2, "context": "We can then apply the results in Ethier and Khoshnevisan [2002] and derive an upper bound on the expected number of labels required by the active learning policy to reach zB :", "startOffset": 33, "endOffset": 64}, {"referenceID": 3, "context": "It must be noted that Equation 17 is not asymptotically tight, but nevertheless provides a better bound for \u201csmall\u201d values of ru compared to other concentration inequalities [Hoeffding, 1963].", "startOffset": 174, "endOffset": 191}, {"referenceID": 8, "context": "These two differences in the setting have the following consequences: \u2022 in order to aggregate the set of labels collected from the crowd we need to rely on the output of some inference method, which is inherently less reliable than the prediction of the optimal rule by Nitzan and Paroush [1982]; \u2022 in general, having the workers execute multiple tasks is beneficial as it improves the accuracy of the estimation of their skills, however it can also interfere with the allocation policy as we cannot assign the same worker j on a particular task i more than once.", "startOffset": 270, "endOffset": 296}, {"referenceID": 8, "context": "First, we selected the approximate mean-field algorithm by Liu et al. [2012] to learn the workers\u2019 parameter pj and predict the classification of the tasks.", "startOffset": 59, "endOffset": 77}], "year": 2016, "abstractText": "Crowdsourcing has been successfully employed in the past as an effective and cheap way to execute classification tasks and has therefore attracted the attention of the research community. However, we still lack a theoretical understanding of how to collect the labels from the crowd in an optimal way. In this paper we focus on the problem of worker allocation and compare two active learning policies proposed in the empirical literature with a uniform allocation of the available budget. To this end we make a thorough mathematical analysis of the problem and derive a new bound on the performance of the system. Furthermore we run extensive simulations in a more realistic scenario and show that our theoretical results hold in practice.", "creator": "LaTeX with hyperref package"}}}