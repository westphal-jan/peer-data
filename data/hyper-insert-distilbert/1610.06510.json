{"id": "1610.06510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Learning variable length units for SMT between related languages via Byte Pair Encoding", "abstract": "together we extensively explore the use of segments recently learnt using byte pair encoding ( referred to as individual bpe poetic units ) acts as basic units implemented for systematic statistical machine translation between related languages each and compare modelling it with orthographic poetry syllables, on which alone are apparently currently debating the five best technologies performing formal basic units for this distributed translation computation task. usually bpe identifies the worlds most frequent encoding character sequences as basic units, while orthographic syllables overall are linguistically motivated containing pseudo - syllables. then we show that bpe units outperform orthographic syllables as binary units of translation, showing up to 11 % consistently increase change in bleu scores. while in addition, bpe can be applied internally to turing any writing scheme system, while different orthographic syllables can be used naturally only jointly for languages specifically whose writing systems usually use vowel representations. we subsequently show proof that bpe interpreting units primarily outperform word structures and verbal morpheme level write units for mass translation involving languages worldwide like urdu, japanese whose writing standard systems do much not use vowels ( either just completely or, partially ). across many language stream pairs, spanning both multiple consonant language family families and types of binary writing systems, we thereby show that compiler translation with bpe reading segments likewise outperforms orthographic literacy syllables, appropriate especially for producing morphologically rich languages.", "histories": [["v1", "Thu, 20 Oct 2016 17:32:32 GMT  (124kb,D)", "http://arxiv.org/abs/1610.06510v1", "A earlier version of this paper is under review at EACL 2107. (10 pages, 2 figures, 9 tables)"], ["v2", "Mon, 12 Jun 2017 17:22:45 GMT  (127kb,D)", "http://arxiv.org/abs/1610.06510v2", "Under review, 10 pages"], ["v3", "Thu, 20 Jul 2017 21:33:00 GMT  (46kb,D)", "http://arxiv.org/abs/1610.06510v3", "Accepted at First Workshop on Subword and Character LEvel Models in NLP (SCLeM) to be held at EMNLP 2017"]], "COMMENTS": "A earlier version of this paper is under review at EACL 2107. (10 pages, 2 figures, 9 tables)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "pushpak bhattacharyya"], "accepted": false, "id": "1610.06510"}, "pdf": {"name": "1610.06510.pdf", "metadata": {"source": "CRF", "title": "Learning variable length units for SMT between related languages via Byte Pair Encoding", "authors": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya"], "emails": ["anoopk@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in prolonged contact for a long period of time (Bhattacharyya et al., 2016). Common ancestry leads to languages being related via language family hierarchies (e.g Indo-Aryan, Slavic languages). Prolonged con-\ntact, on the other hand, leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas. Examples of such linguistic areas are the Indian subcontinent and Standard Average European linguistic areas. There is substantial government, commercial and cultural communication among people speaking related languages (Europe, India and South-East Asia being prominent examples and linguistic regions in Africa possibly in the future). As these regions integrate more closely and move to a digital society, translation between related languages is becoming an important requirement. In addition, translation to/from related languages to a lingua franca like English is also very important. However, in spite of communication between people speaking related languages, most of these languages have few parallel corpora resources. It is therefore important to leverage the relatedness of these languages to build good-quality statistical machine translation (SMT) systems given the lack of parallel corpora.\nModelling the lexical similarity among related languages is the key to building goodquality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages.\nSub-word level transformations are an effective way for translation of such shared words. In this work, we explore the learning of subword units using Byte Pair Encoding (BPE), a encoding method inspired from text compression literature, and use them as the basic translation unit. Generally, the basic units of translation are either linguistically\nar X\niv :1\n61 0.\n06 51\n0v 1\n[ cs\n.C L\n] 2\n0 O\nct 2\n01 6\nmotivated (word, morpheme, syllable, etc.) or adhoc choices (character n-gram). BPE is motivated by statistical analysis of text and brings a new perspective to the investigation of basic units for translation among related languages.\nThe following are the major contributions of our work:\n\u2022 We show that BPE units outperform orthographic syllable units (Kunchukuttan and Bhattacharyya, 2016), the best performing basic unit for translation between related languages, resulting in up to 11% improvement in translation accuracy. \u2022 Unlike orthographic syllables, BPE units are\nwriting system independent. Orthographic syllables can only be applied to alphabetic and abugida writing systems. We show BPE units improve translation over word and morpheme level models for languages using abjad and logographic writing systems. Average translation accuracy improvements of 18% and 6% over a baseline word-level model for language pairs involving abjad and logographic writing systems respectively were observed. \u2022 We have reported results over a large number\nof languages (16 language pairs and 17 languages) which span 4 major language families and 10 writing systems of various types. To the best of our knowledge, this is the largest experiment for translation over related languages and the broad coverage strongly supports our results. \u2022 We also show BPE units outperform other\ntranslation units in a cross-domain translation task. \u2022 We perform a detailed analysis studying the\nrelationship of translation accuracy with lexical similarity and conditional entropy of translation models to understand why BPE units perform better than other units.\nThe paper is organized as follows. Section 2 discusses related work. Section 3 describes BPE. Section 4 describes the experimental setup. Section 5 reports the results of our experiments and provides observations and analyses of the results. Based on experimental results, we analyse why BPE units outperform other units in Section 6. Section 7 concludes the paper by summarizing the work and discussing further research possibilities."}, {"heading": "2 Related Work", "text": "There are two broad set of approaches that have been explored in the literature for translation between related languages to leverage lexical similarity between source and target languages.\nThe first approach is to use transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. The drawback is that transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored. This also allows transliteration vs. translation choices to be made.\nSince there is a high degree of similarity at the sub-word level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Recently, Kunchukuttan and Bhattacharyya (2016) proposed orthographic syllables (OS), a linguistically-motivated variablelength unit. The OS consists of a consonant core with zero or more vowels (a CV+ combination). This unit has outperformed character ngram, word and morpheme level models as well as transliteration post-editing approaches mentioned earlier. They also showed orthographic syllables can outperform other units even when: (i) the lexical distance between related languages is reasonably large, (ii) the languages do not have a genetic relation, but only a contact relation.\nRecently, sub-word level models have also generated interest in the context of neural machine translation (NMT) systems. This has been motivated by the need to limit the vocabulary of neu-\nral MT systems in encoder-decoder architectures (Sutskever et al., 2014). It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn sub-word units for NMT (Sennrich et al., 2016). Other subword units for NMT have also been proposed: character (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al., 2016). Our hypothesis is that such subword units learnt from corpora are particularly suited for translation between related languages. In this paper, we test this hypothesis by using BPE to learn subword units."}, {"heading": "3 Byte Pair Encoding as basic unit", "text": "Byte Pair Encoding is a data compression algorithm which was first adapted for Neural Machine Translation by Sennrich et al. (2016) as a way to learn a limited vocabulary for near open vocabulary translation. The essential idea is identification of the most frequent character sequences to add to the initial vocabulary. The initial vocabulary consists of only the characters in the text. An iterative greedy algorithm is used to add a new vocabulary item in every iteration. In every iteration, the most frequent bigram (based on current vocabulary) in the monolingual corpus is added to the vocabulary (the merge operation). With the updated vocabulary, the text is again encoded and this process is repeated for a pre-determined number of merge\noperations. The number of merge operations is the only hyper-parameter to the system which needs to be tuned. Figure 1 shows a high-level pseudo-code of the training process. So a new word scion may be segmented as sc ion after applying the model, assuming sc and ion as BPE units learnt during training.\nWe used the same BPE algorithm as described above and learn the BPE encodings at the wordlevel. The only way we differ from Sennrich et al. (2016) is in terms of the representation of the encoded words in the corpus. They choose a representation where the last unit of a segmented word is marked by a special character (say \u2018 \u2019), so sc ion would be represented as sc ion . Instead of marking the units of the words, we introduce special whitespace markers which indicate the gap between words. So, the sentence, \u2018the scion came\u2019 would be represented as \u2018the sc ion came\u2019. We choose this format for fair comparison with previous work on related languages."}, {"heading": "4 Experimental Setup", "text": "We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems. Evaluation of the translation systems was done using BLEU (Papineni et al., 2002)."}, {"heading": "4.1 Languages and writing systems", "text": "Our experiments spanned a diverse set of languages: 16 language pairs, 17 languages and 10\nwriting systems. Table 1 summarizes the key aspects of the languages involved in the experiments.\nThe chosen languages span 4 major language families (6 major sub-groups: Indo-Aryan, Slavic and Germanic belong to the larger Indo-European language family). The languages exhibit a range of differences in word order and morphological complexity. The classification of Japanese and Korean into the Altaic family is debated, but various lexical and grammatical similarities are indisputable, either due to genetic or cognate relationship (Robbeets, 2005; Vovin, 2010). However, the source of lexical similarity is immaterial to the current work. For want of a better classification, we use the name Altaic to indicate relatedness between Japanese and Korean.\nThe chosen language pairs also exhibit varying levels of lexical similarity. Table 4 shows an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level (shown only for language pairs where the writing systems are the same or can be easily mapped to do the LCSR computation). At one end of the spectrum, mayind, urd-hin, mac-bul are dialects/registers of the same language and exhibit high lexical similarity. On the other end of the spectrum, some language pairs like Hindi and Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012).\nThe chosen languages cover 5 types of writing systems. Of these, alphabetic and abugida writing systems represent vowels, logographic writing\nLanguage Size Language Size\nhin (Bojar et al., 2014) 10M urd (Jawaid et al., 2014) 5M tam (Ramasamy et al., 2012) 1M mar (news websites) 1.8M mal (Quasthoff et al., 2006) 200K swe (OpenSubtitles2016) 2.4M mac (Tiedemann, 2009b) 680K ind (Tiedemann, 2009b) 640K\nTable 3: Details of additional Monolingual Corpora for word-level models (source and size in number of sentences)\nsystems do not have vowels. The use of vowels is optional in abjad writing systems and depends on various factors and conventions. For instance, Urdu word segmentation can be very inconsistent (Durrani and Hussain, 2010) and generally short vowels are not denoted. The Korean Hangul writing system is syllabic, so the vowels are implicitly represented in the characters."}, {"heading": "4.2 Datasets", "text": "Table 2 shows train, test and tune splits of the parallel corpora used. The Indo-Aryan and Dravidian language parallel corpora are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus (Jha, 2012). Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection (Tiedemann, 2009b). Language models for wordlevel systems were trained on the target side of training corpora plus additional monolingual corpora from various sources (See Table 3 for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPEunit level LMs."}, {"heading": "4.3 System details", "text": "Phrase-based SMT systems were trained using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character, OS and BPE-unit level models. We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. We used unsupervised morphsegmenters (trained using Morfessor (Virpioja et al., 2013)) and orthographic syllabification rules from the Indic NLP Library1. For training BPE\n1http://anoopkunchukuttan.github.io/indic nlp library\nmodels, we used the subword-nmt2 library. We used Juman3 and Mecab4 for Japanese and Korean tokenization respectively."}, {"heading": "5 Results and Analysis", "text": "This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size are studied. We also report initial results with a joint bilingual BPE model."}, {"heading": "5.1 Comparison of BPE with other units", "text": "Table 4 shows the translation accuracy of all the language pairs under experimentation for different translation units. The number of BPE merge operations was chosen such that the resultant vocabulary size would be equivalent to the vocabulary size of the orthographic syllable encoded corpus. Since we could not do orthographic syllabification\n2https://github.com/rsennrich/subword-nmt 3http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN 4https://bitbucket.org/eunjeon/mecab-ko\nfor urd, kor, jpn, we selected the merge operations as follows: (\u2020) For urd, number of merge operations were selected based on hin OS vocabulary since hin and urd are registers of the same language. (\u2021): For kor and jpn, the BPE results reported correspond to the best performing configuration in Table 5.\n\u2022 BPE units are clearly better than the traditional word and morpheme representations. The average BLEU score improvement is 15% over word-based results and 11% over morpheme-based results. The only exception is Malay-Indonesian, which are essentially registers of the same language.\n\u2022 BPE units also show significant improvement over the recently proposed orthographic syllable units over most language pairs (average improvement of 2.6% and maximum improvement of up to 11%). The only exceptions are ben-hin, pan-hin and may-ind - all these languages pairs have relatively less morphological affixing (pan-hin, ben-hin) or are registers of the same language (may-ind). For ban-hin and pan-hin, the BPE unit translation accuracies are quite close to that of the OS level. Since OS level models have been shown to be better than character level models, BPE units are better than character level models by transitivity.\n\u2022 It is worth mentioning that BPE units provide a substantial benefit over OS units when translation involves a morphologically rich language. In translations involving mal, tam and tel, average accuracy improvement of 6.25% were observed."}, {"heading": "5.2 Applicability to different writing systems", "text": "The utility of orthographic syllables as translation units is limited to languages that use writing systems which represent vowels. Alphabetic and abugida writing systems fall into this category. On the other hand, logographic writing systems (Japanese Kanji, Chinese) and abjad writing systems (Arabic, Hebrew, Syriac, etc.) do not represent vowels. To be more precise, abjad writing systems may represent some/all vowels depending on language, pragmatics and conventions. Syllabic writing systems like Korean Hangul do not explicitly represent vowels, since the basic unit (the syllable) implicitly represents the vowels. The\nmajor advantage of Byte Pair Encoding is its writing system independence and our results show that BPE encoded units are useful for translation involving abjad (Urdu uses an extended Arabic writing system), logographic (Japanese Kanji) and syllabic (Korean Hangul) writing systems. For language pairs involving Urdu, there is an 18% average improvement over word-level and 12% average improvement over morpheme-level translation accuracy. For jpn-kor language pairs, an average improvement of 6% in translation accuracy over a word-level baseline is observed."}, {"heading": "5.3 Choosing the number of BPE merge operations", "text": "The above mentioned results for BPE units do not explore optimal values of the number of merge operations. This is the only hyper-parameter that has to be selected for BPE. We experimented with number of merge operations ranging from 1000 to 4000 and the translation results for these are shown in Table 5. Selecting the optimal value of merge operations lead to a modest, average increase of 1.6% and maximum increase of 3.5% in the translation accuracy over Bmatch across different language pairs .\nWe also experimented with higher number of merge operations for some language pairs, but there seemed to be no benefit with a higher number of merge operations. Compared to the number\nof merge operations reported by Sennrich et al. (2016) in a more general setting for NMT (60k), the number of merge operations is far less for translation between related languages with limited parallel corpora. We must bear in mind that their goal was different: available parallel corpus was not an issue, but they wanted to handle as large a vocabulary as possible for open-vocabulary NMT. Yet, the low number of merge operations suggest that BPE encoding is suitable for this translation task."}, {"heading": "5.4 Robustness to Domain Change", "text": "Kunchukuttan and Bhattacharyya (2016) have shown that models trained on orthographic syl-\nBestprev JB1k JB2k JB3k JB4k ben-hin O (33.46) 33.54 33.23 33.54 33.35 pan-hin O (72.51) 72.41 72.35 72.13 72.04 kok-mar B1k (23.84) 24.01 23.76 23.8 23.86 mal-tam B4k (8.74) 8.6 8.82 8.74 8.72 tel-mal B3k (9.12) 8.47 8.84 8.89 8.92 hin-mal B1k (10.96) 11.19 11.09 11.1 10.96 mal-hin B1k (21.23) 20.79 21.22 21.12 21.06\nbul-mac B2k (22.27) 22.11 22.17 21.58 22.24 dan-swe B3k (36.61) 36.15 36.86 36.51 36.71 may-ind O (61.24) 61.26 60.98 61.11 60.66\nTable 7: Translation accuracies for Joint BPE models trained with different number of merge operations (%BLEU). The Bestprev indicates the best performing units and their accuracy scores from Tables 4 and 5 shown for comparison.\nlables are robust to domain change. Since BPE units are learnt from a specific corpus, it is not guaranteed that they would also be robust to domain changes. To study the behaviour of BPE unit trained models, we also tested the translation models trained on tourism & health domains on an agriculture domain test set of 1000 sentences (see Table 6 for results). In this cross-domain translation scenario, the BPE level model significantly outperforms the OS-level model for most language pairs. The kok-mar pair alone shows a degradation using the OS level model. For tel-mal, the BPE model performs almost on par with the OS level model."}, {"heading": "5.5 Effect of training data size", "text": "For different training set sizes, we trained SMT systems with various representation units (Figure 2 shows the learning curves for two language pairs). BPE level models are better than OS, morpheme and word level across a range of dataset sizes. Especially when the training data is very small, the OS level models perform significantly better than the word and morpheme level models. The BPE level model is also better then OS level model at utilizing more training data in the case of mal-hin."}, {"heading": "5.6 Joint bilingual training of BPE models", "text": "We experimented with jointly training BPE models over source and target language corpora as suggested by Sennrich et al. (2016). The idea is to\nlearn an encoding that is consistent across source and target languages and therefore help alignment. If source and target use the same writing system, then a joint model is created by learning BPE over concatenated source and target language corpus. If the writing systems are different, then we transliterate one corpus to another by one-one character mappings. This is possible between Indic scripts. But this scheme cannot be applied between Urdu and Indic scripts as well as between Korean Hangul and Japanese Kanji scripts.\nTable 7 shows the results of the joint BPE model for language pairs where such a model is built. This performance of this model is roughly equivalent to the best performing monolingual BPE model."}, {"heading": "6 Why is BPE unit better than other units?", "text": "The improved performance of BPE units compared to word-level and morpheme-level representations is easy to explain: with a limited vocabulary they address the problem of data sparsity. But unigram level models also have a limited vocabulary, yet they do not improve translation performance except for very close languages. This is because, they can effectively earn character mappings which may be sufficient for transliteration to some extent. But translating cognates, morphological affixes, etc. require a larger context. So, BPE and OS units \u2014 which provide more context \u2014 outperform unigrams.\nTo understand why BPE performs better than OS, we need to understand an important desirable property of the segmentation scheme: the segmen-\ntation should lead to a model with a good predictive capacity (for instance in terms of language modeling). In fact, segmentation schemes like wordpieces (Schuster and Nakajima, 2012) explicity try to maximize this goal. By learning a compression model for the data, BPE is also implicitly learning a predictive model (Minimum Description Length Principle). One the other hand, the OS unit provides a larger context but there is no attempt to maximize the predictive capacity of the segmentation.\nTable 8 offers insights supporting these hypotheses. Unigram models work best when the source and target sentences are lexically very similar. The additional context decouples the OS and BPE units from lexical similarity. The translation models using BPE units are less correlated to the lexical similarity than OS level models since BPE learns a model of the corpus.\nAnother interesting observation is related to the conditional entropy model at the lexical/unit level (as opposed to the phrase/sequence level). The conditional entropy for the lexical translation model given the source language F and the target language E can be computed as: H(F |E) = \u2212 \u2211 e\u2208VE P (e) \u2211 f\u2208VF P (f |e) logP (f |e)\nwhere, VF and VE are the vocabularies of the source and target languages for the chosen unit of representation f and e are basic units in the vocabulary of F and E respectively P (e) is estimated using a 0th order language model over the target language corpus\nTable 9 shows the conditional entropy of lexical translation models for sub-word units over some\nlanguage pairs. It is seen that the translation entropy is lowest for unigram level model and highest for the BPE level model. This indicates that unigram character models are learning unambiguous character mappings, whereas the reality is that the mappings are quite ambiguous. The higher conditional entropy of OS and BPE unit models capture this reality."}, {"heading": "7 Conclusion & Future Work", "text": "We showed that the BPE unit, a translation unit motivated from data compression and information theory, can outperform all previously proposed translation units, including the best-performing orthographic syllables, for the task of translation between related languages when limited parallel corpus is available. Moreover, BPE encoding is writing system independent and hence can be applied to any language. Experimental results on a large number of language pairs spanning diverse language families and writing systems support the benefits of BPE encoding for this task. We also show that BPE units are more robust to change in translation domain and they perform better in extremely data scarce scenarios and for morphologically rich languages. Initial experiments to build joint BPE models across source and target language gave minor improvements.\nCompared to the application of BPE in NMT, only a small number of merge operations are required to learn an optimal vocabulary suggesting that BPE units are particularly suitable for translation between related languages. This also begs the question of connections between data compression and translation accuracy, an area of work which could be relevant to Neural MT also. NMT between related languages using BPE and similar encodings is also an obvious direction to explore. Given the improved performance of the BPE-unit and OS, tasks involving related languages viz. pivot based MT, domain adaptation (Tiedemann, 2012) and translation between a lingua franca and related languages (Wang et al., 2012) can be revisited with these basic units."}, {"heading": "Acknowledgments", "text": "We thank the Technology Development for Indian Languages (TDIL) Programme and the Department of Electronics & Information Technology, Govt. of India for their support."}], "references": [{"title": "Statistical machine translation between related languages", "author": ["Pushpak Bhattacharyya", "Mitesh Khapra", "Anoop Kunchukuttan."], "venue": "NAACL Tutorials.", "citeRegEx": "Bhattacharyya et al\\.,? 2016", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "HindEnCorp \u2013 Hindi-English and Hindi-only Corpus for Machine Translation", "author": ["Ond\u0159ej Bojar", "Vojt\u011bch Diatka", "Pavel Rychl\u00fd", "Pavel Stra\u0148\u00e1k", "V\u0131\u0301t Suchomel", "Ale\u0161 Tamchyna", "Daniel Zeman"], "venue": "In Proceedings of the 9th International Conference", "citeRegEx": "Bojar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Urdu word segmentation", "author": ["Nadir Durrani", "Sarmad Hussain."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Durrani and Hussain.,? 2010", "shortCiteRegEx": "Durrani and Hussain.", "year": 2010}, {"title": "Hindi-to-Urdu machine translation through transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Durrani et al\\.,? 2010", "shortCiteRegEx": "Durrani et al\\.", "year": 2010}, {"title": "A new algorithm for data compression", "author": ["Philip Gage."], "venue": "C Users J.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Urdu monolingual corpus", "author": ["Bushra Jawaid", "Amir Kamran", "Ond\u0159ej Bojar."], "venue": "LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University in Prague.", "citeRegEx": "Jawaid et al\\.,? 2014", "shortCiteRegEx": "Jawaid et al\\.", "year": 2014}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Jha.,? 2012", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "Moses: Open source toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Orthographic syllable as basic unit for smt between related languages", "author": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Kunchukuttan and Bhattacharyya.,? 2016", "shortCiteRegEx": "Kunchukuttan and Bhattacharyya.", "year": 2016}, {"title": "The IIT Bombay SMT System for ICON 2014 Tools contest", "author": ["Anoop Kunchukuttan", "Ratish Pudupully", "Rajen Chatterjee", "Abhijit Mishra", "Pushpak Bhattacharyya."], "venue": "NLP Tools Contest at ICON 2014.", "citeRegEx": "Kunchukuttan et al\\.,? 2014", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2014}, {"title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "author": ["I Dan Melamed."], "venue": "Third Workshop on Very Large Corpora.", "citeRegEx": "Melamed.,? 1995", "shortCiteRegEx": "Melamed.", "year": 1995}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages", "author": ["Preslav Nakov", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short", "citeRegEx": "Nakov and Tiedemann.,? 2012", "shortCiteRegEx": "Nakov and Tiedemann.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Corpus portal for search in monolingual corpora", "author": ["Uwe Quasthoff", "Matthias Richter", "Christian Biemann."], "venue": "Proceedings of the fifth international conference on language resources and evaluation.", "citeRegEx": "Quasthoff et al\\.,? 2006", "shortCiteRegEx": "Quasthoff et al\\.", "year": 2006}, {"title": "Morphological Processing for English-Tamil Statistical Machine Translation", "author": ["Loganathan Ramasamy", "Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd."], "venue": "Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages.", "citeRegEx": "Ramasamy et al\\.,? 2012", "shortCiteRegEx": "Ramasamy et al\\.", "year": 2012}, {"title": "Is Japanese Related to Korean, Tungusic, Mongolic and Turkic? Otto Harrassowitz Verlag", "author": ["Martine Irma"], "venue": "Robbeets", "citeRegEx": "Irma,? \\Q2005\\E", "shortCiteRegEx": "Irma", "year": 2005}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149\u20135152. IEEE.", "citeRegEx": "Schuster and Nakajima.,? 2012", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "South Asian Languages: A Syntactic Typology", "author": ["Karumuri Subbarao."], "venue": "Cambridge University Press.", "citeRegEx": "Subbarao.,? 2012", "shortCiteRegEx": "Subbarao.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets", "author": ["J\u00f6rg Tiedemann", "Preslav Nakov."], "venue": "RANLP.", "citeRegEx": "Tiedemann and Nakov.,? 2013", "shortCiteRegEx": "Tiedemann and Nakov.", "year": 2013}, {"title": "Character-based PBSMT for closely related languages", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the 13th Conference of the European Association for Machine Translation.", "citeRegEx": "Tiedemann.,? 2009a", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "News from opus-a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing.", "citeRegEx": "Tiedemann.,? 2009b", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Character-based pivot translation for under-resourced languages and domains", "author": ["J\u00f6rg Tiedemann."], "venue": "EACL.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Can we translate letters", "author": ["David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Korea-Japonica: A ReEvaluation of a Common Genetic Origin", "author": ["Alexander Vovin."], "venue": "University of Hawaii Press.", "citeRegEx": "Vovin.,? 2010", "shortCiteRegEx": "Vovin.", "year": 2010}, {"title": "Source language adaptation for resource-poor machine translation", "author": ["Pidong Wang", "Preslav Nakov", "Hwee Tou Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi."], "venue": "ArXiv e-prints.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in prolonged contact for a long period of time (Bhattacharyya et al., 2016).", "startOffset": 173, "endOffset": 201}, {"referenceID": 11, "context": "\u2022 We show that BPE units outperform orthographic syllable units (Kunchukuttan and Bhattacharyya, 2016), the best performing basic unit for translation between related languages, resulting in up to 11% improvement in translation accuracy.", "startOffset": 64, "endOffset": 102}, {"referenceID": 14, "context": "This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT.", "startOffset": 82, "endOffset": 136}, {"referenceID": 12, "context": "This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT.", "startOffset": 82, "endOffset": 136}, {"referenceID": 6, "context": "This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so", "startOffset": 91, "endOffset": 113}, {"referenceID": 27, "context": "has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013).", "startOffset": 133, "endOffset": 198}, {"referenceID": 24, "context": "has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013).", "startOffset": 133, "endOffset": 198}, {"referenceID": 23, "context": "has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013).", "startOffset": 133, "endOffset": 198}, {"referenceID": 26, "context": "vides very little context for learning translation models (Tiedemann, 2012).", "startOffset": 58, "endOffset": 75}, {"referenceID": 23, "context": "The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013).", "startOffset": 137, "endOffset": 164}, {"referenceID": 11, "context": "Recently, Kunchukuttan and Bhattacharyya (2016) proposed orthographic syllables (OS), a linguistically-motivated variablelength unit.", "startOffset": 10, "endOffset": 48}, {"referenceID": 20, "context": "Source: Sennrich et al. (2016)", "startOffset": 8, "endOffset": 31}, {"referenceID": 22, "context": "(Sutskever et al., 2014).", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn sub-word units for NMT (Sennrich et al.", "startOffset": 73, "endOffset": 85}, {"referenceID": 20, "context": "It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn sub-word units for NMT (Sennrich et al., 2016).", "startOffset": 131, "endOffset": 154}, {"referenceID": 4, "context": "ter (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 3, "context": ", 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al.", "startOffset": 38, "endOffset": 64}, {"referenceID": 19, "context": ", 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al., 2016).", "startOffset": 78, "endOffset": 124}, {"referenceID": 31, "context": ", 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al., 2016).", "startOffset": 78, "endOffset": 124}, {"referenceID": 20, "context": "Byte Pair Encoding is a data compression algorithm which was first adapted for Neural Machine Translation by Sennrich et al. (2016) as a way to learn a limited vocabulary for near open vocabulary translation.", "startOffset": 109, "endOffset": 132}, {"referenceID": 20, "context": "The only way we differ from Sennrich et al. (2016) is in terms of the representation of the encoded words in the corpus.", "startOffset": 28, "endOffset": 51}, {"referenceID": 15, "context": "Evaluation of the translation systems was done using BLEU (Papineni et al., 2002).", "startOffset": 58, "endOffset": 81}, {"referenceID": 29, "context": "tionship (Robbeets, 2005; Vovin, 2010).", "startOffset": 9, "endOffset": 38}, {"referenceID": 13, "context": "Table 4 shows an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level (shown only for language pairs where the writing systems are the same or can be easily mapped to do the LCSR computation).", "startOffset": 123, "endOffset": 138}, {"referenceID": 21, "context": "On the other end of the spectrum, some language pairs like Hindi and Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012).", "startOffset": 200, "endOffset": 216}, {"referenceID": 1, "context": "hin (Bojar et al., 2014) 10M urd (Jawaid et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 8, "context": ", 2014) 10M urd (Jawaid et al., 2014) 5M tam (Ramasamy et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 17, "context": ", 2014) 5M tam (Ramasamy et al., 2012) 1M mar (news websites) 1.", "startOffset": 15, "endOffset": 38}, {"referenceID": 16, "context": "8M mal (Quasthoff et al., 2006) 200K swe (OpenSubtitles2016) 2.", "startOffset": 7, "endOffset": 31}, {"referenceID": 25, "context": "4M mac (Tiedemann, 2009b) 680K ind (Tiedemann, 2009b) 640K", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "4M mac (Tiedemann, 2009b) 680K ind (Tiedemann, 2009b) 640K", "startOffset": 35, "endOffset": 53}, {"referenceID": 5, "context": "For instance, Urdu word segmentation can be very inconsistent (Durrani and Hussain, 2010) and generally short vowels are not denoted.", "startOffset": 62, "endOffset": 89}, {"referenceID": 9, "context": "tive (ILCI) corpus (Jha, 2012).", "startOffset": 19, "endOffset": 30}, {"referenceID": 25, "context": "Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection (Tiedemann, 2009b).", "startOffset": 112, "endOffset": 130}, {"referenceID": 10, "context": "Phrase-based SMT systems were trained using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters).", "startOffset": 61, "endOffset": 81}, {"referenceID": 2, "context": ", 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters).", "startOffset": 86, "endOffset": 111}, {"referenceID": 28, "context": "We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations.", "startOffset": 69, "endOffset": 92}, {"referenceID": 28, "context": "We used unsupervised morphsegmenters (trained using Morfessor (Virpioja et al., 2013)) and orthographic syllabification rules from the Indic NLP Library1.", "startOffset": 62, "endOffset": 85}, {"referenceID": 20, "context": "of merge operations reported by Sennrich et al. (2016) in a more general setting for NMT (60k), the number of merge operations is far less for translation between related languages with limited parallel corpora.", "startOffset": 32, "endOffset": 55}, {"referenceID": 20, "context": "We experimented with jointly training BPE models over source and target language corpora as suggested by Sennrich et al. (2016). The idea is to Src-Tgt W M Bmatch O C", "startOffset": 105, "endOffset": 128}, {"referenceID": 19, "context": "In fact, segmentation schemes like wordpieces (Schuster and Nakajima, 2012) explicity try to maximize this goal.", "startOffset": 46, "endOffset": 75}, {"referenceID": 26, "context": "pivot based MT, domain adaptation (Tiedemann, 2012) and translation between a lingua franca and related languages (Wang et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 30, "context": "pivot based MT, domain adaptation (Tiedemann, 2012) and translation between a lingua franca and related languages (Wang et al., 2012) can be revisited with these basic units.", "startOffset": 114, "endOffset": 133}], "year": 2016, "abstractText": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU scores. In addition, BPE can be applied to any writing system, while orthographic syllables can be used only for languages whose writing systems use vowel representations. We show that BPE units outperform word and morpheme level units for translation involving languages like Urdu, Japanese whose writing systems do not use vowels (either completely or partially). Across many language pairs, spanning multiple language families and types of writing systems, we show that translation with BPE segments outperforms orthographic syllables, especially for morphologically rich languages.", "creator": "TeX"}}}