{"id": "1610.07667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Predicting Counterfactuals from Large Historical Data and Small Randomized Trials", "abstract": "when developing a new potential treatment is considered for use, whether a drug pharmaceutical drug or a search task engine ) ranking classification algorithm, a typical pricing question that arises is, will its forecast performance steadily exceed that of executing the present current delayed treatment? the fastest conventional design way to answer this subjective counterfactual question is allowed to dramatically estimate the effect instead of targeting the new treatment in comparison outcome to only that of applying the conventional prior treatment applied by running a controlled, randomized experiment. while this experimental approach theoretically ensures an absolutely unbiased estimator,, it suffers indirectly from several key drawbacks, including the difficulty in finding optimal representative experimental populations and as ultra well as from the ongoing cost of running such trials. moreover, such trials neglect the huge sheer quantities of limited available basic control - condition inference data which predictions are often completely ignored.", "histories": [["v1", "Mon, 24 Oct 2016 22:12:52 GMT  (211kb,D)", "https://arxiv.org/abs/1610.07667v1", null], ["v2", "Wed, 26 Oct 2016 06:11:07 GMT  (213kb,D)", "http://arxiv.org/abs/1610.07667v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nir rosenfeld", "yishay mansour", "elad yom-tov"], "accepted": false, "id": "1610.07667"}, "pdf": {"name": "1610.07667.pdf", "metadata": {"source": "CRF", "title": "Predicting Counterfactuals from Large Historical Data and Small Randomized Trials", "authors": ["Nir Rosenfeld", "Yishay Mansour", "Elad Yom-Tov"], "emails": ["nir.rosenfeld@mail.huji.ac.il", "mansour@microsoft.com", "eladyt@microsoft.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "In this paper we propose a discriminative framework for estimating the performance of a new treatment given a large dataset of the control condition and data from a small (and possibly unrepresentative) randomized trial comparing new and old treatments. Our objective, which requires minimal assumptions on the treatments, models the relation between the outcomes of the different conditions. This allows us to not only estimate mean effects but also to generate individual predictions for examples outside the randomized sample.\nWe demonstrate the utility of our approach through experiments in three areas: Search engine operation, treatments to diabetes patients, and market value estimation for houses. Our results demonstrate that our approach can reduce the number and size of the currently performed randomized controlled experiments, thus saving significant time, money and effort on the part of practitioners."}, {"heading": "1. INTRODUCTION", "text": "Novel medical treatments, new government policies, and innovative website designs are all examples of changes to an existing method of interaction with people that need to be evaluated for their effectiveness before they can be put into use. The gold standard for testing such interventions are randomized controlled trials (RCTs) [6]. RCTs are widely\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2016 ACM. ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nused in medicine: Approximately 200,000 RCTs were conducted in the 1990\u2019s alone [6]. Internet website operators were early adopters of RCTs [9]. Most large Internet companies are known to run thousands of RCTs every year [8].\nRCTs work by randomly assigning every subject to either a control group or a treatment group. The average measurement of the result variable for each group is then an unbiased estimator of its corresponding population mean. Given these, unbiased estimators of other desirable quantities such as the mean treatment effect can be easily constructed.\nThis approach, while appealing, has several drawbacks. First, for the estimators to be truly unbiased, subjects must be sampled i.i.d. from the general population of interest. Not only is this unrealistic and seldom the case, but often times the sample represents a very specific sub-population, which can lead to extremely biased estimates. This is especially evident in clinical trials, where subjects (who typically volunteer to take part in an experiment) are often those suffering from severe symptoms, those which no other treatment helped, or simply those who are more prone to volunteer.\nSecond, as controlled trials are expensive and time consuming, sample sizes tend to be small. This greatly limits the amount of information available to researchers and practitioners for drawing conclusions, generating predictions, and deciding on policies. The small samples are typically sufficient for constructing estimators with reasonably low variance, but are seldom enough for generating high-accuracy predictors. For instance, in search engine A/B tests, the decision of whether to use an alternative results ranker (or even whether to continue running the experiment) is often based on the average measures of click-through rate (CTR) or similar measures, and not on predictions regarding specific queries. Larger samples should potentially allow for the application of high-end learning algorithms.\nThird, the price paid for guaranteeing that the estimators are unbiased is that only data from the controlled trial can be used. This completely discards the huge quantities of data that are often times available for the control condition, which in most cases is just the current policy. For instance, consider the case of predicting whether administering a new drug would prove better than the current standard for a given patient. A predictor trained only on the results of a small-scale clinical trial should prove to be inferior to one which also takes into account all the past medical records corresponding to the currently applied drug. Using only the trial results seems wasteful in terms of data, and generally suboptimal for prediction.\nar X\niv :1\n61 0.\n07 66\n7v 2\n[ cs\n.L G\n] 2\n6 O\nct 2\n01 6\nGiven the above, the question we pose here is the following: how can we design a learning algorithm for generating predictors in counterfactual settings, which takes as input both a small randomized trial dataset, and a large labeled dataset of the control population? Answering the above question is the motivation behind this paper.\nNote that although the setting we discuss is of a counterfactual nature, our goal is in essence a predictive one. In pursuing the goal of generating high-accuracy predictors, we knowingly forfeit the ability to explain the underlying causal mechanism. The latter objective has been the focus of an abundant body of works, most based on the framework of causal inference [13]. We argue here that there is an inherent tradeoff between interpretability and predictive performance, and that when the goal is to optimize accuracy, a direct approach is preferred. Our work follows the more recent line of work where a discriminative loss-centric approach is applied in counterfactual settings [17, 18, 7].\nThe paper is organized in the following manner. We begin by covering related material in Sec. 2. We present notations our and problem statement in Secs. 3 and 4, respectively. Sec. 5 contains a detailed description of the core of our approach, followed by Sec. 6 in which several extensions are presented. Sec. 7 contains several experiments on real data. We conclude with a discussion in Sec. 8."}, {"heading": "2. RELATED MATERIAL", "text": "Our setting draws relations to several lines of work. The fundamental property of the prediction task we consider is that it is counterfactual in nature. Causal inference [13] is a standard framework for estimating the causal relation between variables, in a way which can then be used to answer counterfactual questions. In order to achieve this, methods for causal inference are usually based on simple, interpretable models from which actionable conclusions can be drawn [4]. Our approach is different in that it focuses on prediction by introducing an ad-hoc loss function for parametrized predictors. Classic causal-inference models on the other hand do not always allow for arbitrary features, nor is it always straightforward to learn or to generate predictions from a given model.\nAlternatively, counterfactual questions can be answered if data can be collected under a random policy [11, 4, 10]. In practice, true randomization is difficult to obtain for business reasons (in the Internet setting) or for ethical reasons (in the medical and social domains). Because of this, it has been proposed to treat data collected under different settings as randomized, and use it to answer counterfactual questions [12].\nMore recently, notions from causal inference have been incorporated in to discriminative learning methods, with the declared goal of minimizing loss. In analogy to Empirical Risk Minimization, the principle of Countefactual Risk Minimization is proposed in [17, 18]. The proposed method offers a discriminative learning objective based on inverse propensity scores, where the variance is controlled by a regularization term or by self-normalization. In contrast to our setup, this method requires that in addition to examples x and labels y, each sample must also includes its loss logged propensity score. Other works use doubly-robust methods which are based on propensity scores as well [1]. Some parametric non-linear methods for estimating treatment effect are based on Bayesian regression trees [5] and random forests [21].\nA parallel discriminative approach to counterfactual prediction is based on the notion of domain adaptation [2]. Following the work of [15], the authors of [7] observe that generalizing from the observed factual distribution to the unobserved counterfactual distribution is a special case of covariance shift, and in general of domain adaptation. Therefore, the non-convex representation learning method in [7] incorporates a discrepancy-based regularization term which encourages a label-invariant representation. In contrast, our method regularizes the relation between the control and treatment variables themselves, conditioned on a the original shared representation. Moreover, while the method of [7] requires large amounts of data for both labels, our method is tailored for a setting where the treatment variable is rare."}, {"heading": "3. NOTATIONS", "text": "Our setup is similar to a standard supervised learning setup where we are given a sample set of examples x and labels y, but with some additions. We assume examples are from a general domain X , and denote by X \u2032 \u2286 X the subdomain of examples that take part in the controlled trials. Our setup includes two label domains, denoted by YC for the control variable and by YT for the treatment variable. Throughout the paper we use the terms label, variable, and experimental outcome interchangeably.\nInstantiations of examples are denoted by x \u2208 X , and of labels are denoted by yC \u2208 YC and yT \u2208 YT . We assume there exists a single governing joint distribution DX ,YC ,YT for tuples (x, yC , yT ), though we have no direct access to it, nor do we observe such tuples. Rather, for a given example x \u223c DX drawn from the marginal distribution, we observe either the control variable yC \u223c DYC |X=x or the treatment variable yT \u223c DYT |X=x, drawn from their respective conditional distributions.\nIn the setting we consider, we are given as input three sample sets of example-label pairs:\n1. SC = { (x(i), y (i) C ) }M i=1 sampled i.i.d. from DX ,YC 2. S\u2032C = { (x(i), y (i) C ) }MC i=1 sampled i.i.d. from DX \u2032,YC 3. S\u2032T = { (x(i), y (i) T ) }MT i=1 sampled i.i.d. from DX \u2032,YT\nThe first set SC is a large sample of the general population with labels for the control variable, representing past data accumulated by running the default policy. The sets S\u2032C and S\u2032T are smaller and represent the results of the controlled trial for the control and treatment groups.1 We therefore assume that MC ,MT M and that MC \u2248MT . More concretely, we assume that while M is sufficiently large to learn a reasonably accurate predictor for the control variable, MT is insufficiently small for adequate learning of the treatment variable. Note that we are not guaranteed to have any x for which we observe both yC and yT . This is a fundamental problem in counterfactual settings, and makes estimating the individual treatment effect y\u2206 = yT \u2212yC especially challenging [23].\n1Note that the i.i.d. assumption mimics the procedure of random subject assignment found in RCTs."}, {"heading": "4. PROBLEM STATEMENT", "text": "Recall that our goal is to construct a framework for learning predictors by leveraging both the small randomized trial data (S\u2032C , S \u2032 T ) and the large control-labeled dataset SC . The main task we consider is predicting the treatment variable yT for new, unobserved examples from a test set ST \u223c DX ,YT . In other words, we\u2019d like our predictor to generalize well to YT on the general population. The challenge here is that our data contains only a small number of treatment labels. The solution we present in Sec. 5 utilizes all the available data by modeling the relation between the control and treatment variable.\nA related task that is of high interest is to predict the individualized treatment effect y\u2206 = yT \u2212yC [14, 20]. Accurate predictions of y\u2206 can in principle aid decision makers in deciding what treatment to apply. Such predictions can also be used to estimate the mean treatment effect E [y\u2206], and by so offer an alternative to conventional estimators used in randomized trials. As we show in Sec. 5, the individualized treatment effect y\u2206 plays a central role in our learning objective for all tasks we consider."}, {"heading": "5. METHOD", "text": "At the core of our method lie only two simple modeling assumptions: that predictions for both conditions are of the form y\u0302 = \u3008w, x\u3009,2 and that the models wC , wT for the control and treatment conditions, respectively, should be similar under some notion.\nFor ease of exposition, consider first a regression task where x \u2208 Rd, yC , yT \u2208 R, and our goal is to minimize the squared loss of a linear predictor for the treatment variable, namely y\u0302T = \u3008wT , x\u3009. In Sec. 6 we show that our method applies to both regression and classification, and to a wide of loss functions, and to some non-linear predictors as well.\nSince our task is to predict the treatment outcome yT of a given sample x, a reasonable place to start would be in\n2In general we assume that predictors are linear in some feature representation \u03d5(x), as we describe in Sec. 6.1.\nconsidering a learning objective over the sample set S\u2032T , as it is the only one for which we have treatment labels. Applying the squared loss and adding `2 regularization gives us:\nmin wT\u2208Rd\n1\nMT \u2211 i\u2208S\u2032\nT\n( \u3008wT , x(i)\u3009 \u2212 y(i)T )2 + \u03bb\u2016wT\u201622 (1)\nAs in any discriminative objective, the number of samples greatly effects the quality of generalization of the learned predictor. Unfortunately, for the above objective and under our assumptions, S\u2032T will not prove to be sufficiently large for training a high-accuracy predictor for the treatment variable. Put simply, our data does not include enough labeled instances from YT .\nOur approach remedies this deficiency by artificially augmenting S\u2032T with samples that serve as a proxy for treatment labels. As a first step, we will add to the objective in Eq. (1) the samples from SC , our largest available dataset:\nmin wT\u2208Rd\n\u03b3\nMT \u2211 i\u2208S\u2032\nT\n( \u3008wT , x(i)\u3009 \u2212 y(i)T )2 +\n(1\u2212 \u03b3) M \u2211 i\u2208SC ( \u3008wT , x(i)\u3009 \u2212 y(i)C )2 + \u03bb\u2016wT\u201622 (2)\nwhere \u03b3 \u2208 [0, 1] controls the relative weight of each dataset in the training objective. For ease of notation, we overload SC to include all of the available control condition examples, namely S\u2032C \u2282 SC .\nAt a first glance using control outcomes yC when trying to predict the treatment outcome yT may seem peculiar. Nonetheless, work in multi-task learning has shown that training a single predictor over several labels is beneficial in practice when the conditional distribution of different labels is similar. [3]. However, even if the control and treatment distributions do share similarities, our focus here is on their differences. We therefore do not suffice with Eq. (2), in place of the control labels y (i) C use proxy treatment labels y\u0303 (i) T which we define next. Denote by \u2206 = yC \u2212 yT the negative of the individual treatment effect y\u2206, namely the difference between the control and treatment variables. We have already set y\u0302T to be a linear function of x with weights wT ; extending this to y\u0302C with weights wC gives us:\n\u2206\u0302 = \u3008wC , x\u3009 \u2212 \u3008wT , x\u3009 = \u3008w\u2206, x\u3009 (3)\nwhere we use w\u2206 = wC \u2212 wT . This implies that \u2206 is also modeled by a linear function, and readily gives us our proxy:\ny\u0303T = yC \u2212 \u3008w\u2206, x\u3009 (4)\nNote that this derivation is possible due to our view of the tuple (x, yT , yC) as jointly distributed. This is in contrast to the more conventional approach where the distribution is modeled using tuples of the form (x, \u03bd, y\u03bd), where \u03bd \u2208 {C, T} is the experimental condition and y\u03bd is the outcome under that condition [7]. Our formulation induces a joint distribution over pairs (x,\u2206) \u223c DX ,\u2206, which we can model.\nPlugging back into Eq. (2) and further regularizing gives:\nmin wT ,w\u2206\n\u03b3\nMT \u2211 i\u2208S\u2032\nT\n( \u3008wT , x(i)\u3009 \u2212 y(i)T )2 +\n(1\u2212 \u03b3) M \u2211 i\u2208SC ( \u3008wT , x(i)\u3009 \u2212 y\u0303(i)T )2 + \u03bb\u2016wT\u201622 + \u03b7R(w\u2206) (5)\nwhere R is a regularization function, and \u03b3, \u03b7 \u2208 R are additional meta-parameters which we will shortly describe. Note that y\u0303T is in fact a function of w\u2206; the explicit form of summands in the second loss term is (\u3008wT \u2212 w\u2206, x\u3009 \u2212 yC)2.\nTo gain insight into the above construction, we next analyze the learning objective under an alternative formulation. Notice that by Eqs. (3) and (4), the second loss term and the additional regularization term in Eq. (5) can equivalently be written as:3\u2211\ni\u2208SC\n( \u3008wC , x(i)\u3009 \u2212 y(i)C )2 , \u03b7R(wT \u2212 wC)\nUnder this representation, the choice of R and \u03b7 respectively determine the nature and magnitude of similarity between wT and wC . For instance, setting R = \u2016 \u00b7 \u201622 will encourage wT and wC to be close under a Euclidian metric, while setting R = \u2016 \u00b7 \u20161 will induce sparsity on w\u2206, meaning that wT and wC will be different only on a small subset of entries.\nThis gives an intuitively interpretation of our assumption on the similarity of wC and wT via w\u2206; we assume that wC models the baseline effect, while w\u2206 models the deviation of the treatment effect as expressed by wT . This aligns well with our setup. Since SC is large, it should allow for a good fit to the baseline effect of the control condition. Given this, the fewer samples in S\u2032T should now suffice to fit the deviated treatment effect. This is especially true for high-dimensional, where learning requires a large number of samples. We will return to this in Sec. 6.3;\nThe value of \u03b7 sets the de-facto linkage strength of the two loss terms in Eq. (5). Setting \u03b7 = 0 will allow wC to be arbitrarily far away from wT , which will lead to a disjoint objective - minimizing wT over S \u2032 T and wC over SC independently. On the other hand, setting \u03b7 =\u221e will constrain wT = wC and hence revert the objective back to Eq. (2).\nWhile \u03b7 controls the relation between wT and wC , \u03b3 signifies the importance of each sample set for training wT to generalize well to the treatment variable. While S\u2032T contains actual treatment labels but is small, SC is sufficiently large but contains only control labels (used as proxies for the treatment variables). The purpose of \u03b3 is therefore to allow us to balance these complementary properties. Setting \u03b3 = 1 will revert the objective back to Eq. 1, while setting \u03b3 = 0 will result in a training objective based only on SC . In effect, the above notions model our belief in how (and how well) y\u0303T serves as a proxy for yT ."}, {"heading": "6. EXTENSIONS", "text": "In the above section, we presented our method for a regression task under a squared loss function and an `2 regularization term. Note however that our only modeling assumption was that both yT and yC (and accordingly y\u2206)\n3This is similar to the regularization term of the Fused Lasso approach [19] used for time-series prediction.\nadmitted to linear predictors under some joint feature representation. This simple assumption allows us to apply our method to more general settings, provide a closed-form solution for some cases."}, {"heading": "6.1 Linear predictors", "text": "An immediate conclusion from the above is that our method applies to any general loss function L(\u3008w, x\u3009, y) defined over a linear predictor, and to any regularization term Q(w) of the predictor\u2019s parameters. The general form of the training objective in Eq. (5) for linear predictors is given by:\nmin wT ,w\u2206\n\u03b3\nMT \u2211 i\u2208S\u2032\nT\nL ( \u3008wT , x(i)\u3009, y(i)T ) +\n(1\u2212 \u03b3) M \u2211 i\u2208SC L ( \u3008wT \u2212 w\u2206, x(i)\u3009, y(i)C ) +\n\u03bbQ(wT ) + \u03b7R(w\u2206) (6)\nAs we do not make assumptions regarding the nature of the labels, Eq. (6) is not restricted to regression, and hence directly applies to binary classification. For an appropriate definition of y\u2206 = yT \u2212 yC , Eq. (6) can also be applied in principle to multi-class and multi-label classification and to structured prediction. However, note that in such classification settings, the interpretation of y\u2206 as the individual treatment effect no longer holds. For instance, for a margin-based optimization approach for binary classification, y\u2206 signifies the difference in distances to the margin, rather than the difference in the actual outcome. For other tasks the role of the regularization term R may also change."}, {"heading": "6.2 Closed form solution", "text": "When applying our method to ridge regression (as in the example in Sec. 5), setting R(\u00b7) = \u2016 \u00b7 \u201622 allows for a closed form solution of the objective in Eq. (5). This is accomplished by transforming the objective into a canonical ridge regression form:\nmin w \u2016w>X \u2212 Y \u201622 + \u03b1\u2016w\u201622 (7)\nfor which the solution is:\nw\u0302 = (X>X + \u03b1I)\u22121X>Y (8)\nWe now show how to construct the data matrix X, label vector Y , and regularization constant \u03b1, so that the minimizer of Eq. (5) can be extracted from w\u0302.\nSince the objective in Eq. (5) includes the minimization over both wT and w\u2206, we first set w to be their concatenation, namely w = (wT , w\u2206) \u2208 R2d. Under this expanded representation, we next set:\ni \u2208 S\u2032T : Xi\u00b7 = c1\u00b7 (x(i), 0) Yi = c1yT (i) i \u2208 SC : Xi\u00b7 = c2\u00b7 (x(i),\u2212c3x(i)), Yi = c2yC(i)\n(9)\nwhere 0 is a vector of zeros of size d, and the constants are: c1 = \u221a \u03b3/MT , c2 = \u221a (1\u2212 \u03b3)/M, c3 = \u221a \u03bb/\u03b7\nFinally, letting \u03b1 = \u03bb and plugging into Eq. (8) gives the solution for wT and w\u2206 of our original objective in Eq. (5)."}, {"heading": "6.3 Non-linear predictors", "text": "While linear predictors are easy to work with and often work well in practice, they lack the expressive power that\nnon-linear predictors offer. As our method is not constrained to a specific representation, a straightforward way for incorporating non-linearity is via kernels, as we describe next.\nIn Sec. 6.2, the construction in Eq. (9) shows how regularizing of w\u2206 can be achieved by a simple expansion of the feature representation. A similar procedure can be applied to a more general case, specifically when R,Q decompose and R = Q. For \u03b3 = 1/2,4 setting the expanded features \u03c6(x) = (x,0) for x \u2208 S\u2032T and \u03c6(x) = (x,\u2212cx) for x \u2208 SC with c = \u221a \u03bb/\u03b7 allows for R and Q to share a single constant \u03bb, and due to decomposability define a single regularization function over the new expanded model w\u0303 \u2208 R2d.\nSince above holds for any feature representation \u03d5(x), kernel-supporting methods can be readily applied. For a linear kernel K(x, x\u2032) = \u3008x, x\u2032\u3009, the expanded kernel K\u0304 is:\nK\u0304(x, x\u2032) = \u3008\u03c6(x), \u03c6(x)\u2032\u3009 = g(x, x\u2032)\u00b7K(x, x\u2032),\ng(x, x\u2032) = { c2 x, x\u2032 \u2208 SC 1 o.w.\n(10)\nAs kernels are closed under addition, for a general feature representation \u03d5 we have:\nK\u0304(x, x\u2032) = g(x, x\u2032)\u00b7 \u3008\u03d5(x), \u03d5(x\u2032)\u3009 (11)\nHence, our method can be applied to wide class of regularized kernel methods, such as kernel ridge regression, SVMs, SVRs, and others.\nAs the dimension of \u03d5 typically used in kernels is very large or even infinite, they require a considerable number of samples to learn properly. This is also true for many other non-linear predictors. This makes using kernels only on the small S\u2032T unrealistic, while applying them to SC alone is suboptimal. As mentioned in Sec. 5, our method should allow for using the large number of samples in SC to learn the baseline effect of the control condition using kernels, while still taking advantage of the treatment-labeled samples in S\u2032T .\nFinally, we note that since many non-linear deep architectures include a linear output layer, our method can potentially be applied to such. In a similar fashion to the construction in Sec. 6.2, such an architecture should include two linear output layers - one for wT and one for w\u2206 - and corresponding regularization terms. We leave the exploration of such an approach for future work.\n4General values of \u03b3 can be incorporated into losses which support differential sample weights."}, {"heading": "7. EXPERIMENTS", "text": "In this section we evaluate the performance of our method on three counterfactual prediction tasks: A simulated medical clinical trial, a web search engine experiment, and a social choice question. Since our learning goals include predictions regarding the treatment variable yT , our data must contain a large pool of ground-truth labels for this class. This is a necessary condition for ensuring a valid estimation procedure. Unfortunately, for the same reasons that motivate our work, most datasets do not include many labeled treatment instances, as they are typically hard, expensive, and time consuming to acquire.\nTo this end, we focus on three datasets. The first dataset contains information on the clinical status of approximately 100,000 diabetes patients. Our task is to predict the length of hospitalization for each patient, given their treatments so far.\nA second dataset comprises of a large collection of around 20,000 houses along with their attributes. Our task is to estimate the market price of a house given its attributes. While the dataset itself was not collected by a randomized trial procedure, we partition the recordings into control and treatment conditions in a way which emulates a realistic controlled trial scenario. This allows us to validate our predictions on the treatment variable.\nThe third dataset is from the domain of search engine operation. Search engines regularly modify and improve their ranking algorithm and other parameters such as the user interface, in many cases based on the results of a large number of A/B tests, where the current ranking algorithm is compared to a new alternative. In this setting, early and accurate predictions of query-centric measures like the clickthrough rate (CTR) and its derivatives are of great importance. Thus, we focus on this prediction, which can also assist in early termination of treatments which are predicted to be as good as (or worse than) the current treatment.\nAt its core, our method provides a way to model the linkage between a small randomized trial and a large historical dataset. Our goal in this section is therefore to evaluate the added benefit of using our model when such data is available. In Sec. 6, we describe why and how our method can be applied to a large set of loss functions and predictors. To this end, in this section we compare the performance of our method to methods which simply aggregate both datasets, while keeping the loss and predictor class fixed. Specifically, we compare our method to training only on S\u2032T , only on SC , and on both sets ST \u2032\u222aC = S \u2032 T \u222aSC . As for other linear methods described in Sec. 2, [17, 18] assume that samples include loss terms and propensity scores, while the linear method in [7] does not outperform standard ridge regression.\nWe evaluated performance on two tasks: predicting individual treatment outcomes (yT ), and predicting the individual treatment effect (y\u2206).\nAs mentioned, all methods were evaluated on a held-out test set with treatment labels, were results were averaged over 10 random instantiations. For all tasks we used ridge regression as a learning objective, and applied `2 regularization for our method. Meta-parameters were chosen on a small held-out validation set. Our high-end benchmark for performance is based on learning over a large treatmentlabeled training set ST . We note again that this type of data is typically unrealistic to attain, and hence serves as an empirical upper bound on overall predictive accuracy."}, {"heading": "7.1 Hospitalization of diabetes patients", "text": "This dataset contains data from 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks [16]. We attempt to predict the length of hospitalization (in days), or (for the classification task) whether the length of hospitalization would be longer than the median hospitalization length.\nThe \u2019new\u2019 treatment which we attempt to estimate is whether prescription of diabetes medications prior to hospitalization would have changed the hospitalization length. We focused on patients for which the reason of admission was unknown, as these represent the difficult cases, of which there were 4,785 patients in the data. We simulated a clinical trial by randomly selecting 25% of the population into X \u2032, some of whom were prescribed diabetes medications, and some who were not. We used 25:75 train-test splits.\nWe evaluate performance on two tasks: predicting the hospitalization length, and predicting whether the length was above the median. The results of these experiments are shown in Table 1. As the results show, our methods provides better estimation of the treatment effect, compared to methods which are based on subsets of available data. Moreover, this prediction is close in its quality to that achieved by a learner which uses the actual treatment information."}, {"heading": "7.2 House pricing Dataset", "text": "The House Sales in King County dataset5 contains records of 21,613 houses sold in King County, USA, a region which includes Seattle. Along with the market price of each house, the data includes 19 numerical and categorical attributes for each house including the number and types of rooms, size, number of floors, and geographic location. Of special in-\n5https://www.kaggle.com/harlfoxem/housesalesprediction\nterest is an attribute which determines whether the house was renovated or not. By considering this as a treatment indicator variable and partitioning accordingly, we can simulate the following counterfactual question: Does renovating increase a house\u2019s value, and by how much?\nAs houses were not randomly assigned to each condition, the data does not represent a true randomized controlled trial. This of course raises questions as to whether predictions can be used to answer the above question. Nonetheless, our method still applies here, as we do not assume a random assignment, but rather use it as motivation.\nWe evaluated performance on two tasks: a regression task in which we predict the value of a house, and a classification task in which we predict whether the value is in the top decile. In both tasks we assign all houses which did not undergo renovation to the control condition, and all those which did to the treatment condition. These amounted to 20,699 and 914 houses, respectively. We used 75:25 traintest splits, and set the experimental sub-population X \u2032 to be all houses in a random subset of zip codes, representing about 25% of all zip codes. This is similar to a setting where only certain areas residential areas participate in a survey.\nResults for all tasks are presented in Table 2. We report the mean R2 for the task of predicting a house\u2019s value, and mean accuracy for predicting the attribution to the top decile. Results show that the highest accuracy in both prediction tasks is obtained by the proposed method (denoted as \u2206 in the table). Moreover, these accuracies are within a few percentage points from the accuracy obtained when using data from benchmark dataset. Thus, the proposed method can replace the use of a large RCT, which would be expensive and difficult to execute, in this kind of setting."}, {"heading": "7.3 Search Engine Dataset", "text": "We collected queries submitted to the Bing search engine on June 1st 2016 which were randomly assigned to internal A/B tests, and whose frequency was at least 1,000. As examples (x) we took all of the queries that appeared in the control condition, and focused on tests which included all of these queries. Our dataset contains 277 comparable treatment conditions and one control condition, each with 1,572 distinct query examples, for a total of 437,016 instances.\nFeatures included both categories of the queries and features of the query words. Categorization was determined using a proprietary classifier [24] developed by the Microsoft Bing team to assign each query into a set of 63 categories, including, for example, commerce, tourism, video games,\nweather-related, and adult-themed queries. The classifier is used by Bing to determine whether to display special results such as instant answers. Queries can be classified into multiple categories (e.g., purchase of flight tickets would be classified into both tourism and commerce). Word-based features included some basic attributes such as the number of characters, number of tokens, minimal and maximal token size, and a numeric token indicator. In addition, a bag-of-words representation of the tokens was computed, and applied using the feature hashing trick [22].\nTo generate labels, for each query x and for each experimental condition, we computed CTR estimates by pooling all relevant query instances. Since the distribution of CTR is highly skewed, and since some queries have CTR= 0, we set label values to y = log(CTR + ) for = 10\u221210. The above process ensured that for a given x our data included both yC and yT , from which y\u2206 was computed.\nA distinct characteristic of this dataset is that for most search phrases x, the data includes both yC (the CTR under the default ranker) and yT (the CTR under the new ranker). This is because, for common search phrases, responses will be recorded under both control and treatment conditions. The above allows us to directly evaluate the individual treatment effect y\u2206. Moreover, since the data includes a large collection of alternative rankers (but only one default ranker), we can compare the effect of many treatments to the same control condition.\nRunning search engine A/B tests is an expensive procedure. Tests are therefore often limited in time and resources, and are allocated only a small fraction of the overall traffic. This causes CTR estimates to be unrepresentative, as high-frequency queries will be assigned to an A/B test more often than low-frequency queries, which may not appear in trials at all. Therefore, our proposed algorithm can potentially shorten A/B test by reaching a conclusion as to the benefit of a new ranking algorithm using only the popular queries (which are easy to collect), but inferring the benefit for rare queries as well, solely based on a complete historical record of the control condition and a small random trial over a non-representative population (e.g., popular queries).\nSince our estimation procedure requires a full set of groundtruth treatment labels, we can use only query instances which participated in trials. This requires us to mimic the above setting using A/B test data alone, by constraining S\u2032T to contain only queries whose frequency is in the top \u03c4 th-quantile. For instance, by setting \u03c4 = 0.75, we guarantee that S\u2032T contains only queries with frequency in the top quartile. To keep S\u2032T small, we further discard a random 25% of the qualified examples.\nFor comparison of the proposed method we use the results of learning on ST with \u03c4 = 0.75. This means learning on most of the available data, necessitating a long A/B test to collect the rarer queries. We refer to this learning task as the benchmark.\nResults for all prediction tasks appear in Table 3. For individualized treatment outcomes and effects we report the mean R2 and its fraction of the benchmark. As can be seen, our method significantly outperforms learning using the available subsets of the data by a significant margin. Indeed, the accuracy of the proposed algorithm is not far from that of the benchmark, reaching approximately 90% or more of the potential accuracy for both tasks.\nTo explore the effect of trial duration (represented here\nby thresholding frequency), we repeated the above procedure for various values of \u03c4 . To accentuate results, we focus on the the top 10% of trials for which the difference between conditions was a-priori most significant, and employed a 50:50 train-test ratio. Results are presented in Figure 2. Our method enjoys a fast growth rate in accuracy, and should potentially allow for shorter trial lengths, or for early stopping of trials, when the new treatment is deemed to be inferior to existing treatments."}, {"heading": "8. DISCUSSION", "text": "Randomized controlled trials (RCTs) are the gold standard for testing new treatments and interventions. RCTs are widely used by Internet websites, by medical authorities, and, increasingly, by governments. However, RCTs are difficult and expensive to run. Nowadays, historical data is available in many settings where RCTs are considered. However, as these data were collected using an existing policy, utilizing these data has proven difficult.\nIn this paper we proposed a new algorithm for using historical data in conjunction with the results of small RCTs, to counterfactually infer the outcomes of large RCTs. Our method can additionally be used as an early stopping criterion for RCTs, when the method predicts that the benefit of a new treatment will not be larger than those of the existing treatment. Thus, our method can provide benefit to existing RCTs.\nThe proposed method is based on two assumptions: The first is that the outcome of each treatment can be predicted using a linear predictor. The second is that the difference between the predictor of the current treatment and the proposed treatment is not large. In Sec. 6.3 we showed extensions to the method which overcome the first assumption. Moreover, we hypothesize that predictions can be improved by using robust regression, or through inclusion of a confidence measure for each point in our data. Such extensions are left for future work."}, {"heading": "9. REFERENCES", "text": "[1] H. Bang and J. M. Robins. Doubly robust estimation\nin missing data and causal inference models. Biometrics, 61(4):962\u2013973, 2005.\n[2] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151\u2013175, 2010.\n[3] S. Bickel, J. Bogojeska, T. Lengauer, and T. Scheffer. Multi-task learning for hiv therapy screening. In Proceedings of the 25th international conference on Machine learning, pages 56\u201363. ACM, 2008.\n[4] L. Bottou, J. Peters, J. Q. Candela, D. X. Charles, M. Chickering, E. Portugaly, D. Ray, P. Y. Simard, and E. Snelson. Counterfactual reasoning and learning systems: the example of computational advertising. Journal of Machine Learning Research, 14(1):3207\u20133260, 2013.\n[5] H. A. Chipman, E. I. George, and R. E. McCulloch. Bart: Bayesian additive regression trees. The Annals of Applied Statistics, pages 266\u2013298, 2010.\n[6] L. Haynes, O. Service, B. Goldacre, and D. Torgerson. Test, learn, adapt: Developing public policy with randomised controlled trials, 2012.\n[7] F. D. Johansson, U. Shalit, and D. Sontag. Learning representations for counterfactual inference. In Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 3020\u20133029, 2016.\n[8] R. Kohavi, A. Deng, B. Frasca, R. Longbotham, T. Walker, and Y. Xu. Trustworthy online controlled experiments: Five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 786\u2013794. ACM, 2012.\n[9] R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne. Controlled experiments on the web: survey and practical guide. Data mining and knowledge discovery, 18(1):140\u2013181, 2009.\n[10] L. Li, S. Chen, J. Kleban, and A. Gupta. Counterfactual estimation and optimization of click metrics for search engines. arXiv preprint arXiv:1403.1891, 2014.\n[11] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM, 2010.\n[12] L. Li, J. Y. Kim, and I. Zitouni. Toward predicting the outcome of an a/b experiment for search relevance. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, pages 37\u201346. ACM, 2015.\n[13] J. Pearl. Causality. Cambridge university press, 2009.\n[14] D. B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology, 66(5):688, 1974.\n[15] B. Scho\u0308lkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. M. Mooij. On causal and anticausal learning. In Proceedings of the 29th International Conference on Machine Learning, ICML, 2012.\n[16] B. Strack, J. P. DeShazo, C. Gennings, J. L. Olmo, S. Ventura, K. J. Cios, and J. N. Clore. Impact of\nhba1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records. BioMed research international, 2014, 2014.\n[17] A. Swaminathan and T. Joachims. Counterfactual risk minimization. In Proceedings of the 24th International Conference on World Wide Web, pages 939\u2013941. ACM, 2015.\n[18] A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In Advances in Neural Information Processing Systems, pages 3231\u20133239, 2015.\n[19] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91\u2013108, 2005.\n[20] M. J. van der Laan and M. L. Petersen. Causal effect models for realistic individualized treatment and intention to treat rules. The International Journal of Biostatistics, 3(1), 2007.\n[21] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using random forests. arXiv preprint arXiv:1510.04342, 2015.\n[22] K. Q. Weinberger, A. Dasgupta, J. Langford, J. Attenberg, and A. J. Smola. Feature hashing for large scale multitask learning. In Proceedings of the 26th International Conference on Machine Learning (ICML-09), page 140, 2009.\n[23] J. Weiss, F. Kuusisto, K. Boyd, J. Liu, and D. Page. Machine learning for treatment assignment: Improving individualized risk attribution. In AMIA Annual Symposium Proceedings, volume 2015, page 1306. American Medical Informatics Association, 2015.\n[24] E. Yom-Tov, R. W. White, and E. Horvitz. Seeking insights about cycling mood disorders via anonymized search logs. Journal of medical Internet research, 16(2):e65, 2014."}], "references": [{"title": "Doubly robust estimation in missing data and causal inference models", "author": ["H. Bang", "J.M. Robins"], "venue": "Biometrics, 61(4):962\u2013973", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine learning, 79(1-2):151\u2013175", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task learning for hiv therapy screening", "author": ["S. Bickel", "J. Bogojeska", "T. Lengauer", "T. Scheffer"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 56\u201363. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Counterfactual reasoning and learning systems: the example of computational advertising", "author": ["L. Bottou", "J. Peters", "J.Q. Candela", "D.X. Charles", "M. Chickering", "E. Portugaly", "D. Ray", "P.Y. Simard", "E. Snelson"], "venue": "Journal of Machine Learning Research, 14(1):3207\u20133260", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Bart: Bayesian additive regression trees", "author": ["H.A. Chipman", "E.I. George", "R.E. McCulloch"], "venue": "The Annals of Applied Statistics, pages 266\u2013298", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Test", "author": ["L. Haynes", "O. Service", "B. Goldacre", "D. Torgerson"], "venue": "learn, adapt: Developing public policy with randomised controlled trials", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning representations for counterfactual inference", "author": ["F.D. Johansson", "U. Shalit", "D. Sontag"], "venue": "Proceedings of the 33nd International Conference on Machine Learning, ICML, pages 3020\u20133029", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Trustworthy online controlled experiments: Five puzzling outcomes explained", "author": ["R. Kohavi", "A. Deng", "B. Frasca", "R. Longbotham", "T. Walker", "Y. Xu"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 786\u2013794. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Controlled experiments on the web: survey and practical guide", "author": ["R. Kohavi", "R. Longbotham", "D. Sommerfield", "R.M. Henne"], "venue": "Data mining and knowledge discovery, 18(1):140\u2013181", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Counterfactual estimation and optimization of click metrics for search engines", "author": ["L. Li", "S. Chen", "J. Kleban", "A. Gupta"], "venue": "arXiv preprint arXiv:1403.1891", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward predicting the outcome of an a/b experiment for search relevance", "author": ["L. Li", "J.Y. Kim", "I. Zitouni"], "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, pages 37\u201346. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Causality", "author": ["J. Pearl"], "venue": "Cambridge university press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "author": ["D.B. Rubin"], "venue": "Journal of educational Psychology, 66(5):688", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1974}, {"title": "On causal and anticausal learning", "author": ["B. Sch\u00f6lkopf", "D. Janzing", "J. Peters", "E. Sgouritsa", "K. Zhang", "J.M. Mooij"], "venue": "Proceedings of the 29th International Conference on Machine Learning, ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "and J", "author": ["B. Strack", "J.P. DeShazo", "C. Gennings", "J.L. Olmo", "S. Ventura", "K.J. Cios"], "venue": "N. Clore. Impact of  hba1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records. BioMed research international, 2014", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Counterfactual risk minimization", "author": ["A. Swaminathan", "T. Joachims"], "venue": "Proceedings of the 24th International Conference on World Wide Web, pages 939\u2013941. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "The self-normalized estimator for counterfactual learning", "author": ["A. Swaminathan", "T. Joachims"], "venue": "Advances in Neural Information Processing Systems, pages 3231\u20133239", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91\u2013108", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Causal effect models for realistic individualized treatment and intention to treat rules", "author": ["M.J. van der Laan", "M.L. Petersen"], "venue": "The International Journal of Biostatistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Estimation and inference of heterogeneous treatment effects using random forests", "author": ["S. Wager", "S. Athey"], "venue": "arXiv preprint arXiv:1510.04342", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature hashing for large scale multitask learning", "author": ["K.Q. Weinberger", "A. Dasgupta", "J. Langford", "J. Attenberg", "A.J. Smola"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML-09), page 140", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine learning for treatment assignment: Improving individualized risk attribution", "author": ["J. Weiss", "F. Kuusisto", "K. Boyd", "J. Liu", "D. Page"], "venue": "AMIA Annual Symposium Proceedings, volume 2015, page 1306. American Medical Informatics Association", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Seeking insights about cycling mood disorders via anonymized search logs", "author": ["E. Yom-Tov", "R.W. White", "E. Horvitz"], "venue": "Journal of medical Internet research, 16(2):e65", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "The gold standard for testing such interventions are randomized controlled trials (RCTs) [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "1145/1235 used in medicine: Approximately 200,000 RCTs were conducted in the 1990\u2019s alone [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Internet website operators were early adopters of RCTs [9].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Most large Internet companies are known to run thousands of RCTs every year [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "The latter objective has been the focus of an abundant body of works, most based on the framework of causal inference [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "Our work follows the more recent line of work where a discriminative loss-centric approach is applied in counterfactual settings [17, 18, 7].", "startOffset": 129, "endOffset": 140}, {"referenceID": 17, "context": "Our work follows the more recent line of work where a discriminative loss-centric approach is applied in counterfactual settings [17, 18, 7].", "startOffset": 129, "endOffset": 140}, {"referenceID": 6, "context": "Our work follows the more recent line of work where a discriminative loss-centric approach is applied in counterfactual settings [17, 18, 7].", "startOffset": 129, "endOffset": 140}, {"referenceID": 12, "context": "Causal inference [13] is a standard framework for estimating the causal relation between variables, in a way which can then be used to answer counterfactual questions.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "In order to achieve this, methods for causal inference are usually based on simple, interpretable models from which actionable conclusions can be drawn [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 10, "context": "Alternatively, counterfactual questions can be answered if data can be collected under a random policy [11, 4, 10].", "startOffset": 103, "endOffset": 114}, {"referenceID": 3, "context": "Alternatively, counterfactual questions can be answered if data can be collected under a random policy [11, 4, 10].", "startOffset": 103, "endOffset": 114}, {"referenceID": 9, "context": "Alternatively, counterfactual questions can be answered if data can be collected under a random policy [11, 4, 10].", "startOffset": 103, "endOffset": 114}, {"referenceID": 11, "context": "Because of this, it has been proposed to treat data collected under different settings as randomized, and use it to answer counterfactual questions [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 16, "context": "In analogy to Empirical Risk Minimization, the principle of Countefactual Risk Minimization is proposed in [17, 18].", "startOffset": 107, "endOffset": 115}, {"referenceID": 17, "context": "In analogy to Empirical Risk Minimization, the principle of Countefactual Risk Minimization is proposed in [17, 18].", "startOffset": 107, "endOffset": 115}, {"referenceID": 0, "context": "Other works use doubly-robust methods which are based on propensity scores as well [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Some parametric non-linear methods for estimating treatment effect are based on Bayesian regression trees [5] and random forests [21].", "startOffset": 106, "endOffset": 109}, {"referenceID": 20, "context": "Some parametric non-linear methods for estimating treatment effect are based on Bayesian regression trees [5] and random forests [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "A parallel discriminative approach to counterfactual prediction is based on the notion of domain adaptation [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 14, "context": "Following the work of [15], the authors of [7] observe that generalizing from the observed factual distribution to the unobserved counterfactual distribution is a special case of covariance shift, and in general of domain adaptation.", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": "Following the work of [15], the authors of [7] observe that generalizing from the observed factual distribution to the unobserved counterfactual distribution is a special case of covariance shift, and in general of domain adaptation.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "Therefore, the non-convex representation learning method in [7] incorporates a discrepancy-based regularization term which encourages a label-invariant representation.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Moreover, while the method of [7] requires large amounts of data for both labels, our method is tailored for a setting where the treatment variable is rare.", "startOffset": 30, "endOffset": 33}, {"referenceID": 22, "context": "This is a fundamental problem in counterfactual settings, and makes estimating the individual treatment effect y\u2206 = yT \u2212yC especially challenging [23].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "A related task that is of high interest is to predict the individualized treatment effect y\u2206 = yT \u2212yC [14, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 19, "context": "A related task that is of high interest is to predict the individualized treatment effect y\u2206 = yT \u2212yC [14, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 0, "context": "where \u03b3 \u2208 [0, 1] controls the relative weight of each dataset in the training objective.", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This is in contrast to the more conventional approach where the distribution is modeled using tuples of the form (x, \u03bd, y\u03bd), where \u03bd \u2208 {C, T} is the experimental condition and y\u03bd is the outcome under that condition [7].", "startOffset": 215, "endOffset": 218}, {"referenceID": 18, "context": "This is similar to the regularization term of the Fused Lasso approach [19] used for time-series prediction.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "2, [17, 18] assume that samples include loss terms and propensity scores, while the linear method in [7] does not outperform standard ridge regression.", "startOffset": 3, "endOffset": 11}, {"referenceID": 17, "context": "2, [17, 18] assume that samples include loss terms and propensity scores, while the linear method in [7] does not outperform standard ridge regression.", "startOffset": 3, "endOffset": 11}, {"referenceID": 6, "context": "2, [17, 18] assume that samples include loss terms and propensity scores, while the linear method in [7] does not outperform standard ridge regression.", "startOffset": 101, "endOffset": 104}, {"referenceID": 15, "context": "This dataset contains data from 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "Categorization was determined using a proprietary classifier [24] developed by the Microsoft Bing team to assign each query into a set of 63 categories, including, for example, commerce, tourism, video games,", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "In addition, a bag-of-words representation of the tokens was computed, and applied using the feature hashing trick [22].", "startOffset": 115, "endOffset": 119}], "year": 2016, "abstractText": "When a new treatment is considered for use, whether a pharmaceutical drug or a search engine ranking algorithm, a typical question that arises is, will its performance exceed that of the current treatment? The conventional way to answer this counterfactual question is to estimate the effect of the new treatment in comparison to that of the conventional treatment by running a controlled, randomized experiment. While this approach theoretically ensures an unbiased estimator, it suffers from several drawbacks, including the difficulty in finding representative experimental populations as well as the cost of running such trials. Moreover, such trials neglect the huge quantities of available control-condition data which are often completely ignored. In this paper we propose a discriminative framework for estimating the performance of a new treatment given a large dataset of the control condition and data from a small (and possibly unrepresentative) randomized trial comparing new and old treatments. Our objective, which requires minimal assumptions on the treatments, models the relation between the outcomes of the different conditions. This allows us to not only estimate mean effects but also to generate individual predictions for examples outside the randomized sample. We demonstrate the utility of our approach through experiments in three areas: Search engine operation, treatments to diabetes patients, and market value estimation for houses. Our results demonstrate that our approach can reduce the number and size of the currently performed randomized controlled experiments, thus saving significant time, money and effort on the part of practitioners.", "creator": "TeX"}}}