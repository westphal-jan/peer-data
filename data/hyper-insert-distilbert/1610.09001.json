{"id": "1610.09001", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "SoundNet: Learning Sound Representations from Unlabeled Video", "abstract": "we learn how rich natural sonic sound representations arising by inadvertently capitalizing on their large amounts, of unlabeled sound data partially collected accidentally in the wild. theoretically we generate leverage to the natural synchronization between vision itself and sound to learn whether an acoustic information representation using each two - million binary unlabeled videos. unlabeled video synthesis has held the advantage asserting that audio it demonstrates can itself be economically acquired at massive theoretical scales, intact yet lacks contains useful signals about natural audible sound. we both propose a student - teacher developmental training procedure which inherently transfers discriminative synthetic visual audio knowledge from four well prior established visual phrase recognition behavioral models existing into evolving the sound modality using unlabeled video as a bridge. adapting our customized sound representation yields significant performance improvements over presenting the baseline state - of - the - art results on standard benchmarks for acoustic reference scene / object classification. visualizations thus suggest ensuring some potential high - standard level behavioral semantics automatically emerge in the default sound network, namely even though it is generally trained without ground truth labels.", "histories": [["v1", "Thu, 27 Oct 2016 20:23:39 GMT  (5939kb,D)", "http://arxiv.org/abs/1610.09001v1", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.SD", "authors": ["yusuf aytar", "carl vondrick", "antonio torralba 0001"], "accepted": true, "id": "1610.09001"}, "pdf": {"name": "1610.09001.pdf", "metadata": {"source": "CRF", "title": "SoundNet: Learning Sound Representations from Unlabeled Video", "authors": ["Yusuf Aytar", "Carl Vondrick"], "emails": ["yusuf@csail.mit.edu", "vondrick@mit.edu", "torralba@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35]. However, there has not yet been the same corresponding progress in natural sound understanding tasks. We attribute this partly to the lack of large labeled datasets of sound, which are often both expensive and ambiguous to collect. We believe that large-scale sound data can also significantly advance natural sound understanding. In this paper, we leverage over one year of sounds collected in-the-wild to learn semantically rich sound representations.\nWe propose to scale up by capitalizing on the natural synchronization between vision and sound to learn an acoustic representation from unlabeled video. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about sound. Recent progress in computer vision has enabled machines to recognize scenes and objects in images and videos with good accuracy. We show how to transfer this discriminative visual knowledge into sound using unlabeled video as a bridge.\nWe present a deep convolutional network that learns directly on raw audio waveforms, which is trained by transferring knowledge from vision into sound. Although the network is trained with visual supervision, the network has no dependence on vision during inference. In our experiments, we show that the representation learned by our network obtains state-of-the-art accuracy on three standard acoustic scene classification datasets. Since we can leverage large amounts of unlabeled sound data, it is feasible to train deeper networks without significant overfitting, and our experiments suggest deeper models perform better. Visualizations of the representation suggest that the network is also learning high-level detectors, such as recognizing bird chirps or crowds cheering, even though it is trained directly from audio without ground truth labels.\n\u2217contributed equally\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 00\n1v 1\n[ cs\n.C V\n] 2\n7 O\nct 2\nThe primary contribution of this paper is the development of a large-scale and semantically rich representation for natural sound. We believe large-scale models of natural sounds can have a large impact in many real-world applications, such as robotics and cross-modal understanding. The remainder of this paper describes our method and experiments in detail. We first review related work. In section 2, we describe our unlabeled video dataset and in section 3 we present our network and training procedure. Finally in section 4 we conclude with experiments on standard benchmarks and show several visualizations of the learned representation. Code, data, and models will be released."}, {"heading": "1.1 Related Work", "text": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds. Acoustic scene classification, classifying sound excerpts into existing acoustic scene/object categories, is predominantly based on applying a variety of general classifiers (SVMs, GMMs, etc.) to the manually crafted sound features (MFCC, spectrograms, etc.) [4, 29, 21, 30, 34, 32]. Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data. We distinguish ourselves from the existing literature by training a deep fully convolutional network on a large scale dataset (2M videos). This allows us to train much deeper networks. Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42]. [26] also investigates the relation between vision and sound modalities, but focuses on producing sound from image sequences. Concurrent work [11] also explores video as a form of weak labeling for audio event classification.\nTransfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17]. Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13]. In [3, 13] the basic idea is to compress (i.e. transfer) discriminative knowledge from a well-trained complex model to a simpler model without loosing considerable accuracy. In [3] and [13] both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound. [9] also transfer visual supervision into depth models.\nCross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39]. In this work, we leverage the natural synchronization between vision and sound to learn a deep representation of natural sounds without ground truth sound labels."}, {"heading": "2 Large Unlabeled Video Dataset", "text": "We seek to learn a representation for sound by leveraging massive amounts of unlabeled videos. While there are a variety of sources available on the web (e.g., YouTube, Flickr), we chose to use videos from Flickr because they are natural, not professionally edited, short clips that capture various sounds in everyday, in-the-wild situations. We downloaded over two million videos from Flickr by querying for popular tags [36] and dictionary words, which resulted in over one year of continuous natural sound and video, which we use for training. The length of each video varies from a few seconds to several minutes. We show a small sample of frames from the video dataset in Figure 2.\nWe wish to process sound waves in the raw. Hence, the only post-processing we did on the videos was to convert sound to MP3s, reduce the sampling rate to 22 kHz, and convert to single channel audio. Although this slightly degrades the quality of the sound, it allows us to more efficiently operate on large datasets. We also scaled the waveform to be in the range [\u2212256, 256]. We did not need to subtract the mean because it was naturally near zero already."}, {"heading": "3 Learning Sound Representations", "text": ""}, {"heading": "3.1 Deep Convolutional Sound Network", "text": "Convolutional Network: We present a deep convolutional architecture for learning sound representations. We propose to use a series of one-dimensional convolutions followed by nonlinearities (i.e. ReLU layer) in order to process sound. Convolutional networks are well-suited for audio signals for a couple of reasons. Firstly, like images [19], we desire our network to be invariant to translations, a property that reduces the number of parameters we need to learn and increases efficiency. Secondly, convolutional networks allow us to stack layers, which enables us to detect higher-level concepts through a series of lower-level detectors.\nVariable Length Input/Output: Since sound can vary in temporal length, we desire our network to handle variable-length inputs. To do this, we use a fully convolutional network. As convolutional layers are invariant to location, we can convolve each layer depending on the length of the input. Consequently, in our architecture, we only use convolutional and pooling layers. Since the representation adapts to the input length, we must design the output layers to work with variable length inputs as well. While we could have used a global pooling strategy [37] to down-sample variable length inputs to a fixed dimensional vector, such a strategy may unnecessarily discard information useful for high-level representations. Since we ultimately aim to train this network with video, which is also variable length, we instead use a convolutional output layer to produce an output over multiple timesteps in video. This strategy is similar to a spatial loss in images [22], but instead temporally.\nNetwork Depth: Since we will use a large amount of video to train, it is feasible to use deep architectures without significant over-fitting. We experiment with both five-layer and eight-layer networks.\nWe visualize the eight-layer network architecture in Figure 1, which conists of 8 convolutional layers and 3 max-pooling layers. We show the layer configuration in Table 1 and Table 2."}, {"heading": "3.2 Visual Transfer into Sound", "text": "The main idea in this paper is to leverage the natural synchronization between vision and sound in unlabeled video in order to learn a representation for sound. We model the learning problem from a student-teacher perspective. In our case, state-of-the-art networks for vision will teach our network for sound to recognize scenes and objects.\nLet xi \u2208 RD be a waveform and yi \u2208 R3\u00d7T\u00d7W\u00d7H be its corresponding video for 1 \u2264 i \u2264 N , where W,H, T are width, height and number of sampled frames in the video, respectively. During learning, we aim to use the posterior probabilities from a teacher vision network gk(yi) in order to train our student network fk(xi) to recognize concepts given sound. As we wish to transfer knowledge from both object and scene networks, k enumerates the concepts we are transferring. During learning, we optimize min\u03b8 \u2211K k=1 \u2211N i=1DKL (gk(yi)||fk(xi; \u03b8)) whereDKL(P ||Q) = \u2211 j Pj log Pj Qj\nis the KLdivergence. While there are a variety of distance metrics we could have use, we chose KL-divergence because the outputs from the vision network gk can be interpreted as a distribution of categories. As KL-divergence is differentiable, we optimize it using back-propagation [19] and stochastic gradient descent. We transfer from both scene and object visual networks (K = 2)."}, {"heading": "3.3 Sound Classification", "text": "Although we train SoundNet to classify visual categories, the categories we wish to recognize may not appear in visual models (e.g., sneezing). Consequently, we use a different strategy to attach semantic meaning to sounds. We ignore the output layer of our network and use the internal representation as features for training classifiers, using a small amount of labeled sound data for the concepts of interest. We pick a layer in the network to use as features and train a linear SVM. For multi-class classification, we use a one-vs-all strategy. We perform cross-validation to pick the margin regularization hyperparameter. For robustness, we follow a standard data augmentation procedure where each training sample is split into overlapping fixed length sound excerpts, which we compute features on and use for training. During inference, we average predictions across all windows."}, {"heading": "3.4 Implementation", "text": "Our approach is implemented in Torch7. We use the Adam [16] optimizer and a fixed learning rate of 0.001 and momentum term of 0.9 throughout our experiments. We experimented with several batch sizes, and found 64 to produce good results. We initialized all the weights to zero mean Gaussian noise with a standard deviation of 0.01. After every convolution, we use batch normalization [15] and rectified linear activation units [17]. We train the network for 100, 000 iterations. Optimization typically took 1 day on a GPU."}, {"heading": "4 Experiments", "text": "Experimental Setup: We split the unlabeled video dataset into a training set and a held-out validation set. We use 2, 000, 000 videos for training, and the remaining 140, 000 videos for validation. After training the network, we use the hidden representation as a feature extractor for learning on smaller,\nlabeled sound only datasets. We extract features for a given layer, and train an SVM on the task of interest. For training the SVM, we use the standard training/test splits of the datasets. We report classification accuracy.\nBaselines:: In addition to published baselines on standard datasets, we explored an additional baseline trained on our unlabeled videos. We experimented using a convolutional autoencoder for sound, trained over our video dataset. We use an autoencoder with 4 encoder layers and 4 decoder layers. For the encoder layers, we used the same first four convolutional layers as SoundNet. For the decoders, we used a fractionally strided convolutional layers (in order to upsample instead of downsample). Note that we experimented with deeper autoencoders, but they performed worse. We used mean squared error for the reconstruction loss, and trained the autoencoders for several days."}, {"heading": "4.1 Acoustic Scene Classification", "text": "We evaluate the SoundNet representation for acoustic scene classification. The aim in this task is to categorize sound clips into one of the many acoustic scene categories. We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].\nDCASE[34]: One of the tasks in the Detection and Classification of Acoustic Scenes and Events Challenge (DCASE)[34] is to recognize scenes from natural sounds. In the challenge, there are 10 acoustic scene categories, 10 training examples per category, and 100 held-out testing examples. Each example is a 30 seconds audio recording. The task is to categorize natural sounds into existing 10 acoustic scene categories. Multi-class classification accuracy is used as the performance metric.\nESC-50 and ESC-10 [28]: The ESC-50 dataset is a collection of 2000 short (5 seconds) environmental sound recordings of equally balanced 50 categories selected from 5 major groups (animals, natural soundscapes, human non-speech sounds, interior/domestic sounds, and exterior/urban noises). Each category has 40 samples. The data is prearranged into 5 folds and the accuracy results are reported as the mean of 5 leave-one-fold-out evaluations. The performance of untrained human participants on this dataset is 81.3% [28]. ESC-10 is a subset of ESC-50 which consists of 10 classes (dog bark, rain, sea waves, baby cry, clock tic, person sneeze, helicopter, chainsaw, rooster, and fire cracking). The human performance on this dataset is 95.7%.\nWe have two major evaluations on this section: (a) comparison with the existing state of the art results, (b) diagnostic performance evaluation of inner layers of SoundNet as generic features\nfor this task. In DCASE we used 5 second excerpts, and in ESC datasets we used 1 second windows. In both evaluations a multi-class SVM (multiple one-vs all classifiers) is trained over extracted\nAccuracy on Comparison of SoundNet Model ESC-50 ESC-10 Loss 8 Layer, `2 Loss 47.8% 81.5%8 Layer, KL Loss 72.9% 92.2%\nTeacher Net 8 Layer, ImageNet Only 69.5% 89.8% 8 Layer, Places Only 71.1% 89.5% 8 Layer, Both 72.9% 92.2% 5 Layer, Scratch Init 65.0% 82.3% Depth and 8 Layer, Scratch Init 51.1% 75.5% Visual Transfer 5 Layer, Unlabeled Video 66.1% 86.8%\n8 Layer, Unlabeled Video 72.9% 92.2%\nTable 5: Ablation Analysis: We breakdown accuracy of various configurations using pool5 from SoundNet trained with VGG. Results suggest that deeper convolutional sound networks trained with visual supervision on unlabeled data helps recognition.\nDataset Model conv4 conv5 pool5 conv6 conv7 conv8 DCASE [34] 8 Layer, AlexNet 84% 85% 84% 83% 78% 68%8 Layer, VGG 77% 88% 88% 87% 84% 74% ESC50 [28] 8 Layer, AlexNet 66.0% 71.2% 74.2% 74% 63.8% 45.7%8 Layer, VGG 66.0% 69.3% 72.9% 73.3% 59.8% 43.7%\nTable 6: Which layer and teacher network gives better features? The performance comparison of extracting features at different SoundNet layers on acoustic scene/object classification tasks.\nSoundNet features. Same data augmentation procedure is also applied during testing and the mean score of all sound excerpts is used as the final score of a test recording for any particular category.\nComparison to State-of-the-Art: Table 3 and 4 compare recognition performance of SoundNet features versus previous state-of-the-art features on three datasets. In all cases SoundNet features outperformed the existing results by around 10%. Interestingly, SoundNet features approach human performance on ESC-10 dataset, however we stress that this dataset may be easy. We report the confusion matrix across all folds on ESC-50 in Figure 3. The results suggest our approach obtains very good performance on categories such as toilet flush (97% accuracy) or door knocks (95% accuracy). Common confusions are laughing confused as hens, foot steps confused as door knocks, and insects confused as washing machines."}, {"heading": "4.2 Ablation Analysis", "text": "To better understand our approach, we perform an ablation analysis in Table 5 and Table 6.\nComparison of Loss and Teacher Net (Table 5): We tried training with different subsets of target categories. In general, performance generally improves with increasing visual supervision. As expected, our results suggest that using both ImageNet and Places networks as supervision performs better than a single one. This indicates that progress in sound understanding may be furthered by building stronger vision models. We also experimented with using `2 loss on the target outputs instead of KL loss, which performed significantly worse.\nComparison of Network Depth (Table 5): We quantified the impact of network depth. We use five layer version of SoundNet (instead of the full eight) as a feature extractor instead. The five-layer SoundNet architecture performed 8% worse than the eight-layer architecture, suggesting depth is helpful for sound understanding. Interestingly, the five-layer network still generally outperforms previous state-of-the-art baselines, but the margin is less. We hypothesize even deeper networks may perform better, which can be trained without significant over-fitting by leveraging large amounts of unlabeled video.\nComparison of Supervision (Table 5): We also experimented with training the network without video by using only the labeled target training set, which is relatively small (thousands of examples). We simply change the network to output the class probabilities, and train it from random initialization with a cross entropy loss. Hence, the only change is that this baseline does not use any unlabeled video, allowing us to quantify the contribution of unlabeled video. The five layer SoundNet achieves slightly better results than [27] which is also a convolutional network trained with same data but with a different architecture, suggesting our five layer architecture is similar. Increasing the depth from five layers to eight layers decreases the performance from 65% to 51%, probably because it overfits to the small training set. However, when trained with visual transfer from unlabeled video, the eight layer SoundNet achieves a significant gain of around 20% compared to the five layer version. This\nsuggests that unlabeled video is a powerful signal for sound understanding, and it can be acquired at large enough scales to support training high-capacity deep networks.\nComparison of Layer and Teacher Network (Table 6): We analyze the discriminative performance of each SoundNet layer. Generally, features from the pool5 layer gives the best performance. We also compared different teacher networks for visual supervision (either VGGNet or AlexNet). The results are inconclusive on which teacher network to use: VGG is a better teacher network for DCASE while AlexNet is a better teacher network for ESC50."}, {"heading": "4.3 Multi-Modal Recognition", "text": "In order to compare sound features with visual features on scene/object categorization, we annotated additional 9,478 videos (vision+sound) which are not seen by the trained networks before. This new dataset consists of 44 categories from 6 major groups of concepts (i.e. urban, nature, work/home, music/entertainment, sports, and vehicles). It is annotated by Amazon Mechanical Turk workers. The frequency of categories depend on natural occurrences on the web, hence unbalanced.\nVision vs. Sound Embeddings: In order to show the semantic relevance of the features, we performed a two dimensional t-SNE [38] embedding and visualized our dataset in figure 4. The visual features are concatenated fc7 features of the two VGG networks trained using ImageNet and Places2 datasets. We computed the visual features from uniformly selected 4 frames for each video and computed the mean feature as the final visual representation. The sound features are the conv7 features extracted using SoundNet trained with VGG supervision. This visualizations suggests that sound features alone also contain considerable amount of semantic information.\nObject and Scene Classification: We also performed a quantitative comparison between sound features and visual features. We used 60% of our dataset for training and the rest for the testing. The chance level of the task is 2.2% and choosing always the most common category (i.e. music performance) yields 14% accuracy. Similar to acoustic scene classification methods, we trained a multi-class SVM over both sound and visual features individually and then jointly. The results are displayed in Table 7. Visual features alone obtained an accuracy of 49.4%. The SoundNet features obtained 32.4% accuracy. This suggests that even though sound is not as informative as vision, it still contains considerable amount of discriminative information. Furthermore, sound and vision together resulted in a modest improvement of 2% over vision only models."}, {"heading": "4.4 Visualizations", "text": "In order to have a better insight on what network learned, we visualize its representation. Figure 5 displays the first 16 convolutional filters applied to the raw input audio. The learned filters are diverse, including low and high frequencies, wavelet-like patterns, increasing and decreasing amplitude filters. We also visualize some of the hidden units in the last hidden layer (conv7) of our sound representation\nby finding inputs that maximally activate a hidden unit. These visualization are displayed on Figure 6. Note that visual frames are not used during computation of activations; they are only included in the figure for visualization purposes."}, {"heading": "5 Conclusion", "text": "We propose to train deep sound networks (SoundNet) by transferring knowledge from established vision networks and large amounts of unlabeled video. The synchronous nature of videos (sound + vision) allow us to perform such a transfer which resulted in semantically rich audio representations for natural sounds. Our results show that transfer with unlabeled video is a powerful paradigm for learning sound representations. All of our experiments suggest that one may obtain better performance simply by downloading more videos, creating deeper networks, and leveraging richer vision models.\nAcknowledgements: We thank MIT TIG, especially Garrett Wollman, for helping store 26 TB of video. We are grateful for the GPUs donated by NVidia. This work was supported by NSF grant #1524817 to AT and the Google PhD fellowship to CV."}], "references": [{"title": "Tabula rasa: Model transfer for object category detection", "author": ["Yusuf Aytar", "Andrew Zisserman"], "venue": "In ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Part level transfer regularization for enhancing exemplar svms", "author": ["Yusuf Aytar", "Andrew Zisserman"], "venue": "CVIU,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Acoustic scene classification: Classifying environments from the sounds", "author": ["Daniele Barchiesi", "Dimitrios Giannoulis", "Dan Stowell", "Mark D Plumbley"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "In ISMIR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["Emre Cakir", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "venue": "In IJCNN,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Learning aligned cross-modal representations from weakly aligned data", "author": ["Lluis Castrejon", "Yusuf Aytar", "Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Watching unlabeled video helps learn new human actions from very few labeled snapshots", "author": ["Chao-Yeh Chen", "Kristen Grauman"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Cross modal distillation for supervision transfer", "author": ["Saurabh Gupta", "Judy Hoffman", "Jitendra Malik"], "venue": "arXiv preprint arXiv:1507.00448,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Cnn architectures for large-scale audio classification", "author": ["Shawn Hershey", "Sourish Chaudhuri", "Daniel PW Ellis", "Jort F Gemmeke", "Aren Jansen", "R Channing Moore", "Manoj Plakal", "Devin Platt", "Rif A Saurous", "Bryan Seybold"], "venue": "arXiv, 2016", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Comparing time and frequency domain for audio event recognition using deep learning", "author": ["Lars Hertel", "Huy Phan", "Alfred Mertins"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["Jing Huang", "Brian Kingsbury"], "venue": "In ICASSP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "shift. arXiv,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Figure-ground segmentation by transferring window masks", "author": ["Daniel Kuettel", "Vittorio Ferrari"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Honglak Lee", "Peter Pham", "Yan Largman", "Andrew Y Ng"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Auditory scene classification using machine learning techniques", "author": ["David Li", "Jason Tam", "Derek Toub"], "venue": "AASP Challenge,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Robust sound event classification using deep neural networks", "author": ["Ian McLoughlin", "Haomin Zhang", "Zhipeng Xie", "Yan Song", "Wei Xiao"], "venue": "ASL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "The open world of micro-videos", "author": ["Phuc Xuan Nguyen", "Gregory Rogez", "Charless Fowlkes", "Deva Ramamnan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Visually indicated sounds", "author": ["Andrew Owens", "Phillip Isola", "Josh McDermott", "Antonio Torralba", "Edward H Adelson", "William T Freeman"], "venue": "arXiv preprint arXiv:1512.08512,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["Karol J Piczak"], "venue": "In MLSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Esc: Dataset for environmental sound classification", "author": ["Karol J Piczak"], "venue": "In ACM Multimedia,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Histogram of gradients of time-frequency representations for audio scene classification", "author": ["Alain Rakotomamonjy", "Gilles Gasso"], "venue": "TASLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Recurrence quantification analysis features for environmental sound recognition", "author": ["Guido Roma", "Waldo Nogueira", "Perfecto Herrera"], "venue": "In WASPAA,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Unsupervised feature learning for urban sound classification", "author": ["Justin Salamon", "Juan Pablo Bello"], "venue": "In ICASSP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Detection and classification of acoustic scenes and events", "author": ["Dan Stowell", "Dimitrios Giannoulis", "Emmanouil Benetos", "Mathieu Lagrange", "Mark D Plumbley"], "venue": "TM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "Communications of the ACM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Deep content-based music recommendation", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Anticipating visual representations from unlabeled video", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Dense optical flow prediction from a static image", "author": ["Jacob Walker", "Abhinav Gupta", "Martial Hebert"], "venue": "In ICCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In NIPS,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 144, "endOffset": 156}, {"referenceID": 41, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 144, "endOffset": 156}, {"referenceID": 9, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 144, "endOffset": 156}, {"referenceID": 16, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 32, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 9, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 34, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 4, "context": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds.", "startOffset": 113, "endOffset": 120}, {"referenceID": 36, "context": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds.", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds.", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 28, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 20, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 29, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 33, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 31, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 19, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 22, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 5, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 11, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 32, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 144, "endOffset": 152}, {"referenceID": 16, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 144, "endOffset": 152}, {"referenceID": 30, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 249, "endOffset": 257}, {"referenceID": 41, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 249, "endOffset": 257}, {"referenceID": 25, "context": "[26] also investigates the relation between vision and sound modalities, but focuses on producing sound from image sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Concurrent work [11] also explores video as a form of weak labeling for audio event classification.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 130, "endOffset": 136}, {"referenceID": 1, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 130, "endOffset": 136}, {"referenceID": 17, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 32, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 293, "endOffset": 301}, {"referenceID": 16, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 293, "endOffset": 301}, {"referenceID": 2, "context": "Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13].", "startOffset": 46, "endOffset": 52}, {"referenceID": 8, "context": "Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13].", "startOffset": 46, "endOffset": 52}, {"referenceID": 12, "context": "Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "In [3, 13] the basic idea is to compress (i.", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "In [3, 13] the basic idea is to compress (i.", "startOffset": 3, "endOffset": 10}, {"referenceID": 2, "context": "In [3] and [13] both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [3] and [13] both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "[9] also transfer visual supervision into depth models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 13, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 6, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 25, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 24, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 40, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 7, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 39, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 38, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 35, "context": "We downloaded over two million videos from Flickr by querying for popular tags [36] and dictionary words, which resulted in over one year of continuous natural sound and video, which we use for training.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "Firstly, like images [19], we desire our network to be invariant to translations, a property that reduces the number of parameters we need to learn and increases efficiency.", "startOffset": 21, "endOffset": 25}, {"referenceID": 36, "context": "While we could have used a global pooling strategy [37] to down-sample variable length inputs to a fixed dimensional vector, such a strategy may unnecessarily discard information useful for high-level representations.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "This strategy is similar to a spatial loss in images [22], but instead temporally.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "As KL-divergence is differentiable, we optimize it using back-propagation [19] and stochastic gradient descent.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "We use the Adam [16] optimizer and a fixed learning rate of 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "After every convolution, we use batch normalization [15] and rectified linear activation units [17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "After every convolution, we use batch normalization [15] and rectified linear activation units [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "Method Accuracy RG [29] 69%", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "LTT [21] 72%", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "RNH [30] 77%", "startOffset": 4, "endOffset": 8}, {"referenceID": 33, "context": "Ensemble [34] 78% SoundNet 88% Table 3: Acoustic Scene Classification on DCASE: We evaluate classification accuracy on the DCASE dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "Accuracy on Method ESC-50 ESC-10 SVM-MFCC [28] 39.", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "3% Random Forest [28] 44.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "Piczak ConvNet [27] 64.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "2% Human Performance [28] 81.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].", "startOffset": 97, "endOffset": 101}, {"referenceID": 33, "context": "DCASE[34]: One of the tasks in the Detection and Classification of Acoustic Scenes and Events Challenge (DCASE)[34] is to recognize scenes from natural sounds.", "startOffset": 5, "endOffset": 9}, {"referenceID": 33, "context": "DCASE[34]: One of the tasks in the Detection and Classification of Acoustic Scenes and Events Challenge (DCASE)[34] is to recognize scenes from natural sounds.", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "Figure 3: SoundNet confusions on ESC-50 ESC-50 and ESC-10 [28]: The ESC-50 dataset is a collection of 2000 short (5 seconds) environmental sound recordings of equally balanced 50 categories selected from 5 major groups (animals, natural soundscapes, human non-speech sounds, interior/domestic sounds, and exterior/urban noises).", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "3% [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "Dataset Model conv4 conv5 pool5 conv6 conv7 conv8 DCASE [34] 8 Layer, AlexNet 84% 85% 84% 83% 78% 68% 8 Layer, VGG 77% 88% 88% 87% 84% 74%", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "ESC50 [28] 8 Layer, AlexNet 66.", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "The five layer SoundNet achieves slightly better results than [27] which is also a convolutional network trained with same data but with a different architecture, suggesting our five layer architecture is similar.", "startOffset": 62, "endOffset": 66}, {"referenceID": 37, "context": "Sound Embeddings: In order to show the semantic relevance of the features, we performed a two dimensional t-SNE [38] embedding and visualized our dataset in figure 4.", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.", "creator": "LaTeX with hyperref package"}}}