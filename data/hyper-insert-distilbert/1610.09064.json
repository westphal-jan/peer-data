{"id": "1610.09064", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration", "abstract": "abstract predictive system models broadly deployed when in locating the external world may assign incorrect labels to instances with high sampling confidence. proving such regression errors or establishing unknown unknowns are rooted typically in fixed model incompleteness, and typically arise because of investigating the mismatch between training prediction data and the cases originally seen rooted in avoiding the open information world. furthermore as the analytical models are blind to such correct errors, that input from surrounding an inadequate oracle is certainly needed to positively identify from these prediction failures. earlier in this 1992 paper, we strongly formulate and formally address precisely the problem of consciously optimizing the discovery of unknown unknowns of any predictive analysis model under a fixed budget, which limits the number of samples times an incorrectly oracle values can clearly be queried for true labels. we propose pursuing a sophisticated model - agnostic methodology which uses intrinsic feedback from an oracle strategy to both perfectly identify all unknown unknowns and to intelligently guide the discovery. alternately we commonly employ creating a two - vector phase approach which normally first naturally organizes the data into multiple territorial partitions together based on instance boundary similarity, and then furthermore utilizes an objective explore - data exploit strategy for discovering unknown population unknowns relative across these spatial partitions. secondly we demonstrate specifically the efficacy required of designing our framework by varying the underlying parameter causes of unknown unknowns across various applications. returning to the intrinsic best of our knowledge, this paper theoretically presents the first statistical algorithmic approach to the problem of discovering unknown unknowns of predictive models.", "histories": [["v1", "Fri, 28 Oct 2016 02:55:14 GMT  (6393kb,D)", "http://arxiv.org/abs/1610.09064v1", null], ["v2", "Tue, 6 Dec 2016 03:01:21 GMT  (1034kb,D)", "http://arxiv.org/abs/1610.09064v2", "To appear in AAAI 2017"], ["v3", "Sat, 10 Dec 2016 06:02:38 GMT  (1637kb,D)", "http://arxiv.org/abs/1610.09064v3", "To appear in AAAI 2017; Presented at NIPS Workshop on Reliability in ML, 2016"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["himabindu lakkaraju", "ece kamar", "rich caruana", "eric horvitz"], "accepted": true, "id": "1610.09064"}, "pdf": {"name": "1610.09064.pdf", "metadata": {"source": "CRF", "title": "Discovering Blind Spots of Predictive Models: Representations and Policies for Guided Exploration", "authors": ["Himabindu Lakkaraju", "Ece Kamar", "Rich Caruana", "Eric Horvitz"], "emails": ["himalv@cs.stanford.edu,", "horvitz}@microsoft.com"], "sections": [{"heading": "Introduction", "text": "Predictive models are widely used in domains ranging from judiciary and healthcare to autonomous driving. As we increasingly rely on these models for high-stakes decisions, identifying and characterizing their unexpected failures in the open world is critical. We categorize errors of a predictive model as: known unknowns and unknown unknowns (Attenberg, Ipeirotis, and Provost 2015). A known unknown is a data point for which the model makes a low confidence prediction, whereas unknown unknowns correspond to points for which the model is confident about its predictions but is actually making mistakes. Since the model lacks awareness of such unknown unknowns, approaches developed for addressing known unknowns (e.g., active learning) cannot be used for discovering unknown unknowns.\nUnknown unknowns primarily occur when the data used for training a predictive model is not representative of the samples encountered during test time, i.e., when the model is deployed in the wild. This mismatch could be a result of biases in data collection or changes in the underlying distributions due to temporal, spatial or other factors such as a subtle shift in task definition. To illustrate, let us consider"}, {"heading": "Learning Algorithm", "text": ""}, {"heading": "Training Data", "text": ""}, {"heading": "Dogs Cats", "text": "the scenario of predicting if an image contains a cat or a dog (Figure 1). If the training data only contained images of black dogs, and white and brown cats, the resulting classifier would learn to make predictions based on color. During test time, such a model might confidently classify a white dog as a cat. Unknown unknowns encountered in the real-world can be catastrophic, resulting in incorrect treatment recommendations, or unintentional racial biases (Crawford 2016) in settings such as healthcare delivery and criminal justice.\nWe formulate and address the problem of optimizing the discovery of unknown unknowns of any given predictive model under a fixed budget. In this setting, the model seeks to identify unknown unknowns by querying true labels of selected instances from an oracle but the budget limits the number of queries. The formulation assumes no knowledge of the functional form or the associated training data of the predictive model and treats it as a black box which outputs a label and a confidence score (or a proxy) for a given data point. To the best of our knowledge, this is the first work providing an algorithmic solution to address this problem.\nDeveloping an algorithmic solution for the discovery of unknown unknowns introduces a number of challenges: 1) Since unknown unknowns can occur in any portion of the feature space, how do we develop strategies which can effectively and efficiently search the space? 2) As confidence scores associated with model predictions are typically not\nar X\niv :1\n61 0.\n09 06\n4v 1\n[ cs\n.A I]\n2 8\nO ct\n2 01\n6\ninformative for identifying unknown unknowns, how can we make use of the history of feedback from an oracle to guide the discovery of unknown unknowns? 3) How can we effectively manage the tradeoff between searching in neighborhoods where we previously found unknown unknowns and examining unexplored regions of the search space?\nWe present an algorithmic framework for the discovery of unknown unknowns. We build on the intuition that unknown unknowns, which may often occur due to systematic biases, are concentrated in certain portions of the feature space and do not typically occur at random. Our methodology follows a two-step approach which first partitions the search space based on instance similarity and then employs an exploreexploit strategy for navigating through these partitions systematically based on feedback from an oracle.\nThe first step, which we refer to as Descriptive Space Partitioning (DSP), is guided by an objective function which encourages partitioning of the search space such that instance similarity in terms of features and confidence scores is maximized within partitions and minimized across them. We outline a greedy solution to this NP-hard problem and show that it is a ln N approximation to the optimal solution where N is the size of the search space.\nThe second step of the methodology centers on effective exploration of the partitions generated by DSP while exploiting the input from the oracle. We propose a multi-armed bandit algorithm, Bandit for Unknown Unknowns (UUB), which exploits problem-specific characteristics to efficiently discover unknown unknowns.\nOur empirical evaluations cover various diverse tasks, such as sentiment classification, subjectivity detection, and image classification for various types of base predictive models when unknown unknowns occur due to biased training data and domain adaptation. The results demonstrate the effectiveness of the framework and its constituent components for the discovery of unknown unknowns across different experimental conditions, providing evidence that the method can be readily applied to discover unknown unknowns in different real-world settings."}, {"heading": "Related Work", "text": "In this section, we review prior research relevant to the discovery of unknown unknowns and cover the work related to Descriptive Space Partitioning and the Bandit for Unknown Unknowns algorithm in later sections. Unknown Unknowns The problem of model incompleteness and the challenge of grappling with unknown unknowns in the real world has been coming to the fore as a critical topic in discussions about uses of AI technologies (Horvitz 2008). Attenberg et. al. introduced the idea of harnessing human input to identify unknown unknowns but their studies left the task of exploration and discovery completely to humans without any assistance (Attenberg, Ipeirotis, and Provost 2015). In contrast, we propose an algorithmic framework in which the role of the oracle is simpler and more realistic: The oracle is only queried for labels of selected instances chosen by our algorithmic framework. Dataset Shift A common cause of unknown unknowns is dataset shift, which represents the mismatch between train-\ning and test distributions (Quionero-Candela et al. 2009; Jiang and Zhai 2007). Multiple approaches have been proposed to address dataset shift, including importance weighting of training instances based on similarity to test set (Shimodaira 2000), online learning of prediction models (CesaBianchi and Lugosi 2006), and learning models robust to adversarial actions (Teo et al. 2007; Herbrich 2004; Decoste and Scho\u0308lkopf 2002). These approaches cannot be applied to our setting as they make one or more of the following assumptions which limit their applicability to realworld settings: 1) the model is not a black box 2) the data used to train the predictive model is accessible 3) the model can be adaptively retrained. Further, the goal of this work is different as we study the problem of discovering unknown unknowns of models which are already deployed. Active Learning Active learning research has proposed various approaches for selectively learning about known unknowns during training (e.g., uncertainty sampling (Lewis and Gale 1994; Settles 2010), query by committee (Seung, Opper, and Sompolinsky 1992), expected model change (Settles, Craven, and Ray 2008), expected error reduction (Zhu, Lafferty, and Ghahramani ), expected variance reduction (Zhang and Oles 2000)) However, active learning approaches are not applicable to our problem as the model lacks awareness of the presence of its unknown unknowns."}, {"heading": "Problem Formulation", "text": "Given a black-box predictive modelM which takes as input a data point x with features F = {f1, f2, \u00b7 \u00b7 \u00b7 fL}, and returns a class label c\u2032 \u2208 C and a confidence score s \u2208 [0, 1], our goal is to find the unknown unknowns ofMw.r.t a given test setD using a limited number of queries,B, to the oracle, and, more broadly, to maximize the utility associated with the discovered unknown unknowns. The discovery process is guided by a utility function, which not only incentivizes the discovery of unknowns unknowns, but also accounts for the costs associated with querying the oracle (e.g., monetary and time costs of labeling in crowdsourcing).\nAlthough our methodology is generic enough to find unknown unknowns associated with all the classes in the test set, we formulate the problem for a particular class c, a critical class, where false positives are costly and need to be discovered (Elkan 2001). Based on the decisions of the system designer regarding critical class c and confidence threshold \u03c4 , our search space for unknown unknown discovery constitutes all of those data points in D which are assigned the critical class c by modelM with confidence higher than \u03c4 .\nOur approach takes the following inputs: 1) A set of N instances, X = {x1, x2 \u00b7 \u00b7 \u00b7xN} \u2286 D, which were confidently assigned to the critical class c by the modelM, and the corresponding confidence scores, S = {s1, s2 \u00b7 \u00b7 \u00b7 sN}, assigned to these points byM, 2) An oracle o which takes as input a data point x and returns its true label o(x) as well as the cost incurred to determine the true label of x, cost(x) 3) A budget B on the number of times the oracle can be queried.\nOur utility function, u(x(t)), for querying the label of data point x(t) at the tth step of exploration is defined as:\nu(x(t)) = 1{o(xt)6=c} \u2212 \u03b3 \u00d7 cost(x(t)) (1)\nwhere 1{o(xt)6=c} is an indicator function which returns 1 if x(t) is identified as an unknown unknown, and a 0 otherwise. cost(x(t)) \u2208 [0, 1] is the cost incurred by the oracle to determine the label of x(t). Both the indicator and the cost functions in Equation 1 are initially unknown and observed based on oracle\u2019s feedback on x(t). \u03b3 \u2208 [0, 1] is a tradeoff parameter which can be provided by the end user. Problem Statement: Find a sequence of B instances {x(1), x(2) \u00b7 \u00b7 \u00b7x(B)} \u2286 X for which the cumulative util-\nity B\u2211 t=1 u(x(t)) is maximum."}, {"heading": "Methodology", "text": "First we present Descriptive Space Partitioning (DSP), which induces similarity preserving partitions on the set X . Then, we present a novel multi-armed bandit algorithm, which we refer to as Bandit for Unknown Unknowns (UUB), for systematically searching for unknown unknowns within these partitions while leveraging feedback from an oracle."}, {"heading": "Descriptive Space Partitioning", "text": "A key intuition behind our approach is that unknown unknowns are often a result of systematic biases (Attenberg, Ipeirotis, and Provost 2015). We assume that these blindspots do not occur at random, but are instead concentrated in specific portions of the feature space. The first step of our approach, DSP, partitions the instances in X such that instances grouped together are likely to be indistinguishable as far as the logic of the modelM is concerned. Partitioning X enables our bandit algorithm, UUB, to discover regions with high concentrations of unknown unknowns.\nWe consider two instances a and a\u2032 \u2208 X to be indistinguishable w.r.t the modelM if they share similar feature values and are assigned comparable confidence scores by M. In such cases, if a is identified as an unknown unknown, a\u2032 is likely to be an unknown unknown as well since the model M employes similar logic in assigning these points to class c. Based on this intuition, we propose an objective function which encourages grouping of instances in X that are indistinguishable w.r.t the modelM, and facilitates separation of dissimilar instances. The proposed objective also associates a concise, comprehensible description with each partition, which is useful for understanding the exploration behavior of our framework and the types of unknown unknowns of M (details in the Experiments Section).\nDSP takes as input a set of candidate patterns Q = {q1, q2, \u00b7 \u00b7 \u00b7 } where each qi is a conjunction of (feature, value) pairs. Such patterns can be obtained by running an off-the-shelf frequent pattern mining algorithm such as Apriori (Agrawal, Srikant, and others 1994) on X . Each pattern induces a partition on X by covering one or more instances. For each pattern q, the set of instances that satisfy q is represented by covered by(q), the centroid of such instances is x\u0304q , and their mean confidence score is s\u0304q .\nThe partitioning objective minimizes dissimilarities of instances within each partition and maximizes them across partitions. In particular, we define goodness of each pattern q inQ as the combination of following metrics, where d and d\u2032 are standard distance measures defined over feature vectors of instances and their confidence scores respectively:\nAlgorithm 1 Greedy Algorithm for Partitioning 1: Input: Set of instances X , Confidence scores S, Patterns Q,\nMetric functions {g1 \u00b7 \u00b7 \u00b7 g5}, Weights \u03bb 2: Procedure: 3: P = \u2205, E = X 4: while E 6= \u2205 do: 5:\np = arg max q\u2208Q |E \u2229 covered by(q)| \u03bb1g1(q)\u2212 \u03bb2g2(q) + \u03bb3g3(q)\u2212 \u03bb4g4(q) + \u03bb5g5(q)\n6: P = P \u222a p , Q = Q \\ p , E = E \\ covered by(p) 7: end while 8: return P\nIntra-partition feature distance: g1(q) = \u2211\n{x\u2208X : x \u2208 covered by(q)}\nd(x, x\u0304q)\nInter-partition feature distance: g2(q) = \u2211\n{x\u2208X : x \u2208 covered by(q)} \u2211 {q\u2032\u2208Q: q\u2032 6=q} d(x, x\u0304q\u2032)\nIntra-partition confidence score distance: g3(q) = \u2211\n{si: xi\u2208X \u2227xi \u2208 covered by(q)}\nd\u2032(si, s\u0304q)\nInter-partition confidence score distance: g4(q) = \u2211\n{si: xi\u2208X \u2227xi \u2208 covered by(q)} \u2211 {q\u2032\u2208Q: q\u2032 6=q} d\u2032(si, s\u0304q\u2032)\nPattern Length: g5(q) = size(q), the number of (feature, value) pairs in pattern q, included to favor concise descriptions.\nGiven the setsX , S, a collection of patternsQ, and weight vector \u03bb used to combine g1 through g5, our goal is to find a set of patterns P \u2286 Q such that it covers all the points in X and minimizes the following objective:\nmin \u2211 q\u2208Q fq (\u03bb1g1(q)\u2212 \u03bb2g2(q) + \u03bb3g3(q)\u2212 \u03bb4g4(q) + \u03bb5g5(q))\n(2) subject to \u2211\nq: x\u2208covered by(q)\nfq \u2265 1 \u2200x \u2208 X , where fq \u2208 {0, 1} \u2200q \u2208 Q\nThis formulation is identical to that of a weighted set cover problem which is NP-hard (Johnson 1974). It has been shown that a greedy solution provides a ln N approximation to the weighted set cover problem (Johnson 1974; Feige 1998) where N is the size of search space. Algorithm 1 applies a similar strategy which greedily selects patterns with maximum coverage-to-weight ratio at each step, thus resulting in a ln N approximation guarantee. This process is repeated until no instance in X is left uncovered. In case of instances in X which are covered by multiple partitions, ties are broken by assigning it to the closest centroid.\nOur partitioning approach is inspired by a class of clustering techniques commonly referred to as conceptual clustering (Michalski and Stepp 1983; Fisher 1987) or descriptive clustering (Weiss 2006; Li, Peng, and Wu 2008;\nAlgorithm 2 Explore-Exploit Algorithm for Unknown Unknowns 1: Input: 2: Set of partitions (arms) {1, 2 \u00b7 \u00b7 \u00b7K}, Oracle o, Budget B 3: Procedure: 4: for t from 1 to B do: 5: if t \u2264 K then: 6: Choose arm At = t 7: else 8: Choose arm At = arg max\n1\u2264i\u2264K u\u0304t(i) + bt(i)\n9: end if 10: Sample an instance x(t) from partition pAt 11: Observe true label of x(t) and the cost of querying the or-\nacle and compute u(x(t)) using Equation (1). 12: end for\n13: return B\u2211\nt=1\nu(x(t))\nKim, Rudin, and Shah 2014). We make the following contributions to this line of research: We propose a novel objective function, whose components have not been jointly considered before. In contrast to previous solutions which employ post processing techniques or use Bayesian frameworks, we propose a simple, yet elegant solution which offers theoretical guarantees."}, {"heading": "Multi-armed Bandit for Unknown Unknowns", "text": "The output of the first step of our approach, DSP, is a set of K partitions P = {p1, p2 \u00b7 \u00b7 \u00b7 pK} such that each partition pj groups together data points that are indistinguishable w.r.t the model M and feature space F . The partitioning, however, does not guarantee that data points in a partition are identical and each partition has an unobservable concentration of unknown unknown instances. The goal of the second step of our approach is to compute an exploration policy over partitions such that it maximizes the overall utility of the discovery of unknown unknowns.\nWe formalize this problem as a multi-armed bandit problem and the algorithm for deciding which partition to query at each step is given in Algorithm 2. In this formalization, each partition pj corresponds to an arm j of the bandit. At each step, the algorithm chooses a partition and then randomly samples a data point from that partition without replacement and queries the its true label from the oracle. Since querying the data point reveals whether it is an unknown unknown, the point is excluded from future steps.\nIn the first K steps, the algorithm samples a point from each partition. Then, at each step t, the exploration decisions are guided by a combination of u\u0304t(i), the empirical mean utility (reward) of the partition i at time t, and bt(i), which represents the uncertainty over the estimate of u\u0304t(i).\nOur problem setting has the characteristic that the expected utility of each arm is non-stationary; querying a data point from a partition changes the concentration of unknown unknowns in the partition and consequently changes the expected utility of that partition in future steps. Therefore, stationary MAB algorithms such as UCB (Auer, Cesa-Bianchi, and Fischer 2002) are not suitable. A variant of UCB, discounted UCB, addresses the non-stationary settings and can\nbe used as follows to compute u\u0304t(i) and bt(i) (Garivier and Moulines 2008).\nu\u0304t(i) = 1\nNt(\u03d1it, i) t\u2211 j=1 \u03d1ij,t u(x(j)) 1Aj=i\nbt(i) = \u221a\u221a\u221a\u221a\u221a2 log K\u2211i=1Nt(\u03d1it, i) Nt(\u03d1it, i) , Nt(\u03d1it, i) = t\u2211\nj=1\n\u03d1ij,t 1Aj=i\nThe main idea of Discounted UCB is weighing recent observations more to account for the non-stationary nature of the utility function. If \u03d1ij,t denotes the discounting factor applied at time t to the reward obtained from arm i at time j < t, \u03d1ij,t = \u03b3\nt\u2212j in the case of discounted UCB, where \u03b3 \u2208 (0, 1).\nThe discounting factor of Discounting UCB is designed to handle arbitrary changes in the utility distribution, whereas the way the utility of a partition changes in our setting has a certain structure: The utility estimate of arm i only changes when the arm is queried and the magnitude of the change corresponds to the influence of a single data point in the size of partition, Ni. Using this observation, we can customize the calculation of \u03d1ij,t for our setting and eliminate the need to set up the value of \u03b3, which affects the quality of decisions made by Discounted UCB. We compute \u03d1ij,t as the ratio of data points preserved in the partition i between times j and t:\n\u03d1ij,t = (Ni \u2212 t\u2211\nl=1\n1Al=i) / (Ni \u2212 j\u2211\nl=1\n1Al=i) (3)\nThe value of \u03d1ij,t is inversely proportional to the number of pulls of arm i during the interval (j, t). \u03d1ij,t is 1, if the arm is not pulled during this interval, indicating that the expected utility of i remained unchanged. We refer to the version of Algorithm 2 that uses the discounting factor specific to our setting (Eqn. 3) as Bandit for Unknown Unknown (UUB).\nTheorem 1 UUB matches the lower bound on the regret under a non-stationary setting up to a factor of \u221a BlogB. (Please see appendix for details.)"}, {"heading": "Experimental Evaluation", "text": "We now present experiments and results on constituent components of our framework as well as the entire pipeline. Datasets and Nature of Biases We evaluate the performance of our methodology across four different data sets in which the underlying cause of unknown unknowns vary from biases in training data to domain adaptation: (1) Sentiment Snippets: A collection of 10K sentiment snippets/sentences expressing opinions on various movies (Pang and Lee 2005). Each snippet (sentence) corresponds to one data point. We split this data equally into train and test sets. We then bias the training data by randomly removing sub-groups of negative snippets from it. We consider positive sentiment as the critical class for this data. (2) Subjectivity: A set of 10K subjective and objective snippets extracted from Rotten Tomatoes webpages (Pang and Lee 2004). We consider the objective class in this dataset\nas the critical class, split the data equally into train and test sets, and introduce bias in the same way as described above. (3) Amazon Reviews: A random sample of 50K reviews of books and electronics collected from Amazon (McAuley, Pandey, and Leskovec 2015). We use this data set to study unknown unknowns introduced by domain adaptation; we train the predictive models on the electronics reviews and then test them on the book reviews. Similar to the sentiment snippets data set, the positive sentiment is the critical class. (4) Image Data: A set of 25K cat and dog images (ima 2013). We use this data set to assess whether our framework can recognize unknown unknowns that occur when semantically meaningful sub-groups are missing from the training data. To this end, we split the data equally into train and test and bias the training data such that it comprises only of images of dogs which are black, and cats which are not black. We set the class label cat to be the critical class in our experiments. Experimental Setting: We use bag of words features to train the predictive models for all of our textual data sets. As the features for the images, we use super-pixels obtained from the first max-pooling layer of Googles pre-trained Inception neural network (Szegedy et al. 2015; Ribeiro, Singh, and Guestrin 2016). Images are represented with a feature vector comprising of 1\u2019s and 0\u2019s indicating the presence or absence of the corresponding super pixels. We experiment with multiple predictive models: decision trees, SVMs, logistic regression, random forests and neural network. Due to space constraints, this section presents results for decision trees as model M but detailed results for all the other models are included in the Appendix. We set confidence threshold \u03c4 to 0.65 to construct our search space X for each data set. We consider two settings for the cost function (refer Eqn. 1): The cost is set to 1 for all instances (uniform cost) in the image dataset and it is set to [(length(x)\u2212minlength)/(maxlength\u2212minlength)] (variable cost) for all textual data. Note that these cost functions are only available to the oracle. The tradeoff parameter \u03b3 is set to 0.2. The parameters of DSP {\u03bb1, \u00b7 \u00b7 \u00b7\u03bb5} are estimated by setting aside as a validation set 5% of the test instances assigned to the critical class by the predictive models. We search the parameter space using coordinate descent to find parameters which result in the minimum value of the objective function defined in Eqn. 2. We set the budget B to 20% of all the instances in the set X through out our experiments. Further, the results presented for UUB are all averaged across 100 runs."}, {"heading": "Evaluating the Partitions", "text": "The effectiveness of our framework relies on the notion that our partitioning scheme, DSP, creates partitions such that unknown unknowns are concentrated in specific subset of partitions as opposed to being evenly spread out across them. If unknown unknowns are distributed evenly across all the partitions, it is highly unlikely that our bandit algorithm can perform better than a strategy which randomly chooses a partition at each step of the exploration process. Consequently, we measure the quality of partitions created by DSP by measuring the entropy of the distribution of unknown unknowns across the partitions in P . For each parti-\ntion pi \u2208 P , we count the number of unknown unknowns, Upi based on the true labels which are only known to the oracle. We then compute entropy over the following set: {Upi/ \u2211 i Upi | pi \u2208 P}. Smaller entropy values are better as they indicate higher concentrations of unknown unknowns in fewer partitions.\nFigure 2 compares the entropy of the partitions generated by DSP with clusters generated by k-means algorithm using only features in F (kmeans-features), only confidence scores in S (kmeans-conf) and both (kmeans-both) by first clustering using confidence scores and then using features. The entropy values for DSP are consistently smaller compared to the baselines across all the datasets. This can be explained by the fact that DSP jointly optimizes inter and intra-partition distances over both features and confidence scores. As shown in Figure 2, the entropy values are much higher when k-means considers only features or only confidence scores indicating the importance of jointly reasoning about them."}, {"heading": "Evaluating the Bandit Algorithms", "text": "We measure the performance of our multi-armed bandit algorithm UUB in terms of a standard evaluation metric in the MAB literature called cumulative regret. Cumulative regret of a policy \u03c0 is computed as the difference between the total rewards collected by an optimal policy \u03c0\u2217, which at each step plays the arm with the highest expected utility (or reward), and the total rewards collected by the policy \u03c0. Small values of cumulative regret indicate better policies. The utility function defined in Eqn. 1 determines the reward associated with each instance.\nWe compare the performance of our algorithm, UUB, with that of several baseline algorithms such as random, greedy, -greedy strategies (Chapelle and Li 2011), UCB, UCBf (Slivkins and Upfal 2008), sliding window and discounted UCB for various values of the discounting factor \u03b3 = {0.2, 0.5, 0.8}. All algorithms take as input the partitions created by DSP. Figure 3(a) shows the cumulative regret of each of these algorithms on the image data set. Results for the other data sets can be seen in the Appendix. The figure shows that UUB achieves the smallest cumulative regret compared to other baselines on the image data set. Similarly, UUB is the best performing algorithm on the sentiment snippets and subjectivity snippets data sets, whereas discounted UCB (\u03b3 = 0.5) achieves slightly smaller regret than UUB on the Amazon reviews data set. The experiment also highlights a disadvantage of the discounted UCB al-\ngorithm as its performance is sensitive to the choice of the discounting factor \u03b3, where as UUB is parameter free. Further, both UCB and its variant UCBf which are designed for stationary and slowly changing reward distributions respectively have higher cumulative regret than UUB and discounted UCB indicating that they are not as effective in our setting."}, {"heading": "Evaluating the Overall Methodology", "text": "In the previous section, we compared the performance of UUB to other bandit methods when they are given the same data partitions to explore. In this section, we compare the performance of our complete pipeline (DSP + UUB) to other end-to-end heuristic methods we devised as baselines. Due to space constraints, we present results only for the image data. Results for other data sets can be seen in the Appendix.\nWe compare the cumulative regret of our framework to several baselines: 1) Random sampling: Randomly select B instances from set X for querying the oracle. 2) Least average similarity: For each instance in X , compute the average Euclidean distance w.r.t all the data points in the training set and choose B instances with the largest distance. The idea is that instances dissimilar to those encountered during the training of the model could potentially be unknown unknowns. 3) Least maximum similarity: Compute minimum Euclidean distance of each instance in X from the training set and choose B instances with the highest distances. 4) Most uncertain: Rank the instances in X in increasing order of the confidence scores assigned by the modelM and pick the top B instances. Figure 3(b) shows the cumulative regret of our framework and the baselines for the image data. Our framework achieves the least cumulative regret of all the strategies across all data sets.\nQualitative Analysis Figure 3(c) presents an illustrative example of how our methodology explores three of the partitions generated for the image data set. Our partitioning framework associated the super pixels shown in the Figure with each partition. Examining the super pixels reveals that partition 1, 2 and 3 correspond to the images of white chihuahuas (dog), white cats, and brown dogs respectively. The plot shows the number of times these partitions have been played by our bandit algorithm. The figure shows that partition 2 is selected very few times compared to partitions 1 and 3 \u2014 because white cat images are part of the training\ndata for our predictive models and there are not many unknown unknowns in this partition. On the other hand, white and brown dogs are not part of the training data and the algorithm explores these partitions often. Figure 3(c) also indicates that partition 1 was explored often during the initial steps but not later on. This is because there were fewer unknown unknowns in that partition and the algorithm had exhausted all of them after a certain number of plays."}, {"heading": "Discussion & Conclusions", "text": "We presented an algorithmic approach to discovering unknown unknowns of predictive models. In order to guide the discovery, we partition the search space and then use bandit algorithms to identify partitions with larger concentrations of undiscovered unknown unknowns. In addition to presenting novel algorithms for partitioning and sampling, we discuss a new setting where these algorithms can be used to gain insight into deployed predictive models.\nOur work paves the way for alternate formulations of the unknown unknown discovery problem, such as where the budget is defined in terms of total cost of querying the oracle rather than on the number of queries. The method can be applied to more sophisticated settings where diversity in the unknown unknowns are preferred or where the utility of some types of unknown unknowns decreases with time as sufficient examples of the type are discovered (e.g., after informing the engineering team that owns the classifier). In many settings, the oracle can be approximated via crowd-sourced labeling, and the labeling noise of the crowd might be addressed by incorporating repeated labeling into our framework.\nThe discovery of unknown unknowns can help system designers when deploying predictive models in many ways. Our approach provides descriptions of partitions with unknown unknowns that could help a system designer devise strategies to prevent errors or recover from them (e.g., silencing the model when a query falls into a partition with unknown unknowns). Discovered unknown unknowns can be used to turn unknown unknowns into known unknowns and then to known knowns. Figure 8 in the appendix presents a preliminary result on how model performance improves when the underlying model is retrained periodically with discovered unknown unknowns. This scenario motivates future extensions that dynamically revise partitions on the fly based on feedback from the oracle."}], "references": [{"title": "Fast algorithms for mining association rules", "author": ["Srikant Agrawal", "R. others 1994] Agrawal", "R Srikant"], "venue": "In VLDB", "citeRegEx": "Agrawal et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1994}, {"title": "Beat the machine: Challenging humans to find a predictive model\u2019s &ldquo;unknown unknowns&rdquo", "author": ["Ipeirotis Attenberg", "J. Provost 2015] Attenberg", "P. Ipeirotis", "F. Provost"], "venue": "J. Data and Information Quality", "citeRegEx": "Attenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Attenberg et al\\.", "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem. Machine learning 47(2-3):235\u2013256", "author": ["Cesa-Bianchi Auer", "P. Fischer 2002] Auer", "N. CesaBianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "and Lugosi", "author": ["N. Cesa-Bianchi"], "venue": "G.", "citeRegEx": "Cesa.Bianchi and Lugosi 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Li", "author": ["O. Chapelle"], "venue": "L.", "citeRegEx": "Chapelle and Li 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Sch\u00f6lkopf", "author": ["D. Decoste"], "venue": "B.", "citeRegEx": "Decoste and Sch\u00f6lkopf 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "D", "author": ["Fisher"], "venue": "H.", "citeRegEx": "Fisher 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "and Moulines", "author": ["A. Garivier"], "venue": "E.", "citeRegEx": "Garivier and Moulines 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Zhai", "author": ["J. Jiang"], "venue": "C.", "citeRegEx": "Jiang and Zhai 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "D", "author": ["Johnson"], "venue": "S.", "citeRegEx": "Johnson 1974", "shortCiteRegEx": null, "year": 1974}, {"title": "2014", "author": ["B. Kim", "C. Rudin", "J. A Shah"], "venue": "The bayesian case model: A generative approach for case-based reasoning and prototype classification. In Advances in Neural Information Processing Systems, 1952\u2013", "citeRegEx": "Kim. Rudin. and Shah 2014", "shortCiteRegEx": null, "year": 1960}, {"title": "W", "author": ["D.D. Lewis", "Gale"], "venue": "A.", "citeRegEx": "Lewis and Gale 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "X", "author": ["Z. Li", "H. Peng", "Wu"], "venue": "2008. A new descriptive clustering algorithm based on nonnegative matrix factorization. In Granular Computing, 2008. GrC", "citeRegEx": "Li. Peng. and Wu 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Inferring networks of substitutable and complementary products", "author": ["Pandey McAuley", "J. Leskovec 2015] McAuley", "R. Pandey", "J. Leskovec"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "McAuley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "R", "author": ["R.S. Michalski", "Stepp"], "venue": "E.", "citeRegEx": "Michalski and Stepp 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "N", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "Lawrence"], "venue": "D.", "citeRegEx": "Quionero.Candela et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "M", "author": ["Ribeiro"], "venue": "T.; Singh, S.; and Guestrin, C.", "citeRegEx": "Ribeiro. Singh. and Guestrin 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiple-instance active learning", "author": ["Craven Settles", "B. Ray 2008] Settles", "M. Craven", "S. Ray"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Settles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Settles et al\\.", "year": 2008}, {"title": "H", "author": ["Seung"], "venue": "S.; Opper, M.; and Sompolinsky, H.", "citeRegEx": "Seung. Opper. and Sompolinsky 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and Upfal", "author": ["A. Slivkins"], "venue": "E.", "citeRegEx": "Slivkins and Upfal 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "2007", "author": ["C.H. Teo", "A. Globerson", "S.T. Roweis", "A. J Smola"], "venue": "Convex learning with invariances. In Advances in neural information processing systems, 1489\u2013", "citeRegEx": "Teo et al. 2007", "shortCiteRegEx": null, "year": 1496}, {"title": "and Oles", "author": ["T. Zhang"], "venue": "F.", "citeRegEx": "Zhang and Oles 2000", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [], "year": 2016, "abstractText": "Predictive models deployed in the world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases seen in the open world. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of optimizing the discovery of unknown unknowns of any predictive model under a fixed budget, which limits the number of times an oracle can be queried for true labels. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on instance similarity, and then utilizes an exploreexploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models.", "creator": "LaTeX with hyperref package"}}}