{"id": "1610.09158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Towards a continuous modeling of natural language domains", "abstract": "all humans continuously independently adapt within their style and language perceptions to a major variety of domains. however, a reliable definition of ` domain'has eluded most researchers ; thus went far. technically additionally, the basic notion exist of semantic discrete border domains too stands in contrast to the multiplicity of heterogeneous facial domains ecosystems that humans navigate, many representations of categories which overlap. in best order order to feel better understand for the change and changing variation of human language, musically we draw on computational research in domain dwelling adaptation and extend the notion of using discrete geographic domains to the continuous spectrum. we propose representation learning - based inquiry models that can immediately adapt people to exploring continuous domains and detail how these can to be used : to investigate variation in language. to this end, \" we propose reasons to use dialogue modeling as fundamentally a network test bed due to transferring its structural proximity element to digital language modeling and its social component.", "histories": [["v1", "Fri, 28 Oct 2016 10:26:40 GMT  (3086kb,D)", "http://arxiv.org/abs/1610.09158v1", "5 pages, 3 figures, published in Uphill Battles in Language Processing workshop, EMNLP 2016"]], "COMMENTS": "5 pages, 3 figures, published in Uphill Battles in Language Processing workshop, EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": false, "id": "1610.09158"}, "pdf": {"name": "1610.09158.pdf", "metadata": {"source": "CRF", "title": "Towards a continuous modeling of natural language domains", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["sebastian.ruder@insight-centre.org", "john.breslin@insight-centre.org", "sebastian@aylien.com", "parsa@aylien.com"], "sections": [{"heading": "1 Introduction", "text": "The notion of domain permeates natural language and human interaction: Humans continuously vary their language depending on the context, in writing, dialogue, and speech. However, the concept of domain is ill-defined, with conflicting definitions aiming to capture the essence of what constitutes a domain. In semantics, a domain is considered a \u201cspecific area of cultural emphasis\u201d (Oppenheimer, 2006) that entails a particular terminology, e.g. a specific sport. In sociolinguistics, a domain consists of a group of related social situations, e.g. all human activities that take place at home. In discourse a domain is a \u201ccognitive construct (that is) created in response to a number of factors\u201d (Douglas, 2004) and\nincludes a variety of registers. Finally, in the context of transfer learning, a domain is defined as consisting of a feature space X and a marginal probability distribution P (X) where X = {x1, ..., xn} and xi is the ith feature vector (Pan and Yang, 2010).\nThese definitions, although pertaining to different concepts, have a commonality: They separate the world in stationary domains that have clear boundaries. However, the real world is more ambiguous. Domains permeate each other and humans navigate these changes in domain.\nConsequently, it seems only natural to step away from a discrete notion of domain and adopt a continuous notion. Utterances often cannot be naturally separated into discrete domains, but often arise from a continuous underlying process that is reflected in many facets of natural language: The web contains an exponentially growing amount of data, where each document \u201cis potentially its own domain\u201d (McClosky et al., 2010); a second-language learner adapts their style as their command of the language improves; language changes with time and with locality; even the WSJ section of the Penn Treebank \u2013 often treated as a single domain \u2013 contains different types of documents, such as news, lists of stock prices, etc. Continuity is also an element of real-world applications: In spam detection, spammers continuously change their tactics; in sentiment analysis, sentiment is dependent on trends emerging and falling out of favor.\nDrawing on research in domain adaptation, we first compare the notion of continuous natural language domains against mixtures of discrete domains and motivate the choice of using dialogue modeling\nar X\niv :1\n61 0.\n09 15\n8v 1\n[ cs\n.C L\n] 2\n8 O\nct 2\nas a test bed. We then present a way of representing continuous domains and show how continuous domains can be incorporated into existing models. We finally propose a framework for evaluation."}, {"heading": "2 Continuous domains vs. mixtures of discrete domains", "text": "In domain adaptation, a novel target domain is traditionally assumed to be discrete and independent of the source domain (Blitzer et al., 2006). Other research uses mixtures to model the target domain based on a single (Daume\u0301 III and Marcu, 2006) or multiple discrete source domains (Mansour, 2009). We argue that modeling a novel domain as a mixture of existing domains falls short in light of three factors.\nFirstly, the diversity of human language makes it unfeasible to restrict oneself to a limited number of source domains, from which all target domains are modeled. This is exemplified by the diversity of the web, which contains billions of heterogeneous websites; the Yahoo! Directory1 famously contained thousands of hand-crafted categories in an attempt to separate these. Notably, many sub-categories were cross-linked as they could not be fully separated and websites often resided in multiple categories.\nSimilarly, wherever humans come together, the culmination of different profiles and interests gives rise to cliques, interest groups and niche communities that all demonstrate their own unique behaviors, unspoken rules, and memes. A mixture of existing domains fails to capture these varieties.\nSecondly, using discrete domains for soft assignments relies on the assumption that the source domains are clearly defined. However, discrete labels only help to explain domains and make them interpretable, when in reality, a domain is a heterogeneous amalgam of texts. Indeed, Plank and van Noord (2011) show that selection based on humanassigned labels fares worse than using automatic domain similarity measures for parsing.\nThirdly, not only a speaker\u2019s style and command of a language are changing, but a language itself is continuously evolving. This is amplified in fastmoving media such as social platforms. Therefore,\n1https://en.wikipedia.org/wiki/Yahoo! _Directory\napplying a discrete label to a domain merely anchors it in time. A probabilistic model of domains should in turn not be restricted to treat domains as independent points in a space. Rather, such a model should be able to walk the domain manifold and adapt to the underlying process that is producing the data."}, {"heading": "3 Dialogue modeling as a test bed for investigating domains", "text": "As a domain presupposes a social component and relies on context, we propose to use dialogue modeling as a test bed to gain a more nuanced understanding of how language varies with domain.\nDialogue modeling can be seen as a prototypical task in natural language processing akin to language modeling and should thus expose variations in the underlying language. It allows one to observe the impact of different strategies to model variation in language across domains on a downstream task, while being inherently unsupervised.\nIn addition, dialogue has been shown to exhibit characteristics that expose how language changes as conversation partners become more linguistically similar to each other over the course of the conversation (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011). Similarly, it has been shown that the linguistic patterns of individual users in online communities adapt to match those of the community they participate in (Nguyen and Rose\u0301, 2011; DanescuNiculescu-Mizil et al., 2013).\nFor this reason, we have selected reddit as a medium and compiled a dataset from large amounts of reddit data. Reddit comments live in a rich environment that is dependent on a large number of contextual factors, such as community, user, conversation, etc. Similar to Chen et al. (2016), we would like to learn representations that allow us to disentangle factors that are normally intertwined, such as style and genre, and that will allow us to gain more insight about the variation in language. To this end, we are currently training models that condition on different communities, users, and threads."}, {"heading": "4 Representing continuous domains", "text": "In line with past research (Daume\u0301 III, 2007; Zhou et al., 2016), we assume that every domain has an inherent low-dimensional structure, which allows its\nprojection into a lower dimensional subspace. In the discrete setting, we are given two domains, a source domain XS and a target domain XT . We represent examples in the source domain XS as xS1 , \u00b7 \u00b7 \u00b7 , xSnS \u2208 R\nd where xS1 is the i-th source example and nS is number of examples in XS . Similarly, we have nT target domain examples xT1 , \u00b7 \u00b7 \u00b7 , xTnT \u2208 R\nd. We now seek to learn a transformation W that allows us to transform the examples in the XS so that their distribution is more similar to the distribution of XT . Equivalently, we can factorize the transformation W into two transformations A and B with W = ABT that we can use to project the source and target examples into a joint subspace.\nWe assume that XS and XT lie on lowerdimensional orthonormal subspaces, S, T \u2208 RD\u00d7d, which can be represented as points on the Grassman manifold, G(d,D) as in Figure 1, where d D.\nIn computer vision, methods such as Subspace Alignment (Fernando et al., 2013) or the Geodesic Flow Kernel (Gong et al., 2012) have been used to find such transformations A and B. Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages.\nMany recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains. Similarly, in a sequence-to-sequence dialogue model (Vinyals and V. Le, 2015), we can not only train\nthe model to predict the source domain response, but also \u2013 via a reconstruction loss \u2013 its transformations to the target domain.\nFor continuous domains, we can assume that source domain XS and target domain XT are not independent, but that XT has evolved from XS based on a continuous process. This process can be indexed by time, e.g. in order to reflect how a language learner\u2019s style changes or how language varies as words rise and drop in popularity. We thus seek to learn a time-varying transformation Wt between S and T that allows us to transform between source and target examples dependent on t as in Figure 2.\nHoffman et al. (2014) assume a stream of observations z1, \u00b7 \u00b7 \u00b7 , znt \u2208 Rd drawn from a continuously changing domain and regularize Wt by encouraging the new subspace at t to be close to the previous subspace at t \u2212 1. Assuming a stream of (chronologically) ordered input data, a straightforward application of this to a representation-learning based dialogue model trains the parts of the model that auto-encode and transform the original message for each new example \u2013 possibly regularized with a smoothness constraint \u2013 while keeping the rest of the model fixed.\nThis can be seen as an unsupervised variant of fine-tuning, a common neural network domain adaptation baseline. As our learned transformation continuously evolves, we run the risk associated with fine-tuning of forgetting the knowledge acquired from the source domain. For this reason, neural\nnetwork architectures that are immune to forgetting, such as the recently proposed Progressive Neural Networks (Rusu et al., 2016) are appealing for continuous domain adaptation.\nWhile time is the most obvious dimension along which language evolves, other dimensions are possible: Geographical location influences dialectal variations as in Figure 3; socio-economic status, political affiliation as well as a domain\u2019s purpose or complexity all influence language and can thus be conceived as axes that span a manifold for embedding domain subspaces."}, {"heading": "5 Investigating language change", "text": "A continuous notion of domains naturally lends itself to a diachronic study of language. By looking at the representations produced by the model over different time steps, one gains insight into the change of language in a community or another domain. Similarly, observing how a user adapts their style to different users and communities reveals insights about the language of those entities.\nDomain mixture models use various domain similarity measures to determine how similar the languages of two domains are, such as Renyi divergence (Van Asch and Daelemans, 2010), KullbackLeibler (KL) divergence, Jensen-Shannon divergence, and vector similarity metrics (Plank and van\nNoord, 2011), as well as task-specific measures (Zhou et al., 2016).\nWhile word distributions have been used traditionally to compare domains, embedding domains in a manifold offers the possibility to evaluate the learned subspace representations. For this, cosine similarity as used for comparing word embeddings or KL divergence as used in the Variational Autoencoder (Kingma and Welling, 2013) are a natural fit."}, {"heading": "6 Evaluation", "text": "Our evaluation consists of three parts for evaluating the learned representations, the model, and the variation of language itself.\nFirstly, as our models produce new representations for every subspace, we can compare a snapshot of a domain\u2019s representation after every n time steps to chart a trajectory of its changes.\nSecondly, as we are conducting experiments on dialogue modeling, gold data for evaluation is readily available in the form of the actual response. We can thus train a model on reddit data of a certain period, adapt it to a stream of future conversations and evaluate its performance with BLEU or another metric that might be more suitable to expose variation in language. At the same time, human evaluations will reveal whether the generated responses are faithful to the target domain.\nFinally, the learned representations will allow us to investigate the variations in language. Ideally, we would like to walk the manifold and observe how language changes as we move from one domain to the other, similarly to (Radford et al., 2016)."}, {"heading": "7 Conclusion", "text": "We have proposed a notion of continuous natural language domains along with dialogue modeling as a test bed. We have presented a representation of continuous domains and detailed how this representation can be incorporated into representation learning-based models. Finally, we have outlined how these models can be used to investigate change and variation in language. While our models allow us to shed light on how language changes, models that can adapt to continuous changes are key for personalization and the reality of grappling with an ever-changing world."}], "references": [{"title": "Domain Adaptation with Structural Correspondence Learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "EMNLP \u201906 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, (July):120\u2013128.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Domain Separation Networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "NIPS.", "citeRegEx": "Bousmalis et al\\.,? 2016", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel."], "venue": "arXiv preprint arXiv:1606.03657.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "No Country for Old Members : User Lifecycle and Linguistic Change in Online Communities", "author": ["Cristian Danescu-Niculescu-Mizil", "Robert West", "Dan Jurafsky", "Christopher Potts."], "venue": "Proceedings of the 22nd international conference on World Wide Web, pages", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? 2013", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2013}, {"title": "Domain Adaptation for Statistical Classifiers", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Journal of Artificial Intelligence Research, 26:101\u2013126.", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "Association for Computational Linguistic (ACL)s, (June):256\u2013263.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Discourse Domains: The Cognitive Context of Speaking", "author": ["Dan Douglas."], "venue": "Diana Boxer and Andrew D. Cohen, editors, Studying Speaking to Inform Second Language Learning. Multilingual Matters.", "citeRegEx": "Douglas.,? 2004", "shortCiteRegEx": "Douglas.", "year": 2004}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462 \u2013 471.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment", "author": ["Basura Fernando", "Amaury Habrard", "Marc Sebban", "Tinne Tuytelaars", "K U Leuven", "Laboratoire Hubert", "Curien Umr", "Benoit Lauras."], "venue": "Proceedings of the IEEE International Conference on", "citeRegEx": "Fernando et al\\.,? 2013", "shortCiteRegEx": "Fernando et al\\.", "year": 2013}, {"title": "Geodesic Flow Kernel for Unsupervised Domain Adaptation", "author": ["Boqing Gong", "Yuan Shi", "Fei Sha", "Kristen Grauman."], "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Gong et al\\.,? 2012", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Continuous manifold based adaptation for evolving visual domains", "author": ["Judy Hoffman", "Trevor Darrell", "Kate Saenko."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 867\u2013874.", "citeRegEx": "Hoffman et al\\.,? 2014", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "arXiv preprint arXiv:1312.6114, (Ml):1\u201314.", "citeRegEx": "Kingma and Welling.,? 2013", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Entrainment in Speech Preceding Backchannels", "author": ["Rivka Levitan", "Agustn Gravano", "Julia Hirschberg."], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL/HLT), pages 113\u2013117.", "citeRegEx": "Levitan et al\\.,? 2011", "shortCiteRegEx": "Levitan et al\\.", "year": 2011}, {"title": "Domain Adaptation with Multiple Sources", "author": ["Yishay Mansour."], "venue": "NIPS, pages 1\u20138.", "citeRegEx": "Mansour.,? 2009", "shortCiteRegEx": "Mansour.", "year": 2009}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28\u201336.", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification", "author": ["Aditya Mogadala", "Achim Rettinger."], "venue": "NAACL, pages 692\u2013702.", "citeRegEx": "Mogadala and Rettinger.,? 2016", "shortCiteRegEx": "Mogadala and Rettinger.", "year": 2016}, {"title": "Language use as a reflection of socialization in online communities", "author": ["Dong Nguyen", "Carolyn P. Ros\u00e9."], "venue": "Proceedings of the Workshop on Languages in . . . , (June):76\u201385.", "citeRegEx": "Nguyen and Ros\u00e9.,? 2011", "shortCiteRegEx": "Nguyen and Ros\u00e9.", "year": 2011}, {"title": "Linguistic Style Matching in Social Interaction", "author": ["K.G. Niederhoffer", "J.W. Pennebaker."], "venue": "Journal of Language and Social Psychology, 21(4):337\u2013360.", "citeRegEx": "Niederhoffer and Pennebaker.,? 2002", "shortCiteRegEx": "Niederhoffer and Pennebaker.", "year": 2002}, {"title": "The Anthropology of Language: An Introduction to Linguistic Anthropology", "author": ["Harriet J. Oppenheimer."], "venue": "Wadsworth, Belmont (Canada).", "citeRegEx": "Oppenheimer.,? 2006", "shortCiteRegEx": "Oppenheimer.", "year": 2006}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang.,? 2010", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Effective Measures of Domain Similarity for Parsing", "author": ["Barbara Plank", "Gertjan van Noord."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 1:1566\u20131576.", "citeRegEx": "Plank and Noord.,? 2011", "shortCiteRegEx": "Plank and Noord.", "year": 2011}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "ICLR, pages 1\u201315.", "citeRegEx": "Radford et al\\.,? 2016", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Progressive Neural Networks", "author": ["Andrei A Rusu", "Neil C Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell", "Google Deepmind."], "venue": "arXiv preprint arXiv:1606.04671.", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Using Domain Similarity for Performance Estimation", "author": ["Vincent Van Asch", "Walter Daelemans."], "venue": "Computational Linguistics, (July):31\u201336.", "citeRegEx": "Asch and Daelemans.,? 2010", "shortCiteRegEx": "Asch and Daelemans.", "year": 2010}, {"title": "Bi-Transferring Deep Neural Networks for Domain Adaptation", "author": ["Guangyou Zhou", "Zhiwen Xie", "Jimmy Xiangji Huang", "Tingting He."], "venue": "ACL, pages 322\u2013332.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "In semantics, a domain is considered a \u201cspecific area of cultural emphasis\u201d (Oppenheimer, 2006) that entails a particular terminology, e.", "startOffset": 76, "endOffset": 95}, {"referenceID": 6, "context": "In discourse a domain is a \u201ccognitive construct (that is) created in response to a number of factors\u201d (Douglas, 2004) and includes a variety of registers.", "startOffset": 102, "endOffset": 117}, {"referenceID": 19, "context": ", xn} and xi is the ith feature vector (Pan and Yang, 2010).", "startOffset": 39, "endOffset": 59}, {"referenceID": 14, "context": "from a continuous underlying process that is reflected in many facets of natural language: The web contains an exponentially growing amount of data, where each document \u201cis potentially its own domain\u201d (McClosky et al., 2010); a second-language learner adapts their style as their command of the language improves; language changes with time and with locality; even the WSJ section of the Penn Treebank \u2013 often treated as a single domain \u2013 contains different types of documents, such as news, lists of stock prices, etc.", "startOffset": 201, "endOffset": 224}, {"referenceID": 0, "context": "In domain adaptation, a novel target domain is traditionally assumed to be discrete and independent of the source domain (Blitzer et al., 2006).", "startOffset": 121, "endOffset": 143}, {"referenceID": 13, "context": "Other research uses mixtures to model the target domain based on a single (Daum\u00e9 III and Marcu, 2006) or multiple discrete source domains (Mansour, 2009).", "startOffset": 138, "endOffset": 153}, {"referenceID": 16, "context": "Similarly, it has been shown that the linguistic patterns of individual users in online communities adapt to match those of the community they participate in (Nguyen and Ros\u00e9, 2011; DanescuNiculescu-Mizil et al., 2013).", "startOffset": 158, "endOffset": 218}, {"referenceID": 2, "context": "Similar to Chen et al. (2016), we would like to learn representations that allow us to disentangle factors that are normally intertwined, such as style and genre, and that will allow us to gain more insight about the variation in language.", "startOffset": 11, "endOffset": 30}, {"referenceID": 24, "context": "In line with past research (Daum\u00e9 III, 2007; Zhou et al., 2016), we assume that every domain has an inherent low-dimensional structure, which allows its", "startOffset": 27, "endOffset": 63}, {"referenceID": 8, "context": "In computer vision, methods such as Subspace Alignment (Fernando et al., 2013) or the Geodesic Flow Kernel (Gong et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 9, "context": ", 2013) or the Geodesic Flow Kernel (Gong et al., 2012) have been used to find such transformations A and B.", "startOffset": 36, "endOffset": 55}, {"referenceID": 7, "context": "Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages.", "startOffset": 47, "endOffset": 71}, {"referenceID": 15, "context": "Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages.", "startOffset": 96, "endOffset": 126}, {"referenceID": 1, "context": "Many recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains.", "startOffset": 42, "endOffset": 85}, {"referenceID": 24, "context": "Many recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains.", "startOffset": 42, "endOffset": 85}, {"referenceID": 22, "context": "network architectures that are immune to forgetting, such as the recently proposed Progressive Neural Networks (Rusu et al., 2016) are appealing for continuous domain adaptation.", "startOffset": 111, "endOffset": 130}, {"referenceID": 24, "context": "Domain mixture models use various domain similarity measures to determine how similar the languages of two domains are, such as Renyi divergence (Van Asch and Daelemans, 2010), KullbackLeibler (KL) divergence, Jensen-Shannon divergence, and vector similarity metrics (Plank and van Noord, 2011), as well as task-specific measures (Zhou et al., 2016).", "startOffset": 330, "endOffset": 349}, {"referenceID": 11, "context": "For this, cosine similarity as used for comparing word embeddings or KL divergence as used in the Variational Autoencoder (Kingma and Welling, 2013) are a natural fit.", "startOffset": 122, "endOffset": 148}, {"referenceID": 21, "context": "Ideally, we would like to walk the manifold and observe how language changes as we move from one domain to the other, similarly to (Radford et al., 2016).", "startOffset": 131, "endOffset": 153}], "year": 2016, "abstractText": "Humans continuously adapt their style and language to a variety of domains. However, a reliable definition of \u2018domain\u2019 has eluded researchers thus far. Additionally, the notion of discrete domains stands in contrast to the multiplicity of heterogeneous domains that humans navigate, many of which overlap. In order to better understand the change and variation of human language, we draw on research in domain adaptation and extend the notion of discrete domains to the continuous spectrum. We propose representation learningbased models that can adapt to continuous domains and detail how these can be used to investigate variation in language. To this end, we propose to use dialogue modeling as a test bed due to its proximity to language modeling and its social component.", "creator": "LaTeX with hyperref package"}}}