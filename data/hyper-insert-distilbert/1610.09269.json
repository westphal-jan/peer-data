{"id": "1610.09269", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Hierarchical Clustering via Spreading Metrics", "abstract": "we study merely the cost function computed for hierarchical clusterings introduced informally by [", "histories": [["v1", "Fri, 28 Oct 2016 15:30:21 GMT  (2214kb,D)", "http://arxiv.org/abs/1610.09269v1", "Extended abstract in proceedings of NIPS 2016"]], "COMMENTS": "Extended abstract in proceedings of NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aurko roy", "sebastian pokutta"], "accepted": true, "id": "1610.09269"}, "pdf": {"name": "1610.09269.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Clustering via Spreading Metrics", "authors": ["Aurko Roy", "Sebastian Pokutta"], "emails": ["aurko@gatech.edu", "sebastian.pokutta@isye.gatech.edu"], "sections": [{"heading": null, "text": "( log3/2 n ) times the cost of the optimal solution. We improve this by giving an O(log n)-\napproximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2016]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the assumption of the Small Set Expansion hypothesis."}, {"heading": "1 Introduction", "text": "Hierarchical clustering is an important method in cluster analysis where a data set is recursively partitioned into clusters of successively smaller size. They are typically represented by rooted trees where the root corresponds to the entire data set, the leaves correspond to individual data points and the intermediate nodes correspond to a cluster of its descendant leaves. Such a hierarchy represents several possible flat clusterings of the data at various levels of granularity; indeed every pruning of this tree returns a possible clustering. Therefore in situations where the number of desired clusters is not known beforehand, a hierarchical clustering scheme is often preferred to flat clustering. The most popular algorithms for hierarchical clustering are bottoms-up agglomerative algorithms like single linkage, average linkage and complete linkage. In terms of theoretical guarantees these algorithms are known to correctly recover a ground truth clustering if the similarity function on the data satisfies corresponding stability properties (see, e.g., [Balcan et al., 2008]). Often, however, one wishes to think of a good\nar X\niv :1\n61 0.\n09 26\n9v 1\n[ cs\n.L G\n] 2\n8 O\nclustering as optimizing some kind of cost function rather than recovering a hidden \u201cground truth\u201d. This is the standard approach in the classical clustering setting where popular objectives are k-means, k-median, minsum and k-center (see Chapter 14, [Friedman et al., 2001]). However as pointed out by [Dasgupta, 2016] for a lot of popular hierarchical clustering algorithms including linkage based algorithms, it is hard to pinpoint explicitly the cost function that these algorithms are optimizing. Moreover, much of the existing cost function based approaches towards hierarchical clustering evaluate a hierarchy based on a cost function for flat clustering, e.g., assigning the k-means or k-median cost to a pruning of this tree. Motivated by this, [Dasgupta, 2016] introduced a cost function for hierarchical clustering where the cost takes into account the entire structure of the tree rather than just the projections into flat clusterings. This cost function is shown to recover the intuitively correct hierarchies on several synthetic examples like planted partitions and cliques. In addition, a top-down graph partitioning algorithm is presented that outputs a tree with cost at most O(\u03b1n log n) times the cost of the optimal tree and where \u03b1n is the approximation guarantee of the Sparsest Cut subroutine used. Thus using the Leighton-Rao algorithm [Leighton and Rao, 1988, Leighton and Rao, 1999] or the AroraRao-Vazirani algorithm [Arora et al., 2009] gives an approximation factor of O ( log2 n ) and O ( log3/2 n\n) respectively. In this work we give a polynomial time algorithm to recover a hierarchical clustering of cost at most O(log n) times the cost of the optimal clustering according to this cost function. We also analyze a generalization of this cost function studied by [Dasgupta, 2016] and show that our algorithm still gives an O(log n) approximation in this setting. We do this by viewing the cost function in terms of the ultrametric it induces on the data, writing a convex relaxation for it and concluding by analyzing a popular rounding scheme used in graph partitioning algorithms. We also implement the integer program, its LP relaxation, and the rounding algorithm and test it on some synthetic and real world data sets to compare the cost of the rounded solutions to the true optimum as well as to compare its performance to other hierarchical clustering algorithms used in practice. Our experiments suggest that the hierarchies found by this algorithm are often better than the ones found by linkage based algorithms as well as the k-means algorithm in terms of the error of the best pruning of the tree compared to the ground truth."}, {"heading": "1.1 Related Work", "text": "The immediate precursor to this work is [Dasgupta, 2016] where the cost function for evaluating a hierarchical clustering was introduced. Prior to this there has been a long line of research on hierarchical clustering in the context of phylogenetics and taxonomy (see, e.g., [Jardine and Sibson, 1971, Sneath et al., 1973, Felsenstein and Felenstein, 2004]). Several authors have also given theoretical justifications for the success of the popular linkage based algorithms for hierarchical clustering (see, e.g. [Jardine and Sibson, 1968, Zadeh and Ben-David, 2009, Ackerman et al., 2010]). In terms of cost functions, one approach has been to evaluate a hierarchy in terms of the k-means or k-median cost that it induces (see [Dasgupta and Long, 2005]). The cost function and the top-down algorithm in [Dasgupta, 2016] can also be seen as a theoretical justification for several graph partitioning heuristics that are used in practice. Besides this prior work on hierarchical clustering we are also motivated by the long line of work in the classical clustering setting where a popular strategy is to study convex relaxations of these problems and to round an optimal fractional solution into an integral one with the aim of getting a good approximation to the cost function. A long line of work (see, e.g., [Charikar et al., 1999, Jain and Vazirani, 2001, Jain et al., 2003, Charikar and Li, 2012]) has employed this approach on LP relaxations for the k-median problem, including [Li and Svensson, 2013] which gives the best known approximation factor of 1 + \u221a 3 + \u03b5. Similarly, a few authors have studied LP and SDP relaxations for the k-means problem (see, e.g., [Peng and Xia, 2005, Peng and Wei, 2007, Awasthi et al., 2015]), while one of the best known algorithms for kernel k-means and spectral clustering is due to [Recht et al., 2012] which approximates the nonnegative matrix factorization (NMF) problem by LPs.\nLP relaxations for hierarchical clustering have also been studied in [Ailon and Charikar, 2005] where the objective is to fit a tree metric to a data set given pairwise dissimilarities. While the LP relaxation and rounding algorithm in [Ailon and Charikar, 2005] is similar in flavor, the result is incomparable to ours (see Section 7 for a discussion). Another work that is indirectly related to our approach is [Di Summa et al., 2015] where the authors study an ILP to obtain a closest ultrametric to arbitrary functions on a discrete set. Our approach is to give a combinatorial characterization of the ultrametrics induced by the cost function of [Dasgupta, 2016] which allows us to use the tools from [Di Summa et al., 2015] to model the problem as an ILP. The natural LP relaxation of this ILP turns out to be closely related to LP relaxations considered before for several graph partitioning problems (see, e.g., [Leighton and Rao, 1988, Leighton and Rao, 1999, Even et al., 1999, Krauthgamer et al., 2009]) and we use a rounding technique studied in this context to round this LP relaxation. Recently, we became aware of independent work by [Charikar and Chatziafratis, 2016] obtaining similar results for hierarchical clustering. In particular [Charikar and Chatziafratis, 2016] improve the approximation factor to O (\u221a log n ) by showing how to round a spreading metric SDP relaxation for this cost function. The analysis of this rounding procedure also enabled them to show that the top-down heuristic of [Dasgupta, 2016] actually returns an O( \u221a log n) approximate clustering rather than an O ( log3/2 n\n) approximate clustering. They also analyzed a very similar LP relaxation using the divide-and-conquer approximation algorithms using spreading metrics paradigm of [Even et al., 2000] together with a result of [Bartal, 2004] to show an O(log n) approximation. Finally, they also gave similar constant factor inapproximability results for this problem."}, {"heading": "1.2 Contribution", "text": "While studying convex relaxations of optimization problems is fairly natural, for the cost function introduced in [Dasgupta, 2016] however, it is not immediately clear how one would go about writing such a relaxation. Our first contribution is to give a combinatorial characterization of the family of ultrametrics induced by this cost function on hierarchies. Inspired by the approach in [Di Summa et al., 2015] where the authors study an integer linear program for finding the closest ultrametric, we are able to formulate the problem of finding the minimum cost hierarchical clustering as an integer linear program. Interestingly and perhaps unsurprisingly, the specific family of ultrametrics induced by this cost function give rise to linear constraints studied before in the context of finding balanced separators in weighted graphs. We then show how to round an optimal fractional solution using the sphere growing technique first introduced in [Leighton and Rao, 1988] (see also [Garg et al., 1996, Even et al., 1999, Charikar et al., 2003]) to recover a tree of cost at most O(log n) times the optimal tree for this cost function. The generalization of this cost function involves scaling every pairwise distances by an arbitrary strictly increasing function f satisfying f (0) = 0. We modify the integer linear program for this general case and show that the rounding algorithm still finds a hierarchical clustering of cost at most O(log n) times the optimal clustering in this setting. We also show a constant factor inapproximability result for this problem for any polynomial sized LP and SDP relaxations and under the assumption of the Small Set Expansion hypothesis. We conclude with an experimental study of the integer linear program and the rounding algorithm on some synthetic and real world data sets to show that the approximation algorithm often recovers clusters close to the true optimum (according to this cost function) and that its projections into flat clusters often has a better error rate than the linkage based algorithms and the k-means algorithm."}, {"heading": "2 Preliminaries", "text": "A similarity based clustering problem consists of a dataset V of n points and a similarity function \u03ba : V \u00d7 V \u2192 R\u22650 such that \u03ba(i, j) is a measure of the similarity between i and j for any i, j \u2208 V. We will assume that the similarity function is symmetric i.e., \u03ba(i, j) = \u03ba(j, i) for every i, j \u2208 V. Note that we do not make any assumptions about the points in V coming from an underlying metric space. For a given instance of a clustering problem we have an associated weighted complete graph Kn with vertex set V and weight function\ngiven by \u03ba. A hierarchical clustering of V is a tree T with a designated root r and with the elements of V as its leaves, i.e., leaves(T) = V. For any set S \u2286 V we denote the lowest common ancestor of S in T by lca(S). For pairs of points i, j \u2208 V wewill abuse the notation for the sake of simplicity and denote lca({i, j}) simply by lca(i, j). For a node v of T we denote the subtree of T rooted at v by T[v]. The following cost function was introduced by [Dasgupta, 2016] to measure the quality of the hierarchical clustering T\ncost(T) := \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) |leaves(T[lca(i, j)])| . (1)\nThe intuition behind this cost function is as follows. Let T be a hierarchical clustering with designated root r so that r represents the whole data set V. Since leaves(T) = V, every internal node v \u2208 T represents a cluster of its descendant leaves, with the leaves themselves representing singleton clusters of V. Starting from r and going down the tree, every distinct pair of points i, j \u2208 V will be eventually separated at the leaves. If \u03ba(i, j) is large, i.e., i and j are very similar to each other then we would like them to be separated as far down the tree as possible if T is a good clustering of V. This is enforced in the cost function (1): if \u03ba(i, j) is large then the number of leaves of lca(i, j) should be small i.e., lca(i, j) should be far from the root r of T. Such a cost function is not unique however; see Section 7 for some other cost functions of a similar flavor. Note that while requiring \u03ba to be non-negative might seem like an artificial restriction, cost function (1) breaks down when all the \u03ba(i, j) < 0, since in this case the trivial clustering r, T\u2217 where T\u2217 is the star graph with V as its leaves is always the minimizer. Therefore in the rest of this work we will assume that \u03ba \u2265 0. This is not a restriction compared to [Dasgupta, 2016], since the Sparsest Cut algorithm used as a subroutine also requires this assumption. Let us now briefly recall the notion of an ultrametric.\nDefinition 2.1 (Ultrametric). An ultrametric on a set X of points is a distance function d : X \u00d7 X \u2192 R satisfying the following properties for every x, y, z \u2208 X\n1. Nonnegativity: d(x, y) \u2265 0 with d(x, y) = 0 iff x = y\n2. Symmetry: d(x, y) = d(y, x)\n3. Strong triangle inequality: d(x, y) \u2264 max{d(y, z), d(z, x)}\nUnder the cost function (1), one can interpret the tree T as inducing an ultrametric dT on V given by dT(i, j) := |leaves(T[lca (i, j)])| \u2212 1. This is an ultrametric since dT(i, j) = 0 iff i = j and for any triple i, j, k \u2208 V we have dT(i, j) \u2264 max{dT(i, k), dT(j, k)}. The following definition introduces the notion of non-trivial ultrametrics. These turn out to be precisely the ultrametrics that are induced by tree decompositions of V corresponding to cost function (1), as we will show in Corollary 3.4.\nDefinition 2.2. An ultrametric d on a set of points V is non-trivial if the following conditions hold.\n1. For every non-empty set S \u2286 V, there is a pair of points i, j \u2208 S such that d(i, j) \u2265 |S| \u2212 1.\n2. For any t if St is an equivalence class ofV under the relation i \u223c j iff d(i, j) \u2264 t, thenmaxi,j\u2208St d(i, j) \u2264 |St| \u2212 1.\nNote that for an equivalence class St where d(i, j) \u2264 t for every i, j \u2208 St it follows from Condition 1 that t \u2265 |St| \u2212 1. Thus in the case when t = |St| \u2212 1 the two conditions imply that the maximum distance between any two points in S is t and that there is a pair i, j \u2208 S for which this maximum is attained. The following lemma shows that non-trivial ultrametrics behave well under restrictions to equivalence classes St of the form i \u223c j iff d(i, j) \u2264 t.\nLemma 2.3. Let d be a non-trivial ultrametric on V and let St \u2286 V be an equivalence class under the relation i \u223c j iff d(i, j) \u2264 t. Then d restricted to St is a non-trivial ultrametric on St.\nProof. Clearly d restricted to St is an ultrametric on St and sowe need to establish that it satisfies Conditions 1 and 2 of Definition 2.2. Let S \u2286 St be any set. Since d is a non-trivial ultrametric on V it follows that there is a pair i, j \u2208 S with d(i, j) \u2265 |S| \u2212 1, and so d restricted to St satisfies Condition 1. If S\u2032r is an equivalence class in St under the relation i \u223c j iff d(i, j) \u2264 r then clearly S\u2032r = St if r > t. Since d is a non-trivial ultrametric on V, it follows that maxi,j\u2208S\u2032r d(i, j) = maxi,j\u2208St d(i, j) \u2264 |St| \u2212 1 = |S\u2032r| \u2212 1. Thus we may assume that r \u2264 t. Consider an i \u2208 S\u2032r and let j \u2208 V be such that d(i, j) \u2264 r. Since r \u2264 t and i \u2208 St, it follows that j \u2208 St and so j \u2208 S\u2032r. In other words S\u2032r is an equivalence class in V under the relation i \u223c j iff d(i, j) \u2264 r. Since d is an ultrametric on V it follows that maxi,j\u2208S\u2032r d(i, j) \u2264 |S\u2032r| \u2212 1. Thus d restricted to St satisfies Condition 2.\nThe intuition behind the two conditions in Definition 2.2 is as follows. Condition 1 imposes a certain lower bound by ruling out trivial ultrametrics where, e.g., d(i, j) = 1 for every distinct pair i, j \u2208 V. On the other hand Condition 2 discretizes and imposes an upper bound on d by restricting its range to the set {0, 1, . . . , n\u2212 1} (see Lemma 2.4). This rules out the other spectrum of triviality where for example d(i, j) = n for every distinct pair i, j \u2208 V with |V| = n.\nLemma 2.4. Let d be a non-trivial ultrametric on the set V as in Definition 2.2. Then the range of d is contained in the set {0, 1, . . . , n\u2212 1} with |V| = n.\nProof. We will prove this by induction on |V|. The base case when |V| = 1 is trivial. Therefore, we now assume that |V| > 1. By Condition 1 there is a pair i, j \u2208 V such that d(i, j) \u2265 n \u2212 1. Let t = maxi,j\u2208V d(i, j), then the only equivalence class under the relation i \u223c j iff d(i, j) \u2264 t is V. By Condition 2 it follows that maxi,j\u2208V d(i, j) = t = n\u2212 1. Let V1, . . . Vm denote the set of equivalence classes of V under the relation i \u223c j iff d(i, j) \u2264 n \u2212 2. Note that m > 1 as there is a pair i, j \u2208 V with d(i, j) = n \u2212 1, and therefore each Vl ( V. By Lemma 2.3, d restricted to each of these Vi\u2019s is a non-trivial ultrametric on those sets. The claim then follows immediately: for any i, j \u2208 V either i, j \u2208 Vl for some Vl in which case by the induction hypothesis d(i, j) \u2208 {0, 1, . . . , |Vl | \u2212 1}, or i \u2208 Vl and j \u2208 Vl\u2032 for l 6= l\u2032 in which case d(i, j) = n\u2212 1."}, {"heading": "3 Ultrametrics and Hierarchical Clusterings", "text": "We start with the following easy lemma about the lowest common ancestors of subsets of V in a hierarchical clustering T of V.\nLemma 3.1. Let S \u2286 V with |S| \u2265 2. If r = lca(S) then there is a pair i, j \u2208 S such that lca(i, j) = r.\nProof. We will proceed by induction on |S|. If |S| = 2 then the claim is trivial and so we may assume |S| > 2. Let i \u2208 S be an arbitrary point and let r\u2032 = lca(S \\ {i}). We claim that r = lca(i, r\u2032). Clearly the subtree rooted at lca(i, r\u2032) contains S and since T[r] is the smallest such tree it follows that r \u2208 T[lca(i, r\u2032)]. Conversely, T[r] contains S \\ {i} and so r\u2032 \u2208 T[r] and since i \u2208 T[r], it follows that lca(i, r\u2032) \u2208 T[r]. Thus we conclude that r = lca(i, r\u2032). If lca(i, r\u2032) = r\u2032, then we are done by the induction hypothesis. Thus we may assume that i /\u2208 T[r\u2032]. Consider any j \u2208 S such that j \u2208 T[r\u2032]. Then we have that lca(i, j) = r as lca(i, r\u2032) = r and j \u2208 T[r\u2032] and i /\u2208 T[r\u2032].\nWe will now show that non-trivial ultrametrics on V as in Definition 2.2 are exactly those that are induced by hierarchical clusterings on V under cost function (1). The following lemma shows the forward direction: the ultrametric dT induced by any hierarchical clustering T is non-trivial.\nLemma 3.2. Let T be a hierarchical clustering on V and let dT be the ultrametric on V induced by it. Then dT is non-trivial.\nProof. Let S \u2286 V be arbitrary and r = lca(S), then T[r] has at least |S| leaves. By Lemma 3.1 there must be a pair i, j \u2208 S such that r = lca(i, j) and so dT(i, j) \u2265 |S| \u2212 1. This satisfies Condition 1 of non-triviality. For any t, let St be a non-empty equivalence class under the relation i \u223c j iff dT(i, j) \u2264 t. Since dT satisfies Condition 1 it follows that |St| \u2212 1 \u2264 t. Let us assume for the sake of contradiction that there is a pair i, j \u2208 St such that dT(i, j) > |St| \u2212 1. Let r = lca(St); using the definition of dT it follows that t + 1 \u2265 |leaves (T[r])| > |St| since i, j \u2208 St. Let k \u2208 leaves (T[r]) \\ St be an arbitrary point, then for every l \u2208 St it follows that dT(k, l) \u2264 |leaves(T[r])| \u2212 1 \u2264 t since the subtree rooted at r contains both k and l. This is a contradiction to St being an equivalence class under i \u223c j iff dT(i, j) \u2264 t since k /\u2208 St. Thus dT also satisfies Condition 2 of Definition 2.2.\nThe following crucial lemma shows the converse: every non-trivial ultrametric on V is realized by a hierarchical clustering T of V.\nLemma 3.3. For every non-trivial ultrametric d on V there is a hierarchical clustering T on V such that for any pair i, j \u2208 V we have\ndT(i, j) = |leaves(T[lca (i, j)])| \u2212 1 = d(i, j).\nMoreover this hierarchy can be constructed in time O ( n3 ) by Algorithm 1 where |V| = n.\nProof. The proof is by induction on n. The base case when n = 1 is straightforward. We now suppose that the statement is true for sets of size < n. Note that i \u223c j iff d(i, j) \u2264 n\u2212 2 is an equivalence relation on V and thus partitions V into m equivalence classes V1, . . . , Vm. We first observe that m > 1 since by Condition 1 there is a pair of points i, j \u2208 V such that d(i, j) \u2265 n\u2212 1 and in particular |V|l < n for every l \u2208 {1, . . . , m}. By Lemma 2.3, d restricted to any Vl is a non-trivial ultrametric on Vl and there is a pair of points i, j \u2208 Vl such that d(i, j) = |Vl | \u2212 1 by Conditions 1 and 2. Therefore by the induction hypothesis we construct trees T1, . . . , Tm such that for every l \u2208 {1, . . . , m} we have leaves(Tl) = Vl . Further for any pair of points i, j \u2208 Vl for some l \u2208 {1, . . . , m}, we also have d(i, j) = dTl (i, j). We construct the tree T as follows: we first add a root r and then connect the root rl of Tl to r for every l \u2208 {1, . . . , m}. Consider a pair of points i, j \u2208 V. If i, j \u2208 Vl for some l \u2208 {1, . . . , m} then we are done since dTl (i, j) = dT(i, j) as lca(i, j) \u2208 Tl . If i \u2208 Vl and j \u2208 Vl\u2032 for some l 6= l\u2032 then d(i, j) = n \u2212 1 since d(i, j) \u2265 n\u2212 1 by definition of the equivalence relation and the range of d lies in {0, 1, . . . , n\u2212 1} by Lemma 2.4. Moreover i and j are leaves in Tl and Tl\u2032 respectively, and thus by construction of T we have lca(i, j) = r, i.e., dT(i, j) = n\u2212 1 and so the claim follows. Algorithm 1 simulates this inductive argument can be easily implemented to run in time O ( n3 ) .\nLemmas 3.2 and 3.3 together imply the following corollary about the equivalence of hierarchical clusterings and non-trivial ultrametrics.\nCorollary 3.4. There is a bijection between the set of hierarchical clusterings T on V and the set of nontrivial ultrametrics d on V satisfying the following conditions.\n1. For every hierarchical clustering T on V, there is a non-trivial ultrametric dT defined as dT(i, j) := |leaves T[lca(i, j)]| \u2212 1 for every i, j \u2208 V.\n2. For every non-trivial ultrametric d on V, there is a hierarchical clustering T on V such that for every i, j \u2208 V we have |leaves T[lca(i, j)]| \u2212 1 = d(i, j).\nMoreover this bijection can be computed in O(n3) time, where |V| = n.\nInput: Data set V of n points, non-trivial ultrametric d : V \u00d7V \u2192 R\u22650 Output: Hierarchical clustering T of V with root r\n1 r \u2190 arbitrary choice of designated root in V 2 X \u2190 {r} 3 E\u2190 \u2205 4 if n = 1 then 5 T \u2190 (X, E) 6 return r, T 7 else 8 Partition V into {V1, . . . Vm} under the equivalence relation i \u223c j iff d(i, j) < n\u2212 1 9 for l \u2208 {1, . . . , m} do\n10 Let rl , Tl be output of Algorithm 1 on Vl , d|Vl 11 X \u2190 X \u222aV(Tl) 12 E\u2190 E \u222a {r, rl} 13 end 14 T \u2190 (X, E) 15 return r, T 16 end\nAlgorithm 1: Hierarchical clustering of V from non-trivial ultrametric\nTherefore to find the hierarchical clustering of minimum cost, it suffices to minimize \u3008\u03ba, d\u3009 over non-trivial ultrametrics d : V \u00d7 V \u2192 {0, . . . , n \u2212 1}, where V is the data set. Note that the cost of the ultrametric dT corresponding to a tree T is an affine offset of cost(T). In particular, we have \u3008\u03ba, dT\u3009 = cost(T) \u2212 \u2211{i,j}\u2208E(Kn) \u03ba(i, j). A natural approach is to formulate this problem as an Integer Linear Program (ILP) and then study LP or SDP relaxations of it. We consider the following ILP for this problem that ismotivated by [Di Summa et al., 2015]. We have the variables x1ij, . . . , x n\u22121 ij for every distinct pair i, j \u2208 V with xtij = 1 if and only if d(i, j) \u2265 t. For any positive integer n, let [n] := {1, 2, . . . , n}.\nmin n\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j)xtij (ILP-ultrametric)\ns.t. xtij \u2265 xt+1ij \u2200i, j \u2208 V, t \u2208 [n\u2212 2] (2) xtij + x t jk \u2265 xtik \u2200i, j, k \u2208 V, t \u2208 [n\u2212 1] (3)\n\u2211 i,j\u2208S xtij \u2265 2 \u2200t \u2208 [n\u2212 1], S \u2286 V, |S| = t + 1 (4)\n\u2211 i,j\u2208S\nx|S|ij \u2264 |S| \u2211 i,j\u2208S\nxtij + \u2211 i\u2208S j/\u2208S\n( 1\u2212 xtij ) \u2200t \u2208 [n\u2212 1], S \u2286 V (5) xtij = x t ji \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (6) xtii = 0 \u2200i \u2208 V, t \u2208 [n\u2212 1] (7) xtij \u2208 {0, 1} \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (8)\nConstraints (2) and (7) follow from the interpretation of the variables xtij: if d(i, j) \u2265 t, i.e., xtij = 1 then\nclearly d(i, j) \u2265 t\u2212 1 and so xt\u22121ij = 1. Furthermore, for any i \u2208 V we have d(i, i) = 0 and so xtii = 0 for every t \u2208 [n\u2212 1]. Note that constraint (3) is the same as the strong triangle inequality (Definition 2.1) since the variables xtij are in {0, 1}. Constraint 6 ensures that the ultrametric is symmetric. Constraint 4 ensures the ultrametric satisfies Condition 1 of non-triviality: for every S \u2286 V of size t + 1 we know that there must be points i, j \u2208 S such that d(i, j) = d(j, i) \u2265 t or in other words xtij = xtji = 1. Constraint 5 ensures that the ultrametric satisfies Condition 2 of non-triviality. To see this note that the constraint is active only when \u2211i,j\u2208S xtij = 0 and \u2211i\u2208S,j/\u2208S(1\u2212 xtij) = 0. In other words d(i, j) \u2264 t\u2212 1 for every i, j \u2208 S and S is a maximal such set since if i \u2208 S and j /\u2208 S then d(i, j) \u2265 t. Thus S is an equivalence class under the relation i \u223c j iff d(i, j) \u2264 t\u2212 1 and so for every i, j \u2208 S we have d(i, j) \u2264 |S| \u2212 1 or equivalently x|S|ij = 0. The ultrametric d represented by a feasible solution xtij is given by d(i, j) = \u2211 n\u22121 t=1 x t ij.\nDefinition 3.5. For any { xtij | t \u2208 [n\u2212 1], i, j \u2208 V } let Et be defined as Et := { {i, j} | xtij = 0 } . Note that if xtij is feasible for ILP-ultrametric then Et \u2286 Et+1 for any t since xtij \u2265 xt+1ij . The sets {Et}n\u22121t=1 induce a natural sequence of graphs {Gt}n\u22121t=1 where Gt = (V, Et) with V being the data set. For a fixed t \u2208 {1, . . . , n \u2212 1} it is instructive to study the combinatorial properties of the so called layer-t problem, where we restrict ourselves to the constraints corresponding to that particular t and drop constraints (2) and (5) since they involve different layers in their expression.\nmin \u2211 {i,j}\u2208E(Kn) \u03ba(i, j)xtij (ILP-layer)\ns.t. xtij + x t jk \u2265 xtik \u2200i, j, k \u2208 V (9)\n\u2211 i,j\u2208S xtij \u2265 2 \u2200S \u2286 V, |S| = t + 1 (10)\nxtij = x t ji \u2200i, j \u2208 V (11) xtii = 0 \u2200i \u2208 V (12) xtij \u2208 {0, 1} \u2200i, j \u2208 V (13)\nThe following lemma provides a combinatorial characterization of feasible solutions to the layer-t problem.\nLemma 3.6. Let Gt = (V, Et) be the graph as in Definition 3.5 corresponding to a solution xtij to the layer-t problem ILP-layer. Then Gt is a disjoint union of cliques of size \u2264 t. Moreover this exactly characterizes all feasible solutions of ILP-layer.\nProof. We first note that Gt = (V, Et)must be a disjoint union of cliques since if {i, j} \u2208 Et and {j, k} \u2208 Et then {i, k} \u2208 Et since xtik \u2264 xtij + xtjk = 0 due to constraint (9). Suppose there is a clique in Gt of size > t. Choose a subset S of this clique of size t + 1. Then \u2211i,j\u2208S xtij = 0 which violates constraint (10). Conversely, let Et be a subset of edges such that Gt = (V, Et) is a disjoint union of cliques of size\u2264 t. Let xtij = 0 if {i, j} \u2208 Et and 1 otherwise. Clearly xtij = xtji by definition. Suppose xtij violates constraint (9), so that there is a pair i, j, k \u2208 V such that xtik = 1 but xtij = xtjk = 0. However this implies that Gt is not a disjoint union of cliques since {i, j}, {j, k} \u2208 Et but {i, k} /\u2208 Et. Suppose xtij violates constraint (10) for some set S of size t + 1. Therefore for every i, j \u2208 S, we have xtij = 0 since xtij = xtji for every i, j \u2208 V and so S must be a clique of size t + 1 in Gt which is a contradiction.\nBy Lemma 3.6 the layer-t problem is to find a subset Et \u2286 E(Kn) of minimum weight under \u03ba, such that the complement graph Gt = (V, Et) is a disjoint union of cliques of size \u2264 t. Note that this implies that\nthe number of components in the complement graph is \u2265 dn/te.The converse however, is not necessarily true: when t = n \u2212 1 then the layer t-problem is the minimum (weighted) cut problem whose partitions may have size larger than 1. Our algorithmic approach is to solve an LP relaxation of ILP-ultrametric and then round the solution to obtain a feasible solution to ILP-ultrametric. The rounding however proceeds iteratively in a layer-wise manner and so we need to make sure that the rounded solution satisfies the interlayer constraints (2) and (5). The following lemma gives a combinatorial characterization of solutions that satisfy these two constraints.\nLemma 3.7. For every t \u2208 [n\u2212 1], let xtij be feasible for the layer-t problem ILP-layer. Let Gt = (V, Et) be the graph as in Definition 3.5 corresponding to xtij, so that by Lemma 3.6, Gt is a disjoint union of cliques Kt1, . . . , K t lt each of size at most t. Then x t ij is feasible for ILP-ultrametric if and only if the following conditions hold.\nNested cliques For any s \u2264 t every clique Ksp for some p \u2208 [ls] in Gs is a subclique of some clique Ktq in Gt where q \u2208 [lt].\nRealization If \u2223\u2223\u2223Ktp\u2223\u2223\u2223 = s for some s \u2264 t, then Gs contains Ktp as a component clique, i.e., Ksq = Ktp for some\nq \u2208 [ls].\nProof. Since xtij is feasible for the layer-t problem ILP-layer it is feasible for ILP-ultrametric if and only if it satisfies constraints (2) and (5). The solution xtij satisfies constraint (2) if and only if Et \u2286 Et+1 by definition and so Condition Nested cliques follows. Let us now assume that xtij is feasible for ILP-ultrametric, so that by the above argument Condition Nested cliques is satisfied. Note that every clique Ktp in the clique decomposition of Gt corresponds to an equivalence class St under the relation i \u223c j iff xtij = 0. Moreover, by Lemma 3.6 we have |St| \u2264 t. Constraint (5) implies that x|St|ij = 0 for every i, j \u2208 St. In other words, if |St| = s \u2264 t, then xsij = 0 for every i, j \u2208 St and so St is a subclique of some clique Ksq in the clique decomposition of Gs. However by Condition Nested cliques, Ksq must be a subclique of a clique Ktp\u2032 in the clique decomposition of Gt, since s \u2264 t. However, as Ktp \u2229 Ktp\u2032 = St and the clique decomposition decomposes Gt into a disjoint union of cliques, it follows that St \u2286 Ksq \u2286 Ktp\u2032 = Ktp = St and so Ksq = Ktp. Therefore Condition Realization is satisfied. Conversely, suppose that xtij satisfies Conditions Nested cliques and Realization, so that by the argument in the paragraph above xtij satisfies constraint (2). Let us assume for the sake of contradiction that for a set S \u2286 V and a t \u2208 [n\u2212 1] constraint (5) is violated, i.e.,\n\u2211 i,j\u2208S\nx|S|ij > |S| \u2211 i,j\u2208S\nxtij + \u2211 i\u2208S j/\u2208S\n( 1\u2212 xtij ) . Since xtij \u2208 {0, 1} it follows that xtij = 0 for every i, j \u2208 S and xtij = 1 for every i \u2208 S, j /\u2208 S so that S is a clique in Gt. Note that |S| < t since \u2211i,j\u2208S x|S|ij > 0. This contradicts Condition Realization however, since S is clearly not a clique in G|S|.\nThe combinatorial interpretation of the individual layer-t problems allow us to simplify the formulation of ILP-ultrametric by replacing the constraints for sets of a specific size (constraint (4)) by a global constraint about all sets (constraint (14)).\nLemma 3.8. We may replace constraint (4) of ILP-ultrametric by the following equivalent constraint\n\u2211 j\u2208S xtij \u2265 |S| \u2212 t \u2200t \u2208 [n\u2212 1], S \u2286 V, i \u2208 S. (14)\nProof. Let xtij be a feasible solution to ILP-ultrametric. Note that if |S| \u2264 t then the constraints are redundant since xtij \u2208 {0, 1}. Thus we may assume that |S| > t and let i be any vertex in S. Let us suppose for the sake of a contradiction that \u2211j\u2208S xtij < |S| \u2212 t. This implies that there is a t sized subset S\u2032 \u2286 S \\ {i} such that for every j \u2208 S\u2032 we have xtij\u2032 = 0. In other words {i, j\u2032} is an edge in Gt = (V, Et) for every j\u2032 \u2208 S\u2032 and since Gt is a disjoint union of cliques (constraint (3)), this implies the existence of a clique of size t + 1. Thus by Lemma 3.6, xtij could not have been a feasible solution to ILP-ultrametric. Conversely, suppose xtij is feasible for the modified ILP where constraint (4) is replaced by constraint (14). Then again Gt = (V, Et) is a disjoint union of cliques since xtij satisfies constraint (3). Assume for contradiction that constraint (4) is violated: there is a set S of size t + 1 such that \u2211i,j\u2208S xtij < 2. Note that this implies that \u2211i,j xtij = 0 since x t ij = x t ji for every i, j \u2208 V and t \u2208 [n \u2212 1]. Fix any i \u2208 S, then \u2211j\u2208S xtij < 1 = |S| \u2212 t since xtij = xtji by constraint (6), a violation of constraint (14). Thus xtij is feasible for ILP-ultrametric since it satisfies every other constraint by assumption."}, {"heading": "4 Rounding an LP relaxation", "text": "In this section we consider the following natural LP relaxation for ILP-ultrametric. We keep the variables xtij for every t \u2208 [n \u2212 1] and i, j \u2208 V but relax the integrality constraint on the variables as well as drop constraint (5).\nmin n\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j)xtij (LP-ultrametric)\ns.t. xtij \u2265 xt+1ij \u2200i, j \u2208 V, t \u2208 [n\u2212 2] (15) xtij + x t jk \u2265 xtik \u2200i, j, k \u2208 V, t \u2208 [n\u2212 1] (16)\n\u2211 j\u2208S xtij \u2265 |S| \u2212 t \u2200t \u2208 [n\u2212 1], S \u2286 V, i \u2208 S (17)\nxtij = x t ji \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (18) xtii = 0 \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (19) 0 \u2264 xtij \u2264 1 \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (20)\nA feasible solution xtij to LP-ultrametric induces a sequence {dt}t\u2208[n\u22121] of distance metrics over V defined as dt(i, j) := xtij. Constraint 17 enforces an additional structure on this metric: informally points in a \u201clarge enough\u201d subset S should be spread apart according to the metric dt. Metrics of type dt are called spreading metrics and were first studied in [Even et al., 1999, Even et al., 2000] in relation to graph partitioning problems. The following lemma gives a technical interpretation of spreading metrics (see, e.g., [Even et al., 1999, Even et al., 2000, Krauthgamer et al., 2009]); we include a proof for completeness.\nLemma 4.1. Let xtij be feasible for LP-ultrametric and for a fixed t \u2208 [n\u2212 1], let dt be the induced spreading metric. Let i \u2208 V be an arbitrary vertex and let S \u2286 V be a set with i \u2208 S such that |S| > (1+ \u03b5)t for some \u03b5 > 0. Then maxj\u2208S dt(i, j) > \u03b51+\u03b5 .\nProof. For the sake of a contradiction suppose that for every j \u2208 S we have dt(i, j) = xtij \u2264 \u03b51+\u03b5 . This implies that xtij violates constraint (17) leading to a contradiction:\n\u2211 j\u2208S\nxtij \u2264 \u03b5\n1 + \u03b5 |S| < |S| \u2212 t,\nwhere the last inequality follows from |S| > (1 + \u03b5)t.\nThe following lemma shows that we can optimize over LP-ultrametric in polynomial time. Lemma 4.2. An optimal solution to LP-ultrametric can be computed in time polynomial in n and log ( maxi,j \u03ba(i, j) ) .\nProof. Weargue in the standard fashion via the application of the Ellipsoidmethod (see e.g., [Schrijver, 1998]). As such it suffices to verify that the encoding length of the numbers is small (which is indeed the case here) and that the constraints can be separated in polynomial time in the size of the input, i.e., in n and the logarithm of the absolute value of the largest coefficient. Since constraints of type (15), (16), (18), and (19) are polynomially many in n, we only need to check separation for constraints of type (17). Given a claimed solution xtij we can check constraint (17) by iterating over all t \u2208 [n\u2212 1], vertices i \u2208 V, and sizes m of the set S from t+ 1 to n. For a fixed t, i, and set size m sort the vertices in V \\ {i} in increasing order of distance from i (according to the metric dt) and let S be the first m vertices in this ordering. If \u2211j\u2208S x t ij < m\u2212 t then clearly xtij is not feasible for LP-ultrametric, so we may assume that \u2211j\u2208S x t ij \u2265 m\u2212 t. Moreover this is the only set to check: for any set S \u2286 V containing i such that |S| = m, \u2211j\u2208S xtij \u2265 \u2211j\u2208S xtij \u2265 m\u2212 t. Thus for a fixed t \u2208 [n\u2212 1], i \u2208 V and set size m, it suffices to check that xtij satisfies constraint (17) for this subset S.\nFrom now on we will simply refer to a feasible solution to LP-ultrametric by the sequence of spreading metrics {dt}t\u2208[n\u22121] it induces. The following definition introduces the notion of an open ball BU (i, r, t) of radius r centered at i \u2208 V according to the metric dt and restricted to the set U \u2286 V.\nDefinition 4.3. Let {dt | t \u2208 [n\u2212 1]} be the sequence of spreading metrics feasible for LP-ultrametric. Let U \u2286 V be an arbitrary subset of V. For a vertex i \u2208 U, r \u2208 R, and t \u2208 [n\u2212 1] we define the open ball BU (i, r, t) of radius r centered at i as\nBU (i, r, t) := {j \u2208 U | dt(i, j) < r} \u2286 U.\nIf U = V then we denote BU (i, r, t) simply by B (i, r, t).\nRemark 4.4. For every pair i, j \u2208 V we have dt(i, j) \u2265 dt+1(i, j) by constraint (15). Thus for any subset U \u2286 V, i \u2208 U, r \u2208 R, and t \u2208 [n\u2212 2], it holds BU (i, r, t) \u2286 BU (i, r, t + 1).\nTo round LP-ultrametric to get a feasible solution for ILP-ultrametric, we will use the technique of sphere growingwhich was introduced in [Leighton and Rao, 1988] to show anO(log n) approximation for the maximum multicommodity flow problem. Recall from Lemma 3.6 that a feasible solution to ILP-layer consists of a decomposition of the graph Gt into a set of disjoint cliques of size at most t. One way to obtain such a decomposition is to choose an arbitrary vertex, grow a ball around this vertex until the expansion of this ball is below a certain threshold, chop off this ball and declare it as a partition and then recurse on the remaining vertices. This is the main idea behind sphere growing, and the parameters are chosen depending on the constraints of the specific problem (see, e.g., [Garg et al., 1996, Even et al., 1999, Charikar et al., 2003] for a few representative applications of this technique). The first step is to associate to every ball BU (i, r, t) a\nvolume vol (BU (i, r, t)) and a boundary \u2202BU (i, r, t) so that its expansion is defined. For any t \u2208 [n\u2212 1] and U \u2286 V we denote by \u03b3Ut the value of the layer-t objective for solution dt restricted to the set U, i.e.,\n\u03b3Ut := \u2211 i,j\u2208U i<j \u03ba(i, j)dt(i, j).\nWhen U = V we refer to \u03b3Ut simply by \u03b3t. Since \u03ba : V \u00d7 V \u2192 R\u22650, it follows that \u03b3Ut \u2264 \u03b3t for any U \u2286 V. We are now ready to define the volume, boundary, and expansion of a ball BU (i, r, t). We use the definition of [Even et al., 1999] modified for restrictions to arbitrary subsets U \u2286 V.\nDefinition 4.5. [Even et al., 1999] Let U be an arbitrary subset of V. For a vertex i \u2208 U, radius r \u2208 R\u22650, and t \u2208 [n\u2212 1], let BU (i, r, t) be the ball of radius r as in Definition 4.3. Then we define its volume as\nvol (BU (i, r, t)) := \u03b3Ut\nn log n + \u2211\nj,k\u2208BU(i,r,t) j<k \u03ba(j, k)dt(j, k) + \u2211 j\u2208BU(i,r,t) k/\u2208BU(i,r,t)\nk\u2208U\n\u03ba(j, k) (r\u2212 dt(i, j)) .\nThe boundary of the ball \u2202BU (i, r, t) is the partial derivative of volume with respect to the radius:\n\u2202BU (i, r, t) := \u2202 vol (BU (i, r, t))\n\u2202r = \u2211\nj\u2208BU(i,r,t) k/\u2208BU(i,r,t)\nk\u2208U\n\u03ba(j, k).\nThe expansion \u03c6(BU (i, r, t)) of the ball BU (i, r, t) is defined as the ratio of its boundary to its volume, i.e.,\n\u03c6 (BU (i, r, t)) := \u2202BU (i, r, t)\nvol (BU (i, r, t)) .\nThe following lemma shows that the volume of a ball BU (i, r, t) is differentiable with respect to r in the interval (0, \u2206] except at finitely many points (see e.g., [Even et al., 1999]).\nLemma 4.6. Let BU (i, r, t) be the ball corresponding to a set U \u2286 V, vertex i \u2208 U, radius r \u2208 R and t \u2208 [n\u2212 1]. Then vol (BU (i, r, t)) is differentiable with respect to r in the interval (0, \u2206] except at finitely many points.\nProof. Note that for any fixed U \u2286 V, vol (BU (i, r, t)) is a monotone non-decreasing function in r since for a pair j, k \u2208 U such that j \u2208 BU (i, r, t) and k /\u2208 BU (i, r, t) we have r \u2212 dt(i, j) \u2264 dt(j, k) otherwise r\u2212 dt(i, j) > dt(j, k) so that r > dt(i, j)+ dt(j, k) \u2265 dt(i, k), a contradiction to the fact that k /\u2208 BU (i, r, t). Therefore adding the vertex k to the ball centered at i is only going to increase its volume as r\u2212 dt(i, j) \u2264 dt(j, k) (see Definition 4.3). Thus vol (BU (i, r, t)) is differentiable with respect to r in the interval (0, \u2206] except at finitely many points which correspond to a new vertex from U being added to the ball.\nThe following theorem establishes that the rounding procedure of Algorithm 2 ensures that the cliques in Ct are \u201csmall\u201d and that the cost of the edges removed to form them are not too high. It also shows that Algorithm 2 can be implemented to run in time polynomial in n.\nTheorem 4.7. Let m\u03b5 := \u230a n\u22121\n1+\u03b5\n\u230b as in Algorithm 2 and let { xtij | t \u2208 [m\u03b5], i, j \u2208 V } be the output of Al-\ngorithm 2 run on a feasible solution {dt}t\u2208[n\u22121] of LP-ultrametric and any choice of \u03b5 \u2208 (0, 1). For any\nInput: Data set V, {dt}t\u2208[n\u22121] : V \u00d7V, \u03b5 > 0, \u03ba : V \u00d7V \u2192 R\u22650 Output: A solution set of the form { xtij \u2208 {0, 1} | t \u2208 [\u230a n\u22121 1+\u03b5 \u230b] , i, j \u2208 V } 1 m\u03b5 \u2190 \u230a n\u22121 1+\u03b5\n\u230b 2 t\u2190 m\u03b5 3 Ct+1 \u2190 {V} 4 \u2206\u2190 \u03b51+\u03b5 5 while t \u2265 1 do 6 Ct \u2190 \u2205 7 for U \u2208 Ct+1 do 8 if |U| \u2264 (1 + \u03b5)t then 9 Ct \u2190 Ct \u222a {U}\n10 Go to line 7 11 end 12 while U 6= \u2205 do 13 Let i be arbitrary in U 14 Let r \u2208 (0, \u2206] be s.t. \u03c6 (BU (i, r, t)) \u2264 1\u2206 log ( vol(BU(i,\u2206,t)) vol(BU(i,0,t))\n) 15 Ct \u2190 Ct \u222a {BU (i, r, t)} 16 U \u2190 U \\ BU (i, r, t) 17 end 18 end 19 xtij = 1 if i \u2208 U1 \u2208 Ct, j \u2208 U2 \u2208 Ct and U1 6= U2, else xtij = 0 20 t\u2190 t\u2212 1 21 end 22 return { xtij | t \u2208 [m\u03b5], i, j \u2208 V\n} Algorithm 2: Iterative rounding algorithm to find a low cost ultrametric\nt \u2208 [m\u03b5], we have that xtij is feasible for the layer-b(1 + \u03b5) tc problem ILP-layer and there is a constant c(\u03b5) > 0 depending only on \u03b5 such that\n\u2211 {i,j}\u2208E(Kn) \u03ba(i, j)xtij \u2264 c(\u03b5)(log n)\u03b3t.\nMoreover, Algorithm 2 can be implemented to run in time polynomial in n.\nProof. We first show that for a fixed t, the constructed solution xtij is feasible for the layer-b(1+ \u03b5)tc problem ILP-layer. Let Ct be as in Algorithm 2 so that xtij = 1 if i, j belong to different sets in Ct and xtij = 0 otherwise. Let Gt = (V, Et) be as in Definition 3.5 corresponding to xtij. Note that for any t \u2208 [m\u03b5], every Vi \u2208 Ct is a clique in Gt by construction (line 19) and for every distinct pair Vi, Vj \u2208 Ct we have Vi \u2229Vj = \u2205 (lines 15 and 16). Therefore by Lemma 3.6, it suffices to prove that for any Vi \u2208 Ct, it holds |Vi| \u2264 b(1 + \u03b5)tc. If Vi is added to Ct in line 9 then there is nothing to prove. Thus let us assume that Vi is of the form BU (i, r, t) for some U \u2286 V as in line 14 so that \u03c6 (BU (i, r, t)) \u2264 1 \u2206 log ( vol(BU(i,\u2206,t)) vol(BU(i,0,t)) ) . Note that by Lemma 4.1 it suffices to show that there is such an r \u2208 (0, \u2206]. This property follows from the rounding scheme due to [Even et al., 1999] as we will explain now. By Lemma 4.6 vol (BU (i, r, t)) is differentiable everywhere in the interval (0, \u2206] except at finitely many points X. Let the set of discontinuous points be X = {x1, x2, . . . , xk\u22121} with x0 = 0 < x1 < x2 . . . xk\u22121 < xk = \u2206. We claim that there must be an r \u2208 (0, \u2206] \\ X such that \u03c6 (BU (i, r, t)) \u2264 1\u2206 log ( vol(BU(i,\u2206,t)) vol(BU(i,0,t)) ) . Let us assume for the sake of a contradiction that for every r \u2208 (0, \u2206] \\ X we have \u03c6 (BU (i, r, t)) > 1 \u2206 log ( vol(BU(i,\u2206,t)) vol(BU(i,0,t)) ) . However integrating both sides from 0 to \u2206 results in a contradiction:\n\u222b \u2206 r=0 \u03c6 (BU (i, r, t)) dr = \u222b \u2206 r=0 \u2202BU (i, r, t) vol (BU (i, r, t)) dr (21)\n= k\n\u2211 i=1 \u222b xi r=xi\u22121 \u2202BU (i, r, t) vol (BU (i, r, t)) dr (22)\n= k\n\u2211 i=1 \u222b xi r=xi\u22121 d (vol (BU (i, r, t))) vol (BU (i, r, t))\n(23)\n\u2264 log vol (BU (i, \u2206, t))\u2212 log vol (BU (i, 0, t)) (24) = \u222b \u2206\nr=0\n1 \u2206\nlog (\nvol (BU (i, \u2206, t)) vol (BU (i, 0, t))\n) dr, (25)\nwhere line 24 follows since f is monotonic increasing. For any t \u2208 [m\u03b5] the set Ct is a disjoint partition of V with balls of the form BU (i, r, t\u2032) for some t\u2032 \u2265 t and U \u2286 Ul \u2208 Ct\u2032+1: this is easily seen by induction since Cm\u03b5+1 is initialized as V. Further, a cluster Vi is added to Ct either in line 15 in which case it is a ball of the form BU (i, r, t) for some U \u2208 Ct+1, i \u2208 U, and r \u2208 R or it is added in line 9 in which case it must have been a ball BU (i\u2032, r\u2032, t\u2032) for some t\u2032 > t, U \u2286 Ul \u2208 Ct\u2032+1, i\u2032 \u2208 V, and r\u2032 \u2208 R. Note that for any t\u2032 \u2265 t and U \u2286 V, it holds \u03b3Ut\u2032 \u2264 \u03b3Ut since for every pair i, j \u2208 V we have \u03ba(i, j) \u2265 0 and dt(i, j) \u2265 dt\u2032(i, j) because of constraint (15). Moreover, for any subset U \u2286 V we have \u03b3Ut \u2264 \u03b3t since \u03ba, dt \u2265 0. We claim that for any t \u2208 [m\u03b5] the total volume of the balls in Ct is at most ( 2 + 1log n ) \u03b3t. First note that the affine term \u03b3 U t\u2032\nn log n in the volume of a ball BU (i, r, t\u2032) in Ct is upper bounded by \u03b3t\nn log n and appears at most n times. Next we claim that the contribution to the total volume from the term involving the edges inside and crossing a ball BU (i, r, t\u2032) \u2208 Ct is at most 2\u03b3t. This is because the balls are disjoint, r \u2212 dt\u2032(i, k) \u2264 dt\u2032(j, k) \u2264 dt(j, k) for the crossing edges of a ball BU (i, r, t\u2032) \u2208 Ct and a crossing edge contributes to the\nvolume of at most 2 balls in Ct. Note that for any U \u2286 V, i \u2208 U, and r \u2208 R\u22650 we have vol (BU (i, r, t)) \u2208[ \u03b3Ut n log n , ( 1 + 1n log n ) \u03b3Ut ] . Using this observation and the stopping condition of line 14 it follows that\n\u2211 {i,j}\u2208E(Kn) \u03ba(i, j)xtij = \u2211 {i,j}\u2208E(Kn):\ni,j separated in Ct\n\u03ba(i, j)\n= 1 2 \u2211BU(i,r,t\u2032)\u2208Ct :\nt\u2032\u2265t U\u2286Ul\u2208Ct\u2032+1\n\u2211 j\u2208BU(i,r,t\u2032) k/\u2208BU(i,r,t\u2032) \u03ba(j, k)\n\ufe38 \ufe37\ufe37 \ufe38 Since \u03ba is symmetric\n= 1 2 \u2211BU(i,r,t\u2032)\u2208Ct :\nt\u2032\u2265t U\u2286Ul\u2208Ct\u2032+1\n\u2202BU ( i, r, t\u2032 )\n= 1 2 \u2211BU(i,r,t\u2032)\u2208Ct :\nt\u2032\u2265t U\u2286Ul\u2208Ct\u2032+1\n\u03c6 ( BU ( i, r, t\u2032 )) vol ( BU ( i, r, t\u2032 ))\n\u2264 \u2211 BU(i,r,t\u2032)\u2208Ct :\nt\u2032\u2265t U\u2286Ul\u2208Ct\u2032+1\n1 2\u2206\nlog (\nvol (BU (i, \u2206, t\u2032)) vol (BU (i, 0, t\u2032))\n) vol ( BU ( i, r, t\u2032 ))\n\u2264 1 2\u2206 (log (n log n + 1))\ufe38 \ufe37\ufe37 \ufe38 via interval bounds \u2211 BU(i,r,t\u2032)\u2208Ct :\nt\u2032\u2265t U\u2286Ul\u2208Ct\u2032+1\nvol ( BU ( i, r, t\u2032 ))\n\u2264 1 + \u03b5 2\u03b5\n(log (n log n + 1)) ( 2 + 1\nlog n ) \u03b3t\ufe38 \ufe37\ufe37 \ufe38\ncontribution of affine term \u2264 \u03b3tlog n contribution of edge terms \u2264 2\u03b3t\n\u2264 c(\u03b5)(log n)\u03b3t,\nfor some constant c(\u03b5) > 0 depending only on \u03b5. For the run time of Algorithm 2 note that the loop in line 5 runs for at most n\u2212 1 steps, while the loop in line 7 runs for at most n steps. For a set U \u2286 V, to compute the ball BU (i, r, t) of least radius r such that \u03c6 (BU (i, r, t)) \u2264 1\u2206 log ( vol(BU(i,\u2206,t)) vol(BU(i,0,t)) ) , sort the vertices in U \\ {i} in increasing order of distance from i\naccording to dt. Let the vertices in U \\ {i} in this sorted order be { j1, . . . , j|U|\u22121 } . Then it suffices to check the expansion of the balls {i} and {i} \u222a {j1, . . . , jk} for every k \u2208 [|U| \u2212 1]. It is straightforward to see that all the other steps in Algorithm 2 run in time polynomial in n.\nRemark 4.8. A discrete version of the volumetric argument for region growing can be found in [Gupta, 2005].\nWe are now ready to prove the main theorem showing that we can obtain a low cost non-trivial ultrametric from Algorithm 2.\nTheorem 4.9. Let {xtij | t \u2208 [m\u03b5] , i, j \u2208 V} be the output of Algorithm 2 on an optimal solution {dt}t\u2208[n\u22121] of LP-ultrametric for any choice of \u03b5 \u2208 (0, 1). Define the sequence { ytij } for every t \u2208 [n\u2212 1] and i, j \u2208 V\nas\nytij := { xbt/(1+\u03b5)cij if t > 1 + \u03b5 1 if t \u2264 1 + \u03b5.\nThen ytij is feasible for ILP-ultrametric and satisfies\nn\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j)ytij \u2264 (2c(\u03b5) log n)OPT\nwhereOPT is the optimal solution to ILP-ultrametric and c(\u03b5) is the constant in the statement of Theorem 4.9.\nProof. Note that by Theorem 4.7 for every t \u2208 [m\u03b5], xtij is feasible for the layer-b(1 + \u03b5)tc problem ILPlayer and that there is a constant c(\u03b5) > 0 such that for every t \u2208 [m\u03b5], we have \u2211{i,j}\u2208E(Kn) \u03ba(i, j)xtij \u2264 (c(\u03b5) log n) \u03b3t. Let ytij be as in the statement of the theorem. The graph Gt = (V, Et) as in Definition 3.5 corresponding to ytij for t \u2264 1+ \u03b5 consists of isolated vertices, i.e., cliques of size 1: By definition ytij is feasible for the layer-t problem ILP-layer. The collection C1 corresponding to x1ij consists of cliques of size at most 1 + \u03b5, however since 0 < \u03b5 < 1 it follows that the cliques in C1 are isolated vertices and so x1ij = 1 for every {i, j} \u2208 E(Kn). Thus \u2211i,j \u03ba(i, j)ytij = \u2211i,j \u03ba(i, j)x 1 ij \u2264 (c(\u03b5) log n) \u03b31 for t \u2264 1 + \u03b5 by Theorem 4.7. Moreover for every t > 1+ \u03b5, we have \u2211i,j \u03ba(i, j)ytij \u2264 (c(\u03b5) log n)\u03b3bt/(1+\u03b5)c again by Theorem 4.7. We claim that ytij is feasible for ILP-ultrametric. The solution ytij corresponds to the collection Cb t1+\u03b5 c for t > 1 + \u03b5 or to the collection C1 for t \u2264 1 + \u03b5 from Algorithm 2. For any t < m\u03b5, every ball BU (i, r, t) \u2208 Ct comes from the refinement of a ball BU\u2032 (i\u2032, r\u2032, t\u2032) for some i\u2032 \u2208 V, r\u2032 \u2265 r, t\u2032 \u2265 t and U\u2032 \u2287 U. Thus ytij satisfies Condition Nested cliques of Lemma 3.7. On the other hand line 8 ensures that if |BU (i, r, t)| = b(1 + \u03b5)sc for some U \u2286 V and s < t then BU (i, r, t) also appears as a ball in Cs. Therefore ytij also satisfies Condition Realization of Lemma 3.7 and so is feasible for ILP-ultrametric. The cost of ytij is at most\nn\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn)\n\u03ba(i, j)ytij \u2264 (c(\u03b5) log n) (\n\u03b31 + n\u22121 \u2211 t=2 \u03b3bt/(1+\u03b5)c\n)\n\u2264 2c(\u03b5) log n n\u22121 \u2211 t=1 \u03b3t \u2264 2c(\u03b5) log n OPT,\nwhere we use the fact that \u2211n\u22121t=1 \u03b3t = OPT(LP) \u2264 OPT since LP-ultrametric is a relaxation of ILPultrametric.\nTheorem 4.9 implies the following corollary where we put everything together to obtain a hierarchical clustering of V in time polynomial in n with |V| = n. Let T denote the set of all possible hierarchical clusterings of V.\nCorollary 4.10. Given a data set V of n points and a similarity function \u03ba : V \u00d7 V \u2192 R\u22650, Algorithm 3 returns a hierarchical clustering T of V satisfying\ncost(T) \u2264 O (log n) min T\u2032\u2208T cost(T\u2032).\nMoreover Algorithm 3 runs in time polynomial in n and log ( maxi,j\u2208V \u03ba(i, j) ) .\nInput: Data set V of n points, similarity function \u03ba : V \u00d7V \u2192 R\u22650 Output: Hierarchical clustering of V\n1 Solve LP-ultrametric to obtain optimal sequence of spreading metrics {dt | dt : V \u00d7V \u2192 [0, 1]} 2 Fix a choice of \u03b5 \u2208 (0, 1) 3 m\u03b5 \u2190 \u230a n\u22121 1+\u03b5 \u230b 4 Let { xtij | t \u2208 [m\u03b5] } be the output of Algorithm 2 on V, \u03ba, {dt}t\u2208[n\u22121]\n5 Let ytij := { xbt/(1+\u03b5)cij if t > 1 + \u03b5 1 if t \u2264 1 + \u03b5 for every t \u2208 [n\u2212 1], i, j \u2208 E(Kn) 6 d(i, j)\u2190 \u2211n\u22121t=1 ytij for every i, j \u2208 E(Kn) 7 d(i, i)\u2190 0 for every i \u2208 V 8 Let r, T be the output of Algorithm 1 on V, d 9 return r, T\nAlgorithm 3: Hierarchical clustering of V for cost function (1)\nProof. Let T\u0302 be the optimal hierarchical clustering according to cost function (1). By Corollary 3.4 and Theorem 4.9 we can find a hierarchical clustering T satisfying\n\u2211 {i,j}\u2208E(Kn) \u03ba(i, j)(|leaves(T[lca(i, j)])| \u2212 1) \u2264 O(log n)  \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) (\u2223\u2223\u2223leaves(T\u0302[lca(i, j)])\u2223\u2223\u2223\u2212 1)  . Let K := \u2211{i,j}\u2208E(Kn) \u03ba(i, j). Then it follows from the above expression that cost(T) \u2264 O(log n) cost(T\u0302)\u2212 O(log n)K + K \u2264 O(log n) cost(T\u0302).\nWe can find an optimal solution to LP-ultrametric due to Lemma 4.2 using the Ellipsoid algorithm in time polynomial in n and log ( maxi,j\u2208V \u03ba(i, j) ) . Algorithm 2 runs in time polynomial in n due to Theorem 4.7.\nFinally, Algorithm 1 runs in time O ( n3 ) due to Lemma 3.3."}, {"heading": "5 Generalized Cost Function", "text": "In this sectionwe study the following natural generalization of cost function (1) also introduced by [Dasgupta, 2016] where the distance between the two points is scaled by a function f : R\u22650 \u2192 R\u22650, i.e.,\ncost f (T) := \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) f (|leaves T[lca(i, j)]|) . (26)\nIn order that cost function (26) makes sense, f should be strictly increasing and satisfy f (0) = 0. Possible choices for f could be { x2, ex \u2212 1, log(1 + x) } . The top-down heuristic in [Dasgupta, 2016] finds the optimal hierarchical clustering up to an approximation factor of cn log n with cn being defined as\ncn := 3\u03b1n max 1\u2264n\u2032\u2264n f (n\u2032) f (dn\u2032/3e)\nand where \u03b1n is the approximation factor from the Sparsest Cut algorithm used. A naive approach to solving this problem using the ideas of Algorithm 2 would be to replace the objective function of ILP-ultrametric by\n\u2211 {i,j}\u2208E(Kn) \u03ba(i, j) f ( n\u22121 \u2211 t=1 xtij ) .\nThis makes the corresponding analogue of LP-ultrametric non-linear however, and for a general \u03ba and f it is not clear how to compute an optimum solution in polynomial time. One possible solution is to assume that f is convex and use the Frank-Wolfe algorithm to compute an optimum solution. That still leaves the problem of how to relate f ( \u2211n\u22121t=1 x t ij ) to \u2211n\u22121t=1 f ( xtij ) as one would have to do to get a corresponding version of Theorem 4.9. The following simple observation provides an alternate way of tackling this problem.\nObservation 5.1. Let d : V \u00d7 V \u2192 R be an ultrametric and f : R\u22650 \u2192 R\u22650 be a strictly increasing function such that f (0) = 0. Define the function f (d) : V \u00d7V \u2192 R as f (d)(i, j) := f (d(i, j)). Then f (d) is also an ultrametric on V.\nTherefore by Corollary 3.4 to find a minimum cost hierarchical clustering T of V according to the cost function (26), it suffices to minimize \u3008\u03ba, d\u3009 where d is the f -image of a non-trivial ultrametric as in Definition 2.2. The following lemma lays down the analogue of Conditions 1 and 2 from Definition 2.2 that the f -image of a non-trivial ultrametric satisfies.\nLemma 5.2. Let f : R\u22650 \u2192 R\u22650 be a strictly increasing function satisfying f (0) = 0. An ultrametric d on V is the f -image of a non-trivial ultrametric on V iff\n1. for every non-empty set S \u2286 V, there is a pair of points i, j \u2208 S such that d(i, j) \u2265 f (|S| \u2212 1),\n2. for any t if St is an equivalence class ofV under the relation i \u223c j iff d(i, j) \u2264 t, thenmaxi,j\u2208St d(i, j) \u2264 f (|St| \u2212 1).\nProof. If d is the f -image of a non-trivial ultrametric d\u2032 on V then clearly d satisfies Conditions 1 and 2. Conversely, let d be an ultrametric on V satisfying Conditions 1 and 2. Note that f is strictly increasing and V is a finite set and thus f\u22121 exists and is strictly increasing as well, with f\u22121(0) = 0. Define d\u2032 as d\u2032(i, j) := f\u22121(d(i, j)) for every i, j \u2208 V. By Observation 5.1 d\u2032 is an ultrametric on V satisfying Conditions 1 and 2 of Definition 2.2 and so d\u2032 is a non-trivial ultrametric on V.\nLemma 5.2 allows us to write the analogue of ILP-ultrametric for finding the minimum cost ultrametric that is the f -image of a non-trivial ultrametric on V. Note that by Lemma 2.4 the range of such an ultrametric is the set { f (0), f (1), . . . , f (n\u2212 1)}. We have the binary variables xtij for every distinct pair i, j \u2208 V and t \u2208 [n\u2212 1], where xtij = 1 if d(i, j) \u2265 f (t) and xtij = 0 if d(i, j) < f (t).\nmin n\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) ( f (t)\u2212 f (t\u2212 1)) xtij (f-ILP-ultrametric)\ns.t. xtij \u2265 xt+1ij \u2200i, j \u2208 V, t \u2208 [n\u2212 2] (27) xtij + x t jk \u2265 xtik \u2200i, j, k \u2208 V, t \u2208 [n\u2212 1] (28)\n\u2211 i,j\u2208S xtij \u2265 2 \u2200t \u2208 [n\u2212 1], S \u2286 V, |S| = t + 1 (29)\n\u2211 i,j\u2208S\nx|S|ij \u2264 |S| \u2211 i,j\u2208S\nxtij + \u2211 i\u2208S j/\u2208S\n( 1\u2212 xtij ) \u2200t \u2208 [n\u2212 1], S \u2286 V (30) xtij = x t ji \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (31) xtii = 0 \u2200i \u2208 V, t \u2208 [n\u2212 1] (32) xtij \u2208 {0, 1} \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (33)\nIf xtij is a feasible solution to f-ILP-ultrametric then the ultrametric represented by it is defined as\nd(i, j) := n\u22121 \u2211 t=1 ( f (t)\u2212 f (t\u2212 1))xtij.\nConstraint (29) ensures that d satisfies Condition 1 of Lemma 5.2, since for every S \u2286 V of size t + 1 we have a pair i, j \u2208 S such that d(i, j) \u2265 f (t). Similarly constraint (30) ensures that d satisfies Condition 2 of Lemma 5.2 since it is active if and only if S is an equivalence class of V under the relation i \u223c j iff d(i, j) < f (t). In this case Condition 2 of Lemma 5.2 requires maxi,j\u2208S d(i, j) \u2264 f (|S| \u2212 1) or in other words x|S|ij = 0 for every i, j \u2208 S. Similar to ILP-layer we define an analogous layer-t problem where we fix a choice of t \u2208 [n\u2212 1] and drop the constraints that relate the different layers to each other.\nmin \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) ( f (t)\u2212 f (t\u2212 1)) xtij (f-ILP-layer)\ns.t. xtij + x t jk \u2265 xtik \u2200i, j, k \u2208 V (34)\n\u2211 i,j\u2208S xtij \u2265 2 \u2200S \u2286 V, |S| = t + 1 (35)\nxtij = x t ji \u2200i, j \u2208 V (36) xtii = 0 \u2200i \u2208 V (37) xtij \u2208 {0, 1} \u2200i, j \u2208 V (38)\nNote that f-ILP-ultrametric and f-ILP-layer differ from ILP-ultrametric and ILP-layer respectively only in the objective function. Therefore Lemmas 3.6 and 3.7 also give a combinatorial characterization of the set of feasible solutions to f-ILP-layer and f-ILP-ultrametric respectively. Similarly, by Lemma 3.8 we may replace constraint (29) by the following equivalent constraint over all subsets of V\n\u2211 j\u2208S xtij \u2265 |S| \u2212 t \u2200t \u2208 [n\u2212 1], S \u2286 V, i \u2208 S.\nThis provides the analogue of LP-ultrametric in which we drop constraint (30) and enforce it in the rounding procedure.\nmin n\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) ( f (t)\u2212 f (t\u2212 1)) xtij (f-LP-ultrametric)\ns.t. xtij \u2265 xt+1ij \u2200i, j \u2208 V, t \u2208 [n\u2212 2] (39) xtij + x t jk \u2265 xtik \u2200i, j, k \u2208 V, t \u2208 [n\u2212 1] (40)\n\u2211 j\u2208S xtij \u2265 |S| \u2212 t \u2200t \u2208 [n\u2212 1], S \u2286 V, i \u2208 S (41)\nxtij = x t ji \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (42) xtii = 0 \u2200i \u2208 V, t \u2208 [n\u2212 1] (43) 0 \u2264 xtij \u2264 1 \u2200i, j \u2208 V, t \u2208 [n\u2212 1] (44)\nSince f-LP-ultrametric differs fromLP-ultrametric only in the objective function, it follows fromLemma 4.2 that an optimum solution to f-LP-ultrametric can be computed in time polynomial in n. As before, a feasible solution xtij of f-LP-ultrametric induces a sequence {dt}t\u2208[n\u22121] of spreading metrics on V defined as dt(i, j) := xtij. Note that in contrast to the ultrametric d, the spreading metrics {dt}t\u2208[n\u22121] are independent of the function f . Let BU (i, r, t) be a ball of radius r centered at i \u2208 U for some set U \u2286 V as in Definition 4.3. For a subset U \u2286 V, let \u03b3Ut be defined as before to be the value of the layer-t objective corresponding to a solution dt of f-LP-ultrametric restricted to U, i.e.,\n\u03b3Ut := \u2211 i,j\u2208U i<j ( f (t)\u2212 f (t\u2212 1)) \u03ba(i, j)dt(i, j).\nAs before, we denote \u03b3Vt by \u03b3t. We will associate a volume vol (BU (i, r, t)) and a boundary \u2202BU (i, r, t) to the ball BU (i, r, t) as in Section 4.\nDefinition 5.3. Let U be an arbitrary subset of V. For a vertex i \u2208 U, radius r \u2208 R\u22650, and t \u2208 [n\u2212 1], let BU (i, r, t) be the ball of radius r as in Definition 4.3. Then we define its volume as\nvol (BU (i, r, t)) := \u03b3Ut\nn log n + ( f (t)\u2212 f (t\u2212 1))  \u2211j,k\u2208BU(i,r,t) j<k \u03ba(j, k)dt(j, k) + \u2211 j\u2208BU(i,r,t) k/\u2208BU(i,r,t)\nk\u2208U\n\u03ba(j, k) (r\u2212 dt(i, j))  .\nThe boundary of the ball \u2202BU (i, r, t) is the partial derivative of volume with respect to the radius:\n\u2202BU (i, r, t) := ( f (t)\u2212 f (t\u2212 1)) (\n\u2202 vol (BU (i, r, t)) \u2202r\n) = ( f (t)\u2212 f (t\u2212 1))  \u2211j\u2208BU(i,r,t) k/\u2208BU(i,r,t)\nk\u2208U\n\u03ba(j, k)  .\nThe expansion \u03c6 (BU (i, r, t)) of the ball BU (i, r, t) is defined as the ratio of its boundary to its volume, i.e.,\n\u03c6 (BU (i, r, t)) := \u2202BU (i, r, t)\nvol (BU (i, r, t)) .\nNote that the expansion \u03c6 (BU (i, r, t)) ofDefinition 5.3 is the same as inDefinition 4.5 since the ( f (t)\u2212 f (t\u2212 1)) term cancels out. Thus one could run Algorithm 2 with the same notion of volume as in Definition 4.5, however in that case the analogous versions of Theorems 4.7 and 4.9 do not follow as naturally. The following is then a simple corollary of Theorem 4.7.\nCorollary 5.4. Let m\u03b5 := \u230a n\u22121\n1+\u03b5\n\u230b as in Algorithm 2. Let { xtij | t \u2208 [n\u2212 1], i, j \u2208 V } be the output of Al-\ngorithm 2 using the notion of volume, boundary and expansion as in Definition 5.3, on a feasible solution to f-LP-ultrametric and any choice of \u03b5 \u2208 (0, 1). For any t \u2208 [m\u03b5], we have that xtij is feasible for the layer-b(1 + \u03b5)tc problem f-ILP-layer and there is a constant c(\u03b5) > 0 depending only on \u03b5 such that\n\u2211 {i,j}\u2208E(Kn) \u03ba(i, j) ( f (t)\u2212 f (t\u2212 1)) xtij \u2264 (c(\u03b5) log n) \u03b3t.\nCorollary 5.4 allows us to prove the analogue of Theorem 4.9, i.e., we can use Algorithm 2 to get an ultrametric that is an f -image of a non-trivial ultrametric and whose cost is at most O(log n) times the cost of an optimal hierarchical clustering according to cost function (26).\nTheorem 5.5. Let {xtij | t \u2208 [m\u03b5] , i, j \u2208 V} be the output of Algorithm 2 using the notion of volume, boundary, and expansion as in Definition 5.3 on an optimal solution {dt}t\u2208[n\u22121] of f-LP-ultrametric for any choice of \u03b5 \u2208 (0, 1). Define the sequence { ytij } for every t \u2208 [n\u2212 1] and i, j \u2208 V as\nytij := { xbt/(1+\u03b5)cij if t > 1 + \u03b5 1 if t \u2264 1 + \u03b5.\nThen ytij is feasible for f-ILP-ultrametric and there is a constant c(\u03b5) > 0 such that\nn\u22121 \u2211 t=1 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) ( f (t)\u2212 f (t\u2212 1)) ytij \u2264 (c(\u03b5) log n)OPT\nwhere OPT is the optimal solution to f-ILP-ultrametric.\nProof. Immediate from Corollary 5.4 and Theorem 4.9.\nFinally we put everything together to obtain the corresponding Algorithm 4 that outputs a hierarchical clustering of V of cost at most O (log n) times the optimal clustering according to cost function (26).\nCorollary 5.6. Given a data set V of n points and a similarity function \u03ba : V\u00d7V \u2192 R, Algorithm 4 returns a hierarchical clustering T of V satisfying\ncost f (T) \u2264 O (an + log n) min T\u2032\u2208T cost f (T\u2032),\nwhere an := maxn\u2032\u2208[n] f (n\u2032)\u2212 f (n\u2032 \u2212 1). Moreover Algorithm 4 runs in time polynomial in n, log f (n) and log ( maxi,j\u2208V \u03ba(i, j) ) .\nProof. Let T\u0302 be an optimal hierarchical clustering according to cost function (26). ByCorollary 3.4, Lemma 5.2 and Theorem 5.5 it follows that we can find a hierarchical clustering T satisfying\n\u2211 {i,j}\u2208E(Kn) \u03ba(i, j) f (|leaves(T[lca(i, j)]| \u2212 1) \u2264 O(log n)  \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) f (\u2223\u2223\u2223leaves(T\u0302[lca(i, j)]\u2223\u2223\u2223\u2212 1)  . Recall that cost f (T) := \u2211{i,j}\u2208E(Kn) \u03ba(i, j) f (|leaves(T[lca(i, j)]|). Let K := \u2211{i,j}\u2208E(Kn) \u03ba(i, j). Note that for any hierarchical clustering T\u2032 we have K \u2264 cost f (T\u2032) since f is an increasing function. From the above expression we infer that\ncost f (T)\u2212 anK \u2264 \u2211 {i,j}\u2208E(Kn) \u03ba(i, j) f (|leaves(T[lca(i, j)]| \u2212 1) \u2264 O(log n) cost f (T\u0302),\nand so cost f (T) \u2264 O(log n) cost f (T\u0302) + anK \u2264 O(an + log n) cost f (T\u0302). We can find an optimal solution to f-LP-ultrametric due to Lemma 4.2 using the Ellipsoid algorithm in time polynomial in n, log f (n), and log ( maxi,j\u2208V \u03ba(i, j) ) . Note the additional log f (n) in the running time since now we need to binary\nsearch over the interval [ 0, maxi,j\u2208V \u03ba(i, j) \u00b7 f (n) \u00b7 n ] . Algorithm 2 runs in time polynomial in n due to\nTheorem 4.7. Finally, Algorithm 1 runs in time O ( n3 ) due to Lemma 3.3.\nInput: Data set V of n points, similarity function \u03ba : V \u00d7V \u2192 R\u22650, f : R\u22650 \u2192 R\u22650 strictly increasing with f (0) = 0\nOutput: Hierarchical clustering of V 1 Solve f-LP-ultrametric to obtain optimal sequence of spreading metrics {dt | dt : V \u00d7V \u2192 [0, 1]} 2 Fix a choice of \u03b5 \u2208 (0, 1) 3 m\u03b5 \u2190 \u230a n\u22121 1+\u03b5\n\u230b 4 Let { xtij | t \u2208 [m\u03b5] } be the output of Algorithm 2 on V, \u03ba, {dt}t\u2208[n\u22121]\n5 Let ytij := { xbt/(1+\u03b5)cij if t > 1 + \u03b5 1 if t \u2264 1 + \u03b5 for every t \u2208 [n\u2212 1], i, j \u2208 E(Kn) 6 d(i, j)\u2190 \u2211n\u22121t=1 ( f (t)\u2212 f (t\u2212 1)) ytij for every i, j \u2208 E(Kn) 7 d(i, i)\u2190 0 for every i \u2208 V 8 Let r, T be the output of Algorithm 1 on V, f\u22121(d) 9 return r, T\nAlgorithm 4: Hierarchical clustering of V for cost function (26)"}, {"heading": "6 Experiments", "text": "Finally, we describe the experiments we performed. For small data sets ILP-ultrametric and f-ILP-ultrametric describe integer programming formulations that allow us to compute the exact optimal hierarchical clustering for cost functions (1) and (26) respectively. We implement f-ILP-ultrametric where one can plug in any strictly increasing function f satisfying f (0) = 0. In particular, setting f (x) = x gives us ILP-ultrametric. We use the Mixed Integer Programming (MIP) solver Gurobi 6.5 [Gurobi Optimization, 2015]. Similarly, we also implement Algorithms 1, 2, and 4 using Gurobi as our LP solver. Note that Algorithm 4 needs to fix a parameter choice \u03b5 \u2208 (0, 1). In Sections 4 and 5 we did not discuss the effect of the choice of the parameter \u03b5 in detail. In particular, we need to choose an \u03b5 small enough such that for every U \u2286 V encountered in Algorithm 2, vol (BU (i, \u2206, t)) is of the same sign as vol (BU (i, 0, t)) for every t \u2208 [n \u2212 1], so that log (\nvol(BU(i,\u2206,t)) vol(BU(i,0,t))\n) is defined. In our experiments we start with a particular value of \u03b5 (say 0.5) and halve it\ntill the volumes have the same sign. For the sake of exposition, we limit ourselves to the following choices for the function f {\nx, x2, log(1 + x), ex \u2212 1 } .\nBy Lemma 4.2 we can optimize over f-LP-ultrametric in time polynomial in n using the Ellipsoid method. In practice however, we use the dual simplex method where we separate triangle inequality constraints (40) and spreading constraints (41) to obtain fast computations. For the similarity function \u03ba : V \u00d7 V \u2192 R we limit ourselves to using cosine similarity and the Gaussian kernel with \u03c3 = 1. They are defined formally below.\nDefinition 6.1 (Cosine similarity). Given a data set V \u2208 Rm for some m \u2265 0, the cosine similarity \u03bacos is defined as \u03bacos(x, y) := \u3008x,y\u3009\u2016x\u2016\u2016y\u2016 .\nSince the LP rounding Algorithm 2 assumes that \u03ba \u2265 0 in practice we implement 1+ \u03bacos rather than \u03bacos. Definition 6.2 (Gaussian kernel). Given a data set V \u2208 Rm for some m \u2265 0, the Gaussian kernel \u03bagauss with standard deviation \u03c3 is defined as \u03bagauss(x, y) := exp ( \u2212 \u2016x\u2212y\u2016 2\n2\u03c32\n) .\nThe main aim of our experiments was to answer the following two questions.\n1. How good is the hierarchal clustering obtained from Algorithm 4 as opposed to the true optimal output by f-ILP-ultrametric?\n2. How good does Algorithm 4 perform compared to other hierarchical clustering methods?\nFor the first question, we are restricted to working with small data sets since computing an optimum solution to f-ILP-ultrametric is expensive. In this case we consider synthetic data sets of small size and samples of some data sets from the UCI database [Lichman, 2013]. The synthetic data sets we consider are mixtures of Gaussians in various small dimensional spaces. Figure 1 shows a comparison of the cost of the hierarchy (according to cost function (26)) returned by solving f-ILP-ultrametric and by Algorithm 4 for various forms of f when the similarity function is \u03bacos and \u03bagauss. Note that we normalize the cost of the tree returned by f-ILP-ultrametric and Algorithm 4 by the cost of the trivial clustering r, T\u2217 where T\u2217 is the star graph with V as its leaves and r as the internal node. In other words dT\u2217(i, j) = n\u2212 1 for every distinct pair i, j \u2208 V and so the normalized cost of any tree lies in the interval (0, 1]. For the study of the second question, we consider some of the popular algorithms for hierarchical clustering are single linkage, average linkage, complete linkage, and Ward\u2019s method [Ward Jr, 1963]. To get a numerical handle on how good a hierarchical clustering T of V is, we prune the tree to get the best k flat clusters and measure its error relative to the target clustering. We use the following notion of error also known as Classification Error that is standard in the literature for hierarchical clustering (see, e.g., [Meil\u0103 and Heckerman, 2001]). Note that we may think of a flat k-clustering of the data V as a function h mapping elements of V to a label set L := {1, . . . , k}. Let Sk denote the group of permutations on k letters. Definition 6.3 (Classification Error). Given a proposed clustering h : V \u2192 L its classification error relative to a target clustering g : V \u2192 L is denoted by err (g, h) and is defined as\nerr (g, h) := min \u03c3\u2208Sk\n[ Pr\nx\u2208V [h(x) 6= \u03c3(g(x))\n] .\nWe compare the error of Algorithm 4 with the various linkage based algorithms that are commonly used for hierarchical clustering, as well as Ward\u2019s method and the k-means algorithm. We test Algorithm 4 most extensively for f (x) = x while doing a smaller number of tests for f (x) \u2208 { x2, log(1 + x), ex \u2212 1 } . Note that both Ward\u2019s method and the k-means algorithm work on the squared Euclidean distance \u2016x\u2212 y\u201622 between two points x, y \u2208 V, i.e., they both require an embedding of the data points into a normed vector space which provides extra information that can be potentially exploited. For the linkage based algorithms\nwe use the same notion of similarity 1 + \u03bacos or \u03bagauss that we use for Algorithm 4. For comparison we use a mix of synthetic data sets as well as the Wine, Iris, Soybean-small, Digits, Glass, and Wdbc data sets from the UCI repository [Lichman, 2013]. For some of the larger data sets, we sample uniformly at random a smaller number of data points and take the average of the error over the different runs. Figures 2, 3, 4, and 5 show that the hierarchical clustering returned by Algorithm 4 with f (x) \u2208 { x, x2, log(1 + x), ex \u2212 1 } often has better projections into flat clusterings than the other algorithms. This is especially true when we compare it to the linkage based algorithms, since they use the same pairwise similarity function as Algorithm 4, as opposed to Ward\u2019s method and k-means."}, {"heading": "7 Discussion", "text": "In this work we have studied the cost functions (1) and (26) for hierarchical clustering given a pairwise similarity function over the data and shown an O(log n) approximation algorithm for this problem. As briefly mentioned in Section 2 however, such a cost function is not unique. Further, there is an intimate connection between hierarchical clusterings and ultrametrics over discrete sets which points to other directions for for-\nmulating a cost function over hierarchies. In particular we briefly mention the related notion of hierarchically well-separated trees (HST) as defined in [Bartal, 1996] (see also [Bartal et al., 2001, Bartal et al., 2003]). A k-HST for k \u2265 1 is a tree T such that each vertex u \u2208 T has a label \u2206(u) \u2265 0 such that \u2206(u) = 0 if and only if u is a leaf of T. Further, if u is a child of v in T then \u2206(u) \u2264 \u2206(v)/k. It is well known that any ultrametric d on a finite set V is equivalent to a 1-HST where V is the set of leaves of T and d(i, j) = \u2206 (lca(i, j)) for every i, j \u2208 V. Thus in the special case when \u2206(u) = |leaves T[u]| \u2212 1 we get the cost function (1), while if \u2206(u) = f (|leaves T[u]| \u2212 1) for a strictly increasing function f with f (0) = 0 then we get cost function (26). It turns out this assumption on \u2206 enables us to prove the combinatorial results of Section 3 and give a O(log n) approximation algorithm to find the optimal cost tree according to these cost functions. It is an interesting problem to investigate cost functions and algorithms for hierarchical clustering induced by other families of \u2206 that arise from a k-HST on V, i.e., if the cost of T is defined as\ncost\u2206(T) := \u2211 {i,j}\u2208E(Kn) \u03ba(i, j)\u2206 (lca(i, j)) . (45)\nNote that not all choices of\u2206 lead to ameaningful cost function. For example, choosing\u2206(u) = diam (T[u])\u2212 1 gives rise to the following cost function\ncost(T) := \u2211 {i,j}\u2208E(Kn) \u03ba(i, j)distT(i, j) (46)\nwhere distT(i, j) is the length of the unique path from i to j in T. In this case, the trivial clustering r, T\u2217 where T\u2217 is the star graph with V as its leaves and r as the root is always a minimizer; in other words, there is no incentive for spreading out the hierarchical clustering. Also worth mentioning is a long line of related work on fitting tree metrics to metric spaces (see e.g., [Ailon and Charikar, 2005, R\u00e4cke, 2008, Fakcharoenphol et al., 2003]). In this setting, the data points V are assumed to come from a metric space dV and the objective is to find a hierarchical clustering T so as to minimize \u2016dV \u2212 dT\u2016p. If the points in V lie on the unit sphere and the similarity function \u03ba is the cosine similarity \u03bacos(i, j) = 1\u2212 dV(i, j)/2, then the problem of fitting a tree metric with p = 2 minimizes the same objective as cost function (46). Since dV \u2264 1 in this case, the minimizer is the trivial tree r, T\u2217 (as remarked above). In general, when the points in V are not constrained to lie on the unit sphere, the two problems are incomparable."}, {"heading": "8 Acknowledgments", "text": "Research reported in this paper was partially supported by NSF CAREER award CMMI-1452463 and NSF grant CMMI-1333789. We would like to thank Kunal Talwar and Mohit Singh for helpful discussions and anonymous reviewers for helping improve the presentation of this paper."}, {"heading": "A Hardness of finding the optimal hierarchical clustering", "text": "In this section we study the hardness of finding the optimal hierarchical clustering according to cost function (1). We show that under the assumption of the Small Set Expansion (SSE) hypothesis there is no constant factor approximation algorithm for this problem. We also show that no polynomial sized Linear Program (LP) or Semidefinite Program (SDP) can give a constant factor approximation for this problem without the need for any complexity theoretic assumptions. Both these results make use of the similarity of this problem with the minimum linear arrangement problem. To show hardness under Small Set Expansion, we make use of the result of [Raghavendra et al., 2012] showing that there is no constant factor approximation algorithm for the Minimum Linear Arrangement problem under the assumption of SSE. To show the LP and SDP inapproximability results, we make use of the reduction framework of [Braun et al., 2015] together with the NP-hardness proof for Minimum Linear Arrangement due to [Garey et al., 1976]. We also note that both these hardness results hold even for unweighted graphs (i.e., when \u03ba \u2208 {0, 1}).\nNote that the individual layer-t problem f-ILP-layer for t = bn/2c is equivalent to the minimum bisection problem for which the best known approximation is O(log n) due to [R\u00e4cke, 2008], while the best known bicriteria approximation is O (\u221a log n ) due to [Arora et al., 2009] and improving these approximation factors is a major open problem. However it is not clear if an improved approximation algorithm for hierarchical clustering under cost function (1) would imply an improved algorithm for every layer-t problem, which is why a constant factor inapproximability result is of interest. We start by recalling the definition of an optimization problem in the framework of [Braun et al., 2015].\nDefinitionA.1 (Optimization problem). [Braun et al., 2015] An optimization problem is a tupleP = (S , I, val) consisting of a set S of feasible solutions, a set I of instances, and a real-valued objective called measure val : I\u00d7 S \u2192 R. We shall use valI (s) for the objective value of a feasible solution s \u2208 S for an instance I \u2208 I.\nSince we are interested in the integrality gaps of LP and SDP relaxations for an optimization problemP = (S , I, val), we represent the approximation gap by two functions C, S : I\u2192 R where C is the completeness guarantee while S is the soundness guarantee. Note that the ratio C/S represents the approximation factor for the problem P . We recall below the formal definition of an LP relaxation of P that achieves a (C, S)approximation guarantee. We assume without loss of generality that P is a maximization problem.\nDefinition A.2 (LP formulation of an optimization problem). [Braun et al., 2015] Let P = (S , I, val) be an optimization problem, and C, S : I\u2192 R. Then let IS := {I \u2208 I |max valI \u2264 S(I)} denote the set of sound instances, i.e., for which the soundness guarantee S is an upper bound on the maximum. A (C, S)approximate LP formulation of P consists of a linear program Ax \u2264 b with x \u2208 Rr for some r and the following realizations:\nFeasible solutions as vectors xs \u2208 Rr for every s \u2208 S satisfying\nAxs \u2264 b for all s \u2208 S , (47)\ni.e., the system Ax \u2264 b is a relaxation of conv (xs | s \u2208 S).\nInstances as affine functions wI : Rr \u2192 R for all I \u2208 IS satisfying\nwI (xs) = valI (s) for all s \u2208 S , (48)\ni.e., the linearization wI of valI is required to be exact on all xs with s \u2208 S .\nAchieving (C, S) approximation guarantee by requiring\nmax {wI (x) | Ax \u2264 b} \u2264 C(I) for all I \u2208 IS, (49)\nThe size of the formulation is the number of inequalities in Ax \u2264 b. Finally, the (C, S)-approximate LP formulation complexity fcLP(P , C, S) of P is the minimal size of all its LP formulations.\nOne can similarly define a (C, S)-approximate SDP formulation for a problem P where instead of a LP, we now have a SDP relaxation A(X) = b with X \u2208 Sr+ and where Sr+ denotes the space of r \u00d7 r positive semidefinite matrices. The size of such an SDP formulation is measured by the dimension r and fcSDP(P , C, S) is defined as the minimum size of an SDP formulation achieving (C, S)-approximation for problemP . Belowwe recall the precise notion of a reduction between two problems as in [Braun et al., 2015].\nDefinition A.3 (Reduction). [Braun et al., 2015] Let P1 = (S1, I1, val) and P2 = (S2, I2, val) be optimization problems with guarantees C1, S1 and C2, S2, respectively. Let \u03c41 = +1 if P1 is a maximization problem, and \u03c41 = \u22121 if P1 is a minimization problem. Similarly, let \u03c42 = \u00b11 depending on whether P2 is a maximization problem or a minimization problem. A reduction from P1 to P2 respecting the guarantees consists of 1. two mappings: \u2217 : I1 \u2192 I2 and \u2217 : S1 \u2192 S2 translating instances and feasible solutions indepen-\ndently;\n2. two nonnegative I1 \u00d7 S1 matrices M1, M2 subject to the conditions\n\u03c41 [C1(I1)\u2212 valI1(s1)] = \u03c42 [ C2(I\u22171 )\u2212 valI\u22171 (s \u2217 1) ]\nM1(I1, s1) + M2(I1, s1) (50-complete) \u03c42 OPT (I\u22171 ) \u2264 \u03c42S2(I\u22171 ) if \u03c41 OPT (I1) \u2264 \u03c41S1(I1). (50-sound)\nThe matrices M1 and M2 control the parameters of the reduction relating the integrality gap of relaxations for P1 to the integrality gap of corresponding relaxations for P2. For a matrix A, let rk+ A and rkpsd A denote the nonnegative rank and psd rank of A respectively. The following theorem is a restatement of Theorem 3.2 from [Braun et al., 2015] ignoring constants.\nTheorem A.4. [Braun et al., 2015] Let P1 and P2 be optimization problems with a reduction from P1 to P2 respecting the completeness guarantees C1, C2 and soundness guarantees S1, S2 of P1 and P2, respectively. Then\nfcLP(P1, C1, S1) \u2264 rk+ M2 + rk+ M1 + rk+ M1 \u00b7 fcLP(P2, C2, S2), (51) fcSDP(P1, C1, S1) \u2264 rkpsd M2 + rkpsd M1 + rkpsd M1 \u00b7 fcSDP(P2, C2, S2), (52)\nwhere M1 and M2 are the matrices in the reduction as in Definition A.3.\nTherefore to obtain a lower bound for problem P2, it suffices to find a source problem P1 and matrices M1 and M2 of low nonnegative rank and low psd rank, satisfying Definition A.3. Below, we cast the hierarchical clustering problem (HCLUST) as an optimization problem. We also recall a different formulation of cost function (1) due to [Dasgupta, 2016] that will be useful in the analysis of the reduction.\nDefinition A.5 (HCLUST as optimization problem). The minimization problem HCLUST of size n consists of\ninstances similarity function \u03ba : E(Kn)\u2192 R\u22650 feasible solutions hierarchical clustering r, T of V(Kn)\nmeasure val\u03ba(T) = \u2211{i,j}\u2208E(Kn) \u03ba(i, j) |leaves(T[lca(i, j)])|. Wewill alsomake use of the following alternate interpretation of cost function (1) given by [Dasgupta, 2016]. Let \u03ba : V \u00d7 V \u2192 R\u22650 be an instance of HCLUST. For a subset S \u2286 V, a split S1, . . . , Sk is a partition of S into k disjoint pieces. For a binary split S1, S2 we can define \u03ba(S1, S2) := \u2211i\u2208S1,j\u2208S2 \u03ba(i, j). This can be extended to k-way splits in the natural way:\n\u03ba(S1, . . . , Sk) := \u2211 1\u2264i\u2264j\u2264k \u03ba(Si, Sj).\nThen the cost of a tree T is the sum over all the internal nodes of the splitting costs at the nodes, as follows.\ncost(T) = \u2211 splits S\u2192(S1,...,Sk) in T |S| \u03ba(S1, . . . , Sk).\nWe now briefly recall the MAXCUT problem.\nDefinition A.6 (MAXCUT as optimization problem). Themaximization problemMAXCUT of size n consists of\ninstances all graphs G with V(G) \u2286 [n]\nfeasible solutions all subsets X of [n]\nmeasure valG(X) = |\u03b4G(X)|.\nSimilarly, the Minimum Linear Arrangement problem can be phrased as an optimization problem as follows.\nDefinition A.7 (MLA as optimization problem). The minimization problem MLA of size n consists of\ninstances weight function w : E(Kn)\u2192 R\u22650 feasible solutions all permutations \u03c0 : V(Kn)\u2192 [n]\nmeasure valw(\u03c0) := \u2211{i,j}\u2208E(Kn) w(i, j) |\u03c0(i)\u2212 \u03c0(j)|.\nWe now describe the reduction from MAXCUT to HCLUST which is a modification of the reduction from MAXCUT to MLA due to [Garey et al., 1976]. Note that an instance of MAXCUT maps to an unweighted instance of HCLUST, i.e., \u03ba \u2208 {0, 1}.\nMapping instances Given an instance G = (V, E) of MAXCUT of size n, let r = n4 andU = {u1, u2, . . . , ur}. The instance \u03ba of HCLUST is on the graph with vertex set V \u2032 := V \u222aU and has weights in {0, 1}. For any distinct pair i, j \u2208 V \u2032, if {i, j} \u2208 E then we define \u03ba(i, j) := 0 and otherwise we set \u03ba(i, j) := 1.\nMapping solutions Given a cut X \u2286 V of MAXCUT we map it to the clustering r, T of V \u2032 where the root r has the following children: n4 leaves corresponding to U, and 2 internal vertices corresponding to X and X. The internal vertices for X and X are split into |X| and\n\u2223\u2223X\u2223\u2223 leaves respectively at the next level.\nThe following lemma relates the LP and SDP formulations for MAXCUT and MLA.\nLemma A.8. For any completeness and soundness guarantee (C, S), we have the following\nfcLP (MAXCUT, C, S) \u2264 fcLP ( HCLUST, C\u2032, S\u2032 ) + O(n2)\nfcSDP (MAXCUT, C, S) \u2264 fcSDP ( HCLUST, C\u2032, S\u2032 ) + O(n2).\nwhere C\u2032 := (n 4+n)3\u2212(n4+n) 3 \u2212 C(n4 + n) and S\u2032 := (n 4+n+1 3 )\u2212 Sn4.\nProof. To show completeness, we analyze the cost of the tree T that a cut X maps to, using the alternate interpretation of the cost function (1) due to [Dasgupta, 2016] (see above). Let H be the graph on vertex set V \u2032 induced by \u03ba, i.e. {i, j} \u2208 E(H) iff \u03ba(i, j) = 1. Let H denote the complement graph of H and let \u03ba be the similarity function induced by it, i.e., \u03ba(i, j) = 1 iff {i, j} 6\u2208 E(H) and \u03ba(i, j) = 0 otherwise. For a hierarchical clustering T of V \u2032, we denote by costH(T) and costH(T) the cost of T induced by \u03ba and \u03ba respectively, i.e., costH(T) := \u2211{i,j}\u2208E(H) |leaves(T[lca(i, j)])| and costH(T) := \u2211{i,j}6\u2208E(H) |leaves(T[lca(i, j)])|. Let X := V \u2032 \\ X. The cost of the tree T that the cut X maps to, is given by\ncost(T) = costH(T)\n=\n( n + n4 )3 \u2212 (n + n4) 3 \u2212 costH(T)\n=\n( n + n4 )3 \u2212 (n + n4) 3\n\u2212 \u2211 splits S\u2192(S1,...,Sk) in T |S| \u03ba(S1, . . . , Sk)\n=\n( n + n4 )3 \u2212 (n + n4) 3 \u2212 ( n + n4 ) valG(X)\u2212 ( |X| |E[X]|+ \u2223\u2223X\u2223\u2223 \u2223\u2223E[X]\u2223\u2223) , where E[X] and E[X] are the edges of E(H) induced on the set X and X respectively. Therefore, we have the following completeness relationship between the two problems\nC\u2212 valG(X) = 1\nn + n4\n( cost(T)\u2212 ( (n + n4)3 \u2212 (n + n4)\n3 \u2212 C(n + n4)\n)) + |X| |E[X]|+ \u2223\u2223X\u2223\u2223 \u2223\u2223E[X]\u2223\u2223 n4 + n .\nWenowdefine thematrices M1 and M2 as M1(H, X) := 1n+n4 and M2(H, X) := |X| |E[X]|+ \u2223\u2223X\u2223\u2223 \u2223\u2223E[X]\u2223\u2223.\nClearly, M1 has O(1) nonnegative rank and psd rank. We claim that the nonnegative rank of M2 is at most 2(n2). The vectors vH \u2208 R2( n 2) corresponding to the instances H is defined as the concatenation [uH, wH ] of two vectors uH, wH \u2208 R( n 2). Both the vectors uH, wH encode the edges of H scaled by n4 + n, i.e., uH({i, j}) = wH({i, j}) = 1/(n4 + n) iff {i, j} \u2208 E(H) and 0 otherwise. The vectors vX \u2208 R2( n 2) corresponding to the solutions are also defined as the concatenation [uX, wX] of two vectors uX, wX \u2208 Rn. The vector uX encodes the vertices in X scaled by |X| i.e., uX({i, j}) = |X| iff i, j \u2208 X and 0 otherwise. The vector wX encodes the vertices in X scaled by\n\u2223\u2223X\u2223\u2223 i.e., wX({i, j}) = \u2223\u2223X\u2223\u2223 iff i, j \u2208 X and 0 otherwise. Clearly, we have M2(H, X) = \u3008vH, vX\u3009 and so the nonnegative (and psd) rank of M2 is at most 2(n2). Soundness follows due to the analysis in [Garey et al., 1976] and by noting that the cost of a linear arrangement obtained by projecting the leaves of T is a lower bound on cost(T). By the analysis in [Garey et al., 1976] if the optimal value OPT(G) of MAXCUT is at most S, then the optimal value of MLA on V \u2032, \u03ba is at least (n\n4+n+1 3 ) \u2212 Sn4. Therefore, it follows that the optimal value of HCLUST on V \u2032, \u03ba is also at least\n(n 4+n+1\n3 )\u2212 Sn4.\nThe constant factor inapproximability result for HCLUST now follows due to the following theorems.\nTheorem A.9 ([Chan et al., 2013, Theorem 3.2]). For any \u03b5 > 0 there are infinitely many n such that\nfcLP\n( MAXCUT, 1\u2212 \u03b5, 1\n2 +\n\u03b5\n6\n) \u2265 n\u2126(log n/ log log n).\nTheorem A.10 ([Braun et al., 2015, Theorem 7.1]). For any \u03b4, \u03b5 > 0 there are infinitely many n such that\nfcSDP\n( MAXCUT,\n4 5 \u2212 \u03b5, 3 4 + \u03b4\n) = n\u2126(log n/ log log n). (53)\nThus we have the following corollary about the LP and SDP inapproximability for the problem HCLUST.\nCorollary A.11 (LP and SDP hardness for HCLUST). For any constant c \u2265 1, HCLUST is LP-hard and SDP-hard with an inapproximability factor of c.\nProof. Straightforward by using Theorems A.9 and A.10 together with Lemma A.8 and by choosing n large enough.\nThe following lemma shows that a minor modification of the argument in [Raghavendra et al., 2012] also implies a constant factor inapproximability result under the Small Set Expansion (SSE) hypothesis. Note that this reduction is also true for unit capacity graphs, i.e., \u03ba \u2208 {0, 1}. We briefly recall the formulation of the Small Set Expansion hypothesis. Informally, given a graph G = (V, E) the problem is to decide whether all \u201csmall\u201d sets in the graph are expanding. Let d(i) denote the degree of a vertex i \u2208 V. For a subset S \u2286 V let \u00b5(S) := |S| / |V| be the volume of S, and let \u03c6(S) := E(S, S)/ \u2211i\u2208S d(i) be the expansion of S. Then the SSE problem is defined as follows.\nDefinition A.12 (Small set expansion (SSE) hypothesis [Raghavendra et al., 2012]). For every constant \u03b7 > 0, there exists sufficiently small \u03b4 > 0 such that given a graph G = (V, E), it is NP-hard to decide the following cases,\nCompleteness there exists a subset S \u2286 V with volume \u00b5(S) = \u03b4 and expansion \u03c6(S) \u2264 \u03b7,\nSoundness every subset S \u2286 V of volume \u00b5(S) = \u03b4 has expansion \u03c6(S) \u2265 1\u2212 \u03b7.\nUnder this assumption, [Raghavendra et al., 2012] proved the following amplification result about the expansion of small sets in the graph.\nTheorem A.13 (Theorem 3.5 [Raghavendra et al., 2012]). For all q \u2208 N and \u03b5\u2032, \u03b3 > 0 it is SSE-hard to distinguish the following for a given graph H = (VH, EH)\nCompleteness There exist disjoint sets S1, . . . , Sq \u2286 VH satisfying \u00b5(Si) = 1q and \u03c6(Si) \u2264 \u03b5\u2032 + o(\u03b5\u2032) for all i \u2208 [n],\nSoundness For all sets S \u2286 VH we have \u03c6(S) \u2265 \u03c6G(1\u2212 \u03b5\u2032/2)(\u00b5(S))\u2212 \u03b3/\u00b5(S),\nwhere \u03c6G(1\u2212 \u03b5\u2032/2)(\u00b5(S)) is the expansion of sets of volume \u00b5(S) in the infinite Gaussian graph G(1\u2212 \u03b5\u2032/2).\nThe following lemma establishes that it is SSE-hard to approximateHCLUST to within any constant factor. The argument closely parallels Corollary A.5 of [Raghavendra et al., 2012] where it was shown that it is SSEhard to approximate MLA to within any constant factor.\nLemma A.14. Let G = (V, E) be a graph on V with \u03ba induced by the edges E i.e., \u03ba(i, j) = 1 iff {i, j} \u2208 E and 0 otherwise. Then it is SSE-hard to distinguish between the following two cases\nCompleteness There exists a hierarchical clustering T of V with cost(T) \u2264 \u03b5n |E|, Soundness Every hierarchical clustering T of V satisfies cost(T) \u2265 c\u221a\u03b5n |E|\nfor some constant c not depending on n.\nProof. Apply Theorem A.13 on the graph G with the following choice of parameters: q = d2/\u03b5e, \u03b5\u2032 = \u03b5/3 and \u03b3 = \u03b5. Suppose there exist S1, . . . , Sq \u2286 V satisfying \u03c6(Si) \u2264 \u03b5\u2032+ o(\u03b5\u2032) and |Si| = |V| /q \u2264 \u03b5 |V| /2. Then consider the tree r, T with the root r having q children corresponding to each Si, and each Si being further separated into |Si| leaves at the next level. We claim that cost(T) \u2264 \u03b5n |E|. We analyze this using the alternate interpretation of cost function (1) (see above). Every crossing edge between Si, Sj for distinct i, j \u2208 [q] incurs a cost of n, but by assumption there are at most \u03b5 |E| /2 such edges. Further, any edge in Si incurs a cost nq \u2264 \u03b5n/2 and thus their contribution is upper bounded by \u03b5n |E|. The analysis for soundness follows by the argument of Corollary A.5 in [Raghavendra et al., 2012]. In particular, if for every S \u2286 V we have \u03c6(S) \u2265 \u03c6G(1\u2212 \u03b5\u2032/2)(\u00b5(S))\u2212 \u03b3/\u00b5(S) then the cost of the optimal linear arrangement on G is at most \u221a \u03b5n |E|. Since the cost of any tree (including the optimal tree) is at least the cost of the linear arrangement induced by projecting the leaf vertices, the claim about soundness follows."}], "references": [{"title": "Characterization of linkagebased clustering", "author": ["Ackerman et al", "M. 2010] Ackerman", "S. Ben-David", "D. Loker"], "venue": "In COLT,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Fitting tree metrics: Hierarchical clustering and phylogeny", "author": ["Ailon", "Charikar", "N. 2005] Ailon", "M. Charikar"], "venue": "In 46th Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "Ailon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2005}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["Arora et al", "S. 2009] Arora", "S. Rao", "U. Vazirani"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Relax, no need to round: Integrality of clustering formulations", "author": ["Awasthi et al", "P. 2015] Awasthi", "A.S. Bandeira", "M. Charikar", "R. Krishnaswamy", "S. Villar", "R. Ward"], "venue": "In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Balcan et al", "2008] Balcan", "M.-F", "A. Blum", "S. Vempala"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Y", "author": ["Bartal"], "venue": "(1996). Probabilistic approximation of metric spaces and its algorithmic applications. In Foundations of Computer Science,", "citeRegEx": "Bartal. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "M", "author": ["Y. Bartal", "B. Bollob\u00e1s", "Mendel"], "venue": "(2001). A ramsey-type theorem for metric spaces and its applications for metrical task systems and related problems. In Foundations of Computer Science,", "citeRegEx": "Bartal et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Onmetric ramsey-type phenomena", "author": ["Bartal et al", "Y. 2003] Bartal", "N. Linial", "M. Mendel", "A. andNaor"], "venue": "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Strong reductions for extended formulations", "author": ["Braun et al", "G. 2015] Braun", "S. Pokutta", "A. Roy"], "venue": "CoRR, abs/1512.04932. 29,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Approximate constraint satisfaction requires large lp relaxations", "author": ["Chan et al", "S.O. 2013] Chan", "J. Lee", "P. Raghavendra", "D. Steurer"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Approximate hierarchical clustering via sparsest cut and spreading metrics", "author": ["Charikar", "Chatziafratis", "M. 2016] Charikar", "V. Chatziafratis"], "venue": "arXiv preprint arXiv:1609.09548", "citeRegEx": "Charikar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2016}, {"title": "A constant-factor approximation algorithm for the k-median problem", "author": ["Charikar et al", "M. 1999] Charikar", "S. Guha", "\u00c9. Tardos", "D.B. Shmoys"], "venue": "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "A", "author": ["M. Charikar", "V. Guruswami", "Wirth"], "venue": "(2003). Clustering with qualitative information. In Foundations of Computer Science,", "citeRegEx": "Charikar et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A dependent lp-rounding approach for the k-median problem", "author": ["Charikar", "Li", "M. 2012] Charikar", "S. Li"], "venue": "In Automata, Languages, and Programming,", "citeRegEx": "Charikar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2012}, {"title": "Performance guarantees for hierarchical clustering", "author": ["Dasgupta", "Long", "S. 2005] Dasgupta", "P.M. Long"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Dasgupta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2005}, {"title": "Finding the closest ultrametric", "author": ["Di Summa et al", "M. 2015] Di Summa", "D. Pritchard", "L. Sanit\u00e0"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fast approximate graph partitioning algorithms", "author": ["Even et al", "G. 1999] Even", "J. Naor", "S. Rao", "B. Schieber"], "venue": "SIAM Journal on Computing,", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Divide-and-conquer approximation algorithms via spreading metrics", "author": ["Even et al", "G. 2000] Even", "J.S. Naor", "S. Rao", "B. Schieber"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "al. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "al. et al\\.", "year": 2000}, {"title": "A tight bound on approximating arbitrary metrics by tree metrics", "author": ["Fakcharoenphol et al", "J. 2003] Fakcharoenphol", "S. Rao", "K. Talwar"], "venue": "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics Springer, Berlin", "author": ["Friedman et al", "J. 2001] Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Some simplified np-complete graph problems", "author": ["Garey et al", "M.R. 1976] Garey", "D.S. Johnson", "L. Stockmeyer"], "venue": "Theoretical computer science, 1(3):237\u2013267", "citeRegEx": "al. et al\\.,? \\Q1976\\E", "shortCiteRegEx": "al. et al\\.", "year": 1976}, {"title": "Approximatemax-flowmin-(multi) cut theorems and their applications", "author": ["Garg et al", "N. 1996] Garg", "V.V. Vazirani", "M. andYannakakis"], "venue": "SIAM Journal on Computing,", "citeRegEx": "al. et al\\.,? \\Q1996\\E", "shortCiteRegEx": "al. et al\\.", "year": 1996}, {"title": "Greedy facility location algorithms analyzed using dual fitting with factor-revealing lp", "author": ["Jain et al", "K. 2003] Jain", "M. Mahdian", "E. Markakis", "A. Saberi", "V.V. andVazirani"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation", "author": ["Jain", "Vazirani", "K. 2001] Jain", "V.V. Vazirani"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Jain et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2001}, {"title": "The construction of hierarchic and nonhierarchic classifications", "author": ["Jardine", "Sibson", "N. 1968] Jardine", "R. Sibson"], "venue": "The Computer Journal,", "citeRegEx": "Jardine et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Jardine et al\\.", "year": 1968}, {"title": "Partitioning graphs into balanced components", "author": ["Krauthgamer et al", "R. 2009] Krauthgamer", "J.S. Naor", "R. Schwartz"], "venue": "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "S", "author": ["T. Leighton", "Rao"], "venue": "(1988). An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms. In Foundations of Computer Science,", "citeRegEx": "Leighton and Rao. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms", "author": ["Leighton", "Rao", "T. 1999] Leighton", "S. Rao"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Leighton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Leighton et al\\.", "year": 1999}, {"title": "Approximating k-median via pseudoapproximation", "author": ["Li", "Svensson", "S. 2013] Li", "O. Svensson"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "An experimental comparison of model-based clustering methods", "author": ["Meil\u0103", "Heckerman", "M. 2001] Meil\u0103", "D. Heckerman"], "venue": "Machine learning,", "citeRegEx": "Meil\u0103 et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Meil\u0103 et al\\.", "year": 2001}, {"title": "Approximating k-means-type clustering via semidefinite programming", "author": ["Peng", "Wei", "J. 2007] Peng", "Y. Wei"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Peng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2007}, {"title": "A new theoretical framework for k-means-type clustering", "author": ["Peng", "Xia", "J. 2005] Peng", "Y. Xia"], "venue": "In Foundations and advances in data mining,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Reductions between expansion problems", "author": ["Raghavendra et al", "P. 2012] Raghavendra", "D. Steurer", "M. Tulsiani"], "venue": "In Computational Complexity (CCC),", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["Recht et al", "B. 2012] Recht", "C. Re", "J. Tropp", "V. Bittorf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Numerical taxonomy. The principles and practice of numerical classification", "author": ["Sneath et al", "P.H. 1973] Sneath", "Sokal", "R. R"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1973\\E", "shortCiteRegEx": "al. et al\\.", "year": 1973}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Ward Jr.", "J.H. 1963] Ward Jr."], "venue": "Journal of the American statistical association,", "citeRegEx": "Jr and Jr,? \\Q1963\\E", "shortCiteRegEx": "Jr and Jr", "year": 1963}, {"title": "A uniqueness theorem for clustering", "author": ["Zadeh", "Ben-David", "R.B. 2009] Zadeh", "S. Ben-David"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "Zadeh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zadeh et al\\.", "year": 2009}, {"title": "Wewill alsomake use of the following alternate interpretation of cost function (1) given by [Dasgupta, 2016", "author": ["leaves(T[lca(i"], "venue": null, "citeRegEx": "leaves.T.lca.i and j...|.,? \\Q2016\\E", "shortCiteRegEx": "leaves.T.lca.i and j...|.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "We then show how to round an optimal fractional solution using the sphere growing technique first introduced in [Leighton and Rao, 1988] (see also [Garg et al.", "startOffset": 112, "endOffset": 136}, {"referenceID": 26, "context": "To round LP-ultrametric to get a feasible solution for ILP-ultrametric, we will use the technique of sphere growingwhich was introduced in [Leighton and Rao, 1988] to show anO(log n) approximation for the maximum multicommodity flow problem.", "startOffset": 139, "endOffset": 163}, {"referenceID": 5, "context": "In particular we briefly mention the related notion of hierarchically well-separated trees (HST) as defined in [Bartal, 1996] (see also [Bartal et al.", "startOffset": 111, "endOffset": 125}], "year": 2016, "abstractText": "We study the cost function for hierarchical clusterings introduced by [Dasgupta, 2016] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [Dasgupta, 2016] that a top-down algorithm returns a hierarchical clustering of cost at most O (\u03b1n log n) times the cost of the optimal hierarchical clustering, where \u03b1n is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O ( log3/2 n ) times the cost of the optimal solution. We improve this by giving an O(log n)approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2016]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the assumption of the Small Set Expansion hypothesis.", "creator": "LaTeX with hyperref package"}}}