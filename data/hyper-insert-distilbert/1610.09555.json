{"id": "1610.09555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "TensorLy: Tensor Learning in Python", "abstract": "tensor processing methods are gaining drastically increasing traction developments in machine software learning. however, naturally there are scant little to no resources available to perform tensor learning and braid decomposition operations in python. anything to answer this criticisms need we formally developed from tensorly. written tensorly is virtually a state of the art general purpose numerical library container for understanding tensor learning. albeit written in basic python, it aims at following the previous same standards adopted widely by the main projects of the python automated scientific community and and fully integrating with these. it immediately allows for fast and straightforward tensor decomposition and statistical learning machines and comes alive with exhaustive tests, unusually thorough descriptive documentation sheets and almost minimal polynomial dependencies. it can too be easily extended and its bsd licence required makes aix it suitable for both complex academic and commercial applications. downloadable tensorly version is available at", "histories": [["v1", "Sat, 29 Oct 2016 18:32:27 GMT  (155kb,D)", "http://arxiv.org/abs/1610.09555v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jean kossaifi", "yannis panagakis", "maja pantic"], "accepted": false, "id": "1610.09555"}, "pdf": {"name": "1610.09555.pdf", "metadata": {"source": "CRF", "title": "TensorLy: Tensor Learning in Python", "authors": ["Jean Kossaifi", "Yannis Panagakis"], "emails": ["jean.kossaifi@imperial.ac.uk", "i.panagakis@imperial.ac.uk", "m.pantic@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Tensors, or multi-way arrays, of order higher than two, are multi-dimensional arrays indexed by three or more indices, generalizing the notion of matrix (second order tensor). Tensors and tensor decompositions or factorizations have a rich history, stretching almost a century, but only recently i.e., roughly a decade ago, have become ubiquitous in signal processing, statistics, data analytics, and machine learning.\nIndeed, psychometrics and chemometrics have historically been the application areas driving theoretical and algorithmic developments in the field. However, tensors and their decompositions were popularized by the (statisitical) signal processing and machine learning communities when they realized the power of tensor decompositions in practice. Examples of such applications include, speech, music, image, communications, biomedical, and social network signal processing and analysis, as well as clustering, dimensionality reduction, subspace, dictionary and features learning e.g., [23, 24, 30, 22, 20, 25, 14, 2].\nMore recently, there has been a considerable amount of research in establishing connections between tensor decompositions, the method of (high-order) moments, and compositional function spaces in order to learn latent variable models (e.g., mulitiview mixture model, Gaussian mixtures, Independent Component Analysis) [3], train deep neural networks with theoretical guarantees [13], and also theoretically analyze their impressive empirical performance [8].\nThe interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].\nBased on the above discussion, tensors and their decompositions have profound impact in signal (data) analytics and machine learning with clear theoretical, algorithmic, and practical advantages\nar X\niv :1\n61 0.\n09 55\n5v 1\n[ cs\n.L G\n] 2\n9 O\nover their matrix counterparts. However, as opposed to matrix factorizations and matrix-based machine learning methods, tensor decompositions have not been widely adopted by data scientists and practitioners, yet. This can be mainly attributed to the fact that there is a lack in available libraries for tensor operations and decompositions, accessible from programming languages (e.g., Python, Java, Scala, etc) that data scientists and practitioners are familiar with. Even though some libraries exist for handling tensors, these are implemented in non-free platforms (e.g. MATLAB\u2019s TensorToobox [5] and TensorLab [21]) or in low-level languages like C++ (e.g. TH++) and the deep learning libraries e.g., Tensorflow and Torch can only suboptimally handle tensors.\nPython is emerging as a language of choice for machine learning, as witnessed with the success of scikit-learn [27], and is increasingly used in both academic and industrial research projects. However, there is to date no Python library implementing tensor decomposition and learning. The exisiting ones (e.g., scikit-tensor) offer only limited algorithms (e.g., decomposition only) and/or have restrictive licenses. For applications to data analytics and machine learning, open source, well-developed and -documented libraries that include methods for tensor decompositions are urgently needed.\nTensorLy1 introduces several contributions over the existing libraries: a) it provides state of the art tensor learning including core tensor operations, tensor decomposition and tensor regression methods. b) it is open source and BSD licensed. c) it comes with extensive tests and documentation; and d) it depends exclusively on numpy and scipy.\n2 TensorLy functionality and implementation\nTensorLy has been developed with the goal to make tensor learning more accessible and to allow for seemless integration with the python scientific environment. It builds on top of two core Python libraries, Numpy [31] and Scipy [15] while having a soft-dependency on Matplotlib [12] for plotting:\nNumpy The standard library for numerical computation in Python. It offers high performance structures for manipulating multi-dimensional arrays. In particular, in TensorLy, we leverage this convenient structure for efficient tensor operations.\nScipy Provides high performance mathematical functions, advantageously using numpy\u2019s ndarray structure.\nMatplotlib Cross-compatible 2D graphics package offering high quality image and graphics generation.\nThe Application Programming Interface (API) aims at compatibility with scikit-learn [27], which is the de-facto standard library for performing classical Machine Learning, preprocessing and crossvalidation. While scikit-learn is built to work with observations (samples) represented as vectors, this library focuses on higher order arrays.\nTensorLy\u2019s current functionalities in term of tensor operations are summarised in Table. 2, where inside the parenthesis the mathematical notation of Kolda and Bader [17] is adopted. Furthermore, we have implemented core tensor decomposition and tensor regression methods listed in Table. 2.\nTensorLy has been tailored for the Python ecosystem and the operations are optimised for speed: tensor operations have been redefined when possible to allow for better performance. In particular, we\n1TensorLy is available at https://github.com/tensorly/tensorly\npropose an efficient unfolding of tensors which differs from the traditional one [17] by the ordering of the columns.\nGiven a tensor, X\u0303 \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , the mode-n unfolding of X\u0303 is a matrix X[n] \u2208 RIn,IM , with M = \u220fN k=1, k 6=n Ik and is defined by the mapping from element (i1, i2, \u00b7 \u00b7 \u00b7 , iN ) to (in, j), with\nj = \u2211N\nk=1, k 6=n\nik \u00d7 \u220fN m=k+1 Im.\nNot only does this formulation achieve better performance when using C-ordering of the elements (as numpy does by default), it also translates into more natural properties. For instance, given a tensor X\u0303 \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN and its Tucker decomposition JG\u0303; U(1), \u00b7 \u00b7 \u00b7 ,U(N)K, we can express the mode-n unfolding of X\u0303 as :\nX[n] = U (n)G[n] ( U(1) \u2297 \u00b7 \u00b7 \u00b7U(n\u22121) \u2297U(n+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297U(N) )T Finally, we emphasize code quality and ease of utilisation for the end user. To that extent, both testing and documentation are an essential part of the package. Each function comes with its documentation and unit-tests (at the time of writing, the coverage is of 99 %)."}, {"heading": "3 Experiments", "text": "In TensorLy, tensors are simply numpy mutli-dimensional arrays which are passed directly to the various methods, decomposition or regression. This allows for competitive performance even though the library is implemented in a high-level, interactive language."}, {"heading": "3.1 Tensor regression", "text": "TensorLy offers a simple interface for tensor regression with the same API as Scikit-Learn. The regressors are object that expose a fit method that takes as parameters the data tensor X\u0303 and the corresponding array of labels y. Given new data X\u0303T , the predict method returns the regressed labels yT .\nTo illustrate the effectiveness of tensor regression, we fixed the regression weights W\u0303 to be a second order tensor (a cross or a yin-yang image) of size (50\u00d750). We then generated a random tensor X\u0303 of size (1500\u00d750\u00d750) of which each element was sampled from a normal distribution. Finally, we constructed the corresponding response array y of size 1500 as: \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , 1500},yi = \u3008X\u0303i, W\u0303\u3009. We use this data to train a rank-10 Kruskal Tucker Regression and a rank (10, 10, 10) Tucker Ridge Regression. We also train a classical Ridge Regression model on the vectorised training samples (we use the scikit-learn implementation).\nIn Fig. 3.1, we present in the first column the original weight. The second and third column present the weight\nlearnt from our Tucker and Kruskal regression models while the last column presents the weights\nlearned by a classical Ridge Regression. As can be observed, tensor regression gives more interpretable result due to its ability to take into account the structure in the data that is lost when vectorising it, and thanks to its low rank weight coefficient."}, {"heading": "3.2 Tensor decomposition", "text": "We generated third order random tensors of size (K \u00d7K \u00d7K) for K varying from 10 to 400 with a step of 10, for a total of elements varying from 1,000 to 64,000,000. We then compared runtime and performance with scikit-tensor (sktensor) and the Matlab Tensor Toolbox (tensor toolbox) for CANDECOMP-PARAFAC and Tucker decomposition of these tensors.\nWe first apply a rank 10 CANDECOMP-PARAFAC decomposition via Alternating Least Squares (ALS). In Fig. 2 we show the evolution of the performance and runtime as a function of the number of elements in each mode of the tensor. Each method was run for exactly 100 iterations with an SVD based initialisation.\nSimilarly, in Fig. 3, we show the evolution of the performance and the execution time for a rank (10, 10, 10) Tucker decomposition via Higher Order Orthogonal Iteration (HOI), when running each method for exactly 100 iterations with an SVD based initialisation.\nAs can be observed, all methods yield similar performance, whilst our library offers competitive speed."}, {"heading": "4 Conclusion and future work", "text": "TensorLy makes tensor learning accessible and easy by offering state-of-the-art tensor methods and operations through simple consistent interfaces under a permissive license. Interestingly, experimental evaluation indicates that tensor decomposition implemented in TensorLy are executed faster than their corresponding Matlab implementation. Thus, the library allows for efficient comparison of existing methods and can be easily extended to new ones, with systematic unit-tests and documentation.\nGoing forward we will further extend the available decompositions with other state-of-the-art methods such as PARAFAC2, DEDICOM, etc and also include robust tensor decomposition [4, 9]. It is worth noting that proximal operators for the `1 norm and nuclear norm are already available in TensorLy."}], "references": [{"title": "Unsupervised Multiway Data Analysis: A Literature Survey", "author": ["E. Acar", "B. Yener"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Analyzing Tensor Power Method Dynamics: Applications to Learning Overcomplete Latent Variable Models", "author": ["Anima Anandkumar", "Rong Ge", "Majid Janzamin"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Tensor Decompositions for Learning Latent Variable Models", "author": ["Animashree Anandkumar"], "venue": "In: J. Mach. Learn. Res", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations", "author": ["Animashree Anandkumar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "MATLAB Tensor Toolbox Version 2.6", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "Available online", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Tensor Decompositions for Signal Processing Applications: From two-way to multiway component analysis", "author": ["A. Cichocki"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation", "author": ["Andrzej Cichocki"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "On the Expressive Power of Deep Learning: A Tensor Analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Robust Low-Rank Tensor Recovery: Models and Algorithms", "author": ["Donald Goldfarb", "Zhiwei (Tony) Qin"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A literature survey of low-rank tensor approximation techniques", "author": ["Lars Grasedyck", "Daniel Kressner", "Christine Tobler"], "venue": "GAMM-Mitteilungen", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Tensor Learning for Regression", "author": ["W. Guo", "I. Kotsia", "I. Patras"], "venue": "IEEE Transactions on Image Processing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Matplotlib: A 2D Graphics Environment", "author": ["J.D. Hunter"], "venue": "Computing in Science Engineering", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Score Function Features for Discriminative Learning: Matrix and Tensor Framework", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "SciPy: Open source scientific tools for Python. [Online; accessed 2016-10-21", "author": ["Eric Jones", "Travis Oliphant", "Pearu Peterson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Nonnegative Tucker Decomposition", "author": ["Yong-Deok Kim", "Seungjin Choi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Tensor Decompositions and Applications", "author": ["Tamara G. Kolda", "Brett W. Bader"], "venue": "SIAM REVIEW", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Tucker tensor regression and neuroimaging analysis", "author": ["Xiaoshan Li", "Hua Zhou", "Lexin Li"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A survey of multilinear subspace learning for tensor data", "author": ["Haiping Lu", "Konstantinos N. Plataniotis", "Anastasios N. Venetsanopoulos"], "venue": "Pattern Recognition", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Applications of tensor (multiway array) factorizations and decompositions in data mining", "author": ["M. M\u00f8rup"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Tensor algebra and multidimensional harmonic retrieval in signal processing for MIMO radar", "author": ["Dimitri Nion", "Nicholas D. Sidiropoulos"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Batch and Adaptive PARAFAC-based Blind Separation of Convolutive Speech Mixtures", "author": ["Dimitri Nion"], "venue": "In: Trans. Audio, Speech and Lang. Proc", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Non-negative multilinear principal component analysis of auditory temporal modulations for music genre classification", "author": ["Yannis Panagakis", "Constantine Kotropoulos", "Gonzalo R Arce"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "ParCube: Sparse Parallelizable Tensor Decompositions", "author": ["Evangelos E. Papalexakis", "Christos Faloutsos", "Nicholas D. Sidiropoulos"], "venue": "ECML PKDD", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Tensors for Data Mining and Data Fusion: Models, Applications, and Scalable Algorithms", "author": ["Evangelos E. Papalexakis", "Christos Faloutsos", "Nicholas D. Sidiropoulos"], "venue": "ACM Trans. Intell. Syst. Technol", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F. Pedregosa"], "venue": "Journal of Machine Learning Research", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["Amnon Shashua", "Tamir Hazan"], "venue": "Proceedings of the International Conference on Machine Learning (ICML). ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "tensor decomposition for signal processing and machine learning", "author": ["Nicholas D Sidiropoulos"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Multilinear Analysis of Image Ensembles: TensorFaces", "author": ["M.A.O. Vasilescu", "Demetri Terzopoulos"], "venue": "Proceedings of the 7th European Conference on Computer Vision-Part I. ECCV \u201902", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "The NumPy Array: A Structure for Efficient Numerical Computation", "author": ["S. van der Walt", "S.C. Colbert", "G. Varoquaux"], "venue": "Computing in Science Engineering", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["Hua Zhou", "Lexin Li", "Hongtu Zhu"], "venue": "Journal of the American Statistical Association", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 22, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 28, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 20, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 19, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 23, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 13, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 1, "context": ", [23, 24, 30, 22, 20, 25, 14, 2].", "startOffset": 2, "endOffset": 33}, {"referenceID": 2, "context": ", mulitiview mixture model, Gaussian mixtures, Independent Component Analysis) [3], train deep neural networks with theoretical guarantees [13], and also theoretically analyze their impressive empirical performance [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 12, "context": ", mulitiview mixture model, Gaussian mixtures, Independent Component Analysis) [3], train deep neural networks with theoretical guarantees [13], and also theoretically analyze their impressive empirical performance [8].", "startOffset": 139, "endOffset": 143}, {"referenceID": 7, "context": ", mulitiview mixture model, Gaussian mixtures, Independent Component Analysis) [3], train deep neural networks with theoretical guarantees [13], and also theoretically analyze their impressive empirical performance [8].", "startOffset": 215, "endOffset": 218}, {"referenceID": 16, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 189, "endOffset": 197}, {"referenceID": 27, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 189, "endOffset": 197}, {"referenceID": 6, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 223, "endOffset": 241}, {"referenceID": 18, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 223, "endOffset": 241}, {"referenceID": 9, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 223, "endOffset": 241}, {"referenceID": 5, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 223, "endOffset": 241}, {"referenceID": 24, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 223, "endOffset": 241}, {"referenceID": 0, "context": "The interested reader is referred to several surveys in the topic, which focus range from basics of multilinear (tensor) algebra and overview of different established tensor decompositions [17, 29], to algorithmic advances [7, 19, 10, 6, 26] and applications [1].", "startOffset": 259, "endOffset": 262}, {"referenceID": 4, "context": "MATLAB\u2019s TensorToobox [5] and TensorLab [21]) or in low-level languages like C++ (e.", "startOffset": 22, "endOffset": 25}, {"referenceID": 25, "context": "Python is emerging as a language of choice for machine learning, as witnessed with the success of scikit-learn [27], and is increasingly used in both academic and industrial research projects.", "startOffset": 111, "endOffset": 115}, {"referenceID": 29, "context": "It builds on top of two core Python libraries, Numpy [31] and Scipy [15] while having a soft-dependency on Matplotlib [12] for plotting:", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "It builds on top of two core Python libraries, Numpy [31] and Scipy [15] while having a soft-dependency on Matplotlib [12] for plotting:", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "It builds on top of two core Python libraries, Numpy [31] and Scipy [15] while having a soft-dependency on Matplotlib [12] for plotting:", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "The Application Programming Interface (API) aims at compatibility with scikit-learn [27], which is the de-facto standard library for performing classical Machine Learning, preprocessing and crossvalidation.", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "2, where inside the parenthesis the mathematical notation of Kolda and Bader [17] is adopted.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "[17]) Non-Negative CP decomposition [28]) Tucker decomposition (Higher-Order SVD) (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[17]) Non-Negative CP decomposition [28]) Tucker decomposition (Higher-Order SVD) (e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "[17]) Non-Negative Tucker decomposition [16]) Tensor Ridge regression (Tucker and Kruskal) [11, 32, 18] Table 2: Algorithms implemented", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17]) Non-Negative Tucker decomposition [16]) Tensor Ridge regression (Tucker and Kruskal) [11, 32, 18] Table 2: Algorithms implemented", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "[17]) Non-Negative Tucker decomposition [16]) Tensor Ridge regression (Tucker and Kruskal) [11, 32, 18] Table 2: Algorithms implemented", "startOffset": 91, "endOffset": 103}, {"referenceID": 30, "context": "[17]) Non-Negative Tucker decomposition [16]) Tensor Ridge regression (Tucker and Kruskal) [11, 32, 18] Table 2: Algorithms implemented", "startOffset": 91, "endOffset": 103}, {"referenceID": 17, "context": "[17]) Non-Negative Tucker decomposition [16]) Tensor Ridge regression (Tucker and Kruskal) [11, 32, 18] Table 2: Algorithms implemented", "startOffset": 91, "endOffset": 103}, {"referenceID": 16, "context": "propose an efficient unfolding of tensors which differs from the traditional one [17] by the ordering of the columns.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "Going forward we will further extend the available decompositions with other state-of-the-art methods such as PARAFAC2, DEDICOM, etc and also include robust tensor decomposition [4, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "Going forward we will further extend the available decompositions with other state-of-the-art methods such as PARAFAC2, DEDICOM, etc and also include robust tensor decomposition [4, 9].", "startOffset": 178, "endOffset": 184}], "year": 2016, "abstractText": "Tensor methods are gaining increasing traction in machine learning. However, there are scant to no resources available to perform tensor learning and decomposition in Python. To answer this need we developed TensorLy. TensorLy is a state of the art general purpose library for tensor learning. Written in Python, it aims at following the same standard adopted by the main projects of the Python scientific community and fully integrating with these. It allows for fast and straightforward tensor decomposition and learning and comes with exhaustive tests, thorough documentation and minimal dependencies. It can be easily extended and its BSD licence makes it suitable for both academic and commercial applications. TensorLy is available at https://github.com/tensorly/tensorly.", "creator": "LaTeX with hyperref package"}}}