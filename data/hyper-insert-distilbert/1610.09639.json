{"id": "1610.09639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Compact Deep Convolutional Neural Networks With Coarse Pruning", "abstract": "decreasing the learning capability of a neural computing network improves with no increasing depth at higher computational costs. assuming wider layers present with increasing dense functional kernel connectivity patterns furhter commonly increase this cost and may usually hinder better real - parameter time inference. we propose special feature map designs and kernel upper level pruning for reducing the perceived computational complexity size of a generally deep connected convolutional neural resource network. pruning data feature maps therefore reduces the width of a simpler layer pattern and hence does assume not need in any sparse representation. a further, weak kernel pruning converts the dense connectivity feature pattern into a very sparse one. generally due to local coarse algebraic nature, these cluster pruning granularities can mainly be thoroughly exploited immediately by gpus and vlsi mesh based implementations. frequently we propose studying a simple sensitivity and generic strategy program to choose the least significant adversarial pruning masks selected for determining both granularities. initially the poorer pruned fiber networks are retrained carriers which compensates the enormous loss in accuracy. we obtain thus the significantly best pruning weight ratios when we prune board a large network supplied with both granularities. experiments with developing the dynamic cifar - mn 10 dataset controller show that more no than 85 % discrete sparsity defect can be induced in practicing the convolution where layers connected with less than 1 % increase result in keeping the missclassification rate of the smallest baseline distributed network.", "histories": [["v1", "Sun, 30 Oct 2016 11:57:20 GMT  (132kb,D)", "http://arxiv.org/abs/1610.09639v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sajid anwar", "wonyong sung"], "accepted": false, "id": "1610.09639"}, "pdf": {"name": "1610.09639.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["COARSE PRUNING", "Sajid Anwar", "Wonyong Sung"], "emails": ["sajid@dsp.snu.ac.kr,", "wysung@snu.ac.kr"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep and wider neural networks have the capacity to learn a complex unknown function from the training data. The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset. The high computationally complexity of wide and deep networks is a major obstacle in porting the benefits of deep learning to resource limited devices. Therefore, many researchers have proposed ideas to accelerate deep networks for real-time inference Yu et al. (2012); Han et al. (2015b;a); Mathieu et al. (2013); Anwar et al. (2015b).\nNetwork pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance. The works of Han et al. (2015b;a) introduce fine grained sparsity in a network by pruning scalar weights. Due to unstructured sparsity, the authors employ compressed sparse row/column (CSR/CSC) for sparse representation. Thus the fine grained irregular sparsity cannot be easily translated into computational speedups.\nSparsity in a deep convolutional neural network (CNN) can be induced at various levels. Figure 1 shows four pruning granularities. At the coarsest level, a full hidden layer can be pruned. This is shown with a red colored rectangle in Fig. 1(a). Layer wise pruning affects the depth of the network and a deep network can be converted into a shallow network. Increasing the depth improves the network performance and layer-wise pruning therefore demand intelligent techniques to mitigate the performance degradation. The next pruning granularity is removing feature maps Polyak & Wolf (2015); Anwar et al. (2015b). Feature map pruning removes a large number of kernels and is therefore very destructive in nature. We therefore may not achieve higher pruning ratios. For the depicted architecture in Fig. 1 (b)., pruning a single feature map, zeroes four kernels. Feature map pruning affects the layer width and we directly obtain a thinner network and no sparse representation\nar X\niv :1\n61 0.\n09 63\n9v 1\n[ cs\n.L G\n] 3\n0 O\nct 2\n01 6\nis needed. Kernel purning is the next pruning granularity and it prunes k \u00d7 k kernels. It is neither too fine nor too coarse and is shown in Fig. 1(c). Kernel pruning is therefore a balanced choice and it can change the dense kernel connectivity pattern to sparse one. Each convolution connection involves W \u00d7 H \u00d7 k \u00d7 k MAC operations where W, H and k represents the feature map width, height and the kernel size respectively. Further the sparse representation for kernel-Pruning is also very simple. A single flag is enough to represent one convolution connection.The conventional pruning techniques induce sparsity at the finest granularity by zeroing scalar weights. This sparsity can be induced in much higher rates but demands sparse representation for computational benefits Han et al. (2015b). Therefore the high pruning ratios do not directly translate into computational speedups. Figure 1(d) shows this with red colored zeroes in the kernel. Further Fig. 1 summarizes the relationship between three related factors: the pruning granularities, the pruning ratios and the sparse representations. Coarse pruning granularities demand very simple sparse representation but we cannot achieve very high pruning ratios. Similarly fine grained pruning granularities can achieve higher pruning ratios but the sparse representation is more complicated.\nThe reference work of Anwar et al. (2015b) analysed feature map pruning with intra-kernel strided sparsity. To reduce the size of feature map and kernel matrices, they further imposed a constraint that all the outgoing kernels from a feature map must have the same pruning mask. In this work, we do not impose any such constraint and the pruning granularities are coarser. We argue that this kind of sparsity is useful for VLSI and FFT based implementations. Moreover we show that the best pruning results are obtained when we combine feature map and kernel level pruning. The selection of feature map and kernel pruning candidates with a simple technique is another contribution of this work.\nThe rest of the paper is organized as follows. In Section 2, recent related works are revisited. Section 3 discusses the pruning candidate selection. Section 4 discusses the two pruning granularities. Section 5 presents the experimental results while Section 6 concludes the discussion and adds the future research dimensions of this work."}, {"heading": "2 RELATED WORK", "text": "In the literature, network pruning has been studied by several researches Han et al. (2015b;a); Yu et al. (2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993). Collins & Kohli (2014) have proposed a technique where irregular sparsity is used to reduce the computational complexity in convolutional and fully connected layers. However they have not discussed how the sparse representation will affect the computational benefits. The works of Han et al. (2015b;a) introduce fine-grained sparsity in a network by pruning scalar weights. If the absolute magnitude of any weight is less than a scalar threshold, the weight is pruned. This work therefore favours learning with small valued weights and train the network with the L1/L2 norm augmented loss function. Due to pruning at very fine scales, they achieve excellent pruning ratios.\nHowever this kind of pruning results in irregular connectivity patterns and demand complex sparse representation for computational benefits. Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation. The work of Lebedev & Lempitsky (2015) also induce intra-kernel sparsity in a convolutional layer. Their target is efficient computation by unrolling convolutions as matrx-matrix multiplication. Their sparse representation is not also simple because each kernel has an equally sized pruning mask. A recently published work propose sparsity at a higher granularity and induce channel level sparsity in a CNN network for deep face application Polyak & Wolf (2015). The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network. Fixed point optimization for deep neural networks is employed by Anwar et al. (2015a); Hwang & Sung (2014); Sung et al. for VLSI based implementations."}, {"heading": "3 PRUNING CANDIDATE SELECTION", "text": "The learning capability of a network is determined by its architecture and the number of effective learnable parameters. Pruning reduces this number and inevitably degrades the classification performance. The pruning candidate selection is therefore of prime importance. Further the pruned network is retrained to compensate for the pruning losses Yu et al. (2012). For a specific pruning ratio, we search for the best pruning masks which afflicts the least adversary on the pruned network. Indeed retraining can partially or fully recover the pruning losses, but the lesser the losses, the more pluasible is the recovery. Further small performace degradation also means that the successor student network has lost little or no knowledge of the predecessor teacher network. If there are M potential pruning candidates, the total number of pruning masks is (2M ) and an exhaustive search is therefore infeasible even for a small sized network. We therefore propose a simple strategy for selecting pruning candiates.\nWe evaluate N random combinations and compute the MCR for each one. We then choose the best pruning mask which causes the least degradation to the network performance on the validation set. Consider that for the depicted architecture in Fig.2b, we need to select feature map pruning candidates in Layer L2 and L3 with 1/3 pruning ratio. If N = 4, the following N ordered pairs of feature maps may be randomly selected for (L2, L3) : (1, 2), (2, 3), (3, 1), (1, 1). These combinations generate random paths in the network and we evaluate the validation set MCR through these routes in the network. However, this further raises the question of how to approximate N. We report the relationship between pruning ratio and N in Fig. 2a. This analaysis is conducted for kernel level\npruning but is also applicable to feature map level pruning. From Fig. 2a., we can observe that for higher pruning ratios, high value of N is beneficial as it results in better pruning candidate selection. For the pruning ratio of no more than 40%, N = 100 random evaluations generate good selections. For lower pruning ratios, retraining is also more likely to compensate the losses as the non-pruned parameters may still be in good numbers. The computational cost of this technique is not much as the evaluation is done on the small sized validation set. By observing Fig. 2a., we propose that the value of N can be estimated intially and later used in several pruning passes.\nWe further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig. 2b. The set of filters or kernels from the previous layer constitute a group. This is shown with the similar color in Fig. 2b. According to Li et al. (2016), the absolute sum of weights determine the importance of a feature map. Suppose that in Fig.2b, the Layer L2 undergoes feature map pruning. The weight sum criterion computes the absolute weight sum at S1, S2 and S3. If we further suppose that the pruning ratio is 1/3, then the min(S1, S2, S3) is pruned. All the incoming and outgoing kernels from the pruned feature map are also removed. We argue that the sign of a weight in kernel plays important role in well-known feature extractors and therefore this is not a good criterion.\nWe compare the performance of the two algoirthms and Fig. 3a shows the experimental results. These results present the network status before any retraining is conducted. We report the performance degradation in the network classifcation against the pruning ratio. From Fig. 3a, we can observe that our proposed method outperforms the weight sum method particularly for higher pruning ratios. This is attributed to evaluating pruning candidates in combinations. The criterion in Li et al. (2016) evaluates the importance of a pruning unit in isolation while our proposed approach evaluates several paths through the network and selects the best one. The combinations work togehter and matter more instead of individidual units. Further, our proposed technique is generic and can be used for any pruning granularity: feature map, kernel and intra-kernel pruning."}, {"heading": "4 FEATURE MAP AND KERNEL PRUNING", "text": "In this section we discuss feature map and kernel pruning granularities. For a similar sized network, we analyse the achievable pruning ratios with feature map and kernel pruning. In terms of granularity, feature map pruning is coarser than kernel pruning. Feature map pruning does not need any sparse representation and the pruned network can be implemented in a conventional way, convolution lowering Chellapilla et al. (2006) or convolution with FFTs Mathieu et al. (2013). The main\nfocus of the proposed work is analysing the unconstrained kernel pruning and feature map pruning. Pruning a feature map causes all the incoming and outgoing kernels to be zeroed because the outgoing kernels are no more meaningful.\nKernel pruning is comparatively finer. The dimension and connectivity pattern of 2D kernels determine the computing cost of a convolutional layer. The meshed fully connected convolution layers increases this cost and can hinder the real-time inference. The unconstrained kernel pruning converts this dense connectivity to sparse one. Kernel-pruning zeroes k\u00d7k kernels and is neither too fine nor too coarse. Kernel level pruning provides a balance between fine graind and coarse graind pruining. It is coarse than intra-kernel sparsity and finer than feature map pruning. Thats why good pruing ratios can be achieved at very small sparse representation and computational cost. Each convolution connection represents one convolution operation which involves Width \u00d7 Height \u00d7 k \u00d7 k MAC operations. In LeNet LeCun et al. (1998), the second convolution layer has 6 \u00d7 16 feature maps and the kernel connectivity has a fixed sparse pattern. With kernel pruning, we learn this pattern and achieve the best possible pruning ratios. We first select pruning candidates with the criterion outlined in Section 2. The pruned network is then retrained to compensate for the losses incurred due to pruning. Figure 3b shows the feature map and kernel level pruning applied to MNIST LeCun et al. (1998) network. When pruning ratios increase beyond 60%, feature map pruning degrades the performance much. However the kernel level pruning can achieve higher pruning ratios due to finer scale granularity.\nAs the sparse granularities are coarse, a generic set of computing platform can benefit from it. One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling. Mathieu et. al., have proposed FFT based convolutions for faster CNN training and evaluation Mathieu et al. (2013). The GPU based parallel implementation showed very good speedups. As commonly known that the IFFT (FFT (kerenel) \u00d7 FFT (fmap)) = kernel \u2217 fmap, the kernel level pruning can relieve this task. Although the kernel size is small, massive reusability enables the use of FFT. The FFT of each kernel is computed only once and reused for multiple input vectors in a mini-batch. In a feed forward and backward path, the summations can be carried in the FFT domain and once the sum is available, the IFFT can be performed Mathieu et al. (2013). Similarly, a customized VLSI based implementation can also benefit from the kernel level pruning. If the VLSI implementation imposes a constraint on the pruning criterion, such as the fixed number of convolution kernels from the previous to the next layer, the pruning criterion can be adapted accordingly. Figure 3b shows that the kernel pruning can be induced in much higher rates with minor increase in the MCR of the baseline MNIST network. In the next Section, we report and discuss the experimental results in detail."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In this section, we present detailed experimental results with the CIFAR-10 and SVHN datasets Krizhevsky & Hinton (2009). During training and pruning, we use the stochastic gradient descent (SGD) with a mini-batch size of 128 and RMSProp Tieleman & Hinton (2012). We train all the networks with batch normalization Ioffe & Szegedy (2015). We do not prune the network in small steps, and instead one-shot prune the network for a given pruning ratio followed by retraining. The experimental results are reported in the corresponding two subsections."}, {"heading": "5.1 CIFAR-10", "text": "The CIFAR-10 dataset includes samples from ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. The training set consists of 50,000 RGB samples and we allocate 20% of these samples as validation set. Test set contains 10,000 samples and each sample has 32 \u00d7 32 \u00d7 RGB resolution. We evaluate the proposed pruning granularities with two networks. CNNsmall and CNNlarge. CNNsmall has six convolution and two overlapped max pooling layers. We report the network architecture with an alphanumeric string as reported in Courbariaux et al. (2015) and outlined in Table 1. The (2\u00d7 128C3) represents two convolution layers with each having 128 feature maps and 3 \u00d7 3 convolution kernels. MP2 represents 3 \u00d7 3 overlapped maxpooling layer with a stride size of 2. We pre-process the original CIFAR-10 dataset with global contrast normalization followed by zero component analysis (ZCA) whitening.\nThe CNNlarge has seven convolution and two max-pooling layers. Furthe, online data augmentations are employed to improve the classification accurracy. We randomly crop 28\u00d7 28\u00d7 3 patches from the 32 \u00d7 32 \u00d7 3 input vectors. These cropped vectors are then geometrically transformed randomly. A vector may be flipped horizontally or vertically, rotated, translated and scaled. At evaluation time, we crop patches from the four corners and the center of a 32\u00d7 32\u00d7 3 patch and flip it horizontally. We average the evaluation on these ten 28 \u00d7 28 \u00d7 3 patches to decide the final label. Due to larger width and depth, the CNNlarge achieves more than 90% accurracy on the CIFAR-10 dataset. The CNNsmall is smaller than CNNlarge and trained without any data augmentation. The CNNsmall therefore achieves 84% accurracy."}, {"heading": "5.1.1 FEATURE MAP AND KERNEL LEVEL PRUNING", "text": "After layer pruning, feature map pruning is the 2nd coarsest pruning granularity. Feature map pruning reduces the width of a convolutional layer and generates a thinner network. Pruning a single feature map, zeroes all the incoming and outgoing weights and therefore, higher pruning ratios degrade the network classification performance significantly. Feature map pruning for the CNNcifar is shown in Fig. 4a with a circle marked red colored line. The sparsity reported here is for Conv2 to Conv6. We do not pruned the first convolution layer as it has only 3 \u00d7 128 \u00d7 (3 \u00d7 3) = 3456 weights. The horizontal solid line shows the baseline MCR of 16.26% whereas the dashed line shows the 1% tolerance bound. Training the network with batch normalization Ioffe & Szegedy (2015) enables us to directly prune a network for a target ratio, instead of taking small sized steps. With a baseline performance of 16.26%, the network performance is very bad at 80% feature map pruning. We can observe that 62% pruning ratio is possible with less than 1% increase in MCR. The CNNcifar is reduced to (128C3\u2212 83C3)-MP3-(83C3\u2212 83C3)-MP3-(166C3\u2212 166C3)256FC-10Softmax. As pruning is only applied in Conv2 to Conv6, therefore the Figure 4a pruning ratios are computed only for these layers.\nFor the same network, we can see that kernel level pruning performs better. We can achieve 70% sparsity with kernel level pruning. This is attributed to the fact that kernel pruning is finer and hence it achieves higher ratios. Further kernel pruning may ultimately prune a feature map if all the incoming kernels are pruned. However at inference time, we need to define the kernel connectivity pattern which can simply be done with a binary flag. So although the sparse representation is needed, it is quite simple and straightforward. Experimental results confirm that fine grained sparsity can be induced in higher rates. We achieved 70% kernel wise sparsity for Conv2 - Conv6 and the best pruned network is layer wise reported in Table ??. The speedup and acceleration with these pruning granularities is platform independent."}, {"heading": "5.1.2 COMBINATIONS OF KERNEL AND FEATURE MAP PRUNING", "text": "In this section we discuss the various pruning granularities applied in different combinations. We first apply the feature map and kernel pruning to the CNNsmall network in different orders. With feature map pruning, we can achieve 60% sparsity under the budget of 1% increase in MCR. But at this pruning stage, the network learning capability is affected much. So we take a 50% feature map pruned network, where the CNNsmall is reduced to (128C3\u2212 89C3)-MP3-(89C3\u2212 89C3)-MP3(179C3\u2212 179C3)-256FC-10Softmax. As pruning is only applied to Conv2\u2212Conv6, therefore in Fig. 4., pruning ratios are computed only for these layers. This network then undergoes kernel level pruning. The blue rectange line in Figure 4 shows the pruning results. We achieve the best pruning results in this case and the final pruned network is reported in detail in Table 3. Overall we achieve more than 75% pruning ratio in the final pruned network.\nWe further conducted experiments on the CNNlarge and the corresponding plots are shown in Fig. 5. The CNNlarge is much wider and deeper than the CNNsmall as reported in Table 1. Therefore there are more chances of redundancy and hence more room for pruning. Further we observe similar trends as CNNsmall where the kernel pruning can be induced in higher ratios compared to the feature map pruning. When the kernel pruning is applied to the feature map pruned network, we can achieve more than 88% sparsity in the Conv2 \u2212 Conv7 of the CNNlarge network. This way we show that our proposed technique has good scalability. These results are in conformity to the resiliency analysis of fixed point deep neural networks Sung et al.."}, {"heading": "5.2 SVHN", "text": "The SVHN dataset consists of 32 \u00d7 32 \u00d7 3 cropped images of house numbers [Netzer et al. 2011] and bears similarity with the MNIST handwritten digit recognition dataset [LeCun et al. 1998]. The classification is challenging as more than one digit may appear in sample and the goal is to identify a digit in the center of a patch. The dataset consists of 73,257 digits for training, 26,032 for testing and 53,1131 extra for training. The extra set consists of easy samples and may augment the training set. We generate a validation set of 6000 samples which consists of 4000 samples from the training set and 2000 samples from the extra [Sermanet et al. 2012]. The network architecture is reported like this: (2 \u00d7 64C3)-MP2- (2 \u00d7 128C3)-MP2-(2 \u00d7 128C3)-512FC-512FC-10Softmax. This network is trained with batch normalization and we achieve the baseline MCR of 3.5% on the test set. The corresponding pruning plots are reported in Fig. 6. We can observe a similar trend where kernels can be pruned by a bigger ratio compred to feature maps. More than 70% pruning ratio can be implemented in the reported network. Thus we show that the lessons learnt generalize well on various datasets."}, {"heading": "6 CONCLUDING REMARKS", "text": "In this work we proposed feature map and kernel pruning for reducing the computational complexity of deep CNN. We have discussed that the cost of sparse representation can be avoided with coarse\npruning granularities. We demonstrated a simple and generic algorithm for selecting the best pruning mask. We conducted experiments with several benchmarks and networks and showed that the proposed technique has good scalability. We are exploring online pruning in future for exploiting run-time benefits."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIP) (No. 2015R1A2A1A10056051)."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "arXiv preprint arXiv:1512.08571,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "An iterative pruning algorithm for feedforward neural networks", "author": ["Giovanna Castellano", "Anna Maria Fanelli", "Marcello Pelillo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Castellano et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Castellano et al\\.", "year": 1997}, {"title": "High performance convolutional neural networks for document processing", "author": ["Kumar Chellapilla", "Sidd Puri", "Patrice Simard"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D Collins", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1412.1442,", "citeRegEx": "Collins and Kohli.,? \\Q2014\\E", "shortCiteRegEx": "Collins and Kohli.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "A deep neural network compression pipeline: Pruning, quantization, huffman encoding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Fast convnets using group-wise brain damage", "author": ["Vadim Lebedev", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1506.02515,", "citeRegEx": "Lebedev and Lempitsky.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev and Lempitsky.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Pruning filters for efficient convnets", "author": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "venue": "arXiv preprint arXiv:1608.08710,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fast training of convolutional networks through ffts", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Channel-level acceleration of deep face representations", "author": ["Adam Polyak", "Lior Wolf"], "venue": "Access, IEEE,", "citeRegEx": "Polyak and Wolf.,? \\Q2015\\E", "shortCiteRegEx": "Polyak and Wolf.", "year": 2015}, {"title": "Pruning algorithms-a survey", "author": ["Russell Reed"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Reed.,? \\Q1993\\E", "shortCiteRegEx": "Reed.", "year": 1993}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Pruning backpropagation neural networks using modern stochastic optimisation techniques", "author": ["Slawomir W Stepniewski", "Andy J Keane"], "venue": "Neural Computing & Applications,", "citeRegEx": "Stepniewski and Keane.,? \\Q1997\\E", "shortCiteRegEx": "Stepniewski and Keane.", "year": 1997}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.", "startOffset": 24, "endOffset": 43}, {"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset.", "startOffset": 24, "endOffset": 158}, {"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset. The high computationally complexity of wide and deep networks is a major obstacle in porting the benefits of deep learning to resource limited devices. Therefore, many researchers have proposed ideas to accelerate deep networks for real-time inference Yu et al. (2012); Han et al.", "startOffset": 24, "endOffset": 524}, {"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset. The high computationally complexity of wide and deep networks is a major obstacle in porting the benefits of deep learning to resource limited devices. Therefore, many researchers have proposed ideas to accelerate deep networks for real-time inference Yu et al. (2012); Han et al. (2015b;a); Mathieu et al. (2013); Anwar et al.", "startOffset": 24, "endOffset": 569}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al.", "startOffset": 8, "endOffset": 205}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al.", "startOffset": 8, "endOffset": 225}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance.", "startOffset": 8, "endOffset": 247}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance. The works of Han et al. (2015b;a) introduce fine grained sparsity in a network by pruning scalar weights. Due to unstructured sparsity, the authors employ compressed sparse row/column (CSR/CSC) for sparse representation. Thus the fine grained irregular sparsity cannot be easily translated into computational speedups. Sparsity in a deep convolutional neural network (CNN) can be induced at various levels. Figure 1 shows four pruning granularities. At the coarsest level, a full hidden layer can be pruned. This is shown with a red colored rectangle in Fig. 1(a). Layer wise pruning affects the depth of the network and a deep network can be converted into a shallow network. Increasing the depth improves the network performance and layer-wise pruning therefore demand intelligent techniques to mitigate the performance degradation. The next pruning granularity is removing feature maps Polyak & Wolf (2015); Anwar et al.", "startOffset": 8, "endOffset": 1296}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance. The works of Han et al. (2015b;a) introduce fine grained sparsity in a network by pruning scalar weights. Due to unstructured sparsity, the authors employ compressed sparse row/column (CSR/CSC) for sparse representation. Thus the fine grained irregular sparsity cannot be easily translated into computational speedups. Sparsity in a deep convolutional neural network (CNN) can be induced at various levels. Figure 1 shows four pruning granularities. At the coarsest level, a full hidden layer can be pruned. This is shown with a red colored rectangle in Fig. 1(a). Layer wise pruning affects the depth of the network and a deep network can be converted into a shallow network. Increasing the depth improves the network performance and layer-wise pruning therefore demand intelligent techniques to mitigate the performance degradation. The next pruning granularity is removing feature maps Polyak & Wolf (2015); Anwar et al. (2015b). Feature map pruning removes a large number of kernels and is therefore very destructive in nature.", "startOffset": 8, "endOffset": 1318}, {"referenceID": 5, "context": "This sparsity can be induced in much higher rates but demands sparse representation for computational benefits Han et al. (2015b). Therefore the high pruning ratios do not directly translate into computational speedups.", "startOffset": 111, "endOffset": 130}, {"referenceID": 0, "context": "The reference work of Anwar et al. (2015b) analysed feature map pruning with intra-kernel strided sparsity.", "startOffset": 22, "endOffset": 43}, {"referenceID": 6, "context": "In the literature, network pruning has been studied by several researches Han et al. (2015b;a); Yu et al. (2012); Castellano et al.", "startOffset": 74, "endOffset": 113}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993).", "startOffset": 8, "endOffset": 33}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993).", "startOffset": 8, "endOffset": 57}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993).", "startOffset": 8, "endOffset": 85}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993). Collins & Kohli (2014) have proposed a technique where irregular sparsity is used to reduce the computational complexity in convolutional and fully connected layers.", "startOffset": 8, "endOffset": 98}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993). Collins & Kohli (2014) have proposed a technique where irregular sparsity is used to reduce the computational complexity in convolutional and fully connected layers.", "startOffset": 8, "endOffset": 122}, {"referenceID": 14, "context": "(b) This Figure explains the idea presented in Li et al. (2016) and shows three layers, L1, L2 and L3.", "startOffset": 47, "endOffset": 64}, {"referenceID": 0, "context": "Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation.", "startOffset": 61, "endOffset": 87}, {"referenceID": 0, "context": "Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation. The work of Lebedev & Lempitsky (2015) also induce intra-kernel sparsity in a convolutional layer.", "startOffset": 61, "endOffset": 156}, {"referenceID": 0, "context": "Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation. The work of Lebedev & Lempitsky (2015) also induce intra-kernel sparsity in a convolutional layer. Their target is efficient computation by unrolling convolutions as matrx-matrix multiplication. Their sparse representation is not also simple because each kernel has an equally sized pruning mask. A recently published work propose sparsity at a higher granularity and induce channel level sparsity in a CNN network for deep face application Polyak & Wolf (2015). The work of Castellano et al.", "startOffset": 61, "endOffset": 579}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 37}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 61}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 89}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 102}, {"referenceID": 0, "context": "Fixed point optimization for deep neural networks is employed by Anwar et al. (2015a); Hwang & Sung (2014); Sung et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 0, "context": "Fixed point optimization for deep neural networks is employed by Anwar et al. (2015a); Hwang & Sung (2014); Sung et al.", "startOffset": 65, "endOffset": 107}, {"referenceID": 21, "context": "Further the pruned network is retrained to compensate for the pruning losses Yu et al. (2012). For a specific pruning ratio, we search for the best pruning masks which afflicts the least adversary on the pruned network.", "startOffset": 77, "endOffset": 94}, {"referenceID": 14, "context": "(a) Best of N random masks vs Li et al. (2016) 0 0.", "startOffset": 30, "endOffset": 47}, {"referenceID": 14, "context": "We further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig.", "startOffset": 85, "endOffset": 102}, {"referenceID": 14, "context": "We further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig. 2b. The set of filters or kernels from the previous layer constitute a group. This is shown with the similar color in Fig. 2b. According to Li et al. (2016), the absolute sum of weights determine the importance of a feature map.", "startOffset": 85, "endOffset": 277}, {"referenceID": 14, "context": "We further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig. 2b. The set of filters or kernels from the previous layer constitute a group. This is shown with the similar color in Fig. 2b. According to Li et al. (2016), the absolute sum of weights determine the importance of a feature map. Suppose that in Fig.2b, the Layer L2 undergoes feature map pruning. The weight sum criterion computes the absolute weight sum at S1, S2 and S3. If we further suppose that the pruning ratio is 1/3, then the min(S1, S2, S3) is pruned. All the incoming and outgoing kernels from the pruned feature map are also removed. We argue that the sign of a weight in kernel plays important role in well-known feature extractors and therefore this is not a good criterion. We compare the performance of the two algoirthms and Fig. 3a shows the experimental results. These results present the network status before any retraining is conducted. We report the performance degradation in the network classifcation against the pruning ratio. From Fig. 3a, we can observe that our proposed method outperforms the weight sum method particularly for higher pruning ratios. This is attributed to evaluating pruning candidates in combinations. The criterion in Li et al. (2016) evaluates the importance of a pruning unit in isolation while our proposed approach evaluates several paths through the network and selects the best one.", "startOffset": 85, "endOffset": 1304}, {"referenceID": 3, "context": "Feature map pruning does not need any sparse representation and the pruned network can be implemented in a conventional way, convolution lowering Chellapilla et al. (2006) or convolution with FFTs Mathieu et al.", "startOffset": 146, "endOffset": 172}, {"referenceID": 3, "context": "Feature map pruning does not need any sparse representation and the pruned network can be implemented in a conventional way, convolution lowering Chellapilla et al. (2006) or convolution with FFTs Mathieu et al. (2013). The main", "startOffset": 146, "endOffset": 219}, {"referenceID": 12, "context": "In LeNet LeCun et al. (1998), the second convolution layer has 6 \u00d7 16 feature maps and the kernel connectivity has a fixed sparse pattern.", "startOffset": 9, "endOffset": 29}, {"referenceID": 12, "context": "In LeNet LeCun et al. (1998), the second convolution layer has 6 \u00d7 16 feature maps and the kernel connectivity has a fixed sparse pattern. With kernel pruning, we learn this pattern and achieve the best possible pruning ratios. We first select pruning candidates with the criterion outlined in Section 2. The pruned network is then retrained to compensate for the losses incurred due to pruning. Figure 3b shows the feature map and kernel level pruning applied to MNIST LeCun et al. (1998) network.", "startOffset": 9, "endOffset": 490}, {"referenceID": 3, "context": "One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling.", "startOffset": 123, "endOffset": 149}, {"referenceID": 3, "context": "One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling. Mathieu et. al., have proposed FFT based convolutions for faster CNN training and evaluation Mathieu et al. (2013). The GPU based parallel implementation showed very good speedups.", "startOffset": 123, "endOffset": 370}, {"referenceID": 3, "context": "One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling. Mathieu et. al., have proposed FFT based convolutions for faster CNN training and evaluation Mathieu et al. (2013). The GPU based parallel implementation showed very good speedups. As commonly known that the IFFT (FFT (kerenel) \u00d7 FFT (fmap)) = kernel \u2217 fmap, the kernel level pruning can relieve this task. Although the kernel size is small, massive reusability enables the use of FFT. The FFT of each kernel is computed only once and reused for multiple input vectors in a mini-batch. In a feed forward and backward path, the summations can be carried in the FFT domain and once the sum is available, the IFFT can be performed Mathieu et al. (2013). Similarly, a customized VLSI based implementation can also benefit from the kernel level pruning.", "startOffset": 123, "endOffset": 905}, {"referenceID": 5, "context": "We report the network architecture with an alphanumeric string as reported in Courbariaux et al. (2015) and outlined in Table 1.", "startOffset": 78, "endOffset": 104}, {"referenceID": 13, "context": "2011] and bears similarity with the MNIST handwritten digit recognition dataset [LeCun et al. 1998].", "startOffset": 80, "endOffset": 99}], "year": 2016, "abstractText": "The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns furhter increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple and generic strategy to choose the least adversarial pruning masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a network with both granularities. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the missclassification rate of the baseline network.", "creator": "LaTeX with hyperref package"}}}