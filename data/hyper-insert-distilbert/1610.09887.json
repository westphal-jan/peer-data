{"id": "1610.09887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks", "abstract": "likewise we instead provide a depth - spectral based separation result for feed - bounded forward domain relu evolutionary neural networks, independently showing that integrating a equally wide family topology of purely non - existent linear, twice - infinite differentiable separation functions on $ [ equation 0, \u00b7 1 ] ^ d $, which n can just be temporarily approximated to accuracy $ \\ global epsilon $. by integrating relu social networks means of increased depth scaling and width $ \\ mathcal { c o } ( \\ q text { ^ poly } ( \\ log ( 1 / \\ epsilon ) ) ) $, m cannot be approximated to similar accuracy by allowing constant - depth relu networks, consequently unless indeed their width = is conserved at no least $ \\ omega ( 1 / \\ epsilon ) $.", "histories": [["v1", "Mon, 31 Oct 2016 12:08:46 GMT  (16kb)", "http://arxiv.org/abs/1610.09887v1", null], ["v2", "Thu, 9 Mar 2017 18:07:37 GMT  (75kb,D)", "http://arxiv.org/abs/1610.09887v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["itay safran", "ohad shamir"], "accepted": true, "id": "1610.09887"}, "pdf": {"name": "1610.09887.pdf", "metadata": {"source": "CRF", "title": "Depth Separation in ReLU Networks for Approximating Smooth Non-Linear Functions", "authors": ["Itay Safran"], "emails": ["itay.safran@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n09 88\n7v 1\n[ cs\n.L G\n] 3\n1 O\nct 2"}, {"heading": "1 Introduction", "text": "Deep learning, in the form of artificial neural networks have seen a dramatic resurgence in the past recent years, achieving great improvement over various fields of artificial intelligence such as computer vision and speech recognition. While empirically successful, our theoretical understanding of them is still limited at best.\nIn this work, we focus on the aspect of the expressive power of neural networks. Specifically, We investigate how the accuracy to which they are able to approximate various target functions increases with their size, and in particular, what is the trade-off between increasing their width vs. increasing their depth. It is well-known that networks of depth 2 can already approximate any continuous target function on the boolean hypercube [0, 1]d to arbitrary accuracy, albeit at the cost of width exponential in the dimension d [Cybenko, 1989]. In contrast, it has long been evident that deeper networks tend to perform better than shallow ones, a phenomenon supported by the intuition that depth, providing compositional expressibility, is necessary for efficiently representing some functions. Indeed, recent empirical evidence suggests that even at large depths, deeper networks can offer benefits over shallower networks [He et al., 2015].\nIn this paper, we consider feed-forward neural networks with the commonly-used rectified linear unit (ReLU, [z]+ = max {0, z}) as the activation function. We focus mainly on the following question: Given some function f : [0, 1]d \u2192 R and accuracy parameter \u01eb, what lower and upper bounds can be derived on the required width and depth of a ReLU neural network, in order to approximate f up to an accuracy of \u01eb? Here, we define accuracy in terms of expected squared loss, or sometimes maximal loss, over some domain (see Sec. 2 for more details).\nIn the first part of our work (Sec. 3), we discuss lower bounds on the size of a network required for achieving approximation error of \u01eb for functions which are C2 (that is, twice-continuously differentiable) on the bounded domain [0, 1]d. The approximation quality is measured in terms of the expected squared loss w.r.t. the uniform distribution on the domain. Essentially, we show that any f : [0, 1]d \u2192 R in C2, which is \u201cnon-linear\u201d in a portion of its domain (in terms of having non-zero curvature, at least in some direction),\ncannot be approximated to accuracy \u01eb (for arbitrary \u01eb > 0) using ReLU networks of constant depth, unless the width is at least of order 1/\u01eb.\nIn the second part of our work (Sec. 4), we show that a wide class of functions on the domain [0, 1]d (including some of those for which our lower bounds apply) can be approximated to accuracy \u01eb using a network of width and depth only O ( poly ( log (\n1 \u01eb\n)))\n. Specifically, this is shown for functions which (i) Can be approximated well using a series of at most O ( poly ( log (\n1 \u01eb\n)))\noperations involving additions and multiplications; and (ii) Each intermediate stage in the series of computations has its value bounded by a quantity not larger than O ( poly (\n1 \u01eb\n))\n. In Sec. 5, we combine the upper and lower bounds, to derive an exponential separation result (in the accuracy parameter \u01eb) for the wide class of functions lying in the intersection of those described earlier: That is, for any non-linear C2 function, which can be efficiently approximated using additions and multiplications.\nRelated Work\nThe question of studying the effect of depth in neural networks has received considerable attention recently. Here, we briefly review some of the most relevant work, providing formal depth separation results.\nIn Eldan and Shamir [2016], the authors show that depth in neural networks can be exponentially valuable even if increased only by 1, by proving the existence of a family of radial functions which are expressible using a depth 3 feed-forward neural network of size polynomial in the dimension d, whereas a network of depth 2 cannot approximate these functions to more than constant accuracy (even if the activation function is arbitrary), unless its width is exponential in d. However, it is not clear whether their proof techniques extend to deeper networks. In contrast, our results apply to potentially deeper networks, and the separation is in terms of the required accuracy \u01eb, rather than some fixed depths.\nThe work perhaps most similar to ours is Telgarsky [2016]. In that work, the author proves a separation\nresult between networks of depth k and depth o (\nk log(k)\n)\n(for arbitrary k), by showing that in one dimension,\na depth k network can realize a saw-tooth function with exp(O(k)) oscillations, whereas any network of depth o (\nk log(k)\n)\nwould require a width super-polynomial in k to approximate it by more than a constant.\nLike our work, it potentially applies to networks of arbitrary depth, and the trade-off between depth and width is roughly the same (doubling the depth is equivalent to squaring the width). However, the papers differ in two main aspects: (i) The notion of separation is in a parameter k rather than the accuracy to which the target function is approximated to; (ii) Unlike the tailored construction of a highly oscillatory function with Lipschitz constant exponential in k, our result applies to a wide variety of more naturally encountered functions, including functions with a bounded Lipschitz constant.\nFinally, in Cohen et al. [2016], the authors establish a depth separation result for networks with a certain tensor structure. The authors achieve a very strong separation; not only does it show the existence of target functions expressible by deep networks which are hard to approximate using shallow networks, it in fact shows that the class of functions expressible by shallower networks has measure zero among the class of functions expressible using deep networks. Albeit very powerful, this separation result does not apply for the kind of ReLU neural networks discussed in this paper."}, {"heading": "2 Preliminaries", "text": "To formalize our setup, we recall that a fully-connected feed-forward artificial neural network computes a function Rd \u2192 R, and is composed of neurons connected according to a directed acyclic graph. Specifically,\nthe neurons can be decomposed into layers, where the output of each neuron is connected to all neurons in the succeeding layer and them alone. We focus on ReLU networks, where each neuron computes a function of the form x 7\u2192 [ w \u22a4 x+ b ]\n+ where w is a weight vector, b is a bias term specific to that neuron and\n[z]+ = max {0, z} is the ReLU activation function. For a vector b = (b1, . . . , bn) and a matrix W = (w1,w2, . . . ,wn)\u22a4, we let [Wx+ b]+ be a shorthand for ( [\nw \u22a4 1 x+ b1\n]\n+ , . . . ,\n[\nw \u22a4 n x+ bn\n]\n+\n)\n. We define a layer of n neurons as\nx 7\u2192 [Wx+ b]+ .\nFinally, by denoting the output of the ith layer as Oi, we can define a network of arbitrary depth recursively by\nOi+1 = [Wi+1Oi + bi+1]+ ,\nwhere Wi,bi represent the matrix of weights and bias of the ith layer, respectively. Following a standard convention for multi-layer networks, the final layer h is a purely linear function with no bias, i.e.\nOh = Wh \u00b7Oh\u22121.\nDefine the depth of the network as the number of layers l, and denote the number of neurons ni in the ith layer as the size of the layer. We define the width of a network as maxi\u2208{1,...,l} ni.\nGiven some function f on [0, 1]d and approximation f\u0303 , we generally define the quality of approximation as\n\u222b\n[0,1]d (f \u2212 f\u0303)2d\u00b5d,\nwhere \u00b5d is the uniform measure on [0, 1] d. This corresponds to the expected squared loss w.r.t. the uniform distribution. We refer to this as approximation in the L2-norm sense. In some of the results, we also consider approximation with respect to the infinity norm, defined as\nsup x\u2208[0,1]d |f(x)\u2212 f\u0303(x)|.\nClearly, this upper-bounds the (square root of the) L2 approximation error defined above."}, {"heading": "3 Lower Bounds on the Accuracy of Approximating C2 Functions", "text": "In this section, we establish a lower bound for approximating C2 functions using ReLU neural networks. We first note that such a bound can be trivial if we consider cases where the function being approximated is piece-wise linear and continuous, as in this case, the function might be possible to approximate to accuracy 0 using a large enough network. To ensure the lower bound we derive considers these trivial cases, we need to introduce a measure of how \u2018non-linear\u2019 our target function is. This non-linearity of the target function is captured by the following quantity:\nDefinition 1. Let \u00b5d denote the uniform measure on [0, 1] d. For a function f : [0, 1]d \u2192 R and some \u03bb > 0, denote \u03c3\u03bb (f) = sup\nU\u2208U s.t. \u2016H(f)(x)\u2016sp\u2265\u03bb \u2200x\u2208U \u00b5d (U) ,\nwhere U is the set of all connected and measurable subsets of [0, 1]d, and \u2016H(f)(x)\u2016sp is the spectral norm of the Hessian of f at x.\nIn words, \u03c3\u03bb (f) is the measure (w.r.t. the uniform distribution on [0, 1] d) of the largest connected set in the domain of f , where at any point, f has curvature at least \u03bb along some direction. Clearly, if f is piece-wise linear then \u03bb = 0 for all \u03bb > 0. We note that whenever clear from context, we omit f from the notation and use \u03c3\u03bb instead.\nWe can now state the main result of this section:\nTheorem 1. Let Gn be the family of piece-wise linear functions on the domain [0, 1] comprised of at most n linear segments. Let Gdn be the family of piece-wise linear functions on the domain [0, 1]\nd, with the property that for any g \u2208 Gdn and any affine transformation h : R \u2192 R d, g \u25e6 h \u2208 Gn. Suppose f : [0, 1] d \u2192 R is C2. Then for all \u03bb > 0\ninf g\u2208Gdn\n\u222b\n[0,1]d (f \u2212 g)2d\u00b5d \u2265\nc \u00b7 \u03bb2 \u00b7 \u03c3\u03bb(f) 5\nn4 ,\nwhere c > 0 is a universal constant.\nThm. 1 establishes that the error of a piece-wise linear approximation of a C2 function cannot decay faster than quartically in the number of linear segments of any one-dimensional projection of the approximating function. Note that this result is stronger than a bound in terms of the total number of linear regions in Rd, since that number can be exponentially higher (in the dimension) than n as defined in the theorem.\nTo translate this result to the context of ReLU neural networks, we use the result in Telgarsky [2016, Lemma 3.2], of which the following is an immediate corollary.\nCorollary 1. Let N dm,l denote the family of ReLU neural networks receiving input of dimension d and having depth l and maximal width m. Then\nN dm,l \u2286 G d (2m)l .\nThe following corollary bears great resemblance to the lower bound provided in Telgarsky [2016]. Albeit lower bounding the accuracy to which a ReLU network of a given size can approximate a certain function, rather than indicating what is a lower bound on the minimal size required for achieving non-constant approximation error. By combining Thm. 1 and Corollary 1, we have that\nCorollary 2. Suppose f : [0, 1]d \u2192 R is C2. Then for all \u03bb > 0,\ninf g\u2208N d\nm,l\n\u222b\n[0,1]d (f \u2212 g)2d\u00b5d \u2265 c \u00b7 \u03bb2 \u00b7 \u03c35\u03bb 2lml .\nCorollary 2 conveys the key tradeoff between depth and width when approximating C2 functions using ReLU networks: The error cannot decay faster than polynomially in the width, yet the bound deteriorates exponentially in the depth. As we show in the following section, this deterioration does not stem from the bound in Corollary 2 being loose: For well-behaved f , it is indeed possible to construct ReLU networks, where the approximation error decays exponentially with depth.\nIn the rest of the section, we prove Thm. 1 by a series of intermediate results, some of which may be of independent interest. In a nutshell, we show that strictly curved functions cannot be well-approximated by piecewise linear functions, unless the number of regions is large. To that end, we first establish some necessary tools based on Legendre polynomials. We then prove a result specific to the one-dimensional case, including an explicit lower bound if the target function is quadratic (Thm. 3) or strongly convex or concave (Thm. 4). Finally, we expand the construction to get a result in general dimension d."}, {"heading": "3.1 Some Technical Tools", "text": "Definition 2. Let Pi denote the ith Legendre Polynomial given by Rodrigues\u2019 formula:\nPi (x) = 1\n2ii!\ndi\ndxi\n[\n( x2 \u2212 1 )i ] .\nThese polynomials are useful for the following analysis since they obey the orthogonality relationship\n\u222b 1\n\u22121 Pi (x)Pj (x) dx =\n2\n2i+ 1 \u03b4ij .\nSince we are interested in approximations on small intervals where the approximating function is linear, we use the change of variables x = 2\u2113 t \u2212 2 \u2113a \u2212 1 to obtain an orthogonal family { P\u0303i }\u221e\ni=1 of shifted Legendre\npolynomials on the interval [a, a+ \u2113] with respect to the L2 norm. The first few polynomials of this family are given by\nP\u03030 (x) = 1\nP\u03031 (x) = 2\n\u2113 x\u2212\n(\n2 \u2113 a+ 1\n)\nP\u03032 (x) = 6\n\u21132 x2 \u2212\n(\n12a\n\u21132 +\n6\n\u2113\n)\nx+\n(\n6a2\n\u21132 +\n6a\n\u2113 + 1\n)\n. (1)\nThe shifted Legendre polynomial obey the orthogonality relationship\n\u222b a+\u2113\na P\u0303i (x) P\u0303j (x) dx =\n\u2113\n2i+ 1 \u03b4ij . (2)\nDefinition 3. We define the Fourier-Legendre series of a function f : [a, a+ \u2113] \u2192 R to be\nf (x) =\n\u221e \u2211\ni=0\na\u0303iP\u0303i (x) ,\nwhere the Fourier-Legendre Coefficients a\u0303i are given by\na\u0303i = 2i+ 1\n\u2113\n\u222b a+\u2113\na P\u0303i (x) f (x) dx.\nTheorem 2. A generalization of Parseval\u2019s identity yields\n\u2016f\u20162L2 = \u2113 \u221e \u2211\ni=0\na\u03032i 2i+ 1 . (3)\nDefinition 4. A function f is \u03bb-strongly convex if for all w,u and \u03b1 \u2208 (0, 1),\nf(\u03b1w + (1\u2212 \u03b1)u) \u2264 \u03b1f(w) + (1\u2212 \u03b1)f(u) \u2212 \u03bb\n2 \u03b1(1 \u2212 \u03b1) \u2016w \u2212 u\u201622 .\nA function is \u03bb-strongly concave, if \u2212f is \u03bb-strongly convex."}, {"heading": "3.2 One-dimensional Lower Bounds", "text": "We begin by proving two useful lemmas; the first will allow us to compute the error of a linear approximation of one-dimensional functions on arbitrary intervals, and the second will allow us to infer bounds on the entire domain of approximation, from the lower bounds we have on small intervals where the approximating function is linear.\nLemma 1. Let f \u2208 C2. Then the error of the optimal linear approximation of f denoted Pf on the interval [a, a+ \u2113] satisfies\n\u2016f \u2212 Pf\u20162L2 = \u2113 \u221e \u2211\ni=2\na\u03032i 2i+ 1 . (4)\nProof. A standard result on Legendre polynomials is that given any function f on the interval [a, a+ \u2113], the best linear approximation (w.r.t. the L2 norm) is given by\nPf = a\u03030P\u03030 (x) + a\u03031P\u03031 (x) ,\nwhere P\u03030, P\u03031 are the shifted Legendre polynomials of degree 0 and 1 respectively, and a\u03030, a\u03031 are the first two Fourier-Legendre coefficients of f as defined in Eq. (3). The square of the error obtained by this approximation is therefore\n\u2016f \u2212 Pf\u20162 = \u2016f\u20162 \u2212 2 \u3008f, Pf\u3009+ \u2016Pf\u20162\n= \u2113\n(\n\u221e \u2211\ni=0\na\u03032i 2i+ 1 \u2212 2\n(\na\u030320 + a\u030321 3\n)\n+ a\u030320 + a\u030321 3\n)\n= \u2113 \u221e \u2211\ni=2\na\u03032i 2i+ 1 .\nWhere in the second equality we used the orthogonality relationship from Eq. (2), and the generalized Parseval\u2019s identity from Eq. (3).\nLemma 2. Suppose f : [0, 1] \u2192 R satisfies \u2016f \u2212 Pf\u20162L2 \u2265 c\u2113 5 for some constant c > 0, and on any interval [a, a+ \u2113] \u2286 [0, 1]. Then\ninf g\u2208Gn\n\u222b 1\n0 (f \u2212 g)2d\u00b5 \u2265\nc\nn4 .\nProof. Let g \u2208 Gn be some function, let a0 = 0, a1, . . . , an\u22121, an = 1 denote its partition into segments of length \u2113j = aj \u2212 aj\u22121, where g is linear when restricted to any interval [aj\u22121, aj ], and let gj , j = 1, . . . , n denote the linear restriction of g to the interval [aj\u22121, aj ]. Then\n\u222b 1\n0 (p\u2212 g)2 d\u00b5 =\nn \u2211\nj=1\n\u222b aj\naj\u22121\n(p\u2212 gj) 2 d\u00b5\n\u2265 n \u2211\nj=1\nc\u21135j\n= c\nn \u2211\nj=1\n\u21135j . (5)\nNow, recall Ho\u0308lder\u2019s sum inequality which states that for any p, q satisfying 1p + 1 q = 1 we have\nn \u2211\nj=1\n|xjyj | \u2264\n\n\nn \u2211\nj=1\n|xj | p\n\n\n1/p \n\nn \u2211\nj=1\n|yj| q\n\n\n1/q\n.\nPlugging in xj = \u2113j , yj = 1 \u2200j \u2208 {1, . . . , n} we have\nn \u2211\nj=1\n|\u2113j | \u2264\n\n\nn \u2211\nj=1\n|\u2113j| p\n\n\n1/p\nn1/q,\nand using the equalities \u2211n j=1 |\u2113j| = 1 and p q = p\u2212 1 we get that\n1\nnp\u22121 \u2264\nn \u2211\nj=1\n|\u2113j| p . (6)\nPlugging the inequality from Eq. (6) with p = 5 in Eq. (5) yields \u222b 1\n0 (p\u2212 g)2 d\u00b5 \u2265\nc\nn4 ,\nconcluding the proof of the lemma.\nOur first lower bound for approximation using piece-wise linear functions is for non-linear target functions of the simplest kind. Namely, we obtain lower bounds on quadratic functions.\nTheorem 3. If Gn is the family of piece-wise linear functions with at most n linear segments in the interval [0, 1], then for any quadratic function p(x) = p2x2 + p1x+ p0, we have\ninf g\u2208Gn\n\u222b 1\n0 (p\u2212 g)2d\u00b5 \u2265 p22 180n4 . (7)\nProof. Observe that since p is a degree 2 polynomial, we have that its coefficients satisfy a\u0303i = 0 \u2200i \u2265 3, so from Lemma 1 its optimal approximation error equals a\u0303 2 2 \u2113\n5 . Computing a\u03032 can be done directly from the equation\np (x) =\n2 \u2211\ni=0\na\u0303iP\u0303i (x) ,\nWhich gives a\u03032 = p2\u21132\n6 due to Eq. (1). This implies that\n\u2016p\u2212 Pp\u20162 = p22\u2113 5\n180 .\nNote that for quadratic functions, the optimal error is dependent solely on the length of the interval. Using Lemma 2 with c = p 2 2\n180 we get \u222b 1\n0 (p\u2212 g)2 d\u00b5 \u2265 p22 180n4 ,\nconcluding the proof of the theorem.\nComputing a lower bound for quadratic functions is made easy since the bound on any interval [a, a+ \u2113] depends on \u2113 but not on a. This is not the case in general, as can be seen by observing monomials of high degree k. As k grows, xk on the interval [0, 0.5] converges rapidly to 0, whereas on [\n1\u2212 1k , 1 ] its second\nderivative is lower bounded by k(k\u22121)4 , which indicates that indeed a lower bound for x k will depend on a.\nFor non-quadratic functions, however, we now show that a lower bound can be derived under the assumption of strong convexity (or strong concavity) in [0, 1].\nTheorem 4. Suppose f : [0, 1] \u2192 R is C2 and either \u03bb-strongly convex or \u03bb-strongly concave. Then\ninf g\u2208Gn\n\u222b 1\n0 (f \u2212 g)2d\u00b5 \u2265 c\u03bb2n\u22124, (8)\nwhere c > 0 is a universal constant.\nProof. We first stress that an analogous assumption to \u03bb-strong convexity would be that f is \u03bb-strongly concave, since the same bound can be derived under concavity by simply applying the theorem to the additive inverse of f , and observing that the additive inverse of any piece-wise linear approximation of f is in itself, of course, a piece-wise linear function. For this reason from now on we shall use the convexity assumption, but will also refer without loss of generality to concave functions.\nAs in the previous proof, we first prove a bound on intervals of length \u2113 and then generalize for the unit interval. From Lemma 1, it suffices that we lower bound a\u03032 (although this might not give the tightest lower bound in terms of constants, it is possible to show that it does give a tight bound over all C2 functions). We compute\na\u03032 = 5\n\u2113\n\u222b a+\u2113\na P\u03032 (x) f (x) dx\n= 5\n\u2113\n\u222b a+\u2113\na P2\n(\n2 \u2113 x\u2212 2 \u2113 a\u2212 1\n)\nf (x) dx,\nusing the change of variables t = 2\u2113x\u2212 2 \u2113a\u2212 1, dt = 2 \u2113dx, we get the above equals\n5\n2\n\u222b 1\n\u22121 P2 (t) f\n(\n\u2113 2 t+ \u2113 2 + a\n)\ndt\n= 5\n4\n\u222b 1\n\u22121\n( 3t2 \u2212 1 ) f\n(\n\u2113 2 t+ \u2113 2 + a\n)\ndt.\nWe now integrate by parts twice, taking the anti-derivative of the polynomial to obtain\n5\n4\n\u222b 1\n\u22121\n( 3t2 \u2212 1 ) f\n(\n\u2113 2 t+ \u2113 2 + a\n)\ndt\n= 5\n4\n[\n( t3 \u2212 t ) f\n(\n\u2113 2 t+ \u2113 2 + a\n)]1\n\u22121\n\u2212 5\u2113\n8\n\u222b 1\n\u22121\n( t3 \u2212 t ) f \u2032 ( \u2113\n2 t+\n\u2113 2 + a\n)\ndt\n= 5\u2113\n8\n\u222b 1\n\u22121\n( t\u2212 t3 ) f \u2032 ( \u2113\n2 t+\n\u2113 2 + a\n)\ndt\n= 5\u2113\n8\n[(\nt2 2 \u2212 t4 4\n) f \u2032 ( \u2113\n2 t+\n\u2113 2 + a\n)]1\n\u22121\n\u2212 5\u21132\n16\n\u222b 1\n\u22121\n(\nt2 2 \u2212 t4 4\n) f \u2032\u2032 ( \u2113\n2 t+\n\u2113 2 + a\n)\ndt\n= 5\u2113\n32\n( f \u2032 (a+ \u2113)\u2212 f \u2032 (a) ) \u2212 5\u21132\n16\n\u222b 1\n\u22121\n(\nt2 2 \u2212 t4 4\n) f \u2032\u2032 ( \u2113\n2 t+\n\u2113 2 + a\n)\ndt. (9)\nBut since t 2 2 \u2212 t4 4 \u2208 [ 0, 14 ] \u2200t \u2208 [\u22121, 1] and since f \u2032\u2032 > 0 due to strong convexity, we have that \u222b 1\n\u22121\n(\nt2 2 \u2212 t4 4\n) f \u2032\u2032 ( \u2113\n2 t+\n\u2113 2 + a\n)\ndt \u2264 1\n4\n\u222b 1\n\u22121 f \u2032\u2032\n(\n\u2113 2 t+ \u2113 2 + a\n)\ndt.\nPlugging this inequality in Eq. (9) yields\na\u03032 \u2265 5\u2113\n32\n( f \u2032 (a+ \u2113)\u2212 f \u2032 (a) ) \u2212 5\u21132\n64\n\u222b 1\n\u22121 f \u2032\u2032\n(\n\u2113 2 t+ \u2113 2 + a\n)\ndt\n= 5\u2113\n32\n( f \u2032 (a+ \u2113)\u2212 f \u2032 (a) ) \u2212 5\u21132\n64\n( f \u2032 (a+ \u2113)\u2212 f \u2032 (a) )\n=\n(\n1\u2212 \u2113\n2\n)\n5\u2113\n32\n( f \u2032 (a+ \u2113)\u2212 f \u2032 (a) ) ,\nbut \u2113 \u2264 1, so the above is at least 5\u2113\n64\n( f \u2032 (a+ \u2113)\u2212 f \u2032 (a) ) . (10)\nBy Lagrange\u2018s intermediate value theorem, there exists some \u03be \u2208 [a, a+ \u2113] such that f \u2032 (a+ \u2113)\u2212 f \u2032 (a) = \u2113f \u2032\u2032 (\u03be), so Eq. (10) is at least\n5\u21132 64 f \u2032\u2032 (\u03be) ,\nand by using the strong convexity of f again, we get that\na\u03032 \u2265 5\u03bb\u21132\n64 .\nLemma 1 now gives\n\u2016f \u2212 Pf\u20162 = \u2113 \u221e \u2211\ni=2\na\u03032i 2i+ 1\n\u2265 \u2113 a\u030322 5 \u2265 5\u03bb2\u21135\n4096 .\nFinally, by using Lemma 2 we conclude\ninf g\u2208Gn\n\u222b 1\n0 (f \u2212 g)2d\u00b5 \u2265\n5\u03bb2\n4096n4 .\nWe now derive a general lower bound for functions f : [0, 1] \u2192 R.\nTheorem 5. Suppose f : [0, 1] \u2192 R is piece-wise C2. Then for any \u03bb > 0\ninf g\u2208Gn\n\u222b 1\n0 (f \u2212 g)2d\u00b5 \u2265\nc \u00b7 \u03bb2 \u00b7 \u03c3\u03bb(f) 5\nn4 .\nProof. First, observe that if f is \u03bb-strongly convex on [a, b], then f ((b\u2212 a)x+ a) is \u03bb (b\u2212 a)2-strongly convex on [0, 1] since \u2200x \u2208 [0, 1],\n\u2202\n\u2202x2 f ((b\u2212 a)x+ a) = (b\u2212 a)2 f \u2032\u2032 ((b\u2212 a) x+ a) \u2265 \u03bb (b\u2212 a)2 .\nNow, we use the change of variables x = (b\u2212 a) t+ a, dx = (b\u2212 a) dt\ninf g\u2208Gn\n\u222b b\na (f(x)\u2212 g(x))2dx\n= inf g\u2208Gn (b\u2212 a)\n\u222b 1\n0 (f ((b\u2212 a) t+ a)\u2212 g ((b\u2212 a) t+ a))2 dt\n= inf g\u2208Gn (b\u2212 a)\n\u222b 1\n0 (f ((b\u2212 a) t+ a)\u2212 g (t))2 dt\n\u2265 c \u00b7 \u03bb2 \u00b7 (b\u2212 a)5\nn4 , (11)\nwhere the inequality follows from an application of Thm. 4. Back to the theorem statement, if \u03c3\u03bb = 0 then the bound trivially holds, therefore assume \u03bb > 0 such that \u03c3\u03bb > 0. Since f is strongly convex on a set of measure \u03c3\u03bb > 0, the theorem follows by applying the inequality from Eq. (11)."}, {"heading": "3.3 Multi-dimensional Lower Bounds", "text": "We now move to generalize the bounds in the previous subsection to general dimension d. Namely, we can now turn to proving Thm. 1.\nProof of Thm. 1. Analogously to the proof of Thm. 5, we identify a neighborhood of f in which the restriction of f to a line in a certain direction is non-linear. We then integrate along all lines in that direction and use the result of Thm. 5 to establish the lower bound.\nBefore we can prove the theorem, we need to assert that indeed there exists a set having a strictly positive measure where f has strong curvature along a certain direction. Assuming f is not piece-wise linear; namely, we have some x0 \u2208 [0, 1]\nd such that H (f) (x0) 6= 0. Since H (f) is continuous, we have that the function hv (x) = v\u22a4H (f) (x)v is continuous and there exists a direction v \u2208 Rd where without loss of generality hv (x0) > 0. Thus, we have an open neighborhood containing x0 where restricting f to the direction v forms a strongly convex function, which implies that indeed \u03c3\u03bb > 0 for small enough \u03bb > 0.\nWe now integrate the approximation error on f in the neighborhood U along the direction v, \u2016v\u20162 = 1, where v is the eigenvector of \u03bb, to derive a lower bound. We compute\ninf g\u2208Gdn\n\u222b\n[0,1]d (f \u2212 g)2d\u00b5d\n= inf g\u2208Gdn\n\u222b\nu:\u3008u,v\u3009=0\n\u222b\n\u03b2:(u+\u03b2v)\u2208[0,1]d (f \u2212 g)2 d\u00b51d\u00b5d\u22121\n\u2265 inf g\u2208Gdn\n\u222b\nu:\u3008u,v\u3009=0\n\u222b\n\u03b2:(u+\u03b2v)\u2208U (f \u2212 g)2 d\u00b51d\u00b5d\u22121\n\u2265\n\u222b\nu:\u3008u,v\u3009=0 (\u00b51 ({\u03b2 : (u+ \u03b2v) \u2208 U}))\n5 5\u03bb 2\n4096n4 d\u00b5d\u22121\n= 5\u03bb2\n4096n4\n\u222b\nu:\u3008u,v\u3009=0 |\u00b51 ({\u03b2 : (u+ \u03b2v) \u2208 U})|\n5 d\u00b5d\u22121\n\u2265 5\u03bb2\n4096n4\n(\n\u222b\nu:\u3008u,v\u3009=0 \u00b51 ({\u03b2 : (u+ \u03b2v) \u2208 U}) d\u00b5d\u22121\n)5\n= 5\u03bb2\u03c35\u03bb 4096n4 ,\nwhere in the second inequality we used Thm. 5 and in the third inequality we used Jensen\u2018s inequality with respect to the convex function x 7\u2192 |x|5."}, {"heading": "4 Efficiently Approximating Functions with Small Representations", "text": "In this section, we show that it is possible to approximate a wide family of functions using ReLU neural networks, where the error decays exponentially with the depth. Specifically, we consider functions which can be approximated using a moderate number of multiplications and additions, where the values of intermediate computations are bounded (for example, a special case is any function approximable by a moderately-sized Boolean circuit, or a polynomial).\nThe key result to show this is the following, which shows that the multiplication of two (bounded-size) numbers can be approximated by a neural network, with error decaying exponentially with depth:\nTheorem 6. Let f : [\u2212M,M ]2 \u2192 R, f (x, y) = x \u00b7y and let \u01eb > 0 be arbitrary. Then exists a ReLU neural network g of width 4 \u2308 log (\nM \u01eb\n)\u2309 + 13 and depth \u2308 2 log ( M \u01eb )\u2309 + 9 satisfying\nsup (x,y)\u2208[\u2212M,M ]2 |f (x, y)\u2212 g (x, y)| \u2264 \u01eb.\nThe idea of the construction is that depth allows us to compute highly-oscillating functions, which can extract high-order bits from the binary representation of the inputs. Given these bits, one can compute the product by a procedure resembling long multiplication:\nProof of Thm. 6. We begin by observing that by using a simple linear change of variables on x, we may assume without loss of generality that x \u2208 [0, 1], as we can just rescale x to the interval [0, 1], and then map it back to its original domain [\u2212M,M ], where the error will multiply by a factor of 2M . Then by requiring accuracy \u01eb2M instead of \u01eb, the result will follow.\nThe key behind the proof is that performing bit-wise operations on the first k bits of x \u2208 [0, 1] yields an estimation of the product to accuracy 21\u2212kM . Let x =\n\u2211\u221e i=1 2 \u2212ixi be the binary representation of x where xi is the ith bit of x, then\nx \u00b7 y = \u221e \u2211\ni=1\n2\u2212ixi \u00b7 y\n=\nk \u2211\ni=1\n2\u2212ixi \u00b7 y + \u221e \u2211\ni=k+1\n2\u2212ixi \u00b7 y. (12)\nBut since \u2223\n\u2223 \u2223 \u2223 \u2223\n\u221e \u2211\ni=k+1\n2\u2212ixi \u00b7 y\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 \u2223 \u2223 \u2223 \u2223 \u2223 \u221e \u2211\ni=k+1\n2\u2212i \u00b7 y\n\u2223 \u2223 \u2223 \u2223 \u2223 = 2\u2212k |y| \u2264 21\u2212kM,\nEq. (12) implies \u2223\n\u2223 \u2223 \u2223 \u2223\nx \u00b7 y \u2212 k \u2211\ni=1\n2\u2212ixi \u00b7 y\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 21\u2212kM.\nRequiring that 22\u2212kM \u2264 \u01eb2M , it suffices to show the existence of a network which approximates the function \u2211k\ni=1 2 \u2212ixi \u00b7 y to accuracy \u01eb2 , where k = 2\n\u2308 log ( 8M \u01eb )\u2309 . This way both approximation will be at most \u01eb2 , resulting in the desired accuracy of \u01eb.\nBefore specifying the architecture which extracts the ith bit of x, we first describe the last 2 layers of the network. Let the penultimate layer comprise of k neurons, each receiving both y and xi as input, and having the set of weights ( 2\u2212i, 1,\u22121 ) . Thus, the output of the ith neuron in the penultimate layer is\n[ 2\u2212iy + xi \u2212 1 ] + = 2\u2212ixiy.\nLet the final single output neuron have the set of weights (1, . . . , 1) \u2208 Rk, this way, the output of the network will be\n\u2211k i=1 2 \u2212ixi \u00b7 y as required. We now specify the architecture which extracts the first most significant k bits of x. In Telgarsky [2016],\nthe author demonstrates how the composition of the function\n\u03d5 (x) = [2x]+ \u2212 [4x\u2212 2]+\nwith itself i times, \u03d5i, yields a highly oscillatory triangle wave function in the domain [0, 1]. Furthermore, we observe that \u03d5 (x) = 0 \u2200x \u2264 0, and thus \u03d5i (x) = 0 \u2200x \u2264 0. Now, a linear shift of the input of \u03d5i by 2\u2212i\u22121, and composing the output with\n\u03c3\u03b4 (x) =\n[\n1\n2\u03b4 \u2212\n1\n4\u03b4 +\n1\n2\n]\n+\n\u2212\n[\n1\n2\u03b4 \u2212\n1\n4\u03b4 \u2212\n1\n2\n]\n+\n,\nwhich converges to 1[x\u22650.5] (x) as \u03b4 \u2192 0, results in an approximation of x 7\u2192 xi:\n\u03c3\u03b4 ( \u03d5i ( x\u2212 2\u2212i\u22121 )) .\nWe stress that choosing \u03b4 such that the network approximates the bit-wise product to accuracy \u01eb2 will require \u03b4 to be of magnitude 1\u01eb , but this poses no problem as representing such a number requires log ( 1 \u01eb )\nbits, which is also the magnitude of the size of the network, as suggested by the following analysis.\nNext, we compute the size of the network required to implement the above approximation. To compute \u03d5 only two neurons are required, therefore \u03d5i can be computed using i layers with 2 neurons in each, and finally composing this with \u03c3\u03b4 requires a subsequent layer with 2 more neurons. To implement the ith bit extractor we therefore require a network of size 2\u00d7 (i+ 1). Using dummy neurons to propagate the ith bit for i < k, the architecture extracting the k most significant bits of x will be of size 2k \u00d7 (k + 1). Adding the final component performing the multiplication estimation will require 2 more layers of width k and 1 respectively, and an increase of the width by 1 to propagate y to the penultimate layer, resulting in a network of size (2k + 1)\u00d7 (k + 1).\nThe previous result shows that multiplication can be performed accurately by deep networks. Moreover, additions can be computed by ReLU networks exactly, using only a single layer with 4 neurons: Let \u03b1, \u03b2 \u2208 R be arbitrary, then (x, y) 7\u2192 \u03b1 \u00b7 x+ \u03b2 \u00b7 y is given in terms of ReLU summation by\n\u03b1 [x]+ \u2212 \u03b1 [\u2212x]+ + \u03b2 [y]+ \u2212 \u03b2 [\u2212y]+ .\nThus, any function which can be approximated by a reasonable amount of operations involving additions and multiplications, can also be approximated well by moderately-sized networks. This is formalized in the following theorem:\nTheorem 7. Suppose Ft,M,\u01eb is the family of functions on the domain [0, 1] d with the property that f \u2208 Ft,M,\u01eb is approximable to accuracy \u01eb with respect to the infinity norm, using at most t operations involving weighted addition, (x, y) 7\u2192 \u03b1 \u00b7 x + \u03b2 \u00b7 y, where \u03b1, \u03b2 \u2208 R are fixed; and multiplication, (x, y) 7\u2192 x \u00b7 y, where each intermediate computation stage is bounded in the interval [\u2212M,M ]. Then there exists a universal constant c, and a ReLU network g of width and depth at most c ( t log (\n1 \u01eb\n) + t2 log (M) ) , such that\nsup x\u2208[0,1]d |f (x)\u2212 g (x)| \u2264 2\u01eb.\nProof. We begin by monitoring the rate of growth of the error when performing either an addition or a multiplication. Suppose that the given input a\u0303, b\u0303 is of distance at most \u03b4 > 0 from the desired target values a, b, i.e., |a\u2212 a\u0303| \u2264 \u03b4, \u2223 \u2223 \u2223 b\u2212 b\u0303 \u2223 \u2223\n\u2223 \u2264 \u03b4. Then for addition we have \u2223\n\u2223 \u2223 (a+ b)\u2212\n( a\u0303+ b\u0303 )\u2223 \u2223 \u2223 \u2264 |a\u2212 a\u0303|+ \u2223 \u2223 \u2223 b\u2212 b\u0303 \u2223 \u2223 \u2223 \u2264 2\u03b4,\nand for multiplication we compute the product error estimation \u2223\n\u2223 \u2223 a\u0303 \u00b7 b\u0303\u2212 a \u00b7 b\n\u2223 \u2223 \u2223 \u2264 |(a+ \u03b4) \u00b7 (b+ \u03b4)\u2212 a \u00b7 b|\n= \u2223 \u2223\u03b4 (a+ b) + \u03b42 \u2223 \u2223 .\nNow, on top of the cumulative error, we add the error of approximating the product of a \u00b7 b, which we may assume is at most \u03b4 for each product computed in the series of operations performed, if enough bits are used as in the proof of Thm. 6, to have that\n\u2223 \u2223\u03b4 (a+ b) + \u03b42 + \u03b4 \u2223 \u2223 \u2264 3M\u03b4.\nThat is, at each stage the error grows by at most a multiplicative factor of 3M . After t operations, and with an initial estimation error of \u03b4, we have that the error is bounded by (3M)t\u22121 \u03b4. Choosing \u03b4 \u2264 (3M)1\u2212t \u01eb to guarantee approximation \u01eb, we have from Thm. 6 that each operation will require at most\n4\n\u2308\nlog\n(\nM (3M)t\u22121\n\u01eb\n)\u2309\n+ 13 \u2264 c\n(\nlog\n(\n1\n\u01eb\n)\n+ t log (M)\n)\nwidth and\n2\n\u2308\nlog\n(\nM (3M)t\u22121\n\u01eb\n)\u2309\n+ 9 \u2264 c\n(\nlog\n(\n1\n\u01eb\n)\n+ t log (M)\n)\ndepth for some universal c > 0. Composing the networks performing each operation, we arrive at a total network width and depth of at most\nc\n(\nt log\n(\n1\n\u01eb\n)\n+ t2 log (M)\n)\n.\nNow, our target function is approximated to accuracy \u01eb by a function which our network approximates to the same accuracy \u01eb, for a total approximation error of the target function by our network of 2\u01eb."}, {"heading": "5 The Formal Separation Result", "text": "With the previous results in hand, we are now finally ready to present our main result, which formally shows how depth can be exponentially more valuable than width as a function of the target accuracy \u01eb:\nCorollary 3. Suppose f \u2208 C2 \u2229Ft(\u01eb),M(\u01eb),\u01eb, where t (\u01eb) = O ( poly ( log ( 1 \u01eb ))) and M (\u01eb) = O ( poly ( 1 \u01eb ))\n. Then approximating f to accuracy \u01eb in the L2 norm using a fixed depth ReLU network requires width at least c\u01eb , whereas there exists a ReLU network of depth and width at most p ( log ( 1 \u01eb ))\nwhich approximates f to accuracy \u01eb in the infinity norm, where c, p are a constant and a polynomial respectively, depending solely on f .\nThe proof of Corollary 3 follows immediately from Corollary 2 and Thm. 7. While the lower bound is straightforward, for the upper bound observe that Thm. 7 implies an \u01eb approximation by a network of width and depth at most\nc\n(\nt ( \u01eb\n2\n)\nlog\n(\n2\n\u01eb\n)\n+ ( t ( \u01eb\n2\n))2 log ( M ( \u01eb\n2\n))\n)\n,\nwhich by the assumption of Corollary 3, can be bounded by\np\n(\nlog\n(\n1\n\u01eb\n))\nfor some polynomial p which depends solely on f ."}], "references": [{"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Eldan and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "It is well-known that networks of depth 2 can already approximate any continuous target function on the boolean hypercube [0, 1] to arbitrary accuracy, albeit at the cost of width exponential in the dimension d [Cybenko, 1989].", "startOffset": 211, "endOffset": 226}, {"referenceID": 3, "context": "Indeed, recent empirical evidence suggests that even at large depths, deeper networks can offer benefits over shallower networks [He et al., 2015].", "startOffset": 129, "endOffset": 146}, {"referenceID": 2, "context": "In Eldan and Shamir [2016], the authors show that depth in neural networks can be exponentially valuable even if increased only by 1, by proving the existence of a family of radial functions which are expressible using a depth 3 feed-forward neural network of size polynomial in the dimension d, whereas a network of depth 2 cannot approximate these functions to more than constant accuracy (even if the activation function is arbitrary), unless its width is exponential in d.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "In Eldan and Shamir [2016], the authors show that depth in neural networks can be exponentially valuable even if increased only by 1, by proving the existence of a family of radial functions which are expressible using a depth 3 feed-forward neural network of size polynomial in the dimension d, whereas a network of depth 2 cannot approximate these functions to more than constant accuracy (even if the activation function is arbitrary), unless its width is exponential in d. However, it is not clear whether their proof techniques extend to deeper networks. In contrast, our results apply to potentially deeper networks, and the separation is in terms of the required accuracy \u01eb, rather than some fixed depths. The work perhaps most similar to ours is Telgarsky [2016]. In that work, the author proves a separation result between networks of depth k and depth o (", "startOffset": 3, "endOffset": 771}, {"referenceID": 0, "context": "Finally, in Cohen et al. [2016], the authors establish a depth separation result for networks with a certain tensor structure.", "startOffset": 12, "endOffset": 32}, {"referenceID": 4, "context": "To translate this result to the context of ReLU neural networks, we use the result in Telgarsky [2016, Lemma 3.2], of which the following is an immediate corollary. Corollary 1. Let N d m,l denote the family of ReLU neural networks receiving input of dimension d and having depth l and maximal width m. Then N d m,l \u2286 G d (2m) . The following corollary bears great resemblance to the lower bound provided in Telgarsky [2016]. Albeit lower bounding the accuracy to which a ReLU network of a given size can approximate a certain function, rather than indicating what is a lower bound on the minimal size required for achieving non-constant approximation error.", "startOffset": 86, "endOffset": 425}, {"referenceID": 4, "context": "In Telgarsky [2016], the author demonstrates how the composition of the function \u03c6 (x) = [2x]+ \u2212 [4x\u2212 2]+ with itself i times, \u03c6i, yields a highly oscillatory triangle wave function in the domain [0, 1].", "startOffset": 3, "endOffset": 20}], "year": 2017, "abstractText": "We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on [0, 1]d, which can be approximated to accuracy \u01eb by ReLU networks of depth and width O(poly (log(1/\u01eb))), cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least \u03a9(1/\u01eb).", "creator": "LaTeX with hyperref package"}}}