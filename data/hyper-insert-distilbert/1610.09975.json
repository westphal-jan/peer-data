{"id": "1610.09975", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition", "abstract": "but we thoroughly present results again that show ; it is finally possible to strongly build aboard a competitive, greatly simplified, large vocabulary continuous speech recognition algorithm system with coding whole words as acoustic grammar units. we also model the optical output vocabulary of about 100, 000 words directly reproduced using deep frequency bi - directional lstm rnns with continuous ctc loss. also the latter model is strongly trained on 125, 000 samples hours availability of rigorous semi - supervised formal acoustic word training data, which enables us to alleviate using the data extraction sparsity restriction problem for generic word models. we show recently that the simpler ctc word models generally work relatively very effectively well as solving an early end - to - end all - neural speech output recognition model without the use of traditional context - dependent sub - word phone units programs that nevertheless require clearly a pronunciation controlled lexicon, easily and without any language coding model removing the need factor to decode. \" we demonstrate proving that exploring the ctc channel word classification models perform better than preparing a strong, ultimately more readily complex, composite state - of - the - art baseline with identical sub - word units.", "histories": [["v1", "Mon, 31 Oct 2016 15:36:42 GMT  (28kb,D)", "http://arxiv.org/abs/1610.09975v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hagen soltau", "hank liao", "hasim sak"], "accepted": false, "id": "1610.09975"}, "pdf": {"name": "1610.09975.pdf", "metadata": {"source": "CRF", "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition", "authors": ["Hagen Soltau", "Hank Liao", "Hasim Sak"], "emails": ["soltau@google.com", "hankliao@google.com", "hasim@google.com"], "sections": [{"heading": "1 Introduction", "text": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137]. In the past, the best speech recognition systems have used many complex modeling techniques for improving accuracy: for example the use of hand-crafted feature representations, speaker or environment adaptation with feature or affine transformations, and context-dependent (CD) phonetic models with decision tree clustering [8, 9] to name a few. For automatic speech recognition, the goal is to minimize the word error rate. Therefore it is a natural choice to use words as units for acoustic modeling and estimate word probabilities. While some attempts had been made to model words directly, in particular for isolated word recognition with very limited vocabularies [10], the dominant approach is to model clustered CD sub-word units instead.\nOn the other hand, these clustered units were a necessity when data were limited, but may now be a sub-optimal choice. Recently, the amount of user-uploaded captions for public YouTube videos has grown dramatically. Using powerful neural network models with large amounts of training data can allow us to directly model words and greatly simplify an automatic speech recognition system. It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13]. Although training data sparsity was an issue, bi-directional LSTM RNN models, with a large output vocabulary could be trained, i.e. up to 90,000 words, and obtained respectable accuracy without doing any speech decoding, however this was still far from the sub-word phone-based recognizer. In this paper, we show that these techniques coupled with a larger amount of acoustic training data enable us to build a neural speech recognizer (NSR) that can be trained end-to-end to recognize words directly without needing to decode.\nThere have been many different approaches to end-to-end neural network models for speech recognition. Lu et al. [6] use an encoder-decoder model of the conditional probability of the full word output sequence given the input sequence. However due to the limited capacity, they extend it with a dynamic attention scheme that makes the attention vector a function of the internal state of the encoder-decoder which is then added to the encoder RNN. They compare this with explicitly increasing the capacity\nar X\niv :1\n61 0.\n09 97\n5v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\n01 6\nof the RNN. In both cases, there is still a gap in performance with the conventional hybrid neural network phone-based HMM recognizer. Instead of word outputs, letters can be generated directly in a grapheme-based neural network recognizer [3, 7]; while good results are obtained for a large vocabulary task, they are not quite comparable to the phone-based baseline system."}, {"heading": "2 Neural Speech Recognizer", "text": "Here, we describe the techniques that we used for building the NSR: a single neural network model capable of accurate speech recognition with no search or decoding involved. The NSR model has a deep LSTM RNN architecture built by stacking multiple LSTM layers. Since the bidirectional RNN models [14] have better accuracy and our application is offline speech recognition, we use two LSTM layers at each depth\u2014one operating in the forward and another operating in the backward direction in time over the input sequence. Both these layers are connected to both previous forward and backward layers.\nWe train the NSR model with the CTC loss criterion [12] which is a sequence alignment/labeling technique with a softmax output layer that has an additional unit for the blank label used to represent outputting no label at a given time. The output label probabilities from the network define a probability distribution over all possible labelings of input sequences including the blank labels. The network is trained to optimize the total probability of correct labelings for training data as estimated using the network outputs and forward-backward algorithm. The correct labelings for an input sequence are defined as the set of all possible labelings of the input with the target labels in the correct sequence order possibly with repetitions and with blank labels permitted between labels. The CTC loss can be efficiently and easily computed using finite state transducers (FSTs) as described in Sak et al. [15]\nLCTC = \u2212 \u2211 (x,l) lnp(zl|x) = \u2212 \u2211 (x,l) L(x, zl) (1)\nwhere x is the input sequence of acoustic frames, l is the input label sequence (e.g. a sequence of words for the NSR model), zl is the lattice encoding all possible alignments of x with l which allows label repetitions possibly interleaved with blank labels. The probability for correct labelings p(zl|x) can be computed using the forward-backward algorithm. The gradient of the loss function w.r.t. input activations atl of the softmax output layer for a training example can be computed as follows:\n\u2202L(x, zl) \u2202atl = ytl \u2212 1 p(zl|x) \u2211\nu\u2208{u:zlu=l}\n\u03b1x,zl(t, u)\u03b2x,zl(t, u) (2)\nwhere ytl is the softmax activation for a label l at time step t, and u represents the lattice states aligned with label l at time t, \u03b1x,zl(t, u) is the forward variable representing the summed probability of all paths in the lattice zl starting in the initial state at time 0 and ending in state u at time t, \u03b2(t, u) is the backward variable starting in state u of the lattice at time t and going to a final state.\nThe NSR model has a final softmax predicting word posteriors with the number of outputs equaling the vocabulary size. Modeling words directly can be problematic due to data sparsity, but we use a large amount of acoustic training data to alleviate it. We experiment with both written and spoken vocabulary. The vocabulary obtained from the training data transcripts is mapped to the spoken forms to reduce the data sparsity further and limit label ambiguity for the spoken vocabulary experiments. For written-to-spoken domain mapping a FST verbalization model is used [16]. For example, \u201d104\u201c is converted to \u201done hundred four\u201c and \u201cone oh four\u201c. Given all possible verbalizations for an entity, the one that aligns best with acoustic training data is chosen.\nThe NSR model is essentially an all-neural network speech recognizer that does not require any beam search type of decoding. The network takes as input mel-spaced log filterbank features. The word posterior probabilities output from the model can be simply used to get the recognized word sequence. Since this word sequence is in spoken domain for the spoken vocabulary model, to get the written forms, we also create a simple lattice by enumerating the alternate words and blank label at each time step, and rescore this lattice with a written-domain word language model (LM) by FST composition after composing it with the verbalizer FST. For the written vocabulary model, we directly compose the lattice with the language model to assess the importance of language model rescoring for accuracy.\nWe train the models in a distributed manner using asynchronous stochastic gradient descent (ASGD) with a large number of machines [17, 18]. We found the word acoustic models performed better when\ninitialized using the parameters from hidden states of phone models\u2014the output layer weights are randomly initialized and the weights in the initial networks are randomly initialized with a uniform (-0.04, 0.04) distribution. For training stability, we clip the activations of memory cells to [-50, 50], and the gradients to [-1, 1] range. We implemented an optimized native TensorFlow CPU kernel (multi_lstm_op) for multi-layer LSTM RNN forward pass and gradient calculations. The multi_lstm_op allows us to parallelize computations across LSTM layers using pipelining and the resulting speed-up decreases the parameter staleness in asynchronous updates and improves accuracy."}, {"heading": "3 Experimental Setup", "text": "YouTube is a video sharing website with over a billion users. To improve accessibility, Google has functionality to caption YouTube videos using automatic speech recognition technology [19]. While generated caption quality can vary, and are generally no better than human created ones, they can be produced at scale. On the whole, users have found them helpful: Google received a technology breakthrough award from the US National Association of the Deaf in 2015 for automatic captioning on YouTube. For this work, we evaluate our models on videos sampled from Google Preferred channels on YouTube [20]. The test set is comprised of 296 videos from 13 categories, with each video averaging 5 minutes in length. The total test set duration is roughly 25 hours and 250,000 words [21]. As the bulk of our training data is not supervised, an important question is how valuable this type of the data is for training acoustic models. In all our experiments, we keep our language model constant and use a 5-gram model with 30M N-grams over a vocabulary of 500,000 words.\nTraining large, accurate neural network models for speech recognition requires abundant data. While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours. This \u201cislands of confidence\u201d filtering, allows us to use user-uploaded captions for labels, by selecting only audio segments in a video where the user uploaded caption matches the transcript produced by an ASR system constrained to be more likely to produce N-grams found in the uploaded caption. Of the approximately 500,000 hours of video available with English captions, a quarter remained after filtering."}, {"heading": "3.1 Conventional Context Dependent Phone Models", "text": "The initial acoustic model was trained on 650 hours of supervised training data that comes from YouTube, Google Videos, and Broadcast News described in [24]. The acoustic model is a 3-state HMM with 6400 CD triphone states. This system gave us a 29.0% word error rate on the Google Preferred test set as shown in table 1. By training with a sequence-level state-MBR criterion and using a two-pass adapted decoding setup, the best we could do was 24.0% with a 650 hour training set. Contrast this with adding more semi-supervised training data: at 5000 hours, we reduced the error rate to 21.2% for the same model size. Since we have more data available, and models that can capture longer temporal context, we show results for single-state CD phone units [25]; this gives a 4% relative improvement over the 3-state triphone models. This type of model improves with the amount of training data and there is little difference between CE and CTC training criteria."}, {"heading": "3.2 Neural Speech Recognizer", "text": "The entire acoustic training corpus has 1.2 billions words with a vocabulary of 1.7 million words. For the neural speech recognizer, we experimented with both spoken and written output vocabularies with the CTC loss. For the spoken vocabulary, we decided to only model words that are seen more than 100 times. This resulted in a vocabulary of 82473 words and an OOV rate of 0.63%. For the written vocabulary, we chose words seen more than 80 times, resulting in 97827 words and an OOV rate of 0.7%. For comparison, the full test vocabulary of our baseline has 500,000 words and an OOV rate of 0.24%. We evaluated the impact of the reduced vocabulary with our best CD phone models and observed an increase of 0.5% in WER (Table 2).\nIn table 2 we compare CTC CD phone with CTC word models. For both units, we trained models with 5\u00d7 600 and 7\u00d7 1000 bidirectional LSTM layers. As the output layer for the word models is substantially larger, the total number of parameters for the word models is larger than for the CD phone models for the same number and size of LSTM layers. We also want to note that we tried to increase the number of parameters for CD phone models, but the reported results in table 2 are the best results we could obtain with CD phone models. As show in table 2, increasing the number of CD phones to 35326 does not yield a reduction in error rate. Deep decision trees tend to work mostly in scenarios when the phonetic contexts are well matched in train and test data. Open domains such as Youtube videos typically don\u2019t gain from a very large number of context dependent models, in particular if temporal context is already covered with deep bidirectional LSTM models.\nAs the difference in performance between CTC and CE phone models is not too big, we ran the same comparison for word models. As this was part of our earlier experiments, we trained only on 50,000 hours: with CE training, the model performed poorly with an error rate of 23.1%, while training with CTC loss performed substantially better at 18.7%. This is not unexpected as predicting longer units on a frame by frame basis with CE makes the prediction task substantially harder. Overall, table 2 shows that the word models outperform the CD phone models even with the handicap of a higher OOV rate for the word models.\nAs mentioned earlier we can use the CTC word model directly without any decoding or language model and the recognition output becomes the output from the CTC layer, essentially making the CTC word model an end-to-end all-neural speech recognition model. The entire speech recognizer becomes a single neural network. Figure 1 shows the word posterior probabilities as predicted by the model for a music video. Even though it has not been trained on music videos, the model is quite robust and accurate in transcribing the songs. The results are shown in the last columns in table 2 for the CTC word models. Without any use of a language model and decoding, the CTC spoken word model has an error rate of 14.8% and the CTC written word model has 13.9% WER. The written word model is better than the conventional CD phone model which has 14.2% WER obtained with decoding with a language model. This shows that bi-directional LSTM CTC word models are capable of accurate speech recognition with no language model or decoding involved. As a sanity check we pruned our language model heavily to a de-weighted uni-gram model and used it with our CTC CD phone models. As expected, the error rate increases drastically, from 14.2% to 21%, showing that the language model is important for conventional models but less important for whole word CTC models. For the spoken word model, the WER improves to 14.8% when the word lattices obtained from the model are rescored with a language model. The improvements are mostly due to conversion\nof spoken word forms to written forms (such as numeric entities) since the WER scoring is done in the written domain. The WER of written word model improves only by 0.5% to 13.4% when the word lattices are rescored with the LM, showing the relatively small impact of the LM in the accuracy of the system.\nThe error rate calculation disadvantages the CTC spoken word model as the references are in written domain, but the output of the model is in spoken domain, creating artificial errors like \"three\" vs \"3\". This is not the case for our conventional CD phone baseline and the CTC written word model, as words are there modeled in the written domain. To evaluate the error rate in the spoken domain, we automatically converted the test data by force aligning the utterances with a graph built as C * L * project(V * T), where C is the context transducer, L the lexicon transducer, V the spoken-to-written transducer, and T the written transcript. Project maps the input symbols to the output symbols, thereby the output symbols of the entire graph will be in the spoken domain. We are using the same approach to convert the written language model G to a spoken form by calculating project(V * G) and using the spoken LM to build the decoding graph. The word error rates in the spoken domain are shown in table 3. The models are the same as in the previous table 2. We can see that word models without use of any language model or decoding performs at 12.0% WER, slightly better than the CD phone model that uses an LVCSR decoder and incorporates a 30m 5-gram language model. We can also separate the effect of the language model from the spoken-to-written text normalization. Adding the language model for the CTC spoken word model improves the error rate from 12.0% to 11.6%, showing the CTC spoken word models perform very well even without the language model."}, {"heading": "4 Conclusions", "text": "We presented our Neural Speech Recognizer: an end-to-end all-neural large vocabulary continuous speech recognizer that forgoes the use of a pronunciation lexicon and a decoder. Mining 125,000 hours of training data using public captions allows us to train a large and powerful bi-directional LSTM RNN model for speech recognition with a CTC loss that predicts words. The neural speech recognizer can model a written vocabulary of 100K words including numeric entities. Unlike many end-to-end systems that compromise accuracy for system simplicity, our final system performs better than a well-trained, conventional context-dependent phone-based system achieving a 13.4% word error rate on a difficult YouTube video transcription task."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proc. ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "In Proc. ASRU,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "In Proc. ICASSP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Towards end-to-end speech recognition with deep convolutional neural networks", "author": ["Ying Zhang", "Mohammad Pezeshki", "Phil\u00e9mon Brakel", "Saizheng Zhang", "C\u00e9sar Laurent", "Yoshua Bengio", "Aaron Courville"], "venue": "In Proc. Interspeech,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Deep Speech 2: End-to-end speech recognition in English and Mandarin", "author": ["Dario Amodei"], "venue": "In Proc. ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["Liang Lu", "Xing-Xing Zhang", "Steve Renals"], "venue": "In Proc. ICASSP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "In Proc. ICASSP,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Context dependent modelling of phones in continuous speech using decision trees", "author": ["L.R. Bahl", "P.V. de Souza", "P.S. Gopalakrishnan", "D. Nahamoo", "M.A. Picheny"], "venue": "In Proc. DARPA Speech and Natural Language Processing Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Tree-based state tying for high accuracy acoustic modelling", "author": ["S.J. Young", "J.J. Odell", "P.C. Woodland"], "venue": "In Proc. ARPA Workshop on Human Language Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "A time-delay neural network architecture for isolated word recognition", "author": ["Kevin J. Lang", "Alex H. Waibel", "Geoffrey E. Hinton"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan Irsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "In Proc. ICASSP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Language model verbalization for automatic speech recognition", "author": ["Ha\u015fim Sak", "Fran\u00e7oise Beaufays", "Kaisuke Nakajima", "Cyril Allauzen"], "venue": "In Proc. ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew W. Senior", "Paul A. Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling", "author": ["Hasim Sak", "Andrew Senior", "Francoise Beaufays"], "venue": "In Proc. Interspeech,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning N-gram language models from uncertain data", "author": ["Vitaly Kuznetsov", "Hank Liao", "Mehryar Mohri", "Michael Riley", "Brian Roark"], "venue": "In Proc. Interspeech,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "LibriSpeech: an ASR corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "In Proc. ICASSP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["Olga Kapralova", "John Alex", "Eugene Weinstein", "Pedro Moreno", "Olivier Siohan"], "venue": "In Proc. Interspeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription", "author": ["Hank Liao", "Erik McDermott", "Andrew Senior"], "venue": "In Proc. ASRU,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Context dependent phone models for LSTM RNN acoustic modelling", "author": ["A. Senior", "H. Sak", "I. Shafran"], "venue": "In Proc. ICASSP,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 1, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 2, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 3, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 4, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 5, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 6, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 7, "context": "In the past, the best speech recognition systems have used many complex modeling techniques for improving accuracy: for example the use of hand-crafted feature representations, speaker or environment adaptation with feature or affine transformations, and context-dependent (CD) phonetic models with decision tree clustering [8, 9] to name a few.", "startOffset": 324, "endOffset": 330}, {"referenceID": 8, "context": "In the past, the best speech recognition systems have used many complex modeling techniques for improving accuracy: for example the use of hand-crafted feature representations, speaker or environment adaptation with feature or affine transformations, and context-dependent (CD) phonetic models with decision tree clustering [8, 9] to name a few.", "startOffset": 324, "endOffset": 330}, {"referenceID": 9, "context": "While some attempts had been made to model words directly, in particular for isolated word recognition with very limited vocabularies [10], the dominant approach is to model clustered CD sub-word units instead.", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13].", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13].", "startOffset": 261, "endOffset": 265}, {"referenceID": 5, "context": "[6] use an encoder-decoder model of the conditional probability of the full word output sequence given the input sequence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Instead of word outputs, letters can be generated directly in a grapheme-based neural network recognizer [3, 7]; while good results are obtained for a large vocabulary task, they are not quite comparable to the phone-based baseline system.", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "Instead of word outputs, letters can be generated directly in a grapheme-based neural network recognizer [3, 7]; while good results are obtained for a large vocabulary task, they are not quite comparable to the phone-based baseline system.", "startOffset": 105, "endOffset": 111}, {"referenceID": 13, "context": "Since the bidirectional RNN models [14] have better accuracy and our application is offline speech recognition, we use two LSTM layers at each depth\u2014one operating in the forward and another operating in the backward direction in time over the input sequence.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "We train the NSR model with the CTC loss criterion [12] which is a sequence alignment/labeling technique with a softmax output layer that has an additional unit for the blank label used to represent outputting no label at a given time.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "[15]", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For written-to-spoken domain mapping a FST verbalization model is used [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "We train the models in a distributed manner using asynchronous stochastic gradient descent (ASGD) with a large number of machines [17, 18].", "startOffset": 130, "endOffset": 138}, {"referenceID": 17, "context": "We train the models in a distributed manner using asynchronous stochastic gradient descent (ASGD) with a large number of machines [17, 18].", "startOffset": 130, "endOffset": 138}, {"referenceID": 0, "context": "For training stability, we clip the activations of memory cells to [-50, 50], and the gradients to [-1, 1] range.", "startOffset": 99, "endOffset": 106}, {"referenceID": 18, "context": "The total test set duration is roughly 25 hours and 250,000 words [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 43, "endOffset": 50}, {"referenceID": 19, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 43, "endOffset": 50}, {"referenceID": 20, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 201, "endOffset": 205}, {"referenceID": 21, "context": "The initial acoustic model was trained on 650 hours of supervised training data that comes from YouTube, Google Videos, and Broadcast News described in [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "Since we have more data available, and models that can capture longer temporal context, we show results for single-state CD phone units [25]; this gives a 4% relative improvement over the 3-state triphone models.", "startOffset": 136, "endOffset": 140}], "year": 2016, "abstractText": "We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.", "creator": "LaTeX with hyperref package"}}}