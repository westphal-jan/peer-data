{"id": "1611.00429", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Distributed Mean Estimation with Limited Communication", "abstract": "if motivated by the computational need for computational distributed optimization, algorithms with truly low communication cost, we study communication thereby efficient algorithms need to properly perform weak distributed mean variance estimation. effectively we will study scenarios in which practically each client vendor sends one bit per dimension. consequently we researchers first show that for $ d $ dimensional sample data with $ a n $ 3d clients, finding a naive infinite stochastic rounding approach yields merely a residual mean squared error $ \\ unit theta ( k d / n ) $. we then ourselves show by applying a structured only random rotation of the data ( an $ \\ \\ mathcal { w o } ( e d \\ > log d ) $ k algorithm ), the error can be reduced to $ \\ th mathcal { y o } ( ( \\ log g d ) / n ) $. while the subsequent algorithms construct and the analysis make only no internal distributional correlation assumptions dependent on the initial data.", "histories": [["v1", "Wed, 2 Nov 2016 00:16:18 GMT  (12kb)", "http://arxiv.org/abs/1611.00429v1", null], ["v2", "Mon, 27 Feb 2017 17:37:14 GMT  (77kb)", "http://arxiv.org/abs/1611.00429v2", null], ["v3", "Mon, 25 Sep 2017 15:10:54 GMT  (77kb)", "http://arxiv.org/abs/1611.00429v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ananda theertha suresh", "felix x yu", "sanjiv kumar", "h brendan mcmahan"], "accepted": true, "id": "1611.00429"}, "pdf": {"name": "1611.00429.pdf", "metadata": {"source": "CRF", "title": "Distributed Mean Estimation with Limited Communication", "authors": ["Ananda Theertha Suresh", "Felix X. Yu", "H. Brendan McMahan", "Sanjiv Kumar"], "emails": ["sanjivk}@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 42\n9v 1\n[ cs\n.L G\n] 2\nN ov\n2 01"}, {"heading": "1 Introduction", "text": "Distributed learning algorithms are widely used in training large-scale neural networks [8, 4]. In a typical scenario of synchronized distributed learning, each client obtains a copy of a global model. The clients then update the model independently based on their local data. The updates of the model (usually in the form of gradient) are then sent to a server, where they are averaged and applied to update the global model. One critical step in these algorithms is to estimate the mean of the updates. Motivated by these applications, we study the problem of distributed mean estimation with a limited communication budget. Formally, given n vectors Xn def = X1,X2 . . . ,Xn \u2208 Rd that reside on n clients, the goal of distributed mean estimation is to estimate the mean of the vectors:\nX\u0304 def =\n1\nn\nn \u2211\ni=1\nXi.\nIn any protocol, each client transmits a function of Xi say f(Xi) and a central server estimates the mean by some function of f(X1), f(X2), . . . , f(Xn). Let \u03c0 be any such protocol and let C(\u03c0,Xn) be the expected number of transmitted bits by clients during protocol \u03c0 i.e.,\nC(\u03c0,Xn) def= n \u2211\ni=1\nCi(\u03c0,Xi),\nwhere Ci(\u03c0,Xi) is the expected number of bits transmitted by client i. Let the estimated mean be \u02c6\u0304X. For a protocol \u03c0, the mean squared error of the estimate is\nE(\u03c0,Xn) = E [ ( \u02c6\u0304X \u2212 X\u0304)2 ] .\nThroughout this paper, the expectation is over the randomization in \u03c0.\nUnlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings. Specifically, we study estimating the empirical mean of the data. Furthermore, all of our proposed algorithms are simultaneous and independent i.e., the clients independently send data to the server, and they can transmit at the same time.\nOur method requires the use of both private and public randomness. Private randomness refers to randomness that is generated by each machine separately, and public randomness refers to a sequence of randomness that is shared among all parties1.\nWe analyze the mean squared error E(\u03c0,Xn) for two algorithms when C(\u03c0,Xn) = n\u00b7(d+O\u0303(1))2, i.e., each client sends one bit per dimension. In Section 2, we show that a naive stochastic binary quantization algorithm achieves a mean squared error of\nE(\u03c0sb,Xn) = \u0398 ( d\nn \u00b7 1 n\nn \u2211\ni=1\n||Xi||22\n)\n.\nIn many practical scenarios d is much larger than n and the above error is prohibitive [7]. In Section 3 we propose a modified algorithm, where we randomly rotate the vectors before quantization. In Theorem 3, we show that this new scheme achieves a mean squared error of\nE(\u03c0srb,Xn) = O ( log d\nn \u00b7 1 n\nn \u2211\ni=1\n||Xi||22\n)\n.\nFurthermore, the random ration can be achieved in O(d log d) time. Finally in Section 4, we show a simple modification that can be used to achieve different communication and mean squared error trade-offs."}, {"heading": "2 Stochastic Binary Quantization: A Naive Approach", "text": "We propose the stochastic binary quantization as follows. For a vectorXi, letX max i = max1\u2264j\u2264dXi(j) and similarly let Xmini = min1\u2264j\u2264dXi(j). In the stochastic binary quantization protocol \u03c0sb, for each client i, the quantized value for each coordinate j is generated independently with private randomness as\nYi(j) =\n\n\n\nXmaxi w.p. Xi(j)\u2212Xmini Xmaxi \u2212Xmini Xmini otherwise.\nObserve E [\nYi(j)] = Xi(j). The server estimates X\u0304 by\n\u02c6\u0304X = 1\nn\nn \u2211\ni=1\nYi.\nWe first bound the communication complexity of the above protocol.\n1In the absence of public randomness, the server can communicate a random seed that can be used by clients to emulate public randomness.\n2We use O\u0303(1) denote O(log(dnB)), where B is the maximum norm of the underlying vectors.\nLemma 1. There exists an implementation of stochastic binary quantization that uses d + O\u0303(1) bits per client and hence C(\u03c0sb,Xn) \u2264 n \u00b7 ( d+ O\u0303(1) ) .\nProof. Instead of sending vectors Yi, the clients transmit two real values X max i and X min i (to a desired error) and a vector bit vector Y \u2032i such that Y \u2032 i (j) = 1 if Yi = X max i and 0 otherwise. Hence each client transmits d + 2r bits, where r is the number of bits to transmit the real value to a desired error.\nTo bound r, observe that using r bits, one can represent a number between \u2212B/2 and B/2 to an error of B/2r. Thus using 3 log2(dnB) bits one can represent the minimum and maximum to an additive error of 1/(nd)3. This error in transmitting minimum and maximum of the vector does not affect our calculations and we ignore it for simplicity. We note that in practice, each dimension of Xi is often stored as 32 bit or 64 bit float [7], and r should be set as either 32 or 64. In such case, using an even larger r does not further reduce the error.\nWe now compute the estimation error of the proposed protocol.\nLemma 2. For any sequence Xn,\nE(\u03c0sb,Xn) = 1\nn2\nn \u2211\ni=1\nd \u2211\nj=1\n(Xmaxi \u2212Xi(j))(Xi(j)\u2212Xmini ).\nProof. The error is\nE(\u03c0sb,Xn) = E [ \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u02c6\u0304X \u2212 X\u0304 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n]\n= 1\nn2 E\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 n \u2211\ni=1\n(Yi \u2212Xi) \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\n\n= 1\nn2\nn \u2211\ni=1\nE[ [ ||Yi \u2212Xi||22 ] ,\nwhere the last inequality follows by observing each Yi \u2212 Xi are independent zero mean random variables. The proof follows by observing that for every i,\nE\n[ ||Yi \u2212Xi||22 ]\n= d \u2211\nj=1\nE[(Yi(j)\u2212Xi(j))2]\n=\nd \u2211\nj=1\n[\nXi(j) \u2212Xmini Xmaxi \u2212Xmini (Xmaxi \u2212Xi(j))2 + Xmaxi \u2212Xi(j) Xmaxi \u2212Xmini (Xmini \u2212Xi(j))2 ]\n=\nd \u2211\nj=1\n(Xmaxi \u2212Xi(j))(Xi(j)\u2212Xmini ). (1)\nLemma 2 implies the following upper bound.\nTheorem 1. For any sequence Xn,\nE(\u03c0sb,Xn) \u2264 d\n4n2\nn \u2211\ni=1\n(Xmaxi \u2212Xmini )2 \u2264 d 2n \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 .\nProof. The proof follows by Lemma 2 upon observing that\n(Xmaxi \u2212Xi(j))(Xi(j)\u2212Xmini ) \u2264 (Xmaxi \u2212Xmini )2\n4 , (2)\nand (Xmaxi \u2212Xmini )2 \u2264 2(Xmaxi )2 + 2(Xmini )2 \u2264 2 ||Xi||22 .\nWe also show that the above bound is tight up to a factor of d/(d \u2212 2): Theorem 2. There exists a set of vectors Xn such that\nE(\u03c0sb,Xn) \u2265 d\u2212 2 2n \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 .\nProof. For every i, let Xi be defined as follows. Xi(1) = 1/ \u221a 2, Xi(2) = \u22121/ \u221a 2, and for all j > 2, Xi(j) = 0. For every i, X max i = 1\u221a 2 and Xmini = \u2212 1\u221a2 . Substituting these bounds in the conclusion of Lemma 2 (which is an equality) yields the theorem.\nThe results above show that for vectors with norm 1, the simple algorithm proposed in this section gives mean squared error \u0398(d/n). Such an error is too large for real-world use. For example, in the application of neural networks [7], d can be on the order of millions, yet the number of clients is much lower than that. In such a case, the mean squared error is even larger than the norm of the vector."}, {"heading": "3 Stochastic Rotated Binary Quantization", "text": "We show that the algorithm of the previous section can be significantly improved by a new protocol. The motivation comes from the fact that the mean squared error is small when Xmaxi and X min i are close to each other (Theorem 1). For example, when Xi is generated uniformly on the unit sphere, with high probability, Xmaxi \u2212Xmini is O ( \u221a log d d ) [3]. In such case, E(\u03c0sb,Xn) is O( log dn ) instead\nof O( dn). In this section, we show that even without any assumption of the distribution of the data, we can \u201creduce\u201d Xmaxi \u2212Xmini with a structured random rotation, yielding an O( log dn ) error. We call the method stochastic rotated binary quantization and denote it by \u03c0srb.\nUsing public randomness, all clients and the central server generate a random rotation matrix R \u2208 Rd\u00d7d according to some known distribution. Let Zi = RXi and Z\u0304 = RX\u0304. In the stochastic rotated binary quantization protocol \u03c0srb(R), for each client i, the quantized value for each coordinate j is generated independently with private randomness as\nYi(j) =\n\n\n\nZmaxi w.p. Zi(j)\u2212Zmini Zmaxi \u2212Zmini Zmini otherwise.\nThe server estimates X\u0304 by\n\u02c6\u0304X = R\u22121 \u02c6\u0304Z, \u02c6\u0304Z = 1\nn\nn \u2211\ni=1\nYi.\nAs before, the communication complexity of this protocol is bounded.\nLemma 3. There exists an implementation of Stochastic rotated binary quantization that uses d+ O\u0303(1) bits per client and hence E(\u03c0sb,Xn) \u2264 n \u00b7 ( d+ O\u0303(1) ) .\nThe next lemma upper bounds the mean squared error.\nLemma 4. For any sequence Xn,\nE(\u03c0srb(R),Xn) \u2264 d\n2n2\nn \u2211\ni=1\nER\n[\n(Zmaxi ) 2 +\n( Zmini )2 ] ,\nwhere Zi = RXi.\nProof. We have\nE(\u03c0srb,Xn) = E\u03c0 [ \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u02c6\u0304X \u2212 X\u0304 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 ]\n= E\u03c0\n[\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223R\u22121 \u02c6\u0304Z \u2212R\u22121Z\u0304 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n2 ]\n(a) = E\u03c0\n[\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u02c6\u0304Z \u2212 Z\u0304 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n2 ]\n(b) = E\u03c0\n[\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u02c6\u0304Z \u2212 Z\u0304 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2 |Z\u0304\n]\n\u2264 d 4n2\nn \u2211\ni=1\nER[(Z max i \u2212 Zmini )2],\nwhere the last inequality follows Eqs. (1) and (2), (a) follows from the fact that rotation does not change norm of the vector and (b) follows from the tower law of expectation. The lemma follows from observing that\n(Zmaxi \u2212 Zmini )2 \u2264 2(Zmaxi )2 + 2(Zmini )2.\nTo obtain strong bounds, we need to find an orthogonal matrix R that achieves low (Zmaxi ) 2 and (Zmini ) 2. In addition, due to the fact that d can be huge in practice, we need a type of orthogonal matrix that has fast matrix-vector product algorithm. Naive orthogonal matrices that support fast multiplication such as block-diagonal matrices often result in high values of (Zmaxi ) 2 and (Zmini ) 2. Motivated by recent works in structured matrices [1, 9], we propose to use R = HD, where H is a Walsh-Hadamard matrix and D is a diagonal matrix with i.i.d. Rademacher entries. Both applying the rotation and inverse rotation takes O(d log d) and O(1) addition space (with an inplace algorithm). The next lemma bounds E [ (Zmaxi ) 2 ] and E [ ( Zmini )2 ] for this choice of R. The lemma is similar to that of results in [1] and we give the proof for completeness.\nLemma 5. If R = HD, then for every i and every sequence Xn,\nE [ (Zmini ) 2 ] = E [ (Zmaxi ) 2 ]\n\u2264 ||Xi|| 2 2 (2 log d+ 2)\nd .3\nProof. The equality follows from the symmetry in HD. To prove the upper bound, observe that\nE [ (Zmaxi ) 2 ] = Var (Zmaxi ) + (E [Z max i ]) 2 .\nLet D(i) be the ith diagonal entry of D. To bound the first term observe that Zmaxi is a function of d independent random variables D(1),D(2), . . . D(d). Changing D(j) changes the Zmaxi by at most 2Xi(j)\u221a d . Hence, applying Efron-Stein variance bound yields\nVar (Zmaxi ) \u2264 d \u2211\nj=1\n4X2i (j)\n2d = 2 ||Xi||22 d .\nTo bound the second term, observe that for every \u03b2 > 0,\n\u03b2Zmaxi = log exp (\u03b2Z max i ) \u2264 log\n\n\nd \u2211\nj=1\ne\u03b2Zi(j)\n\n .\nNote that Zi(k) = 1\u221a d \u2211d j=1D(j)H(k, j)Xi(j). Since D(j)s are Radamacher random variables and |H(k, j)| = 1 for all k, j, the distributions of Zi(k) is same for all k. Hence by Jensen\u2019s inequality,\nE [Zmaxi ] \u2264 1\n\u03b2 E\n\nlog\n\n\nd \u2211\nj=1\ne\u03b2Zi(j)\n\n\n\n \u2264 1 \u03b2 log\n\n\nd \u2211\nj=1\nE[e\u03b2Zi(j)]\n\n = 1\n\u03b2 log\n( dE[e\u03b2Zi(1)] ) .\nSince Zi(1) = 1\u221a d \u2211d j=1D(j)Xi(j),\nE[e\u03b2Zi(1)] = E\n[ e \u03b2 \u2211 j D(j)Xi(j) \u221a d ]\n(a) =\nd \u220f\nj=1\nE\n[\ne \u03b2D(j)Xi(j) \u221a d\n]\n= d \u220f\nj=1\ne\u2212\u03b2Xi(j)/ \u221a d + e\u03b2Xi(j)/ \u221a d\n2\n(b) \u2264 d \u220f\nj=1\ne\u03b2 2X2(j)/2d\n= e\u03b2 2||Xi||22/2d,\nwhere (a) follows from the fact that D(i)s are independent and (b) follows from the fact that ea + e\u2212a \u2264 2ea2/2 for any a. Hence,\nE[Zmaxi ] \u2264 min \u03b2\nlog d\n\u03b2 + \u03b2 ||Xi||22 2d\n\u2264 2 ||Xi||2 \u221a log d\u221a\n2d .\n3All logarithms are to base e.\nCombining the above two lemmas results in the main result.\nTheorem 3. For any Xn, \u03c0srb(HD) protocol satisfies,\nE(\u03c0srb(HD),Xn) \u2264 2 log d+ 2 n \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 ."}, {"heading": "4 The effect of sampling", "text": "It can be shown that the above protocols can be combined by client or coordinate sampling to obtain trade-offs between the mean squared error and the communication cost. We outline the approach for combining client sampling and \u03c0rsb. Similar analysis holds for coordinate sampling and other protocols.\nLet \u03c0srb(HD, p) be the protocol in which only p fraction of clients send their values independently and the server sums up these values and scales them by 1/p, i.e., the server estimates X\u0304 by\n\u02c6\u0304X = (HD)\u22121 \u00b7 1 np \u2211\ni\u2208S Yi,\nwhere Yis are defined in the previous section and S is the set of clients that transmitted.\nCorollary 1. For any sequence Xn and p > 0,\nE(\u03c0srb(HD, p),Xn) \u2264 2(log d+ 1) np \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 + 1\u2212 p np \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 .\nFurthermore, there exists an implementation of \u03c0rsb(HD, p) such that\nC(\u03c0srb(HD, p),Xn) \u2264 np \u00b7 (d+ O\u0303(1)).\nProof. The proof of communication cost follows from Lemma 3 and the fact that in expectation, np clients transmit the data. We now bound the mean squared error. Let S be the set of clients that decide to transmit. Then the error is\nE(\u03c0srb(HD, p),Xn) = E [ \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u02c6\u0304X \u2212 X\u0304 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n]\n= E\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 np \u2211\ni\u2208S (HD)\u22121Yi \u2212 X\u0304\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\n\n=E\n\n\n1\nn2p2\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\ni\u2208S ((HD)\u22121Yi \u2212Xi)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\n+ E\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 np \u2211\ni\u2208S Xi \u2212 X\u0304\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\n ,\nwhere the last equality follows by observing each (HD)\u22121Yi\u2212Xi are independent zero mean random variables and hence for any i, |E[((HD)\u22121Yi \u2212 Xi)T ( 1np \u2211 i\u2208S Xi \u2212 X\u0304)] = 0. To bound the first\nterm, observe that\nE\n\n\n1\nn2p2\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\ni\u2208S ((HD)\u22121Yi \u2212Xi)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\n (a) =\n1\nn2p2\n\u2211\ni\u2208S\n\u2223 \u2223 \u2223 \u2223 \u2223E [ ((HD)\u22121Yi \u2212Xi) \u2223 \u2223 \u2223 \u2223 2\n2\n]\n= 1\nn2p2\nn \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 \u2223E [ ((HD)\u22121Yi \u2212Xi) \u2223 \u2223 \u2223 \u2223 2\n2 1i\u2208S\n]\n= 1\nn2p\nn \u2211\ni=1\nE\n[\n\u2223 \u2223 \u2223 \u2223(HD)\u22121Yi \u2212Xi \u2223 \u2223 \u2223 \u2223 2\n2\n]\n\u2264 2(log d+ 1) np \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 ,\nwhere the proof of the last inequality is similar to that of Theorem 3. (a) follows from the fact that (HD)\u22121Yi \u2212Xi are independent zero mean random variables. Furthermore, the second term can be bounded as\nE\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 np n \u2211\ni=1\nXi \u2212 X\u0304 \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\n = 1\nn2\nn \u2211\ni=1\nE\n[\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1\np Xi1i\u2208S \u2212Xi\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n]\n= 1\nn2\nn \u2211\ni=1\n(\np (1\u2212 p)2\np2 ||Xi||22 + (1\u2212 p) ||Xi|| 2 2\n)\n= 1\u2212 p np \u00b7 1 n\nn \u2211\ni=1\n||Xi||22 ,\nand hence the lemma."}, {"heading": "5 Conclusion", "text": "We studied the distributed mean estimation where each client sends one bit for each dimension of the data. We showed that a naive stochastic quantization algorithm achieves \u0398(d/n) error, and an improved stochastic rotated quantization algorithm achieves O((log d)/n) error."}, {"heading": "Acknowledgments", "text": "We thank Jayadev Acharya, Keith Bonawitz, Jakub Konecny, Dan Holtmann-Rice, and Tengyu Ma for helpful comments and discussions."}, {"heading": "27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,", "text": "Montreal, Quebec, Canada, pages 2726\u20132734, 2014.\n[7] Jakub Konec\u030cny\u0300, H Brendan McMahan, Felix X Yu, Peter Richta\u0301rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\n[8] H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning of deep networks using model averaging. arXiv:1602.05629, 2016.\n[9] Felix X Yu, Ananda Theertha Suresh, Krzysztof Choromanski, Daniel Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features. In NIPS, 2016."}], "references": [{"title": "Fast dimension reduction using rademacher series on dual BCH codes", "author": ["Nir Ailon", "Edo Liberty"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Communication lower bounds for statistical estimation problems via a distributed data processing inequality", "author": ["Mark Braverman", "Ankit Garg", "Tengyu Ma", "Huy L. Nguyen", "David P. Woodruff"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Informationtheoretic lower bounds for distributed statistical estimation with communication", "author": ["John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright", "Yuchen Zhang"], "venue": "constraints. CoRR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "On communication cost of distributed statistical estimation and dimensionality", "author": ["Ankit Garg", "Tengyu Ma", "Huy L. Nguyen"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Federated learning: Strategies for improving communication efficiency", "author": ["Jakub Kone\u010dn\u1ef3", "H Brendan McMahan", "Felix X Yu", "Peter Richt\u00e1rik", "Ananda Theertha Suresh", "Dave Bacon"], "venue": "arXiv preprint arXiv:1610.05492,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Federated learning of deep networks using model averaging", "author": ["H. Brendan McMahan", "Eider Moore", "Daniel Ramage", "Blaise Aguera y Arcas"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Orthogonal random features", "author": ["Felix X Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "1 Introduction Distributed learning algorithms are widely used in training large-scale neural networks [8, 4].", "startOffset": 103, "endOffset": 109}, {"referenceID": 3, "context": "1 Introduction Distributed learning algorithms are widely used in training large-scale neural networks [8, 4].", "startOffset": 103, "endOffset": 109}, {"referenceID": 4, "context": "Unlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings.", "startOffset": 22, "endOffset": 31}, {"referenceID": 5, "context": "Unlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings.", "startOffset": 22, "endOffset": 31}, {"referenceID": 1, "context": "Unlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings.", "startOffset": 22, "endOffset": 31}, {"referenceID": 6, "context": "In many practical scenarios d is much larger than n and the above error is prohibitive [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "We note that in practice, each dimension of Xi is often stored as 32 bit or 64 bit float [7], and r should be set as either 32 or 64.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "For example, in the application of neural networks [7], d can be on the order of millions, yet the number of clients is much lower than that.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Motivated by recent works in structured matrices [1, 9], we propose to use R = HD, where H is a Walsh-Hadamard matrix and D is a diagonal matrix with i.", "startOffset": 49, "endOffset": 55}, {"referenceID": 8, "context": "Motivated by recent works in structured matrices [1, 9], we propose to use R = HD, where H is a Walsh-Hadamard matrix and D is a diagonal matrix with i.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "The lemma is similar to that of results in [1] and we give the proof for completeness.", "startOffset": 43, "endOffset": 46}], "year": 2016, "abstractText": "Abstract Motivated by the need for distributed optimization algorithms with low communication cost, we study communication efficient algorithms to perform distributed mean estimation. We study scenarios in which each client sends one bit per dimension. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error \u0398(d/n). We then show by applying a structured random rotation of the data (an O(d log d) algorithm), the error can be reduced to O((log d)/n). The algorithms and the analysis make no distributional assumptions on the data.", "creator": "LaTeX with hyperref package"}}}