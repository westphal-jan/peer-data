{"id": "1611.00601", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Ordinal Common-sense Inference", "abstract": "currently humans have shown the full capacity to mentally draw common - sense inferences seemingly from any natural everyday language : various things that are likely but not certain algorithms to help hold out based agree on certain established genetic discourse, namely and are rarely stated explicitly. precisely we propose starting an easy evaluation platform of automated common - means sense inference based completely on an extension of recognizing textual entailment : predicting : ordinal similarity human behavior responses of subjective likelihood of an inference holding myself in from a given context. immediately we describe a concrete framework for extracting common - sense knowledge for corpora, to which notation is then used to construct a dataset for doing this ordinal linguistic entailment task, which : we then use informally to continually train and evaluate a minimal sequence mapping to sequence hierarchical neural learning network model. further, we carefully annotate subsets of structurally previously compiled established topological datasets via generating our ordinal annotation selection protocol protocol in preparation order for to continually then analyze the distinctions between these and what we have constructed.", "histories": [["v1", "Wed, 2 Nov 2016 13:38:32 GMT  (236kb,D)", "http://arxiv.org/abs/1611.00601v1", null], ["v2", "Thu, 3 Nov 2016 01:44:41 GMT  (237kb,D)", "http://arxiv.org/abs/1611.00601v2", null], ["v3", "Fri, 2 Jun 2017 13:54:23 GMT  (358kb,D)", "http://arxiv.org/abs/1611.00601v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sheng zhang", "rachel rudinger", "kevin duh", "benjamin van durme"], "accepted": true, "id": "1611.00601"}, "pdf": {"name": "1611.00601.pdf", "metadata": {"source": "CRF", "title": "Ordinal Common-sense Inference", "authors": ["Sheng Zhang", "Rachel Rudinger", "Kevin Duh", "Benjamin Van Durme"], "emails": ["vandurme}@cs.jhu.edu"], "sections": [{"heading": null, "text": "Humans have the capacity to draw commonsense inferences from natural language: various things that are likely but not certain to hold based on established discourse, and are rarely stated explicitly. We propose an evaluation of automated common-sense inference based on an extension of recognizing textual entailment: predicting ordinal human responses of subjective likelihood of an inference holding in a given context. We describe a framework for extracting common-sense knowledge for corpora, which is then used to construct a dataset for this ordinal entailment task, which we then use to train and evaluate a sequence to sequence neural network model. Further, we annotate subsets of previously established datasets via our ordinal annotation protocol in order to then analyze the distinctions between these and what we have constructed."}, {"heading": "1 Introduction", "text": "We use words to talk about the world. Therefore, to understand what words mean, we must have a prior explication of how we view the world. \u2013 Hobbs (1987)\nResearchers in Artificial Intelligence and (Computational) Linguistics have long-cited the requirement of common-sense knowledge in language understanding.1 This knowledge is viewed as a key\n1Schank (1975): It has been apparent ... within ... natural language understanding ... that the eventual limit to our solution ... would be our ability to characterize world knowledge.\ncomponent in filling in the gaps between the telegraphic style of natural language statements: we are able to convey considerable information in a relatively sparse channel, presumably owing to a partially shared model at the start of any discourse.2\nCommon-sense inference \u2013 inferences based on common-sense knowledge \u2013 is possibilistic: things everyone more or less would expect to hold in a given context, but without always the strength of logical entailment.3 Owing to human reporting bias (Gordon and Van Durme, 2013), deriving the knowledge needed to perform these inferences exclusively from corpora has led to results most accu-\n2McCarthy (1959): a program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\n3E.g., many of the bridging inferences of Clark (1975) make use of common-sense knowledge, such as the following example of \u201cProbable part\u201d: I walked into the room. The windows looked out to the bay. In order for the definite reference the windows to be resolved, one needs to know that rooms have windows is probable or at least reasonable.\nar X\niv :1\n61 1.\n00 60\n1v 1\n[ cs\n.C L\n] 2\nN ov\nrately considered models of language, rather than of the world (Rudinger et al., 2015). Facts such that a person walking into a room is very likely to be regularly blinking and breathing are not often explicitly stated, so their real-world likelihoods do not align to language model probabilities.4 We would like a system capable of reading a sentence describing some situation, such as found in a newspaper, and be able to infer how likely other statements hold of that situation, in the real world. This as compared to, e.g., knowing the likelihood of the next observed sentence in that newspaper article.\nWe therefore propose a model of knowledge acquisition based on first deriving possibilistic properties from text: as the relative frequency of these statements suffer the mentioned reporting bias, we then follow up with human annotation of derived examples. Our derived common-sense knowledge, which we initially are uncertain as to the real-world likelihood of it holding in any particular context, is paired with various grounded contexts which are presented to humans for their own assessment. As these examples vary in assessed plausibility, we propose the task of ordinal common-sense inference, which embraces a wider set of natural conclusions arising from language comprehension (see Fig 1).\nIn the following, we first describe prior efforts in common-sense knowledge acquisition and textual inference tasks (\u00a72). We then state our position on how common-sense inference should be defined (\u00a73), and detail our own framework for large-scale extraction and abstraction, along with a crowdsourcing protocol for assessment (\u00a74). This includes a novel neural model for forward generation of textual inference statements. Together these methods are applied to contexts derived from various prior textual inference resources, resulting in a large collection of diverse common-sense inference examples, judged to hold with varying levels of subjective likelihood (\u00a75). We provide baseline results (\u00a76) for prediction on the Johns Hopkins Ordinal Commonsense Inference (JOCI) collection, which we will release freely.\n4For further background see discussions by Van Durme (2010), Gordon and Van Durme (2013), Rudinger et al. (2015) and Misra et al. (2016)."}, {"heading": "2 Background", "text": "Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004). Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor. Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pas\u0327ca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007). In this article we are concerned with knowledge mining for purposes of seeding a text generation process (constructing common-sense inference examples).\nCommon-sense Tasks A large portion of textual inference tasks has been designed to require common-sense knowledge at some degree, e.g., the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts such the construction of the FRACAS test suite (Cooper et al., 1996), or they rely on crowdsourced elicitation (Bowman et al., 2015). Crowdsourcing is scalable, but elicitation protocols can lead to biased responses unlikely to contain a wide range of possible common-sense inferences: humans can generally agree on the plausibility of a wide range of possible inference pairs, but they are not likely to generate them from an initial prompt.5\nThe construction of SICK (Sentences Involving Compositional Knowledge) made use of existing paraphrastic sentence pairs (descriptions by differ-\n5McRae et al. (2005): For example, features such as <is larger than a tulip> or <moves faster than an infant>, although logically possible, do not occur in [human responses] [...] Although people are capable of verifying that a <dog is larger than a pencil>.\nent people of the same image), which were modified through a series of rule-based transformations then judged by humans (Marelli et al., 2014). As with SICK, we rely on humans only for judging provided examples, rather than elicitation of text. Unlike SICK, our generation is based on a more extensive process targeted specifically at common sense.\nTextual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006):\nWe say that T entails H if, typically, a human reading T would infer that H is most likely true. This somewhat informal definition is based on (and assumes) common human understanding of language as well as common background knowledge.\nThis definition strayed from the more strict notion of entailment as used by linguistic semanticists, such as those involved with FRACAS. While Giampiccolo et al. (2008) extended binary RTE with an \u201cunknown\u201d category, the entailment community has primarily focussed on issues such as paraphrase and monotonicity (such as captured by MacCartney and Manning (2007)).\nGarrette et al. (2011) and Beltagy et al. (2013) introduced weighted inference rules based words/phrases similarity, and treated textual entailment as probabilistic logical inference in Markov Logic Networks (Richardson and Domingos, 2006). Supported by Hobbs et al. (1988), who proposed viewing natural language interpretation as abductive inference, their approach to weighted inference is to measure how likely H is best entailed, i.e., the probability of H being an entailment. However, it is not guaranteed that there is a map between the probability of being an entailment to the likelihood of an inference being true. For example, \u201dEd has a convertible\u201d does not strictly entail \u201dThe convertible has a retractable roof \u201d, which should receive a low probability of being an entailment. But we can infer here, the convertible is indeed very likely to have a retractable roof.\nNon-entailing Inference Of the various non\u201centailment\u201d textual inference tasks, a few are most salient here. Agirre et al. (2012) piloted a Textual Similarity evaluation which has been refined in each\nsubsequent year: systems produce scalar values corresponding to predictions of how similar the meaning is between two provided sentences. For example, the following pair from SICK was judged very similar (4.2 out of 5), while also being a contradiction: There is no biker jumping in the air and A lone biker is jumping in the air. The ordinal approach we advocate for here relies on a graded notion, like textual similarity.\nThe Choice of Plausible Alternative (COPA) task (Roemmele et al., 2011) was a reaction to RTE, similarly motivated to probe a system\u2019s ability to understand inferences that are not strictly entailed: a single context was provided, with two alternative inferences, and a system had to judge which was more plausible. The COPA dataset was manually elicited, and is not large: we discuss this data further in \u00a7 5.\nThe Narrative Cloze task (Chambers and Jurafsky, 2008) requires a system to score candidate inferences as to how likely they are to appear in a document that also included the provided context. Many such inferences are then not strictly entailed by the context. Further, the cloze task gives the benefit of being able to generate very large numbers of examples automatically by simply occluding parts of existing documents and asking a system to predict what is missing. As our concern is with inferences that are often true but never stated in a document, this approach is not viable here. The ROCStories corpus (Mostafazadeh et al., 2016) elicited a more \u201cplausible\u201d collection of documents in order to retain the narrative cloze in the context of commonsense inference; we consider this dataset in \u00a7 5.\nAlongside the narrative cloze, Pichotta and Mooney (2016) made use of a 5-point Likert scale (very likely to very unlikely) as a secondary evaluation of various script induction techniques. While they were concerned with measuring their ability to generate very likely inferences, here we are interested in generating a wide swath of inference candidates, including those that are impossible."}, {"heading": "3 Ordinal Common-sense Inference", "text": "Our goal is a system that can perform speculative, common-sense inference as part of understanding language. Based on the observed shortfalls of prior work, we propose the notion of Ordinal Common-\nsense Inference (OCI). OCI embraces the notion of Dagan et al. (2006), in that we are concerned with human judgements of whether something is likely. However, rather than weakening the notion of logical truth to result in bins of (most likely) true and (most likely) false, with possibly a gray area between, we refine the task definition to directly embrace subjective likelihood on an ordinal scale. Humans when presented with a context C are asked whether a provided inference I is very likely, likely, plausible, technically possible, or impossible. Furthermore, an important part of this process is the generation of I by automatic methods, which seeks to avoid the elicitation bias of many prior works."}, {"heading": "4 Framework for collecting JOCI", "text": "We now describe our framework for collecting ordinal common-sense inferences. It is natural to collect this data in two stages. In the first stage (\u00a74.1), we automatically generate inference candidates given some context. These are called automatically-generated common-sense inference candidates (AGCI), and we propose two broad approaches using either general world knowledge (AGCI-WK) or neural sequence-to-sequence methods (AGCI-NN). In the second stage (\u00a74.2), we annotate AGCI with ordinal labels."}, {"heading": "4.1 Generation of Common-sense Inference Candidates", "text": ""}, {"heading": "4.1.1 Generation based on World Knowledge", "text": "Our motivation for this approach was first introduced by Schubert (2002):\nThere is a largely untapped source of general knowledge in texts, lying at a level beneath the explicit assertional content. This knowledge consists of relationships implied to be possible in the world, or, under certain conditions, implied to be normal or commonplace in the world.\nFollowing Schubert (2002) and Van Durme and Schubert (2008), we define an approach for abstracting over explicit assertions derived from corpora, leading to a large-scale collection of general possibilistic statements. As shown in Fig 2, this approach generates common-sense inference candidates in four steps: (a) extracting propositions with predicate-argument structures from texts, (b)\nabstracting over propositions to generate templates for concepts, (c) deriving properties of concepts via different strategies, and (d) generating possibilistic inferences from contexts.\n(a) Extracting propositions: First we extract a large set of propositions with predicate-argument structures from noun phrases and clauses, under which general world presumptions often lie. Following Rudinger and Van Durme (2014), we define a series of conversion rules for post-processing a sentential syntactic dependency tree into zero or more predicate argument structures. Fig 2 (a) shows an example extraction.\nWe use the Gigaword corpus (Parker et al., 2011) for extracting propositions as it is a comprehensive text archive. There exists a version containing automatically generated syntactic annotation (Ferraro et al., 2014), which bootstraps large-scale knowledge extraction. We use PyStanfordDependencies6 to convert constituency parses to depedency parses, from which we extract structured propositions. (b) Abstracting propositions: In this step, we abstract the propositions into a more general form. This involves lemmatization, stripping inessential modifiers and conjuncts, and replacing specific arguments with generic types (e.g., replacing \u201cJohn\u201d with \u201cperson\u201d)7. An example is shown in Fig 2 (b). This method of abstraction often yields general presumptions about the world. To reduce noise from predicate-argument extraction, we only keep 1-place and 2-place predicates after abstraction.\nWe further generalize individual arguments to concepts by attaching semantic-class labels to them. Here we choose WordNet (Miller, 1995) noun synsets8 as the semantic-class set. (A synset is a group of words denoting the same concept, as represented by a single WordNet sense, e.g., person.n.01.) When selecting the correct sense for an argument, we adopt a fast and relatively accurate method: always taking the first sense which is usually the most commonly used sense (Suchanek et al., 2007; Pasca, 2008). By doing so, we attach 84 million abstracted propositions with senses, covering 43.7% (35,811/81,861) of WordNet noun senses.\nEach of these WordNet senses, then, is associated with a set of abstracted propositions. The abstracted propositions are turned into templates by replacing the sense\u2019s corresponding argument with a placeholder, similar to Van Durme et al. (2009). (See Fig 2 (b).) We remove any template associated with a sense if it occurs less than two times for that sense, leaving 38 million unique templates. (c) Deriving properties via WordNet: At this step, we want to associate with each WordNet sense a set of possible properties. We employ three strategies.\n6https://pypi.python.org/pypi/ PyStanfordDependencies\n7See Schubert (2002) for detail. 8In order to avoid too general senses, we set cut points at the depth of 4 (Pantel et al., 2007) to truncate the hierarchy and consider all 81,861 senses below these points.\nThe first strategy is to use a decision tree to pick out highly discriminative properties for each WordNet sense. Specifically, for each set of cohyponyms9, we train a decision tree using the associated templates as features. For example, in Fig 2 (c), we train a decision tree over the cohyponyms of publication.n.01. Then the template \u201cperson subscribe to \u201d would be selected as a property of magazine.n.01, and the template \u201cperson borrow from library\u201d for book.n.01.\nThe second strategy is simply to select the most frequent templates associated with each sense as properties of that sense.\nThe third strategy uses WordNet ISA relations to derive new properties of senses. For example, for the sense book.n.01 and its hypernym publication.n.01, we generate a property \u201c be publication\u201d. (d) Generating inference candidates: As shown in Fig 2 (d), given a discourse context (Tanenhaus and Seidenberg, 1980), we first extract an argument of the context, then select the derived properties for the argument. Since we don\u2019t assume any specific sense for the argument, these properties could come from any of its candidate senses. They could also come from any sense on the hypernym path of all candidate senses, as the downward polarity of the properties guarantees their applicability to more specific senses. We then generate inferences by replacing the placeholder in the selected properties with the argument, and verbalizing the properties10."}, {"heading": "4.1.2 Generation via Neural Methods", "text": "As an alternative to the knowledge-based methods described above, we also adapt a neural sequence-tosequence model (Vinyals et al., 2015; Bahdanau et al., 2014) to directly generate inferences given contexts. The model is trained on sentence pairs labeled \u201centailment\u201d from the train set of the SNLI corpus (Bowman et al., 2015). Here, the SNLI \u201cpremise\u201d is the input to the network (i.e., context), and the SNLI \u201chypothesis\u201d is the output (inference).\n9Senses sharing a hypernym with each other are called cohyponyms (e.g., book.n.01, magazine.n.01 and collections.n.02 are co-hyponyms of publication.n.01).\n10 We use the pattern.en module (http://www.clips. ua.ac.be/pages/pattern-en) for verbalization, which includes determining plurality of the argument, adding proper articles, and conjugating verbs.\nWe employ two different strategies for forward generation of inferences given any context. The sentence-prompt strategy uses the entire sentence in the context as an input, and generates output using greedy decoding. The word-prompt strategy differs by using only a single word from the context as input. This word is chosen in the same fashion as the AGCI-WK step (d), i.e. an argument of the context. See example in Fig 3. For both strategies, we later present the full context and decoded inference to crowdsource workers for annotation."}, {"heading": "4.2 Ordinal Label Annotation", "text": "In this stage, we turn to human efforts to annotate AGCI with ordinal labels. The annotator will be given a context, and then will be ask assess the likelihood of the inference being true. These contextinference pairs are annotated with one of the five labels: very likely, likely, plausible, technically possible, and impossible, corresponding to the ordinal values of {5,4,3,2,1} respectively.\nIn the case that the inference does not make sense, or has grammatical errors, judges can provide an additional label, NA, so that we can filter these inferences in post-processing. The combination of AGCI with human filtering seeks to avoid the problem of elicitation bias."}, {"heading": "5 JOCI Corpus", "text": "We now describe in depth how we created a largescale ordinal common-sense inference corpus. The main part of our corpus consists of contexts chosen from SNLI and ROCStories, paired with autogenerated inferences using methods described in \u00a7 4.1. These pairs are then annotated with ordinal labels using crowdsourcing (\u00a7 4.2). We also include context-inference pairs directly taken from SNLI and other corpora (e.g., as premise-hypothesis pairs), and re-annotate them with ordinal labels."}, {"heading": "5.1 Data sources for Context-Inference Pairs", "text": "In order to compare with existing inference corpora, we choose contexts from two resources: (1) the first sentence in the sentence pairs of the SNLI corpus (Bowman et al., 2015) which are captions from the Flickr30k corpus (Young et al., 2014), and (2) the first sentence in the stories of the ROCStories corpus (Mostafazadeh et al., 2016).\nWe then automatically generated common-sense inferences (AGCI) against these contexts. Specifically, in the SNLI train set, there are over 150K different first sentences, involving 7,414 different arguments according to predicate-argument extraction. We randomly choose 4,600 arguments. For each argument, we sample one first sentence that has the argument, and run AGCI against this as context. We also do the same generation for the SNLI dev set and test set. We also run AGCI against randomly sampled first sentences in the ROCStories corpus. Collectively, these pairs and their ordinal labels (to be described in \u00a7 5.2) make up the main part of the JOCI corpus. The statistics of this subset are shown in Table 1 (first five rows).\nFor comprehensiveness, we also produced ordinal labels on ( C , I ) pairs directly drawn from existing corpora. For SNLI, we randomly select 1000 contexts (premises) from the SNLI train set. Then, the corresponding inference is one of the entailment, neutral, or contradiction hypotheses taken from SNLI. For ROCStories, we defined C as the first sentence of the story, and I as the second or third sentence. For COPA, ( C , I ) corresponds to premise-effect. The statistics are shown in the bottom rows of Table 1."}, {"heading": "5.2 Crowdsourced Ordinal Label Annotation", "text": "We use Amazon Mechanical Turk to annotate the inferences with ordinal labels. In each HIT (Human Intelligence Task), a worker is presented with one context and one or two inferences, as shown in Fig 4. First, the annotator sees an \u201cInitial Sentence\u201d (context), e.g. \u201cJohn\u2019s goal was to learn how to draw well.\u201d, and is then asked about the plausibility of the inference, e.g. \u201cA person accomplishes the goal\u201d. In particular, we ask the annotator how plausible the inference is true during or shortly after, because without this constraint, most sentences are technically plausible in some imaginary world.\nIf the inference does not make sense11, the workers can check the box under the question and skip the ordinal annotation. In the annotation, about 25% of AGCI are marked as not making sense, and are removed from our data.\nWith the sampled contexts and the common-sense inferences, we prepare about 35K examples for crowdsourced annotation in bulk. In order to guarantee the quality of annotation, we have each example annotated by three workers. We take the median of the three as the ordinal label. Table 3 shows the statistics of the crowdsourced efforts.\n11\u201cNot making sense\u201d means that inferences that are incom-\nplete sentences or grammatically wrong.\nTo make sure non-expert workers have a correct understanding of our task, before launching the later tasks in bulk, we run two pilots to create a pool of qualified workers. In the first pilot, we publish 100 examples. Each example is annotated by five workers. From this pilot, we collect a set of \u201cgood\u201d examples which have 100% annotation agreement among workers. The ordinal labels chosen by the workers are regarded as the gold labels. In the second pilot, we randomly select two \u201cgood\u201d examples for each gold label (i.e., 12 \u201cgood\u201d examples for all six ordinal labels), and publish a HIT with these examples. To measure workers\u2019 agreement, we calculate the average of quadratic weighted Cohen\u2019s \u03ba scores between workers\u2019 annotation. By setting a threshold of the average of \u03ba scores to 0.7, we are able to create a pool that has over 150 qualified workers."}, {"heading": "5.3 Corpus Characteristics", "text": "In this section, we want to see the quality of the auto-generated common-sense inferences (AGCI). We are also interested in comparing AGCI with related resources under our annotation protocol to see its other characteristics such as the coverage on different ordinals of likelihood. Quality We measure the quality of each pair by calculating the standard deviation of workers\u2019 annotations. Fig 5 shows the growth of the size of AGCI as we decrease the threshold of the averaged \u03ba score to filter pairs. Even if we place a relatively strict threshold (> 0.6), we can still get a large subset of AGCI with over 15K pairs. We randomly sample several pairs from this subset. As shown in table 2, they are all high-quality common-sense inferences with different ordinal labels.\nLabel Distribution We compare the distribution\nof labels between our AGCI subset and our SNLI, ROCStories, and COPA subsets.\nFig 6a shows the normalized distribution of SNLI and AGCI, where AGCI covers a wide range of ordinal likelihoods. We believe datasets with wide support are important in training systems to recognize ordinal scale inferences.\nFig 6a also shows how traditional RTE labels are related to ordinal labels, although lots of inferences in the SNLI corpus are just textual inferences requiring no common-sense knowledge (e.g. paraphrases). Expectedly, entailments are mostly considered very likely, neutral inferences are mostly plausible, and contradictions are more likely to be either impossible or just technically possible, but there are also a considerable number of exceptions.\nFig 6b shows the normalized distributions of AGCI and ROCStories. Compared with ROCStories, AGCI still covers a wider range of ordinal likelihood. We observe in ROCStories that while 2nd\nsentences are in general more likely to be true than 3rd, a large proportion of both 2nd and 3rd sentences are plausible, as compared to likely or very likely. This matches intuition: pragmatics dictates that subsequent sentences in a standard narrative carry new information.12 That our protocol picks this up is an encouraging sign for our ordinal protocol, as well as suggestive that the makeup of the elicited ROCStories collection is indeed \u201cstory like\u201d.\nFor the COPA dataset, we make use only of the pairs in which the alternatives are plausible effects (rather than causes) of the premise, as our protocol more easily accommodates these pairs.13 Annotating this section of COPA with OCI labels pro-\n12I.e., if subsequent sentences in a story were always very likely, then those would be boring tales; the reader could infer the conclusion based on the introduction. While at the same time if most subsequent sentences were only technically possible, the reader would give up in confusion.\n13Specifically, we treat premises as contexts and effect alternatives as possible inferences.\nvides an enlightening and validating view of the dataset. Fig 6c shows the normalized distribution of COPA next to that of AGCI. (COPA-1 alternatives are marked as most plausible; COPA-0 are not.) True to its name, the majority of COPA alternatives are labeled as either plausible or likely; almost none are impossible. This is consistent with the idea that the COPA task is to determine which of two possible options is the more plausible. Fig 7 shows the joint distribution of ordinal labels on (COPA-0,COPA-1) pairs. As expected, the densest areas of the heatmap lie above the diagonal, indicating that in almost every pair, COPA-1 received a higher OCI score than COPA-0.\nAGCI Comparisons: We compare the label distributions of different AGCI methods in Fig 8. Among ACGI-WK methods, the ISA strategy yields a bimodal distribtuion, with the majority of inferences labeled impossible or very likely. This is likely because most copular statements generated with the ISA strategy will either be categorically true or false. In contrast, the decision tree and frequency based strategies generate many more inferences with intermediate ordinal labels. This suggests the propositional templates (learned from text) capture many \u201cpossibilistic\u201d inferences, as we aim to achieve.\nThe two AGCI-NN strategies show interesting differences in label distribution, as well. Sequence-tosequence decodings with full-sentence prompts lead to more very likely labels than single-word prompts. This is unsurprising, as the sequence-to-sequence model can make more precise inferences when it has access to all the information in the context. When combined, the five AGCI strategies (three AGCIWK and two AGCI-NN) provide reasonable coverage\nover all five categories, as can be seen in Fig 6."}, {"heading": "6 Predicting Ordinal Judgments", "text": "We would like to be able to predict ordinal judgments of the kind presented in this corpus. Our goal in this section is to establish baseline results and explore what kinds of features are useful for predicting common-sense ordinal inference. To do so, we train and test a logistic ordinal regression model g\u03b8(\u03c6(C, I)), which outputs ordinal labels using features \u03c6 defined on context-inference pairs. Here, g\u03b8(\u00b7) is a regression model with \u03b8 as trained parameters; we train using the margin-based method of (Rennie and Srebro, 2005), implemented in (Pedregosa-Izquierdo, 2015)14."}, {"heading": "6.1 Features", "text": "Bag of words features (BOW): We compute (1) \u201cBOW overlap\u201d (size of word overlap in C and I ), and (2) BOW overlap divided by the length of I . Similarity features (SIM): Using Google\u2019s word2vec vectors pretrained on 100 billion tokens of GoogleNews, we (1) sum the vectors in both\n14LogisticSE: http://github.com/fabianp/mord\nthe context and inference and compute the cosinesimilarity of the resulting two vectors (\u201csimilarity of average\u201d), and (2) compute the cosine-similarity of all word pairs across the context and inference, and compute an average of those similarities (\u201caverage of similarity\u201d). Seq2seq score features (S2S): We compute the log probability logP (I|C) under the sequence-tosequence model described in \u00a7 4.1.2. There are five variants: (1) Seq2seq trained on SNLI \u201centailment\u201d pairs only, (2) \u201cneutral\u201d pairs only, (3) \u201ccontradiction\u201d pairs only, (4) \u201cneutral\u201d and \u201ccontradiction\u201d pairs, and (5) SNLI pairs (any label) with the context (premise) replaced by an empty string. Seq2seq binary features (S2S-BIN): Binary indicator features for each of the five seq2seq model variants, indicating that model achieved the lowest score on the context-inference pair. Length features (LEN): This set comprises three features: the length of the context (in tokens), the difference in length between the context and inference, and a binary feature indicating if the inference is longer than the context."}, {"heading": "6.2 Analysis", "text": "We train and test our regression model on two subsets of the JOCI corpus, which, for brevity, we call \u201cA\u201d and \u201cB.\u201d \u201cA\u201d consists of 2,976 sentence pairs (i.e., context-inference pairs) from SNLI-train annotated with ordinal labels. This corresponds to the three rows labeled SNLI in Table 1 (993 + 988 + 995 = 2, 976 pairs), and can be viewed as a textual entailment dataset re-labeled with ordinal judgments. \u201cB\u201d consists of 6,375 context-inference pairs, in which the contexts are the same 2,976 SNLI-train premises as \u201cA\u201d, and the inferences are generated from different AGCI-WK methods; these pairs are also annotated with ordinal labels. This corresponds to a subset of the row labeled AGCI in Table 1. A key difference between \u201cA\u201d and \u201cB\u201d is that the inferences in \u201cA\u201d are human-elicited, while those in \u201cB\u201d are auto-generated; we are interested in seeing whether this affects the difficulty of the task.15\nFig 9 shows the performance (mean squared error) of predicting ordinal labels. The ordinal regres-\n15Details of the data split is reported in the dataset release.\nsion model g\u03b8(\u00b7) trained on the aforementioned features achieves 1.96 MSE on A-test, and 2.74 MSE on B-test. As reference, a model which selects the ordinal class that appears most often in the training data performs worse at 5.56 and 7.00 MSE (Most Frequent). Other comparisons include:\nFrequency Sampling: Select an ordinal label according to their distribution in train\nRounded Average: Average over all labels from train rounded to nearest ordinal.\nOne-vs-All: Train one SVM classifier per ordinal class and select the class label with the largest corresponding margin. We train this model with the same set of features as the ordinal regression model.\nOverall, the regression model achieves lower MSE, which implies that this dataset is learnable and tractable. Naturally, we would desire a model that achieves MSE under 1.0, and we hope that the release of our dataset will encourage more concerted effort in this common-sense inference task. Importantly, note that MSE on B-test is substantially higher than on A-test. We believe \u201cB\u201d is a more challenging dataset because auto-generation of inference leads to wider variety than elicitation.\nWe also run feature ablation tests to compare the importance of different features. In Fig 10, it is notable that on A-test and B-test, the most useful sets of features differ. On A-test, where the inferences are elicited from humans, the similarity- and bow-based features together yield the biggest gains. In contrast, similarity- and bow-based features offer little to no gain on B-test, in which the inferences have been automatically generated. Instead, most gains on B-test come from the sequence-tosequence based features. (Note that, as a result of the generation techniques in B-test, the value of bowoverlap is nearly always 1.) These observations belie systematic statistical differences between humanelicited and automatically generated inferences, a\nfundamental motivating point of the JOCI corpus."}, {"heading": "7 Conclusions and Future Work", "text": "In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote:\n\u201cChina launched a meteorological satellite into orbit Wednesday.\u201d suggests to a human reader that (among other things) there was a rocket launch; China probably owns the satellite; the satellite is for monitoring weather; the orbit is around Earth; etc\nThe use of \u201cetc\u201d summarizes an infinite number of other statements that a human reader would find to be very likely, likely, technically plausible, or impossible, given the provided context.\nPreferably we could build systems that would automatically learn common-sense exclusively from available corpora; extracting not just statements about what is possible, but also the associated probabilities of how likely certain things are to obtain in any given context. We are unaware of existing work that has demonstrated this to be feasible.\nWe have therefore described a multi-stage approach to common-sense textual inference: we first extract large amounts of possible statements from a corpus, and use those statements to generate contextually grounded context-inference pairs. These are presented to humans for direct assessment of subjective likelihood, rather than relying on corpus data alone. As the data is automatically generated, we\ncan bypass issues in human elicitation bias. Further, since subjective likelihood judgments are not difficult for humans, our crowdsourcing technique is inexpensive and could be scaled beyond what is already described here.\nFuture work will extend our techniques for forward inference generation, further scale up the annotation of additional examples, and explore the use of larger, more complex contexts."}], "references": [{"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings of the Sixth International Workshop on Semantic Evaluation.", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1,", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Strategies for Lifelong Knowledge Extraction from the Web", "author": ["Michele Banko", "Oren Etzioni."], "venue": "Proceedings of K-CAP.", "citeRegEx": "Banko and Etzioni.,? 2007", "shortCiteRegEx": "Banko and Etzioni.", "year": 2007}, {"title": "Montague meets markov: Deep semantics with probabilistic logical form", "author": ["Islam Beltagy", "Cuong Chau", "Gemma Boleda", "Dan Garrette", "Katrin Erk", "Raymond Mooney."], "venue": "2nd Joint Conference on Lexical and Computational Semantics: Proceeding of the", "citeRegEx": "Beltagy et al\\.,? 2013", "shortCiteRegEx": "Beltagy et al\\.", "year": 2013}, {"title": "Global learning of typed entailment rules", "author": ["Jonathan Berant", "Ido Dagan", "Jacob Goldberger."], "venue": "Proceedings of ACL.", "citeRegEx": "Berant et al\\.,? 2011", "shortCiteRegEx": "Berant et al\\.", "year": 2011}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Unsupervised Learning of Narrative Event Chains", "author": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of ACL.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "LEARNER: A System for Acquiring Commonsense Knowledge by Analogy", "author": ["Timothy Chklovski."], "venue": "Proceedings of Second International Conference on Knowledge Capture (K-CAP 2003).", "citeRegEx": "Chklovski.,? 2003", "shortCiteRegEx": "Chklovski.", "year": 2003}, {"title": "A knowledge-driven approach to text meaning processing", "author": ["Peter Clark", "Phil Harrison", "John Thompson."], "venue": "Proceedings of the HLT-NAACL 2003 workshop on Text meaning-Volume 9, pages 1\u20136. Association for Computational Linguistics.", "citeRegEx": "Clark et al\\.,? 2003", "shortCiteRegEx": "Clark et al\\.", "year": 2003}, {"title": "Bridging", "author": ["Herbert H. Clark."], "venue": "R. C. Schank and B. L. Nash-Webber, editors, Theoretical issues in natural language processing. Association for Computing Machinery, New York.", "citeRegEx": "Clark.,? 1975", "shortCiteRegEx": "Clark.", "year": 1975}, {"title": "The pascal recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment.", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Concretely Annotated Corpora", "author": ["Francis Ferraro", "Max Thomas", "Matthew R. Gormley", "Travis Wolfe", "Craig Harman", "Benjamin Van Durme."], "venue": "4th Workshop on Automated Knowledge Base Construction (AKBC).", "citeRegEx": "Ferraro et al\\.,? 2014", "shortCiteRegEx": "Ferraro et al\\.", "year": 2014}, {"title": "Project halo: Towards a digital aristotle", "author": ["Noah S Friedland", "Paul G Allen", "Gavin Matthews", "Michael Witbrock", "David Baxter", "Jon Curtis", "Blake Shepard", "Pierluigi Miraglia", "Jurgen Angele", "Steffen Staab"], "venue": "AI magazine,", "citeRegEx": "Friedland et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Friedland et al\\.", "year": 2004}, {"title": "Integrating logical representations with probabilistic information using markov logic", "author": ["Dan Garrette", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the Ninth International Conference on Computational Semantics, pages 105\u2013114. Association for Computa-", "citeRegEx": "Garrette et al\\.,? 2011", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "The fourth pascal recognizing textual entailment challenge", "author": ["Danilo Giampiccolo", "Hoa Trang Dang", "Bernardo Magnini", "Ido Dagan", "Elena Cabrio", "Bill Dolan."], "venue": "Proceedings of TAC 2008.", "citeRegEx": "Giampiccolo et al\\.,? 2008", "shortCiteRegEx": "Giampiccolo et al\\.", "year": 2008}, {"title": "Reporting bias and knowledge extraction", "author": ["Jonathan Gordon", "Benjamin Van Durme."], "venue": "Automated Knowledge Base Construction (AKBC): The 3rd Workshop on Knowledge Extraction at CIKM.", "citeRegEx": "Gordon and Durme.,? 2013", "shortCiteRegEx": "Gordon and Durme.", "year": 2013}, {"title": "ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge", "author": ["Catherine Havasi", "Robert Speer", "Jason Alonso."], "venue": "Proceedings of RANLP.", "citeRegEx": "Havasi et al\\.,? 2007", "shortCiteRegEx": "Havasi et al\\.", "year": 2007}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["Marti Hearst."], "venue": "Proceedings of COLING.", "citeRegEx": "Hearst.,? 1992", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Methodology for knowledge acquisition (unpublished manuscript)", "author": ["Jerry R. Hobbs", "Costanza Navarretta."], "venue": "http://www.isi.edu/h\u0303obbs/damage.text.", "citeRegEx": "Hobbs and Navarretta.,? 1993", "shortCiteRegEx": "Hobbs and Navarretta.", "year": 1993}, {"title": "Interpretation as abduction", "author": ["Jerry R Hobbs", "Mark Stickel", "Paul Martin", "Douglas Edwards."], "venue": "Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 95\u2013103. Association for Computational Linguistics.", "citeRegEx": "Hobbs et al\\.,? 1988", "shortCiteRegEx": "Hobbs et al\\.", "year": 1988}, {"title": "World Knowledge And Word Meaning", "author": ["Jerry R. Hobbs."], "venue": "Theoretical Issues In Natural Language Processing.", "citeRegEx": "Hobbs.,? 1987", "shortCiteRegEx": "Hobbs.", "year": 1987}, {"title": "Cyc: A large-scale investment in knowledge infrastructure", "author": ["Douglas B Lenat."], "venue": "Communications of the ACM, 38(11):33\u201338.", "citeRegEx": "Lenat.,? 1995", "shortCiteRegEx": "Lenat.", "year": 1995}, {"title": "The winograd schema challenge", "author": ["Hector J. Levesque", "Ernest Davis", "Leora Morgenstern."], "venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.", "citeRegEx": "Levesque et al\\.,? 2011", "shortCiteRegEx": "Levesque et al\\.", "year": 2011}, {"title": "From Trees to Predicate Argument Structures", "author": ["Maria Liakata", "Stephen Pulman."], "venue": "Proceedings of COLING.", "citeRegEx": "Liakata and Pulman.,? 2002", "shortCiteRegEx": "Liakata and Pulman.", "year": 2002}, {"title": "DIRT - Discovery of Inference Rules from Text", "author": ["Dekang Lin", "Patrick Pantel."], "venue": "Proceedings of KDD.", "citeRegEx": "Lin and Pantel.,? 2001", "shortCiteRegEx": "Lin and Pantel.", "year": 2001}, {"title": "Natural logic for textual inference", "author": ["Bill MacCartney", "Christopher D. Manning."], "venue": "Proceedings of ACL: Workshop on Textual Entailment and Paraphrasing.", "citeRegEx": "MacCartney and Manning.,? 2007", "shortCiteRegEx": "MacCartney and Manning.", "year": 2007}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Proceedings of the Ninth International Conference on Lan-", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Programs with common sense", "author": ["John McCarthy."], "venue": "Proceedings of the Teddington Conference on the Mechanization of Thought Processes, London: Her Majesty\u2019s Stationery Office.", "citeRegEx": "McCarthy.,? 1959", "shortCiteRegEx": "McCarthy.", "year": 1959}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["Ken McRae", "George S. Cree", "Mark S. Seidenberg", "Chris McNorgan."], "venue": "Behavior Research Methods, Instruments, & Computers, 37(4):547\u2013559.", "citeRegEx": "McRae et al\\.,? 2005", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels", "author": ["Ishan Misra", "C. Lawrence Zitnick", "Margaret Mitchell", "Ross Girshick."], "venue": "Proceedings of CVPR.", "citeRegEx": "Misra et al\\.,? 2016", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "author": ["Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen."], "venue": "Proceedings of the 2016 Confer-", "citeRegEx": "Mostafazadeh et al\\.,? 2016", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "What You Seek is What You Get: Extraction of Class Attributes from Query Logs", "author": ["Marius Pa\u015fca", "Benjamin Van Durme."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Pa\u015fca and Durme.,? 2007", "shortCiteRegEx": "Pa\u015fca and Durme.", "year": 2007}, {"title": "Isp: Learning inferential selectional preferences", "author": ["Patrick Pantel", "Rahul Bhagat", "Bonaventura Coppola", "Timothy Chklovski", "Eduard H Hovy."], "venue": "HLTNAACL, pages 564\u2013571.", "citeRegEx": "Pantel et al\\.,? 2007", "shortCiteRegEx": "Pantel et al\\.", "year": 2007}, {"title": "English gigaword fifth edition, june", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, LDC2011T07.", "citeRegEx": "Parker et al\\.,? 2011", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Turning web text and search queries into factual knowledge: Hierarchical class attribute extraction", "author": ["Marius Pasca"], "venue": null, "citeRegEx": "Pasca.,? \\Q2008\\E", "shortCiteRegEx": "Pasca.", "year": 2008}, {"title": "Feature extraction and supervised learning on fMRI: from practice to theory", "author": ["Fabian Pedregosa-Izquierdo."], "venue": "Ph.D. thesis, Universit\u00e9 Pierre et Marie CurieParis VI.", "citeRegEx": "Pedregosa.Izquierdo.,? 2015", "shortCiteRegEx": "Pedregosa.Izquierdo.", "year": 2015}, {"title": "Learning statistical scripts with lstm recurrent neural networks", "author": ["Karl Pichotta", "Raymond J Mooney."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Pichotta and Mooney.,? 2016", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2016}, {"title": "Loss functions for preference levels: Regression with discrete ordered labels", "author": ["Jason DM Rennie", "Nathan Srebro."], "venue": "Proceedings of the IJCAI multidisciplinary workshop on advances in preference handling, pages 180\u2013186. Kluwer Norwell, MA.", "citeRegEx": "Rennie and Srebro.,? 2005", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Markov logic networks", "author": ["Matthew Richardson", "Pedro Domingos."], "venue": "Machine learning, 62(12):107\u2013136.", "citeRegEx": "Richardson and Domingos.,? 2006", "shortCiteRegEx": "Richardson and Domingos.", "year": 2006}, {"title": "MindNet: Acquiring and Structuring Semantic Information from Text", "author": ["Stephen D. Richardson", "William B. Dolan", "Lucy Vanderwende."], "venue": "Proceedings of ACL.", "citeRegEx": "Richardson et al\\.,? 1998", "shortCiteRegEx": "Richardson et al\\.", "year": 1998}, {"title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning", "author": ["Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S. Gordon."], "venue": "AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning, Stanford Univer-", "citeRegEx": "Roemmele et al\\.,? 2011", "shortCiteRegEx": "Roemmele et al\\.", "year": 2011}, {"title": "Is the stanford dependency representation semantic? In ACL Workshop: EVENTS", "author": ["Rachel Rudinger", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Rudinger and Durme.,? \\Q2014\\E", "shortCiteRegEx": "Rudinger and Durme.", "year": 2014}, {"title": "Script induction as language modeling", "author": ["Rachel Rudinger", "Pushpendre Rastogi", "Francis Ferraro", "Benjamin Van Durme."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Rudinger et al\\.,? 2015", "shortCiteRegEx": "Rudinger et al\\.", "year": 2015}, {"title": "Using knowledge to understand", "author": ["Roger C. Schank."], "venue": "TINLAP \u201975: Proceedings of the 1975 workshop on Theoretical issues in natural language processing.", "citeRegEx": "Schank.,? 1975", "shortCiteRegEx": "Schank.", "year": 1975}, {"title": "Can we derive general world knowledge from texts", "author": ["Lenhart Schubert"], "venue": null, "citeRegEx": "Schubert.,? \\Q2002\\E", "shortCiteRegEx": "Schubert.", "year": 2002}, {"title": "The public acquisition of commonsense knowledge", "author": ["Push Singh."], "venue": "Proceedings of AAAI Spring Symposium: Acquiring (and Using) Linguistic (and World) Knowledge for Information Access. AAAI.", "citeRegEx": "Singh.,? 2002", "shortCiteRegEx": "Singh.", "year": 2002}, {"title": "Semantic Taxonomy Induction from Heterogenous Evidence", "author": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "Proceedings of COLING-ACL.", "citeRegEx": "Snow et al\\.,? 2006", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."], "venue": "Proceedings of WWW.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Discourse context and sentence perception", "author": ["Michael K. Tanenhaus", "Mark S. Seidenberg."], "venue": "Technical Report 176, Center for the Study of Reading, Illinois University, Urbana.", "citeRegEx": "Tanenhaus and Seidenberg.,? 1980", "shortCiteRegEx": "Tanenhaus and Seidenberg.", "year": 1980}, {"title": "Open knowledge extraction through compositional language processing", "author": ["Benjamin Van Durme", "Lenhart Schubert."], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, pages 239\u2013254. Association for Computational Linguistics.", "citeRegEx": "Durme and Schubert.,? 2008", "shortCiteRegEx": "Durme and Schubert.", "year": 2008}, {"title": "Deriving generalized knowledge from corpora using WordNet abstraction", "author": ["Benjamin Van Durme", "Phillip Michalak", "Lenhart Schubert."], "venue": "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 808\u2013816, Athens,", "citeRegEx": "Durme et al\\.,? 2009", "shortCiteRegEx": "Durme et al\\.", "year": 2009}, {"title": "Extracting implicit knowledge from text", "author": ["Benjamin Van Durme."], "venue": "Ph.D. thesis, University of Rochester, Department of Computer Science, Rochester, NY 14627-0226.", "citeRegEx": "Durme.,? 2010", "shortCiteRegEx": "Durme.", "year": 2010}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28,", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "\u2013 Hobbs (1987)", "startOffset": 2, "endOffset": 15}, {"referenceID": 10, "context": ", many of the bridging inferences of Clark (1975) make use of common-sense knowledge, such as the following example of \u201cProbable part\u201d: I walked into the room.", "startOffset": 37, "endOffset": 50}, {"referenceID": 44, "context": "rately considered models of language, rather than of the world (Rudinger et al., 2015).", "startOffset": 63, "endOffset": 86}, {"referenceID": 51, "context": "For further background see discussions by Van Durme (2010), Gordon and Van Durme (2013), Rudinger et al.", "startOffset": 46, "endOffset": 59}, {"referenceID": 51, "context": "For further background see discussions by Van Durme (2010), Gordon and Van Durme (2013), Rudinger et al.", "startOffset": 46, "endOffset": 88}, {"referenceID": 43, "context": "For further background see discussions by Van Durme (2010), Gordon and Van Durme (2013), Rudinger et al. (2015) and Misra et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 31, "context": "(2015) and Misra et al. (2016). 2 Background", "startOffset": 11, "endOffset": 31}, {"referenceID": 19, "context": "Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable time and expense (Miller, 1995; Lenat, 1995; Baker et al.", "startOffset": 112, "endOffset": 140}, {"referenceID": 30, "context": "Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004).", "startOffset": 179, "endOffset": 250}, {"referenceID": 22, "context": "Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004).", "startOffset": 179, "endOffset": 250}, {"referenceID": 2, "context": "Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004).", "startOffset": 179, "endOffset": 250}, {"referenceID": 13, "context": "Mining Common Sense Building large collections of common-sense knowledge can be done manually via professionals (Hobbs and Navarretta, 1993), but at considerable time and expense (Miller, 1995; Lenat, 1995; Baker et al., 1998; Friedland et al., 2004).", "startOffset": 179, "endOffset": 250}, {"referenceID": 47, "context": "Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor.", "startOffset": 32, "endOffset": 66}, {"referenceID": 17, "context": "Efforts have pursued volunteers (Singh, 2002; Havasi et al., 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor.", "startOffset": 32, "endOffset": 66}, {"referenceID": 8, "context": ", 2007) and games with a purpose (Chklovski, 2003), but are still left fully reliant on human labor.", "startOffset": 33, "endOffset": 50}, {"referenceID": 18, "context": "Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al.", "startOffset": 83, "endOffset": 116}, {"referenceID": 48, "context": "Many have pursued automating the process, such as in expanding lexical hierarchies (Hearst, 1992; Snow et al., 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al.", "startOffset": 83, "endOffset": 116}, {"referenceID": 25, "context": ", 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al.", "startOffset": 41, "endOffset": 84}, {"referenceID": 5, "context": ", 2006), constructing inference patterns (Lin and Pantel, 2001; Berant et al., 2011), reading reference materials (Richardson et al.", "startOffset": 41, "endOffset": 84}, {"referenceID": 41, "context": ", 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pa\u015fca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al.", "startOffset": 37, "endOffset": 85}, {"referenceID": 49, "context": ", 2011), reading reference materials (Richardson et al., 1998; Suchanek et al., 2007), mining search engine query logs (Pa\u015fca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al.", "startOffset": 37, "endOffset": 85}, {"referenceID": 46, "context": ", 2007), mining search engine query logs (Pa\u015fca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007).", "startOffset": 171, "endOffset": 258}, {"referenceID": 24, "context": ", 2007), mining search engine query logs (Pa\u015fca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007).", "startOffset": 171, "endOffset": 258}, {"referenceID": 9, "context": ", 2007), mining search engine query logs (Pa\u015fca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007).", "startOffset": 171, "endOffset": 258}, {"referenceID": 3, "context": ", 2007), mining search engine query logs (Pa\u015fca and Van Durme, 2007), and most relevant here: abstracting from instance-level predications discovered in descriptive texts (Schubert, 2002; Liakata and Pulman, 2002; Clark et al., 2003; Banko and Etzioni, 2007).", "startOffset": 171, "endOffset": 258}, {"referenceID": 6, "context": ", 1996), or they rely on crowdsourced elicitation (Bowman et al., 2015).", "startOffset": 50, "endOffset": 71}, {"referenceID": 22, "context": ", the Winograd Schema Challenge discussed by Levesque et al. (2011). The data for these tasks are either smaller, carefully constructed evaluation sets by professionals, following efforts such the construction of the FRACAS test suite (Cooper et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 27, "context": "ent people of the same image), which were modified through a series of rule-based transformations then judged by humans (Marelli et al., 2014).", "startOffset": 120, "endOffset": 142}, {"referenceID": 11, "context": "Textual Entailment A multi-year source of textual inference examples were generated under the Recognizing Textual Entailment (RTE) Challenges, introduced by Dagan et al. (2006):", "startOffset": 157, "endOffset": 177}, {"referenceID": 15, "context": "While Giampiccolo et al. (2008) extended binary RTE with an \u201cunknown\u201d category, the entailment community has primarily focussed on issues such as paraphrase and monotonicity (such as captured by MacCartney and Manning (2007)).", "startOffset": 6, "endOffset": 32}, {"referenceID": 15, "context": "While Giampiccolo et al. (2008) extended binary RTE with an \u201cunknown\u201d category, the entailment community has primarily focussed on issues such as paraphrase and monotonicity (such as captured by MacCartney and Manning (2007)).", "startOffset": 6, "endOffset": 225}, {"referenceID": 40, "context": "(2013) introduced weighted inference rules based words/phrases similarity, and treated textual entailment as probabilistic logical inference in Markov Logic Networks (Richardson and Domingos, 2006).", "startOffset": 166, "endOffset": 197}, {"referenceID": 4, "context": "(2011) and Beltagy et al. (2013) introduced weighted inference rules based words/phrases similarity, and treated textual entailment as probabilistic logical inference in Markov Logic Networks (Richardson and Domingos, 2006).", "startOffset": 11, "endOffset": 33}, {"referenceID": 4, "context": "(2011) and Beltagy et al. (2013) introduced weighted inference rules based words/phrases similarity, and treated textual entailment as probabilistic logical inference in Markov Logic Networks (Richardson and Domingos, 2006). Supported by Hobbs et al. (1988), who proposed viewing natural language interpretation as abductive inference, their approach to weighted inference is to measure how likely H is best entailed, i.", "startOffset": 11, "endOffset": 258}, {"referenceID": 0, "context": "Agirre et al. (2012) piloted a Textual Similarity evaluation which has been refined in each subsequent year: systems produce scalar values corresponding to predictions of how similar the meaning is between two provided sentences.", "startOffset": 0, "endOffset": 21}, {"referenceID": 42, "context": "The Choice of Plausible Alternative (COPA) task (Roemmele et al., 2011) was a reaction to RTE, similarly motivated to probe a system\u2019s ability to understand inferences that are not strictly entailed: a single context was provided, with two alternative inferences, and a system had to judge which was more plausible.", "startOffset": 48, "endOffset": 71}, {"referenceID": 7, "context": "The Narrative Cloze task (Chambers and Jurafsky, 2008) requires a system to score candidate inferences as to how likely they are to appear in a document that also included the provided context.", "startOffset": 25, "endOffset": 54}, {"referenceID": 32, "context": "The ROCStories corpus (Mostafazadeh et al., 2016) elicited a more \u201cplausible\u201d collection of documents in order to retain the narrative cloze in the context of commonsense inference; we consider this dataset in \u00a7 5.", "startOffset": 22, "endOffset": 49}, {"referenceID": 38, "context": "Alongside the narrative cloze, Pichotta and Mooney (2016) made use of a 5-point Likert scale (very likely to very unlikely) as a secondary evaluation of various script induction techniques.", "startOffset": 31, "endOffset": 58}, {"referenceID": 11, "context": "OCI embraces the notion of Dagan et al. (2006), in that we are concerned with human judgements of whether something is likely.", "startOffset": 27, "endOffset": 47}, {"referenceID": 46, "context": "Our motivation for this approach was first introduced by Schubert (2002):", "startOffset": 57, "endOffset": 73}, {"referenceID": 46, "context": "Following Schubert (2002) and Van Durme and Schubert (2008), we define an approach for abstracting over explicit assertions derived from corpora, leading to a large-scale collection of general possibilistic statements.", "startOffset": 10, "endOffset": 26}, {"referenceID": 46, "context": "Following Schubert (2002) and Van Durme and Schubert (2008), we define an approach for abstracting over explicit assertions derived from corpora, leading to a large-scale collection of general possibilistic statements.", "startOffset": 10, "endOffset": 60}, {"referenceID": 53, "context": "Following Rudinger and Van Durme (2014), we define a series of conversion rules for post-processing a sentential syntactic dependency tree into zero or more predicate argument structures.", "startOffset": 27, "endOffset": 40}, {"referenceID": 35, "context": "We use the Gigaword corpus (Parker et al., 2011) for extracting propositions as it is a comprehensive text archive.", "startOffset": 27, "endOffset": 48}, {"referenceID": 12, "context": "There exists a version containing automatically generated syntactic annotation (Ferraro et al., 2014), which bootstraps large-scale knowledge extraction.", "startOffset": 79, "endOffset": 101}, {"referenceID": 30, "context": "Here we choose WordNet (Miller, 1995) noun synsets8 as the semantic-class set.", "startOffset": 23, "endOffset": 37}, {"referenceID": 49, "context": ") When selecting the correct sense for an argument, we adopt a fast and relatively accurate method: always taking the first sense which is usually the most commonly used sense (Suchanek et al., 2007; Pasca, 2008).", "startOffset": 176, "endOffset": 212}, {"referenceID": 36, "context": ") When selecting the correct sense for an argument, we adopt a fast and relatively accurate method: always taking the first sense which is usually the most commonly used sense (Suchanek et al., 2007; Pasca, 2008).", "startOffset": 176, "endOffset": 212}, {"referenceID": 52, "context": "The abstracted propositions are turned into templates by replacing the sense\u2019s corresponding argument with a placeholder, similar to Van Durme et al. (2009). (See Fig 2 (b).", "startOffset": 137, "endOffset": 157}, {"referenceID": 34, "context": "In order to avoid too general senses, we set cut points at the depth of 4 (Pantel et al., 2007) to truncate the hierarchy and consider all 81,861 senses below these points.", "startOffset": 74, "endOffset": 95}, {"referenceID": 45, "context": "PyStanfordDependencies See Schubert (2002) for detail.", "startOffset": 27, "endOffset": 43}, {"referenceID": 50, "context": "(d) Generating inference candidates: As shown in Fig 2 (d), given a discourse context (Tanenhaus and Seidenberg, 1980), we first extract an argument of the context, then select the derived properties for the argument.", "startOffset": 86, "endOffset": 118}, {"referenceID": 54, "context": "As an alternative to the knowledge-based methods described above, we also adapt a neural sequence-tosequence model (Vinyals et al., 2015; Bahdanau et al., 2014) to directly generate inferences given contexts.", "startOffset": 115, "endOffset": 160}, {"referenceID": 1, "context": "As an alternative to the knowledge-based methods described above, we also adapt a neural sequence-tosequence model (Vinyals et al., 2015; Bahdanau et al., 2014) to directly generate inferences given contexts.", "startOffset": 115, "endOffset": 160}, {"referenceID": 6, "context": "The model is trained on sentence pairs labeled \u201centailment\u201d from the train set of the SNLI corpus (Bowman et al., 2015).", "startOffset": 98, "endOffset": 119}, {"referenceID": 6, "context": "In order to compare with existing inference corpora, we choose contexts from two resources: (1) the first sentence in the sentence pairs of the SNLI corpus (Bowman et al., 2015) which are captions from the Flickr30k corpus (Young et al.", "startOffset": 156, "endOffset": 177}, {"referenceID": 55, "context": ", 2015) which are captions from the Flickr30k corpus (Young et al., 2014), and (2) the first sentence in the stories of the ROCStories corpus (Mostafazadeh et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 32, "context": ", 2014), and (2) the first sentence in the stories of the ROCStories corpus (Mostafazadeh et al., 2016).", "startOffset": 76, "endOffset": 103}, {"referenceID": 39, "context": "Here, g\u03b8(\u00b7) is a regression model with \u03b8 as trained parameters; we train using the margin-based method of (Rennie and Srebro, 2005), implemented in (Pedregosa-Izquierdo, 2015)14.", "startOffset": 106, "endOffset": 131}, {"referenceID": 37, "context": "Here, g\u03b8(\u00b7) is a regression model with \u03b8 as trained parameters; we train using the margin-based method of (Rennie and Srebro, 2005), implemented in (Pedregosa-Izquierdo, 2015)14.", "startOffset": 148, "endOffset": 175}, {"referenceID": 9, "context": "In motivating the need for automatically building collections of common-sense knowledge, Clark et al. (2003) wrote:", "startOffset": 89, "endOffset": 109}], "year": 2016, "abstractText": "Humans have the capacity to draw commonsense inferences from natural language: various things that are likely but not certain to hold based on established discourse, and are rarely stated explicitly. We propose an evaluation of automated common-sense inference based on an extension of recognizing textual entailment: predicting ordinal human responses of subjective likelihood of an inference holding in a given context. We describe a framework for extracting common-sense knowledge for corpora, which is then used to construct a dataset for this ordinal entailment task, which we then use to train and evaluate a sequence to sequence neural network model. Further, we annotate subsets of previously established datasets via our ordinal annotation protocol in order to then analyze the distinctions between these and what we have constructed.", "creator": "LaTeX with hyperref package"}}}