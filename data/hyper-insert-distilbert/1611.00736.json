{"id": "1611.00736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Extensions and Limitations of the Neural GPU", "abstract": "the neural gpu is thus a recent instructional model platform that can learn algorithms regarding such languages as multi - arbitrary digit binary addition and repetitive binary multiplication in a large way backwards that primarily generalizes calculus to inputs of arbitrary length. we show easily that there must are two simple ways of improving the performance curve of the mature neural gpu : by by carefully assessing designing to a curriculum, and by increasing model size. the latter requires applying careful memory management, as a naive implementation implementation of calculating the artificial neural gpu is sufficiently memory requirement intensive. likewise we consequently find that for these techniques to increase than the required set weights of algorithmic instruction problems above that may can be solved by the neural gpu : we have been able to learn to perform all the arithmetic manipulation operations ( and generalize strategies to arbitrarily require long short numbers ) when accepting the arguments measured are significantly given up in assuming the decimal representation ( which, surprisingly, has not been possible before ). but we have also arguably been able practically to train out the neural gpu to spontaneously evaluate long delayed arithmetic expressions with suitable multiple operands that require respecting the precedence order bound of the paired operands, typically although certainly these have literally succeeded only accurately in their own binary representation, though and still not compete with an 100 \\ % accuracy.", "histories": [["v1", "Wed, 2 Nov 2016 19:18:17 GMT  (1804kb,D)", "http://arxiv.org/abs/1611.00736v1", null], ["v2", "Fri, 4 Nov 2016 20:46:40 GMT  (2731kb,D)", "http://arxiv.org/abs/1611.00736v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["eric price", "wojciech zaremba", "ilya sutskever"], "accepted": false, "id": "1611.00736"}, "pdf": {"name": "1611.00736.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wojciech Zaremba", "Ilya Sutskever"], "emails": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Networks (DNNs) are extremely useful for solving difficult pattern recognition tasks for two reasons: first, because they can compactly represent good solutions to difficult pattern recognition tasks; and second, because these good solutions can be found with stochastic gradient descent. It is not immediately obvious that a DNN should have the ability to represent solutions to such problems compactly. This is the case because the depth and width of DNNs allows them to simulate any parallel computer that runs for a modest number of steps, making it possible for the DNN to match the performance of any parallelizeable statistical ML model by simply simulating it. This is one of the reasons DNNs are successful relative to other ML models.\nDNNs are especially useful in the supervised learning setting, where the goal is achieve low test error over a given data distribution. Statistical learning theory (Vapnik, 2013) guarantees that this can be done by minimizing training error, as long as the training data is drawn from the same data distribution and when there is more training data than parameters.\nWhile the ability to achieve low test error on a specific data distribution is extremely useful in practice and has already enabled a wide range of practical applications, there is evidence that this ability does not capture the full extent of the intuitive notion of pattern recognition. For example, the existence of adversarial examples, which are data cases that are nearly indistinguishable from real data cases that are able to confuse all existing discriminative classifiers by being outside the data distribution, suggests that supervised DNNs are substantially less robust than human pattern recognition: we would expect a system that has fully \u201cunderstood\u201d the relevant visual (say) concepts\n1Work done while at OpenAI.\nar X\niv :1\n61 1.\n00 73\n6v 1\n[ cs\n.N E\n] 2\nN ov\n2 01\n6\nto not be fooled by adversarial examples. Understanding and fixing this problem is an active ongoing research area.\nAnother domain where low error on a specific data distribution seems unsatisfying is the domain of simple algorithms. Simple algorithms have a well-defined output for all conceivable inputs, and if we collect a number of input-output examples from some distribution where the outputs are computed by some (unknown) simple algorithm, a sufficiently good learning method ought to be able to infer the \u201ctrue\u201d algorithm: one that can perfectly generalize to all possible inputs, and not just the inputs that tend to occur in the input data distribution.\nThis problem lies at the core of program induction, an old field that has significant past work (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964; Holland, 1992; Goldberg, 1989; Gomez et al., 2008). More recently, researchers have begun investigating this problem using the deep learning techniques of neural network function approximation and stochastic gradient descent (Graves et al., 2014; Zaremba & Sutskever, 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Andrychowicz & Kurach, 2016). All these works have been able to learn algorithms that correctly generalize to inputs of much greater length than the training cases on some problems. The Neural GPU (Kaiser & Sutskever, 2015) is notable among these since it is the only model that has, thus far, been able to learn to correctly multiply integers of length much greater than it has been trained on.\nThe phenomenon of generalization to inputs that are outside the training distribution is poorly understood. The space of problems for which such generalization is possible has not been identified, and a detailed explanation of the reasons why such generalization should succeed is missing as well. Given that the test inputs do not come from the training data distribution, we do not have a formal reason to believe that such out-of-distribution generalization should actually succeed.\nIn this paper, we attempt to better understand this generalization in the context of the Neural GPU. We empirically study the parameters that affect its probability of successfully generalizing to inputs much greater length, and also study its failures. We report three notable findings: first, that larger models can learn harder tasks; second, that very detailed curriculum can enable the training of otherwise un-trainable neural networks; and third, those models that achieve perfect performance on test cases from the uniform distribution over much longer inputs may still fail on highly structured inputs. This suggests that these models fail to learn the \u201ctrue algorithm\u201d as well as we\u2019ve hoped, and that additional research is required for to learn models that can generalize much better. Such structured examples are reminiscent of adversarial examples for images.\nThe code for this work can be found at https://github.com/openai/ ecprice-neural-gpu."}, {"heading": "2 RELATED WORK", "text": "The problem of learning algorithms from data has been investigated in the field of program synthesis (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964) and genetic programming (Holland, 1992; Goldberg, 1989; Gomez et al., 2008). These approaches usually aim to directly produce the source code of an algorithm that solves the problem specified by the training data.\nA recent approach to algorithm learning attempts to use the power of neural networks and their learning algorithm. Neural networks are inherently robust, and are capable of dealing with \u201cimprecise\u201d data (like images or text) that can be difficult to handle in models that directly work with source code. There exist a number of neural network architecture that implement this idea: the Neural Turing Machine (NTM) (Graves et al., 2014), learning programs with LSTM (Zaremba & Sutskever, 2014), grid LSTM (Kalchbrenner et al., 2015), the Stack RNN (Joulin & Mikolov, 2015), the Neural DeQue (Grefenstette et al., 2015), End-to-End Memory Networks (Sukhbaatar et al., 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), Neural random-access machines (Kurach et al., 2015).\nFor a neural model to be able to learn an algorithm, it is essential that it is capable of running the necessary number of computational steps. Most of the above models have only been successfully used to learn algorithms that require a linear number of computational steps in the size of the in-\nput. While some models (Zaremba & Sutskever, 2015; Zaremba et al., 2015; Graves, 2016) can in principle learn the correct runtime for a given algorithm, in practice it has not been possible to learn algorithms requiring superlinear runtime, such as integer multiplication. The only known neural model that can solve tasks whose runtime is truly superlinear in the size of the input is the Neural GPU (Kaiser & Sutskever, 2015), which is the model that we investigate further in this paper.\nThe Grid LSTM (Kalchbrenner et al., 2015) is a powerful architecture that can learn 15-digit decimal addition. This model is similar to the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent. The Neural GPU has been shown to be able to generalize outside its training distribution, and while this has not been shown for the Grid LSTM, we believe that with appropriate modification, it should be capable of such generalization as well.\nNeural network models that are capable of learning and representing algorithms tend to be extremely deep and have an elaborate architecture, which makes the problem of minimizing their training error to be very challenging for stochastic gradient descent. To reduce the difficulty of the training problem, curriculum learning has been found to be critical. In particular, all the aforementioned models require a curriculum learning (Bengio et al., 2009; Zaremba & Sutskever, 2014) in order to successfully learn sophisticated functions, and the results in this paper are no different."}, {"heading": "3 MODEL", "text": "The Neural GPU is a recent architecture developed by Kaiser & Sutskever (2015) that can learn algorithms such as binary multi-digit addition and multi-digit multiplication. The main idea of the Neural GPU is to build an architecture capable of performing the necessary computation for the task without being overly deep. By being less deep, the problem of minimizing its training error becomes easier. Tasks that require super-linear number of computational steps (in the size of the input) cannot be solved by a neural architecture that can only perform a linear number of steps. Table 1 lists the number of computational steps that can be performed by several different neural network architectures. In our notation, a feed forward network consumes input of a predefined, constant size O ( 1 ) , and performs a fixed number of computation steps, O ( 1 ) . Other architectures, such as\nclassical convolution networks, also have a predefined size O ( 1 ) , and work only with fixed number\nof operations O ( 1 ) . However, it is possible to apply the convolution operation to an input of variable size. This approach is sometimes used in object detection, where the same convolutional neural network is applied on images of variable size (Sermanet et al., 2013). Similarly, the recurrent neural network (RNN) is capable of processing inputs of variable length, where the amount of computation performed by the RNN is linear in the length of the sequence.\nThe Neural GPU architecture is the combination of a convolution on variable size inputs with a recurrent neural network Fig. 1. The Neural GPU consumes an input of a variable length n. It repeatedly applies several convolutional layers n times, where n is the length of the input; thus, the depth of the model is dependent on the length of the input. It performs O ( n2 )\noperations for each input of the length O ( n ) . This is a desirable property, because is allows the possibility of solving problems that are believed to be unsolvable using a linear number of computational steps, such as\ninteger multiplication. Harder problems may require vastly more computational steps. While we could, in principle, unroll our models for an enormous number of timesteps to make them capable of solving even NP-hard problems, it is exceedingly unlikely that gradient learning would succeed with such depth.\nThe architecture most similar to Neural GPU is the Grid LSTM (Kalchbrenner et al., 2015). It has been shown to learn 15 digit long decimal addition task, although it has not yet been shown to generalize to inputs of length greater than the training data.\nTo successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques:\n\u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al., 2014).\n\u2022 Tanh cutoff: the hyperbolic tangent activations used by the GRU are truncated once they reach a critical upper (and lower) bound. The hope is that this makes the results more \u201cdigital\u201d.\n\u2022 The use of Dropout Srivastava et al. (2014). \u2022 Instead of using the same weights at every pair of layers, the Neural GPU starts out by\ncycling through 6 independent sets of weights, which are gradually annealed to become identical as optimization progresses.\n\u2022 Gradient noise: the original paper notes it as important, but the released code has this option disabled.\nIn this paper, we use all these techniques and analyze their importance Fig. 2. More details about the meaning of each modification are available in the original work (Kaiser & Sutskever, 2015)."}, {"heading": "4 IMPROVEMENTS TO THE NEURAL GPU", "text": "The performance obtained by the Neural GPU is very sensitive to the random seed used to produce the initialization and the order of the training data. For example, when we ran several identical models with different random seeds, we find, similarly to Kaiser & Sutskever (2015), that only a fraction of them generalizes to over 99% cases (where the test cases are of length much greater than the training cases, and the model is considered to have made an error when it mispredicts even one bit of the output vector). Such variability makes it hard to hill-climb in hyperparameter space. We address this problem by running \u2248 15 instances of the each model with different seeds on each of our 13 tasks. We measure the fraction of seeds that cause the model to surpass 99% accuracy on the (longer and out of distribution) test data. This fraction provides us with a reliable estimate of the quality of the hyperparameter configuration. Moreover, such measurements had to be done on several problems. Particular \u201ctricks\u201d often are effective just for a single problem, and damage performance on others.\nFig. 2 shows the success rate for a given size and model modification. We find that many model modifications help reduce training error but do not with generalization. Note that most changes to the model increase the success rate by no more than 10%, which is too small to be detected without the thorough experimental framework that we\u2019ve outlined above.\nOur experiments in Fig. 2 show that the simplest way to increase performance is to increase the size of the model. The rows of Table 2 consistently show that larger models are more likely to generalize. In contrast, smaller models are often unable to minimize training error. For example,\nwe were not able to achieve 0% training error when models with 128 and 256 filters are trained on decimal multiplication task (left Fig. 3), while the larger models with 512 filters are achieve 0% training error (although they do not generalize to long test inputs since curriculum was not used). It is not self-evident that larger model would generalize better, but the empirical results are clear. We suspect that the over-parameterization of the model makes the optimization task easier.\n0 50k 100k 150k 200k 0%\n20%\n40%\n60%\n80%\n100%\nTr ai\nni ng\ner ro\nr\n0 50k 100k 150k 200k Steps of training\n0%\n20%\n40%\n60%\n80%\n100%\nTe st\ner ro\nr\nDecimal multiplication is hard to train end-to-end\n128 filters 256 filters\n512 filters\n0 50k 100k 150k 200k 0%\n20%\n40%\n60%\n80%\n100%\nTr ai\nni ng\ner ro\nr\n256 filters\n0 50k 100k 150k 200k Steps of training\n0%\n20%\n40%\n60%\n80%\n100%\nTe st\ner ro\nr\nDecimal multiplication is easier with curriculum\nBase 10 Base 2 to 10\nBase 2 to 5 to 10 Base 2 to 4 to 10\nSize and curriculum both help\nRunning models with 512 filters is challenging, because they do not fit into the memory of the GPU on the problems that we have been exploring. In its current form, a Neural GPU with 128 filters does not fit into GPU memory (12 GB) due to its large number of activations. To train larger models, we used several techniques for reducing the model\u2019s memory footprint. First, instead of unrolling the graph, we used TensorFlow\u2019s symbolic tf.while loop, which can store intermediate activations on CPU thanks to the swap memory option (implemented in tensorflow (Abadi, 2015)). Second, we have decreased the batch size, which further lowered the memory footprint. A third way would be to use the methods described by Martens & Sutskever (2012); Gruslys et al. (2016), however we have not experimented with them.\nIn its current form, the Neural GPU cannot reliably learn to solve hard tasks such as decimal multiplication from examples (although sporadically it does). We find that an elaborate curriculum is necessary in order to learn such problems reliably, and to solve harder tasks. Our curriculum involves starting with short examples first, and solving intermediate task on the way to solve the target task. For instance, a curriculum for decimal multiplication could consist of first learning multiplication in base 2, then base 4, and finally base 10. The second plot in Fig. 3 shows the training and test error for models trained with curriculum. The third plot in Fig. 3 presents results for different curriculum types and model sizes. Table 2 summarizes the success rates for different model sizes, and curriculum.\nWe have obtained other notable results on the 3-numbers multiplication task Fig. 4. We were not able to reach perfect error when training on such the task without a curriculum. We found that training on multiplication of 2 numbers and then moving to multiplication of 3 numbers improved performance. However, such model doesn\u2019t generalize. The best performance on the 3-numbers multiplication is achieved when each training instance has a random l numbers, where l is chosen between 1 and bn\u221212 c. Another experiment is to train a model on sequences of arithmetic operations with multiple numbers. We train on expressions of length 41, and test on expressions of length 201. In binary, our model is able to solve 30% of length-201 expressions, where as always we count success only when the whole number is predicted correctly. For example, our trained model correctly predicts that 001110111001/1+10-10*0/1/1*111/10*010+0/010011-10/10101*0+ 010/1*00-110*1*0/1/101000- 00000+ 000-1+ 1-1011111*010-010/0011111011-010-1100-0/0010000*01*0010000+ 0111110+ 00001+10/10*111111111-10*10-1*11111+01\nshould be 10100001011. It\u2019s conceivable that better curriculum is needed. Fig. 5 summarizes this result."}, {"heading": "5 GENERALIZATION", "text": "Addition is a well defined algorithm, so knowledge of its mechanics is sufficient to add arbitrary numbers. Our trained model generalize perfectly to > 99% of uniformly random test cases (having a single digit error in the whole 100-digit number is considered an error). However, we found that it fails on a much larger fraction of \u201cstructured\u201d examples. For example, it fails on cases that involve carrying a digit for many steps.\nThe probability that a bit is carried k times, on uniformly random binary inputs, is 2\u2212k. Training and testing on random examples will thus not notice failure to carry more than about 20 bits. We find that the model fails to generalize on test cases that require more than 20 steps of carry, as summarized in Fig. 6.\nAdditions with long carries\nWe observe similar generalization issues with multiplication. The model that has < 1% error on two random 100-digit long decimal examples fails on very symmetric numbers and on numbers that\nrequires to carry for many steps (we test it by verifying performance on numbers that multiply to k100 or k100 \u2212 1). However, it failed much more often on single digit examples when they are prepended with many zeros (Table 3); one trained model failed on 38 of the 100 options.\nThe multiplication model also fails on examples that require a lot of steps of carry. The outputs of the multiplication model are similar to the correct answer, however it seems that the model made a mistake in the middle of the computation. For instance, we took two numbers that multiply to 10180 \u2212 1. Such a number is full of nines. However, model predicted: 10180 + 10103 \u2212 1 instead. The predicted number differs on 78 positions, because it has many zeros instead of nines; at some point, it starts to guess that it is getting a carry. Moreover, the model has trouble with very structured expressions, such as 1111111111\u00d7 1111111111. It fails for any number of ones above 8."}, {"heading": "6 GLOBAL OPERATION", "text": "The Neural GPU is a cellular automaton, which is a Turing complete computational model (Chopard & Droz, 1998; Wolfram, 1984) However, the automaton is often computationally inefficient compared to von Neumann architecture. It is difficult for a cellular automaton to move data globally as the entirety of its computation operates locally at each step. We wanted to understand the importance of globally moving data around for solving algorithmic tasks.\nThe Neural GPU could be made more powerful by adding a global operation in each of its computational steps. We have briefly tried to use attention that shifts data, but we were not able to improve empirical results. We skip these experiments here, and we ask a simpler question.\nGiven that the Neural GPU cannot easily move information across long distances, it is conceivable that having both arguments concatenated would hurt its performance. We therefore experimented with several input representations, as described below:\n\u2022 Padded: 12345+00067 \u2022 Unpadded: 12345+67\n\u2022 Aligned in two lines: 12345+00067\nAlignment helps addition, hurts multiplication\nCould we manually align data in a way that helps to solve a task? This could indicate that an architectural modification that performs a global operation is needed. And indeed, we found that the addition task on aligned numbers has higher success rate than on concatenated numbers, and addition on unpadded numbers is the hardest one (left Fig. 7).\nHowever, we observe the opposite outcome for the multiplication task (right Fig. 7). Aligning numbers for multiplication makes the task more difficult. These results suggest that an architectural modification that makes it easy to move the data globally need not provide an improvement on all problems."}, {"heading": "7 CONCLUSION", "text": "In this paper, we investigated the generalization ability of the Neural GPU. We have discovered that larger Neural GPUs generalize better, and provided examples of curriculums that made it possible for the Neural GPU to solve tasks that it was not able to solve before. Finally, we showed that its generalization is incomplete, as while it has successfully generalized to longer inputs, there still exist highly structured test cases that cause the model to fail.\nIt is desirable to develop learning methods that can learn algorithms that achieve perfect generalization. One way of moving forward is to investigate ways in which the model can benefit from additional sources of information that are not present in the task itself."}, {"heading": "8 ACKNOWLEDGMENT", "text": "We wish to thank Rafal Jozefowicz for useful discussions, and comments."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems, 2015", "author": ["Abadi", "Mart\u0131n"], "venue": "Software available from tensorflow. org,", "citeRegEx": "Abadi and Mart\u0131n,? \\Q2015\\E", "shortCiteRegEx": "Abadi and Mart\u0131n", "year": 2015}, {"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Andrychowicz", "Marcin", "Kurach", "Karol"], "venue": "arXiv preprint arXiv:1602.03218,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR 2015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Genetic Algorithms in Search, Optimization and Machine Learning", "author": ["Goldberg", "David E"], "venue": "AddisonWesley Longman Publishing Co., Inc.,", "citeRegEx": "Goldberg and E.,? \\Q1989\\E", "shortCiteRegEx": "Goldberg and E.", "year": 1989}, {"title": "Accelerated neural evolution through cooperatively coevolved synapses", "author": ["Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen", "Miikkulainen", "Risto"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gomez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gomez et al\\.", "year": 2008}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves and Alex.,? \\Q2016\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Memoryefficient backpropagation through time", "author": ["Gruslys", "Audr\u016bnas", "Munos", "Remi", "Danihelka", "Ivo", "Lanctot", "Marc", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1606.03401,", "citeRegEx": "Gruslys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gruslys et al\\.", "year": 2016}, {"title": "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence", "author": ["Holland", "John H"], "venue": null, "citeRegEx": "Holland and H.,? \\Q1992\\E", "shortCiteRegEx": "Holland and H.", "year": 1992}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Neural random-access machines", "author": ["Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["Martens", "James", "Sutskever", "Ilya"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "Evolutionary program induction of binary machine code and its applications", "author": ["Nordin", "Peter"], "venue": "Krehl Munster,", "citeRegEx": "Nordin and Peter.,? \\Q1997\\E", "shortCiteRegEx": "Nordin and Peter.", "year": 1997}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "A formal theory of inductive inference", "author": ["Solomonoff", "Ray J"], "venue": "Part I. Information and control,", "citeRegEx": "Solomonoff and J.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff and J.", "year": 1964}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Weakly supervised memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "The nature of statistical learning theory", "author": ["Vapnik", "Vladimir"], "venue": "Springer Science & Business Media,", "citeRegEx": "Vapnik and Vladimir.,? \\Q2013\\E", "shortCiteRegEx": "Vapnik and Vladimir.", "year": 2013}, {"title": "A representation scheme to perform program induction in a canonical genetic algorithm. In Parallel Problem Solving from NaturePPSN", "author": ["Wineberg", "Mark", "Oppacher", "Franz"], "venue": null, "citeRegEx": "Wineberg et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wineberg et al\\.", "year": 1994}, {"title": "Cellular automata as models of complexity", "author": ["Wolfram", "Stephen"], "venue": "Nature, 311(5985):419\u2013424,", "citeRegEx": "Wolfram and Stephen.,? \\Q1984\\E", "shortCiteRegEx": "Wolfram and Stephen.", "year": 1984}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "This problem lies at the core of program induction, an old field that has significant past work (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964; Holland, 1992; Goldberg, 1989; Gomez et al., 2008).", "startOffset": 96, "endOffset": 226}, {"referenceID": 5, "context": "This problem lies at the core of program induction, an old field that has significant past work (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964; Holland, 1992; Goldberg, 1989; Gomez et al., 2008).", "startOffset": 96, "endOffset": 226}, {"referenceID": 7, "context": "More recently, researchers have begun investigating this problem using the deep learning techniques of neural network function approximation and stochastic gradient descent (Graves et al., 2014; Zaremba & Sutskever, 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Andrychowicz & Kurach, 2016).", "startOffset": 173, "endOffset": 297}, {"referenceID": 14, "context": "More recently, researchers have begun investigating this problem using the deep learning techniques of neural network function approximation and stochastic gradient descent (Graves et al., 2014; Zaremba & Sutskever, 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Andrychowicz & Kurach, 2016).", "startOffset": 173, "endOffset": 297}, {"referenceID": 15, "context": "The problem of learning algorithms from data has been investigated in the field of program synthesis (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964) and genetic programming (Holland, 1992; Goldberg, 1989; Gomez et al.", "startOffset": 101, "endOffset": 180}, {"referenceID": 5, "context": ", 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964) and genetic programming (Holland, 1992; Goldberg, 1989; Gomez et al., 2008).", "startOffset": 77, "endOffset": 128}, {"referenceID": 7, "context": "There exist a number of neural network architecture that implement this idea: the Neural Turing Machine (NTM) (Graves et al., 2014), learning programs with LSTM (Zaremba & Sutskever, 2014), grid LSTM (Kalchbrenner et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 13, "context": ", 2014), learning programs with LSTM (Zaremba & Sutskever, 2014), grid LSTM (Kalchbrenner et al., 2015), the Stack RNN (Joulin & Mikolov, 2015), the Neural DeQue (Grefenstette et al.", "startOffset": 76, "endOffset": 103}, {"referenceID": 8, "context": ", 2015), the Stack RNN (Joulin & Mikolov, 2015), the Neural DeQue (Grefenstette et al., 2015), End-to-End Memory Networks (Sukhbaatar et al.", "startOffset": 66, "endOffset": 93}, {"referenceID": 21, "context": ", 2015), End-to-End Memory Networks (Sukhbaatar et al., 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), Neural random-access machines (Kurach et al.", "startOffset": 36, "endOffset": 61}, {"referenceID": 14, "context": ", 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), Neural random-access machines (Kurach et al., 2015).", "startOffset": 100, "endOffset": 121}, {"referenceID": 26, "context": "While some models (Zaremba & Sutskever, 2015; Zaremba et al., 2015; Graves, 2016) can in principle learn the correct runtime for a given algorithm, in practice it has not been possible to learn algorithms requiring superlinear runtime, such as integer multiplication.", "startOffset": 18, "endOffset": 81}, {"referenceID": 13, "context": "The Grid LSTM (Kalchbrenner et al., 2015) is a powerful architecture that can learn 15-digit decimal addition.", "startOffset": 14, "endOffset": 41}, {"referenceID": 3, "context": "In particular, all the aforementioned models require a curriculum learning (Bengio et al., 2009; Zaremba & Sutskever, 2014) in order to successfully learn sophisticated functions, and the results in this paper are no different.", "startOffset": 75, "endOffset": 123}, {"referenceID": 18, "context": "This approach is sometimes used in object detection, where the same convolutional neural network is applied on images of variable size (Sermanet et al., 2013).", "startOffset": 135, "endOffset": 158}, {"referenceID": 13, "context": "The architecture most similar to Neural GPU is the Grid LSTM (Kalchbrenner et al., 2015).", "startOffset": 61, "endOffset": 88}, {"referenceID": 2, "context": "To successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques: \u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al., 2014).", "startOffset": 168, "endOffset": 191}, {"referenceID": 12, "context": "The architecture most similar to Neural GPU is the Grid LSTM (Kalchbrenner et al., 2015). It has been shown to learn 15 digit long decimal addition task, although it has not yet been shown to generalize to inputs of length greater than the training data. To successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques: \u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al.", "startOffset": 62, "endOffset": 319}, {"referenceID": 2, "context": "To successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques: \u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al., 2014). \u2022 Tanh cutoff: the hyperbolic tangent activations used by the GRU are truncated once they reach a critical upper (and lower) bound. The hope is that this makes the results more \u201cdigital\u201d. \u2022 The use of Dropout Srivastava et al. (2014). \u2022 Instead of using the same weights at every pair of layers, the Neural GPU starts out by cycling through 6 independent sets of weights, which are gradually annealed to become identical as optimization progresses.", "startOffset": 169, "endOffset": 427}, {"referenceID": 9, "context": "A third way would be to use the methods described by Martens & Sutskever (2012); Gruslys et al. (2016), however we have not experimented with them.", "startOffset": 81, "endOffset": 103}], "year": 2016, "abstractText": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires careful memory management, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques to increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with 100% accuracy. In addition, we attempt to gain insight into the Neural GPU by understanding its failure modes. We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on 000000 . . . 002\u00d7 000000 . . . 002 while succeeding at 2\u00d7 2. These failure modes are reminiscent of adversarial examples.", "creator": "LaTeX with hyperref package"}}}