{"id": "1611.01190", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Conspiracies between Learning Algorithms, Circuit Lower Bounds and Pseudorandomness", "abstract": "recently we prove several constructive results there giving new edges and slightly stronger internal connections involving between informal learning, virtual circuit covering lower order bounds and infinite pseudorandomness. concurrent among myriad other results, till we subsequently show a generic learning mode speedup optimization lemma, equivalences distinguish between various learning models computed in the exponential time trial and uniform subexponential time operating regimes, solving a dichotomy between effective learning and operational pseudorandomness, consequences of differential non - necessarily trivial learning for circuit area lower bounds, karp - lipton theorems for exponential probabilistic differential exponential time, and nc $ ^ 1 $ - hardness for the minimum approximation circuit size problem.", "histories": [["v1", "Thu, 3 Nov 2016 21:08:38 GMT  (745kb,D)", "http://arxiv.org/abs/1611.01190v1", null]], "reviews": [], "SUBJECTS": "cs.CC cs.CR cs.DS cs.LG", "authors": ["igor c oliveira", "rahul santhanam"], "accepted": false, "id": "1611.01190"}, "pdf": {"name": "1611.01190.pdf", "metadata": {"source": "CRF", "title": "Conspiracies between Learning Algorithms, Circuit Lower Bounds and Pseudorandomness", "authors": ["Igor C. Oliveira", "Rahul Santhanam"], "emails": [], "sections": [{"heading": null, "text": "Learning Speedups. If C[poly(n)] admits a randomized weak learning algorithm under the uniform distribution with membership queries that runs in time 2n/n\u03c9(1), then for every k \u2265 1 and \u03b5 > 0 the class C[nk] can be learned to high accuracy in timeO(2n \u03b5 ). There is \u03b5 > 0 such that C[2n \u03b5 ] can be learned in time 2n/n\u03c9(1) if and only if C[poly(n)] can be learned in time 2(logn) O(1) .\nEquivalences between Learning Models. We use learning speedups to obtain equivalences between various randomized learning and compression models, including sub-exponential time learning with membership queries, sub-exponential time learning with membership and equivalence queries, probabilistic function compression and probabilistic average-case function compression.\nA Dichotomy between Learnability and Pseudorandomness. In the non-uniform setting, there is non-trivial learning for C[poly(n)] if and only if there are no exponentially secure pseudorandom functions computable in C[poly(n)].\nLower Bounds from Nontrivial Learning. If for each k \u2265 1, (depth-d)-C[nk] admits a randomized weak learning algorithm with membership queries under the uniform distribution that runs in time 2n/n\u03c9(1), then for each k \u2265 1, BPE * (depth-d)-C[nk]. If for some \u03b5 > 0 there are P-natural proofs useful against C[2n \u03b5 ], then ZPEXP * C[poly(n)].\nKarp-Lipton Theorems for Probabilistic Classes. If there is a k > 0 such that BPE \u2286 i.o.Circuit[nk], then BPEXP \u2286 i.o.EXP/O(log n). If ZPEXP \u2286 i.o.Circuit[2n/3], then ZPEXP \u2286 i.o.ESUBEXP.\nHardness Results for MCSP. All functions in non-uniform NC1 reduce to the Minimum Circuit Size Problem via truth-table reductions computable by TC0 circuits. In particular, if MCSP \u2208 TC0 then NC1 = TC0.\nar X\niv :1\n61 1.\n01 19\n0v 1\n[ cs\n.C C\n] 3\nN ov\nContents"}, {"heading": "1 Introduction 3", "text": "1.1 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.1 Speedups in Complexity Theory . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2.2 Connections between Pseudorandomness, Learning and Cryptography . . . . 8 1.2.3 Lower Bounds from Learning Algorithms . . . . . . . . . . . . . . . . . . . . 8 1.2.4 Useful Properties, Natural Properties, and Circuit Lower Bounds . . . . . . . 9 1.2.5 Karp-Lipton Theorems in Complexity Theory . . . . . . . . . . . . . . . . . . 9 1.2.6 The Minimum Circuit Size Problem . . . . . . . . . . . . . . . . . . . . . . . 10\n1.3 Main Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.3.2 Sketch of Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"}, {"heading": "2 Preliminaries and Notation 15", "text": "2.1 Boolean Function Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2 Learning and Compression Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.3 Natural Proofs and the Minimum Circuit Size Problem . . . . . . . . . . . . . . . . . 17 2.4 Randomness and Pseudorandomness . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"}, {"heading": "3 Learning Speedups and Equivalences 19", "text": "3.1 The Speedup Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.2 Equivalences for Learning, Compression, and Distinguishers . . . . . . . . . . . . . . 22"}, {"heading": "4 Learning versus Pseudorandom Functions 25", "text": "4.1 The PRF-Distinguisher Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.2 A (Non-Uniform) Converse to \u201cLearning Implies no PRFs\u201d . . . . . . . . . . . . . . 27"}, {"heading": "5 Lower Bounds from Nontrivial Algorithms 29", "text": ""}, {"heading": "6 Karp-Lipton Collapses for Probabilistic Classes 34", "text": "6.1 A Lemma About Learning with Advice . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.2 Karp-Lipton Results for Bounded-Error Exponential Time . . . . . . . . . . . . . . . 38 6.3 Karp-Lipton Results for Zero-Error Exponential Time . . . . . . . . . . . . . . . . . 40"}, {"heading": "7 Hardness of the Minimum Circuit Size Problem 42", "text": "8 Open Problems and Further Research Directions 45"}, {"heading": "1 Introduction", "text": "Which classes of functions can be efficiently learned? Answering this question has been a major research direction in computational learning theory since the seminal work of Valiant [Val84] formalizing efficient learnability.\nFor concreteness, consider the model of learning with membership queries under the uniform distribution. In this model, the learner is given oracle access to a target Boolean function and aims to produce, with high probability, a hypothesis that approximates the target function well on the uniform distribution. Say that a circuit class C is learnable in time T if there is a learner running in time T such that for each function f \u2208 C, when given oracle access to f the learner outputs the description of a Boolean function h approximating f well under the uniform distribution. The hypothesis h is not required to be from the same class C of functions. (This and other learning models that appear in our work are defined in Section 2.)\nVarious positive and conditional negative results are known for natural circuit classes in this model, and here we highlight only a few. Polynomial-time algorithms are known for polynomial-size DNF formulas [Jac97]. Quasi-polynomial time algorithms are known for polynomial-size constantdepth circuits with AND, OR and NOT gates [LMN93] (i.e., AC0 circuits), and in a recent breakthrough [CIKK16], for polynomial-size constant-depth circuits which in addition contain MOD[p] gates, where p is a fixed prime (AC0[p] circuits). In terms of hardness, it is known that under certain cryptographic assumptions, the class of polynomial-size constant-depth circuits with threshold gates (TC0 circuits) is not learnable in sub-exponential time [NR04]. (We refer to Section 2 for a review of the inclusions between standard circuit classes.)\nHowever, even under strong hardness assumptions, it is still unclear how powerful a circuit class needs to be before learning becomes utterly infeasible. For instance, whether non-trivial learning algorithms exist for classes beyond AC0[p] remains a major open problem.\nInspired by [CIKK16], we show that a general and surprising speedup phenomenon holds unconditionally for learnability of strong enough circuit classes around the border of currently known learning algorithms. Say that a class is non-trivially learnable if it is learnable in time \u2264 2n/nw(1), where n is the number of inputs to a circuit in the class, and furthermore the learner is only required to output a hypothesis that is an approximation for the unknown function with inverse polynomial advantage. We show that for \u201ctypical\u201d circuit classes such as constant-depth circuits with Mod[m] gates where m is an arbitrary but fixed composite (ACC0 circuits), constant-depth threshold circuits, formulas and general Boolean circuits, non-trivial learnability in fact implies high-accuracy learnability in time 2n o(1) , i.e., in sub-exponential time.\nLemma 1 (Speedup Lemma, Informal Version). Let C be a typical circuit class. Polynomialsize circuits from C are non-trivially learnable if and only if polynomial-size circuits from C are (strongly) learnable in sub-exponential time. Subexponential-size circuits from C are non-trivially learnable if and only if polynomial-size circuits from C are (strongly) learnable in quasi-polynomial time.\nNote that the class of all Boolean functions is learnable in time\u2264 2n/n\u2126(1) with\u2265 1/n advantage simply by querying the function oracle on 2n/nO(1) inputs, and outputting the best constant in {0, 1} for the remaining (unqueried) positions of the truth-table. Our notion of non-trivial learning corresponds to merely beating this trivial brute-force algorithm \u2013 this is sufficient to obtain much more dramatic speedups for learnability of typical circuit classes.\nIn order to provide more intuition for this result, we compare the learning scenario to another widely investigated algorithmic framework. Consider the problem of checking if a circuit from a fixed circuit class is satisfiable, a natural generalization of the CNF-SAT problem. Recall that\nACC0 circuits are circuits of constant depth with AND, OR, NOT, and modulo gates. There are non-trivial satisfiability algorithms for ACC0 circuits of size up to 2n \u03b5 , where \u03b5 > 0 depends on the depth and modulo gates [Wil14c]. On the other hand, if such circuits admitted a non-trivial learning algorithm, it follows from the Speedup Lemma that polynomial size ACC0 circuits can be learned in quasi-polynomial time (see Figure 1).\nThe Speedup Lemma suggests new approaches both to designing learning algorithms and to proving hardness of learning results. To design a quasi-polynomial time learning algorithm for polynomial-size circuits from a typical circuit class, it suffices to obtain a minimal improvement over the trivial brute-force algorithm for sub-exponential size circuits from the same class. Conversely, to conclude that the brute-force learning algorithm is essentially optimal for a typical class of polynomial-size circuits, it suffices to use an assumption under which subexponential-time learning is impossible.\nWe use the Speedup Lemma to show various structural results about learning. These include equivalences between several previously defined learning models, a dichotomy between subexponential time learnability and the existence of pseudo-random function generators in the nonuniform setting, and implications from non-trivial learning to circuit lower bounds.\nThe techniques we explore have other consequences for complexity theory, such as Karp-Lipton style results for bounded-error exponential time, and results showing hardness of the Minimum Circuit Size Problem for a standard complexity class. In general, our results both exploit and strengthen the rich web of connections between learning, pseudo-randomness and circuit lower bounds, which promises to have further implications for our understanding of these fundamental notions. We now describe these contributions in more detail.\nSatisfiability Algorithms vs. Learning Algorithms"}, {"heading": "1.1 Summary of Results", "text": "We state below informal versions of our main results. We put these results in perspective and compare them to previous work in Section 1.2.\nEquivalences for Learning Models.\nThe Speedup Lemma shows that learnability of polynomial size circuits for typical circuit classes is not sensitive to the distinction between randomized sub-exponential time algorithms and randomized non-trivial algorithms. We use the Speedup Lemma to further show that for such classes, learnability for a range of previously defined learning models is equivalent. These include the worst-case and average-case versions of function compression as defined by Chen et al. [CKK+15] (see also [Sri15]), and randomized learning with membership and equivalence queries [Ang87].1 The equivalence between function compression and learning in particular implies that accessing the entire truth table of a function represented by the circuit from the class confers no advantage in principle over having limited access to the truth table.\nTheorem 1 (Equivalences for Learning Models, Informal Version). The following are equivalent for polynomial-size circuits from a typical circuit class C:\n1. Sub-exponential time learning with membership queries.\n2. Sub-exponential time learning with membership and equivalence queries.\n3. Probabilistic function compression.\n4. Average-case probabilistic function compression.\n5. Exponential time distinguishability from random functions.\nIn particular, in the randomized sub-exponential time regime and when restricted to learning under the uniform distribution, Valiant\u2019s model [Val84] and Angluin\u2019s model [Ang87] are equivalent in power with respect to the learnability of typical classes of polynomial size circuits."}, {"heading": "A Dichotomy between Learning and Pseudorandomness.", "text": "It is well-known that if the class of polynomial-size circuits from a class C is learnable, then there are no pseudo-random function generators computable in C, as the learner can be used to distinguish random functions from pseudo-random ones [KV94b]. A natural question is whether the converse is true: can we in general build pseudo-random functions in the class from non-learnability of the class? We are able to use the Speedup Lemma in combination with other techniques to show such a result in the non-uniform setting, where the pseudo-random function generator as well as the learning algorithm are non-uniform. As a consequence, for each typical circuit class C, there is a dichotomy between pseudorandomness and learnability \u2013 either there are pseudo-random function generators computable in the class, or the class is learnable, but not both.\nTheorem 2 (Dichotomy between Learning and Pseudorandomness, Informal Version). Let C be a typical circuit class. There are pseudo-random function generators computable by polynomialsize circuits from C that are secure against sub-exponential size Boolean circuits if and only if polynomial-size circuits from C are learnable non-uniformly in sub-exponential time.\nNontrivial Learning implies Circuit Lower Bounds.\nIn the algorithmic approach of Williams [Wil13], non-uniform circuit lower bounds against a class C of circuits are shown by designing algorithms for satisfiability of C-circuits that beat\n1Our notion of randomized learning with membership and equivalence queries allows the learner\u2019s hypothesis to be incorrect on a polynomially small fraction of the inputs.\nthe trivial brute-force search algorithm. Williams\u2019 approach has already yielded the result that NEXP 6\u2286 ACC0 [Wil14c].\nIt is natural to wonder if an analogue of the algorithmic approach holds for learning, and if so, what kinds of lower bounds would follow using such an analogue. We establish such a result \u2013 nontrivial learning algorithms yield lower bounds for bounded-error probabilistic exponential time, just as non-trivial satisfiability algorithms yield lower bounds for non-deterministic exponential time. Our connection between learning and lower bounds has a couple of nice features. Our notion of \u201cnon-trivial algorithm\u201d can be made even more fine-grained than that of Williams \u2013 it is not hard to adapt our techniques to show that it is enough to beat the brute-force algorithm by a superconstant factor for learning algorithms with constant accuracy, as opposed to a polynomial factor in the case of Satisfiability. Moreover, non-trivial learning for bounded-depth circuits yields lower bounds against circuits with the same depth, as opposed to the connection for Satisfiability where there is an additive loss in depth [Oli15, JMV15].\nTheorem 3 (Circuit Lower Bounds from Learning and from Natural Proofs, Informal Version). Let C be any circuit class closed under projections.\n(i) If polynomial-size circuits from C are non-trivially learnable, then (two-sided) bounded-error probabilistic exponential time does not have polynomial-size circuits from C.\n(ii) If sub-exponential size circuits from C = ACC0 are non-trivially learnable, then one-sided error probabilistic exponential time does not have polynomial-size circuits from ACC0.\n(iii) If there are natural proofs useful against sub-exponential size circuits from C, then zero-error probabilistic exponential time does not have polynomial-size circuits from C.\nObserve that the existence of natural proofs against sub-exponential size circuits yields stronger lower bounds than learning and satisfiability algorithms. (We refer to Section 2 for a review of the inclusions between exponential time classes.)\nKarp-Lipton Theorems for Probabilistic Exponential Time.\nOur main results are about learning, but the techniques have consequences for complexity theory. Specifically, our use of pseudo-random generators has implications for the question of Karp-Lipton theorems for probabilistic exponential time. A Karp-Lipton theorem for a complexity class gives a connection between uniformity and non-uniformity, by showing that a non-uniform inclusion of the complexity class also yields a uniform inclusion. Such theorems were known for a range of classes such as NP, PSPACE, EXP, and NEXP [KL80, BFNW93, IKW02], but not for bounded-error probabilistic exponential time. We show the first such theorem for bounded-error probabilistic exponential time. A technical caveat is that the inclusion in our consequent is not completely uniform, but requires a logarithmic amount of advice.\nTheorem 4 (Karp-Lipton Theorem for Probabilistic Exponential Time, Informal Version). If bounded-error probabilistic exponential time has polynomial-size circuits infinitely often, then bounded-error probabilistic exponential time is infinitely often in deterministic exponential time with logarithmic advice.\nHardness of the Minimum Circuit Size Problem.\nOur techniques also have consequences for the complexity of the Minimum Circuit Size Problem (MCSP). In MCSP, the input is the truth table of a Boolean function together with a parameter s\nin unary, and the question is whether the function has Boolean circuits of size at most s. MCSP is a rare example of a problem in NP which is neither known to be in P or NP-complete. In fact, we don\u2019t know much unconditionally about the complexity of this problem. We know that certain natural kinds of reductions cannot establish NP-completeness [MW15], but until our work, it was unknown whether MCSP is hard for any standard complexity class beyond AC0 [ABK+06]. We show the first result of this kind.\nTheorem 5 (Hardness of the Minimum Circuit Size Problem, Informal Version). The Minimum Circuit Size Problem is hard for polynomial-size formulas under truth-table reductions computable by polynomial-size constant-depth threshold circuits.\nRemark. This work contains several related technical contributions to the research topics mentioned above. We refer to the appropriate sections for more details. Finally, in Section 8 we highlight some open problems and directions that we find particularly attractive."}, {"heading": "1.2 Related Work", "text": ""}, {"heading": "1.2.1 Speedups in Complexity Theory", "text": "We are not aware of any unconditional speedup result of this form involving the time complexity of a natural class of computational problems, under a general computational model. In any case, it is instructive to compare Lemma 1 to a few other speedup theorems in computational complexity.\nA classic example is Blum\u2019s Speedup Theorem [Blu67]. It implies that there is a recursive function f : N\u2192 N such that if an algorithm computes this function in time T (n), then there is an algorithm computing f in time O(log T (n)). Lemma 1 differs in an important way. It refers to a natural computational task, while the function provided by Blum\u2019s Theorem relies on an artificial construction. Another well-known speedup result is the Linear Speedup Theorem (cf. [Pap94, Section 2.4]). Roughly, it states that if a Turing Machine computes in time T (n), then there is an equivalent Turing Machine that computes in time T (n)/c. The proof of this theorem is based on the simple trick of increasing the alphabet size of the machine. It is therefore dependent on the computational model, while Lemma 1 is not.\nPerhaps closer to our result are certain conditional derandomization theorems in complexity theory. We mention for concreteness two of them. In [IKW02], it is proved that if MA 6= NEXP, then MA \u2286 i.o.NTIME[2n\u03b5 ]/n\u03b5, while in [IW01], it is shown that if BPP 6= EXP, then BPP \u2286 i.o.pseudoDTIME[2n \u03b5 ]. It is possible to interpret these results as computational speedups, but observe that the faster algorithms have either weaker correctness guarantees, or require advice. Lemma 1 on the other hand transforms a non-trivial learning algorithm into a sub-exponential time learning algorithm of the same type.\nFurther results have been discovered in more restricted computational models. For instance, in the OPP model, [PP10] proved that if Circuit-SAT has algorithms running in time 2(1\u2212\u03b4)n, then it also has OPP algorithms running in time 2\u03b5n. In bounded-depth circuit complexity, [AK10] established among other results that if the Formula Evaluation Problem has uniform TC0-circuits of size O(nk), then it also has uniform TC0-circuits of size O(n1+\u03b5).\nIf one considers other notions of complexity, we can add to this list several results that provide different, and often rather unexpected, forms of speedup. We mention, for instance, depth reduction in arithmetic circuit complexity (see .e.g. [AV08]), reducing the number of rounds in interactive proofs [BM88], decreasing the randomness complexity of bounded-space algorithms [NZ96], cryptography in constant locality [AIK06], among many others."}, {"heading": "1.2.2 Connections between Pseudorandomness, Learning and Cryptography", "text": "There are well-known connections between learning theory, theoretical cryptography and pseudorandomness (see e.g. [Gol01]). Indeed, pseudorandom distributions lie at the heart of the definition of semantic security [GM82, GM84], which permeates modern cryptography, and to every secure encryption scheme there is a naturally associated hard-to-learn (decryption) problem.\nThe other direction, i.e., that from a generic hard learning problem it is always possible to construct secure cryptographic schemes and other basic primitives, is much less clear.2 Following a research line initiated in [IL90], results more directly related to our work were established in [BFKL93]. They proved in particular that private-key encryption and pseudorandom generators exist under a stronger average-case hardness-of-learning assumption, where one also considers the existence of a hard distribution over the functions in the circuit class C.\nHowever, these results and subsequent work leave open the question of whether hardness of learning in the usual case, i.e., the mere assumption that any efficient learner fails on some f \u2208 C, implies the existence of pseudorandom functions computable by C-circuits. While there is an extensive literature basing standard cryptographic primitives on a variety of conjecturally hard learning tasks (see e.g., [Reg09] and references therein for such a line of work), to our knowledge Theorem 2 is the first result to establish a general equivalence between the existence of pseudorandom functions and the hardness of learning, which holds for any typical circuit class. A caveat is that our construction requires non-uniformity, and is established only in the exponential security regime."}, {"heading": "1.2.3 Lower Bounds from Learning Algorithms", "text": "While several techniques from circuit complexity have found applications in learning theory in the past (see e.g., [LMN93]), Fortnow and Klivans [FK09] were the first to systematically investigate the connection between learning algorithms and lower bounds in a generic setting.3\nFor deterministic learning algorithms using membership and equivalence queries, initial results from [FK09] and [HH13] were strengthened and simplified in [KKO13], where it was shown that non-trivial deterministic learning algorithms for C imply that EXP * C.\nThe situation for randomized algorithms using membership queries is quite different, and only the following comparably weaker results were known. First, [FK09] proved that randomized polynomial time algorithms imply BPEXP lower bounds. This result was refined in [KKO13], where a certain connection involving sub-exponential time randomized learning algorithms and PSPACE was observed. More recently, [Vol14] combined ideas from [KKO13] and [San09] to prove that efficient randomized learning algorithms imply lower bounds for BPP/1, i.e., probabilistic polynomial time with advice. However, in contrast to the deterministic case, obtaining lower bounds from weaker running time assumptions had been elusive.4\nIndeed, we are not aware of any connection between two-sided non-trivial randomized algorithms and circuit lower bounds, even when considering different algorithmic frameworks in addition to learning. In particular, Theorem 3 (i) seems to be the first result in this direction. It can be seen\n2Recall that secure private-key encryption is equivalent to the existence of one-way functions, pseudorandom generators and pseudorandom functions, with respect to polynomial time computations (cf. [KL07]). Nevertheless, not all these equivalences are known to hold when more refined complexity measures are considered, such as circuit depth. In particular, generic constructions of pseudorandom functions from the other primitives are not known in small-depth classes. This can be done under certain specific hardness assumptions [NR04], but here we restrict our focus to generic relations between basic cryptographic primitives.\n3For a broader survey on connections between algorithms and circuit lower bounds, we refer to [Wil14a]. 4Some connections to lower bounds are also known in the context of learnability of arithmetic circuits. We refer\nto [FK09, Vol16] for more details.\nas an analogue of the connection between satisfiability algorithms and lower bounds established by Williams [Wil13, Wil14c]. But apart from this analogy, the proof of Theorem 3 employs significantly different techniques."}, {"heading": "1.2.4 Useful Properties, Natural Properties, and Circuit Lower Bounds", "text": "The concept of natural proofs, introduced by Razborov and Rudich [RR97], has had a significant impact on research on unconditional lower bounds. Recall that a property P of Boolean functions is a natural property against a circuit class C if it is: (1) efficiently computable (constructivity); (2) rejects all C-functions, and accepts at least one \u201chard\u201d function (usefulness), and (3) is satisfied by most Boolean functions (denseness). In case P satisfies only conditions (1) and (2), is it said to be useful against C.\nThere are natural properties against AC0[p] circuits, when p is prime [RR97]. But under standard cryptographic assumptions, there is no natural property against TC0 [NR04]. Consequently, the situation for classes contained in AC0[p] and for those that contain TC0 is reasonably wellunderstood. More recently, [Wil16] (see also [IKW02]) proved that if NEXP * C then there are useful properties against C. This theorem combined with the lower bound from [Wil14c] show that ACC0 admits useful properties.\nGiven these results, the existence of natural properties against ACC0 has become one of the most intriguing problems in connection with the theory of natural proofs. Theorem 3 (iii) shows that if there are P-natural properties against sub-exponential size ACC0 circuits, then ZPEXP * ACC0. This would lead to an improvement of Williams\u2019 celebrated lower bound which does not seem to be accessible using his techniques alone.5"}, {"heading": "1.2.5 Karp-Lipton Theorems in Complexity Theory", "text": "Karp-Lipton theorems are well-known results in complexity theory relating non-uniform circuit complexity and uniform collapses. A theorem of this form was first established in [KL80], where they proved that if NP \u2286 Circuit[poly], then the polynomial time hierarchy collapses. This result shows that non-uniform circuit lower bounds cannot be avoided if our goal is a complete understanding of uniform complexity theory.\nSince their fundamental work, many results of this form have been discovered for complexity classes beyond NP. In some cases, the proof required substantially new ideas, and the new KarpLipton collapse led to other important advances in complexity theory. Below we discuss the situation for two exponential complexity classes around BPEXP, which is connected to Theorem 4.\nA stronger Karp-Lipton theorem for EXP was established in [BFNW93], using techniques from interactive proofs and arithmetization. An important application of this result appears in [BFT98] in the proof that MAEXP * Circuit[poly]. This is still one of the strongest known non-uniform lower bounds. For NEXP, a Karp-Lipton collapse was proved in [IKW02]. This time the proof employed the easy witness method and techniques from pseudorandomness, and the result plays a fundamental role in Williams\u2019 framework [Wil13], which culminated in the proof that NEXP * ACC0 [Wil14c]. (We mention that a Karp-Lipton theorem for EXPNP has also been established in [BH92].) Karp-Lipton collapse theorems are known for a few other complexity classes contained in EXP, and they have found applications in a variety of contexts in algorithms and complexity theory (see e.g., [Yap83, FS11]).\n5The result that P-natural properties against sub-exponential size circuits yield ZPEXP lower bounds was also obtained in independent work by Russell Impagliazzo, Valentine Kabanets and Ilya Volkovich (private communication).\nDespite this progress on proving Karp-Lipton collapses for exponential time classes, there is no published work on such for probabilistic classes. Theorem 4 is the first such result for the class BPEXP."}, {"heading": "1.2.6 The Minimum Circuit Size Problem", "text": "The Minimum Circuit Size Problem (MCSP) and its variants has received a lot of attention in both applied and theoretical research. Its relevance in practice is clear. From a theoretical point of view, it is one of the few natural problems in NP that has not been shown to be in P or NP-complete. The hardness of MCSP is also connected to certain fundamental problems in proof complexity (cf. [Kra11, Raz15]).\nInterestingly, a well-understood variant of MCSP is the Minimum DNF Size Problem, for which both NP-hardness [Mas79] and near-optimal hardness of approximation have been established [AHM+08, KS08]. However, despite the extensive literature on the complexity of the MCSP problem [KC00, ABK+06, AD14, HP15, AHK15, MW15, HP15, AGM15, HW16], and the intuition that it must also be computationally hard, there are few results providing evidence of its difficulty. Among these, we highlight the unconditional proof that MCSP /\u2208 AC0 [ABK+06], and the reductions showing that Factoring \u2208 ZPPMCSP [ABK+06] and SZK \u2286 BPPMCSP [AD14]. The lack of further progress has led to the formulation and investigation of a few related problems, for which some additional results have been obtained (cf. [ABK+06, AHK15, AGM15, HW16]).\nMore recently, [MW15] provided some additional explanation for the difficulty of proving hardness of MCSP. They unconditionally established that a class of local reductions that have been used for many other NP-completeness proofs cannot work, and that the existence of a few other types of reductions would have significant consequences in complexity theory. Further results along this line appear in [HP15].\nTheorem 5 contributes to our understanding of the difficulty of MCSP by providing the first hardness results for a standard complexity class beyond AC0. We hope this result will lead to further progress on the quest to determine the complexity of this elusive problem.6"}, {"heading": "1.3 Main Techniques", "text": ""}, {"heading": "1.3.1 Overview", "text": "Our results are obtaining via a mixture of techniques from learning theory, computational complexity, pseudo-randomness and circuit complexity. We refer to Figure 2 for a web of connections involving the theorems stated in Section 1.1 and the methods employed in the proofs. We start with an informal description of most of the techniques depicted in Figure 2, with pointers to some relevant references.7\nNisan-Wigderson Generator [NW94]. The NW-Generator allows us to convert a function f : {0, 1}n \u2192 {0, 1} into a family of functions NW(f). Crucially, if an algorithm A is able to distinguish NW(f) from a random function, there is a reduction that only needs oracle access to\n6We have learned from Eric Allender (private communication) that in independent work with Shuichi Hirahara, they have shown some hardness results for the closely related problem of whether a string has high KT complexity. These results do not yet seem to transfer to MCSP and its variants. In addition, we have learned from Valentine Kabanets (private communication) that in recent independent work with Russell Impagliazzo and Ilya Volkovich, they have also obtained some results on the computational hardness of MCSP.\n7This is not a comprehensive survey of the original use or appearance of each method. It is included here only as a quick guide to help the reader to assimilate the main ideas employed in the proofs.\nf and A, and that can be used to weakly approximate f . The use of the NW-Generator in the context of learning, for a function f that is not necessarily hard, appeared recently in [CIKK16].8\n(Uniform) Hardness Amplification. This is a well-known technique in circuit complexity (cf. [GNW11]), allowing one to produce a not much more complex function g\u0303 : {0, 1}m(n) \u2192 {0, 1}, given oracle access to some function g : {0, 1}n \u2192 {0, 1}, that is much harder to approximate than g. The uniform formulation of this result shows that a weak approximator for g\u0303 can be converted into a strong approximator for g. The connection to learning was explicitly observed in [BL93].\nCounting and Concentration Bounds. This is a standard argument which allows one to prove that most Boolean functions on n-bit inputs cannot be approximated by Boolean circuits of size \u2264 2n/n\u03c9(1) (Lemma 4). In particular, learning algorithm running in non-trivial time can only successfully learn a negligible fraction of all Boolean functions.\nSmall-Support Min-Max Theorem [Alt94, LY94]. This is an approximate version of the wellknown min-max theorem from game theory. It provides a bound on the support size of the mixed strategies. To prove Theorem 2, we consider a game between a function family generator and a\n8Interestingly, another unexpected and somewhat related use of the NW-generator appears in proof complexity (see e.g., [Pic15] and references therein).\ncandidate distinguisher, and this result allows us to move from a family of distinguishers against different classes of functions to a single universal distinguisher of bounded complexity.\nWorst-Case to Average-Case Reduction. The NW-Generator and hardness amplification can be used to boost a very weak approximation into a strong one. In some circuit classes such as NC1, a further reduction allows one to obtain a circuit that is correct on every input with high probability (see e.g. [AAW10]). This is particularly useful when proving hardness results for MCSP.\nEasy Witness Method [Kab01] and Impagliazzo-Wigderson Generator [IW97]. The easy witness method is usually employed as a win-win argument: either a verifier accepts a string encoded by a small circuit, or every accepted string has high worst-case circuit complexity. No matter the case, it can be used to our advantage, thanks to the generator from [IW97] that transforms a worst-case hard string (viewed as a truth table) into a pseudorandom distribution of strings.\n(Almost Everywhere) Hierarchy Theorems. A difficulty when proving Theorems 3 and 4 is that there are no known tight hierarchy theorems for randomized time. Our approach is therefore indirect, relying on the folklore result that bounded-space algorithms can diagonalize on every input length against all bounded-size circuits (Lemma 11 and Corollary 2).\nRandom-self-reducibility and Downward-self-reducibility. These are important notions of self-reducibility shared by certain functions. Together, they can be used via a recursive procedure to obtain from a learning algorithm for such a function, which requires oracle access to the function, a standard randomized algorithm computing the same function [IW01, TV07].\nAdvice Elimination. This idea is important in the contrapositive argument establishing Theorem 4. Assuming that a certain deterministic simulation of a function in BPEXP is not successful, it is not clear how to determine on each input length a \u201cbad\u201d string of that length for which the simulation fails. Such bad strings are passed as advice in our reduction, and in order to eliminate the dependency on them, we use an advice-elimination strategy from [TV07]."}, {"heading": "1.3.2 Sketch of Proofs", "text": "We describe next in a bit more detail how the techniques described above are employed in the proof of our main results. We stress that the feasibility of all these arguments crucially depend on the parameters associated to each result and technique. However, for simplicity our focus here will be on the qualitative connections.\nLemma 1 (Speedup Lemma). Given query access to a function f \u2208 C that we would like to learn to high accuracy, the first idea is to notice that if there is a distinguisher against NW(f), then we can non-trivially approximate f using membership queries. But since this is not the final goal of a strong learning algorithm, we consider NW(f\u0303), the generator applied to the amplified version of f . Using properties of the NW-generator and hardness amplification, it follows that if there is a distinguisher against NW(f\u0303), it is possible to approximate f\u0303 , which in turn provides a strong approximator for f . (A similar strategy is employed in [CIKK16], where a natural property is used instead of a distinguisher.)\nNext we use the assumption that C has non-trivial learning algorithms to obtain a distinguisher against C. (For this approach to work, it is fundamental that the functions in NW(f\u0303) \u2286 C. In other words, the reductions discussed above should not blow-up the complexity of the involved functions\nby too much. For this reason, C must be a sufficiently strong circuit class.) By a counting argument and a concentration bound, while a non-trivial learning algorithm will weakly learn every function in C, it must fail to learn a random Boolean function with high probability. We apply this idea to prove that a non-trivial learner can be used as a distinguisher against NW(f\u0303).\nThese techniques can therefore be combined in order to boost a non-trivial learner for C into a high-accuracy learner for C. This takes care of the accuracy amplification. The running time speedup comes from the efficiency of the reductions involved, and from the crucial fact that each function in NW(f\u0303) is a function over m n input bits. In particular, the non-trivial but still exponential time learning algorithm for C only needs to be invoked on Boolean functions over m input bits. (This argument only sketches one direction in Lemma 1.)\nTheorem 1 (Learning Equivalences). At the core of the equivalence between all learning and compression models in Theorem 1 is the idea that in each case we can obtain a certain distinguisher from the corresponding algorithm. Again, this makes fundamental use of counting and concentration bounds to show that non-trivial algorithms can be used as distinguishers. On the other hand, the Speedup Lemma shows that a distinguisher can be turned into a sub-exponential time randomized learning algorithm that requires membership queries only.\nIn some models considered in the equivalence, additional work is necessary. For instance, in the learning model where equivalence queries are allowed, they must be simulated by the distinguisher. For exact compression, a hypothesis output by the sub-exponential time learner might still contain errors, and these need to be corrected by the compression algorithm. A careful investigation of the parameters involved in the proof make sure the equivalences indeed hold.\nTheorem 2 (Dichotomy between Learning and PRFs). It is well-known that the existence of learning algorithms for a class C implies that C-circuits cannot compute pseudorandom functions. Using the Speedup Lemma, it follows that the existence of non-trivial learning algorithms for C implies that C cannot compute exponentially secure pseudorandom functions.\nFor the other direction, assume that every samplable family F of functions from C can be distinguished from a random function by some procedure DF of sub-exponential complexity. By introducing a certain two-player game (Section 4.1), we are able to employ the small-support minmax theorem to conclude that there is a single circuit of bounded size that distinguishes every family of functions in C from a random function. In turn, the techniques behind the Speedup Lemma imply that every function in C can be learned in sub-exponential time.\nWe remark that the non-uniformity in the statement of Theorem 2 comes from the application of a non-constructive min-max theorem.\nTheorem 3 (Lower Bounds from Non-trivial Learning and Natural Proofs). Here we combine the Speedup Lemma with the self-reducibility approach from [IW01, TV07, FK09, KKO13] and other standard arguments. Assuming a non-trivial learning algorithm for C, we first boost it to a highaccuracy sub-exponential time learner. Now if PSPACE * C we are done, since PSPACE \u2286 BPEXP. Otherwise, using a special self-reducible complete function f \u2208 PSPACE [TV07], we are able obtain from a sub-exponential time learning algorithm for f a sub-exponential time decision algorithm computing f . Using the completeness of f and a strong hierarchy theorem for bounded-space algorithms, standard techniques allow us to translate the hardness of PSPACE against bounded-size circuits and the non-trivial upper bound on the randomized complexity of f into a non-uniform circuit lower bound for randomized exponential time. A win-win argument is used crucially to establish that no depth blow-up is necessary when moving from a non-trivial algorithm for (depthd)-C to a (depth-d)-C circuit lower bound. For C = ACC0, we combine certain complexity collapses\ninside the argument with Williams\u2019 lower bound [Wil14c]. In order to obtain even stronger lower bounds from natural properties against sub-exponential size circuits, we further combine this approach with an application of the easy witness method. This and other standard techniques lead to the collapse BPEXP = ZPEXP, which strengthens the final circuit lower bound.\nTheorem 4 (Karp-Lipton Collapse for Probabilistic Time). This result does not rely on the Speedup Lemma, but its argument is somewhat more technically involved than the proof of Theorem 3. The result is established in the contrapositive. Assuming that an attempted derandomization of BPEXP fails, we show that polynomial space can be simulated in sub-exponential randomized time. Arguing similarly to the proof of Theorem 3, we conclude that there are functions in randomized exponential time that are not infinitely often computed by small circuits.\nThe first difficulty is that the candidate derandomization procedure on n-bit inputs requires the use of the NW-generator applied to a function on nc-bit inputs, due to our setting of parameters. However, in order to invoke the self-reducibility machinery, we need to make sure the generator can be broken on every input length, and not on infinitely many input lengths. To address this, we introduce logarithmic advice during the simulation, indicating which input length in [nc, (n+ 1)c] should be used in the generator. This amount of advice is reflected in the statement of the theorem.\nA second difficulty is that if the derandomization fails on some input string of length n, it is important in the reduction to know a \u201cbad\u201d string with this property. For each input length, a bad string is passed as advice to the learning-to-decision reduction (this is the second use of advice in the proof). This time we are able to remove the advice using an advice-elimination technique, which makes use of self-correctability as in [TV07]. Crucially, the advice elimination implies that randomized exponential time without advice is not infinitely often contained in C, which completes the proof of the contrapositive of Theorem 4.\nTheorem 5 (Hardness of MCSP). Recall that this result states that MCSP is hard for NC1 with respect to non-uniform TC0 reductions. The proof of Theorem 5 explores the fine-grained complexity of the Nisan-Wigderson reconstruction procedure and of the hardness amplification reconstruction algorithm. In order words, the argument depends on the combined circuit complexity of the algorithm that turns a distinguisher for NW(f\u0303) into a high-accuracy approximating circuit for f , under the notation of the proof sketch for Lemma 1. This time we obtain a distinguisher using an oracle to MCSP. It is possible to show that this reduction can be implemented in non-uniform TC0.\nObserve that the argument just sketched only provides a randomized reduction that approximates the initial Boolean function f under the uniform distribution. But Theorem 5 requires a worst-case reduction from NC1 to MCSP. In other words, we must be able to compute any NC1 function correctly on every input. This can be achieved using that there are functions in NC1 that are NC1-hard under TC0-reductions, and that in addition admit randomized worst-case to average-case reductions computable in TC0. Using non-uniformity, randomness can be eliminated by a standard argument. Altogether, this completes the proof that NC1 reduces to MCSP via a non-uniform TC0 computation.\nThese proofs provide a few additional examples of the use of pseudorandomness in contexts where this notion is not intrinsic to the result under consideration. For instance, the connection between non-trivial learning algorithms and lower bounds (Theorem 3), the Karp-Lipton collapse for probabilistic exponential time (Theorem 4), and the hardness of the Minimum Circuit Size Problem (Theorem 5) are statements that do not explicitly refer to pseudorandomness. Nevertheless, the\narguments discussed above rely on this concept in fundamental ways. This motivates a further investigation of the role of pseudorandomness in complexity theory, both in terms of finding more applications of the \u201cpseudorandom method\u201d, as well as in discovering alternative proofs relying on different techniques."}, {"heading": "2 Preliminaries and Notation", "text": ""}, {"heading": "2.1 Boolean Function Complexity", "text": "We use Fm to denote the set of all Boolean functions f : {0, 1}m \u2192 {0, 1}. If W is a probability distribution, we use w \u223c W to denote an element sampled according to W . Similarly, for a finite set A, we use a \u223c A to denote that a is selected uniformly at random from A. Under this notation, f \u2208 Fm represents a fixed function, while f \u223c Fm is a uniformly random function. For convenience, we let Un def = {0, 1}n. Following standard notation, X \u2261 Y denotes that random variables X and Y have the same distribution. We use standard asymptotic notation such as o(\u00b7) and O(\u00b7), and it always refer to a parameter n\u2192\u221e, unless stated otherwise.\nWe say that f, g \u2208 Fn are \u03b5-close if Prx\u223cUn [f(x) = g(x)] \u2265 1\u2212 \u03b5. We say that h \u2208 Fn computes f with advantage \u03b4 if Prx\u223cUn [f(x) = h(x)] \u2265 1/2 + \u03b4. It will sometimes be convenient to view a Boolean function f \u2208 Fm as a subset of {0, 1}m in the natural way.\nWe often represent Boolean functions as strings via the truth table mapping. Given a Boolean function f \u2208 Fn, tt(f) is the 2n-bit string which represents the truth table of f in the standard way, and conversely, given a string y \u2208 {0, 1}2n , fn(y) is the Boolean function in Fn whose truth table is represented by y.\nLet C = {Cn}n\u2208N be a class of Boolean functions, where each Cn \u2286 Fn. Given a language L \u2286 {0, 1}\u2217, we write L \u2208 C if for every large enough n we have that Ln def = {0, 1}n \u2229 L is in Cn. Often we will abuse notation and view C as a class of Boolean circuits. For convenience, we use number of wires to measure circuit size. We denote by C[s(n)] the set of n-variable C-circuits of size at most s(n). As usual, we say that a uniform complexity class \u0393 is contained in C[poly(n)] if for every L \u2208 \u0393 there exists k \u2265 1 such that L \u2208 C[nk].\nWe say that C is typical if C \u2208 {AC0,AC0[p],ACC0,TC0,NC1,Formula,Circuit}. Recall that\nCNF,DNF ( AC0 ( AC0[p] ( ACC0 \u2286 TC0 \u2286 NC1 = Formula[poly] \u2286 Circuit[poly].\nWe assume for convenience that TC0 is defined using (unweighted) majority gates instead of weighted threshold gates. Also, while NC1 typically refers to circuits of polynomial size and logarithmic depth, we consider the generalized version where NC1[s] is the class of languages computed by circuits of size \u2264 s and depth \u2264 log s.\nWhile we restrict our statements to typical classes, it is easy to see that they generalize to most circuit classes of interest. When appropriate we use Cd to restrict attention to C-circuits of depth at most d. In this work, we often find it convenient to suppress the dependence on d, which is implicit for instance in the definition of a circuit family from a typical bounded-depth circuit class, such as the first four typical classes in the list above. It will be clear from the context whether the quantification over d is existential or universal.\nGiven a sequence of Boolean functions {fn}n\u2208N with fn : {0, 1}n \u2192 {0, 1}, we let Cf denote the extension of C that allows Cn-circuits to have oracle gates computing fn.\nFor a complexity class \u0393 and a language L \u2286 {0, 1}\u2217, we say that L \u2208 i.o.\u0393 if there is a language L\u2032 \u2208 \u0393 such that Ln = L\u2032n for infinitely many values of n. Consequently, if \u03931 * i.o.\u03932 then there is a language in \u03931 that disagrees with each language in \u03932 on every large enough input length.\nRecall the following diagram of class inclusions involving standard complexity classes:9\nZPP \u2286 NP \u2286 \u2286\nRP MA\u2286 \u2286 BPP\n\u2286 PSPACE \u2286 EXP \u2286 ZPEXP \u2286 NEXP \u2286 \u2286\nREXP MAEXP\u2286 \u2286 BPEXP\n\u2286 EXPSPACE.\nIn order to avoid confusion, we fix the following notation for exponential complexity classes. E refers to languages computed in time 2O(n). EXP refers to languages computed with bounds of the form 2n c for some c \u2208 N. SUBEXP denotes complexity 2n\u03b5 for a fixed but arbitrarily small \u03b5 > 0. Finally, ESUBEXP refers to a bound of the form 22 n\u03b5\n, again for a fixed but arbitrarily small \u03b5 > 0. These conventions are also used for the DSPACE(\u00b7) and BPTIME(\u00b7) variants, such as BPE, BPSUBEXP and EXPSPACE. For instance, a language L \u2286 {0, 1}\u2217 is in BPSUBEXP if for every \u03b5 > 0 there is a bounded-error randomized algorithm that correctly computes L in time \u2264 2n\u03b5 on every input of length n, provided that n is sufficiently large. For quasi-polynomial time classes such as RQP and BPQP, the convention is that for each language in the class there is a constant c \u2265 1 such that the corresponding algorithm runs in time at most O(n(logn) c ).\nWe will use a few other standard notions, and we refer to standard textbooks in computational complexity and circuit complexity for more details."}, {"heading": "2.2 Learning and Compression Algorithms", "text": "The main learning model with which we concern ourselves is PAC learning under the uniform distribution with membership queries.\nDefinition 1 (Learning Algorithms). Let C be a circuit class. Given a size function s : N\u2192 N and a time function T : N\u2192 N, we say that C[s] has (\u03b5(n), \u03b4(n))-learners running in time T (n) if there is a randomized oracle algorithm Af (the learner) such that for every large enough n \u2208 N:\n\u2022 For every function f \u2208 C[s(n)], given oracle access to f , with probability at least 1\u2212\u03b4(n) over its internal randomness, Af (1n) outputs a Boolean circuit h such that Prx\u223cUn [f(x) 6= h(x)] \u2264 \u03b5(n).\n\u2022 For every function f , Af (1n) runs in time at most T (n).\nIt is well-known that the confidence of a learning algorithm can be amplified without significantly affecting the running time (cf. [KV94a]), and unless stated otherwise we assume that \u03b4(n) = 1/n.\nA weak learner for C[s(n)] is a (1/2\u22121/nc, 1/n)-learner, for some fixed c > 0 and sufficiently large n. We say C[s] has strong learners running in time T if for each k \u2265 1 there is a (1/nk, 1/n)-learner for C[s] running in time T . Different values for the accuracy parameter k can lead to different running times, but we will often need only a fixed large enough k when invoking the learning algorithm. On the other hand, when proving that a class has a strong learner, we show that the claimed asymptotic running time holds for all fixed k \u2208 N. For simplicity, we may therefore omit the dependence of T on k. We say that C[s] has non-trivial learners if it has (1/2\u2212 1/nk, 1/n)-learners running in time T (n) = 2n/n\u03c9(1), for some fixed k \u2208 N.\nWe also discuss randomized learning under the uniform distribution with membership queries and equivalence queries [Ang87]. In this stronger model, the learning algorithm is also allowed to make queries of the following form: Is the unknown function f computed by the Boolean circuit\n9Non-uniform lower bounds against unrestricted polynomial size circuits are currently known only for MAEXP, the exponential time analogue of MA [BFT98].\nC? Here C is an efficient representation of a Boolean circuit produced be the learner. The oracle answers \u201cyes\u201d if the Boolean function computed by C is f ; otherwise it returns an input x such that C(x) 6= f(x).\nDefinition 2 (Compression Algorithms). Given a circuit class C and a size function s : N\u2192 N, a compression algorithm for C[s] is an algorithm A for which the following hold :\n\u2022 Given an input y \u2208 {0, 1}2n, A outputs a circuit D (not necessarily in C) of size o(2n/n) such that if fn(y) \u2208 C[s(n)] then D computes fn(y).\n\u2022 A runs in time polynomial in |y| = 2n.\nWe say C[s] admits compression if there is a (polynomial time) compression algorithm for C[s].\nWe will also consider the following variations of compression. If the algorithm is probabilistic, producing a correct circuit with probability \u2265 2/3, we say C[s] has probabilistic compression. If the algorithm produces a circuit D which errs on at most \u03b5(n) fraction of inputs for fn(y) in C[s], we say that A is an average-case compression algorithm with error \u03b5(n). We define correspondingly what it means for a circuit class to have average-case compression or probabilistic average-case compression."}, {"heading": "2.3 Natural Proofs and the Minimum Circuit Size Problem", "text": "We say that R = {Rn}n\u2208N is a combinatorial property (of Boolean functions) if Rn \u2286 Fn for all n. We use LR to denote the language of truth-tables of functions in R. Formally, LR = {y | y = tt(f) for some f \u2208 Rn and n \u2208 N}.\nDefinition 3 (Natural Properties [RR97]). Let R = {Rn} be a combinatorial property, C a circuit class, and D a (uniform or non-uniform) complexity class. We say that R is a D-natural property useful against C[s(n)] if there is n0 \u2208 N such that the following holds:\n(i) Constructivity. LR \u2208 D.\n(ii) Density. For every n \u2265 n0, Prf\u223cFn [f \u2208 Rn] \u2265 1/2.\n(iii) Usefulness. For every n \u2265 n0, we have Rn \u2229 Cn[s(n)] = \u2205.\nDefinition 4 (Minimum Circuit Size Problem). Let C be a circuit class. The Minimum Circuit Size Problem for C, abbreviated as MCSP-C, is defined as follows:\n\u2022 Input. A pair (y, s), where y \u2208 {0, 1}2n for some n \u2208 N, and 1 \u2264 s \u2264 2n is an integer (inputs not of this form are rejected).\n\u2022 Question. Does fn(y) have C-circuits of size at most s?\nWe also define a variant of this problem, where the circuit size is not part of the input.\nDefinition 5 (Unparameterized Minimum Circuit Size Problem). Let C be a circuit class, and s : N\u2192 N be a function. The Minimimum Circuit Size Problem for C with parameter s, abbreviated as MCSP-C[s], is defined as follows:\n\u2022 Input. A string y \u2208 {0, 1}2n, where n \u2208 N (inputs not of this form are rejected).\n\u2022 Question. Does fn(y) have C-circuits of size at most s(n)?\nNote that a dense property useful against C[s(n)] is a dense subset of the complement of MCSPC[s]."}, {"heading": "2.4 Randomness and Pseudorandomness", "text": "Definition 6 (Pseudorandom Generators). Let ` : N\u2192 N, h : N\u2192 N and \u03b5 : N\u2192 [0, 1] be functions, and let C be a circuit class. A sequence {Gn} of functions Gn : {0, 1}`(n) \u2192 {0, 1}n is an (`, \u03b5) pseudorandom generator (PRG) against C[h(n)] if for any sequence of circuits {Dn} with Dn \u2208 C[h(n)] and for all large enough n,\u2223\u2223\u2223\u2223 Prw\u223cUn[Dn(w) = 1]\u2212 Prx\u223cU`(n)[Dn(Gn(x)) = 1]\n\u2223\u2223\u2223\u2223 \u2264 \u03b5(n). The pseudorandom generator is called quick if its range is computable in time 2O(`(n)).\nTheorem 6 (PRGs from computational hardness [NW94, IW97]). Let s : N \u2192 N be a timeconstructible function such that n \u2264 s(n) \u2264 2n for every n \u2208 N. There is a constant c > 0 and an algorithm which, given as input n in unary and the truth table of a Boolean function on s\u22121(n) bits which does not have circuits of size nc, computes the range of a (`(n), 1/n) pseudorandom generator against Circuit[n] in time 2O(`(n)), where `(n) = O((s\u22121(n))2/ log n).\nDefinition 7 (Distinguishers and Distinguishing Circuits). Given a probability distribution Wn with Support(Wn) \u2286 {0, 1}n and a Boolean function hn : {0, 1}n \u2192 {0, 1}, we say that hn is a distinguisher for Wn if \u2223\u2223\u2223\u2223 Prw\u223cWn[hn(w) = 1]\u2212 Prx\u223cUn[hn(x) = 1]\n\u2223\u2223\u2223\u2223 \u2265 1/4. We say that a circuit Dn is a circuit distinguisher for Wn if Dn computes a function hn that is a distinguisher for Wn. A function f : {0, 1}\u2217 \u2192 {0, 1} is a distinguisher for a sequence of distributions {Wn} if for each large enough n, fn is a distinguisher for Wn, where fn is the restriction of f to n-bit inputs.\nThe following is a slight variant of a definition in [CIKK16].\nDefinition 8 (Black-Box Generator). Let ` : N \u2192 N, \u03b3(n) \u2208 [0, 1], and C be a circuit class. A black-box (\u03b3, `)-function generator within C is a mapping that associates to any f : {0, 1}n \u2192 {0, 1} a family GEN(f) = {gz}z\u2208{0,1}m of functions gz : {0, 1}` \u2192 {0, 1}, for which the following conditions hold :\n(i) Family size. The parameter m \u2264 poly(n, 1/\u03b3).\n(ii) Complexity. For every z \u2208 {0, 1}m, we have gz \u2208 Cf [poly(m)].\n(iii) Reconstruction. Let L = 2` and WL be the distribution supported over {0, 1}L that is generated by tt(gz), where z \u223c Um. There is a randomized algorithm Af , taking as input a circuit D and having oracle access to f , which when D is a distinguishing circuit for WL, with probability at least 1\u22121/n outputs a circuit of size poly(n, 1/\u03b3, size(D)) that is \u03b3-close to f . Furthermore, Af runs in time at most poly(n, 1/\u03b3, L(n)).\nThis definition is realized by the following result.\nTheorem 7 (Black-Box Generators for Restricted Classes [CIKK16]). Let p be a fixed prime, and C be a typical circuit class containing AC0[p]. For every \u03b3 : N\u2192 [0, 1] and ` : N\u2192 N there exists a black-box (\u03b3, `)-function generator within C.\nDefinition 9 (Complexity Distinguisher). Let C be a circuit class and consider functions s, T : N\u2192 N. We say that a probabilistic oracle algorithm Ag is a complexity distinguisher for C[s(n)] running in time T if Ag(1n) always halts in time T (n) with an ouput in {0, 1}, and the following hold :\n\u2022 For every g \u2208 C[s(n)], PrA[Ag(1n) = 1] \u2264 1/3.\n\u2022 Eg\u223cFn,A[Ag(1n)] \u2265 2/3.\nDefinition 10 (Zero-Error Complexity Distinguisher). Let C be a circuit class and s, T : N\u2192 N be functions. We say that a probabilistic oracle algorithm Ag is a zero-error complexity distinguisher for C[s(n)] running in time T if Ag(1n) always halts in time T (n) with an output in {0, 1, ?}, and the following hold :\n\u2022 If g \u2208 C[s(n)], Ag(1n) always outputs 0 or ?, and PrA[Ag(1n) = ?] \u2264 1/3.\n\u2022 For every n \u2265 1 there exists a family of functions Sn \u2286 Fn with |Sn|/|Fn| \u2265 1 \u2212 o(1) such that for every f \u2208 Sn, Af (1n) always outputs 1 or ?, and PrA[Af (1n) = ?] \u2264 1/3.\nWe will make use of the following standard concentration of measure result.\nLemma 2 (Chernoff Bound, cf. [J LR00, Theorem 2.1]). Let X \u223c Bin(m, p) and \u03bb = mp. For any t \u2265 0,\nPr[|X \u2212 E[X]| \u2265 t] \u2264 exp ( \u2212 t 2\n2(\u03bb+ t/3)\n) ."}, {"heading": "3 Learning Speedups and Equivalences", "text": ""}, {"heading": "3.1 The Speedup Lemma", "text": "We start with the observation that the usual upper bound on the number of small Boolean circuits also holds for unbounded fan-in circuit classes with additional types of gates.\nLemma 3 (Bound on the number of functions computed by small circuits). Let C be a typical circuit class. For any s : N\u2192 N satisfying s(n) \u2265 n there are at most 250s(n) log s(n) functions in Fn computed by C-circuits of size at most s(n).\nProof. A circuit over n input variables and of size at most s(n) can be represented by its underlying directed graph together with information about the type of each gate. A node of the graph together with its gate type can be described using O(log s(n)) bits, since for a typical circuit class there are finitely many types of gates. In addition, each input variable can be described as a node of the graph using O(log n) = O(log s(n)) bits, since by assumption s(n) \u2265 n. Finally, using this indexing scheme, each wire of the circuit corresponding to a directed edge in the circuit graph can be represented with O(log s(n)) bits. Consequently, as we measure circuit size by number of wires, any circuit of size at most s(n) can be represented using at most O(s(n) log s(n)) bits. The lemma follows from the trivial fact that a Boolean circuit computes at most one function in Fn and via a conservative estimate for the asymptotic notation.\nLemmas 2 and 3 easily imply the following (folklore) result.\nLemma 4 (Random functions are hard to approximate). Let C be a typical circuit class, s \u2265 n, and \u03b4 \u2208 [0, 1/2]. Then,\nPr f\u223cFn\n[\u2203C-circuit of size \u2264 s(n) computing f with advantage \u03b4(n)] \u2264 exp ( \u2212\u03b422n\u22121 + 50s log s ) .\nProof. Let g \u2208 Fn be a fixed function. It follows from Lemma 2 with p = 1/2, m = 2n, t = \u03b42n, and using \u03b4 \u2264 1/2 that\nPr f\u223cFn\n[g computes f with advantage \u03b4(n)] \u2264 exp ( \u2212\u03b4 22n\n2\n) .\nThe claim follows immediately from this estimate, Lemma 3, and a union bound.\nLemma 5 (Non-trivial learners imply distinguishers). Let C be a typical circuit class, s : N\u2192 N be a size bound, and T : N\u2192 N be a time bound such that T (n) = 2n/n\u03c9(1). If C[s] has weak learners running in time T , then C[s(n)] has complexity distinguishers running in time T (n) \u00b7 poly(n).\nProof. By the assumption that C is weakly learnable, there is a probabilistic oracle algorithm Aflearn, running in time T (n) on input 1\nn, which when given oracle access to f \u2208 C[s], outputs with probability at least 1\u2212 1/n a Boolean circuit h which agrees with f on at least a 1/2 + 1/nk fraction of inputs of length n, for some universal constant k. We show how to construct from Aflearn an oracle algorithm Afdist which is a complexity distinguisher for C[s].\nAfdist operates as follows on input 1 n. It runs Aflearn on input 1 n. If Aflearn does not output a hypothesis, Afdist outputs \u20181\u2019. Otherwise A f dist estimates the agreement between the hypothesis h output by the learning algorithm and the function f by querying f on n5k inputs of length n chosen uniformly at random, and checking for each such input whether f agrees with h. The estimated agreement is computed to be the fraction of inputs on which f agrees with h. If it is greater than 1/2 + 1/n2k, Afdist outputs \u20180\u2019, otherwise it outputs \u20181\u2019.\nBy the assumption on efficiency of the learner Aflearn, it follows that A f dist runs in time T (n) \u00b7\npoly(n). Thus we just need to argue that Afdist is indeed a complexity distinguisher.\nFor a uniformly random f , the probability that Aflearn outputs a hypothesis h that has agreement greater than 1/2 + 1/n4k with f is exponentially small. This is because Aflearn runs in time T (n) = 2n/n\u03c9(1), and hence if it outputs a hypothesis, it must be of size at most 2n/n\u03c9(1). By Lemma 4, only an exponentially small fraction of functions can be approximated by circuits of such size. Also, given that a circuit h has agreement at most 1/2 + 1/n4k with f , the probability that the estimated agreement according to the procedure above is greater than 1/2 + 1/n2k is exponentially small by Lemma 2. Thus, for a uniformly random f , the oracle algorithm Afdist outputs \u20180\u2019 with exponentially small probability, and hence for large enough n, it outputs \u20181\u2019 with probability at least 2/3.\nFor f \u2208 C[s(n)], by the correctness and efficiency of the learning algorithm, Aflearn outputs a hypothesis h with agreement at least 1/2 + 1/nk with f , with probability at least 1\u22121/n. For such a hypothesis h, using Lemma 2 again, the probability that the estimated agreement is smaller than 1/2 + 1/n2k is exponentially small. Thus, for n large enough, with probability at least 2/3, Afdist outputs \u20180\u2019.\nLemma 6 (Faster learners from distinguishers). Let C be a typical circuit class. If C[poly(n)] has complexity distinguishers running in time 2O(n), then for every \u03b5 > 0, C[poly(n)] has strong learners running in time O(2n \u03b5 ). If for some \u03b5 > 0, C[2n \u03b5 ] has complexity distinguishers running in time 2O(n), then C[poly(n)] has strong learners running in time 2log(n) O(1) .\nProof. We prove the first part of the Lemma, and the second part follows analogously using a different parameter setting.\nLet C be a typical circuit class. If C = AC0, the lemma holds unconditionally since this class can be learned in quasi-polynomial time [LMN93]. Assume otherwise that C contains AC0[p], for\nsome fixed prime p. By assumption, C[poly(n)] has a complexity distinguisher Ag0 running in time 2O(n). We show that for every \u03b5 > 0 and every k > 0, C[poly(n)] has (1/nk, 1/n)-learners running in time O(2n \u03b5 ). Let \u03b5\u2032 > 0 be any constant such that \u03b5\u2032 < \u03b5. By Theorem 7 there exists a black-box (\u03b3, `)-function generator GEN within C, where \u03b3 = 1/nk and ` = n\u03b5 \u2032 . For this setting of \u03b3 and ` we have that the parameter m for GEN(f) in Definition 8 is poly(n), and that for each z \u2208 {0, 1}m, we have gz \u2208 Cf [poly(n)]. Let Af1 be the randomized reconstruction algorithm for GEN(f).\nWe define a (1/nk, 0.99)-learner Af for C[poly(n)] running in time O(2n \u03b5 ); the confidence can then be amplified to satisfy the definition of a strong learner while not increasing the running time of the learner by more than a polynomial factor. The learning algorithm operates as follows. It interprets the oracle algorithm Ag0 on input 1\n` as a probabilistic polynomial-time algorithm D(\u00b7, ~r) which is explicitly given the truth table of g, of size L = 2`, as input, with ~r the randomness for this algorithm. It guesses ~r at random and then computes a circuit DL of size 2\nO(`) which is equivalent to D(\u00b7, ~r) on inputs of size 2`, using the standard transformation of polynomial-time algorithms into circuits. It then runs Af1 on input DL, and halts with the same output as A f 1 . Observe that the queries made by the reconstruction algorithm can be answered by the learner, since it also has query access to f .\nUsing the bounds on running time of A0 and A1, it is easy to see that A f can be implemented\nto run in time 2O(`), which is at most 2n for large enough n. We need to argue that Af is a correct strong learner for C[poly(n)]. The critical point is that when f \u2208 C[poly(n)], with noticeable probability, DL is a distinguishing circuit for WL (using the terminology of Definition 8), and we can then take advantage of the properties of the reconstruction algorithm. We now spell this out in more detail.\nWhen f \u2208 C[poly(n)], using the fact that C is typical and thus closed under composition with itself, and that it contains AC0[p], we have that for each z \u2208 {0, 1}m, gz \u2208 C[poly(n)]. Note that the input size for gz is ` = n\n\u03b5\u2032 , and hence also gz \u2208 C[poly(`)]. Using now that A0 is a complexity distinguisher, we have that for any z \u2208 {0, 1}m, PrA[Agz(1`) = 1] \u2264 1/3, while Eg\u223cF`,A[Ag(1`)] \u2265 2/3. By a standard averaging argument and the fact that probabilities are bounded by 1, this implies that with probability at least 0.05 over the choice of ~r, DL is a distinguishing circuit for WL. Under the properties of the reconstruction algorithm A f 1 , when given as input such a circuit DL, with probability at least 1\u2212 1/n, the output of Af1 is 1/nk-close to f . Hence with probability at least 0.05 \u00b7 (1 \u2212 1/n) > 0.01 over the randomness of A, the output of Af is 1/nk-close to f , as desired. As observed before, the success probability of the learning algorithm can be amplified by standard techniques (cf. [KV94a]).\nThe second part of the lemma follows by the same argument with a different choice of parameters, using a black-box (\u03b3, `)-function generator with \u03b3 = 1/nk and ` = (log n)c, where c is chosen large enough as a function of \u03b5. Again, the crucial point is that the relative circuit size of each gz compared to its number of input bits is within the size bound of the distinguisher.\nRemark 1. While Lemma 6 is sufficient for our purposes, we observe that the same argument shows in fact that the conclusion holds under the weaker assumption that the complexity distinguisher runs in time 2n c , for a fixed c \u2208 N. In other words, it is possible to obtain faster learners from complexity distinguishers running in time that is quasi-polynomial in the length of the truth-table of its oracle function.\nLemma 7 (Speedup Lemma). Let C be a typical circuit class. The following hold :\n\u2022 (Low-End Speedup) C[poly(n)] has non-trivial learners if and only if for each \u03b5 > 0, C[poly(n)] has strong learners running in time O(2n \u03b5 ).\n\u2022 (High-End Speedup) There exists \u03b5 > 0 such that C[2n\u03b5 ] has non-trivial learners if and only if C[poly(n)] has strong learners running in time 2log(n) O(1) .\nProof. First we show the Low-End Speedup result. The \u201cif\u201d direction is trivial, so we only need to consider the \u201conly if\u201d case. This follows from Lemma 6 and Lemma 5. Indeed, by Lemma 5, if C[poly(n)] has non-trivial learners, it has complexity distinguishers running in time 2n/n\u03c9(1). By Lemma 6, the existence of such complexity distinguishers implies that for each \u03b5 > 0, C[poly(n)] has strong learners running in time O(2n \u03b5 ), and we are done.\nNext we show the High-End Speedup result. The proof for the \u201conly if\u201d direction is completely analogous to the corresponding proof for the Low-End Speedup result. The \u201cif\u201d direction, however, is not entirely trivial. We employ a standard padding argument to establish this case, thus completing the proof of Lemma 7.\nSuppose that C[poly(n)] has a strong learner running in time 2log(n) c , for some constant c > 0. Let Alow be a learning algorithm witnessing this fact. We show how to use Alow to construct a learning algorithm Ahigh which is a (1/n, 1/poly(n))-learner for C[2 n1/3c ], and runs in time \u2264 2 \u221a n. As usual, confidence can be boosted without a significant increase of running time, and it follows that C[2n 1/3c ] has non-trivial learners according to our definition.\nOn input 1n and with oracle access to some function f : {0, 1}n \u2192 {0, 1}, Afhigh(1 n) simulates\nAf \u2032 low(1 n\u2032), where n\u2032 def = n + 2dn 1/3ce, and f \u2032 : {0, 1}n\u2032 \u2192 {0, 1} is the (unique) Boolean function satisfying the following properties. For any input x\u2032 = xy \u2208 {0, 1}n\u2032 , where |y| = 2dn1/3ce and |x| = n, f \u2032(x\u2032) is defined to be f(x). Note that if f \u2208 C[2n1/3c ] then f \u2032 \u2208 C[O(n\u2032)]: the linear-size C-circuit for f \u2032 on an input x\u2032 of length n\u2032 just simulates the smallest C-circuit for f on its n-bit prefix. During the simulation, whenever Alow makes an oracle call x\n\u2032 to f \u2032, Ahigh answers it using an oracle call x to f , where x is the prefix of x\u2032 of length n. By definition of f \u2032, this simulation step is always correct. When Af \u2032\nlow completes its computation and outputs a hypothesis h \u2032 on n\u2032\ninput bits, Ahigh outputs a modified hypothesis h as follows: it chooses a random string r of length 2dn 1/3ce, and outputs the circuit hr defined by hr(x) def = h\u2032(xr). Note that by the assumed efficiency of Alow, Ahigh halts in time \u2264 2 \u221a n on large enough n.\nBy the discussion above, it is enough to argue that the hypothesis h output by Ahigh is a good hypothesis with probability at least 1/poly(n). Since Alow is a strong learner and since the f\n\u2032 used as oracle to Alow in the simulation has linear size, for all large enough n, with probability at least 1 \u2212 1/n\u2032, h\u2032 disagrees with f \u2032 on at most a 1/(n\u2032)k fraction of inputs of length n\u2032, where k is a large enough constant fixed in the construction above. Consider a randomly chosen r of length n\u2032 \u2212 n. By a standard Markov-type argument, when h\u2032 is good, for at least a 1/poly(n) fraction of the strings r, hr(x) disagrees with f(x) on at most a 1/n fraction of inputs. This completes the argument."}, {"heading": "3.2 Equivalences for Learning, Compression, and Distinguishers", "text": "Theorem 8 (Algorithmic Equivalences). Let C be a typical circuit class. The following statements are equivalent :\n1. C[poly(n)] has non-trivial learners.\n2. For each \u03b5 > 0 and k \u2208 N, C[poly(n)] can be learned to error \u2264 n\u2212k in time O(2n ).\n3. C[poly(n)] has probabilistic (exact) compression.\n4. C[poly(n)] has probabilistic average-case compression with error o(1).\n5. C[poly(n)] has complexity distinguishers running in time 2O(n).\n6. For each \u03b5 > 0, C[poly(n)] has complexity distinguishers running in time O(2n ).\n7. C[poly(n)] can be learned using membership and equivalence queries to sub-constant error in non-trivial time.\nProof. We establish these equivalences via the following complete set of implications:\n(5)\u21d2 (2): This follows from Lemma 6. (2)\u21d2 (1) and (6)\u21d2 (5): These are trivial implications. (2)\u21d2 (4): Probabilistic compression for C[poly(n)] follows from simulating a (1/n3, 1/n)-learner for the class running in time O(2 \u221a n), and answering any oracle queries by looking up the corresponding bit in the truth table of the function, which is given as input to the compression algorithm. The compression algorithm returns as output the hypothesis of the strong learner, and by assumption this agrees on a (1\u2212 1/n3) fraction of inputs of length n with the input function, with probability at least 1 \u2212 1/n. Moreover, since the simulated learner runs in time O(2 \u221a n), the circuit that is output has size at most O(2 \u221a n). It is clear that the simulation of the learner can be done in time 2O(n), as required for a compression algorithm. (2) \u21d2 (3): This follows exactly as above, except that there is an additional step after the simulation of the learner. Once the learner has output a hypothesis h, the compression algorithm compares this hypothesis with its input truth table entry by entry, simulating h whenever needed. If h differs from the input truth table on more than a 1/n3 fraction of inputs, the compression algorithm rejects \u2013 this happens with probability at most 1/n by assumption on the learner. If h and the input truth table differ on at most 1/n3 fraction of inputs of length n, the compression algorithm computes by brute force a circuit of size at most 2n/n2 which computes the function h\u2032 that is the XOR of h and the input truth table. The upper bound on size follows from the fact that h\u2032 has at most 2n/n3 1\u2019s. Finally, the compression algorithm outputs h\u2295 h\u2032. For any typical circuit class, the size of the corresponding circuit is O(2n/n2). Note that h\u2295h\u2032 computes the input truth table exactly.\n(2)\u21d2 (6): This follows from Lemma 5. (1) \u21d2 (5), (3) \u21d2 (5), and (4) \u21d2 (5): The distinguisher runs the circuit output by the learner or compression algorithm on every input of length n, and computes the exact agreement with its input f on length n by making 2n oracle queries to f . If the circuit agrees with f on at least a 2/3 fraction of inputs, the distinguisher outputs 0, otherwise it outputs 1. By the assumption on the learner/compression algorithm, for f \u2208 C[poly(n)], the distinguisher outputs 0 with probability at least 2/3. Using Lemma 4, for a random function, the probability that the distinguisher outputs 1 is at least 2/3.\n(7) \u21d2 (5): The complexity distingisher has access to the entire truth-table, and can answer the membership and equivalence queries of the learner in randomized time 2O(n). Randomness is needed only to simulate the random choices of the learning algorithm, while the answer to each query can be computed in deterministic time. Since the learner runs in time 2n/n\u03c9(1), whenever it succeeds it outputs a hypothesis circuit of at most this size. The complexity distinguisher can compare this hypothesis to its input truth-table, and similarly to the arguments employed before, is able to distinguish random functions from functions in C[poly(n)].\n(2) \u21d2 (7): This is immediate since the algorithm from (2) is faster, has better accuracy, and makes no equivalence queries.\nTheorem 9 (Equivalences for zero-error algorithms). Let C[poly(n)] be a typical circuit class. The following statements are equivalent :\n1. There are P-natural proofs useful against C[poly(n)].\n2. There are ZPP-natural proofs useful against C[poly(n)].\n3. For each \u03b5 > 0, there are DTIME(O(2(logN) \u03b5 ))-natural proofs useful against C[poly(n)], where\nN = 2n is the truth-table size.\n4. For each \u03b5 > 0, there are zero-error complexity distinguishers for C[poly(n)] running in time O(2n \u03b5 ).\nProof. We establish these equivalences via the following complete set of implications:\n(1)\u21d2 (2): This is a trivial implication. (3)\u21d2 (4): This is almost a direct consequence of the definitions, except that the density of the natural property has to be amplified to 1 \u2212 o(1) before converting the algorithm into a zero-error complexity distinguisher. This is a standard argument, and can be achieved by defining a new property from the initial one. More details can be found, for instance, in the proof of [CIKK16, Lemma 2.7].\n(2) \u21d2 (1):10 Let A be an algorithm running in zero-error probabilistic time mk on inputs of length m and with error probability \u2264 1/4, for m large enough and k an integer, and deciding a combinatorial property R useful against C[poly(n)]. We show how to define a combinatorial property R\u2032 useful against C[poly(n)] such that R\u2032 \u2208 P, and such that at least a 1/8 fraction of the truth tables of any large enough input length belong to LR\u2032 . This fraction can be amplified by defining a new natural property R\u2032\u2032 such that any string yz with |y| = |z| belongs to LR\u2032\u2032 if and only if either y \u2208 LR\u2032 or z \u2208 LR\u2032 (see e.g. [CIKK16]).\nWe define R\u2032 via a deterministic polynomial-time algorithm A\u2032 deciding LR\u2032 . Given an input truth table y of size 2n \u2032 , A\u2032 acts as follows: it determines the largest integer n such that n(k+1) < n\u2032. It decomposes the input truth table as y = xzw, where |x| = 2n, |z| = 2kn, and the remaining part w is irrelevant. It runs A on x, using z as the randomness for the simulation of A. If A accepts, it accepts; if A rejects or outputs \u2018?\u2019, it rejects.\nIt should be clear that A\u2032 runs in polynomial time. The fact that A\u2032 accepts at least a 1/8 fraction of truth tables of any large enough input length follows since for any x \u2208 LR, A outputs \u2018?\u2019 with probability at most 1/3, and at least a 1/2 fraction of strings of length 2n are in LR. It only remains to argue that the property R\u2032 is useful against C[poly(n)]. But any string y of length 2n \u2032 accepted by A\u2032 has as a substring the truth table of a function on n = \u2126(n\u2032) bits which is accepted by A and hence is in LR. Since R is useful against C[poly(n)], this implies that R\n\u2032 is useful against C[poly(n)].\n(4)\u21d2 (3): The proof is analogous to (2)\u21d2 (1). (3)\u21d2 (1): This is a trivial direction since N = 2n. (1) \u21d2 (3): This implication uses an idea of Razborov and Rudich [RR97]. Suppose there are P-natural proofs useful against C[poly(n)]. This means in particular that for every c \u2265 1, there is a polynomial-time algorithm Ac, which on inputs of length 2\nn, where n \u2208 N, accepts at least a 1/2 fraction of inputs, and rejects all inputs y such that fn(y) \u2208 C[nc]. Consider an input y to Ac of length 2n, and let \u03b5 > 0 be fixed. Let y\u2032 be the substring of y such that fn(y\u2032) is the subfunction of fn(y) obtained by fixing the first n\u2212 n\u03b5 bits of the input to fn(y) to 0. It is easy to see that if fn(y) \u2208 C[nc], then fn(y\u2032) \u2208 C[(n\u2032)c/\u03b5], where n\u2032 denotes the number of input bits of fn(y\u2032).\nLet d \u2265 1 be any constant, and \u03b5 > 0 be fixed. We show how to define an algorithm Bd which runs in time O(2log(N) \u03b5 ) on an input of length N = 2n, deciding a combinatorial property which\n10This argument is folklore. It has also appeared in more recent works, such as [Wil16].\nis useful against C-circuits of size nd. (Using the same approach, it is possible to design a single algorithm that works for any fixed d whenever n is large enough, provided that we start with a natural property that is useful in this stronger sense.) On input y of length N , Bd computes y\n\u2032 of length 2log(N) \u03b5 , as defined in the previous paragraph. For the standard encoding of truth tables, y\u2032 is a prefix of y, and can be computed in time O(|y\u2032|). Bd then simulates Add/\u03b5e on y\u2032, accepting if and only if the simulated algorithm accepts. The simulation halts in time poly(|y\u2032|), as Add/\u03b5e is a poly-time algorithm. For a random input y, Bd accepts with probability at least 1/2, using that y \u2032 is uniformly distributed, and the assumption that Add/\u03b5e witnesses natural proofs against a circuit class. For an input y such that fn(y) \u2208 C[nd], Bd always rejects, as in this case, fn(y\u2032) \u2208 C[(n\u2032)d/\u03b5], and so Add/\u03b5e rejects y \u2032, using the assumption that Add/\u03b5e decides a combinatorial property useful against n-bit Boolean functions in C[ndd/\u03b5e]."}, {"heading": "4 Learning versus Pseudorandom Functions", "text": ""}, {"heading": "4.1 The PRF-Distinguisher Game", "text": "In this section we consider (non-uniform) randomized oracle circuits BO from CircuitO[t], where t is an upper bound on the number of wires in the circuit. Recall that a circuit from this class has a special gate type that computes according to the oracle O, which will be set to some fixed Boolean function f : {0, 1}m \u2192 {0, 1} whenever we discuss the computation of the circuit.\nWe will view such circuits either as distinguishers or learning algorithms, where the oracle is the primary input to the circuit. For this reason and because our results are stated in the non-uniform setting, we assume from now on that such circuits have no additional input except for variables y1, . . . , y` representing the random bits, where ` \u2264 t. If w \u2208 {0, 1}` is a fixed sequence of bits, we use BOw to denote the deterministic oracle circuit obtaining from the circuit B\nO by setting its randomness to w. Observe that (non-uniform) learning algorithms can be naturally described by randomized oracle circuits from CircuitO with multiple output bits. The output bits describe the output hypothesis, under some reasonable encoding for Boolean circuits.11\nWe will consider pairs (Gn,Dn) where Gn \u2286 Fn and Dn is a distribution with Support(Dn) \u2286 Gn. This notation is convenient when defining samplable function families and pseudorandom function families.\nDefinition 11 (Pseudorandom Function Families). We say that a pair (Gn,Dn) is a (t(n), \u03b5(n))pseudorandom function family (PRF) in C[s(n)] if Gn \u2286 C[s(n)] and for every randomized oracle circuit BO \u2208 CircuitO[t(n)],\u2223\u2223\u2223\u2223 Prg\u223cDn, w[Bg(w) = 1]\u2212 Prf\u223cFn, w[Bf (w) = 1]\n\u2223\u2223\u2223\u2223 \u2264 \u03b5. This definition places no constraint on the complexity of generating the pair (Gn,Dn). In order to capture this, we restrict attention to Gn \u2286 Cn for some typical circuit class C = {Cn}, and assume a fixed encoding of circuits from C by strings of length polynomial in the size of the circuit. We say that a circuit A \u2208 Circuit[S] is a Cn-sampler if A outputs valid descriptions of circuits from Cn.\nDefinition 12 (Samplable Function Families). We say that a pair (Gn,Dn) with Gn \u2286 Cn is Ssamplable if there exists a Cn-sampler A \u2208 Circuit[S] on ` \u2264 S input variables such that A(U`) \u2261 Dn, where we associate each output string of A to its corresponding Boolean function.\n11In this non-uniform framework it is possible to derandomize a learning circuit with some blow-up in circuit size, but we will not be concerned with this matter here.\nIt is well-known that the existence of learning algorithms for a circuit class Cn[s(n)] implies that there are no secure pseudorandom function families in Cn[s(n)]. Moreover, this remains true even for function families that are not efficiently samplable. Following the notation from Definition 1, we can state a particular form of this observation as follows.\nProposition 1 (Learning C implies no PRFs in C). Assume there is a randomized oracle circuit in CircuitO[t(n)] that (1/3, 1/n)-learns every function in Cn[s(n)], where n \u2264 t(n) \u2264 2n/n2. Then for large enough n there are no (poly(t(n)), 1/10)-pseudorandom function families in Cn[s(n)].\nOur goal for the rest of this section is to establish a certain converse of Proposition 1 (Theorem 11 and Corollary 1). An important technical tool will be a \u201csmall-support\u201d version of the min-max theorem, described next.\nSmall-Support Approximate Min-Max Theorem for Bounded Games [Alt94, LY94]. We follow the notation from [LY94]. Let M be an r \u00d7 c real-valued matrix, p be a probability distribution over its rows, and q be a probability distribution over its columns. The classic minmax theorem [vN28] states that\nmin p max j\u2208[c] Ei\u223cp[M(i, j)] = max q min i\u2208[r] Ej\u223cq[M(i, j)]. (1)\nThe distributions p and q are called mixed strategies, while individual indexes i and j are called pure strategies. We use v(M) to denote the value in Equation 1. (Recall that this can be interpreted as a game between a row player, or Minimizer, and a column player, or Maximizer. The min-max theorem states that the order in which the players reveal their strategies does not change the value of the game. It is easy to see that the second player can be restricted to pure strategies.)\nWe will consider a game played on a matrix of exponential size, and will be interested in nearoptimal mixed strategies with succinct descriptions. This motivates the following definitions. A mixed strategy is k-uniform if it is selected uniformly from a multiset of at most k pure strategies. We use Pk and Qk to denote the set of k-uniform strategies for the row player and the column player, respectively. For convenience, given a mixed row strategy p, we let v(p) = vM (p) = maxj\u2208[c] Ei\u223cp[M(i, j)]. Similarly, we use v(q) = vM (q) = mini\u2208[r] Ej\u223cq[M(i, j)] for a column mixed strategy q. We say that a mixed strategy u is \u03b4-optimal if |v(u)\u2212 v(M)| \u2264 \u03b4.\nWe will need the following \u201cefficient\u201d version of the min-max theorem.\nTheorem 10 (Small-Support Min-Max Theorem [Alt94, LY94]). Let M be a r \u00d7 c real-valued matrix with entries in the interval [\u22121, 1]. For every \u03b4 > 0, if kr \u2265 10 ln(c)/\u03b42 and kc \u2265 10 ln(r)/\u03b42 then\nmin p\u2208Pkr v(p) \u2264 v(M) + \u03b4, and max q\u2208Qkc v(q) \u2265 v(M)\u2212 \u03b4.\nIn other words, there are \u03b4-optimal strategies for the row and column players with relatively small support size.\nThe PRF-Distinguisher Game. Let Cn[s] be a circuit class and CircuitO[t] be an oracle circuit class, with size parameters s(n) and t(n), respectively. We consider a [\u22121, 1]-valued matrix M = MCn[s],Circuit\nO[t], defined as follows. The rows of M are indexed by Boolean functions in Cn[s], and the columns of M are indexed by (single-output) deterministic oracle circuits from CircuitO[t]. In other words, such circuit have access to constants 0 and 1, compute according to the values of the oracle gates, and produce an output value in {0, 1}. In order not to introduce further notation,\nwe make the simplifying assumption that the negation of every circuit from CircuitO[t] is also in CircuitO[t]. For h \u2208 Cn[s] and CO \u2208 CircuitO[t], we let\nM(h,CO) def = Ch \u2212 Pr f\u223cFn [Cf = 1],\nwhere Cg \u2208 {0, 1} denotes the output of CO when computing with oracle O = g, for a fixed g : {0, 1}n \u2192 {0, 1}. We say that the matrix M is the PRF-Distinguisher game for Cn[s] and CircuitO[t]. Observe that this is a finite matrix, for every choice of n.\nFollowing our notation, we use v(M) to denote the value of the game corresponding to M , which can be interpreted as follows. Let p be a mixed strategy for the row player. In other words, p is simply a distribution over functions from Cn[s]. Consequently, to each row strategy p we can associate a pair (Gp,Dp), where p = Dp and Gp = Support(Dp), as in Definition 11. On the other hand, a mixed strategy q over the columns is simply a distribution over deterministic oracle circuits from CircuitsO[t], which can be interpreted as a (non-constructive) randomized circuit BO. Under this interpretation, the value of the game when played with strategies p and q is given by\nEh\u223cp, CO\u223cq[M(h,CO)] = Eh,CO [Ch \u2212 Pr f\u223cFn [Cf = 1]]\n= Eh,CO [Ch]\u2212 Pr f,CO\u223cq [Cf = 1]\n= Pr g\u223cDp, BO [Bg = 1]\u2212 Pr f\u223cFn, BO [Bf = 1],\nwhich corresponds to the distinguishing probability in Definition 11 without taking absolute values. But since we assumed that the circuits indexing the columns of M are closed under complementation, it follows that the (global) value v(M) of this game captures the security of PRFs from Cn[s] against CircuitO[t]-distinguishers. (Notice though that this value does not take into account the samplability of the function families involved, nor the constructivity of the ensemble of distinguishers corresponding to a \u201crandomized\u201d oracle distinguisher in the argument above.)"}, {"heading": "4.2 A (Non-Uniform) Converse to \u201cLearning Implies no PRFs\u201d", "text": "We proceed with our original goal of establishing a converse of Proposition 1. Roughly speaking, we want to show that if every samplable function family from Cn can be distinguished from a random function (possibly by different distinguishers), then there is a single algorithm that learns every function in Cn. Formally, what we get is a sequence of subexponential size (non-uniform) circuits learning C.\nThe proofs of Lemmas 8 and 9 below rely on Theorem 10.\nLemma 8 ( \u2203 samplable PRF \u2192 \u2203 PRF against ensembles of circuits). There exists a universal constant c \u2208 N for which the following holds. Let t(n) \u2265 n, s(n) \u2265 n, \u03b4(n) > 0, and \u03b5(n) > 0 be arbitrary functions. If there is no O(t \u00b7s \u00b71/\u03b4)c-samplable pair (G\u0303n, D\u0303n) that is a (t(n), \u03b5(n)+\u03b4(n))PRF in Cn[s(n)], then there is no pair (Gn,Dn) with Gn \u2286 Cn[s(n)] that \u03b5(n)-fools every ensemble of deterministic CircuitO[t(n)]-circuits.\nProof. We use Theorem 10 to establish the contrapositive. Assume there exists a pair (Gn,Dn) where Dn is distributed over Gn \u2286 Cn[s(n)] such that for every distribution q over CircuitO[t(n)] we have \u2223\u2223\u2223\u2223 Pr\ng\u223cDn,CO\u223cq [Cg = 1]\u2212 Pr f\u223cFn,CO\u223cq [Cf = 1]\n\u2223\u2223\u2223\u2223 \u2264 \u03b5(n).\nLet p = Dn, and observe that in the corresponding PRF-Distinguisher game we get vM (p) \u2264 \u03b5(n). Consequently, v(M) \u2264 minp vM (p) \u2264 \u03b5(n). It follows from Theorem 10 and a bound on the number of columns of M (similar to Lemma 3) that there exists a k-uniform distribution p\u0303 over functions in Cn[s(n)] with k \u2264 O(ln 2O(t log t)/\u03b4(n)2) = O((t log t)/\u03b4(n)2) such that vM (p\u0303) \u2264 \u03b5(n) + \u03b4(n).\nIn other words, each f \u2208 Support(p\u0303) is in Cn[s(n)], the support of this distribution contains at most O((t log t)/\u03b4(n)2) different functions, and each such function can be encoded by a string of length poly(s(n)) that describes the corresponding circuit. Using that p\u0303 is a k-uniform distribution, it is not hard to see that there exists a circuit A \u2208 Circuit[S] with A(U`) \u2261 p\u0303 for some ` \u2264 S, where S \u2264 poly(t, s, 1/\u03b4). Since every randomized circuit BO can be seen as a distribution over deterministic oracle circuits, it follows that there is an S-samplable pair (G\u0303n, D\u0303n) that is a (t(n), \u03b5(n) + \u03b4(n))-PRF in Cn[s(n)]. This completes the proof.\nLemma 9 ( \u2203 PRF against ensembles of circuits \u2192 \u2203 universal distinguisher). There exists a universal constant c \u2208 N for which the following holds. Let s(n) \u2265 n, t(n) \u2265 n, \u03b5(n) > 0, and \u03b3(n) > 0 be arbitrary functions. If there is no pair (Gn,Dn) with Gn \u2286 Cn[s(n)] that \u03b5(n)-fools every ensemble of deterministic CircuitO[t(n)]-circuits, then there is a randomized oracle circuit BO \u2208 CircuitO[O(t \u00b7 s \u00b7 1/\u03b3)c] that distinguishes every such pair from a random function with advantage at least \u03b5(n)\u2212 \u03b3(n).\nProof. We rely on the classical min-max theorem and on Theorem 10. It follows from the assumption of the lemma that the corresponding PRF-Distinguisher game has value v(M) \u2265 \u03b5(n). By the min-max theorem, there is an ensemble of CircuitO[t(n)]-circuits that distinguishes every pair (Gn,Dn) satisfying Gn \u2286 Cn[s(n)] with advantage at least \u03b5(n). Applying Theorem 10, we obtain a k-uniform distribution q over deterministic CircuitO[t(n)]-circuits with distinguishing probability at least \u03b5(n)\u2212\u03b3(n) and support size at most k = O(ln 2O(s log s)/\u03b3(n)2) = O((s log s)/\u03b3(n)2). Similarly to the proof of Lemma 8, this ensemble of circuits implies the existence of a single randomized oracle circuit BO \u2208 CircuitO[O(s\u00b7t\u00b71/\u03b3)c] that distinguishes every pair (Gn,Dn) with Gn \u2286 Cn[s(n)] from a random function with advantage at least \u03b5(n)\u2212 \u03b3(n). This completes the proof.\nLemmas 8 and 9 hold for each value of n. The next lemma is a reduction involving different values of this parameter.\nLemma 10 (\u2203 universal distinguishers \u2192 \u2203 learning circuits). Assume that for every k \u2265 1 and large enough n there exists a randomized oracle circuit BOn in Circuit\nO[2O(n)] that distinguishes every pair (Gn,Dn) with Gn \u2286 Cn[nk] from a random function with advantage \u2265 1/40. Then for every ` \u2265 1 and \u03b5 > 0 there is a non-uniform sequence of randomized oracle circuits in CircuitO[2n\u03b5 ] that learn every function f \u2208 Cn[n`] to error at most n\u2212`.\nProof. This lemma is simply a (weaker) non-uniform version of the proof of Lemma 6 from Section 3. It is enough to use the sequence of randomized oracle circuits BOn as distinguishing circuits, and to observe that the statement of Theorem 7 holds with an arbitrarily small constant in the distinguishing probability.\nRecall that C = {Cn} is an arbitrary typical circuit class. The main technical result of this section follows from Lemmas 8, 9, and 10 together with an appropriate choice of parameters.\nTheorem 11 (No samplable PRFs in C implies Learning C). If t(n) \u2264 2O(n) and c\u2032 \u2265 1 is a large enough constant, the following holds. Suppose that for every k \u2265 1 each O((t(n) \u00b7 nk)c\u2032)-samplable pair (Gn,Dn) with Gn \u2286 Cn[nk] can be distinguished from a random function by some randomized oracle circuit from CircuitO[t(n)] with advantage at least 1/10. Then, for every k \u2265 1, \u03b5 > 0, and\nlarge enough n, there is a randomized oracle circuit from CircuitO[2n \u03b5 ] that learns every function in Cn[nk] to error at most n\u2212k.\nProof. The existence of the learning circuit will follow if we can prove that the hypothesis of Lemma 10 is satisfied. Thus it is enough to argue that, for every k \u2265 1 and large enough n, there is a (single) randomized oracle circuit BO from CircuitO[2O(n)] that distinguishes with advantage \u2265 1/40 every pair (Gn,Dn) with Gn \u2286 Cn[nk]. In turn, this follows from Lemma 9 for s(n) = nk, \u03b5(n) = 1/20, and \u03b3(n) = 1/40 if there is no pair (Gn,Dn) with Gn \u2286 Cn[nk] that 1/20-fools every ensemble of deterministic oracle circuits from CircuitO[2O(n)], for a slightly smaller constant in the latter exponent. But this is implied by the hypothesis of Theorem 11 together with Lemma 8, instantiated with our value t(n) \u2264 2O(n), s(n) = nk, \u03b5(n) = 1/20, and \u03b4(n) = 1/20, provided that we take c\u2032 sufficiently large. This completes the proof.\nDropping the samplability condition, we get the following weaker statement, which provides a converse of Proposition 1 in the regime where t(n) is exponential and s(n) is polynomial.\nCorollary 1 (No PRFs in C implies Learning C). Let t(n) \u2264 2O(n). If for every k \u2265 1 and large enough n there are no (poly(t(n)), 1/10)-pseudorandom function families in Cn[nk], then for every \u03b5 > 0, k \u2265 1, and large enough n, there is a randomized oracle circuit in CircuitO[2n\u03b5 ] that (n\u2212k, 1/n)-learns every function in Cn[nk].\nWe observe that smaller time bounds t(n) do not necessarily lead to smaller learning circuits, due to the running time of the black-box generator in Definition 8 and Theorem 7. However, a smaller t(n) implies a weaker samplability condition in the statement of Theorem 11, which makes it stronger. A natural question is whether a more efficient distinguisher implies that larger circuits can be distinguished by subexponential size oracle circuits, in analogy to Lemma 7. We mention that no simple reduction via padding seems to work, since a random function on n bits mapped into a larger domain via projections is no longer a uniformly random function. Finally, the distinguishing advantage 1/10 is arbitrary. Indeed, it can be assumed to be much lower, by following the estimates in the proof of Theorem 7.\nRemark 2. In order to prove Theorem 11, we have made essential use of the \u201cefficient\u201d min-max theorem from [Alt94, LY94], which guarantees the existence of near-optimal mixed strategies with simple descriptions. Unfortunately, this result does not provide an efficient algorithm to produce such strategies, which would lead to an equivalence between learning algorithms and the nonexistence of pseudorandom functions with respect to uniform computations. While there are more recent works that explore uniform versions of the min-max theorem (cf. [VZ13]), they assume the existence of certain auxiliary algorithms in order to construct the near-optimal strategies, and it is unclear to us if they can be applied in the context of Theorem 11."}, {"heading": "5 Lower Bounds from Nontrivial Algorithms", "text": "Theorem 12 (Circuit lower bounds from nontrivial learning algorithms). Let C be any typical circuit class. If for each k, C[nk] has non-trivial learning algorithms, then for each k, BPTIME(2O(n)) 6\u2286 C[nk].\nOur proof of Theorem 12 relies on previous results relating randomized learning algorithms and lower bounds. The following connection was established in [KKO13], using ideas from [IW01, FK09]) and most crucially the construction of a downward self-reducible and random self-reducible PSPACE-complete language in [TV07].\nTheorem 13 (Connection between learning and lower bounds [KKO13, FK09, IW01]). There is a PSPACE-complete language L? \u2208 DSPACE(n) and a constant b \u2208 N for which the following holds. Let C be any typical circuit class, and s : N\u2192 N be any function with s(n) \u2265 n. If C[s(n)] is learnable to error \u2264 n\u2212b in time T (n) \u2265 n, then at least one of the following conditions hold:\n(i) L? /\u2208 C[s(n)].\n(ii) L? \u2208 BPTIME(poly(T (n))).\nA self-contained proof of a generalization of Theorem 13 is presented in Section 6. We will also need a consequence of the following diagonalization lemma.\nLemma 11 (A nonuniform almost everywhere hierarchy for space complexity). Let S, S\u2032 : N \u2192 N be space-constructible functions such that S(n) = o(S\u2032(n)), S(n) = \u2126(log n) and S\u2032(n) = o(2n). There is a language L \u2208 DSPACE(S\u2032) such that L 6\u2208 i.o.DSPACE(S)/S.\nProof. This is a folklore argument. We define a space-bounded Turing machine M operating in space S\u2032 such that L(M) 6\u2208 i.o.DSPACE(S)/S. On inputs of length n, M uses the spaceconstructibility of S\u2032 to compute S\u2032(n) in unary using space O(S\u2032(n)). It marks out S\u2032(n) cells on each of its tapes, and if at any point in its computation, it reads an unmarked cell, it halts and rejects. Thus, on any input of length n, M uses space O(S\u2032(n)). M also computes and stores S(n) on one of its tapes.\nThe high-level intuition is that M diagonalizes against machine Mi with advice z, for each 1 \u2264 i \u2264 log n and advice z \u2208 {0, 1}S(n). In particular, for any fixed i and large enough n, M diagonalizes against Mi with any advice z \u2208 {0, 1}S(n), and hence L(M) satisfies the conclusion of the Lemma.\nBy a counting argument, there are at most log n \u00b72S(n) truth tables of Boolean functions f on n bits such that f is computed by a machine Mi with 1 \u2264 i \u2264 log n operating in space S(n) and using S(n) bits of advice. Thus, since S(n) = o(2n), for large enough n, by the pigeon-hole principle there exists a Boolean function f \u2032 : {0, 1}n \u2192 {0, 1} which is 0 on all but the first logn+ S(n) inputs of length n, such that f \u2032 is not computed by machine Mi with advice z for any i with 1 \u2264 i \u2264 log n and z \u2208 {0, 1}S(n).\nM computes such a function iteratively as follows. It processes the inputs of length n in lexicographic order. At stage i + 1, where i \u2265 0, M has stored a binary string yi of length i representing the values of f \u2032 on the first i inputs of length n, and M is trying to determine f \u2032 on the (i+ 1)-th input of length n. For each machine Mi, 1 \u2264 i \u2264 log n, and each advice string z for Mi of length S(n), by simulating those Mi\u2019s with advice z which do not use space more than S(n) on any of the first i inputs, M determines if the truth table of Mi with advice z is consistent with yi on the first i inputs. Call such a pair (i, z) a consistent machine-advice pair at stage i+ 1. M sets f \u2032 to 0 for the (i+ 1)-th string if a minority of consistent machine-advice pairs halt with 0 on the (i+ 1)-th string, and to 1 otherwise. Determining whether a minority of consistent machine-advice pairs halt with 0 on the (i + 1)-th string can be done by merely keeping a count of how many consistent machine-advice pairs halt with 0, and how many halt with 1, which only requires space O(S(n)). Note that using the minority value cuts down the number of consistent machine-advice pairs for the next stage by at least a factor of half. This implies that at stage log n + S(n), there are no consistent machine-advice pairs left, and hence M has successfully diagonalized. It is not hard to see that the overall simulation can be carried out in space O(S(n)), using the fact that S(n) = \u2126(log n).\nCorollary 2 (Diagonalizing in uniform space against non-uniform circuits). Let S1, S2 : N \u2192 N be space-constructible functions such that S2(n)2 = o(S1(n)), S2(n) = \u2126(log n) and S1(n) = o(2\nn). There is a language L \u2208 DSPACE(S1) such that L 6\u2208 i.o.Circuit[S2]. In particular, for each k, there is a language Lk \u2208 PSPACE such that Lk 6\u2208 Circuit[nk].\nProof. Corollary 2 follows from Lemma 11 using the fact that Circuit[S] \u2286 DSPACE(S2)/S2.\nIn fact, a tighter simulation holds, and therefore a tighter separation in Corollary 2, but we will not need this for our purposes. We are now ready to prove Theorem 12.\nProof of Theorem 12. Let C be a typical circuit class. By assumption, C[nk] has a non-trivial learner for each k > 0. Since C is typical, we can use Lemma 7 to conclude that for each \u03b5 > 0 and for each k > 0, C[nk] is strongly learnable in time 2n \u03b5 .\nLet L? be the PSPACE-complete language in the statement of Theorem 13. Using Theorem 13 and the conclusion of the previous paragraph, we have that at least one of the following is true: (1) For all k, L? 6\u2208 C[nk], or (2) For all \u03b5 > 0, L? \u2208 BPTIME(2n\u03b5).\nIn case (1), since L? \u2208 DSPACE(n) \u2286 DTIME(2O(n)), we have that for each k > 0, DTIME(2O(n)) 6\u2286 C[nk], and hence also BPTIME(2O(n)) 6\u2286 C[nk].\nIn case (2), we have that L? \u2208 BPTIME(2n\u03b5) for every \u03b5 > 0. Since L? is PSPACE-complete, this implies that the language Lk in the statement of Corollary 2 is also in BPTIME(2\nn\u03b5), for every fixed \u03b5 > 0 and k \u2208 N. (Here the polynomial blowup of instance size in the reduction from Lk to L? is taken care of by the universal quantification over \u03b5.) In particular, we have Lk \u2208 BPTIME(2n), for every k. Since for any typical circuit class we have C[nk] \u2286 Circuit[nc] for a large enough c = c(k), there is a language Lc \u2208 BPTIME[2n] such that Lc /\u2208 C[nk]. This establishes the desired result.\nWe mention for completeness that the same approach yields a trade-off involving the running time of the learning algorithm and its accuracy in the hypothesis of Theorem 12.\nTheorem 14 (Trade-off between error and running time). Let C be a typical circuit class, and \u03b3 : N\u2192 (0, 1/2] \u2229 Q be a polynomial time computable function. If for each k, C[nk] can be learned with advantage at least \u03b3(n) in time \u03b3(n)2 \u00b7 2n/n\u03c9(1), then for each k, BPTIME[2O(n)] * C[nk].\nProof. (Sketch) The proof is entirely analogous to the argument in Theorem 12. It is enough to observe that such learning algorithms yield the complexity distinguishers required in Lemma 7 via a natural generalization of the proof of Lemma 5. The quantitative trade-off between accuracy and running time is a consequence of Lemma 4.\nRemark 3. Observe that as the advantage \u03b3(n) approaches 2\u2212n/2 from above, the running time required in Theorem 14 becomes meaningless. This quantitative connection between \u03b3(n) and the running time is not entirely unexpected. On the one hand, it is a consequence of the concentration bound, which is essentially optimal. But also note that every function g : {0, 1}n \u2192 {0, 1} can be approximated with advantage \u2265 2\u2212n/2 by a parity function (or its negation), and that heavy fourier coefficients corresponding to such parity functions can be found using membership queries by the Goldreich-Levin Algorithm (see e.g. [O\u2019D14]).\nWe can expand the scope of application of Theorem 12, using a win-win argument. The more general result below applies to subclasses of Boolean circuits satisfying the very weak requirement that they are closed under projections, rather than just to the more specialized \u201ctypical\u201d classes.\nTheorem 15 (Lower bounds from non-trivial learning algorithms for subclasses of circuits). Let C be any subclass of Boolean circuits closed under projections. If for each k, C[nk] has non-trivial learning algorithms, then for each k, BPTIME(2O(n)) 6\u2286 C[nk].\nProof. Consider the Circuit Value Problem (CVP), which is complete for Circuit[poly] under polynomial size projections. Either CVP is in C[nc] for some fixed c, or it is not. If it is not, then we have the desired lower bound for CVP and hence also for the class BPTIME(2O(n)), which contains this problem. If CVP is in C[nc], then since CVP is closed under poly-size projections, we have by completeness and the assumption of the theorem that for each k, Circuit[nk] has non-trivial learning algorithms. Now applying Theorem 12, we have that for each k, BPTIME(2O(n)) 6\u2286 Circuit[nk], which implies that BPTIME(2O(n)) 6\u2286 C[nk], since C is a subclass of Boolean circuits.\nRemark 4. Observe that it is possible to instantiate Theorem 15 for very particular classes such as AND \u25e6 OR \u25e6 THR circuits, and that the lower bound holds for exactly the same circuit class. In particular, there is no circuit depth blow-up.\nWe get an improved lower bound consequence for the circuit class ACC0, but under the assumption that subexponential-size circuits are non-trivially learnable. (Recall that there are satisfiability algorithms for such circuits with non-trivial running time [Wil14c].)\nTheorem 16 (Improved lower bounds from non-trivial learning algorithms for ACC0). If for every depth d \u2208 N and modulo m \u2208 N there is \u03b5 > 0 such that ACC0d,m[2n \u03b5 ] has non-trivial learning algorithms, then REXP 6\u2286 ACC0[poly].\nProof. Under the assumption on learnability, using Lemma 7, we have that for each k > 0, ACC0[nk] has strong learners running in time 2polylog(n). Now applying Theorem 13, we have that at least one of the following is true for the PSPACE-complete language L? in the statement of the theorem: (1) L? 6\u2208 ACC0[nk] for any k, or (2) L? \u2208 BPQP, where BPQP is bounded error probabilistic quasi-polynomial time.\nIn case (1), we have that L? 6\u2208 ACC0[poly], and are done as in the proof of Theorem 12. In case (2), by PSPACE-completeness of L?, we have that PSPACE \u2286 BPQP. This implies that NP \u2286 BPQP, and hence that NP \u2286 RQP, where RQP is probabilistic quasi-polynomial time with one-sided error. The second implication follows using downward self-reducibility to find a witness for SAT given the assumption that SAT is in BPQP, thus eliminating error on negative instances. Now NP \u2286 RQP implies NEXP = REXP, using a standard translation argument. Williams showed that NEXP 6\u2286 ACC0[poly], and so it follows that REXP 6\u2286 ACC0[poly], as desired.\nMore generally, the same argument combined with the connection between non-trivial satisfiability algorithms and circuit lower bounds [Wil14c] imply the following result.\nCorollary 3 (Lower bounds from learning and satisfiability). Let C be any typical circuit class. Assume that for each k, C[nk] admits a non-trivial satisfiability algorithm, and that for some \u03b5 > 0, C[2n \u03b5 ] admits a non-trivial learning algorithm. Then REXP * C[poly].\nRecall that randomized learning algorithms and BPP-natural properties are strongly related by results of [CIKK16]. We can give still stronger lower bound conclusions from assumptions about P-natural proofs. The idea is to combine the arguments above with an application of the easy witness method of Kabanets [Kab01].\nTheorem 17 (Improved lower bounds from natural proofs). Let C be any subclass of Boolean circuits closed under projections. If there are P-natural proofs useful against C[2n \u03b5 ] for some \u03b5 > 0, then ZPEXP 6\u2286 C[poly].\nThe following immediate consequence is of particular interest.\nCorollary 4 (ACC0 lower bounds from natural proofs). If for some \u03b4 > 0 there are P-natural proofs against ACC0[2n \u03b4 ] then ZPEXP * ACC0[poly].\nIn order to prove Theorem 17, we will need the following lemma.\nLemma 12 (Simulating bounded error with zero error given natural proofs). Suppose there is a constant \u03b4 > 0 such that there are P-natural proofs against Circuit[2n \u03b4 ]. Then BPEXP = ZPEXP.\nProof. Note that zero-error probabilistic time is trivially contained in bounded-error probabilistic time, so we only need to show that BPEXP \u2286 ZPEXP under the assumption. We will in fact show that BPP \u2286 ZPQP, where ZPQP is zero-error bounded probabilistic quasi-polynomial time. The desired conclusion follows from this using a standard translation argument.\nBy assumption, there is a natural property R useful against Circuit[2n \u03b4 ] for some constant \u03b4 > 0, such that LR \u2208 P. Let M be any machine operating in bounded-error probabilistic time nd for some d > 0. We define a zero-error machine M \u2032 deciding L(M) in quasi-polynomial time as follows. On input x of length n, M \u2032 guesses a random string r of size 2log(n) d\u2032\n, where d\u2032 is a large enough constant to be defined later. It then checks if r \u2208 LR or not, using the polynomial-time decision procedure for the natural property R. If not, it outputs \u2018?\u2019 and halts. If it does, it runs the procedure of Theorem 6 on input n2d in unary and r, to obtain the range of a (polylog(n), 1/n2d) PRG against Circuit[n2d]. Since r \u2208 LR, Theorem 6 applies, and the output of the procedure is guaranteed to be the range of such a PRG. M \u2032 then runs M on x independently with each element of the range of the PRG used as randomness, and takes the majority vote. This is guaranteed to be correct when r \u2208 LR, which happens with probability at least 1/2 by the density property of R. Thus M \u2032 is a zero-error machine, and it is clear that M \u2032 can be implemented in quasi-polynomial time.\nProof of Theorem 17. We proceed as in the proof of Theorem 15. Either CVP is in C[poly], or it is not. If not, we have the desired lower bound for CVP, and hence for ZPEXP, which contains this problem.\nOn the other hand, if CVP is in C[poly], we have that CVP is in C[nk] for some k > 0. By the completeness of CVP for poly-size circuits under poly-size projections, and the closure of C under projections, we have that Circuit[n] \u2286 C[nk] for some k > 0, and hence by a standard translation argument, we have that Circuit[2n \u03b4 ] \u2286 C[2n\u03b5 ] for any \u03b4 < \u03b5. By assumption, we have Pnatural properties useful against C[2n \u03b5 ] and hence we also have P-natural properties useful against Circuit[2n \u03b4 ] for any \u03b4 < \u03b5. Now, applying Lemma 12, we get BPEXP = ZPEXP.\nWe argue next that under the existence of P-natural properties useful against Circuit[2n \u03b4 ] for a fixed \u03b4 > 0, we also have EXPSPACE = BPEXP. The mentioned hypothesis implies that there exist complexity distinguishers against Circuit[2n \u03b4 ] running in deterministic time 2O(n) (the acceptance probability can be amplified using truth-table concatenation). As a consequence, Lemma 6 provides strong learning algorithms for Circuit[poly] running in quasi-polynomial time. By Theorem 13, either PSPACE * Circuit[poly], and we are done, or PSPACE \u2286 BPQP. Now a standard upward translation gives EXPSPACE \u2286 BPEXP, which shows that EXPSPACE = BPEXP.\nAltogether, we have EXPSPACE = ZPEXP. Now this collapse and Corollary 2 with S1(n) = 2 \u221a n and S2(n) = n logn yield a language L \u2208 ZPEXP such that L /\u2208 Circuit[poly], which completes the proof of Theorem 17.\nRecall that the existence of useful properties against a circuit class C is essentially equivalent to the existence of non-deterministic exponential time lower bounds against C [Wil16, Oli15]. We do\nnot expect a similar equivalence in the case of natural properties and lower bounds for probabilistic exponential time. The results described in this section show that natural properties imply such lower bounds. However, if the other direction were true, then any lower for C with respect to probabilistic exponential time classes would also provide a non-trivial learning algorithm for C. In particular, since we believe in separations such as EXP * Circuit[poly], this would imply via the Speedup Lemma that polynomial size circuits can be learned in sub-exponential time, which seems unlikely."}, {"heading": "6 Karp-Lipton Collapses for Probabilistic Classes", "text": ""}, {"heading": "6.1 A Lemma About Learning with Advice", "text": "In this section we will need some notions of computability with advice. While this is a standard notion, we provide some definitions, as bounded-error randomized algorithms taking advice can be defined in different ways.\nRecall that an advice-taking Turing machine is a Turing machine equipped with an extra tape, the advice tape. At the start of any computation of an advice-taking Turing machine, the input is present on the input tape of the machine and a string called the \u201cadvice\u201d on the advice tape of the machine, to both of which the machine has access.\nDefinition 13 (Probabilistic time with advice). Let T : N \u2192 N and a : N \u2192 N be functions. BPTIME(T )/a is the class of languages L \u2286 {0, 1}\u2217 for which there is an advice-taking probabilistic Turing machine M which always halts in time T (n) and a sequence {zn}n\u2208N of strings such that :\n1. For each n, |zn| \u2264 a(n).\n2. For any input x \u2208 L such that |x| = n, M accepts x with probability at least 2/3 when using advice string zn.\n3. For any input x 6\u2208 L such that |x| = n, M rejects x with probability at least 2/3 when using advice string zn.\nNote that in the above definition, there are no guarantees on the behaviour of the machine for advice strings other than the \u201ccorrect\u201d advice string zn. In particular, for an arbitrary advice string, the machine does not have to satisfy the bounded-error condition on an input, though it does have to halt within time T .\nThe notion of resource-bounded computation with advice is fairly general and extends to other models of computation, such as deterministic computation and computation of non-Boolean functions. These extensions are natural, and we will not define them formally.\nA slightly less standard notion of computation with advice is learnability with advice. We extend Definition 1 to capture learning with advice by giving the learning algorithm an advice string, and only requiring the learning algorithm to work correctly for a \u201ccorrect\u201d advice string of the requisite length.\nWe will also need the standard notions of downward self-reducibility and random self-reducibility.\nDefinition 14 (Downward self-reducibility). A function f : {0, 1}\u2217 \u2192 {0, 1} is said to be downward self-reducible if there is a polynomial-time oracle procedure Af (x) such that :\n1. On any input x of length n, Af (x) only makes queries of length < n.\n2. For every input x, Af (x) = f(x).\nDefinition 15 (Random self-reducibility). A function f : {0, 1}\u2217 \u2192 {0, 1} is said to be random self-reducible if there are constants k, ` \u2265 1 and polynomial-time computable functions g : {0, 1}\u2217 \u2192 {0, 1}\u2217 and h : {0, 1}\u2217 \u2192 {0, 1} satisfying the following conditions:\n1. For large enough n, for every x \u2208 {0, 1}n and for each i \u2208 N such that 1 \u2264 i \u2264 nk, g(i, x, r) \u223c Un when r \u223c Un`.\n2. For large enough n and for every function f\u0303n : {0, 1}n \u2192 {0, 1} that is (1/nk)-close to f on n-bit strings, for every x \u2208 {0, 1}n:\nf(x) = h(x, r, f\u0303n(g(1, x, r)), f\u0303n(g(2, x, r)), . . . , f\u0303n(g(n k, x, r)))\nwith probability \u2265 1\u2212 2\u22122n when r \u223c Un`.\nTheorem 18 (A special PSPACE-complete function [TV07]). There is a PSPACE-complete function fTV : {0, 1}\u2217 \u2192 {0, 1} such that fTV is downward self-reducible and random self-reducible.\nBelow we consider the learnability of the class of Boolean functions {fTV} that contains only the function fTV.\nLemma 13 (Learnability with advice for PSPACE implies randomized algorithms). For any polynomial-time computable non-decreasing function a : N\u2192 N with a(n) \u2264 n, and for any non-decreasing function T : N\u2192 N such that n \u2264 T (n) \u2264 2n, if {fTV} is strongly learnable in time T with a bits of advice, then fTV is computable in bounded-error probabilistic time T (n)\n2\u00b72a(n)\u00b7nO(1), and hence PSPACE \u2286 BPTIME(T (poly(n))2 \u00b7 2a(poly(n)) \u00b7 poly(n)).\nProof. The argument is based on and extends ideas from [KKO13, FK09, TV07, IW01]. Recall that fTV is the same Boolean function as in the statement of Theorem 18. As stated there, this function is downward self-reducible and random self-reducible. Now suppose {fTV} is learnable in time T with a bits of advice. We design a probabilistic machine M solving fTV on inputs of length n with bounded error in time T (n)2 \u00b72a(n) \u00b7nO(1). The addition inclusion in the statement of Lemma 13 follows from the completeness of fTV.\nLet x \u2208 {0, 1}n be the input to M . Let Alearn be a (1/n4k, 1/22n)-learner for {fTV} that takes a(n) bits of advice and runs in time T (n).12 Here k is the exponent in the number of queries in the random self-reduction for fTV given by Theorem 18.\nOverview. The plan of the proof is that M will use the advice-taking learner to inductively produce, with high probability, circuits computing fTV correctly on inputs of length 1 . . . n. The crucial aspect is not to allow the size of these circuits to grow too large. There will be n phases in the operation of M \u2013 during Phase i, M will produce with high probability a randomized circuit computing fTV on inputs of length i.\nEach phase consists of 2 parts. In Part 1 of Phase i, M computes, for each possible advice string z of length a(i) that can be fed to the advice-taking learner on input 1i, a candidate deterministic circuit Czi on i-bit inputs of size at most T (i). In order for M to do this, it uses the properties of the learner, as well as the circuits for smaller lengths that have already been computed. The only guarantee on the candidate circuits is that at least one of them is a approximately correct\n12In this argument, we do not care about poly(n) multiplicative factors applied to the final running time, so we can assume the failure probability of the learner to be exponentially small by amplification. This is a standard argument, and we refer to [KV94a] for more details.\ncircuit for fTV at length i, in the sense that it is correct on most inputs of this length. In Part 2 of Phase i, M uses the random self-reducibility and downward self-reducibility of fTV to select the \u201cbest-performing\u201d candidate among these circuits and compute a \u201ccorrection\u201d Ci of the bestperforming circuit. The circuit Ci will have size T (i) \u00b7 poly(n), and with high probability, it will be a randomized circuit that computes fTV correctly on all i-bit inputs, in the sense that on each such string it is correct with overwhelming probability over its internal randomness. At the end of Phase n, M evaluates the circuit Cn on x and outputs the answer.\nWe now give the details of how Part 1 and Part 2 work for each phase. We will then need to argue that M is correct, and that it is as efficient as claimed. Phase 1, which is the base case for M \u2019s inductive operation, is trivial. The circuit C1 computing fTV correctly on inputs of length 1 is simply hard-coded into M .\nNow let i > 1 be an integer. We describe how Part 1 and Part 2 of Phase i work, assuming inductively that M already has stored in memory a sequence of circuit {Cj}, for 1 \u2264 j \u2264 i \u2212 1, such that for each such j, Cj has size at most T (j) \u00b7 poly(n), and with all but exponentially small probability, computes fTV correctly on each input of length j.\nPart 1. M first uses the polynomial-time computability of a to compute a(i). It then cycles over strings z \u2208 {0, 1}a(i), and for each string z it does the following. It simulates Alearn(1i) with advice z. Each time Alearn makes a membership query of length i, M answers the membership query using the downward self-reducibility of fTV as follows. If the downward self-reduction makes a query of length j < i, M answers it by running the stored circuit Cj on the corresponding query.\nIf Alearn(1 i) with advice z does not halt with an output that is a circuit on i bits, M sets Czi to be a trivial circuit on i bits, say the circuit that always outputs 0. Otherwise M sets Czi to be the circuit output by the learning algorithm. Since Alearn is guaranteed to halt in time T (i) for every advice string, the circuit Czi has size at most T (i).\nPart 2. M samples strings y1, . . . , yt, where t = n 10k, uniformly and independently at random amongst i-bit strings. It computes \u201cguesses\u201d b1, . . . , bt \u2208 {0, 1} for the values of fTV on these inputs by running the downward self-reducibility procedure for fTV, and answering any queries of length j < i using the stored circuits Cj . Then, for each advice string z, it simulates C z i on each input y`, where 1 \u2264 ` \u2264 t, and computes the fraction \u03c1z of inputs y` for which Czi (y`) = b`. Let zmax be the advice string z for which \u03c1zmax is maximum among all such advice strings. Let Di be the (deterministic) circuit Czmaxi . M produces a randomized circuit Ci from Di as follows. Ci applies the random self-reduction procedure for fTV O(n) times independently, using the circuit Di to answer the random queries to fTV, and outputs the majority answer of these runs. Note that Ci can easily be implemented in size T (i) \u00b7poly(n), using the fact that the random self-reduction procedure runs in polynomial time. (We stress that Ci is a randomized circuit even though Di is deterministic.)\nIt is sufficient to argue that M halts in time T (n) \u00b7 poly(n) \u00b7 2a(n), and that the final circuit Cn computed by M is a correct randomized circuit for fTV on inputs of length n with high probability over the random choices of M .\nComplexity of M . We will show that M uses time at most T (i)2 \u00b7 poly(n) \u00b7 2a(i) in Phase i, and computes a circuit Ci of size at most T (i) \u00b7 poly(n). Since a and T are non-decreasing, this implies that M uses time at most T (n)2 \u00b7 poly(n) \u00b7 2a(n) in total. We will analyze Part 1 and Part 2 separately.\nThe first step in Part 1, which is computing a(i), can be done in time poly(n). For each z,\nsimulating the learner and computing the circuit Czi can be done in time at most T (i) \u00b7 T (i \u2212 1) \u00b7 poly(n), since the learner runs in time T (i) and makes at most that many oracle queries, each of which can be answered by simulating a circuit Cj of size at most T (j) \u00b7 poly(n), where j \u2264 i \u2212 1. There are 2a(i) advice strings z which M cycles over, hence the total time taken by M in Part 1 of Phase i is at most T (i)2 \u00b7 2a(i) \u00b7 poly(n) by the non-decreasing property of T .\nIn Part 2 of Phase i, computing the bits b1, . . . , bt takes time at most T (i) \u00b7 poly(n), since the downward self-reducibility procedure runs in time poly(n), and every query can be answered by simulation of a circuit Cj with j < i in time at most T (i) \u00b7 poly(n). For each Czi , computing the fraction \u03c1z takes time at most T (i) \u00b7 poly(n), since it involves simulating Czi on poly(n) inputs, and Czi is of size at most T (i). Doing this for each z takes time at most T (i) \u00b72a(i) \u00b7poly(n) time, as there are 2a(i) possible advice strings of length a(i). Computing zmax takes time poly(n), and then computing the \u201ccorrected\u201d circuit Ci takes time at most T (i)\u00b7poly(n), since the random self-reducibility procedure runs in polynomial time and can therefore be simulated using polynomial-size circuits.\nCorrectness of M . Clearly Phase 1 concludes with a correct circuit C1 for fTV on 1-bit inputs. We will argue inductively that, given that the randomized circuit Ci\u22121 computed at the end of Phase i\u2212 1 is a correct circuit for fTV on inputs of length i\u2212 1 such that its error probability is at most 2\u22122n on any input, with all but exponentially small probability over the random choices of M in Phase i, the randomized circuit Ci computed at the end of Phase i is a correct circuit for fTV on inputs of length i, with error probability at most 2\u22122n on any input. By a union bound over the phases, it follows from this that with all but exponentially small probability, the final circuit Cn is a correct randomized circuit for fTV on inputs of length n (with error probability at most 2\n\u22122n), and hence that carrying out all the phases and then simulating Cn on x yields the correct value fTV(x) with overwhelming probability.\nTherefore our task reduces to arguing the correctness of Phase i given the correctness of Phase i \u2212 1, for an arbitrary i such that 1 < i \u2264 n. We discuss the correctness of Part 1 and Part 2 separately.\nIn Part 1, we argue that with all but exponentially small probability, at least one of the circuits Czi computes fTV correctly on all but a 1/i\n3k fraction inputs of length i. Consider the string zi of length a(i) that is the \u201ccorrect\u201d advice string for Alearn on input 1\ni. We only analyze Part 1 for the advice string zi \u2013 the other advice strings are irrelevant to our analysis of correctness for this part. Alearn with advice zi is a correct learner for {fTV}; hence with probability at least 1 \u2212 2\u22122i, it outputs a circuit that computes fTV on at least a 1 \u2212 1/i4k fraction of inputs of length i, when it is given access to a correct oracle for fTV. By running the learner poly(n) times independently and doing standard amplification, the success probability can be boosted to 1\u22122\u22122n, while keeping the agreement of the hypothesis with fTV at least 1\u2212 1/i3k, and not affecting the efficiency of M by more than a polynomial factor. M might not be able to answer queries to fTV with perfect accuracy, however by the inductive hypothesis that Cj has error at most 2\n\u22122n on any specific input for j < i, it follows by a union bound that with probability at least 1 \u2212 T (i)2\u22122n \u2265 1 \u2212 2\u2212n over the internal randomness of M , the simulation of the learner is correct. Hence with probability at least 1\u2212 2\u2212n, M outputs a circuit Czii during Phase i, Part 1, such that C zi i agrees with fTV on at least a 1\u2212 1/i3k fraction of inputs of length i. Next we analyze Part 2 of Phase i. By a union bound, with probability at least 1\u2212poly(n)/22n, the \u201cguesses\u201d b1, . . . , bt \u2208 {0, 1} are all the correct values for fTV on inputs y1, . . . , yt \u2208 {0, 1}i, where by construction t = n10k. By using a standard concentration bound such as Lemma 2, we have that the estimate \u03c1zi is at least 1 \u2212 1/i2k with probability at least 1 \u2212 2\u22124n, and that with probability at least 1\u22122\u22124n any z such that the agreement \u03c1z is at least 1\u22121/i2k must be such that Czi agrees with fTV on at least a 1 \u2212 1/i3k/2 fraction of inputs of length i. Thus with probability\nat least 1\u2212 poly(n)/22n, we have that Czmaxi has agreement at least 1\u2212 1/i3k/2 with fTV on inputs of length i. By again using a union bound and a standard concentration bound such as Lemma 2, we have that with all but exponentially small probability, the corrected circuit Ci is a randomized circuit which computes fTV correctly on all inputs of length i, making error < 2\n\u22122n on any single input. This completes the inductive argument for correctness."}, {"heading": "6.2 Karp-Lipton Results for Bounded-Error Exponential Time", "text": "Lemma 14 (Learnability with advice from distinguishability). Let f \u2208 EXP be a Boolean function and a : N\u2192 N be a advice function.\n1. (High-End Generator) There is a constant c \u2265 1 such that for any \u03b5 \u2208 (0, 1], there is a sequence of functions {GHEn }n\u2208N with GHEn : {0, 1}n c \u2192 {0, 1}2n \u03b5\ncomputable in deterministic time 2O(n\nc) such that if there is a probabilistic procedure A(1n) taking a(n) bits of advice and running in time 2O(n\n\u03b5), and outputting a circuit distinguisher for GHEn (Unc) with constant probability for all large enough n, then {f} is strongly learnable in time 2O(n\u03b5) with a(n) bits of advice.\n2. (Low-End Generator) There is a constant c \u2265 1 such that for any d \u2265 1, there is a sequence of functions {GLEn }n\u2208N with GLEn : {0, 1}n c \u2192 {0, 1}2(logn) d\ncomputable in deterministic time 2O(n\nc) such that if there is a probabilistic quasipolynomial-time procedure A(1n) taking a(n) bits of advice and outputting a circuit distinguisher for GLEn (Unc) with constant probability for all large enough n, then {f} is strongly learnable in quasi-polynomial time with a(n) bits of advice.\nProof. (Nutshell) This follows from the reconstruction procedure for the Nisan-Wigderson generator together with hardness amplification. We refer to [NW94] for more details.\nTheorem 19 (Low-end Karp-Lipton Theorem for bounded-error exponential time). If there is a k \u2265 1 such that BPE \u2286 i.o.Circuit[nk], then BPEXP \u2286 i.o.EXP/O(log n).\nProof. We will prove the contrapositive. For each bounded-error probabilistic exponential time machine M , we will define for each rational \u03b5 > 0 a deterministic exponential-time machine M\u03b5 taking logarithmic advice which attempts to simulate it. If all of the attempted simulations M\u03b5 fail almost everywhere, we will show that PSPACE \u2286 BPSUBEXP, and we will then use a translation argument and Corollary 2 to conclude that BPE 6\u2286 i.o.Circuit[nk], thus establishing the contrapositive.\nLet M be any bounded-error probabilistic machine running in time 2m j , where m is the input length, and j is a constant. We assume without loss of generality that j \u2265 1, and that M has error < 1/4 on any input. Let \u03b5 > 0 be any rational. We define the deterministic exponential-time machine M\u03b5 taking O(logm) bits of advice on inputs of length m below. It uses the generators {GHEn } given by Lemma 14 corresponding to the PSPACE-complete language fTV in the statement of Theorem 18, which is clearly in exponential time.\nOn input x of length m, M\u03b5 first uses the advice on its tape to determine an integer n such that 22m\nj \u2264 2n\u03b5 < 22(m+1)j . Note that any n \u2208 N satisfying these conditions is such that n = \u0398(mj/\u03b5). Hence there are poly(m) possibilities for n, and any of these possibilities can be encoded using O(logm) bits on the advice tape. Given a number i on the advice tape, M\u03b5 can decode the relevant n by determining the i-th number in increasing order satisfying both inequalities. This can be done in poly(m) time since we can assume \u03b5 is hard-coded into M\u03b5, and any single inequality verification can be done in poly(m) time. M\u03b5 then computes R(y) = G HE n (y) for every string y \u2208 {0, 1}n c . It\nsimulates M on x using each string R(y) in turn as the randomness for M , and outputs the majority result of these simulations. It is easy to see that M\u03b5 can be implemented to run in 2 O(nc) = 2O(m cj/\u03b5) time, i.e, in time that is exponential on its input length m. If any of the simulations M\u03b5 succeeds on infinitely many input lengths m, we have that L(M) \u2208 i.o.EXP/O(logm). Suppose, contrariwise, that all of the simulations M\u03b5 fail almost everywhere. We will argue that fTV \u2208 BPSUBEXP and hence, by completeness of fTV, PSPACE \u2286 BPSUBEXP.\nFor any x \u2208 {0, 1}m, let Cx be the circuit of size at most 22m j\ndefined as follows: the input of Cx is the sequence of random bits r used by M in its computation on x. Cx(r) accepts iff M accepts on x using the sequence r of random bits. By the standard translation of deterministic computations into circuits, Cx can be implemented in size at most 2\n2mj , using the fact that M halts in time 2m j .\nFix any \u03b5 > 0. Let n be an arbitrary positive integer, and let m(n) be the unique m such that 22m j \u2264 2n\u03b5 < 22(m+1)j (observe that h(a) def= 22aj is an increasing function, so this m is indeed unique if n is not too small). We claim that for every large enough n, there is an input x of length m(n) such that Cx is a distinguisher for G HE n (Unc). Indeed, if not, there are infinitely many n such that for all inputs x of length m(n), Cx is not a distinguisher, but this implies that the simulation M\u03b5 on inputs of length m(n) would succeed infinitely often with advice encoding the input length n. Since for each m, there are only finitely many n such that m = m(n), it follows that the simulation M\u03b5 succeeds on infinitely many input lengths with logarithmic advice. But this contradicts our assumption that the simulation M\u03b5 fails almost everywhere.\nNow that our claim is established, we define a deterministic procedure A(1n) taking O(n\u03b5) bits of advice and running in time 2O(n\n\u03b5), which for each large enough n produces a circuit distinguisher for GHEn . The procedure A computes m(n) in polynomial time. Note that m(n) = O(n\n\u03b5/j) = O(n\u03b5), by our assumption that j \u2265 1. A then interprets its advice as an string x of length m(n). It computes Cx, which it can do given x in time polynomial in the size of Cx, and outputs Cx. The time taken by A is dominated by the time required to compute Cx, which is 2\nO(n\u03b5), and the advice used by A is of size O(n\u03b5).\nBy applying Lemma 14, we get that {fTV} is strongly learnable in time 2O(n \u03b5) with O(n\u03b5) bits of advice. By applying Lemma 13, we get that fTV is computable in bounded-error probabilistic time 2O(n\n\u03b5). Note that this is the case for every \u03b5 > 0, since our choice of \u03b5 was arbitrary. Thus we have fTV \u2208 BPSUBEXP, and hence by completeness that PSPACE \u2286 BPSUBEXP. Using a standard upward translation argument and applying Corollary 2, we get that for every k > 0, BPE 6\u2286 i.o.Circuit[nk], which is the desired conclusion.\nTheorem 20 (High-end Karp-Lipton Theorem for bounded-error exponential time). If BPEXP \u2286 i.o.Circuit[2n/3], then for each \u03b5 > 0, BPEXP \u2286 i.o.DTIME(22n \u03b5 )/n\u03b5.\nProof. (Sketch) The proof is entirely analogous to the proof of Theorem 19, except that we use generators GLEn rather than the generators G HE n , adjusting other parameters accordingly. We get that either PSPACE \u2286 BPQP, or that for every \u03b5 > 0, BPEXP \u2286 i.o.DTIME(2n \u03b5\n)/n\u03b5. In the first case, by upward translation, we get that EXPSPACE = BPEXP, and then by using Corollary 2, we conclude that BPEXP 6\u2286 i.o.Circuit[2n/3].\nTheorem 21 (Low-end fully uniform Karp-Lipton style theorem for probabilistic time). If there is a k \u2265 1 such that BPE \u2286 i.o.Circuit[nk], then REXP \u2286 i.o.EXP.\nProof. (Sketch) We use the crucial fact that the union of hitting sets is also a hitting set to eliminate the advice in the simulation. The argument is the same as in the proof of Theorem 19, except that the simulating machine M\u03b5 runs M on x using as randomness R every element in turn that is in\nthe range of GHEn for every n such that 2 2mj \u2264 2n\u03b5 < 22(m+1)j , accepting if and only if any of these runs accepts. Note that M\u03b5 does not take advice. We do not need to give the \u201ccorrect\u201d n as advice to the machine because, if any n in the interval produces an accepting path (corresponding to a string in the range of the generator), then \u22c3 nG HE n (Unc) for n as above contains an accepting path for M on x. Finally, we observe that computing the range of the generator for every such n does not blow-up the complexity of the simulation by more than a polynomial factor.\nTheorem 22 (High-end fully uniform Karp-Lipton style theorem for probabilistic time). If BPEXP \u2286 i.o.Circuit[2n/3], then REXP \u2286 i.o.ESUBEXP.\nProof. (Sketch) The proof is entirely analogous to the proof of Theorem 21, except that we use generators GLEn rather than the generators G HE n , adjusting other parameters accordingly.\nThese results can be combined with a Karp-Lipton collapse for deterministic exponential time. For instance, the following holds.\nCorollary 5. If there is k \u2208 N such that BPE \u2286 Circuit[nk], then REXP \u2286 i.o.MA.\nProof. It follows from the hypothesis that E \u2286 Circuit[nk], and hence EXP \u2286 Circuit[poly] by translation. This in turn implies that EXP = MA [BFNW93]. Moreover, the hypothesis gives REXP \u2286 i.o.EXP using Theorem 21. Consequently, we get REXP \u2286 i.o.MA, which completes the proof."}, {"heading": "6.3 Karp-Lipton Results for Zero-Error Exponential Time", "text": "Lemma 15 (Fully uniform simulations using easy witness and truth-table concatenation). Either BPP \u2286 ZPQP, or ZPEXP \u2286 i.o.ESUBEXP.\nProof. We use the \u201ceasy witness\u201d method of Kabanets [Kab01]. Let M be any probabilistic Turing machine with zero error running in time 2m j for some j \u2265 1, such that on each random computation path of M on any input x, the output is either the correct answer for M on x or \u2018?\u2019, and moreover the probability of outputting \u2018?\u2019 is less than 2\u22122m for any input x \u2208 {0, 1}m. For each \u03b5 > 0, we define the following attempted deterministic simulation M\u03b5 for M . On input x of length m, M\u03b5 cycles over all circuits C of size 2m \u03b5/2 on mj inputs. For each such circuit, it explicitly computes the truth table tt(C) of the circuit C, and runs M on x with tt(C) as randomness. If the run accepts, it accepts; if the run rejects, it rejects. If the run outputs \u2018?\u2019, it moves on to the next circuit C in lexicographic order of circuit encodings. If all runs output \u2018?\u2019, the machine rejects. It should be clear that the simulation M\u03b5 runs in deterministic time \u2264 22 m\u03b5\non inputs of length m for any sufficiently large m, and only accepts inputs x \u2208 L(M).\nIf for each \u03b5 > 0, we have that the simulation M\u03b5 solves L(M) correctly on all inputs of length m for infinitely many input lengths m, we have that L(M) \u2286 i.o.ESUBEXP.\nSuppose, on the contrary, that there is some \u03b5 > 0 such that the simulation M\u03b5 fails on at least one input x of each large enough input length m. We show how to use this to decide every language in BPP in ZPQP.\nLet N be any bounded-error probabilistic machine running in time at most nk for some constant k and large enough n. Assume without loss of generality that N has error \u2264 1/10 on any input of length n. We use M to give a zero-error simulation N \u2032 of N on all inputs of large enough length. Given an input y of length n, N \u2032 simulates M on each input x of length m(n) def = (dlog ne)d in turn, for some constant d \u2265 1 to be specified later. If M outputs \u2018?\u2019, N \u2032 outputs \u2018?\u2019, otherwise it moves on to the next input in lexicographic order. If running M gives \u2018?\u2019 outputs for every input x of length\nm(n), N \u2032 outputs \u2018?\u2019. Otherwise, N \u2032 concatenates the random strings used on the computation paths of M for each input of length m(n) into a single string Rn of length O(2\npolylog(n)). It then uses Rn as the truth-table of the hard function for the generator in Theorem 6, setting parameters so that at least n2k pseudorandom bits are produced by the generator. It cycles over all possible seeds of the generator and runs N using each output in turn as the sequence of random choices, accepting if and only if a majority of runs accepts.\nSetting d to be a large enough constant depending on j, k, \u03b5 and the constant c in the statement of Theorem 6, it can be shown that this simulation can be done in quasi-polynomial time, and that it is correct for each input y of large enough length whenever N \u2032 does not output \u2018?\u2019. The key here is that by the failure of M\u03b5 for at least one input of any large enough length, the string Rn is guaranteed to be hard enough that the generator is correct. This is because Rn contains a subfunction of sufficiently large worst-case circuit complexity. Hence cycling over all seeds of the generator and taking the majority value gives the correct answer for N on input y. Finally, under our initial assumption that M has exponentially small failure probability, by a union bound the probability that N \u2032 outputs \u2018?\u2019 on any large enough input is small. This concludes the proof that BPP \u2286 ZPQP.\nTheorem 23 (High-end fully uniform Karp-Lipton theorem for zero-error exponential time). If ZPEXP \u2286 i.o.Circuit[2n/3], then ZPEXP \u2286 i.o.ESUBEXP.\nProof. Observe that the proof of Theorem 22 establishes that if REXP 6\u2286 i.o.ESUBEXP, then PSPACE \u2286 BPQP. By Lemma 15, if ZPEXP 6\u2286 i.o.ESUBEXP, then BPP \u2286 ZPQP, and hence by upward translation, BPQP = ZPQP. Putting these together, we have that if ZPEXP 6\u2286 i.o.ESUBEXP, then PSPACE \u2286 ZPQP. Now by upward translation, we have that EXPSPACE = ZPEXP, and hence by Corollary 2, we get ZPEXP 6\u2286 i.o.Circuit[2n/3].\nWe have learned from Valentine Kabanets (private communication) that he has independently established Theorem 23 in an unpublished manuscript.\nIn fact, we can get a non-trivial consequence from the weakest possible non-trivial assumption about the circuit size of Boolean functions computable in zero-error exponential time. This extension of Theorem 23 relies on the following simple lemma.\nLemma 16 (Maximally hard functions in exponential space). Let smax : N \u2192 N be such that for each n \u2208 N, smax(n) is the maximum circuit complexity among Boolean functions on n bits. Then EXPSPACE 6\u2286 i.o.Circuit[smax \u2212 1].\nProof. (Sketch) The proof is by simple diagonalization. In exponential space, we can systematically list the truth tables of Boolean functions on n bits, and maintain the one with the highest circuit complexity. To compute the circuit complexity of a listed truth table can be done by cycling over all circuits, starting from the smallest one, and checking for each circuit whether it computes the given truth table. Once the truth table of a function with maximum circuit complexity has been computed, we simply look up the corresponding entry in the truth table for any particular input.\nNow by using the same proof as for Theorem 23 but applying Lemma 16 instead of Corollary 2, we have the following stronger version of Theorem 23. (We note that Theorems 20 and 22 admit similar extensions.)\nTheorem 24 (Strong Karp-Lipton Theorem for zero-error probabilistic exponential time). Let smax : N \u2192 N be such that for each n \u2208 N, smax(n) is the maximum circuit complexity among Boolean functions on n bits. If ZPEXP \u2286 i.o.Circuit[smax \u2212 1], then ZPEXP \u2286 i.o.ESUBEXP."}, {"heading": "7 Hardness of the Minimum Circuit Size Problem", "text": "We will be dealing with various notions of non-uniform reduction to versions of the Minimum Circuit Size Problem (MCSP). Reductions computable in a non-uniform class C are formalized using oracle C-circuits, which are C-circuits with oracle gates. We only use oracle circuits where oracle gates appear all at the same level. In this setting, we can define size and depth of oracle circuits to be the size and depth respectively of the oracle circuits with oracle gates replaced by AND/OR gates.\nDefinition 16 (Non-uniform Reductions). Let C be a typical class of circuits, and L and L\u2032 be languages.\n\u2022 (m-reduction) We say L C-reduces to L\u2032 via m-reductions if there is a sequence of poly-size oracle C-circuits computing the slices Ln of L when the circuits are given oracle L\n\u2032, and such that each oracle circuit has a single oracle gate, which is also the top gate of the circuit.\n\u2022 (tt-reduction) We say L C-reduces to L\u2032 via tt-reductions if there is a sequence of poly-size oracle C-circuits computing the slices Ln of L when the circuits are given oracle L\n\u2032, and such that no oracle circuit has a directed path from one oracle gate to another.\n\u2022 (\u03b5-approximate reductions) We extend these notions to hold between approximations of a language. Given a function \u03b5 : N \u2192 [0, 1] and languages L and L\u2032, we say that L reduces to \u03b5-approximating L\u2032 under a certain notion of reduction if for each L\u0303\u2032 which agrees with L\u2032 on at least a 1 \u2212 \u03b5(n) fraction of inputs of length n for large enough n, L reduces to L\u0303\u2032 under that notion of reduction. We say that \u03b5-approximating L reduces to L\u2032 if there is a language L\u0303 which agrees with L on at least a 1 \u2212 \u03b5(n) fraction of inputs of length n for large enough n, such that L\u0303 reduces to L\u2032. More generally, we say that \u03b5-approximating L reduces to \u03b5\u2032-approximating L\u2032 under a certain notion of reduction if for any language L\u0303\u2032 that \u03b5\u2032(n)-approximates L\u2032 on inputs of length n for large enough n, there is a language L\u0303 that \u03b5(n)-approximates L on inputs of length n for large enough n, and a corresponding reduction from L\u0303 to L\u0303\u2032.\n\u2022 (Parameterized reduction) If a reduction is not computed by polynomial size circuits, we extend these definitions in the natural way, and say that L C[s]-reduces to L\u2032, where s : N\u2192 N is the appropriate circuit size bound.\nThe following proposition is immediate from the definitions and the fact that typical circuit classes are closed under composition.\nProposition 2 (Transitivity of reductions). Let C be a typical circuit class, L, L\u2032, L\u2032\u2032 be languages, and \u03b5, \u03b5\u2032, \u03b5\u2032\u2032 : N\u2192 [0, 1] be functions.\n(i) If L C-reduces to L\u2032 via m-reductions (resp. tt-reductions) and L\u2032 C-reduces to L\u2032\u2032 via m-reductions (resp. tt-reductions), then L C-reduces to L\u2032\u2032 via m-reductions (resp. ttreductions).\n(ii) If \u03b5(poly(n))-approximating L C-reduces to \u03b5\u2032(poly(n))-approximating L\u2032 via m-reductions (tt-reductions) and \u03b5\u2032(poly(n))-approximating L\u2032 C-reduces to \u03b5\u2032\u2032(poly(n))-approximating L\u2032\u2032\nvia m-reductions (tt-reductions), it follows that \u03b5(poly(n))-approximating L C-reduces to \u03b5\u2032\u2032(poly(n))-approximating L\u2032\u2032 via m-reductions (tt-reductions).\nUsing the notation introduced above, the following fact is trivial to establish.\nProposition 3 (Relation between parameterized and unparameterized versions of MCSP). For any typical circuit class C, MCSP-C[2n/2] AC0-reduces to MCSP-C via m-reductions.\nTheorem 25 (Hardness of MCSP for weakly approximating functions in typical circuit classes). Let C be a typical circuit class that contains AC0[p], for some fixed prime p. For every Boolean function f \u2208 C[nk] there exists c = c(k, \u03b4) \u2208 N such that (1/2 \u2212 \u2126(1/nc))-approximating f AC0reduces to MCSP-C[2n/2] via tt-reductions, as well as to any property with density at least 1/4 that is useful against C[2\u03b4n] for some fixed \u03b4 \u2208 (0, 1).\nProof. (Sketch) Let f = {fn}n\u2208N be a function in C[nk], where fn : {0, 1}n \u2192 {0, 1} and AC0[p] \u2286 C[poly]. Further, let 0 < \u03b4 < 1 be a constant. We let\nNWc(fn) def = {gz : {0, 1}c logn \u2192 {0, 1} | z \u2208 {0, 1}\u0398(n 2)}\nbe the family (multiset) of functions obtained by instantiating the Nisan-Wigderson [NW94] construction with the AC0[p]-computable designs from [CIKK16] and fn. A bit more precisely, each gz is a function specified by a seed z of length \u0398(n\n2), the family of sets Sn = {Sw \u2286 [\u0398(n2)] | w \u2208 {0, 1}c logn}, and fn, and we have gz(w) def = fn(zSw). Here each Sw \u2286 [\u0398(n2)] contains exactly n elements (Sw is the w-th set in the design), and zSw \u2208 {0, 1}n is the projection of z to coordinates Sw. By taking c = c(\u03b4, k) sufficiently large and using that \u03b4 > 0, fn \u2208 C[nk], and that the design can be implemented in C[poly], it follows from [NW94, CIKK16] that for large enough n:\n(A) Each gz is a function on m def = c log n input bits of C-circuit complexity \u2264 2\u03b4m.\nOn the other hand, if hm \u223c Fm is a uniformly random Boolean function on m input bits, using that \u03b4 < 1 it easily follows from a counting argument (e.g. Lemma 3) that for large enough n (recall that m = c log n):\n(B) hm has C-circuit complexity > 2 \u03b4m with probability 1\u2212 o(1).\nConsequently, from (A) and (B) we get that an oracle to MCSP-C[2n/2] (corresponding to \u03b4 = 1/2) can be used to distinguish the multiset NWc(fn) (sampled according to z \u223c {0, 1}\u0398(n\n2)) from a random function on m input bits.\nWe argue next that it follows from the description of the Nisan-Wigderson reconstruction procedure [NW94] that there is a tt-reduction from (1/2\u2212\u2126(1/nc))-approximating fn to MCSP-C[2n/2] that is computable by AC0-circuits. That some non-uniform approximate reduction with oracle access to fn exists immediately follows from the proof of their main result. That it can be computed in AC0[poly(n)] with oracle access to the distinguisher MCSP-C[2n/2] (and without oracle access to fn) follows by our choice of parameters (in particular, |Sw1 \u2229 Sw2 | = O(log n) for every pair w1 6= w2), non-uniformity of the reduction, and the fact that the output of fn on any particular n-bit input can be hardwired into the (non-uniform) AC0 circuit computing the reduction. Finally, we remark that the \u2126(1/nc) advantage in the approximation comes from the truth-table size of each gz and the hybrid argument in [NW94], and that we get a tt-reduction because the reconstruction procedure is non-adaptive.\nIn fact, the same argument shows that (1/2 \u2212 \u2126(1/nc))-approximating f AC0-reduces via ttreductions to any property useful against C[2\u03b4n] for some \u03b4 \u2208 (0, 1) and with density at least 1/4, since this suffices to implement the Nisan-Wigderson reconstruction routine. This completes the proof of Theorem 25.\nCorollary 6 (Hardness of the standard circuit version of MCSP). For any Boolean function f \u2208 Circuit[poly(n)], there exists c \u2265 1 such that (1/2 \u2212 1/nc)-approximating f AC0-reduces via ttreductions to MCSP-Circuit and to any property with density at least 1/4 that is useful against Circuit[2n/2].\nProof. The second item follows immediately from Theorem 25, since Circuit is typical. The first item follows from Theorem 25, Proposition 3 and Proposition 2.\nProposition 4 (Hardness amplification for Formula). There exists a Boolean function f \u2208 Formula that is Formula-hard under AC0-reductions such that for every integer d \u2265 1, f TC0-reduces via tt-reductions to (1/2\u2212 1/nd)-approximating f .\nProof. (Sketch) This is achieved using a standard hardness amplification argument using the existence of a random self-reducible complete problem in NC1, as well as the XOR lemma. It is known that the circuits used in the hardness amplification reconstruction procedure and for random-self-reducibility can be implemented in non-uniform TC0. For more details, we refer to [SV10, AAW10, GNW11].\nCorollary 7 (Hardness of MCSP for NC1). For every Boolean function f \u2208 Formula, f TC0-reduces to the following problems via tt-reductions:\n1. MCSP-Formula[2n/2].\n2. Any property useful against Formula[2\u03b4n] for \u03b4 \u2208 (0, 1) and with density at least 1/4.\n3. MCSP-Formula.\n4. MCSP-C for any typical circuit class C \u2287 Formula.\nProof. Items 1 and 2 follow from Theorem 25 applied to the typical class Formula, together with Propositions 2 and 4. Item 3 follows from Item 1 and Propositions 2 and 3. Finally, in order to prove Item 4, note that MCSP-C[2n/2] is useful against Formula[2n/2], using the assumption that formulas are subclasses of C circuits. Moreover, MCSP-C[2n/2] as a property has density 1 \u2212 o(1), since a random function has circuit complexity higher than 2n/2 with probability exponentially close to 1 by the usual counting argument. Thus, it follows using the same argument as for Item 2 that MCSP-C[2n/2] is TC0-hard under tt-reductions for Formula. Item 4 now follows from this via Propositions 2 and 3.\nHardness results as in Corollary 7 also follow for other classes such as non-uniform logarithmic space and the class of problems reducible to the determinant using non-uniform TC0 reductions, since these classes also have random self-reducible complete problems and admit worst-case to average-case reducibility in low complexity classes. We will not further elaborate on this here.\nA closely related problem is whether a string has high KT complexity (cf. [ABK+06]). KT complexity is a version of Kolmogorov complexity, where a string has low complexity if it has a short description from which its bits are efficiently computable. We will not explore consequences for this notion in this work, but we expect that some of our results can be transferred to the problem of whether a string has high KT-complexity using standard observations about the relationship between this problem and MCSP."}, {"heading": "8 Open Problems and Further Research Directions", "text": "We describe here a few directions and problems that we find particularly interesting, and that deserve further investigation.\n\u2022 Speedups in Computational Learning Theory. One of our main conceptual contributions is the discovery of a surprising speedup phenomenon in learning under the uniform distribution using membership queries (Lemma 7). Naturally, it would be relevant to understand which learning models admit similar speedups. In particular, is there an analogous result for learning under the uniform distribution using random examples? An orthogonal question is to weaken the assumptions on concept classes for which learning speedups hold.\n\u2022 Applications in Machine Learning. Is it possible to use part of the machinery behind the proof of the Speedup Lemma (Lemma 7) to obtain faster algorithms in practice? Notice that speedups are available for classes containing a constant number of layers of threshold gates, as TC0 is a typical circuit class according to our definition. Since these circuits can be seen as discrete analogues of neural networks, which have proven quite successful in several contexts of practical relevance, we believe that it is worth exploring these implications.\n\u2022Non-Uniform Circuit Lower Bounds from Learning Algorithms. As discussed in [Wil14b], strong lower bounds are open even for seemingly weak classes such as MOD2 \u25e6 AND \u25e6 THR and AND \u25e6 OR \u25e6MAJ circuits. We would like to know if the learning approach to non-uniform lower bounds (Theorem 15) can lead to new lower bounds against such heavily constrained circuits. More ambitiously, it would be extremely interesting to understand the learnability of ACC0, given that the existence of a nontrivial algorithm for large enough circuits implies REXP * ACC0 (Theorem 16).\n\u2022 The Frontier of Natural Proofs. Is there a natural property against ACC0? Williams [Wil14c] designed a non-trivial satisfiability algorithm for sub-exponential size ACC0 circuits, which implies in particular that NEXP * ACC0. On the other hand, Corollary 4 shows that the existence of a natural property against such circuits implies the stronger lower bound ZPEXP * ACC0.\n\u2022 Connections between Learning, Proofs, Satisfiability, and Derandomization. Together with previous work (e.g. [Wil13, Wil14c, SW14, JMV15]), it follows that non-trivial learning, non-trivial proofs of tautologies (in particular, nontrivial satisfiability algorithms), and non-trivial derandomization algorithms all imply (randomized or nondeterministic) exponential time circuit lower bounds. These are distinct algorithmic frameworks, and the argument in each case is based on a different set of techniques. Is there a more general theory that is able to explain and to strengthen these connections? We view Corollary 3 as a very preliminary result indicating that a more general theory along these lines might be possible.\n\u2022 Unconditional Nontrivial Zero-Error Simulation of REXP. Establish unconditionally that REXP \u2286 i.o.ZPESUBEXP. We view this result as an important step towards the ambitious goal of unconditionally derandomizing probabilistic computations, and suspect that it might be within the reach of current techniques. In particular, this would follow if one can improve Lemma 15, which unconditionally establishes that either BPP \u2286 ZPQP or ZPEXP \u2286 i.o.ESUBEXP, to a result of the same form but with REXP in place of ZPEXP.\n\u2022 Learning Algorithms vs. Pseudorandom Functions. The results from Section 4 establish\nan equivalence between learning algorithms and the lack of pseudorandom functions in a typical circuit class, in the non-uniform exponential time regime. It would be interesting to further investigate this dichotomy, and to understand whether a more uniform equivalence can be established.\n\u2022 Hardness of the Minimum Circuit Size Problem. Show that MCSP /\u2208 AC0[p]. We have established that if MCSP \u2208 TC0 then NC1 \u2286 TC0. Prove that if MCSP \u2208 TC0 then Circuit[poly] \u2286 TC0.\nAcknowledgements. We thank Eric Allender, Marco Carmosino, Russell Impagliazzo, Valentine Kabanets, Antonina Kolokolova, Jan Kraj\u0301\u0131c\u030cek, Tal Malkin, Ja\u0301n Pich, Rocco Servedio and Ryan Williams for discussions. We also thank Ruiwen Chen for several conversations during early stages of this work.\nThe first author received support from CNPq grant 200252/2015-1. The second author was supported by the European Research Council under the European Union\u2019s Seventh Framework Programme (FP7/2007-2013)/ERC Grant No. 615075. Part of this work was done during a visit of the first author to Oxford supported by the second author\u2019s ERC grant."}], "references": [{"title": "Uniform derandomization from pathetic lower bounds", "author": ["Eric Allender", "Vikraman Arvind", "Fengming Wang"], "venue": "In International Workshop on Randomization and Computation (RANDOM),", "citeRegEx": "Allender et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Allender et al\\.", "year": 2010}, {"title": "Ronneburger. Power from random strings", "author": ["Eric Allender", "Harry Buhrman", "Michal Kouck\u00fd", "Dieter van Melkebeek", "Detlef"], "venue": "SIAM J. Comput.,", "citeRegEx": "Allender et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Allender et al\\.", "year": 2006}, {"title": "Zero knowledge and circuit minimization", "author": ["Eric Allender", "Bireswar Das"], "venue": "In Symposium on Mathematical Foundations of Computer Science (MFCS),", "citeRegEx": "Allender and Das.,? \\Q2014\\E", "shortCiteRegEx": "Allender and Das.", "year": 2014}, {"title": "Graph isomorphism and circuit size", "author": ["Eric Allender", "Joshua A. Grochow", "Cristopher Moore"], "venue": "CoRR, abs/1511.08189,", "citeRegEx": "Allender et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allender et al\\.", "year": 2015}, {"title": "The minimum oracle circuit size problem", "author": ["Eric Allender", "Dhiraj Holden", "Valentine Kabanets"], "venue": "In Symposium on Theoretical Aspects of Computer Science (STACS),", "citeRegEx": "Allender et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allender et al\\.", "year": 2015}, {"title": "Minimizing disjunctive normal form formulas and AC0 circuits given a truth table", "author": ["Eric Allender", "Lisa Hellerstein", "Paul McCabe", "Toniann Pitassi", "Michael E. Saks"], "venue": "SIAM J. Comput.,", "citeRegEx": "Allender et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Allender et al\\.", "year": 2008}, {"title": "Cryptography in NC0", "author": ["Benny Applebaum", "Yuval Ishai", "Eyal Kushilevitz"], "venue": "SIAM J. Comput.,", "citeRegEx": "Applebaum et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Applebaum et al\\.", "year": 2006}, {"title": "Amplifying lower bounds by means of selfreducibility", "author": ["Eric Allender", "Michal Kouck\u00fd"], "venue": "J. ACM,", "citeRegEx": "Allender and Kouck\u00fd.,? \\Q2010\\E", "shortCiteRegEx": "Allender and Kouck\u00fd.", "year": 2010}, {"title": "On sparse approximations to randomized strategies and convex combinations", "author": ["Ingo Alth\u00f6fer"], "venue": "Linear Algebra Appl.,", "citeRegEx": "Alth\u00f6fer.,? \\Q1994\\E", "shortCiteRegEx": "Alth\u00f6fer.", "year": 1994}, {"title": "Queries and concept learning", "author": ["Dana Angluin"], "venue": "Machine Learning,", "citeRegEx": "Angluin.,? \\Q1987\\E", "shortCiteRegEx": "Angluin.", "year": 1987}, {"title": "Arithmetic circuits: A chasm at depth four", "author": ["Manindra Agrawal", "V. Vinay"], "venue": "In Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Agrawal and Vinay.,? \\Q2008\\E", "shortCiteRegEx": "Agrawal and Vinay.", "year": 2008}, {"title": "Cryptographic primitives based on hard learning problems", "author": ["Avrim Blum", "Merrick L. Furst", "Michael J. Kearns", "Richard J. Lipton"], "venue": "In Advances in Cryptology (CRYPTO),", "citeRegEx": "Blum et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1993}, {"title": "BPP has subexponential time simulations unless EXPTIME has publishable proofs", "author": ["L\u00e1szl\u00f3 Babai", "Lance Fortnow", "Noam Nisan", "Avi Wigderson"], "venue": "Computational Complexity,", "citeRegEx": "Babai et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Babai et al\\.", "year": 1993}, {"title": "Nonrelativizing separations", "author": ["Harry Buhrman", "Lance Fortnow", "Thomas Thierauf"], "venue": "In Conference on Computational Complexity (CCC),", "citeRegEx": "Buhrman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Buhrman et al\\.", "year": 1998}, {"title": "Superpolynomial circuits, almost sparse oracles and the exponential hierarchy", "author": ["Harry Buhrman", "Steven Homer"], "venue": "In Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS),", "citeRegEx": "Buhrman and Homer.,? \\Q1992\\E", "shortCiteRegEx": "Buhrman and Homer.", "year": 1992}, {"title": "Amplification of weak learning under the uniform distribution", "author": ["Dan Boneh", "Richard J. Lipton"], "venue": "In Conference on Computational Learning Theory (COLT),", "citeRegEx": "Boneh and Lipton.,? \\Q1993\\E", "shortCiteRegEx": "Boneh and Lipton.", "year": 1993}, {"title": "A machine-independent theory of the complexity of recursive functions", "author": ["Manuel Blum"], "venue": "J. ACM,", "citeRegEx": "Blum.,? \\Q1967\\E", "shortCiteRegEx": "Blum.", "year": 1967}, {"title": "Arthur-Merlin games: A randomized proof system, and a hierarchy of complexity classes", "author": ["L\u00e1szl\u00f3 Babai", "Shlomo Moran"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Babai and Moran.,? \\Q1988\\E", "shortCiteRegEx": "Babai and Moran.", "year": 1988}, {"title": "Learning algorithms from natural proofs", "author": ["Marco L. Carmosino", "Russell Impagliazzo", "Valentine Kabanets", "Antonina Kolokolova"], "venue": "In Conference on Computational Complexity (CCC),", "citeRegEx": "Carmosino et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carmosino et al\\.", "year": 2016}, {"title": "Mining circuit lower bound proofs for meta-algorithms", "author": ["Ruiwen Chen", "Valentine Kabanets", "Antonina Kolokolova", "Ronen Shaltiel", "David Zuckerman"], "venue": "Computational Complexity,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Efficient learning algorithms yield circuit lower bounds", "author": ["Lance Fortnow", "Adam R. Klivans"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Fortnow and Klivans.,? \\Q2009\\E", "shortCiteRegEx": "Fortnow and Klivans.", "year": 2009}, {"title": "Infeasibility of instance compression and succinct PCPs for NP", "author": ["Lance Fortnow", "Rahul Santhanam"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Fortnow and Santhanam.,? \\Q2011\\E", "shortCiteRegEx": "Fortnow and Santhanam.", "year": 2011}, {"title": "Probabilistic encryption and how to play mental poker keeping secret all partial information", "author": ["Shafi Goldwasser", "Silvio Micali"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Goldwasser and Micali.,? \\Q1982\\E", "shortCiteRegEx": "Goldwasser and Micali.", "year": 1982}, {"title": "Probabilistic encryption", "author": ["Shafi Goldwasser", "Silvio Micali"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Goldwasser and Micali.,? \\Q1984\\E", "shortCiteRegEx": "Goldwasser and Micali.", "year": 1984}, {"title": "On Yao\u2019s XOR-Lemma", "author": ["Oded Goldreich", "Noam Nisan", "Avi Wigderson"], "venue": "In Studies in Complexity and Cryptography,", "citeRegEx": "Goldreich et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldreich et al\\.", "year": 2011}, {"title": "The Foundations of Cryptography - Volume 1, Basic Techniques", "author": ["Oded Goldreich"], "venue": null, "citeRegEx": "Goldreich.,? \\Q2001\\E", "shortCiteRegEx": "Goldreich.", "year": 2001}, {"title": "Exact learning algorithms, betting games, and circuit lower bounds", "author": ["Ryan C. Harkins", "John M. Hitchcock"], "venue": "Transactions on Computation Theory (TOCT),", "citeRegEx": "Harkins and Hitchcock.,? \\Q2013\\E", "shortCiteRegEx": "Harkins and Hitchcock.", "year": 2013}, {"title": "On the NP-completeness of the minimum circuit size problem", "author": ["John M. Hitchcock", "Aduri Pavan"], "venue": "In Conference on Foundation of Software Technology and Theoretical Computer Science (FSTTCS),", "citeRegEx": "Hitchcock and Pavan.,? \\Q2015\\E", "shortCiteRegEx": "Hitchcock and Pavan.", "year": 2015}, {"title": "Limits of minimum circuit size problem as oracle", "author": ["Shuichi Hirahara", "Osamu Watanabe"], "venue": "In Conference on Computational Complexity (CCC),", "citeRegEx": "Hirahara and Watanabe.,? \\Q2016\\E", "shortCiteRegEx": "Hirahara and Watanabe.", "year": 2016}, {"title": "In search of an easy witness: exponential time vs. probabilistic polynomial time", "author": ["Russell Impagliazzo", "Valentine Kabanets", "Avi Wigderson"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Impagliazzo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Impagliazzo et al\\.", "year": 2002}, {"title": "No better ways to generate hard NP instances than picking uniformly at random", "author": ["Russell Impagliazzo", "Leonid A. Levin"], "venue": "In Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Impagliazzo and Levin.,? \\Q1990\\E", "shortCiteRegEx": "Impagliazzo and Levin.", "year": 1990}, {"title": "P = BPP if E requires exponential circuits: Derandomizing the XOR lemma", "author": ["Russell Impagliazzo", "Avi Wigderson"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Impagliazzo and Wigderson.,? \\Q1997\\E", "shortCiteRegEx": "Impagliazzo and Wigderson.", "year": 1997}, {"title": "Randomness vs time: Derandomization under a uniform assumption", "author": ["Russell Impagliazzo", "Avi Wigderson"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Impagliazzo and Wigderson.,? \\Q2001\\E", "shortCiteRegEx": "Impagliazzo and Wigderson.", "year": 2001}, {"title": "An efficient membership-query algorithm for learning DNF with respect to the uniform distribution", "author": ["Jeffrey C. Jackson"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Jackson.,? \\Q1997\\E", "shortCiteRegEx": "Jackson.", "year": 1997}, {"title": "Local reductions", "author": ["Hamid Jahanjou", "Eric Miles", "Emanuele Viola"], "venue": "In International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "Jahanjou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jahanjou et al\\.", "year": 2015}, {"title": "Easiness assumptions and hardness tests: Trading time for zero error", "author": ["Valentine Kabanets"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Kabanets.,? \\Q2001\\E", "shortCiteRegEx": "Kabanets.", "year": 2001}, {"title": "Circuit minimization problem", "author": ["Valentine Kabanets", "Jin-Yi Cai"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Kabanets and Cai.,? \\Q2000\\E", "shortCiteRegEx": "Kabanets and Cai.", "year": 2000}, {"title": "Constructing hard functions using learning algorithms", "author": ["Adam Klivans", "Pravesh Kothari", "Igor C. Oliveira"], "venue": "In Conference on Computational Complexity (CCC),", "citeRegEx": "Klivans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2013}, {"title": "Some connections between nonuniform and uniform complexity classes", "author": ["Richard M. Karp", "Richard J. Lipton"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Karp and Lipton.,? \\Q1980\\E", "shortCiteRegEx": "Karp and Lipton.", "year": 1980}, {"title": "Introduction to Modern Cryptography", "author": ["Jonathan Katz", "Yehuda Lindell"], "venue": null, "citeRegEx": "Katz and Lindell.,? \\Q2007\\E", "shortCiteRegEx": "Katz and Lindell.", "year": 2007}, {"title": "Forcing with Random Variables and Proof Complexity, volume 382 of London Mathematical Society Lecture Note Series", "author": ["Jan Kraj\u0301\u0131cek"], "venue": null, "citeRegEx": "Kraj\u0301\u0131cek.,? \\Q2011\\E", "shortCiteRegEx": "Kraj\u0301\u0131cek.", "year": 2011}, {"title": "Hardness of minimizing and learning DNF expressions", "author": ["Subhash Khot", "Rishi Saket"], "venue": "In Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Khot and Saket.,? \\Q2008\\E", "shortCiteRegEx": "Khot and Saket.", "year": 2008}, {"title": "An Introduction to Computational Learning Theory", "author": ["Michael Kearns", "Umesh Vazirani"], "venue": null, "citeRegEx": "Kearns and Vazirani.,? \\Q1994\\E", "shortCiteRegEx": "Kearns and Vazirani.", "year": 1994}, {"title": "Cryptographic limitations on learning boolean formulae and finite automata", "author": ["Michael J. Kearns", "Leslie G. Valiant"], "venue": "J. ACM,", "citeRegEx": "Kearns and Valiant.,? \\Q1994\\E", "shortCiteRegEx": "Kearns and Valiant.", "year": 1994}, {"title": "Constant depth circuits, fourier transform, and learnability", "author": ["Nathan Linial", "Yishay Mansour", "Noam Nisan"], "venue": "J. ACM,", "citeRegEx": "Linial et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Linial et al\\.", "year": 1993}, {"title": "Simple strategies for large zero-sum games with applications to complexity theory", "author": ["Richard J. Lipton", "Neal E. Young"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Lipton and Young.,? \\Q1994\\E", "shortCiteRegEx": "Lipton and Young.", "year": 1994}, {"title": "Some NP-complete set covering problems", "author": ["William J. Masek"], "venue": "Unpublished Manuscript,", "citeRegEx": "Masek.,? \\Q1979\\E", "shortCiteRegEx": "Masek.", "year": 1979}, {"title": "On the (non) NP-hardness of computing circuit complexity", "author": ["Cody D. Murray", "Richard Ryan Williams"], "venue": "In Conference on Computational Complexity (CCC),", "citeRegEx": "Murray and Williams.,? \\Q2015\\E", "shortCiteRegEx": "Murray and Williams.", "year": 2015}, {"title": "Number-theoretic constructions of efficient pseudorandom functions", "author": ["Moni Naor", "Omer Reingold"], "venue": "J. ACM,", "citeRegEx": "Naor and Reingold.,? \\Q2004\\E", "shortCiteRegEx": "Naor and Reingold.", "year": 2004}, {"title": "Hardness vs randomness", "author": ["Noam Nisan", "Avi Wigderson"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Nisan and Wigderson.,? \\Q1994\\E", "shortCiteRegEx": "Nisan and Wigderson.", "year": 1994}, {"title": "Randomness is linear in space", "author": ["Noam Nisan", "David Zuckerman"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Nisan and Zuckerman.,? \\Q1996\\E", "shortCiteRegEx": "Nisan and Zuckerman.", "year": 1996}, {"title": "Analysis of Boolean Functions", "author": ["Ryan O\u2019Donnell"], "venue": null, "citeRegEx": "O.Donnell.,? \\Q2014\\E", "shortCiteRegEx": "O.Donnell.", "year": 2014}, {"title": "Unconditional Lower Bounds in Complexity Theory", "author": ["Igor C. Oliveira"], "venue": "PhD thesis, Columbia University,", "citeRegEx": "Oliveira.,? \\Q2015\\E", "shortCiteRegEx": "Oliveira.", "year": 2015}, {"title": "Circuit lower bounds in bounded arithmetics", "author": ["J\u00e1n Pich"], "venue": "Ann. Pure Appl. Logic,", "citeRegEx": "Pich.,? \\Q2015\\E", "shortCiteRegEx": "Pich.", "year": 2015}, {"title": "On the complexity of circuit satisfiability", "author": ["Ramamohan Paturi", "Pavel Pudl\u00e1k"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Paturi and Pudl\u00e1k.,? \\Q2010\\E", "shortCiteRegEx": "Paturi and Pudl\u00e1k.", "year": 2010}, {"title": "Pseudorandom generators hard for k-DNF resolution and polynomial calculus resolution", "author": ["Alexander A. Razborov"], "venue": "Ann. of Math", "citeRegEx": "Razborov.,? \\Q2015\\E", "shortCiteRegEx": "Razborov.", "year": 2015}, {"title": "On lattices, learning with errors, random linear codes, and cryptography", "author": ["Oded Regev"], "venue": "J. ACM,", "citeRegEx": "Regev.,? \\Q2009\\E", "shortCiteRegEx": "Regev.", "year": 2009}, {"title": "Circuit lower bounds for Merlin\u2013Arthur classes", "author": ["Rahul Santhanam"], "venue": "SIAM J. Comput.,", "citeRegEx": "Santhanam.,? \\Q2009\\E", "shortCiteRegEx": "Santhanam.", "year": 2009}, {"title": "A compression algorithm for AC0[\u2295] circuits using certifying polynomials", "author": ["Srikanth Srinivasan"], "venue": "Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Srinivasan.,? \\Q2015\\E", "shortCiteRegEx": "Srinivasan.", "year": 2015}, {"title": "Hardness amplification proofs require majority", "author": ["Ronen Shaltiel", "Emanuele Viola"], "venue": "SIAM J. Comput.,", "citeRegEx": "Shaltiel and Viola.,? \\Q2010\\E", "shortCiteRegEx": "Shaltiel and Viola.", "year": 2010}, {"title": "On uniformity and circuit lower bounds", "author": ["Rahul Santhanam", "Ryan Williams"], "venue": "Computational Complexity,", "citeRegEx": "Santhanam and Williams.,? \\Q2014\\E", "shortCiteRegEx": "Santhanam and Williams.", "year": 2014}, {"title": "Pseudorandomness and average-case complexity via uniform reductions", "author": ["Luca Trevisan", "Salil P. Vadhan"], "venue": "Computational Complexity,", "citeRegEx": "Trevisan and Vadhan.,? \\Q2007\\E", "shortCiteRegEx": "Trevisan and Vadhan.", "year": 2007}, {"title": "A theory of the learnable", "author": ["Leslie Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "On learning, lower bounds and (un)keeping promises", "author": ["Ilya Volkovich"], "venue": "In International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "Volkovich.,? \\Q2014\\E", "shortCiteRegEx": "Volkovich.", "year": 2014}, {"title": "A guide to learning arithmetic circuits", "author": ["Ilya Volkovich"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Volkovich.,? \\Q2016\\E", "shortCiteRegEx": "Volkovich.", "year": 2016}, {"title": "A uniform min-max theorem with applications in cryptography", "author": ["Salil P. Vadhan", "Colin J. Zheng"], "venue": "In Cryptology Conference (CRYPTO),", "citeRegEx": "Vadhan and Zheng.,? \\Q2013\\E", "shortCiteRegEx": "Vadhan and Zheng.", "year": 2013}, {"title": "Improving exhaustive search implies superpolynomial lower bounds", "author": ["Ryan Williams"], "venue": "SIAM J. Comput.,", "citeRegEx": "Williams.,? \\Q2013\\E", "shortCiteRegEx": "Williams.", "year": 2013}, {"title": "Algorithms for circuits and circuits for algorithms: Connecting the tractable and intractable", "author": ["Ryan Williams"], "venue": "Proceedings of the International Congress of Mathematicians (ICM), Volume", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "New algorithms and lower bounds for circuits with linear threshold gates", "author": ["Ryan Williams"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Nonuniform ACC circuit lower bounds", "author": ["Ryan Williams"], "venue": "J. ACM,", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Natural proofs versus derandomization", "author": ["Ryan Williams"], "venue": "SIAM J. Comput.,", "citeRegEx": "Williams.,? \\Q2016\\E", "shortCiteRegEx": "Williams.", "year": 2016}, {"title": "Some consequences of non-uniform conditions on uniform classes", "author": ["Chee-Keng Yap"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Yap.,? \\Q1983\\E", "shortCiteRegEx": "Yap.", "year": 1983}], "referenceMentions": [], "year": 2016, "abstractText": "We prove several results giving new and stronger connections between learning theory, circuit complexity and pseudorandomness. Let C be any typical class of Boolean circuits, and C[s(n)] denote n-variable C-circuits of size \u2264 s(n). We show: Learning Speedups. If C[poly(n)] admits a randomized weak learning algorithm under the uniform distribution with membership queries that runs in time 2/n, then for every k \u2265 1 and \u03b5 > 0 the class C[n] can be learned to high accuracy in timeO(2 \u03b5 ). There is \u03b5 > 0 such that C[2 \u03b5 ] can be learned in time 2/n if and only if C[poly(n)] can be learned in time 2 O(1) . Equivalences between Learning Models. We use learning speedups to obtain equivalences between various randomized learning and compression models, including sub-exponential time learning with membership queries, sub-exponential time learning with membership and equivalence queries, probabilistic function compression and probabilistic average-case function compression. A Dichotomy between Learnability and Pseudorandomness. In the non-uniform setting, there is non-trivial learning for C[poly(n)] if and only if there are no exponentially secure pseudorandom functions computable in C[poly(n)]. Lower Bounds from Nontrivial Learning. If for each k \u2265 1, (depth-d)-C[n] admits a randomized weak learning algorithm with membership queries under the uniform distribution that runs in time 2/n, then for each k \u2265 1, BPE * (depth-d)-C[n]. If for some \u03b5 > 0 there are P-natural proofs useful against C[2 \u03b5 ], then ZPEXP * C[poly(n)]. Karp-Lipton Theorems for Probabilistic Classes. If there is a k > 0 such that BPE \u2286 i.o.Circuit[n], then BPEXP \u2286 i.o.EXP/O(log n). If ZPEXP \u2286 i.o.Circuit[2], then ZPEXP \u2286 i.o.ESUBEXP. Hardness Results for MCSP. All functions in non-uniform NC reduce to the Minimum Circuit Size Problem via truth-table reductions computable by TC circuits. In particular, if MCSP \u2208 TC then NC = TC. 1 ar X iv :1 61 1. 01 19 0v 1 [ cs .C C ] 3 N ov 2 01 6", "creator": "LaTeX with hyperref package"}}}