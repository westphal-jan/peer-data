{"id": "1611.01260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning Identity Mappings with Residual Gates", "abstract": "we jointly propose a technique known to augment network layers by briefly adding a standard linear gating mechanism, which basically provides open a way mechanism to learn identity field mappings by optimizing initially only each one parameter. we also proposed introduce effectively a new metric which simultaneously served as prototype basis tools for describing the technique. it currently captures the major difficulty involved implementation in learning identity mappings primarily for different types of implicit network network models, and provides a rigorous new evolutionary theoretical algorithm intuition specifically for the increased output depths of models such as highway and residual branching networks. meantime we propose a new standard model, named the term gated and residual spanning network, which is presenting the result when augmenting residual networks. consistent experimental recovery results show that repeated augmenting layers grants increased breakdown performance, less issues with depth, endurance and more layer independence - - when fully automated removing them does now not overly cripple into the model. \" we evaluate our adaptive method greatly on mnist models using fully - connected networks and on cifar - 10 servers using wide resnets, achieving successfully a relative error log reduction of precision more down than 8 % in the latter when utilized compared to the original model.", "histories": [["v1", "Fri, 4 Nov 2016 04:34:38 GMT  (556kb,D)", "http://arxiv.org/abs/1611.01260v1", null], ["v2", "Thu, 29 Dec 2016 01:36:47 GMT  (832kb,D)", "http://arxiv.org/abs/1611.01260v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["pedro h p savarese", "leonardo o mazza", "daniel r figueiredo"], "accepted": false, "id": "1611.01260"}, "pdf": {"name": "1611.01260.pdf", "metadata": {"source": "CRF", "title": "LEARNING IDENTITY MAPPINGS WITH RESIDUAL GATES", "authors": [], "emails": ["savarese@land.ufrj.br"], "sections": [{"heading": "1 INTRODUCTION", "text": "As deep networks started to achieve state-of-the-art results on many computer vision tasks, increasing the depth of models without compromising its training has become a central problem to machine learning (Larochelle et al. (2009)).\nMany problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem.\nRecently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function.\nOn the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Montu\u0301far et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al. (2014)). He et al. (2015b) showed that, by construction, one can increase a network\u2019s depth while preserving its performance \u2013 therefore retaining the best local minimum. These two observations indicate that it suffices to stack more layers to a network in order to increase its performance. The fact that this is not observed in practice is an indication that the effect of depth on optimization is not well understood.\nInspired by the importance of skip-connection in Highway and Residual Networks, in this work we introduce the notion of degeneration capacity \u2013 an attempt to measure how easily layers can degenerate into identity mappings.\nar X\niv :1\n61 1.\n01 26\n0v 1\n[ cs\n.C V\n] 4\nN ov\n2 01\nWe define a metric called Squared Distance To Identity (SDI), which captures the distance in the parameter surface corresponding to learning identity functions. It is defined as the squared elementwise distance between parameter sets W and W\u0302 , where a layer with parameter set W\u0302 is equivalent to the identity function.\nBased on this metric, we develop a technique to augment network layers that enables identity mappings to be learned by optimizing only one parameter. This results in degeneration being simpler and less dependent on the layer\u2019s size. As the effective depth of a network quickly decreases if necessary, depth becomes less damaging to the optimizer.\nBy shortening the distance between layers and identity mappings, we also hope to strengthen the supervision on lower layers, aiding the network optimization by making gradients flow more smoothly.\nThe augmentation is defined as, given a layer u = f(x,W ) as input, generating u = g(k)f(x,W )+ (1 \u2212 g(k))x as illustrated in Figure 1. Here u is the layer\u2019s output, x its input, and k is a scalar parameter, and not a tensor as in Highway Networks. We show that this simplification of Highway\nlayers increases performance, decreases dependency between layers and provides an easy method to prune layers from a network.\nWe augment Residual Networks \u2013 the current standard architecture for computer vision tasks \u2013 resulting in a layer u = g(k)fr(x,W ) + x, where fr denotes a residual unit. As shown in Figure 2, the shortcut connection suffers no processing through the layer.\nLastly, we show how sparse encoding (Glorot et al. (2011)) can be applied to augmented layers: transposing the sparsity from neurons to layers provides a form to prune them entirely from the network. We show that, unlike residual layers, their augmented counterparts retain good performance after more than half of its layers have been completely removed.\nWe present results on two experiments. First, we test augmented fully-connected residual networks on MNIST, showing performance increase and how layer pruning affects the model. Second, we augment Wide ResNets (Zagoruyko & Komodakis (2016)) and test them on CIFAR-10, observing indications that our method can be used to surpass state-of-the-art results.\nOur findings indicate that layer degeneration is a fundamental aspect of learning in deep networks, and designing models with this capability seems highly effective."}, {"heading": "2 SQUARED DISTANCE TO IDENTITY (SDI)", "text": "A network\u2019s depth can always be increased without affecting its performance \u2013 it suffices to add layers that perform identity mappings. Consider a classic fully-connected ReLU network with layers defined as u = ReLU(\u3008x,W \u3009). When adding a new layer, we initialize W as the identity matrix I . The layer becomes u = ReLU(\u3008x, I\u3009) = ReLU(x) = x. The last step holds since x is an output of a previous ReLU layer, and ReLU(ReLU(x)) = ReLU(x). Figure 3 illustrates this idea.\nTherefore, we can always add as many layers as desired to a network without harming its performance. This conforms with the hypothesis that depth is mostly an issue to the non-convex nature\nof the optimization. It also suggests that if layers could quickly become identity mappings, then stacking more layers would have less negative impact on the model\u2019s optimization.\nTo compare Highway and Residual networks, we inspect how easily layers in each model can become identity mappings \u2013 we call this property the model\u2019s degeneration capacity. To degenerate into an identity mapping, a layer\u2019s parameters have to change from a state W to another state W\u0302 defined by u(x, W\u0302 ) = x.\nA comparison of how W changes to W\u0302 in each model requires knowledge on the steps taken by the optimizer in the cost surface. Given a path formed by such steps, its length can be calculated to measure how quickly the network learns identity mappings. However, the gradient steps change according to the optimization technique, the ordering of data points, the pre-processing, and so on.\nAlternatively, assume that the squared parameter-wise distance between W and W\u0302 is a credible surrogate to the path\u2019s length. This measure, named Squared Distance to Identity (SDI), serves as a tool to investigate why Highway and Residual networks can be severely deeper than previous models.\nWe define it for the weight parameters as:\nSDI(W ) =M = E [ (W \u2212 W\u0302 )2 ] (1)\nAnd for the bias, as:\nSDI(b) = B = E [ (b\u2212 b\u0302)2 ] (2)\nThe expected value is due to the parameters\u2019 random initialization.\nThrough this section, we always assume thatW is a n\u00d7nmatrix and b an n-dimensional vector. The same definition holds for higher order tensors, for example when analyzing convolutional networks.\nThe sum of all elements inM andB can be calculated for a lower bound on the squared path length. We call it Total Squared Distance to Identity (TSDI):\nTSDI = n\u2211 i=1 n\u2211 j=1 Mij + n\u2211 i=1 Bi (3)"}, {"heading": "2.1 CLASSICAL NEURAL NETWORKS", "text": "Hidden layers in classical fully-connected ReLU networks have mappings u = ReLU(\u3008x,W \u3009) and degenerate when W = I: u = ReLU(\u3008x, I\u3009) = ReLU(x) = x. For the bias, b = ~0 is also required.\nThe SDI for the weight parameters is given by using W\u0302 = I in Equation 1. For the bias, we have b\u0302 = ~0 in Equation 2:\nM = E [ (W \u2212 I)2 ] B = E [ (b\u2212~0)2\n] For He\u2019s initialization (He et al. (2015a)), we have \u00b5 = \u00b5b = 0 and \u03c32 = \u03c32b = 2 n in Equations 15, 16 and 17 from the appendix:\nMij = { 2 n + 1 if i = j 2 n if i 6= j\n(4)\nBi = 2\nn (5)\nThe sum of all elements \u2013 which is a lower bound on the squared length of the path from {W, b} to {I,~0} \u2013 is (Equations 4 and 5):\nTSDI = n\u2211 i=1 n\u2211 j=1 Mij + n\u2211 i=1 Bi = n 2 2 n + n+ n 2 n\n= 3n+ 2\n(6)"}, {"heading": "2.2 HIGHWAY NETWORKS", "text": "ReLU fully-connected Highway Networks have hidden layers defined as:\nu = ReLU(\u3008x,W \u3009).g(\u3008x,Wt\u3009) + x.(1\u2212 g(\u3008x,Wt\u3009))\nDegeneration into identity mappings occurs if W = I, b = ~0 or g(\u3008x,Wt\u3009) = 0. For the first case:\nu = x.g(\u3008x,Wt\u3009) + x.(1\u2212 g(\u3008x,Wt\u3009)) = x\nAs for the second, g(\u3008x,Wt\u3009) approaches 0 as \u3008x,Wt\u3009 goes to negative infinity since g is the sigmoid function. We focus on this case.\nThe degeneration should occur independently of x, therefore Wt = 0:\nM = SDI(Wt) = E [ (Wt \u2212 0)2 ] (7)\nFor simplicity, we set the requirement bt = ~c, where c is a negative constant. Smaller values for c result in layers closer to degeneration.\nB = SDI(bt) = E [ (bt \u2212 ~c)2 ] (8)\nIn Srivastava et al. (2015), bt is initialized with a fixed value of \u22122 or \u22124: we use \u00b5b = \u22124 and \u03c32b = 0. For \u00b5 = 0 and \u03c3 2 = 2n , Equations 18 and 19 from the appendix result in:\nMij = 2\nn (9)\nBi = 2\nn + (4 + c)2 (10)\nThe TSDI gives a lower bound on the length of the path from {Wt, bt} to {0,~c}. Equations 9 and 10 yield:\nTSDI = n\u2211 i=1 n\u2211 j=1 Mij + n\u2211 i=1 Bi\n= n2 2\nn + n ( 2 n + (4 + c)2 ) = ( 2 + (4 + c)2 ) n+ 2\n(11)\nTable 1 shows how c, g(.) and TSDI relate for states close to degeneration."}, {"heading": "2.3 RESIDUAL NETWORKS", "text": "A residual fully-connected ReLU layer is defined as:\nu = ReLU(\u3008x,W \u3009) + x\nIt degenerates in case W = 0, b = ~0:\nM = E [ (W \u2212 0)2 ] B = E [ (b\u2212~0)2\n] Both measures have been already calculated (Equations 5 and 9):\nMij = 2\nn (12)\nBi = 2\nn (13)\nAnd the TSDI is:\nTSDI = n\u2211 i=1 n\u2211 j=1 Mij + n\u2211 i=1 Bi\n= n2 2\nn + n\n2\nn = 2n+ 2\n(14)"}, {"heading": "2.4 COMPARISON", "text": "Highway and Residual networks differ from classical networks on the composition of theM matrix. For the latter, each neuron\u2019s parameters \u2013 a row in the W matrix \u2013 have to achieve a different state. This property is also observed in the M matrix: elements in its diagonal are different from the rest.\nEach neuron receives error backpropagated from upper layers, therefore have no information on the behavior of other neurons in its layer. These two facts indicate that degeneration in classical layers require coordination between neurons in the same layer, even though they don\u2019t communicate directly.\nConversely, all elements in M for Highway and Residual networks are equal, thus no coordination is required to learn an identity mapping. For the B vector, all elements are equal for the three types of network.\nA comparison of the Total Squared Distance to Identity shows that Residual Networks have the smallest value out of the three networks. It is also observed that Highway Networks can\u2019t fully degenerate by learning Wt and bt, but states close to degeneration are achievable.\nThis analyzis indicates that uniform degeneration states \u2013 all elements in W\u0302 and b\u0302 being equal \u2013 and small values for TSDI help the learning of identity mappings. Therefore, these properties can prove advantageous when designing deep networks."}, {"heading": "3 AUGMENTATION", "text": "The SDI of classical, Highway and Residual models suggests that an uniform M and B with small values provides easier optimization for deep networks. Ideally, a layer should have a constant SDI, since it guarantees that its degeneration capacity is not affected by its size.\nHaving all elements in M and B to be equal removes the need of coordination between a layer\u2019s neurons when degenerating into an identity mapping. Since it is not known how W\u0302 \u2019s dimensionality affects its degeneration capacity \u2013 other than how increased dimension impacts it negatively \u2013 keeping it to a minimum might also be advantageous.\nWe thus propose adding scalar gates to a layer. More specifically, a layer u = f(x,W ) becomes u = g(k)f(x,W ) + (1\u2212 g(k))x, where k is a scalar parameter. Such layer can quickly degenerate by setting g(k) to 0. Using the ReLU activation function as g, it suffices that k \u2264 0 for g(k) = 0. This simplified version of Highway layers as an augmentation technique provides a constant degeneration capacity of SDI(k) = E[(k\u2212 0)2] = E[k2]. A deterministic initialization of k as kinit sets each augmented layer\u2019s TSDI to k2init.\nUnlike in the previously analyzed models, degeneration can occur by learning only one parameter. For fully-connected and convolutional networks, the number of parameters to be learned are O(n2) \u2013 where for convolutional layers, n stands for the number of feature maps, ignoring the fixed kernel sizes.\nWe use Residual layers as basis for the augmentation for two reasons. First, they are the current standard for computer vision tasks. Second, ResNets lack means to regulate the residuals, therefore a linear gating mechanism might not only allow deeper models, but could also improve performance.\nWe can rewrite a residual layer as follows:\nu = f(x,W ) = fr(x,W ) + x\nWhere fr(x,W ) is the layer\u2019s residual function \u2013 in our case, BN-ReLU-Conv-BN-ReLU-Conv. Augmenting this layer by adding the scalar gates yields:\nu = g(k)f(x,W ) + (1\u2212 g(k))x = g(k)(fr(x,W ) + x) + (1\u2212 g(k))x = g(k)fr(x,W ) + x\nThe augmentation maintains the shortcut connection unaltered, which according to He et al. (2016) is a desired property when designing residual blocks. As (1\u2212 g(k)) vanishes from the formulation, g(k) stops acting as a dual gating mechanism and can be interpreted as a flow regulator. We call this model Gated Residual Network or GResNet through the paper.\nFor g(k) > 1, the residual signal fr(x,W ) is amplified and the shortcut connection loses significance when generating the layer\u2019s output. On the other hand, 0 < g(k) < 1 suppresses the residual, increasing the shortcut connection\u2019s importance. States where k < 0 result in g(k) = 0, producing an identity mapping."}, {"heading": "4 EXPERIMENTS", "text": "All models were implemented on Keras (Chollet (2015)) and were executed on a Geforce GTX 970M. Larger models or more complex datasets, such as the ImageNet (Russakovsky et al. (2015)), were not explored due to hardware limitations."}, {"heading": "4.1 MNIST", "text": "The MNIST dataset (Lecun et al. (1998)) is composed of 60, 000 greyscale images with 28 \u00d7 28 pixels. Images represent handwritten digits, resulting in a total of 10 classes. We trained three types of fully-connected models: classical plain networks, ResNets and GResNets.\nThe networks consist of a linear layer with 50 neurons, followed by d layers with 50 neurons each, and lastly a softmax layer for classification. Only the d middle layers differ between the three architectures \u2013 the first linear layer and the softmax layer are the same in all experiments.\nFor plain networks, each layer performs dot product, followed by batch normalization and a ReLU activation function.\nInitial tests with pre-activations (He et al. (2016)) resulted in poor performance on the validation set, therefore we opted for the traditional Dot-BN-ReLU layer when designing Residual Networks. Identity shortcut connections connected every 2 layers, as conventional.\nAll networks were trained using Adam (Kingma & Ba (2014)) with Nesterov momentum (Dozat) for a total of 100 epochs using mini-batches of size 128. No learning rate decay was used: we kept the learning rate and momentum fixed to 0.002 and 0.9 during the whole training.\nFor pre processing, we divided each pixel value by 255, constraining their values to [0, 1].\nThe training curves for classical plain networks, ResNets and GResNets with varying depth are shown in figure 4. The distance between the curves increase with the depth, showing that the augmentation helps the training of deeper models.\nTable 2 shows the test error for each depth and architecture. ResNets converge in experiments with d = 50 and d = 100 (52 and 102 layers, respectively), while classical models do not.\nGated Residual Networks perform better in all settings, and the performance boost is more noticeable with increased depths. The relative error decreased approximately 2.5% for d = {2, 10, 20}, 8.7% for d = 50 and 16% for d = 100.\nAs observed in table 3, the mean values of k decrease as the model gets deeper, showing that shortcut connections have less impact on shallow networks. This agrees with empirical results that ResNets perform better than plain networks as the depth increases.\nWe also analyzed how layer removal affects ResNets and GResNets. We compared how the deepest networks (d = 100) behave as residual units \u2013 consecutive pairs of layers between shortcut connections \u2013 are completely removed from the models.\nResults are shown in Figure 5. For Gated Residual Networks, we prune pairs of layers following two strategies. One consists of pruning layers in a greedy fashion, where units with the smallest k are removed first. In the other we remove units randomly. We present results using both strategies for GResNets, and only random pruning for ResNets since they lack the k parameter.\nThe greedy strategy is slightly better for Gated Residual Networks, showing that the k parameter is indeed a good indicator of a layer\u2019s importance for the model, but that layers tend to assume the same level of significance. In a fair comparison, where both models are pruned randomly, GResNets retain a satisfactory performance even after half of its layers have been removed, while ResNets suffer performance decrease after just a few layers.\nTherefore augmented models are not only more robust to layer removal, but can have a fair share of their layers pruned and still perform well. Faster predictions can be generated by using a pruned version of an original model."}, {"heading": "4.2 CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky (2009)) consists of 60, 000 color images with 32 \u00d7 32 pixels each. The dataset has a total of 10 classes, including pictures of cats, birds and airplanes, for example.\nResidual Networks have suppressed state-of-the-art results on CIFAR-10. We test GResnets and Wide GResNets (Zagoruyko & Komodakis (2016)) and compare them with their original, nonaugmented models.\nWe use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9.\nWe set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time.\nImages are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images.\nTable 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware.\nBoth augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4).\nResults of different models on the CIFAR-10 dataset are shown in table 5. Our results don\u2019t surpass the state-of-the-art, which was expected considering the hardware limitations. However, taking into account the improvement observed when augmenting a smaller Wide ResNet, we believe that the technique proposed can be used to surpass the state-of-the-art, given the required hardware and time to train a model like the Wide ResNet (4,10)."}, {"heading": "5 CONCLUSION", "text": "We have proposed a layer augmentation technique based on Highway Neural Networks, which can be applied to provide general layers a quick way to learn identity mappings. Unlike Highway or Residual Networks, layers generated by our technique require optimizing only one parameter to degenerate into identity. We have analyzed the squared parameter-wise distance when learning identity mappings, showing that our technique decreases this measure to a small constant. By designing our method such that randomly initialized layers are always close to identity mappings, models are more robust to depth after augmentation.\nWe have shown that augmenting ResNets yield a model that can regulate the residuals, which we named Gated Residual Networks. This model performed better in all our experiments, and we believe that it can be used to surpass the state-of-the-art on benchmark tasks with negligible extra training time and parameters. Lastly, we have shown how it can be used for layer pruning, effectively removing large numbers of parameters from a network without necessarily harming its performance."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "We thank Leonardo Mazza, Daniel Ratton and Carlos Pedreira for many insightful discussions."}, {"heading": "7 APPENDIX", "text": ""}, {"heading": "7.1 SDI OF CLASSICAL NETWORKS", "text": "Having:\nM = E [ (W \u2212 I)2 ]\nB = E [ (b\u2212~0)2 ] For i = j:\nMij = E [ (Wij \u2212 1)2 ] = E[W 2ij ]\u2212 2E[Wij ] + 1 = V ar[Wij ] + E\n2[Wij ]\u2212 2E[Wij ] + 1 = \u03c32 + \u00b52 \u2212 2\u00b5+ 1 = \u03c32 + (\u00b52 \u2212 1)2\n(15)\nAnd for i 6= j:\nMij = E [ (Wij \u2212 0)2 ] = E[W 2ij ]\n= V ar[Wij ] + E 2[Wij ]\n= \u03c32 + \u00b52\n(16)\nWhere \u03c3 and \u00b5 are the mean and standard deviation when initializing W . For the bias:\nBi = E [ (bi \u2212 0)2 ] = E[b2i ]\n= V ar[bi] + E 2[bi]\n= \u03c32b + \u00b5 2 b\n(17)\nHere \u03c3b and \u00b5b are the bias\u2019 mean and standard deviation."}, {"heading": "7.2 SDI OF HIGHWAY NETWORKS", "text": "SDI(Wt) for Highway layers will have its values equal to the i 6= j elements in SDI(W ) for classical networks (Equations 7 and 16):\nMij = \u03c3 2 + \u00b52 (18)\nFor the bias, we use Equation 8:\nBi = E [ (bt i \u2212 c)2 ] = E[bt 2 i ]\u2212 2cE[bt i] + E[c2]\n= V ar[bt i] + E 2[bt i]\u2212 2cE[bt i] + c2\n= \u03c32b + \u00b5 2 b \u2212 2c\u00b5b + c2\n= \u03c32b + (\u00b5b \u2212 c)2\n(19)"}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D Popovici", "H Larochelle"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Bianchini and Scarselli.,? \\Q2014\\E", "shortCiteRegEx": "Bianchini and Scarselli.", "year": 2014}, {"title": "The Power of Depth for Feedforward Neural Networks", "author": ["R. Eldan", "O. Shamir"], "venue": "ArXiv e-prints,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "AISTATS,,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition. ArXiv e-prints, December 2015b", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep Networks with Stochastic Depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "ArXiv e-prints,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv e-prints,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Exploring strategies for training deep neural networks", "author": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Pascal Lamblin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "ArXiv e-prints,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "On the Number of Linear Regions of Deep Neural Networks", "author": ["G. Mont\u00fafar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Mont\u00fafar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mont\u00fafar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Johannes Frnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "ArXiv e-prints,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}, {"title": "Revise Saturated Activation Functions", "author": ["B. Xu", "R. Huang", "M. Li"], "venue": "ArXiv e-prints,", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "1 INTRODUCTION As deep networks started to achieve state-of-the-art results on many computer vision tasks, increasing the depth of models without compromising its training has become a central problem to machine learning (Larochelle et al. (2009)).", "startOffset": 222, "endOffset": 247}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al.", "startOffset": 98, "endOffset": 119}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)).", "startOffset": 98, "endOffset": 171}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)).", "startOffset": 98, "endOffset": 227}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks.", "startOffset": 98, "endOffset": 296}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks.", "startOffset": 98, "endOffset": 355}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks.", "startOffset": 98, "endOffset": 398}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al.", "startOffset": 98, "endOffset": 600}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks.", "startOffset": 98, "endOffset": 655}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al.", "startOffset": 98, "endOffset": 1185}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al.", "startOffset": 98, "endOffset": 1202}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al.", "startOffset": 98, "endOffset": 1231}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)).", "startOffset": 98, "endOffset": 1254}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al.", "startOffset": 98, "endOffset": 1369}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al. (2014)).", "startOffset": 98, "endOffset": 1391}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al. (2014)). He et al. (2015b) showed that, by construction, one can increase a network\u2019s depth while preserving its performance \u2013 therefore retaining the best local minimum.", "startOffset": 98, "endOffset": 1411}, {"referenceID": 5, "context": "Lastly, we show how sparse encoding (Glorot et al. (2011)) can be applied to augmented layers: transposing the sparsity from neurons to layers provides a form to prune them entirely from the network.", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "Lastly, we show how sparse encoding (Glorot et al. (2011)) can be applied to augmented layers: transposing the sparsity from neurons to layers provides a form to prune them entirely from the network. We show that, unlike residual layers, their augmented counterparts retain good performance after more than half of its layers have been completely removed. We present results on two experiments. First, we test augmented fully-connected residual networks on MNIST, showing performance increase and how layer pruning affects the model. Second, we augment Wide ResNets (Zagoruyko & Komodakis (2016)) and test them on CIFAR-10, observing indications that our method can be used to surpass state-of-the-art results.", "startOffset": 37, "endOffset": 596}, {"referenceID": 6, "context": "For He\u2019s initialization (He et al. (2015a)), we have \u03bc = \u03bcb = 0 and \u03c3 = \u03c3 b = 2 n in Equations 15, 16 and 17 from the appendix:", "startOffset": 25, "endOffset": 43}, {"referenceID": 6, "context": "u = g(k)f(x,W ) + (1\u2212 g(k))x = g(k)(fr(x,W ) + x) + (1\u2212 g(k))x = g(k)fr(x,W ) + x The augmentation maintains the shortcut connection unaltered, which according to He et al. (2016) is a desired property when designing residual blocks.", "startOffset": 163, "endOffset": 180}, {"referenceID": 12, "context": "1 MNIST The MNIST dataset (Lecun et al. (1998)) is composed of 60, 000 greyscale images with 28 \u00d7 28 pixels.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Initial tests with pre-activations (He et al. (2016)) resulted in poor performance on the validation set, therefore we opted for the traditional Dot-BN-ReLU layer when designing Residual Networks.", "startOffset": 36, "endOffset": 53}, {"referenceID": 6, "context": "Initial tests with pre-activations (He et al. (2016)) resulted in poor performance on the validation set, therefore we opted for the traditional Dot-BN-ReLU layer when designing Residual Networks. Identity shortcut connections connected every 2 layers, as conventional. All networks were trained using Adam (Kingma & Ba (2014)) with Nesterov momentum (Dozat) for a total of 100 epochs using mini-batches of size 128.", "startOffset": 36, "endOffset": 327}, {"referenceID": 8, "context": "2 CIFAR-10 The CIFAR-10 dataset (Krizhevsky (2009)) consists of 60, 000 color images with 32 \u00d7 32 pixels each.", "startOffset": 33, "endOffset": 51}, {"referenceID": 8, "context": "2 CIFAR-10 The CIFAR-10 dataset (Krizhevsky (2009)) consists of 60, 000 color images with 32 \u00d7 32 pixels each. The dataset has a total of 10 classes, including pictures of cats, birds and airplanes, for example. Residual Networks have suppressed state-of-the-art results on CIFAR-10. We test GResnets and Wide GResNets (Zagoruyko & Komodakis (2016)) and compare them with their original, nonaugmented models.", "startOffset": 33, "endOffset": 349}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme.", "startOffset": 46, "endOffset": 63}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.", "startOffset": 46, "endOffset": 1377}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.", "startOffset": 46, "endOffset": 1427}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.2M 4.91% ResNet-1001 (He et al. (2016)) 10.", "startOffset": 46, "endOffset": 1470}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.2M 4.91% ResNet-1001 (He et al. (2016)) 10.2M 4.62% FractalNet (Larsson et al. (2016)) 38.", "startOffset": 46, "endOffset": 1517}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.2M 4.91% ResNet-1001 (He et al. (2016)) 10.2M 4.62% FractalNet (Larsson et al. (2016)) 38.6M 4.60% Wide ResNet (4,10) (Zagoruyko & Komodakis (2016)) 36.", "startOffset": 46, "endOffset": 1579}], "year": 2016, "abstractText": "We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence \u2013 fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model.", "creator": "LaTeX with hyperref package"}}}