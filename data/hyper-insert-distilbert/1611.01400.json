{"id": "1611.01400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning to Rank Scientific Documents from the Crowd", "abstract": "gradually finding semantic related published articles is an also important task in any science, but with the rapidly explosion of new work tools in developing the biomedical domain wikipedia it has become especially challenging. basically most existing knowledge methodologies use synthetic text similarity model metrics firstly to initially identify whether two articles are related or not. however biomedical knowledge curve discovery modelling is chiefly hypothesis - base driven. the perhaps most related articles may not be ones with the highest text similarities. just in this study, initially we literally first develop myself an innovative crowd - pleasing sourcing assessment approach proposing to build an expert - sourced annotated online document - ranking hierarchical corpus. later using this corpus description as the gold standard, \" we then evaluate \u2026 the approaches of using simple text similarity to rank : the entire relatedness gaps of articles. just finally, together we develop and evaluate a truly new cooperative supervised model to automatically rank all related reference scientific articles. our results show that authors'text ranking differ significantly from rankings performed by text - wise similarity - based models. typically by training a lifelong learning - efficiency to - work rank recommendation model that on forming a weaker subset variant of the annotated reference corpus, we found yet the best supervised learning - to - rank model ( svm - recommended rank ) significantly yet surpassed conventional state - standards of - fact the - art baseline score systems.", "histories": [["v1", "Fri, 4 Nov 2016 14:43:44 GMT  (271kb,D)", "http://arxiv.org/abs/1611.01400v1", "12 pages, 1 figure"]], "COMMENTS": "12 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.DL cs.LG cs.SI", "authors": ["jesse m lingeman", "hong yu"], "accepted": false, "id": "1611.01400"}, "pdf": {"name": "1611.01400.pdf", "metadata": {"source": "CRF", "title": "Learning to Rank Scientific Documents from the Crowd", "authors": ["Jesse M Lingeman", "Hong Yu"], "emails": ["lingeman@cs.umass.edu", "hong.yu@umassmed.edu"], "sections": [{"heading": null, "text": "I. Introduction\nThe number of biomedical research papers published has increased dramatically in recent years. As of October, 2016, PubMed houses over 26 million citations, with almost 1 million from the first 3 quarters of 2016 alone 1. It has become impossible for any one person to actually read all of the work being published. We require tools to help us determine which research articles would be most informative and related to a particular question or document. For example, a common task when reading articles is to find articles that are most related to another. Major research search engines offer such a \u201crelated articles\u201d feature. However, we propose that instead of measuring relatedness by text-similarity \u2217Corresponding author \u2020http://github.com/umassbionlp/crowdranking.git 1Accessed on October 6, 2016.\nmeasures, we build a model that is able to infer relatedness from the authors\u2019 judgments.\nWilbur and Coffee (1994) consider two kinds of queries important to bibliographic information retrieval: the first is a search query written by the user and the second is a request for documents most similar to a document already judged relevant by the user. Such a query-by-document (or query-by-example) system has been implemented in the de facto scientific search engine PubMed\u2014called Related Citation Search. Lin and Wilbur (2007) show that 19% of all PubMed searches performed by users have at least one click on a related article. Google Scholar provides a similar Related Articles system. Outside of bibliographic retrieval, query-by-document systems are commonly used for patent retrieval, Internet search, and plagiarism detection, amongst others. Most work in the area of query-by-document uses text-\nar X\niv :1\n61 1.\n01 40\n0v 1\n[ cs\n.I R\n] 4\nN ov\n2 01\n6\nbased similarity measures (Ganguly et al. (2011); Mahdabi et al. (2012); Kim et al. (2014)). However, scientific research is hypothesis driven and therefore we question whether text-based similarity alone is the best model for bibliographic retrieval. In this study we asked authors to rank documents by \u201ccloseness\u201d to their work. The definition of \u201ccloseness\u201d was left for the authors to interpret, as the goal is to model which documents the authors subjectively feel are closest to their own. Throughout the paper we will use \u201ccloseness\u201d and \u201crelatedness\u201d interchangeably.\nWe found that researchers\u2019 ranking by closeness differs significantly from the ranking provided by a traditional IR system. Our contributions are three fold:\n1. We show that asking biomedical researchers to rank documents related to their work is an easy and effective way to generate customized annotation data.\n2. We show that ranking by a researcher\u2019s definition of closeness differs from textsimilarity-based ranking.\n3. We implement a supervised learning-torank system to better emulate the author\u2019s choices, compared to the unsupervised systems used in the past.\nThe principal ranking algorithms of query-bydocument in bibliographic information retrieval rely mainly on text similarity measures (Lin and Wilbur (2007); Wilbur and Coffee (1994)). For example, the foundational work of Wilbur and Coffee (1994) introduced the concept of a \u201cdocument neighborhood\u201d in which they pre-compute a text-similarity based distance between each pair of documents. When a user issues a query, first an initial set of related documents is retrieved. Then, the neighbors of each of those documents is retrieved, i.e., documents with the highest text similarity to those in the initial set. In a later work, Lin and Wilbur (2007) develop the PMRA algorithm for PubMed related article search. PMRA is an unsupervised probabilistic topic model that is trained to model \u201crelatedness\u201d between documents. Smucker and Allan (2006) introduce the competing algorithm Find-Similar\nfor this task, treating the full text of documents as a query and selecting related documents from the results.\nOutside bibliographic IR, prior work in queryby-document includes patent retrieval (Xue and Croft (2009); Mahdabi et al. (2012)), finding related documents given a manuscript (Lin and Wilbur (2007); Ontrup et al. (2003)), and web page search (Weng et al. (2011); Lee and Croft (2012)). Much of the work focuses on generating shorter queries from the lengthy document. For example, noun-phrase extraction has been used for extracting short, descriptive phrases from the original lengthy text (Yang et al. (2009)). Topic models have been used to distill a document into a set of topics used to form query (Nallapati et al. (2008)). Xue and Croft (2009) generated queries using the top TF*IDF weighted terms in each document. Kim et al. (2014) suggested extracting phrasal concepts from a document, which are then used to generate queries. Ganguly et al. (2011) combined query extraction and pseudo-relevance feedback for patent retrieval. Lee and Croft (2012) employ supervised machine learning model (i.e., Conditional Random Fields) (Lafferty et al. (2001)) for query generation. Gobeill et al. (2009) explored ontology to identify chemical concepts for queries. There are also many biomedical-document specific search engines available. Many information retrieval systems focus on question answering systems such as those developed for the TREC Genomics Track (Hersh and Voorhees (2009)) or BioASQ Question-Answer (Krithara et al. (2016)) competitions. Systems designed for question-answering use a combination of natural language processing techniques to identify biomedical entities, and then information retrieval systems to extract relevant answers to questions. Systems like those detailed in Yang et al. (2015) can provide answers to yes/no biomedical questions with high precision. However what we propose differs from these systems in a fundamental way: given a specific document, suggest the most important documents that are related to it. The body of work most related to ours is that of citation recommendation. The goal of\ncitation recommendation is to suggest a small number of publications that can be used as high quality references for a particular article (Ren et al. (2014); Lin and Wilbur (2007)). Topic models have been used to rank articles based on the similarity of latent topic distribution (Nallapati et al. (2008); Tang and Zhang (2009); Lin and Wilbur (2007)). These models attempt to decompose a document into a few important keywords. Specifically, these models attempt to find a latent vector representation of a document that has a much smaller dimensionality than the document itself and compare the reduced dimension vectors.\nCitation networks have also been explored for ranking articles by importance, i.e., authority (Brin and Page (2007); Wu et al. (2012)). Ren et al. (2014) introduced heterogeneous network models, called meta-path based models, to incorporate venues (the conference where a paper is published) and content (the term which links two articles, for citation recommendation). Another highly relevant work is Weng et al. (2011) who decomposed a document to represent it with a compact vector, which is then used to measure the similarity with other documents. Note that we exclude the work of context-aware recommendation, which analyze each citation\u2019s local context, which is typically short and does not represent a full document.\nOne of the key contributions of our study is an innovative approach for automatically generating a query-by-document gold standard. Crowdsourcing has generated large databases, including Wikipedia and Freebase. Recently, Ipeirotis and Gabrilovich (2014) concluded that unpaid participants performed better than paid participants for question answering. They attribute this to unpaid participants being more intrinsically motivated than the paid test takers: they performed the task for fun and already had knowledge about the subject being tested. In contrast, another study, Kobren et al. (2014), compared unpaid workers found through Google Adwords (GA) to paid workers found through Amazon Mechanical Turk (AMT). They found that the paid participants from AMT outperform the unpaid ones. This is attributed to the\npaid workers being more willing to look up information they didn\u2019t know. In the bibliographic domain, authors of scientific publications have contributed annotations (Yu et al. (2010)). They found that authors are more willing to annotate their own publications (Yu et al. (2010)) than to annotate other publications (Ramesh et al. (2015)) even though they are paid. In this work, our annotated dataset was created by the unpaid authors of the articles."}, {"heading": "Materials and Methods", "text": ""}, {"heading": "Benchmark Datasets", "text": "In order to develop and evaluate ranking algorithms we need a benchmark dataset. However, to the best of our knowledge, we know of no openly available benchmark dataset for bibliographic query-by-document systems. We therefore created such a benchmark dataset. The creation of any benchmark dataset is a daunting labor-intensive task, and in particular, challenging in the scientific domain because one must master the technical jargon of a scientific article, and such experts are not easy to find when using traditional crowd-sourcing technologies (e.g., AMT). For our task, the ideal annotator for each of our articles are the authors themselves. The authors of a publication typically have a clear knowledge of the references they cite and their scientific importance to their publication, and therefore may be excellent judges for ranking the reference articles.\nGiven the full text of a scientific publication, we want to rank its citations according to the author\u2019s judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. PLoS articles were selected because its journals cover a wide array of topics and the full text articles are available in XML format. We selected the most recent publications as previous work in crowd-sourcing annotation shows that authors\u2019 willingness to participate in an unpaid annotation task declines with the age of publication (Yu et al. (2010)). We then extracted\nthe abstract, citations, full text, authors, and corresponding author email address from each document. The titles and abstracts of the citations were retrieved from PubMed, and the cosine similarity between the PLoS abstract and the citation\u2019s abstract was calculated. We selected the top five most similar abstracts using TF*IDF weighted cosine similarity, shuffled their order, and emailed them to the corresponding author for annotation. We believe that ranking five articles (rather than the entire collection of the references) is a more manageable task for an author compared to asking them to rank all references. Because the documents to be annotated were selected based on text similarity, they also represent a challenging baseline for models based on text-similarity features. In total 416 authors were contacted, and 92 responded (22% response rate). Two responses were removed from the dataset for incomplete annotation.\nWe asked authors to rank documents by how \u201cclose to your work\u201d they were. The definition of closeness was left to the discretion of the author. The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations."}, {"heading": "Learning to Rank", "text": "Learning-to-rank is a technique for reordering the results returned from a search engine query. Generally, the initial query to a search engine is concerned more with recall than precision: the goal is to obtain a subset of potentially related documents from the corpus. Then, given this set of potentially related documents, learningto-rank algorithms reorder the documents such that the most relevant documents appear at the top of the list. This process is illustrated in Figure 1.\nThere are three basic types of learning-to-rank algorithms: point-wise, pair-wise, and list-wise. Point-wise algorithms assign a score to each retrieved document and rank them by their scores. Pair-wise algorithms turn learning-to-rank into a binary classification problem, obtaining a ranking by comparing each individual pair of doc-\numents. List-wise algorithms try to optimize an evaluation parameter over all queries in the dataset.\nSupport Vector Machine (SVM) (Cortes and Vapnik (1995)) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks. SVM can be thought of as a binary linear classifier where the goal is to maximize the size of the gap between the class-separating line and the points on either side of the line. This helps avoid over-fitting on the training data. SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked (Joachims (2006)). We use SVMRank in the experiments below. SVMRank has previously been used in the task of document retrieval in (Cao et al. (2006)) for a more traditional short query task and has been shown to be a top-performing system for ranking. SVMRank is a point-wise learning-to-rank algorithm that returns scores for each document. We rank the documents by these scores. It is possible that sometimes two documents will have the same score, resulting in a tie. In this case, we give both documents the same rank, and then leave a gap in the ranking. For example,\nDatabase\nInitial Retrieval Rerank Results\nA\nB\nC\nD\nE\nC\nA\nD\nB\nE\nQuery\nFigure 1: The basic pipeline of a learning-to-rank system. An initial set of results for a query is retrieved from a search engine, and then that subset is reranked. During the reranking phase new features may be extracted.\nif documents 2 and 3 are tied, their ranked list will be [5, 3, 3, 2, 1].\nModels are trained by randomly splitting the dataset into 70% training data and 30% test data. We apply a random sub-sampling approach where the dataset is randomly split, trained, and tested 100 times due to the relatively small size of the data. A model is learned for each split and a ranking is produced for each annotated document. We test three different supervised models. The first supervised model uses only text similarity features, the second model uses all of the features, and the third model runs forward feature selection to select the best performing combination of features. We also test using two different models trained on two different datasets: one trained using the gold standard annotations, and another trained using the judgments based on text similarity that were used to select the citations to give to the authors.\nWe tested several different learning to rank algorithms for this work. We found in preliminary testing that SVMRank had the best performance, so it will be used in the following experiments."}, {"heading": "Features", "text": "Each citation is turned into a feature vector representing the relationship between the published article and the citation. Four types of features are used: text similarity, citation count and location, age of the citation, and the number of times the citation has appeared in the literature (citation impact). Text similarity features measure the similarity of the words used in different parts of the document. In this work, we calculate the similarity between a document D and a document it cites C by transforming the their text into term vectors. For example, to calculate the similarity of the abstracts between D and C we transform the abstracts into two term vectors, DA and CA. The length of each of the term vectors is |DA \u222aCA|. We then weight each word by its Term-frequency * Inverse-document frequency (TF*IDF) weight. TF*IDF is a technique to give higher weight to words that appear frequently in a document but infrequently\nin the corpus. Term frequency is simply the number of times that a word w appears in a document. Inverse-document frequency is the logarithmically-scaled fraction of documents in the corpus in which the word w appears. Or, more specifically:\nidf(t,D) = log N\n1 + |{d \u2208 D : t \u2208 d}|\nwhere N is the total number of documents in the corpus, and the denominator is the number of documents in which a term t appears in the corpus D. Then, TF*IDF is defined as:\ntfidf(t, d,D) = tf(t, d)idf(t,D)\nwhere t is a term, d is the document, andD is the corpus. For example, the word \u201cthe\u201d may appear often in a document, but because it also appears in almost every document in the corpus it is not useful for calculating similarity, thus it receives a very low weight. However, a word such as \u201cneurogenesis\u201d may appear often in a document, but does not appear frequently in the corpus, and so it receives a high weight. The similarity between term vectors is then calculated using cosine similarity:\nsimilarity = cos(\u03b8) = A \u00b7B \u2016A\u2016\u2016B\u2016\nwhere A and B are two term vectors. The cosine similarity is a measure of the angle between the two vectors. The smaller the angle between the two vectors, i.e., the more similar they are, then the closer the value is to 1. Conversely, the more dissimilar the vectors, the closer the cosine similarity is to 0.\nWe calculate the text similarity between several different sections of the document D and the document it cites C. From the citing article D, we use the title, full text, abstract, the combined discussion/conclusion sections, and the 10 words on either side of the place in the document where the actual citation occurs. From the document it cites C we only use the title and the abstract due to limited availability of the full text. In this work we combine the discussion and conclusion sections of each document because some documents have only a conclusion\nsection, others have only a discussion, and some have both. The similarity between each of these sections from the two documents is calculated and used as features in the model.\nThe age of the citation may be relevant to its importance. As a citation ages, we hypothesize that it is more likely to become a \u201cfoundational\u201d citation rather than one that directly influenced the development of the article. Therefore more recent citations may be more likely relevant to the article. Similarly, \u201ccitation impact\u201d, that is, the number of times a citation has appeared in the literature (as measured by Google Scholar) may be an indicator of whether or not an article is foundational rather than directly related. We hypothesize that the fewer times an article is cited in the literature, the more impact it had on the article at hand. We also keep track of the number of times a citation is mentioned in both the full text and discussion/conclusion sections. We hypothesize that if a citation is mentioned multiple times, it is more important than citations that are mentioned only once. Further, citations that appear in the discussion/conclusion sections are more likely to be crucial to understanding the results. We normalize the counts of the citations by the total number of citations in that section. In total we select 15 features, shown in Table 2. The features are normalized within each document so that each of citation features is on a scale from 0 to 1, and are evenly distributed within that range. This is done because some of the features (such as years since citation) are unbounded."}, {"heading": "Baseline Systems", "text": "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered\nfrom highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author\u2019s judgments.\nWe also compare our rankings to those found on the popular scientific article search engine Google Scholar. Google Scholar is a \u201cblack box\u201d IR system: they do not release details about which features they are using and how they judge relevance of documents. Google Scholar provides a \u201cRelated Articles\u201d feature for each document in its index that shows the top 100 related documents for each article. To compare our rankings, we search through these related documents and record the ranking at which each of the citations we selected appeared. We scale these rankings such that the lowest ranked article from Google Scholar has the highest relevance ranking in our set. If the cited document does not appear in the set, we set its relevanceranking equal to one below the lowest relevance ranking found. Four comparisons are performed with the Google Scholar data. (1) We first train a model using our gold standard and see if we can predict Google Scholar\u2019s ranking. (2) We compare to a baseline of using Google Scholar\u2019s rankings to train and compare with their own rankings using our feature set. (3) Then we train a model using Google Scholar\u2019s rankings and try to predict our gold standard. (4) We compare it to the model trained on our gold standard to predict our gold standard."}, {"heading": "Evaluation Measures", "text": "NDCG Normalized Discounted Cumulative Gain (NDCG) is a common measure for comparing a list of estimated document relevance judgments with a list of known judgments (Croft et al. (2010)). To calculate NDCG we first calculate a ranking\u2019s Discounted Cumulative Gain\n(DCG) as:\nDCG = rel1 + N\u2211 i=1 2reli \u2212 1 log2(i+ 1)\n(1)\nwhere reli is the relevance judgment at position i. Intuitively, DCG penalizes retrieval of documents that are not relevant (reli = 0). However, DCG is an unbounded value. In order to compare the DCG between two models, we must normalize it. To do this, we use the ideal DCG (IDCG), i.e., the maximum possible DCG given the relevance judgments. The maximum possible DCG occurs when the relevance judgments are in the correct order.\nNDCG = DCG IDCG\n(2)\nThe NDCG value is in the range of 0 to 1, where 0 means that no relevant documents were retrieved, and 1 means that the relevant documents were retrieved and in the correct order of their relevance judgments.\nKendall\u2019s \u03c4 Kendall\u2019s \u03c4 is a measure of the correlation between two ranked lists. It compares the number of concordant pairs with the number of discordant pairs between each list. A concordant pair is defined over two observations (xi, yi) and (xj , yj). If xi > xj and yi > yj , then the pair at indices i, j is concordant, that is, the ranking at i, j in both ranking sets X and Y agree with each other. Similarly, a pair i, j is discordant if xi > xj and yi < yj or xi < xj and yi > yj . Kendall\u2019s \u03c4 is then defined as:\n\u03c4 = C \u2212D\n1 2n(n\u2212 1)\n(3)\nwhere C is the number of concordant pairs, D is the number of discordant pairs, and the denominator represents the total number of possible pairs. Thus, Kendall\u2019s \u03c4 falls in the range of [\u22121, 1], where -1 means that the ranked lists are perfectly negatively correlated, 0 means that they are not significantly correlated, and 1 means that the ranked lists are perfectly correlated. One downside of this measure is that it does not take into account where in the ranked\nlist an error occurs. Information retrieval, in general, cares more about errors near the top of the list rather than errors near the bottom of the list.\nAverage-Precision \u03c4 (\u03c4ap) AveragePrecision \u03c4 (Yilmaz et al. (2008)) (or \u03c4ap) extends on Kendall\u2019s \u03c4 by incorporating the position of errors. If an error occurs near the top of the list, then that is penalized heavier than an error occurring at the bottom of the list. To achieve this, \u03c4ap incorporates ideas from the popular Average Precision measure, were we calculate the precision at each index of the list and then average them together. \u03c4ap is defined as:\n\u03c4ap = 2\nN \u2212 1 N\u2211 i=2 ( C(i) i\u2212 1 ) \u2212 1 (4)\nIntuitively, if an error occurs at the top of the list, then that error is propagated into each iteration of the summation, meaning that it\u2019s penalty is added multiple times. \u03c4ap\u2019s range is between -1 and 1, where -1 means the lists are perfectly negatively correlated, 0 means that they are not significantly correlated, and 1 means that they are perfectly correlated."}, {"heading": "Forward Feature Selection", "text": "Forward feature selection was performed by iteratively testing each feature one at a time. The highest performing feature is kept in the model, and another sweep is done over the remaining features. This continues until all features have been selected. This approach allows us to explore the effect of combinations of features and the effect of having too many or too few features. It also allows us to evaluate which features and combinations of features are the most powerful."}, {"heading": "Results", "text": "We first compare our gold standard to the baselines. A random baseline is provided for reference. Because all of the documents that we rank are relevant, NDCG will be fairly high simply by chance. We find that the number of times\na document is mentioned in the annotated document is significantly better than the random baseline or the citation impact. The more times a document is mentioned in a paper, the more likely the author was to annotate it as important. Interestingly, we see a negative correlation with the citation impact. The more times a document is mentioned in the literature, the less likely it is to be important. These results are shown in Table 1.\nNext we rank the raw values of the features and compare them to our gold standard to obtain a baseline (Table 2). The best performing text similarity feature is the similarity between the abstract of the annotated document and the abstract of the cited document. However, the number of times that a cited document is mentioned in the text of the annotated document are also high-scoring features, especially in the \u03c4ap correlation coefficient. These results indicate that text similarity alone may not be a good measure for judging the rank of a document.\nNext we test three different feature sets for our supervised learning-to-rank models. The model using only the text similarity features performs poorly: NDCG stays at baseline and the correlation measures are low. Models that incorporate information about the age, number of times a cited document was referenced, and the citation impact of that document in addition to the text similarity features significantly outperformed models that used only text similarity features \u03c4ap = 0.35. Because \u03c4ap takes into account the position in the ranking of the errors, this indicates that the All Features model was able to better correctly place highly ranked documents above lower ranked ones. Similarly, because Kendall\u2019s \u03c4 is an overall measure of correlation that does not take into account the position of errors, the higher value here means that more rankings were correctly placed. Interestingly, feature selection (which is optimized for NDCG) does not outperform the model using all of the features in terms of our correlation measures. The features chosen during forward feature selection are (1) the citation impact, (2) number of mentions in the full text, (3) text similarity between the annotated document\u2019s\ntitle and the referenced document\u2019s abstract, (4) the text similarity between the annotated document\u2019s discussion/conclusion section and the referenced document\u2019s title. These results are shown in Table 3. The models trained on the text similarity judgments perform worse than the models trained on the annotated data. However, in terms of both NDCG and the correlation measures, they perform significantly better than the random baseline. Next we compare our model to Google Scholar\u2019s rankings. Using the ranking collected from Google Scholar, we build a training set to try to predict our authors\u2019 rankings. We find that Google Scholar performs similarly to the text-only features model. This indicates that the rankings we obtained from the authors are substantially different than the rankings that Google Scholar provides. Results appear in Table 4."}, {"heading": "Discussion", "text": "We found that authors rank the references they cite substantially differently from rankings based on text-similarity. Our results show that decomposing a document into a set of features that is able to capture that difference is key. While text similarity is indeed important (as evidenced by the Similarity(a,a) feature in Table 2), we also found that the number of times a document is referenced in the text and the number of times a document is referenced in the literature are also both important features (via feature selection). The more often a citation is mentioned in the text, the more likely it is to be important. This feature is often overlooked in article citation recommendation. We also found that recency is important: the age of the citation is negatively correlated with the rank. Newer citations are more likely to be directly important than older, more foundational citations. Additionally, the number of times a document is cited in the literature is negatively correlated with rank. This is likely due to highly cited documents being more foundational works; they may be older papers that are important to the field but not directly influential to the new work.\nThe model trained using the author\u2019s judgments does significantly better than the model trained using the text-similarity-based judgments. An error analysis was performed to find out why some of the rankings disagreed with the author\u2019s annotations. We found that in some cases our features were unable to capture the relationship: for example a biomedical document applying a model developed in another field to the dataset may use very different language to describe the model than the citation. Previous work adopting topic models to query document search may prove useful for such cases.\nA small subset of features ended up performing as well as the full list of features. The number of times a citation was mentioned and the citation impact score in the literature ended up being two of the most important features. Indeed, without the citation-based features, the model performs as though it were trained with the text-similarity rankings. Feature engineering is a part of any learning-to-rank system, especially in domain-specific contexts. Citations are an integral feature of our dataset. For learningto-rank to be applied to other datasets feature engineering must also occur to exploit the unique properties of those datasets. However, we show that combining the domain-specific features with more traditional text-based features does improve the model\u2019s scores over simply using the domain-specific features themselves. Interestingly, citation impact and age of the citation are both negatively correlated with rank. We hypothesize that this is because both measures can be indicators of recency: a new publication is more likely to be directly influenced by more recent work. Many other related search tools, however, treat the citation impact as a positive feature of relatedness: documents with\na higher citation impact appear higher on the list of related articles than those with lower citation impacts. This may be the opposite of what the user actually desires. We also found that rankings from our textsimilarity based IR system or Google Scholar\u2019s IR system were unable to rank documents by the authors\u2019 annotations as well as our system. In one sense, this is reasonable: the rankings coming from these systems were from a different system than the author annotations. However, in domain-specific IR, domain experts are the best judges. We built a system that exploits these expert judgments. The text similarity and Google Scholar models were able to do this to some extent, performing above the random baseline, but not on the level of our model.\nAdditionally, we observe that NDCG may not be the most appropriate measure for comparing short ranked lists where all of the documents are relevant to some degree. NDCG gives a lot of credit to relevant documents that occur in the highest ranks. However, all of the documents here are relevant, just to varying degrees. Thus, NDCG does not seem to be the most appropriate measure, as is evident in our scores. The correlation coefficients from Kendall\u2019s \u03c4 and \u03c4ap seem to be far more appropriate for this case, as they are not concerned with relevance, only ranking.\nOne limitation of our work is that we selected a small set of references based on their similarities to the article that cites them. Ideally, we would have had authors rank all of their citations for us, but this would have been a daunting task for authors to perform. We chose to use the Google Scholar dataset in order to attempt to mitigate this: we obtain a ranking for the set of references from a system that is also rank-\ning many other documents. The five citations selected by TF*IDF weighted cosine similarity represent a \u201chard\u201d gold standard: we are attempting to rank documents that are known to all be relevant by their nature, and have high similarity with the text. Additionally, there are plethora of other, more expensive features we could explore to improve the model. Citation network features, phrasal concepts, and topic models could all be used to help improve our results, at the cost of computational complexity. We have developed a model for fast relateddocument ranking based on crowd-sourced data. The model, data, and data collection software are all publicly available 2 and can easily be used in future applications as an automatic search to help users find the most important citations given a particular document. The experimental setup is portable to other datasets with some\n2http://github.com/umassbionlp/crowd-ranking.git\nfeature engineering. We were able to identify that several domain-specific features were crucial to our model, and that we were able to improve on the results of simply using those features alone by adding more traditional features.\nQuery-by-document is a complicated and challenging task. We provide an approach with an easily obtained dataset and a computationally inexpensive model. By working with biomedical researchers we were able to build a system that ranks documents in a quantitatively different way than previous systems, and to provide a tool that helps researchers find related documents."}, {"heading": "Acknowledgments", "text": "We would like to thank all of the authors who took the time to answer our citation ranking survey. This work is supported by Na-\ntional Institutes of Health with the grant number 1R01GM095476. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\nReferences\nBrin, S. and Page, L. (2007). The anatomy of a large-scale hypertextual web search engine, 1998. In the Seventh World Wide Web Conference, pages 107\u2013117. Proceedings of the Seventh World Wide Web Conference.\nCao, Y., Xu, J., Liu, T.-Y., Li, H., Huang, Y., and Hon, H.-W. (2006). Adapting ranking SVM to document retrieval. In The 29th annual international ACM SIGIR conference, pages 186\u2013193, New York, New York, USA. ACM.\nCortes, C. and Vapnik, V. (1995). Supportvector networks. Machine Learning .\nCroft, W. B., Metzler, D., and Strohman, T. (2010). Search engines: Information retrieval in practice. Reading: Addison-Wesley.\nGanguly, D., Leveling, J., Magdy, W., and Jones, G. (2011). Patent query reduction based on\npseudo-relevant documents. Proceedings of CIKM.\nGobeill, J., Teodoro, D., Pasche, E., and Ruch, P. (2009). Report on the TREC 2009 Experiments: Chemical IR Track. In Text REtrieval Conference.\nHersh, W. and Voorhees, E. (2009). TREC genomics special issue overview. Information retrieval , 12(1), 1.\nIpeirotis, P. G. and Gabrilovich, E. (2014). Quizz: targeted crowdsourcing with a billion (potential) users. In the Proceedings of the 23rd international conference on World wide web, pages 143\u2013154. International World Wide Web Conferences Steering Committee.\nJoachims, T. (2006). Training linear SVMs in linear time. In the 12th ACM SIGKDD international conference, pages 217\u2013226, New York, New York, USA. ACM.\nKim, Y., Seo, J., Croft, W. B., and Smith, D. A. (2014). Automatic suggestion of phrasalconcept queries for literature search. Information Processing and Management , 50(4), 568\u2013583.\nKobren, A., Logan, T., Sampangi, S., and McCallum, A. (2014). Domain Specific Knowledge Base Construction via Crowdsourcing. In Neural Information Processing Systems Workshop on Automated Knowledge Base Construction AKBC , Montreal, Canada.\nKrithara, A., Kakadiaris, I., Nentidis, A., and Paliouras, G. (2016). Results of the 4th edition of BioASQ Challenge. Proceedings of the Fourth BioASQ workshop.\nLafferty, J., McCallum, A., and Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proc. 18th International Conf. on Machine Learning , pages 282\u2013289.\nLee, C.-J. and Croft, W. B. (2012). Generating queries from user-selected text. In the 4th Information Interaction in Context Symposium, pages 100\u2013109, New York, New York, USA. ACM.\nLin, J. and Wilbur, W. J. (2007). PubMed related articles: a probabilistic topic-based model for content similarity. BMC Bioinformatics, 8, 423\u2013423.\nMahdabi, P., Andersson, L., Keikha, M., and Crestani, F. (2012). Automatic refinement of patent queries using concept importance predictors. In SIGIR \u201912: Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval . ACM Request Permissions.\nNallapati, R. M., Ahmed, A., Xing, E. P., and Cohen, W. W. (2008). Joint latent topic models for text and citations. In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining . ACM Request Permissions.\nOntrup, J., Nattkemper, T. W., Gerstung, O., and Ritter, H. (2003). A MeSH term based distance measure for document retrieval and labeling assistance. In Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE , pages 1303\u20131306. IEEE.\nRamesh, B. P., Sethi, R. J., and Yu, H. (2015). Figure-Associated Text Summarization and Evaluation. PloS one, 10(2), e0115671.\nRen, X., Liu, J., Yu, X., Khandelwal, U., Gu, Q., Wang, L., and Han, J. (2014). ClusCite: effective citation recommendation by information network-based clustering. In the 20th ACM SIGKDD international conference, pages 821\u2013 830, New York, New York, USA. ACM.\nSmucker, M. D. and Allan, J. (2006). Findsimilar: similarity browsing as a search tool. In The 29th annual international ACM SIGIR conference, pages 461\u2013468, New York, New York, USA. ACM.\nTang, J. and Zhang, J. (2009). A Discriminative Approach to Topic-Based Citation Recommendation. In Advances in Knowledge Discovery and Data Mining , pages 572\u2013579. Springer Berlin Heidelberg, Berlin, Heidelberg.\nWeng, L., Li, Z., Cai, R., Zhang, Y., Zhou, Y., Yang, L. T., and Zhang, L. (2011). Query by document via a decomposition-based two-level retrieval approach. In the 34th international ACM SIGIR conference, pages 505\u2013514, New York, New York, USA. ACM.\nWilbur, W. J. and Coffee, L. (1994). The effectiveness of document neighboring in search enhancement. Information Processing and Management .\nWu, H., Hua, Y., Li, B., and Pei, Y. (2012). Enhancing citation recommendation with various evidences. In the 9th International Conference on Fuzzy Systems and Knowledge Discovery , pages 1160\u20131165. IEEE.\nXue, X. and Croft, W. B. (2009). Transforming patents into prior-art queries. In the 32nd international ACM SIGIR conference, pages 808\u2013809, New York, New York, USA. ACM.\nYang, Y., Bansal, N., Dakka, W., Ipeirotis, P., Koudas, N., and Papadias, D. (2009). Query by document. In WSDM \u201909: Proceedings of the Second ACM International Conference on Web Search and Data Mining . ACM.\nYang, Z., Gupta, N., Sun, X., and Xu, D. (2015). Learning to Answer Biomedical Factoid and List Questions OAQA at BioASQ 3B. Proceedings of the Fourth BioASQ workshop.\nYilmaz, E., Aslam, J. A., and Robertson, S. (2008). A new rank correlation coefficient for information retrieval. In the Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval , pages 587\u2013594, New York, New York, USA. ACM.\nYu, H., Liu, F., and Ramesh, B. P. (2010). Automatic Figure Ranking and User Interfacing for Intelligent Figure Search. PloS one, 5(10), e12983."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Motivation: Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Results: Our results show that authors\u2019 ranking differ significantly from rankings by textsimilarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM) significantly surpassed state-of-the-art baseline systems. Availability: Code and data are both publicly available. \u2020", "creator": "LaTeX with hyperref package"}}}