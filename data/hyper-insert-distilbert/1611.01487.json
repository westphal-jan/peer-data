{"id": "1611.01487", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Morphological Inflection Generation with Hard Monotonic Attention", "abstract": "we both present a supervised sequence to classic sequence echo transduction model with a hard attention matching mechanism which combines largely the more traditional statistical alignment methods with the power properties of recurrent auditory neural networks. we instead evaluate the cue model depends on exiting the task ahead of discovering morphological inflection generation and show that it merely provides state of largely the art descriptive results in essentially various verbal setups compared uniformly to previous the earliest previous neural and generally non - neural retrieval approaches. eventually we present an output analysis of mainly the learned preference representations for both earlier hard and soft attention models, usually shedding light on the features such models extract in order mainly to solve off the task.", "histories": [["v1", "Fri, 4 Nov 2016 18:42:47 GMT  (2156kb,D)", "http://arxiv.org/abs/1611.01487v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Thu, 8 Dec 2016 12:33:44 GMT  (2145kb,D)", "http://arxiv.org/abs/1611.01487v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Tue, 11 Apr 2017 08:51:27 GMT  (591kb,D)", "http://arxiv.org/abs/1611.01487v3", "Accepted as a long paper in ACL 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roee aharoni", "yoav goldberg"], "accepted": true, "id": "1611.01487"}, "pdf": {"name": "1611.01487.pdf", "metadata": {"source": "CRF", "title": "HARD MONOTONIC ATTENTION", "authors": ["Roee Aharoni"], "emails": ["roee.aharoni@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural sequence to sequence transduction became a prominent approach to natural language processing tasks like morphological inflection generation (Faruqui et al., 2016) and automatic summarization (Rush et al., 2015) among others. A common way to improve the vanilla encoder-decoder framework for sequence to sequence tasks is the (soft) attention mechanism (Bahdanau et al., 2014) which enables the decoder to attend at specific elements in the encoded sequence, overcoming the issues in encoding very long sequences to a single vector.\nIt was also shown that the attention mechanism effectively learns an alignment between the input and the output sequences which is naturally found in the data, and practically learns this alignment to attend at the relevant elements of he input. However, in many NLP tasks like automatic transliteration or morphological inflection generation, the data is roughly monotonically aligned \u2013 meaning the ability of the soft attention model to attend at the entire input sequence may not be optimal for such tasks, while also requiring a relatively large amount of training examples which is not always available, especially for morphologically rich, low resource languages.\nThe more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combined expert knowledge with data-driven parameter tuning. While the FST approaches may work well even on small datasets due to their engineered structure, it may be cumbersome to use them while conditioning on the entire output history as it requires a very large set of states, resulting in a model conditioning only on the last predicted output symbol (Rastogi et al., 2016).\nWe propose a new model which handles the previous issues by directly modeling a monotonic alignment between the input and output sequences. The model consists of an encoder-decoder neural network with a dedicated control mechanism, which enables to perform hard attention while forcing monotonicity: in each step, the decoder is either writing a symbol to the output sequence or advancing an attention pointer to the next element from the bi-directionally encoded input sequence, as described visually in Figure 1.\nThis modeling suits the natural monotonic alignment between the input and output very well, as the network learns to attend at the relevant inputs before writing the output which they are aligned to. A bi-directional encoder together with the hard attention mechanism enables to condition on the entire input sequence, as each element in the input is represented using a concatenation of a\nar X\niv :1\n61 1.\n01 48\n7v 1\n[ cs\n.C L\n] 4\nN ov\n2 01\n6\nforward LSTM running from the beginning of the sequence to the element and a backward LSTM running from the end of the sequence to the element. Since each element representation is aware to the entire context, non-monotone alignments are also modeled, which is important in tasks where segments in the output sequence are a result of long range dependencies in the input sequence. The recurrent nature of the decoder, together with a dedicated feedback connection that passes the last prediction to the next decoder step explicitly, enables the model to condition on the entire output history at each prediction step. The hard attention mechanism allows the network to jointly align and transduce while using a focused representation at each step, rather then the weighted sum of representations used in the soft attention model.\nTo evaluate our model, we perform extensive experiments on three previously studied datasets for the morphological inflection generation task, which involves generating a target word (e.g. \u201cha\u0308rtestem\u201d, the German word for \u201chardest\u201d), given a source word (e.g. \u201chart\u201d, the German word for \u201chard\u201d) and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative etc.). Several studies showed that inflection generation is beneficial for phrase-based machine translation (Chahuneau et al., 2013) and more recently for neural machine translation (Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016). We show that while our model is better or on-par with the previous state-of-the-art on the task, it is also performing significantly better for very small training sets, being the first neural model to surpass the performance of a weighted FST model with latent variables specifically tailored for it (Dreyer et al., 2008). Finally, we analyze and compare our model and the soft attention model, showing how they function very similarly with respect to the alignments and representations they learn, in spite of our model being much simpler."}, {"heading": "2 THE HARD ATTENTION MODEL", "text": ""}, {"heading": "2.1 MOTIVATION", "text": "We would like to transduce the input sequence, x1:n \u2208 \u03a3\u2217x into the output sequence, y1:m \u2208 \u03a3\u2217y , where \u03a3x and \u03a3y are the input and output vocabularies, respectively. Imagine a machine with readonly, random access to the encoding of the input sequence, and a single pointer that determines the current read location. We can then model the sequence transduction as a series of write operations and pointer movement operations. In the case where the alignment between the sequences is monotonic, the pointer movement can be controlled by a single \u201cmove one step forward\u201d operation (step) which we add to the output vocabulary. We implement this behavior using an encoder-decoder neural network, with a control mechanism which determines in each step of the decoder whether it is time to predict an output symbol or promote the attention pointer the next element of the encoded input."}, {"heading": "2.2 MODEL DEFINITION", "text": "In prediction time, we seek the output sequence y1:m \u2208 \u03a3\u2217y , for which:\ny1:m = arg max y\u2032\np(y\u2032|x, f) (1)\nWhere: x \u2208 \u03a3\u2217x is the input sequence and: f = {f1, ..., fm} is a set of features influencing the transduction task (for example, in the inflection generation task these would be the desired morphosyntactic features of the output sequence). Since we want our model to force a monotonic alignment between the input and the output, we instead look for a sequence of actions: s1:q \u2208 \u03a3\u2217s , where: \u03a3s = \u03a3y \u222a{step}. This sequence is the step/write action sequence required to go from x1:n to y1:m according to the monotonic alignment between them. In this case we define:\ns1:q = arg max s\u2032 p(s\u2032|x1:n, f) = arg max s\u2032 \u220f s\u2032i\u2208s\u2032 p(s\u2032i|s\u20320...s\u2032i\u22121, x1:n, f) (2)\nWe can then estimate this using a neural network:\ns1:q = arg max s\u2032 NN(x, f,\u0398) (3)\nWhere the network\u2019s parameters \u0398 are learned using a set of training examples. We will now describe the network architecture."}, {"heading": "2.3 NETWORK ARCHITECTURE", "text": "Notation We use bold letters for vectors and matrices. We treat LSTM as a parameterized function LSTM\u03b8(x1...xn) mapping a sequence of input vectors x1...xn to a an output vector hn.\nEncoder For every element in the input sequence: x1:n = x1...xn, we take the corresponding embedding: ex1 ...exn , where: exi \u2208 RE . We then feed the embeddings into a bi-directional LSTM encoder (Graves & Schmidhuber, 2005) which results in a sequence of vectors: x1:n = x0...xn \u2208 R2H , where each vector xi = [LSTMforward(x1,x2, ...xi),LSTMbackward(xn,xn\u22121...xi)].\nDecoder Once the input sequence is encoded, we feed the decoder RNN, LSTMdec, with three inputs at each step:\n1. The current attended input, xa \u2208 R2H , initialized with the first element of the encoded sequence, x0.\n2. A set of feature embeddings that influence the generation process, concatenated to a single vector: f = [f0...fm] \u2208 RF \u00b7m.\n3. yi\u22121 \u2208 RE , which is the embedding of the predicted output symbol in the previous decoder step.\nThose three inputs are concatenated into a single vector zi = [xa, f ,yi\u22121] \u2208 R2H+F \u00b7m+E , which is fed into the decoder, providing the decoder output vector: LSTMdec(z1...zi) \u2208 RH . Finally, to model the distribution over the possible actions, we project the decoder output to a vector of |\u03a3y\u0302| elements, followed by a softmax layer:\np(si = cj) = softmax j(R \u00b7 LSTMdec(z1...zi) + b) (4) Control Mechanism When the most probable action is step, the attention is promoted so xa contains the next encoded input representation to be used in the next step of the decoder. This process is demonstrated visually in Figure 1."}, {"heading": "2.4 TRAINING THE MODEL", "text": "For every example: (x1:n, y1:m, f) in the training data, we should produce a sequence of step and write actions s1:q to be predicted by the decoder, which is dependent on the alignment between\nthe input and the output \u2013 the network must attend at all the input elements aligned to an output element before writing it. While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process. For that purpose, we first run a character level alignment process on the training data. We use the character alignment model of Sudoh et al. (2013) which is based on a Chinese Restaurant Process which weights single alignments (characterto-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments. Specifically, we use the implementation provided by the organizers of the SIGMORPHON2016 shared task.1 Once we have the character level alignment per input-output sequence pair in the training set, we deterministically infer the sequence of actions s1:q that results in the desired output by attending at all the input elements aligned to an output element (using the step action) before writing it. We then train the network to produce this sequence of actions when given the input sequence and the set of morpho-syntactic attributes matching the desired inflection by using a conventional cross entropy loss function per example:\nL(x1:n, y1:m, f,\u0398) = \u2212 \u2211\nsj\u2208s1:q\nlog softmax j(R \u00b7 LSTMdec(z0...zi) + b) (5)"}, {"heading": "3 EXPERIMENTS", "text": "We perform extensive experiments with three previously studied morphological inflection generation datasets to evaluate our hard attention model in various settings. In all experiments we report the results of the best performing neural and non-neural baselines which were previously published on those datasets to our knowledge. The implementation details for the models are available in the supplementary material of this paper. The source code for the models is available on github.2"}, {"heading": "3.1 CELEX", "text": "In order to examine if our model fits the task, we first evaluate it on a very small dataset to see if it avoids the tendency to overfit on few training examples. For this purpose we report exact match accuracy on the German inflection generation dataset compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Schu\u0308tze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT). Following previous reports on this dataset, we use the same data splits as Dreyer et al. (2008), dividing the data for each inflection type into five folds, each consisting of 500 training, 1000 development and 1000 test examples. We train a separate model for each fold and report exact match accuracy, averaged over the five folds."}, {"heading": "3.2 WIKTIONARY", "text": "To neutralize the negative effect of very small training sets on the performance of the different learning approaches, we evaluate our model on the dataset created by Durrett & DeNero (2013), which contains up to 360k training examples per language. It was built by extracting Finnish, German and Spanish inflection tables from Wiktionary, used in order to evaluate their system based on string alignments and a semi-CRF sequence classifier with linguistically inspired features. We also used the expansion made by Nicolai et al. (2015) to include French and Dutch inflections as well. Their\n1https://github.com/ryancotterell/sigmorphon2016 2https://github.com/roeeaharoni/morphological-reinflection 3The acronyms stand for: 13SIA=1st/3rd person, singular, indefinite, past;13SKE=1st/3rd person, subjunctive, present; 2PIE=2nd person, plural, indefinite, present;13PKE=1st/3rd person, plural, subjunctive, present; 2PKE=2nd person, plural, subjunctive, present; z=infinitive; rP=imperative, plural; pA=past participle.\nsystem also performs an align-and-transduce approach, extracting rules from the aligned training set and applying them in inference time with a proprietary character sequence classifier. In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al. (2016), which coupled the alignment and transduction tasks, requiring a beam search decoding procedure."}, {"heading": "3.3 SIGMORPHON", "text": "As different languages show different morphological phenomena, we also experiment with how our model copes with this variety using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016). Here the training data consists of ten languages, with five morphological system types (detailed in Table 3): Russian (RU), German (DE), Spanish (ES), Georgian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 development examples per language. We compare our model to two soft attention baselines on this dataset: MED (Kann & Schu\u0308tze, 2016a), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by Luong et al. (2015).\n4 RESULTS\nOn the low resource setting (CELEX), our model significantly outperforms both the recent neural models of Kann & Schu\u0308tze (2016b) and Rastogi et al. (2016) and the morphologically aware latent variable model of Dreyer et al. (2008), as detailed in Table 1. It is also, to our knowledge, the first model that surpassed in overall accuracy the latent variable model on this dataset. We explain our advantage over the soft attention model by the ability of the hard attention control mechanism to harness the monotonic alignments found in the data, while also conditioning on the entire output history which wasn\u2019t available in the FST models. Figure 2 plots the train-set and dev-set accuracies of the soft and hard attention models as a function of the training epoch. While both models perform similarly on the train-set (with the soft attention model fitting it slightly faster), the hard attention model performs significantly better on the dev-set. This shows the soft attention model\u2019s tendency to overfit on the small dataset, as it has significantly more parameters and modeling power and is not enforcing the monotonic assumption of the hard attention model.\nOn the large training set experiments (Wiktionary), our model is the best performing model on German verbs, Finnish nouns/adjectives and Dutch verbs, resulting in the highest average accuracy across all the inflection types when compared to the four previous neural and non-neural state of the art baselines. This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of Faruqui et al. (2016) which does not employ an attention mechanism. Our model is also significantly\nmore accurate than the model of Yu et al. (2016), showing the advantage in using independently learned alignments to guide the network from the beginning of the training process.\nAs can be seen in Table 3, on the SIGMORPHON 2016 dataset our model performs better than both soft-attention baselines for the suffixing+stem-change languages (Russian, German and Spanish) and is slightly less accurate than our implementation of the soft attention model on the rest of the languages, which is now the best performing model on this dataset to our knowledge. We explain this by looking at the languages from a linguistic typology point of view, as detailed in Cotterell et al. (2016). Since Russian, German and Spanish employ a suffixing morphology with internal stem changes, they are more suitable for monotonic alignment as the transformations they need to model are the addition of suffixes and changing characters in the stem. The rest of the languages in the dataset employ more context sensitive morphological phenomena like vowel harmony and consonant harmony, which require to model long range dependencies in the input sequence which better suits the soft attention mechanism. While our implementation of the soft attention model and MED are very similar model-wise, we hypothesize that our soft attention results are better due to the fact that we trained the model for 100 epochs and picked the best performing model on the development set, while the MED system was trained for a fixed amount of 20 epochs (although trained on both train and development data)."}, {"heading": "5 ANALYSIS", "text": ""}, {"heading": "5.1 COMPARISON OF LEARNED ALIGNMENTS", "text": "In order to see if the alignments our model predict fit the monotonic alignment structure found in the data, and are they more suitable for the task when compared to the alignments found by the soft\nattention model, we examined alignment predictions of the two models from the CELEX dataset, depicted in Figure 3. First, we notice the alignments found by the soft attention model are also monotonic, encouraging our modeling approach for the task. We also notice how our model learns to handle morphological phenomena like deletion, as can be seen on the right part of Figure 3, showing the alignments for the inflection: legte\u2192lege. This inflection requires the model to delete the fourth character of the input sequence. We can see the model learned to delete by reading and writing each character until it reaches the t character, which is deleted by performing two consecutive step operations. Another notable morphological transformation is the one-to-many alignment, found on the example in the left: flog\u2192fliege, where the model needs to transform a character in the input, o, to two characters in the output, ie. We can see the model learns to do this by performing two consecutive write operations after the step operation of the relevant character to be replaced. We also notice that in this case, the soft attention model performs a slightly different alignment by aligning the character i to o and the character g to the sequence eg, which is not the expected alignment in this case from a linguistic point of view."}, {"heading": "5.2 LEARNED REPRESENTATIONS ANALYSIS", "text": "When witnessing the success of the hard and soft attention models in sequence to sequence transduction, the following questions arise: how does the models manage to learn monotonic alignments? perhaps the network learns to encode the sequential position as part of its encoding of an input element? In an attempt to answer those questions, we performed the following analysis. We took 500 continuous character representations in context from each model, where every representation is a vector in R200 which is the output of the bi-LSTM encoder of the model. Every vector of this\nform carries information of a specific character with its context. We perform dimension reduction to reduce those two sets of vectors, each in: R500\u00d7200 into: R500\u00d72 using SVD. We then plot the 2D character-in-context representations and color them in two ways. First, we color by the character they represent, where we have different color for each character in the alphabet (Figure 4). In the second plot we color the representaions by the location of the character in the input sequence: here blue implies the character is closer to the beginning of the sequence and red implies it is closer to the end (Figure 5)\nWe can see that while both models tend to cluster similar character representations together (Figure 4), the hard attention model tends to have more dense character clusters. This is explained by looking at the location information: while both models encode the positional information to some extent, this information is much more pronounced in the soft-attention mechanism, where the X dimension correlates very strongly with the position information. It seems that the soft-attention mechanism encourages the encoder to encode positional information in its representation. In contrast, our hardattention model has other means of obtaining the position information in the decoder using the step actions, and indeed does not encode it as strongly in the continuous representations. This behavior allows it to perform well even with fewer examples as the location information is not learned implicitly."}, {"heading": "6 RELATED WORK", "text": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). While these studies achieved high accuracies, they also make specific assumptions about the set of possible morphological processes that create the inflection, and require feature engineering over the input. More recently, Faruqui et al. (2016) used encoder-decoder neural networks for inflection generation. The general idea is to use an encoder-decoder network over characters, that encodes the input lemma into a vector and decodes it one character at a time into the inflected surface word. Kann & Schu\u0308tze (2016b;a) explored the soft attention model proposed for machine translation (Bahdanau et al., 2014). This work also gave the best results in the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another notable contribution is the work on weighting finite state transducers with neural context (Rastogi et al., 2016). There, the arcs of an FST are scored by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with Bi-Directional LSTM\u2019s. Another recent work by Yu et al. (2016) introduced an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. Our model differs from the latter in the fact that it learns the alignments independently, enabling the use of correct alignments from the beginning of the training phase."}, {"heading": "7 CONCLUSION", "text": "We presented the hard attention model for sequence to sequence learning of monotonically aligned sequences and evaluated it on the task of morphological inflection generation. The model employs an explicit alignment model learned independently in train time which is used to teach a neural network to perform both alignment and transduction while decoding with a hard attention mechanism. We showed that our model performs better or on par with more complex soft attention models on wellstudied morphological inflection datasets, forming a new state of the art on the CELEX dataset and the Wiktionary dataset and outperforming the best system in the SIGMORPHON2016 inflection generation task. Future work may include experimenting with different external alignment methods, or applying the model to other tasks which require a monotonic align-and-transduce approach like abstractive summarization or transliteration."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "Paradigm classification in supervised learning of morphology", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden"], "venue": "The Association for Computational Linguistics,", "citeRegEx": "Ahlberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2015}, {"title": "The {CELEX} lexical data base on {CDROM", "author": ["R Harald Baayen", "Richard Piepenbrock", "Rijn van H"], "venue": null, "citeRegEx": "Baayen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1993}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Victor Chahuneau", "Eva Schlinger", "Noah A. Smith", "Chris Dyer"], "venue": "In EMNLP, pp. 1677\u20131687", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "The SIGMORPHON 2016 shared task\u2014morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden"], "venue": "In Proceedings of the 2016 Meeting of SIGMORPHON, Berlin,", "citeRegEx": "Cotterell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Markus Dreyer", "Jason Eisner"], "venue": "In EMNLP,", "citeRegEx": "Dreyer and Eisner.,? \\Q2011\\E", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2011}, {"title": "Latent-variable modeling of string transductions with finite-state methods", "author": ["Markus Dreyer", "Jason R Smith", "Jason Eisner"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Dreyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dreyer et al\\.", "year": 2008}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Durrett and DeNero.,? \\Q2013\\E", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Parameter estimation for probabilistic finite-state transducers", "author": ["Jason Eisner"], "venue": "In Proceedings of the 40th annual meeting on Association for Computational Linguistics,", "citeRegEx": "Eisner.,? \\Q2002\\E", "shortCiteRegEx": "Eisner.", "year": 2002}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "venue": "In NAACL HLT", "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "arXiv preprint arXiv:1609.04621,", "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Mans Hulden", "Markus Forsberg", "Malin Ahlberg"], "venue": "In Gosse Bouma and Yannick Parmentier 0001 (eds.), EACL,", "citeRegEx": "Hulden et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hulden et al\\.", "year": 2014}, {"title": "Single-model encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Regular models of phonological rule systems", "author": ["Ronald M. Kaplan", "Martin Kay"], "venue": "Computational Linguistics,", "citeRegEx": "Kaplan and Kay.,? \\Q1994\\E", "shortCiteRegEx": "Kaplan and Kay.", "year": 1994}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Two-level morphology: A general computational model of word-form recognition and production", "author": ["Kimmo Koskenniemi"], "venue": "Technical Report Publication No. 11,", "citeRegEx": "Koskenniemi.,? \\Q1983\\E", "shortCiteRegEx": "Koskenniemi.", "year": 1983}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A rational design for a weighted finite-state transducer library", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "In International Workshop on Implementing Automata,", "citeRegEx": "Mohri et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 1997}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Nicolai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner"], "venue": "In Proc. of NAACL,", "citeRegEx": "Rastogi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Noise-aware character alignment for bootstrapping statistical machine transliteration from bilingual corpora", "author": ["Katsuhito Sudoh", "Shinsuke Mori", "Masaaki Nagata"], "venue": "In EMNLP,", "citeRegEx": "Sudoh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sudoh et al\\.", "year": 2013}, {"title": "Minimally supervised morphological analysis by multimodal alignment", "author": ["David Yarowsky", "Richard Wicentowski"], "venue": "In ACL. The Association for Computational Linguistics,", "citeRegEx": "Yarowsky and Wicentowski.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky and Wicentowski.", "year": 2000}, {"title": "Online segment to segment neural transduction", "author": ["Lei Yu", "Jan Buys", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1609.08194,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Neural sequence to sequence transduction became a prominent approach to natural language processing tasks like morphological inflection generation (Faruqui et al., 2016) and automatic summarization (Rush et al.", "startOffset": 147, "endOffset": 169}, {"referenceID": 22, "context": ", 2016) and automatic summarization (Rush et al., 2015) among others.", "startOffset": 36, "endOffset": 55}, {"referenceID": 2, "context": "A common way to improve the vanilla encoder-decoder framework for sequence to sequence tasks is the (soft) attention mechanism (Bahdanau et al., 2014) which enables the decoder to attend at specific elements in the encoded sequence, overcoming the issues in encoding very long sequences to a single vector.", "startOffset": 127, "endOffset": 150}, {"referenceID": 17, "context": "The more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al.", "startOffset": 119, "endOffset": 158}, {"referenceID": 19, "context": "The more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combined expert knowledge with data-driven parameter tuning.", "startOffset": 230, "endOffset": 264}, {"referenceID": 8, "context": "The more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combined expert knowledge with data-driven parameter tuning.", "startOffset": 230, "endOffset": 264}, {"referenceID": 21, "context": "While the FST approaches may work well even on small datasets due to their engineered structure, it may be cumbersome to use them while conditioning on the entire output history as it requires a very large set of states, resulting in a model conditioning only on the last predicted output symbol (Rastogi et al., 2016).", "startOffset": 296, "endOffset": 318}, {"referenceID": 3, "context": "Several studies showed that inflection generation is beneficial for phrase-based machine translation (Chahuneau et al., 2013) and more recently for neural machine translation (Garc\u0131\u0301a-Mart\u0131\u0301nez et al.", "startOffset": 101, "endOffset": 125}, {"referenceID": 10, "context": ", 2013) and more recently for neural machine translation (Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 57, "endOffset": 89}, {"referenceID": 6, "context": "We show that while our model is better or on-par with the previous state-of-the-art on the task, it is also performing significantly better for very small training sets, being the first neural model to surpass the performance of a weighted FST model with latent variables specifically tailored for it (Dreyer et al., 2008).", "startOffset": 301, "endOffset": 322}, {"referenceID": 2, "context": "While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process.", "startOffset": 90, "endOffset": 130}, {"referenceID": 25, "context": "While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process.", "startOffset": 90, "endOffset": 130}, {"referenceID": 2, "context": "While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process. For that purpose, we first run a character level alignment process on the training data. We use the character alignment model of Sudoh et al. (2013) which is based on a Chinese Restaurant Process which weights single alignments (characterto-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments.", "startOffset": 91, "endOffset": 575}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993).", "startOffset": 31, "endOffset": 52}, {"referenceID": 4, "context": "For this purpose we report exact match accuracy on the German inflection generation dataset compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al.", "startOffset": 32, "endOffset": 403}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al.", "startOffset": 32, "endOffset": 473}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al.", "startOffset": 32, "endOffset": 523}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT).", "startOffset": 32, "endOffset": 628}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT). Following previous reports on this dataset, we use the same data splits as Dreyer et al. (2008), dividing the data for each inflection type into five folds, each consisting of 500 training, 1000 development and 1000 test examples.", "startOffset": 32, "endOffset": 847}, {"referenceID": 20, "context": "We also used the expansion made by Nicolai et al. (2015) to include French and Dutch inflections as well.", "startOffset": 35, "endOffset": 57}, {"referenceID": 9, "context": "In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 9, "context": "In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al. (2016), which coupled the alignment and transduction tasks, requiring a beam search decoding procedure.", "startOffset": 95, "endOffset": 181}, {"referenceID": 4, "context": "As different languages show different morphological phenomena, we also experiment with how our model copes with this variety using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016).", "startOffset": 204, "endOffset": 228}, {"referenceID": 4, "context": "As different languages show different morphological phenomena, we also experiment with how our model copes with this variety using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016). Here the training data consists of ten languages, with five morphological system types (detailed in Table 3): Russian (RU), German (DE), Spanish (ES), Georgian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 development examples per language. We compare our model to two soft attention baselines on this dataset: MED (Kann & Sch\u00fctze, 2016a), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by Luong et al. (2015).", "startOffset": 205, "endOffset": 807}, {"referenceID": 20, "context": "On the low resource setting (CELEX), our model significantly outperforms both the recent neural models of Kann & Sch\u00fctze (2016b) and Rastogi et al. (2016) and the morphologically aware latent variable model of Dreyer et al.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "(2016) and the morphologically aware latent variable model of Dreyer et al. (2008), as detailed in Table 1.", "startOffset": 62, "endOffset": 83}, {"referenceID": 9, "context": "This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of Faruqui et al. (2016) which does not employ an attention mechanism.", "startOffset": 178, "endOffset": 200}, {"referenceID": 25, "context": "more accurate than the model of Yu et al. (2016), showing the advantage in using independently learned alignments to guide the network from the beginning of the training process.", "startOffset": 32, "endOffset": 49}, {"referenceID": 4, "context": "We explain this by looking at the languages from a linguistic typology point of view, as detailed in Cotterell et al. (2016). Since Russian, German and Spanish employ a suffixing morphology with internal stem changes, they are more suitable for monotonic alignment as the transformations they need to model are the addition of suffixes and changing characters in the stem.", "startOffset": 101, "endOffset": 125}, {"referenceID": 17, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al.", "startOffset": 121, "endOffset": 160}, {"referenceID": 12, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015).", "startOffset": 246, "endOffset": 388}, {"referenceID": 0, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015).", "startOffset": 246, "endOffset": 388}, {"referenceID": 20, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015).", "startOffset": 246, "endOffset": 388}, {"referenceID": 2, "context": "Kann & Sch\u00fctze (2016b;a) explored the soft attention model proposed for machine translation (Bahdanau et al., 2014).", "startOffset": 92, "endOffset": 115}, {"referenceID": 4, "context": "This work also gave the best results in the SIGMORPHON 2016 shared task (Cotterell et al., 2016).", "startOffset": 72, "endOffset": 96}, {"referenceID": 21, "context": "Another notable contribution is the work on weighting finite state transducers with neural context (Rastogi et al., 2016).", "startOffset": 99, "endOffset": 121}, {"referenceID": 0, "context": ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015). While these studies achieved high accuracies, they also make specific assumptions about the set of possible morphological processes that create the inflection, and require feature engineering over the input. More recently, Faruqui et al. (2016) used encoder-decoder neural networks for inflection generation.", "startOffset": 8, "endOffset": 298}, {"referenceID": 0, "context": ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015). While these studies achieved high accuracies, they also make specific assumptions about the set of possible morphological processes that create the inflection, and require feature engineering over the input. More recently, Faruqui et al. (2016) used encoder-decoder neural networks for inflection generation. The general idea is to use an encoder-decoder network over characters, that encodes the input lemma into a vector and decodes it one character at a time into the inflected surface word. Kann & Sch\u00fctze (2016b;a) explored the soft attention model proposed for machine translation (Bahdanau et al., 2014). This work also gave the best results in the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another notable contribution is the work on weighting finite state transducers with neural context (Rastogi et al., 2016). There, the arcs of an FST are scored by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with Bi-Directional LSTM\u2019s. Another recent work by Yu et al. (2016) introduced an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read.", "startOffset": 8, "endOffset": 1110}], "year": 2017, "abstractText": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.", "creator": "LaTeX with hyperref package"}}}