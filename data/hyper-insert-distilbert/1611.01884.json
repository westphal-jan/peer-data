{"id": "1611.01884", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification", "abstract": "following recently applied deeplearning models devices have been shown routinely to be incredibly capable of typically making reasonably remarkable performance footprints in sentences modeling and documents pending classification tasks. in doing this first work, we propose launching a fully novel framework called ac - blstm for modeling their setences and documents, which typically combines processing the asymmetric and convolution neural network ( acnn ) with the bidirectional \u2013 long band short - term memory network ( blstm ). experiment results will demonstrate that performing our cell model regularly achieves sophisticated state - be of - the - art assessment results implemented on all above six computing tasks, additionally including sentiment query analysis, question room type classification, and formal subjectivity classification.", "histories": [["v1", "Mon, 7 Nov 2016 03:39:52 GMT  (112kb,D)", "https://arxiv.org/abs/1611.01884v1", "7 pages"], ["v2", "Thu, 15 Dec 2016 03:22:12 GMT  (112kb,D)", "http://arxiv.org/abs/1611.01884v2", "7 pages"], ["v3", "Mon, 5 Jun 2017 03:47:15 GMT  (114kb,D)", "http://arxiv.org/abs/1611.01884v3", "9 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["depeng liang", "yongdong zhang"], "accepted": false, "id": "1611.01884"}, "pdf": {"name": "1611.01884.pdf", "metadata": {"source": "CRF", "title": "AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification", "authors": ["Depeng Liang", "Yongdong Zhang", "Guang Zhou"], "emails": ["liangdp@mail2.sysu.edu.cn", "lnszyd@mail.sysu.edu.cn", "lnszyd@mail.sysu.edu.cn."], "sections": [{"heading": null, "text": "Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling sentences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long ShortTerm Memory network (BLSTM). Experiment results demonstrate that our model achieves state-ofthe-art results on five tasks, including sentiment analysis, question type classification, and subjectivity classification. In order to further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM."}, {"heading": "1 Introduction", "text": "Deep neural models recently have achieved remarkable results in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015a; Simonyan and Zisserman, 2014; He et al., 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al., 2015). Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) especially Long Short-term Memory Network (LSTM), are used wildly in natural language processing tasks. With increasing datas, these two methods can reach considerable performance by requiring only limited domain knowledge and easy to be finetuned to specific applications at the same time.\nCNNs, which have the ability of capturing local correlations of spatial or temporal structures, have achieved excellent performance in computer vision and NLP tasks. And recently the emerge of some new techniques, such as Inception module (Szegedy et al.,\n\u2217Corresponding author. E-mail: lnszyd@mail.sysu.edu.cn.\n2015b), Batchnorm (Ioffe and Szegedy, 2015) and Residual Network (He et al., 2015) have also made the performance even better. For sentence modeling, CNNs perform excellently in extracting n-gram features at different positions of a sentence through convolutional filters.\nRNNs, with the ability of handling sequences of any length and capturing long-term dependencies, , have also achieved remarkable results in sentence or document modeling tasks. LSTMs (Hochreiter and Schmidhuber, 1997) were designed for better remembering and memory accesses, which can also avoid the problem of gradient exploding or vanishing in the standard RNN. Be capable of incorporating context on both sides of every position in the input sequence, BLSTMs introduced in (Graves et al., 2005; Graves and Schmidhuber, 2005) have reported to achieve great performance in Handwriting Recognition (Liwicki et al., 2007), and Machine Translation (Peris and Casacuberta, 2015) tasks.\nGenerative adversarial networks (GANs) (Goodfellow et al., 2014) are a class of generative models for learning how to produce images. Basically, GANs consist of a generator G and a discriminator D, which are trained based on game theory. G maps a input noise vector to an output image, while D takes in an image then outputs a prediction whether the input image is a sample generated by G. Recently, applications of GANs have shown that they can generate promising results (Radford et al., 2015; Denton et al., 2015). Several recent papers have also extended GANs to the semi-supervised context (Odena, 2016; Salimans et al., 2016) by simply increasing the dimension of the classifier output from K to K + 1, which the samples of the extra class are generated by G.\nIn this paper, We proposed an end-to-end architecture named AC-BLSTM by combining the ACNN with the BLSTM for sentences and documents modeling. In order to make the model deeper, instead of using the normal convolution, we apply the technique proposed in (Szegedy et al., 2015b) which employs a\nar X\niv :1\n61 1.\n01 88\n4v 3\n[ cs\n.C L\n] 5\nJ un\n2 01\n7\n1\u00d7 n convolution followed by a n\u00d7 1 convolution by spatial factorizing the n\u00d7n convolution. And we use the pretrained word2vec vectors (Mikolov et al., 2013) as the ACNN input, which were trained on 100 billion words of Google News to learn the higher-level representations of n-grams. The outputs of the ACNN are organized as the sequence window feature to feed into the multi-layer BLSTM. So our model does not rely on any other extra domain specific knowledge and complex preprocess, e.g. word segmentation, part of speech tagging and so on. We evaluate AC-BLSTM on sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach compared with other state-of-the-art methods. Further more, inspired by the ideas of extending GANs to the semi-supervised learning context by (Odena, 2016; Salimans et al., 2016), we propose a semi-supervised learning framework for text classification which further improve the performance of AC-BLSTM.\nThe rest of the paper is organized as follows. Sec-\ntion 2 presents a brief review of related work. Section 3 discusses the architecture of our AC-BLSTM and our semi-supervised framework. Section 4 presents the experiments result with comparison analysis. Section 5 concludes the paper."}, {"heading": "2 Related Work", "text": "Deep learning models have made remarkable progress in various NLP tasks recently. For example, word embeddings (Mikolov et al., 2013; Pennington et al., 2014), question answearing (Sukhbaatar et al., 2015), sentiment analysis (Tang et al., 2015; Chen et al., 2016a,b), machine translation (Sutskever et al., 2014) and so on. CNNs and RNNs are two wildly used architectures among these models. The success of deep learning models for NLP mostly relates to the progress in learning distributed word representations (Mikolov et al., 2013; Pennington et al., 2014). In these mothods, instead of using one-hot vectors by indexing words into a vocabulary, each word is modeled as a low dimensional and dense vector which encodes\nboth semantic and syntactic information of words.\nOur model mostly relates to (Kim, 2014) which combines CNNs of different filter lengths and either static or fine-tuned word vectors, and (Zhou et al., 2015) which stacks CNN and LSTM in a unified architecture with static word vectors. It is known that in computer vision, the deeper network architecture usually possess the better performance. We consider NLP also has this property. In order to make our model deeper, we apply the idea of asymmetric convolution introduced in (Szegedy et al., 2015b), which can reduce the number of the parameters, and increase the representation ability of the model by adding more nonlinearity. Then we stack the multilayer BLSTM, which is cable of analysing the future as well as the past of every position in the sequence, on top of the ACNN. The experiment results also demonstrate the effectiveness of our model."}, {"heading": "3 AC-BLSTM Model", "text": "In this section, we will introduce our AC-BLSTM architecture in detail. We first describe the ACNN which takes the word vector represented matrix of the sentence as input and produces higher-level presentation of word features. Then we introduce the BLSTM which can incorporate context on both sides of every position in the input sequence. Finally, we introduce the techniques to avoid overfitting in our model. An overall illustration of our architecture is shown in Figure 1."}, {"heading": "3.1 Asymmetric Convolution", "text": "Let xj \u2208 Rd be the d-dimensional word vector corresponding to the j-th word in the sentence and L be the maximum length of the sentence in the dataset. Then the sentence with length L is represented as\nx1:L = [x1,x2, ...,xL]. (1)\nFor those sentences that are shorter than L, we simply pad them with space.\nIn general, let ki in which i \u2208 {1, 2, 3} be the length of convolution filter. Then instead of employing the ki \u00d7 d convolution operation described in (Kim, 2014; Zhou et al., 2015), we apply the asymmetric convolution operation inspired by (Szegedy et al., 2015b) to the input matrix which factorize the ki \u00d7 d convolution into 1\u00d7 d convolution followed by a ki \u00d7 1 convolution. And in experiments, we found that employ this technique can imporve the performance. The following part of this subsection describe how we define the asymmetric convolution layer.\nFirst, the convolution operation corresponding to the 1 \u00d7 d convolution with filter w1i \u2208 Rd is applied to each word xj in the sentence and generates corresponding feature mij\nmij = f(w 1 i \u25e6 xj + b). (2)\nwhere \u25e6 is element-wise multiplication, b is a bias term and f is a non-linear function such as the sigmoid, hyperbolic tangent, etc. In our case, we choose ReLU (Nair and Hinton, 2010) as the nonlinear function. Then we get the feature map mi \u2208 RL\nmi = [mi1,m i 2, ...,m i L]. (3)\nAfter that, the second convolution operation of the asymmetric convolution layer corresponding to the ki \u00d7 1 convolution with filter w2i \u2208 Rki is applied to a window of ki features in the feature map m\ni to produce the new feature cij and the feature map c i\ncij = f(w 2 i \u25e6mij:j+ki\u22121 + b). (4)\nci = [ci1, c i 2, ..., c i L\u2212ki+1]. (5)\nwith ci \u2208 RL\u2212ki+1. Where \u25e6, b and f are the same as described above.\nAs shown in Figure 1, we simultaneously apply three asymmetric convolution layers to the input matrix, which all have the same number of filters denoted as n. Thus the output of the asymmetric convolution layer has n feature maps. To generate the input sequence of the BLSTM, for each output sequence\nof the second convolution operation in the aysmmetric convolution layer, we slice the feature maps by channel then obtained sequence of L\u2212ki +1 new features cit \u2208 Rn where t \u2208 {1, 2, ..., L \u2212 ki + 1}. Then we concatanate c1t , c 2 t and c 3 t to get the input feature for each time step\nc\u0302t = [c 1 t , c 2 t , c 3 t ]. (6)\nwhere c\u0302t \u2208 R3n for t \u2208 {1, 2, ..., L \u2212 k\u0302 + 1} and k\u0302 = max\ni ki. In general, those c\ni t where ki < k\u0302 and t > L\u2212\nk\u0302+ 1 must be dropped in order to maintain the same sequence length, which will cause the loss of some information. In our model, instead of simply cutting the sequence, we use a simple trick to obtain the same sequence length without losing the useful information as shown in Figure 2. For each output sequence cit obtained from the second convolution operation with filter length ki, we take those c i t where t >= L\u2212 k\u0302+1 then apply a fullyconnected layer to get a new feature, which has the same dimension of cit, to replace the (L\u2212 k\u0302+1)-th feature in the origin sequence."}, {"heading": "3.2 Bidirectional Long Short-Term Memory Network", "text": "First introduced in (Hochreiter and Schmidhuber, 1997) and shown as a successful model recently, LSTM is a RNN architecture specifically designed to bridge long time delays between relevant input and target events, making it suitable for problems where long range context is required, such as handwriting recognition, machine translation and so on.\nFor many sequence processing tasks, it is useful to analyze the future as well as the past of a given point in the series. Whereas standard RNNs make use of previous context only, BLSTM (Graves et al., 2005) is explicitly designed for learning long-term dependencies of a given point on both side, which has also been shown to outperform other neural network architectures in framewise phoneme recognition (Graves and Schmidhuber, 2005).\nTherefore we choose BLSTM on top of the ACNN to learn such dependencies given the sequence of higher-level features. And single layer BLSTM can extend to multi-layer BLSTM easily. Finally, we concatenate all hidden state of all the time step of BLSTM, or concatenate the last layer of all the time step hidden state of multi-layer BLSTM, to obtain final representation of the text and we add a softmax layer on top of the model for classification."}, {"heading": "3.3 Semi-supervised Framework", "text": "Our semi-supervised text classification framewrok is inspired by works (Odena, 2016; Salimans et al., 2016). We assume the original classifier classify a sample into one of K possible classes. So we can do semi-supervised learning by simply adding samples from a generative network G to our dataset and labeling them to an extra class y = K + 1. And correspondingly the dimension of our classifier output increases from K to K + 1. The configuration of our generator network G is inspired by the architecture proposed in (Radford et al., 2015). And we modify the architecture to make it suitable to the text classification tasks. Table 1 shows the configuration of each layer in the generator G. Lets assume the training batch size is m and the percentage of the generated samples among a batch training samples is pg. At each iteration of the training process, we first generate m\u00d7 pg samples from the generator G then we draw m \u2212 m \u00d7 pg samples from the real dataset. We then perform gradient descent on the AC-BLSTM and generative net G and finally update the parameters of both nets."}, {"heading": "3.4 Regularization", "text": "For model regularization, we employ two commonly used techniques to prevent overfitting during training: dropout (Srivastava et al., 2014) and batch normalization (Ioffe and Szegedy, 2015). In our model, we apply dropout to the input feature of the BLSTM, and the output of BLSTM before the softmax layer. And we apply batch normalization to outputs of each convolution operation just before the relu activation. During training, after we get the gradients of the ACBLSTM network, we first calculate the L2 norm of all gradients and sum together to get sum norm. Then we compare the sum norm to 0.5. If the sum norm is greater than 0.5, we let all the gradients multiply with 0.5/sum norm, else just use the original gradients to update the weights."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "We evaluate our model on various benchmarks. Stanford Sentiment Treebank (SST) is a popular sentiment classification dataset introduced by (Socher et al., 2013). The sentences are labeled in a finegrained way (SST-1): very negative, negative, neutral, positive, very positive. The dataset has been split into 8,544 training, 1,101 validation, and 2,210\ntesting sentences. By removing the neutral sentences, SST can also be used for binary classification (SST2), which has been split into 6,920 training, 872 validation, and 1,821 testing. Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013; Kalchbrenner et al., 2014).\nMovie Review Data (MR) proposed by (Pang and Lee, 2005) is another dataset for sentiment analysis of movie reviews. The dataset consists of 5,331 positive and 5,331 negative reviews, mostly in one sentence. We follow the practice of using 10-fold cross validation to report the result.\nFurthermore, we apply AC-BLSTM on the subjectivity classification dataset (SUBJ) released by (Pang and Lee, 2004). The dataset contains 5,000 subjective sentences and 5,000 objective sentences. We also follow the practice of using 10-fold cross validation to report the result.\nWe also benchmark our system on question type classification task (TREC) (Li and Roth, 2002), where sentences are questions in the following 6 classes: abbreviation, human, entity, description, location, numeric. The entire dataset consists of 5,452 training examples and 500 testing examples.\nFor document-level dataset, we use the sentiment classification dataset Yelp 2013 (YELP13) with user and product information, which is built by (Tang et al., 2015). The dataset has been split into 62,522\ntraining, 7,773 validation, and 8,671 testing documents. But in the experiment, we neglect the user and product information to make it consistent with the above experiment settings."}, {"heading": "4.2 Training and Implementation Details", "text": "We implement our model based on Mxnet (Chen et al., 2015) - a C++ library, which is a deep learning framework designed for both efficiency and flexibility. In order to benefit from the efficiency of parallel computation of the tensors, we train our model on a Nvidia GTX 1070 GPU. Training is done through stochastic gradient descent over shuffled mini-batches with the optimizer RMSprop (Tieleman and Hinton, 2012). For all experiments, we simultaneously apply three asymmetric convolution operation with the second filter length ki of 2, 3, 4 to the input, set the dropout rate to 0.5 before feeding the feature into BLSTM, and set the initial learning rate to 0.0001. But there are some hyper-parameters that are not the same for all datasets, which are listed in table 2. We conduct experiments on 3 datasets (MR, SST and SUBJ) to verify the effectiveness our semi-supervised framework. And the setting of pg and cg for different datasets are listed in table 3."}, {"heading": "4.3 Word Vector Initialization", "text": "We use the publicly available word2vec vectors that were trained on 100 billion words from Google News. The vectors have dimensionality of 300 and were trained using the continuous bag-of-words architecture (Mikolov et al., 2013). Words not present in the set of pre-trained words are initialized from the uniform distribution [-0.25, 0.25]. We fix the word vectors and learn only the other parameters of the model during training."}, {"heading": "4.4 Results and Discussion", "text": "We used standard train/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation. We repeated each experiment 10 times and report the mean accuracy. Results of our models against other methods are listed in table 4. To the best of our knowledge, AC-BLSTM achieves the best results on five tasks.\nCompared to methods (Kim, 2014) and (Zhou et al., 2015), which inspired our model mostly, ACBLSTM can achieve better performance which show that deeper model actually has better performance. By just employing the word2vec vectors, our model can achieve better results than (Zhang et al., 2016b) which combines multiple word embedding methods such as word2vec(Mikolov et al., 2013), glove (Pennington et al., 2014) and Syntactic embedding. And the AC-BLSTM performs better when trained with the semi-supervised framework, which proves the success of combining the generative net with ACBLSTM.\nThe experiment results show that the number of the convolution filter and the lstm memory dimension should keep the same for our model. Also the configuration of hyper-parameters: number of the convolution filter, the lstm memory dimension and the lstm layer are quiet stable across datasets. If the task is simple, e.g. TREC, we just set number of convolution filter to 100, lstm memory dimension to 100 and lstm layer to 1. And as the task becomes complicated, we simply increase the lstm layer from 1 to 4. The SST-2\nis a special case, we find that if we set the number of convolution filter and lstm memory dimension to 300 can get better result. And the dropout rate before softmax need to be tuned."}, {"heading": "5 Conclusions", "text": "In this paper we have proposed AC-BLSTM: a novel framework that combines asymmetric convolutional neural network with bidirectional long short-term memory network. The asymmetric convolutional layers are able to learn phrase-level features. Then output sequences of such higher level representations are fed into the BLSTM to learn long-term dependencies of a given point on both side. To the best of our knowledge, the AC-BLSTM model achieves top performance on standard sentiment classification, question classification and document categorization tasks. And then we proposed a semi-supervised framework for text classification which further improve the performance of AC-BLSTM. In future work, we plan to explore the combination of multiple word embeddings which are described in (Zhang et al., 2016b)."}], "references": [{"title": "Neural sentiment classification with user and product attention", "author": ["Huimin Chen", "Maosong Sun", "Cunchao Tu", "Yankai Lin", "Zhiyuan Liu"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning user and product distributed representations using a sequence model for sentiment analysis", "author": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Yunqing Xia", "Xuan Wang"], "venue": "IEEE Computational Intelligence Magazine,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Bidirectional LSTM networks for improved phoneme classification and recognition", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber"], "venue": "In ICANN,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In arXiv prepring arXiv:1506.01497,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP, pages 1746\u20131751,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Structural attention neural networks for improved sentiment analysis", "author": ["Filippos Kokkinos", "Alexandros Potamianos"], "venue": null, "citeRegEx": "Kokkinos and Potamianos.,? \\Q2017\\E", "shortCiteRegEx": "Kokkinos and Potamianos.", "year": 2017}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Li and Roth.,? \\Q2002\\E", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Semi-supervised learning with generative adversarial networks", "author": ["Augustus Odena"], "venue": "arXiv preprint arXiv:1606.01583,", "citeRegEx": "Odena.,? \\Q2016\\E", "shortCiteRegEx": "Odena.", "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee"], "venue": "In ACL,", "citeRegEx": "Pang and Lee.,? \\Q2005\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543", "author": ["Doha", "Qatar"], "venue": null, "citeRegEx": "Doha and Qatar,? \\Q2014\\E", "shortCiteRegEx": "Doha and Qatar", "year": 2014}, {"title": "A bidirectional recurrent neural language model for machine translation", "author": ["Alvaro Peris", "Francisco Casacuberta"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "Peris and Casacuberta.,? \\Q2015\\E", "shortCiteRegEx": "Peris and Casacuberta.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Rethinking the inception architecture for computer", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "vision. CoRR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In ACL,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Rui Zhang", "Honglak Lee", "Dragomir Radev"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Ye Zhang", "Stephen Roller", "Byron C. Wallace"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Topic-aware deep compositional models for sentence classification", "author": ["Rui Zhao", "Kezhi Mao"], "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing,", "citeRegEx": "Zhao and Mao.,? \\Q2017\\E", "shortCiteRegEx": "Zhao and Mao.", "year": 2017}, {"title": "A C-LSTM neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau"], "venue": "CoRR, abs/1511.08630,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Deep neural models recently have achieved remarkable results in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015a; Simonyan and Zisserman, 2014; He et al., 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al.", "startOffset": 80, "endOffset": 175}, {"referenceID": 4, "context": "Deep neural models recently have achieved remarkable results in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015a; Simonyan and Zisserman, 2014; He et al., 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al.", "startOffset": 80, "endOffset": 175}, {"referenceID": 8, "context": ", 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 29, "context": ", 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 7, "context": ", 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 22, "context": ", 2014), and questionanswering (Sukhbaatar et al., 2015).", "startOffset": 31, "endOffset": 56}, {"referenceID": 6, "context": "2015b), Batchnorm (Ioffe and Szegedy, 2015) and Residual Network (He et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 4, "context": "2015b), Batchnorm (Ioffe and Szegedy, 2015) and Residual Network (He et al., 2015) have also made the performance even better.", "startOffset": 65, "endOffset": 82}, {"referenceID": 5, "context": "LSTMs (Hochreiter and Schmidhuber, 1997) were designed for better remembering and memory accesses, which can also avoid the problem of gradient exploding or vanishing in the standard RNN.", "startOffset": 6, "endOffset": 40}, {"referenceID": 3, "context": "Be capable of incorporating context on both sides of every position in the input sequence, BLSTMs introduced in (Graves et al., 2005; Graves and Schmidhuber, 2005) have reported to achieve great performance in Handwriting Recognition (Liwicki et al.", "startOffset": 112, "endOffset": 163}, {"referenceID": 2, "context": "Be capable of incorporating context on both sides of every position in the input sequence, BLSTMs introduced in (Graves et al., 2005; Graves and Schmidhuber, 2005) have reported to achieve great performance in Handwriting Recognition (Liwicki et al.", "startOffset": 112, "endOffset": 163}, {"referenceID": 18, "context": ", 2007), and Machine Translation (Peris and Casacuberta, 2015) tasks.", "startOffset": 33, "endOffset": 62}, {"referenceID": 13, "context": "Several recent papers have also extended GANs to the semi-supervised context (Odena, 2016; Salimans et al., 2016) by simply increasing the dimension of the classifier output from K to K + 1, which the samples of the extra class are generated by G.", "startOffset": 77, "endOffset": 113}, {"referenceID": 13, "context": "Further more, inspired by the ideas of extending GANs to the semi-supervised learning context by (Odena, 2016; Salimans et al., 2016), we propose a semi-supervised learning framework for text classification which further improve the performance of AC-BLSTM.", "startOffset": 97, "endOffset": 133}, {"referenceID": 16, "context": "For example, word embeddings (Mikolov et al., 2013; Pennington et al., 2014), question answearing (Sukhbaatar et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 22, "context": ", 2014), question answearing (Sukhbaatar et al., 2015), sentiment analysis (Tang et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 23, "context": ", 2016a,b), machine translation (Sutskever et al., 2014) and so on.", "startOffset": 32, "endOffset": 56}, {"referenceID": 16, "context": "The success of deep learning models for NLP mostly relates to the progress in learning distributed word representations (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 120, "endOffset": 167}, {"referenceID": 8, "context": "Our model mostly relates to (Kim, 2014) which combines CNNs of different filter lengths and either static or fine-tuned word vectors, and (Zhou et al.", "startOffset": 28, "endOffset": 39}, {"referenceID": 29, "context": "Our model mostly relates to (Kim, 2014) which combines CNNs of different filter lengths and either static or fine-tuned word vectors, and (Zhou et al., 2015) which stacks CNN and LSTM in a unified architecture with static word vectors.", "startOffset": 138, "endOffset": 157}, {"referenceID": 8, "context": "Then instead of employing the ki \u00d7 d convolution operation described in (Kim, 2014; Zhou et al., 2015), we apply the asymmetric convolution operation inspired by (Szegedy et al.", "startOffset": 72, "endOffset": 102}, {"referenceID": 29, "context": "Then instead of employing the ki \u00d7 d convolution operation described in (Kim, 2014; Zhou et al., 2015), we apply the asymmetric convolution operation inspired by (Szegedy et al.", "startOffset": 72, "endOffset": 102}, {"referenceID": 12, "context": "In our case, we choose ReLU (Nair and Hinton, 2010) as the nonlinear function.", "startOffset": 28, "endOffset": 51}, {"referenceID": 5, "context": "First introduced in (Hochreiter and Schmidhuber, 1997) and shown as a successful model recently, LSTM is a RNN architecture specifically designed to bridge long time delays between relevant input and target events, making it suitable for problems where long range context is required, such as handwriting recognition, machine translation and so on.", "startOffset": 20, "endOffset": 54}, {"referenceID": 3, "context": "Whereas standard RNNs make use of previous context only, BLSTM (Graves et al., 2005) is explicitly designed for learning long-term dependencies of a given point on both side, which has also been shown to outperform other neural network architectures in framewise phoneme recognition (Graves and Schmidhuber, 2005).", "startOffset": 63, "endOffset": 84}, {"referenceID": 2, "context": ", 2005) is explicitly designed for learning long-term dependencies of a given point on both side, which has also been shown to outperform other neural network architectures in framewise phoneme recognition (Graves and Schmidhuber, 2005).", "startOffset": 206, "endOffset": 236}, {"referenceID": 13, "context": "Our semi-supervised text classification framewrok is inspired by works (Odena, 2016; Salimans et al., 2016).", "startOffset": 71, "endOffset": 107}, {"referenceID": 6, "context": ", 2014) and batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 32, "endOffset": 57}, {"referenceID": 20, "context": "Stanford Sentiment Treebank (SST) is a popular sentiment classification dataset introduced by (Socher et al., 2013).", "startOffset": 94, "endOffset": 115}, {"referenceID": 20, "context": "Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013; Kalchbrenner et al., 2014).", "startOffset": 168, "endOffset": 216}, {"referenceID": 7, "context": "Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013; Kalchbrenner et al., 2014).", "startOffset": 168, "endOffset": 216}, {"referenceID": 15, "context": "Movie Review Data (MR) proposed by (Pang and Lee, 2005) is another dataset for sentiment analysis of movie reviews.", "startOffset": 35, "endOffset": 55}, {"referenceID": 14, "context": "Furthermore, we apply AC-BLSTM on the subjectivity classification dataset (SUBJ) released by (Pang and Lee, 2004).", "startOffset": 93, "endOffset": 113}, {"referenceID": 11, "context": "We also benchmark our system on question type classification task (TREC) (Li and Roth, 2002), where sentences are questions in the following 6 classes: abbreviation, human, entity, description, location, numeric.", "startOffset": 73, "endOffset": 92}, {"referenceID": 25, "context": "For document-level dataset, we use the sentiment classification dataset Yelp 2013 (YELP13) with user and product information, which is built by (Tang et al., 2015).", "startOffset": 144, "endOffset": 163}, {"referenceID": 8, "context": "Compared to methods (Kim, 2014) and (Zhou et al.", "startOffset": 20, "endOffset": 31}, {"referenceID": 29, "context": "Compared to methods (Kim, 2014) and (Zhou et al., 2015), which inspired our model mostly, ACBLSTM can achieve better performance which show that deeper model actually has better performance.", "startOffset": 36, "endOffset": 55}, {"referenceID": 16, "context": ", 2013), glove (Pennington et al., 2014) and Syntactic embedding.", "startOffset": 15, "endOffset": 40}], "year": 2017, "abstractText": "Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling sentences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long ShortTerm Memory network (BLSTM). Experiment results demonstrate that our model achieves state-ofthe-art results on five tasks, including sentiment analysis, question type classification, and subjectivity classification. In order to further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM.", "creator": "LaTeX with hyperref package"}}}