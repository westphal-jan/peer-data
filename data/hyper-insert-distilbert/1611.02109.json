{"id": "1611.02109", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Differentiable Programs with Neural Libraries", "abstract": "we eventually introduce and constantly develop solutions specially for the problem analysis of spontaneous lifelong perceptual programming modeled by example ( lppbe ). the fundamental problem response is to induce nearly a growing series of programs that will require completely understanding perceptual data widely like disk images or text. novice lppbe systems learn from very weak supervision ( avoiding input - mediated output examples ) then and incrementally sometimes construct virtually a shared library of macro components equally that grows and thereafter improves as more productive tasks together are more solved. methodologically, we aggressively extend differentiable statistical interpreters to operate on perceptual input data internally and to share components across tasks. rather empirically we show that this feedback leads to a lifelong agile learning system that transfers knowledge to new tasks potentially more seemingly effectively simply than baselines, and the performance of on earlier analytical tasks initially continues tending to quickly improve even as the mature system learns on new, different structured tasks.", "histories": [["v1", "Mon, 7 Nov 2016 15:25:53 GMT  (1704kb,D)", "http://arxiv.org/abs/1611.02109v1", "Submitted to ICLR 2017"], ["v2", "Thu, 2 Mar 2017 13:34:48 GMT  (2397kb,D)", "http://arxiv.org/abs/1611.02109v2", null]], "COMMENTS": "Submitted to ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander l gaunt", "marc brockschmidt", "nate kushman", "daniel tarlow"], "accepted": true, "id": "1611.02109"}, "pdf": {"name": "1611.02109.pdf", "metadata": {"source": "CRF", "title": "LIFELONG PERCEPTUAL PROGRAMMING BY EXAMPLE", "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "A goal of artificial intelligence is to build a single large neural network model that can be trained in a lifelong learning setting; i.e., on a sequence of diverse tasks over a long period of time, and gain cumulative knowledge about different domains as it is presented with new tasks. The hope is that such systems will learn more accurately and from less data than existing systems, and that they will exhibit more flexible intelligence. However, despite some work showing promise towards multitask learning (training on many tasks at once) and transfer learning (using source tasks to improve learning in a later target task) (Caruana, 1997; Luong et al., 2015; Parisotto et al., 2015; Rusu et al., 2016), most successes of neural networks today come from training a single network on a single task, indicating that this goal is highly challenging to achieve.\nWe argue for two properties that such systems should have in addition to the ability to learn from a sequence of diverse tasks. First is the ability to learn from weak supervision. Gathering high-quality labeled datasets is expensive, and this effort is multiplied if all tasks require strong labelling. In this work, we focus on weak supervision in the form of pairs of input-output examples that come from executing simple programs with no labelling of intermediate states. Second is the ability to distill knowledge into subcomponents that can be shared across tasks. If we can learn models where the knowledge about shared subcomponents is disentangled from task-specific knowledge, then the sharing of knowledge across tasks will likely be more effective. Further, by isolating shared subcomponents, we expect that we could develop systems that exhibit reverse transfer (i.e., performance on earlier tasks automatically improves by improving the understanding of the object class in later tasks).\nA key challenge in achieving these goals with neural models is the difficulty in interpreting weights inside a trained network. Most notably, subcomponents of knowledge gained after training on one task cannot be easily transferred to related tasks. Conversely, traditional computer programs naturally structure solutions to diverse problems in an interpretable, modular form allowing (1) re-use of subroutines in solutions to new tasks and (2) modification or error correction by humans. Inspired by this fact, we develop end-to-end trainable models that structure their solutions as a library of functions, some of which are represented as source code, and some of which are neural networks.\nMethodologically, we start from recent work on programming by example (PBE) with differentiable interpreters, which shows that it is possible to use gradient descent to induce source code operating on basic data types (e.g. integers) from input-output examples (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016). In this work we combine these differentiable interpreters with neural network classifiers in an end-to-end trainable system that learns programs that manipulate perceptual\nar X\niv :1\n61 1.\n02 10\n9v 1\n[ cs\n.L G\n] 7\nN ov\n2 01\n6\ndata. In addition, we make our interpreter modular, which allows lifelong learning on a sequence of related tasks: rather than inducing one fresh program per task, the system is able to incrementally build a library of (neural) functions that are shared across task-specific programs. To encapsulte the challenges embodied in this problem formulation, we name the problem Lifelong Perceptual Programming By Example (LPPBE). Our extension of differentiable interpreters that allows perceptual data types, neural network function definitions, and lifelong learning is called NEURAL TERPRET (NTPT).\nEmpirically, we show that a NTPT-based model learns to perform a sequence of tasks based on images of digits and mathematical operators. In early tasks, the model learns the concepts of digits and mathematical operators from a variety of weak supervision, then in a later task it learns to compute the results of variable-length mathematical expressions. The approach is resilient to catastrophic forgetting (McCloskey & Cohen, 1989; Ratcliff, 1990); on the contrary, results show that performance continues to improve on earlier tasks even when only training on later tasks. In total, the result is a method that can gather knowledge from a variety of weak supervision, distill it into a cumulative, re-usable library, and use the library within induced algorithms to exhibit strong generalization."}, {"heading": "2 PERCEPTUAL PROGRAMMING BY EXAMPLE", "text": "We briefly review the TERPRET language (Gaunt et al., 2016) for constructing differentiable interpreters. To address LPPBE, we develop NEURAL TERPRET, an extension to support lifelong learning, perceptual data types, and neural network classifiers. We also define our tasks."}, {"heading": "2.1 TERPRET", "text": "TERPRET programs describe differentiable interpreters by defining the relationship between Inputs and Outputs via a set of inferrable Params that define an executable program and Vars that store intermediate results. TERPRET requires all of these variables to be finite integers. To learn using gradient descent, the model is made differentiable by a compilation step that lifts the\nrelationships between integers specified by the TERPRET code to relationships between marginal distributions over integers in finite ranges. There are two key operations in this compilation process:\n\u2022 Function application. The statement z.set to(foo(x, y)) is translated into \u00b5zi =\u2211 jk Iijk\u00b5 x j\u00b5 y k where \u00b5\na represents the marginal distribution for the variable a and I is an indicator tensor 1[i = foo(j, k)]. This approach extends to all functions mapping any number of integer arguments to an integer output.\n\u2022 Conditional statements The statements if x == 0: z.set to(a); elif x == 1: z.set to(b) are translated to \u00b5z = \u00b5x0\u00b5 a+\u00b5x1\u00b5 b. More complex statements\nfollow a similar pattern, with details given in Gaunt et al. (2016).\nThis compilation process yields a TensorFlow Abadi et al. (2016) computation graph containing many of these two operations, which can then be trained using standard methods."}, {"heading": "2.2 NEURAL TERPRET", "text": "To handle perceptual data, we relax the restriction that all variables need to be finite integers. We introduce a new tensor type whose dimensions are fixed at declaration, and which is suitable to store perceptual data. Additionally, we introduce learnable functions that can process vector variables. A learnable function is declared using @Learn([d1, . . . , dD], dout, hid sizes=[`1, . . . , `L]), where the first component specifies the dimensions d1, . . . , dD of the inputs (which can be finite integers or tensors) and the second the dimension of the output. NTPT compiles such functions into a fully-connected feed-forward neural network whose layout can be controlled by the hid sizes component, which specifies the number of layers and neurons in each layer. The inputs of the function are simply concatenated. Vector output is generated by learning a mapping from the last hidden layer, and finite integer output is generated by a softmax layer generating a distribution over integers up to the declared bound. Learnable parameters for the generated network are shared across every use in the NTPT program, and as they naturally fit into the computation graph for the remaining TERPRET program, can be trained the same way.\nA simple TERPRET program counting bits on a tape, and a related NTPT program that counts up images of a particular class on a tape are displayed in Fig. 1."}, {"heading": "2.3 TASKS", "text": "To demonstrate the benefits of our approach for combining neural networks with program-like architecture, we consider three toy scenarios consisting of several related tasks depicted in Fig. 2.\nADD2X2 scenario: The first scenario in Fig. 2(a) uses of a 2\u00d7 2 grid of MNIST digits. We set 4 tasks based on this grid: compute the sum of the digits in the (1) top row, (2) left column, (3) bottom row, (4) right column. All tasks require classification of MNIST digits, but need different programs to compute the result. As training examples, we supply only a grid and the resulting sum. Thus, we never directly label an MNIST digit with its class.\nAPPLY2X2 scenario: The second scenario in Fig. 2(b) presents a 2 \u00d7 2 grid of of handwritten arithmetic operators. Providing three auxiliary random integers d1, d2, d3, we again set 4 tasks\nbased on this grid, namely to evaluate the expression1 d1 op1 d2 op2 d3 where (op1, op2) are the operators represented in the (1) top row, (2) left column, (3) bottom row, (4) right column. In comparison to the first scenario, the dataset of operators is relatively small and consistent2, making the perceptual task of classifying operators considerably easier. However, the algorithmic part is more difficult, requiring non-linear operations on the supplied integers.\nMATH scenario: The final task in Fig. 2(c) requires combination of the knowledge gained from the weakly labelled data in the first two scenarios to execute a handwritten arithmetic expression."}, {"heading": "3 MODELS", "text": "We design one NTPT model for each of the three scenarios outlined above. Knowledge transfer is achieved by defining a library of 2 neural networks shared across all tasks and scenarios. Training on each task should produce a task-specific source code solution (from scratch) and improve the overall usefulness of the shared networks. Below we outline the details of the specific models for each scenario along with baseline models."}, {"heading": "3.1 SHARED COMPONENTS", "text": "We refer to the 2 networks in the shared library as net 0 and net 1. Both networks have similar architectures: they take a 28 \u00d7 28 monochrome image as input and pass this sequentially through two fully connected layers each with 256 neurons and ReLU activations. The last hidden vector is passed through a fully connected layer and a softmax to produce a 10 dimensional output (net 0) or 4 dimensional output (net 1) to feed to the differentiable interpreter. Note that the output sizes are chosen to match the number of classes of MNIST digits and arithmetic operators respectively.\nIf we create an interpreter model containing N untrained networks, and part of the interpreter uses a parameter net choice = Param(N) to deciding which network to apply, then the system effectively sees one large network, which cannot usefully be split apart into the N components after training. To avoid this, we enforce that no more than one untrained network is introduced at a time (i.e. the first task has access to only net 0, and all other tasks have access to both nets). We find that this breaks the symmetry sufficiently to learn separate, useful classifiers."}, {"heading": "3.2 ADD2X2 MODEL", "text": "For the ADD2X2 scenario we build a model capable of writing short straight line algorithms with up to 4 instructions. The model consists of a read head containing net 0 and net 1 (with the exception of the very first task, which only has access to net 0, as discussed above) which are connected to a set of registers each capable of holding integers in the range 0, . . . ,M , where M = 18. The head is initialized reading the top left cell of the 2\u00d7 2 grid, and at each step in the program, one instruction can be executed from the following instruction set:\n\u2022 NOOP: a trivial no-operation instruction 1Note that for simplicity, our toy system ignores operator precedence and executes operations from left to right - i.e. the sequence in the text is executed as ((d1 op1 d2) op2 d3). 2200 handwritten examples of each operator were collected from a single author to produce a training set of 600 symbols and a test set of 200 symbols from which to construct random 2\u00d7 2 grids.\n\u2022 MOVE NORTH, MOVE EAST, MOVE SOUTH, MOVE WEST: translate the head (if possible) and return the result of applying the neural network chosen by net choice to the image in the new cell\n\u2022 ADD(\u00b7, \u00b7): accepts two register addresses and returns the sum of their contents.\nwhere the parameter net choice is to be learned and decides which of net 0 and net 1 to apply.\nTo construct each line of code requires choosing an instruction and (in the case of SUM) addresses of arguments for that instruction. We follow Feser et al. (2016) and allow each line to store its result in a separate immutable register. Finally, we learn a parameter specifying which register to return after execution of the program. An example program in this model is shown in Fig. 3(a). Even this simple model permits \u223c 107 syntactically distinct programs for the differentiable interpreter to search over.\nBaseline To compare with a purely neural approach, we construct a baseline in which the interpreter is replaced with a task specific neural network. This model (which resembles a multitask network) contains two shared networks with the same architecture as net 0 and net 1 which are are applied to the 4 digits to produce a 10 dimensional and a 4 dimensional embedding for each digit. We pass the concatenation of all embeddings to a task specific network consisting of a single ReLU hidden layer of 128 neurons. The task specific network outputs a classification of the sum into one of the 19 possible answers."}, {"heading": "3.3 APPLY2X2 MODEL", "text": "We adapt the ADD2X2 model to the APPLY2X2 scenario by initializing three immutable registers with the auxiliary integers supplied with each 2\u00d72 operator grid [see Fig. 2(b)]. In addition, we swap the ADD(\u00b7, \u00b7) instruction for APPLY(\u00b7, \u00b7, \u00b7). The action of APPLY(a, b, op) is to interpret the integer stored at op as an arithmetic operator and to compute a op b. All operations are performed modulo (M + 1) and division by zero returns M . In total, this model exposes a program space of size \u223c 1012 syntactically distinct programs.\nBaseline Additional task specific networks are added to the baseline from the ADD2X2 model. These networks concatenate embeddings from the shared networks with one-hot representations of the auxiliary integers before passing through 3 hidden layers of 128 neurons each3."}, {"heading": "3.4 MATH MODEL", "text": "We design the final scenario to investigate the synthesis of more complex control flow than straight line code. A natural solution to execute the expression on the tape is to build a loop with a body that alternates between moving the head and applying the operators [see Fig. 4(b)]. This loopy solution has the advantage that it generalises to handle arbitrary length arithmetic expressions.\n3Using a validation data set, we find that a deeper task-specific network is needed to achieve good performance in the APPLY2X2 tasks. Since the perceptual part of this task is very simple, we believe that the additional capacity is required to handle the nonlinear arithmetic operations.\nFig. 4(a) shows the basic architecture of the interpreter used in this scenario. We provide a set of blocks each containing the instruction MOVE or APPLY. A MOVE instruction increments the position of the head and loads the new symbol into a block specific immutable register using either net 0 or net 1 as determined by a block specific net choice. After executing the instruction, the interpreter executes a GOTO IF statement which checks whether the head is over the end of the tape and if not then it passes control to the block specified by goto addr, otherwise control passes to a halt block which returns a chosen register value and exits the program. This model describes a space of \u223c 106 syntactically distinct programs.\nBaseline To compare the generalisation properties of the proposed model with standard sequential neural architectures we add a recurrent neural network to the bank of task-specific networks in our baseline. At each step, this network takes in the shared embeddings of the current symbol, updates an LSTM hidden state and then proceeds to the next symbol. We make a classification of the final answer using the last hidden states of the LSTM. After tuning on a validation data set, we find that we achieve best performance with a 2 layer LSTM with 256 elements in each hidden state."}, {"heading": "4 EXPERIMENTS", "text": "In this section we report results from three sets of experiments: We first demonstrate that learning with weak supervision is possible in the proposed settings by examining a single task, we then show that NTPT shows significant advantages over the baseline model when learning on a sequence of related tasks. Finally we demonstrate the generalization properties of learned NTPT models afforded by the source code representation.\n4.1 WEAK SUPERVISION\nFig. 5 shows a comparison of the test accuracy of our weakly supervised NTPT model on the first ADD2X2 task (summing up the top row) with two alternative models: (1) a strongly supervised model (a classifier (identical to net 0) directly trained on labelled digits) and (2) the purely neural baseline described above. In each case we report the accuracy for the task on a held-out test set containing 10k examples, and we also extract just the classifier component from each model and measure accuracy directly on a digit classification task4.\nEach model is trained on 64k data instances drawn randomly from a data set containing either 4k distinct 2\u00d7 2 grid examples or 1k distinct examples. We draw two conclusions from these experiments: (1) When the data set is sufficiently varied (4k examples), both of the weakly supervised models are able to complete the task as well as a model with strong supervision, and (2) By restricting the data set size to 1k examples, we can arrange a regime whereby weak supervision does not produce an accurate result on a single task, meaning that we must pool knowledge from many tasks to succeed in any one task. We work in this regime in the next section."}, {"heading": "4.2 LIFELONG LEARNING", "text": "To test knowledge transfer between tasks we train on batches of data drawn from a probability distribution over all 8 tasks in the ADD2X2 and APPLY2X2 scenarios which evolves in time according to the schedule shown in the top left of Fig. 6(a). The identity of the task is provided with each batch such that the correct NTPT model or task-specific baseline network can be chosen. The learning rate for the shared networks and the task-specific components is tuned separately, and using an RMSProp\n4Note that there is no reason for the digit embeddings to emit interpretable class labels in the purely neural baseline, and we include these values only for completeness\noptimizer we find that for best knowledge transfer the learning rate of the shared networks should be 100 fold smaller than the task-specific components.\nFig. 6(a) shows the accuracy of NTPT and the baseline model on the first task of ADD2X2 (summing up the top row), for 10 random restarts. In all cases NTPT learns correct source code to solve the problem. Furthermore, we observe 3 key features:\n\u2022 NTPT shows reverse transfer: even when we have stopped presenting examples for the first task (such examples are indicated by the red bars), the performance on this task continues to increase. We verify that this is due to continuous improvement of net 0 by observing that the accuracy on the ADD2X2 task closely tracks measurements of the accuracy of net 0 directly on the digit classification task (the final accuracy of net 0 on the direct classification task is 93%).\n\u2022 With the chosen balance of learning rates NTPT does not display catastrophic forgetting. \u2022 NTPT considerably outperforms the baseline whose performance on ADD2X2 tasks starts\nto drop while training on later tasks.\nFig. 6(b) shows the performance on the remaining ADD2X2 tasks. We see that sharing of meaningful classifiers allows NTPT to learn solutions faster and with higher accuracy that the baseline. The results are similar for the APPLY2X2 scenario, with Fig. 6(c) showing that NTPT solves these tasks with \u223c 100% accuracy due to the simple nature of the operator data set, while the baseline struggles due to the complexity of the required program.\n4.3 GENERALIZATION\nIn the final experiment we take net 0/1 from the end of the experiment above and start training on the MATH scenario with arithmetic expressions containing 2 digits. The loopy structure of the MATH model introduces many local optima into the optimisation landscape and only 2/100 random restarts converge on a correct program. During a successful training run, the accuracy of net 0 on the digit classification task increases from 93% to 95%, and the accuracy of net 1 remains at 100%. Once the inferred source code is discretized, the model generalises well to longer expressions containingN digits, with the accuracy following the expected form 0.95N due to the repeated application of net 0. The LSTM baseline on the other hand shows excellent performance on N = 2, but does not generalize well (see Fig. 7)"}, {"heading": "5 RELATED WORK", "text": "Lifelong Machine Learning. We operate in the paradigm of Lifelong Machine Learning (LML) (Thrun, 1994; 1995; Thrun & O\u2019Sullivan, 1996; Silver et al., 2013; Chen et al., 2015), where a\nlearner is presented a sequence of different tasks and the aim is to retain and re-use knowledge from earlier tasks to more efficiently and effectively learn new tasks. This is distinct from related paradigms of multitask learning (presentation of a finite set of tasks simultenaously rather than in sequence (Caruana, 1997; Kumar & Daume III, 2012; Luong et al., 2015; Rusu et al., 2016)), transfer learning (transfer of knowledge from a source to target domain without notion of knowledge retention (Pan & Yang, 2010)), and curriculum learning (training a single model for a single task of varying difficulty (Bengio et al., 2009)).\nThe challenge for LML with neural networks is the problem of catastrophic forgetting: if the distribution of examples changes during training, then neural networks are prone to forget knowledge gathered from early examples. Solutions to this problem involve instantiating a knowledge repository (KR) either directly storing data from earlier tasks or storing (sub)networks trained on the earlier tasks with their weights frozen. This knowledge base allows either (1) rehearsal on historical examples (Robins, 1995), (2) rehearsal on virtual examples generated by the frozen networks (Silver & Mercer, 2002; Silver & Poirier, 2006) or (3) creation of new networks containing frozen sub networks from the historical tasks (Rusu et al., 2016; Shultz & Rivest, 2001)\nTo frame our approach in these terms, our KR contains partially-trained neural network classifiers which we call from learned source code. Crucially, we never freeze the weights of the networks in the KR: all parts of the KR can be updated during the training of all tasks - this allows us to improve performance on earlier tasks by continuing training on later tasks (so-called reverse transfer). Reverse transfer has been demonstrated previously in systems which assume that each task can be solved by a model parameterized by an (uninterpretable) task-specific linear combination of shared basis weights (Ruvolo & Eaton, 2013). The representation of task-specific knowledge as source code, learning from weak supervision, and shared knowledge as a deep neural networks distinguishes this work from the linear model used in Ruvolo & Eaton (2013).\nNeural Networks Learning Algorithms. Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components.\nWith the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers. In Reed & de Freitas (2016), this comes at the price of extremely strong supervision, where the learner is shown all intermediate steps to solving a problem; our learner only observes input and output examples. Reed & de Freitas (2016) also show the performance of their system in a multitask setting, however, in some cases additional tasks harm performance of the model and they freeze parts of their model when adding to their library of functions. Only Bunel et al. (2016), Riedel et al. (2016) and Gaunt et al. (2016) aim to consume and produce source code that can be provided by a human (e.g. as sketch of a solution) to or returned to a human (to potentially provide feedback)."}, {"heading": "6 DISCUSSION", "text": "We have presented NEURAL TERPRET, a framework for building end-to-end trainable models that structure their solution as a library of functions represented as source code or neural networks. Experimental results show that these models can successfully be trained in a lifelong learning context, and they are resistant to catastrophic forgetting; in fact, they show that even after instances of earlier tasks are no longer presented to the model, performance still continues to improve.\nLearning neural network models within differentiable interpreters has several benefits. First, learning programs imposes a bias that favors learning models that exhibit strong generalization, as illustrated by many works on program-like neural networks. Second, the source code components are interpretable by humans, allowing incorporation of domain knowledge describing the shape of the problem through the source code structure. Third, source code components can be inspected, and\nthe neural network components can be queried with specific instances to inspect whether the shared classifiers have learned the expected mappings. A final benefit is that the differentiable interpreter can be seen as focusing the supervision. If a component is un-needed for a given task, then the differentiable interpreter can choose not to use the component, which shuts off any gradients from flowing to the component. We speculate that this could be a reason for the models being resistant to catastrophic forgetting, as the model either chooses to use a classifier (which further trains it as on related tasks and leads to reverse transfer), or ignores it (which leaves the component unchanged).\nIt is known that differentiable interpreters are difficult to train (Kurach et al., 2015; Neelakantan et al., 2016; Gaunt et al., 2016), and being dependent on diffentiable interpreters is the primary limitation of this work. However, if progress can be made on more robust training of differentiable interpreters (perhaps extending ideas in Neelakantan et al. (2016); Feser et al. (2016)), then we believe there to be great promise in using the models we have presented here to build large lifelong neural networks."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Marcin Andrychowicz", "Karol Kurach"], "venue": "arXiv preprint arXiv:1602.03218,", "citeRegEx": "Andrychowicz and Kurach.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz and Kurach.", "year": 2016}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Lifelong learning for sentiment classification", "author": ["Zhiyuan Chen", "Nianzu Ma", "Bing Liu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Neural functional programming. 2016", "author": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "venue": null, "citeRegEx": "Feser et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Feser et al\\.", "year": 2017}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L. Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "CoRR, abs/1608.04428,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of the 28th Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Joulin and Mikolov.,? \\Q1989\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 1989}, {"title": "Neural GPUs learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations (ICLR),", "citeRegEx": "Kaiser and Sutskever.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2016}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daume III"], "venue": "arXiv preprint arXiv:1206.6417,", "citeRegEx": "Kumar and III.,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations 2016,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["Michael McCloskey", "Neal J Cohen"], "venue": "Psychology of learning and motivation,", "citeRegEx": "McCloskey and Cohen.,? \\Q1989\\E", "shortCiteRegEx": "McCloskey and Cohen.", "year": 1989}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Lei Jimmy Ba", "Ruslan Salakhutdinov"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Connectionist models of recognition memory: constraints imposed by learning and forgetting functions", "author": ["Roger Ratcliff"], "venue": "Psychological review,", "citeRegEx": "Ratcliff.,? \\Q1990\\E", "shortCiteRegEx": "Ratcliff.", "year": 1990}, {"title": "Programming with a differentiable forth interpreter", "author": ["Sebastian Riedel", "Matko Bosnjak", "Tim Rockt\u00e4schel"], "venue": "CoRR, abs/1605.06640,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Catastrophic forgetting, rehearsal and pseudorehearsal", "author": ["Anthony Robins"], "venue": "Connection Science,", "citeRegEx": "Robins.,? \\Q1995\\E", "shortCiteRegEx": "Robins.", "year": 1995}, {"title": "Ella: An efficient lifelong learning algorithm", "author": ["Paul Ruvolo", "Eric Eaton"], "venue": "ICML (1),", "citeRegEx": "Ruvolo and Eaton.,? \\Q2013\\E", "shortCiteRegEx": "Ruvolo and Eaton.", "year": 2013}, {"title": "Knowledge-based cascade-correlation: Using knowledge to speed learning", "author": ["Thomas R Shultz", "Francois Rivest"], "venue": "Connection Science,", "citeRegEx": "Shultz and Rivest.,? \\Q2001\\E", "shortCiteRegEx": "Shultz and Rivest.", "year": 2001}, {"title": "The task rehearsal method of life-long learning: Overcoming impoverished data", "author": ["Daniel L Silver", "Robert E Mercer"], "venue": "In Conference of the Canadian Society for Computational Studies of Intelligence,", "citeRegEx": "Silver and Mercer.,? \\Q2002\\E", "shortCiteRegEx": "Silver and Mercer.", "year": 2002}, {"title": "Machine life-long learning with csmtl networks", "author": ["Daniel L Silver", "Ryan Poirier"], "venue": "In AAAI,", "citeRegEx": "Silver and Poirier.,? \\Q2006\\E", "shortCiteRegEx": "Silver and Poirier.", "year": 2006}, {"title": "Lifelong machine learning systems: Beyond learning algorithms", "author": ["Daniel L Silver", "Qiang Yang", "Lianghao Li"], "venue": "In AAAI Spring Symposium: Lifelong Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2013}, {"title": "A lifelong learning perspective for mobile robot control", "author": ["Sebastian Thrun"], "venue": "In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Thrun.,? \\Q1994\\E", "shortCiteRegEx": "Thrun.", "year": 1994}, {"title": "Discovering structure in multiple learning tasks: The TC algorithm", "author": ["Sebastian Thrun", "Joseph O\u2019Sullivan"], "venue": "In Machine Learning, Proceedings of the Thirteenth International Conference (ICML),", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Zaremba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "However, despite some work showing promise towards multitask learning (training on many tasks at once) and transfer learning (using source tasks to improve learning in a later target task) (Caruana, 1997; Luong et al., 2015; Parisotto et al., 2015; Rusu et al., 2016), most successes of neural networks today come from training a single network on a single task, indicating that this goal is highly challenging to achieve.", "startOffset": 189, "endOffset": 267}, {"referenceID": 13, "context": "However, despite some work showing promise towards multitask learning (training on many tasks at once) and transfer learning (using source tasks to improve learning in a later target task) (Caruana, 1997; Luong et al., 2015; Parisotto et al., 2015; Rusu et al., 2016), most successes of neural networks today come from training a single network on a single task, indicating that this goal is highly challenging to achieve.", "startOffset": 189, "endOffset": 267}, {"referenceID": 17, "context": "However, despite some work showing promise towards multitask learning (training on many tasks at once) and transfer learning (using source tasks to improve learning in a later target task) (Caruana, 1997; Luong et al., 2015; Parisotto et al., 2015; Rusu et al., 2016), most successes of neural networks today come from training a single network on a single task, indicating that this goal is highly challenging to achieve.", "startOffset": 189, "endOffset": 267}, {"referenceID": 6, "context": "integers) from input-output examples (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016).", "startOffset": 37, "endOffset": 98}, {"referenceID": 19, "context": "integers) from input-output examples (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016).", "startOffset": 37, "endOffset": 98}, {"referenceID": 18, "context": "The approach is resilient to catastrophic forgetting (McCloskey & Cohen, 1989; Ratcliff, 1990); on the contrary, results show that performance continues to improve on earlier tasks even when only training on later tasks.", "startOffset": 53, "endOffset": 94}, {"referenceID": 6, "context": "We briefly review the TERPRET language (Gaunt et al., 2016) for constructing differentiable interpreters.", "startOffset": 39, "endOffset": 59}, {"referenceID": 6, "context": "More complex statements follow a similar pattern, with details given in Gaunt et al. (2016).", "startOffset": 72, "endOffset": 92}, {"referenceID": 0, "context": "This compilation process yields a TensorFlow Abadi et al. (2016) computation graph containing many of these two operations, which can then be trained using standard methods.", "startOffset": 45, "endOffset": 65}, {"referenceID": 5, "context": "We follow Feser et al. (2016) and allow each line to store its result in a separate immutable register.", "startOffset": 10, "endOffset": 30}, {"referenceID": 26, "context": "We operate in the paradigm of Lifelong Machine Learning (LML) (Thrun, 1994; 1995; Thrun & O\u2019Sullivan, 1996; Silver et al., 2013; Chen et al., 2015), where a", "startOffset": 62, "endOffset": 147}, {"referenceID": 25, "context": "We operate in the paradigm of Lifelong Machine Learning (LML) (Thrun, 1994; 1995; Thrun & O\u2019Sullivan, 1996; Silver et al., 2013; Chen et al., 2015), where a", "startOffset": 62, "endOffset": 147}, {"referenceID": 4, "context": "We operate in the paradigm of Lifelong Machine Learning (LML) (Thrun, 1994; 1995; Thrun & O\u2019Sullivan, 1996; Silver et al., 2013; Chen et al., 2015), where a", "startOffset": 62, "endOffset": 147}, {"referenceID": 3, "context": "This is distinct from related paradigms of multitask learning (presentation of a finite set of tasks simultenaously rather than in sequence (Caruana, 1997; Kumar & Daume III, 2012; Luong et al., 2015; Rusu et al., 2016)), transfer learning (transfer of knowledge from a source to target domain without notion of knowledge retention (Pan & Yang, 2010)), and curriculum learning (training a single model for a single task of varying difficulty (Bengio et al.", "startOffset": 140, "endOffset": 219}, {"referenceID": 13, "context": "This is distinct from related paradigms of multitask learning (presentation of a finite set of tasks simultenaously rather than in sequence (Caruana, 1997; Kumar & Daume III, 2012; Luong et al., 2015; Rusu et al., 2016)), transfer learning (transfer of knowledge from a source to target domain without notion of knowledge retention (Pan & Yang, 2010)), and curriculum learning (training a single model for a single task of varying difficulty (Bengio et al.", "startOffset": 140, "endOffset": 219}, {"referenceID": 2, "context": ", 2016)), transfer learning (transfer of knowledge from a source to target domain without notion of knowledge retention (Pan & Yang, 2010)), and curriculum learning (training a single model for a single task of varying difficulty (Bengio et al., 2009)).", "startOffset": 230, "endOffset": 251}, {"referenceID": 20, "context": "This knowledge base allows either (1) rehearsal on historical examples (Robins, 1995), (2) rehearsal on virtual examples generated by the frozen networks (Silver & Mercer, 2002; Silver & Poirier, 2006) or (3) creation of new networks containing frozen sub networks from the historical tasks (Rusu et al.", "startOffset": 71, "endOffset": 85}, {"referenceID": 2, "context": ", 2016)), transfer learning (transfer of knowledge from a source to target domain without notion of knowledge retention (Pan & Yang, 2010)), and curriculum learning (training a single model for a single task of varying difficulty (Bengio et al., 2009)). The challenge for LML with neural networks is the problem of catastrophic forgetting: if the distribution of examples changes during training, then neural networks are prone to forget knowledge gathered from early examples. Solutions to this problem involve instantiating a knowledge repository (KR) either directly storing data from earlier tasks or storing (sub)networks trained on the earlier tasks with their weights frozen. This knowledge base allows either (1) rehearsal on historical examples (Robins, 1995), (2) rehearsal on virtual examples generated by the frozen networks (Silver & Mercer, 2002; Silver & Poirier, 2006) or (3) creation of new networks containing frozen sub networks from the historical tasks (Rusu et al., 2016; Shultz & Rivest, 2001) To frame our approach in these terms, our KR contains partially-trained neural network classifiers which we call from learned source code. Crucially, we never freeze the weights of the networks in the KR: all parts of the KR can be updated during the training of all tasks - this allows us to improve performance on earlier tasks by continuing training on later tasks (so-called reverse transfer). Reverse transfer has been demonstrated previously in systems which assume that each task can be solved by a model parameterized by an (uninterpretable) task-specific linear combination of shared basis weights (Ruvolo & Eaton, 2013). The representation of task-specific knowledge as source code, learning from weak supervision, and shared knowledge as a deep neural networks distinguishes this work from the linear model used in Ruvolo & Eaton (2013).", "startOffset": 231, "endOffset": 1865}, {"referenceID": 8, "context": "Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).", "startOffset": 162, "endOffset": 480}, {"referenceID": 12, "context": "Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).", "startOffset": 162, "endOffset": 480}, {"referenceID": 28, "context": "Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).", "startOffset": 162, "endOffset": 480}, {"referenceID": 7, "context": "Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).", "startOffset": 162, "endOffset": 480}, {"referenceID": 19, "context": "Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).", "startOffset": 162, "endOffset": 480}, {"referenceID": 6, "context": "Recently, extensions of neural networks with primitives such as memory and discrete computation units have been studied to learn algorithms from inputoutput data (Graves et al., 2014; Weston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Bunel et al., 2016; Andrychowicz & Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016; Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).", "startOffset": 162, "endOffset": 480}, {"referenceID": 7, "context": "With the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers.", "startOffset": 51, "endOffset": 72}, {"referenceID": 5, "context": ", 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components. With the exception of Reed & de Freitas (2016) and (Graves et al.", "startOffset": 8, "endOffset": 359}, {"referenceID": 5, "context": ", 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components. With the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers. In Reed & de Freitas (2016), this comes at the price of extremely strong supervision, where the learner is shown all intermediate steps to solving a problem; our learner only observes input and output examples.", "startOffset": 8, "endOffset": 481}, {"referenceID": 5, "context": ", 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components. With the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers. In Reed & de Freitas (2016), this comes at the price of extremely strong supervision, where the learner is shown all intermediate steps to solving a problem; our learner only observes input and output examples. Reed & de Freitas (2016) also show the performance of their system in a multitask setting, however, in some cases additional tasks harm performance of the model and they freeze parts of their model when adding to their library of functions.", "startOffset": 8, "endOffset": 689}, {"referenceID": 5, "context": ", 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components. With the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers. In Reed & de Freitas (2016), this comes at the price of extremely strong supervision, where the learner is shown all intermediate steps to solving a problem; our learner only observes input and output examples. Reed & de Freitas (2016) also show the performance of their system in a multitask setting, however, in some cases additional tasks harm performance of the model and they freeze parts of their model when adding to their library of functions. Only Bunel et al. (2016), Riedel et al.", "startOffset": 8, "endOffset": 930}, {"referenceID": 5, "context": ", 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components. With the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers. In Reed & de Freitas (2016), this comes at the price of extremely strong supervision, where the learner is shown all intermediate steps to solving a problem; our learner only observes input and output examples. Reed & de Freitas (2016) also show the performance of their system in a multitask setting, however, in some cases additional tasks harm performance of the model and they freeze parts of their model when adding to their library of functions. Only Bunel et al. (2016), Riedel et al. (2016) and Gaunt et al.", "startOffset": 8, "endOffset": 952}, {"referenceID": 5, "context": ", 2016; Feser et al., 2016). Whereas many of these works use a neural network controller managing a differentiable computer architecture, we flip this relationship. The controller in our approach is a differentiable interpreter that is expressible as source code and can make calls to neural network components. With the exception of Reed & de Freitas (2016) and (Graves et al., 2016), the methods above operate on inputs that are (arrays of) integers. In Reed & de Freitas (2016), this comes at the price of extremely strong supervision, where the learner is shown all intermediate steps to solving a problem; our learner only observes input and output examples. Reed & de Freitas (2016) also show the performance of their system in a multitask setting, however, in some cases additional tasks harm performance of the model and they freeze parts of their model when adding to their library of functions. Only Bunel et al. (2016), Riedel et al. (2016) and Gaunt et al. (2016) aim to consume and produce source code that can be provided by a human (e.", "startOffset": 8, "endOffset": 976}], "year": 2017, "abstractText": "We introduce and develop solutions for the problem of Lifelong Perceptual Programming By Example (LPPBE). The problem is to induce a series of programs that require understanding perceptual data like images or text. LPPBE systems learn from weak supervision (input-output examples) and incrementally construct a shared library of components that grows and improves as more tasks are solved. Methodologically, we extend differentiable interpreters to operate on perceptual data and to share components across tasks. Empirically we show that this leads to a lifelong learning system that transfers knowledge to new tasks more effectively than baselines, and the performance on earlier tasks continues to improve even as the system learns on new, different tasks.", "creator": "LaTeX with hyperref package"}}}