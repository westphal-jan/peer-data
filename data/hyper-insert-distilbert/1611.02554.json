{"id": "1611.02554", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "The Neural Noisy Channel", "abstract": "hence we formulate computational sequence to sequence transduction cascade as a noisy channel decoding problem then and rapidly use recurrent neural control networks to best parameterise the respective source and channel operator models. thus unlike direct model models which models can suffer entirely from the explaining - away effects during training, noisy external channel models must actually produce meaningful outputs that properly explain correctly their particular inputs, procedures and their component models can be simultaneously trained with culturally not only certain paired training samples but also unpaired samples from the marginal mean output distribution. simultaneously using a timed latent variable to control how represents much of time the successful conditioning sequence the authentic channel model thus needs to read together in order to also generate a subsequent symbol, where we obtain a tractable and effective beam frequency search decoder. experimental evaluation results modelled on conditional abstractive sentence spectral summarisation, morphological inflection, etymology and cellular machine translation show positively that unreliable noisy conditioned channel models outperform direct models, and that they significantly benefit from increased amounts of correlated unpaired output data ensuring that authentic direct models clearly cannot be easily use.", "histories": [["v1", "Tue, 8 Nov 2016 15:18:44 GMT  (27kb)", "http://arxiv.org/abs/1611.02554v1", "ICLR 2017 submission"], ["v2", "Mon, 6 Mar 2017 12:37:12 GMT  (27kb)", "http://arxiv.org/abs/1611.02554v2", "ICLR 2017"]], "COMMENTS": "ICLR 2017 submission", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["lei yu", "phil blunsom", "chris dyer", "edward grefenstette", "tomas kocisky"], "accepted": true, "id": "1611.02554"}, "pdf": {"name": "1611.02554.pdf", "metadata": {"source": "CRF", "title": "THE NEURAL NOISY CHANNEL", "authors": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette"], "emails": ["lei.yu@cs.ox.ac.uk,", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "tkocisky@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n02 55\n4v 1\n[ cs\n.C L\n] 8\nN ov\n2 01\n6 Under review as a conference paper at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "Recurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input\u2013output (x,y) pairs are available for estimating their parameters. However, in many domains, vastly more unpaired output examples are available than input\u2013output pairs (e.g., transcribed speech is relatively rare although non-spoken texts are abundant; Swahili\u2013English translations are rare although English texts are abundant; etc.). A classic strategy for exploiting both kinds of data is to use Bayes\u2019 rule to rewrite p(y | x) as p(x | y)p(y)/p(x), a factorisation which is called a noisy channel model (Shannon, 1948). A noisy channel model thus consists of two component models: the conditional channel model, p(x | y), which characterizes the reverse transduction problem and whose parameters are estimated from the paired (x,y) samples, and the unconditional source model, p(y), whose parameters are estimated from both the paired and (usually much more numerous) unpaired samples.1\nBeyond their data omnivorousness, noisy channel models have other benefits. First, the two component models mean that two different aspects of the transduction problem can be addressed independently. For example, in many applications, source models are language models and innovations in these can be leveraged to obtain improvements in any system that uses them as a component. Second, the component models can have complementary strengths, since inference is carried out in the product space; this simplifies design because a single model does not have to get everything perfectly right. Third, the noisy channel operates by selecting outputs that both are a priori likely and that explain the input well. This addresses a failure mode that can occur in conditional models in which inputs are \u201cexplained away\u201d by highly predictive output prefixes, resulting in poor training (Klein & Manning, 2001). Since the noisy channel formulation requires its outputs to explain the observed input, this problem is avoided.\nIn principle, the noisy channel decomposition is straightforward; however, in practice, decoding (i.e., computing argmaxy p(x | y)p(y)) is a significant computational challenge, and tractability concerns impose restrictions on the form the component models can take. To illustrate, an appealing parameterization would be to use an attentional seq2seq network (Bahdanau et al., 2015) to model\n\u2217Work completed at DeepMind. 1We do not model p(x) since, in general, we will be interested in finding argmax\ny p(y | x), and\nargmax y p(y | x) = argmax y p(x|y)p(y) p(x) = argmax y p(x | y)p(y).\nthe channel probability p(x | y). However, seq2seq models are designed under the assumption that the complete conditioning sequence is available before any prefix probabilities of the output sequence can be computed. This assumption is problematic for channel models since it means that a complete output sequence must be constructed before the channel model can be evaluated (since the channel model conditions on the output). Therefore, to be practical, the channel probability must decompose in terms of prefixes of the conditioning variable, y. While the chain rule justifies decomposing output variable probabilities in terms of successive extensions of a partial prefix, no such convenience exists for conditioning variables, and approximations must be introduced.\nIn this work, we use a variant of the newly proposed online seq2seq model of Yu et al. (2016) which uses a latent alignment variable to enable its probabilities to factorize in terms of prefixes of both the input and output, making it an appropriate channel model (\u00a72). Using this channel model, the decoding problem then becomes similar to the problem faced when decoding with direct models (\u00a73). Experiments on abstractive summarization, machine translation, and morphological inflection show that the noisy channel can significantly improve performance and exploit unpaired output training samples and that models that combine the direct model and a noisy channel model offer further improvements still (\u00a74)."}, {"heading": "2 BACKGROUND: SEGMENT TO SEGMENT NEURAL TRANSDUCTION", "text": "Our model is based on the Segment to Segment Neural Transduction model (SSNT) of Yu et al., 2016. At a high level, the model alternates between encoding more of the input sequence and decoding output tokens from the encoded representation. This presentation deviates from the Yu et al.\u2019s presentation so as to emphasize the incremental construction of the conditioning context that is enabled by the latent variable."}, {"heading": "2.1 MODEL DESCRIPTION", "text": "Similar to other neural sequence to sequence models, SSNT models the conditional probability p(y | x) of a output sequence y given a input sequence x.\nTo avoid having to observe the complete input sequence x before making a prediction of the beginning of the output sequence, we introduce a latent alignment variable z which indicates when each token of the output sequence is to be generated as the input sequence is being read. Since we assume that the input is read just once from left to right, we restrict z to be a monotonically increasing alignment (i.e., zj+1 \u2265 zj is true with probability 1), where zj = i denotes that the output token at position j (yj) is generated when the input sequence up through position i is has been read. The SSNT model is:\np(y | x) = \u2211\nz\np(y, z | x)\np(y, z | x) \u2248\n|y| \u220f\nj=1\np(zj | zj\u22121,x zj 1 ,y j\u22121 1 ) \ufe38 \ufe37\ufe37 \ufe38\nalignment probability\np(yj | x zj 1 ,y j\u22121 1 ) \ufe38 \ufe37\ufe37 \ufe38\nword probability\n. (1)\nWe explain the model in terms of its two components, starting with the word generation term. In the SSNT, the input and output sequences x, y are encoded with two separate LSTMs (Hochreiter & Schmidhuber, 1997), resulting in sequences of hidden states representing prefixes of these sequences. In Yu et al.\u2019s formulation, the input sequence encoder (i.e., the conditioning context encoder) can either be a unidirectional or bidirectional LSTM, but here we assume that it is a unidirectional LSTM, which ensures that it will function well as a channel model that can compute probabilities with incomplete conditioning contexts (this is necessary since, at decoding time, we will be constructing the conditioning context incrementally). Let hi represent the input sequence encoding for the prefix xi1. Since the final action at timestep j will be to predict yj , it is convenient to let sj denote the hidden state that excludes yj , i.e., the encoding of the prefix y j\u22121 1 .\nThe probability of the next token yj is calculated by concatenating the aligned hidden state vectors sj and hzj followed by a softmax layer,\np(yj | x zj 1 ,y j\u22121 1 ) \u221d exp(Ww[hzj ; sj ] + bw).\nThe model thus depends on the current alignment position zj , which determines how far into x it has read.\nWe now discuss how the sequence of zj\u2019s are generated. First, we remark that modelling this distribution requires some care so as to avoid conditioning on the entire input sequence. To illustrate why one might induce a dependency on the entire input sequence in this model, it is useful to compare to a standard attention model. Attention models operate by computing a score using a representation of alignment candidate (in our case, the candidates would be every unread token remaining in the input). If we followed this strategy, it would be necessary to observe the full input sequence when making the first alignment decision.\nWe instead model the alignment transition from timestep j to j+1 by decomposing it into a sequence of conditionally independent SHIFT and EMIT operations that progressively decide whether to read another token or stop reading. That is, at input position i, the model decides to EMIT, i.e., to set zj = i and predict the next output token yj from the word model, or it decides to SHIFT, i.e., to read one more input token and increment the input position i \u2190 i + 1. The probability p(ai,j = EMIT | x i 1,y j\u22121 1 ) is calculated using the encoder and decoder states defined above as:\np(ai,j = EMIT | x i 1,y j\u22121 1 ) = \u03c3(MLP(Wt[hi; sj] + bt)).\nThe probability of SHIFT is simply 1 \u2212 p(ai,j = EMIT). In this formulation, the probabilities of aligning zj to each alignment candidate i can be computed by reading just xi1 (rather than the entire sequence). The probabilities are also independent of the contents of the suffix x|x|i+1.\nUsing the probabilities of the auxiliary ai,j variables, the alignment probabilities needed in Eq. 1 are computed as:\np(zj = i | zj\u22121,y j\u22121 1 ,x i 1) =\n \n\n0 if i < zj\u22121 p(ai,j = EMIT) if i = zj\u22121( \u220fi\u22121\ni\u2032=zj\u22121 p(ai\u2032,j = SHIFT)\n)\np(ai,j = EMIT) if i > zj\u22121"}, {"heading": "2.2 INFERENCE ALGORITHMS", "text": "In SSNT, the probability of generating each yj depends only on the current output position\u2019s alignment (zj), the current output prefix (y j\u22121 1 ), the input prefix up to the current alignment (x zj 1 ). It does not depend on the history of the alignment decisions. Likewise, the alignment decisions at each position are also conditionally independent of the history of alignment decisions. Because of these independence assumptions, z can be marginalised using a O(|x|2 \u00b7 |y|) time dynamic programming algorithm where each fill in a chart with computing the following marginal probabilities:\n\u03b1(i, j) = p(zj = i,y j 1 | x zj 1 ) =\ni\u2211\ni\u2032=1\n\u03b1(i\u2032, j \u2212 1) p(zj | zj\u22121,x zj 1 ,y j\u22121 1 )\n\ufe38 \ufe37\ufe37 \ufe38\nalignment probability\np(yj | x zj 1 ,y j\u22121 1 ) \ufe38 \ufe37\ufe37 \ufe38\nword probability\n.\nThe model is trained to minimize the negative log likelihood of the parallel corpus S:\nL(\u03b8) = \u2212 \u2211\n(x,y)\u2208S\nlog p(y | x; \u03b8)\n= \u2212 \u2211\n(x,y)\u2208S\nlog\u03b1(|x|, |y|). (2)\nThe gradients of this objective with respect to the component probability models can be computed using automatic differentiation or using a secondary dynamic program that computes \u2018backward\u2019 probabilities. We refer the reader to Section 3.1 of Yu et al. (2016) for details.\nIn this paper, we use a slightly different objective from the one described in Yu et al. (2016). Rather than marginalizing over the paths that end in any possible input positions\n\u2211I i=1 \u03b1(i, |y|), we require\nthat the full input be consumed when the final output symbol is generated. This constraint biases away from predicting outputs without explaining them using the input sequence."}, {"heading": "3 DECODING", "text": "We now turn to the problem of decoding, that is, of computing\ny\u0302 = argmax y p(x | y)p(y),\nwhere we are using the SSNT model described in the previous section as the channel model and a language model that delivers prior probabilities of the output sequence in left-to-right order, i.e., p(yi | y i\u22121).\nMarginalizing the latent variable during search is computationally hard (Sima\u2019an, 1996), and so we approximate the search problem as\ny\u0302 = argmax y max z p(x, z | y)p(y).\nHowever, even with this simplification, the search problem remains nontrivial. On one hand, we must search over the space of all possible outputs with a model that makes no Markovian assumptions. This is similar to the decoding problem faced in standard seq2seq transducers. On the other hand, our model computes the probability of the given input conditional on the predicted output hypothesis. Therefore, instead of just relying on a single softmax to provide a probability for every output word type (as we conveniently can in the direct model), we must loop over each output word type, and run a softmax over the input vocabulary\u2014a computational expense that is quadratic in the size of the vocabulary!\nTo reduce this computational effort, we make use of an auxiliary direct model q(y, z | x) to explore probable extensions of partial hypotheses, rather than trying to perform an exhaustive search over the vocabulary each time we extend an item on the beam.\nAlgorithm 1, in Appendix A, describes the decoding algorithm based on a formulation by Tillmann et al. (1997). The idea is to create a matrix Q of partial hypotheses. Each hypothesis in cell (i, j) covers the first i words of the input (xi1) and corresponds to an output hypothesis prefix of length j (yj1). The hypothesis is associated with a model score. For each cell (i, j), the direct proposal model first calculates the scores of possible extensions of previous cells that could then reach (i, j) by considering every token in the output vocabulary, from all previous candidate cells (i\u2212 1,\u2264 j). The top K1 partial output sequences. These partial output sequences are subsequently rescored by the noisy channel model, and the K2 best candidates are kept in the beam and used for further extension. The beam size K1 and K2 are hyperparameters to be tuned in the experiments."}, {"heading": "3.1 MODEL COMBINATION", "text": "The decoder we have just described makes use of an auxiliary decoding model. This means that, as a generalisation, it is capable of decoding under an objective that is a linear combination of the direct model, channel model, language model and a bias for the output length,\nO xi\n1 ,y\nj 1\n= \u03bb1 log p(y j 1 | x i 1 ) + \u03bb2 log p(x i 1 | y j 1) + \u03bb3 log p(y j 1) + \u03bb4|y j 1|. (3)\nThe bias is used to penalize the noisy channel model for generating too-short (or long) sequences. The \u03bb\u2019s are hyperparameters to be tuned using on a small amount of held-out development data."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate our model on three natural language processing tasks, abstractive sentence summarisation, machine translation and morphological inflection generation. For each task, we compare the performance of the direct model, noisy channel model, and the interpolation of the two models."}, {"heading": "4.1 ABSTRACTIVE SENTENCE SUMMARISATION", "text": "Sentence summarisation is the problem of constructing a shortened version of a sentence while preserving the majority of its meaning. In contrast to extractive summarisation, which can only copy words from the original sentence, abstractive summarisation permits arbitrary rewording of the sentence. The dataset (Rush et al., 2015) that we use is constructed by pairing the first sentence and\nthe headline of each article from the annotated Gigaword corpus (Graff et al., 2003; Napoles et al., 2012). There are 3.8m, 190k and 381k sentence pairs in the training, validation and test sets, respectively. Yu et al. (2016) filtered the dataset by restricting the lengths of the input and output sentences to be no greater than 50 and 25 tokens, respectively. From the filtered data, they further sampled 1 million sentence pairs for training. We experimented on training the direct model and channel model with both the sampled 1 million and the full 3.8 million parallel data. The language model is trained on the target side of the parallel data, i.e. the headlines. We evaluated the generated summaries of 2000 randomly sampled sentence pairs using full length ROUGE F1. This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; Gu\u0308lc\u0327ehre et al., 2016; Yu et al., 2016).\nThe same configuration is used to train the direct model and the channel model. The loss (Equation 2) is optimized by Adam (Kingma & Ba, 2015), with initial learning rate of 0.001. We use LSTMs with 1 layer for both the encoder and decoders, with hidden units of 256. The mini-batch size is 32, and dropout of 0.5 is applied to the input and output of LSTMs. For the language model, we use a 2-layer LSTM with 1024 hidden units and 0.5 dropout. The learning rate is 0.0001. All the hyperparameters are optimised via grid search on the perplexity of the validation set. During decoding, beam search is employed with the number of proposals generated by the direct model K1 = 20, and the number of best candidates selected by the noisy channel model K2 = 10.\nTable 1 presents the ROUGE-F1 scores of the test set from the direct model, noisy channel model (channel + LM + bias) and the interpolation of the direct model and the noisy channel model (direct + channel + LM + bias) trained on different sizes of data. The noisy channel model with the language model trained on the target side of the 1 million parallel data outperforms the direct model by approximately 1 point. Such improvement indicates that the language model helps improve the quality of the output sequence when no extra unlabelled data is available. Training the language model with all the headlines in the dataset, i.e. 3.8 million sentences, gives a further boost to the ROUGE score. This is in line with our expectation that the model benefits from adding large amounts of unlabelled data. The interpolation of the direct model, channel model, language model and bias of the output length achieves the best results \u2014 the ROUGE score is close to the direct model trained on all the parallel data. Although there is still improvement, when the direct model is trained with more data, the gap between the direct model and the noisy channel model is smaller.\nTable 2 surveys published results on this task, and places our best models in the context of the current state-of-the-art results. ABS+ (Rush et al., 2015), RAS-LSTM and RAS-Elman (Chopra et al., 2016) are different variations of the attentive models. Pointing the unkown words uses pointer networks (Vinyals et al., 2015) to select the output token from the input sequence in order to avoid generating unknown tokens. ASC + FSC (Miao & Blunsom, 2016) is a semi-supervised model based on a variational autoencoder. Trained on 1m paired samples and 3.8m unpaired samples, the\nnoisy channel achieves comparable or better results than (direct) models trained with 3.8m paired samples. Compared to Miao & Blunsom (2016), whose ASC + FSC models is an alternative strategy for using unpaired data, the noisy channel is significantly more effective \u2014 33.35 versus 31.09 in ROUGE-1.\nFinally, motivated by the qualitative observation that noisy channel model outputs were quite fluent and often used reformulations of the input rather than a strict compression (which would be poorly scored by ROUGE), we carried out a human preference evaluation whose results are summarised in Table 3. This confirms that noisy channel summaries are strongly preferred over those of the direct model."}, {"heading": "4.2 MACHINE TRANSLATION", "text": "We next evaluate our models on a Chinese\u2013English machine translation task. We used parallel data with 184k sentence pairs (from the FBIS corpus, LDC2003E14) and monolingual data with 4.3 million of English sentences (selected from the English Gigaword). The training data is preprocessed by lowercasing the English sentences, replacing digits with \u2018#\u2019 token, and replacing tokens appearing less than 5 times with an UNK token. This results in vocabulary sizes of 30k and 20k for Chinese sentences and English sentences, respectively.\nThe models are trained using Adam (Kingma & Ba, 2015) with initial learning rate of 0.001 for the direct model and the channel model, and 0.0001 for the language model. The LSTMs for the direct and channel models have 512 hidden units and 1 layer, and 2 layers with 1024 hidden units per layer for the language model. Dropout of 0.5 on the input and output of LSTMs is set for all the model training. The noisy channel decoding uses K1 = 20 and K2 = 10 as the beam sizes.\nTable 4 lists the translation performance of different models in BLEU scores. To set a benchmark, we train the attentional sequence to sequence model (Bahdanau et al., 2015) using the same parallel data. For direct models, we leverage bidirectional LSTMs as the encoder for this task. We can see that the noisy channel model is approximately 2 points higher in BLEU, and the combination of\nnosiy channel and direct model gives extra boost. This pattern is the same as the results we obtained from the sentence summarisation task."}, {"heading": "4.3 MORPHOLOGICAL INFLECTION GENERATION", "text": "Morphological inflection is the task of generating a target (inflected form) word from a source word (base form), given a morphological attribute, e.g. number, tense, and person etc.. It is useful for reducing data sparsity issues in translating morphologically rich languages. The transformation from the base form to the inflected form is usually to add prefix or suffix, or to do character replacement. The dataset (Durrett & DeNero, 2013) that we use in the experiments is created from Wiktionary, including inflections for German nouns, German verbs, Spanish Verbs, Finnish noun and adjective, and Finnish verbs. We only experimented on German nouns and German verbs, as German nouns is the most difficult task2, and the direct model does not perform as well as other state-of-theart systems on German verbs. The train/dev/test split for German nouns is 2364/200/200, and for German verbs is 1617/200/200. There are 8 and 27 inflection types in German nouns and German verbs, respectively. Following previous work, we learn a separate model for each type of inflection independent of the other inflections. We report results on the average accuracy across different inflections. Our language models were trained on word types extracted by running a morphological analysis tool on the WMT 2016 monolingual data and extracting examples of appropriately inflected word forms.3 After annotation the number of instances for training the language model ranged from 300k to 3.8m for different inflection types in German nouns, and from 200 to 54k in German verbs.\nThe experimental setup that we use on this task is K1 = 60, K2 = 30,\n\u2022 direct and channel model: 1 layer LSTM with 128 hidden, \u03b7 = 0.001, dropout = 0.5.\n\u2022 language model: 2 layer LSTM with 512 hidden, \u03b7 = 0.0001, dropout = 0.5.\nTable 1 summarises the results from our models. On both datasets, the noisy channel model (channel + LM + bias) does not perform as well as the direct model, but the interpolation of the direct model and noisy channel model (direct + channel + LM + bias) significantly outperforms the direct model. For further comparison, we also included the state-of-the-art results as benchmarks. NCK15 (Nicolai et al., 2015) tackles the task based on the three-stage approach: (1) align the source and target word, (2) extract inflection rules, (3) apply the rule to new examples. FTND16 (Faruqui et al., 2016) is based on neural sequence to sequence models. Both models (NCK15+ and FTND16+) rerank the candidate outputs by the scores predicted from n-gram language models, together with other features."}, {"heading": "5 ANALYSIS", "text": "By observing the output generated by the direct model and noisy channel model, we find (in line with theoretical critiques of conditional models) that the direct model may leave out key information. By contrast, the noisy channel model does seem to avoid this issue. To illustrate, in Example 1 (see Appendix B) in Table 5, the direct model ignores the key phrase \u2018coping with\u2019, resulting in incomplete meaning, but the noisy channel model covers it. Similarly, in Example 4, the direct model does not translate the Chinese word corresponding to \u2018investigation\u2019. We also observe that\n2While state-of-the-art systems can achieve 99% accuracies on Spanish verbs and Finnish verbs, they can only get 89% accuracy on German nouns.\n3http://www.statmt.org/wmt16/translation-task.html\nwhile the direct model mostly copies words from the source sentence, the noisy channel model prefers generating paraphrases. For instance, in Example 2, while the direct model copies the word \u2018accelerate\u2019 in the generated output, the noisy channel model generate \u2018speed up\u2019 instead. While one might argue that copying is a preferable compression technique than paraphrasing (as long as it produces grammatical outputs), it does show the power of these models."}, {"heading": "6 RELATED WORK", "text": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. Gu\u0308lc\u0327ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective.\nA number of papers have remarked on the tendency for content to get dropped (or repeated) in translation. Liu et al. (2016) propose translating in both a left-to-right and a left-to-right direction and seeking a consensus. Tu et al. (2016) propose augmenting a direct model\u2019s decoding objective with a reverse translation model (similar to our channel model except it conditions on the direct model\u2019s output RNN\u2019s hidden states rather than the words); however, that work just reranks complete translation hypotheses rather than developing a model that permits an incremental search.\nAnother trend of work that is related to our model is the investigation of making online prediction for machine translation (Gu et al., 2016; Grissom II et al., 2014; Sankaran et al., 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al., 2016)."}, {"heading": "7 CONCLUSION", "text": "We have presented and empirically validated a noisy channel transduction model that uses component models based on recurrent neural networks. This formulation lets us use unpaired outputs to estimate the parameters of the source model and input-output pairs to train the channel model. Despite the channel model\u2019s ability to condition on long sequences, we are able to maintain tractable decoding by using a latent segmentation variable that breaks the conditioning context up into a series of monotonically growing segments. Our experiments show that this model makes excellent use of unpaired training data."}, {"heading": "A ALGORITHM", "text": "Algorithm 1 Noisy Channel Decoding\nNotation: Q is the Viterbi matrix, bp is the backpointer, W stores the predicted tokens, V refers to the vocabulary, I = |x|, and Jmax denotes the maximum number of output tokens that can be predicted. Input: source sequence x Output: best output sequence y\u2217 Initialisation: Q \u2208 RI\u00d7Jmax\u00d7K1 , bp \u2208 NI\u00d7Jmax\u00d7K1 , W \u2208 NI\u00d7Jmax\u00d7K1 , Qtemp \u2208 R\nK1 , bptemp \u2208 NK1 , Wtemp \u2208 NK1 for i \u2208 [1, I] do\nQtemp \u2190 topk(K1)y\u2208Vq(z1 = i) q(y | START, z1,x z1 1 ) \u22b2 Candidates generated by q(y | x). bptemp \u2190 0 Wtemp \u2190 arg topk(K1)y\u2208Vq(z1 = i) q(y | START, z1,x z1 1 ) Q[i, 1]\u2190 topk(K2)y\u2208WtempOxi 1 ,y \u22b2 Rerank the candidates by objective (O). W [i, 1]\u2190 arg topk(K2)y\u2208WtempOxi 1 ,y\nend for for j \u2208 [2, Jmax] do\nfor i \u2208 [1, I] do Qtemp \u2190 topk(K1)y\u2208V,k\u2208[1,i]Q[k, j \u2212 1]\u00b7 q(zj = i | zj\u22121 = k)q(y | y j\u22121 1 , zj,x zj 1 )\nbptemp,Wtemp \u2190 arg topk(K1)y\u2208V,k\u2208[1,i] Q[k, j \u2212 1]q(zj = i | zj\u22121 = k)\u00b7\nq(y | yj\u221211 , zj,x)\nY \u2190 getCandidateOutputs(bptemp,Wtemp) \u22b2 Get partial candidate y j 1. Q[i, j]\u2190 topk(K2)yj\u2208Y Oxi 1 ,y j 1 bp[i, j],W [i, j]\u2190 arg topk(K2)yj 1 \u2208Y Oxi 1 ,y j 1 end for end for return a sequence of words stored in W by following backpointers starting from (I, argmaxj Q[I, j]).\nB EXAMPLE OUTPUTS\nSummarisation Example 1: source: the european commission on health and consumers protection \u2212lrb\u2212 unk \u2212rrb\u2212\nhas offered cooperation to indonesia in coping with the spread of avian influenza in the country , official news agency antara said wednesday .\nreference: eu offers indonesia cooperation in avian flu eradication direct: eu offers cooperation to indonesia in avian flu nc: eu offers cooperation to indonesia in coping with bird flu\nExample 2: source: vietnam will accelerate the export of industrial goods mainly by developing auxil-\niary industries , and helping enterprises sharpen competitive edges , according to the ministry of industry on thursday .\nreference: vietnam to boost industrial goods export direct: vietnam to accelerate export of industrial goods nc: vietnam to speed up export of industrial goods\nTranslation Example 3: source: \u6b27\u76df \u548c \u7f8e\u56fd \u90fd\u8868\u793a \u53ef\u4ee5 \u63a5\u53d7 \u8fd9 \u4e00\u59a5\u534f \u65b9\u6848 \u3002 reference: both the eu and the us indicated that they can accept this plan for a compromise . direct: the eu and the united states indicated that it can accept this compromise . nc: the european union and the united states have said that they can accept such a com-\npromise plan .\nExample 4: source: \u90a3\u4e48 \u8fd9\u4e9b \u8fd9\u4e2a \u65b9\u9762\u5462 \u662f \u73b0\u5728 \u8b66\u65b9 \u8c03\u67e5\u91cd\u70b9 \u3002 reference: well , this is the current focus of police investigation . direct: these are present at the current police . nc: then these are the key to the current police investigation .\nTable 5: Example outputs on the test set from the direct model and noisy channel model for the summarisation task and machine translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "An improved error model for noisy channel spelling correction", "author": ["Eric Brill", "Robert C. Moore"], "venue": "In Proc. ACL,", "citeRegEx": "Brill and Moore.,? \\Q2000\\E", "shortCiteRegEx": "Brill and Moore.", "year": 2000}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert. L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proc. ACL,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush"], "venue": "In Proc. NAACL,", "citeRegEx": "Chopra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero"], "venue": "In HLT-NAACL,", "citeRegEx": "Durrett and DeNero.,? \\Q2013\\E", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "A noisy-channel approach to question answering", "author": ["Abdessamad Echihabi", "Daniel Marcu"], "venue": "In Proc. ACL,", "citeRegEx": "Echihabi and Marcu.,? \\Q2003\\E", "shortCiteRegEx": "Echihabi and Marcu.", "year": 2003}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "venue": "In HLT-NAACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Don\u2019t until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["Alvin Grissom II", "Jordan Boyd-Graber", "He He", "John Morgan", "Hal Daum\u00e9 III"], "venue": "In Empirical Methods in Natural Language Processing,", "citeRegEx": "II et al\\.,? \\Q2014\\E", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Learning to translate in real-time with neural machine", "author": ["Jiatao Gu", "Graham Neubig", "Kyunghyun Cho", "Victor O.K. Li"], "venue": "translation. CoRR,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Proc. ICASSP,", "citeRegEx": "Hwang and Sung.,? \\Q2016\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2016}, {"title": "A neural transducer", "author": ["Navdeep Jaitly", "David Sussillo", "Quoc V Le", "Oriol Vinyals", "Ilya Sutskever", "Samy Bengio"], "venue": null, "citeRegEx": "Jaitly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaitly et al\\.", "year": 2016}, {"title": "Statistical Methods for Speech Recognition", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q1998\\E", "shortCiteRegEx": "Jelinek.", "year": 1998}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proc. ICIR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Conditional structure versus conditional estimation in nlp models", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Klein and Manning.,? \\Q2001\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2001}, {"title": "Agreement on target-bidirectional neural machine translation", "author": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "venue": "In Proc. NAACL,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "author": ["Yishu Miao", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Miao and Blunsom.,? \\Q2016\\E", "shortCiteRegEx": "Miao and Blunsom.", "year": 2016}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak"], "venue": "In HLT-NAACL,", "citeRegEx": "Nicolai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proc. EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Incremental decoding for phrase-based statistical machine translation", "author": ["Baskaran Sankaran", "Ajeet Grewal", "Anoop Sarkar"], "venue": "In Proc. WMT,", "citeRegEx": "Sankaran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sankaran et al\\.", "year": 2010}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proc. ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A mathematical theory of communication", "author": ["Claude Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon.,? \\Q1948\\E", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Computational complexity of probabilistic disambiguation by means of treegrammars", "author": ["Khalil Sima\u2019an"], "venue": "In Proc. COLING,", "citeRegEx": "Sima.an.,? \\Q1996\\E", "shortCiteRegEx": "Sima.an.", "year": 1996}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A DP-based search using monotone alignments in statistical translation", "author": ["Christoph Tillmann", "Stephan Vogel", "Hermann Ney", "Alex Zubiaga"], "venue": "In Proc. EACL,", "citeRegEx": "Tillmann et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tillmann et al\\.", "year": 1997}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Online segment to segment neural transduction", "author": ["Lei Yu", "Jan Buys", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "Recurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input\u2013output (x,y) pairs are available for estimating their parameters.", "startOffset": 53, "endOffset": 130}, {"referenceID": 0, "context": "Recurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input\u2013output (x,y) pairs are available for estimating their parameters.", "startOffset": 53, "endOffset": 130}, {"referenceID": 26, "context": "A classic strategy for exploiting both kinds of data is to use Bayes\u2019 rule to rewrite p(y | x) as p(x | y)p(y)/p(x), a factorisation which is called a noisy channel model (Shannon, 1948).", "startOffset": 171, "endOffset": 186}, {"referenceID": 0, "context": "To illustrate, an appealing parameterization would be to use an attentional seq2seq network (Bahdanau et al., 2015) to model Work completed at DeepMind.", "startOffset": 92, "endOffset": 115}, {"referenceID": 31, "context": "In this work, we use a variant of the newly proposed online seq2seq model of Yu et al. (2016) which uses a latent alignment variable to enable its probabilities to factorize in terms of prefixes of both the input and output, making it an appropriate channel model (\u00a72).", "startOffset": 77, "endOffset": 94}, {"referenceID": 31, "context": "1 of Yu et al. (2016) for details.", "startOffset": 5, "endOffset": 22}, {"referenceID": 31, "context": "1 of Yu et al. (2016) for details. In this paper, we use a slightly different objective from the one described in Yu et al. (2016). Rather than marginalizing over the paths that end in any possible input positions \u2211I i=1 \u03b1(i, |y|), we require that the full input be consumed when the final output symbol is generated.", "startOffset": 5, "endOffset": 131}, {"referenceID": 27, "context": "Marginalizing the latent variable during search is computationally hard (Sima\u2019an, 1996), and so we approximate the search problem as", "startOffset": 72, "endOffset": 87}, {"referenceID": 29, "context": "Algorithm 1, in Appendix A, describes the decoding algorithm based on a formulation by Tillmann et al. (1997). The idea is to create a matrix Q of partial hypotheses.", "startOffset": 87, "endOffset": 110}, {"referenceID": 23, "context": "The dataset (Rush et al., 2015) that we use is constructed by pairing the first sentence and", "startOffset": 12, "endOffset": 31}, {"referenceID": 21, "context": "the headline of each article from the annotated Gigaword corpus (Graff et al., 2003; Napoles et al., 2012).", "startOffset": 64, "endOffset": 106}, {"referenceID": 23, "context": "This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Yu et al., 2016).", "startOffset": 58, "endOffset": 138}, {"referenceID": 4, "context": "This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Yu et al., 2016).", "startOffset": 58, "endOffset": 138}, {"referenceID": 31, "context": "This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Yu et al., 2016).", "startOffset": 58, "endOffset": 138}, {"referenceID": 23, "context": "ABS+ (Rush et al., 2015), RAS-LSTM and RAS-Elman (Chopra et al.", "startOffset": 5, "endOffset": 24}, {"referenceID": 4, "context": ", 2015), RAS-LSTM and RAS-Elman (Chopra et al., 2016) are different variations of the attentive models.", "startOffset": 32, "endOffset": 53}, {"referenceID": 18, "context": ", 2003; Napoles et al., 2012). There are 3.8m, 190k and 381k sentence pairs in the training, validation and test sets, respectively. Yu et al. (2016) filtered the dataset by restricting the lengths of the input and output sentences to be no greater than 50 and 25 tokens, respectively.", "startOffset": 8, "endOffset": 150}, {"referenceID": 23, "context": "ABS+ (Rush et al., 2015) is the attentive model with bag-of-words as the encoder.", "startOffset": 5, "endOffset": 24}, {"referenceID": 4, "context": "RAS-LSTM and RAS-Elman (Chopra et al., 2016) are the sequence to sequence models with attention with the RNN cell implemented as LSTMs and an Elman architecture (Elman, 1990), respectively.", "startOffset": 23, "endOffset": 44}, {"referenceID": 7, "context": ", 2016) are the sequence to sequence models with attention with the RNN cell implemented as LSTMs and an Elman architecture (Elman, 1990), respectively.", "startOffset": 124, "endOffset": 137}, {"referenceID": 0, "context": "To set a benchmark, we train the attentional sequence to sequence model (Bahdanau et al., 2015) using the same parallel data.", "startOffset": 72, "endOffset": 95}, {"referenceID": 22, "context": "NCK15 (Nicolai et al., 2015) tackles the task based on the three-stage approach: (1) align the source and target word, (2) extract inflection rules, (3) apply the rule to new examples.", "startOffset": 6, "endOffset": 28}, {"referenceID": 8, "context": "FTND16 (Faruqui et al., 2016) is based on neural sequence to sequence models.", "startOffset": 7, "endOffset": 29}, {"referenceID": 22, "context": "NCK15 (Nicolai et al., 2015) and FTND16 (Faruqui et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 8, "context": ", 2015) and FTND16 (Faruqui et al., 2016) are previous state-of-the-art on this task, with NCK15 based on feature engineering, and FTND16 based on neural networks.", "startOffset": 19, "endOffset": 41}, {"referenceID": 15, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al.", "startOffset": 112, "endOffset": 127}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003).", "startOffset": 149, "endOffset": 169}, {"referenceID": 10, "context": "Another trend of work that is related to our model is the investigation of making online prediction for machine translation (Gu et al., 2016; Grissom II et al., 2014; Sankaran et al., 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al.", "startOffset": 124, "endOffset": 189}, {"referenceID": 24, "context": "Another trend of work that is related to our model is the investigation of making online prediction for machine translation (Gu et al., 2016; Grissom II et al., 2014; Sankaran et al., 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al.", "startOffset": 124, "endOffset": 189}, {"referenceID": 14, "context": ", 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al., 2016).", "startOffset": 31, "endOffset": 72}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model.", "startOffset": 150, "endOffset": 400}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data.", "startOffset": 150, "endOffset": 940}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective.", "startOffset": 150, "endOffset": 1254}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective. A number of papers have remarked on the tendency for content to get dropped (or repeated) in translation. Liu et al. (2016) propose translating in both a left-to-right and a left-to-right direction and seeking a consensus.", "startOffset": 150, "endOffset": 1478}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective. A number of papers have remarked on the tendency for content to get dropped (or repeated) in translation. Liu et al. (2016) propose translating in both a left-to-right and a left-to-right direction and seeking a consensus. Tu et al. (2016) propose augmenting a direct model\u2019s decoding objective with a reverse translation model (similar to our channel model except it conditions on the direct model\u2019s output RNN\u2019s hidden states rather than the words); however, that work just reranks complete translation hypotheses rather than developing a model that permits an incremental search.", "startOffset": 150, "endOffset": 1594}], "year": 2016, "abstractText": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.", "creator": "LaTeX with hyperref package"}}}