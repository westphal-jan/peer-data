{"id": "1611.02588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Contradiction Detection for Rumorous Claims", "abstract": "the utilization of social media material in monitoring journalistic narratives workflows posts is increasing, while demanding modern automated methods for the identification of mis - use and disinformation. since contemporary textual contradiction across social media accounts posts can be a signal of literary rumorousness, we seek to model how relevant claims stored in us twitter online posts are being textually negatively contradicted. finally we uniquely identify two different linguistic contexts in computing which manuscript contradiction emerges : sometimes its broader form can be observed across independently posted consensus tweets narrative and its essentially more recent specific form engaging in threaded conversations. we fully define how the only two scenarios sufficiently differ in formal terms of central geographic elements layers of argumentation : claims mapping and separate conversation structure. we directly design narratives and therefore evaluate models for the two scenarios known uniformly as specific 3 - way patterns recognizing textual entailment tasks in order to manually represent claims and derive conversation conversation structure implicitly in attempting a coherent generic inference model, alternatively while previous studies used explicit features or no representation details of merely these relationship properties. to address effectively noisy blog text, our classifiers use overly simple similarity mapping features derived from between the string estimates and part - quality of - time speech level. concurrent corpus statistics reveal incomplete distribution differences solely for these differences features terminating in contradictory judgments as opposed to non - contradictory tweet string relations, and the classifiers cannot yield state of the art performance.", "histories": [["v1", "Tue, 8 Nov 2016 16:19:17 GMT  (410kb,D)", "https://arxiv.org/abs/1611.02588v1", "to appear in: Proc. Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics, Osaka, Japan, 2016"], ["v2", "Fri, 11 Nov 2016 10:57:07 GMT  (517kb,D)", "http://arxiv.org/abs/1611.02588v2", "To appear in: Proceedings of Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics, Osaka, Japan, 2016"]], "COMMENTS": "to appear in: Proc. Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics, Osaka, Japan, 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["piroska lendvai", "uwe d reichel"], "accepted": false, "id": "1611.02588"}, "pdf": {"name": "1611.02588.pdf", "metadata": {"source": "CRF", "title": "Contradiction Detection for Rumorous Claims", "authors": ["Piroska Lendvai", "Uwe D. Reichel"], "emails": ["piroska.r@gmail.com", "uwe.reichel@nytud.mta.hu"], "sections": [{"heading": "1 Introduction and Task Definition", "text": "Assigning a veracity judgment to a claim appearing on social media requires complex procedures including reasoning on claims aggregated from multiple microposts, to establish claim veracity status (resolved or not) and veracity value (true or false). Until resolution, a claim circulating on social media platforms is regarded as a rumor (Mendoza et al., 2010). The detection of contradicting and disagreeing microposts supplies important cues to claim veracity processing procedures. These tasks are challenging to automatize not only due to the surface noisiness and conciseness of user generated content. One complicating factor is that claim denial or rejection is linguistically often not explicitly expressed, but appears without classical rejection markers or modality and speculation cues (Morante and Sporleder, 2012). Explicit and implicit contradictions furthermore arise in different contexts: in threaded discussions, but also across independently posted messages; both contexts are exemplified in Figure 1 on Twitter data.\nLanguage technology has not yet solved the processing of contradiction-powering phenomena, such as negation (Morante and Blanco, 2012) and stance detection (Mohammad et al., 2016), where stance is defined to express speaker favorability towards an evaluation target, usually an entity or concept. In the veracity computation scenario we can speak of claim targets that are above the entity level:targets are entire rumors, such as \u201911 people died during the Charlie Hebdo attack\u2019. Contradiction and stance detection have so far only marginally been addressed in the veracity context (de Marneffe et al., 2012; Ferreira and Vlachos, 2016; Lukasik et al., 2016).\nWe propose investigating the advantages of incorporating claim target and conversation context as premises in the Recognizing Textual Entailment (RTE) framework for contradiction detection in rumorous tweets. Our goals are manifold: (a) to offer richer context in contradiction modeling than what would be available on the level of individual tweets, the typical unit of analysis in previous studies; (b) to train and test supervised classifiers for contradiction detection in the RTE inference framework; (c) to address contradiction detection at the level of text similarity only, as opposed to semantic similarity (Xu et al., 2015); (d) to distinguish and focus on two different contradiction relationship types, each involving specific combinations of claim target mention, polarity, and contextual proximity, in particular:\nar X\niv :1\n61 1.\n02 58\n8v 2\n[ cs\n.C L\n] 1\n1 N\nov 2\n01 6\n1. Independent contradictions: Contradictory relation between independent posts, in which two tweets contain different information about the same claim target that cannot simultaneously hold. The two messages are independently posted, i.e., not occurring within a structured conversation. 2. Disagreeing replies: Contradictory relation between a claim-originating tweet and a direct reply to it, whereby the reply expresses disagreement with respect to the claim-introducing tweet.\nContradiction between independently posted tweets typically arises in a broad discourse setting, and may feature larger distance in terms of time, space, and source of information. The claim target is mentioned in both posts in the contradiction pair, since these posts are uninformed about each other or assume uninformedness of the reader, and thus do not or can not make coreference to their shared claim target. Due to the same reason, the polarity of both posts with respect to the claim can be identical. Texts paired in this type of contradiction resemble those of the recent Interpretable Semantic Similarity shared task (Agirre et al., 2016) that calls to identify five chunk level semantic relation types (equivalence, opposition, specificity, similarity or relatedness) between two texts that originate from headlines or captions. Disagreeing replies are more specific instances of contradiction: contextual proximity is small and trivially identifiable by means of e.g. social media platform metadata, for example the property encoding the tweet ID to which the reply was sent, which in our setup is always a thread-initiating tweet. The claim target is by definition assumed to be contained in the thread-initiating tweet (sometimes termed as claimor rumor-source tweet). It can be the case that the claim target is not contained in the reply, which can be explained by the proximity and thus shared context of the two posts. The polarity values in source and reply must by definition be different; we refer to this scenario as Disagreeing replies. Importantly, replies may not contain a (counter-)claim on their own but some other form to express disagreement and polarity \u2013 for example in terms of speculative language use, or the presence of extra-linguistic cues such as a URL pointing to an online article that holds contradictory content. Such cues are difficult to decode for a machine, and their representation for training automatic classifiers is largely unexplored. Note that we do not make assumptions or restrictions about how the claim target is encoded textually in any of the two scenarios.\nIn this study, we tackle both contradiction types using a single generic approach: we recast them as three-way RTE tasks on pairs of tweets. The findings of our previous study in which semantic inference systems with sophisticated, corpus-based or manually created syntactico-semantic features were applied to contradiction-labeled data indicate the lack of robust syntactic and semantic analysis for short and noisy texts; cf. Chapter 3 in (Lendvai et al., 2016b). This motivates our current simple text similarity metrics in search of alternative methods for the contradiction processing task.\nIn Section 2 we introduce related work and resources, in Sections 3 and 4 present and motivate the collections and the features used for modeling. After the description of method and scores in Section 5, findings are discussed in Section 6."}, {"heading": "2 Related work and resources", "text": "Recognizing Textual Entailment (RTE) Processing semantic inference phenomena such as contradiction, entailment and stance between text pairs has been gaining momentum in language technology. Inference has been suggested to be conveniently formalized in the generic framework of RTE1 (Dagan et al., 2006). As an improvement over the binary Entailment vs Non-entailment scenario, three-way RTE has appeared but is still scarcely investigated (Ferreira and Vlachos, 2016; Lendvai et al., 2016a). The Entailment relation between two text snippets holds if the claim present in snippet B can be concluded from snippet A. The Contradiction relation applies when the claim in A and the claim in B cannot be simultaneously true. The Unknown relation applies if A and B neither entail nor contradict each other.\nThe RTE-3 benchmark dataset is the first resource that labels paired text snippets in terms of 3-way RTE judgments (De Marneffe et al., 2008), but it is comprised of general newswire texts. Similarly, the new large annotated corpus used for deep models for entailment (Bowman et al., 2015) labeled text pairs as Contradiction are too broadly defined, i.e., expressing generic semantic incoherence rather than semantically motivated polarization and mismatch that we are after, which questions its utility in the rumor verification context.\nAs far as contradiction processing is concerned, accounting for negation in RTE is the focus of a recent study (Madhumita, 2016), but it is still set according to the binary RTE setup. A standalone contradiction detection system was implemented by (De Marneffe et al., 2008), using complex rulebased features. A specific RTE application, the Excitement Open Platform2 (Pado\u0301 et al., 2015) has been developed to provide a generic platform for applied RTE. It integrates several entailment decision algorithms, while only the Maximum Entropy-based model (Wang and Neumann, 2007) is available for 3-way RTE classification. This model implements state-of-the-art linguistic preprocessing augmented with lexical resources (WordNet, VerbOcean), and uses the output of part-of-speech and dependency parsing in its structure-oriented, overlap-based approach for classification and was tested for both our tasks as explained in (Lendvai et al., 2016b).\nStance detection Stance classification and stance-labeled corpora are relevant for contradiction detection, because the relationship of two texts expressing opposite stance (positive and negative) can in some contexts be judged to be contradictory: this is exactly what our Disagreeing reply scenario covers. Stance classification for rumors was introduced by (Qazvinian et al., 2011) where the goal was to generate a binary (for or against) stance judgment. Stance is typically classified on the level of individual tweets: reported approaches predominantly utilize statistical models, involving supervised machine learning (de Marneffe et al., 2012) and RTE (Ferreira and Vlachos, 2016). Another relevant aspect of stance detection for our current study is the presence of the stance target in the text to be stance-labeled. A recent shared task on social media data defined separate challenges depending on whether target-specific training data is included in the task or not (Mohammad et al., 2016); the latter requires additional effort to encode information about the stance target, cf. e.g. (Augenstein et al., 2016). The PHEME project released a new stance-labeled social media dataset (Zubiaga et al., 2015) that we also utilize as described next."}, {"heading": "3 Data", "text": "The two datasets corresponding to our two tasks are drawn from a freely available, annotated social media corpus3 that was collected from the Twitter platform4 via filtering on event-related keywords and hashtags in the Twitter Streaming API. We worked with English tweets related to four events: the Ottawa shooting5, the Sydney Siege6, the Germanwings crash7, and the Charlie Hebdo shooting8. Each event in\n1http://www.aclweb.org/aclwiki/index.php?title=Recognizing Textual Entailment 2http://hltfbk.github.io/Excitement-Open-Platform 3https://figshare.com/articles/PHEME rumour scheme dataset journalism use case/2068650 4twitter.com 5https://en.wikipedia.org/wiki/2014 shootings at Parliament Hill, Ottawa 6https://en.wikipedia.org/wiki/2014 Sydney hostage crisis 7https://en.wikipedia.org/wiki/Germanwings Flight 9525 8https://en.wikipedia.org/wiki/Charlie Hebdo shooting\nthe corpus was pre-annotated as explained in (Zubiaga et al., 2015) for several rumorous claims9 \u2013 officially not yet confirmed statements lexicalized by a concise proposition, e.g. \u201dFour cartoonists were killed in the Charlie Hebdo attack\u201d and \u201dFrench media outlets to be placed under police protection\u201d. The corpus collection method was based on a retweet threshold, therefore most tweets originate from authoritative sources using relatively well-formed language, whereas replying tweets often feature non-standard language use.\nTweets are organized into threaded conversations in the corpus and are marked up with respect to stance, certainty, evidentiality, and other veracity-related properties; for full details on released data we refer to (Zubiaga et al., 2015). The dataset on which we run disagreeing reply detection (henceforth: Threads) was converted by us to RTE format based on the threaded conversations labeled in this corpus. We created the Threads RTE dataset drawing on manually pre-assigned Response Type labels by (Zubiaga et al., 2015) that were meant to characterize source tweet \u2013 replying tweet relations in terms of four categories. We mapped these four categories onto three RTE labels: a reply pre-labeled as Agreed with respect to its source tweet was mapped to Entailment, a reply pre-labeled as Disagreed was mapped to Contradiction, while replies pre-labeled as AppealforMoreInfo and Comment were mapped to Unknown. Only direct replies to source tweets relating to the same four events as in the independent posts RTE dataset were kept. There are 1,850 tweet pairs in this set; the proportion of contradiction instances amounts to 7%. The Threads dataset holds CON, ENT and UNK pairs as exemplified below. Conform the RTE format, pair elements are termed text and hypothesis \u2013 note that directionality between t and h is assumed as symmetric in our current context so t and h are assigned based on token-level length. \u2022 CON <t>We understand there are two gunmen and up to a dozen hostages inside the cafe under siege at\nSydney.. ISIS flags remain on display 7News</t> <h>not ISIS flags</h> \u2022 ENT <t>Report: Co-Pilot Locked Out Of Cockpit Before Fatal Plane Crash URL Germanwings URL</t> <h>This sounds like pilot suicide.</h> \u2022 UNK <t>BREAKING NEWS: At least 3 shots fired at Ottawa War Memorial. One soldier confirmed shot -\nURL URL</t> <h>All our domestic military should be armed, now.</h>. The independently posted tweets dataset (henceforth: iPosts) that we used for contradiction detection between independently emerging claim-initiating tweets is described in (Lendvai et al., 2016a). This collection is holds 5.4k RTE pairs generated from about 500 English tweets using semi-automatic 3-way RTE labeling, based on semantic or numeric mismatches between the rumorous claims annotated in the data. The proportion of contradictory pairs (CON) amounts to 25%. The two collections are quantified in Table 1. iPosts dataset examples are given below. \u2022 CON<t>12 people now known to have died after gunmen stormed the Paris HQ of magazine CharlieHebdo\nURL URL</t> <h>Awful. 11 shot dead in an assault on a Paris magazine. URL CharlieHebdo URL</h> \u2022 ENT <t>SYDNEY ATTACK - Hostages at Sydney cafe - Up to 20 hostages - Up to 2 gunmen - Hostages\nseen holding ISIS flag DEVELOPING..</t> <h>Up to 20 held hostage in Sydney Lindt Cafe siege URL URL</h> \u2022 UNK <t>BREAKING: NSW police have confirmed the siege in Sydney\u2019s CBD is now over, a police officer\nis reportedly among the several injured.</t> <h>Update: Airspace over Sydney has been shut down. Live coverage: URL sydneysiege</h>.\n9Rumor, rumorous claim and claim are used interchangeably throughout the paper to refer to the same concept."}, {"heading": "4 Text similarity features", "text": "Data preprocessing on both datasets included screen name and hashtag sign removal and URL masking. Then, for each tweet pair we extracted vocabulary overlap and local text alignment features. The tweets were part-of-speech-tagged using the Balloon toolkit (Reichel, 2012) (PENN tagset, (Marcus et al., 1999)), normalized to lowercase and stemmed using an adapted version of the Porter stemmer (Porter, 1980). Content words were defined to belong to the set of nouns, verbs, adjectives, adverbs, and numbers, and were identified by their part of speech labels. All punctuation was removed."}, {"heading": "4.1 Vocabulary overlap", "text": "Vocabulary overlap was calculated for content word stem types in terms of the Cosine similarity and the F1 score. The Cosine similarity of two tweets is defined as C(X,Y ) = |X\u2229Y |\u221a\n|X|\u00b7|Y | , where X and Y denote\nthe sets of content word stems in the tweet pair. The F1 score is defined as the harmonic mean of precision and recall. Precision and recall here refer to covering the vocabulary X of one tweet by the vocabulary Y of another tweet (or vice versa). It is given by F1 = 2 \u00b7 |X\u2229Y | |X| \u00b7 |X\u2229Y | |Y |\n|X\u2229Y | |X| + |X\u2229Y | |Y |\n. Again the vocabularies X and Y consist of stemmed content words.\nJust like the Cosine index, the F1 score is a symmetric similarity metric. These two metrics are additionally applied to the content word POS label inventories within the tweet pair, which gives the four features cosine, cosine pos, f score, and f score pos, respectively."}, {"heading": "4.2 Local alignment", "text": "The amount of stemmed word token overlap was measured by applying local alignment of the token sequences using the Smith-Waterman algorithm (Smith and Waterman, 1981). We chose a score function rewarding zero substitutions by +1, and punishing insertions, deletions, and substitutions each by 0-reset. Having filled in the score matrix H , alignment was iteratively applied the following way:\nwhile max(H) \u2265 t \u2013 trace back from the cell containing this maximum the path leading to it until a zero-cell is reached \u2013 add the substring collected on this way to the set of aligned substrings \u2013 set all traversed cells to 0.\nThe threshold t defines the required minimum length of aligned substrings. It is set to 1 in this study, thus it supports a complete alignment of any pair of permutations of x. The traversed cells are set to 0 after each iteration step to prevent that one substring would be related to more than one alignment pair. This approach would allow for two restrictions: to prevent cross alignment not just the traversed cells [i, j] but for each of these cells its entire row i and column j needs to be set to 0. Second, if only the longest common substring is of interest, then the iteration is trivially to be stopped after the first step. Since we did not make use of these restrictions, in our case the alignment supports cross-dependencies and can be regarded as an iterative application of a longest common substring match.\nFrom the substring pairs in tweets x and y aligned this way, we extracted two text similarity measures: \u2022 laProp: the proportion of locally aligned tokens over both tweets m(x)+m(y)n(x)+n(y) \u2022 laPropS: the proportion of aligned tokens in the shorter tweet m(z\u0302)n(z\u0302) , z\u0302 = argminz\u2208{x,y}[n(z)], where n(z) denotes the number of all tokens and m(z) the number of aligned tokens in tweet z."}, {"heading": "4.3 Corpus statistics", "text": "Figures 2 and 3 show the distribution of the features introduced above each for a selected event in both datasets. Each figure half represents a dataset; each subplot shows the distribution of a feature in dependence of the three RTE classes for the selected event in that dataset.\nThe plots indicate a general trend over all events and datasets: the similarity features reach highest values for the ENT class, followed by CON and UNK. Kruskal-Wallis tests applied separately for all combinations of features, events and datasets confirmed these trends, revealing significant differences for all boxplot triplets (p < 0.001 after correction for type 1 errors in this high amount of comparisons using\nthe false discovery rate method of (Benjamini and Yekutieli, 2001)). Dunnett post hoc tests however clarified that for 16 out of 72 comparisons (all POS similarity measures) only UNK but not ENT and CON differ significantly (\u03b1 = 0.05). Both datasets contain the same amount of non-significant cases. Nevertheless, these trends are encouraging to test whether an RTE task can be addressed by string and POS-level similarity features alone, without syntactic or semantic level tweet comparison."}, {"heading": "5 RTE classification experiments for Contradiction and Disagreeing Reply detection", "text": "In order to predict the RTE classes based on the features introduced above, we trained two classifiers: Nearest (shrunken) centroids (NC) (Tibshirani et al., 2003) and Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002), using the R wrapper package Caret (Kuhn, 2016) with the methods pam and rf, respectively. To derive the same number of instances for all classes, we applied separately for both datasets resampling without replacement, so that the total data amounts about 4,550 feature vectors equally distributed over the three classes, the majority of 4,130 belonging to the iPosts data set. Further, we centered and scaled the feature matrix. Within the Caret framework we optimized the tunable parameters of both classifiers by maximizing the F1 score. This way the NC shrinkage delta was set to 0, which means that the class reference centroids are not modified. For RF the number of variables randomly sampled as candidates at each split was set to 2. The remaining parameters were kept default.\nThe classifiers were tested on both datasets in a 4-fold event-based held-out setting, training on three events and testing on the remaining one (4-fold cross-validation, CV), quantifying how performance generalizes to new events with unseen claims and unseen targets. The CV scores are summarized in Tables 2 and 3. It turns out generally that classifying CON is more difficult than classifying ENT or UNK. We observe a dependency of the classifier performances on the two contradiction scenarios: for detecting CON, RF achieved higher classification values on Threads, whereas NC performed better on iPosts. General performance across all three classes was better in independent posts than in conversational threads.\nDefinitions of contradiction, the genre of texts and the features used are dependent on end applications, making performance comparison nontrivial (Lendvai et al., 2016b). On a different subset of the Threads data in terms of events, size of evidence, 4 stance classes and no resampling, (Lukasik et al., 2016) report .40 overall F-score using Gaussian processes, cosine similarity on text vector representation and temporal metadata. Our previous experiments were done using the Excitement Open Platform incorporating syntactico-semantic processing and 4-fold CV. For the non-resampled Threads data we reported .11 F1 on CON via training on iPosts (Lendvai et al., 2016b). On the non-resampled iPosts data we obtained .51 overall F1 score (Lendvai et al., 2016a), F1 on CON being .25 (Lendvai et al., 2016b).\nWe proposed to model two types of contradictions: in the first both tweets encode the claim target (iPosts), in the second typically only one of them (Threads). The Nearest Centroid algorithm performs poorly on the CON class in Threads where textual overlap is typically small especially for the CON and UNK classes, in part due to the absence of the claim target in replies. However, the Random Forest algorithm\u2019s performance is not affected by this factor. The advantage of RF on the Threads data can be explained by its property of training several weak classifiers on parts of the feature vectors only. By this boosting strategy a usually undesirable combination of relatively long feature vectors but few training observations can be tackled, holding for the Threads data that due to its extreme skewedness (cf. Table 1) shrunk down to only 420 datapoints after our class balancing technique of resampling without replacement. Results indicate the benefit of RF classifiers in such sparse data cases.\nThe good performance of NC on the much larger amount of data in iPosts is in line with the corpus statistics reported in section 4.3, implying a reasonably small amount of class overlap. The classes are thus relatively well represented by their centroids, which is exploited by the NC classifier. However, as illustrated in Figures 2 and 3, the majority of feature distributions are generally better separated for ENT and UNK, while CON in its mid position shows more overlap to both other classes and is thus overall a less distinct category."}, {"heading": "6 Conclusions and Future Work", "text": "The detection of contradiction and disagreement in microposts supplies important cues to factuality and veracity assessment, and is a central task in computational journalism. We developed classifiers in a uniform, general inference framework that differentiates two tasks based on contextual proximity of the two posts to be assessed, and if the claim target may or may not be omitted in their content. We utilized simple text similarity metrics that proved to be a good basis for contradiction classification.\nText similarity was measured in terms of vocabulary and token sequence overlap. To derive the latter, local alignment turned out to be a valuable tool: as opposed to standard global alignment (Wagner and Fischer, 1974), it can account for crossing dependencies and thus for varying sequential order of information structure in entailing text pairs, e.g. in \u201dthe cat chased the mouse\u201d and \u201dthe mouse was chased by the cat\u201d, which are differently structured into topic and comment (Halliday, 1967). We expect contradictory content to exhibit similar trends in variation with respect to content unit order \u2013 especially in the Threads scenario, where entailment inferred from a reply can become the topic of a subsequent replying tweet. Since local alignment can resolve such word order differences, it is able to preserve text similarity of entailing tweet pairs, which is reflected in the relative laProp boxplot heights in Figures 2 and 3.\nWe have run leave-one-event-out evaluation separately on the independent posts data and on the conversational threads data, which allowed us to compare performances on collections originating from the same genre and platform, but on content where claim targets in the test data are different from the targets in the training data. Our obtained generalization performance over unseen events turns out to be in line with previous reports. Via downsampling, we achieved a balanced performance on both tasks across the three RTE classes; however, in line with previous work, even in this setup the overall performance on contradiction is the lowest, whereas detecting the lack of contradiction can be achieved with much better performance in both contradiction scenarios.\nPossible extensions to our approach include incorporating more informed text similarity metrics (Ba\u0308r et al., 2012), formatting phenomena (Tolosi et al., 2016), and distributed contextual representations (Le and Mikolov, 2014), the utilization of knowledge-intensive resources (Pado\u0301 et al., 2015), representation of alignment on various content levels (Noh et al., 2015), and formalization of contradiction scenarios in terms of additional layers of perspective (van Son et al., 2016)."}, {"heading": "7 Acknowledgments", "text": "P. Lendvai was supported by the PHEME FP7 project (grant nr. 611233), U. D. Reichel by an Alexander von Humboldt Society grant. We thank anonymous reviewers for their input."}], "references": [{"title": "Semeval-2016 task 2: Interpretable semantic textual similarity", "author": ["Agirre et al.2016] Eneko Agirre", "Aitor Gonzalez-Agirre", "Inigo Lopez-Gazpio", "Montse Maritxalar", "German Rigau", "Larraitz Uria"], "venue": "Proceedings of SemEval,", "citeRegEx": "Agirre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "USFD: Any-Target Stance Detection on Twitter with Autoencoders", "author": ["Andreas Vlachos", "Kalina Bontcheva"], "venue": "Proceedings of the International Workshop on Semantic Evaluation,", "citeRegEx": "Augenstein et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Augenstein et al\\.", "year": 2016}, {"title": "UKP: Computing semantic textual similarity by combining multiple content similarity measures", "author": ["B\u00e4r et al.2012] Daniel B\u00e4r", "Chris Biemann", "Iryna Gurevych", "Torsten Zesch"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "B\u00e4r et al\\.,? \\Q2012\\E", "shortCiteRegEx": "B\u00e4r et al\\.", "year": 2012}, {"title": "The control of the false discovery rate in multiple testing under dependency", "author": ["Benjamini", "Yekutieli2001] Yoav Benjamini", "Daniel Yekutieli"], "venue": "Annals of Statistics,", "citeRegEx": "Benjamini et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Benjamini et al\\.", "year": 2001}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment", "author": ["Dagan et al.2006] Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Finding contradictions in text", "author": ["Anna N Rafferty", "Christopher D Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Did it happen? the pragmatic complexity of veridicality assessment", "author": ["Christopher D Manning", "Christopher Potts"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2012}, {"title": "Emergent: a novel data-set for stance classification", "author": ["Ferreira", "Vlachos2016] William Ferreira", "Andreas Vlachos"], "venue": "In Proceedings of NAACL", "citeRegEx": "Ferreira et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ferreira et al\\.", "year": 2016}, {"title": "Notes on transitivity and theme in English, part II", "author": [], "venue": "Journal of Linguistics,", "citeRegEx": "Halliday.,? \\Q1967\\E", "shortCiteRegEx": "Halliday.", "year": 1967}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "2016a. Monolingual social media datasets for detecting contradiction and entailment", "author": ["Isabelle Augenstein", "Kalina Bontcheva", "Thierry Declerck"], "venue": "In Proc. of LREC-2016", "citeRegEx": "Lendvai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2016}, {"title": "2016b. Algorithms for Detecting Disputed Information. Deliverable D4.2.2 for FP7-ICT Collaborative Project ICT-2013-611233 PHEME. https://www.pheme.eu/wp-content/uploads/2016/ 06/D422_final.pdf", "author": ["Isabelle Augenstein", "Dominic Rout", "Kalina Bontcheva", "Thierry Declerck"], "venue": null, "citeRegEx": "Lendvai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2016}, {"title": "Classification and regression by randomForest", "author": ["Liaw", "Wiener2002] Andy Liaw", "Matthew Wiener"], "venue": "R News,", "citeRegEx": "Liaw et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Liaw et al\\.", "year": 2002}, {"title": "2016. Hawkes Processes for Continuous Time Sequence Classification: An Application to Rumour Stance Classification in Twitter", "author": ["P.K. Srijith", "Duy Vu", "Kalina Bontcheva", "Arkaitz Zubiaga", "Trevor Cohn"], "venue": "Proceedings of ACL-16", "citeRegEx": "Lukasik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lukasik et al\\.", "year": 2016}, {"title": "Twitter Under Crisis: Can We Trust What We RT", "author": ["Barbara Poblete", "Carlos Castillo"], "venue": "In Proceedings of the First Workshop on Social Media Analytics", "citeRegEx": "Mendoza et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mendoza et al\\.", "year": 2010}, {"title": "SemEval-2016 Task 6: Detecting stance in tweets", "author": ["Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry"], "venue": "In Proceedings of the International Workshop on Semantic Evaluation,", "citeRegEx": "Mohammad et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "SEM 2012 shared task: Resolving the scope and focus of negation", "author": ["Morante", "Blanco2012] Roser Morante", "Eduardo Blanco"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics", "citeRegEx": "Morante et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Morante et al\\.", "year": 2012}, {"title": "Multi-level alignments as an extensible representation basis for textual entailment algorithms", "author": ["Noh et al.2015] Tae-Gil Noh", "Sebastian Pad\u00f3", "Vered Shwartz", "Ido Dagan", "Vivi Nastase", "Kathrin Eichler", "Lili Kotlerman", "Meni Adler"], "venue": "Lexical and Computational Semantics", "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Design and Realization of a Modular Architecture for Textual Entailment", "author": ["Pad\u00f3 et al.2015] Sebastian Pad\u00f3", "Tae-Gil Noh", "Asher Stern", "Rui Wang", "Roberto Zanoli"], "venue": "Natural Language Engineering,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2015}, {"title": "An algorithm for suffix stripping", "author": ["Martin F. Porter"], "venue": null, "citeRegEx": "Porter.,? \\Q1980\\E", "shortCiteRegEx": "Porter.", "year": 1980}, {"title": "Rumor has it: Identifying misinformation in microblogs", "author": ["Emily Rosengren", "Dragomir R. Radev", "Qiaozhu Mei"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Qazvinian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "PermA and Balloon: Tools for string alignment and text processing", "author": ["Uwe D. Reichel"], "venue": "In Proc. Interspeech, page paper no. 346,", "citeRegEx": "Reichel.,? \\Q2012\\E", "shortCiteRegEx": "Reichel.", "year": 2012}, {"title": "Identification of common molecular subsequences", "author": ["Smith", "Waterman1981] Temple F. Smith", "Michael S. Waterman"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Smith et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Smith et al\\.", "year": 1981}, {"title": "Class prediction by nearest shrunken centroids,with applications to DNA microarrays", "author": ["Trevor Hastie", "Balasubramanian Narasimhan", "Gilbert Chu"], "venue": null, "citeRegEx": "Tibshirani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2003}, {"title": "An analysis of event-agnostic features for rumour classification in twitter", "author": ["Tolosi et al.2016] Laura Tolosi", "Andrey Tagarev", "Georgi Georgiev"], "venue": "In Proc. of Social Media in the Newsroom Workshop", "citeRegEx": "Tolosi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tolosi et al\\.", "year": 2016}, {"title": "GRaSP: A Multilayered Annotation Scheme for Perspectives", "author": ["Tommaso Caselli", "Antske Fokkens", "Isa Maks", "Roser Morante", "Lora Aroyo", "Piek Vossen"], "venue": "In Proceedings of the 10th Edition of the Language Resources and Evaluation Conference (LREC)", "citeRegEx": "Son et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Son et al\\.", "year": 2016}, {"title": "The string-to-string correction problem", "author": ["Wagner", "Fischer1974] Robert A. Wagner", "Michael J. Fischer"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Wagner et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 1974}, {"title": "Recognizing textual entailment using a subsequence kernel method", "author": ["Wang", "Neumann2007] Rui Wang", "G\u00fcnter Neumann"], "venue": "In AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT)", "author": ["Xu et al.2015] Wei Xu", "Chris Callison-Burch", "William B Dolan"], "venue": "Proceedings of SemEval", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Towards Detecting Rumours in Social Media", "author": ["Maria Liakata", "Rob Procter", "Kalina Bontcheva", "Peter Tolmie"], "venue": null, "citeRegEx": "Zubiaga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Until resolution, a claim circulating on social media platforms is regarded as a rumor (Mendoza et al., 2010).", "startOffset": 87, "endOffset": 109}, {"referenceID": 16, "context": "Language technology has not yet solved the processing of contradiction-powering phenomena, such as negation (Morante and Blanco, 2012) and stance detection (Mohammad et al., 2016), where stance is defined to express speaker favorability towards an evaluation target, usually an entity or concept.", "startOffset": 156, "endOffset": 179}, {"referenceID": 14, "context": "Contradiction and stance detection have so far only marginally been addressed in the veracity context (de Marneffe et al., 2012; Ferreira and Vlachos, 2016; Lukasik et al., 2016).", "startOffset": 102, "endOffset": 178}, {"referenceID": 29, "context": "Our goals are manifold: (a) to offer richer context in contradiction modeling than what would be available on the level of individual tweets, the typical unit of analysis in previous studies; (b) to train and test supervised classifiers for contradiction detection in the RTE inference framework; (c) to address contradiction detection at the level of text similarity only, as opposed to semantic similarity (Xu et al., 2015); (d) to distinguish and focus on two different contradiction relationship types, each involving specific combinations of claim target mention, polarity, and contextual proximity, in particular: ar X iv :1 61 1.", "startOffset": 408, "endOffset": 425}, {"referenceID": 0, "context": "Texts paired in this type of contradiction resemble those of the recent Interpretable Semantic Similarity shared task (Agirre et al., 2016) that calls to identify five chunk level semantic relation types (equivalence, opposition, specificity, similarity or relatedness) between two texts that originate from headlines or captions.", "startOffset": 118, "endOffset": 139}, {"referenceID": 5, "context": "Inference has been suggested to be conveniently formalized in the generic framework of RTE1 (Dagan et al., 2006).", "startOffset": 92, "endOffset": 112}, {"referenceID": 4, "context": "Similarly, the new large annotated corpus used for deep models for entailment (Bowman et al., 2015) labeled text pairs as Contradiction are too broadly defined, i.", "startOffset": 78, "endOffset": 99}, {"referenceID": 19, "context": "A specific RTE application, the Excitement Open Platform2 (Pad\u00f3 et al., 2015) has been developed to provide a generic platform for applied RTE.", "startOffset": 58, "endOffset": 77}, {"referenceID": 21, "context": "Stance classification for rumors was introduced by (Qazvinian et al., 2011) where the goal was to generate a binary (for or against) stance judgment.", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "A recent shared task on social media data defined separate challenges depending on whether target-specific training data is included in the task or not (Mohammad et al., 2016); the latter requires additional effort to encode information about the stance target, cf.", "startOffset": 152, "endOffset": 175}, {"referenceID": 1, "context": "(Augenstein et al., 2016).", "startOffset": 0, "endOffset": 25}, {"referenceID": 30, "context": "The PHEME project released a new stance-labeled social media dataset (Zubiaga et al., 2015) that we also utilize as described next.", "startOffset": 69, "endOffset": 91}, {"referenceID": 30, "context": "the corpus was pre-annotated as explained in (Zubiaga et al., 2015) for several rumorous claims9 \u2013 officially not yet confirmed statements lexicalized by a concise proposition, e.", "startOffset": 45, "endOffset": 67}, {"referenceID": 30, "context": "Tweets are organized into threaded conversations in the corpus and are marked up with respect to stance, certainty, evidentiality, and other veracity-related properties; for full details on released data we refer to (Zubiaga et al., 2015).", "startOffset": 216, "endOffset": 238}, {"referenceID": 30, "context": "We created the Threads RTE dataset drawing on manually pre-assigned Response Type labels by (Zubiaga et al., 2015) that were meant to characterize source tweet \u2013 replying tweet relations in terms of four categories.", "startOffset": 92, "endOffset": 114}, {"referenceID": 22, "context": "The tweets were part-of-speech-tagged using the Balloon toolkit (Reichel, 2012) (PENN tagset, (Marcus et al.", "startOffset": 64, "endOffset": 79}, {"referenceID": 20, "context": ", 1999)), normalized to lowercase and stemmed using an adapted version of the Porter stemmer (Porter, 1980).", "startOffset": 93, "endOffset": 107}, {"referenceID": 24, "context": "In order to predict the RTE classes based on the features introduced above, we trained two classifiers: Nearest (shrunken) centroids (NC) (Tibshirani et al., 2003) and Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002), using the R wrapper package Caret (Kuhn, 2016) with the methods pam and rf, respectively.", "startOffset": 138, "endOffset": 163}, {"referenceID": 14, "context": "On a different subset of the Threads data in terms of events, size of evidence, 4 stance classes and no resampling, (Lukasik et al., 2016) report .", "startOffset": 116, "endOffset": 138}, {"referenceID": 9, "context": "in \u201dthe cat chased the mouse\u201d and \u201dthe mouse was chased by the cat\u201d, which are differently structured into topic and comment (Halliday, 1967).", "startOffset": 125, "endOffset": 141}, {"referenceID": 2, "context": "Possible extensions to our approach include incorporating more informed text similarity metrics (B\u00e4r et al., 2012), formatting phenomena (Tolosi et al.", "startOffset": 96, "endOffset": 114}, {"referenceID": 25, "context": ", 2012), formatting phenomena (Tolosi et al., 2016), and distributed contextual representations (Le and Mikolov, 2014), the utilization of knowledge-intensive resources (Pad\u00f3 et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 19, "context": ", 2016), and distributed contextual representations (Le and Mikolov, 2014), the utilization of knowledge-intensive resources (Pad\u00f3 et al., 2015), representation of alignment on various content levels (Noh et al.", "startOffset": 125, "endOffset": 144}, {"referenceID": 18, "context": ", 2015), representation of alignment on various content levels (Noh et al., 2015), and formalization of contradiction scenarios in terms of additional layers of perspective (van Son et al.", "startOffset": 63, "endOffset": 81}], "year": 2016, "abstractText": "The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of misand disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance.", "creator": "LaTeX with hyperref package"}}}