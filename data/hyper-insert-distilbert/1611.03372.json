{"id": "1611.03372", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "A stochastically verifiable autonomous control architecture with reasoning", "abstract": "a yet new unique agent architecture called limited instruction skill set agent ( lisa ) is introduced for autonomous control. perhaps the latest new implemented architecture, is based on previous batch implementations of classic agentspeak databases and it is structurally simpler than continuing its predecessors envisioned with meeting the aim of mainly facilitating standardized design - time and run - level time verification methods. the simpler process of slowly abstracting the system lisa system to isolate two different types of discrete probabilistic models ( dtmc and layered mdp ) is actively investigated furthermore and illustrated. using the lisa - system paradigm provides a complementary tool for complete evaluation modelling of evaluate the agent behaviors and the environment for simple probabilistic verification. calculating the agent program analysis can be either automatically randomly compiled into a dtmc standard or represents a simple mdp model for adaptive verification only with prism. the automatically generated prism model can however be used for testing both design - time and independent run - - time verification. the complex run - time verification is investigated and then illustrated in depicting the lisa system as using an internal application modelling analysis mechanism for prediction of future successful outcomes.", "histories": [["v1", "Thu, 10 Nov 2016 16:06:31 GMT  (22kb,D)", "http://arxiv.org/abs/1611.03372v1", "Accepted at IEEE Conf. Decision and Control, 2016"]], "COMMENTS": "Accepted at IEEE Conf. Decision and Control, 2016", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.SE cs.SY", "authors": ["paolo izzo", "hongyang qu", "sandor m veres"], "accepted": false, "id": "1611.03372"}, "pdf": {"name": "1611.03372.pdf", "metadata": {"source": "CRF", "title": "A stochastically verifiable autonomous control architecture with reasoning", "authors": ["Paolo Izzo", "Hongyang Qu"], "emails": ["s.veres}@sheffield.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Autonomous control is an area within control sciences that emerged by upgrading classical feedback control by decision making on what control references to use. The purpose of feedback control is to regulate a system in order to make it follow a predefined reference input. Autonomous controllers are designed to make decisions what reference signal to use and, more generally, what goals to achieve and how to achieve them. They do so by generating and executing plans of action that work toward goals [1, 2]. Autonomous controllers aim to introduce a certain level of \u201cintelligence\u201d in control systems, that is the ability of a system to act appropriately in an uncertain environment [3].\nar X\niv :1\n61 1.\n03 37\n2v 1\n[ cs\n.R O\n] 1\nA first attempt towards autonomous decision-making software was initially made by using Object Oriented Programming (OOP). However the passive nature of objects in OOP, led to the development of active objects called \u201cagents\u201d [4], which implement decision-making processes. A formal description of autonomous agents can be found in [4\u20136].\nOne of the most widely used \u201canthropomorphic\u201d approaches to the implementation of autonomous agents is the Belief-Desire-Intention (BDI) architecture [4,7]. BDI agent architectures are characterised by three large sets of atomic predicates: Beliefs, Desires and Intentions. The most known implementations of the BDI architecture are the Procedural Reasoning System (PRS) [8, 9] and AgentSpeak [10]. AgentSpeak fully embraces the philosophy of Agent Oriented Programming (AOP) [11], and it offers a customisable Java based interpreter.\nAutonomous agents have a considerable potential for implementation in all sorts of different applications. However their introduction in real-world scenarios brings along safety concerns, creating the need for model checking [12]. An early attempt to BDI agent verification can be found in [13,14], where the authors present a translation software from AgentSpeak to either Promela or Java, and then use the associated model checkers Spin [15, 16] and Java PathFinder (JPF) [17]. A subsequent effort towards verifiable agents was made by Dennis et al. [18] with a BDI agent programming language called Gwendolen, which is implemented in the Agent Infrastructure Layer (AIL) [19, 20], a collection of Java classes intended for use in model checking agent programs, particularly with JPF. An evolution of JPF is Agent Java PathFinder (AJPF) [21], specifically designed to verify agent programs. However JPF and AJPF introduce a significant bottleneck in the workflow as the internal generation of the program model, which is created by executing all possible paths, is highly computationally expensive. In [22] it is suggested to alleviate this problem by using JPF to generate models of agent programs that can be executed in other model-checkers. This idea is further developed in [23], which shows how AJPF can be modified to output models in the input languages of Spin or Prism [24], a probabilistic model checker. All of the approaches towards agent verification to date, do not provide the user with a complete framework to build and verify a probabilistic model, and mostly they do not perform at a level suitable for real-time applications.\nIn this paper we introduce a new agent architecture called Limited Instruction Set Agent (LISA). The architecture of LISA is based on the threelayer architecture [25] and the agent program is an evolution of Jason [7,26]. The aim is to simplify the structure and the execution of the agent program in order to reduce the size of the state-space required to abstract it and ultimately allow for a fast verification process. The agent program is developed and described with sEnglish [27,28], a natural language programming interface. The use of sEnglish provides a way to define both the agent program\nand the environment model in an intuitive, natural-language document. The document will then be automatically translated into Prism source code for verification by probabilistic model checking. This is done by first proving that LISA can be abstracted as a DTMC or a MDP, based on design choices made by the user. We also propose the use of probabilistic model checking in Prism to improve the non-deterministic decision making capabilities of the agent in a run-time verification process. Using run-time verification the agent is able to look into the consequences of its own choices by running model checking queries through the previously generated Prism model."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Rational Agents", "text": "An agent-based system is characterised by its architecture, a description of how the agent reasoning communicates with lower abstraction subsystems and ultimately with the environment. By analogy to previous definitions [4, 5, 27], we define the agent reasoning as follows.\nDefinition 1 (Rational agent). A rational BDI agent is defined as a tuple\nR = {F , B,B0, L,A,A0,\u03a0}\nwhere:\n\u2022 F = {p1, p2, . . . , pnp} is the set of all predicates.\n\u2022 B \u2282 F is the total atomic Beliefs set.\n\u2022 B0 is the Initial Beliefs set.\n\u2022 L = {l1, l2, . . . lnl} is a set of logic-based implication rules on the predicates of B.\n\u2022 A = {a1, a2, . . . , ana} \u2282 F \\B is a set of all available actions. Actions can be either internal, when they modify the Beliefs set to generate internal events, or external, when they are linked to external functions. Beliefs generated by internal actions are also called \u2018mental notes\u2019.\n\u2022 A0 is the set of Initial Actions.\n\u2022 \u03a0 = {\u03c01, \u03c02, . . . , \u03c0n\u03c0} is the set of executable plans or plans library. Each plan j is a sequence \u03c0j(\u03bbj), with \u03bbj \u2208 [0, n\u03bbj ] being the plan index, where \u03c0(0) is a logic statement called triggering condition, and \u03c0j(\u03bbj) with \u03bbj > 0 is an action from A.\nDuring an execution the agent also uses the following dynamic subsets of the sets defined above:\n\u2022 B[t] \u2282 B is the Current Beliefs set, the set of all beliefs available at time t. Beliefs can be negated with a \u2018~\u2019 symbol.\n\u2022 E[t] \u2282 B is the Current Events set, which contains events available at time t. An event is a belief paired with either a \u2018+\u2019 or a \u2018\u2212\u2019 operator to indicate that the belief is either added or removed.\n\u2022 D[t] \u2282 \u03a0 is the Applicable Plans or Desires set at time t, which contains plans triggered by current events.\n\u2022 I[t] \u2282 \u03a0 is the Intentions set, which contains plans that the agent is committed to execute.\nThe triggering condition of each plan of the plan library is composed by two parts: a triggering event and a context, a logic condition to be verified for the plan to apply. We write B[t] c when the Current Beliefs set \u201csatisfies\u201d the expression c, or in other words when the conditions expressed by c are true at time t. Note that in all our definitions and throughout the paper, time t \u2208 N\u22651 refers to the integer count of reasoning cycles.\nAlthough different AOP languages implement the agent in different ways, generally speaking an agent program is iterative. Each iteration is called reasoning cycle. The reasoning cycle of the LISA system is explained in Sec. 3."}, {"heading": "2.2 Model checking and verification", "text": "Probabilistic model checking is an automated verification method that aims to verify the correctness of probabilistic systems, by establishing if a desired property holds in a probabilistic model of the system [29]. For the purpose of this work we will consider models in particular: DTMC and MDP. Referring to [29\u201331] we give the following definitions:\nDefinition 2 (Discrete-Time Markov Chain (DTMC)). A (labelled) DTMC is a tuple D = (S, s0,P , L), where S is a countable set of states, s0 \u2208 S is the initial state, P : S \u00d7 S \u2192 [0, 1] is a Transition Probability Matrix such that \u2211 s\u2032\u2208S P (s, s\n\u2032) = 1 and L : S \u2192 \u2118(F) is a labelling function that assigns to each state a set of atomic prepositions that are valid in the state.\nDefinition 3 (Markov Decision Process (MDP)). A (labelled) MDP is a tuple M = (S, s0, C, Step, L), where S is a countable set of states, s0 \u2208 S is the initial state, C is an alphabet of choices with C(s\u2032) being the set of choices available in any state s\u2032, Step : S \u00d7 C \u2192 Dist(S) is a probabilistic transition function with Dist(S) being the set of all probability distributions over S and L : S \u2192 \u2118(F) is a labelling function that assigns to each state a set of atomic prepositions that are valid in the state.\nDetailed explanation on the techniques used to perform model checking on probabilistic models goes beyond the scope of this paper. However we report here the syntax of the language used to write properties to verify with model checkers, which is called Probabilistic Computation Tree Logic (PCTL) [32].\nDefinition 4 (Syntax of PCTL).\n\u03c6 ::= true | a | \u03c6 \u2227 \u03c6 | \u00ac\u03c6 | P./p[\u03c8] \u03c8 ::= X \u03c6 | \u03c6 U\u2264k \u03c6\nwhere a is an atomic proposition, ./\u2208 {\u2264, <,>,\u2265} and p \u2208 [0, 1].\nWe also allow the usual abbreviations such as \u2018F\u03c6\u2019 (equivalent to \u2018trueU\u03c6\u2019). A commonly used extension of PCTL is the addition of quantitative versions of the P operator. For example P=?[\u03c8] asks: \u201cwhat is the probability of \u03c8 holding?\u201d. In the same way we can add the operators Pmin=?[\u03c8] and Pmax=?[\u03c8] for MDP models that ask: \u201cwhat is the minimum/maximum probability of \u03c8 holding?\u201d.\nPCTL formulas can be extended with reward properties [31] by the addition of the reward operator R./r[\u00b7] and the following state formulas:\nR./r[C \u2264k] | R./r[F \u03c6] (1)\nwhere C is the cumulative reward operator, r \u2208 R, k \u2208 N and \u03c6 is a PCTL state formula."}, {"heading": "3 The Limited Instruction Set Agent", "text": "The architecture of LISA, depicted in Fig. 1, is based on the three-layer architecture [25]. Each block with rounded corners is a collection of so called skills that the agent reasoning is able to execute when invoking actions. Note the hybrid nature of the system: the dotted lines represent symbolic flows of information, while the solid line represent numeric information.\nThe agent program is an evolution of Jason [7, 26]. Here follows a brief overview of the modification that were made to Jason.\nPerception. In LISA perception predicates can be of two types: sensory perception (p \u2208 Bs) and action feedbacks (p \u2208 Ba), therefore the Beliefs set is defined as:\nB = {Bs, Ba, Bm} (2)\nwhere Bm is the set of all possible mental notes. The action feedbacks are percepts that actions feedback to the Beliefs set of the agent in order to make the agent aware of the outcome of the action itself, i.e. success, partial success or failure. For the purpose of modelling, this classification is very important: the different nature of sensory percepts and action feedbacks needs to be modelled in a different way to accurately describe the behaviour of the environment. Messages are also handled as percepts.\nGoals. In Jason there is a distinction between beliefs and goals. In a practical sense this distinction does not have a great influence: beliefs and goals can both trigger plans. For this reason in LISA we drop the definition of goals, by also defining goals as beliefs. This can simplify the process of generating a model directly from the agent code, by simplifying the syntax, and it also simplifies the modelling of the belief update process, by reducing the number of states required to describe it.\nLogic rules. In Jason logic-based implication rules are present but yet not well implemented, to the point that the main text itself [7] advises against their use. In LISA we allow for rule to change the Beliefs set and therefore generate events. This feature potentially reduces the state space by allowing the definition of shorter plans with less actions.\nIn Fig. 2 we describe the reasoning cycle for LISA. The first step is to update the Current Beliefs set with the Beliefs Update Function (fBU ), based on percepts, messages and mental notes, where logic rules are also applied. The Belief Review Function (fBR) then checks what changes have been made to the Current Beliefs set and it generates the new Events set. The function fP gathers all the plans from the Plan Library that are triggered by the current events, if the plan context is applicable to the Current Beliefs set, the plan is copied to the Desires set. An external function called Plan Selection Function (FO) selects one plan for each event and it copies it from the Desires set to the Intentions set. Finally for every cycle the function fact executes the next action for each plan.\nThe general flow is similar to that of Jason with mainly one distinction: in every reasoning cycle the Jason agent only allows for the handling of a single event (selected with a function called Event Selection Function FE), and then the execution of a single action from the Intentions set (selected with a function called Intention Selection Function FI). In LISA we implement a multi-threaded work flow that allows the handling of multiple events, and then the execution of multiple actions at the same time. This implies that the Desires set becomes:\nD[t] = {D1[t], . . . , Dne [t]} (3)\nwhere each Dj [t] is the set of plans triggered by an event ej \u2208 E[t] and ne = |E[t]| is the number of events. Consequently, the function FO, must be applied to every Dj [t] \u2282 D[t]. It is important to note that plans are copied into the Desires set from the Plan library, but not exclusively, which implies that different subsets of D[t] may have a copy of the same plan. However if a plan is selected multiple times in the same reasoning cycle, it will only be executed once. Furthermore once a plan is selected from the Desires set and copied to the Intentions set for execution, if the plan is selected again in the future it will not be executed a second time, but it will carry on from the current state unless a plan interruption action is issued.\nThis multi-threaded implementation greatly simplifies the modelling process of the agent reasoning by drastically reducing the number of states required to describe it. By eliminating the need for specialised non-deterministic functions the model does not have to keep track of the events and actions activated in previous reasoning cycles therefore reducing the number of states. This also reduces the level of non-determinism in the agent reasoning, which then allows for a more precise generalisation of the abstraction process and in turn the application of an automatic modelling software that generates a complete and verifiable model directly from the agent code."}, {"heading": "4 Abstraction to discrete finite-state machine", "text": "In this section we give a detailed description of the abstraction of the LISA reasoning to two kinds of discrete state machines: DTMC and MDP (see Definitions 2 and 3).\nThe agent defined in Definition 1 is in principle a deterministic system\nwith well defined rules and states. In the Jason implementation however, there are three functions that introduce non-determinism in the reasoning cycle (FE , FO and FI), which we reduce to one (FO) with our LISA implementation. In Theorem 1 we show that the LISA system can still be modelled as a DTMC under the right conditions, and in Theorem 2 we show that the LISA system can always be modelled as a MDP.\nIn Definition 1 we introduced the concept of plan as a sequence \u03c0 = {\u03c0(0), \u03c0(1), . . . , \u03c0(n\u03bb)}. Assuming that a plan is not allowed to be executed multiple times in parallel, let us define a set of plan indices \u03bb[t] = {\u03bb1, \u03bb2, . . . , \u03bbn\u03c0}, which represents the state of all plans at time t. Note that, according to this definition, a plan \u03c0j is a member of the Intentions set I[t] at time t if and only if \u03bbj > 0 at time t. From \u03bb[t] we can define a set of all possible indices as \u039b = {\u039b1,\u039b2, . . . ,\u039bn\u03c0}, where \u039bj = {1, . . . , n\u03bbj} is the set of natural numbers between 1 and the total number n\u03bbj of actions for each plan \u03c0j .\nTheorem 1 (LISA abstraction to DTMC). Assuming the existence of sets of (discrete) probability distributions Dist(Bs) and Dist(Ba), over the set of percepts and the set of action feedbacks, if \u2200 i, j \u2208 [1, n\u03c0], \u03c0i(0) 6= \u03c0j(0) the LISA can be modelled as a DTMC .\nProof. A DTMC is completely characterised given a countable set of states S and a transition function P : S \u00d7 S \u2192 [0, 1]. According to the definition of LISA, for a reasoning cycle to be completed the agent needs to be aware of E[t] in order to recall plans from the plan library, B[t] in order to check the plans context, and the state of the plans in I[t] in order to execute the next actions. The state of a LISA is only relevant at the end of a reasoning cycle, therefore a generic state can be expressed as s[t] = {B[t], E[t],\u03bb[t]}. The state space, given by S = \u2118(B) \u00d7 \u2118(B) \u00d7 \u039b, is therefore finite and countable. The state of the agent is initialised by s0 = {B0, \u2205,0} and by triggering the actions listed in the initial actions set A0. The transition function describes the way in which the state changes at every step. For each reasoning cycle, events can be generated from change in beliefs, namely mental notes, action feedbacks and percepts. Changes in mental notes are given by internal actions, which are known from the plan indices \u03bb. Changes in action feedbacks and percept are given by known probability distributions. If \u2200 i, j, \u03c0i(0) 6= \u03c0j(0), e.g. if all plans have different triggering conditions, then\n\u2200t \u2208 N\u22651 , \u2223\u2223\u2223\u2223\u2223 ne\u22c3 k=1 Dk[t] \u2223\u2223\u2223\u2223\u2223 = |D[t]| \u2264 |E[t]| (4) each event will trigger at most one plan, therefore FO becomes a trivial oneto-one mapping, therefore the system does not show any non-deterministic behaviour, hence the LISA reasoning can be modelled as a DTMC.\nTheorem 2 (LISA abstraction to MDP). Assuming the existence of sets of (discrete) probability distributions Dist(Bs) and Dist(Ba), over the set of percepts and the set of action feedbacks, the any LISA reasoning can be modelled as a MDP.\nProof. A MDP is completely described given a countable set of states S and a transition function Step : S \u00d7 C \u2192 Dist(S), with C(s\u2032) being the set of available choices in any state s\u2032. The set of states can be built as shown in Theorem 1. If \u2200 i, j \u2208 [1, n\u03c0], \u03c0i(0) 6= \u03c0j(0), according to Theorem 1, the system does not show any non-determinism. However, if \u2203i, j \u2208 [1, n\u03c0] : \u03c0i(0) = \u03c0j(0), then\n\u2203t\u2032 \u2208 N\u22651 : \u2223\u2223\u2223\u2223\u2223 ne\u22c3 k=1 Dk[t \u2032] \u2223\u2223\u2223\u2223\u2223 > \u2223\u2223E[t\u2032]\u2223\u2223 (5) the number of applicable plans is greater than the number of events, therefore for some event ek[t\n\u2032] (k \u2208 [1, ne]), the application of the Plan Selection Function (FO(Dk[t\n\u2032]) = \u03c0) involves a non-deterministic choice that implies different future probabilistic outcomes from action feedbacks, which prevents the modelling with DTMC. However this choice represents the only non-deterministic part of the agent, thus C(s\u2032) = D[t\u2032]. Once a choice is made by the Plan Selection Function, the transitions can be defined by changes in beliefs, given by internal actions and known probability distributions as shown in Theorem 1, and therefore the LISA modelling as a MDP is complete.\nProbabilistic models such as DTMCs and MDPs can be verified by means of probabilistic model checking, by using dedicated software such as Prism. Theorems 1 and 2 therefore imply that LISA can be verified, assuming that probability distributions of the percepts and action feedbacks are well defined. Theorems 1 and 2 also imply the availability of two options when designing the agent program: to design an agent with all unique triggering conditions, so to possibly improve model checking speed but requiring more effort from the designer, or design an agent with matching triggering conditions so to simplify the design but requiring more computation for the model checking."}, {"heading": "5 Probabilistic modelling within agent programs", "text": "In this section we describe the process of modelling probabilistic behaviour of the environment and the action feedbacks in the agent code. The aim is to use a unified approach that allows to obtain a complete model of the agent and its interactions with the environment from a single document. The reasoning of the agent is implemented in sEnglish [27, 28], to which\nwe add a few features that give the programmer the option of defining the probabilistic parts of the system. Along with the probabilistic modelling we also introduce a reward structure which allows to define and use the reward properties supported by Prism.\nThe action feedbacks are modelled within the action definition of sEnglish by defining the following three parameters: a probability value p, the average number of reasoning cycles \u00b5 in which the action feedback is expected to become true, and a variance \u03c3. In this way we can simulate a time-delay-uncertain phenomenon without the need for real time models. For the percept process we use a similar notation with the possibility of defining probability distributions that are conditional to other beliefs. In particular the user defines: a list of percepts or mental notes to which the percept being modelled is conditioned to, probability, average number of reasoning cycles and variance of activation and deactivation.\nThe last feature we introduce is the possibility for the programmer to describe reward structures, that then allow to use reward properties as described in Equation 1. The reward values can be declared by adding a new \u2018{\u00b7 \u00b7 \u00b7 }\u2019 structure to any percept declaration within the Percept Process section, or to any action within any of the executable plans.\nBy specifying all the necessary information, as described above, the designer is able to implement a complete model that includes a probabilistic description of the environment behaviour, e.g. percepts and action feedbacks. This allows to automatically generate Prism input code for verification (see Sec. 6)."}, {"heading": "6 Design-time verification", "text": "The software used to perform the design-time and run-time verifications is Prism [24,33]. The modelling approach showed in Sec. 5 gives an sEnglish program that provides enough information to generate a complete Prism model for verification. The Prism model is generated here with a dedicated Matlab script. The translator only operates on the agent program itself, and it runs in the order of the tens of milliseconds on the laptop PC we used for the testing. For this reason the performances of the translator itself will be considered to be negligible for the results presented in this paper.\nThe automatically generated Prism model is structured as follows: a variable is defined for every belief (percept, mental note and action feedback), a variable is also defined for every plan, representing the plan index \u03bb which captures the state of the plan at any given time. By using the synchronisation feature offered by the Prism software, the reasoning cycle is simulated in two steps: a Beliefs set update, where variables associated with beliefs are updated, and a plan index update, where variables associated to plan indexes are updated according to the beliefs. With this method we en-\nsure that plans only advance when the appropriate conditions on the Beliefs set are met. Note that there are no variables associated with actions as they are not part of the definition of state of the agent, as shown in Theorem 1.\nNote that by using the approach presented in this paper, during the verification process, the user has access to every single belief and plan. This means that the property specification can touch any part of the system, allowing the user to define arbitrarily complex properties on any aspect of the reasoning process. This can be used to drastically reduce the design errors for autonomous agents.\nFor example, assume that an agent is implemented to have two opposite actions such as \u2018go left\u2019 and \u2018go right\u2019. Assuming that the agent is programmed to have \u03c02(1)=\u2018go left\u2019 and \u03c04(2)=\u2018go right\u2019, the property:\nPmax=? [F (plan 2 = 1 & plan 4 = 2)]\nwill ask the model checker to generate \u201cthe maximum probability of \u2018go left\u2019 and \u2018go right\u2019 to be executed at the same time at some point in the future\u201d."}, {"heading": "7 Run-time verification for improved decision-making", "text": "In this section we propose two different methods for using a run-time verification process as an internal model for improving the decision-making capabilities for the LISA system. The automatically generated Prism, presented in Sec. 6, can also be used for run-time verification. Most of the computational power required to verify such a model is usually spent by the model checker when building the model itself, which does not influence the verification time. In other words, once the model is built, the user can run different verification queries without having to rebuild the model. In many cases, PRISM is able to compute the answers to those queries in a matter of seconds, even for a fairly complex model, therefore this can be a reasonable technique to use in this framework.\nThe first method is to implement the run-time verification process as a skill of the agent, e.g. as a module of the full system.The DTMC or MDP model is verified against a set of predefined queries. In particular, in Prism, it is possible to check a query by selecting a starting state with the use of filters [33]. The run-time verification is then used to generate a set of results that will be interpreted by a \u2018generate beliefs\u2019 function that will activate or deactivate certain beliefs in the agent Beliefs set.\nThe second method consists of implementing a Plan Selection Function that makes use of model checking to assess probability of success based on user-defined specifications, and selects the most suitable plan. A clear advantage to this approach is that, since the probabilistic model is generated\nautomatically, the user does not need to implement a specialised function for each agent. A possible implementation for this is as follows. The function takes as input the Current Beliefs and Desires set. The model generated at design time can be initialised with the current state and then checked against predefined queries. This results in probability values that can be used as indices to select the most likely to succeed plan amongst the ones in the Desires set.\nNote that the two methods described here for run-time model checking, are not mutually exclusive: in case the programmer chooses to implement the LISA as a MDP, they could both be used at the same time."}, {"heading": "8 A case study", "text": "Consider an Autonomous Surface Vehicle (ASV) designed for mine detection and disposal. The ASV is equipped with sensing equipment such as sonars and cameras that allow the detection of unidentified objects in the area of interest. These sensors give the vehicle a cone shaped visibility range. Using its pose in the environment and the information from the sensing equipment, the system is able to assess, on the fly, whether or not there has been any area left unclear. All the data collected is continuously sent back to the control centre. Once the mission is started, lower level tasks, such as for example collision avoidance, are carried out automatically by dedicated subsystems. During the exploration of the area, the system tags mine-like objects and logs their positions and available information for the human operators at the control centre to analyse and deliberate. In this scenario, a mission consists of a set of points (in terms of latitude and longitude) that outlines a specific area. An algorithm generates a sequence of waypoints connected by linear tracks, the parallel distance between tracks is calculated by considering the range of the available sensing equipment. We will call the linear tracks, and the area surrounding the tracks, \u201cblocks\u201d.\nIn a best case scenario the exploration plan will be carried out as it is defined. However a number of problems can occur. In case that the weather condition becomes too harsh, the agent will wait for instructions from human operators. If the agent realises that there are areas left unexplored in the last block, it will make a non-deterministic decision on whether to immediately go back to re-explore missed spots, or keep going and come back at the end of the mission.\nA fragment of the LISA program developed for this example is shown in Fig. 3. In Fig. 4 is shown a fragment of the Prism program that is automatically generated from the agent code. In Table 1 results are reported by running the model in Prism. All the testing was done by using an Apple laptop with a dual-core Intel Core i5-4258U 2.4GHz CPU and 16GB of memory running 64-bit Mac OS X 10.11.3. We implemented two different\nversions of agent programs for this case study: one by defining the agent to be abstracted as a DTMC, as per Theorem 1, and one as a MDP, as per Theorem 2. Both of the models feature 10 executable plans and 3 logic rules each, 4 percepts, 4 possible actions with a total of 5 action feedbacks, with additional conditions for the DTMC version.\nReferring to Table 1, as expected, the MDP case generates a model that is larger than the DTMC counterpart. However even though the number of states in the MDP case is about 70% larger than the number of state for the DTMC model, the time required to build the model is very similar. This is possibly due to the way Prism handles the model building: the software constructs a MTBDD [34] structure, that is very much dependent on the logic structure of the model.\nBoth models were then ran with a standard verification query that calculates the minimum probability of completing the mission within 100 steps. Note that for the DTMC model the probability is a single value, which is still indicated here with \u2018minimum probability\u2019. The verification time in this case is consistent with the increase in number of states from the DTMC model to the MDP model."}, {"heading": "9 Conclusions", "text": "In this article we introduced a new architecture for BDI agent programming called Limited Instruction Set Agent (LISA). The architecture builds on previous implementations of AgentSpeak but with a simpler structure, in order to facilitate automatic verification of agent reasoning. The reasoning of the agent is defined in the Natural Language Programming (NLP) language sEnglish. We proved that the LISA architecture can be abstracted as a Markovian model (DTMC or MDP), we then showed how to define a full probabilistic model that includes the agent reasoning and the environment, all defined within the agent reasoning program. From the improved agent program we can then automatically generate a full probabilistic model in Prism\u2019s input language for verification.\nThe model generated from the agent code is used for both design-time and run-time verification. The design-time verification can serve to im-\nprove and validate the agent design. The run-time verification can be used to improve the decision-making capabilities of the agent by implementing model-checking techniques as a means of simulation by the agent in order to predict future events and choose the most suitable strategy."}, {"heading": "Acknowledgment", "text": "This work was supported by the EPSRC project EP/J011894/2."}], "references": [{"title": "Astr\u00f6m, Autonomous Control. Berlin: Springer-Verlag, 1992, vol. Future Tendencies in Computer Science, Edts: A", "author": ["J. K"], "venue": "Bensoussan and J.P. Verius,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Feedback systems: an introduction for scientists and engineers", "author": ["K.J. Astr\u00f6m", "R.M. Murray"], "venue": "Princeton university press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Intelligent Systems: architecture, design and control", "author": ["A. Meystel", "J. Albus"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Autonomous vehicle control systems - a review of decision making", "author": ["S.M. Veres", "L. Molnar", "N.K. Lincoln", "C. Morice"], "venue": "vol. 225, no. 2, pp. 155\u2013195, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "An Introduction to MultiAgent Systems", "author": ["M. Wooldridge"], "venue": "Chichester: Wiley,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Intelligent agents: theory and practice", "author": ["M. Wooldridge", "N.R. Jennings"], "venue": "The Knowledge Engineering Review, vol. 10, no. 2, pp. 115\u2013 152, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Programming multiagent systems in AgentSpeak using Jason", "author": ["R.H. Bordini", "J.F. Hubner", "M. Wooldridge"], "venue": "Chichester: Wiley,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Procedural knowledge", "author": ["M.P. Georgeff", "A.L. Lansky"], "venue": "Proceedings of the IEEE, vol. 74, no. 10, pp. 1383\u20131398, 1986.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1986}, {"title": "Agentspeak (l): Bdi agents speak out in a logical computable language", "author": ["A.S. Rao"], "venue": "Agents Breaking Away. Springer, 1996, pp. 42\u201355.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Agent-oriented programming", "author": ["Y. Shoham"], "venue": "Artificial intelligence, vol. 60, no. 1, pp. 51\u201392, 1993.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Model checking agentspeak", "author": ["R.H. Bordini", "M. Fisher", "C. Pardavila", "M. Wooldridge"], "venue": "Proceedings of the second international joint conference on Autonomous agents and multiagent systems. ACM, 2003, pp. 409\u2013416.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Verifying multi-agent programs by model checking", "author": ["R.H. Bordini", "M. Fisher", "W. Visser", "M. Wooldridge"], "venue": "Autonomous agents and multi-agent systems, vol. 12, no. 2, pp. 239\u2013256, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Design and Validation of Computer Protocols", "author": ["G.J. Holzmann"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1991}, {"title": "The model checker spin", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on software engineering, vol. 23, no. 5, p. 279, 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Model checking programs", "author": ["W. Visser", "K. Havelund", "G. Brat", "S. Park", "F. Lerda"], "venue": "Automated Software Engineering, vol. 10, no. 2, pp. 203\u2013232, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Gwendolen: A bdi language for verifiable agents", "author": ["L.A. Dennis", "B. Farwer"], "venue": "Proceedings of the AISB 2008 Symposium on Logic and the Simulation of Interaction and Reasoning, Society for the Study of Artificial Intelligence and Simulation of Behaviour, 2008, pp. 16\u201323.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A flexible framework for verifying agent programs", "author": ["L.A. Dennis", "B. Farwer", "R.H. Bordini", "M. Fisher"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 3. International Foundation for Autonomous Agents and Multiagent Systems, 2008, pp. 1303\u20131306.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Programming verifiable heterogeneous agent systems", "author": ["L.A. Dennis", "M. Fisher"], "venue": "Programming Multi-Agent Systems. Springer, 2008, pp. 40\u201355.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Model checking agent programming languages", "author": ["L.A. Dennis", "M. Fisher", "M.P. Webster", "R.H. Bordini"], "venue": "Automated software engineering, vol. 19, no. 1, pp. 5\u201363, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A synergistic and extensible framework for multi-agent system verification", "author": ["J. Hunter", "F. Raimondi", "N. Rungta", "R. Stocker"], "venue": "Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems. International Foundation for Autonomous Agents and Multiagent Systems, 2013, pp. 869\u2013876.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stage agent program verification", "author": ["L.A. Dennis", "M. Fisher", "M. Webster"], "venue": "Journal of Logic and Computation, p. exv003, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Prism 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Computer aided verification. Springer, 2011, pp. 585\u2013591.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "On three-layer architectures", "author": ["E. Gat"], "venue": "Artificial Intelligence and Mobile Robots: Case Studies of Successful Robot Systems, pp. 195\u2013210, 1998, mIT Press.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "A Java-based interpreter for an extended version of AgentSpeak, 2007, manual version 0.9.5", "author": ["R.H. Bordini", "J.F. Hubner", "Jason"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Natural language programming of complex robotic bdi agents", "author": ["N.K. Lincoln", "S.M. Veres"], "venue": "Intelligent and Robotic Systems, vol. 71, no. 2, pp. 211\u2013230, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Advances in probabilistic model checking", "author": ["M. Kwiatkowska", "D. Parker"], "venue": "Verification, Model Checking and Abstract Interpretation (VMCAI). Springer, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Automated verification techniques for probabilistic systems", "author": ["V. Forejt", "M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Formal Methods for Eternal Networked Software Systems. Springer, 2011, pp. 53\u2013113.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic model checking", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Formal methods for performance evaluation. Springer, 2007, pp. 220\u2013270.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A logic for reasoning about time and reliability", "author": ["H. Hansson", "B. Jonsson"], "venue": "Formal aspects of computing, vol. 6, no. 5, pp. 512\u2013535, 1994.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Multi-terminal binary decision diagrams: An efficient data structure for matrix representation", "author": ["M. Fujita", "P.C. McGeer", "J.-Y. Yang"], "venue": "Formal methods in system design, vol. 10, no. 2-3, pp. 149\u2013169, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "and executing plans of action that work toward goals [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 1, "context": "and executing plans of action that work toward goals [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 2, "context": "Autonomous controllers aim to introduce a certain level of \u201cintelligence\u201d in control systems, that is the ability of a system to act appropriately in an uncertain environment [3].", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "However the passive nature of objects in OOP, led to the development of active objects called \u201cagents\u201d [4], which implement decision-making processes.", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "A formal description of autonomous agents can be found in [4\u20136].", "startOffset": 58, "endOffset": 63}, {"referenceID": 4, "context": "A formal description of autonomous agents can be found in [4\u20136].", "startOffset": 58, "endOffset": 63}, {"referenceID": 5, "context": "A formal description of autonomous agents can be found in [4\u20136].", "startOffset": 58, "endOffset": 63}, {"referenceID": 3, "context": "One of the most widely used \u201canthropomorphic\u201d approaches to the implementation of autonomous agents is the Belief-Desire-Intention (BDI) architecture [4,7].", "startOffset": 150, "endOffset": 155}, {"referenceID": 6, "context": "One of the most widely used \u201canthropomorphic\u201d approaches to the implementation of autonomous agents is the Belief-Desire-Intention (BDI) architecture [4,7].", "startOffset": 150, "endOffset": 155}, {"referenceID": 7, "context": "The most known implementations of the BDI architecture are the Procedural Reasoning System (PRS) [8, 9] and AgentSpeak [10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 8, "context": "The most known implementations of the BDI architecture are the Procedural Reasoning System (PRS) [8, 9] and AgentSpeak [10].", "startOffset": 119, "endOffset": 123}, {"referenceID": 9, "context": "AgentSpeak fully embraces the philosophy of Agent Oriented Programming (AOP) [11], and it offers a customisable Java based interpreter.", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "An early attempt to BDI agent verification can be found in [13,14], where the authors present a translation software from AgentSpeak to either Promela or Java, and then use the associated model checkers Spin [15, 16] and Java PathFinder (JPF) [17].", "startOffset": 59, "endOffset": 66}, {"referenceID": 11, "context": "An early attempt to BDI agent verification can be found in [13,14], where the authors present a translation software from AgentSpeak to either Promela or Java, and then use the associated model checkers Spin [15, 16] and Java PathFinder (JPF) [17].", "startOffset": 59, "endOffset": 66}, {"referenceID": 12, "context": "An early attempt to BDI agent verification can be found in [13,14], where the authors present a translation software from AgentSpeak to either Promela or Java, and then use the associated model checkers Spin [15, 16] and Java PathFinder (JPF) [17].", "startOffset": 208, "endOffset": 216}, {"referenceID": 13, "context": "An early attempt to BDI agent verification can be found in [13,14], where the authors present a translation software from AgentSpeak to either Promela or Java, and then use the associated model checkers Spin [15, 16] and Java PathFinder (JPF) [17].", "startOffset": 208, "endOffset": 216}, {"referenceID": 14, "context": "An early attempt to BDI agent verification can be found in [13,14], where the authors present a translation software from AgentSpeak to either Promela or Java, and then use the associated model checkers Spin [15, 16] and Java PathFinder (JPF) [17].", "startOffset": 243, "endOffset": 247}, {"referenceID": 15, "context": "[18] with a BDI agent programming language called Gwendolen, which is implemented in the Agent Infrastructure Layer (AIL) [19, 20], a collection of Java classes intended for use in model checking agent programs, particularly with JPF.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] with a BDI agent programming language called Gwendolen, which is implemented in the Agent Infrastructure Layer (AIL) [19, 20], a collection of Java classes intended for use in model checking agent programs, particularly with JPF.", "startOffset": 122, "endOffset": 130}, {"referenceID": 17, "context": "[18] with a BDI agent programming language called Gwendolen, which is implemented in the Agent Infrastructure Layer (AIL) [19, 20], a collection of Java classes intended for use in model checking agent programs, particularly with JPF.", "startOffset": 122, "endOffset": 130}, {"referenceID": 18, "context": "An evolution of JPF is Agent Java PathFinder (AJPF) [21], specifically designed to verify agent programs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "In [22] it is suggested to alleviate this problem by using JPF to generate models of agent programs that can be executed in other model-checkers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "This idea is further developed in [23], which shows how AJPF can be modified to output models in the input languages of Spin or Prism [24], a probabilistic model checker.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "This idea is further developed in [23], which shows how AJPF can be modified to output models in the input languages of Spin or Prism [24], a probabilistic model checker.", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "The architecture of LISA is based on the threelayer architecture [25] and the agent program is an evolution of Jason [7,26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "The architecture of LISA is based on the threelayer architecture [25] and the agent program is an evolution of Jason [7,26].", "startOffset": 117, "endOffset": 123}, {"referenceID": 23, "context": "The architecture of LISA is based on the threelayer architecture [25] and the agent program is an evolution of Jason [7,26].", "startOffset": 117, "endOffset": 123}, {"referenceID": 24, "context": "The agent program is developed and described with sEnglish [27,28], a natural language programming interface.", "startOffset": 59, "endOffset": 66}, {"referenceID": 3, "context": "By analogy to previous definitions [4, 5, 27], we define the agent reasoning as follows.", "startOffset": 35, "endOffset": 45}, {"referenceID": 4, "context": "By analogy to previous definitions [4, 5, 27], we define the agent reasoning as follows.", "startOffset": 35, "endOffset": 45}, {"referenceID": 24, "context": "By analogy to previous definitions [4, 5, 27], we define the agent reasoning as follows.", "startOffset": 35, "endOffset": 45}, {"referenceID": 25, "context": "Probabilistic model checking is an automated verification method that aims to verify the correctness of probabilistic systems, by establishing if a desired property holds in a probabilistic model of the system [29].", "startOffset": 210, "endOffset": 214}, {"referenceID": 25, "context": "Referring to [29\u201331] we give the following definitions:", "startOffset": 13, "endOffset": 20}, {"referenceID": 26, "context": "Referring to [29\u201331] we give the following definitions:", "startOffset": 13, "endOffset": 20}, {"referenceID": 27, "context": "Referring to [29\u201331] we give the following definitions:", "startOffset": 13, "endOffset": 20}, {"referenceID": 0, "context": "A (labelled) DTMC is a tuple D = (S, s0,P , L), where S is a countable set of states, s0 \u2208 S is the initial state, P : S \u00d7 S \u2192 [0, 1] is a Transition Probability Matrix such that \u2211 s\u2032\u2208S P (s, s \u2032) = 1 and L : S \u2192 \u2118(F) is a labelling function that assigns to each state a set of atomic prepositions that are valid in the state.", "startOffset": 127, "endOffset": 133}, {"referenceID": 28, "context": "However we report here the syntax of the language used to write properties to verify with model checkers, which is called Probabilistic Computation Tree Logic (PCTL) [32].", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "/\u2208 {\u2264, <,>,\u2265} and p \u2208 [0, 1].", "startOffset": 22, "endOffset": 28}, {"referenceID": 27, "context": "PCTL formulas can be extended with reward properties [31] by the addition of the reward operator R.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "1, is based on the three-layer architecture [25].", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "The agent program is an evolution of Jason [7, 26].", "startOffset": 43, "endOffset": 50}, {"referenceID": 23, "context": "The agent program is an evolution of Jason [7, 26].", "startOffset": 43, "endOffset": 50}, {"referenceID": 6, "context": "In Jason logic-based implication rules are present but yet not well implemented, to the point that the main text itself [7] advises against their use.", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "A DTMC is completely characterised given a countable set of states S and a transition function P : S \u00d7 S \u2192 [0, 1].", "startOffset": 107, "endOffset": 113}, {"referenceID": 24, "context": "The reasoning of the agent is implemented in sEnglish [27, 28], to which", "startOffset": 54, "endOffset": 62}, {"referenceID": 21, "context": "The software used to perform the design-time and run-time verifications is Prism [24,33].", "startOffset": 81, "endOffset": 88}, {"referenceID": 0, "context": "{[I am at global waypoint],[1,1,0]} 8 .", "startOffset": 27, "endOffset": 34}, {"referenceID": 0, "context": "{[I am at global waypoint],[1,1,0]} 8 .", "startOffset": 27, "endOffset": 34}, {"referenceID": 29, "context": "This is possibly due to the way Prism handles the model building: the software constructs a MTBDD [34] structure, that is very much dependent on the logic structure of the model.", "startOffset": 98, "endOffset": 102}], "year": 2016, "abstractText": "A new agent architecture called Limited Instruction Set Agent (LISA) is introduced for autonomous control. The new architecture is based on previous implementations of AgentSpeak and it is structurally simpler than its predecessors with the aim of facilitating designtime and run-time verification methods. The process of abstracting the LISA system to two different types of discrete probabilistic models (DTMC and MDP) is investigated and illustrated. The LISA system provides a tool for complete modelling of the agent and the environment for probabilistic verification. The agent program can be automatically compiled into a Discrete-Time Markov Chain (DTMC) or a Markov Decision Process (MDP) model for verification with Prism. The automatically generated Prism model can be used for both designtime and run-time verification. The run-time verification is investigated and illustrated in the LISA system as an internal modelling mechanism for prediction of future outcomes.", "creator": "LaTeX with hyperref package"}}}