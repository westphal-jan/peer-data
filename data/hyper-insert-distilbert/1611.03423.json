{"id": "1611.03423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "DiffSharp: An AD Library for .NET Languages", "abstract": "augmented diffsharp is an independent algorithmic angular differentiation or automatic differentiation ( objective ad ) demonstration library for shaping the. u net ecosystem, which is targeted by acquiring the classic c # literature and for f # languages, among others. currently the prototype library generator has traditionally been largely designed with machine inspired learning applications in mind, allowing very short succinct implementations versions of class models and similar optimization type routines. diffsharp basic is implemented in var f # and exposes exceptional forward loops and below reverse both ad defined operators as generic general nestable independent higher - order functions, therefore usable by any. 0 net pascal language. it provides simple high - performance nonlinear linear algebra and primitives - - - generated scalars, factor vectors, and matrices, together with a generalization to tensors underway - - - that are fully available supported now by eliminating all the underlying ad operators, platforms and forms which use a blas / schwarz lapack pascal backend algebra via the highly parallel optimized openblas build library. diffsharp python currently uses operator element overloading, but we are officially developing implementing a transformation - based development version primarily of the db library working using native f #'s \" code quotation \" metaprogramming facility. work on a cuda - based gpu backend editor is notably also been underway.", "histories": [["v1", "Thu, 10 Nov 2016 17:50:06 GMT  (10kb)", "http://arxiv.org/abs/1611.03423v1", "Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK"]], "COMMENTS": "Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK", "reviews": [], "SUBJECTS": "cs.MS cs.LG", "authors": ["at{\\i}l{\\i}m g\\\"une\\c{s} baydin", "barak a pearlmutter", "jeffrey mark siskind"], "accepted": false, "id": "1611.03423"}, "pdf": {"name": "1611.03423.pdf", "metadata": {"source": "CRF", "title": "DiffSharp: An AD Library for .NET Languages", "authors": ["At\u0131l\u0131m G\u00fcne\u015f Baydin", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n03 42\n3v 1\n[ cs\n.M S]\n1 0\nN ov\n2 01\n6\nDiffSharp: An AD Library for .NET Languages\u2217\nAt\u0131l\u0131m Gu\u0308nes\u0327 Baydin\u2020 Barak A. Pearlmutter\u2021 Jeffrey Mark Siskind\u00a7\nApril 2016"}, {"heading": "Introduction", "text": "DiffSharp1 is an algorithmic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind [1], allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives\u2014scalars, vectors, and matrices, with a generalization to tensors underway\u2014that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library.2 DiffSharp currently uses operator overloading, but we are developing a transformationbased version of the library using F#\u2019s \u201ccode quotation\u201d metaprogramming facility [2]. Work on a CUDA-based GPU backend is also underway."}, {"heading": "The .NET platform and F#", "text": "DiffSharp contributes a much needed advanced AD library to the .NET ecosystem, which encompasses primarily the languages of C#, F#, and VB in addition to others with less following, such as C++/CLI, ClojureCLR, IronScheme, and IronPython.3 In terms of popularity, C# is the biggest among these, coming fourth (after Javascript, SQL, and Java; and before Python, C++, and C) in the 2015 Stack Overflow developer survey;4 and again fourth (after Java, C, and C++; and before Python, PHP, and VB) in the TIOBE index for March 2016.5. Initially developed by Microsoft, the .NET platform has recently transformed into a fully open source endeavor overseen by the .NET Foundation, and it is currently undergoing a transition to the open source and cross platform .NET Core project,6 supporting Linux, OS X, FreeBSD, and Windows.\nF# is a strongly typed functional language\u2014also supporting imperative and object oriented paradigms\u2014that originated as an ML dialect for .NET and maintains a degree of compatibility with OCaml [2]. F# is gaining popularity as a cross-platform functional language, particularly in the field of computational finance. Languages that support higher-order functions are particularly appropriate for AD as the AD operators are themselves higher-order functions. We have developed implementation strategies that allow AD to be smoothly integrated into such languages, and allow the construction of aggressively optimizing compilers.7 F# allows DiffSharp to expose a natural API defined by higher-order functions, which can be freely nested and curried, accept first-class functions as arguments, and return derivative functions. The library is usable from F# and all other .NET languages. We provide an optional helper interface for C# and other procedural languages.\nProject organization and example code\nThe code for DiffSharp is released under the GNU Lesser General Public License (LGPL)8 and maintained in a GitHub repository.9 The user community has been engaged in the project by raising issues on GitHub and joining in the Gitter chat room.10\nThe library is structured into several namespaces. The main AD functionality is in the DiffSharp.AD namespace, but numerical and symbolic differentiation are also provided in DiffSharp.Numerical and DiffSharp.Symbolic.\n\u2217Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK. \u2020Corresponding Author, Dept of Computer Science, National University of Ireland Maynooth, gunes@cs.nuim.ie\n(Current address: Dept of Engineering Science, University of Oxford, gunes@robots.ox.ac.uk) \u2021Dept of Computer Science, National University of Ireland Maynooth, barak@pearlmutter.net \u00a7School of Electrical and Computer Engineering, Purdue University, qobi@purdue.edu 1http://diffsharp.github.io/DiffSharp/ 2http://www.openblas.net/ 3For a full list, see: https://en.wikipedia.org/wiki/List_of_CLI_languages 4http://stackoverflow.com/research/developer-survey-2015 5http://www.tiobe.com/tiobe_index 6https://dotnet.github.io/ 7R6RS-AD: https://Functional-AutoDiff/R6RS-AD, Stalin\u2207: https://github.com/Functional-AutoDiff/STALINGRAD, DVL: https://github.com/Functional-AutoDiff/dysvunctional-language [3, 4]. 8The LGPL license allows the use of unmodified DiffSharp binaries in any (including non-GPL or proprietary) project, with attribution. 9https://github.com/DiffSharp/DiffSharp\n10https://gitter.im/DiffSharp/DiffSharp\nAll of these implementations share the same differentiation API (Table 1) to the extent possible, and have support both 32-bit and 64-bit floating point. (Lower precision floating point is of particular utility in deep learning.) The DiffSharp.Backend namespace contains the optimized computation backends (currently OpenBLAS, with work on a CUDA backend underway). These namespaces and functionality are directly usable from C# and other .NET languages. For making the user experience even better for non-functional languages, we provide the DiffSharp.Interop interface that wraps the AD and numerical differentiation functionality, automatically taking care of issues such as conversions to and from FSharp.Core.FSharpFunc objects.11\nExtensive documentation on the library API,12 along with tutorials and examples, are available on the project website. The examples include machine learning applications, gradient-based optimization algorithms, clustering, Hamiltonian Markov Chain Monte Carlo, and various neural network architectures."}, {"heading": "Key features and contributions", "text": ""}, {"heading": "Higher-order functional AD API", "text": "The fundamental elements of DiffSharp\u2019s API are the jacobianv\u2019 and jacobianTv\u2019\u2019 operations, corresponding to the Jacobian-vector product (forward mode) and the Jacobian-transpose-vector product (reverse mode), respectively. The library exposes differentiation functionality through a higher-order functional API (Table 1), where operators accept functions as arguments and return derivative functions. For instance, for a function f : Rn \u2192 Rm, the jacobianTv\u2019\u2019 operation, with type (Rn \u2192 Rm) \u2192 Rn \u2192 (Rm \u00d7 (Rm \u2192 Rn)), evaluates the function at a given point, and returns the function value together with another function that can be repeatedly called to compute the adjoints of the inputs using reverse mode AD. The API also includes specialized operations (e.g., hessianv for Hessian-vector product) to cover common use cases and encourage modular code. This allows succinct implementation of differentiation-based algorithms. For instance, Newton\u2019s method for optimization can be simply coded as:\n// eps: threshold , f: function , x: starting point let rec argminNewton eps f x =\nlet g, h = gradhessian f x if DV.l2norm g < eps then x else argminNewton eps f (x - DM. solveSymmetric h g)\nNote that the caller of argminNewton need not be aware of what, if any, derivatives are being taken within it. DiffSharp provides a fixed-point-iteration operator, with appropriate forward and reverse AD rules [5]. The forward mode is handled by iterating until convergence of both the primal and the tangent values, while reverse mode13 uses the \u201ctwo-phases\u201d strategy [6]."}, {"heading": "Nesting", "text": "All the AD operators can be curried or nested. For instance, making use of currying, the internal implementation of the hessian operator in DiffSharp is simply\nlet inline hessian f x = jacobian (grad f) x\nresulting in a forward-on-reverse AD evaluation of the Hessian of a function at a point.\nIn another example, we can implement z = ddx\n(\nx\n(\nd dy (x+ y)\n\u2223 \u2223 \u2223\ny=1\n))\u2223\n\u2223 \u2223 \u2223\nx=1\nin F# as\nlet z = diff (fun x -> x * (diff (fun y -> x + y) (D 1.))) (D 1.)\nThis can be written in C#, using DiffSharp.Interop, as\nvar z = AD.Diff (x => x * AD.Diff (y => x + y, 1) , 1);\nCorrectness of AD in the presence of nesting requires avoiding perturbation confusion [7]. For instance, in the above example of nested derivatives, DiffSharp correctly returns 1 (val z : D = D 1.0), while an implementation suffering from perturbation confusion might return 2. We avoid perturbation confusion by tagging values to distinguish nested invocations of the AD operators. See [8, 3, 4, 9] for further discussion."}, {"heading": "Linear algebra primitives", "text": "One can automatically handle the derivatives of linear algebra primitives using an \u201carray-of-structures\u201d approach where arrays of AD-enabled scalars would give correct results for derivatives, albeit with poor performance and high memory consumption. This approach was used in DiffSharp until version 0.7, at which point the library was rewritten using a \u201cstructure-of-arrays\u201d approach where vector and matrix types internally hold separate arrays for their primal\n11http://diffsharp.github.io/DiffSharp/csharp.html 12http://diffsharp.github.io/DiffSharp/api-overview.html 13Currently, when using reverse mode, closed-over variables in the functional argument to the fixed-point operator should be exposed by\nmanual closure conversion. We hope to lift this restriction soon.\nand derivative values, and the library recognizes linear algebra operations such as matrix multiplication as intrinsic functions [10]. This allows efficient vectorization of AD, where the underlying linear algebra operations can be delegated to highly optimized BLAS/LAPACK libraries. This approach to AD with linear algebra primitives has been adopted, for example, for the GPU-based C++ reverse AD implementation of Gremse et al. [11]. It is interesting to note that for application domains heavily using linear algebra, such as training a neural network,14 applications typically spend more than 90% of their running time in external BLAS/LAPACK libraries. The AD library\u2019s role in a setting like this is reduced to the intelligent plumbing of primal and derivative arrays to the external library."}, {"heading": "Benchmarks", "text": "We provide benchmarks measuring the AD runtime overhead of the differentiation operations in the API.15 The code for the benchmarks is available in the GitHub repository and we also distribute a command line benchmarking tool with each release.16 We intend to add memory consumption figures to these benchmarks in the upcoming release."}, {"heading": "Current work", "text": ""}, {"heading": "Generalization to tensors", "text": "DiffSharp currently provides scalar (D), vector (DV), and matrix (DM) types. We are working on generalizing these to an n-dimensional array type, with capabilities similar to those of the Torch Tensor class17 or the NumPy ndarray.18 The main motivation for this is our interest in efficiently implementing convolutional neural networks."}, {"heading": "Source transformation", "text": "The library is currently implemented using operator overloading. One of the reasons why F# is an interesting language for AD is its advanced metaprogramming features. The \u201ccode quotations\u201d feature [2] allows one to programmatically\n14http://diffsharp.github.io/DiffSharp/examples-neuralnetworks.html 15http://diffsharp.github.io/DiffSharp/benchmarks.html 16http://github.com/DiffSharp/DiffSharp/releases 17http://torch7.readthedocs.org/en/rtd/tensor/index.html 18http://docs.scipy.org/doc/numpy-1.10.0/reference/arrays.ndarray.html\nread and generate abstract syntax trees of functions passed as arguments. The symbolic differentiation module in DiffSharp already makes use of code quotations. We are developing a source-transformation-based AD implementation using this feature, which should result in both speedups and simplification of the API."}, {"heading": "GPU backend", "text": "The backend interface that we defined while vectorizing DiffSharp allows us to plug in other computation backends that the user can select to run their AD code. Our current work on DiffSharp includes the implementation of a CUDA-based backend using cuBLAS for BLAS operations, custom CUDA kernels for non-BLAS operations such as element-wise function application, and cuDNN for convolution operations."}, {"heading": "The Hype library", "text": "DiffSharp will be maintained as a basis library providing an AD infrastructure to .NET languages, independent of the application domain. In addition to setting up this infrastructure, we are interested in using generalized nested AD for implementing machine learning models and algorithms. For this purpose, we started developing the Hype library19 which uses DiffSharp. Hype is in early stages of its development and is currently shared as a proof-of-concept for using generalized AD in machine learning. It showcases how the combination of nested AD and functional programming allows succinct implementations of optimization routines20 (e.g., stochastic gradient descent, AdaGrad, RMSProp), and feedforward and recurrent neural networks. Upcoming GPU and tensor support in DiffSharp is particularly relevant in this application domain, as these are essential to modern deep learning models."}, {"heading": "Conclusions", "text": "Although DiffSharp started as a vehicle for conducting research at the intersection of AD and machine learning, it has grown into an industrial-strength AD solution for F# in particular and the cross-platform .NET platform in general. Its functional API, combined with the ability to freely nest constructs, allows for the convenient implementation of highly modular AD-using software, as seen in the Hype library. We aim to finalize our work on the GPU backend and tensors before September 2016. Readers are invited to refer to the online documentation and code for more in-depth information."}, {"heading": "Acknowledgments", "text": "This work was supported, in part, by Science Foundation Ireland grant 09/IN.1/I2637 and by NSF grant 1522954-IIS. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."}], "references": [{"title": "Diffsharp: Automatic differentiation", "author": ["At\u0131l\u0131m G\u00fcne\u015f Baydin", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "venue": "library. arXiv:1511.07727,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Leveraging .NET meta-programming components from F#: Integrated queries and interoperable heterogeneous execution", "author": ["Don Syme"], "venue": "Workshop on ML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Nesting forward-mode AD in a functional framework", "author": ["Jeffrey Mark Siskind", "Barak A. Pearlmutter"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Reverse-mode AD in a functional framework: Lambda the ultimate backpropagator", "author": ["Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Differentiating fixed point iterations with ADOL-C", "author": ["Sebastian Schlenkirch", "Andrea Walther"], "venue": "Presentation at the 2nd European Workshop on Automatic Differentiation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Reverse accumulation and attractive fixed points", "author": ["Bruce Christianson"], "venue": "Optimization Methods and Software,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Nesting forward-mode AD in a functional framework", "author": ["Jeffrey Mark Siskind", "Barak A. Pearlmutter"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Method for computing all occurrences of a compound event from occurrences of primitive events", "author": ["Jeffrey Mark Siskind"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Collected matrix derivative results for forward and reverse mode algorithmic differentiation", "author": ["Mike B. Giles"], "venue": "In Advances in Automatic Differentiation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "GPU-accelerated adjoint algorithmic differentiation", "author": ["Felix Gremse", "Andreas H\u00f6fter", "Lukas Razik", "Fabian Kiessling", "Uwe Naumann"], "venue": "Computer Physics Communications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The library has been designed with machine learning applications in mind [1], allowing very succinct implementations of models and optimization routines.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "DiffSharp currently uses operator overloading, but we are developing a transformationbased version of the library using F#\u2019s \u201ccode quotation\u201d metaprogramming facility [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "NET and maintains a degree of compatibility with OCaml [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "com/Functional-AutoDiff/dysvunctional-language [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 3, "context": "com/Functional-AutoDiff/dysvunctional-language [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 4, "context": "DiffSharp provides a fixed-point-iteration operator, with appropriate forward and reverse AD rules [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "The forward mode is handled by iterating until convergence of both the primal and the tangent values, while reverse mode uses the \u201ctwo-phases\u201d strategy [6].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "Correctness of AD in the presence of nesting requires avoiding perturbation confusion [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "See [8, 3, 4, 9] for further discussion.", "startOffset": 4, "endOffset": 16}, {"referenceID": 2, "context": "See [8, 3, 4, 9] for further discussion.", "startOffset": 4, "endOffset": 16}, {"referenceID": 3, "context": "See [8, 3, 4, 9] for further discussion.", "startOffset": 4, "endOffset": 16}, {"referenceID": 8, "context": "and derivative values, and the library recognizes linear algebra operations such as matrix multiplication as intrinsic functions [10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The \u201ccode quotations\u201d feature [2] allows one to programmatically", "startOffset": 30, "endOffset": 33}], "year": 2016, "abstractText": "DiffSharp is an algorithmic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind [1], allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives\u2014scalars, vectors, and matrices, with a generalization to tensors underway\u2014that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformationbased version of the library using F#\u2019s \u201ccode quotation\u201d metaprogramming facility [2]. Work on a CUDA-based GPU backend is also underway.", "creator": "LaTeX with hyperref package"}}}