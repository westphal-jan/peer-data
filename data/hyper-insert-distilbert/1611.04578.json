{"id": "1611.04578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Earliness-Aware Deep Convolutional Networks for Early Time Series Classification", "abstract": "\" we present earliness - aware deep convolutional geographic networks ( ea - mediated convnets ), an integrated end - to - rear end deep motif learning database framework, meant for detecting early classification programs of stored time video series data. unlike most existing enabling methods described for early classification schemes of multiple time event series data, that are designed to solve this problem under the widespread assumption of demanding the availability of developing a sufficiently good set of pre - manually defined ( often commonly hand - crafted ) embedded features, our own framework can effectively jointly perform integrated feature learning ( by learning a static deep hierarchy graph of \\ emph { shapelets } capturing across the salient characteristics collected in framing each time location series ), along partnerships with develop a dynamic truncation model to help our deep generic feature learning architecture focus on the early parts of tracking each time series. consequently, our original framework component is able collectively to make these highly reliable interactive early predictions, rapidly outperforming various state - of - the - art filtering methods particularly for early time series branch classification, including while indirectly also being competitive. when compared to the state - characteristics of - that the - art time series classification computing algorithms that work only with \\ emph { fully observed } time temporal series / data. to the proven best defense of processing our knowledge, the core proposed framework package is the worldwide first to solely perform discrete data - validation driven ( via deep ) feature language learning in detailing the crucial context stage of early classification of embedded time series data. we nevertheless perform a typical comprehensive skill set summary of six experiments, on several benchmark referenced data core sets, which specifically demonstrate that our method yields seemingly significantly better predictions today than those various state - of - the - art methods uniquely designed for early actual time region series classification. in addition contrary to obtaining generally high accuracies, outcomes our experiments also show showed that the learned under deep shapelets based features are also highly broadly interpretable and can help historians gain better prediction understanding recognition of the underlying principal characteristics components of time series data.", "histories": [["v1", "Mon, 14 Nov 2016 20:55:33 GMT  (924kb,D)", "http://arxiv.org/abs/1611.04578v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wenlin wang", "changyou chen", "wenqi wang", "piyush rai", "lawrence carin"], "accepted": false, "id": "1611.04578"}, "pdf": {"name": "1611.04578.pdf", "metadata": {"source": "CRF", "title": "Earliness-Aware Deep Convolutional Networks for Early Time Series Classification", "authors": ["Wenlin Wang", "Changyou Chen", "Wenqi Wang", "Piyush Rai", "Lawrence Carin"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Early time series classification (ETSC), the task of predicting the label of a time series as early as possible, by only looking at an initial subsequence of the entire series, is becoming increasingly important in many timesensitive domains. Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23]. While the problem of classifying fully observed time series has been studied extensively in recent years, the earliness constraints posed by ETSC, which demands reliable early predictions, makes it a considerably more challenging problem than standard time series classification. An ETSC system must ensure that it not only cuts down on the prediction time, but also makes reliable predictions based on very few initial timestamps. These two goals are often conflicting with each other.\nIn order to ensure reliable predictions from an ETSC system, it is desirable to have methods that can extract highly discriminative features, even from very limited observations in the time series. Unfortunately, most existing methods for this problem are usually based on ad-hoc ways of defining subsequences or hand-crafted patterns [38], mapping them to fixed sized vectors, and subsequently applying a classification method (e.g., nearest neighbors or support vector machine). One of the key limitations of such approaches is that the extracted \u2217Department of Electrical & Computer Engineering, Duke University, USA \u2020Department of Industrial Engineering, Purdue University, USA \u2021Department of Computer Science & Engineering, IIT Kanpur, India\nar X\niv :1\n61 1.\n04 57\n8v 1\n[ cs\n.L G\n] 1\n4 N\nov 2\nfeatures can usually capture only label-based local patterns, and may have difficulties in generalizing to more complex time series data sets. Therefore, the ability to automatically learn highly discriminative features from (limited-length) time series data can be very important in these problems. Motivated by this, we present a deep learning framework for learning highly discriminative and interpretable features in the context of early time series classification.\nShapelets [38] based representations have recently emerged as an effective way to model time series data in problems such as time series classification. Shapelets are subsequences in time series with good discriminative power. Shapelets also provide good interpretability by capturing the salient characteristics in the time series data. However, most of the existing methods perform shapelet extraction using time-consuming and ad-hoc ways. e.g., by trying out all possible prefix subsequences and evaluating them based on their ability to predict the target variable of interest. Some recent work has tried learning shapelets from data [15] but the learned shapelets are usually not ideal for early time series classification.\nIn this paper, we present a deep feature learning and classification framework based on an earliness-aware deep convolutional neural network architecture. Our framework, henceforth referred to as Earliness-Aware Deep Convolutional Networks (EA-ConvNets), provides, to the best of our knowledge, the first deep, nonlinear feature learning based method for fast shapelet discovery, and integrates it with a nonlinear classification model to perform end-to-end deep learning for early classification of time-series data. Our deep learning framework can learn highly discriminative shapelets that are also highly interpretable, a property that is especially desirable in many ETSC problems, such as applications in medical and health informatics. As compared to the stateof-the-art methods for that can learn interpretable features for ETSC problems [36], the deep shapelet based representations learned using our framework, in a purely data-driven manner, can capture patterns in the time series at multiple levels of granularities.\nOur framework is flexible to dynamic lengths of the input time series, and can make predictions at any time based on the current observations. A key step towards this goal is to train the ConvNet on stochastic truncated time series data. Through an extensive set of experiments, we show that EA-ConvNets outperforms several state-of-the-art models for early time series classification, and is also competitive with time series classification methods that work with full time series."}, {"heading": "2 Convolutional Neural Nets for Time-Series", "text": "Our framework is based on a convolutional neural network (CNN) architecture for time series data. We first provide a brief review of a basic CNN for time series data [37], before describing the details of our proposed framework.\nConsider a sequence representing a univariate time series d = [d1, d2, ..., dL], where L is the length of the sequence. A classical CNN for such time series data works by composing alternating layers of convolution and pooling operations. The convolutional layer extracts patterns within local regions throughout the input sequence. This operation is performed by convolving a filter, g = [g1, g2, ..., gm], of lengthm over the sequence, and computing the inner product of the filter at each location in the sequence. For each layer, let us denote the outputs of this filter applied over the entire sequence, as a feature map vector o, which the i-th element written as\noi = n\u2211 j=1 g(j) \u00b7 d(i\u2212 j + L 2 ) .\n. Intuitively, it measures the similarity between the filter to each portion of the sequence. Afterwards, a nonlinear function f(\u00b7) is applied element-wise to the feature map o as a = f(o). The activations a are then passed to a pooling layer for nonlinear dimension reduction.\nThe pooling layer aggregates the information within a set of small regions of a, say {rk}Kk=1 with rk being some element of a, and produces a pooled feature map s = [s1, . . . , sK ] as the output. Let us denote the aggregation function to be POOL(). Then for each feature map, o, we have\nsk = POOL(f({oi}i\u2208rk))\nwhere rk is pooling region k in the feature map o."}, {"heading": "3 Earliness-Aware ConvNets", "text": "In this section, we formally define the early time series classification (ETSC) problem, and describe our framework, Earliness-Aware Deep Convolutional Networks (EA-ConvNets), in detail."}, {"heading": "3.1 Problem Definition", "text": "We will denote a univariate time series (a single sequence) as T = {t1, t2, ..., tL}, where ti is a scalar measurement\u2217 of this time series at timestamp i, L is the total number of timestamps for this time series. Following this notation, we denote a collection of time series as D = {(Ti, yi)}Ni=1, where N is the number of time series in the dataset, Ti = {t1i , t2i , ..., tLi } represents the ith time series and yi is the associated label. The element tji indicates the jth timestamp value in the ith time series. For simplicity of exposition, we will assume all the time series in D to have identical timestamps, though our framework does not require this.\nThe task in time series classification (TSC) is to build a classifier to predict the class label y\u2217 of a new time series T\u2217, given past training data D. The time series in the training set as well as in the test data are assumed to be fully observed.\nIn contrast, ETSC requires making prediction when the time series may only be partially observed (i.e., upto a certain timestamp l < L) in the training and/or the test data. In these settings, it is desirable to predict class labels as early as possible, while retaining an acceptable level of accuracy."}, {"heading": "3.2 Earliness-Aware ConvNets", "text": "We present a deep feature learning framework for early time series classification by building a multi-scale convolutional neural network (CNN) architecture, which is coupled with a time series truncation model that allows focusing on early parts of each time series. The multi-scale CNN enables extracting features at multiple scales from the time series data. In the context of time series data, the extracted features correspond to shapelets [38] which are subsequences with high discriminative power. Unlike most other existing methods that extract shapelets from time series data using hand-crafted rules (which is often ad-hoc, tedious, and timeconsuming), a distinguishing aspect of our framework is that the shapelets are learned in a purely data-driven manner. Although some recent methods have looked at ways of learning shapelets that are optimized for a given data set [15], unlike our approach which learns a deep representation of the shapelets, these methods can only learn a shallow representation, which consequently are less expressive and have less discriminative power.\nWe call our propose framework Earliness-Aware Deep Convolutional Networks (EA-ConvNets). The EAConvNets takes as input a raw time series and outputs the class label of that time series. The overall architecture of EA-ConvNets is shown in Figure 1, which consists of three stages: Early-Awareness, deep convolutional feature (shapelet) extractor, and the final classifier, each of which is described below. \u2217Extending to the vector-value setting is straightforward in our framework."}, {"heading": "3.2.1 Early-Awareness", "text": "The Early-Awareness component lets EA-ConvNets to focus on the early stage of each time series. To accomplish Earliness-Aware, we draw inspiration from the idea of nested dropout [27], which was originally proposed to stochastically drop coherent nested sets of hidden units in a neural network by drawing a decay point from some decay distribution. We leverage a similar strategy to stochastically choose a truncation point for each time series, which gives rise to a new truncated version of the original time series (with the same label). To select the truncation point, a prior distribution P (\u00b7) on the timestamps (length) of the time series is employed. For each time series Ti, we first sample an index s \u223c P (\u00b7), generate the stochastically truncated time series as Ti\u2193s = {t1i , \u00b7 \u00b7 \u00b7 , tsi}, which then forms the input for the subsequent convolutional neural networks based deep shaplet detector module. For choosing the truncation point, as also done in [27], we use a geometric distribution because of its properties of exponential decay and memorylessness. The geometric distribution is parametrized as\nP (s; \u03c1) = \u03c1s\u22121(1\u2212 \u03c1) . An illustration of applying a geometric distribution based truncation of the original full-length time series is shown in the left sub-figure in Fig 1. Specifically, we refer to the process of drawing a truncation point s from P (\u00b7), and truncating the time series at s as s-truncation; the s-truncation of a time series T is denoted as T\u2193s."}, {"heading": "3.2.2 Deep Convolutional Feature (Shapelet) Extractor", "text": "The deep feature extraction stage of our framework is based on a deep convolutional neural network architecture, which takes as input the truncated time series obtained from the Earliness-Aware stage, extracts a hierarchy of shapelet based features for each time series, and passes these on to the final classifier.\nIn order to extract the deep, shapelet based features, we apply independent 1D convolutions over the (truncated) time series. Since the time series data may exhibit latent features at multiple scales, a single filter of fixed length may not be able to capture this. Therefore, to capture the multi-scale characteristics of the time series, we apply three channels of ConvNets in parallel. Each channel of the ConvNets has a different filter size, which is fixed within each ConvNet.\nIn our experiments, we found that choosing the filter sizes from {3%, 5%, 10%} of L performs well. The specific architecture is shown in Fig 1, with different filters shown in different colors. Applying each filter\non the (truncated) time series gives as output a sequence of real values, each of which can be viewed as the similarity of a local subsequence of the time series with the applied filters. A larger filter size means a wider receptive field, capturing a larger scale of information.\nAfter the convolutional operation, down-sampling is applied to reduce the output dimensions. Two types of down-sampling strategies are commonly used: average-pooling and max-pooling. In our experiments, we found that max-pooling produces better results and also leads to faster training. As a result, we adopt the max-pooling without overlapping. In addition to reducing the output dimension, down-sampling also has other benefits, such as capturing scale-invariant features and reducing model complexity to avoid overfitting.\nWe would also like to note that our EA-ConvNets framework is adaptive to dynamic timestamps. Denoting the output feature map of the last convolutional layer to be O(T) = {O1(T), O2(T), ..., OM (T)}, where M is the number of filters in the layer. Note that the feature map Oi(T) itself is vector-valued (representing another time-series in a deep feature space). From the convolutional property, in the output feature map vector, the one with the largest value contains the most significant information. Collecting all these maximum values from the feature maps O(T) constitutes the final layer shapelet feature representation, F (T), for the time-series T. This is essentially the max-over-time method described in [39], with the ith element of F (T) defined as\nFi(T) = max j Oi(T)[j]\nOur feature extractor can be viewed as a hierarchical feature extractor, aiming for extracting high level abstract features, which correspond to shapelets learned in a purely data-driven fashion. To maintain the independence of each filter and avoid overfitting, we employ SpatialDropout\u2020 (originally proposed in [31]) on F (T). Our experiments demonstrate the effectiveness of the regularization achieved by SpatialDropout."}, {"heading": "3.2.3 The Final Classifier", "text": "For the final classifier, we concatenate all the shapelet based features from the outputs of the three ConvNets (one for each filter), and feed it into the final output layer, followed by a softmax transformation. The output of EA-ConvNets is the predictive distribution of each possible label for the input time series. The objective function is to minimize the cross-entropy loss defined as\nL(W) = \u2212 N\u2211 i=1 log pyi(Ti)\nwhere W represents all the model parameters (e.g., the weights of the deep network), pyi(Ti) is the probability of Ti having the true label yi, which is easily calculated by forwarding the input Ti thought the network (line 6 in Algorithm 1). Since all the operations in the objective function are differentiable, all the model parameters can be optimized jointly through backpropagation.\nThe full algorithm is shown in Algorithm 1. In the algorithm, Forward(B\u2217) in line 8 denotes the standard feedforward pass of a deep neural network, which takes a minibatch as input, and returns the loss on the corresponding minibatch. Backprop(L, y) in line 9 is the standard backpropagation pass, which takes the loss and labels of a minibatch as input, calculates the gradients of model parameters, and then update the parameters via a stochastic optimization algorithm, e.g., stochastic gradient descent. For the cross-validation in line 10, we propose a method to emphasize early-stage classification accuracy, with details provided in Section 5.2 in the experiments.\n\u2020Applying DropOut on the convolutional layers is also possible, but it typically increases the computation burden without significantly improving the results [17], we thus did not consider this setting.\nAlgorithm 1 EA-ConvNets in pseudo-code. 1: Input: D = {Ti, yi}Ni=1; parameter of P (\u00b7) to be \u03c1 2: Output: weights W of EA-ConvNets 3: Initialize weights W of EA-ConvNets with uniform distribution range from [\u22120.5, 0.5] 4: for iteration do 5: Sample mini-batch B = {Tj , yj}nj=1 from D 6: Sample a set of truncation variables s = {s1, \u00b7 \u00b7 \u00b7 , sn} for the current mini-batch, where sj \u223c P (\u03c1) 7: Get the s-truncation mini-batch B\u2217 = {T\u2193sjj , yj}nj=1 8: L = Forward(B\u2217) 9: Backprop(L, {yj}nj=1) 10: Evaluate on a validation set with EA-ConvNets 11: if early-stop criteria satisfied then 12: Break; 13: end if 14: end for 15: Return W"}, {"heading": "4 Related Work", "text": "Here we review the related work in two directions relevant to this work: algorithms for early time series classification (ETSC) and deep learning methods for time series classification (TSC). ETSC was arguably first proposed in [10] and developed along several independent lines, where classifiers make predictions from partial observations of temporal data.\nSome of the early work on ETSC is based on ensemble learning methods. In particular, [24, 32] use boosting to facilitate a chronological restriction of features to achieve early recognition. More recently, [1] proposed an ensemble of classifiers learned on subsequences of different lengths, and the prediction is made by the earliest individual classifier. Among other recent work, [2] learns the ensemble based early classifier by minimizing an empirical risk function and the response time required to achieve the minimum risk, which is formulized as a quadratic programming problem and solved by an iterative constraint generation algorithm.\nAnother prominent line of work is based on instance-based learning methods. In particular, [35] proposed ETCS (Early Classification on Time Series), which introduced the notion of Minimum Prediction Length (MPL), the earliest timestamp for each time series in the training data to find the correct nearest neighbor, and searches all MPLs in a hierarchical fashion. At test time, for a given time series, we predict its label as that of its nearest neighbor with MPL.\nInspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time. These methods predict a label for a test instance when a matching time series is found in the shaplets library. Further, [20] generalizes these methods to the multivariate setting with both numerical and categorical features. Another recent interesting approach is MD-MDPP+TD [19], which models multivariate time series as a Multivariate Marked Point-Process (Multi-MPP) and leverages the sequential cues as the temporal patterns to make early classification.\nAnother class of methods for ETCS are based on learning a classifier and a detector simultaneously. The former aims to make real-time classifications while the latter is used to decide which result can be trusted and the label will be assigned subsequently. For example, [14] combines hidden markov models (HMM) and support vector machines (SVM) for early classification and set up a threshold on the prediction probability to ensure reliability. [26] proposes a local quadratic discriminant analysis (QDA) based approach. Its reliability is guaranteed by the\nthreshold on the similarity of prediction on truncated and complete time series. [8] introduces a formal criterion for the detector to express the trade-off between earliness and accuracy.\nAmong other recent work, [33] utilizes the structure characteristic of the long short term memory (LSTM) to do early recognition during testing time. More recently, [25] proposes a probability based classifier, which learns timestamps in which the prediction accuracy for each class surpass a user defined threshold. At test time, a decision will be made no early than the learned timestamps and reliability is controlled by the differences between the top two probabilities.\nFor TSC, shaplets based methods have received a significant attention recently [15][29][18]. Since, shaplets can be viewed as a special case of features learned by ConvNets [7], ConvNets to TSC is a promosing direction. Recently, [40] explored multi-channel CNN to deal with multivariate time series. Features are extracted through a parallel channel and then combined via an ensemble. In another recent work, [7] proposed a generalized CNN capable of extracting multi-scale characteristics in time series data and achieves state-of-the-art performance on multiple benchmark datasets. In contrast, ConvNets based models for early classification of time series have not been explored before and the EA-ConvNets framework proposed in our work is a first attempt in this direction."}, {"heading": "5 Experiments", "text": "We evaluate the effectiveness of EA-ConvNets on 12 publicly available benchmark data sets. In the following, we give a brief description of the data sets and the experimental settings, and then conduct the following experiments\n\u2022 Evaluating our model for the task of early time series classification by comparing with several state-ofthe-art methods for this problem.\n\u2022 Comparing EA-ConvNets with state-of-the-art algorithms for fully observed time series data, and show that our model is also competitive with these algorithms in terms of classification accuracies.\n\u2022 Visualizing the learned features (shapelets) in each of the layers of the deep architecture.\n\u2022 Assessing the sensitivity of EA-ConvNets to the hyper-parameters.\n\u2022 Comparing EA-Convnets with the setting of training multiple ConvNets, where each ConvNet is trained only on the data with a particular truncated level."}, {"heading": "5.1 Datasets", "text": "We select 12 benchmark data sets from the UCR time series archive [5], with time series having a variety of lengths. Our data sets include: ADIAC, FISH, GUN POINT, ITALYPOWERDEMAND, SYNTHETIC CONTROL, TRACE, CRICKET-X, CRICKET-Y, CRICKET-Z, TWO PATTERNS, NON-INVASIVE FETAL ECG THORAX1(NONINVTHORAX1) and NON-INVASIVE FETAL ECG THORAX2(NONINVTHORAX2). Some statistics of the data sets are listed in Table 1. Our experiments use the default training and test splits provided by UCR."}, {"heading": "5.2 Experimental Setting", "text": "For consistency, we conduct all the experiments on all the data sets using the same architecture as shown in Fig 1. The number of filters is set to {48, 48, 96} for each layer in each channel of the networks, and is the same\nacross the three channel of the network. As a result, the final feature representation has a size 96 \u00d7 3 = 288. ReLU is adopted as the nonlinear activation function after each hidden layer. EA-ConvNets is implemented using Torch7 [6] and trained on NVIDIA GTA TITAN graphics cards with 6GB RAM. We optimize the model with RMSprop [30] using minibatches of size 50. We use 5-fold cross-validation for hyperparameter tunning.\nSpecifically, the hyperparameters of EA-ConvNets include the value of geometric distribution hyperparameter \u03c1, pooling factor chosen from among {2, 3, 5}, SpatialDropout rate chosen from among {0.4, 0.5, 0.6} and L2 weight decay chosen from {0, 10\u22126, 10\u22125, 10\u22124}. We apply early stopping to avoid overfitting. We use \u201cAUC\u201d on the validation set as the criteria to determine when to early stop. The\u201cAUC\u201d (not the usual area under the ROC curve) indicates the area under a curve, plotting the trend of average truncation length vs accuracy. Early stopping is triggered when the maximum \u201cAUC\u201d on validation sets maintains for a number of epochs.\nAlso note that, unlike several existing time series classification methods, we use raw time series as input and do not use any other data augmentation, except for the stochastic truncation. The automatic feature learning capability of our deep model allows it to learn the best features for the task."}, {"heading": "5.3 Baselines", "text": "We conduct two types of experiment: (1) early time series classification (ETSC), and (2) fully observed time series classification (TSC). For the former, we compare EA-ConvNets against the following state-of-the-art baselines:\n\u2022 Early classification on time series (ECTS)[35]\n\u2022 Early Distinctive Shaplet Classification (EDSC) [36]\n\u2022 Reliable early classification method (RelClass) [26]\n\u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25]\n\u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).\nNote that our baselines also include methods that can learn shapelets from data (EDSC), just like EA-ConvNets (which however can learn a deep feature representation of the shapelets).\nThe goal of the latter experiment is to show that EA-ConvNets is able to do early classification without compromising much on the classification accuracies as compared to methods that can work with fully observed time series. For this experiment, we compare EA-ConvNets against the following state-of-the-art baselines for fully-observed time series:\n\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11]\n\u2022 1NN with Dynamic Time Warping (DTW) [4]\n\u2022 Bag-of-SFA-Symbols (BOSS) [28]\n\u2022 Elastic Ensemble (PROP) [21]\n\u2022 Learning Shapelets Models(LTS) [15]\n\u2022 COTE [3], a weighted vote of over 35 classifiers\n\u2022 Multi-scale convolutional nerual networks (MCNN)[7]"}, {"heading": "5.4 Early Time Series Classification", "text": "In this section, we compare EA-ConvNets with several state-of-the-art baseline methods for early time series classification. To assess the robustness of all the methods on short time series data, for each data set, we also evaluate each method on varying lengths of the input time series (Fig 2).\nFor ECTS [35], we set the parameter minimum support from 0 to 1 with the dense interval to be 0.05. For EDSC [36], we adopt the version based on Chebyshev Inequality and set the bound for Chebyshev condition to be 2.5, 3 and 3.5, which has been reported to be efficient by the authors. For RelClass [26], we choose the values for the reliability threshold from among {10\u221230, 10\u221225, 10\u221220, 10\u221215, 10\u221210, 10\u22125, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 0.25, 0.50, 0.75, 0.90, 0.99} and for each individual test, the reliability threshold is calculated with Gaussian Naive Bayes box method. In contrast, for EA-ConvNets, a range of classification thresholds is set for the output of the SoftMax layer in EA-ConvNet. For a particular threshold, if the highest confidence score (the output of the SoftMax layer) surpasses the value, we accept the prediction for the current time series (thus can determine if the time series is classified correctly, and in the end calculate the average length under this threshold); otherwise we proceed until next available timestamps. The overall results are plotted in Fig 2, which show that.\n1. For all fractions of the time series given as input to each method, EA-ConvNets outperforms the other baselines on all the data sets by a significantly large margin, except for NONINVTHORAX1 and NONINVTHORAX2 data sets, on which the ECDIRE baseline is comparable to EA-ConvNets. This demonstrates the robustness and generality of EA-ConvNets.\n2. Even when the full time series is given, EA-ConvNets is competitive with EA-ConvNets-Full (which does not perform stochastic truncation and thus does not have the early-awareness). This indicates that the earliness aspect of EA-ConvNets does not compromise on the classification accuracies.\n3. It is also worthwhile to note that the variance of EA-ConvNets is much smaller than that of EA-ConvNetsFull. Therefore, earliness-aware can also be viewed as a robust regularizor (akin to a \u201cdropout\u201d mechanism).\n4. EA-ConvNets is particularly effective at extreamly early classification. This is evidenced especially from the high classification accuracies even when only 20%-40% of the time series is given as the input to each of the baselines and EA-ConvNets."}, {"heading": "5.5 Comparison with (Full) Time Series Classification Methods", "text": "We also compare EA-ConvNets with several state-of-the-art baselines for full time series classification (TSC). The results are shown in Table 2. None of the previous work on has compared ETSC classifiers with TSC classifiers under full observations, due to the fact that ETSC trades off accuracies for early classification with full classification\u2021. To the best of our knowledge, we are the first to conduct such experiments, to investigate the extent of the gap between ETSC and TSC algorithms. As can be seen in Table 2, EA-ConvNets with full observations outperforms most TSC baselines on multiple datasets, and is mostly competitive with state-of-theart models. For some datasets, though EA-ConvNets does not outperform the state-of-the-art baselines, the gaps are marginal. We would like to stress that EA-ConvNets is not designed for full time-series classification tasks, though a comparable performance could be achieved as compared to state-of-the-art TSC baselines. Therefore, EA-ConvNets is practically a reliable model for anytime time-series classification."}, {"heading": "5.6 Qualitative Results: Deep Shapelet Discovery", "text": "An appealing aspect of our framework is its ability to learn a deep feature representation of the time series data in form of a hierarchy of shapelets (highly discriminative subsequences) automatically from data. In addition to being highly discriminative features, shapelets can also be useful in understanding/visualizing the characteristics of the underlying time series and can be useful in other decision-making tasks (e.g., in medical diagnosis). To show the interpretability of the shapelets learned by EA-ConvNets, we visualize the learned shapelets at each stage of the deep architecture, using data set TRACE as an example.\nWe visualize the learned filters across all the three channels of the network. To visualize the feature maps of a given time series at each stage, we only plot those in the first channel for ease of visualization. Feature maps in the other two channels show similar patterns as the first channel, and are now shown here. Fig 3 shows the results. \u2021As a result, ETSC usually performs worse than TSC under full observations.\nIt is remarkable to note that our EA-ConvNets can automatically learn different layers of features (shapelets) simultaneously, as shown in the first layer of the networks. These features can be either low or high frequency patterns, or a mixture, depending on the data. By comparing the output feature maps of each layer, we notice that the deeper the layer goes, the more abstract features it can learn, which is typically a group of sparse and spiked features. It is also interesting to note that, for each feature map (either a sparse vector or a vector with spike in Figure 3) in the final layer, the spike (corresponding to a spike in the final feature representation) usually appears at the beginning of the feature map. This indicates that the highly discriminative features are extracted in the early stages of a time series, which provides a qualitative evidence of the effectiveness of EA-ConvNets. Finally, as shown in the figure, the final feature representation for different classes distribute discriminatively. Therefore, EA-ConvNets is a powerful model to capture robust, highly interpretable, deep features (which correspond to a deep hierarchy of shapelets) to make early, reliable predictions."}, {"heading": "5.7 Sensitivity to hyperparameters", "text": "EA-ConvNets includes a numbers of hyperparameters. Among them, the pooling factor, SpatialDropout rate and L2 weight decay, affect the convergence of the training algorithm. The hyperparameter, \u03c1, in the geometric distribution for data augmentation, not only affects the convergence of the training algorithm, but also controls the degree of earliness-aware of our model. Therefore, in this section, we mainly analyze the sensitivity of our model to \u03c1. We use FISH, SYNTHETIC CONTROL, CRICKET-X and NONINVTHORAX1 data sets to show the effect of this hyperparameter. The results of EA-ConvNets on these datasets for different values of \u03c1 are shown in Fig 4. As we can see, the effect of \u03c1 varies across different data sets. On FISH, the performance is sensitive to \u03c1. With an increase in \u03c1, EA-ConvNets results in higher accuracies across the board for various fractions of the entire time series. The story is slightly different on the SYNTHETIC CONTROL and CRICKET-X data sets, where with an increase on \u03c1, the curves converge to a higher accuracies under full observations but the accuracies are much lower when the fraction of observed time-series is small. This is because the larger \u03c1 is, the higher the probability is that EA-ConvNets will observe most of the later time steps of the time series. As a result, the model would move its awareness towards the later parts of the time series. There is a trade-off between the accuracy and the earliness-aware when \u03c1 increases. On the other hand, on NONINVTHORAX1, for a wide range of values of \u03c1, the accuracies are roughly the same. This can be possibly because the highly discriminative features are present in the early stage for particular data set, and are captured effectively by EA-ConvNets. To summarize, although EA-ConvNets relies on a good choice of this hyperparameter, for a sufficiently broad range of value of \u03c1, our model yields reasonable results. Nevertheless, we recommend using cross validation to select an appropriate \u03c1 for different data sets."}, {"heading": "5.8 Comparison with Ensemble of Multiple Models", "text": "To further evaluate the reliability of EA-ConvNets, we compare it with a combination (ensemble) of multiple models, each trained by truncating the original time series to a fixed length. Specifically, we compare EAConvNets with a group of deep learning models sharing the same architecture as EA-ConvNets but having a fixed truncation length (Fixed EA-ConvNets) and multiple kNN classifiers with fixed truncation length (Fixed 1-NN). For each truncation length, we train independent models and use them at test time to predict the label of a test time series having the same truncation length. We conduct experiments on four data sets: FISH, CRICKET-X, TWO PATTERNS and NONINVTHORAX1. Fig 5 shows the results.\nIt is interesting to see that on data set TWO PATTERNS, our EA-ConvNest outperforms both Fixed EA-ConvNets and Fixed 1-NN by a significant margin. The reasons may be attribute to: (1) the dynamic adaptation of EA-ConvNets. EA-ConvNets performs prediction adaptively, based on a threshold on prediction confidence; while for the ensemble of models, each model is restricted to predict based on a particular truncation length,\nirrespective to specific data sets; and (2) the ability to deal with noise in the time series with EA-ConvNets. When the time series data contains noise, there is no guarantee for higher prediction accuracy with longer time series length. Our EA-ConvNets provides a proper way to deal with the noise by making early predictions with a reasonable prediction confidence. This is especially reflected in the results on NONINVTHORAX1, where EA-ConvNets performs consistently similar to Fixed EA-ConvNets, with the accuracies for both saturating after roughly 20% of the observed time-series length. Note although EA-ConvNets is a little weaker than the corresponding multiple models on the CRICKET-X dataset, the gap is very marginal."}, {"heading": "6 Conclusion", "text": "We have introduced EA-ConvNets, a deep model based on a multi-scale convolutional neural network architecture for early classification of time series data. EA-ConvNets leverages the information at different scales and captures the interpretable features (shapelets) at a very early stages, which also makes it a robust model when each time series is inherently of a short length. Our experiments indicate that EA-ConvNets yields lower or comparable test errors given a pre-defined observed length budget at test time, while maintaining comparable classification accuracies to the state-of-the-art time series classification models that use full time series. To interpret the effectiveness of EA-ConvNets, we present the learned local features (shapelets) and visualize the nonlinear feature representations in each layers of the deep model. Our experiments clearly show that the learned feature representation is highly discriminative and can achieve effective early classification.\nThere are a number of interesting future directions. One direction is to extend our earliness-aware framework using other neural models such as LSTMs, which are natural tools for deal with time series data. Moreover, although in this paper we only considered EA-ConvNets for univariate time series, the framework can be extended to multivariate time series data sets. Finally, our framework is not only limited to classification tasks, and can be extended to more general (early) time series prediction tasks."}], "references": [{"title": "Time-sensitive classification of behavioral data", "author": ["Shin Ando", "Einoshin Suzuki"], "venue": "In SDM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Minimizing response time in time series classification", "author": ["Shin Ando", "Einoshin Suzuki"], "venue": "Knowledge and Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Time-series classification with cote: the collective of transformation-based ensembles", "author": ["Anthony Bagnall", "Jason Lines", "Jon Hills", "Aaron Bostrom"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Using dynamic time warping to find patterns in time series", "author": ["Donald J Berndt", "James Clifford"], "venue": "In KDD workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "The ucr time series classification", "author": ["Yanping Chen", "Eamonn Keogh", "Bing Hu", "Nurjahan Begum", "Anthony Bagnall", "Abdullah Mueen", "Gustavo Batista"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Multi-scale convolutional neural networks for time series classification", "author": ["Zhicheng Cui", "Wenlin Chen", "Yixin Chen"], "venue": "arXiv preprint arXiv:1603.06995,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Early classification of time series as a non myopic sequential decision making problem", "author": ["Asma Dachraoui", "Alexis Bondu", "Antoine Cornu\u00e9jols"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Early classification of network traffic through multi-classification", "author": ["Alberto Dainotti", "Antonio Pescap\u00e9", "Carlo Sansone"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Boosting interval-based literals: Variable length and early classification. Data Mining in Time Series Databases, page", "author": ["Grupo de Sistemas Inteligentes"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Fast subsequence matching in time-series databases, volume 23", "author": ["Christos Faloutsos", "Mudumbai Ranganathan", "Yannis Manolopoulos"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Extraction of interpretable multivariate patterns for early diagnostics", "author": ["Mohamed F Ghalwash", "Vladan Radosavljevic", "Zoran Obradovic"], "venue": "In Data Mining (ICDM),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Utilizing temporal patterns for estimating uncertainty in interpretable early decision making", "author": ["Mohamed F Ghalwash", "Vladan Radosavljevic", "Zoran Obradovic"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Early classification of multivariate time series using a hybrid hmm/svm model", "author": ["Mohamed F Ghalwash", "Du\u0161an Ramljak", "Zoran Obradovi\u0107"], "venue": "In Bioinformatics and Biomedicine (BIBM),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Learning time-series shapelets", "author": ["Josif Grabocka", "Nicolas Schilling", "Martin Wistuba", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Early classification on multivariate time series", "author": ["Guoliang He", "Yong Duan", "Rong Peng", "Xiaoyuan Jing", "Tieyun Qian", "Lingling Wang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Efficient learning of timeseries shapelets", "author": ["Lu Hou James T Kwok", "Jacek M Zurada"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Early classification of ongoing observation", "author": ["Kang Li", "Sheng Li", "Yun Fu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Reliable early classification on multivariate time series with numerical and categorical attributes", "author": ["Yu-Feng Lin", "Hsuan-Hsu Chen", "Vincent S Tseng", "Jian Pei"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Time series classification with ensembles of elastic distance measures", "author": ["Jason Lines", "Anthony Bagnall"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "An integrated data mining approach to real-time clinical monitoring and deterioration warning", "author": ["Yi Mao", "Wenlin Chen", "Yixin Chen", "Chenyang Lu", "Marin Kollef", "Thomas Bailey"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Identifying predictive multi-dimensional time series motifs: an application to severe weather prediction", "author": ["Amy McGovern", "Derek H Rosendahl", "Rodger A Brown", "Kelvin K Droegemeier"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Early recognition and prediction of gestures", "author": ["Akihiro Mori", "Seiichi Uchida", "Ryo Kurazume", "Rin-ichiro Taniguchi", "Tsutomu Hasegawa", "Hiroaki Sakoe"], "venue": "In Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Reliable early classification of time series based on discriminating the classes over time", "author": ["Usue Mori", "Alexander Mendiburu", "Eamonn Keogh", "Jose A Lozano"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Classifying with confidence from incomplete information", "author": ["Nathan Parrish", "Hyrum S Anderson", "Maya R Gupta", "Dun Yu Hsiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Learning ordered representations with nested dropout", "author": ["Oren Rippel", "Michael A Gelbart", "Ryan P Adams"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "The boss is concerned with time series classification in the presence of noise", "author": ["Patrick Sch\u00e4fer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Learning dtwshapelets for time-series classification", "author": ["Mit Shah", "Josif Grabocka", "Nicolas Schilling", "Martin Wistuba", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the 3rd IKDD Conference on Data Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. E Hinton"], "venue": "Technical report, Coursera: Neural Networks for Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Efficient object localization using convolutional networks", "author": ["Jonathan Tompson", "Ross Goroshin", "Arjun Jain", "Yann LeCun", "Christoph Bregler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Early recognition of sequential patterns by classifier combination", "author": ["Seiichi Uchida", "Kazuma Amamoto"], "venue": "In ICPR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Lstm-based early recognition of motion patterns", "author": ["Markus Weber", "Marcus Liwicki", "Didier Stricker", "Christopher Scholzel", "Seiichi Uchida"], "venue": "In ICPR. IEEE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "A brief survey on sequence classification", "author": ["Zhengzheng Xing", "Jian Pei", "Eamonn Keogh"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Early classification on time series", "author": ["Zhengzheng Xing", "Jian Pei", "S Yu Philip"], "venue": "Knowledge and information systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Extracting interpretable features for early classification on time series", "author": ["Zhengzheng Xing", "Jian Pei", "S Yu Philip", "Ke Wang"], "venue": "In SDM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["Jian Bo Yang", "Minh Nhut Nguyen", "Phyo Phyo San", "Xiao Li Li", "Shonali Krishnaswamy"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["Lexiang Ye", "Eamonn Keogh"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Time series classification using multichannels deep convolutional neural networks", "author": ["Yi Zheng", "Qi Liu", "Enhong Chen", "Yong Ge", "J Leon Zhao"], "venue": "In Web-Age Information Management,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 33, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 8, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 21, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 13, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 22, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 37, "context": "Unfortunately, most existing methods for this problem are usually based on ad-hoc ways of defining subsequences or hand-crafted patterns [38], mapping them to fixed sized vectors, and subsequently applying a classification method (e.", "startOffset": 137, "endOffset": 141}, {"referenceID": 37, "context": "Shapelets [38] based representations have recently emerged as an effective way to model time series data in problems such as time series classification.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Some recent work has tried learning shapelets from data [15] but the learned shapelets are usually not ideal for early time series classification.", "startOffset": 56, "endOffset": 60}, {"referenceID": 35, "context": "As compared to the stateof-the-art methods for that can learn interpretable features for ETSC problems [36], the deep shapelet based representations learned using our framework, in a purely data-driven manner, can capture patterns in the time series at multiple levels of granularities.", "startOffset": 103, "endOffset": 107}, {"referenceID": 36, "context": "We first provide a brief review of a basic CNN for time series data [37], before describing the details of our proposed framework.", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "In the context of time series data, the extracted features correspond to shapelets [38] which are subsequences with high discriminative power.", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "Although some recent methods have looked at ways of learning shapelets that are optimized for a given data set [15], unlike our approach which learns a deep representation of the shapelets, these methods can only learn a shallow representation, which consequently are less expressive and have less discriminative power.", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "To accomplish Earliness-Aware, we draw inspiration from the idea of nested dropout [27], which was originally proposed to stochastically drop coherent nested sets of hidden units in a neural network by drawing a decay point from some decay distribution.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "For choosing the truncation point, as also done in [27], we use a geometric distribution because of its properties of exponential decay and memorylessness.", "startOffset": 51, "endOffset": 55}, {"referenceID": 38, "context": "This is essentially the max-over-time method described in [39], with the ith element of F (T) defined as", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "To maintain the independence of each filter and avoid overfitting, we employ SpatialDropout\u2020 (originally proposed in [31]) on F (T).", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "\u2020Applying DropOut on the convolutional layers is also possible, but it typically increases the computation burden without significantly improving the results [17], we thus did not consider this setting.", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "ETSC was arguably first proposed in [10] and developed along several independent lines, where classifiers make predictions from partial observations of temporal data.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "In particular, [24, 32] use boosting to facilitate a chronological restriction of features to achieve early recognition.", "startOffset": 15, "endOffset": 23}, {"referenceID": 31, "context": "In particular, [24, 32] use boosting to facilitate a chronological restriction of features to achieve early recognition.", "startOffset": 15, "endOffset": 23}, {"referenceID": 0, "context": "More recently, [1] proposed an ensemble of classifiers learned on subsequences of different lengths, and the prediction is made by the earliest individual classifier.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Among other recent work, [2] learns the ensemble based early classifier by minimizing an empirical risk function and the response time required to achieve the minimum risk, which is formulized as a quadratic programming problem and solved by an iterative constraint generation algorithm.", "startOffset": 25, "endOffset": 28}, {"referenceID": 34, "context": "In particular, [35] proposed ETCS (Early Classification on Time Series), which introduced the notion of Minimum Prediction Length (MPL), the earliest timestamp for each time series in the training data to find the correct nearest neighbor, and searches all MPLs in a hierarchical fashion.", "startOffset": 15, "endOffset": 19}, {"referenceID": 37, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 33, "endOffset": 37}, {"referenceID": 35, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "Further, [20] generalizes these methods to the multivariate setting with both numerical and categorical features.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "Another recent interesting approach is MD-MDPP+TD [19], which models multivariate time series as a Multivariate Marked Point-Process (Multi-MPP) and leverages the sequential cues as the temporal patterns to make early classification.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "For example, [14] combines hidden markov models (HMM) and support vector machines (SVM) for early classification and set up a threshold on the prediction probability to ensure reliability.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "[26] proposes a local quadratic discriminant analysis (QDA) based approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] introduces a formal criterion for the detector to express the trade-off between earliness and accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "Among other recent work, [33] utilizes the structure characteristic of the long short term memory (LSTM) to do early recognition during testing time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "More recently, [25] proposes a probability based classifier, which learns timestamps in which the prediction accuracy for each class surpass a user defined threshold.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "For TSC, shaplets based methods have received a significant attention recently [15][29][18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "For TSC, shaplets based methods have received a significant attention recently [15][29][18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "For TSC, shaplets based methods have received a significant attention recently [15][29][18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "Since, shaplets can be viewed as a special case of features learned by ConvNets [7], ConvNets to TSC is a promosing direction.", "startOffset": 80, "endOffset": 83}, {"referenceID": 39, "context": "Recently, [40] explored multi-channel CNN to deal with multivariate time series.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "In another recent work, [7] proposed a generalized CNN capable of extracting multi-scale characteristics in time series data and achieves state-of-the-art performance on multiple benchmark datasets.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "We select 12 benchmark data sets from the UCR time series archive [5], with time series having a variety of lengths.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "EA-ConvNets is implemented using Torch7 [6] and trained on NVIDIA GTA TITAN graphics cards with 6GB RAM.", "startOffset": 40, "endOffset": 43}, {"referenceID": 29, "context": "We optimize the model with RMSprop [30] using minibatches of size 50.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 44, "endOffset": 48}, {"referenceID": 35, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 154, "endOffset": 158}, {"referenceID": 24, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 291, "endOffset": 295}, {"referenceID": 10, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 127, "endOffset": 130}, {"referenceID": 27, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 228, "endOffset": 232}, {"referenceID": 2, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 240, "endOffset": 243}, {"referenceID": 6, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 334, "endOffset": 337}, {"referenceID": 34, "context": "For ECTS [35], we set the parameter minimum support from 0 to 1 with the dense interval to be 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "For EDSC [36], we adopt the version based on Chebyshev Inequality and set the bound for Chebyshev condition to be 2.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "For RelClass [26], we choose the values for the reliability threshold from among {10\u221230, 10\u221225, 10\u221220, 10\u221215, 10\u221210, 10\u22125, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 0.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an end-to-end deep learning framework, for early classification of time series data. Unlike most existing methods for early classification of time series data, that are designed to solve this problem under the assumption of the availability of a good set of pre-defined (often hand-crafted) features, our framework can jointly perform feature learning (by learning a deep hierarchy of shapelets capturing the salient characteristics in each time series), along with a dynamic truncation model to help our deep feature learning architecture focus on the early parts of each time series. Consequently, our framework is able to make highly reliable early predictions, outperforming various state-of-the-art methods for early time series classification, while also being competitive when compared to the state-of-the-art time series classification algorithms that work with fully observed time series data. To the best of our knowledge, the proposed framework is the first to perform data-driven (deep) feature learning in the context of early classification of time series data. We perform a comprehensive set of experiments, on several benchmark data sets, which demonstrate that our method yields significantly better predictions than various state-of-the-art methods designed for early time series classification. In addition to obtaining high accuracies, our experiments also show that the learned deep shapelets based features are also highly interpretable and can help gain better understanding of the underlying characteristics of time series data.", "creator": "LaTeX with hyperref package"}}}