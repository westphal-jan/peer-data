{"id": "1611.04953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "End-to-End Neural Sentence Ordering Using Pointer Network", "abstract": "sentence similarity ordering is becoming one system of important main tasks in nlp. arguably previous seminal works previously mainly focused on improving at its performance considerably by using common pair - wise strategy. however, it typically is substantially nontrivial for many pair - wise web models to initially incorporate the contextual sentence information. in addition, error prorogation could be weakly introduced by using the pipeline strategy stored in pair - wise models. in designing this paper, theoretically we both propose an end - segment to - wire end collaborative neural algorithms approach solution to address overcome the sentence ordering suppression problem, formulation which uses the context pointer network ( ptr - net ) device to alleviate almost the original error citation propagation ordering problem and utilize the resultant whole contextual textual information. experimental results however show exemplary the effectiveness of the proposed model. source codes and dataset packages of text this paper are available.", "histories": [["v1", "Tue, 15 Nov 2016 17:38:10 GMT  (114kb,D)", "http://arxiv.org/abs/1611.04953v1", null], ["v2", "Fri, 25 Nov 2016 16:38:30 GMT  (116kb,D)", "http://arxiv.org/abs/1611.04953v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jingjing gong", "xinchi chen", "xipeng qiu", "xuanjing huang"], "accepted": false, "id": "1611.04953"}, "pdf": {"name": "1611.04953.pdf", "metadata": {"source": "CRF", "title": "End-to-End Neural Sentence Ordering Using Pointer Network", "authors": ["Jingjing Gong", "Xinchi Chen", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["xjhuang}@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Recently, sentence ordering task attracts more focus in NLP community (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc.\nThe goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008).\nMost of previous researches of sentence ordering are pair-wise models. Chen et al. (2016) and Agrawal et al. (2016) proposed pair-wise models\n1https://github.com/fudannlp 2http://nlp.fudan.edu.cn/data/ \u2217Jingjing Gong and Xinchi Chen contributed equally to this\nwork.\nfollowed by a beam search decoder to seek the ground truth sentence order. Li and Jurafsky (2016) additionally employed the graph based method (Lapata, 2003) to rank sentences. However, their methods could be easily affected by the performance of independent sentence pairs without their contextual sentences.\nIn this paper, we propose an end-to-end neural sentence ordering approach based on the pointer network (Ptr-Net) (Vinyals et al., 2015). Pointer network can deal with some combinatorial optimization problems with attention mechanism, such as sorting the elements of a given set. Our proposed model can take as input a set of random sorted sentences, and generate an ordered sequences. With pointer network, our proposed model could utilize the whole contextual information to specify the sort order. In addition, we further evaluate the robustness of our model by adding unrelated noisy sentence to the sentence set, which is nontrivial for pair-wise models. Experimental results on two datasets show that proposed model achieves the state-of-the-art performance even it only works with the greedy decoding.\nThe contributions of this paper can be summarized as follows:\n1. Instead of pair-wise sentence ordering, we propose an end-to-end neural approach to generate the order of sentences, which could exploit the contextual information and alleviate the error propagation problem. 2. We perform extensive empirical experiments and achieve the state-of-the-art performance even it only works with the greedy decoding. 3. We design a new and harder experiment to eval-\nar X\niv :1\n61 1.\n04 95\n3v 1\n[ cs\n.C L\n] 1\n5 N\nov 2\nuate the robustness of our model. We add unrelated noisy sentences to the candidate set, and wish the system can sort the sentences correctly while discarding the noisy sentences."}, {"heading": "2 Pointer Network for Neural Sentence Ordering", "text": ""}, {"heading": "2.1 Task Description", "text": "Sentence ordering task aims to rank a set of sentences in a clear and consistent manner. Specifically, given n sentences s = s1, s2, . . . , sn, the aim is to find the gold order o\u2217 for these sentences:\nso\u22171 so\u22172 \u00b7 \u00b7 \u00b7 so\u2217n , (1)\nwhich has the maximal probability of given sentences P (o\u2217|s):\nP (o\u2217|s) > P (o|s), \u2200o \u2208 \u03a8, (2)\nwhere o indicates any order of these sentences and \u03a8 indicates the set of all possible orders."}, {"heading": "2.2 Model Architecture", "text": "Previous works mainly focused on pair-wise models (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016), which lack contextual information and introduce error propagation for the pipe line strategy. Instead of decomposing the score to independent pairs, in this paper, we propose an end-toend neural approach based on pointer network (PtrNet) to score the entire sequence of sentences.\nThe model architecture is shown in Figure 1. Specifically, the probability of a specific order o of given sentences P (o|s) could be formalized as:\nP (o|s) = n\u220f\ni=1\nP (oi|oi\u22121, . . . , o1, s). (3)\nThe probability P (oi|oi\u22121, . . . , o1, s) is calculated by Ptr-Net:\nP (oi|oi\u22121, . . . , o1, s) = softmax(ui)oi (4)\nuij = v \u1d40 tanh(W\u1d40 [ ej di ] ), j = (1, . . . , n) (5)\nwhere ej ,di \u2208 Rh are outputs of encoder and decoder of Ptr-Net respectively. v \u2208 Rh and W \u2208 R(2h)\u00d7h are trainable parameters.\nEncoder The encoding representation of Ptr-Net could be formalized as:\nej = LSTM(Enc(soj ), ej\u22121), j = (1, . . . , n), (6)\nwhere Enc(soj ) indicates the encoding of sentence soj . The sentence encoding function Enc(\u00b7) and function LSTM(\u00b7) will be further interpreted in Section 2.3.\nNotably, the initial state of encoder is e0 = 0.\nDecoder Similarity, the decoding representation of Ptr-Net is formalized as:\ndi = LSTM(Enc(soi),di\u22121), i = (1, . . . , n). (7)\nNotably, the initial state of decoder is d0 = en."}, {"heading": "2.3 Sentence Encoding", "text": "Since Ptr-Net receives fixed length vectors as inputs, we need firstly encode sentences with variational length. Inspired by Chen et al. (2016), we tried three types of encoders: continues bag of words (CBoW), convolutional neural networks (CNNs) and long short-term (LSTM) neural networks."}, {"heading": "2.3.1 Continues Bag of Words", "text": "Continues bag of words (CBoW) model (Mikolov et al., 2013) simply averages the embeddings of words of a sentence. Formally, given the embeddings of nw words of a sentence s, w1, . . . ,wnw , the sentence embedding Enc(s) is:\nEnc(s) = 1\nnw nw\u2211 k=1 wk, (8)\nwhere Enc(s),wk \u2208 Rde . de is a hyper-parameter, indicating word embedding size."}, {"heading": "2.3.2 Convolutional Neural Networks", "text": "Convolutional neural networks (CNNs) (Simard et al., 2003) are biologically-inspired variants of multiple layer perceptions (MLPs). Formally, sentence s with nw words could be encoded as:\ncovk = \u03c6(W \u1d40 cov(\u2295 lf\u22121 u=0 wk+u) + bcov), (9)\nEnc(s) = max k covk, (10)\nwhere Wcov \u2208 R(d\u00d7lf )\u00d7df and bcov \u2208 Rdf are trainable parameters, and \u03c6(\u00b7) is tanh function. Here, k = 1, . . . , nw \u2212 lf + 1, and lf and df are hyper-parameters indicating the filter length and number of feature maps respectively. Notably, max operation in Eq (10) is a element-wise operation."}, {"heading": "2.3.3 Long Short-term Neural Networks", "text": "Long short-term (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) are advanced recurrent neural networks (RNNs), which alleviate the problems of gradient vanishment and explosion. Formally, LSTM has memory cells c \u2208 Rdr controlled by three kinds of gates: input gate i \u2208 Rdr , forget gate f \u2208 Rdr and output gate o \u2208 Rdr :\nit ot ft c\u0303t  =  \u03c3 \u03c3 \u03c3 \u03c6 (Wg\u1d40 [ wtht\u22121 ] + bg ) , (11)\nct = ct\u22121 ft + c\u0303t it, (12) ht = ot \u03c6(ct), (13)\nwhere Wg \u2208 R(d+dr)\u00d74dr and bg \u2208 R4dr are trainable parameters. dr is a hyper-parameter indicating the cell unit size as well as gate unit size. \u03c3(\u00b7) is sigmoid function and \u03c6(\u00b7) is tanh function. Here, t = 1, . . . , nw. Thus, we would represent sentence s as:\nEnc(s) = hnw . (14)"}, {"heading": "2.4 Order Prediction", "text": "Given P (o|s), the predicted order o\u0302 is the one with highest probability:\no\u0302 = arg max o\nP (o|s). (15)\nSince the decoding process, to find o\u0302, is a NP hard problem. Instead, we use two strategies to decode a sub optimal result: greedy decoding and beam search decoding.\nGreedy Decoding At decoding phase of Prt-Net, greedy strategy determines o\u0302 = o\u03021, . . . , o\u0302n step by step as:\no\u0302i = arg max oi\nP (oi|o\u0302i\u22121, . . . , o\u03021, s). (16)\nBeam Search Decoding Beam search strategy always keeps top b terms as candidates each step. Formally, at step t, each candidate o\u0302t1 = o\u03021, . . . , o\u0302t has a probability:\nP (o\u0302t1|s) = t\u220f\ni=1\nP (o\u0302i|o\u0302i\u22121, . . . , o\u03021, s), (17)\nand b candidates with higher probabilities will be kept at t step in the beam."}, {"heading": "3 Training", "text": "Assuming that we have m training examples (xi, yi) m i=1, where xi indicates a sequence of sentences with a specific permutation of yi, and yi is in gold order o\u2217. For obtaining more training data, we randomly generate new permutation for xi at each epoch. The goal is to minimize the loss function J(\u03b8):\nJ(\u03b8) = \u2212 1 m m\u2211 i=1 logP (yi|xi; \u03b8) + \u03bb 2 \u2016\u03b8\u201622, (18) where P (yi|xi; \u03b8) = P (o\u2217|s = xi; \u03b8) and \u03bb is a hyper-parameter of regularization term. \u03b8 indicates all trainable parameters.\nIn addition, we use AdaGrad (Duchi et al., 2011) with shuffled mini-batch to train our model. We also use pre-trained embeddings (Turian et al., 2010) as initialization."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "To evaluate proposed model, we adopt two datasets: abstracts on arXiv (Chen et al., 2016) and SIND (Sequential Image Narrative Dataset) (Ferraro et al., 2016). Examples in SIND dataset all contain 5 sentences, and we only use captions in this paper. The details of two datasets are shown in Table 1."}, {"heading": "4.2 Hyper-parameters", "text": "Table 2 gives the details of hyper-parameter configurations. The CNN sentence encoder uses three different filter lengths as Kim et al. (2015)."}, {"heading": "4.3 Metrics", "text": "To evaluate our model, we use three different metrics: (1) Pairwise metrics; (2) Longest sequence ratio; (3) Perfect match ratio.\nPairwise Metrics Pairwise metrics (PM) is the fraction of pairs of sentences whose predicted relative order is the same as the ground truth order (higher is better). Formally, pairwise metrics can be denoted as three scores: precision P , recall R and F -value.\nP = 1\nm m\u2211 i=1 |S(o\u0302i) \u22c2 S(o\u2217i )| |S(o\u0302i)| , (19)\nR = 1\nm m\u2211 i=1 |S(o\u0302i) \u22c2 S(o\u2217i )| |S(o\u2217i )| , (20)\nF = 2 \u2217 P \u2217R P +R , (21)\nwhere function S(\u00b7) denotes the set of all skip bigram sentence pairs of a text, and function | \u00b7 | indicates the size of set.\nConcretely, we take {o\u0302 = (2, 3, 1, 4),o\u2217 = (1, 3, 4)} as an example, where 2nd sentence is a noisy term. The pairwise scores of this example are: P = 1/6, R = 1/3, F = 2/9.\nLongest Sequence Ratio Longest sequence ratio (LSR) calculates the radio of longest correct sub-sequence (consecutiveness is not necessary, and higher is better). Formally, LSR can be denoted as three scores: precision P , recall R and F -value.\nP = 1\nm m\u2211 i=1 |L(o\u0302i,o\u2217i )| |o\u0302i| , (22)\nR = 1\nm m\u2211 i=1 |L(o\u0302i,o\u2217i )| |o\u2217i | , (23)\nF = 2 \u2217 P \u2217R P +R , (24)\nwhere function L(\u00b7) denotes the number of elements in longest correct sub-sequence. The value of function L(o\u0302 = (2, 3, 1, 4),o\u2217 = (1, 3, 4)) of the example above is 2.\nPerfect Match Ratio Perfect match ratio (PMR) calculates the radio of exactly matching case (higher is better):\nPMR = 1\nm m\u2211 i=1 1{o\u0302i = o\u2217i } (25)"}, {"heading": "4.4 Results", "text": "Results on different datasets are shown in Table 4. Since there is no noisy sentence here and the number of output sentences is the same as inputs, three scores of PM and LSR are the same in this table. Thus, we only use PM and LSR to denote the same P,R, F values. As we can see, our model outperforms previous works, and achieves the state-of-theart performance. Our model with LSTM sentence encoder reaches 40.00% in PMR, which means that 2/5 texts in test set of arXiv dataset are ranked exactly right, 6.57% boosted compared with work of Chen et al. (2016).\nMoreover, we investigate the performance of our model when sentence number varies. As Chen et al. (2016) did not evaluate their results on LSR metrics, we only compare with them on PM and PMR. As shown in Figure 2, our model with CNN and LSTM sentence encoders outperforms the work of (Chen et al., 2016). Although the accuracy of our model drops as the number of sentences increases, we could find that our model performs better on text with more sentences compared to pair-wise model.\nSince first and last sentences are more special, we also evaluate the accuracy of finding the first and last sentences. As shown in Table 3, we also achieve significant boost compared to work of (Chen et al., 2016).\nAccording to the experimental results above, we could find that Ptr-Net with LSTM sentence encoder always outperforms the one with CBoW or CNN\nsentence encoder. Thus, we mainly focus on evaluating our model with LSTM sentence encoder in the rest of this paper."}, {"heading": "4.5 Visualization", "text": "To further understand how our model works, we do some visualizations of Ptr-Net with CNN and LSTM sentence encoders. As shown in Table 5, the left side (first three columns) blue terms are input sentences. They are firstly encoded by sentence encoder (CNN or LSTM sentence encoder), then sent to the decoder of Ptr-Net. The right side (last column) red term is the first sentence that our model predicts. After that, this red item is encoded and sent to Prt-Net decoder to generate the 2nd sentence. This visualization shows how important each word is in generating the 2nd sentence. The more important the words are, the darker the color is. As we can see, by using CNN sentence encoder, the model focuses more on individual words, like \u201cfirst\u201d, \u201csecond\u201d, which give strong signal for ordering. Contrastly, the model with LSTM sentence encoder trends to find a specific pattern (or compositional features). In this case, the model emphasizes the pattern \u201cOur X question regarding\u201d, where X could be ordinal numbers like first and second here. However, both of them have some signals like words \u201cindices\u201d, \u201calgorithm\u201d, which might be disturbances and hard to interpret.\nThere still a question remains. How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives. Formally, we aim to rank n sentences in gold order s1, . . . , sn. Assuming the predicted order would be o\u0302 and we are predicting the i-th sentence so\u0302i , the importance of each word w j k (k-th word in j-th sentence so\u0302j ) is:\nA(wjk) = \u2223\u2223\u2223\u2223\u2223\u2202P (o\u0302i|o\u0302i\u22121, . . . , o\u03021, s)\u2202wjk \u2223\u2223\u2223\u2223\u2223 , (26)\nwhere wjk is word embedding of word w j k. Function | \u00b7 | is the second order normalization operation. P (o\u0302i|o\u0302i\u22121, . . . , o\u03021, s) is detailed in Eq. 5. For\nCBoW sentence encoder, the gradients of words in a sentence are the same for the simple average operation. Thus, we only visualize Ptr-Net with CNN and LSTM sentence encoders."}, {"heading": "4.6 Noisy Sentences", "text": "Interestingly, we find our model could deal with the case well when additional noisy sentence exists. To evaluate the performance of our model on noisy sentences, we compare three different strategies in adding noises: (1) 0 noisy sentence (0 noise); (2) 1 noisy sentence (1 noise); (3) 1 noisy sentence with 50% probability (0/1 noise). All noisy sentences come from own datasets. Since all texts in SIND dataset contain 5 sentences, it is easy for model to tell if there is noisy sentence. Thus, we only evaluate our model on arXiv dataset as shown in Table 6. As mentioned above, we only use the model with LSTM sentence encoder to evaluate the ability of our model on disambiguating noisy sentences, since it always performs better than the one with CBoW or CNN sentence encoder.\nAlthough 0 noise version seems the same as the case in Table 4, they are actually different with each other. In this section, 0 noise version do not constrain the length of predicted sequence to be the same with input. In addition, the P,R, F values of PM and LSR of 0 noise and 1 noise version are not the same actually, and very little difference exists (4 numbers after decimal point). It is because that our model is very good at eliminating the noise terms on 0 noise and 1 noise cases.\nAs we can see, our model performs best on 0 noise case, which implies that it is easier than 1 noise and 0/1 noise cases for our model. However, it is hard to say which one is more difficult among 1 noise and 0/1 noise cases, as the performance on different metrics are not consistent."}, {"heading": "4.7 Potential Oracle", "text": "In progress..."}, {"heading": "5 Related Work", "text": "Previous works on sentence ordering mainly focused on the external and downstream applications, such as multi-document summarization and discourse coherence (Van Dijk, 1985; Grosz et al., 1995;\nVan Berkum et al., 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization. Lapata (2003) proposed a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts. Okazaki et al. (2004) improved chronological ordering by resolving antecedent sentences of arranged sentences and combining topical segmentation. Bollegala et al. (2010) presented a bottom-up approach to arrange sentences extracted for multi-document summarization.\nRecently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering. In addition, they designed an interesting task of ordering the coherent sentences from academic abstracts. Agrawal et al. (2016) focused on a very similar ordering task which ranks image-caption pairs, additionally considering the image information. Li and Jurafsky (2016) mainly applied neural models to judge if a given text is coherent.\nDespite of their success, they are all pair-wise based models, lack of contextual information. Unlike these work, we propose an end-to-end neural model based on Ptr-Net to address sentence ordering problem."}, {"heading": "6 Conclusions", "text": "Sentence ordering is an important factor in natural language generation and attracts increasing focus recently. Previous works are mainly based on pairwise learning framework, which do not take contextual information into consideration and always lead to error propagation for their pipeline learning strategy. In this paper, we propose an end-to-end neural model based on Ptr-Net to address sentence ordering problem. Experimental results show that our model achieves the state-of-the-art performance even using greedy decoding strategy.\nIn the future, we would like to further improve\nthe performance by reranking the candidates in the beam search."}], "references": [{"title": "2016. Sort story: Sorting jumbled images and captions into stories", "author": ["Arjun Chandrasekaran", "Dhruv Batra", "Devi Parikh", "Mohit Bansal"], "venue": "arXiv preprint arXiv:1606.07493", "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["Barzilay", "Elhadad2002] Regina Barzilay", "Noemie Elhadad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "Modeling local coherence: An entitybased approach", "author": ["Barzilay", "Lapata2008] Regina Barzilay", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Barzilay et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2008}, {"title": "A bottom-up approach to sentence ordering for multi-document summarization", "author": ["Naoaki Okazaki", "Mitsuru Ishizuka"], "venue": "Information processing & management,", "citeRegEx": "Bollegala et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A unified local and global model for discourse coherence", "author": ["Elsner et al.2007] Micha Elsner", "Joseph L Austerweil", "Eugene Charniak"], "venue": "In HLTNAACL,", "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan et al.2009] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["Scott Weinstein", "Aravind K Joshi"], "venue": null, "citeRegEx": "Grosz et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["Mirella Lapata"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Lapata.,? \\Q2003\\E", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Neural net models for open-domain discourse coherence", "author": ["Li", "Jurafsky2016] Jiwei Li", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1606.01545", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Li et al.2015] Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1506.01066", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving chronological sentence ordering by precedence relation", "author": ["Yutaka Matsuo", "Mitsuru Ishizuka"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Okazaki et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Okazaki et al\\.", "year": 2004}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Dave Steinkraus", "John C Platt"], "venue": "In null,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Semantic integration in sentences and discourse: Evidence from the n400", "author": ["Peter Hagoort", "Colin Brown"], "venue": "Cognitive Neuroscience, Journal", "citeRegEx": "Berkum et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Berkum et al\\.", "year": 1999}, {"title": "Semantic discourse analysis", "author": ["Van Dijk1985] Teun A Van Dijk"], "venue": "Handbook of discourse analysis,", "citeRegEx": "Dijk.,? \\Q1985\\E", "shortCiteRegEx": "Dijk.", "year": 1985}], "referenceMentions": [{"referenceID": 0, "context": "Recently, sentence ordering task attracts more focus in NLP community (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc.", "startOffset": 70, "endOffset": 134}, {"referenceID": 7, "context": "The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008).", "startOffset": 117, "endOffset": 189}, {"referenceID": 0, "context": ", 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008). Most of previous researches of sentence ordering are pair-wise models. Chen et al. (2016) and Agrawal et al.", "startOffset": 8, "endOffset": 424}, {"referenceID": 0, "context": ", 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008). Most of previous researches of sentence ordering are pair-wise models. Chen et al. (2016) and Agrawal et al. (2016) proposed pair-wise models", "startOffset": 8, "endOffset": 450}, {"referenceID": 10, "context": "Li and Jurafsky (2016) additionally employed the graph based method (Lapata, 2003) to rank sentences.", "startOffset": 68, "endOffset": 82}, {"referenceID": 0, "context": "Previous works mainly focused on pair-wise models (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016), which lack contextual information and introduce error propagation for the pipe line strategy.", "startOffset": 50, "endOffset": 114}, {"referenceID": 13, "context": "Continues bag of words (CBoW) model (Mikolov et al., 2013) simply averages the embeddings of words of a sentence.", "startOffset": 36, "endOffset": 58}, {"referenceID": 15, "context": "Convolutional neural networks (CNNs) (Simard et al., 2003) are biologically-inspired variants of multiple layer perceptions (MLPs).", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "In addition, we use AdaGrad (Duchi et al., 2011) with shuffled mini-batch to train our model.", "startOffset": 28, "endOffset": 48}, {"referenceID": 17, "context": "We also use pre-trained embeddings (Turian et al., 2010) as initialization.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "43 - - (Agrawal et al., 2016) - - - 73.", "startOffset": 7, "endOffset": 29}, {"referenceID": 9, "context": "The CNN sentence encoder uses three different filter lengths as Kim et al. (2015).", "startOffset": 64, "endOffset": 82}, {"referenceID": 6, "context": "How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives.", "startOffset": 106, "endOffset": 185}, {"referenceID": 16, "context": "How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives.", "startOffset": 106, "endOffset": 185}, {"referenceID": 12, "context": "How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives.", "startOffset": 106, "endOffset": 185}, {"referenceID": 7, "context": "Previous works on sentence ordering mainly focused on the external and downstream applications, such as multi-document summarization and discourse coherence (Van Dijk, 1985; Grosz et al., 1995; Van Berkum et al., 1999; Elsner et al., 2007; Barzilay and Lapata, 2008).", "startOffset": 157, "endOffset": 266}, {"referenceID": 5, "context": "Previous works on sentence ordering mainly focused on the external and downstream applications, such as multi-document summarization and discourse coherence (Van Dijk, 1985; Grosz et al., 1995; Van Berkum et al., 1999; Elsner et al., 2007; Barzilay and Lapata, 2008).", "startOffset": 157, "endOffset": 266}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016).", "startOffset": 88, "endOffset": 152}, {"referenceID": 3, "context": ", 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization.", "startOffset": 8, "endOffset": 85}, {"referenceID": 3, "context": ", 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization. Lapata (2003) proposed a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts.", "startOffset": 8, "endOffset": 250}, {"referenceID": 3, "context": ", 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization. Lapata (2003) proposed a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts. Okazaki et al. (2004) improved chronological ordering by resolving antecedent sentences of arranged sentences and combining topical segmentation.", "startOffset": 8, "endOffset": 474}, {"referenceID": 2, "context": "Bollegala et al. (2010) presented a bottom-up approach to arrange sentences extracted for multi-document summarization.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering.", "startOffset": 89, "endOffset": 173}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering. In addition, they designed an interesting task of ordering the coherent sentences from academic abstracts. Agrawal et al. (2016) focused on a very similar ordering task which ranks image-caption pairs, additionally considering the image information.", "startOffset": 89, "endOffset": 404}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering. In addition, they designed an interesting task of ordering the coherent sentences from academic abstracts. Agrawal et al. (2016) focused on a very similar ordering task which ranks image-caption pairs, additionally considering the image information. Li and Jurafsky (2016) mainly applied neural models to judge if a given text is coherent.", "startOffset": 89, "endOffset": 548}], "year": 2016, "abstractText": "Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pairwise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network (Ptr-Net) to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes1 and dataset2 of this paper are available.", "creator": "LaTeX with hyperref package"}}}