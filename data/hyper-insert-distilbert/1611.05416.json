{"id": "1611.05416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Composing Music with Grammar Argumented Neural Networks and Note-Level Encoding", "abstract": "creating any authentic aesthetically most pleasing piece like of dance art, like electronic music, has been generating a uniquely long time dream for future artificial intelligence research. based precisely on developments recent found success of his long - for short interval term dynamic memory ( lstm ) on adaptive sequence learning, we put further forward a uniquely novel system to reflect the thinking evolution pattern of aspiring a musician. for data flow representation, we propose forming a note - level encoding coding method, fundamentally which enables our animal model analyst to simulate how human syntax composes and polishes music phrases. thinking to avoid failure against evolutionary music theory, traditionally we automatically invent a novel design method, simple grammar argumented ( ga ) method. it can explicitly teach understanding machine basic composing principles. in overcoming this input method, we propose three rules as argumented to grammars and three semantic metrics asking for rapid evaluation learning of suitable machine - made music. results show effect that for comparing to basic lstm, grammar trees argumented to model'\u2018 s compositions probably have higher contents of diatonic temperament scale notes, short note pitch initial intervals, and chords.", "histories": [["v1", "Wed, 16 Nov 2016 19:42:40 GMT  (351kb,D)", "http://arxiv.org/abs/1611.05416v1", "5 pages, 5 figures"], ["v2", "Wed, 7 Dec 2016 20:51:36 GMT  (1776kb,D)", "http://arxiv.org/abs/1611.05416v2", "6 pages, 4 figures"]], "COMMENTS": "5 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["zheng sun", "jiaqi liu", "zewang zhang", "jingwen chen", "zhao huo", "ching hua lee", "xiao zhang"], "accepted": false, "id": "1611.05416"}, "pdf": {"name": "1611.05416.pdf", "metadata": {"source": "CRF", "title": "Grammar Argumented LSTM Neural Networks with Note-Level Encoding for Music Composition", "authors": ["Zheng Sun", "Jiaqi Liu", "Zewang Zhang", "Jingwen Chen", "Zhao Huo", "Ching Hua Lee", "Xiao Zhang"], "emails": ["zhangxiao}@{mail2,", "huozhao@cupl.edu.cn", "calvin-lee@ihpc.a-star.edu.sg"], "sections": [{"heading": null, "text": "Index Terms\u2014Music composition, note-level encoding, LSTM neural networks, grammar argumented method\nF"}, {"heading": "1 INTRODUCTION", "text": "C REATING all forms of art [1], [2], [3], [4], includingmusic, has been a long time pursue for artificial intelligence (AI) research. Even AI follows the principle as stated by the musicologist Leonard B. Meyer, that styles of music are in effect complex systems of probability relationships. In the early years, symbolic AI methods were popular and specific grammars describing a set of rules drive the composition [5], [6]. This method was latter improved by evolutionary algorithms in different ways [7], including the famous EMI project [8]. Latter, statistic model such as Markov chains and Hidden Markov model(HMM) became popular in algorithmic composition [9]. In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].\nRNN and LSTM perform well in modeling sequential data, however, when they are applied on music composition, the outcomes will be drab and dull, and even fall into a group of harsh notes. Moreover, the generated music sometimes go against general music theory. We hope to teach machine basic composing principles, but merely neural networks or older grammar methods are incapable of it.\nIn this work, we improvise LSTM with an original method named grammar argumented method (GA), which combine neural networks and grammars. We begin by training a LSTM neural network with a dataset full of musicianmade music. In the training process, machine learns sequencial information as much as possible. Then we feed a short\n\u2022 Z. Sun, J. Liu, Z. Zhang J. Chen and X. Zhang are with the Department of Physics, Sun Yat-sen University, Guangzhou, 510275 P. R. China. E-mail: {sunzh6, liujq33, zhangzw3, chenjw93, zhangxiao}@{mail2, mail2, mail2, mail2, mail}.sysu.edu.cn \u2022 Z. Huo is with China University of Political Science and Law. E-mail: huozhao@cupl.edu.cn \u2022 C. H. Lee is with Institute of High Performance Computing. E-mail: calvin-lee@ihpc.a-star.edu.sg\nManuscript received xx/xx/xxxx; revised xx/xx/xxxx.\nmusic phrase to trigger the first phase of generation. Instead of adding the generated notes directly to music score, we evaluate each predicted result with some composing rules. The notes go against general music theory will be abandoned and new notes confirmed to rules can be accessed by repredicting. All amended results and their corresponding inputs will be added to training set, we retrain our model with updated training set and use the original generating method to do the second phase of (real) generation. We also adopt a new representation of notes by concatenating each note\u2019s duration and pitch as a single input vector. With this note-level encodig method, we enable machine to process real notes and think like human. Results show that our system is capable of composing pleasing melody, and it performs better than non-GA system in the percentages of notes in diatonic scale, pitch intervals in one octave, and chords. In other words, our specific system can learn basic composing principles and bring out common and melodious music."}, {"heading": "2 METHODS", "text": ""}, {"heading": "2.1 Note-Level Encoding", "text": "Although machine learning method makes a significant progress in music composition, none of the related works really simulates how human creates music. During music composition, composers often focus on several typical music phrases. In the process of polishing music phrases, they need to deliberate each music note. Music note is a particular combination of pitch and duration, like a quarter note of E6. However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21]. In this work, we encode music with note-level method.\nBecause it is troublesome to simulate the process of polishing and generating notes with raw MIDI files, we need to convert each file to a sequence of music notes. First, we choose MIDI files of 106 Soft Piano music whose composers include Joe Hisaishi, Yiruma, Yoko Kanno and Shi Jin, et\nar X\niv :1\n61 1.\n05 41\n6v 1\n[ cs\n.L G\n] 1\n6 N\nov 2\n01 6\n2\nFig. 1. We extract music score from the MIDI files. Each note symbol contains the information of duration and pitch, both of which are then converted into one-hot vector and concatenated into one specific binary vector.\nal. These compositions\u2019 time signature are all 4/4. We only pick music with time signature 4/4 because music with different time structures is intractable for modeling. Then, we delete all the accompaniments and score annotations like Control Change Events and grace notes. Besides, we ignore the change of intensity and just keep the highest pitch note when more than two notes appear at the same time. This treatment can discretize melodies into a sequence of Note On Events and Note Off Events. We then encode each pair of Note On Event and Note Off Event into one 2-D vector. Each vector includes two dimensions: duration ranging from semiquaver to breve and pitch ranging from A0 to C8. For reducing dimentions of model\u2019s input layer, we transpose all melodies into C major/A minor, which eventually include 30 kinds of durations and 59 kinds of pitches. Then, we convert each note\u2019s duration and pitch into corresponding one-hot vector, one has 30 bits, another has 59 bits, as in Fig.1. Concatenate these two binary vectors, we can get specific representation for each note.\nUnlike the common used encoding methods, piano-roll representation, this method can represent both pitch and duration in one single vector. What is more significant, a sequence of real music notes enable our model to simulate how human generates and polishes music phrases."}, {"heading": "2.2 Long-Short Term Memory Neural Networks", "text": "Recurrent neural networks (RNN) perform well in sequence learning. Each hidden layer receives both input from the previous layer and input from itself one time step in the past. It enables the networks remember information of previous time steps. However, simple RNN does not perform well in long-term dependency because of vanishing gradients [22]. Long-short term memory (LSTM) neural network is a kind of advanced RNN and solve this problem. It use memory cell to store necessary information and various gates to control other information.\nThe graphic structure of LSTM in Fig.2 shows how the data flows through the LSTM module. In the following we describe how the hidden state u(t) of LSTM is computed. To obtain u(t) for each time step, we input data v(t) and u(t\u22121) from last time step into the LSTM. bi, bo, bf , bc denote corresponding vectors of biases, whereas the subscript i, o, f, c means input, output, forget and cell gate, sequentially.Wi,Wo,Wf ,Wc denote the matrices of weights between input vector and corresponding gate units and\n(a)\n(b)\nFig. 2. The graphic structure of LSTM. With v(t) and u(t\u22121) as inputs, LSTM layer outputs a hidden state u(t) that can convey temporal information. denotes element-wise multiplication and + denotes elementwise addition. The schematic diagram (a) shows what to input and output and (b) indicates the details that how the LSTM layer computes the current hidden state u(t).\nUi, Uo, Uf , Uc denote the matrices of weights connecting hidden state from last time step to the corresponding gate units.\nFirst, with the previous temporal information u(t\u22121), we construct input gate i(t), output gate o(t) and forget gate f (t) to decide which parts of information from memory cell to pass through the gate. \u03c3(x) = (1 + e(\u2212x))\u22121 is an elementwise sigmoid function whose output value is between (0, 1) interval.\ni(t) = \u03c3(Wiv (t) + Uiu (t\u22121) + bi) (1)\no(t) = \u03c3(Wov (t) + Uou (t\u22121) + bo) (2)\nf (t) = \u03c3(Wfv (t) + Ufu (t\u22121) + bf ) (3)\nIn the same way, we compute the new state C\u0303(t) of memory cell depending on the value of input v(t) and hidden state u(t\u22121) from last time step. The difference is that the activation here is tanh(x) = (1\u2212 e\u22122x)/(1 + e\u22122x).\nC\u0303(t) = tanh(Wcv (t) + Ucu (t\u22121) + bc) (4)\nThen we can define the update rule for the current state C(t) of memory cell of LSTM, a unit that stores and accumulates information. It is the new state C\u0303(t) of memory cell multiplied by the output of input gate i(t) plus the old state C(t\u22121) of memory cell from last time step multiplied by the output of forget gate f (t):\nC(t) = i(t) C\u0303(t) + f (t) C(t\u22121) (5)\n3 Finally, the current hidden state u(t) of LSTM is computed by the activated current state of memory cell under the control of output gate.\nu(t) = o(t) tanh(C(t)) (6)"}, {"heading": "2.3 Grammar Argumented Method", "text": "In an original method, after hundreds of epoches, the loss of model stops decreasing, we will consider machine has learned as much information as possible from the dataset. Once a seed input is given, the model will be able to predict notes continuously. However, it does not perform as well as expected because the model often generates results not conformed to basic composing principles, for example, too many overtones, acute change on pitch and unpleasing melodies. These results can ruin the composition. Our GA method can teach machine to learn general music knowledge and compose more harmonious notes without any manual intervention on the second phase of (real) generation. Before the details of GA go, we need to determine what kinds of rules to use in GA method. According to music theory and unmelodious pieces in results, we put forward three kinds of rules.\nThe first rule is diatonic scale (Dia). In music theory, notes in a diatonic scale are the most pleasing. The C major scale is one of the diatonic scales and it is made up of seven distinct notes (C, D, E, F, G, A, and B). Because all data is converted to C major in this work, we choose C major scale rule in our experiments. Conventionally, machine without GA method often produces overtones (C#, D#, F#, G#, and A#) and ruins a nice piece. Although overtones sometimes have positive effects on music pieces, it is too difficult for machine to compose melodious music with all twelve tones in one octave. So we hope there are less overtones and more notes in C major scale in machine\u2019s compositions. The second rule is short pitch interval (SPI). The pitch interval of two successive notes usually does not span over one octave. Acute change on pitch often makes listener feel uncomfortable. We think only experienced composers can use pitch intervals spanning over one octave well, so short pitch intervals are more preferred in our results. The last rule is triads (Tri). In addition to notes in a scale, chords are also the key of composing tuneful music. Triads are the simplest chords and each kind of triads represents a pair of pitch intervals. There are four kinds of triads in total and each of them has specific music emotion. Furthermore, triads are also the basis of all seventh chords, which result in variegated music. We believe that composition\u2019s level is closely related to the amount of chords.\nWith GA method, in the first phase of generation, we aim at achieving data amended with composing rules stated above. Before a fresh note is added to music score, we check that whether or not this new note are conformed to rules. When a discordant note is predicted, we go back to the output layer of model and resample from the output distribution, as in Fig.3. Loop this operation until a note conformed to rule is generated. For example, we suppose that the fresh note is (eighth, B6) and the last note in music score is (eighth, A5), their pitch interval will be 14 semitones. Because one octave only includes 12 semitones and this new note is non-conformed to SPI. Although pitch B6 may\nhas the highest probability in output layer, we abandon it and resample for other notes conformed to SPI. When a note whose pitch is conformed to SPI appears, we add it to music score. At the same time, we record this amended result and the current input phrase. After the generating process, we mix all recored data with original training set, like what is shown at the bottom of Fig.3. In other words, we add a handful of fabricated data which includes the information of basic composing rules. Lastly, we retrain the model with the updated training set. In the second phase of (real) generation, we adopt original method, let machine generate notes continuously without any extra mechanism. This GA method enables machine to learn basic composing principles from data amended with music theory and bring out melodious music."}, {"heading": "3 EXPERIMENTS", "text": "Our model consists of one LSTM layer and one fully connected layer. The LSTM layer includes 128 cells and its input dimension is 89, which equals to the length of note\u2019s binary representation. There are 89 nodes in fully connected layer and it is also the output layer. The size of our dataset is 30000, to speed up the training process, we divide it into mini-batches, and the batch size is 64. We use Adam [23] to perform gradient descent optimization, learning rate is set to 0.001. We build our model on a high-level neural networks library Keras [24] and use TensorF low [25] as its tensor manipulation library.\nWe train this model with original dataset and label this set of weights with Orig. With GA method, we use Orig to generate 100k notes for each rule and get three sets of amended data. Then we mix each set of data with dataset, mix all three sets with dataset, and get four new training sets at last. We retrain our model with them and label these four sets of weights with Dia, SPI, Tri, and MIX, corresponding to their rules. For statistics analysis, we adopt a public random seed to generate 100k notes with all five sets of weights, including Orig."}, {"heading": "4 RESULTS", "text": "We take a representative piece from machine\u2019s long composition, which is generated in MIX mode. This piece\u2019s music score is shown in Fig.4. At first, it is a strong evidence that machine excludes overtones and prefers notes in C major scale. There is only one overtone in this piece. Secondly, it is worth noticing that machine has already learned to use repeated rhythmic structure in one piece as human always does, for example, bar 4-5 and bar 10-11. Listen from the beginning, we notice that the whole piece sounds soft and lyrical, which is consistent with music in dataset. Bar 3, 4, 6, and 12 have faster paces sandwiched between slow moving melodies. This variety of paces is also seen in musicianmade music. Besides, the bottom half is so pleasing that it can be directly used as theme of a new song."}, {"heading": "5 EVALUATIONS", "text": "According to music theory described in section2.3, we put forward three kinds of metrics: percentages of notes in\n4\nFig. 3. The grammar argumented method. With a music phrase or note sequence, a well-trained LSTM neural network is able to predict the next note. The output layer consists of 89 nodes, according to note-level encoding method, the left 30 nodes represent duration and the right 59 nodes are for pitch. We sample from these two parts because duration and pitch\u2019s representations are all one-hot codes. After the sampling result is converted to note, we check it with composing rule, only notes comformed to rule can be added to phrase. When a non-comforming note is predicted, we resample on the output distribution until the comforming one appears. Then, we add this amended note and its corresponding input phrase to the original training set.\nFig. 4. An example of machine\u2019s composition. It is generated in MIX mode and includes about 100 notes.\ndiatonic scale (pDia), percentage of pitch intervals in one octave (pSPI) and percentages of triads (pTri). All of them are based on note-level encoding, but they are generally valid as evaluation criteria for other composing algorithms."}, {"heading": "5.1 pDia", "text": "Table 1 shows the percentages of seven notes in C major scale, we find that GA method works with C major scale rule. For E6, its ratio increases by one percentage points. Except for Dia and MIX, Tri mode also produces high pDia. This phenomenon can be explained by pitch interval. Because pitch intervals in triads are included in major scale, adding data amended with triads rule also improves pDia."}, {"heading": "5.2 pSPI", "text": "We figure out each pitch interval in the composition and calculate the percentage of pitch intervals in one octave.\nHigh pSPI contributes to a harmonious composition. As what is shown in Table 2, SPI and MIX mode can compose music with high pSPI. We also notice that without GA method (Orig mode), model generate more unharmonious notes, which result in more poor compositions.\n5"}, {"heading": "5.3 pTri", "text": "Table 3 shows percentages of triads. In Tri and MIX mode, machines composition includes more triads than other results. We notice that music composed in MIX mode performs well with all three metrics. It suggests that those three rules are not conflicting, they are coherent rules in music theory."}, {"heading": "6 CONCLUSION", "text": "Although it is difficult for machine to learn music theory rules with simple LSTM neural networks, we propose grammar argument method and enable our model to learn those rules by adding rule-amended data to dataset. GA method reduce unharmonious notes\u2019 amount significantly. Our original note-level encoding method also contributes to this successful system. On note level, machine thinks like musicians and is able to learn advanced logic from the dataset. At last, three metrics provide a solution for evaluating machine-made music. Our GA method has the potential to include high-level music theory rules, such as repeated paragraphs, and it gives an approach towards music with global structure."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Chuangjie Ren and Qingpei Liu for helpful discussions on neural networks."}], "references": [{"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "Computer Science, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "Computer Science, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "Computer Science, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A method for composing simple traditional music by computer", "author": ["G.M. Rader"], "venue": "Communications of the ACM, vol. 17, no. 11, pp. 631\u2013 638, 1974.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1974}, {"title": "Ai methods in algorithmic composition: a comprehensive survey", "author": ["J.D. Fern\u00e1nd Ndez", "F. Vico"], "venue": "Journal of Artificial Intelligence Research, vol. 48, no. 48, pp. 513\u2013582, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Genotator: an environment for exploring the application of evolutionary techniques in computerassisted composition", "author": ["THYWISSEN", "KURT"], "venue": "Organised Sound, vol. 4, no. 2, pp. 127\u2013133, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Computer modeling of musical intelligence in emi", "author": ["D. Cope"], "venue": "Computer Music Journal, vol. 16, no. 16, pp. 69\u201387, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Harmonising chorales in the style of johann sebastian bach", "author": ["M. Allan"], "venue": "Master\u2019s Thesis, School of Informatics, University of Edinburgh, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "A connectionist approach to algorithmic composition", "author": ["P.M. Todd"], "venue": "Computer Music Journal, vol. 13, no. 4, pp. 27\u201343, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multiscale processing", "author": ["M.C. MOZER"], "venue": "Connection Science, vol. 6, no. 2-3, pp. 247\u2013280, 1994.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "arXiv preprint arXiv:1206.6392, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Artificial neural networks and machine learning icann 2014", "author": ["S. Wermter", "C. Weber", "W. Duch", "T. Honkela", "P. Koprinkovahristova"], "venue": "Lecture Notes in Computer Science, vol. 8681, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning musical structure directly from sequences of music", "author": ["D. Eck", "J. Lapalme"], "venue": "University of Montreal, Department of Computer Science, CP, vol. 6128, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural networks for music computation", "author": ["J.A. Franklin"], "venue": "Informs Journal on Computing, vol. 18, no. 3, pp. 321\u2013338, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Polyphonic music generation by modeling temporal dependencies using a rnn-dbn", "author": ["K. Goel", "R. Vohra", "J. Sahoo"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2014. Springer, 2014, pp. 217\u2013224.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding temporal structure in music: Blues improvisation with lstm recurrent networks", "author": ["D. Eck", "J. Schmidhuber"], "venue": "Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on. IEEE, 2002, pp. 747\u2013756.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Polyphonic music modelling with lstm-rtrbm", "author": ["Q. Lyu", "Z. Wu", "J. Zhu"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. ACM, 2015, pp. 991\u2013994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multiscale processing", "author": ["M.C. Mozer"], "venue": "Connection Science, vol. 6, no. 2-3, pp. 247\u2013280, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Recurrent neural networks for music computation", "author": ["J.A. Franklin"], "venue": "INFORMS Journal on Computing, vol. 18, no. 3, pp. 321\u2013338, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Computer Science, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "In the early years, symbolic AI methods were popular and specific grammars describing a set of rules drive the composition [5], [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "In the early years, symbolic AI methods were popular and specific grammars describing a set of rules drive the composition [5], [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "This method was latter improved by evolutionary algorithms in different ways [7], including the famous EMI project [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "This method was latter improved by evolutionary algorithms in different ways [7], including the famous EMI project [8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Latter, statistic model such as Markov chains and Hidden Markov model(HMM) became popular in algorithmic composition [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 11, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 224, "endOffset": 228}, {"referenceID": 14, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 230, "endOffset": 234}, {"referenceID": 9, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "However, simple RNN does not perform well in long-term dependency because of vanishing gradients [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "We use Adam [23] to perform gradient descent optimization, learning rate is set to 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "We build our model on a high-level neural networks library Keras [24] and use TensorF low [25] as its tensor manipulation library.", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "We build our model on a high-level neural networks library Keras [24] and use TensorF low [25] as its tensor manipulation library.", "startOffset": 90, "endOffset": 94}], "year": 2016, "abstractText": "Creating any aesthetically pleasing piece of art, like music, has been a long time dream for artificial intelligence research. Based on recent success of long-short term memory (LSTM) on sequence learning, we put forward a novel system to reflect the thinking pattern of a musician. For data representation, we propose a note-level encoding method, which enables our model to simulate how human composes and polishes music phrases. To avoid failure against music theory, we invent a novel method, grammar argumented (GA) method. It can teach machine basic composing principles. In this method, we propose three rules as argumented grammars and three metrics for evaluation of machine-made music. Results show that comparing to basic LSTM, grammar argumented model\u2019s compositions have higher contents of diatonic scale notes, short pitch intervals, and chords.", "creator": "LaTeX with hyperref package"}}}