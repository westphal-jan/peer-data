{"id": "1611.05546", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Zero-Shot Visual Question Answering", "abstract": "part of the appeal of rigorous visual question analytics answering ( to vqa ) is primarily its mutual promise that to automatically answer new questions about previously unseen available images. most current methods demand training functional questions that illustrate every coherent possible concept, process and will so therefore never possibly achieve this theoretical capability, since the volume of subjects required basic training data would accordingly be prohibitive. answering less general questions about images requires training methods very capable of predicting zero - process shot imaging vqa, that is, finding methods able to answer questions anywhere beyond the scope considerations of the training examination questions. \u2026 we each propose historically a significant new technical evaluation protocol employed for locating vqa using methods which measures their current ability to directly perform zero - shot vqa, while and in doing did so highlights significant - practical deficiencies of early current approaches, some requirements of particular which are readily masked by influencing the biases in current diagnostic datasets. we propose and evaluate recently several strategies established for consistently achieving zero - motion shot quantitative vqa, including methods based variously on pretrained word form embeddings, object definition classifiers with semantic term embeddings, and test - time retrieval strategies of example images. our relatively extensive experiments are consciously intended, to now serve sufficiently as baselines for proving zero - shot vqa, and they theoretically also achieve perfect state - of - the - art performance in the standard underlying vqa evaluation test setting.", "histories": [["v1", "Thu, 17 Nov 2016 03:21:00 GMT  (6168kb,D)", "https://arxiv.org/abs/1611.05546v1", null], ["v2", "Sun, 20 Nov 2016 21:51:24 GMT  (6168kb,D)", "http://arxiv.org/abs/1611.05546v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["damien teney", "anton van den hengel"], "accepted": false, "id": "1611.05546"}, "pdf": {"name": "1611.05546.pdf", "metadata": {"source": "CRF", "title": "Zero-Shot Visual Question Answering", "authors": ["Damien Teney", "Anton van den Hengel"], "emails": ["damien.teney@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "The task of Visual Question Answering (VQA) spans the fields of computer vision and natural language processing by requiring an algorithm to answer a previously unseen text question about an image. The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14]. The ability to answer truly general questions about images would also constitute an concrete step towards real Artificial Intelligence.\nA number of VQA datasets have been introduced, and a variety of methods have demonstrated impressive (yet possibly converging) results (see [30] for a survey). The training of most current VQA methods relies on a dataset of {question,image,answer} tuples illustrating all question types applied to all items of interest, in all situations of in-\nterest. No finite set of exemplars, however, can cover the diversity of the world that an ideal VQA system should be prepared to consider. A secondary problem with this approach is that the incentive to perform well on benchmark datasets that do not encourage addressing rare, or novel, words and concepts. Most current methods are therefore designed to best learn \u2013 and often overfit \u2013 dataset biases. For example, it is common to consider a vocabulary limited to only the most frequent words and answers in the dataset. That practice thus completely discards rare concepts, let alone those that do not appear in the training set at all.\nAs an example, the training question How many giraffes are in the image ? is currently taken as an opportunity to learn to count giraffes specifically. We propose here that the opportunity is instead to learn to count. An ideal VQA system should therefore be able to generalize and answer questions about objects and situations that are not present\n1\nar X\niv :1\n61 1.\n05 54\n6v 2\n[ cs\n.C V\n] 2\n0 N\nov 2\n01 6\nin the VQA training set. We label this capability Zero-Shot VQA, inspired by the task of zero-shot classification. Our first contribution is an evaluation setting for VQA in which all test instances (questions and answers) include words not present in the training set. Our experiments in this setting expose common weaknesses of current VQA systems, namely poor generalization and over-reliance on dataset biases.\nWe show that most VQA datasets contain strong biases that render the interpretation and comparison of performances difficult. A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34]. For example, most questions starting with How many. . . have two or three as their correct answer, and rarely zero or seventeen. Although such biases are actually present in the questions that humans ask, VQA methods that overfit to these biases may improve on the benchmarks without making any significant progress towards visual scene understanding. For example, the stateof-the-art method in [11] achieves an impressive accuracy of almost 65% correct answers on the Visual7W dataset. However, the authors also train a similar, but blind model, that answers the question without analyising the image, that achieves 56% accuracy. This second figure is really more illuminating than the first, and one of the primary implications is that current methods for evaluating VQA performance are not a particularly good measure of a method\u2019s ability to understand visual scenes.\nThe contributions of this paper are summarized as follows. 1) We define the Zero-Shot Visual Question Answering\n(ZS-VQA) problem, and propose a corresponding evaluation setting where each test instance contains one or several unseen words, i.e. words not present in any training instance. 2) We propose a dataset that focuses exclusively on this setting based on the Visual7W dataset [36], of which we define new training and test splits. 3) We show that the paradigm followed by current VQA methods performs poorly in this setting, as correct answers cannot be as easily guessed by learning dataset biases. 4) We describe and evaluate extensively a set of strategies for ZS-VQA, including incorporating auxiliary data at training and test time. They result in large improvements for ZS-VQA, and also in state-of-the-art performance in the standard VQA setting (on the original splits of the Visual7W dataset)."}, {"heading": "2. Related work", "text": "The task of visual question answering has received increasing interest since the seminal paper of Antol et al. [3].\nMost recent methods are variations on the idea of a joint embedding of the image and the question using a deep neural network. The image and the question are passed respectively through a convolutional (CNN) and a recurrent neural network (e.g. an LSTM). They produce representations of the image and the question in a joint space, which can then be fed together into a classifier over an output vocabulary of possible answers. Consult [30] for a recent survey of the literature.\nMost VQA systems are trained end-to-end, i.e. with supervision solely for their final output, on closed datasets of images, questions, and their correct answers. Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36]. They remain however expensive to produce and are thus necessarily of limited size. This drives an increased interest in using additional sources of data.\nOn the language side, the word embeddings, i.e. the vectors used to represent the words, can be pretrained on a language modeling task [19, 17]. They capture semantics by mapping words of similar meaning to similar representations. Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24]. They can be pretrained unsupervised on large corpora and incorporate words not necessarily present in the training questions/answers. This simple strategy can enable VQA systems to generalize to words not present in training questions. The syntactic structure of language for VQA has received much less attention, but recent work suggest that explicit parsing can bring useful information [24].\nOn the image side, the common representation uses features produced within a convolutional neural network (CNN) pretrained for image classification. As is the case for pretrained word embeddings, this leverages the larger amounts of data available for the pretraining task. Most VQA methods do not use the actual output of the classifier but its hidden features. An exception is [28], where the authors use language as an explicit intermediate representation for VQA. They represent an image as a set of recognized attributes, actions, and objects. In this paper, we evaluate both traditional CNN features and explicit object detections.\nFinally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29]. Importantly, this data is not incorporated within the learned weights of the network. Only the behaviour for retrieving and incorporating external information is learned, and can then be applied at test time to concepts unseen during training. We experiment with a similar principle by retrieving additional visual exemplars at test time. In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.\nThe presence of strong biases and the long-tailed distri-\nbutions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9]. Zhang et al. address the related evaluation issues with a balanced dataset of binary (yes/no) questions. It contains two versions of each question, with slightly different images that elicit opposite answers. Their evaluation setting better capture a system ability to focus of the fine, meaningful details of the scene. However, it is only feasible for binary questions and synthetic (clip art) images. A similar motivation led to the zero-shot setting proposed in this paper, which has the advantage of being applicable to real images.\nLet us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5]."}, {"heading": "3. Dataset for Zero-Shot VQA", "text": "We propose a dataset for VQA with a \u201czero-shot\u201d aspect with respect to the questions and answers, but not to\nthe visual concepts in the associated images. For example, we consider the question How many zebras are in this image ? to be zero-shot if no training question involves zebras. Images containing zebras may, however, appear in the training set (with questions involving other elements of those images) or be used to train auxiliary components, e.g. an image classifier that recognizes zebras. This distinction reflects the fact that CNNs pre-trained on ImageNet [4] are commonly used in existing VQA methods, and the fact that VQA is the task that we are actually interested in."}, {"heading": "3.1. Repurposing the Visual7W dataset", "text": "In multiple-choice VQA, each training or test instance is a tuple of an image, a question, and multiple answer choices (four in the dataset considered here). The question and answers are given as text in natural language. Exactly one of the answers is marked as correct, and is used for supervised training and for evaluation. A dataset is partitioned into training, evaluation, and test splits.\nThe words used in the questions and answers of VQA datasets follow a long-tail distribution typical in natural language [12, 36]. In other words, most questions and answers are made of words from a small vocabulary, but a large number of other words appear very infrequently. The typical strategy in VQA methods is to focus on a limited vocabulary and a limited set of possible answers. This makes the training practically easier, and the performance penalty remains reasonable since the rare words arise in only a small fraction of the test instances. It however implies a fundamental limitation to the restricted set of words and answers.\nThe Zero-Shot VQA dataset we propose is formed by defining new training, validation, and test splits for the the \u201ctelling\u201d task of the Visual7W dataset [36]. Visual7W is itself a subset of the Visual Genome dataset [12] the highest quality dataset for VQA currently available, in terms of size, answer distribution, human performance, and the quality of the multiple choice answers.\nWe define the new splits such that every validation and test instance is zero-shot question, which we define as using at least one word that was not present in any training instance. The zero-shot instances can be further broken down according to whether unseen words appears in the question itself, in the correct answer, or in the other (incorrect) answers. These three sets are not mutually exclusive, as multiple unseen words can appear in the question and its answers. An analysis of the original splits of the Visual7W shows that only 9% of test questions qualify as zero-shot (see Table 1)."}, {"heading": "3.2. Building zero-shot splits", "text": "To build our new splits, we hold out two distinct subsets of the words used throughout the whole dataset and reserve them for the validation and test splits, respectively. The words in the held-out subsets are randomly selected from those which appear less than 20 times over the whole dataset. This ensures that these unseen words are semantically rich, as opposed to common verbs or stop words. These words typically describe fine-grained categories and very specific concepts (see examples in Fig. 2 and in the\nsupplementary material). The validation and test splits are formed from all instances containing at least one word from their reserved set, ensuring no overlap between sets. The training set is composed of all remaining instances, making sure, as in the original splits, to keep the images disjoint between the training and validation/test sets so as not to encourage overfitting. An analysis of the resulting splits is given in Table 1. Note that we preserve the other qualities of the original dataset, e.g. in the approximate distribution of question types.\nWe also annotate the test instances as to whether they contain unseen words in the question itself, in the correct answer, or in the other (incorrect) answers. We recommend reporting accuracy over the whole test set and on those nondisjoint subsets. We provide those same annotations for the standard splits, making it possible to report performance on zero-shot questions (albeit on a small number of them) of a method trained on the standard splits. Splits and annotations are available from the authors\u2019 website."}, {"heading": "4. Methods for zero-shot VQA", "text": "We consider a neural network for VQA with straightforward architecture. Our main objective is to evaluate additional features and pretrained representations of the inputs (image, question, and candidate answers). A simple architecture lets us evaluate these in relative isolation. Our method particularly does not include an attention mechanism, in contrast to many current approaches. The application of attention to the proposed features has no single obvious implementation and may warrant another research study of its own. Note also that each of the proposed improvements is evaluated on the basis of a relatively simple implementation. The goal is not to obtain the single best performing model, but to guide future research to areas with the most promise and provide reference performances of basic implementations."}, {"heading": "4.1. Baseline method", "text": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35]. The overall principle (see Fig. 1) is to map the inputs, i.e. a question, an image, and candidate answers, to vector representations in a common space. The mappings to produce these representations are learned such that interactions (e.g. distances, products, order comparisons, . . . ) between elements in this space capture semantic compatibility.\nOur baseline represents the question with a bag of words (BoW). Each word is represented by a fixed-length vector with a look-up table that associates every possible word to a learned vector (unknown words at test time receive an empty vector of zeros). The BoW representation is the average of all non-empty vectors of the question words. We refer to this representation as the learned word embedding. Additional features, described below, are concatenated where required (Fig. 3), giving the final question features xQ. Each candidate answer is treated similarly, using a BoW and optionally concatenated with additional features, giving the answer features xAi for each multiple choice i. The image is represented with global (image-wide) features of dimension 2048 extracted from the last pooling layer of a ResNet-152 [10] pretrained for image recognition on ImageNet. Note that this common practice is already a form of transfer learning, as opposed to the baseline language representation which learns the word embeddings from scratch. The CNN features are optionally concatenated with additional features described below, giving the image features xI.\nThe features xQ, xA, and xI are combined in two stages with multiplicative interactions, first between the question and image representations, then with the candidate answers:\nxQI = f(W1x Q + b1) \u25e6 f(W2xI + b2) (1)\nxQIAi = f(W3x QI + b3) \u25e6 f(W4xA + b4) (2)\nwith W1 . . .W4 and b1 . . . b4 learned weights and biases, f\na ReLU, and \u25e6 the Hadamard (element-wise) product. Each candidate answer i then receives a score si obtained with a logistic regression using the combined features\nsi = \u03c3(W5x QIA i + b5) (3)\nwith learned weights W5 and biases b5, and \u03c3 being the logistic function. The score represents the compatibility between the input question, image, and a candidate answer. All weights, biases, and embeddings are trained end-to-end to minimize the a cross-entropy loss, using +1 labels for the correct candidate answers and 0 for the incorrect multiple choices."}, {"heading": "4.2. Improved language representations", "text": "Pretrained word embeddings We compare the learned word embeddings (trained end-to-end within the VQA system) to embeddings pretrained on a language modeling task. This common practice [19, 17] has two advantages. First, the pretrained embeddings reflect word co-occurrences, and have shown empirically to capture complex semantic relationships in their vector space. Second, the pretraining task is unsupervised and embeddings can be learned from very large amounts of data, covering a much richer vocabulary than a VQA training set. Concretely, we evaluate GloVe embeddings of various dimensions {50,100,200,300} pretrained on Wikipedia and the Gigaword newswire corpus.\nSharing embeddings across stems We propose sharing a embeddings across words with a same stem (e.g. flower, flowers, and flowering). We hypothesize that the semantic meaning of words is often more important in the context of VQA than verb conjugation or plural forms of nouns. The procedure reduces the number of unique embeddings to be learned (e.g. from\u223c22k to\u223c17k words in the original V7W training set). Moreover, novel words at test time that have appeared in another form or declension during training can now be associated with a relevant embedding. An obvious drawback of the approach is to potentially associate a same\nrepresentation to multiple words of different meanings, e.g. runner, ran, runs, and runnable. This exacerbates the issue of polysemy already present with standard word embeddings. A set of homonyms are indeed mapped a single \u2013 thus necessarily ambiguous \u2013 representation. Concretely, we replace every word in the input question and/or answer by its stem, obtained either with the classical Porter algorithm [20], or with the dictionary-based algorithm of the Stanford CoreNLP library [16]. Sharing embeddings between questions and answers Our baseline implementation learns independent embeddings for words in questions and answers. One may hypothesize that both inputs could benefit from similar representations, and we compare independent embeddings versus a common shared one. The latter reduces the number of parameters to learn, but it forces the semantics of questions and answers to be represented identically. Test-time exemplar retrieval A hallmark feature of Z.S.\u2013capable methods is their extensibility to novel concepts without retraining. We implement such a capability by retrieving, at test-time, exemplar images from the web for all words (known or unknown) in the test questions and/or answers. Concretely, we build an additional representation of the question and/or of the candidate answers by retrieving the top-k images of each of its words from Google Images (k=1 to 4). We extract global CNN features from the images (as described above for the input image) and average them over words and exemplars. The resulting vector of dimension 2048 is referred to as a visual embedding."}, {"heading": "4.3. Improved image representations", "text": "Explicit object detection In addition to the global CNN features to represent the input image, we consider using explicit detections of objects in the scene. We obtain candidate detection from the YOLO method pretrained on Pascal VOC [21] as set of detections scores, each with the class of the detected object. We keep all detections above a certain threshold (varied throughout multiple experiments as to vary recall). We turn the set of detections into a fixedsize vector with a similar bag of words as for our questions (see above): we associate the possible classes with a learned embedding (i.e. a look-up table) and sum these embeddings over all detections. Semantic object class embeddings The detections presented above do not asociate any semantic prior on the classes considered by the detector. Those classes are however known by their name, and we experiment associating the detections with the pretrained GloVe word embedding (as used for the language model, see above) of their recognized class. We simply replace the look-up table with the pretrained embeddings of the words corresponding to class names. We refer to this representation as semantic class embeddings.\nOrder embeddings We experiment with the idea of imposing an order between the representation xQI of question/image and xA of candidate answers, as proposed by Vendrov et al. for other vision and language tasks [25]. Whereas our baseline uses a symmetric product to relate xQI and xA, the idea of an order embedding is to place a hierarchy between the two modalities by measuring their compatibility with an antisymmetric operation (consult [25] for details). Practically, we replace Eq. 2 with\nxQIAi = max ( 0, |f(W3xA + b3)| (4)\n\u2212 |f(W4xQI + b4)| )2\n(5)\nThis imposes a partial order over the spaces of xQI and xA, the candidate answers being placed higher in the hierarchy (with smaller absolute coordinates) and thus deemed more general than a particular pair of question/image. Crucially, we experiment with a reversed ordering by swapping xQI and xA in the above, which results in a dramatically lower performance (see Section 5.2)."}, {"heading": "5. Experiments", "text": "We conduct extensive experiments on both the original and the zero shot splits of the Visual7W dataset. As hypothesized in our premise, we observe different behaviors in the two cases, and the proposed improvements have different impact on the overall average performance of the two settings. Each experiment below considers one variation at a time of the baseline model including pretrained word embeddings of dimension 300, unless otherwise noted. Pretrained word embeddings are common practice and have a large positive impact, and they thus constitute our de facto reference for fair comparisons of additional improvements. Implementation details are provided in the supplementary material."}, {"heading": "5.1. Masking inputs", "text": "We first obtain an indication of the difficulty of the datasets by training a model with limited input, masking the question and/or the image. This forces the model to rely on dataset biases. Indeed, when masking both the question and the image, the only input is the set of multiple-choice answers, and the model can only learn to pick the common ones seen during training. As observed before [11], this strategy is sufficient to achieve a high performance with the standard splits. It is far less effective in the Z.S. setting, giving for example, when masking the question, 62.7% vs 52.6% in the standard and Z.S. settings, respectively (see Table 2 and Fig. 4, bottom right). In other words, answers in the Z.S. setting cannot be as easily guessed."}, {"heading": "5.2. Improved representations", "text": "Pretrained word embeddings We compare pretrained GloVe word embeddings of dimension 50 to 300 and an\nembedding learned from scratch of dimension 300 (noted \u201c0\u201d in the plot). Pretrained word embeddings over learned ones have the largest impact of all tested improvements in both the standard and Z.S. settings (Fig. 4, bottom left). There is an appreciable correlation between accuracy and the dimension of the embedding. Pretrained embeddings are more beneficial to represent the candidate answers than the questions, which can be explained by the larger amount of data (i.e. question words) available to learn the latter. We evaluate fine-tuning the pretrained embeddings with a relative learning rate between 0 and 1 to the other network parameters. Some fine-tuning always proves beneficial, but the Z.S. setting favors a smaller learning rate (Fig. 4, top left). We suppose that fine-tuning rate may otherwise significantly alter the embeddings of frequent training words. The remaining of the model then co-adapts, but the embeddings of rare and zero-shot words will however no be updated as much, leading to a negative performance impact in the Z.S. setting. Finally, we oberve that fine-tuning a common shared embedding between question and answer performs worse than independent ones. We conclude that the information captured by pretrained embeddings is relevant but not perfectly adequate for VQA, and that a same representation cannot capture the ideal semantics from both questions and answers.\nSharing embeddings across word stems We obtain a clear benefit from sharing embeddings across words of common stem (Fig. 4, top center). The procedure reduces the number of unique embeddings by about 25%, which regularizes their learning, and addresses some of the novel words at test time by mapping them to known stems. The observed impact is indeed larger in the Z.S. setting than on the standard one. The quality of the stemming algorithm does matter. The classical rule-based Porter algorithm performs worse than our baseline, and the improvements are obtained with a modern algorithm [16].\nTest-time exemplar retrieval We evaluate the proposed visual embeddings for representing the question, the candidate answers, or both. We obtain a net advantage in the Z.S. setting (Fig. 4, top right), correlated with the number of retrieved examples. The benefit is only appreciable when including visual embeddings for both the question and the answers. This indicates that the network may not succeed in learning to correlate visual features of the input image and of the visual embeddings, but only between the visual embeddings of the question and answers. A possible culprit is the different nature of top retrievals from Google and of the images in the Visual7W dataset. Surprisingly, visual embeddings impact performance negatively in the standard setting. We suspect that the language cues and dataset bi-\nases that can be exploited in the standard setting are more reliable than the visual embeddings. The inclusion of the latter during inference results in a negative impact frequent enough to hurt overall performance. In the Z.S. setting, the balance is shifted to more test cases that benefit from visual embeddings, resulting in a net benefit on overall performance.\nOur straightforward implementation for using exemplars only hints at potential benefits. Obvious extensions include applying it only to prominent words (images for does or will are likely uninformative) and retrieving exemplars of complete expressions (images of blue jay are more informative than images of blue and jay). Test-time use of novel exemplars is akin to the setting of one-shot and few-shot recognition methods (e.g. [26]) which could be adapted here.\nExplicit object detections We use detections at different levels of recall from YOLO [21] by varying a threshold on the minimum detection score (a specific model is trained for a specific threshold). The optimal trheshold lies in a tight range (Fig. 4, middle left). A low recall misses important objects, while too high a recall can overwhelm the VQA system with irrelevant detections (false positives). An open research question is how to better integrate those detections, possible with attention mechanisms. At the optimal thresh-\nold, we obtain a minor improvement with the proposed semantic class embeddings. Order embeddings The proposed order embeddings significantly improves over a symmetric interaction between features. Crucially, we verify that the improvement is caused by the actual order imposed on the embeddings and not merely by the different interactions. To do so, we replace the proposed order (xA above than xQI) by its reverse, which results in performance well below the baseline (Fig. 4, middle center). Data augmentation We propose a simple form of data augmentation with additional training examples of incorrect answers. Our model ultimately measures the compatibility between a question/image and a candidate answer, and the intuition is to expand training to more combinations, drawn randomly within mini-batches to form additional incorrect candidate answers. The procedure proves beneficial (Fig. 4, middle right), with a larger relative improvement in the Z.S. setting. The augmentation ratio correspond to the fraction of additional pairs of question / candidate answer (originally four per question)."}, {"heading": "5.3. Comparison with the state-of-the-art", "text": "We finally evaluate a model incorporating all proposed improvements (see Table 2). It achieves best performance overall in both the standard and Z.S. splits. The relative gains from combined improvements are not strictly cumulative, which indicates some overlap in the capability brought in by each. Part of the individual gains is likely attributable to increased model capacity, the benefit of which saturates at some point. On the standard splits, our best model clearly surpasses the existing state-of-the-art on this dataset [11]. We also trained our baseline and best models on reduced training data (random subsets). We appreciate a smooth drop-off in performance, especially in the Z.S. setting with our best method (Fig. 4, bottom center). This indicates good generalization, which, as argued in the introduction, should be a chief objective of VQA systems."}, {"heading": "6. Conclusions", "text": "This paper defined a setting of visual question answering where questions and answers contain words that were not seen during training. We rearranged the Visual7W dataset to allow an evaluation that focuses exclusively on such test cases. This setting requires more generalization capabilities and leads to a more honest evaluation of deep image understanding. This setting also motivates alternative strategies. We showed that additional, auxiliary data, used for pretraining language visual representations as well as during test time was beneficial, not only for ZS-VQA, but in the traditional setting as well. The extensions of those strategies constitute promising directions for future research."}, {"heading": "B. Examples of test questions", "text": "We provide below additional examples from the proposed zero-shot test split of the Visual7W dataset, in the same format as in Fig. 2.\nHow would you hear this cat coming ? 4 bell on his collar\nmeowing walking on piano walking through water\nWhat animals are these ? 4 giraffes\nelephants zebras emus\nWhat is in the background ? meadows lakes trees\n4 hills\nWhat kind of picture ? 4 black white\nphotoshopped sepia tone color\nWhen was this photo taken ? july december august\n4 during say\nWhat has happened to the truck over time ?\nfell apart paint came off dirty\n4 rusting\nWhat time of day is it ? lunchtime\n4 daytime morning teatime\nWhere is the cow ? petting zoo barn milking pen\n4 field\nWhere is this picture taken ? swimmimg pool playground volleyball court\n4 tennis court\nWhat has many archways ? house garden\n4 building bridge\nWhere are the giraffes ? in jungle on reserve on plains\n4 in zoo\nWhere is the number 06 ? on back bottom middle\n4 bottom left upper right\nWhat is written on the train ? 4 csx\n1017 zap love\nWhat gender dominates this picture ?\nmen both transgender\n4 women\nWhat is in the background ? forests sea\n4 window mountains\nWhy is she on the side of the road ?\nwalking hitchhiking broken down\n4 selling food\nWho is in the picture ? leprechauns captain ship\n4 woman elephant\nHow does the bathroom look ? discusting messy\n4 clean dirty\nWhat brown object is around the cow \u2019s neck ?\nrope 4 rope\nbell twine\nWhat colors are the zebra ? milticolored grey\n4 black white striped\nWhat is the hand like ? bruised browned 4 tanned darkened\nWhat is the suitcase for ? carrying diamonds\n4 trip international man mystery donating clothes drive\nWhat sport are they displaying ? snowboarding\n4 skiing sledding bogging\nWhat color is the car ? 4 silver\nwhite redl black\nHow is the photo ? fuzzy underdeveloped\n4 clear tilted\nWhere is the water ? 4 behind birds\nin pool in birdbath in pond\nWhat time of day is it ? midnight dusk\n4 daytime 6:48 pm\nWhy is the truck in a ditch ? avoided crash\n4 wheels came off h deer driver fell asleep\nWho is crossing the intersection ? policman chicken old woman\n4 woman man\nWhat is white ? wedding dress childs skin snow 4 clock\u2019s face\nWhat time is it ? 11:09 4 12:09 12:14 12:05\nWhat does the white sign say ? mott\u2019s turn right\n4 keep left upstairs\nWhat is printed on the road ? 4 white arrows\nwhite line numbers spraypainted by utility company double yellow lines\nWhere is the drawer unit ? behind bed in front bed\n4 left bed right bed\nWho captured this photo ? mario bertoli anthony bourdain guy fieri\n4 photographer\nWhy is the girl on the horse ? get somewhere fun pleasure\n4 ride\nWhat does the sign say ? yield 4 compact cars only speed bumps ahead slow children at play\nHow is the man dressed ? in jeans t shirt in work su\n4 in uniform in sweatsu\nWhen was this picture taken ? while was raining during evening at celebration 4 during waking hours\nWhat kind of tree is shown ? 4 weeping willow\nmaple oak cherry\nWhere was this picture taken ? at beach at pond at gulf\n4 at ocean\nWhat does the bottom sign say ? fifth ave caution 4 bigelow ave n 450 lesly st\nHow are the scissors arranged ? on top each other close together next each other 4 overlapping 1 another\nWhat is on the woman \u2019s hands ? dirt wedding ring\n4 gloves moisturizing lotion\nWhat is the green plant for ? decoration\n4 elephant eat cooking seasoning\nWhat is on the desk ? lunchbox weekly planner laptop\n4 empty pencil holder\nWhy is it a rounded picture ? panoramic view cut way\n4 camera lens fitted frame\nWhat type of photo is shown ? outside inside unfocused\n4 black white\nWhat is in the foreground ? atv\n4 train car bike\nWho has longer wool ? unsheered sheep cold weather sheep\n4 adult sheep better fabric shop\nWho is shown ? man crowd protestors child\n4 few people\nWhat colors contrast in this picture ?\ngray black\n4 blue white tan emerald\nWhat drink is advertised on the truck ?\npepsi 4 vita coco\nmountain dew coke cola\nWhere is there a bird ? 4 flying over ocean\nin birdcage in branches on ground\nWhat are the yaks eating ? hay\n4 grass feed clover\nWhat condition is the old stove ? some chips on enamel perfect dangerous leak\n4 antique\nWhat does the monitor in the upper left of the photo say ?\nno pain no gain just do\n4 rock bike rock boat\nWhat is the number of busts in the room ?\n3 1 4\n4 2\nWhat does the sign say ? stop wet paint pedestrian walking 4 nothing but bumps\nHow do you know it \u2019s an outdoor scene ?\ni can hear birds 4 sunny\nplenty deep shadows sun out\nWhat type of boat is in the picture ?\nrowboat canoe\n4 fishing boat yacht\nWhat color are the giraffe spots ? 4 brown\ntanish brown black reddish brown\nWhat is on the sink ? spoon 4 scrub brush cup plate\nWhat color are the double-deck buses ?\nyellow green\n4 red white\nWho is playing ? borg evert 4 rafael nadal sampras\nWho is riding on the elephant ? monkey sultan some kids\n4 man\nWho has their suit jackets buttoned ?\nall men 4 some men\n3 men only 1 man\nWho has a sewing kit ? everyday person tailor\n4 traveler seamstress\nWhy is there a fire ? ambiance\n4 cold roast marshmelloes burning yard waste\nWhere are smudges of dirt ? gloves shoes shirt\n4 pants\nWho is standing on the tennis court ?\nball boy umpire\n4 tennis player opponent\nWhere is this picture taken ? soccer game 4 big wheels demo swimming competition tennis match\nWhat is on the plate ? dog food scraps cat food\n4 food\nWho is in this picture ? women kids preachers\n4 men\nWhat is the table made of ? plastic mahogany\n4 wood lucite\nWhat is the status of the tv ? paused on\n4 off broken\nWhat is the woman doing ? 4 hitting tennis ball\nhitting hockey puck throwing football shooting basketball\nWhat kind of video game is it ? soccer basketball\n4 golf mine craft\nWhat has tassels ? 4 drapery valance\nwestern style vest graduation cap fancy blouse\nHow is the place ? 4 bushy\nrocky clear hilly\nWhy is this photo illuminated ? 4 sunlight\nphoto effects moon stream effects\nWhat kind of bus is this ? 4 double decker\n1 moves red 1 big 1"}], "references": [{"title": "Zipfs law and the internet", "author": ["L.A. Adamic", "B.A. Huberman"], "venue": "Glottometrics, 3(1):143\u2013150,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "Proc. IEEE Int. Conf. Comp. Vis.,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": "Proc. Conf. Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "arXiv preprint arXiv:1606.01847,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. Int. Conf. Artificial Intell. & Stat., pages 249\u2013256,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Pointing the unknown words", "author": ["\u00c7. G\u00fcl\u00e7ehre", "S. Ahn", "R. Nallapati", "B. Zhou", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Proc. Advances in Neural Inf. Process. Syst., pages 1682\u20131690,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards a Visual Turing Challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "arXiv preprint arXiv:1410.8027,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Proc. IEEE Int. Conf. Comp. Vis.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u2013 60,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "An algorithm for suffix stripping", "author": ["M. Porter"], "venue": "Program, pages 130\u2013137,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1980}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S.K. Divvala", "R.B. Girshick", "A. Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Image Question Answering: A Visual Semantic Embedding Model and a New Dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Proc. Advances in Neural Inf. Process. Syst.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph-structured representations for visual question answering", "author": ["D. Teney", "L. Liu", "A. van den Hengel"], "venue": "arXiv preprint arXiv:1609.05600,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Order- Embeddings of Images and Language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "Proc. Int. Conf. Learn. Representations,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Matching networks for one shot learning", "author": ["O. Vinyals", "C. Blundell", "T.P. Lillicrap", "K. Kavukcuoglu", "D. Wierstra"], "venue": "arXiv preprint arXiv:1606.04080,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Explicit knowledge-based reasoning for visual question answering", "author": ["P. Wang", "Q. Wu", "C. Shen", "A. v. d. Hengel", "A. Dick"], "venue": "arXiv preprint arXiv:1511.02570,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "What Value Do Explicit High Level Concepts Have in Vision to Language Problems", "author": ["Q. Wu", "C. Shen", "A. v. d. Hengel", "L. Liu", "A. Dick"], "venue": "In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Image captioning and visual question answering based on attributes and their related external knowledge", "author": ["Q. Wu", "C. Shen", "A. v. d. Hengel", "P. Wang", "A. Dick"], "venue": "arXiv preprint arXiv:1603.02814,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Visual Question Answering: A Survey of Methods and Datasets", "author": ["Q. Wu", "D. Teney", "P. Wang", "C. Shen", "A. Dick", "A. van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. Dick", "A. v. d. Hengel"], "venue": "In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Hierarchical memory networks for answer selection on unknown words", "author": ["J. Xua", "J. Shia", "Y. Yaoa", "S. Zhenga", "B. Xua", "B. Xu"], "venue": "arXiv preprint arXiv:1609.08843,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 20, "endOffset": 31}, {"referenceID": 1, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 20, "endOffset": 31}, {"referenceID": 27, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 20, "endOffset": 31}, {"referenceID": 5, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 185, "endOffset": 192}, {"referenceID": 12, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 185, "endOffset": 192}, {"referenceID": 27, "context": "A number of VQA datasets have been introduced, and a variety of methods have demonstrated impressive (yet possibly converging) results (see [30] for a survey).", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34].", "startOffset": 171, "endOffset": 182}, {"referenceID": 9, "context": "A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34].", "startOffset": 171, "endOffset": 182}, {"referenceID": 31, "context": "A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34].", "startOffset": 171, "endOffset": 182}, {"referenceID": 9, "context": "For example, the stateof-the-art method in [11] achieves an impressive accuracy of almost 65% correct answers on the Visual7W dataset.", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "Consult [30] for a recent survey of the literature.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 10, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 19, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 16, "context": "the vectors used to represent the words, can be pretrained on a language modeling task [19, 17].", "startOffset": 87, "endOffset": 95}, {"referenceID": 15, "context": "the vectors used to represent the words, can be pretrained on a language modeling task [19, 17].", "startOffset": 87, "endOffset": 95}, {"referenceID": 20, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 4, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 9, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 21, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 21, "context": "The syntactic structure of language for VQA has received much less attention, but recent work suggest that explicit parsing can bring useful information [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "An exception is [28], where the authors use language as an explicit intermediate representation for VQA.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Finally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29].", "startOffset": 96, "endOffset": 108}, {"referenceID": 24, "context": "Finally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29].", "startOffset": 96, "endOffset": 108}, {"referenceID": 26, "context": "Finally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29].", "startOffset": 96, "endOffset": 108}, {"referenceID": 28, "context": "In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.", "startOffset": 55, "endOffset": 67}, {"referenceID": 24, "context": "In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.", "startOffset": 55, "endOffset": 67}, {"referenceID": 26, "context": "In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.", "startOffset": 55, "endOffset": 67}, {"referenceID": 1, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 50, "endOffset": 65}, {"referenceID": 10, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 50, "endOffset": 65}, {"referenceID": 31, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 50, "endOffset": 65}, {"referenceID": 0, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "Let us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 29, "context": "Let us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 3, "context": "Let us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 2, "context": "This distinction reflects the fact that CNNs pre-trained on ImageNet [4] are commonly used in existing VQA methods, and the fact that VQA is the task that we are actually interested in.", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "The words used in the questions and answers of VQA datasets follow a long-tail distribution typical in natural language [12, 36].", "startOffset": 120, "endOffset": 128}, {"referenceID": 10, "context": "Visual7W is itself a subset of the Visual Genome dataset [12] the highest quality dataset for VQA currently available, in terms of size, answer distribution, human performance, and the quality of the multiple choice answers.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35].", "startOffset": 83, "endOffset": 94}, {"referenceID": 9, "context": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35].", "startOffset": 83, "endOffset": 94}, {"referenceID": 32, "context": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35].", "startOffset": 83, "endOffset": 94}, {"referenceID": 8, "context": "The image is represented with global (image-wide) features of dimension 2048 extracted from the last pooling layer of a ResNet-152 [10] pretrained for image recognition on ImageNet.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "This common practice [19, 17] has two advantages.", "startOffset": 21, "endOffset": 29}, {"referenceID": 15, "context": "This common practice [19, 17] has two advantages.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "Concretely, we replace every word in the input question and/or answer by its stem, obtained either with the classical Porter algorithm [20], or with the dictionary-based algorithm of the Stanford CoreNLP library [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "Concretely, we replace every word in the input question and/or answer by its stem, obtained either with the classical Porter algorithm [20], or with the dictionary-based algorithm of the Stanford CoreNLP library [16].", "startOffset": 212, "endOffset": 216}, {"referenceID": 18, "context": "We obtain candidate detection from the YOLO method pretrained on Pascal VOC [21] as set of detections scores, each with the class of the detected object.", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "for other vision and language tasks [25].", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Whereas our baseline uses a symmetric product to relate x and x, the idea of an order embedding is to place a hierarchy between the two modalities by measuring their compatibility with an antisymmetric operation (consult [25] for details).", "startOffset": 221, "endOffset": 225}, {"referenceID": 9, "context": "As observed before [11], this strategy is sufficient to achieve a high performance with the standard splits.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "The classical rule-based Porter algorithm performs worse than our baseline, and the improvements are obtained with a modern algorithm [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "LSTM Q+I [15] 52.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "6 \u2013 \u2013 \u2013 \u2013 MCB [6] 62.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "[11] 64.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26]) which could be adapted here.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Explicit object detections We use detections at different levels of recall from YOLO [21] by varying a threshold on the minimum detection score (a specific model is trained for a specific threshold).", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "On the standard splits, our best model clearly surpasses the existing state-of-the-art on this dataset [11].", "startOffset": 103, "endOffset": 107}], "year": 2016, "abstractText": "Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-theart performance in the standard VQA evaluation setting.", "creator": "LaTeX with hyperref package"}}}