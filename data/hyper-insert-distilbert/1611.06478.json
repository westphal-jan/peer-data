{"id": "1611.06478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Visualizing Linguistic Shift", "abstract": "neural visual network based models are a quite very powerful tool for creating word embeddings, the objective of these based models is best to group two similar programming words literally together. despite these visual embeddings historically have effectively been basically used in as features to improve input results in various applications such as document facial classification, alternatively named entity recognition, etc. neural neurological language display models are simply able to learn interesting word representations which have later been used to capture semantic shifts efficiently across subject time and geography. supposedly the objective of managing this objective paper is to first identify and then uniquely visualize almost how few words change meaning in different text corpus. perhaps we too will train a neural neural language model on texts formulated from a pretty diverse set palette of content disciplines philosophy, muslim religion, fiction ethics etc. sometimes each text will considerably alter the traditional embeddings rather of moving the words to uniquely represent easily the meaning of the word printed inside almost that text. we will present essentially a computational linguistics technique to detect words that exhibit significant linguistic ability shift in meaning and usage. we thus then possibly use enhanced scatterplots and storyline cognitive visualization to visualize exactly the linguistic shift.", "histories": [["v1", "Sun, 20 Nov 2016 06:50:16 GMT  (291kb)", "http://arxiv.org/abs/1611.06478v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["salman mahmood", "rami al-rfou", "klaus mueller"], "accepted": false, "id": "1611.06478"}, "pdf": {"name": "1611.06478.pdf", "metadata": {"source": "CRF", "title": "Visualizing Linguistic Shift", "authors": ["Salman Mahmood", "Rami Al-Rfou", "Klaus Mueller"], "emails": ["@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "Keywords: linguistic change, neural language models, word embeddings, storyline visualization."}, {"heading": "1 INTRODUCTION", "text": "The internet is a collection of materials produced by people from different time, geography, culture and disciplines. Therefore, the language on the internet demonstrates a rich variety of words and meanings. Words can gain new meanings which might be relevant to only a niche group of people. As the internet becomes more and more conversational, it is necessary to detect these linguistic changes.\nWords can have more than one meanings; however, the context of the word can be used to determine the intended meaning of the word. For example, the word \u201cfair\u201d has multiple meanings. But in the sentence \u201cIt is not fair that I have to do all the work\u201d, the word fair means reasonable or just, whereas in the sentence \u201cIf you have fair skin you are more likely to get sunburnt\u201d, the meaning of the word fair is light or pale skin. In these sentences it is clear that the context in which the word is used can determine the meaning of the word. Therefore, the semantic distance between words can be determined by the context in which the word is used.\nIn order to find the semantic distance between words various models have been developed that use vectors to keep track of context in which the word was used, and then use different geometric techniques on the vectors to measure the similarity between words. The simplest method to achieve this is by creating a raw co-occurrence matrix of all the words that exist inside a window of a certain size. This method doesn\u2019t perform that well and better results can be achieved by using transformations on the data, such as reweighting the counts and using dimensionality reduction to smooth the data. These vector representations can then be used to calculate the linguistic shifts in the word. A lot of\nSalman Mahmood, Rami Al-Rfou and Klaus Mueller are with Stony Brook\nUniversity, E-mail:{samahmood, ralrfou, mueller} @cs.stonybrook.edu\nresearch has been directed towards linguistic shift, which we will discuss in section 2.\nThe objective of this paper is to use neural language models to find words that exhibit linguistic shifts in corpora belonging to different disciplines and then be able to visualize the linguistic shift. In order to find interesting words, we will be using neural language models to learn word embeddings. We propose a computational technique which allows us to discover words that have experienced significant change. In order to visualize the linguistic shift, we propose the use of storyline visualizations and scatterplots.\nThe structure of this paper is as follows in section 2 we will detail the related work, section 3 provides the problem statement, in section 4 we will present our method, section 5 discusses the different methods that can be used for visualization and finally section 6 provides the conclusion and future work."}, {"heading": "2 RELATED WORK", "text": "Since our work lies at the intersection of different fields we will divide this section into four parts (1) language shift, (2) language shift visualization (3) word embeddings (4) storyline visualization."}, {"heading": "2.1 Linguistic Shift", "text": "There is a large body of work that deals with how language changes, Bamman et al. [1] use Twitter data to study language shift in different genders, Bamman et al. [2, 11] also explore how geography and other contextual factors affect word meaning by using vector space representation of words. Einstein et al. [10] propose a model that identifies geographic and demographic factors that spread new words in online text. [8,12] present a model for measuring language change over time. In [18] Muhammad et al. present detailed study of distribution measures."}, {"heading": "2.2 Linguistic Shift Visualization", "text": "Recently there has been an increase in the use of visual and dynamic applications in corpora and literature studies. Hilpert [6] uses dynamic motion charts to track the arrangements of the relative words, the sequence of graphs give the researcher an understanding of the complicated linguistic development. Motion charts are also used by others in this area as well [4, 5, 24]. Siirtola et al. [24] designed a tool, Text Variation Explorer (TVE), that allows interactive examination of the behavior of linguistic parameters affected by text window size and overlap, and performs interactive principal component analysis based on a user given set of words. Kulkarni et al. [12] use dimension reduction on their word vector and visualize them using a scatterplot."}, {"heading": "2.3 Word Embeddings", "text": "The mapping of concepts to continuous space can be traced back to Hinton [Hinton] who proposed that concepts can be represented by distributed patterns of activity in networks on neuron like units. Bengio et al. [3] presented a neural language model for learning word embeddings, their model outperformed the traditional n-gram based techniques. Various methods have been proposed that scale and speed up large neural models [19, 23]. Mikolov et al. [15]\nproposed skip-gram models for learning word embeddings and demonstrated that these models have the capacity to learn linguistic patterns as linear relationships between vectors [16, 17]. These embeddings have been used in various Natural Language Processing tasks like such as named entity recognition (NER). Tsuboi et al. [27] incorporated word2vec embeddings with GloVe embeddings into a Part of Speech (POS) tagging system and show that it produces better results compared to the individual systems."}, {"heading": "2.4 Storyline Visualization", "text": "Storyline visualizations have been inspired by the xkcd comic \u201cMovie Narrative Charts\u201d [20]. In a storyline visualization we visualize a collection of entities using line charts, where the lines converge if there is an interaction between the entities and diverge otherwise. Storyline visualizations have been used in visualization tools for wide variety of datasets. Vehlow et al. [28] use this technique to visualize dynamic community structure, by portraying communities as converged entities. Kim et al. [10] visualize genealogical data where individuals are represented using lines that converge and diverge to indicate marriage and divorce. Cui et al. [4] use text stream to visualize the evolution of topics, topics merge and split as they interact with each other.\nA lot of research effort has been directed towards creating a generic visualization scheme, Ogievetsky et al. created an online editing tool, Plot Weaver [21], that allows users to create a storyline visualization by interactive editing. Automated schemes have also been produced by Tanahashi et al. [25] and Liu et al. [14], they have created schemes that produce results on par with professional artists. Further optimizations have been suggested by Kostitsyna et al. [11] and Tanahashi et al. [26] have developed a scheme to create storyline visualizations for streaming data."}, {"heading": "3 PROBLEM STATEMENT", "text": "Our objective is to find words that exhibit significant shift in meaning across corpora from different disciplines. Given a corpora \ud835\udc36, that contains texts from different fields \ud835\udc39, for example, physics, law, religion etc, where corpus from each discipline is defined as \ud835\udc36#. We build a common vocabulary \ud835\udc49 by intersecting the words in the corpus of each discipline \ud835\udc36# . For each word \ud835\udc64 in \ud835\udc49 we want to quantify how much that word changed meaning and be able to visualize how the word evolved across different corpus."}, {"heading": "4 METHOD", "text": "An overview of the method is given in Figure 1, the method can be divided into three phases, we will first perform some data preprocessing, we then train the embeddings on each corpus using a neural language model and the final step is to identify words that exhibit the most shift in meaning."}, {"heading": "4.1 Data Preprocessing", "text": "The text from the corpora is converted into sentences and all the punctuation is removed from the text. The initial training is done on a large dataset in order to make sure that there is a large number of words in model and that the embeddings of these words are well defined. We use the Wikipedia dataset to train the initial word2vec model."}, {"heading": "4.2 Word Embeddings", "text": "Word2vec is a neural language model that can be used to train words in one of two ways, the first method predicts the word given the context, the model uses the preceding and following words therefore the input to the model is \ud835\udc64\ud835\udc61 - 2, \ud835\udc64\ud835\udc61 - 1, \ud835\udc64\ud835\udc61 + 1, \ud835\udc64\ud835\udc61 + 2 and the output of the neural network will be the word \ud835\udc64\ud835\udc61. This method is called continuous bag of words (CBOW). The second method predicts the context given the word, the model takes as input the word \ud835\udc64\ud835\udc61 and returns the context i.e. is \ud835\udc64\ud835\udc61 - 2, \ud835\udc64\ud835\udc61 - 1, \ud835\udc64\ud835\udc61 + 1, \ud835\udc64\ud835\udc61 + 2. This method is called skip-gram. We will be using the skip-gram model in our experiments because it is more suitable for large datasets, we use a window size of 5.\nAs shown in Figure 1 we train the model on the texts of corpus from different disciplines. We start by training the first model and saving the normalized word embeddings of the model, we repeat the process for all texts inside the corpora. We now have the embeddings of each training set. Here it should be mentioned that the order of the corpora might alter the results since we are building on the results of the previous corpus.\nGiven a corpora \ud835\udc36, with text from different fields \ud835\udc39 our objective is to learn field specific word embeddings \ud835\udf19# \u2236 \ud835\udc49, \ud835\udc36# \u21a3 \u211d, . Where \ud835\udc49 is vocaubulary of the corpora, and \ud835\udc36# represents the corpus of the field \ud835\udc53. Our objective is to find a word representations that can be used to predict the context words. Given a word \ud835\udc64/ , the context words are defined as the words that appear before or after \ud835\udc64/ within a window of size n. We model the probability of a context word \ud835\udc640 given \ud835\udc64/ as:\nPr \ud835\udc640 w4 = 678 ( :; < =>) 678 ( :; < =>)@A \u2208 C (1)\nhere w/ is the vector representation of \ud835\udc64/. To train the model we iterate over each word occurrence of \ud835\udc36# to minimize the negative log-likelihood of the context word. The objective function is defined as follows: \ud835\udc3d = \u2212 log Pr \ud835\udc640 w4IJK0KJ\n0 !MN => \u2208 OP (2)\nHere n is the size of the context window. This formulation is proportional to the size of \ud835\udc49 , which is generally very large. We optimize the process by mapping the problem from a classification of 1-out-of-V to a hierarchical classification problem [19]. This reduces the complexity from \u039f( \ud835\udc49 ) to \u039f(log \ud835\udc49 )."}, {"heading": "4.3 Distance Metric", "text": "The word embeddings that were learned are then used to find words that exhibit maximum linguistic change, in order to find these words, we use an ensemble technique in which we use the sum of two distance metrics, step-wise Euclidean distance and the nearest neighbor distance.\nThe step-wise Euclidean distance is calculated by finding the distance moved by the word in the embeddings of each model and this distance is normalized by using the log of the word count in each model. To be more concrete for a word \ud835\udc64 the step-wise Euclidean distance is defined as follows:\n\ud835\udc38\ud835\udc48\ud835\udc36 \ud835\udc64 = TUV WXYP\nZ,WXYP[\\ Z\n]^_ (`VZ P )# \u2208a\n(3)\nwhere \ud835\udc38\ud835\udc48\ud835\udc36 \ud835\udc64 is the step-wise Euclidean distance, \ud835\udc38\ud835\udc5a\ud835\udc4f#` is the embedding of the word \ud835\udc64 in field \ud835\udc53. \ud835\udc52\ud835\udc62\ud835\udc50(\ud835\udc4e, \ud835\udc4f) is the Euclidean distance between vectors a and b, and \ud835\udc64\ud835\udc50`\n# is the word count of \ud835\udc64 in \ud835\udc53.\nWords that have similar context tend to have similar meaning, therefore a change in the nearest neighbors of a word is a good indicator of a shift in the meaning of a word. The nearest neighbor distance is the difference between the nearest neighbors of two words, summed over all embeddings. More precisely the nearest neighbor distance is defined as follows:\n\ud835\udc41\ud835\udc41 \ud835\udc64 = iI JJj Z,k \u2229 JJj Z,k[\\ k \u2208m\n]^_ (`VZ P )\n(4)\nWhere \ud835\udc3e is the number of nearest neighbours, \ud835\udc5b\ud835\udc5bp `,TIq is defined as the \ud835\udc3e nearest neighbors of the word \ud835\udc64 in the embeddings e, and \ud835\udc64\ud835\udc50`\n# is the word count of \ud835\udc64 in \ud835\udc53. In our experiments we used"}, {"heading": "K =20.", "text": ""}, {"heading": "5 VISUALIZATION", "text": "Visualizing linguistic shift can be challenging, simply observing the change in embeddings of a word does not suffice. In order to provide a frame of reference we will use neighboring words at each segment. The neighboring words will have a similar context and hence they can provide an understanding of what the word means at that point. In order to visualize the linguistic shifts in the data we will be using enhanced scatterplots and storyline visualization. In this section we will explain the methodology used to create these visualizations, in order to create these visualization, we trained our model on corpus from four different fields, Wikipedia, Fiction (Lord of the Rings trilogy), Religious (the King James version) and Politics (A Preface to Politics by Walter Lippmann). We used these texts because they represent a very diverse group and it will be interesting to see how words change meaning inside\nthese texts. These corpora are used to train the model and find the words that exhibit most linguistic shift, as described in section 4, Table 1 shows some of the results of our method, it shows a list of words and their nearest neighbour from the embeddings trained on different texts. We will demonstrate the visualizations using the word \u201cheart\u201d and show how different visualization methods can be used to observe how the word changes meaning in texts from different fields."}, {"heading": "5.1 Enhanced Scatterplot", "text": "The scatterplot shows how the word heart changes position in different embeddings. We use the nearest neighbors of the words to identify the meaning of the word. Since the embedding of the neighboring words also change we use the mean of embeddings across different sections. Then we apply a dimension reduction technique to reduce the number of dimensions to 2 and visualize the graph as a scatterplot (see Figure 2). The figure shows the word \u201cheart\u201d as it is affected by the text of different corpus. We can see that the word heart is closer to biological organs in Wikipedia corpus. In fictional works its meaning is closer to words representing pain like ache and throb. In the religious and political texts, the meaning is closer to feelings like cheerfulness and grief. The visualization is able to show how the word heart changes meanings across different corpus. The limitation of this visualization is that the words are close together which makes it a little difficult to determine how the words are grouped in the embeddings. Also since the dimensionality of the embeddings is generally very large, a lot of information is lost in the dimension reduction step."}, {"heading": "5.2 Storyline Visualization", "text": "Storyline visualization is a technique that has been inspired by the illustration of Munroe [20], the visualization shows the interaction between various characters in The Lord of the Rings movie. Lines that are clustered together depict the spatial proximity of the characters. Storyline visualization is an effective way to convey how relationships between different entities change over time. Storyline visualization fits nicely into our scheme since it allows us to show how the relationship between different words evolves over different segments. We believe that Storyline Visualization is an effective visualization technique for this scenario because even though it doesn\u2019t convey the exact location of the words but it very effectively conveys how the words are grouped together. It will allow us to determine if the word has moved to a different neighborhood and also show the entire evolution of the word.\nThe algorithm to draw storyline visualization attempts to arrange the lines in a way that minimizes the number of crossings of the lines. The method we use to generate storylines is inspired by [26], however their method is made primarily for streaming data and one of their main concerns is the conservation of the user\u2019s mental mapping during the constant layout updates. Therefore, their layout modification is based on only the previous time steps. Since we are not using streaming data we will use all the data available to create our layout.\nIn order to create the visualization for a given word we need to cluster words together. To find these clusters we find the K nearest neighbors of the target word at each embedding. Out of the K nearest neighbors we pick the top M words (where M < K) and cluster them together with the given word. To this cluster we then add any word in the top M words of the previous cluster which are also in the top K words of this segment. In the example shown we have used M =4 and K = 32.\nFigure 3 shows the linguistic shift of the word \u201cheart\u201d and the groupings of the words are much more apparent in this visualization as compared to the scatterplot, very clearly conveying the meaning of the word in each embedding. However, the information related to the distances between the words is lost, making it difficult to tell which word is the closest match."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "In this paper we have presented a method that can be used to find words that exhibit linguistic shift and we evaluated visualization techniques that can be used to visualize the linguistic shift of words. We show that the scatterplot visualization can be augmented with the storyline visualization to get more insight into the data. In the future we intend to create a comprehensive tool which can be used to analyse linguistic change in data. We will use some of the visualization techniques mentioned in this paper as well as provide other data, such Part of Speech tags, word frequency etc. that is commonly used in analysing linguistic change."}], "references": [{"title": "Distributed representations of geographically situated language.", "author": ["Bamman", "David", "Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Gender identity and lexical variation in social media.", "author": ["Bamman", "David", "Jacob Eisenstein", "Tyler Schnoebelen"], "venue": "Journal of Sociolinguistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Sen\u00e9cal. \"Adaptive importance sampling to accelerate training of a neural probabilistic language model.", "author": ["Bengio", "Yoshua", "Jean-S\u00e9bastien"], "venue": "Neural Networks, IEEE Transactions on 19.4", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Using Visual Analytics Tool for Improving Data Comprehension.", "author": ["G\u00e9ryk", "Jan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Dynamic visualizations of language change: Motion charts on the basis of bivariate and multivariate data from diachronic corpora.\"International", "author": ["Hilpert", "Martin"], "venue": "Journal of Corpus Linguistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "The time course of language change.\" Computers and the Humanities", "author": ["Juola", "Patrick"], "venue": "77-96.cognitive science society. Vol", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Diffusion of lexical change in social media", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": "PLoS ONE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Tracing genealogical data with timenets.", "author": ["Kim", "Nam Wook", "Stuart K. Card", "Jeffrey Heer"], "venue": "Proceedings of the International Conference on Advanced Visual Interfaces. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "On Minimizing Crossings in Storyline Visualizations.\"arXiv preprint arXiv:1509.00442", "author": ["Kostitsyna", "Irina"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet Language.", "author": ["Kulkarni", "Vivek", "Bryan Perozzi", "Steven Skiena"], "venue": "arXiv preprint arXiv:1510.06786", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Statistically Significant Detection of Linguistic Change.\"Proceedings of the 24th International", "author": ["Kulkarni", "Vivek"], "venue": "Conference on World Wide Web. International World Wide Web Conferences Steering Committee,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Storyflow: Tracking the evolution of stories.\" Visualization and Computer Graphics", "author": ["Liu", "Shixia"], "venue": "IEEE Transactions on 19.12", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Efficient estimation of word representations in vector space.\" arXiv preprint", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations.\" HLT- NAACL", "author": ["Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Distributional measures of semantic distance: A survey.\" arXiv preprint", "author": ["Mohammad", "Saif M", "Graeme Hirst"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model.", "author": ["Morin", "Frederic", "Yoshua Bengio"], "venue": "Proceedings of the international workshop on artificial intelligence and statistics", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Plotweaver.\" Ogievetsky. http://ogievetsky", "author": ["Ogievetsky", "Vadim"], "venue": "com/PlotWeaver", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Large-scale deep unsupervised learning using graphics processors.", "author": ["Raina", "Rajat", "Anand Madhavan", "Andrew Y. Ng"], "venue": "Proceedings of the 26th annual international conference on machine learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Text Variation Explorer: Towards interactive visualization tools for corpus linguistics.", "author": ["Siirtola", "Harri"], "venue": "International Journal of Corpus Linguistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Design considerations for optimizing storyline visualizations.\" Visualization and Computer Graphics", "author": ["Tanahashi", "Yuzuru", "Kwan-Liu Ma"], "venue": "IEEE Transactions on 18.12", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "An Efficient Framework for Generating Storyline Visualizations from Streaming", "author": ["Tanahashi", "Yuzuru", "C. Hsueh", "K. Ma"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Neural networks leverage corpus-wide information for part-of-speech tagging.", "author": ["Tsuboi", "Yuta"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Visualizing the evolution of communities in dynamic graphs.\"Computer", "author": ["C Vehlow"], "venue": "Graphics Forum. Vol. 34. No", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1] use Twitter data to study language shift in different genders, Bamman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2, 11] also explore how geography and other contextual factors affect word meaning by using vector space representation of words.", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[2, 11] also explore how geography and other contextual factors affect word meaning by using vector space representation of words.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[10] propose a model that identifies geographic and demographic factors that spread new words in online text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[8,12] present a model for measuring language change over time.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[8,12] present a model for measuring language change over time.", "startOffset": 0, "endOffset": 6}, {"referenceID": 15, "context": "In [18] Muhammad et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Hilpert [6] uses dynamic motion charts to track the arrangements of the relative words, the sequence of graphs give the researcher an understanding of the complicated linguistic development.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Motion charts are also used by others in this area as well [4, 5, 24].", "startOffset": 59, "endOffset": 69}, {"referenceID": 19, "context": "Motion charts are also used by others in this area as well [4, 5, 24].", "startOffset": 59, "endOffset": 69}, {"referenceID": 19, "context": "[24] designed a tool, Text Variation Explorer (TVE), that allows interactive examination of the behavior of linguistic parameters affected by text window size and overlap, and performs interactive principal component analysis based on a user given set of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] use dimension reduction on their word vector and visualize them using a scatterplot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] presented a neural language model for learning word embeddings, their model outperformed the traditional n-gram based techniques.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Various methods have been proposed that scale and speed up large neural models [19, 23].", "startOffset": 79, "endOffset": 87}, {"referenceID": 12, "context": "[15]", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "proposed skip-gram models for learning word embeddings and demonstrated that these models have the capacity to learn linguistic patterns as linear relationships between vectors [16, 17].", "startOffset": 177, "endOffset": 185}, {"referenceID": 14, "context": "proposed skip-gram models for learning word embeddings and demonstrated that these models have the capacity to learn linguistic patterns as linear relationships between vectors [16, 17].", "startOffset": 177, "endOffset": 185}, {"referenceID": 22, "context": "[27] incorporated word2vec embeddings with GloVe embeddings into a Part of Speech (POS) tagging system and show that it produces better results compared to the individual systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[28] use this technique to visualize dynamic community structure, by portraying communities as converged entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[10] visualize genealogical data where individuals are represented using lines that converge and diverge to indicate marriage and divorce.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] use text stream to visualize the evolution of topics, topics merge and split as they interact with each other.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "created an online editing tool, Plot Weaver [21], that allows users to create a storyline visualization by interactive editing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "[25] and Liu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14], they have created schemes that produce results on par with professional artists.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] and Tanahashi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[26] have developed a scheme to create storyline visualizations for streaming data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We optimize the process by mapping the problem from a classification of 1-out-of-V to a hierarchical classification problem [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "The method we use to generate storylines is inspired by [26], however their method is made primarily for streaming data and one of their main concerns is the conservation of the user\u2019s mental mapping during the constant layout updates.", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "Neural network based models are a very powerful tool for creating word embeddings, the objective of these models is to group similar words together. These embeddings have been used as features to improve results in various applications such as document classification, named entity recognition, etc. Neural language models are able to learn word representations which have been used to capture semantic shifts across time and geography. The objective of this paper is to first identify and then visualize how words change meaning in different text corpus. We will train a neural language model on texts from a diverse set of disciplines \u2013 philosophy, religion, fiction etc. Each text will alter the embeddings of the words to represent the meaning of the word inside that text. We will present a computational technique to detect words that exhibit significant linguistic shift in meaning and usage. We then use enhanced scatterplots and storyline visualization to visualize the linguistic shift", "creator": "Word"}}}