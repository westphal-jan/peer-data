{"id": "1611.06791", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Generalized Dropout", "abstract": "deep neural networks domains often naturally require good regularizers to generalize well. nonlinear dropout modeling is one such regularizer that is widely enthusiastically used among academic deep cognitive learning practitioners. recent work has partially shown recently that serial dropout can also basically be viewed as performing normal approximate bayesian inference behavior over simultaneously the same network parameters. in running this work, we generalize this generic notion and introduce a super rich family of clustered regularizers which we we simultaneously call massive generalized cache dropout. one set version of methods in this research family, called dropout + +, alone is generally a specific version of dropout with only trainable design parameters. classical sparse dropout topology emerges repeatedly as in a rather special case of this optimization method. hopefully another member of this programming family selects the preferred width of neural network layers. experiments currently show that these hybrid methods better help in improving generalization performance over dropout.", "histories": [["v1", "Mon, 21 Nov 2016 14:06:48 GMT  (48kb,D)", "http://arxiv.org/abs/1611.06791v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["suraj srinivas", "r venkatesh babu"], "accepted": false, "id": "1611.06791"}, "pdf": {"name": "1611.06791.pdf", "metadata": {"source": "CRF", "title": "Generalized Dropout", "authors": ["Suraj Srinivas"], "emails": ["surajsrinivas@grads.cds.iisc.ac.in", "venky@cds.iisc.ac.in"], "sections": [{"heading": "1 Introduction", "text": "For large-scale tasks like image classification, the general practice in recent times [1] has been to train large Convolutional Neural Network (CNN) models. Even with large datasets, the risk of overfitting runs high because of the large model size. As a result, strong regularizers are required to restrict the complexity of these models. Dropout [2] is a stochastic regularizer that has been widely used in recent times. However, the rule itself was proposed as a heuristic - with the objective of reducing co-adaption among neurons. As a result, it\u2019s behaviour was (and still is) not well understood. Gal and Gharamani [3] showed that dropout implicitly performs approximate Bayesian inference - making it a Bayesian Neural Net.\nBayesian Neural Nets (BNNs) view parameters of a Neural Network as random variables rather than fixed unknown quantities. As a result, there exists a distribution of possible values that each parameter can take. By placing an appropriate prior over these random variables, it is possible to restrict the model\u2019s capacity and implicitly perform regularization. The theoretical attractiveness of these methods is that one can now use tools from probability theory to work with these models. What advantages do BNNs offer over plain Neural Nets? First, they inherently capture uncertainty - both in the model parameters as well as predictions. Second, they are ideal for learning from small amounts of data. Third, a Bayesian approach has the advantage of distilling complex assumptions about the model in the form of prior distributions.\nInference over BNNs is typically intractable. As a result, one often uses approximations to the posterior distribution. MCMC and Variational Inference (VI) [4] are two popular methods for performing these approximations. In recent times, VI has emerged as the preferred method of performing this approximation as it is scalable to large models. When using VI, it is common to assume independence of model parameters. For Neural Networks, this assumption may seem unnecessarily stringent. After all, weights in a particular filter are highly correlated to produce specific patterns (an oriented edge, for instance). However, different filters in a CNN are more-or-less independent as they compute different features. In fact, it might even be advantageous to enforce independence of different filters through VI, as they reduce co-adaptation among features. In this work, we strive to enforce independence among features rather than weights.\nar X\niv :1\n61 1.\n06 79\n1v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\n01 6\nThe overall contributions of the paper are as follows:\n\u2022 We derive a Bayesian approach to performing inference with neural networks. In doing so, we introduce a rich family of regularizers - Generalized Dropout (GD).\n\u2022 We perform experimental analysis with Dropout++, a set of methods under GD, to understand it\u2019s behaviour.\n\u2022 We perform experiments with Stochastic Architecture Learning, another set of methods under GD, and show that they can be used to select the width of neural networks.\n\u2022 We test Dropout++ on standard networks and show that it can be used to boost performance."}, {"heading": "2 Bayesian Neural Networks", "text": "In this section, we shall formally introduce the notion of BNNs and also discuss our proposed method. Let f(\u00b7;w) denote a neural network function with parameters w. For a given input x, the neural network produces y = f(x;w) a probability distribution over possible labels (through softmax) for a classification problem. Given training data D, the parameter vector w is updated using Bayes\u2019 Rule.\nP (w|D) \u221d P (D|w)P (w) (1)\nAfter computing the posterior distribution, we perform inference on a new data point x as follows. Since the neural network produces a probability distribution over labels, P (y|x,w) = f(x;w)\nP (y|x,D) = \u222b w P (y|x,w)P (w|D)dw = \u222b w f(x;w)P (w|D)dw (2)\nComputing the posterior from equation 1 is intractable due to the complicated neural network structure. In Variational Inference, we define another distribution q(w), called the variational distribution, which approximates the posterior P (w|D). This distribution is used instead of the posterior in equation 2 for inference.\nOne common assumption when working with the variational distribution is the mean-field assumption, which requires that\nq(w) = \u220f i q(wi) (3)\nThis states that all parameters in w are independent. Such an assumption is certainly not true, especially at the feature-level, where parameters are highly correlated. Works like those of Denil et al.[5] explicitly show that parameters of a NN can be predicted given other parameters - hinting at the large amount of correlation present. Trying to enforce independence among such weights may end up having adverse effects.\nIt is very difficult to overcome the independence assumption within the framework of VI. In the next section, we introduce an approach to overcome this difficulty."}, {"heading": "2.1 Bayesian Neural Networks with Gates", "text": "We add multiplicative gates after each feature / neuron in the neural network, as in Figure 1a. These gates modulate the output of each neuron. Let these new parameters be denoted by \u03b8. Let us also assume that they lie in [0, 1]. Intuitively, these gates now control the relative importance of each particular feature as viewed by the next layer. Such relative importance may be fundamentally uncertain given a particular feature. Hence, it may be useful to think of gate parameters \u03b8 as random variables. We shall now crystallize all these assumptions in the form of choices for the variational distribution.\nWe first place the following prior-hyperprior pair over the gate parameters \u03b8 , and a prior over the regular parameters w.\n\u03b8 \u223c \u220f i bernoulli(ki) k \u223c \u220f i beta(\u03b1, \u03b2) w \u223c \u220f i N (0, \u03c3) (4)\nNote that the products are over all possible variables defined in the network. Here, k denotes the bernoulli parameters and also needs to be estimated along with \u03b8,w. Now, given that we use variational inference, let us now define the forms of the variational distributions q(w,\u03b8,k;\u00b5) we use. Let \u00b5 = {\u00b51,\u00b52}.\nq(w,\u03b8,k;\u00b5) = q1(w;\u00b51) q2(k;\u00b52) q3(\u03b8;\u00b52)\nq1(w) = \u220f i \u03b4(wi ; \u00b5 i 1) q2(k) = \u220f i \u03b4(ki ; \u00b5 i 2) q3(\u03b8) = \u220f i bernoulli(\u03b8i ; \u00b5 i 2) (5)\nNote that even though we make an independence assumption on the weights w (equation 5), we overcome the disadvantages described in the previous section by effectively not being Bayesian with respect to w, using a delta distribution. Also note that we use the same parameter \u00b52 for both distributions q2(.) and q3(.). While it is true that using different parameters for both distributions could make the formulation more powerful, we use the same parameter for simplicity. Now we write equations describing the variational approximation by using the definitions above.\n\u00b5\u22171,\u00b5 \u2217 2 = argmin\n\u00b51,\u00b52\nKL[q(w,\u03b8,k;\u00b5)||P (w,\u03b8,k|D)]\n= argmin \u00b51,\u00b52\n\u2212 \u2211 \u03b8 logP (D|\u03b8,w,k) q3(\u03b8;\u00b52)\u2212 logP (k;\u03b1, \u03b2) (6)\nOur objective is now to solve equation 6. We observe that the exhaustive summation in equation 6 in intractable for large models. A popular method to deal with this is to use a Monte-Carlo approximation of the summation. However, even this may be infeasible for large models. As a result, we further approximate this with a single Monte-Carlo sample. In other words, we perform the following approximation:\n\u2211 \u03b8 logP (D|\u03b8,w,k) q3(\u03b8;\u00b52) \u2248 logP (D|\u03b8 s,w,k)\nwhere \u03b8s \u223c q3(\u03b8;\u00b52)\nWhile this approximation seems to be drastic, we soon shall see that Classic Dropout also implicitly performs the same approximation."}, {"heading": "2.2 Generalized Dropout", "text": "Given all the assumptions and approximations discussed above, we now write the complete objective function we aim to solve. Since the variational distributions for w and k are delta distributions, we shall now use w,k instead of \u00b51,\u00b52 in our notations, for simplicity.\nw\u2217,k\u2217 = argmin w,k\n\u2212 logP (D|\u03b8s,w,k)\u2212 (\u03b1\u2212 1) logk\u2212 (\u03b2 \u2212 1) log(1\u2212 k) (7)\nwhere \u03b8s \u223c bernoulli(\u03b8;k)\nIn the expression above, we have used the fact that P (k;\u03b1, \u03b2) is a beta distribution. This form of the objective function 7, with gates \u03b8,k constitutes the Generalized Dropout regularizer.\nLet us now briefly look at the behaviour of the beta distribution at various values of \u03b1, \u03b2, as shown in Figure 1b. We shall refer to each of these specific cases as different versions of Dropout++. For reasons to be discussed later, we shall refer to the last case as Stochastic Architecture Learning (SAL).\n\u2022 Dropout++ (0.5), where \u03b1 = \u03b2 > 1: k = 0.5 is the most probable value of k. \u2022 Dropout++ (flat), where \u03b1 = \u03b2 = 1: All values of k are equally probable. \u2022 Dropout++ (1), where \u03b1 = 1, \u03b2 > 1: k = 1 is the most probable value of k. \u2022 Dropout++ (0), where \u03b1 > 1, \u03b2 = 1: k = 0 is the most probable value of k. \u2022 SAL, where \u03b1 < 1, \u03b2 < 1: k = 0 and k = 1 are the most probable values of k.\nNote that Dropout++ (0.5) becomes indistinguishable from Classic Dropout (with 0.5 Dropout rate) at \u03b1 = \u03b2 \u2192 \u221e. To obtain other Dropout rates, we simply ensure \u03b1 6= \u03b2. In the next section, we shall discuss another algorithm called Architecture Learning, and how it relates to the SAL method above."}, {"heading": "2.3 Architecture Learning", "text": "Srinivas and Babu [6] recently introduced a method to learn the width and depth of neural network architectures. They also add additional learnable parameters similar to gates. Also, their objective function has the following form in our notation.\nw\u2217,k\u2217 = argmin w,k\n\u2212 logP (D|\u03b8s,w,k) + \u03bb1k(1\u2212 k) + \u03bb3k (8)\nwhere \u03b8s = heaviside(k\u2212 0.5)\nNote that our objective function 7 looks very similar to this when \u03b1, \u03b2 < 1 and \u03b1 < \u03b2, except that we use logk instead of k. Another difference is that they use a heaviside threshold to select \u03b8 rather than sampling from a bernoulli. We observe that this is equivalent to taking a maximum likelihood sample from the bernoulli distribution. Given these similarities, we found it apt to name the corresponding method with \u03b1, \u03b2 < 1 as Stochastic Architecture Learning, as it is a stochastic version of the algorithm described above.\nMost surprisingly, we find that the motivation to arrive at this algorithm was completely different - they intended to minimize the number of neurons in the network. We arrive at a very similar formulation from a purely Bayesian perspective."}, {"heading": "2.4 A Practitioner\u2019s Perspective", "text": "In this section, we shall attempt to provide an intuitive explanation for Generalized Dropout. Going back to Fig. 1a, each neuron is augmented with a gate which learns values between 0 and 1. This is enforced by our regularizers and well as by parameter clipping. During the forward pass, we treat\neach of these gate values as probabilities and toss a coin with that probability. The output of the coin toss is used to block / allow neuron outputs. As a result of the learning, important features tend to have higher probability values than unimportant features.\nAt test time, we do not perform any sampling. Rather, we simply use the real-valued probability values in the gate variables. This approximation - called re-scaling - is used in classical Dropout as well.\nWhat do the different Generalized Dropout methods do? Intuitively, they place restriction on the gate values (probabilities) that can be learnt. As an example, Dropout++ (0) encourages most gate values to be close to 0, with only a few important ones being high. On the other hand, Dropout++ (1) encourages gates values to be close to 1. Intuitively, this means that Dropout++ (0) restricts the capacity of a layer by a large amount, whereas Dropout++ (1) hardly changes anything. SAL, on the other hand, encourages neurons to be close to either 0 or 1. In contrast to other methods, SAL produces neural network layers that are very close to being deterministic - neurons close to 0 are almost never \u2019on\u2019 and those close to 1 are almost always \u2019on\u2019. Dropout++ (flat) is also unique in the sense that it doesn\u2019t place any restriction on the gate values. As a result, we do not require to set any hyper-parameters for this method. From a Bayesian Perspective, when we have no prior beliefs on what the gate values should be, we use the most non-informative prior - which is Dropout++ (flat) in this case.\nDropout++ (0.5) encourages values to be close to 0.5. If the regularization constants are increased, then gate values other than 0.5 are penalized more and more heavily. In the limiting case we get Dropout, where any deviation from probability value of 0.5 is \u201dinfinitely\u201d penalized."}, {"heading": "2.5 Estimating gradients for binary stochastic gates", "text": "Given our formalism of stochastic gate variables, it is unclear how one might compute error gradients through them. Bengio et al. [7] investigated this problem for binary stochastic neurons and empirically verified the efficacy of different solutions. They conclude that the simplest way of computing gradients - the straight-through estimator works best overall. This involves simply back-propagating through a stochastic neuron as if it were an identity function. If the sampling step is given by \u03b8 \u223c bernoulli(k), then the gradient d\u03b8dk = 1 is used. Another issue of consideration is that of ensuring that k always lies in [0, 1] so that it is a valid bernoulli parameter. Bengio et al. [7] use a sigmoid activation over k. Our experiments showed clipping functions worked better. This can be thought of as a \u2018linearized\u2019 sigmoid. The clipping function is given by the following expression.\nclip(k) =  1, k \u2265 1 0, k \u2264 0 k, otherwise\nThe overall sampling function is hence given by \u03b8 \u223c bernoulli(clip(k)), and the straight-through estimator is used to estimate gradients overall."}, {"heading": "2.6 Applying to Convolution Layers", "text": "Here we shall discuss how to apply this to convolutional layers. Let us assume that the output feature map from a convolutional layer is k \u00d7 k \u00d7 n, i.e; n feature maps of size k \u00d7 k. Classical dropout samples k\u00d7k\u00d7n bernoulli random variables and performs pointwise multiplication with the output feature map. We follow the same for Generalized Dropout as well.\nHowever, if we wish to perform architecture selection like Architecture Learning [6], we need to select a subset of the n feature maps. In this case, we only have n gate variables, multiplying to the output of each feature map. When a gate is close to zero, and entire feature map\u2019s output becomes close to zero at test time. By selecting few feature maps out of n, we determine which of the n filters in the previous layer are essential."}, {"heading": "3 Related Work", "text": "There are plenty of works which aim to extend Dropout. DropConnect [8] stochastically drops weights instead of neurons to obtain better accuracy on ensembles of networks. As stated earlier, using the independence assumption for weights may not be correct. Indeed, DropConnect is shown to work on only fully connected layers. Standout [9] is a version of Dropout where the dropout rate depends on the output activations of a layer. Variational Dropout [10] proposes a Bayesian interpretation for Gaussian Dropout rather than the canonical multiplicative Dropout. By considering multiplicative Dropout, we make important connections to Architecture Learning / neuron pruning. Gal and Gharamani [3] showed a Bayesian interpretation for binary dropout and show that test performance improves by performing Monte-Carlo averaging rather than re-scaling. For simplicity, we use the re-scaling method at test time for Generalized Dropout. Our work can be seen as an extension of this work by considering a hyper-prior along with a bernoulli prior.\nHinton and Van Camp [11] first introduced variational inference for making Neural Networks Bayesian. Recent work by Graves [12] and Blundell et al. [13] further investigated this notion by using different priors and relevant approximations for large networks. Probabalistic Backpropagation [14] is an algorithm for inferring marginal posterior probabilities for special classes of Bayesian Neural Networks. Our method is different from any of these methods as they are all Bayesian over the weights, whereas we are only Bayesian with respect to the gates."}, {"heading": "4 Experiments", "text": "In this section, we perform experiments with the Generalized Dropout family to test their usefulness. First, we perform a wide variety of analysis with the Generalized Dropout family. Later, we study some specific applications of this method. We perform experiments primarily using Theano [15] and Lasagne."}, {"heading": "4.1 Analysis of Generalized Dropout", "text": "We shall now analyze the behaviours of different members of Generalized Dropout family to find out which ones are useful. For the experiments on the MNIST dataset, we use the standard LeNet-like architecture [16], which consists of two 5 \u00d7 5 convolutional layers with 20 and 50 filters, and two fully connected layers with 500 and 10 (output layer) neurons. While there is nothing particularly special about this architecture, we simply use this as a standard net to analyze our method."}, {"heading": "4.1.1 Effect of data-size", "text": "We investigate whether Generalized Dropout indeed has any advantage over Dropout in terms of accuracy. Here, we apply Dropout and Generalized Dropout only to the last fully connected layer. Our experiments reveal that for the network considered, the accuracies achieved by any Generalized Dropout method are not always strictly better than Dropout, as shown in Figure 2a. This indicates that most of the regularization power of Dropout comes from the independence assumption of Variational Inference, rather than particular values of the dropout parameter. This is a surprising result which we shall use to our advantage in the paper.\nHowever, we note that for small data-sizes, Dropout++ (0) seems to be advantageous over Dropout (Figure 2b). This is possibly because Dropout++ (0) forces most neurons (but not all) to have very low capacity due to low value of the parameters. 1"}, {"heading": "4.1.2 Effect of Layer-width", "text": "Inspired from the above results about Dropout++ (0), we look at the relationship between using different layer-widths for the fully connected layer and the learnt gate parameters. Intuitively, it is natural to assume that larger layers should learn lower gate values, whereas smaller layers should learn much higher values, if we wish for the overall capacity of the layer to remain roughly the same. Our experiments confirm this intuition as shown in Figure 2c.\n1Note that in our notation, a large value of Dropout++ indicates a large probability of retaining the neuron, contrary to popularly used notation for Dropout.\nWe also test if this flexibility translates to higher accuracy numbers over a fixed dropout value, and we find this to be indeed the case. We find that for small layer-widths, Dropout (at p = 0.5 for example), tends to remove too many neurons, while Dropout++ adjusts it\u2019s parameter values to account for small layer-widths, as shown in Figure 2d."}, {"heading": "4.1.3 Effect of Initialization", "text": "Initialization of good parameters is known to play a key-role in generalization of deep learning systems. To test whether this holds for the newly introduced Generalized Dropout parameters as well, we try different initializations of the Generalized Dropout parameters. In this example, we simply initialize all gates to a single constant value. As expected, we find that the choice of this initialization is much less crucial when compared to setting the Dropout value, as shown in Figure 2e.\nThe choice of initialization, however, affects training time. As an example, it is empirically observed that Dropout with p = 0.1 is much slower than p = 0.9. Therefore, it is helpful to have higher Dropout rates to facilitate faster training. To help faster training in Dropout++, we simply initialize with p = 1.0, i.e; start with a network with no Dropout and gradually learn how much Dropout to add. We observe that this indeed helps training time and at the same time provides the flexibility of Dropout, as shown in Figure 2f."}, {"heading": "4.1.4 Visualization of Learnt Parameters", "text": "Until this point, we have focussed on using Generalized Dropout on the fully connected layers. Similar effects hold when we apply these to convolutional layers as well. Here, we visualize the learnt parameters in convolutional layers. First, we add Dropout++ only to the input layer. The resulting gate parameters are shown in Figure 2g. We observe a similar effect when we add Dropout++ only to the first convolutional layer, as shown in Figure 2h, which shows the average gate map of all the convolutional filters in that layer. In both cases, we observe that Dropout++ learns to selectively attend to the centre of the image rather than towards the corners.\nThis has multiple advantages. First, by not looking at the corners of each feature, we can potentially decrease model evaluation time. Second, this breaks translation equivariance implicit in convolutions, as in our case certain spatial locations are more important for a filter than others. This could be helpful when using CNNs for face images (for example), where a filter need not look for an \u201deye\u201d everywhere in the image. Such locally connected layers have been previously used in works such as DeepFace [19]. Dropout++ could offer a more natural way to incorporate such an assumption."}, {"heading": "4.1.5 Architecture Selection", "text": "We shall now attempt to use Stochastic Architecture Learning (SAL) to automatically learn the required layer width of the network. The inherent assumption here is that the initial architecture is over-complete, and that a sub-set of neurons is sufficient to get similar performance. We first learn the parameters of the network using SAL regularizer, later we prune neurons with low gates parameters. Figure 2i shows that SAL learns gate parameters that are often close to either 0 or 1, resulting in a much sharper rise compared to the other methods. We use this sharp rise as a criterion to select the width of a layer. We observe that varying the \u03b2/\u03b1 parameter encourages the method to get smaller architectures, sometimes at the cost of accuracy, as shown in Table 1."}, {"heading": "4.2 Dropout++ on standard models", "text": "So far we have studied the various properties of Generalized Dropout by performing various experiments on LeNet. We shall now shift to larger networks to test the effectiveness of Dropout++. Modern networks mainly use dropout only in the fully connected layers, or simply not at all, owing to much powerful regularizers such as Batch Normalization. Here we shall take such networks, simply add Dropout++ (flat) after each layer, and see if we get an increase in accuracy. We per-\nform experiments with ResNet32, ResNet56 and a Generic VGG-like network, all trained on the CIFAR-10 dataset. As in Table 2, we see that for all three models, adding Dropout++ is largely helpful."}, {"heading": "5 Conclusion", "text": "We have proposed Generalized Dropout, a family of methods that generalize Dropout-like behaviour. One set of methods in this family, Dropout++, is an adaptive version of Dropout. Stochastic Architecture Learning is another set of methods that performs architecture selection. An uninformed choice of the Dropout parameter usually hurts performance. Dropout++ helps in setting a useful parameter value regardless of factors such as layer width and initialization. Experiments show that it is generally beneficial to simply add Dropout++ (flat) after every layer of a Deep Network."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1929}, {"title": "Bayesian convolutional neural networks with bernoulli approximate variational inference", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "arXiv preprint arXiv:1506.02158,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Pattern recognition", "author": ["Christopher M Bishop"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning the architecture of deep neural networks", "author": ["Suraj Srinivas", "R Venkatesh Babu"], "venue": "arXiv preprint arXiv:1511.05497,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Adaptive dropout for training deep neural networks", "author": ["Jimmy Ba", "Brendan Frey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1506.02557,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Geoffrey E Hinton", "Drew Van Camp"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Weight uncertainty in neural networks", "author": ["Charles Blundell", "Julien Cornebise", "Koray Kavukcuoglu", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1505.05424,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1502.05336,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md Patwary", "Mostofa Ali", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1502.05700,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For large-scale tasks like image classification, the general practice in recent times [1] has been to train large Convolutional Neural Network (CNN) models.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Dropout [2] is a stochastic regularizer that has been widely used in recent times.", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "Gal and Gharamani [3] showed that dropout implicitly performs approximate Bayesian inference making it a Bayesian Neural Net.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "MCMC and Variational Inference (VI) [4] are two popular methods for performing these approximations.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "[5] explicitly show that parameters of a NN can be predicted given other parameters - hinting at the large amount of correlation present.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Let us also assume that they lie in [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "5] Dropout++ [flat] Dropout++ [1] Dropout++ [0] Stochastic Architecture Learning", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "3 Architecture Learning Srinivas and Babu [6] recently introduced a method to learn the width and depth of neural network architectures.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "[7] investigated this problem for binary stochastic neurons and empirically verified the efficacy of different solutions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Another issue of consideration is that of ensuring that k always lies in [0, 1] so that it is a valid bernoulli parameter.", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "[7] use a sigmoid activation over k.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "However, if we wish to perform architecture selection like Architecture Learning [6], we need to select a subset of the n feature maps.", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "DropConnect [8] stochastically drops weights instead of neurons to obtain better accuracy on ensembles of networks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "Standout [9] is a version of Dropout where the dropout rate depends on the output activations of a layer.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "Variational Dropout [10] proposes a Bayesian interpretation for Gaussian Dropout rather than the canonical multiplicative Dropout.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Gal and Gharamani [3] showed a Bayesian interpretation for binary dropout and show that test performance improves by performing Monte-Carlo averaging rather than re-scaling.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "Hinton and Van Camp [11] first introduced variational inference for making Neural Networks Bayesian.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Recent work by Graves [12] and Blundell et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "[13] further investigated this notion by using different priors and relevant approximations for large networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Probabalistic Backpropagation [14] is an algorithm for inferring marginal posterior probabilities for special classes of Bayesian Neural Networks.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "We perform experiments primarily using Theano [15] and Lasagne.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "For the experiments on the MNIST dataset, we use the standard LeNet-like architecture [16], which consists of two 5 \u00d7 5 convolutional layers with 20 and 50 filters, and two fully connected layers with 500 and 10 (output layer) neurons.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "82 431k Architecture Learning [6] 20-50-20-10 0.", "startOffset": 30, "endOffset": 33}, {"referenceID": 16, "context": "8) ResNet-32 [17] 7.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "8 ResNet-56 [17] 6.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "18 GenericNet [18] 6.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Such locally connected layers have been previously used in works such as DeepFace [19].", "startOffset": 82, "endOffset": 86}], "year": 2016, "abstractText": "Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.", "creator": "LaTeX with hyperref package"}}}