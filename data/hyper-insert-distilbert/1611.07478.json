{"id": "1611.07478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "An unexpected unity among methods for interpreting model predictions", "abstract": "providing understanding why a model made a certain prediction is necessarily crucial in many data & science emerging fields. interpretable predictions engender good appropriate trust and consistently provide fresh insight into then how the model may be improved. namely however, that with large modern datasets the best data accuracy ratio is often achieved even by evaluating complex programming models presented even unless experts struggle to quite interpret, which creates too a tension tie between accuracy and interpretability. due recently, these several design methods have been proposed for interpreting predictions from complex models by estimating the importance of manipulating input numerical features. here, additionally we excitedly present how a balanced model - agnostic additive representation of the importance of input features unifies understanding current methods. this representation data is reasonably optimal, in both the proper sense that it is usually the possible only set of suitable additive values represented that reasonably satisfies important properties. we show how we already can positively leverage these properties to correctly create novel visual structural explanations of model predictions. identifying the organizational thread of unity literature that manages this general representation weaves directly through the underlying literature gradually indicates that there are common many principles seek to best be learned effectively about refining the interpretation of model predictions models that apply in many scenarios.", "histories": [["v1", "Tue, 22 Nov 2016 19:30:28 GMT  (1028kb,D)", "https://arxiv.org/abs/1611.07478v1", "Short extended abstract"], ["v2", "Wed, 23 Nov 2016 06:44:36 GMT  (1224kb,D)", "http://arxiv.org/abs/1611.07478v2", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"], ["v3", "Thu, 8 Dec 2016 08:24:15 GMT  (1224kb,D)", "http://arxiv.org/abs/1611.07478v3", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"]], "COMMENTS": "Short extended abstract", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["scott lundberg", "su-in lee"], "accepted": false, "id": "1611.07478"}, "pdf": {"name": "1611.07478.pdf", "metadata": {"source": "CRF", "title": "An unexpected unity among methods for interpreting model predictions", "authors": ["Scott M. Lundberg"], "emails": ["slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "sections": [{"heading": "An unexpected unity among methods for interpreting model predictions", "text": "Scott M. Lundberg University of Washington\nslund1@cs.washington.edu\nSu-In Lee University of Washington\nsuinlee@cs.washington.edu\nUnderstanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios."}, {"heading": "Introduction", "text": "A correct interpretation of a prediction model\u2019s output is extremely important. This often leads to the use of simple models (e.g., linear models) although they are often less accurate than complex models. The growing availability of big data from complex systems has lead to an increased use of complex models, and so an increased need to improve their interpretability. Historically, models have been considered interpretable if the behavior of the model as a whole can be summarized succinctly. Linear models, for example, have a single vector of coefficients, which describe the relationships between features and a prediction across all samples. Although these relationships are not succinctly summarized in complex models, if we focus on a prediction made on a particular sample, we can describe the relationships more easily. Recent model-agnostic methods leverage this property by summarizing the behaviour of the complex models only with respect to a single prediction [3, 6].\nHere, we extend a prediction explanation method based on game theory, specifically on the Shapley value, which describes a way to distribute the total gains to players, assuming they all collaborate [6]. We show how this method by \u0160trumbelj et al. can be extended to unify and justify a wide variety of recent approaches to interpreting model predictions (Figure 1). We term these feature importance values expectation Shapley (ES) values; because when the model output is viewed as a conditional expectation (of y given x), these values are equivalent to the Shapley values, i.e., distribution of credit from coalescent game theory. Intriguingly, ES values connect with and motivate several other current prediction explanation methods:\nLIME is a method for interpreting individual model predictions based on locally approximating the model around a given prediction [3]. ES values fit into the formalism proposed by LIME and justify a specific local sample weighting kernel. The examples in Ribeiro et al. (2016) [3] can be viewed as approximations of ES values with a different weighting kernel defining locality.\nDeepLIFT was recently proposed as a recursive prediction explanation method for deep learning [5]. DeepLIFT values are ES values for a linearized version of the deep network. This connection motivates the use of DeepLIFT as an extremely efficient sampling-free approximation to ES values. ES values can be also used to uniquely justify specific linearization choices DeepLIFT must make.\nLayer-wise relevance propagation is another method for interpreting the predictions of compositional models, such as deep learning [1]. As noted by Shrikumar et al., layer-wise relevance propagation is equivalent to DeepLIFT with the reference activations of all neurons fixed to zero\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n07 47\n8v 3\n[ cs\n.A I]\n8 D\nec 2\n01 6\n[5]. This implies that layer-wise relevance propagation is also an approximation of ES values, where the primary difference from DeepLIFT is the choice of a reference input to approximate the effect of missing values. By noting that both DeepLIFT and layer-wise relevance propagation are ES value approximations, we can see that DeepLIFT\u2019s proposed improvement over layer-wise relevance propagation is a change that makes DeepLIFT a better approximation of ES values.\nShapley regression values are an approach to computing feature importance in the presence of multicollinearity [2]. They were initially designed to mitigate problems with the interpretability of linear models (although those are typically considered easy to interpret), though they can be applied to other models as well. Shapley regression values require retraining the model on all feature subsets, and can be considered a brute force method of computing ES values. By viewing the model output as an expected value, ES values allow fast approximations in situations where training models on all feature subsets would be intractable."}, {"heading": "Expectation Shapley values and LIME", "text": "Understanding why a model made a prediction requires understanding how a set of interpretable model inputs contributed to the prediction. The original inputs x \u2208 RP may be hard for a user to interpret, so a transformation to a new set x\u2032 of interpretable inputs is often needed. ES values set x\u2032 = hx(x) to a binary vector of length M representing if an input value (or group of values) is known or missing. This mapping hx takes an arbitrary input space and converts it to an interpretable binary vector of feature presence. For example, if the model inputs are word embedding vectors, then x\u2032 could be a binary vector of our knowledge of word presence vs. absence. If the model input is a vector of real-valued measurements, x\u2032 could be a binary vector representing if a group of measurements was observed or missing.\nPrediction interpretation methods seek to explain how the interpretable inputs contributed to the prediction. While the parameters of the original model define this relationship, they do so in a potentially complex manner and do not utilize the interpretable inputs x\u2032. To provide interpretability, these methods learn a simple approximation g(x\u2032) to the original model for an individual prediction. Inspecting g(x\u2032) provides an understanding of the original model\u2019s behavior near the prediction. This approach to local model approximation was formalized recently in Ribeiro et al. as finding an interpretable local model \u03be that minimizes the following objective function [3]:\n\u03be = arg min g\u2208G L(f, g, \u03c0x\u2032) + \u2126(g) (1)\nFaithfulness of the simple model g(x\u2032) to the original model f(x) is enforced through the loss L over a set of samples in the interpretable data space x\u2032 weighted by \u03c0x\u2032 . \u2126 penalizes the complexity of g.\nGiven the above formulation for \u03be we show the potentially surprising result that if g is assumed to follow the simple additive form:\ng(x\u2032) = \u03c60 + M\u2211 i=1 \u03c6ix \u2032 i, (2)\nwhere \u03c6i (a shortened version of \u03c6i(f, x) when f and x are clear) are parameters to be optimized, then the loss function L, the sample weighting kernel \u03c0x\u2032 , and the regularization term \u2126 are all uniquely determined (up to transformations that do not change \u03be) given three basic assumptions from game theory. These assumptions are:\n1. Efficiency. f(x) = M\u2211 i=0 \u03c6i (3)\nThis assumption forces the model to correctly capture the original predicted value. 2. Symmetry. Let 1S \u2208 {0, 1}M be an indicator vector equal to 1 for indexes i \u2208 S, and 0\nelsewhere, and let fx(S) = f(h\u22121x (1S)). If for all subsets S that do not contain i or j\nfx(S \u222a {i}) = fx(S \u222a {j}) (4) then \u03c6i(f, x) = \u03c6j(f, x). This states that if two features contribute equally to the model then their effects must be the same.\n3. Monotonicity. For any two models f and f \u2032, if for all subsets S that do not contain i fx(S \u222a {i})\u2212 fx(S) \u2265 f \u2032x(S \u222a {i})\u2212 f \u2032x(S) (5)\nthen \u03c6i(f, x) \u2265 \u03c6i(f \u2032, x). This states that if observing a feature increases f more than f \u2032 in all situations, then that feature\u2019s effect should be larger for f than for f \u2032.\nBreaking any of these axioms would lead to potentially confusing behavior. In 1985, Peyton Young demonstrated that there is only one set of values that satisfies the above assumptions and they are the Shapley values [7, 4]. ES values are Shapley values of expected value functions, therefore they are the only solution to Equation 1 that conforms to Equation 2 and satisfies the three axioms above. This optimality of ES values holds over a large class of possible models, including the examples used in the LIME paper that originally proposed this formalism [3].\nWe found the specific forms of x\u2032, L, and \u2126 that lead to Shapley values as the solution and they are:\n\u2126(g) = 0\n\u03c0x\u2032(z \u2032) = (M \u2212 1) (M choose |z\u2032|)|z\u2032|(M \u2212 |z\u2032|)\nL(f, g, \u03c0x\u2032) = \u2211 z\u2032\u2208Z [ f(h\u22121x (z \u2032))\u2212 g(z\u2032) ]2 \u03c0x\u2032(z \u2032)\n(6)\nIt is important to note that \u03c0x\u2032(z\u2032) = \u221e when |z\u2032| \u2208 {0,M}, which enforces \u03c60 = fx(\u2205) and f(x) = \u2211M i=0 \u03c6i. In practice these infinite weights can be avoided during optimization by analytically eliminating two variables using these constraints. Figure 2A compares our Shapley kernel with previous kernels chosen heuristically. The intuitive connection between linear regression and classical Shapley value estimates is that classical Shapley value estimates are computed as the mean of many function outputs. Since the mean is also the best least squares point estimate for a set of data points it is natural to search for a weighting kernel that causes linear least squares regression to recapitulate the Shapley values."}, {"heading": "Expectation Shapley values and DeepLIFT", "text": "DeepLIFT computes the impact of inputs on the outputs of compositional models such as deep neural networks. The impact of an input xj on a model output y is denoted by Cxjy and\nf(x(0)) + P\u2211 j=1 Cxjy = f(x) (7)\nwhere x(0) is a \"reference input\" designed to represent typical input values. ES value implementations approximate the impact of missing data by taking expectations, so when interpreting x(0) as an estimate of E[x] DeepLIFT is an additive model of the same form as ES values. To enable efficient recursive computation of Cxjy DeepLIFT assumes a linear composition rule that is equivalent to linearizing the non-linear components of the neural network. Their back-propagation rules that define how each component is linearized are intuitive, but arbitrary. If we interpret DeepLIFT as an approximation of ES values, then we can justify a unique set of linearizations for network components based on analytic solutions of the ES values for that component type. One example where this leads to a different, potentially improved, assignment of responsibility is the max function (Figure 2B)."}, {"heading": "Visualization of Expectation Shapley values", "text": "Model interpretability is closely tied to human perception. We designed a simple visualization based on analogy with physical force (Figure 3A). Each interpretable input x\u2032i is assigned a bar segment. The width of the segment is equal to the ES value \u03c6i. Red bar segments correspond to inputs where \u03c6i > 0, and blue segments to inputs where \u03c6i < 0. The model output starts at the base value \u03c60 = f(\u2205) in the center and then is pushed right by the red bars or left by the blue bars in proportion to their length. The final location of the model output is then equal to f(x) = \u2211M i=0 \u03c6i.\nWhile explaining a single prediction is very useful, we often want to understand how a model is performing across a dataset. To enable this we designed a visualization based on rotating the single prediction visualization (Figure 3A) by 90\u25e6, then stacking many horizontally. By ordering the predictions by explanation similarity we can see interesting patterns (Figure 3B). One such insight for the popular UCI adult census dataset is that marriage status is the most powerful predictor of income, suggesting that many joint incomes were reported, not simply individual incomes as might be at first assumed. For implementation code see https://github.com/slundberg/esvalues."}, {"heading": "Sample Efficiency and the Importance of the Shapley Kernel", "text": "Connecting Shapley values from game theory with locally weighted linear models brings advantages to both concepts. Shapley values can be estimated more efficiently, and locally weighted linear models gain theoretical justification for their weighting kernel. Here we briefly illustrate both the improvement in efficiency for Shapley values, and the importance of kernel choice for locally weighted linear models (Figure 4).\nShapley values are classically defined by the impact of a feature when it is added to features that came before it in an ordering. The Shapley value for that feature is the average impact over all possible orderings:\n\u03c6i(f, x) = 1\nP ! \u2211 r\u2208R [fx(B r i \u222a {i})\u2212 fx(Bri )] (8)\nwhere R is the set of all permutations of length P , and Bri is the set of all features whose index comes before i in permutation r. This leads to a natural estimation approach which involves taking the average over a small sample of all orderings [6]. While this standard approach is effective in small (or nearly linear) models, penalized regression (using Equation 6) produces much more accurate Shapley value estimates for non-linear models such as a dense decision tree over 10 features (Figure 4A), and a sparse decision tree using only 3 of 100 features (Figure 4B).\nWhile the axioms presented above provide a compelling reason to use the Shapley kernel (Equation 6), it natural to wonder if any reasonable local weighting kernel would produce results similar to the Shapley kernel. It turns out this is not the case, and the Shapley kernel significantly effects how we attribute non-linear effects to various features when compared to the standard exponential kernel used by LIME. For the sparse decision tree used above there is a noticeable change in the magnitude of feature impacts (Figure 4B), and for the dense decision tree we even see the direction of estimated effects reversed (Figure 4A)."}], "references": [{"title": "On pixel-wise explanations for non-linear classifier decisions by layerwise relevance propagation", "author": ["Sebastian Bach"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Analysis of regression in game theory approach", "author": ["Stan Lipovetsky", "Michael Conklin"], "venue": "Applied Stochastic Models in Business and Industry", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": " Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The Shapley value: essays in honor of Lloyd S", "author": ["Alvin E Roth"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences", "author": ["Avanti Shrikumar"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Explaining prediction models and individual predictions with feature contributions", "author": ["Erik \u0160trumbelj", "Igor Kononenko"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Monotonic solutions of cooperative games", "author": ["H Peyton Young"], "venue": "Journal of Game Theory", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1985}], "referenceMentions": [{"referenceID": 2, "context": "Recent model-agnostic methods leverage this property by summarizing the behaviour of the complex models only with respect to a single prediction [3, 6].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "Recent model-agnostic methods leverage this property by summarizing the behaviour of the complex models only with respect to a single prediction [3, 6].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "Here, we extend a prediction explanation method based on game theory, specifically on the Shapley value, which describes a way to distribute the total gains to players, assuming they all collaborate [6].", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "Intriguingly, ES values connect with and motivate several other current prediction explanation methods: LIME is a method for interpreting individual model predictions based on locally approximating the model around a given prediction [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "(2016) [3] can be viewed as approximations of ES values with a different weighting kernel defining locality.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "DeepLIFT was recently proposed as a recursive prediction explanation method for deep learning [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Layer-wise relevance propagation is another method for interpreting the predictions of compositional models, such as deep learning [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Shapley regression values are an approach to computing feature importance in the presence of multicollinearity [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "as finding an interpretable local model \u03be that minimizes the following objective function [3]:", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "In 1985, Peyton Young demonstrated that there is only one set of values that satisfies the above assumptions and they are the Shapley values [7, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 3, "context": "In 1985, Peyton Young demonstrated that there is only one set of values that satisfies the above assumptions and they are the Shapley values [7, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 2, "context": "This optimality of ES values holds over a large class of possible models, including the examples used in the LIME paper that originally proposed this formalism [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 5, "context": "This leads to a natural estimation approach which involves taking the average over a small sample of all orderings [6].", "startOffset": 115, "endOffset": 118}], "year": 2016, "abstractText": "Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.", "creator": "LaTeX with hyperref package"}}}