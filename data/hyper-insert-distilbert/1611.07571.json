{"id": "1611.07571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Quad-networks: unsupervised learning to rank for interest point detection", "abstract": "studying several functional machine learning tasks might require to represent the data using only a sparse set estimate of interest points. perhaps an ideal component detector user is likewise able to further find the corresponding assigned interest points even further if repeating the descriptive data undergo provides a quantitative transformation relatively typical way for identifying a culturally given domain. since the ideal task is subject of currently high potential practical optimization interest in computer neural vision, many hand - crafted solutions subsequently were routinely proposed. in reviewing this paper, do we ask here a fundamental question : can we learn such independent detectors clearly from scratch? since it is often indefinitely unclear, what points are \" interesting \", human labelling cannot be further used afterwards to better find all a truly attractive unbiased solution. basically therefore, ask the task which requires an unsupervised formulation. assume we assume are the first science to propose itself such a new formulation : training a neural network to yield rank points in under a specified transformation - invariant euclidean manner. interest tracking points are then ultimately extracted from the top / bottom buffer quantiles of this ranking. we validate our naive approach on two tasks : expanding standard modeling rgb image interest point detection software and challenging constrained cross - frequency modal interest point detection between rgb and depth images. we would quantitatively show that comparing our unsupervised method efficiently performs better on or on - par poorly with baselines.", "histories": [["v1", "Tue, 22 Nov 2016 22:46:17 GMT  (7464kb,D)", "https://arxiv.org/abs/1611.07571v1", null], ["v2", "Mon, 10 Apr 2017 21:15:18 GMT  (7521kb,D)", "http://arxiv.org/abs/1611.07571v2", "Accepted at CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["nikolay savinov", "akihito seki", "lubor ladicky", "torsten sattler", "marc pollefeys"], "accepted": false, "id": "1611.07571"}, "pdf": {"name": "1611.07571.pdf", "metadata": {"source": "CRF", "title": "Quad-networks: unsupervised learning to rank for interest point detection", "authors": ["Nikolay Savinov", "Akihito Seki", "L\u2019ubor Ladick\u00fd", "Torsten Sattler", "Marc Pollefeys"], "emails": ["nikolay.savinov@inf.ethz.ch,", "ladickyl@inf.ethz.ch,", "sattlert@inf.ethz.ch,", "marc.pollefeys@inf.ethz.ch,", "akihito.seki@toshiba.co.jp"], "sections": [{"heading": "1. Introduction", "text": "Machine learning tasks are typically subdivided into two groups: supervised (when labels for data are provided by human annotators) and unsupervised (no data labelled). Recently, more labelled data with millions of examples have become available (for example, Imagenet [30], Microsoft COCO [17]), which led to significant progress in supervised learning research. This progress is partly due to the emergence of convenient labelling systems like Amazon Mechanical Turk. Still, the human labelling process is expensive and does not scale well. Moreover, it often requires a substantial effort to explain human annotators how to label data.\nLearning an interest point detector is a task where labelling ambiguity goes to extremes. In images, for example, we are interested in a sparse set of image locations which can be detected repeatably even if the image undergoes a significant viewpoint or illumination change. These points\ncan further be matched for correspondences in related images and used for estimating the sparse 3D structure of the scene or camera positions. Although we have some intuition about what properties interest points should possess, it is unclear how to design an optimal detector that satisfies them. As a result, if we give this task to a human assessor, he would probably select whatever catches his eye (maybe corners or blobs), but that might not be repeatable.\nIn some cases, humans have no intuition what points could be \"interesting\". Let\u2019s assume one wants to match new images to untextured parts of an existing 3D model [27]. The first step could be an interest point detection in two different modalities: RGB and depth map, representing the 3D model. The goal would be to have the same points detected in both. It is particularly challenging to design such a detector since depth maps look very different from natural images. That means simple heuristics will fail: the strongest corners/blobs in RGB might come from texture which is missing in depth maps.\nAiming at being independent of human assessment, we propose a novel approach to interest point detection via unsupervised learning. Up to our knowledge, unsupervised learning for this task has not yet been explored in previous work. Some earlier works hand-crafted detectors like DoG [18]. More recent works used supervised learning to select a \"good\" subset of detections from a hand-crafted detector. For example, LIFT [38] aims to extract a subset of DoG detections that are matched correctly in the later stages of the sparse 3D reconstruction. However, relying on existing detectors is not an option in complicated cases like a cross-modal one. Our method, by contrast, learns the solution from scratch.\nThe idea of our method is to train a neural network that maps an object point to a single real-valued response and then rank points according to this response. This ranking is optimized to be repeatable under the desired transformation classes: if one point is higher in the ranking than another one, it should still be higher after a transformation. Consequently, the top/bottom quantiles of the response are repeatable and can be used as interest points. This idea is illustrated in Fig. 1.\nar X\niv :1\n61 1.\n07 57\n1v 2\n[ cs\n.C V\n] 1\n0 A\npr 2\n01 7\nWhen detecting interest points, it is often required to output not only the position of the point in the image, but also some additional parameters like scale or rotation. The detected values of those parameters are influenced by the transformations applied to the images. All transformations can be split into two groups based on their desired impact on the output of the detector. Transformations for which the detector is supposed to give the same result are called invariant. Transformations that should transform the result of a detector together with the transformation \u2014 and thus their parameters have to be estimated as latent variables \u2014 are called covariant [22]. When learning a detector with our method, we can choose covariant and invariant transformations as it suits our goals. This choice is implemented as a choice of training data and does not influence the formulation.\nThe paper is organized as follows. In section 2, we discuss the related work. In section 3, we introduce our formulation of the detection problem as the unsupervised learning to rank problem, and show how to optimize it. In section 4, we demonstrate how to apply our method to interest point detection in images. Finally, in section 5 we validate our approach experimentally and conclude in section 6 by summarizing the paper and listing possibilities for future work."}, {"heading": "2. Related work", "text": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).\nWhile some tasks actually have a non-human label (for example, in solver simulation we can obtain the solution by running a solver), others (for example, representation learning) have none at all. Instead, they try to find an auxiliary task which is hard enough in order to learn a representation that is useful for already existing tasks (classification, for example). Designing such a task is non-trivial, therefore only few successful approaches exist (for example, [6]).\nOur approach, on the other hand, does not require designing an unrelated auxiliary task. If we can obtain a repeatable ranking, then the top/bottom quantiles of this ranking can be used as detections.\nOne particular application of our method is interest point detection in images. Most of the existing image interest point detectors are hand-crafted to select particular visual elements like blobs, corners or edges. These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19]. Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response. Building upon a hand-crafted detector restricts those supervised approaches to a subset of their basic method detections \u2014 which makes those approaches inapplicable in the cases where there is no good detector yet. Our unsupervised method instead learns the detector completely from scratch by optimizing for a repeatable ranking.\nFinally, a particularly challenging case in image interest point detection is the cross-modal one: the interest points should be repeatable among different image modalities. Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution. Our approach, on the contrary, is general in a sense that the same learning procedure could be applied to different tasks: we show it to work for RGB/RGB and RGB/depth modality pairs."}, {"heading": "3. Detection by ranking", "text": "In this section we introduce the problem of learning an interest point detector as the problem of learning to rank points. We consider interest points to come from the top/bottom quantiles of some response function. If these quantiles are preserved under certain transformation classes, we have a good detector: it re-detects the same points. For the quantiles of the ranking to be preserved, we search for a ranking which is invariant to those transformations.\nLet us consider a set D of objects, every object d \u2208 D being an Nd-dimensional tuple of points (p1d, . . . , p Nd d ). Each point pid comes from a set P of points. Each object d can undergo transformations from a set T : D 7\u2192 D. Each transformation t \u2208 T preserves certain point correspondences: some points in object t(d) will correspond to points in object d. We assume one point can have at most one correspondence in the other object. To simplify the notation, we assume the correspondences have the same indexes in an object d before and after a transformation. We denote the set of corresponding point indexes asCdt = {i1, . . . , iKdt}, where Kdt is the number of correspondences for points in d and t(d).\nWe want to rank object points and represent this ranking with a single real-valued response function H(p|w), where\np \u2208 P is a point and w \u2208 Rn is a vector of parameters (one possible choice of H is a neural network). Thus, invariance of the ranking under transformation t \u2208 T can be stated as follows: for every quadruple (pid, p j d, p i t(d), p j t(d)) satisfying i, j \u2208 Cdt, i 6= j, it holds that H(pid|w) > H(p j d|w) & H(pit(d)|w) > H(p j t(d)|w)\nor H(pid|w) < H(p j d|w) & H(pit(d)|w) < H(p j t(d)|w) .\n(1)\nFrom the condition above it follows that\nObservation 1. If H satisfies the ranking constraints (1) and every point has a correspondence, the top/bottom quantiles of H before a transformation correspond to the top/bottom quantiles of H after it.\nThus, to get a repeatable interest point detector, one just needs to sort all points p of the object d by their response H(p|w) and take the top/bottom quantiles as interest points.\nIn the next section, we will state the optimization objective aiming at preserving the ranking (1)."}, {"heading": "3.1. Ranking objective and optimization", "text": "First, let us introduce a ranking agreement function for quadruples:\nR(pid, p j d, p i t(d), p j t(d)|w) =\n(H(pid|w)\u2212H(p j d|w))(H(p i t(d)|w)\u2212H(p j t(d)|w)) .\n(2)\nThen the ranking invariance condition (1) can be re-written as\nR(pid, p j d, p i t(d), p j t(d)|w) > 0 . (3)\nIn order to give preference to this invariance, we will assume the object set D and the transformation set T to be finite (for the sake of training) and minimize the objective:\nL(w) = \u2211 d\u2208D \u2211 t\u2208T \u2211 i,j\u2208Cdt `(R(pid, p j d, p i t(d), p j t(d)|w)) , (4)\nwhere `(R) is a function penalizing non-positive values. One naive solution would be to use a \"misranking count\" loss\n`(R) = { 1, if R \u2264 0 , 0, otherwise .\n(5)\nUnfortunately, this loss is hard to optimize as it either does not have a gradient or its gradient is zero. Instead, we upperbound the discontinuous loss with a differentiable one. In this work, we choose to use the hinge loss\n`(R) = max(0, 1\u2212R) . (6)\nThen the final form of our minimized objective will be L(w)= \u2211 d\u2208D \u2211 t\u2208T \u2211 i,j\u2208Cdt max(0, 1\u2212R(pid, p j d, p i t(d), p j t(d)|w)),\n(7)\nwhich is differentiable as long as H(p|w) is differentiable w.r.t w (that is satisfied if H is a neural network; note that the objective above is non-convex even if H is linear). Therefore, we can use gradient descent for the optimization."}, {"heading": "4. Image interest point detector learning", "text": "Learning detectors from scratch is a hard task since it is non-trivial to formulate good detector criteria in the optimization framework. As investigated by [22], a good detector should produce interest points that are robust to viewpoint/illumination changes (to detect the same points and further match them) and sparse (to make feature matching feasible). To comply with the earlier introduced terminology, d is an image, p is a position in the image represented by a patch, a transformation t is a viewpoint/illumination change and correspondence sets Cdt are patch-to-patch correspondences between images observing the same 3D scene.\nIt is typical for interest point detectors to ensure sparsity in two ways: by retaining the top/bottom quantiles of the response function (contrast filtering) and by retaining the local extrema of the response function (non-maximum suppression). While Observation 1 suggests reproducibility\nof our detections under contrast filtering, it turns out that non-maximum suppression is also suitable for our detector according to the following\nObservation 2. If H satisfies the ranking constraints (1) and the vicinity of the correspondence (pd, pt(d)) is visible in both images d and t(d), then pd is a local extremum in image d \u21d0\u21d2 pt(d) is a local extremum in image t(d).\nIt is easy to see why this observation is true: if the position is ranked higher/lower than all the neighbors in one image, the corresponding position should be ranked higher/lower than the corresponding neighbors in another image.\nThus the proposed objective is beneficial for the detector pipeline which consists of non-maximum suppression and contrast filtering. This pipeline is followed by many detectors including the popular DoG detector [18]. In the following section, we explain how to train an image interest point detector with our objective."}, {"heading": "4.1. Training", "text": "We need to sample from both the image set D and the transformation set T to perform training with the objective (7). We could, of course, take images and transformations from any available image dataset with correspondences. But this does not address two important questions:\n\u2022 How to achieve invariance exactly to the transformations that we want? For example, most real images are taken up-right, so there is no relative rotation between any pair of them. But we want our detector to be robust to cases where there is such a rotation.\n\u2022 How to augment the training images? For example, all the objects in the training images might be wellilluminated. But in the testing images some objects might be in the shadow, while others are in the light. We might want to be robust to such cases.\nIn this chapter we will show how to achieve each goal by randomly transforming training quadruples\nQ = (pkd, p m d , p k t(d), p m t(d)) . (8)\nTo achieve invariance to a transformation class Ti, we can sample two random transformations ti1 \u2208 Ti, ti2 \u2208 Ti and apply a quadruple of transformations (ti1 , ti1 , ti2 , ti2) to the training quadruples Q element-wise. This expresses our preference to preserve the ranking even if a random transformation from Ti is applied to the image.\nTo augment the data with a transformation class Ta, we can sample two random transformations ta1 \u2208 Ta, ta2 \u2208 Ta and apply (ta1 , ta2 , ta1 , ta2) toQ. This means that we apply the same transformation to both patches in the correspondence to create more training data.\nFinally, there are some invariant/augmenting transformations which can\u2019t be easily parametrized and sampled (e.g., the non-Lambertian effect). In that case, we fully rely on their distribution, coming from the real data."}, {"heading": "5. Experiments", "text": "Our objective function (7) is based on pairs of correspondences, forming training quadruples (an example of such a quadruple is shown in Fig. 2). To train a detector, we need to obtain those correspondences. We investigated learning\n\u2022 an RGB detector from ground-truth correspondences (they come from projecting laser-scanned 3D points onto images),\n\u2022 a fully-unsupervised RGB detector (correspondences are obtained by randomly warping images and changing illumination),\n\u2022 a cross-modal RGB/depth detector (correspondences are trivially obtained as coinciding locations in viewaligned Kinect RGBD frames).\nWe further describe the setup of those experiments. Detector class. We concentrate on the most commonly used type of detectors: scale-space-covariant, rotationinvariant ones (although our method is suitable for any combination of detector covariances/invariances). For example, DoG belongs to that type. Those detectors consider an interest point p to be characterized by an image location x, y and a scale s. The points are detected in a 3-dimensional space (scale-space) using a response function\nH(p|w) = H(x, y, s|w) . (9)\nConsequently, non-maximum suppression and contrast filtering work in this 3-dimensional space as well (with a 3 \u00d7 3 \u00d7 3 neighborhood). Since rotation is not estimated, the detector is required to be invariant to it. The invariance is achieved by the random sampling (see Section 4.1). Detector evaluation. DoG is the most widely used detector nowadays, so we use it as a baseline in our evaluation. The whole detector is a multi-stage pipeline in which we aim to substitute a crucial part: the filter used to convolve the image. In order to make a fair evaluation, we fix all the other stages of the pipeline. The whole procedure works as follows. First, we apply the response function H(p) to all spatial positions of the image at all considered scales. This is the stage we are aiming to substitute with a learned function (DoG filter in the standard pipeline). Second, we do non-maximum suppression in scale-space. Third, we do accurate localization based on the second-order Taylor expansion of the response function around potential interest points [18]. Finally, we only take points for which the absolute value of the response is larger than a threshold.\nFor quantitative evaluation, we use the repeatability measure described in [22] (with the overlap threshold parameter equal 40%). The repeatability is the ratio between the number of points correctly detected in a pair of images and the number of detected points in the image with the lowest number of detections. It is only meaningful to compare methods producing the same number of interest points: otherwise some method might report too many points and unfairly outperform others (e.g., if we take all points as \"interesting\", repeatability will be very high). Therefore, we consider a range of top/bottom quantiles, producing the desired numbers of points and compare all methods for those fixed numbers. Response function. In all experiments, the response function H(p|w) is a neural network. We describe it as a tuple of layers and use the notation:\n\u2022 c(f, i, o, p) for convolutional layers with filter size f \u00d7 f , taking i input channels, outputting o channels, using zero-padding of p pixels on each border (stride is always 1 in all experiments),\n\u2022 f(i, o) for fully-connected layers, taking i features and outputting o features,\n\u2022 e for the ELU non-linearity function [4],\n\u2022 b for a batch normalization layer,\n\u2022 (\u00b7)n for applying the same network n times.\nIn all the experiments, the response function is applied to grayscale 17x17 patches. If the training data is in color, we convert it to grayscale. The patches are preprocessed as it is typical for neural networks: the mean over the whole patch is subtracted, then it is divided by the standard deviation over the patch. Augmentation. We augment the training data (see Section 4.1) with random rotations from [0, 2\u03c0] and random scale changes from [ 13 , 3]. Optimization details. To optimize the objective (7), we use the Adadelta algorithm [39], which is a version of gradient\ndescent that chooses the gradient step size per-parameter automatically. We implement the model and optimization on a GPU (Nvidia Titan X) using the Torch7 framework [5]. The batch size is 256, our models are trained for 2000 epochs, each consisting of randomly sampling a pair of corresponding images and then randomly sampling 10000 quadruples from this pair. Eventually, by the time training stops our models have seen 20 million sampled quadruples."}, {"heading": "5.1. RGB detector from ground-truth correspondences", "text": "In this experiment, we show how to use existing 3D data to establish correspondences for training a detector. Training. We used the DTU Robot Image Dataset [1]. It has 3D points, coming from a laser scanner, and camera poses, which allow to project 3D points into the pairs of images and extract image patches centered at the projections. Those projections form the correspondence pairs for training. Testing. We used the Oxford VGG dataset [22], commonly chosen for this kind of evaluation. This dataset consists of 40 image pairs. NN architectures. In this experiment, we tested two NN architectures: a linear model (c(17, 1, 1, 0)) and a non-linear NN with one hidden layer (c(17, 1, 32, 0), e, f(32, 1)). Results. We demonstrate that the filter of our learned linear model is different from the filters of the baselines in Fig. 4. Furthermore, we show the detections of the linear model in comparison to DoG in Fig. 5. Our learned model detects points different from DoG: they are more evenly distributed in images. That is usually profitable for estimating geometric transformations between camera frames.\nThe learned response functions with both investigated architectures (linear, non-linear) demonstrate better performance than baselines in most cases, as shown in Table 1 (results are averaged over all image pairs for each transformation type). Moreover, the non-linear model performs better than the linear one in the majority of the cases.\nFinally, we combine our detector with the SIFT descriptor and measure how well the detected points can be\nmatched. For that we use the same matching score as in [22], i.e., the ratio of correct matches to all matches. Our detectors (Linear, Non-linear)+SIFT are slightly better than DoG+SIFT in most cases, as shown in Fig. 3. Our methods performed worse than DoG only for the UBC dataset, measuring the robustness to the JPEG compression, which was not included in our training."}, {"heading": "5.2. Fully-unsupervised RGB detector", "text": "The goal of this experiment is to show that ground-truth correspondences from an additional data source (like 3D points from a laser scanner) are not necessary to train a detector with our method. Instead, we can sample random transformations to obtain correspondences. Training. In this experiment, we only used images from the DTU dataset with different illuminations. To generate the correspondence, a patch was randomly selected from an image and randomly transformed. We considered affine warps, preserving area, together with illumination changes, uniformly sampled from those provided by the dataset. The affine warps were parametrized as rot(\u03b1) \u2217 diag(s, 1s ) \u2217 rot(\u2212\u03b1)) by a rotation \u03b1 (uniformly sampled from [0, 2\u03c0]) and a scaling factor s. We considered two settings: small warps (s sampled uniformly from [1, 1.1]) and large warps (s sample uniformly from [1, 2]). Testing. We used the Oxford VGG dataset [22] (same as in the previous experiment). NN architectures. We considered linear models. Results. As shown in Table 2, our methods outperform DoG in more than half of the cases."}, {"heading": "5.3. Cross-modal RGB/depth detector", "text": "In this experiment, we show how to use our method for learning a cross-modal detector \u2014 a hard problem where\nwe do not have an understanding on how to design a good solution by hand. We learn a detector between RGB and depth images by training on the NYUv2 dataset [24]. Such a detector has an application in augmenting an un-colored 3D point cloud with colors from a newly obtained image. Training. We use 40 random frames from NYUv2, which contains view-aligned Kinect RGBD frames (an RGB pixel corresponds to a depth pixel at the same location). Testing. We use 40 random frames from NYUv2 (unrelated to the training set). NN architectures. We evaluated the following architectures for the response function H:\n\u2022 Deep convolutional network (Deep Conv Net): (c(7, 1, 32, 3), b, e, (c(7, 32, 32, 3), b, e)8, c(17, 32, 1, 0)),\n\u2022 Shallow fully-connected network (Shallow FC Net): (c(17, 1, 32, 0), e, f(32, 32), e, f(32, 1)),\n\u2022 Deep fully-connected network (Deep FC Net): (c(17, 1, 32, 0), e, (f(32, 32), e)8, f(32, 1)).\nResults. The repeatability and filters from the best model (Deep Conv Net) are shown in Fig. 6 and Fig. 8. Our best model outperformes others by a large relative value. As shown in the repeatability plot, DoG produces a relatively small number of interest points. That is because we extract the same number of points from both sensors \u2014 for the fair comparison as explained at the beginning of the section \u2014 and DoG produces very few of them (after non-maximum suppression) in the depth channel, which is very smooth and lacks texture. On the opposite, our methods produce more points as they learn to \"spread\" image patches during training, making the response distribution more peaky. We compare the detections of our best model to DoG in Fig. 7."}, {"heading": "6. Conclusion", "text": "In this work, we have proposed an unsupervised approach to learning an interest point detector. The key idea of the method is to produce a repeatable ranking of points of the object and use top/bottom quantiles of the ranking as interest points. We have demonstrated how to learn such a detector for images. We show superior or comparable performance of our method with respect to DoG in two different settings: learning standard RGB detector from scratch and learning a detector, repeatable between different modalities (RGB and depth from Kinect). Future work includes learning the descriptor jointly with our detector. Also, one could investigate applying our method to detection beyond images (e.g., to interest frame detection in videos).\nAcknowledgements: This work is partially funded by the Swiss NSF project 163910, the Max Planck CLS Fellowship and the Swiss CTI project 17136.1 PFES-ES."}], "references": [{"title": "Interesting interest points", "author": ["H. Aan\u00e6s", "A.L. Dahl", "K. Steenstrup Pedersen"], "venue": "IJCV, 97:18\u201335", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Multispectral image feature points", "author": ["C. Aguilera", "F. Barrera", "F. Lumbreras", "A.D. Sappa", "R. Toledo"], "venue": "Sensors, 12(9):12661\u201312672", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Opencv library", "author": ["G. Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "ICCV, pages 1422\u20131430", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A combined corner and edge detector", "author": ["C. Harris", "M. Stephens"], "venue": "Alvey Vision Conference", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Algorithm as 136: A kmeans clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):100\u2013108", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1979}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, 14(8):1771\u2013 1800", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Keypoint descriptors for matching across multiple image modalities and nonlinear intensity variations", "author": ["A. Kelman", "M. Sofka", "C.V. Stewart"], "venue": "2007 IEEE conference on computer vision and pattern recognition, pages 1\u20137. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Data-driven fluid simulations using regression forests", "author": ["L. Ladicky", "S. Jeong", "B. Solenthaler", "M. Pollefeys", "M. Gross"], "venue": "ACM TOG, 34(6):199", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning covariant feature detectors", "author": ["K. Lenc", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1605.01224", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus"], "venue": "arXiv preprint arXiv:1603.01312", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "IJCV, 60(2):91\u2013110", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust widebaseline stereo from maximally stable extremal regions", "author": ["J. Matas", "O. Chum", "M. Urban", "T. Pajdla"], "venue": "Image and Vision Computing, 22(10):761\u2013767", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Finite mixture models", "author": ["G. McLachlan", "D. Peel"], "venue": "John Wiley & Sons", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Scale & affine invariant interest point detectors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IJCV, 60(1):63\u201386", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A comparison of affine region detectors", "author": ["K. Mikolajczyk", "T. Tuytelaars", "C. Schmid", "A. Zisserman", "J. Matas", "F. Schaffalitzky", "T. Kadir", "L. Van Gool"], "venue": "IJCV, 65(1-2):43\u201372", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Wxbs: Wide baseline stereo generalizations", "author": ["D. Mishkin", "J. Matas", "M. Perdoch", "K. Lenc"], "venue": "arXiv preprint arXiv:1504.06603", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Derek Hoiem and R", "author": ["P.K. Nathan Silberman"], "venue": "Fergus. Indoor segmentation and support inference from rgbd images. In ECCV", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "arXiv preprint arXiv:1603.09246", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "arXiv preprint arXiv:1604.07379", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Registering images to untextured geometry using average shading gradients", "author": ["T. Plotz", "S. Roth"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u2013 2326", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. IJCV, 115(3):211\u2013252", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, 22(8):888\u2013905", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "Science, 290(5500):2319\u20132323", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast corner detection", "author": ["M. Trajkovi\u0107", "M. Hedley"], "venue": "Image and Vision Computing, 16(2):75\u201387", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G.E. Hinton"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Tilde: A temporally invariant learned detector", "author": ["Y. Verdie", "K.M. Yi", "P. Fua", "V. Lepetit"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5279\u20135288. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "ICCV", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning descriptors for object recognition and 3d pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "CVPR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Lift: Learned invariant feature transform", "author": ["K.M. Yi", "E. Trulls", "V. Lepetit", "P. Fua"], "venue": "arXiv preprint arXiv:1603.09114", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 29, "context": "Recently, more labelled data with millions of examples have become available (for example, Imagenet [30], Microsoft COCO [17]), which led to significant progress in supervised learning research.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Recently, more labelled data with millions of examples have become available (for example, Imagenet [30], Microsoft COCO [17]), which led to significant progress in supervised learning research.", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "Let\u2019s assume one wants to match new images to untextured parts of an existing 3D model [27].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Some earlier works hand-crafted detectors like DoG [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 37, "context": "For example, LIFT [38] aims to extract a subset of DoG detections that are matched correctly in the later stages of the sparse 3D reconstruction.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Transformations that should transform the result of a detector together with the transformation \u2014 and thus their parameters have to be estimated as latent variables \u2014 are called covariant [22].", "startOffset": 188, "endOffset": 192}, {"referenceID": 19, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 6, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 232, "endOffset": 235}, {"referenceID": 8, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 313, "endOffset": 316}, {"referenceID": 28, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 322, "endOffset": 326}, {"referenceID": 31, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 335, "endOffset": 339}, {"referenceID": 11, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 367, "endOffset": 371}, {"referenceID": 33, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 379, "endOffset": 383}, {"referenceID": 13, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 503, "endOffset": 507}, {"referenceID": 15, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 509, "endOffset": 513}, {"referenceID": 10, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 607, "endOffset": 611}, {"referenceID": 27, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 649, "endOffset": 653}, {"referenceID": 5, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 686, "endOffset": 689}, {"referenceID": 35, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 724, "endOffset": 728}, {"referenceID": 36, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 746, "endOffset": 750}, {"referenceID": 25, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 786, "endOffset": 790}, {"referenceID": 24, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 827, "endOffset": 831}, {"referenceID": 5, "context": "Designing such a task is non-trivial, therefore only few successful approaches exist (for example, [6]).", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 64, "endOffset": 67}, {"referenceID": 20, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 159, "endOffset": 163}, {"referenceID": 37, "context": "Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response.", "startOffset": 114, "endOffset": 118}, {"referenceID": 34, "context": "Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response.", "startOffset": 183, "endOffset": 187}, {"referenceID": 14, "context": "Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response.", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "As investigated by [22], a good detector should produce interest points that are robust to viewpoint/illumination changes (to detect the same points and further match them) and sparse (to make feature matching feasible).", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "This pipeline is followed by many detectors including the popular DoG detector [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Third, we do accurate localization based on the second-order Taylor expansion of the response function around potential interest points [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 21, "context": "For quantitative evaluation, we use the repeatability measure described in [22] (with the overlap threshold parameter equal 40%).", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "\u2022 e for the ELU non-linearity function [4],", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "1) with random rotations from [0, 2\u03c0] and random scale changes from [ 13 , 3].", "startOffset": 68, "endOffset": 77}, {"referenceID": 2, "context": "1) with random rotations from [0, 2\u03c0] and random scale changes from [ 13 , 3].", "startOffset": 68, "endOffset": 77}, {"referenceID": 38, "context": "To optimize the objective (7), we use the Adadelta algorithm [39], which is a version of gradient descent that chooses the gradient step size per-parameter automatically.", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "We implement the model and optimization on a GPU (Nvidia Titan X) using the Torch7 framework [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "We used the DTU Robot Image Dataset [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 21, "context": "We used the Oxford VGG dataset [22], commonly chosen for this kind of evaluation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "For that we use the same matching score as in [22], i.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Matching score (the higher, the better) of DoG and our methods (Linear, Non-linear) on the benchmark from [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "1]) and large warps (s sample uniformly from [1, 2]).", "startOffset": 45, "endOffset": 51}, {"referenceID": 1, "context": "1]) and large warps (s sample uniformly from [1, 2]).", "startOffset": 45, "endOffset": 51}, {"referenceID": 21, "context": "We used the Oxford VGG dataset [22] (same as in the previous experiment).", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "We learn a detector between RGB and depth images by training on the NYUv2 dataset [24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "The DoG filter parameters default to the standard implementation [3].", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear what points are \"interesting\", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines.", "creator": "LaTeX with hyperref package"}}}