{"id": "1612.00385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Temporal Attention-Gated Model for Robust Sequence Classification", "abstract": "typical techniques usable for sequence classification are designed for well - segmented recognition sequences which theoretically has however been commonly edited continually to formally remove noisy or electronically irrelevant identifying parts. largely therefore, such symbolic methods now cannot effectively be easily performed applied on noisy sequences which exactly are expected in real - real world applications. and we additionally present the temporal attention - gated model ( vs tagm ) which is able to deal locally with certain noisy sequences. principally our attribute model assimilates ideas from attention models and gated recurrent networks. specifically, here we employ only an attention model interpretation to primarily measure out the relevance of each time step portion of a sequence to the final item decision. formally we then extensively use count the currently relevant sequential segments analyzed based on all their attention scores in a novel logic gated recurrent domain network, to learn the hidden representation for aiding the classification. more heavily importantly, our mathematical attention time weights observations provide me a remarkably physically meaningful interpretation structure for the salience of each time step in analyzing the sequence. broadly we demonstrate twice the merits of our model in designing both statistical interpretability and classification performance algorithms on a large variety of tasks, possibly including speech recognition, textual sentiment analysis and event recognition.", "histories": [["v1", "Thu, 1 Dec 2016 19:11:24 GMT  (3920kb,D)", "https://arxiv.org/abs/1612.00385v1", null], ["v2", "Sat, 15 Apr 2017 12:53:28 GMT  (4716kb,D)", "http://arxiv.org/abs/1612.00385v2", "Accepted by CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["wenjie pei", "tadas baltru\\v{s}aitis", "david m j tax", "louis-philippe morency"], "accepted": false, "id": "1612.00385"}, "pdf": {"name": "1612.00385.pdf", "metadata": {"source": "CRF", "title": "Temporal Attention-Gated Model for Robust Sequence Classification", "authors": ["Wenjie Pei", "Tadas Baltru\u0161aitis", "David M.J. Tax", "Louis-Philippe Morency"], "emails": ["W.Pei-1@tudelft.nl,", "tbaltrus@cs.cmu.edu,", "D.M.J.Tax@tudelft.nl,", "morency@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Sequence classification is posed as a problem of assigning a label to a sequence of observations. Sequence classification models have extensive applications ranging from computer vision [17] to natural language processing [1]. Most existing sequence classification models are designed for well segmented sequences and do not explicitly model the fact that irrelevant (noisy) parts may be present in the sequence. To reduce the interference of these irrelevant parts, researchers will often manually pre-process the dataset to remove irrelevant subsequences. This manual pre-processing can be very time consuming and reduce applicability in real-world scenarios.\nA popular approach for sequence classification is gated recurrent networks like Gated Recurrent Units (GRU) [4] and Long Short-Term Memory (LSTM) [11]. They employ gates (e.g., the input gate in the LSTM model) to balance\nbetween current and previous time steps when memorizing the temporal information flow. However, these vectorial gates are applied individually to each dimension of the information flow, thus it is hard to interpret the relative importance of the input time observations (i.e., time steps). What subset of sequential observations is the most salient for the classification task? Another way to balance the information flow, as we do in this work, is the adoption of attention-based mechanism, which applies individual attention scores to each observation (time step), allowing for better interpretability.\nIn this paper, we introduce the Temporal AttentionGated Model (TAGM) which extends the idea of attentionbased mechanism to sequence classification tasks (see overview in Figure 1). TAGM\u2019s attention module automatically localizes the salient observations which are relevant to the final decision and ignore the irrelavant (noisy) parts of the input sequence. We created a new recurrent neural unit that can learn a better sequence hidden representation based on the attention scores. Consequently, TAGM\u2019s classification decision is made based on the selected relevant segments, improving accuracy over the conventional models that take into account the whole input sequence.\nar X\niv :1\n61 2.\n00 38\n5v 2\n[ cs\n.C V\n] 1\n5 A\npr 2\nNotably, compared to conventional sequence classification models, TAGM benefits from the following advantages:\n\u2022 It is able to automatically capture salient parts of the input sequences thereby leading to better performance. \u2022 The inferred attention (scalar) scores provide a mean-\ningful interpretation for the informativeness of each observation in the sequence. \u2022 Compared to conventional gated recurrent models such\nas LSTM, our model reduces the number of parameters which leads to faster training and inference and better generalizability with less training data. \u2022 The proposed model is able to generalize to tasks in\ncomputer vision, speech recognition, and natural language processing."}, {"heading": "2. Related Work", "text": "While a full review of previous sequence classification models is beyond the scope of this paper, in this section we summarize approaches most relevant to our proposed approach, grouping them in three areas: sequence classification, attention models and recurrent networks. Sequence Classification. The conventional sequence classification models can be divided roughly into two categories: generative and discriminative models.\nThe first category focuses on learning an effective intermediate representation based on generative models. These methods are typically based on the Hidden Markov Models (HMMs) [31]. The HMM is a generative model which can be extended to class-conditional HMMs for sequence classification by combining class priors via Bayes\u2019 rule. HMM can also be used as the base model for Fisher Kernel [14] to learn a sequence representation.\nThe second category is the discriminative graphical models which model the distribution over all class labels conditioned on the input data. Conditional random fields (CRF) [20] are discriminative models for sequence labeling which aims to assign one label for each sequence observation. A potential drawback of common CRFs is that the linear mapping between observations and labels cannot model complex decision boundaries, which gives rise to many non-linear CRF-variants (e.g., latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]). Hidden-state CRF (HCRF) [30] employs a chain of k-nomial latent variables to model the latent structure and has been successfully used in the sequence labeling. Similarly, hidden unit logistic model (HULM) [26] utilizes binary stochastic hidden units to represent the exponential hidden states so as to model more complex latent decision boundaries.\nAforementioned works are specifically designed for well segmented sequences and hence cannot cope well with noisy or unsegmented sequences. Attention Models. Inspired by the attention scheme of hu-\nman foveal vision, attention model was proposed to focus selectively on certain relevant parts of the input by measuring the sensitivity of output to variances of the input. Doing so can not only improve the performance of the model but can also result in better interpretability [41]. Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8]. To the best of our knowledge, our TAGM is the first end-toend recurrent neural network to employ the attention mechanism in the temporal domain of sequences, with the added advantage of interpretability of its temporal salience indicators (i.e., temporal attention) at each time step (sequence observation). Our work is different from prior work focused on spatial domain (e.g., images) such as the model proposed by Sharma et al. [34]. Recurrent Networks. Recurrent Neural Networks (RNN) learn a representation for each time step by taking into account both the observation at current time step and the representation in the previous one [33]. The biggest advantage of recurrent neural networks lies in their capability of preserving information over time by the recurrent mechanism. Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7]. To address the gradient vanishing problem of plain-RNN when dealing with long sequences, LSTM [11] and GRU [4] were proposed. They are equipped with the gates to balance the information flow from the previous time step and current time step dynamically. Inspired by this setup, our TAGM model also employs a gate to filter out the noisy time steps and preserve the salient ones. The difference from the LSTM and GRU is that the gate value in our model is fed from the attention module which focuses on learning the salience at each time step."}, {"heading": "3. Temporal Attention-Gated Model", "text": "Given as input an unsegmented sequence of possibly noisy observations, our goal is to: (1) calculate a salience score for each time step observation in our input sequence, and (2) construct a hidden representation based on the salience scores, best suited for the sequence classification task. To achieve these goals, we propose the Temporal Attention-Gated Model (TAGM) which consists of two modules: temporal attention module, and recurrent attention-gated units. Our TAGM model can be trained in an end-to-end manner efficiently. The graphical structure of the model is illustrated in Figure 2."}, {"heading": "3.1. Recurrent Attention-Gated Units", "text": "The goal of the recurrent attention-gated units is to learn a hidden sequence representation which integrates the attention scores (inferred from the temporal attention module that will be discussed in the next section). In order to inte-\ngrate the attention scores in the recurrent network units, we define an attention gate to control how much information is incorporated from the input of the current time step based on the salience and relevance to the final task.\nFormally, given an input sequence x1,...,T = {x1, . . . ,xT } of length T in which xt \u2208 RD denotes the observation at the t-th time step, the attention score at time step t is denoted as at, which is a scalar value that indicates the salience of current time step to the final decision. For this purpose, we define our core recurring process where the hidden state ht at time step t is modeled as a convex summation:\nht = (1\u2212 at) \u00b7 ht\u22121 + at \u00b7 h\u2032t (1)\nWherein, ht\u22121 is the previous hidden state and h\u2032t is the candidate hidden state value which fully incorporates the input information xt in the current time step:\nh\u2032t = g(W \u00b7 ht\u22121 +U \u00b7 xt + b) (2)\nHerein, W and U are respectively the linear transformation parameters for previous and current time steps while b is the bias term. We use the rectified linear unit (ReLU)[24] as the activation function g. Equation 1 uses attention score at to balance the information flow between current candidate hidden state h\u2032t and previous hidden state ht\u22121. High attention value will push the model to focus more on the current hidden state h\u2032t and input feature xt, while low attention value\nwould make the model ignore the current input feature and inherit more information from previous time steps.\nThe learned hidden representation at the last time step hT of the sequence is further fed into the final classifier, often a softmax function, to perform a classification task, which calculates the probability of a predicted label yk among K classes as:\nP (yk|hT ) = exp{W>k hT + bk}\u2211K i=1 exp{W>i hT + bi}\n(3)\nwhere W>i and bi refer to the parameters calculating the linear mapping score for the i-th class."}, {"heading": "3.2. Temporal Attention Module", "text": "The goal of this module is to estimate the saliency and relevance of each sequence observation. This saliency score should not only be based on the input observation at the current time step, but also take into consideration information from neighboring observations in both directions. To model this neighborhood influence, we infer the attention score at in Equation 1 using a bi-directional RNN:\nat = \u03c3(m >( \u2212\u2192 h t; \u2190\u2212 h t) + b) (4)\nHerein, m is the weight vector of our fusion layer which integrates both directional layers of our bi-directional RNN and b is the bias term. A sigmoid function is employed as the activation function \u03c3 at the top layer of the attention module in Equation 4 to constraint the attention weight to lie between [0, 1]. \u2212\u2192 h t and \u2190\u2212 h t are the hidden representations of a bi-directional RNN model: \u2212\u2192 h t = g( \u2212\u2192 Wxt + \u2212\u2192 U \u2212\u2192 h t\u22121 + \u2212\u2192 b ) (5)\n\u2190\u2212 h t = g( \u2190\u2212 Wxt + \u2190\u2212 U \u2190\u2212 h t+1 + \u2190\u2212 b ) (6)\nThe ReLU functions are used as the activation functions g. Our choice of using plain bi-directional RNN model is motivated by the design goal of reducing the number of parameters in our model.\nThe learned attention weights at serve as the attention gate for Recurrent Attention-Gated Units to control the involved information flow. Furthermore, another important role the learned attention weights play is to provide an interpretability about the degree of salience of each time step."}, {"heading": "3.3. End-to-End Parameter Learning", "text": "Suppose we are given a training set D = {(x(n)1,...,T , y(n))}n=1,...,N containing N sequences of length T and their associated labels y(n). x(n)t \u2208 RD denotes the observation at the t-th time step of the n-th sample and T can differ from sequence to sequence. We learn jointly the two TAGM modules (temporal attention module and recurrent attention-gated units) and the final sequence classifier by minimizing the conditional negative\nlog-likelihood of the training data with respect to the parameters:\nL = \u2212 N\u2211\nn=1\nlogP ( y(n)|x(n)1,...,T ) (7)\nSince all three modules (including the final sequence classifier) are analytically differentiable, our TAGM model can be readily trained in an end-to-end manner. The loss is back-propagated through top recurrent attention-gated units and temporal attention module successively using backpropagation through time algorithm [40]."}, {"heading": "3.4. Comparison with LSTM and GRU", "text": "While our model is similar to RNN variants like GRU and LSTM, it is specifically designed with salience detection in mind and has four key differences when compared to them:\n\u2022 We only focus on one scalar attention score to measure the relevance of the current time step instead of generally modeling gate\u2019s multi-dimensional values for each hidden unit as done by GRU and LSTM. In this way, we can obtain an interpretable salience detection (demonstrated on three tasks in Section 4). \u2022 We separate the attention modeling and recurrent hid-\nden representation learning as two independent modules to decrease the degree of coupling. One of the advantages of this is our ability to customize the specific recurrent structure for each module with different complexity according to the requirements (eg., different size of hidden units in two modules of TAGM in Table 1). \u2022 We employ a bi-directional RNN to take into account\nboth the preceding and the following information of the sequence in the temporal attention module. It helps to model the temporal smoothness of the sequence of salience scores (demonstrated in Figure 4). It should be noted that it is different from the design of the gates in the bi-directional LSTM model since the latter just concatenates the hidden representations of two unidirectional LSTMs, which does not remedy the downside that all vectorial gates are still calculated by considering only one-directional information. \u2022 Our model only contains one scalar gate, namely the\nattention gate, rather than 2 vectorial gates in GRU and 3 gates in LSTM. Doing so enforces the attention gate to take full responsibility of modeling all the salience information. In addition, the model contains fewer parameters (compared to LSTM) and simpler gate structure with less redundancy (compared to GRU and LSTM). It eases the training procedure and can alleviate the potential over-fitting and has better generalization given small amount of training data, which is demonstrated in Section 4.1.3."}, {"heading": "4. Experiments", "text": "We performed experiments with TAGM on three publicly available datasets , selected to show generalization across different tasks and modalities: (1) speech recognition on an audio dataset, (2) sentiment analysis on a text dataset, and (3) event recognition on a video dataset.\nExperimental setup shared across experiments. For all the recurrent networks mentioned in this work (TAGM, GRU, LSTM and plain-RNN), the number of hidden units is tuned by selecting the best configuration from the option set {64, 128, 256} using a validation set. The dropout value is validated from the option set {0.0, 0.25, 0.5} to avoid potential overfitting. We employ RMSprop as the gradient descent optimization algorithm with gradient clipping between \u22125 and 5 [2].\nWe validate the learning rate for parameters m and b in Equation 4 to make the effective region of the sigmoid function of TAGM model adaptive to the specific data. Larger learning rate leads to sharper distribution of attention weights. Code reproducing the results of our experiments is available 1."}, {"heading": "4.1. Speech Recognition Experiments", "text": "We first conduct preliminary experiments on a modified dataset constructed from the Arabic spoken digit dataset [9] to (1) evaluate the effectiveness of the two main modules of TAGM; (2) compare the generalizability of three different gate-setup recurrent models (TAGM, GRU and LSTM) with the varying size of the training data."}, {"heading": "4.1.1 Dataset", "text": "The Arabic spoken digit dataset contains 8800 utterances, which were collected by asking 88 Arabic native speakers to utter all 10 digits ten times. Each sequence consists of 13-dimensional Mel Frequency Cepstral Coefficents (MFCCs) which were sampled at 11,025Hz, 16-bits using a Hamming window. We append white noise to the beginning and the end of each sample to simulate the problem with unsegmented sequences. The length of the unrelated sub-sequences before and after the original audio clips is randomized to ensure that the model does not learn to just focus on the middle of the sequence."}, {"heading": "4.1.2 Experimental Setup", "text": "We use the same data division as Hammami and Bedda [9]: 6600 samples as training set and 2200 samples as test set. We further set aside 1100 samples from training set as the validation set. There is no subject overlap in the three sets.\nWe compare the performance of our TAGM with three types of baseline models: Attention Module + Neural Network (AM-NN). To study the impact of our recurrent attention-gated unit, we include a baseline model which employs a feed-forward network\n1https://github.com/wenjiepei/TAGM\ndirectly on top of the temporal attention module. In this AM-NN model, v is defined as the weighted sum of input features:\nv = T\u2211 t=1 at \u00b7 xt, h = g(W \u00b7 v + b) (8)\nSequence classification is performed by passing h into a softmax layer, as done for our TAGM (see Equation 3). Discriminative Graphical Models. HCRF and HULM are both extensions of CRF [20] by inserting hidden layers to model the non-linear latent structure in the data. The difference lies in the structure of hidden layers: HCRF uses a chain of k-nomial latent variables while HULM utilizes k binary stochastic hidden units. Recurrent neural networks. Since our model is a recurrent network equipped with a gate mechanism, we compare it with other recurrent networks: plain-RNN, GRU, LSTM. We also investigate the bi-directional variant of our TAGM model (referred as Bi-TAGM), which employs the bi-directional recurrent configuration in the recurrent attention-gated units.\nIn our experiments, we also evaluate the generalizability when varying size of training data: from 1,100 to 5,500 training samples. During these experiments, the optimal configuration is selected automatically during validation from the option set {64,128,256}."}, {"heading": "4.1.3 Results and Discussion", "text": "Evaluation of Classification Performance Table 1 presents the classification performance of several sequence classifiers on Arabic dataset. In order to investigate the effect of the manually added noise information, we perform experiments on both clean and noisy versions of data.\nWhile the Plain-RNN is unable to recognize spoken digits in a noisy setting, other three recurrent models with gatesetup do not suffer from the noise and obtain comparable performance with the result achieved by HCRF on clean data. Our model achieves the best results among all classifiers with single-directional recurrent configuration. This probably results from better generalization of our model on the relatively small dataset due to the simpler gate setup and also the attention mechanism. We also perform experiments with the bi-directional version of GRU, LSTM and TAGM, in which our Bi-TAGM performs best. Bi-GRU achieves its best performance with 64 hidden units. It is worth mentioning that our (single-directional) TAGM using 47 K parameters already achieves comparable result with the Bi-LSTM and Bi-GRU, which indicates that the bi-directional mechanism in the attention module of TAGM enables it to capture most bi-directional information in the attention layer alone.\nComparison of generalizability with the varying size of training data. We first conduct experiments to compare\nthe generalizability of TAGM to GRU and LSTM by varying the size of training data on the noisy Arabic dataset. Figure 3 presents the experimental results. It can be seen that TAGM exhibits better generalizability than GRU and LSTM on smaller training data sizes, which we believe is caused by the need to learn fewer model parameters, avoiding overfitting.\nSequence Salience Detection. In order to evaluate the performance of sequence salience detection by our TAGM model, we visualize the attention weights of our model trained on the noisy Arabic dataset, which is illustrated in Figure 4.a. It shows that the attention model can correctly detect the informative section of the raw signal.\nTo investigate the effect of the temporal information contained in the hidden representation, we also visualize the attention weight of the Attention module + Neural Network classifier, which is shown in Figure 4.b. It shows that the\nTAGM results in a cleaner and smoother attention weight profile, also notice the spiky behavior, which is mainly achieved by the bi-directional RNN in our temporal attention module."}, {"heading": "4.2. Sentiment Analysis Experiments", "text": "Sentiment analysis is a popular research topic in the field of natural language processing (NLP) which aims to identify the viewpoint(s) underlying a text span [25]. We conduct experiments for sentiment analysis to evaluate the performance of our TAGM model on the text modality."}, {"heading": "4.2.1 Dataset", "text": "The Stanford Sentiment Treebank (SST) [36] is a data corpus of movie review excerpts. It consists of 11,855 sentences each of which is assigned a score to indicate the sentimental attitude towards the movie reviews. The dataset offers two types of annotations, sentiment annotations at the sentence level (with a total of 11,855 sentences) and at the phrase level (with a total of 215,154 phrases). The sentencelevel and phrase-level labels are provided with two resolutions: binary-classification task (positive or negative) and fine-grained task (5-level classes)."}, {"heading": "4.2.2 Experimental Setup", "text": "Following previous work [36], we utilize 300-d Glove word vectors (300 dimensions) pretrained over the Common Crawl [28] as the features for each word of the sentences. Our model is well suited to perform sentiment analysis using sentence-level labels. Nevertheless, we also perform experiments with phrase-level labels so as to have a fair and intuitive comparison with state-of-the-art baselines.\nWe follow the same data split as described by Socher et al. [36]: 8544/1101/2210 samples are used for training, validation and testing respectively in the 5-class task. The corresponding splits in the binary classification task are 6920/872/1821."}, {"heading": "4.2.3 Results and Discussion", "text": "Evaluation of Classification Performance We conduct two sets of experiments to evaluate the performance of our model in comparison with the baseline models. Since our model is designed for unsegmented and possibly noisy sequences modeling, it is more suitable to only use sentencelevel labels, although phrase-level labels are also provided in SST dataset. Table 2 shows the experimental results of several sequential models trained with only sentencelevel labels. Our model achieves the best result in both binary classification task and fine-grained (5-class) task. LSTM and GRU outperform plain-RNN model due to the information-filtering capability performed by additional gates. It is worth mentioning that our model achieves better performance than LSTM with only half the hidden parameters.\nTo have a fair comparison with the existing sentiment analysis models, we conduct the second set of experiments with both sentence-level and phrase-level labels. The results are presented in Table 3. It shows that our model outper-\nforms most of the existing models and achieves comparable accuracy with the state-of-the-art results. Our TAGM model actually obtains overall best results considering both binary and fine-grained cases. This is an encouraging result, in particular, since our model is not specifically designed towards NLP tasks.\nSequence Salience Detection In order to investigate the performance of salience detection by our TAGM model on Sentiment dataset (SST), we visualize the calculated attention weights for each word in the test sentences. Group (a) in Figure 5 presents a number of examples that are predicted correctly by our model in the binary-classification task. It shows that our model is able to successfully capture the key sentimental words and omit irrelevant words, even for the sentences with complicated syntax. We also test the examples that include negated expressions. As shown in the last two sentences of group (a), our model can deal with them very well. We also investigate the samples our model fails to predict the correct sentiment label (see Figure 5b)."}, {"heading": "4.3. Event recognition Experiments", "text": "We subsequently conduct experiments for video event recognition to evaluate our model on the visual modality."}, {"heading": "4.3.1 Dataset", "text": "Columbia Consumer Video (CCV) Database [16] is an unconstrained video database collected from YouTube videos without any post-editing. It consists of 9317 web videos with average duration of 80 seconds (210 hours in total). Except for some negative background videos, each video is manually annotated into one or more of 20 semantic categories such as \u2018basketball\u2019, \u2018ice skating\u2019, \u2018biking\u2019, \u2018birthday\u2019 and so on. It is a very challenging database due to the many noisy and irrelevant segments contained inside these videos. 4.3.2 Experimental Setup Following Jiang et al [16], we use the same split for training and test sets: 4659 videos as the training set and 4658 as the test set. We compare our model with the baseline method [15] on this dataset, which performs classification separately with Support Vector Machine (SVM) models trained on the bag-of-words representations for several popular features separately and then combines the results using late fusion. Its experimental results show that Convolutional Neural networks (CNNs) features perform best among all features they tried, hence we choose to use CNN features with the same setup, i.e., the outputs (4,096 dimensions) of the seventh fully-connected layer of a pre-trained AlexNet model [19]. For the sake of computational efficiency, we extract CNN features with a sampling rate 1/8\n(one out of every eight frame). We adopt mean Average Precision (mAP) as the evaluation metric, which is typically used for CCV dataset [16, 15]. Since more than one event (correct label) can happen in a sample, we perform binary classification for each category but train them jointly, hence the prediction score for each category is calculated by a sigmoid function instead of softmax Equation 3:\nP (yk = 1|hT ) = 1\n1 + exp{\u2212(W>k hT + bk)} (9)\nand joint binary cross-entropy over K categories is minimized: L=\u2212 N\u2211\nn=1 K\u2211 k=1 [ logP (yk = 1|hT ) + log(1\u2212 P (yk = 0|hT )) ]"}, {"heading": "4.3.3 Results and Discussion", "text": "Evaluation of Classification Performance. We compare our model with the event recognition system proposed by dataset authors [15]. Table 4 presents the performance of several models for event recognition, in which our TAGM outperforms the other recurrent models by a large margin. The baseline BOW+SVM employs the one-vs-all strategy to train a separate classifier for each event while our model trains all events jointly in a single classifier. Our model still shows encouraging results since it is quite a challenging task for TAGM to capture salient sections for 20 events with complex scenes simultaneously. Moreover, our TAGM can provide a meaningful interpretation which the baseline models cannot do. Sequence Salience Detection. Salience detection for CCV database is a difficult but appealing task due to complex and long scenes in videos. Figure 6 shows some examples where TAGM correctly locates the salient subsequences by the attention weights. Our model is able to\ncapture the relevant action, object and scene to the event, e.g., the action of riding bike for the event \u2018biking\u2019, cake for the event \u2018birthday\u2019 and baseball playground for the event \u2018baseball\u2019. It is interesting to note that the frame with the score 0.42 in event \u2018baseball\u2019 achieves the high score probably because of the real-time screen in the top right corner."}, {"heading": "5. Conclusion", "text": "In this work, we presented the Temporal Attention-Gated Model (TAGM), a new model for classifying noisy and unsegmented sequences. The model is inspired by attention models and gated recurrent networks and is able to detect salient parts of the sequence while ignoring irrelevant and noisy ones. The resulting hidden representation suffers less from the effect of noise and and thus leads to better performance. Furthermore, the learned attention scores provide a physically meaningful interpretation of relevance of each time step observation for the final decision. We showed the generalization of our approach on three very different datasets and sequence classification tasks. As future work, our model could be extended to help with document or video summarization."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "ICASSP, pages 8624\u20138628. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CVPR, pages 2422\u20132431. IEEE Computer Society", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoderdecoder approaches. In Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103\u2013111", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural conditional random fields", "author": ["T.-M.-T. Do", "T. Artieres"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollr", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR, pages 1473\u20131482. IEEE Computer Society", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1308.0850", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fernndez", "F. Gomez"], "venue": "In Proceedings of the International Conference on Machine Learning, ICML 2006, pages 369\u2013376", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved tree model for Arabic speech recognition", "author": ["N. Hammami", "M. Bedda"], "venue": "Int. Conf. on Computer Science and Information Technology, pages 521\u2013526", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent attention models for depth-based person identification", "author": ["A. Haque", "A. Alahi", "L. Fei-Fei"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["O. \u0130rsoy", "C. Cardie"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2096\u20132104. Curran Associates, Inc.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["M. Iyyer", "V. Manjunatha", "J. Boyd-Graber", "H. Daum\u00e9 III"], "venue": "Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A discriminative framework for detecting remot protein homologies", "author": ["T. Jaakkola", "M. Diekhans", "D. Haussler"], "venue": "Journal of Computational Biology, 7(1-2):95\u2013114", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Super fast event recognition in internet videos", "author": ["Y.-G. Jiang", "Q. Dai", "T. Mei", "Y. Rui", "S.-F. Chang"], "venue": "IEEE Transactions on Multimedia, 17(8):1\u201313", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Consumer video understanding: A benchmark database and  an evaluation of human and machine performance", "author": ["Y.-G. Jiang", "G. Ye", "S.-F. Chang", "D. Ellis", "A.C. Loui"], "venue": "Proceedings of ACM International Conference on Multimedia Retrieval (ICMR)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751. Association for Computational Linguistics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, pages 282\u2013 289", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["Q. Le", "T. Mikolov"], "venue": "T. Jebara and E. P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188\u20131196. JMLR Workshop and Conference Proceedings", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "EMNLP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": "ICASSP, pages 5528\u20135531. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "J. Frnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814. Omnipress", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Stroudsburg, PA, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "and L", "author": ["W. Pei", "H. Dibeklio\u01e7lu", "D.M.J. Tax"], "venue": "van der Maaten. Multivariate time-series classification using the hidden-unit logistic model. IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Conditional neural fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["A. Quattoni", "T. Darrell"], "venue": "Proceedings of CVPR07, pages 1\u20138", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Readings in speech recognition", "author": ["L.R. Rabiner"], "venue": "chapter A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, pages 267\u2013296. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1990}, {"title": "Temporal attention model for neural machine translation", "author": ["B. Sankaran", "H. Mi", "Y. Al-Onaizan", "A. Ittycheriah"], "venue": "CoRR, abs/1608.02927", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "A local learning algorithm for dynamic feedforward and recurrent networks", "author": ["J. SCHMIDHUBER"], "venue": "Connection Science, 1(4):403\u2013412", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1989}, {"title": "Action recognition using visual attention", "author": ["S. Sharma", "R. Kiros", "R. Salakhutdinov"], "venue": "CoRR, abs/1511.04119", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 151\u2013161, Stroudsburg, PA, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1556\u20131566. The Association for Computer Linguistics", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative image modeling using spatial lstms", "author": ["L. Theis", "M. Bethge"], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915, pages 1927\u20131935, Cambridge, MA, USA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden-unit conditional random fields", "author": ["L. van der Maaten", "M. Welling", "L.K. Saul"], "venue": "AISTATS, volume 15 of JMLR Proceedings,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks, 1", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1988}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. In D. Blei and F. Bach, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2048\u2013 2057. JMLR Workshop and Conference Proceedings", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "Computer Vision (ICCV), 2015 IEEE International Conference on. IEEE", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Sequence classification models have extensive applications ranging from computer vision [17] to natural language processing [1].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Sequence classification models have extensive applications ranging from computer vision [17] to natural language processing [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "A popular approach for sequence classification is gated recurrent networks like Gated Recurrent Units (GRU) [4] and Long Short-Term Memory (LSTM) [11].", "startOffset": 108, "endOffset": 111}, {"referenceID": 10, "context": "A popular approach for sequence classification is gated recurrent networks like Gated Recurrent Units (GRU) [4] and Long Short-Term Memory (LSTM) [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 30, "context": "These methods are typically based on the Hidden Markov Models (HMMs) [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "HMM can also be used as the base model for Fisher Kernel [14] to learn a sequence representation.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "Conditional random fields (CRF) [20] are discriminative models for sequence labeling which aims to assign one label for each sequence observation.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 93, "endOffset": 96}, {"referenceID": 38, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "Hidden-state CRF (HCRF) [30] employs a chain of k-nomial latent variables to model the latent structure and has been successfully used in the sequence labeling.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Similarly, hidden unit logistic model (HULM) [26] utilizes binary stochastic hidden units to represent the exponential hidden states so as to model more complex latent decision boundaries.", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "Doing so can not only improve the performance of the model but can also result in better interpretability [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 2, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 5, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 41, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 0, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 101, "endOffset": 112}, {"referenceID": 21, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 101, "endOffset": 112}, {"referenceID": 31, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 101, "endOffset": 112}, {"referenceID": 9, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Recurrent Neural Networks (RNN) learn a representation for each time step by taking into account both the observation at current time step and the representation in the previous one [33].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7].", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "To address the gradient vanishing problem of plain-RNN when dealing with long sequences, LSTM [11] and GRU [4] were proposed.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "To address the gradient vanishing problem of plain-RNN when dealing with long sequences, LSTM [11] and GRU [4] were proposed.", "startOffset": 107, "endOffset": 110}, {"referenceID": 23, "context": "We use the rectified linear unit (ReLU)[24] as the activation function g.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "lie between [0, 1].", "startOffset": 12, "endOffset": 18}, {"referenceID": 39, "context": "The loss is back-propagated through top recurrent attention-gated units and temporal attention module successively using backpropagation through time algorithm [40].", "startOffset": 160, "endOffset": 164}, {"referenceID": 1, "context": "We employ RMSprop as the gradient descent optimization algorithm with gradient clipping between \u22125 and 5 [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "We first conduct preliminary experiments on a modified dataset constructed from the Arabic spoken digit dataset [9] to (1) evaluate the effectiveness of the two main modules of TAGM; (2) compare the generalizability of three different gate-setup recurrent models (TAGM, GRU and LSTM) with the varying size of the training data.", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "We use the same data division as Hammami and Bedda [9]: 6600 samples as training set and 2200 samples as test set.", "startOffset": 51, "endOffset": 54}, {"referenceID": 19, "context": "HCRF and HULM are both extensions of CRF [20] by inserting hidden layers to model the non-linear latent structure in the data.", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "Model #Hidden units #Parameters Accuracy HULM\u2217 [26] \u2212 \u2212 95.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "32 HCRF\u2217 [26] \u2212 \u2212 96.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "Sentiment analysis is a popular research topic in the field of natural language processing (NLP) which aims to identify the viewpoint(s) underlying a text span [25].", "startOffset": 160, "endOffset": 164}, {"referenceID": 35, "context": "The Stanford Sentiment Treebank (SST) [36] is a data corpus of movie review excerpts.", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "Following previous work [36], we utilize 300-d Glove word vectors (300 dimensions) pretrained over the Common Crawl [28] as the features for each word of the sentences.", "startOffset": 24, "endOffset": 28}, {"referenceID": 27, "context": "Following previous work [36], we utilize 300-d Glove word vectors (300 dimensions) pretrained over the Common Crawl [28] as the features for each word of the sentences.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "Syntactic compositions DAN-ROOT [13] 85.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "compositions NBOW-RAND [13] 81.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "NBOW [13] 83.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "BiNB [13] 83.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "compositions RecNN [35] 82.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "RecNTN [36] 85.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "DRecNN [12] 86.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "DAN [13] 86.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "TreeLSTM [37] 86.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "CNN-MC [18] 88.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "PVEC [21] 87.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "[36]: 8544/1101/2210 samples are used for training, validation and testing respectively in the 5-class task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Columbia Consumer Video (CCV) Database [16] is an unconstrained video database collected from YouTube videos without any post-editing.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "2 Experimental Setup Following Jiang et al [16], we use the same split for training and test sets: 4659 videos as the training set and 4658 as the test set.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "We compare our model with the baseline method [15] on this dataset, which performs classification separately with Support Vector Machine (SVM) models trained on the bag-of-words representations for several popular features separately and then combines the results using late fusion.", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": ", the outputs (4,096 dimensions) of the seventh fully-connected layer of a pre-trained AlexNet model [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "We adopt mean Average Precision (mAP) as the evaluation metric, which is typically used for CCV dataset [16, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 14, "context": "We adopt mean Average Precision (mAP) as the evaluation metric, which is typically used for CCV dataset [16, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 14, "context": "We compare our model with the event recognition system proposed by dataset authors [15].", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper, we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.", "creator": "LaTeX with hyperref package"}}}