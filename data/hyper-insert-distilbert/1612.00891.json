{"id": "1612.00891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Parameter Compression of Recurrent Neural Networks and Degradation of Short-term Memory", "abstract": "the sometimes significant computational costs of deploying neural networks in large - scale or resource constrained functional environments, such as compact data creation centers and mobile phone devices, has currently spurred subsequent interest in model compression, which additionally can achieve a reduction deficit in both arithmetic operations encoding and storage memory. several techniques have specifically been proposed for repeatedly reducing or compressing the parameters for predicting feed - forward and convolutional neural string networks, but less is understood about the fourier effect correlation of parameter amplitude compression on recurrent dynamic neural networks ( cyclic rnn )., in particular, understanding the extent to date which region the stored recurrent specification parameters displaced can be properly compressed and the theoretical impact on short - term memory performance, consequently is reportedly not notably well understood. \" in performing this paper, we study for the effect profile of complexity reduction, through singular temporal value threshold decomposition rank node reduction, on alternating rnn functions and with minimal gated recurrent unit ( nh mgru ) orthogonal networks for organizing several tasks. because we thoroughly show precisely that considerable generic rank trace reduction instead is possible performance when calculating compressing and recurrent minimum weights, even although without fine harmonic tuning. thus furthermore, we propose introducing a perturbation recovery model formula for the additive effect of general ensemble perturbations, motives such either as designing a compression, on the key recurrent parameters of evolving rnns. the model material is tested against a noiseless linear memorization therapy experiment that separately elucidates dramatically the critical short - term memory performance. in examining this analytical way, we independently demonstrate that only the psychological effect of compression of recurrent parameters is dependent partly on the degree of temporal root coherence present in the data and task. incorporating this results work can guide on - thread the - fly rnn compression for managing novel target environments or traditional tasks, and provides insight for extensively applying rnn compression in low - power devices, accessories such as broadband hearing gain aids.", "histories": [["v1", "Fri, 2 Dec 2016 23:11:10 GMT  (1940kb)", "http://arxiv.org/abs/1612.00891v1", "Submitted to IJCNN 2017"], ["v2", "Fri, 24 Feb 2017 18:22:30 GMT  (1954kb)", "http://arxiv.org/abs/1612.00891v2", "Accepted to IJCNN 2017. Final camera ready paper"]], "COMMENTS": "Submitted to IJCNN 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jonathan a cox"], "accepted": false, "id": "1612.00891"}, "pdf": {"name": "1612.00891.pdf", "metadata": {"source": "CRF", "title": "Parameter Compression of Recurrent Neural Networks and Degredation of Short-term Memory", "authors": ["Jonathan A. Cox"], "emails": ["joncox@alum.mit.edu"], "sections": [{"heading": null, "text": "Keywords\u2014compression; recurrent neural networks; complexity reduction; SVD; RNN; MGRU; MRU\nI. INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3]. However, such models are often extremely computationally complex, and can have hundreds of millions of parameters for both recurrent and convolutional neural networks [3], [4]. Reducing the complexity of neural networks is imperative, not only for improved efficiency, but for enabling novel applications in resource constrained environments, such as for hearing aids. Recent progress in complexity reduction, or compression, of feed-forward neural networks has demonstrated reductions of at least 5-10x for the parameters of fully connected layers. Related methods applied to convolutional neural networks (CNN) have demonstrated a reduction of 3-5x [5]\u2013[7].\nRecurrent neural networks are especially interesting for low-power and mobile applications since they can often involve real-time processing of sequential information. Although there has been work investigating pruning, compression and rank reduction of feed-forward and convolutional neural networks, it is not well understood how complexity reduction impacts recurrent neural networks (RNN), and memory-cell based architectures such as the long short-term memory (LSTM), gated recurrent unit (GRU) or the recently proposed minimal gated recurrent unit (MGRU) [8]\u2013[10]. While work by Geras et al. [11] has investigated using model compression to train a CNN from an LSTM, it is not always possible or desirable to transform an RNN into a CNN for practical and theoretical reasons (for instance, when very long-range time dependencies are inherent in the task). Nevertheless, there is significant interest in complexity reduction of RNNs, as they have witnessed large-scale adoption in industrial systems [12], [13]. In contrast to feed-forward neural networks, such as CNNs, information can flow through the recurrent feedback connections of an RNN an indeterminate number of cycles. In general, during inference, it is often not known in advance how long an RNN must be unfolded, such as during image captioning or object detection [14], [15]. While this capability makes RNNs extremely powerful and expressive, applying and understanding complexity reduction is more challenging [16]. As a result, compression of RNN parameters becomes dependent on the temporal dependencies embedded in the data and task, which may not be fully known during inference and can change over time.\nIn this paper, we show that recurrent neural networks, including those using a memory cell based architecture, such as MGRU, achieve significant complexity reduction of the feed-forward and recurrent connection weights, for both classification and language modeling sequence prediction tasks. In addition, we provide a more fundamental understanding of how complexity reduction, viewed as a general perturbation or corruption, is impacted by temporal dependency. Therefore, we devise a perturbation model of the effect of a general compression method, such as singular value decomposition (SVD) rank reduction, on the short-term memory performance of recurrent networks. This model is tested on a noiseless memorization task to elucidate the conditions over which scaling of short-term memory performance agrees. In this way, it is shown how the\nachievable compression is dependent on the degree of temporal coherence present in the task and data. II. RANK REDUCTION OF RECURRENT NEURAL NETWORKS Long short-term memory (LSTM) networks have been extremely popular due to their considerable practical success. However, it has recently been shown that there is redundancy in the LSTM structure, which has led to new architectures, such as the GRU [10]. There has also been renewed interest in RNNs owing to more powerful optimization algorithms, such as Hessian-free optimization [17]. It has been suggested that the core attribute of the LSTM is the memory cell architecture, and that comparable performance is obtained with fewer [18], and possibly even a single, flow control gate, as for the MGRU [8]. Therefore, to best capture the underlying dynamics, our analysis is performed with RNN and MGRU architectures. The MGRU represents the most fundamental incarnation of a gated differentiable memory cell (GDMC), and is valuable for understanding how compression impacts both RNN and GDMC-based networks. A standard recurrent unit (RNN) is an artificial neural network with recurrent, or feedback, connections within a fully connected layer, as in Eqn. (1).        1 1l ra t f W a t W a t       (1)\nHere, f is a general non-linear function, such as the hyperbolic tangent or rectifying nonlinearity (ReLU), W is the matrix of feed-forward connections, Wr is the matrix of recurrent connections and a(l-1) are the activations from the previous layer. The recurrent matrix transforms the output of the RNN layer from the previous time step, t.\nThe minimal gated recurrent unit (MGRU) is a recently proposed reduction of the GRU, as shown in Fig. 1. The MGRU has only a single gate for controlling the memory cell state. In contrast to the basic RNN, differentiable memory cell architectures demonstrate superior long short-term memory performance on a variety of tasks. Consequently, it is reasonable to presume that they may be less susceptible to perturbations in the recurrent weights, such as from rank reduction, since information could remain in the cell state with less influence due to successive perturbations from the recurrent weights. The MGRU, as described by Eqn. (2), differs from the RNN in that there is an additional gate that \u201cswitches\u201d the output state between a linear combination of the input and the prior state, s(t-1). In this way, it represents the simplest form of differentiable memory cell, and lacks several features of the LSTM that add complexity, such as: an output nonlinearity, an input gate, an output gate and peephole connections.                \n           1 1 1 1 1 1 lf r c c c lf r i i i\nc i c\na t W a t W s t a t f W a t W s t s t a t a t a t s t                       (2)\nAn effective form of complexity reduction, which has been demonstrated on feed-forward and convolutional neural networks, is rank reduction via singular value decomposition on the network parameters. For RNNs, the forward and recurrent matrix of weights can be individually decomposed into their singular values and orthonormal bases, \u2211 and U, V, respectively. By eliminating the smallest singular values, an optimal reduced rank representation, Q and V is found, as in Eqn. (3). This compressed representation has only R\u2219(M+N) parameters, where R is the rank and M and N are the original dimensions of a particular weight matrix. The rank reduced matrix-vector product can be viewed as decomposing a single layer into two linear layers, parameterized byQ and V , such that the computation is performed as in (4), where b is the bias and x is the vector of gate inputs.\n T T TW U V U V QV        (3)     Ta f Wx b f Q V x b     (4)\nIII. PERTUBATION MODEL FOR SHORT-TERM MEMORY One of the strengths of recurrent neural networks is the ability to learn temporal sequence tasks requiring some degree of short-term memory capability that is learned directly from the data. In contrast to feed-forward networks, the recurrent neural network is also unfolded into a deep network in time, with shared recurrent weights at every time step. In this way, information is repeatedly transformed by the recurrent weights. As a result, they are especially sensitive to corruptions and perturbations, which is why performance suffers when Dropout is applied to the recurrent connections [19]. For long sequence problems, information is corrupted by the perturbation to the\nrecurrent weights over many time steps. Consequently, it is reasonable to suspect that recurrent connections cannot benefit significantly from compression. Compounding the problem, the propagation length of information in an unfolded RNN is unknown at inference time and dependent on the specific task and data encountered. We therefore wish to describe a perturbation model for understanding how general perturbations to the recurrent weights, such as SVD rank reduction, impact the fundamental performance scaling of recurrent networks.\nConsider a standard RNN with tanh activation nonlinearity, which is biased near zero and has small activations, perhaps through a sparsity activation penalty [20]. In this case, it is reasonable to linearize the activation function (for the purposes of our analysis), within some regime, as in Eqn. (5). Furthermore, we can simplify the effect of an arbitrary compression scheme, such as SVD rank reduction, as a perturbation \u03b4 on the original weight\nmatrix, TW QV W     , where 1  .      tanh rf z z z W x    (5)\nTo understand the effect of a small perturbation on short-term memory performance, consider the noiseless memorization experiment described by Martens and Sutskever [17] and shown in Fig. 2. For this task, an RNN is presented with a sequence of bits, Nb long, while it is unfolded over T time-steps. At t = T-Nb, the network is asked to reproduce the bit sequence that was initially presented when the stop-bit, s, is presented. This task elucidates the short-term memory performance scaling of a network and allows us to model the degradation due to a perturbation, such as complexity reduction, and gain fundamental insight. Performance on this task is evaluated by the difference in the ground-truth output and the actual output, \u0394b, after T successive unfoldings. Since the input and output weights, Wi and Wo, are unperturbed we neglect them for simplicity. After neglecting higher order terms of \u03b4 in Eqn. (6), we have a model for the error due to the effect of a perturbation. Clearly, in the regime where the assumptions remain valid, the error scales linearly with the temporal coherence, T, and the magnitude of the perturbation \u03b4. Also, the spectral radius of the recurrent weight matrix, \u03c1, should be set   1rW  so that the error does not blow up. Nevertheless, there is a tradeoff between the desire to set   1rW  to encourage short-term memory and to reduce the amplification of error due to a perturbation [21].\n      1 T T r r T r\nb b b W x W x\nT W x           \n(6)\nBased on this model, we expect that the effect of RNN complexity reduction on network performance is dependent on the degree of temporal coherence in the data and task, which is\noften determined by the environment and stochastic. In general, it is not known at inference and is not stationary. The particular examples of image caption generation and object detection in crowded scenes illustrate this point [14], [15], since the output sequence is highly dependent on the input data found in the field. IV. EXPERIMENTS Three separate experiments are performed on three different tasks for both RNN and MGRU networks. The goal of the first two experiments is to understand how complexity reduction through SVD rank reduction separately impacts performance for feed-forward and recurrent connections. In particular, we are interested in real-time complexity reduction of RNNs without the benefit of additional fine-tuning. Lastly, we perform the noiseless memorization experiment to understand the regime over which the perturbation model is applicable, and how it applies to RNN and MGRU networks."}, {"heading": "A. Language Model", "text": "In the first experiment, we train a recurrent language model to predict the next word in a sequence by minimizing the cross-entropy error over the full vocabulary, as in [19]. We use the complete works of Shakespeare as a corpus for this task, and apply the Stanford Treebank Tokenizer (PTBTokenizer) library to tokenize the corpus [22]. The resulting corpus has a vocabulary of 26,430 words. All models and experiments consist of a single recurrent layer and are trained using the Adam optimizer [23] with Dropout applied to the hidden layer outputs. For the language modeling task, we use a batch size of 20 (by dividing the corpus into equal portions) and train the network via continuous, ordered passes through the corpus for 30 epochs. Both the RNN and MGRU networks have a single recurrent layer with 500 units, which is fed by a word embedding matrix of 500 dimensions per word. The performance on this task is measured as the mean perplexity over the full vocabulary distribution. After training, we construct a new, lower rank model with SVD compressed parameters. Both the feed-forward weights that are incoming to the recurrent layer and the recurrent connections within the recurrent layer are compressed. The ranks are separately swept from 1 to 500 (full rank) as the perplexity is recorded over an entire epoch (see Fig. 3). No fine-tuning is performed after rank reduction.\nThe isolines of the contour plots in Fig. 3 are shown for a logarithmic increase in perplexity,  10 min20log P P . Thus, a 1 dB increase in perplexity is approximately 12%. For this experiment, relatively greater rank reduction is possible for feed-forward connections than for recurrent connections, by a factor of about 2:1\u2014for both the RNN and MGRU models. Without fine tuning, significant rank reduction is possible with minor degradation in performance. Moreover, we have observed that practical models (see [19]) have even greater redundancy, and tend to be highly over-parameterized in comparison to this simplified example."}, {"heading": "B. MNIST Classifier", "text": "The second experiment is performed with a single-layer recurrent MNIST classifier. In this case, the data is presented to the RNN as one 28-dimension column vector per time-step, over 28 time-steps. In effect, the RNN observes the image one \u201cscan line\u201d at a time and must make sense of the total image. The output of the recurrent hidden layer, which has also 500 units, is temporally mean-pooled and sent to a fully-connected\noutput layer with softmax activation and cross-entropy loss over the 10 classes. The rank of the feed-forward connections is at most 28, which is the dimensionality of the input, while it is at most 500 for the recurrent connections. In contrast to the language model, the rank of both feed-forward and recurrent weights is reduced by similar ratios of about 6x, along the 98.5% isoline. However, as we shall see from the next experiment, the degree of reduction is dependent on the task and data, and it may not always be possible to achieve significant compression in the recurrent weights when long short-term memory performance is critical. Understanding this tradeoff is imperative when deploying real-world models in the field."}, {"heading": "C. Noiseless Memorization and Temporal Coherence", "text": "The final experiment was conducted to verify the performance scaling of the perturbation model for short-term memory. To this end, we trained RNN and MGRU noiseless memorization models as in Fig. 2 [17], with 100 recurrent hidden units. In both cases, a sequence of Nb = 8 bits drawn from a Bernoulli distribution with p = 0.5 are presented to the\nnetwork. After a \u201csilent\u201d period of 0 to 30 steps is observed, a stop-word is presented and the network must recall the sequence. A period of T = 0 corresponds to the trivial case where the network must immediately reproduce the input within a single time step (no memorization). Similarly, T = 1 would ask the network to remember for only a single time step. Both RNN and MGRU networks are trained with mini-batches of 64 where  0,30T  is randomly drawn, once per batch. Both RNN and MGRU models fully converge, however MGRU does so much faster.\nAfter training, the rank of the recurrent connections is swept for various values of T, as shown in Fig. 4. Each point on the surface is the average of 1000 trials. For large \u03b4, the approximation breaks down, and the error saturates almost immediately. Here, \u03b4 is the root mean squared (RMS) error from a rank R approximation of the recurrent weight matrix. To estimate how the error, \u0394b, scales with duration, T, the 2D plot is collapsed by integrating over \u03b4 in the linear regime (for small perturbation). Thus, \u03b2 in Eqn. (7) is the mean integrated RMS error up to some peak perturbation, at which point the perturbation model is no longer in the valid regime.\n   2 0 0 1 1 ,f Tf d t b d tN T          (7)\nThe results shown on the bottom row of Fig. 4 confirm that the error scales linearly with \u03b4 and T, supporting the proposed perturbation model. Interestingly, the memory cell architecture, or MGRU, exhibits similar behavior, but is less sensitive when T is small. This may indicate that for moderate temporal durations (T<30) the MGRU is able to accurately retain short-term memory in the cell state without subjecting it to repeated perturbations. However, beyond this duration, it becomes more sensitive to degradation, perhaps because the model was trained with durations of T up to 30. In contrast, RNN error scales quite linearly, even for short durations of T. V. CONCLUSION\nOur results demonstrate that both the feed-forward and recurrent connections of RNN and differentiable memory-cell architectures (MGRU) benefit from parameter compression, which has considerable practical benefit for low-power and resource constrained operating environments. Unlike for strictly feed-forward networks, such as CNNs, compression of recurrent connections impacts performance in the temporal domain, which is dependent on the sequencial coherence in the data and task. This temporal dependency is often unknown until during inference, and may vary over time. Results suggest that MGRU is less sensitive to recurrent parameter compression when faced with varying temporal depenence in the data. Finally, we proposed and experimentally validated a pertubation model governing the scaling of short-term memory performance due to parameter compression. Consequently, this work will guide real-time RNN compression for practical applications, when deploying trained models in the field. For instance, by estimating the temporal coherence of the data and adjusting the compression in real-time, minimal resource utilization can be achieved for applications ranging from hearing aids to mobile devices.\nREFERENCES [1] S. R. Park and J. Lee, \u201cA Fully Convolutional Neural Network for\nSpeech Enhancement,\u201d ArXiv Prepr. ArXiv160907132, 2016. [2] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin,\n\u201cCompression of deep convolutional neural networks for fast and low power mobile applications,\u201d ArXiv Prepr. ArXiv151106530, 2015. [3] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K. Keutzer, \u201cSqueezeNet: AlexNet-level accuracy with 50x fewer\nparameters and< 1MB model size,\u201d ArXiv Prepr. ArXiv160207360, 2016. [4] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112. [5] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, \u201cExploiting linear structure within convolutional networks for efficient evaluation,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 1269\u20131277. [6] M. Denil, B. Shakibi, L. Dinh, N. de Freitas, and others, \u201cPredicting parameters in deep learning,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156. [7] M. Jaderberg, A. Vedaldi, and A. Zisserman, \u201cSpeeding up convolutional neural networks with low rank expansions,\u201d ArXiv Prepr. ArXiv14053866, 2014. [8] G.-B. Zhou, J. Wu, C.-L. Zhang, and Z.-H. Zhou, \u201cMinimal gated unit for recurrent neural networks,\u201d Int. J. Autom. Comput., vol. 13, no. 3, pp. 226\u2013234, 2016. [9] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997. [10] K. Cho, B. van Merri\u00ebnboer, D. Bahdanau, and Y. Bengio, \u201cOn the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches,\u201d Syntax Semant. Struct. Stat. Transl., p. 103, 2014. [11] K. J. Geras et al., \u201cBlending LSTMs into CNNs,\u201d ArXiv Prepr. ArXiv151106433, 2015. [12] G. Tucker, M. Wu, M. Sun, S. Panchapagesan, G. Fu, and S. Vitaladevuni, \u201cModel compression applied to small-footprint keyword spotting,\u201d in Proc. Interspeech, 2016. [13] Y. Wu et al., \u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,\u201d ArXiv Prepr. ArXiv160908144, 2016. [14] K. Xu et al., \u201cShow, Attend and Tell: Neural Image Caption Generation with Visual Attention,\u201d in Proceedings of the 32nd International Conference on Machine Learning (ICML-15), 2015, pp. 2048\u20132057. [15] R. Stewart and M. Andriluka, \u201cEnd-to-end people detection in crowded scenes,\u201d ArXiv Prepr. ArXiv150604878, 2015. [16] S. Zhang et al., \u201cArchitectural Complexity Measures of Recurrent Neural Networks,\u201d ArXiv Prepr. ArXiv160208210, 2016. [17] J. Martens and I. Sutskever, \u201cLearning recurrent neural networks with hessian-free optimization,\u201d in Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1033\u20131040. [18] K. Greff, R. K. Srivastava, J. Koutn\u00edk, B. R. Steunebrink, and J. Schmidhuber, \u201cLSTM: A search space odyssey,\u201d ArXiv Prepr. ArXiv150304069, 2015. [19] W. Zaremba, I. Sutskever, and O. Vinyals, \u201cRecurrent neural network regularization,\u201d ArXiv Prepr. ArXiv14092329, 2014. [20] A. Ng, \u201cSparse autoencoder,\u201d CS294A Lect. Notes, vol. 72, pp. 1\u201319, 2011. [21] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, \u201cOn the importance of initialization and momentum in deep learning.,\u201d ICML, vol. 3, no. 28, pp. 1139\u20131147, 2013. [22] C. Manning, T. Grow, T. Grenager, J. Finkel, and J. Bauer, \u201cStanford Tokenizer.\u201d [Online]. Available: http://nlp.stanford.edu/software/tokenizer.html. [Accessed: 14-Nov2016]. [23] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d ArXiv Prepr. ArXiv14126980, 2014."}], "references": [{"title": "A Fully Convolutional Neural Network for Speech Enhancement", "author": ["S.R. Park", "J. Lee"], "venue": "ArXiv Prepr. ArXiv160907132, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "ArXiv Prepr. ArXiv151106530, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer  parameters and< 1MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": "ArXiv Prepr. ArXiv160207360, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1269\u20131277.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas", "others"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "ArXiv Prepr. ArXiv14053866, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Minimal gated unit for recurrent neural networks", "author": ["G.-B. Zhou", "J. Wu", "C.-L. Zhang", "Z.-H. Zhou"], "venue": "Int. J. Autom. Comput., vol. 13, no. 3, pp. 226\u2013234, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Syntax Semant. Struct. Stat. Transl., p. 103, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Blending LSTMs into CNNs", "author": ["K.J. Geras"], "venue": "ArXiv Prepr. ArXiv151106433, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Model compression applied to small-footprint keyword spotting", "author": ["G. Tucker", "M. Wu", "M. Sun", "S. Panchapagesan", "G. Fu", "S. Vitaladevuni"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Y. Wu"], "venue": "ArXiv Prepr. ArXiv160908144, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), 2015, pp. 2048\u20132057.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end people detection in crowded scenes", "author": ["R. Stewart", "M. Andriluka"], "venue": "ArXiv Prepr. ArXiv150604878, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Architectural Complexity Measures of Recurrent Neural Networks", "author": ["S. Zhang"], "venue": "ArXiv Prepr. ArXiv160208210, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1033\u20131040.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u00edk", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "ArXiv Prepr. ArXiv150304069, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "ArXiv Prepr. ArXiv14092329, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lect. Notes, vol. 72, pp. 1\u201319, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "ICML, vol. 3, no. 28, pp. 1139\u20131147, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv Prepr. ArXiv14126980, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3].", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "However, such models are often extremely computationally complex, and can have hundreds of millions of parameters for both recurrent and convolutional neural networks [3], [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "However, such models are often extremely computationally complex, and can have hundreds of millions of parameters for both recurrent and convolutional neural networks [3], [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Related methods applied to convolutional neural networks (CNN) have demonstrated a reduction of 3-5x [5]\u2013[7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Related methods applied to convolutional neural networks (CNN) have demonstrated a reduction of 3-5x [5]\u2013[7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Although there has been work investigating pruning, compression and rank reduction of feed-forward and convolutional neural networks, it is not well understood how complexity reduction impacts recurrent neural networks (RNN), and memory-cell based architectures such as the long short-term memory (LSTM), gated recurrent unit (GRU) or the recently proposed minimal gated recurrent unit (MGRU) [8]\u2013 [10].", "startOffset": 393, "endOffset": 396}, {"referenceID": 9, "context": "Although there has been work investigating pruning, compression and rank reduction of feed-forward and convolutional neural networks, it is not well understood how complexity reduction impacts recurrent neural networks (RNN), and memory-cell based architectures such as the long short-term memory (LSTM), gated recurrent unit (GRU) or the recently proposed minimal gated recurrent unit (MGRU) [8]\u2013 [10].", "startOffset": 398, "endOffset": 402}, {"referenceID": 10, "context": "[11] has investigated using model compression to train a CNN from an LSTM, it is not always possible or desirable to transform an RNN into a CNN for practical and theoretical reasons (for instance, when very long-range time dependencies are inherent in the task).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Nevertheless, there is significant interest in complexity reduction of RNNs, as they have witnessed large-scale adoption in industrial systems [12], [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "Nevertheless, there is significant interest in complexity reduction of RNNs, as they have witnessed large-scale adoption in industrial systems [12], [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 13, "context": "In general, during inference, it is often not known in advance how long an RNN must be unfolded, such as during image captioning or object detection [14], [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "In general, during inference, it is often not known in advance how long an RNN must be unfolded, such as during image captioning or object detection [14], [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "While this capability makes RNNs extremely powerful and expressive, applying and understanding complexity reduction is more challenging [16].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "However, it has recently been shown that there is redundancy in the LSTM structure, which has led to new architectures, such as the GRU [10].", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "There has also been renewed interest in RNNs owing to more powerful optimization algorithms, such as Hessian-free optimization [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "It has been suggested that the core attribute of the LSTM is the memory cell architecture, and that comparable performance is obtained with fewer [18], and possibly even a single, flow control gate, as for the MGRU [8].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "It has been suggested that the core attribute of the LSTM is the memory cell architecture, and that comparable performance is obtained with fewer [18], and possibly even a single, flow control gate, as for the MGRU [8].", "startOffset": 215, "endOffset": 218}, {"referenceID": 18, "context": "As a result, they are especially sensitive to corruptions and perturbations, which is why performance suffers when Dropout is applied to the recurrent connections [19].", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "Consider a standard RNN with tanh activation nonlinearity, which is biased near zero and has small activations, perhaps through a sparsity activation penalty [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "To understand the effect of a small perturbation on shortterm memory performance, consider the noiseless memorization experiment described by Martens and Sutskever [17] and shown in Fig.", "startOffset": 164, "endOffset": 168}, {"referenceID": 20, "context": "Nevertheless, there is a tradeoff between the desire to set \uf028 \uf029 1 r W \uf072 \uf03e to encourage short-term memory and to reduce the amplification of error due to a perturbation [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "The particular examples of image caption generation and object detection in crowded scenes illustrate this point [14], [15], since the output sequence is highly dependent on the input data found in the field.", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "The particular examples of image caption generation and object detection in crowded scenes illustrate this point [14], [15], since the output sequence is highly dependent on the input data found in the field.", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "Language Model In the first experiment, we train a recurrent language model to predict the next word in a sequence by minimizing the crossentropy error over the full vocabulary, as in [19].", "startOffset": 184, "endOffset": 188}, {"referenceID": 21, "context": "All models and experiments consist of a single recurrent layer and are trained using the Adam optimizer [23] with Dropout applied to the hidden layer outputs.", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "The unfolded RNN for the noiseless memorization experiment as described in [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Moreover, we have observed that practical models (see [19]) have even greater redundancy, and tend to be highly over-parameterized in comparison to this simplified example.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "2 [17], with 100 recurrent hidden units.", "startOffset": 2, "endOffset": 6}], "year": 2016, "abstractText": "The significant computational costs of deploying neural networks in large-scale or resource constrained environments, such as data centers and mobile devices, has spurred interest in model compression, which can achieve a reduction in both arithmetic operations and storage memory. Several techniques have been proposed for reducing or compressing the parameters for feed-forward and convolutional neural networks, but less is understood about the effect of parameter compression on recurrent neural networks (RNN). In particular, the extent to which the recurrent parameters can be compressed and the impact on short-term memory performance, is not well understood. In this paper, we study the effect of complexity reduction, through singular value decomposition rank reduction, on RNN and minimal gated recurrent unit (MGRU) networks for several tasks. We show that considerable rank reduction is possible when compressing recurrent weights, even without fine tuning. Furthermore, we propose a perturbation model for the effect of general perturbations, such as a compression, on the recurrent parameters of RNNs. The model is tested against a noiseless memorization experiment that elucidates the short-term memory performance. In this way, we demonstrate that the effect of compression of recurrent parameters is dependent on the degree of temporal coherence present in the data and task. This work can guide on-the-fly RNN compression for novel environments or tasks, and provides insight for applying RNN compression in low-power devices, such as hearing aids. Keywords\u2014compression; recurrent neural networks; complexity reduction; SVD; RNN; MGRU; MRU", "creator": null}}}