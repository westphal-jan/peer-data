{"id": "1612.00944", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "Using Discourse Signals for Robust Instructor Intervention Prediction", "abstract": "^ we firmly tackle the prediction of traditional instructor text intervention in student posts stemming from discussion forums in countless massive open inquiry online lectures courses ( notably moocs ). our key finding is that implementation using traditional automatically obtained discourse relations values improves against the prediction outcome of when instructors intervene easily in student edited discussions, likewise when randomly compared with a truly state - of - only the - art, feature - defined rich baseline. our supervised classifier currently makes good use of an automatic shared discourse parser which outputs penn discourse treebank ( pdtb ) parameter tags vectors that then represent in - post scripted discourse features. we show pdtb relation - property based features increase the robustness complexity of combining the classifier and complement your baseline features in recalling more visibly diverse instructor intervention patterns. in 2017 comprehensive experiments over 14 mooc tested offerings from within several discourse disciplines, improving the corresponding pdtb discourse features visibly improve school performance patterns on average. the resultant specification models there are historically less likely dependent on domain - specific assessment vocabulary, allowing onto them teachers to better dramatically generalize to new courses.", "histories": [["v1", "Sat, 3 Dec 2016 09:08:51 GMT  (215kb)", "http://arxiv.org/abs/1612.00944v1", "To appear in proceedings of the 31st AAAI Conference on Artificial Intelligence, San Francisco, USA"]], "COMMENTS": "To appear in proceedings of the 31st AAAI Conference on Artificial Intelligence, San Francisco, USA", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CY", "authors": ["muthu kumar chandrasekaran", "carrie demmans epp", "min-yen kan", "diane j litman"], "accepted": true, "id": "1612.00944"}, "pdf": {"name": "1612.00944.pdf", "metadata": {"source": "META", "title": "Using Discourse Signals for Robust Instructor Intervention Prediction", "authors": ["Muthu Kumar Chandrasekaran", "Carrie Demmans Epp", "Min-Yen Kan", "Diane Litman"], "emails": ["kanmy}@comp.nus.edu.sg", "cdemmans@pitt.edu", "dlitman@pitt.edu"], "sections": [{"heading": "Introduction", "text": "Massive Open Online Courses (MOOCs) aim to scale learning by creating virtual classrooms that eliminate the need for students to be co-located with instructional staff and each other. To facilitate interaction, MOOC platforms have discussion forums where students can interact with instructional staff \u2013 hereafter called instructors \u2013 and their classmates. Forums are typically the only mode of interaction between instructors and students. Forums often contain hundreds of posts from several thousand students, each post competing with others for instructor attention. Reading and responding to student queries in forums is an essential teaching activity that helps instructors gauge student understanding of course content. Intervention is argued to facilitate student learning where an instructor\u2019s presence and intervention in student discussions improves learning outcomes in MOOCs (Chen et al. 2016) and other online learning environments (Garrison, Anderson, and Archer 1999; Phirangee, Demmans Epp, and Hewitt 2016). However, instructors need to be selective when answering student posts due to their limited bandwidth. One selection strategy is to respond to posts that will maximally benefit the most students in a course. Along these lines, Chandrasekaran et al.\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n(2015b) proposed an intervention taxonomy based on transactive discourse that details the situations in which certain types of interventions would maximally benefit students.\nConsistent with this taxonomy, Chandrasekaran et al. showed that intervention strategies in MOOC forums differ widely. The factors behind different instructor intervention strategies include the instructors\u2019 pedagogical philosophy, a desire to encourage learner interaction, a desire to ensure students understand course content, and a need to correct misconceptions (Phirangee 2016). The intervention strategy chosen was found to impact student learning significantly (Mazzolini and Maddison 2003; 2007).\nEarlier work also shows that content-based features, which include simple linguistic features derived from student vocabulary (e.g., word unigrams), signal common traits that are useful for predicting instructor interventions (Chandrasekaran et al. 2015a; Chaturvedi, Goldwasser,\nand Daume\u0301 III 2014). However, a key problem with surfacelevel vocabulary features is that they vary widely across courses, as courses from different subject areas use different domain-specific vocabulary. Predictive models trained on such word-based features do not generalize well when applied to new unseen courses in different disciplines. In contrast, function words such as conjunctions (e.g., \u201cand\u201d, \u201cbecause\u201d) occur frequently across corpora and can be leveraged to create more robust features, such as what was done in the related task of predicting transactivity in educational dialogues (Joshi and Rose\u0301 2007).\nOur work focuses on a specific subclass of function words \u2014 discourse connectives \u2014 as they serve to connect clauses and signal the communicative intent of the writer. The Penn Discourse Treebank (PDTB; Prasad et al. 2008) formalism identifies connectives that signal discourse relations and categorises them into senses. As can be seen in Figure 1, both student posts contain a number of if...then, but connectives that belong to the contingency and comparison senses of the PDTB. It is common to find such patterns in student posts expressing confusion as they hedge and hypothesise to check their understanding, which can call for instructor intervention. In contrast, the student post in Figure 3 is confident in tone, uses the imperative form, and is devoid of such connectives. These examples motivate us further to extract discourse-based features from student discussion forum posts. We hypothesize that student posts differ in their discourse structures and that some of these structures will attract instructor intervention. We additionally hypothesize discourse features will yield models for predicting instructor intervention that generalize well to unseen courses.\nFollowing prior work, we cast the problem of predicting instructor intervention as a binary classification problem where intervened and non-intervened threads are treated as positive and negative instances, respectively. We test our hypotheses extrinsically using automatically extracted discourse features that follow the PDTB formalism, enriching a state-of-the-art baseline model for predicting instructor intervention. In contrast to prior work on single MOOC instances, our experiments are comprehensive, covering a corpus of 14 MOOC instances from various disciplines, offered by two different universities.\nOur results show that PDTB features improve the state-ofthe-art baseline performance by 3.4% (Table 4) when trained on a large out-of-domain dataset and by 0.4% (Table 3) when trained on a smaller in-domain dataset. Further, PDTB features on their own perform comparably to the state-of-the-art on select MOOC offerings. We show that unlike vocabulary based features, PDTB features are robust to domain differences across MOOCs."}, {"heading": "Related Work", "text": "Predicting instructor intervention became a viable problem to study with the availability of large amounts of educational discussion forum data from MOOCs. Chaturvedi et al. (2014) first specified the problem as predicting which MOOC discussion forum threads instructors would post to, where an instructor post is considered an intervention.\nThey modelled macro-level thread discourse structure (i.e., across posts), demonstrating that their model outperformed a representative classifier endowed with many lexical and other surface level features. However, later work failed to replicate their results across a much broader study of MOOC forums culled from several universities (Chandrasekaran et al. 2015a). This work cited the large variety of course content and instructor preference as likely causes to the non-portability of the initial study\u2019s results. Chandrasekaran et al. also showed that other simple features such as sub-forum type, thread length and surface level linguistic cues outperform the discourse model from the earlier work.\nOur work uses Chandrasekaran et al. (2015a), hereafter denoted as EDM\u201915, as a starting point and as a state-of-theart baseline for comparison. In contrast to both prior works, we model microscopic discourse structures \u2013 i.e., sentence and clause-level discourse within student posts. We also eschew vocabulary-dependent approaches, such as those suggested by Ramesh et al. (2015) where intervention was based on emergent topics and subtopics from each course, since we seek models that generalize across a wide variety of courses.\nDiscourse Parsing Applications. As forum discussions feature dialogue and argumentation, we felt strongly that providing discourse analyses would improve prediction performance. Automatic discourse parsing discovers the relationship between clauses or sentences in contiguous text. Discourse parsing usually categorizes the inferred relation with a discourse type.\nWith the availability of large-scale discourse annotations on top of the Penn Treebank, the PDTB formalism for discourse annotation has become a de facto standard for automated discourse parsing and analyses. Importantly, the PDTB formalism splits the detection of discourse relations into ones signaled explicitly by a discourse connective (e.g., the connective \u201cif\u201d often signals a Contingency relation between its arguments, as in the first connective from Figure 1) from implicitly signaled ones that have no overt connective. As a result, automatic discourse relation identification relying solely on explicit connectives is rather precise but provides low overall coverage.\nWhile the PDTB annotated corpus is built largely on newswire (e.g., Wall Street Journal), the PDTB tag set and derived parsers have found applicability in a variety of NLP tasks on different corpora: Li et al. (2014) showed the influence of PDTB explicit relations on machine translation quality. Miha\u0306ila\u0306 and Ananiadou (2014) found causal relations using PDTB in scientific Biomedical journals. Importantly, PDTB\u2019s applicability to the related form of usergenerated text, especially expository texts, has also been studied: Faulkner et al. (2014) used PDTB discourse features to support argument classification in student essays from the International Corpus of Learner English1. Also similarly, in performing a selection problem close to ours, Wang et al. (2012) studied discourse parsing\u2019s utility for retweetability of tweets and found correlations between the discourse type and sentiment polarity. Swanson, Ecker, and\n1https://www.uclouvain.be/en-cecl-icle. html\nWalker (2015) also found these relations to be useful in argument extraction from general web forum text.\nMOOC discussion forum text is user-generated, expository and conversational all at once. For this reason, we hypothesize that (explicitly marked) discourse parsing would improve the prediction of instructor intervention. Our hypothesis extends to forums in any online learning environment, such as the learning management systems that schools and universities host for their students."}, {"heading": "Data and Preprocessing", "text": "The corpus for our experiments consists of data from 14 Coursera MOOC offerings2 that are spread across 7 courses from the authors\u2019 universities. The included MOOCs taught a variety of subjects spanning the humanities and sciences. All courses relied on videos to deliver core content. Different instructional approaches and learning activities (e.g., peer/self-assessments, prompted discussions, tests, or papers) were used and influenced discussion forum activities. This variety is apparent through the number of instructional staff (i.e., instructor or teaching assistants) who posted in the forums. The varied approaches are also apparent through their intervention ratios where CLINICAL-13 (Row 6; Table 1) had the highest intervention ratio (0.73) and DISASTER-3 (Row 10 of Table 1) had the lowest (0.02).\nCoursera forums are divided into several sub-forums. Each of the sub-forums was manually categorized, using the definitions from (Chandrasekaran et al. 2015a), as belonging to one of the following types: errata, exam, lecture, homework, general, peer review, study group, or technical issues. Similar to prior work on intervention prediction, the general, study group, peer review and technical issues subforums and their threads were removed since they are noisy and do not focus on course content (e.g., social discussions and reports of technical issues). As the task is instructor intervention prediction, we also omit threads where the first post was made by an instructor. Table 1 shows the number of threads that are used in our model.\nWe truncate threads after the first instructor post (dropping subsequent student posts) because predicting the first instructor intervention is a viable problem and distinct from predicting subsequent, follow-up interventions that can be motivated by different reasons. Further, after an intervention, discussions gain visibility which can inflate feature counts in our prediction task. To extract features, we first tokenize thread text. We replaced instances of non-lexical references such as equations, URLs and timestamps, with the tokens: <EQU>, <URL>, <TIMEREF>, respectively. These tokens are a feature of the baseline prediction system (see \u201cBaseline (EDM\u201915)\u201d section). They also enable the discourse parser to skip unparsable text4. Stopwords and\n2As of September 2016, Coursera, a commercial MOOC platform: https://www.coursera.org, hosted 1157 courses in English spanning the humanities, social sciences, engineering, and sciences.\n3\u201c-n\u201d refers to the nth time the course was offered; \u201cCLINICAL-1\u201d stands for the first offering of the Clinical course.\n4The discourse parser extracts syntactic and dependency parse\nwords of length less than 3 were removed before extracting the baseline features. Stopwords were not removed when extracting discourse features (cf \u201cDiscourse Feature Extraction\u201d Section). Our work examines three predictive models: 1) the baseline (EDM\u201915), 2) a system with only PDTB discourse relations as features (PDTB), and 3) an augmented system where discourse relations are also used (EDM\u201915 + PDTB; E+P for short)."}, {"heading": "Baseline (EDM\u201915)", "text": "The baseline system uses a maximum entropy classifier with the following set of features: unigrams, thread forum type, student affirmations to a previous post, thread properties (# of posts, comments, and posts+comments), average # of comments per post, # of sentences in the thread, # of URLs, and # of timestamped references to parts of a lecture video. The authors noted the imbalanced nature of their datasets, with non-intervened threads greatly outnumbering intervened ones. This motivated the use of class weights to counterbalance the # of non-intervened instances. Class weights, an important parameter of this model, were estimated as the ratio of negative to positive samples in the training instances."}, {"heading": "Discourse Feature Extraction", "text": "We experimented with a prediction system based solely on automatically-acquired discourse features from the PDTBbased discourse parser from (Lin, Ng, and Kan 2014). We employ this shallow discourse parser on the input to categorize identified discourse connectives according to the PDTB\nfeatures from the Stanford parser, which fails on these types of non-lexical strings; available at http://nlp.stanford. edu/software.\ntag set, and subsequently extract them for our use. The parser first distinguishes discourse connectives (e.g., \u201cand\u201d can signal a discourse relation of \u2018Expansion\u2019, but can also act as a coordinating conjunction). It then classifies them into one of several senses as specified by PDTB. PDTB categorizes the connectives into implicit and explicit connectives, each of which is assigned a sense. Senses are organized hierarchically, where the top Level\u20131 senses discriminate among 4 relations: \u2018Contingency\u2019, \u2018Expansion\u2019, \u2018Comparison\u2019 and \u2018Temporal\u2019. We only used explicit connectives and Level\u20131 senses as features. This is because (Lin, Ng, and Kan 2014) report a low F1 of 39.6% for extracting implicit connectives while that of explicit connectives is much better (86.7%). Limiting to Level\u20131 senses also avoids sparsity issues. We found the distribution of the 4 Level\u20131 senses as tagged by the discourse parser in our MOOC corpus was similar to that of the original PDTB annotated corpus built from newswire (see Table 2), supporting our decision to use Level\u20131 senses.\nWe used the Java version of the parser5, which comes pretrained with Sections 2\u201321 of the PDTB annotated corpus, using the Level\u20131 relation senses. We note that although the discourse parser\u2019s performance on MOOC forum text had not been previously evaluated, we decided to use the pretrained parser given that such PDTB discourse parsers have been used to support a variety of downstream tasks using different corpora, without retraining (cf Related Work). Additionally, re-training is a resource intensive task, and we judged it to be a lower priority to evaluate it specifically for MOOC data.\nEach forum thread is treated as a document, where each post in the thread is treated as a paragraph. Since the parser identifies discourse relations within paragraphs of text, only within-post discourse relations were identified; this is appropriate as the parser was trained on single-party narrative (newswire) rather than multiparty dialogue. We derive 25 features from the PDTB relation senses output by the parser. These constitute the discourse features identified as PDTB in Tables 3, 4 and 5:\n\u2022 Total number of all relation senses (1 feature): The sum of the frequencies (number of occurrences) of all four Level\u2013 1 senses;\n\u2022 Proportion of each sense (8 features): The absolute and relative frequency of each sense in a thread. Absolute frequency is normalized by thread length;\n\u2022 Proportion of sense sequences of length 2 (16 features): 5Available at https://github.com/WING-NUS/ pdtb-parser\nNormalized number of occurrences of each sense sequence of length 2 (e.g., \u2018Expansion\u2019-\u2018Contingency\u2019) in a thread divided by the total number of occurrences of all sense sequences of length 2.\nWe use the maximum entropy classifier with class weights as in the EDM \u201915 baseline for both PDTB and E+P systems. The implementations of the EDM \u201915 and the discourse based systems are publicly available6."}, {"heading": "Evaluation", "text": "We evaluate the models under two evaluation schemes: an (i) in-domain scheme, and an (ii) out-of-domain scheme. We report the performance of the models in terms of precision (P), recall (R) and F1 of the positive class.\nThe in-domain setting models were trained and evaluated separately on each MOOC using stratified five-fold cross validation. Stratification accounts for the highly imbalanced data and ensures that each fold had both positive and negative samples. We see that the combined model E+P outperforms the baseline EDM\u201915 (Table 3), on average.\nDrilling down, we see that while EDM\u201915 does well on the first offering (those with the \u201c-1\u201d suffix) of many courses which have higher intervention ratios, E+P outperforms EDM\u201915 on subsequent offerings which typically have lower intervention ratios. We also observe the F1 scores for CLASSIC-2 and DISASTER-3 are 0. Despite stratification, the # of intervened threads per fold was too low for both courses (\u223c3 to 4 per fold) due to their low intervention ratios (see Table 1). As a result, both models are unable to predict any intervention for either course. In the out-of-domain setting, we use leave-one-out cross-coursevalidation (LOO-CCV) where models trained on 13 courses are tested on the 14th unseen course. This evaluation setting more closely approximates the real world where universities hosting MOOCs have data from previously offered MOOCs and would want to train predictive models that could be deployed in upcoming courses. This evaluation shows which models are more robust when adapting to unseen out-ofdomain data. Table 4 shows the performance of the EDM\u201915 and E+P models on each of the 14 MOOCs from LOO\u2013CCV.\nE+P betters EDM\u201915 performance by 3.4% on average. The improved F1 is largely due to a 5.7% improvement in recall. We argue that, for the problem of intervention prediction, improving recall is more important than precision since missing an intervention is costlier than intervening on a less important thread. Here the performance outlier is the CLINICAL-1 MOOC, where EDM\u201915 performs significantly better than E+P, which may be partially attributed to the course having the smallest test set.\nFurther, Tables 3 and 4 show that the E+P and EDM\u201915 models both benefit from access to more data in the outof-domain setting. The EDM\u201915 model only improved by 2.7%, whereas the E+P model improved by 5.7%, showing the benefits of using domain-independent linguistic features to predict instructor intervention.\n6https://github.com/WING-NUS/lib4moocdata"}, {"heading": "Discussion", "text": "To understand the observed performance of the PDTBbased features, we probe further, answering two research questions that are natural extensions of the results.\nRQ1. Are the PDTB features useful supplemental evidence, especially when simple features do not perform well?\nIn each of the 5 courses where E+P performs better than EDM\u201915, the course iterations have smaller intervention ratios (see Tables 1 and 3). For example, E+P betters EDM\u201915 on CLINICAL-2, REASON-2 and DISASTER-1 while EDM\u201915 has a better score on CLINICAL-1, REASON1 and DISASTER-2. That is, PDTB features boost EDM\u201915 performance when there are fewer positive instances to learn from. This could be due to EDM\u201915\u2019s much larger feature space that requires more data to prevent sparsity. Note EDM\u201915 excluded stopwords, a subset of which are PDTB connectives, meaning that PDTB features contribute different information to the signal in the E+P model. Our analysis of the contributions of features showed \u2018Contingency\u2019 and \u2018Expansion\u2019 relations to contribute the most. This may be due to their higher prevalence relative to the other discourse relations in the corpus.\nConsider the example in Figure 2. E+P classifies this thread correctly while the EDM\u201915 model fails. This short thread does not contain many content words. In contrast, the discourse connectives in these student posts activate 16 of the 25 PDTB features.\nWhile the use of explicit discourse connectives helped the classification task in many cases (e.g., Figure 2 and 4), the PDTB parser does not cover all of the observed discourse connectives or their expressed senses. Examples of connectives (in bold) that were not recognized include:\n\u2013 ...not as nice as I thought it would be... \u2013 There\u2019s only so much melodic expressiveness...\nwhen using nothing but chord tones...\nThe presence of these connectives in our data is consistent with recent calls (Forbes-Riley, Zhang, and Litman 2016) to modify the PDTB relation inventory by adding a broader set of tags, such as those suggested by Tonelli et al. (2010). Increasing the PDTB\u2019s coverage of explicit connectives would likely improve results. The added use of implicit connectives may also improve prediction performance, should implicit connective detection and classification be improved. Figure 4 shows an example where implicit connectives may strengthen signals from discourse features. We also note that there are cases, such as in Figure 3, which lack discourse and lexical signals. The intervention here is instead triggered by domain knowledge. These excerpts exemplify the difficulty of our prediction task.\nRQ2. Are PDTB features more robust than vocabularybased features?\nConsider the performance differences of the EDM\u201915 and PDTB models between the in-domain and out-of-domain\nevaluation settings (see Table 5). Using PDTB features results in an average improvement of 11.6% when evaluating models out-of-domain, whereas EDM\u201915 only improves by 2.7%. EDM\u201915 performance drops greatly on CLASSIC-1, REASON-1, and ACCTALK due to the out-of-domain data; in contrast, PDTB gains on 10 of the 14 courses. In the example in Figure 4, EDM\u201915 and E+P fail while PDTB predicts correctly. Frequent words from this course (e.g., \u201ctornado\u201d, \u201cearthquake\u201d) are rare across MOOCs; this weakens the EDM\u201915 model.\nThese findings suggest that PDTB may result in further gains were data added from more courses, while that of the vocabulary-based EDM\u201915 model may worsen or not scale. Similarly, Chandrasekaran et al. (2015a) did not see improvements in the EDM\u201915 model when they went from a 13 course training set to 60 courses. This demonstrated lack of robustness in the EDM\u201915 model is not surprising given its abundant use of domain-specific vocabulary. There were considerably more out-of-domain unigram features (76,382) than in-domain unigram features (15,161, on average). This steep increase in feature space and resulting sparsity hampers the EDM\u201915 model\u2019s ability to benefit from a scaled corpus. In contrast, both in-domain and out-of-domain versions of the PDTB model have the same number of features.\nPotential Improvements. The above results indicate that performance improvements due to PDTB features scale better than vocabulary-based features. To harness similar improvements from large scale data, the performance and ro-\nbustness of the discourse parser needs to be improved. Current limitations of discourse parsers (e.g., their inability to process equations and symbols, lack of near real-time output) inhibit scaling the predictive power of PDTB and E+P models. Discourse parser improvements would therefore enable the development and use of better predictive models."}, {"heading": "Conclusion", "text": "In this study, we better the state-of-the-art for intervention prediction by augmenting it with PDTB relation based features. Further, on select MOOC offerings PDTB relations alone performed comparably to the state-of-the-art. Unlike vocabulary based models, PDTB based features were shown to be robust to domain differences across MOOCs. This domain independence supports the improved prediction of instructor interventions. The current F1 scores are still markedly low. Better modeling of the instructor may help boost performance. We plan to tackle this in two ways. First, we will model instructor intervention based on the threads they have seen because they cannot intervene in threads that they have not seen. Second, we will model intervention based on the role that different types of instructional staff play. We expect teaching assistants, course alumni (also known as \u201ccommunity TAs\u201d and \u201cmentors\u201d), and faculty to have different motivations for intervening. They may also dedicate different amounts of time to course forums. As a result, modelling these factors or individual instructor preferences could improve prediction performance."}, {"heading": "Acknowledgments", "text": "This research is funded in part by NUS Learning Innovation Fund \u2013 Technology grant #C-252-000-123-001, and by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office, and by an NUS Shaw Visiting Professor Award. The research is also funded in part by the Learning Research and Development Center at the University of Pittsburgh and by a Google Faculty Research Award. We would like to thank the University of Pittsburgh\u2019s Center for Teaching and Learning, for sharing their MOOC data."}], "references": [{"title": "Learning instructor intervention from mooc forums: Early results and issues", "author": ["M.K. Chandrasekaran", "M.-Y. Kan", "B.C. Tan", "K. Ragupathi"], "venue": "Proc. of EDM, Madrid, Spain, 218\u2013225. IEDM.", "citeRegEx": "Chandrasekaran et al\\.,? 2015a", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2015}, {"title": "Towards feasible instructor intervention in mooc discussion forums", "author": ["M.K. Chandrasekaran", "K. Ragupathi", "M.-Y. Kan", "B.C. Tan"], "venue": "Proc. of ICIS, Fort Worth, USA.", "citeRegEx": "Chandrasekaran et al\\.,? 2015b", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2015}, {"title": "Predicting instructor\u2019s intervention in mooc forums", "author": ["S. Chaturvedi", "D. Goldwasser", "H. Daum\u00e9 III"], "venue": "Proc. of the ACL (1), 1501\u20131511. ACL.", "citeRegEx": "Chaturvedi et al\\.,? 2014", "shortCiteRegEx": "Chaturvedi et al\\.", "year": 2014}, {"title": "What kinds of forum activities are important for promoting learning continuance in MOOCS? In Proc", "author": ["H. Chen", "C.W. Phang", "C. Zhang", "S. Cai"], "venue": "of Pacific Asia Conference on Information Systems.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Automated classification of argument stance in student essays: A linguistically motivated approach with an application for supporting argument summarization", "author": ["A.R. Faulkner"], "venue": "Ph.D. Dissertation, CUNY Academic Works.", "citeRegEx": "Faulkner,? 2014", "shortCiteRegEx": "Faulkner", "year": 2014}, {"title": "Extracting PDTB Discourse Relations from Student Essays", "author": ["K. Forbes-Riley", "F. Zhang", "D.J. Litman"], "venue": "Proc. of the SIGDIAL, 117\u2013127.", "citeRegEx": "Forbes.Riley et al\\.,? 2016", "shortCiteRegEx": "Forbes.Riley et al\\.", "year": 2016}, {"title": "Critical Inquiry in a Text-Based Environment: Computer Conferencing in Higher Education", "author": ["D. Garrison", "T. Anderson", "W. Archer"], "venue": "The Internet and Higher Education 2(2-3):87\u2013105.", "citeRegEx": "Garrison et al\\.,? 1999", "shortCiteRegEx": "Garrison et al\\.", "year": 1999}, {"title": "Using transactivity in conversation for summarization of educational dialogue", "author": ["M. Joshi", "C.P. Ros\u00e9"], "venue": "Proc. of SLaTE, 53\u201356.", "citeRegEx": "Joshi and Ros\u00e9,? 2007", "shortCiteRegEx": "Joshi and Ros\u00e9", "year": 2007}, {"title": "Assessing the Discourse Factors that Influence the Quality of Machine Translation", "author": ["J.J. Li", "M. Carpuat", "A. Nenkova"], "venue": "Proc. of the ACL (2), 283\u2013288. ACL.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A PDTB-styled end-to-end discourse parser", "author": ["Z. Lin", "H.T. Ng", "M.-Y. Kan"], "venue": "Natural Language Engineering 20(02):151\u2013184.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Sage, guide or ghost? The effect of instructor intervention on student participation in online discussion forums", "author": ["M. Mazzolini", "S. Maddison"], "venue": "Computers & Education 40(3):237\u2013253.", "citeRegEx": "Mazzolini and Maddison,? 2003", "shortCiteRegEx": "Mazzolini and Maddison", "year": 2003}, {"title": "When to jump in: The role of the instructor in online discussion forums", "author": ["M. Mazzolini", "S. Maddison"], "venue": "Computers & Education 49(2):193\u2013213.", "citeRegEx": "Mazzolini and Maddison,? 2007", "shortCiteRegEx": "Mazzolini and Maddison", "year": 2007}, {"title": "Semi-supervised learning of causal relations in biomedical scientific discourse", "author": ["C. Mih\u0103il\u0103", "S. Ananiadou"], "venue": "Biomedical engineering online 13(2):1.", "citeRegEx": "Mih\u0103il\u0103 and Ananiadou,? 2014", "shortCiteRegEx": "Mih\u0103il\u0103 and Ananiadou", "year": 2014}, {"title": "Exploring the relationships between facilitation methods, students\u2019 sense of community and their online behaviours", "author": ["K. Phirangee", "C. Demmans Epp", "J. Hewitt"], "venue": "Special Issue on Online Learning Analytics. Online Learning Journal 20(2):134\u2013154.", "citeRegEx": "Phirangee et al\\.,? 2016", "shortCiteRegEx": "Phirangee et al\\.", "year": 2016}, {"title": "Exploring the Role of Community in Online Learning", "author": ["K. Phirangee"], "venue": "PhD, University of Toronto, Toronto, ON, Canada.", "citeRegEx": "Phirangee,? 2016", "shortCiteRegEx": "Phirangee", "year": 2016}, {"title": "The Penn Discourse TreeBank 2.0", "author": ["R. Prasad", "N. Dinesh", "A. Lee", "E. Miltsakaki", "L. Robaldo", "A.K. Joshi", "B.L. Webber"], "venue": "In Proc. of the LREC. ELRA", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Weakly supervised models of aspect-sentiment for online course discussion forums", "author": ["A. Ramesh", "S.H. Kumar", "J. Foulds", "L. Getoor"], "venue": "Proc. of the ACL (1), 74\u201383. ACL.", "citeRegEx": "Ramesh et al\\.,? 2015", "shortCiteRegEx": "Ramesh et al\\.", "year": 2015}, {"title": "Argument mining: Extracting arguments from online dialogue", "author": ["R. Swanson", "B. Ecker", "M. Walker"], "venue": "Proc. the SIGDIAL, 217\u2013227. ACL.", "citeRegEx": "Swanson et al\\.,? 2015", "shortCiteRegEx": "Swanson et al\\.", "year": 2015}, {"title": "Annotation of discourse relations for conversational spoken dialogs", "author": ["S. Tonelli", "G. Riccardi", "R. Prasad", "A.K. Joshi"], "venue": "Proc. of the LREC. ELRA.", "citeRegEx": "Tonelli et al\\.,? 2010", "shortCiteRegEx": "Tonelli et al\\.", "year": 2010}, {"title": "Re-tweeting from a linguistic perspective", "author": ["A. Wang", "T. Chen", "M.-Y. Kan"], "venue": "Proc. of the second workshop on language in social media, 46\u201355. ACL.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Intervention is argued to facilitate student learning where an instructor\u2019s presence and intervention in student discussions improves learning outcomes in MOOCs (Chen et al. 2016) and other online learning environments (Garrison, Anderson, and Archer 1999; Phirangee, Demmans Epp, and Hewitt 2016).", "startOffset": 161, "endOffset": 179}, {"referenceID": 14, "context": "The factors behind different instructor intervention strategies include the instructors\u2019 pedagogical philosophy, a desire to encourage learner interaction, a desire to ensure students understand course content, and a need to correct misconceptions (Phirangee 2016).", "startOffset": 248, "endOffset": 264}, {"referenceID": 10, "context": "The intervention strategy chosen was found to impact student learning significantly (Mazzolini and Maddison 2003; 2007).", "startOffset": 84, "endOffset": 119}, {"referenceID": 7, "context": ", \u201cand\u201d, \u201cbecause\u201d) occur frequently across corpora and can be leveraged to create more robust features, such as what was done in the related task of predicting transactivity in educational dialogues (Joshi and Ros\u00e9 2007).", "startOffset": 200, "endOffset": 221}, {"referenceID": 15, "context": "The Penn Discourse Treebank (PDTB; Prasad et al. 2008) formalism identifies connectives that signal discourse relations and categorises them into senses.", "startOffset": 28, "endOffset": 54}, {"referenceID": 0, "context": "However, later work failed to replicate their results across a much broader study of MOOC forums culled from several universities (Chandrasekaran et al. 2015a).", "startOffset": 130, "endOffset": 159}, {"referenceID": 0, "context": "Chaturvedi et al. (2014) first specified the problem as predicting which MOOC discussion forum threads instructors would post to, where an instructor post is considered an intervention.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Our work uses Chandrasekaran et al. (2015a), hereafter denoted as EDM\u201915, as a starting point and as a state-of-theart baseline for comparison.", "startOffset": 14, "endOffset": 44}, {"referenceID": 0, "context": "Our work uses Chandrasekaran et al. (2015a), hereafter denoted as EDM\u201915, as a starting point and as a state-of-theart baseline for comparison. In contrast to both prior works, we model microscopic discourse structures \u2013 i.e., sentence and clause-level discourse within student posts. We also eschew vocabulary-dependent approaches, such as those suggested by Ramesh et al. (2015) where intervention was based on emergent topics and subtopics from each course, since we seek models that generalize across a wide variety of courses.", "startOffset": 14, "endOffset": 381}, {"referenceID": 7, "context": ", Wall Street Journal), the PDTB tag set and derived parsers have found applicability in a variety of NLP tasks on different corpora: Li et al. (2014) showed the influence of PDTB explicit relations on machine translation quality.", "startOffset": 134, "endOffset": 151}, {"referenceID": 7, "context": ", Wall Street Journal), the PDTB tag set and derived parsers have found applicability in a variety of NLP tasks on different corpora: Li et al. (2014) showed the influence of PDTB explicit relations on machine translation quality. Mih\u0103il\u0103 and Ananiadou (2014) found causal relations using PDTB in scientific Biomedical journals.", "startOffset": 134, "endOffset": 260}, {"referenceID": 4, "context": "Importantly, PDTB\u2019s applicability to the related form of usergenerated text, especially expository texts, has also been studied: Faulkner et al. (2014) used PDTB discourse features to support argument classification in student essays from the International Corpus of Learner English1.", "startOffset": 129, "endOffset": 152}, {"referenceID": 4, "context": "Importantly, PDTB\u2019s applicability to the related form of usergenerated text, especially expository texts, has also been studied: Faulkner et al. (2014) used PDTB discourse features to support argument classification in student essays from the International Corpus of Learner English1. Also similarly, in performing a selection problem close to ours, Wang et al. (2012) studied discourse parsing\u2019s utility for retweetability of tweets and found correlations between the discourse type and sentiment polarity.", "startOffset": 129, "endOffset": 369}, {"referenceID": 0, "context": "Each of the sub-forums was manually categorized, using the definitions from (Chandrasekaran et al. 2015a), as belonging to one of the following types: errata, exam, lecture, homework, general, peer review, study group, or technical issues.", "startOffset": 76, "endOffset": 105}, {"referenceID": 15, "context": "Table 2: Distribution of PDTB level-1 senses for explicit connectives (top row) as tagged by the discourse parser, which are similar to those reported for the PDTB corpus (Prasad et al. 2008), bottom row.", "startOffset": 171, "endOffset": 191}, {"referenceID": 8, "context": "The presence of these connectives in our data is consistent with recent calls (Forbes-Riley, Zhang, and Litman 2016) to modify the PDTB relation inventory by adding a broader set of tags, such as those suggested by Tonelli et al. (2010). Increasing the PDTB\u2019s coverage of explicit connectives would likely improve results.", "startOffset": 220, "endOffset": 237}, {"referenceID": 0, "context": "Similarly, Chandrasekaran et al. (2015a) did not see improvements in the EDM\u201915 model when they went from a 13 course training set to 60 courses.", "startOffset": 11, "endOffset": 41}], "year": 2016, "abstractText": "We tackle the prediction of instructor intervention in student posts from discussion forums in Massive Open Online Courses (MOOCs). Our key finding is that using automatically obtained discourse relations improves the prediction of when instructors intervene in student discussions, when compared with a state-of-the-art, feature-rich baseline. Our supervised classifier makes use of an automatic discourse parser which outputs Penn Discourse Treebank (PDTB) tags that represent in-post discourse features. We show PDTB relationbased features increase the robustness of the classifier and complement baseline features in recalling more diverse instructor intervention patterns. In comprehensive experiments over 14 MOOC offerings from several disciplines, the PDTB discourse features improve performance on average. The resultant models are less dependent on domain-specific vocabulary, allowing them to better generalize to new courses.", "creator": "TeX"}}}