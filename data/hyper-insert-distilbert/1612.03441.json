{"id": "1612.03441", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2016", "title": "Lock-Free Optimization for Non-Convex Problems", "abstract": "continuous stochastic convex gradient descent ~ ( sgd ) compression and its variants have all attracted unusually much growing attention informally in machine reinforcement learning due to ultimately their efficiency estimation and apparent effectiveness for optimization. to handle promising large - cluster scale sorting problems, analytical researchers have well recently proposed several lock - free strategy systems based near parallel sgd ~ ( via lf - mapped psgd ) linear methods for conducting multi - core systems. ) however, existing works have \" only proved the remarkable convergence of these these lf - smooth psgd expansion methods for convex problems. to advance the best standards of both our knowledge, whilst no later work has proved the correct convergence of employing the lf - psgd methods for non - convex computation problems. resulting in \u00bb this paper, \u2033 we provide the objective theoretical proof about testing the convergence of my two widely representative lf - psgd relaxation methods, hogwild! and asysvrg, respectively for non - convex approximation problems. empirical results also show conjecture that whenever both hogwild! and asysvrg are demonstrated convergent convergence on most non - static convex problems, both which helps successfully verifies our overall theoretical characterization results.", "histories": [["v1", "Sun, 11 Dec 2016 17:26:43 GMT  (326kb,D)", "http://arxiv.org/abs/1612.03441v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shen-yi zhao", "gong-duo zhang", "wu-jun li"], "accepted": true, "id": "1612.03441"}, "pdf": {"name": "1612.03441.pdf", "metadata": {"source": "META", "title": "Lock-Free Optimization for Non-Convex Problems", "authors": ["Shen-Yi Zhao", "Gong-Duo Zhang", "Wu-Jun Li"], "emails": ["zhanggd}@lamda.nju.edu.cn,", "liwujun@nju.edu.cn"], "sections": [{"heading": "Introduction", "text": "Many machine learning models can be formulated as the following optimization problem:\nmin w\n1\nn n\u2211 i=1 fi(w), (1)\nwhere w is the parameter to learn (optimize), n is the number of training instances, fi(w) is the loss defined on instance i. For example, assuming we are given a set of labeled instances {(xi, yi)|i = 1, 2, . . . , n}, where xi \u2208 Rd is the feature vector and yi \u2208 {1,\u22121} is the label of xi, fi(w) can be log(1 + e\u2212yix\nT i w) + \u03bb2 \u2016w\u2016 2 which is known as the regularized loss in logistic regression (LR). We can also take fi(w) to be max{0, 1\u2212 yixTi w}+ \u03bb2 \u2016w\u2016\n2 which is known as the regularized loss in support vector machine (SVM). Here, \u03bb is the regularization hyper-parameter. Moreover, many other machine learning models, including neural networks (Krizhevsky, Sutskever, and Hinton 2012), matrix factorization (Koren, Bell, and Volinsky 2009), and principal component analysis (PCA) (Shamir 2015) and so on, can also be formulated as that in (1).\nWhen the problem in (1) is large-scale, i.e., n is large, researchers have recently proposed stochastic gradient descent (SGD) and its variants like SVRG (Johnson and\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nZhang 2013) to solve it. Many works (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013) have found that SGD-based methods can achieve promising performance in large-scale learning problems. According to the implementation platforms or systems, existing SGD-based methods can be divided into three categories: sequential SGD (SSGD) methods, parallel SGD (PSGD) methods, and distributed SGD (DSGD) methods. SSGD methods are designed for a single thread on a single machine, PSGD methods are designed for multicore (multi-thread) on a single machine with a shared memory1, and DSGD methods are designed for multiple machines.\nWhen the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al. 2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance. Furthermore, good theoretical results about the convergence of the SGD methods are also provided by these existing works.\nIn many real applications, the problems to optimize can be non-convex. For example, the problems for the neural networks are typically non-convex. Because many researchers (Li et al. 2014; Xing et al. 2015) find that the SGD methods can also achieve good empirical results for nonconvex problems, theoretical proof about the convergence of SGD methods for non-convex problems has recently attracted much attention. Some progress has been achieved. For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems. There are also some other theoretical results for some particular non-convex problems, like PCA (Shamir 2015; 2016a; 2016b) and matrix factorization (Sa, Re, and Olukotun 2015). But all these works are only for SSGD methods.\nThere have appeared only two works (Lian et al. 2015; Huo and Huang 2016) which propose PSGD methods for non-convex problems with theoretical proof of convergence.\n1In some literatures, PSGD refers to the methods implemented on both multi-core and multi-machine systems. In this paper, PSGD only refers to the methods implemented on multi-core systems with a shared memory.\nar X\niv :1\n61 2.\n03 44\n1v 1\n[ st\nat .M\nL ]\n1 1\nD ec\n2 01\nHowever, the PSGD methods in (Lian et al. 2015) need write-lock or atomic operation for the memory to prove the convergence 2. Similarly, the work in (Huo and Huang 2016) also does not prove the convergence for the lockfree case in our paper. Recent works (Recht et al. 2011; Chaturapruek, Duchi, and Re\u0301 2015; J. Reddi et al. 2015; Zhao and Li 2016) find that lock-free strategy based parallel SGD (LF-PSGD) methods can empirically outperform lockbased PSGD methods for multi-core systems. Although some existing works (Chaturapruek, Duchi, and Re\u0301 2015; Zhao and Li 2016) have proved the convergence of these LFPSGD methods for convex problems, no work has proved the convergence of the LF-PSGD methods for non-convex problems.\nIn this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! (Recht et al. 2011; Chaturapruek, Duchi, and Re\u0301 2015) and AsySVRG (Zhao and Li 2016), for non-convex problems. The contribution of this work can be outlined as follows:\n\u2022 Theoretical results show that both Hogwild! and AsySVRG can converge with lock-free strategy for nonconvex problems.\n\u2022 Hogwild! gets a convergence rate of O(1/ \u221a T\u0303 ) for non-\nconvex problems, where T\u0303 = p \u00d7 T is the total iteration number of p threads.\n\u2022 AsySVRG gets a convergence rate of O(1/T\u0303 ) for nonconvex problems.\n\u2022 To get an -local optimal solution for AsySVRG, the computation complexity by all threads is O(n 2 3 / ), or\nequivalently the computation complexity of each thread is O(n 2 3\np ). This is faster than traditional parallel gradient decent methods whose computation complexity is O( np ) for each thread.\n\u2022 Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results."}, {"heading": "Preliminary", "text": "We use f(w) to denote the objective function in (1), which means f(w) = 1n \u2211n i=1 fi(w). And we use \u2016 \u00b7 \u2016 to denote the L2-norm \u2016 \u00b7 \u20162. Assumption 1. The function fi(\u00b7) in (1) is smooth, which means that there exists a constant L > 0, \u2200a,b,\nfi(b) \u2264 fi(a) +\u2207fi(a)T (b\u2212 a) + L\n2 \u2016b\u2212 a\u20162,\nor equivalently\n\u2016\u2207fi(b)\u2212\u2207fi(a)\u2016 \u2264 L\u2016b\u2212 a\u2016.\n2Although the implementation of AsySG-incon in (Lian et al. 2015) is lock-free, the theoretical analysis about the convergence of AsySG-incon is based on an assumption that no over-writing happens, i.e., the theoretical analysis is not for the lock-free case.\nThis is a common assumption for the convergence analysis of most existing gradient-based methods.\nSince we focus on non-convex problems in this paper, it is difficult to get the global solution of (1) based on the gradient methods. Hence, we use \u2016\u2207f(w)\u20162 to measure the convergence instead of f(w)\u2212min\nw f(w).\nHere, we give a Lemma which is useful in the convergence analysis of Hogwild! and AsySVRG.\nLemma 1. Assume B is a positive semi-definite matrix with the largest eigenvalue less than or equal to 1 and the minimum eigenvalue \u03b1 > 0, we have: \u2200x,y,\n\u2212\u2207f(x)TB\u2207f(y) \u2264 L 2 2 \u2016x\u2212 y\u20162 \u2212 \u03b1 2 \u2016\u2207f(x)\u20162 .\nProof.\n\u03b1 2 \u2016\u2207f(x)\u20162 \u2212\u2207f(x)TB\u2207f(y)\n\u22641 2 \u2225\u2225\u2225B 12\u2207f(x)\u2225\u2225\u22252 \u2212\u2207f(x)TB\u2207f(y) \u22641 2 \u2225\u2225\u2225B 12\u2207f(x)\u2225\u2225\u22252 \u2212\u2207f(x)TB\u2207f(y) + 1 2\n\u2225\u2225\u2225B 12\u2207f(y)\u2225\u2225\u22252 = 1\n2 \u2225\u2225\u2225B 12 (\u2207f(x)\u2212\u2207f(y))\u2225\u2225\u22252 \u2264L 2\n2 \u2016x\u2212 y\u20162 ."}, {"heading": "Hogwild! for Non-Convex Problems", "text": "The Hogwild! method (Recht et al. 2011) is listed in Algorithm 1. Each thread reads w from the shared memory, computes a stochastic gradient and updates the w in the shared memory. Please note that Hogwild! in (Recht et al. 2011) has several variants with locks or lock-free. Here, we only focus on the lock-free variant of Hogwild!, which means that we do not use any locks, either read-lock or write-lock, for all threads.\nAlgorithm 1 Hogwild! Initialization: p threads, initialize w0, \u03b7; For each thread, do: for l = 0, 1, 2, ..., T \u2212 1 do\nRead current w in the shared memory, denoted as w\u0302; Randomly pick up an i from {1, . . . , n} and compute the gradient\u2207fi(w\u0302); w\u2190 w \u2212 \u03b7\u2207fi(w\u0302);\nend for\nAs in (Zhao and Li 2016), we can construct an equivalent write sequence {wt}:\nwt+1 = wt \u2212 \u03b7Bt\u2207fit(w\u0302t), (2)\nwhere 0 \u2264 t \u2264 p\u00d7T , Bt is a random diagonal matrix whose diagonal entries are 0 or 1. The Bt is used to denote whether over-writing happens. If the kth diagonal entry of Bt is 0, it means that the kth element in the gradient vector \u2207fit(w\u0302t)\nis overwritten by other threads. Otherwise, that element is not overwritten.\nw\u0302t is read by the thread who computes\u2207fit(w\u0302t) and has the following format:\nw\u0302t = wa(t) \u2212 \u03b7 t\u22121\u2211 j=a(t) Pt,j\u2212a(t)\u2207fij (w\u0302j), (3)\nwhere a(t) means that some old stochastic gradients have been completely written on the w in the shared memory. Pt,j\u2212a(t) is a diagonal matrix whose diagonal entries are 0 or 1, which means w\u0302t might include parts of new stochastic gradients.\nIn the lock-free strategy, we need the following assumptions to guarantee convergence:\nAssumption 2. a(t) is bounded by: 0 \u2264 t\u2212 a(t) \u2264 \u03c4 It means that the old stochastic gradients \u2207fi0 , . . . ,\u2207fit\u2212\u03c4\u22121 have been completely written on w in the shared memory.\nAssumption 3. We consider the matrix Bt as a random matrix and E[Bt|wt, w\u0302t] = B 0 with the minimum eigenvalue \u03b1 > 0.\nAccording to the definition of Bt, it is easy to find Bt,B are positive semi-definite matrices and the largest eigenvalue of B is less than or equal to 1. Assumption 3 means that the probability that over-writing happens is at most 1 \u2212 \u03b1 < 1 for each write step.\nAssumption 4. Bt and it are independent. Since it is the random index selected by each thread while Bt is highly affected by the hardware, the independence assumption is reasonable.\nFor Hogwild!, the following assumption is also necessary:\nAssumption 5. There exists a constant V , \u2016\u2207fi(w)\u2016 \u2264 V, i = 1, . . . , n.\nFor convenience, in this section, we denote\nq(x) = 1\nn n\u2211 i=1 \u2016fi(x)\u20162.\nIt is easy to find that Eq(w\u0302t) = E[\u2016\u2207fit(w\u0302t)\u20162] and note that when x is close to some stationary point, q(x) may still be far away from 0. Hence, it is not a variance reduction method and we need to control the variance of the stochastic gradient.\nThe difficulty of the analysis is wt 6= w\u0302t. Here, we give the following Lemmas 3:\nLemma 2. In Hogwild!, we have Eq(w\u0302t) \u2264 \u03c1Eq(w\u0302t+1) if \u03c1, \u03b7 satisfy\n1\n1\u2212 \u03b7 \u2212 9\u03b7(\u03c4+1)L 2(\u03c1\u03c4+1\u22121)\n\u03c1\u22121\n\u2264 \u03c1.\n3The proof of some Lemmas can be found in the supplementary material, which can be downloaded from http://cs.nju. edu.cn/lwj/paper/LFnonConvex_sup.pdf.\nLemma 3. With the condition about \u03c1, \u03b7 in Lemma 2, we have\nE\u2016wt \u2212 w\u0302t\u20162 \u2264 4\u03b72\u03c4\u03c1(\u03c1\u03c4 \u2212 1)\n\u03c1\u2212 1 Eq(w\u0302t) (4)\nCombining with Assumption 5, we can find that the gap of the write sequence and read sequence can always be bounded by a constant 4\u03b7\n2V 2\u03c4\u03c1(\u03c1\u03c4\u22121) \u03c1\u22121 .\nTheorem 1. LetA = 2f(w0)\u03b1 andB = 2V 2( 2\u03c4L 2\u03b7\u03c1(\u03c1\u03c4\u22121) \u03b1(\u03c1\u22121) +\nL 2\u03b1 ). If we take the stepsize \u03b7 = \u221a A T\u0303B\n, where T\u0303 = p \u00d7 T , we can get the following result:\n1\nT\u0303 T\u0303\u22121\u2211 t=0 E\u2016\u2207f(wt)\u20162 \u2264 \u221a AB T\u0303 .\nProof. According to Assumption 1, we have\nE[f(wt+1)|wt, w\u0302t] \u2264f(wt)\u2212 \u03b7E[\u2207f(wt)TBt\u2207fit(w\u0302t)|wt, w\u0302t]\n+ L\u03b72\n2 E[\u2016\u2207fit(w\u0302t)\u20162|wt, w\u0302t]\n=f(wt)\u2212 \u03b7\u2207f(wt)TB\u2207f(w\u0302t)\n+ L\u03b72\n2 E[\u2016\u2207fit(w\u0302t)\u20162|wt, w\u0302t]\n\u2264f(wt)\u2212 \u03b1\u03b7\n2 \u2016\u2207f(wt)\u20162 +\nL2\u03b7\n2 \u2016wt \u2212 w\u0302t\u20162\n+ L\u03b72\n2 E[\u2016\u2207fit(w\u0302t)\u20162|wt, w\u0302t],\nwhere the first equality uses Assumption 4, the second inequality uses Lemma 1. Taking expectation on the above inequality, we obtain\nEf(wt+1)\n\u2264Ef(wt)\u2212 \u03b1\u03b7\n2 E\u2016\u2207f(wt)\u20162 +\nL2\u03b7\n2 E\u2016wt \u2212 w\u0302t\u20162\n+ L\u03b72V 2\n2\n\u2264Ef(wt)\u2212 \u03b1\u03b7\n2 E\u2016\u2207f(wt)\u20162\n+ \u03b72V 2( 2\u03c4L2\u03b7\u03c1(\u03c1\u03c4 \u2212 1) \u03c1\u2212 1 + L 2 ),\nwhere the first inequality uses Assumption 5 and second inequality uses Lemma 3. Summing the above inequality from t = 0 to T\u0303 \u2212 1, we get\nT\u0303\u22121\u2211 t=0 E\u2016\u2207f(wt)\u20162\n\u2264 2 \u03b1\u03b7 f(w0) + 2\u03b7T\u0303V 2( 2\u03c4L2\u03b7\u03c1(\u03c1\u03c4 \u2212 1) \u03b1(\u03c1\u2212 1) + L 2\u03b1 ).\nFor convenience, let A = 2f(w0)\u03b1 and B = 2V 2( 2\u03c4L 2\u03b7\u03c1(\u03c1\u03c4\u22121) \u03b1(\u03c1\u22121) + L 2\u03b1 ), which are two bounded constants.\nIf we take the stepsize \u03b7 = \u221a\nA T\u0303B , we get\n1\nT\u0303 T\u0303\u22121\u2211 t=0 E\u2016\u2207f(wt)\u20162 \u2264 \u221a AB T\u0303 .\nHence, our theoretical result shows that Hogwild! with lock-free strategy gets a convergence rate of O(1/ \u221a T\u0303 ) for\nnon-convex problems, where T\u0303 = p\u00d7T is the total iteration number of p threads."}, {"heading": "AsySVRG for Non-Convex Problems", "text": "The AsySVRG method (Zhao and Li 2016) is listed in Algorithm 2. AsySVRG provides a lock-free parallel strategy for the original sequential SVRG (Johnson and Zhang 2013). Compared with Hogwild!, AsySVRG includes the full gradient to get a variance reduced stochastic gradient, which has been proved to have linear convergence rate on strongly convex problems (Zhao and Li 2016). In this section, we will prove that AsySVRG is also convergent for non-convex problems, and has faster convergence rate than Hogwild! on non-convex problems.\nAlgorithm 2 AsySVRG Initialization: p threads, initialize w0, \u03b7; for t = 0, 1, 2, ...T \u2212 1 do\nu0 = wt; All threads parallelly compute the full gradient \u2207f(u0) = 1 n \u2211n i=1\u2207fi(u0); u = wt; For each thread, do: for j = 0 to M \u2212 1 do\nRead current value of u, denoted as u\u0302, from the shared memory. And randomly pick up an i from {1, . . . , n}; Compute the update vector: v\u0302 = \u2207fi(u\u0302) \u2212 \u2207fi(u0) + \u2207f(u0); u\u2190 u\u2212 \u03b7v\u0302; end for Take wt+1 to be the current value of u in the shared memory;\nend for\nSimilar to the analysis in the last section, we construct an equivalent write sequence {ut,m} for the tth outer-loop:\nut,0 = wt,\nut,m+1 = ut,m \u2212 \u03b7Bt,mv\u0302t,m, (5)\nwhere v\u0302t,m = \u2207fit,m(u\u0302t,m) \u2212 \u2207fit,m(ut,0) + \u2207f(ut,0). Bt,m is a diagonal matrix whose diagonal entries are 0 or 1. And u\u0302t,m is read by the thread who computes v\u0302t,m. It has the following format:\nu\u0302t,m = ut,a(m) \u2212 \u03b7 m\u22121\u2211 j=a(m) P (t) m,j\u2212a(m)v\u0302t,j ,\nwhere P(t)m,j\u2212a(m) is a diagonal matrix whose diagonal entries are 0 or 1. Note that according to (5), ut,M\u0303 = wt+1\nsince all the stochastic gradients have been written on w at the end of the tth outer-loop. Here, we also need the assumptions: 0 \u2264 m \u2212 a(m) \u2264 \u03c4 ; E[Bt,m|ut,m, u\u0302t,m] = B 0 with the minimum eigenvalue \u03b1 > 0; Bt,m and it,m are independent. These assumptions are similar to those in the previous section.\nFor convenience, let pi(x) = \u2207fi(x) \u2212 \u2207fi(ut,0) + \u2207f(ut,0), and in this section, we denote\nq(x) = 1\nn n\u2211 i=1 \u2016pi(x)\u20162."}, {"heading": "It easy to find that Eq(u\u0302t,m) = E[\u2016v\u0302t,m\u20162].", "text": "The difference between Hogwild! and AsySVRG is the stochastic gradient and we have the following Lemmas which lead to fast convergence rate of AsySVRG: Lemma 4. \u2200x, we have\nq(x) \u2264 2L2\u2016x\u2212 ut,0\u20162 + 2\u2016\u2207f(x)\u20162. Proof.\nq(x) = 1\nn n\u2211 i=1 \u2016\u2207fi(x)\u2212\u2207fi(ut,0) +\u2207f(ut,0)\u20162\n\u2264 2 n n\u2211 i=1 \u2016\u2207fi(x)\u2212\u2207fi(ut,0) +\u2207f(ut,0)\u2212\u2207f(x)\u20162\n+ 2\u2016\u2207f(x)\u20162\n\u2264 2 n n\u2211 i=1 \u2016\u2207fi(x)\u2212\u2207fi(ut,0)\u20162 + 2\u2016\u2207f(x)\u20162\n\u22642L2\u2016x\u2212 ut,0\u20162 + 2\u2016\u2207f(x)\u20162.\nAccording to Lemma 4, we can find that AsySVRG is a variance reduction method for non-convex problems, because when u\u0302t,m,ut,0 get close to some stationary point, q(u\u0302t,m) gets close to 0. And hence we do not need the bounded gradient assumption for the convergence proof.\nSince ut,m 6= u\u0302t,m, the difficulty of convergence analysis lies in the gap between ut,m and u\u0302t,m, and the relation between q(u\u0302t,m) and q(ut,m). Lemma 5. In AsySVRG, we have Eq(u\u0302t,m) < \u03c1Eq(u\u0302t,m+1) if we choose \u03c1 and \u03b7 to satisfy that\n1\n1\u2212 \u03b7 \u2212 9\u03b7(\u03c4+1)L 2(\u03c1\u03c4+1\u22121)\n\u03c1\u22121\n\u2264 \u03c1.\nLemma 6. With the condition about \u03c1, \u03b7 in Lemma 5, we have\nE\u2016ut,m \u2212 u\u0302t,m\u20162 \u2264 4\u03b72\u03c4\u03c1(\u03c1\u03c4 \u2212 1)\n\u03c1\u2212 1 Eq(u\u0302t,m). (6)\nLemma 7. With the condition about \u03c1, \u03b7 in Lemma 5, we have Eq(u\u0302t,m) < \u03c1Eq(ut,m).\nCombining Lemma 6 and Lemma 7, we can directly obtain:\nE \u2016u\u0302t,m \u2212 ut,m\u20162 \u2264 4\u03b72\u03c4\u03c12(\u03c1\u03c4 \u2212 1)\n\u03c1\u2212 1 Eq(ut,m). (7)\nTheorem 2. We define cm = cm+1(1+\u03b2\u03b7)+2L2\u03b72hm+1, hm = ( \u03b7L2 2 + 2cm\u03b7 \u03b2 ) 4\u03c4\u03c12(\u03c1\u03c4\u22121) \u03c1\u22121 +(cm\u03c1+ L\u03c1 2 ) with c0, \u03b2 > 0. Furthermore, we choose c0, \u03b7, \u03b2 such that \u03b3 = min \u03b1\u03b72 \u2212 2cm+1\u03b7\n\u03b2 \u2212 2\u03b7 2hm+1 > 0 and cM\u0303 = 0, where M\u0303 = M \u00d7 p.\nThen we have\n1\nTM\u0303 T\u22121\u2211 t=0 M\u0303\u22121\u2211 m=0 E\u2016\u2207f(ut,m)\u20162 \u2264 Ef(w0)\u2212 Ef(wT ) TM\u0303\u03b3 .\nProof. In the tth outer-loop, similar to (Reddi et al. 2016), we define Rt,m as follows\nRt,m = f(ut,m) + cm\u2016ut,m \u2212 ut,0\u20162.\nThen \u2200\u03b2 > 0,\nE[\u2016ut,m+1 \u2212 ut,0\u20162|ut,m, u\u0302t,m] \u2264E\u2016ut,m+1 \u2212 ut,m\u20162 + \u2016ut,m \u2212 ut,0\u20162\n\u2212 2\u03b7(EBt,mv\u0302t,m)T (ut,m \u2212 ut,0) \u2264\u03b72E\u2016v\u0302t,m\u20162 + (1 + \u03b2\u03b7)\u2016ut,m \u2212 ut,0\u20162\n+ \u03b7\n\u03b2 \u2016\u2207f(u\u0302t,m)\u20162\n\u2264\u03b72E\u2016v\u0302t,m\u20162 + (1 + \u03b2\u03b7)\u2016ut,m \u2212 ut,0\u20162\n+ 2\u03b7\n\u03b2 (\u2016\u2207f(ut,m)\u2016+ \u2016\u2207f(u\u0302t,m)\u2212\u2207f(ut,m)\u20162)\n\u2264\u03b72E\u2016v\u0302t,m\u20162 + (1 + \u03b2\u03b7)\u2016ut,m \u2212 ut,0\u20162\n+ 2\u03b7\n\u03b2 (\u2016\u2207f(ut,m)\u20162\n+ L2\u2016u\u0302t,m \u2212 ut,m\u20162), (8)\nwhere the second inequality uses the fact 2ab \u2264 \u03b2a2 + 1\u03b2 b 2. Since the objective function is L-smooth, we have\nE[f(ut,m+1)|ut,m, u\u0302t,m] \u2264\u2212 \u03b7E[\u2207f(ut,m)TBt,m\u2207fit,m(u\u0302t,m)|ut,m, u\u0302t,m]\n+ f(ut,m) + L\u03b72\n2 E[\u2016v\u0302t,m\u20162|ut,m, u\u0302t,m]\n=f(ut,m)\u2212 \u03b7\u2207f(ut,m)TB\u2207f(u\u0302t,m)\n+ L\u03b72\n2 E[\u2016v\u0302t,m\u20162|ut,m, u\u0302t,m]\n\u2264f(ut,m)\u2212 \u03b1\u03b7\n2 \u2016\u2207f(ut,m)\u20162\n+ \u03b7L2\n2 \u2016ut,m \u2212 u\u0302t,m\u20162\n+ L\u03b72\n2 E[\u2016v\u0302t,m\u20162|ut,m, u\u0302t,m], (9)\nwhere the first equality uses the independence of Bt,m, it,m, the second inequality uses Lemma 1. Combining (8) and (9),\nwe have\nERt,m+1 =Ef(ut,m+1) + cm+1\u2016ut,m+1 \u2212 ut,0\u20162\n\u2264Ef(ut,m)\u2212 ( \u03b1\u03b7 2 \u2212 2cm+1\u03b7 \u03b2 )E\u2016\u2207f(ut,m)\u20162\n+ ( \u03b7L2\n2 +\n2cm+1\u03b7\n\u03b2 )E\u2016ut,m \u2212 u\u0302t,m\u20162\n+ cm+1(1 + \u03b2\u03b7)E\u2016ut,m \u2212 ut,0\u20162\n+ \u03b72(cm+1 + L\n2 )E\u2016v\u0302t,m\u20162\n\u2264Ef(ut,m)\u2212 ( \u03b1\u03b7 2 \u2212 2cm+1\u03b7 \u03b2 )E\u2016\u2207f(ut,m)\u20162\n+ ( \u03b7L2\n2 +\n2cm+1\u03b7 \u03b2 ) 4\u03c4\u03b72\u03c12(\u03c1\u03c4 \u2212 1) \u03c1\u2212 1 Eq(ut,m)\n+ cm+1(1 + \u03b2\u03b7)E\u2016ut,m \u2212 ut,0\u20162\n+ \u03b72(cm+1 + L\n2 )E\u2016v\u0302t,m\u20162,\nwhere the last inequality uses equation (7). For convenience, we use hm = (\u03b7L 2\n2 + 2cm\u03b7 \u03b2 ) 4\u03c4\u03c12(\u03c1\u03c4\u22121) \u03c1\u22121 + \u03c1(cm + L 2 ). Since E[\u2016v\u0302t,m\u2016 2] = Eq(u\u0302t,m) \u2264 \u03c1Eq(ut,m), we have ERt,m+1\n\u2264Ef(ut,m)\u2212 ( \u03b1\u03b7 2 \u2212 2cm+1\u03b7 \u03b2 )E\u2016\u2207f(ut,m)\u20162\n+ cm+1(1 + \u03b2\u03b7)E\u2016ut,m \u2212 ut,0\u20162 + \u03b72hm+1Eq(ut,m) \u2264Ef(ut,m) + [cm+1(1 + \u03b2\u03b7) + 2L 2\u03b72hm+1]E\u2016ut,m \u2212 ut,0\u20162\n\u2212 (\u03b1\u03b7 2 \u2212 2cm+1\u03b7 \u03b2 \u2212 2\u03b72hm+1)E\u2016\u2207f(ut,m)\u20162,\nwhere the second inequality uses Lemma 4. Then we can obtain:\n( \u03b1\u03b7 2 \u2212 2cm+1\u03b7 \u03b2 \u2212 2\u03b72hm+1)E\u2016\u2207f(um)\u20162\n\u2264 ERm \u2212 ERm+1,\nwhere cm = cm+1(1 + \u03b2\u03b7) + 2L2\u03b72hm+1. We set c0 > 0. It is easy to see that cm > cm+1. We can choose c0, \u03b7, \u03b2 to make cM\u0303 = 0. Then we have:\nM\u0303\u22121\u2211 m=0 E\u2016\u2207f(ut,m)\u20162\n\u2264 ER0 \u2212 ERM\u0303\n\u03b3 = Ef(wt)\u2212 Ef(wt+1) \u03b3 ,\nwhich is equivalent to\n1\nTM\u0303 T\u22121\u2211 t=0 M\u0303\u22121\u2211 m=0 E\u2016\u2207f(ut,m)\u20162 \u2264 Ef(w0)\u2212 Ef(wT ) TM\u0303\u03b3 ."}, {"heading": "Computation Complexity", "text": "In Theorem 2, we construct a sequence {cm} and need \u03b3 > 0. According to the definition of hm, we can write hm as hm = gcm + f , where g = 2\u03b7\u03b2 4\u03c4\u03c12(\u03c1\u03c4\u22121) \u03c1\u22121 + \u03c1, f = \u03b7L2\n2 4\u03c4\u03c12(\u03c1\u03c4\u22121) \u03c1\u22121 + L\u03c1 2 are constants.\nFirst, we choose \u03b2 > \u03b7, then both g, f are bounded positive constants. We have\ncm = cm+1(1 + \u03b2\u03b7 + 2L 2\u03b72g) + 2L2\u03b72f.\nLet a = \u03b2\u03b7 + 2L2\u03b72g. Because cM\u0303 = 0, it is easy to get\nc0 = 2L 2\u03b72f (1 + a)M\u0303 \u2212 1 a .\nWe take M\u0303 = b 1ac \u2264 1 a , then we have c0 \u2264 4L2\u03b72f a and\n\u03b3 = \u03b1 2 (\u03b7 \u2212 4c0\u03b7 \u03b1\u03b2 \u2212 4gc0 \u03b1 \u03b72 \u2212 4f \u03b1 \u03b72).\nAs recommended in (Reddi et al. 2016), we can take \u03b7 = \u00b5/n2/3, \u03b2 = v/n1/3 with \u03b7 < \u03b2 (assuming n is large). Then we can get f = O(1), g = O(1), a = O(1/n). By choosing \u00b5, v to satisfy 16L\n2f\u00b5 \u03b1v2 < 1 such that 4c0 \u03b1\u03b2 < 1, it is easy to\nfind that \u03b3 = O(1/n2/3) > 0, M\u0303 = O(n). Hence, to get an -local optimal solution, the computation complexity by all p threads isO(n 2 3 / ), and the computation complexity of each thread is O(n 2 3\np )."}, {"heading": "Experiment", "text": "To verify our theoretical results about Hogwild! and AsySVRG, we use a fully-connected neural network to construct a non-convex function. The neural network has one hidden layer with 100 nodes and the sigmoid function is used for the activation function. We use the soft-max output and a L2 regularization for training. The loss function is:\nf(w,b) = \u2212 1 n n\u2211 i=1 K\u2211 k=1 1{yi = k} log o(k)i + \u03bb 2 \u2016w\u20162,\nwhere w is the weights of the neural network, b is the bias, yi is the label of instance xi, o (k) i is the output corresponding to xi, K is the total number of class labels. We use two datasets: connect-4 and MNIST4 to do experiments and \u03bb = 10\u22123. We initialize w by randomly sampling from a Gaussian distribution with mean being 0 and variance being 0.01, and initialize b = 0. During training, we use a fixed stepsize for both Hogwild! and AsySVRG. The stepsize is chosen from {0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}, and the best is reported. For the iteration number of the inner-loop of AsySVRG, we set M = n/p, where p is the number of threads. The experiments are conducted on a server with 12 Intel cores and 64G memory.\n4https://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/\nFigure 1 illustrates the convergence property of both Hogwild! and AsySVRG. The x-axis denotes the CPU time, where we set the CPU time that Hogwild! passes through the whole dataset once with one thread as 1 unit. The yaxis denotes the training loss. In this experiment, we run Hogwild! and AsySVRG with 10 threads. Hogwild!-10 and AsySVRG-10 denote the corresponding methods with 10 threads. It is easy to see that both Hogwild! and AsySVRG are convergent. Furthermore, AsySVRG is faster than Hogwild!. This is consistent with our theoretical results in this paper.\nFigure 2 reports the results of Hogwild! and AsySVRG with different numbers of threads, where the number of threads p = 1, 4, 10. We can find that in most cases the two methods will become faster with the increase of threads. The only outlier is the case for Hogwild! on dateset connect-4, Hogwild! using 4 threads is slower than using 1 thread. One possible reason is that we have two CPUs in our server, with 6 cores for each CPU. In the 4-thread case, different threads may be allocated on different CPUs, which will cause extra cost."}, {"heading": "Conclusion", "text": "In this paper, we have provided theoretical proof about the convergence of two representative lock-free strategy based parallel SGD methods, Hogwild! and AsySVRG, for nonconvex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results. To the best of our knowledge, this is the first work to prove the convergence of lock-free strategy based parallel SGD methods for non-convex problems."}, {"heading": "Acknowledgements", "text": "This work is partially supported by NSFC (No. 61472182) and a fund from Tencent."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of Lemma 2", "text": "Proof. First, \u2200x,y and r > 0, we have:\n\u2016\u2207fi(x)\u20162 \u2212 \u2016\u2207fi(y)\u20162\n\u22642\u2207fi(x)T (\u2207fi(x)\u2212\u2207fi(y))\n\u22641 r \u2016\u2207fi(x)\u20162 + r \u2016\u2207fi(x)\u2212\u2207fi(y)\u20162 = 1\nr \u2016\u2207fi(x)\u20162 + r \u2016\u2207fi(x)\u2212\u2207fi(y)\u20162\n\u22641 r \u2016\u2207fi(x)\u20162 + rL2 \u2016x\u2212 y\u20162 (10)\nIn the above equation, take x = w\u0302t,y = w\u0302t+1, we obtain:\n\u2016\u2207fi(w\u0302t)\u20162 \u2212 \u2016\u2207fi(w\u0302t+1)\u20162\n\u22641 r \u2016\u2207fi(w\u0302t)\u20162 + rL2 \u2016w\u0302t \u2212 w\u0302t+1\u20162\nAccording to the definition of w\u0302t, we have\n\u2016w\u0302t \u2212 w\u0302t+1\u2016\n=\u2016wa(t) \u2212 \u03b7 t\u22121\u2211 j=a(t) Pt,j\u2212a(t)\u2207fij (w\u0302j)\n\u2212 (wa(t+1) \u2212 \u03b7 t\u2211\nj=a(t+1)\nPt+1,j\u2212a(t+1)\u2207fij (w\u0302j))\u2016\n\u2264 \u2225\u2225wa(t) \u2212wa(t+1)\u2225\u2225 + \u03b7\nt\u22121\u2211 j=a(t) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225+ \u03b7 t\u2211 j=a(t+1) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225 \u2264 a(t+1)\u22121\u2211 j=a(t) \u2016wj \u2212wj+1\u2016\n+ \u03b7 t\u22121\u2211 j=a(t) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225+ \u03b7 t\u2211 j=a(t+1) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225 \u2264\u03b7\na(t+1)\u22121\u2211 j=a(t) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225 + \u03b7\nt\u22121\u2211 j=a(t) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225+ \u03b7 t\u2211 j=a(t+1) \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225 \u22643\u03b7\nt\u2211 j=t\u2212\u03c4 \u2225\u2225\u2207fij (w\u0302j)\u2225\u2225 . Combining the two above equation, we obtain:\n\u2016\u2207fi(w\u0302t)\u20162 \u2212 \u2016\u2207fi(w\u0302t+1)\u20162\n\u22641 r \u2016\u2207fi(w\u0302t)\u20162 + 9r(\u03c4 + 1)L2\u03b72 t\u2211 j=t\u2212\u03c4 \u2225\u2225\u2207fij (w\u0302j)\u2225\u22252\nFor any fixed i, we take expectation on the random index ij , we obtain:\n\u2016\u2207fi(w\u0302t)\u20162 \u2212 \u2016\u2207fi(w\u0302t+1)\u20162\n\u22641 r \u2016\u2207fi(w\u0302t)\u20162 + 9r(\u03c4 + 1)L2\u03b72 t\u2211 j=t\u2212\u03c4 q(w\u0302j)\nSumming up i from 1 to n, we obtain: Eq(w\u0302t)\u2212 Eq(w\u0302t+1)\n\u22641 r Eq(w\u0302t) + 9r(\u03c4 + 1)L2\u03b72 t\u2211 j=t\u2212\u03c4 Eq(w\u0302j)\nNow, we prove the final result by induction and take r = 1\u03b7 . When t = 0, we have\nEq(w\u03020) \u2264 1\n1\u2212 \u03b7 \u2212 9\u03b7(\u03c4 + 1)L2 Eq(w\u03021) \u2264 \u03c1Eq(w\u03021)\nAssuming the result is right for t\u2212 1, then we have Eq(w\u0302t)\u2212 Eq(w\u0302t+1)\n\u22641 r Eq(w\u0302t) + 9\u03b7(\u03c4 + 1)L2Eq(w\u0302j) t\u2211 j=t\u2212\u03c4 \u03c1t\u2212j\nwhich means\nEq(w\u0302t) \u2264 1\n1\u2212 \u03b7 \u2212 9\u03b7(\u03c4+1)L 2(\u03c1\u03c4+1\u22121)\n\u03c1\u22121\nEq(w\u0302t+1) \u2264 \u03c1Eq(w\u0302t+1)"}, {"heading": "Proof of Lemma 3", "text": "Proof.\n\u2016wt \u2212 w\u0302t\u2016\n=\u2016wt \u2212wa(t) + \u03b7 t\u22121\u2211 j=a(t) Pt,j\u2212a(t)\u2207fij (w\u0302j)\u2016 \u2264\u2016wt \u2212wa(t)\u2016+ \u03b7 t\u22121\u2211 j=a(t) \u2016\u2207fij (w\u0302j)\u2016 \u2264 t\u22121\u2211 j=a(t) \u2016wj \u2212wj+1\u2016+ \u03b7 t\u22121\u2211 j=a(t) \u2016\u2207fij (w\u0302j)\u2016 \u22642\u03b7 t\u22121\u2211 j=a(t) \u2016\u2207fij (w\u0302j)\u2016\nThen we get the result\nE\u2016wt \u2212 w\u0302t\u20162 \u22644\u03b72\u03c4 t\u22121\u2211 j=a(t) E\u2016\u2207fij (w\u0302j)\u20162\n=4\u03b72\u03c4 t\u22121\u2211 j=a(t) Eq(w\u0302j)\n\u22644\u03b7 2\u03c4\u03c1(\u03c1\u03c4 \u2212 1) \u03c1\u2212 1 Eq(w\u0302t)\nwhere the last inequality uses Lemma 2 in the appendix."}, {"heading": "Proof of Lemma 5", "text": "Proof. First, \u2200x,y and r > 0, we have:\n\u2016pi(x)\u20162 \u2212 \u2016pi(y)\u20162\n\u22642pi(x)T (pi(x)\u2212 pi(y))\n\u22641 r \u2016pi(x)\u20162 + r \u2016pi(x)\u2212 pi(y)\u20162 = 1\nr \u2016pi(x)\u20162 + r \u2016pi(x)\u2212 pi(y)\u20162\n\u22641 r \u2016pi(x)\u20162 + rL2 \u2016x\u2212 y\u20162 (11)\nIn the above equation, take x = u\u0302t,m,y = u\u0302t,m+1, we obtain:\n\u2016pi(u\u0302t,m)\u20162 \u2212 \u2016pi(u\u0302t,m+1)\u20162\n\u22641 r \u2016pi(u\u0302t,m)\u20162 + rL2 \u2016u\u0302t,m \u2212 u\u0302t,m+1\u20162\nFurthermore, we can get\n\u2016u\u0302t,m \u2212 u\u0302t,m+1\u2016\n=\u2016ut,a(m) \u2212 \u03b7 m\u22121\u2211 i=a(m) P (t) m,i\u2212a(m)v\u0302t,i\n\u2212 (ut,a(m+1) \u2212 \u03b7 m\u2211\ni=a(m+1)\nP (t) m+1,i\u2212a(m+1)v\u0302t,i)\u2016\n\u2264 \u2225\u2225ut,a(m) \u2212 ut,a(m+1)\u2225\u2225+ \u03b7 m\u22121\u2211\ni=a(m)\n\u2016v\u0302t,i\u2016+ \u03b7 m\u2211\ni=a(m+1)\n\u2016v\u0302t,i\u2016\n\u2264 a(m+1)\u22121\u2211 i=a(m) \u2016ut,i \u2212 ut,i+1\u2016+ \u03b7 m\u22121\u2211 i=a(m) \u2016v\u0302t,i\u2016+ \u03b7 m\u2211 i=a(m+1) \u2016v\u0302t,i\u2016 \u2264\u03b7 a(m+1)\u22121\u2211 i=a(m) \u2016v\u0302t,i\u2016+ \u03b7 m\u22121\u2211 i=a(m) \u2016v\u0302t,i\u2016+ \u03b7 m\u2211 i=a(m+1) \u2016v\u0302t,i\u2016\n\u22643\u03b7 m\u2211\ni=m\u2212\u03c4 \u2016v\u0302t,i\u2016 .\nThen, we take r = 1\u03b7 have\n\u2016pi(u\u0302t,m)\u20162 \u2212 \u2016pi(u\u0302t,m+1)\u20162\n\u2264\u03b7 \u2016pi(u\u0302t,m)\u20162 + 9\u03b7(\u03c4 + 1)L2 m\u2211\nj=m\u2212\u03c4 \u2016v\u0302t,j\u20162\nFor any fixed i, we can take expectation for both sides of the above inequality and then sum i from 1 to n. Then we can get:\nEq(u\u0302t,m)\u2212 Eq(u\u0302t,m+1)\n\u2264\u03b7Eq(u\u0302t,m) + 9\u03b7(\u03c4 + 1)L2 m\u2211\nj=m\u2212\u03c4 Eq(u\u0302t,j)\nHere we use the fact that E[\u2016v\u0302t,j\u20162|u\u0302t,j ] = q(u\u0302t,j).\nWe prove our conclusion by induction. For convenience, we use qi to denote Eq(u\u0302t,i).\nWhen m = 0, we have\nq0 \u2264 1\n1\u2212 \u03b7 \u2212 9\u03b7(\u03c4 + 1)L2 q1 \u2264 \u03c1q1\nAssuming that \u2200m \u2264 M0, we have qm\u22121 \u2264 \u03c1qm, then for m =M0, we have\nqm\u2212qm+1 \u2264 \u03b7qm + 9\u03b7(\u03c4 + 1)L2 m\u2211\nj=m\u2212\u03c4 qj\n\u2264\u03b7qm + 9\u03b7(\u03c4 + 1)L2qm m\u2211\nj=m\u2212\u03c4 \u03c1m\u2212j\n=\u03b7qm + 9\u03b7(\u03c4 + 1)L2(\u03c1\u03c4+1 \u2212 1)\n\u03c1\u2212 1 qm\nwhich means that\nqm \u2264 1\n1\u2212 \u03b7 \u2212 9\u03b7(\u03c4+1)L 2(\u03c1\u03c4+1\u22121)\n\u03c1\u22121\nqm+1 \u2264 \u03c1qm+1"}, {"heading": "Proof of Lemma 6", "text": "Proof.\n\u2016u\u0302t,m \u2212 ut,m\u2016\n= \u2225\u2225\u2225\u2225\u2225\u2225ut,a(m) \u2212 \u03b7 m\u22121\u2211 j=a(m) P (t) m,j\u2212a(m)v\u0302t,j \u2212 ut,m \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225ut,a(m) \u2212 ut,m\u2225\u2225+ \u03b7 m\u22121\u2211\nj=a(m)\n\u2016v\u0302t,j\u2016\n\u2264 m\u22121\u2211 j=a(m) \u2016ut,j \u2212 ut,j+1\u2016+ \u03b7 m\u22121\u2211 j=a(m) \u2016v\u0302t,j\u2016 \u2264\u03b7 m\u22121\u2211 j=a(m) \u2016v\u0302t,j\u2016+ \u03b7 m\u22121\u2211 j=a(m) \u2016v\u0302t,j\u2016 \u22642\u03b7 m\u22121\u2211 j=a(m) \u2016v\u0302t,j\u2016 (12)\nThen we get the result\nE \u2016u\u0302t,m \u2212 ut,m\u20162 \u22644\u03b72\u03c4 m\u22121\u2211 j=a(m) E \u2016v\u0302t,j\u20162\n=4\u03b72\u03c4 m\u22121\u2211 j=a(m) Eq(u\u0302t,j)\n\u22644\u03b7 2\u03c4\u03c1(\u03c1\u03c4 \u2212 1) \u03c1\u2212 1 Eq(u\u0302t,m)\nwhere the last inequality uses Lemma 5."}, {"heading": "Proof of Lemma 7", "text": "Proof. First, for any fixed i, we have\n\u2016pi(u\u0302t,m)\u20162 \u2212 \u2016pi(ut,m)\u20162\n\u22642pi(x)T (pi(u\u0302t,m)\u2212 pi(ut,m))\n\u2264\u03b7 \u2016pi(u\u0302t,m)\u20162 + 1\n\u03b7 \u2016pi(u\u0302t,m)\u2212 pi(ut,m)\u20162\n=\u03b7 \u2016pi(u\u0302t,m)\u20162 + 1\n\u03b7 \u2016pi(u\u0302t,m)\u2212 pi(ut,m)\u20162\n\u2264\u03b7 \u2016pi(u\u0302t,m)\u20162 + 1\n\u03b7 L2 \u2016u\u0302t,m \u2212 ut,m\u20162 (13)\nTakeing expectation on both sides, using Lemma 6 and then summing up i from 1 to n and we can get\nEq(u\u0302t,m)\u2212 Eq(ut,m)\n\u2264\u03b7Eq(u\u0302t,m) + 4\u03b7\u03c4L2\u03c1(\u03c1\u03c4 \u2212 1)\n\u03c1\u2212 1 Eq(u\u0302t,m)\nwhich means that\nEq(u\u0302t,m) \u2264 1\n1\u2212 \u03b7 \u2212 4\u03b7\u03c4L 2\u03c1(\u03c1\u03c4\u22121) \u03c1\u22121\nEq(ut,m) < \u03c1Eq(ut,m)"}], "references": [{"title": "Variance reduction for faster non-convex optimization", "author": ["Z. Allen-Zhu", "E. Hazan"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Allen.Zhu and Hazan,? 2016", "shortCiteRegEx": "Allen.Zhu and Hazan", "year": 2016}, {"title": "Improved svrg for nonstrongly-convex or sum-of-non-convex objectives", "author": ["Z. Allen-Zhu", "Y. Yuan"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Allen.Zhu and Yuan,? 2016", "shortCiteRegEx": "Allen.Zhu and Yuan", "year": 2016}, {"title": "Asynchronous stochastic convex optimization: the noise is in the noise and sgd don\u2019t care", "author": ["S. Chaturapruek", "J.C. Duchi", "C. R\u00e9"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Chaturapruek et al\\.,? 2015", "shortCiteRegEx": "Chaturapruek et al\\.", "year": 2015}, {"title": "Stochastic first- and zeroth-order methods for nonconvex stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "SIAM Journal on Optimization 23(4):2341\u20132368.", "citeRegEx": "Ghadimi and Lan,? 2013", "shortCiteRegEx": "Ghadimi and Lan", "year": 2013}, {"title": "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization", "author": ["Z. Huo", "H. Huang"], "venue": "CoRR abs/1604.03584.", "citeRegEx": "Huo and Huang,? 2016", "shortCiteRegEx": "Huo and Huang", "year": 2016}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A.J. Smola"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Reddi et al\\.,? 2015", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Takac", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Jaggi et al\\.,? 2014", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Johnson and Zhang,? 2013", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R.M. Bell", "C. Volinsky"], "venue": "IEEE Computer 42(8):30\u201337.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Scaling distributed machine learning with the parameter", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B. Su"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Stochastic variance reduced optimization for nonconvex sparse learning", "author": ["X. Li", "T. Zhao", "R. Arora", "Han", "J. Haupt"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Lian et al\\.,? 2015", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "Alex."], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Reddi et al\\.,? 2016", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M.W. Schmidt", "F. Bach"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Roux et al\\.,? 2012", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "author": ["C.D. Sa", "C. Re", "K. Olukotun"], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Sa et al\\.,? 2015", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research 14(1):567\u2013599.", "citeRegEx": "Shalev.Shwartz and Zhang,? 2013", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2013}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Shamir,? 2015", "shortCiteRegEx": "Shamir", "year": 2015}, {"title": "Convergence of stochastic gradient descent for pca", "author": ["O. Shamir"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Shamir,? 2016a", "shortCiteRegEx": "Shamir", "year": 2016}, {"title": "Fast stochastic algorithms for svd and pca: Convergence properties and convexity", "author": ["O. Shamir"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Shamir,? 2016b", "shortCiteRegEx": "Shamir", "year": 2016}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["E.P. Xing", "Q. Ho", "W. Dai", "J.K. Kim", "J. Wei", "S. Lee", "X. Zheng", "P. Xie", "A. Kumar", "Y. Yu"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Min-", "citeRegEx": "Xing et al\\.,? 2015", "shortCiteRegEx": "Xing et al\\.", "year": 2015}, {"title": "Asynchronous distributed semi-stochastic gradient optimization", "author": ["R. Zhang", "S. Zheng", "J.T. Kwok"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee", "author": ["Zhao", "S.-Y.", "Li", "W.-J."], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zhao et al\\.,? 2016", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Moreover, many other machine learning models, including neural networks (Krizhevsky, Sutskever, and Hinton 2012), matrix factorization (Koren, Bell, and Volinsky 2009), and principal component analysis (PCA) (Shamir 2015) and so on, can also be formulated as that in (1).", "startOffset": 208, "endOffset": 221}, {"referenceID": 17, "context": "Many works (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013) have found that SGD-based methods can achieve promising performance in large-scale learning problems.", "startOffset": 11, "endOffset": 96}, {"referenceID": 7, "context": "Many works (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013) have found that SGD-based methods can achieve promising performance in large-scale learning problems.", "startOffset": 11, "endOffset": 96}, {"referenceID": 17, "context": "When the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al.", "startOffset": 67, "endOffset": 152}, {"referenceID": 7, "context": "When the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al.", "startOffset": 67, "endOffset": 152}, {"referenceID": 13, "context": "When the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al. 2011) and DSGD (Jaggi et al.", "startOffset": 159, "endOffset": 178}, {"referenceID": 6, "context": "2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance.", "startOffset": 15, "endOffset": 97}, {"referenceID": 10, "context": "2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance.", "startOffset": 15, "endOffset": 97}, {"referenceID": 21, "context": "2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance.", "startOffset": 15, "endOffset": 97}, {"referenceID": 10, "context": "Because many researchers (Li et al. 2014; Xing et al. 2015) find that the SGD methods can also achieve good empirical results for nonconvex problems, theoretical proof about the convergence of SGD methods for non-convex problems has recently attracted much attention.", "startOffset": 25, "endOffset": 59}, {"referenceID": 21, "context": "Because many researchers (Li et al. 2014; Xing et al. 2015) find that the SGD methods can also achieve good empirical results for nonconvex problems, theoretical proof about the convergence of SGD methods for non-convex problems has recently attracted much attention.", "startOffset": 25, "endOffset": 59}, {"referenceID": 3, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 14, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 11, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 0, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 1, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 18, "context": "There are also some other theoretical results for some particular non-convex problems, like PCA (Shamir 2015; 2016a; 2016b) and matrix factorization (Sa, Re, and Olukotun 2015).", "startOffset": 96, "endOffset": 123}, {"referenceID": 12, "context": "There have appeared only two works (Lian et al. 2015; Huo and Huang 2016) which propose PSGD methods for non-convex problems with theoretical proof of convergence.", "startOffset": 35, "endOffset": 73}, {"referenceID": 4, "context": "There have appeared only two works (Lian et al. 2015; Huo and Huang 2016) which propose PSGD methods for non-convex problems with theoretical proof of convergence.", "startOffset": 35, "endOffset": 73}, {"referenceID": 12, "context": "However, the PSGD methods in (Lian et al. 2015) need write-lock or atomic operation for the memory to prove the convergence 2.", "startOffset": 29, "endOffset": 47}, {"referenceID": 4, "context": "Similarly, the work in (Huo and Huang 2016) also does not prove the convergence for the lockfree case in our paper.", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": "Recent works (Recht et al. 2011; Chaturapruek, Duchi, and R\u00e9 2015; J. Reddi et al. 2015; Zhao and Li 2016) find that lock-free strategy based parallel SGD (LF-PSGD) methods can empirically outperform lockbased PSGD methods for multi-core systems.", "startOffset": 13, "endOffset": 106}, {"referenceID": 13, "context": "In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! (Recht et al. 2011; Chaturapruek, Duchi, and R\u00e9 2015) and AsySVRG (Zhao and Li 2016), for non-convex problems.", "startOffset": 118, "endOffset": 171}, {"referenceID": 12, "context": "Although the implementation of AsySG-incon in (Lian et al. 2015) is lock-free, the theoretical analysis about the convergence of AsySG-incon is based on an assumption that no over-writing happens, i.", "startOffset": 46, "endOffset": 64}, {"referenceID": 13, "context": "Hogwild! for Non-Convex Problems The Hogwild! method (Recht et al. 2011) is listed in Algorithm 1.", "startOffset": 53, "endOffset": 72}, {"referenceID": 13, "context": "Please note that Hogwild! in (Recht et al. 2011) has several variants with locks or lock-free.", "startOffset": 29, "endOffset": 48}, {"referenceID": 7, "context": "AsySVRG provides a lock-free parallel strategy for the original sequential SVRG (Johnson and Zhang 2013).", "startOffset": 80, "endOffset": 104}, {"referenceID": 14, "context": "In the t outer-loop, similar to (Reddi et al. 2016), we define Rt,m as follows", "startOffset": 32, "endOffset": 51}, {"referenceID": 14, "context": "As recommended in (Reddi et al. 2016), we can take \u03b7 = \u03bc/n, \u03b2 = v/n with \u03b7 < \u03b2 (assuming n is large).", "startOffset": 18, "endOffset": 37}], "year": 2016, "abstractText": "Stochastic gradient descent (SGD) and its variants have attracted much attention in machine learning due to their efficiency and effectiveness for optimization. To handle largescale problems, researchers have recently proposed several lock-free strategy based parallel SGD (LF-PSGD) methods for multi-core systems. However, existing works have only proved the convergence of these LF-PSGD methods for convex problems. To the best of our knowledge, no work has proved the convergence of the LF-PSGD methods for nonconvex problems. In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results. Introduction Many machine learning models can be formulated as the following optimization problem:", "creator": "TeX"}}}