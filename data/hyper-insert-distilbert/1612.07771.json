{"id": "1612.07771", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2016", "title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "abstract": "the quarter past two year saw concurrently the introduction of vastly new internet architectures such prominently as circular highway networks and residual networks which, for the first time, rapidly enabled the hierarchical training of feedforward networks with some dozens to hold hundreds computers of varying layers using surprisingly simple gradient descent. perhaps while depth of representation has even been posited publicly as a primary security reason for their success, even there exists are practical indications - that these architectures defy a sometimes popular view of deep learning technologies as a powerful hierarchical computation of increasingly abstract features at each communicating layer.", "histories": [["v1", "Thu, 22 Dec 2016 19:57:35 GMT  (164kb,D)", "http://arxiv.org/abs/1612.07771v1", "10 + 3pages, under review for ICLR 2017"], ["v2", "Fri, 3 Mar 2017 19:52:47 GMT  (1447kb,D)", "http://arxiv.org/abs/1612.07771v2", "10 + 4 pages, accepted for ICLR 2017"], ["v3", "Tue, 14 Mar 2017 21:27:03 GMT  (1440kb,D)", "http://arxiv.org/abs/1612.07771v3", "10 + 4 pages, accepted for ICLR 2017"]], "COMMENTS": "10 + 3pages, under review for ICLR 2017", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["klaus greff", "rupesh k srivastava", "j\\\"urgen schmidhuber"], "accepted": true, "id": "1612.07771"}, "pdf": {"name": "1612.07771.pdf", "metadata": {"source": "CRF", "title": "UNROLLED ITERATIVE ESTIMATION", "authors": ["Klaus Greff", "Rupesh K. Srivastava", "J\u00fcrgen Schmidhuber"], "emails": ["klaus@idsia.ch", "rupesh@idsia.ch", "juergen@idsia.ch"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nDeep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth. We refer to this way of thinking about neural networks as the representation view, which probably dates back to Hubel & Wiesel (1962). The representation view links the layers in a network to the abstraction levels of their representations, and as such represents a pervasive assumption in many recent publications including He et al. (2015) who describe the success of their Residual networks like this: \u201cSolely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.\u201d\nSurprisingly, increasing the depth of a network beyond a certain point often leads to a decline in performance even on the training set (Srivastava et al., 2015a). Since adding more layers cannot decrease\nar X\niv :1\n61 2.\n07 77\n1v 1\n[ cs\n.N E\n] 2\n2 D\nec 2\n01 6\nrepresentational power, this phenomenon is likely due to the vanishing gradient problem (Hochreiter, 1991). Therefore, even though deeper models are more powerful in principle, they often fall short in practice.\nRecently, training feedforward networks with hundreds of layers has become feasible through the invention of Highway networks (Srivastava et al. 2015a, May) and Residual networks (ResNets; He et al. 2015, December). The latter have been widely successful, advancing the state of the art on many benchmarks and winning several pattern recognition competitions (He et al., 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016). Both architectures have been introduced with the explicit goal of training deeper models.\nThere are, however, some surprising findings that seem to contradict the applicability of the representation view to these very deep networks. For example, it has been reported that removing almost any layer from a trained Residual or Highway network has only minimal effect on its overall performance (Srivastava et al., 2015b; Veit et al., 2016). This idea has been extended to a layerwise dropout, as a regularizer for ResNets (Huang et al., 2016b). But if each layer supposedly builds a new level of representation from the previous one, then removing any layer should critically disrupt the input for the following layer. So how is it possible that doing so seems to have only a negligible effect on the network output? Veit et al. (2016) even demonstrated that shuffling some of the layers in a trained ResNet barely affects performance.\nIt has been argued that ResNets are better understood as ensembles of shallow networks (Huang et al., 2016b; Veit et al., 2016; Abdi & Nahavandi, 2016). According to this interpretation, ResNets implicitly average exponentially many subnetworks, each of which only use a subset of the layers. But the question remains open as to how a layer in such a subnetwork can successfully operate with changing input representations. This, along with other findings, begs the question as to whether the representation view is appropriate for understanding these new architectures.\nIn this paper, we propose a new interpretation that reconciles the representation view with the operation of Highway and Residual networks: functional blocks1 in these networks do not compute entirely new representations; instead, they perform an unrolled iterative estimation of representations that refine/improve upon their input representation, thus \u201cpreserving feature identity.\u201d The transition to a new level of representation occurs when a dimensionality change\u2014through projection\u2014separates two groups of blocks which we refer to as a stage (figure 1). Taking this perspective, we are able to explain previously elusive findings such as the effects of lesioning and shuffling. Furthermore, we formalize this notion and use it to directly derive Residual and Highway networks. Finally, we present some preliminary experiments to compare these two architectures and investigate some advantages and disadvantages that have recently been proposed."}, {"heading": "2 CHALLENGING THE REPRESENTATION VIEW", "text": "This section provides a brief survey of some the findings and points of contention that seem to contradict a representation view of Highway and Residual networks.\nStaying Close to the Inputs. The success of ResNets has been partly attributed to the fact that they obviate the need to learn the identity mapping, which is difficult. However, learning the negative identity should be at least as difficult. So the fact that the residual form is useful indicates that Residual blocks typically stay close to the input representation, rather than replace it.\nThe analysis in Srivastava et al. (2015a) shows that in trained Highway networks, the activity of the transform gates is often sparse for each individual sample, while their average activity over all training samples is non-sparse. Therefore, most units have learned to copy their inputs and only replace features selectively. Again, this means that most of the features are propagated unchanged rather than being combined and changed between layers\u2014an observation that contradicts the idea of building a new level of abstraction at each layer.\n1We refer to the building blocks of a ResNet\u2014a few layers with an identity skip connection\u2014as a Residual block (He et al., 2015). Analogously, in a Highway network, we refer to a collection of layers with a gated skip connection as a Highway block. See Figure 1 for an illustration.\nFigure 2: (a) A single neural network layer that directly computes the desired representation. (b) The unrolled iterative estimation stage in the middle (e.g. from a Residual network) stretches the computation over three layers by first providing a noisy estimate of that representation, but then iteratively refines it over the next to layers. (c) A classic group of three layers can also distribute the computation, but they would produce a new representation at each layer. So the iterative estimation stage in (b) can be seen as a middle ground between a single classic neural network layer, (a), and a multiple classic layers, (c).\nLesioning. If it were true that each layer computes a whole new set of features, then removing a layer from a trained network would completely change the input distribution of the next layer. We would then expect to see the overall performance drop to almost chance level. This is in fact what Veit et al. (2016) find for the 15-layer VGG network on CIFAR-10: removing any layer from the trained network sets the classification error to around 90%. But the lesioning studies conducted on Highway networks Srivastava et al. (2015a) and ResNets Veit et al. (2016) paint an entirely different picture: only a minor drop in performance is observed for any removed layer. This drop is more pronounced for the early layers and the layers that change dimensionality (i.e. number of filter maps and map sizes), but performance is always still far superior to random guessing.\nHuang et al. (2016b) take lesioning one step further and drop out entire ResNet layers as a regularizer during training. They describe their method as \u201c[...] a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time\u201d. The regularization effect of this procedure is explained as inducing an implicit ensemble of many shallow networks akin to normal dropout. Note that this explanation requires a departure from the representation view in that each layer has to cope with the possibility of having its entire input layer removed. Otherwise, most shallow networks in the ensemble would perform no better than chance level, just like the lesioned VGG net.\nReshuffling. The link between layers and representation levels may be most clearly challenged by the experiment in Veit et al. (2016) where the layers of a trained 110-layer ResNet are reshuffled. Remarkably, error increases smoothly with the amount of reshuffling, and many re-orderings result only in a small increase in error. Note, however, that only layers within a stage are reshuffled, since the dimensionality of the swapped layers must match. Veit et al. (2016) take these results as evidence that ResNets behave as ensembles of exponentially many shallow networks."}, {"heading": "3 UNROLLED ITERATIVE ESTIMATION VIEW", "text": "The representation view has guided neural networks research by providing intuitions about the \u201cmeaning\u201d of their computations. So in this section we will augment the representation view to deal with the incongruities and hopefully enable future research on these very deep architectures to reap the same benefits. The target of our modification is the mapping of layers/blocks of the network to levels of abstraction.\nAt this point it is interesting to note that the one-to-one mapping of neural network layers to levels of abstraction is an implicit assumption rather than a stated part of the view.\nA recent deep learning textbook Goodfellow et al. (2016) explicitly states: \u201c[. . . ] the depth flowchart of the computations needed to compute the representation of each concept may be much deeper than the graph of the concepts themselves.\u201d So in a strict sense the evidence from Section 2 does not in fact contradict a representation view of Residual and Highway networks. It only conflicts with the idea that each layer forms a new level of representation. We can therefore reconcile very deep networks with the representation view by explicitly giving up this implicit assumption.\nUnrolled Iterative Estimation. We propose to think of blocks in Highway and Residual networks as performing unrolled iterative estimation of representations. What we mean by that is that the blocks in a stage work together to estimate and iteratively refine a single level of representation. The first layer in that stage therefore already provides a (rough) estimate for the final representation. Subsequent layer in the stage then refine that estimate without changing the level of representation. So if the first layer in a stage detects simple shapes, then the rest of the layers in that stage will work at that level too.\nFeature Identity. A stage performing iterative estimation is different from one that computes a new level of representation at each block because it preserves the feature identity. They operate differently even if their structure and their final representations are equivalent, because of the way they treat intermediate representations. This is illustrated in Figure 2, where the iterative estimation stage, (b), is contrasted with a single classic block (a), and multiple classic blocks, (c). In the iterative estimation case (middle), all the blocks within the stage produce estimates of the same representation (indicated by having different shades of blue). Whereas, in a classical stage, (c), the intermediate representations would all be different (represented by different colors)."}, {"heading": "3.1 HIGHWAY AND RESIDUAL NETWORKS", "text": "Both Highway and Residual networks adress the problem of training very deep architectures by improving the error flow via identity skip connections that allow units to copy their inputs on to the next layer unchanged.\nHighway networks use design principles from Long Short-Term Memory (LSTM). In essence, they can be thought of as a simplified and unrolled LSTM network, with untied weights. For every unit, there is an additional gating unit, which interpolates between the normal, typically non-linear, transformation and a copy of the activation from the corresponding unit in the previous layer.\nLet H(x) be a nonlinear parametric function of the inputs, x, (typically an affine projection followed by pointwise non-linearity). Then a traditional feed-forward network layer can be written as:\ny(x) = H(x).\nBy adding an additional unit, T (x), (i.e. the transform gate) a Highway layer can be written as:\ny(x) = H(x) \u00b7 T (x) + x \u00b7 (1\u2212 T (x)).\nResNets take an even simpler approach by reformulating the desired transformation as the input plus a residual F (x):\ny(x) = F (x) + x\nThe rationale behind this is that it is easier to optimize the residual form than the original function. For the extreme case where the desired function is the identity, this amounts to the trivial task of pushing the residual to zero.\nA good initial estimate for a representation should on average be correct even though it might have high variance. We can thus formalize the notion of \"preserving feature identity\" as being an unbiased estimator for the target representation. This means the units aki in different layers k \u2208 {1 . . . L} are all estimators for the same latent feature Ai. Where Ai refers to the (unknown) value towards which the i-th feature is converging. The unbiased estimator condition can then be written as the expected difference between the estimator and the final feature:\nE x\u2208X [aki \u2212Ai] = 0 (1)\nNote that both the aki s and Ai depend on the samples x of the data-generating distribution X and are thus random variables2.\n2The fact that they both depend on the same x is also the reason we need to keep them within the same expectation and cannot just write E[aki ] = Ai"}, {"heading": "3.2 DERIVING RESIDUAL NETWORKS", "text": "A traditional feedforward network layer can be written as:\ny(x) = H(x),\nwhere H(x) is a nonlinear parametric function of the inputs x (typically an affine projection followed by pointwise non-linearity). ResNets reformulate the nonlinear transformation of a layer as its input x plus a residual F (x):\ny(x) = F (x) + x\nThe rationale behind this is that it is easier to optimize the residual form than the original function. For the extreme case where the desired function is the identity, this amounts to the trivial task of pushing the residual to zero. It does not however answer the question as to why a function close to the identity is desirable.\nEquation 1 can be used to directly derive the ResNet formula, because it immediately follows that the expected difference between two blocks is also zero:\nE[aki \u2212Ai]\u2212 E[ak\u22121i \u2212Ai] = 0 (2) E[aki \u2212 ak\u22121i ] = 0 (3)\nSo instead of preserving Equation 1 for each feature aki , we can just compute it as a combination of ak\u22121i and a zero-mean residual Fi:\naki = a k\u22121 + Fi (4) E[Fi] = 0 (5)\nTo verify that this implies Equation 3 we plug it in and get:\nE[aki \u2212 ak\u22121i ] = E[a k\u22121 + Fi \u2212 ak\u22121i ] = E[Fi] = 0 (6)\nTherefore, if the residual block, F , has a zero mean over the training set, then it can be said to maintain feature identity. This is a reasonable assumption, especially when using batch normalization."}, {"heading": "3.3 DERIVING HIGHWAY NETWORKS", "text": "Highway networks utilize the design principles used by Long Short-Term Memory (LSTM; Hochreiter & Schmidhuber 1997) recurrent networks to solve the vanishing gradient problem. In essence they correspond to a simplified and unrolled LSTM network, with untied weights. For every unit there is an additional gating unit, which interpolates between the normal transformation and copying the activation from the corresponding unit in the previous layer. By adding an additional unit T (x) (the transform gate) we can write a Highway layer as:\ny(x) = H(x) \u00b7 T (x) + x \u00b7 (1\u2212 T (x)).\nThis formula can be directly derived as an alternative way of ensuring Equation 1 if we assume a Hi to be a separate estimate of Ai. For now it does not matter where Hi comes from, but later we will compute it by a layer or small block based on the previous layer. Highways then result from the optimal way to linearly combine the former estimate ak\u22121i with Hi such that the resulting a k i is a minimum variance estimate of Ai. Thus requiring E[aki \u2212Ai] = 0 and that Var[aki \u2212Ai] is minimal.\nLet Var[aki \u2212Ai] = \u03c321 , Var[Hi \u2212Ai] = \u03c322 , and cov[aki \u2212Ai, Hi] = \u03c312. The optimal linear way of combining them is then given by the following estimator (see Section A.1 for derivation):\nak+1i = \u03b12\n\u03b11 + \u03b12 aki + \u03b11 \u03b11 + \u03b12 Hi (7)\nwhere \u03b11 = \u03c321 \u2212 \u03c312 and \u03b12 = \u03c322 \u2212 \u03c312. If we use a neural network to compute Hi and another one to compute Ti = \u03b11\u03b11+\u03b12 , then we recover the Highway formula: aki = Hi \u00b7 Ti + ak\u22121i \u00b7 (1\u2212 Ti), (8) where Hi and Ti are both functions of the previous layer activations ak\u22121."}, {"heading": "4 DISCUSSION", "text": ""}, {"heading": "4.1 IMPLICATIONS FOR HIGHWAY NETWORKS", "text": "In Highway networks with coupled gates the mixing coefficients always sum to one. This ensures that the expectation of the new estimate will always be correct (cf. Equation 11). The precise value of mixing will only determine the variance of the new estimate. We can bound this variance to be less or equal to the variance of the previous layer by restricting both mixing coefficients to be positive. In Highway networks this is done by using the logistic sigmoid activation function for the transform gate Ti. This restriction is equivalent to the assumption of \u03b11 and \u03b12 having the same sign. This assumption holds, for example, if the error of the new estimate Hi \u2212 Ai is independent of the old ak\u22121i \u2212Ai. Because in that case their covariance is zero and thus both alphas are positive. Using the logistic sigmoid as activation function for the transform gate further means that the preactivation of Ti implicitly estimates log(\u03b12\u03b11 ). This is easy to see because the logistic sigmoid of that term is\n1\n1 + elog( \u03b12 \u03b11\n) =\n1\n1 + \u03b12\u03b11 = \u03b11 \u03b11 + \u03b12 . (9)\nFor the simple case of independent estimates (\u03c312 = 0), this gives us another way of understanding the transform gate bias: It controls our initial belief in the variance of the layers estimate as compared to the previous one \u03c3 2 1\n\u03c322 . A low bias means that the layers on average produce a high variance estimate,\nand should thus only contribute little, which seems a reasonable assumption for initialization."}, {"heading": "4.2 EXPERIMENTAL CORROBORATION OF ITERATIVE ESTIMATION VIEW", "text": "The primary prediction of the iterative estimation view is that the estimation error for Highway or Residual blocks within the same stage should be zero in expectation. To empirically test this claim, we extract the intermediate layer outputs for 5000 validation set images using the 50-layer ResNet trained on the ILSVRC-2015 dataset from He et al. (2015). These are then used to compute the empirical mean and standard deviation of the estimation error over the validation subset, for all blocks in the four Residual stages in the network. Finally the mean of the empirical mean and standard deviation is computed over the three spatial dimensions.\nFigure 3 shows that for the first three stages, the mean estimation error is indeed close to zero. This indicates that it valid to interpret the role of Residual blocks in this network as that of iteratively refining a representation. Moreover, in each stage the standard deviation of the estimation error decreases over successive blocks, indicating the convergence of the refinement procedure. We note that stage four (with three blocks) appears to be underestimating the representation values, indicating a probable weak link in the architecture."}, {"heading": "4.3 STAGES", "text": "ResNet-151 (He et al., 2015) and many other derived architectures share some common characteristics: They are divided into stages of Residual blocks that share the same dimensionality. In between these stages the input dimensionality changes, typically by down-sampling and an increase in the number of channels. These stages typically also increase in length: the first one comprises few layers, while the last stage has many.\nWe can now interpret these design choices from an iterative estimation point of view. From that perspective the level of representation stays the same within each stage, through the use of identity shortcut connections. Between stages the level of representation is changed by the use of a projection to change dimensionality. That means that we expect the type of features that are detected to be very similar within a stage and jump in abstraction between stages. From there it is also clear that the first few stages should be shorter, since early representations tend to be relatively simple, and thus they need little iterative refinement. The features of later stages on the other hand are likely complex with numerous interdependencies and therefore benefit more from iterative refinement."}, {"heading": "4.4 REVISITING THE EVIDENCE", "text": "Staying Close to the Inputs. When iteratively re-estimating a variable, staying close to the old value should be a more common operation than changing it significantly. This is the reason why the ResNet formulation makes sense: Learning the identity is hard, and it is needed frequently. It also explains sparse transform gate activity in trained Highway networks: These networks learn to dynamically and selectively update individual features, while keeping most of the representation intact.\nLesioning. Another implication of the iteration view is that layers become incremental and somewhat interchangeable. Each layer (apart from the first) refines an already reasonable estimate for the representation. From that it follows directly that removing layers, like in the lesioning experiments, should have only a mild effect on the final result. Because removing them does not change the overall representation the next layer receives, only its quality. So the following layer can still perform mostly the same operation, even with a somewhat noisy input. Layer dropout (Huang et al., 2016b) amplifies this effect by explicitly training the network to work with a variable number of iterations. By dropping random layers it further penalizes iterations relying on each other, which could be another explanation for the regularization effect of the technique.\nShuffling. The layers within a stage should also be interchangeable to a certain degree, because they all work with the same input and output representations. Of course, this exchangeability is not without limitations: The network could learn to depend on a specific order of refinements, which would be disturbed by shuffling and lesioning. But we would expect these effects to be moderate, which is indeed what has been found in Srivastava et al. (2015a); Veit et al. (2016)."}, {"heading": "5 COMPARATIVE CASE STUDIES", "text": "The preceding sections show that we can construct both Highway and Residual architectures mathematically grounded in learning unrolled iterative estimation. The common feature between these architectures is that they preserve feature identities, and the primary difference is that they have different biases towards switching feature identities.Unfortunately, since our current understanding of the computations required to solve complex problems is limited, it is extremely hard to say a priori which architecture may be more suitable for which type of problems. Therefore, in this section we perform two case studies comparing and contrasting their behavior experimentally. The studies are each based on applications for which Residual and Highway layers respectively have been effective."}, {"heading": "5.1 IMAGE CLASSIFICATION", "text": "Deep Residual networks outperformed all other entries at the 2016 ImageNet classification challenge. In this study we compare the performance of 50-layer convolutional Highway and Residual networks for ImageNet classification. Note that our aim is not to examine the importance of depth for this\ntask\u2014shallower networks have already outperformed deep Residual networks on all original Residual network benchmarks (Huang et al., 2016a; Szegedy et al., 2016). Instead, our goal is to fairly compare the two architectures, and test the following claims regarding deep convolutional Highway networks (He et al., 2015; 2016; Veit et al., 2016):\n1. They are harder to train, leading to stalled training or poor results. 2. They require extensive tuning of the initial bias, and even then produce much worse results\ncompared to Residual networks. 3. They are wasteful in terms of parameters since they utilize extra learned gates, doubling the\ntotal parameters for the same number of units compared to a Residual layer.\nWe train a 50-layer convolutional Highway network based on the 50-layer Residual network from He et al. (2015). The design of the two networks are identical (including use of batch normalization after every convolution operation), except that unlike Residual blocks, the Highway blocks use two sets of layers to learn both H and T and then combine them using the coupled Highway formulation. As proposed initially for Highway layers, both H and T are learned using the same receptive fields and same number of parameters, and the transform gate biases are set to \u22121 at the start of training. For fair comparison, the number of feature maps throughout the Highway network is reduced such that the total number of parameters is close to the Residual network. The training algorithm and learning rate schedule were kept the same as those used for the Residual network.\nThe plots in Figure 4 show that the Residual network fits the data better\u2014its final training loss is lower than the Highway network. The final performance of both networks on the validation set is very similar, with the Residual network producing a slightly better top-5 classification error of 7.26% vs. 7.59% for the Highway network. These results contradict claims 1 and 2 above, since the Highway network is easy to train without requiring any bias tuning. However, there is some support for claim 3 since the Highway network appears to slightly underfit compared to the Residual network, indicating that it has lower capacity for the same number of parameters.\nImportance of Expressive Gating. The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates. For experiments with Highway networks (which they refer to as Residual networks with exclusive gating) He et al. (2016) used 1 \u00d7 1 convolutions for the transform gate, instead of having the same receptive fields for the gates as the primary transformation (H), as done by Srivastava et al. (2015a). This change in design appears to be the primary cause of instabilities in learning since the gates can no longer function effectively. Therefore, it is important to use equally expressive transformations for H and T in Highway networks.\nRole of Batch Normalization. Since both architectures have better inductive biases and ease of optimization compared to plain networks built-in, it is interesting to understand what role batch normalization plays in training these networks. For this purpose, we retrain both networks above without any batch normalization and plot the results in Figure 5.\nThe Highway network now fits the data better, resulting in lower training loss. This contradicts claim 3, since a Highway network with the same number of parameters as a Residual network demonstrates slightly higher capacity. On the other hand it produces a higher validation error\u201410.00% vs. 9.49% for the Residual network\u2014indicating a clear case of overfitting. This performance gap should be attributed to batch normalization and its interaction with the architectures. It is interesting to note though, that batch normalization is not necessary for training and does not speed up learning for these networks. It reduces sensitivity to initialization and provides some regularization benefits, but no improvements in training."}, {"heading": "5.2 LANGUAGE MODELING", "text": "Next we compare different functional forms (or variants) of the Highway network formulation for the case of character-aware language modeling. Kim et al. (2015) have shown that utilizing a few Highway fully connected layers instead of conventional plain layers improves model performance for a variety of languages. The architecture consists of a stack of convolutional layers followed by Highway layers and then an LSTM layer which predicts the next word based on the history. Similar architectures have since been utilized for obtaining substantial improvements for large-scale language modeling (Jozefowicz et al., 2016) and character level machine translation (Lee et al., 2016). Highway layers with coupled gates have been used in all these studies.\nOnly two to four Highway layers were necessary to obtain significant modeling improvements in the studies above. Thus, it is reasonable to assume that the central advantage of using Highway layers for this task is not easing of credit assignment over depth, but an improved modeling bias. To test how well Residual and other variants of Highway networks perform, we compare several language models trained on the Penn Treebank dataset using the same setup and code provided by Kim et al. (2015). We use the LSTM-Char-Large model, only changing the two Highway layers to different variants. The following variants are tested:\nFull The original Highway formulation based on the LSTM cell. We note that this variant uses more parameters than the others, since changing the layer size to reduce parameters would affect the rest of the network architecture as well.\nCoupled The most commonly used Highway variant, derived in Section 3.3. C-Only A Highway variant with a carry gate but no transform gate (always set to one). T-Only A Highway variant with a transform gate but no carry gate (always set to one). Residual The Residual form from He et al. (2015), in which both transform and carry gate are\nalways one. For this variant we use four layers instead of two, to match the amount of computation/parameters of the other variants.\nThe test set perplexity of each model is shown in Table 1. We find that the the Full, Coupled and C-Only variants have similar performance, better than the T-Only variant and substantially better than the Residual variant. The Residual variant results in performance close to that obtained by\nusing a single plain layer, even though four Residual layers are used. Learned gating of the identity connection is crucial for improving performance for this task.\nRecall that the Highway layers transform character-aware representations before feeding them into an LSTM layer. Thus the non-contextual word-level representations resulting from the convolutional layers are transformed into representations better suited for contextual language modeling. Since it is unlikely that the entire representation needs to change completely, this setting fits well with the iterative estimation perspective.\nInterestingly, Table 1 shows a significant advantage for all variants with a multiplicative gate on the inputs. These results suggest that in this setting it is crucial to dynamically replace parts of the input representation. Some features need to be changed drastically conditioned on other detected features such as word type while other features need to be retained. As a result, even though Residual networks are compatible with iterative estimation, they may not be the best choice for tasks where mixing adaptive feature transform/replacement and reuse is required."}, {"heading": "6 CONCLUSION", "text": "This paper offers a new perspective on Highway and Residual networks as performing unrolled iterative estimation. As an extension of the popular representation view, it stands in contrast to the optimization perspective from which these architectures have originally been introduced. According to the new view, successive layers (within a stage) cooperate to compute a single level of representation. Therefore, the first layer already computes a rough estimate of that representation, which is then iteratively refined by the successive layers. Unlike layers in a conventional neural network, which each compute a new representation, these layers therefore preserve feature identity.\nWe have further shown that both Residual and Highway networks can be directly derived from this new perspective. This offers a unified theory from which these architectures can be understood as two approaches to the same problem. This view further provides a framework from which to understand several surprising recent findings like resilience to lesioning, benefits of layer dropout, and the mild negative effects of layer reshuffling. Together with the derivations these results serve as compelling evidence for the validity of our new perspective.\nMotivated by their conceptual similarities we set out to compare Highway and Residual networks. In preliminary experiments we found that they give very similar results for networks of equal size, thus refuting some claims that Highway networks would need more parameters, or that any form of gating impairs the performance of Residual networks. In another example, we found non-gated identity skip-connections to perform significantly worse, and offered a possible explanation: If the task requires dynamically replacing individual features, then the use of gating is beneficial.\nThe preliminary evidence presented in this report is meant as a starting point for further investigation. We hope to inspire further investigations about the advantages and disadvantages of both approaches. We hope that the unrolled iterative estimation perspective will provide valuable intuitions to help guide research into understanding, improving and possibly combining these exciting techniques."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors wish to thank Faustino Gomez, Bas Steunebrink, Jonathan Masci, Sjoerd van Steenkiste and Christian Osendorfer for their feedback and support. This research was supported by the EU project \u201cINPUT\u201d (H2020-ICT-2015 grant no. 687795)."}, {"heading": "A DERIVATION", "text": "A.1 OPTIMAL LINEAR ESTIMATOR\nAssume two random variables A and B that are both noisy measurements of a third (latent) random variable C: E[A\u2212 C] = E[B \u2212 C] = 0 (10) We call the corresponding variances Var[A \u2212 C] = \u03c32A and Var[B \u2212 C] = \u03c32B and covariance cov[A,B] = \u03c32AB .\nWe are looking for the linear estimator q(A,B) = q0+q1A+q2B of C with E[q\u2212C] = 0 (unbiased) that has minimum variance.\nE[q(A,B)\u2212 C] = 0 E[q0 + q1A+ q2B \u2212 C] = 0\nE[q0 + q1A\u2212 q1C + q2B \u2212 q2C + (q1 + q2 \u2212 1)C] = 0 E[q0 + q1(A\u2212 C) + q2(B \u2212 C) + (q1 + q2 \u2212 1)C] = 0\nq0 + (q1 + q2 \u2212 1)E[C] = 0 E[C](1\u2212 q1 \u2212 q2) = q0\nfor all E[C] which is possible iff:\nq0 = 0 and q1 + q2 = 1. (11)\nThe second condition about minimal variance thus reduces to:\nminimize q1,q2 Var[q1A+ q2B \u2212 C]\nsubject to q1 + q2 = 1\nWe can solve this using Lagrangian multipliers. For that we need to take the derivative of the following term w.r.t. q1, q2 and \u03bb and set them to zero:\nVar[q1A+ q2B \u2212 C]\u2212 \u03bb(q1 + q2 \u2212 1))\nThe first equation is therefore:\nd\ndq1 (Var[q1A+ q2B \u2212 C]\u2212 \u03bb(q1 + q2 \u2212 1)) = 0\nd\ndq1 Var[q1A+ q2B \u2212 C]\u2212 \u03bb = 0\nd\ndq1 Var[q1(A\u2212 C) + q2(B \u2212 C)]\u2212 \u03bb = 0\nd\ndq1 (q21 Var[A\u2212 C] + 2q1q2 cov[A\u2212 C,B \u2212 C])\u2212 \u03bb = 0\n2q1\u03c3 2 A + 2q2\u03c3 2 AB \u2212 \u03bb = 0\nAnalogously we get: 2q2\u03c3 2 B + 2q1\u03c3 2 AB \u2212 \u03bb = 0\nand: q1 + q2 = 1\nSolving these equations gives us:\nq1 = \u03c32B \u2212 \u03c32AB\n\u03c32A \u2212 2\u03c32AB + \u03c32B (12)\nq2 = \u03c32A \u2212 \u03c32AB\n\u03c32A \u2212 2\u03c32AB + \u03c32B (13)\n(14)\nWe can write our estimator in terms of \u03b11 = \u03c32B \u2212 \u03c32AB and \u03b12 = \u03c32A \u2212 \u03c32AB :\nq = \u03b11\n\u03b11 + \u03b12 A+ \u03b12 \u03b11 + \u03b12 B"}], "references": [{"title": "Multi-Residual Networks", "author": ["Abdi", "Masoud", "Nahavandi", "Saeid"], "venue": "[cs],", "citeRegEx": "Abdi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abdi et al\\.", "year": 2016}, {"title": "Deep Learning Methods and Applications", "author": ["Deng", "Li", "Yu", "Dong"], "venue": "Foundations and Trends in Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "Deep Learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "[cs],", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen, pp", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Densely Connected Convolutional Networks. arXiv:1608.06993 [cs], August 2016a", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep Networks with Stochastic Depth", "author": ["Huang", "Gao", "Sun", "Yu", "Liu", "Zhuang", "Sedra", "Daniel", "Weinberger", "Kilian"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["Hubel", "David H", "Wiesel", "Torsten N"], "venue": "The Journal of physiology,", "citeRegEx": "Hubel et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Hubel et al\\.", "year": 1962}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "author": ["Lee", "Jason", "Cho", "Kyunghyun", "Hofmann", "Thomas"], "venue": "arXiv preprint arXiv:1610.03017,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2015}, {"title": "Training Very Deep Networks", "author": ["Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "Juergen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Highway Networks. arXiv:1505.00387 [cs], May 2015b", "author": ["Srivastava", "Rupesh Kumar", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Inception-v4, InceptionResNet and the Impact of Residual Connections on Learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"], "venue": "[cs],", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Residual Networks are Exponential Ensembles of Relatively Shallow Networks", "author": ["Veit", "Andreas", "Wilber", "Michael", "Belongie", "Serge"], "venue": "[cs],", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": ", 2015a) and Residual networks (He et al., 2015) which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent.", "startOffset": 31, "endOffset": 48}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf.", "startOffset": 122, "endOffset": 184}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)).", "startOffset": 140, "endOffset": 250}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth.", "startOffset": 140, "endOffset": 562}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth. We refer to this way of thinking about neural networks as the representation view, which probably dates back to Hubel & Wiesel (1962). The representation view links the layers in a network to the abstraction levels of their representations, and as such represents a pervasive assumption in many recent publications including He et al.", "startOffset": 140, "endOffset": 820}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth. We refer to this way of thinking about neural networks as the representation view, which probably dates back to Hubel & Wiesel (1962). The representation view links the layers in a network to the abstraction levels of their representations, and as such represents a pervasive assumption in many recent publications including He et al. (2015) who describe the success of their Residual networks like this: \u201cSolely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.", "startOffset": 140, "endOffset": 1028}, {"referenceID": 3, "context": "The latter have been widely successful, advancing the state of the art on many benchmarks and winning several pattern recognition competitions (He et al., 2015), while Highway networks have been used to improve language modeling and translation (Kim et al.", "startOffset": 143, "endOffset": 160}, {"referenceID": 11, "context": ", 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016).", "startOffset": 92, "endOffset": 128}, {"referenceID": 12, "context": ", 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016).", "startOffset": 92, "endOffset": 128}, {"referenceID": 17, "context": "For example, it has been reported that removing almost any layer from a trained Residual or Highway network has only minimal effect on its overall performance (Srivastava et al., 2015b; Veit et al., 2016).", "startOffset": 159, "endOffset": 204}, {"referenceID": 17, "context": "It has been argued that ResNets are better understood as ensembles of shallow networks (Huang et al., 2016b; Veit et al., 2016; Abdi & Nahavandi, 2016).", "startOffset": 87, "endOffset": 151}, {"referenceID": 3, "context": "2015a, May) and Residual networks (ResNets; He et al. 2015, December). The latter have been widely successful, advancing the state of the art on many benchmarks and winning several pattern recognition competitions (He et al., 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016). Both architectures have been introduced with the explicit goal of training deeper models. There are, however, some surprising findings that seem to contradict the applicability of the representation view to these very deep networks. For example, it has been reported that removing almost any layer from a trained Residual or Highway network has only minimal effect on its overall performance (Srivastava et al., 2015b; Veit et al., 2016). This idea has been extended to a layerwise dropout, as a regularizer for ResNets (Huang et al., 2016b). But if each layer supposedly builds a new level of representation from the previous one, then removing any layer should critically disrupt the input for the following layer. So how is it possible that doing so seems to have only a negligible effect on the network output? Veit et al. (2016) even demonstrated that shuffling some of the layers in a trained ResNet barely affects performance.", "startOffset": 44, "endOffset": 1188}, {"referenceID": 14, "context": "The analysis in Srivastava et al. (2015a) shows that in trained Highway networks, the activity of the transform gates is often sparse for each individual sample, while their average activity over all training samples is non-sparse.", "startOffset": 16, "endOffset": 42}, {"referenceID": 3, "context": "We refer to the building blocks of a ResNet\u2014a few layers with an identity skip connection\u2014as a Residual block (He et al., 2015).", "startOffset": 110, "endOffset": 127}, {"referenceID": 13, "context": "This is in fact what Veit et al. (2016) find for the 15-layer VGG network on CIFAR-10: removing any layer from the trained network sets the classification error to around 90%.", "startOffset": 21, "endOffset": 40}, {"referenceID": 12, "context": "But the lesioning studies conducted on Highway networks Srivastava et al. (2015a) and ResNets Veit et al.", "startOffset": 56, "endOffset": 82}, {"referenceID": 12, "context": "But the lesioning studies conducted on Highway networks Srivastava et al. (2015a) and ResNets Veit et al. (2016) paint an entirely different picture: only a minor drop in performance is observed for any removed layer.", "startOffset": 56, "endOffset": 113}, {"referenceID": 7, "context": "Huang et al. (2016b) take lesioning one step further and drop out entire ResNet layers as a regularizer during training.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "The link between layers and representation levels may be most clearly challenged by the experiment in Veit et al. (2016) where the layers of a trained 110-layer ResNet are reshuffled.", "startOffset": 102, "endOffset": 121}, {"referenceID": 17, "context": "The link between layers and representation levels may be most clearly challenged by the experiment in Veit et al. (2016) where the layers of a trained 110-layer ResNet are reshuffled. Remarkably, error increases smoothly with the amount of reshuffling, and many re-orderings result only in a small increase in error. Note, however, that only layers within a stage are reshuffled, since the dimensionality of the swapped layers must match. Veit et al. (2016) take these results as evidence that ResNets behave as ensembles of exponentially many shallow networks.", "startOffset": 102, "endOffset": 458}, {"referenceID": 2, "context": "A recent deep learning textbook Goodfellow et al. (2016) explicitly states: \u201c[.", "startOffset": 32, "endOffset": 57}, {"referenceID": 3, "context": "To empirically test this claim, we extract the intermediate layer outputs for 5000 validation set images using the 50-layer ResNet trained on the ILSVRC-2015 dataset from He et al. (2015). These are then used to compute the empirical mean and standard deviation of the estimation error over the validation subset, for all blocks in the four Residual stages in the network.", "startOffset": 171, "endOffset": 188}, {"referenceID": 3, "context": "ResNet-151 (He et al., 2015) and many other derived architectures share some common characteristics: They are divided into stages of Residual blocks that share the same dimensionality.", "startOffset": 11, "endOffset": 28}, {"referenceID": 14, "context": "But we would expect these effects to be moderate, which is indeed what has been found in Srivastava et al. (2015a); Veit et al.", "startOffset": 89, "endOffset": 115}, {"referenceID": 14, "context": "But we would expect these effects to be moderate, which is indeed what has been found in Srivastava et al. (2015a); Veit et al. (2016).", "startOffset": 89, "endOffset": 135}, {"referenceID": 16, "context": "task\u2014shallower networks have already outperformed deep Residual networks on all original Residual network benchmarks (Huang et al., 2016a; Szegedy et al., 2016).", "startOffset": 117, "endOffset": 160}, {"referenceID": 3, "context": "Instead, our goal is to fairly compare the two architectures, and test the following claims regarding deep convolutional Highway networks (He et al., 2015; 2016; Veit et al., 2016):", "startOffset": 138, "endOffset": 180}, {"referenceID": 17, "context": "Instead, our goal is to fairly compare the two architectures, and test the following claims regarding deep convolutional Highway networks (He et al., 2015; 2016; Veit et al., 2016):", "startOffset": 138, "endOffset": 180}, {"referenceID": 3, "context": "We train a 50-layer convolutional Highway network based on the 50-layer Residual network from He et al. (2015). The design of the two networks are identical (including use of batch normalization after every convolution operation), except that unlike Residual blocks, the Highway blocks use two sets of layers to learn both H and T and then combine them using the coupled Highway formulation.", "startOffset": 94, "endOffset": 111}, {"referenceID": 3, "context": "The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates.", "startOffset": 66, "endOffset": 83}, {"referenceID": 3, "context": "The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates. For experiments with Highway networks (which they refer to as Residual networks with exclusive gating) He et al. (2016) used 1 \u00d7 1 convolutions for the transform gate, instead of having the same receptive fields for the gates as the primary transformation (H), as done by Srivastava et al.", "startOffset": 66, "endOffset": 295}, {"referenceID": 3, "context": "The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates. For experiments with Highway networks (which they refer to as Residual networks with exclusive gating) He et al. (2016) used 1 \u00d7 1 convolutions for the transform gate, instead of having the same receptive fields for the gates as the primary transformation (H), as done by Srivastava et al. (2015a). This change in design appears to be the primary cause of instabilities in learning since the gates can no longer function effectively.", "startOffset": 66, "endOffset": 473}, {"referenceID": 10, "context": "Similar architectures have since been utilized for obtaining substantial improvements for large-scale language modeling (Jozefowicz et al., 2016) and character level machine translation (Lee et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 12, "context": ", 2016) and character level machine translation (Lee et al., 2016).", "startOffset": 48, "endOffset": 66}, {"referenceID": 10, "context": "Kim et al. (2015) have shown that utilizing a few Highway fully connected layers instead of conventional plain layers improves model performance for a variety of languages.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "Similar architectures have since been utilized for obtaining substantial improvements for large-scale language modeling (Jozefowicz et al., 2016) and character level machine translation (Lee et al., 2016). Highway layers with coupled gates have been used in all these studies. Only two to four Highway layers were necessary to obtain significant modeling improvements in the studies above. Thus, it is reasonable to assume that the central advantage of using Highway layers for this task is not easing of credit assignment over depth, but an improved modeling bias. To test how well Residual and other variants of Highway networks perform, we compare several language models trained on the Penn Treebank dataset using the same setup and code provided by Kim et al. (2015). We use the LSTM-Char-Large model, only changing the two Highway layers to different variants.", "startOffset": 121, "endOffset": 772}, {"referenceID": 3, "context": "Residual The Residual form from He et al. (2015), in which both transform and carry gate are always one.", "startOffset": 32, "endOffset": 49}, {"referenceID": 11, "context": "Table 1: Comparison of various variants of the Highway formulation for character-aware neural language models introduced by Kim et al. (2015).", "startOffset": 124, "endOffset": 142}], "year": 2016, "abstractText": "The past year saw the introduction of new architectures such as Highway networks (Srivastava et al., 2015a) and Residual networks (He et al., 2015) which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer. In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation\u2014a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of Highway and Residual networks. Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.", "creator": "LaTeX with hyperref package"}}}