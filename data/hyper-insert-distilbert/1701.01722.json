{"id": "1701.01722", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2017", "title": "Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU", "abstract": "a matrix dependent multiplicative elastic weight invariant update ( stylized mmwu ) is an albeit extremely equally powerful abstract algorithmic tool meant for incorporating computer modeling science knowledge and related fields. ultimately however, it comes with both a dramatically slow running time due to dominating the matrix exponential and eigendecomposition efficient computations. for this reason, many influential researchers studied the variance followed - after the - field perturbed - leader ( ftpl ) optimal framework equation which apparently is faster, but a factor $ \\ sqrt { & d } $ worse than obtaining the optimal regret of standard mmwu parameters for arbitrary dimension - $ d $ 512 matrices.", "histories": [["v1", "Fri, 6 Jan 2017 18:43:53 GMT  (582kb,D)", "http://arxiv.org/abs/1701.01722v1", null], ["v2", "Wed, 14 Jun 2017 18:18:21 GMT  (1118kb,D)", "http://arxiv.org/abs/1701.01722v2", null], ["v3", "Mon, 18 Sep 2017 01:04:08 GMT  (1119kb,D)", "http://arxiv.org/abs/1701.01722v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS math.OC stat.ML", "authors": ["zeyuan allen-zhu", "yuanzhi li"], "accepted": true, "id": "1701.01722"}, "pdf": {"name": "1701.01722.pdf", "metadata": {"source": "CRF", "title": "Follow the Compressed Leader: Faster Algorithms for Matrix Multiplicative Weight Updates", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "emails": ["zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu"], "sections": [{"heading": null, "text": "\u221a d\nworse than the optimal regret of MMWU for dimension-d matrices. In this paper, we propose a followed-the-compressed-leader framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs even faster than FTPL. Our main idea is to \u201ccompress\u201d the matrix exponential computation to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16].\nar X\niv :1\n70 1.\n01 72\n2v 1\n[ cs\n.L G\n] 6\nJ an"}, {"heading": "1 Introduction", "text": "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]). Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].\nTo discuss MMWU on an abstract level, let us consider the online linear optimization problem.\nOnline Matrix Optimization. Let \u2206d = {U \u2208 Rd\u00d7d |TrU = 1 \u2227 U 0} be the set of density matrices in dimension d.1 Consider the following online game between a player and an adversary. The player plays T actions W1, . . . ,WT \u2208 \u2206d in a row; after playing Wk, the adversary picks a feedback matrix Ak \u2208 Rd\u00d7d that is symmetric and satisfies 0 A I (both these assumptions are for the sake of simplicity and can be removed2); this Ak may depend on W1, . . . ,Wk. The player receives a gain Ak \u2022Wk def= Tr(AkWk) \u2208 [0, 1]. The regret minimization problem asks us the player to design a strategy to minimize regret, that is, the difference between the total gain obtained by the player and that by the a posteriori best fixed strategy U \u2208 \u2206d:\nminimize max U\u2208\u2206d\n\u2211T k=1Ak \u2022 (U\u2212Wk) = \u03bbmax ( A1 + \u00b7 \u00b7 \u00b7+ AT )\u2212 \u2211T k=1Ak \u2022Wk .\nThe MMWU strategy chooses Wk = exp(\u2212\u03b7\u03a3k\u22121) Tr exp(\u2212\u03b7\u03a3k\u22121) where \u03a3k\u22121 def = A1 + \u00b7 \u00b7 \u00b7 + Ak\u22121 and \u03b7 >\n0 is the so-called learning rate. The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9]. Some authors also refer to MMWU as the follow-the-regularized-leader strategy or FTRL for short, because MMWU can be analyzed from a mirror-descent view with the matrix entropy function as its regularizer [7].\nOnline Eigenvector Problem. If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:\nminimize \u03bbmax ( A1 + \u00b7 \u00b7 \u00b7+ AT )\u2212 \u2211T k=1w > k Akwk .\nThe name comes from the fact that the player chooses only vectors in a row, but wants to compete against the leading eigenvector in hindsight. To make this problem meaningful, the feedback matrix Ak, is not allowed to depend on wk but can depend on w1, . . . , wk\u22121. This more challenging setting is very desirable for multiple reasons:\n\u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate. \u2022 as we shall see next, online eigenvector is more general than online matrix optimization because\nthe player can achieve the same regret by playing a full rank matrix Wk or by simply playing wkw > k , as long as Ak does not depend on Wk.\n1We denote by A B spectral dominance that is equivalent to saying that A\u2212B is positive semidefinite (PSD). 2Firstly, all the results cited and stated in this paper, after scaling, generalize to the scenario when the eigenvalues of Ak are in the range [l, r] for arbitrary l, r \u2208 R. For notational simplicity, we have assumed l = 0 and r = 1 in this paper. Secondly, if Ak is not symmetric or even rectangular, classical reductions can turn such a problem into an equivalent online game with only symmetric matrices [16, Sec 2.1].\naThe total time complexity of the first T\u03b5 rounds where T\u03b5 is the earliest round to achieve an \u03b5 average regret.\nKnown Approach 1 (Better Regret). Applying the same MMWU strategy but randomly yields the same total O( \u221a T log d) regret for the online eigenvector problem. Indeed, given an eigendecomposition Wk = exp(\u2212\u03b7\u03a3k\u22121) Tr exp(\u2212\u03b7\u03a3k\u22121) = \u2211d\nj=1 pj \u00b7 yjy>j where vectors yj are normalized eigenvectors, the player can play each yj with probability pj . This gives O( \u221a T log d) total regret in expectation.3 Unfortunately, the per-iteration running time of this method is O(d3) due to eigendecomposition.4\nSome researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable. They define a sketch matrix Y = W 1/2 k Q using a random Q \u2208 Rd\u00d7m and then use YY> to approximate Wk. The average regret loss is \u03c3 if the dimension m = O\u0303(1/\u03c32). We call this method MMWU-JL for short. Since \u03c3 must be around T\u22121/2, MMWU-JL still runs slowly per iteration, see Table 1.5\nKnown Approach 2 (Faster Run Time). Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21]. Most notably, Garber, Hazan and Ma [16] proposed to compute an (approximate) leading eigenvector of the matrix \u03a3k\u22121 +rr> at iteration k, where r is a random vector whose norm is carefully chosen. They showed that the total regret of FTPL is O\u0303( \u221a dT ), which is interesting only when T \u2265 d and is a factor \u221a d worse than the optimum regret. The per-iteration cost of FTPL is only O\u0303 ( T 3 4d\u2212 1 4 nnz(\u03a3T ) ) because computing the leading eigenvector is faster than eigendecomposition. (We use nnz(M) to denote the time needed to multiply M to a vector.)"}, {"heading": "1.1 Our Main Results", "text": "We propose a follow-the-compressed-leader (FTCL) strategy that, at a high level, compresses the MMWU strategy to dimension m = 3 as opposed to dimension m = \u0398\u0303(1/\u03c32) = \u0398\u0303(T ) in MMWU-\n3It requires some additional, but standard, effort to turn this into a high-confidence result. 4The best eigendecomposition algorithm runs in time O(d3) as opposed to O(d\u03c9). 5More specifically, the computation W\n1/2 k Q becomes m computations of exp(\u2212\u03b7\u03a3k\u22121/2) applied to vectors, and\nthis can be done using Chebyshev approximation with a total running time O\u0303 ( \u03c3\u22122 \u00b7 \u2016\u03b7\u03a3k\u22121\u20161/22 \u00b7 nnz(\u03a3T ) ) . The optimal choice \u03b7 will imply \u2016\u03b7\u03a3k\u22121\u20162 \u2264 T 1/2, so this totals to O\u0303(T 5/4nnz(\u03a3T )) in the per-iteration running time.\nJL. Our FTCL strategy has significant advantages over previous results because:\n\u2022 FTCL has regret O\u0303( \u221a T ) which is optimal up to poly-log factors (as opposed to \u221a d in FTPL). \u2022 FTCL, in its basic form, has a per-iteration complexity O\u0303 ( T 1 4 nnz(\u03a3T ) ) which is already faster\nthan MMWU-JL by a factor \u2126\u0303(T ) and than FTPL by a factor \u2126\u0303(T 1/2d\u22121/4) \u2265 \u2126\u0303(T 1/4). \u2022 FTCL, after using optimization techniques to speed it up, has a per-iteration complexity\nO\u0303 ( T 1 4 nnz(\u03a3T ) 3 4 nnz(A) 1 4 + nnz(\u03a3T ) ) , where nnz(A) def = maxi\u2208[T ]{nnz(Ai)} \u2264 nnz(\u03a3T ) .\nWe compare our running time to known results in Table 1 in full. We stress here that a direct comparison in per-iteration complexity between FTCL (or MMWU) and FTPL is unfair, because FTPL requires d times more iterations in order to achieve the same average regret as FTCL (or MMWU). For this reason, in the last column of Table 1, we also summarize the minimum total time complexity needed to achieve an \u03b5 average regret.\nExamples. If nnz(\u03a3T ) = d 2 and nnz(A) = O(d), the per-iteration running time comparison is\nO\u0303(d2+d1.75T 0.25) (by us) vs. O\u0303(T 0.75d1.75) (by FTPL, only T \u2265 d) or O(d3)(by MMWU) . We also compare the total complexity needed to achieve an \u03b5 average regret:\nO\u0303(d2\u03b5\u22122 + d1.75\u03b5\u22122.5) (by us) vs. O\u0303(d2\u03b5\u22124.5) (by MMWU-JL) or O\u0303(d3\u03b5\u22122) (by MMWU) ."}, {"heading": "1.2 Our Side Result: Stochastic Online Eigenvector", "text": "Our compression idea also gives rise to a faster algorithm for the special case of the online eigenvector problem where the adversary is stochastic, meaning that A1, . . . ,AT are chosen i.i.d. from a common distribution whose expectation equals some matrix B, independent of the player\u2019s actions.\nFor this problem, Garber, Hazan, and Ma [16] showed that a block power method matches the optimum regret and enjoys an efficient O(nnz(\u03a3T ))-time implementation per iteration. Shamir [30] analyzed the so-called Oja\u2019s algorithm but his total regret is O( \u221a dT log(T )) which is a factor \u221a d worse than optimum.6\nIn this paper, we show by showing that Oja\u2019s algorithm in fact only has a total regretO( \u221a T log d)\nfor the stochastic online eigenvector problem, which is optimal up to a \u221a\nlog d factor. Most importantly, the per-iteration complexity of Oja\u2019s is only O(nnz(A)).\nExample. If nnz(\u03a3T ) = d 2 and nnz(A) = O(d), our running time is O(d) times faster than [16].\nOur proof relies on a FTCL view of Oja\u2019s algorithm which compresses MMWU to dimension m = 1. Our proof is less than one page, and essentially three-lined. This indicates that FTCL might be a better way of thinking about these type of problems."}, {"heading": "1.3 Our Stronger Results in a More Refined Language", "text": "Denoting by \u03bb def = 1T \u03bbmax(A1 + \u00b7 \u00b7 \u00b7AT ), we have \u03bb \u2264 1 according to the normalization Ak I. In general, the smaller the value \u03bb is, the better a learning algorithm should behave. In the previous subsections, we have followed the tradition and discussed our results and prior works assuming the worst possibility of \u03bb. This has indeed simplified notations.\n6In the special case of Ak being rank-1, the O\u0303( \u221a T ) regret for Oja\u2019s algorithm was recently shown by [6], using\ndifferent techniques from us.\nIf \u03bb is much smaller than 1, our complexity bounds can be improved to quantities that depend on \u03bb. We call this the \u03bb-refined language. We restate our FTCL results for this language in Table 2. At a high level, for our FTCL, in both the adversarial and stochastic settings,\n\u2022 the total regret formula improves from O\u0303( \u221a T ) to O\u0303( \u221a \u03bbT ); and\n\u2022 for obtaining the same average regret, \u2013 the necessary number of iterations reduces by a factor \u03bb; and \u2013 the total running time improves by a factor between \u03bb and \u03bb3/2.\nWe emphasize that there is an information-theoretic lower bound of \u2126( \u221a \u03bbT ) for the total regret in this \u03bb-refined language, see Appendix I. This lower bound even holds for the simpler stochastic online eigenvector problem, even when the matrices Ak are of rank 1.\nAs for prior works, it has been recorded that (cf. [7, Theorem 3.1]) the MMWU method (or the MMWU-JL) has a total regret of only O( \u221a \u03bbT log d) as opposed to O( \u221a T log d) in this \u03bbrefined language. The running time of MMWU therefore improves by a factor of \u03bb. The total time complexity of MMWU-JL improves only by a factor of \u03bb1/4.7\nThe block power method (for the stochastic online eigenvector problem) can also be analyzed in this \u03bb-refined language, for instance by modifying the proof in [16]. This improves the total regret to O\u0303( \u221a \u03bbT ). To the best of our knowledge, FTPL has not been analyzed in the \u03bb-refined language (and even if it has, the resulting time complexity must be outperformed by ours). We compare our results with prior work in Table 2 for this \u03bb-refined language."}, {"heading": "1.4 Other Related Works", "text": "For the online eigenvector problem, if the feedback matrices Ak are only of rank-1, then the O\u0303( \u221a dT ) total regret of FTPL can be improved to O\u0303(d1/4T 1/2). This is first shown by Dwork et al. [13] and independently shown by Kot lowski and Warmuth [21]. However, this d1/4 factor for the rank-1 case and the d1/2 factor for the high-rank case are tight at least for their proposed FTPL methods [18]. Abernethy et al. showed that FTPL strategies can also be analyzed using a FTRL framework [1].\n7This is so because, in the same notations of Footnote 5, the per-iteration running time is O\u0303 ( \u03c3\u22122 \u00b7 \u2016\u03b7\u03a3k\u22121\u20161/22 \u00b7\nnnz(\u03a3T ) ) . This time, the optimal choice \u03b7 will imply \u2016\u03b7\u03a3k\u22121\u20162 \u2264 (\u03bbT )1/2 which is \u03bb1/4 smaller than before; however, the error tolerance \u03c3 must satisfy \u03c32 \u2248 T/\u03bb, so this totals to a per-iteration complexity of O\u0303(T 5/4\u03bb\u22123/4nnz(\u03a3T )).\nResearchers also put efforts to understand high-rank variants of the online eigenvector problem. Nie et al. studied the high-rank variant using MMWU [25], but their per-iteration running time is still O(d3) due to eigendecomposition. Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.\nThe stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].8 In streaming PCA, we are given i.i.d. random matrices with an expectation B and asked to approximately find a unit vector w with large w>Bw in the end. The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.\nFor the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition)."}, {"heading": "1.5 Roadmap", "text": "We introduce necessary notations in Section 2, and discuss the high-level difficulties and our techniques in Section 3. We introduce a new trace inequality in Section 4 that shall be used in our main proof. In Section 5 we prove our main FTCL result for an oblivious adversary, and then extend it to the adversarial setting in Section 6. We discuss how to implement FTCL fast in Section 7. Finally, in Section 8 we provide our FTCL result for a stochastic adversary.\nAll of our results are stated and proved directly in the \u03bb-refined language."}, {"heading": "2 Notations and Preliminaries", "text": "Define \u03a3k def = \u2211k\ni=1 Ai for every k = 0, 1, . . . , T . Since each Ak is positive semi-definite (PSD), we can find Pk \u2208 Rd\u00d7d such that Ak = PkP>k ; we only use Pk for analysis purpose only. Given two matrices A,B \u2208 Rd\u00d7d, we write A \u2022B def= Tr(A>B). We write A B if A,B are symmetric matrices and A \u2212 B is PSD. We write [A]i,j the (i, j)-th entry of A. We use nnz(M) to denote time needed to multiply matrix M \u2208 Rd\u00d7d with an arbitrary vector in Rd. In particular, nnz(M) is at most d plus the number of non-zero elements in M. We denote nnz(A) def = maxk\u2208[T ] { nnz(Ak) } .\nSuppose x1, \u00b7 \u00b7 \u00b7 , xt \u2208 R are drawn i.i.d. from the standard Gaussian N (0, 1), then \u03c7 = \u2211t i=1 x 2 i has a chi-squared distribution of t-degree freedom. \u03c7\u22121 is called inverse-chi-squared distribution of t-degree freedom. It is known that E[\u03c7\u22121] = 1t\u22122 for t \u2265 3.\nFor a polynomial f : R \u2192 R, we use f (k) to denote the k-th order derivative of f at point x. We use Markov brothers\u2019 inequality: if polynomial f is of degree n, then \u2200k \u2208 N\u2217 and \u2200a > 0:\nmax x\u2208[0,a]\n|f (k)(x)| \u2264 ( 2\na )i n2(n2 \u2212 12)(n2 \u2212 22) . . . (n2 \u2212 (k \u2212 1)2) (2k \u2212 1)!! maxx\u2208[0,a] |f(x)| . (2.1)"}, {"heading": "3 High-Level Discussion of Our Techniques", "text": "Let us first revisit the high-level idea behind MMWU. Recall Wk = exp(ckI + \u03b7\u03a3k\u22121) where ck is the unique constant such that TrWk = 1. Now, the key idea behind the analysis of MMWU is to use the Golden-Thompson inequality:\nTr ( eckI+\u03b7\u03a3k ) \u2264 Tr ( eckI+\u03b7\u03a3k\u22121e\u03b7Ak ) = Tr ( Wke \u03b7Ak ) \u2248 Tr ( eckI+\u03b7\u03a3k\u22121 ) + \u03b7Wk \u2022Ak .\n8This \u201cequivalence\u201d is not a black-box reduction; one usually needs open up the analysis and turn the solution of one to the other.\nIn other words, the gain value Wk \u2022Ak, up to a factor \u03b7, is proportional to the change of the trace function. One can also use convexity to show Tr ( eck+1I+\u03b7\u03a3k ) \u2212 Tr ( eckI+\u03b7\u03a3k ) \u2264 ck+1 \u2212 ck. Using these two inequalities plus a little more work, one can obtain the total regret bound. In the rest of this section, let us perform a thought experiment to \u201cmodify\u201d the MMWU analysis step-by-step. In the end, our FTCL method and its intuition shall become clear to the reader.\nThinking Step 1. Choose a random Gaussian vector u \u2208 Rd and \u201ccompress\u201d MMWU to dimension 1 in the direction of u. More specifically, we define Wk = exp(ckI + \u03b7\u03a3k\u22121) but this time ck is the unique constant such that Tr(Wkuu >) = u>Wku = 1. In such a case, we wish to say that\nTr ( eckI+\u03b7\u03a3kuu> ) = Tr ( eckI+\u03b7\u03a3k\u22121+\u03b7Akuu> ) (?) \u2264 Tr ( e(ckI+\u03b7\u03a3k\u22121)/2uu>e(ckI+\u03b7\u03a3k\u22121)/2e\u03b7Ak )\n= Tr ( W\n1/2 k uu >W1/2k e \u03b7Ak ) \u2248 Tr(Wkuu>) + \u03b7W1/2k uu>W 1/2 k \u2022Ak .\nIf the above inequality were true, then we could define wk def = W 1/2 k u which is a unit vector (because Tr(Wkuu >) = 1) and the gain w>k Akwk = wkw > k \u2022Ak would again be proportional to the change\nof this new potential function Tr ( eckI+\u03b7\u03a3k\u22121uu> ) . This idea almost worked except that inequality (?) is false due to the non-commutativity of matrices.9\nPerhaps the most \u201cimmediate\u201d idea to fix this issue is to use the randomness of uu>. Recall that E[uu>] can be made I and therefore it \u201cseems like\u201d we have E[Tr(Wkuu>)] = Tr(Wk) and the inequality will go through. Unfortunately, this idea fails for a fundamental reason: the normalization constant ck depends on u, so Wk is not independent from the randomness of u. 10\nThinking Step 2. Since Gaussian vectors are rotationally invariant, we temporarily switch to the eigenbasis of \u03a3k\u22121 so Wk is a diagonal matrix. We make an important observation:11\nck depends only on |u1|, . . . , |ud|, but not on the 2d possible signs of u1, . . . , ud. For this reason, we can fix a diagonal matrix D and consider all random uu> which agrees with D on its diagonal,12 All of such vectors u give the same normalization constant ck, and it satisfies E[uu>|D] = D. This implies that we can now study the conditional expected potential change\nE [ Tr ( eckI+\u03b7\u03a3kuu> ) \u2212Tr ( eckI+\u03b7\u03a3k\u22121uu> )\u2223\u2223D ] = Tr ( eckI+\u03b7\u03a3kD ) \u2212Tr ( eckI+\u03b7\u03a3k\u22121D ) ,\nor if we denote by B = ckI + \u03b7\u03a3k\u22121, we want to study the difference Tr ( eB+\u03b7AkD ) \u2212 Tr ( eBD ) only in the special case that D and B are simultaneously diagonalizable. Thinking Step 3. A standard way to bound Tr ( eB+\u03b7AkD ) \u2212 Tr ( eBD ) is to define f(\u03b7) def = Tr ( eB+\u03b7AkD ) and bound f(\u03b7) by its Taylor series f(0) + f \u2032(0)\u03b7 + 12f\n\u2032\u2032(0)\u03b72 + \u00b7 \u00b7 \u00b7 . The zero-order derivative f(0) is Tr ( eBD ) . The first-order derivative f \u2032(0) = Tr(AkeBD) = eB/2DeB/2 \u2022 Ak behaves exactly in the way we hope, and this strongly relies on the commutativity between B and D. Unfortunately, higher-order derivatives f (k)(0) benefit less and less from the commutativity between B and D due to the existence of terms such as Ake\nBDeBAkD. For this reason, we need to (1) truncate the Taylor series and (2) use different analytic tools. This motivates us to use the following regime that can be viewed as a \u201clow-degree\u201d version of MMWU:\nA Quick Detour. In a recent result, the authors of [7] generalized MMWU to `1\u22121/q regularized strategies. For every q \u2265 2, they define Xk = (ckI\u2212\u03b7\u03a3k\u22121)\u2212q where ck is the unique constant such\n9A analogy for this effect can be found in the inequality Tr(eA) \u2264 Tr(eB) for every A B. This inequality becomes false when multiplied with uu> and in general eA eB is false.\n10In fact, ck can be made almost independent from u if we replace uu > with some QQ> where Q is a random\nd\u00d7m matrix for some very large m. That was the main idea behind MMWU-JL. 11This is because, Tr(eckI\u2212\u03b7\u03a3k\u22121uu>) = \u2211d i=1 ( |ui|2/(ck \u2212 \u03b7\u03bbi) ) where \u03bbi is the i-th eigenvalue of \u03a3k\u22121.\n12That is, all random uu> such that \u2016ui\u201622 = Di,i for each i \u2208 [d]. For simplicity we also denote this event as D.\nthat ckI\u2212\u03b7\u03a3k\u22121 0 and TrXk = 1.13 This is a generalization of MMWU because when q \u2248 log d, the matrix Xk behaves nearly the same as Wk; in particular, it gives the same regret bound. The analysis behind this new strategy is to keep track of the potential change in Tr ( (ckI\u2212\u03b7\u03a3k\u22121)\u2212(q\u22121) ) , and then use the so-called Lieb-Thirring inequality (see Section 4) to replace the use of GoldenThompson. (Note that ck is choosen with respect to q but the potential is with respect to q \u2212 1.) Thinking Step 4. Let us now replace MMWU strategies in our Thinking Steps 1,2,3 with `1\u22121/q regularized strategies. Such strategies have two advantages: (1) they help us overcome the issue for higher-order terms in Thinking Step 3, and (2) matrix inversions are more efficient than matrices exponentials in terms of computation. We shall choose q = \u0398(log(dT )) in the end.\nSpecifically, we prepare a random vector u and define the normalization constant ck to be the unique one satisfying Tr ( (ckI\u2212\u03b7\u03a3k\u22121)\u2212quu> ) = Tr(Xkuu >) = 1. At iteration k, we let the player choose strategy X 1/2 k u which is a unit vector.\nIf one goes through all the math carefully (using Woodbury formula), this time we are entitled to upper bound the trace difference of the form Tr ( (B + \u03b7C)q\u22121D ) \u2212 Tr ( Bq\u22121D ) where D is simultaneously diagonalizable with D but not C. Similar to Thinking Step 3, we can define f(\u03b7) def = Tr ( (B + \u03b7C)q\u22121D ) and bound this polynomial f(\u03b7) using its Taylor expansion at point 0. Commutativity between B and D helps us compute f \u2032(0) = (q \u2212 1)Tr(Bq\u22122CD) but again we cannot bound higher-derivatives directly. Fortunately, this time f(\u03b7) is a degree q \u2212 1 polynomial so we can use Markov brothers\u2019 inequality to give an upper bound on its higher-order terms. This is the place we lose a few extra polylogarithmic factors in the total regret.\nThinking Step 5. Somehow necessarily, even the second-order derivative f \u2032\u2032(0) can depend on terms such as 1/Dii where Dii = |ui|2 is the i-th diagonal entry of D. This quantity, over the Gaussian random choice of ui, does not have a bounded mean. More generally, the inverse chisquared distribution with degree t (recall Section 2) has a bounded mean only when t \u2265 3. For this reason, instead of picking a single random vector u \u2208 Rd, we need pick three random vectors u1, u2, u3 \u2208 Rd and replace all the occurrences of uu> with 13 ( u1u > 1 +u2u > 2 +u3u > 3 ) in the previous thinking steps. As a result, each Dii becomes a chi-squared distribution of degree 3 so the issue goes away. This is why we claimed in the introduction that\nwe can compress MMWU to dimension 3.\nRemark. By losing a polylog factor in regret, one can compress it further to dimension 2. This is because the mean of the inverse chi-squared distribution with degree 2, if truncated at some large value v, is only log(v). However, this \u201ctruncated mean\u201d becomes \u2126( \u221a v) for degree 1.\nThinking Step 6. Putting together previous steps, we obtain a FTCL strategy with total regret O( \u221a T log3(dT )), which is worse than MMWU only by a factor O(log2.5(dT )). We call this method FTCLobl and include its analysis in Section 5. However, FTCLobl only works for an oblivious adversary (i.e., when A1, . . . ,AT are fixed a priori) and gives an expected regret. To turn it into a robust strategy against adversarial A1, . . . ,AT , and to make the regret bound work with high confidence, we need to re-sample u1, u2, u3 every iteration. We call this method FTCL\nadv. A careful but standard analysis with Azuma inequality helps us reduce FTCLadv to FTCLobl. We state this result in Section 6.\nRunning Time. As long as q is an even integer, the computation of \u201c(ckI \u2212 \u03b7\u03a3k\u22121)\u22121 applied to a vector\u201d becomes the bottleneck of each iteration of FTCLobl and FTCLadv. However, as long\n13The name of such strategies come from the following fact. Recall that MMWU naturally arises as the strategy in follow-the-regularized-leader when the regularizer is the matrix entropy. If that entropy function is replaced with a negative `1\u22121/q norm, the resulting strategy becomes the so-defined matrix Xk. We encourage interested readers to see the introduction of [7] for more background information, but we shall make this present paper self-contained.\nas q \u2265 \u2126(log(dT )), we show that the condition number of the matrix ckI \u2212 \u03b7\u03a3k\u22121 is at most \u03b7T = \u0398(T 1/2). Using conjugate gradient, we can compute this inversion in time O\u0303(T 1/4) times O(nnz(\u03a3k\u22121)). This gives the FTCL (basic) running time in Table 1. As for the faster FTCL (opt) running time, one need to use more advanced optimization tools \u2014namely, accelerated variance reduction\u2014 to perform inversion. We discuss the details in Section 7.\nCompress MMWU to Dimension 1 in Stochastic Online Eigenvector. If the adversary is stochastic, we observe that Oja\u2019s algorithm corresponds to a potential function Tr ( (I+\u03b7Ak) \u00b7 \u00b7 \u00b7 (I+ \u03b7A1)uu >(I + \u03b7A1) \u00b7 \u00b7 \u00b7 (I + \u03b7Ak) ) . Because the matrices are drawn from a common distribution, this potential behaves similar to the matrix exponential but compressed to dimension 1, namely Tr ( e\u03b7(A1+\u00b7\u00b7\u00b7+Ak)uu> ) . In fact, just using linearity of expectation carefully, one can both upper and lower bound this potential. We state this result in Section 8 (and it can be proved in one page!)"}, {"heading": "4 A New Trace Inequality", "text": "Prior work on MMWU and its extensions relies heavily on one of the following trace inequalities [7]:\nGolden-Thompson inequality : Tr(eA+\u03b7B) \u2264 Tr ( eAe\u03b7B )\nLieb-Thirring inequality : Tr ( (A + \u03b7B)k ) \u2264 Tr ( Ak/2(I + \u03b7A\u22121/2BA\u22121/2)kAk/2 ) .\nDue to our compression framework in this paper, we need inequalities of type\n\u201c Tr(eA+\u03b7BD) \u2264 Tr ( e\u03b7BeA/2DeA/2 ) \u201d \u201c Tr ( (A + \u03b7B)kD ) \u2264 Tr ( (I + \u03b7A\u22121/2BA\u22121/2)kAk/2DAk/2 ) . \u201d (4.1)\nwhich look almost like \u201cgeneralizations\u201d of Golden-Thompson and Lieb-Thirring (by seeting D = I). Unfortunately, such generalizations do not hold for an arbitrary D. For instance, if the first \u201cgeneralization\u201d holds for every PSD matrix D then it would imply \u201c eA+\u03b7B eA/2e\u03b7BeA/2 \u201d which is a false inequality due to matrix non-commutativity.\nIn this paper, we show that if D is commutative with A, then the \u201cgeneralization\u201d (4.1) holds for the zeroth and first order terms with respect to \u03b7. As for the second and higher order terms, we can control it using Markov brothers\u2019 inequality. (Proof in Appendix A.)\nLemma 4.1. For every symmetric matrices A,B,D \u2208 Rd\u00d7d, every integer k \u2265 1, every \u03b7\u2217 \u2265 0, and every \u03b7 \u2208 [0, \u03b7\u2217/k2], if A and D are commutative, then\n(A + \u03b7B)k \u2022D\u2212Ak \u2022D \u2264 k\u03b7B \u2022Ak\u22121D + ( \u03b7k2\n\u03b7\u2217\n)2 max\n\u03b7\u2032\u2208[0,\u03b7\u2217]\n{\u2223\u2223(A + \u03b7\u2032B)k \u2022D\u2212Ak \u2022D \u2223\u2223 } ."}, {"heading": "5 Oblivious Online Eigenvector + Expected Regret", "text": "In this section we first focus on a simpler oblivious setting. A1, . . . ,AT are T PSD matrices chosen by the adversary in advance, and they do not depend on the player\u2019s actions in the T iterations. We are interested in upper bounding the total expected regret\n\u03bbmax (\u2211T k=1 Ak ) \u2212\u2211Tk=1 E[w>k Akwk] ,\nwhere the expectation is over player\u2019s random choices wk \u2208 Rd. Recall \u2016wk\u20162 = 1. (In Section 6 we generalize this result to the full adversarial setting along with high-confidence regret.)\nOur algorithm FTCLobl is presented in Algorithm 1. This algorithm is parameterized by an even integer q \u2265 2 and a learning rate \u03b7 > 0. It initializes with a rank-3 Wishart random matrix U. For\nevery k \u2208 [T + 1], we denote by Xk def= ( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212q where\nck > 0 is the unique constant s.t. ckI\u2212 \u03b7\u03a3k\u22121 0 and Tr ( XkU ) = 1 .\nAt iteration k \u2208 [T ], the player plays a random vector among the three eigenvectors of X1/2k UX 1/2 k .\nWe prove the following theorem in this paper for the total regret of FTCLobl(T, q, \u03b7).\nTheorem 1. In the online eigenvector problem with an oblivious adversary, there exists absolute constant C > 1 such that if q \u2265 3 log(2dT ) and \u03b7 \u2208 [ 0, 1\n11q3\n] , then FTCLobl(T, q, \u03b7) satisfies\nT\u2211\nk=1\nE [ w>k Akwk ] = T\u2211\nk=1\nE [ Ak \u2022X1/2k UX 1/2 k ] \u2265 ( 1\u2212 C \u00b7 \u03b7q5 log(dT ) ) \u03bbmax(\u03a3T )\u2212 4\n\u03b7 .\nCorollary 5.1. Choosing q = 3 log(2dT ) and \u03b7 = \u0398(log\u22123(dT )/ \u221a \u03bbmax(\u03a3T )), we have\n\u2211T k=1 E [ w>k Akwk ] \u2265 \u03bbmax(\u03a3T )\u2212O (\u221a \u03bbmax(\u03a3T ) log 3(dT ) ) , (\u03bb-refined language)\nor choosing the same q but \u03b7 = \u0398(log\u22123(dT )/ \u221a T ) we have\n\u2211T k=1 E [ w>k Akwk ] \u2265 \u03bbmax(\u03a3T )\u2212O (\u221a T log3(dT ) ) . (general language)\nAs discussed in Section 3, our proof of Theorem 1 relies on a careful analysis on how the potential\nfunction Tr(X 1\u22121/q k U) = Tr ( (ckI \u2212 \u03b7\u03a3k\u22121)\u2212(q\u22121)U ) changes across iterations. We analyze this potential increase in two steps: in the first step we replace \u03a3k\u22121 with \u03a3k, and in the second step we replace ck with ck+1. After appropriate telescoping, we can derive the result of Theorem 1.\nWe now discuss the details in the subsequent sections.\nAlgorithm 1 FTCLobl(T, q, \u03b7) Input: T , number of iterations; q \u2265 2, an even integer, theory-predicted choice q = \u0398(log(dT )) \u03b7, the learning rate. theory-predicted choice \u03b7 = log\u22123(dT )/\u221a\u03bbmax(\u03a3T )\n1: Choose 3 vectors u1, u2, u3 \u2208 Rd where the 3d coordinates are i.i.d. drawn from N (0, 1). 2: U\u2190 13 ( u1u > 1 + u2u > 2 + u3u > 3 ) . 3: for k \u2190 1 to T do 4: \u03a3k\u22121 \u2190 \u2211k\u22121 i=1 Ai.\n5: Denote by Xk \u2190 ( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212q where ck is the unique constant satisfying that\nckI\u2212 \u03b7\u03a3k\u22121 0 and Tr ( XkU ) = 1 .\n6: Compute X 1/2 k UX 1/2 k = \u22113 j=1 pj \u00b7 yjy>j where y1, y2, y3 are orthogonal unit vectors in Rd. This is an eigendecomposition and it satisfies p1, p2, p3 \u2265 0 and p1 + p2 + p3 = 1. 7: Choose wk \u2190 yj with probability pj . 8: Play strategy wk and receive matrix Ak. 9: end for"}, {"heading": "5.1 Well-Conditioning Events", "text": "Due to concentration reasons, the potential increase could only be \u201creasonably\u201d bounded for wellconditioned matrices U. We now make this definition formal. Given some parameter \u03b4 > 0 that we shall later choose to be 1/T 3, we introduce the following event:\nDefinition 5.2. For every k \u2208 {0, 1, . . . , T}, define event\nEk(U) def= { \u03bd>1 U\u03bd1 \u2265 \u03b4\n2 and \u2200i \u2208 [d] : \u03bd>i U\u03bdi \u2264 2 log\ned\n\u03b4\n}\nwhere \u03bd1, . . . , \u03bdd are the eigenvectors of \u03a3k with non-increasing eigenvalues. Let E<j(U) def= \u2227j\u22121 k=0 Ek(U).\nIntuitively, event Ek(U) makes sure that the matrix U is \u201cwell-conditioned\u201d in the eigenbasis of \u03a3k: (1) it has a not-so-small first coordinate \u03bd > 1 U\u03bd1, and (2) each coordinate \u03bd > i U\u03bdi is no more than logarithmic. Using tail bounds for Gaussian distributions, it is not hard to show that this event occurs with probability at least 1\u2212 \u03b4 (see Appendix B): Lemma 5.3. For every k = 0, 1, . . . , T , we have PrU[Ek(U)] \u2265 1\u2212 \u03b4.\nUnder event Ek\u22121(U), the barrier ck and the matrix Xk satisfy the following nice properties. (Their proofs are simple manipulations of matrix algebra and included in Appendix B.)\nProposition 5.4. If q \u2265 max{log 2\u03b4 , log(3d log ed\u03b4 )}, then\nevent Ek\u22121(U) implies 1\ne \u2264 ck \u2212 \u03b7\u03bbmax(\u03a3k\u22121) \u2264 e .\nIn particular, Ek\u22121(U) implies (recall Ak = PkP>k )\n(a) : ckI\u2212 \u03b7\u03a3k\u22121 1\ne I (b) : Tr(X\n1\u22121/q k U) \u2264 ck \u2264 \u03b7\u03bbmax(\u03a3k\u22121) + e (c) : \u03b7P>k X 1/q k Pk e\u03b7I ."}, {"heading": "5.2 First Potential Increase", "text": "We next lemma bounds the potential increase if we replace \u03a3k\u22121 with \u03a3k: Lemma 5.5. There exists constant C > 1 such that, if q \u2265 max{log 2\u03b4 , log(3d log ed\u03b4 )} and \u03b7 \u2264 13q3 ,\nE [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) U ) \u00b7 1E<k(U) ]\n\u2264 (q \u2212 1)\u03b7(1 + C \u00b7 \u03b7q5 log(d/\u03b4))E [ Ak \u2022X1/2k UX 1/2 k ] + (\u03b7T + e)T\u03b4 .\nThe proof of Lemma 5.5 is the main technical contribution of this paper, and deviates the most from classical analysis of MMWU. It makes use of our trace inequality in Section 4, and is the only place in our analysis that relies on rank(U) \u2265 3. We include the details in Appendix C. Remark 5.6. We have slightly abused notations here. In principle, the quantity Tr (( ckI\u2212\u03b7\u03a3k )\u2212(q\u22121) U ) can be unbounded if ckI \u2212 \u03b7\u03a3k is not invertible. However, as we shall see in the proof of Lemma 5.5, this necessarily implies 1E<k(U) = 0 because of Proposition 5.4. Therefore, we de-\nfine Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) to be zero if this happens."}, {"heading": "5.3 Second Potential Increase", "text": "The following lemma bounds the potential increase if we replace ck with ck+1. Its proof is included in Appendix D and is reasonably straightforward.\nLemma 5.7. For all q \u2265 2 and \u03b7 > 0, E [ Tr (( ck+1I\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<(k+1)(U) ] \u2212 E [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) ] \u2264 \u2212(q \u2212 1)(E[ck+1]\u2212 E[ck]) Finally, we prove in Appendix E that Theorem 1 is a direct consequence of our two potential increase lemmas above."}, {"heading": "6 Adversarial Online Eigenvector + Regret in High-Confidence", "text": "In this section, we switch to the more challenging adversarial setting: in each iteration k, the adversary picks Ak after seeing the player\u2019s strategies w1, . . . , wk\u22121. In other words, Ak may depend on the randomness used in generating w1, . . . , wk\u22121 as well.\nIn such a case, denoting by D the same rank-3 Wishart distribution we generate U from in FTCLobl, we consider a variant of FTCLobl where a new random Uk is generated from D per iteration. In other words, instead of choosing U \u223c D only once at the beginning, we choose U1, . . . ,UT i.i.d. from D. Then, the normalization constant ck is defined to satisfy Tr((ckI\u2212 \u03b7\u03a3k\u22121)\u2212qUk) = 1. We call this algorithm FTCLadv and present it in the appendix for completeness\u2019 sake.\nOur next theorem shows that, algorithm FTCLadv gives the same regret bound as Theorem 1 even in the adversarial setting; in addition, it elevates the regret bound to a high-confidence level.\nTheorem 2. In the online eigenvector problem with an adversarial adversary, there exists constant C > 1 such that for every p \u2208 (0, 1), q \u2265 3 log(2dT ) and \u03b7 \u2208 [ 0, 1\n11q3\n] , our FTCLadv(T, q, \u03b7) satisfies\nw.p. \u2265 1\u2212 p : T\u2211\nk=1\nw>k Akwk \u2265 ( 1\u2212 C \u00b7 \u03b7 ( q5 log(dT ) + log(1/p) )) \u03bbmax(\u03a3T )\u2212 5\n\u03b7 .\nCorollary 6.1. Let q = 3 log(2dT ) and \u03b7 = \u0398 ( log3(dT )+log1/2(1/p))\u22121\u221a\n\u03bbmax(\u03a3T )\n) , then with prob. \u2265 1\u2212 p:\n\u2211T k=1w > k Akwk \u2265 \u03bbmax(\u03a3T )\u2212 \u221a \u03bbmax(\u03a3T ) \u00b7O ( log3(dT ) + \u221a log(1/p) ) , (\u03bb-refined language)\nor choosing the same q but \u03b7 = \u0398 ( log3(dT )+log1/2(1/p))\u22121\u221a\nT\n) we have with prob. \u2265 1\u2212 p:\n\u2211T k=1w > k Akwk \u2265 \u03bbmax(\u03a3T )\u2212 \u221a T \u00b7O ( log3(dT ) + \u221a log(1/p) ) . (general language)\nProof of Theorem 2 relies on a reduction to the oblivious setting, and is included in Appendix F."}, {"heading": "7 Efficient Implementation", "text": "Recall that our regret theorems were based on the assumption that in each iteration k, the three vectors vj def = X 1/2 k uj = ( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212q/2 uj for j = 1, 2, 3 can be computed exactly. If this is the\ncase, then one can compute the 3\u00d7 3 matrix ( u>i Xkuj ) i,j\u2208[3] explicitly, and then we can obtain its rank-3 eigendecomposition X 1/2 k UX 1/2 k = \u22113 j=1 pj \u00b7 yjy>j in O(d) time.\nTo make such computations efficient, we need to deal with three important issues:\n(a) We need to allow v1, v2, v3 to be computed approximately.\n(b) We need to find the unique normalization constant ck efficiently. (c) We need to compute ( ckI\u2212 \u03b7\u03a3k\u22121 )\u22121 b efficiently for any vector b \u2208 Rd.\nAt a high level, issue (a) is not a big deal because if v\u2032j satisfies \u2016vj \u2212 v\u2032j\u20162 \u2264 \u03b5\u0303/poly(d, T ) and we use v\u2032j instead of vj , then the final regret is affected by less than \u03b5\u0303; issue (b) can be dealt as long as we perform a careful binary search to find ck, similar to prior work [7]; issue (c) can be done as long as we have a good control on the condition number of the matrix ( ckI\u2212 \u03b7\u03a3k\u22121 ) .\nWe discuss the details in Appendix G, and state below our final running-time theorem:\nTheorem 3. As long as q \u2265 3 log(2dT/p), with probability at least 1\u2212 p, each of the T iterations of FTCLobl and FTCLadv can be implemented to run in time\nO\u0303 (\u221a \u03b7\u03bbmax(\u03a3T ) + 1 \u00b7 nnz(\u03a3T ) ) and O\u0303 ( nnz(\u03a3T ) + \u221a \u03b7T \u00b7 nnz(\u03a3T )3/4nnz(A)1/4 ) .\nCorollary 7.1. Let q = 3 log(2dT/p) and \u03b7 = \u0398 ( log\u22123(dT/p)/ \u221a T ) , then with prob. \u2265 1 \u2212 p, each iteration runs in time O\u0303 ( T 1/4 \u00b7 nnz(\u03a3T ) ) and O\u0303 ( nnz(\u03a3T ) + T 1/4 \u00b7 nnz(\u03a3T )3/4nnz(A)1/4 ) .\nAlternatively, if we choose \u03b7 = \u0398 ( log\u22123(dT/p)/ \u221a \u03bbmax(\u03a3T ) ) , then each iteration runs in time\nO\u0303 ( (\u03bbmax(\u03a3T ) + 1) 1/4 \u00b7 nnz(\u03a3T ) ) and O\u0303 ( nnz(\u03a3T ) + (T/ \u221a \u03bbmax(\u03a3T )) 1/2 \u00b7 nnz(\u03a3T )3/4nnz(A)1/4 ) ."}, {"heading": "8 Stochastic Online Eigenvector", "text": "Consider the simplest setting when the matrices A1, . . . ,AT are generated i.i.d. from a common distribution whose expectation equals B. This is known as the stochastic online eigenvector problem, and we wish to minimize the regret \u2211T k=1w > k Akwk \u2212 T \u00b7 \u03bbmax(B).14\nIn this setting, we revisit Oja\u2019s algorithm: beginning with a random vector u \u2208 Rd where each ui is i.i.d. drawn from N (0, 1), at each iteration k, play wk to be (I + \u03b7Ak\u22121) \u00b7 \u00b7 \u00b7 (I + \u03b7A1)u after normalization. It is clear that wk can be computed from wk\u22121 with an additional time nnz(A).\nWe include in Appendix H a one-paged proof of the following theorem:\nTheorem 4. There exists C > 1 such that, for every p \u2208 (0, 1), if \u03b7 \u2208 [ 0, \u221a p/(60T\u03bbmax(B)) ] in Oja\u2019s algorithm, we have with probability at least 1\u2212 p: \u2211T\nk=1w > k Akwk \u2265 (1\u2212 2\u03b7)T \u00b7 \u03bbmax(B)\u2212 C \u00b7 log(d+log(1/p)) \u03b7 .\nCorollary 8.1. Choosing \u03b7 = \u221a p/ \u221a\n60T\u03bbmax(B), we have with prob. \u2265 1\u2212 p: \u2211T\nk=1w > k Akwk \u2265 T \u00b7 \u03bbmax(B)\u2212O (\u221aT \u00b7\u03bbmax(B)\u221a p \u00b7 log(d+ log(1/p)) ) . (\u03bb-refined language)\nChoosing \u03b7 = \u221a p/ \u221a\n60T , we have with prob. \u2265 1\u2212 p: \u2211T\nk=1w > k Akwk \u2265 T \u00b7 \u03bbmax(B)\u2212O (\u221a T\u221a p \u00b7 log(d+ log(1/p)) ) . (general language)\nThe proof of Theorem 4 uses a potential function analysis which is similar to the matrix exponential potential used in MMWU, but compressed to dimension 1."}, {"heading": "Acknowledgement", "text": "We thank Yin Tat Lee for discussing the problem regarding how to compress MMWU to constant dimension in 2015. We thank Elad Hazan for suggesting us the problem and for several insightful discussions. We thank Dan Garber and Tengyu Ma for clarifying some results of prior work [16].\n14In principle, one can also ask to minimize regret where T \u00b7B is replaced with A1 + \u00b7 \u00b7 \u00b7 + AT . However, due to simple concentration results, there is no big difference between the two different notions. [16]\nAppendix"}, {"heading": "A Proof of Lemma 4.1", "text": "Lemma 4.1. For every symmetric matrices A,B,D \u2208 Rd\u00d7d, every integer k \u2265 1, every \u03b7\u2217 \u2265 0, and every \u03b7 \u2208 [0, \u03b7\u2217/k2], if A and D are commutative, then\n(A + \u03b7B)k \u2022D\u2212Ak \u2022D \u2264 k\u03b7B \u2022Ak\u22121D + ( \u03b7k2\n\u03b7\u2217\n)2 max\n\u03b7\u2032\u2208[0,\u03b7\u2217]\n{\u2223\u2223(A + \u03b7\u2032B)k \u2022D\u2212Ak \u2022D \u2223\u2223 } .\nProof. Consider a degree-k polynomial\nf(\u03b7) def = (A + \u03b7B)k \u2022D\u2212Ak \u2022D =\nk\u2211\ni=1\n\u03b7i \u2211\nj0,...,ji\u2208Z\u22650 j0+\u00b7\u00b7\u00b7+ji=k\u2212i\nAj0BAj1B \u00b7 \u00b7 \u00b7BAji \u2022D\nIts first order derivative\nf \u2032(0) = \u2211\nj0,j1\u2208Z\u22650 j0+j1=k\u22121\nAj0BAj1 \u2022D = \u2211\nj0,j1\u2208Z\u22650 j0+j1=k\u22121\nA(k\u22121)/2BA(k\u22121)/2 \u2022D = kB \u2022A(k\u22121)/2DA(k\u22121)/2 .\nAbove, the first equality is due to the commutativity between A and D. Letting f\u2217 def= max\u03b7\u2032\u2208[0,\u03b7\u2217] |f(\u03b7\u2032)|, we can apply Markov brothers\u2019 inequality (2.1) and obtain for every i \u2265 2,\n|f (i)(0)| \u2264 ( 2\n\u03b7\u2217\n)i \u00b7 k\n2(k2 \u2212 1) \u00b7 \u00b7 \u00b7 (k2 \u2212 (i\u2212 1)2) 1 \u00b7 3 \u00b7 5 \u00b7 \u00b7 \u00b7 (2i\u2212 1) max\u03b7\u2032\u2208[0,\u03b7\u2217] |f(\u03b7 \u2032)| \u2264 k 2i (\u03b7\u2217)i f\u2217 .\nTherefore, as long as \u03b7 \u2264 \u03b7\u2217 k2 , we have\nf(\u03b7) = f(0) + f \u2032(0) \u00b7 \u03b7 + k\u2211\ni=2\n\u03b7i \u00b7 f (i)(0)\ni! \u2264 f(0) + f \u2032(0) \u00b7 \u03b7 +\nk\u2211\ni=2\n( \u03b7k2\n\u03b7\u2217\n)i \u00b7 f \u2217\ni!\n\u2264 f(0) + f \u2032(0) \u00b7 \u03b7 + ( \u03b7k2\n\u03b7\u2217\n)2 f\u2217 .\nSince f(0) = 0 we complete the proof."}, {"heading": "B Proof for Section 5.1", "text": "B.1 Proof of Lemma 5.3\nLemma 5.3. For every k = 0, 1, . . . , T , we have PrU[Ek(U)] \u2265 1\u2212 \u03b4. Proof. Let \u03bd1, . . . , \u03bdd be the eigenvectors of \u03a3k with non-increasing eigenvalues. Because Gaussian random vectors are rotationally invariant, we can view each u1, u2, u3 as drawn in the basis of \u03bd1, . . . , \u03bdd, so each \u03bd > i uj is drawn i.i.d. from N (0, 1) for every i \u2208 [d], j \u2208 [3].\nSince \u03bd>1 U\u03bd1 = 1 3 ( (\u03bd>1 u1) 2 + (\u03bd>1 u2) 2 + (\u03bd>1 u3) 2 ) , we immediately know that 3\u03bd>1 U\u03bd1 is distributed according to chi-square distribution \u03c72(3). The probability density function of \u03c72(3) is f(x) = e \u2212x/2\u221ax\u221a\n2\u03c0 (for x \u2208 [0,\u221e)) and therefore Pr [ \u03bd>1 U\u03bd1 \u2264 \u03b4/2 ] \u2264 \u222b 3\u03b4/2\n0\ne\u2212x/2 \u221a x\u221a\n2\u03c0 dx \u2264\n\u222b 3\u03b4/2\n0\n\u221a x\u221a 2\u03c0 dx = 1 2 \u221a 3 \u03c0 \u03b43/2 \u2264 \u03b4 2 .\nAs for the second condition, for every t \u2265 0 and i \u2208 [d],\nPr [ \u03bd>i U\u03bdi \u2265 t/3 ] \u2264 \u222b \u221e\nt\ne\u2212x/2 \u221a x\u221a\n2\u03c0 dx = 1\u2212 Erf (\u221a t\u221a 2 ) + \u221a 2 \u03c0 e\u2212t/2 \u221a t \u2264 e\u2212t/2 + \u221a 2 \u03c0 e\u2212t/2 \u221a t ,\nwhere Erf(x) is the Gauss error function. Picking t = 4 log ed\u03b4 , we have\ne\u2212t/2 +\n\u221a 2\n\u03c0 e\u2212t/2\n\u221a t \u2264 \u03b4 2\ne2d2 +\n\u221a 2\n\u03c0\n\u03b42 e2d2 \u00b7 2 \u221a ed \u03b4 < \u03b4 2d .\nTherefore, we have Pr [ \u2200i \u2208 [d] : \u03bd>i U\u03bdi \u2265 2 log ed\u03b4 ] \u2264 \u03b42 and we conclude by union bound PrU[Ek(U)] \u2264 \u03b42 + \u03b42 = \u03b4 .\nB.2 Proof of Proposition 5.4\nProposition 5.4. If q \u2265 max{log 2\u03b4 , log(3d log ed\u03b4 )}, then\nevent Ek\u22121(U) implies 1\ne \u2264 ck \u2212 \u03b7\u03bbmax(\u03a3k\u22121) \u2264 e . (B.1)\nIn particular, Ek\u22121(U) implies (recall Ak = PkP>k )\n(a) : ckI\u2212 \u03b7\u03a3k\u22121 1\ne I (b) : Tr(X\n1\u22121/q k U) \u2264 ck \u2264 \u03b7\u03bbmax(\u03a3k\u22121) + e (c) : \u03b7P>k X 1/q k Pk e\u03b7I .\nProof. Let \u03bd1, . . . , \u03bdd be the eigenvectors of \u03a3k\u22121 with non-increasing eigenvalues \u03bb1, . . . , \u03bbd. Then,\u2211d i=1 \u03bd>i U\u03bdi (ck\u2212\u03b7\u03bbi)q = Tr(XkU) = 1. However, event Ek(U) tells us \u03bd > i U\u03bdi \u2265 \u03b42 which implies (ck \u2212 \u03b7\u03bb1) q \u2265 \u03b42 . Under our choice of q, we have ck \u2212 \u03b7\u03bb1 \u2265 1e which proves the first inequality in (B.1).\nOn the other hand, letting c = \u03b7\u03bbmax(\u03a3k\u22121) + e, our choice of q implies\nTr((cI\u2212 \u03b7\u03a3k\u22121)\u2212qU) = d\u2211\ni=1\n\u03bd>i U\u03bdi (c\u2212 \u03b7\u03bbi)q \u2264 d\u2211\ni=1\n2 log(ed/\u03b4)\neq \u2264 1 .\nSince the left hand side of the above inequality is an decreasing function in c, and since Tr((ckI\u2212 \u03b7\u03a3k\u22121)\u2212qU) = 1, we must have ck \u2264 c which proves the second inequality in (B.1).\nFinally, (a) is a simple corollary of the first inequality of (B.1). As for (b), it simply comes from the following upper bound\nTr(X 1\u22121/q k U) =\nd\u2211\ni=1\n\u03bd>i U\u03bdi (ck \u2212 \u03b7\u03bbi)q\u22121 \u2264 ck d\u2211\ni=1\n\u03bd>i U\u03bdi (ck \u2212 \u03b7\u03bbi)q = ckTr(XkU) = ck .\nAs for (c), it follows from P>k Pk I so \u03b7P>k X 1/q k Pk \u03b7P>k (eI)Pk e\u03b7I."}, {"heading": "C Proof for Section 5.2", "text": "Lemma 5.5. There is constant C > 1 such that, if q \u2265 max{log 2\u03b4 , log(3d log ed\u03b4 )} and \u03b7 \u2264 111q3 ,\nE [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) U ) \u00b7 1E<k(U) ]\n\u2264 (q \u2212 1)\u03b7(1 + C \u00b7 \u03b7q5 log(d/\u03b4))E [ Ak \u2022X1/2k UX 1/2 k ] + (\u03b7T + e)T\u03b4 .\nProof. Let \u03bd1, . . . , \u03bdd be the eigenvectors of \u03a3k\u22121 with non-increasing eigenvalues. In this proof, let us assume without loss of generality that all vectors and matrices are written in this eigenbasis (so \u03a3k\u22121 and Xk are both diagonal matrix).\nSince Gaussian random vectors are rotationally invariant, we assume that u1, u2, u3 are generated according to the following procedure: first, the absolute values of their 3d coordinates u1, u2, u3 are determined; then, their signs are determined.\nDenoting by D = diag{U11, . . . ,Udd} the diagonal part of U, we immediately notice that D is determined completely at the first step of the above procedure. This has two important consequences that we shall rely crucially in the proof:\n\u2022 fixing the randomness of D, it satisfies EU[U|D] = D;15 \u2022 ck is completely determined by D. 16\nIn addition, since the event Ek\u22121(U) only depends on the diagonal entry of U, slightly abusing notation, we also use Ek\u22121(D) to denote this event on diagonal matrices D. We also use Di to represent the i-th diagonal entry of D. Our proof now has three parts:\nPart I: Potential Increase for D. For every PSD matrix D, denoting by Ak = PkP > k ,\nTr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) D ) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) D )\n\u00ac = Tr (( X \u22121/q k \u2212 \u03b7PkP>k )\u2212(q\u22121) D ) \u2212Tr ( X 1\u22121/q k D )\n = Tr (( X\n1/q k + \u03b7X 1/q k Pk(I\u2212 \u03b7P>k X 1/q k Pk) \u22121P>k X 1/q k\n)q\u22121 D ) \u2212Tr ( X\n1\u22121/q k D\n) (C.1)\nAbove, \u00ac follows from the definition of Xk and  uses the Woodbury formula for matrix inversion. Now, unlike the classical proof for MMWU, our matrix D here is not identity so we cannot rely on the Lieb-Thirring trace inequality to bound the right hande side of (C.1) like it was used in [7]. We can instead consult our new trace inequality Lemma 4.1 because D and Xk are both diagonal matrices so they are commutative. Recall that Lemma 4.1 requires a crude upper bound on the first trace quantity on the term \u201c \u2223\u2223(A+\u03b7\u2032B)k \u2022D\u2212Ak \u2022D \u2223\u2223\u201d, and we shall provide this crude upper bound in Lemma C.1. Formally, choosing \u03b7\u2217 def= 111q , we that for every D satisfying Ek\u22121(D),\nTr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) D ) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) D )\n\u00ae \u2264 (q \u2212 1)\u03b7X1/qk Pk(I\u2212 \u03b7P>k X 1/q k Pk) \u22121P>k X 1/q k \u2022X (q\u22122)/q k D\n+\n( \u03b7(q \u2212 1)2\n\u03b7\u2217\n)2 \u00b7 4(q \u2212 1)\u03b7\u2217\u2016D\u20162PkP>k \u2022Xk\n\u00af \u2264 (q \u2212 1)\u03b7\n1\u2212 e\u03b7 PkP > k \u2022XkD +O\n( \u03b72q6 ) \u00b7 \u2016D\u20162 \u00b7PkP>k \u2022Xk\n\u00b0 \u2264 (q \u2212 1)\u03b7PkP>k \u2022XkD +O ( \u03b72q6 log(d/\u03b4) ) PkP > k \u2022Xk . (C.2)\nAbove, \u00ae follows from Lemma 4.1 (with \u03b7 \u2264 \u03b7\u2217/q2) together with Lemma C.1 (for \u03b7 = \u03b7\u2217); \u00af follows from I \u2212 \u03b7P>k X 1/q k Pk (1 \u2212 e\u03b7)I (see Proposition 5.4), the fact that Tr(AC) \u2264 Tr(BC) for A B and C symmetric, and the choice of \u03b7\u2217; \u00b0 follows from our assumption \u03b7 \u2264 16 as well as \u2016D\u20162 \u2264 2 log 2d\u03b4 which comes from the definition of event Ek\u22121(D). Part II: Potential Increase for All U That Agrees With D. For every fixed D that satisfies\n15More specifically, since the off-diagonal entries of U can still randomly flip signs in the second step of the random procedure, their expectations are all equal to zero.\n16This is because ck is defined as the constant satisfying 1 = Tr((ckI\u2212 \u03b7\u03a3k\u22121)U) = Tr((ckI\u2212 \u03b7\u03a3k\u22121)D).\nEk\u22121(D), taking expectation over all matrices U that agrees with D:17\nE [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<(k\u22121)(U) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) U ) \u00b7 1E<(k\u22121)(U) \u2223\u2223\u2223D ]\n\u00ac \u2264 E [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) U )\n+ Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) U ) \u00b7 (1\u2212 1E<(k\u22121)(U)) \u2223\u2223\u2223D ]\n \u2264 Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) D ) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) D ) + E [ (\u03b7T + e) \u00b7 (1\u2212 1E<(k\u22121)(U)) \u2223\u2223\u2223D ] = Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) D ) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) D ) + (\u03b7T + e) \u00b7Pr [ E<(k\u22121)(U) \u2223\u2223\u2223D ]\n\u00ae \u2264 (q \u2212 1)\u03b7PkP>k \u2022XkD +O ( \u03b72q6 log(d/\u03b4) ) \u00b7PkP>k \u2022Xk + (\u03b7T + e)T\u03b4 \u00af = (q \u2212 1)\u03b7 E [ PkP > k \u2022XkU | D ] +O ( \u03b72q6 log(d/\u03b4) ) \u00b7PkP>k \u2022Xk + (\u03b7T + e)T\u03b4. (C.3)\nAbove, \u00ac is because indicator functions are never greater than 1;  uses Tr(X 1\u22121/q k U) \u2264 \u03b7\u03bbmax(\u03a3k\u22121)+ e \u2264 \u03b7T + e which follows from Proposition 5.4; \u00ae follows from (C.2) as well as Lemma 5.3; and \u00af follows from the observation EU[U|D] = D together with the fact that Xk only depends on D. Part III: Potential Increase for All U. We now claim for all possible diagonal D, it satisfies\nE [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2212Tr (( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2223\u2223\u2223D ]\n\u2264 (q \u2212 1)\u03b7 E [ PkP > k \u2022XkU|D ] +O ( \u03b72q6 log(d/\u03b4) ) \u00b7PkP>k \u2022Xk + (\u03b7T + e)T\u03b4. (C.4)\nThis is because, if D satisfies Ek\u22121(D) then (C.4) comes from (C.3); or if D does not satisfy Ek\u22121(D) then the left hand side of (C.4) is zero (see Remark 5.6) but the right hand side is non-negative.\nTaking expectation with respect to the randomness of D in (C.4), and using Lemma C.2 which upper bounds ED[PkP>k \u2022Xk] by ED[PkP>k \u2022XkD] = EU[PkP>k \u2022XkU] we get the desired inequality. (Note that PkP > k XkU = AkXkU = AkX 1/2 k UX 1/2 k .)\nC.1 Missing Auxiliary Lemmas\nIn this subsection we prove the following two auxiliary lemmas. The first one shall be used to bound the higher-order terms in Lemma 4.1.\nLemma C.1. For every q \u2265 2 and every \u03b7 \u2208 [ 0, 14e(q\u22121) ] , event Ek\u22121(D) implies that\n\u2223\u2223\u2223\u2223Tr (( X 1/q k + \u03b7X 1/q k Pk(I\u2212 \u03b7P>k X 1/q k Pk) \u22121P>k X 1/q k )q\u22121 D ) \u2212Tr ( X q\u22121 q k D )\u2223\u2223\u2223\u2223 \u2264 4\u03b7(q \u2212 1)\u2016D\u20162Tr(XkPkP>k ) .\nThe second one upper bounds the expectation of the right hand side of Lemma C.1. We highlight that the proof of Lemma C.2 is the only place in this paper that we have assumed k(U) = 3.\nLemma C.2. We have ED[Tr(PkP>k Xk)] \u2264 9 \u00b7 ED[Tr(PkP>k XkD)].\nNote that we can assume without loss of generality that \u03a3k\u22121, Xk and D are all diagonal matrices, which has been argued in the proof of Lemma 5.5. Therefore, all the proofs in this subsection will be given under this assumption.\n17Note when D satisfies Ek\u22121(D) we have ckI\u2212 \u03b7\u03a3k\u22121 1e I according to Proposition 5.4. This implies, as long as \u03b7 \u2264 e\u22121, it satisfies ckI\u2212 \u03b7\u03a3k 0 so Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) > 0.\nTo prove Lemma C.1 we need the following lemma:\nLemma C.3 (Monotonicity of Diagonal entries). Let A,D \u2208 Rd\u00d7d be two diagonal positive definite matrices,18 let B \u2208 Rd\u00d7d be PSD, then for every q \u2208 N\u2217 such that q\u2016A\u22121/2BA\u22121/2\u20162 < 1:\n0 \u2264 Tr((A + B)qD)\u2212Tr(AqD) \u2264 \u2016D\u20162 1\u2212 q\u2016A\u22121/2BA\u22121/2\u20162 Tr ( Aq\u22121B ) .\nProof of Lemma C.3. For every i \u2208 [D], let P be a matrix with all zero entries except Pi,i = 1. Then we have:\n[(A + B)q]i,i = Tr(P q(A + B)qPq) \u2265 Tr ((P(A + B)P)q)\n= ([A + B]i,i) q \u2265 [A]qi,i = [Aq]i,i .\nWhere the first inequality is due to the Lieb-Thirring inequality, and the last equality is because A is diagonal. Since D is a diagonal PSD matrix, we can conclude that19\nTr((A + B)qD)\u2212Tr(AqD) = d\u2211\ni=1\n[D]i,i ([(A + B) q \u2212Aq]i,i) \u2265 0 .\nand\nTr((A + B)qD)\u2212Tr(AqD) \u2264 max i\u2208[d] [D]i,i\nd\u2211\ni=1\n[(A + B)q \u2212Aq]i,i = \u2016D\u20162Tr((A + B)q \u2212Aq) .(C.5)\nWe focus on the term (A+B)q. We can re-write it as (A+B)q = ( A1/2(I + A\u22121/2BA\u22121/2)A1/2 )q .\nThen by Lieb-Thirring again, we have:\nTr((A + B)q) \u2264 Tr ( Aq/2 ( I + A\u22121/2BA\u22121/2 )q Aq/2 )\n\u2264 Tr ( Aq/2 ( I + 1\n1\u2212 q\u2016A\u22121/2BA\u22121/2\u20162 A\u22121/2BA\u22121/2\n) Aq/2 )\n\u2264 Tr(Aq) + q 1\u2212 q\u2016A\u22121/2BA\u22121/2\u20162\nTr ( Aq\u22121B ) . (C.6)\nWhere the second inequality uses (I + X)q I + q1\u2212q\u2016X\u20162 X for every PSD matrix X with q\u2016X\u20162 < 1. Putting together (C.5) and (C.6), we obtain:\nTr((A + B)qD)\u2212Tr(AqD) \u2264 q\u2016D\u20162 1\u2212 q\u2016A\u22121/2BA\u22121/2\u20162 Tr ( Aq\u22121B ) .\nProof of Lemma C.1. Under event Ek\u22121(D) , we know I\u2212\u03b7P>k X 1/q k Pk (1\u2212e\u03b7)I (see Proposition 5.4) and thus\n0 \u03b7X1/2qk Pk(I\u2212 \u03b7P>k X 1/2q k Pk) \u22121P>k X 1/q k\ne\u03b7\n1\u2212 e\u03b7 I .\nWe now apply Lemma C.3 with A = X 1/q k , B = \u03b7X 1/q k Pk(I\u2212\u03b7P>k X 1/q k Pk) \u22121P>k X 1/q k , and q = q\u22121. We can do so because A and D are both diagonal and (q\u22121)e\u03b71\u2212e\u03b7 < 1 under our assumption of \u03b7. The\n18In fact, we have only required them to be simultaneously diagonalizable. 19The authors would like to thank Elliott Lieb who has helped us obtain the inequality of the next line.\nconclusion of Lemma C.3 tells us that:\u2223\u2223\u2223\u2223Tr (( X 1/q k + \u03b7X 1/q k Pk(I\u2212 \u03b7P>k X 1/q k Pk) \u22121P>k X 1/q k )q\u22121 D ) \u2212Tr ( X q\u22121 q k D )\u2223\u2223\u2223\u2223\n\u2264 q \u2212 1 1\u2212 (q\u22121)e\u03b71\u2212e\u03b7\n\u2016D\u20162Tr(Aq\u22122B) \u2264 ( 2(q \u2212 1)\u2016D\u20162 )( \u03b7 1\u2212 e\u03b7Tr(XkPkP > k ) )\n\u2264 4\u03b7(q \u2212 1)\u2016D\u20162Tr(XkPkP>k ) . Above, the second and third inequalities have respectively used (q\u22121)e\u03b71\u2212e\u03b7 < 1 2 and 1 1\u2212e\u03b7 \u2264 2, which are both true by our assumption on \u03b7. Proof of Lemma C.2. Let \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd \u2265 0 be the eigenvalues of \u03a3k\u22121 and \u03bd1, . . . , \u03bdd be the corresponding eigenvectors. Let D1, \u00b7 \u00b7 \u00b7 , Dd be the diagonals of D. Recall that \u03a3k\u22121,Xk,D are all diagonal matrices. Define function f : Rd \u2192 R\nf(r1, \u00b7 \u00b7 \u00b7 , rd) def= d\u2211\ni=1\n[PkP > k ]i,i \u00b7 ri\n(ck \u2212 \u03bbi)q (recall that ck depends on (D1, . . . , Dd))\nWe shall prove that for some \u03b3 \u2208 (0, 1) that shall be chosen later, it satisfies for every i \u2208 [d], E[f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di, \u00b7 \u00b7 \u00b7 , Dd)] \u2265 E[f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di+1, \u00b7 \u00b7 \u00b7 , Dd)]\nwhere recall that both expectations are only over the randomness of D1, . . . , Dd. Let D\u2212i def = (D1, . . . , Di, Di+2, \u00b7 \u00b7 \u00b7 , Dd). Then, it is sufficient to prove that for every fixed possibility of D\u2212i, the following inequality holds:\nE Di [f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di, \u00b7 \u00b7 \u00b7 , Dd) | D\u2212i] \u2265 E Di [f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di+1, \u00b7 \u00b7 \u00b7 , Dd) | D\u2212i] .\nTherefore, in the remaining proofs, we shall consider Di as the only random variable, and thus ck only depends on Di. For a fixed value s \u2265 1 that we shall choose later, we can let c be the (unique) value of ck when Di = s\u03b3.\nLetting g(x) def = x(ck\u2212\u03bbi)q , we make three quick observations:\n1. g(\u03b3) = \u03b3(ck\u2212\u03bbi)q is a monotone decreasing function of Di.\nThis is so because ck is a monotone increasing function of Di.\n2. g(Di) = Di\n(ck\u2212\u03bbi)q is a monotone decreasing function of Di.\nThis is because g(Di) = 1\u2212 \u2211 j 6=i Dj (ck\u2212\u03bbj)q = 1 but ck is a monotone increasing function of Di.\n3. When Di \u2264 s\u03b3, we have g(\u03b3) \u2264 s\u03b3Di \u03b3 (c\u2212\u03bbi)q .\nThis is because g(\u03b3) = \u03b3Di ( 1\u2212\u2211j 6=i Dj (ck\u2212\u03bbj)q ) \u2264 \u03b3Di ( 1\u2212\u2211j 6=i Dj (c\u2212\u03bbj)q ) = \u03b3Di s\u03b3 (c\u2212\u03bbj)q , where the first inequality is because ck \u2264 c when Di \u2264 s\u03b3 (by the monotone increasing of ck with respect to Di), and the second equality is according to the definition of c.\nCombining the above three observations, we have:\nE[g(Di)] \u2265 Pr[Di \u2265 s\u03b3]E[g(Di) | Di \u2265 s\u03b3] \u2265 Pr[Di \u2265 s\u03b3] s\u03b3\n(c\u2212 \u03bbi)q\nE[g(\u03b3)] \u2264 Pr[Di \u2265 s\u03b3]E[g(\u03b3) | Di \u2265 s\u03b3] + E [ 1\nDi\n] s\u03b32\n(c\u2212 \u03bbi)q\n\u2264 \u03b3 (c\u2212 \u03bbi)q\n+ E [ 1\nDi\n] s\u03b32\n(c\u2212 \u03bbi)q \u2264 s\u03b3 (c\u2212 \u03bbi)q ( 1 s + E [ \u03b3 Di ]) .\nRecall that each Di = 1 3(\u3008\u03bdi, u1\u30092 + \u3008\u03bdi, u2\u30092 + \u3008\u03bdi, u3\u30092) where u1, u2, u3 are three normal Gaussian random vectors. Therefore, each 3Di has a chi-square distribution of degree 3, which implies E[ 1Di ] = 3 and Pr[Di \u2265 1 3 ] > 2 3 . In sum, if we take \u03b3 = 1 9 and s = 3, we have:\nE Di [g(Di)] \u2265 E Di [g(\u03b3)] .\nFinally, this implies\nE Di [f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di, \u00b7 \u00b7 \u00b7 , Dd)\u2212 f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di+1, \u00b7 \u00b7 \u00b7 , Dd) | D\u2212i] = [PkP>k ]i,i E Di [g(Di)\u2212 g(\u03b3)|D\u2212i] \u2265 0 .\nso we have\nE D [f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di, \u00b7 \u00b7 \u00b7 , Dd)] \u2265 E D [f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3,Di+1, \u00b7 \u00b7 \u00b7 , Dd)] . In particular,\nE[Tr(PkP>k XkD)] = E[f(D1, \u00b7 \u00b7 \u00b7 , Dd)] \u2265 E[f(\u03b3, \u00b7 \u00b7 \u00b7 , \u03b3)] = \u03b3 E[Tr(PkP>k Xk)] ."}, {"heading": "D Proof for Section 5.3", "text": "Lemma 5.7. For all q \u2265 2 and \u03b7 > 0,\nE [ Tr (( ck+1I\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<(k+1)(U) ] \u2212 E [ Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) ]\n\u2264 \u2212(q \u2212 1)(E[ck+1]\u2212 E[ck])\nProof. Recall that ck+1 \u2265 ck because all matrices Ak are PSD. Denoting by \u03bd1, . . . , \u03bdd the eigenvectors of \u03a3k with non-increasing eigenvalues \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd,20 we have for every U,\nTr (( ck+1I\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u2212Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U )\n= d\u2211\ni=1\n\u03bd>i U\u03bdi (ck+1 \u2212 \u03b7\u03bbi)q\u22121 \u2212 d\u2211\ni=1\n\u03bd>i U\u03bdi (ck \u2212 \u03b7\u03bbi)q\u22121 \u00ac \u2264 \u2212(q \u2212 1)(ck+1 \u2212 ck) \u00b7 d\u2211\ni=1\n\u03bd>i U\u03bdi (ck+1 \u2212 \u03b7\u03bbi)q\n= \u2212(q \u2212 1)(ck+1 \u2212 ck) \u00b7Tr (( ck+1I\u2212 \u03b7\u03a3k )\u2212q U )\n= \u2212(q \u2212 1)(ck+1 \u2212 ck)Tr(Xk+1U) = \u2212(q \u2212 1)(ck+1 \u2212 ck) . (D.1)\nAbove, \u00ac is derived from inequality 1 (c+x)q\u22121 \u2212 1xq\u22121 \u2264 \u2212 (q\u22121)c (c+x)q (for every c \u2265 0, x > 0) which follows from the convexity of function f(x) = 1 xq\u22121 .\nNext, we observe that for every U that does not satisfy E<k(U), the very right hand side of (D.1) is still non-negative. Therefore, we conclude that for all U, Tr (( ck+1I\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2212Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2264 \u2212(q \u2212 1)(ck+1 \u2212 ck) . Finally, since 1E<(k+1)(U) \u2264 1E<k(U) and Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u2265 0, we have Tr (( ck+1I\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<(k+1)(U) \u2212Tr (( ckI\u2212 \u03b7\u03a3k )\u2212(q\u22121) U ) \u00b7 1E<k(U) \u2264 \u2212(q \u2212 1)(ck+1 \u2212 ck)\nand taking expectation we finish the proof of Lemma 5.7. 20This is different from the proof of Lemma 5.5 where we defined them to be eigenvectors of \u03a3k\u22121."}, {"heading": "E Proof of Theorem 1: Oblivious Online Eigenvector", "text": "Proof of Theorem 1. Combining Lemma 5.5 and Lemma 5.7, we have\nE [ Tr ( X\n1\u22121/q k+1 U ) \u00b7 1E<k+1(U) ] \u2212 E [ Tr ( X 1\u22121/q k U ) \u00b7 1E<k(U) ]\n\u2264 \u2212(q \u2212 1)(E[ck+1]\u2212 E[ck]) + (q \u2212 1)\u03b7(1 +O(\u03b7q5 log(d/\u03b4))) \u00b7 E [ Ak \u2022X1/2k UX 1/2 k ] + (\u03b7T + e)T\u03b4 .\nTelescoping it for all k = 1, . . . , T , we have\nE [ Tr ( X\n1\u22121/q T+1 U ) \u00b7 1E<T+1(U) ] \u2212 E [ Tr ( X 1\u22121/q 1 U ) \u00b7 1E<1(U) ] (E.1)\n\u2264 \u2212(q \u2212 1)(E[cT+1]\u2212 E[c1]) + (q \u2212 1)\u03b7(1 +O(\u03b7q5 log(d/\u03b4))) \u00b7 E [ T\u2211\nk=1\nAk \u2022X1/2k UX 1/2 k\n] + (\u03b7T + e)T 2\u03b4 .\nWe make four quick observations:\n\u2022 Regardless of the randomness of U, we have Tr ( X\n1\u22121/q T+1 U ) \u00b7 1E<T+1(U) \u2265 0.\n\u2022 Regardless of the randomness of U, we have cT+1 \u2265 \u03b7\u03bbmax(\u03a3T ). \u2022 We have E[c1] \u2264 e. To derive that, we use 1cq1 TrU = Tr(X1U) = 1 which implies c1 =\n(TrU)1/q. Notice that TrU = 13 \u2211 i\u2208[d],j\u2208[3](uj,i) 2 so 3TrU is distributed according to chisquared distribution \u03c72(3d) whose PDF is p(x) = 2 \u2212 3d2 e\u2212 x 2 x 3d 2 \u22121\n\u0393(3d/2) . We thus have\nE[c1] = \u222b \u221e\n0 x1/qp(x)dx =\n21/q\u0393 (\n3d 2 + 1 q\n)\n\u0393 (\n3d 2\n) \u2264 21/q \u00b7 (3d\n2\n)1/q = (3d)1/q \u2264 e .\nAbove, the first inequality uses \u0393(x+a)\u0393(x) \u2264 xa for a \u2208 (0, 1) and x > 0 (cf. Wendell [32]), and the second inequality uses our assumption on q.\n\u2022 E [ Tr ( X\n1\u22121/q 1 U ) \u00b71E<1(U) ] \u2264 e. This is because Tr(X1\u22121/q1 U) = 1cq\u221211 TrU = c1 and E[c1] \u2264 e.\nSubstituting the four observations above into the telescoping sum (E.1), we have\n(q \u2212 1)\u03b7\u03bbmax(\u03a3T ) \u2264 e+ (q \u2212 1)e+ (q \u2212 1)\u03b7(1 +O(\u03b7q5 log(d/\u03b4))) \u00b7 E [ T\u2211\nk=1\nAk \u2022X1/2k UX 1/2 k\n] + (\u03b7T + e)T 2\u03b4 .\nUsing the inequality (\u03b7T + e)T 2\u03b4 \u2264 (1 + e)T 3\u03b4, we conclude that if we choose \u03b4 = 11+eT\u22123, then\n(q \u2212 1)\u03b7\u03bbmax(\u03a3T ) \u2264 (q \u2212 1)\u03b7 ( 1 +O(\u03b7q5 log(dT )) ) \u00b7 E [ T\u2211\nk=1\nAk \u2022X1/2k UX 1/2 k\n] + 4(q \u2212 1) .\nDividing both sides by (q \u2212 1)\u03b7 we arrive at the desired inequality."}, {"heading": "F Proof of Theorem 2: Adversarial Online Eigenvector", "text": "Proof of Theorem 2. Before beginning our proof, let us emphasize that in this adversarial setting,\n\u2022 Ak and \u03a3k can depend on the randomness of U1, . . . ,Uk\u22121. \u2022 Xk and ck depend on the randomness of Uk and \u03a3k\u22121 (and thus also on U1, . . . ,Uk\u22122).\nAlgorithm 2 FTCLadv(T, q, \u03b7)\nInput: T , number of iterations; q \u2265 2, an even integer, theory-predicted choice q = \u0398(log(dT )) \u03b7, the learning rate. theory-predicted choice \u03b7 = log\u22123(dT )/\u221a\u03bbmax(\u03a3T )\n1: for k \u2190 1 to T do 2: Choose 3 vectors u1, u2, u3 \u2208 Rd where the 3d coordinates are i.i.d. drawn from N (0, 1). 3: Uk \u2190 13 ( u1u > 1 + u2u > 2 + u3u > 3 ) .\n4: \u03a3k\u22121 \u2190 \u2211k\u22121 i=1 Ai.\n5: Denote by Xk \u2190 ( ckI\u2212 \u03b7\u03a3k\u22121 )\u2212q where ck is the unique constant satisfying that\nckI\u2212 \u03b7\u03a3k\u22121 0 and Tr ( XkUk ) = 1 .\n6: Compute X 1/2 k UkX 1/2 k = \u22113 j=1 pj \u00b7 yjy>j where y1, y2, y3 are orthogonal unit vectors in Rd. This is an eigendecomposition and it satisfies p1, p2, p3 \u2265 0 and p1 + p2 + p3 = 1. 7: Choose wk \u2190 yj with probability pj . 8: Play strategy wk and receive matrix Ak. 9: end for\nConsider for analysis purpose only another random matrix U\u0303 drawn from distribution D, independent of the randomness of U1, . . . ,UT . Define c\u0303k to be the unique constant satisfying c\u0303kI\u2212 \u03b7\u03a3k\u22121 0 and Tr((c\u0303kI\u2212 \u03b7\u03a3k\u22121)\u2212qU) = 1, and define X\u0303k = (c\u0303kI\u2212 \u03b7\u03a3k\u22121)\u2212q.\nNow, if we fix the randomness of U1, . . . ,Uk\u22121, the matrices \u03a3k\u22121 and Ak become fixed. The fact that Uk and U\u0303 are both drawn from the same distribution D (and the fact that Xk and X\u0303k are computed from Uk and U\u0303 in the same way) implies\nE Uk\n[ Ak \u2022X1/2k UkX 1/2 k \u2223\u2223\u2223U1, . . . ,Uk\u22121 ]\n= E U\u0303\n[ Ak \u2022 X\u0303k 1/2 U\u0303X\u0303k 1/2 \u2223\u2223\u2223U1, . . . ,Uk\u22121 ] (F.1)\nNow, consider random variables Zk = w > k Akwk. We have that Zk is Fk-measurable for Fk generated by U1, ...,Uk, w1, ..., wk. According to the martingale concentration Lemma F.1, we have\nPr\n[ T\u2211\nk=1\nZk \u2264 (1\u2212 \u00b5) T\u2211\nk=1\nE[Zk | Fk\u22121]\u2212 log 1p \u00b5\n] \u2264 p .\nAt the same time, we have\nE[Zk | Fk\u22121] = E wk,Uk\n[ Ak \u2022 wkw>k | U1, . . . ,Uk\u22121 ] = E\nUk\n[ Ak \u2022X1/2k UkX 1/2 k | U1, . . . ,Uk\u22121 ]\n= E U\u0303\n[ Ak \u2022 X\u0303k 1/2 U\u0303X\u0303k 1/2 | U1, . . . ,Uk\u22121 ] ,\nwhere the last inequality comes from (F.1). In sum, with probability at least 1 \u2212 p (over the randomness of U1, . . . ,UT , w1, . . . , wT ), we have\nT\u2211\nk=1\nw>k Akwk \u2265 (1\u2212 \u00b5)E U\u0303\n[ T\u2211\nk=1\nAk \u2022 X\u0303k 1/2 U\u0303X\u0303k 1/2 \u2223\u2223\u2223U1, . . . ,UT\u22121 ] \u2212 log 1p \u00b5 .\nApplying Theorem 1 we have (more specifically, fixing each possible sequence U1, . . . ,UT , we have a fixed sequence of A1, . . . ,AT and can apply Theorem 1):\nT\u2211\nk=1\nw>k Akwk \u2265 (1\u2212 \u00b5) ( 1\u2212O(\u03b7q5 log(dT )) ) \u03bbmax(\u03a3T )\u2212 4\n\u03b7 \u2212 log 1p \u00b5 .\nChoosing \u00b5 = \u03b7 \u00b7 log(1/p), we finish the proof of Theorem 2.\nF.1 A Concentration Inequality for Martingales\nWe show the following (simple) martingale concentration lemma that we believe is classical but have not found anywhere else.\nLemma F.1 (Concentration). Let {Zt}Tt=1 be a random process with respect to a filter {0,\u2126} = F0 \u2282 F1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT and each Zt \u2208 [0, 1] is Ft-measurable. For every p, \u00b5 \u2208 (0, 1),\nPr\n[ T\u2211\nt=1\nZt \u2264 (1\u2212 \u00b5) T\u2211\nt=1\nE[Zt | Ft\u22121]\u2212 log 1p \u00b5\n] \u2264 p .\nWe emphasize here that E[Zt | Ft\u22121] is Ft\u22121-measurable and thus not a constant.\nProof of Lemma F.1. Like in classical concentration proofs, we have\nPr [\u2211T t=1 Zt \u2264 (1\u2212 \u00b5) \u2211T t=1 E[Zt | Ft\u22121]\u2212 log 1 p \u00b5 ]\n= Pr [\u2211T t=1 ((1\u2212 \u00b5)E[Zt | Ft\u22121]\u2212 Zt) \u2265 log 1 p \u00b5 ] = Pr [ exp { \u00b5 (\u2211T t=1 ((1\u2212 \u00b5)E[Zt | Ft\u22121]\u2212 Zt) )} \u2265 1p ] \u2264 pE [ exp { \u00b5 (\u2211T t=1 ((1\u2212 \u00b5)E[Zt | Ft\u22121]\u2212 Zt) )} ] . (F.2)\nDenote by Yt = \u00b5(1\u2212 \u00b5)E[Zt | Ft\u22121]\u2212 \u00b5Zt, we know that each Yt \u2208 [\u22121, 1] is Ft-measurable.\nE [ exp {\u2211T t=1 Yt }] = E [ E [ exp {\u2211T t=1 Yt } \u2223\u2223FT\u22121 ]]\n= E [ exp {\u2211T\u22121 t=1 Yt } E [ eYT \u2223\u2223FT\u22121 ]] \u2264 E [ exp {\u2211T\u22121 t=1 Yt } E [ 1 + YT + Y 2 T \u2223\u2223FT\u22121 ]] .\nNow, we focus on the term YT + Y 2 T :\nYT + Y 2 T \u2264 \u00b5(1\u2212 \u00b5)E[ZT | FT\u22121]\u2212 \u00b5ZT + \u00b52(1\u2212 \u00b5)2 E[ZT | FT\u22121]2 + \u00b52Z2T \u2264 \u00b5(1\u2212 \u00b5)E[ZT | FT\u22121]\u2212 \u00b5ZT + \u00b52(E[ZT | FT\u22121] + \u00b5ZT ) .\n(The first inequality has used (a\u2212 b)2 \u2264 a2 + b2 when a, b \u2265 0, and the second has used Zt \u2208 [0, 1].) Taking the conditional expectation, we obtain E[YT + Y 2T | FT\u22121] \u2264 0 and this implies\nE [ exp {\u2211T t=1 Yt }] \u2264 E [ exp {\u2211T\u22121 t=1 Yt }] \u2264 \u00b7 \u00b7 \u00b7 \u2264 e0 = 1 .\nPlugging this into (F.2) completes the proof of Lemma F.1."}, {"heading": "G Proof of Theorem 3: Implementation Details", "text": "Resolution to Issue (a). We first point out that the final regret blows up by an additive value \u03b5\u0303 as long as the eigendecomposition \u22113 j=1 pj \u00b7 yjy>j is computed to satisfy21\n\u2225\u2225\u2225 3\u2211\nj=1\nX 1/2 k uju > j X 1/2 k \u2212\n3\u2211\nj=1\npj \u00b7 yjy>j \u2225\u2225\u2225\n2 \u2264 \u03b5\u0303 poly(d, T ) .\nMoreover, this can be done in time O(d) as long as we can compute the three vectors { X1/2uj } j\u2208[3] to an additive \u03b5\u0303/poly(d, T ) error in Euclidean norm. This can be done by applying ( ckI\u2212\u03b7\u03a3k\u22121\n)\u22121 a number q/2 times to vector uj , each again to an error \u03b5\u0303/poly(d, T ). In sum, we can repeatedly apply Lemma G.1 and the final running time only logarithmically depends on \u03b5\u0303/poly(d, T ).\nResolution to Issue (c). We choose \u03b4 = p/T and revisit the event Ek(U) defined in Def. 5.2. According to Lemma 5.3 and union bound, it satisfies with probability at least 1 \u2212 p, all the T events E0(U1), . . . , ET\u22121(UT ) are satisfied. If we apply Proposition 5.4, we immediately have that\nq \u2265 3 log(2dT/p) =\u21d2 \u2200k \u2208 [T ] : (\u03b7\u03bbmax(\u03a3k\u22121) + e)I ckI ckI\u2212 \u03b7\u03a3k\u22121 1\ne I . (G.1)\nThis implies, throughout the algorithm, whenever we want to compute ( ckI\u2212\u03b7\u03a3k\u22121 )\u22121 , the matrix under inversion has a bounded condition number. We have the following lemma which relies on classical results from convex optimization: Lemma G.1. Given any b \u2208 Rd, the computation of a \u2208 Rd satisfying \u2225\u2225a\u2212 ( ckI\u2212 \u03b7\u03a3k\u22121 )\u22121 b \u2225\u2225 2 \u2264 \u03b5\u2016b\u20162 can be done in running time \u2022 O\u0303 (\u221a \u03b7\u03bbmax(\u03a3k\u22121) + 1 \u00b7 nnz(\u03a3k\u22121) \u00b7 log \u03b5\u22121 ) if conjugate gradient or accelerated gradient descent\nis used;\n\u2022 O\u0303 (( nnz(\u03a3k\u22121)+ \u221a \u03b7k\u00b7maxi\u2208[k\u22121] { nnz(\u03a3k\u22121)3/4nnz(Ai)1/4 }) log \u03b5\u22121 ) if accelerated SVRG is used.\nProof. This inverse operation is the same as minimizing a convex function f(x) def = 12x >(ckI \u2212 \u03b7\u03a3k\u22121 ) x\u2212 b>x. The condition number of Hessian matrix \u22072f(x) is at most O(\u03b7\u03bbmax(\u03a3k\u22121) + 1) according to (G.1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.\nAs for the SVRG type of result, one can write f(x) = 1k\u22121 \u2211k\u22121 i=1 fi(x) where fi(x) = x >(ckI\u2212\n\u03b7(k \u2212 1)Ai ) x\u2212 b>x. Each computation of \u2207f(x) requires time O(nnz(\u03a3k\u22121)) and that of \u2207fi(x) requires time O(nnz(Ai)). Since \u2016\u22072fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time O\u0303 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) . Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to O\u0303 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)3/4nnz(Ai)1/4} ) .\nResolution to Issue (b). In each iteration, we need to compute some constant ck such that Tr(X1/2UX1/2) = 1. This can be done via a \u201cbinary search\u201d procedure which was used widely for shift-and-invert based methods [15]:\n1. Begin with c = \u03b7k + e which is a safe upper bound on ck according to (G.1).\n21We refrain from doing this precisely here because because MMWU analysis is generally \u201crobust against noise\u201d. The authors of [7] have shown that the potential Tr(X\n1\u22121/q k ) is robust against noise and a completely analogous (but\nlengthy) proof of theirs applies to this paper.\n2. Repeatedly compute some value \u03c3\u0303 which is a 9/10 approximation of \u03c3 def = c \u2212 \u03b7\u03bbmax(\u03a3k\u22121).\n(This requires O(1) iterations of power method applied to (cI\u2212 \u03b7\u03a3k\u22121)\u22121 [15].) 3. If \u03c3\u0303 \u2264 1e \u00b7 910 (which implies \u03c3 \u2264 1e ), we end the procedure; otherwise we update c \u2190 c \u2212 \u03c3\u0303/2\nand go to Step 2.\nIt is a simple exercise (with details given in [15]) to show that when the procedure ends, it satisfies 1 2e \u2264 c \u2212 \u03b7\u03a3k\u22121 \u2264 1e so c is a lower bound on ck. At this point, it suffices to perform a binary search between [ c, \u03b7k + e ] to find ck. Note that, according to resolution to issue (a), it suffices to compute ck to an additive error of \u03b5\u0303/poly(d, T ). In sum, the above binary search procedure requires only a logarithmic number of oracle calls to (cI\u2212 \u03b7\u03a3k\u22121)\u22121, and each time we do so it satisfies c \u2264 \u03b7k + e and (\u03b7k + e)I cI\u2212 \u03b7\u03a3k\u22121 12eI. For this reason, the same computational complexity in Lemma G.1 applies.\nThe three resolutions above, combined together, imply that the running time statements in Theorem 3 hold."}, {"heading": "H Proof of Theorem 4: Stochastic Online Eigenvector", "text": "Proof of Theorem 4. Define \u03a6k def = (I + \u03b7Ak) \u00b7 \u00b7 \u00b7 (I + \u03b7A1)uu>(I + \u03b7A1) \u00b7 \u00b7 \u00b7 (I + \u03b7Ak) and \u03a8k def= (I + \u03b7Ak) \u00b7 \u00b7 \u00b7 (I + \u03b7AT )\u03bd1\u03bd>1 (I + \u03b7AT ) \u00b7 \u00b7 \u00b7 (I + \u03b7Ak). Let \u03bd1 and \u03bb1 respectively denote the largest eigenvector and eigenvalue of B. We first make three simple calculations:\nTr(\u03a6T ) = Tr ( (I + \u03b7AT )\u03a6T\u22121(I + \u03b7AT ) ) = Tr(\u03a6T\u22121) + 2\u03b7Tr ( AT\u03a6T\u22121 ) + \u03b72Tr ( AT\u03a6T\u22121AT )\n\u00ac \u2264 Tr(\u03a6T\u22121) \u00b7 (1 + (2\u03b7 + \u03b72)Tr(ATwTw>T ))  \u2264 Tr(\u03a6T\u22121) \u00b7 e(2\u03b7+\u03b7 2)Tr(ATwTw > T ) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2016u\u201622 \u00b7 e(2\u03b7+\u03b7 2) \u2211T k=1 w > k Akwk . (H.1)\nE[\u03bd>1 \u03a6T \u03bd1] = E [ Tr ( \u03bd1\u03bd > 1 (I + \u03b7AT )\u03a6T\u22121(I + \u03b7AT ) )] = E[Tr(\u03bd1\u03bd>1 (I + 2\u03b7AT )\u03a6T\u22121) + \u03b72\u03bd>1 AT\u03a6T\u22121AT \u03bd1]\n\u2265 E[Tr(\u03bd1\u03bd>1 (I + 2\u03b7B)\u03a6T\u22121) = (1 + 2\u03b7\u03bb1)E[\u03bd>1 \u03a6T\u22121\u03bd1] \u00ae \u2265 e2\u03b7\u03bb1\u22122\u03b72\u03bb21 E[\u03bd>1 \u03a6T\u22121\u03bd1] \u2265 \u00b7 \u00b7 \u00b7 \u00af \u2265 e(2\u03b7\u03bb1\u22122\u03b72\u03bb21)T . (H.2)\nE[(\u03bd>1 \u03a6T \u03bd1)2] = E [ Tr(\u03a821) ] = E [ Tr((I + \u03b7A1) 2\u03a82(I + \u03b7A1) 2\u03a82) ] \u00b0 \u2264 E [ Tr((I + \u03b7A1) 4\u03a822) ]\n\u00b1 \u2264 E [ Tr ( (I + (4\u03b7 + 11\u03b72)A1)\u03a8 2 2 )] = Tr ( (I + (4\u03b7 + 11\u03b72)B)E[\u03a822] ) \u2264 e4\u03b7\u03bb1+11\u03b72\u03bb1 E[Tr(\u03a822)]\n\u2264 \u00b7 \u00b7 \u00b7 \u2264 e(4\u03b7\u03bb1+11\u03b72\u03bb1)T . (H.3) Above, \u00ac uses Tr(AT\u03a6T\u22121AT ) \u2264 Tr(AT\u03a6T\u22121) as well as wkw>k = \u03a6k\u22121/Tr(\u03a6k\u22121),  uses 1+x \u2264 ex, \u00ae uses 1+2x \u2265 e2x\u22122x2 for x \u2208 [0, 1],  uses E[\u03bd>1 \u03a60\u03bd1] = 1, \u00b0 uses the Lieb-Thirring inequality Tr(ABAB) \u2264 Tr(A2B2),22 \u00b1 uses (I + \u03b7A1)4 I + (4\u03b7 + 11\u03b72)A1.\nNow, we can combine (H.2) and (H.3) and apply Chebyshev\u2019s inequality: for every p \u2208 (0, 1)\nPr [ \u03bd>1 \u03a6T \u03bd1 \u2264 e(2\u03b7\u03bb1\u22122\u03b7\n2\u03bb21)T \u2212 1\u221a p\n\u221a e(4\u03b7\u03bb1+11\u03b72\u03bb1)T \u2212 (e(2\u03b7\u03bb1\u22122\u03b72\u03bb21)T )2 ] \u2264 p .\nIn other words, as long as \u03bb1\u03b7 2T \u2264 p/60, we have with probability at least 1\u2212 p,\nTr(\u03a6T ) \u2265 \u03bd>1 \u03a6T \u03bd1 \u2265 e(2\u03b7\u03bb1\u22122\u03b7 2\u03bb21)T \u00b7 (1\u2212 p\u22121/2\n\u221a e15\u03b72\u03bb1T \u2212 1) \u2265 1\n2 e(2\u03b7\u03bb1\u22122\u03b7 2\u03bb21)T . (H.4)\n22In fact, we do not need the full power of Lieb-Thirring here because one of the two matrices is rank-1.\nAt the same time, using tail bound for chi-squared distribution, it is easy to derive that with probability at least 1\u2212 p we have \u2016u\u201622 \u2264 d+O( \u221a d log(1/p)) \u2264 O(d+ log(1/p)).23 Combining this with (H.1) and (H.4) we have\n(2\u03b7 + \u03b72) T\u2211\nk=1\nw>k Akwk \u2265 2\u03b7T\u03bb1 \u2212 2\u03b72\u03bb21T \u2212O ( log(d+ log(1/p)) ) ,\nwhich after dividing both sides by 2\u03b7 + \u03b72 finishes the proof of Theorem 4."}, {"heading": "I A Simple Lower Bound for the \u03bb-Refined Language", "text": "We sketch the proof that for the stochastic online eigenvector problem, for every \u03bb \u2208 (0, 1), there exists a constant C > 0, a PSD matrix B satisfying B \u03bbI, and a distribution D of (even rank-1) matrices with spectral norm at most 1 and expectation equal to B, such that for every learning algorithm Learner, the total regret must be at least C \u00b7 \u221a \u03bbT .\nSuch a lower bound naturally translates to the harder adversarial or oblivious settings. We prove this lower bound by reducing the problem to an information-theoretic lower bound that has appeared in our separate paper [6].\nThe lower bound in [6] states that, for every 1 \u2265 \u03bb \u2265 \u03bb2 \u2265 0, there exists a PSD matrix B with the largest two eigenvalues being \u03bb and \u03bb2, and a distribution D of rank-1 matrices with spectral norm at most 1 and expectation equal to D. Furthermore, for any algorithm Alg that takes T samples from D and outputs a unit vector v \u2208 Rd, it must satisfy\nE[1\u2212 \u3008v, \u03bd1\u30092] \u2265 \u2126 ( \u03bb (\u03bb\u2212 \u03bb2)2T )\nfor every T \u2265 \u2126(\u03bb/(\u03bb\u2212 \u03bb2)2) ,\nwhere \u03bd1 is the first eigenvector of B, and the expectation is over the randomness of Alg and the T samples from D. After rewriting, we have\nE[v>Bv] \u2264 E[\u03bb\u3008v, \u03bd1\u30092 + \u03bb2(1\u2212 \u3008v, \u03bd1\u30092)] = E[\u03bb\u2212 (\u03bb\u2212 \u03bb2)(1\u2212 \u3008v, \u03bd1\u30092)] \u2264 \u03bb\u2212 \u2126 ( \u03bb (\u03bb\u2212 \u03bb2)T ) .\nIf we choose \u03bb2 such that T = \u0398(\u03bb/(\u03bb\u2212 \u03bb2)2), then the above inequality becomes E[v>Bv] \u2264 \u03bb\u2212 \u2126( \u221a \u03bb/T ) .\nFinally, for any algorithm Learner for the stochastic online eigenvector problem, suppose Learner takes T samples A1, . . . ,AT from D and outputs unit vectors v1, . . . , vT , we can define a corresponding algorithm Alg that outputs v = vk each with probability 1/T . In this way, we have\nE [ T\u2211\nk=1\nv>k Akvk ] = E [ T\u2211\nk=1\nv>k Bvk ] = T E [ vBv ] \u2264 \u03bbT \u2212 \u2126( \u221a \u03bbT ) .\nIn other words, the total regret of Learner must be at least \u2126( \u221a \u03bbT )."}], "references": [{"title": "Online linear optimization via smoothing", "author": ["Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari"], "venue": "In COLT, pages 807\u2013823,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Spectral smoothing via random matrix perturbations", "author": ["Jacob Abernethy", "Chansoo Lee", "Ambuj Tewari"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Using optimization to obtain a widthindependent, parallel, simpler, and faster positive SDP solver", "author": ["Zeyuan Allen-Zhu", "Yin Tat Lee", "Lorenzo Orecchia"], "venue": "In Proceedings of the 27th ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Spectral Sparsification and Regret Minimization Beyond Multiplicative Updates", "author": ["Zeyuan Allen-Zhu", "Zhenyu Liao", "Lorenzo Orecchia"], "venue": "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "author": ["Zeyuan Allen-Zhu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "The Multiplicative Weights Update Method: a Meta- Algorithm and Applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A combinatorial, primal-dual approach to semidefinite programs", "author": ["Sanjeev Arora", "Satyen Kale"], "venue": "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing - STOC", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Online principal components analysis", "author": ["Christos Boutsidis", "Dan Garber", "Zohar Karnin", "Edo Liberty"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Analyze gauss: optimal bounds for privacy-preserving principal component analysis", "author": ["Cynthia Dwork", "Kunal Talwar", "Abhradeep Thakurta", "Li Zhang"], "venue": "In STOC,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "ArXiv e-prints,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Online learning of eigenvectors", "author": ["Dan Garber", "Elad Hazan", "Tengyu Ma"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["Moritz Hardt", "Eric Price"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "QIP = PSPACE", "author": ["Rahul Jain", "Zhengfeng Ji", "Sarvagya Upadhyay", "John Watrous"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Online pca with spectral bounds", "author": ["Zohar Karnin", "Edo Liberty"], "venue": "In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Pca with gaussian perturbations", "author": ["Wojciech Kot  lowski", "Manfred K. Warmuth"], "venue": "ArXiv e-prints,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Constructing linear-sized spectral sparsification in almost-linear time", "author": ["Yin Tat Lee", "He Sun"], "venue": "In FOCS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A Universal Catalyst for First-Order Optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Online pca with optimal regrets", "author": ["Jiazhong Nie", "Wojciech Kot  lowski", "Manfred K Warmuth"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Fast Approximation Algorithms for Graph Partitioning using Spectral and Semidefinite-Programming Techniques", "author": ["Lorenzo Orecchia"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Approximating the exponential, the lanczos method and an \u00d5(m)-time spectral algorithm for balanced separator", "author": ["Lorenzo Orecchia", "Sushant Sachdeva", "Nisheeth K. Vishnoi"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Faster and simpler width-independent parallel algorithms for positive semidefinite programming", "author": ["Richard Peng", "Kanat Tangwongsan"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "SDCA without Duality, Regularization, and Individual Convexity", "author": ["Shai Shalev-Shwartz"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Convergence of stochastic gradient descent for pca", "author": ["Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["Jonathan Richard Shewchuk"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "Note on the gamma function", "author": ["J.G. Wendel"], "venue": "The American Mathematical Monthly,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1948}], "referenceMentions": [{"referenceID": 24, "context": "Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields.", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).", "startOffset": 237, "endOffset": 240}, {"referenceID": 11, "context": "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).", "startOffset": 254, "endOffset": 258}, {"referenceID": 24, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 174, "endOffset": 185}, {"referenceID": 9, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 174, "endOffset": 185}, {"referenceID": 26, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 174, "endOffset": 185}, {"referenceID": 25, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 207, "endOffset": 211}, {"referenceID": 6, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 235, "endOffset": 242}, {"referenceID": 20, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 235, "endOffset": 242}, {"referenceID": 17, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 282, "endOffset": 286}, {"referenceID": 0, "context": "The player receives a gain Ak \u2022Wk def = Tr(AkWk) \u2208 [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 24, "context": "The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Some authors also refer to MMWU as the follow-the-regularized-leader strategy or FTRL for short, because MMWU can be analyzed from a mirror-descent view with the matrix entropy function as its regularizer [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 1, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 12, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 15, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 19, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 23, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 6, "context": "This more challenging setting is very desirable for multiple reasons: \u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate.", "startOffset": 117, "endOffset": 124}, {"referenceID": 20, "context": "This more challenging setting is very desirable for multiple reasons: \u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate.", "startOffset": 117, "endOffset": 124}, {"referenceID": 6, "context": "MMWU [7, 9] \u00d5( \u221a T ) O(d) \u00d5 ( d \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 8, "context": "MMWU [7, 9] \u00d5( \u221a T ) O(d) \u00d5 ( d \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 6, "context": "MMWU with JL [7, 28] \u00d5( \u221a T ) \u00d5 ( T 5 4 nnz(\u03a3) ) \u00d5 ( 1 \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 26, "context": "MMWU with JL [7, 28] \u00d5( \u221a T ) \u00d5 ( T 5 4 nnz(\u03a3) ) \u00d5 ( 1 \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 15, "context": "FTPL (T \u2265 d only) [16] \u00d5( \u221a dT ) \u00d5 ( T 3 4 d\u2212 1 4 nnz(\u03a3) ) \u00d5 ( d \u03b53.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "block power method [16] \u00d5( \u221a T ) O ( nnz(\u03a3) ) \u00d5 ( 1 \u03b52 nnz(\u03a3) )", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 6, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 20, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 26, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 1, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 12, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 15, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 19, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 15, "context": "Most notably, Garber, Hazan and Ma [16] proposed to compute an (approximate) leading eigenvector of the matrix \u03a3k\u22121 +rr> at iteration k, where r is a random vector whose norm is carefully chosen.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "For this problem, Garber, Hazan, and Ma [16] showed that a block power method matches the optimum regret and enjoys an efficient O(nnz(\u03a3T ))-time implementation per iteration.", "startOffset": 40, "endOffset": 44}, {"referenceID": 28, "context": "Shamir [30] analyzed the so-called Oja\u2019s algorithm but his total regret is O( \u221a dT log(T )) which is a factor \u221a d worse than optimum.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "If nnz(\u03a3T ) = d 2 and nnz(A) = O(d), our running time is O(d) times faster than [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "In the special case of Ak being rank-1, the \u00d5( \u221a T ) regret for Oja\u2019s algorithm was recently shown by [6], using different techniques from us.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "MMWU [7, 9] \u00d5( \u221a \u03bbT ) O(d) \u00d5 ( \u03bbd \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 8, "context": "MMWU [7, 9] \u00d5( \u221a \u03bbT ) O(d) \u00d5 ( \u03bbd \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 6, "context": "MMWU with JL [7, 28] \u00d5( \u221a \u03bbT ) \u00d5 ( T 5 4 \u03bb\u2212 3 4 nnz(\u03a3) ) \u00d5 ( \u03bb \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 26, "context": "MMWU with JL [7, 28] \u00d5( \u221a \u03bbT ) \u00d5 ( T 5 4 \u03bb\u2212 3 4 nnz(\u03a3) ) \u00d5 ( \u03bb \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 15, "context": "block power method [16] \u00d5( \u221a \u03bbT ) O ( nnz(\u03a3) ) \u00d5 ( 1 \u03b52 nnz(\u03a3) )", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "We denote by \u03a3 = A1 + \u00b7 \u00b7 \u00b7+ AT , by nnz(A) = maxk\u2208[T ] { nnz(Ak) } , and by \u03bb = 1 T \u03bbmax(\u03a3) \u2208 [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 15, "context": "7 The block power method (for the stochastic online eigenvector problem) can also be analyzed in this \u03bb-refined language, for instance by modifying the proof in [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "[13] and independently shown by Kot lowski and Warmuth [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[13] and independently shown by Kot lowski and Warmuth [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "showed that FTPL strategies can also be analyzed using a FTRL framework [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 23, "context": "studied the high-rank variant using MMWU [25], but their per-iteration running time is still O(d3) due to eigendecomposition.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.", "startOffset": 90, "endOffset": 98}, {"referenceID": 18, "context": "Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.", "startOffset": 90, "endOffset": 98}, {"referenceID": 5, "context": "The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].", "startOffset": 92, "endOffset": 99}, {"referenceID": 5, "context": "The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.", "startOffset": 15, "endOffset": 22}, {"referenceID": 16, "context": "The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.", "startOffset": 15, "endOffset": 22}, {"referenceID": 4, "context": "For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "In a recent result, the authors of [7] generalized MMWU to `1\u22121/q regularized strategies.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "We encourage interested readers to see the introduction of [7] for more background information, but we shall make this present paper self-contained.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Prior work on MMWU and its extensions relies heavily on one of the following trace inequalities [7]: Golden-Thompson inequality : Tr(e) \u2264 Tr ( ee )", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "If this is the case, then one can compute the 3\u00d7 3 matrix ( ui Xkuj ) i,j\u2208[3] explicitly, and then we can obtain its rank-3 eigendecomposition X 1/2 k UX 1/2 k = \u22113 j=1 pj \u00b7 yjy> j in O(d) time.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "At a high level, issue (a) is not a big deal because if v\u2032 j satisfies \u2016vj \u2212 v\u2032 j\u20162 \u2264 \u03b5\u0303/poly(d, T ) and we use v\u2032 j instead of vj , then the final regret is affected by less than \u03b5\u0303; issue (b) can be dealt as long as we perform a careful binary search to find ck, similar to prior work [7]; issue (c) can be done as long as we have a good control on the condition number of the matrix ( ckI\u2212 \u03b7\u03a3k\u22121 ) .", "startOffset": 287, "endOffset": 290}, {"referenceID": 15, "context": "We thank Dan Garber and Tengyu Ma for clarifying some results of prior work [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "[16]", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "from N (0, 1) for every i \u2208 [d], j \u2208 [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "1) like it was used in [7].", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Notice that TrU = 13 \u2211 i\u2208[d],j\u2208[3](uj,i) 2 so 3TrU is distributed according to chisquared distribution \u03c72(3d) whose PDF is p(x) = 2 \u2212 3d 2 e\u2212 x 2 x 3d 2 \u22121 \u0393(3d/2) .", "startOffset": 31, "endOffset": 34}, {"referenceID": 30, "context": "Wendell [32]), and the second inequality uses our assumption on q.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "Let {Zt}t=1 be a random process with respect to a filter {0,\u03a9} = F0 \u2282 F1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT and each Zt \u2208 [0, 1] is Ft-measurable.", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "(The first inequality has used (a\u2212 b)2 \u2264 a2 + b2 when a, b \u2265 0, and the second has used Zt \u2208 [0, 1].", "startOffset": 93, "endOffset": 99}, {"referenceID": 2, "context": "Moreover, this can be done in time O(d) as long as we can compute the three vectors { Xuj } j\u2208[3] to an additive \u03b5\u0303/poly(d, T ) error in Euclidean norm.", "startOffset": 94, "endOffset": 97}, {"referenceID": 29, "context": "1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "Since \u2016\u2207fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time \u00d5 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) .", "startOffset": 63, "endOffset": 70}, {"referenceID": 27, "context": "Since \u2016\u2207fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time \u00d5 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) .", "startOffset": 63, "endOffset": 70}, {"referenceID": 13, "context": "Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to \u00d5 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)nnz(Ai)} ) .", "startOffset": 50, "endOffset": 58}, {"referenceID": 21, "context": "Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to \u00d5 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)nnz(Ai)} ) .", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "This can be done via a \u201cbinary search\u201d procedure which was used widely for shift-and-invert based methods [15]:", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "The authors of [7] have shown that the potential Tr(X 1\u22121/q k ) is robust against noise and a completely analogous (but lengthy) proof of theirs applies to this paper.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "(This requires O(1) iterations of power method applied to (cI\u2212 \u03b7\u03a3k\u22121)\u22121 [15].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "It is a simple exercise (with details given in [15]) to show that when the procedure ends, it satisfies 1 2e \u2264 c \u2212 \u03b7\u03a3k\u22121 \u2264 1e so c is a lower bound on ck.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Above, \u00ac uses Tr(AT\u03a6T\u22121AT ) \u2264 Tr(AT\u03a6T\u22121) as well as wkw> k = \u03a6k\u22121/Tr(\u03a6k\u22121), \u00ad uses 1+x \u2264 ex, \u00ae uses 1+2x \u2265 e2x\u22122x for x \u2208 [0, 1], \u00ad uses E[\u03bd> 1 \u03a60\u03bd1] = 1, \u00b0 uses the Lieb-Thirring inequality Tr(ABAB) \u2264 Tr(A2B2),22 \u00b1 uses (I + \u03b7A1) I + (4\u03b7 + 11\u03b7)A1.", "startOffset": 122, "endOffset": 128}, {"referenceID": 5, "context": "We prove this lower bound by reducing the problem to an information-theoretic lower bound that has appeared in our separate paper [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "The lower bound in [6] states that, for every 1 \u2265 \u03bb \u2265 \u03bb2 \u2265 0, there exists a PSD matrix B with the largest two eigenvalues being \u03bb and \u03bb2, and a distribution D of rank-1 matrices with spectral norm at most 1 and expectation equal to D.", "startOffset": 19, "endOffset": 22}], "year": 2017, "abstractText": "Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields. However, it comes with a slow running time due to the matrix exponential and eigendecomposition computations. For this reason, many researchers studied the followed-the-perturbed-leader (FTPL) framework which is faster, but a factor \u221a d worse than the optimal regret of MMWU for dimension-d matrices. In this paper, we propose a followed-the-compressed-leader framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs even faster than FTPL. Our main idea is to \u201ccompress\u201d the matrix exponential computation to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16]. ar X iv :1 70 1. 01 72 2v 1 [ cs .L G ] 6 J an 2 01 7", "creator": "LaTeX with hyperref package"}}}