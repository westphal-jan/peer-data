{"id": "1701.01811", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2017", "title": "Structural Attention Neural Networks for improved sentiment analysis", "abstract": "we introduce a small tree - sized structured attention neural task network problem for sentences and small phrases and additionally apply it extensively to the critical problem of global sentiment identities classification. our model expands the relevant current recursive models by incorporating joint structural information around resembling a node of entering a syntactic tree using algorithms both graphical bottom - off up and top - down information vector propagation. also, increasingly the model utilizes collective structural cognitive attention to identify the most salient representations during viewing the construction of the syntactic credibility tree. to demonstrate our knowledge, evaluating the modern proposed models achieve state of scale the familiar art performance on sequencing the synthetic stanford systemic sentiment identity treebank dataset.", "histories": [["v1", "Sat, 7 Jan 2017 09:58:49 GMT  (31kb,D)", "http://arxiv.org/abs/1701.01811v1", "Submitted to EACL2017 for review"]], "COMMENTS": "Submitted to EACL2017 for review", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["filippos kokkinos", "alexandros potamianos"], "accepted": false, "id": "1701.01811"}, "pdf": {"name": "1701.01811.pdf", "metadata": {"source": "CRF", "title": "Structural Attention Neural Networks for improved sentiment analysis", "authors": ["Filippos Kokkinos", "Alexandros Potamianos"], "emails": ["el11142@central.ntua.gr", "potam@central.ntua.gr"], "sections": [{"heading": "1 Introduction", "text": "Sentiment analysis deals with the assessment of opinions, speculations, and emotions in text (Zhang et al., 2012; Pang and Lee, 2008). It is a relatively recent research area that has attracted great interest as demonstrated by a series of shared evaluation tasks, e.g., analysis of tweets (Nakov et al., 2016). In (Turney and Littman, 2002), the affective ratings of unknown words were predicted utilizing the affective ratings of a small set of words (seeds) and the semantic relatedness between the unknown and the seed words. An example of sentence-level analysis was proposed in (Malandrakis et al., 2013). Other application areas include the detection of public opinion and prediction of election results (Singhal et al., 2015), correlation of mood states and stock market indices (Bollen et al., 2011).\nRecently, Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Chung et al., 2014) have been\napplied to various Natural Language Processing tasks. Tree structured neural networks, which are found in literature as Recursive Neural Networks, hold a linguistic interest due to their close relation to syntactic structures of sentences being able to capture distributed information of structure such as logical terms(Socher et al., 2012). These syntactic structures are N-ary trees which represent either the underlying structure of a sentence, known as constituency trees or the relations between words known as dependency trees.\nThis paper focuses on sentence-level sentiment classification of movie reviews using syntactic parse trees as input for the proposed networks. In order to solve the task of sentiment analysis of sentences, we work upon a variant of Recursive Neural Networks which recursively create representation following the syntactic structure. The proposed computation model exploits information from subnodes as well as parent nodes of the node under examination. This neural network is referred to as Bidirectional Recursive Network (Irsoy and Cardie, 2013). The model is further enhanced with memory units and the proposed structural attention mechanism. It is observed that different nodes of a tree structure hold information of variable saliency. Not all nodes of a tree are equally informative, so the proposed model selectively weights the contribution of each node regarding the sentence level representation using structural attention model.\nWe evaluate our approach on the sentence-level sentiment classification task using one standard movie review dataset (Socher et al., 2013). Experimental results show that the proposed model outperforms the state-of-the art methods.\nar X\niv :1\n70 1.\n01 81\n1v 1\n[ cs\n.C L\n] 7\nJ an\n2 01\n7"}, {"heading": "2 Tree-Structured GRUs", "text": "Recursive GRUs (Tree-Gru) upon tree structures are an extension of the sequential GRUs that allow information to propagate through network topologies. Similar to Recursive LSTM network on tree structures (Tai et al., 2015), for every node of a tree, the Tree-GRU has gating mechanisms that modulate the flow of information inside the unit without the need of a separate memory cell. The activation hj of Tree-GRU for node j is the interpolation of the previous calculated activation hjk of its kth child out ofN total children and the candidate activation h\u0303j .\nhj = zj \u2217 N\u2211 k=1 hjk + (1\u2212 zj) \u2217 h\u0303j (1)\nwhere zj is the update function which decide the degree of update that will occur on the activation based on the input vector xj and previously calculated representation hjk :\nzj = \u03c3(Uz \u2217 xj + N\u2211 k=1 W iz \u2217 hjk) (2)\nThe candidate activation h\u0303j for a node j is computed similarly to that of a Recursive Neural Network as in (Socher et al., 2011):\nh\u0303j = f(Uh \u2217 xj + N\u2211 k=1 W kh \u2217 (hjk \u2217 rj)) (3)\nwhere rj is the reset gate which allows the network to forget effectively previous computed representations when the value is close to 0 and it is computed as follows:\nrj = \u03c3(Ur \u2217 xj + N\u2211 k=1 W kr \u2217 hjk) (4)\nEvery part of a gated recurrent unit xj , hj , rj , zj , h\u0303j \u2208 Rd where d is the input vector dimensionality. \u03c3 is the sigmoid function and f is the non-linear tanh function.The set of matrices W k, U \u2208 Rdxd used in 2 - 4 are the trainable weight parameters which connect the kth children node representation with the jth node representation and the input vector xj ."}, {"heading": "2.1 Bidirectional TreeGRU", "text": "A natural extension of Tree-Structure GRU is the addition of a bidirectional approach. TreeGRUs calculate an activation for node j with the use of previously computed activations lying lower in the tree structure. The bidirectional approach for a tree structure uses information both from under and lower nodes of the tree for a particular node j. In this manner, a newly calculated activation incorporates content from both the children and the parent of a particular node. The bidirectional neural network can be trained in two seperate phases: i) the Upward phase and ii) the Downward phase. During the Upward phase, the network topology is similar to the topology of a TreeGru, every activation is calculated based on the previously calculated activations which are found lower on the structure in a bottom up fashion. When every activation has been computed, from leaves to root, then the root activation is used as input of the Downward phase. The Downward phase calculates the activations for every child of a node using content from the parent in a top down fashion. The process of computing the internal representations between the two phases is separated, so in a first pass the network compute the upward activation and after this is completed, then the downward representations are computed. The upward activation h\u2191j similarly to TreeGRU for node j is the interpolation of the previous calculated activation h\u2191jk of its kth child out of N total children and the candidate activation h\u0303\u2191j .\nh\u2191j = z \u2191 j \u2217 N\u2211 k=1 h\u2191jk + (1\u2212 z \u2191 j ) \u2217 h\u0303 \u2191 j (5)\nThe update gate, rest gate and candidate activation are computed as follows:\nz\u2191j = \u03c3(Uz \u2217 x \u2191 j + N\u2211 k=1 W kz \u2217 h \u2191 jk) (6)\nr\u2191j = \u03c3(Ur \u2217 x \u2191 j + N\u2211 k=1 W kr \u2217 h \u2191 jk) (7)\nh\u0303\u2191j = f(Uh \u2217 x \u2191 j + N\u2211 k=1 W kr \u2217 (h \u2191 jk \u2217 r \u2191 j )) (8)\nThe downward activation h\u2191j for node j is the interpolation of the previous calculated activation h\u2191p(j), where the function p calculates the index of the parent node, and the candidate activation h\u0303\u2193j .\nh\u2193j = z \u2193 j \u2217 h \u2193 p(j) + (1\u2212 z \u2193 j ) \u2217 h\u0303 \u2193 j (9)\nThe update gate, reset gate and candidate activation for the downward phase are computed as follows:\nz\u2193j = \u03c3(U d z \u2217 h \u2191 j +W d z \u2217 h \u2193 p(j)) (10)\nr\u2193j = \u03c3(U d r \u2217 h \u2191 j +W d r \u2217 h \u2193 p(j)) (11)\nh\u0303\u2193j = f(U d h \u2217 h \u2191 j +W d h \u2217 (h \u2193 p(j) \u2217 r \u2193 j )) (12)\nDuring downward phase, matrix Ud \u2208 Rdxd connects the upward representation of node j with the respective jth downward node while W d \u2208 Rdxd connect the parent representation p(j)."}, {"heading": "2.2 Structural Attention", "text": "We introduce Structural Attention, a generalization of sequential attention model (Luong et al., 2015) which extracts informative nodes out of a syntactic tree and aggregates the representation of those nodes in order to form the sentence vector. We feed representation hj of node through a one-layer Multilayer Perceptron with Ww \u2208 Rdxd weight matrix to get the hidden representation uj .\nuj = tanh(Ww \u2217 hj) (13)\nUsing the softmax function, the weights aj for each node are obtained based on the similarity of the hidden representation uj and a global context vetor uw \u2208 Rd. The normalized weights aj\nare used to form the final sentence representation s \u2208 Rd which is a weighted summation of every node representation hj .\naj = u>j \u2217 uw\u2211N i=1 u > i \u2217 uw\n(14)\ns = N\u2211 i=1 aihi (15)\nThe proposed attention model is applied on structural content since all node representations contain syntactic structural information during training because of the recursive nature of the network topology."}, {"heading": "3 Experiments", "text": "We evaluate the performance of the aforementioned models on the task of sentiment classification of sentences sampled from movie reviews. We use the Stanford Sentiment Treebank (Socher et al., 2013) dataset which contains sentiment labels for every syntactically plausible phrase out of the 8544/1101/2210 train/dev/test sentences. Each phrase is labeled with respect to a 5-class sentiment value, i.e. very negative, negative, neutral, positive, very positive. The dataset can also be used for a binary classification subtask by excluding any neutral phrases for the original splits. The binary classification subtask is evaluated on 6920/872/1821 train/dev/test splits."}, {"heading": "3.1 Sentiment Classification", "text": "For all of the aforementioned architectures at each node j we use a softmax classifier to predict the sentiment label y\u0302j . For example, the predicted label y\u0302j corresponds to the sentiment class of the spanned phrase produced from node j. The classifier for unidirectional TreeGRU architectures uses the hidden state hj produced from recursive computations till node j using a set xj of input nodes to predict the label as follows:\np\u0302\u03b8(y|xj) = softmax(Ws \u2217 hj) (16)\nwhere Ws \u2208 Rdxc and c is the number of sentiment classes.\nThe classifier for bidirectional TreeBiGRU architectures uses both the hidden state h\u2191j and h \u2193 j produced from recursive computations till node j during Upward and Downward Phase using a set xj of input nodes to predict the label as follows:\np\u0302\u03b8(y|xj) = softmax(W \u2191s \u2217h \u2191 j +W \u2193 s \u2217h \u2193 j ) (17)\nwhere W \u2191s ,W \u2193 s \u2208 Rdxc and c is the number of sentiment classes. The predicted label y\u0302j is the argument with the maximum confidence:\ny\u0302j = argmax y\n(p\u0302\u03b8(y|xj)) (18)\nFor the Structural Attention models, we use for the final sentence representation s to predict the sentiment label y\u0302j where j is the corresponding root node of a sentence. The cost function used is the negative log-likelihood of the ground-truth label yk at each node:\nE(\u03b8) = m\u2211 k=1 p\u0302\u03b8(y k|xk) + \u03bb 2 ||\u03b8||2 (19)\nwhere m is the number of labels in a training sample and \u03bb is the L2 regularization hyperparameter."}, {"heading": "3.2 Results", "text": "The evaluation results are presented in Table 2 in terms of accuracy, for several state-of-the-art models proposed in the literature as well as for the TreeGRU and TreeBiGRU models proposed in this work. Among the approaches reported in the literature, the highest accuracy is yielded by DRNN and DMN for the binary scheme (88.6), and by DMN for the fine-grained scheme (52.1). We observe that the best performance is achieved by TreeBiGRU with attention, for both binary (89.5) and fine-grained (52.4) evaluation metrics, exceeding any previously reported results. In addition, the attentional mechanism employed in the proposed TreeGRU and TreeBiGRU models improve the performance for both evaluation metrics."}, {"heading": "4 Hyperparameters and Training Details", "text": "The evaluated models are trained using the AdaGrad (Duchi et al., 2010) algorithm using 0.01 learning rate and a minibatch of size 25 sentences. L2-regularization is performed on the model parameters with a \u03bb value 10\u22124. We use dropout\nwith probability 0.5 on both the input layer and the softmax layer. The word embeddings are initialized using the public available Glove vectors with a 300 dimensionality. The Glove vectors provide 95.5% coverage for the SST dataset. All initialized word vectors are finetuned during the training process along with every other parameter. Every matrix is initialized with the identity matrix multiplied by 0.5 except for the matrices of the softmax layer and the attention layer which are randomly initialized from the normal Gaussian distribution. Every bias vectors is initialized with zeros. The training process lasts for 40 epochs. During training, we evaluate the network 4 times every epoch and keep the parameters which give the best root accuracy on the development dataset."}, {"heading": "5 Conclusion", "text": "In this short paper, we propose an extension of Recursive Neural Networks that incorporates a bidirectional approach with gated memory units as well as an attention model on structure level. The proposed models were evaluated on both finegrained and binary sentiment classification tasks on a sentence level. Our results indicate that both the direction of the computation and the attention on a structural level can enhance the performance of neural networks on a sentiment analysis task."}], "references": [{"title": "Twitter mood predicts the stock market", "author": ["Johan Bollen", "Huina Mao", "Xiaojun Zeng."], "venue": "Journal of Computational Science, 2(1):1\u20138.", "citeRegEx": "Bollen et al\\.,? 2011", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Technical report, EECS Department, University of California, Berkeley.", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput., 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "CoRR, abs/1312.0493.", "citeRegEx": "Irsoy and Cardie.,? 2013", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "CoRR, abs/1404.2188.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "CoRR, abs/1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "CoRR, abs/1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Sail: A hybrid approach to sentiment analysis", "author": ["Nikolaos Malandrakis", "Abe Kazemzadeh", "Alexandros Potamianos", "Shrikanth Narayanan."], "venue": "Proceedings SemEval, pages 438\u2013442.", "citeRegEx": "Malandrakis et al\\.,? 2013", "shortCiteRegEx": "Malandrakis et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semeval2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov."], "venue": "Proceedings of the 10th international workshop on semantic evaluation (SemEval 2016).", "citeRegEx": "Nakov et al\\.,? 2016", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Found. Trends Inf. Retr., 2(12):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Modeling indian general elections: sentiment analysis of political twitter data", "author": ["Kartik Singhal", "Basant Agrawal", "Namita Mittal."], "venue": "Information Systems Design and Intelligent Applications, pages 469\u2013477.", "citeRegEx": "Singhal et al\\.,? 2015", "shortCiteRegEx": "Singhal et al\\.", "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "CoRR, abs/1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Unsupervised learning of semantic orientation from a hundred-billion-word corpus", "author": ["Peter Turney", "Michael L Littman"], "venue": null, "citeRegEx": "Turney and Littman.,? \\Q2002\\E", "shortCiteRegEx": "Turney and Littman.", "year": 2002}, {"title": "Deciphering word-of-mouth in social media: Text-based metrics of consumer reviews", "author": ["Zhu Zhang", "Xin Li", "Yubo Chen."], "venue": "ACM Trans. Manage. Inf. Syst., 3(1):5:1\u20135:23.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "Sentiment analysis deals with the assessment of opinions, speculations, and emotions in text (Zhang et al., 2012; Pang and Lee, 2008).", "startOffset": 93, "endOffset": 133}, {"referenceID": 12, "context": "Sentiment analysis deals with the assessment of opinions, speculations, and emotions in text (Zhang et al., 2012; Pang and Lee, 2008).", "startOffset": 93, "endOffset": 133}, {"referenceID": 11, "context": ", analysis of tweets (Nakov et al., 2016).", "startOffset": 21, "endOffset": 41}, {"referenceID": 18, "context": "In (Turney and Littman, 2002), the affective ratings of unknown words were predicted utilizing the affective ratings of a small set of words (seeds) and the semantic relatedness between the unknown and the seed words.", "startOffset": 3, "endOffset": 29}, {"referenceID": 9, "context": "An example of sentence-level analysis was proposed in (Malandrakis et al., 2013).", "startOffset": 54, "endOffset": 80}, {"referenceID": 13, "context": "Other application areas include the detection of public opinion and prediction of election results (Singhal et al., 2015), correlation of mood states and stock market indices (Bollen et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 0, "context": ", 2015), correlation of mood states and stock market indices (Bollen et al., 2011).", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "Recently, Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Chung et al.", "startOffset": 76, "endOffset": 110}, {"referenceID": 1, "context": "Recently, Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Chung et al., 2014) have been applied to various Natural Language Processing tasks.", "startOffset": 142, "endOffset": 162}, {"referenceID": 15, "context": "Tree structured neural networks, which are found in literature as Recursive Neural Networks, hold a linguistic interest due to their close relation to syntactic structures of sentences being able to capture distributed information of structure such as logical terms(Socher et al., 2012).", "startOffset": 265, "endOffset": 286}, {"referenceID": 4, "context": "This neural network is referred to as Bidirectional Recursive Network (Irsoy and Cardie, 2013).", "startOffset": 70, "endOffset": 94}, {"referenceID": 16, "context": "We evaluate our approach on the sentence-level sentiment classification task using one standard movie review dataset (Socher et al., 2013).", "startOffset": 117, "endOffset": 138}, {"referenceID": 17, "context": "Similar to Recursive LSTM network on tree structures (Tai et al., 2015), for every node of a tree, the Tree-GRU has gating mechanisms that modulate the flow of information inside the unit without the need of a separate memory cell.", "startOffset": 53, "endOffset": 71}, {"referenceID": 14, "context": "The candidate activation h\u0303j for a node j is computed similarly to that of a Recursive Neural Network as in (Socher et al., 2011):", "startOffset": 108, "endOffset": 129}, {"referenceID": 8, "context": "We introduce Structural Attention, a generalization of sequential attention model (Luong et al., 2015) which extracts informative nodes out of a syntactic tree and aggregates the representation of those nodes in order to form the sentence vector.", "startOffset": 82, "endOffset": 102}, {"referenceID": 16, "context": "We use the Stanford Sentiment Treebank (Socher et al., 2013) dataset which contains sentiment labels for every syntactically plausible phrase out of the 8544/1101/2210 train/dev/test sentences.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "The evaluated models are trained using the AdaGrad (Duchi et al., 2010) algorithm using 0.", "startOffset": 51, "endOffset": 71}, {"referenceID": 16, "context": "RNN, MV-RNN and RNTN (Socher et al., 2013).", "startOffset": 21, "endOffset": 42}, {"referenceID": 10, "context": "PVec: (Mikolov et al., 2013).", "startOffset": 6, "endOffset": 28}, {"referenceID": 17, "context": "TreeLSTM (Tai et al., 2015).", "startOffset": 9, "endOffset": 27}, {"referenceID": 4, "context": "DRNN (Irsoy and Cardie, 2013).", "startOffset": 5, "endOffset": 29}, {"referenceID": 5, "context": "DCNN (Kalchbrenner et al., 2014).", "startOffset": 5, "endOffset": 32}, {"referenceID": 6, "context": "CNN-multichannel (Kim, 2014).", "startOffset": 17, "endOffset": 28}, {"referenceID": 7, "context": "DMN (Kumar et al., 2015)", "startOffset": 4, "endOffset": 24}], "year": 2017, "abstractText": "We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottomup and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.", "creator": "TeX"}}}