{"id": "1701.02377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "The principle of cognitive action - Preliminary experimental analysis", "abstract": "arrive in this particular document before we shows a first complete implementation and applies some additional preliminary verification results of a new theory, facing machine element learning problems in the frameworks of classical operational mechanics and variational problem calculus. we give a pretty general algebraic formulation scheme of the problem itself and then thus we studies basic behaviors of challenging the model on simple practical implementations.", "histories": [["v1", "Mon, 9 Jan 2017 22:29:08 GMT  (2257kb)", "http://arxiv.org/abs/1701.02377v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marco gori", "marco maggini", "alessandro rossi"], "accepted": false, "id": "1701.02377"}, "pdf": {"name": "1701.02377.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\nContents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Formulation of the problem 3", "text": ""}, {"heading": "3 Construction of Cost Functional 4", "text": ""}, {"heading": "4 First Application 5", "text": "4.1 First Order Operator . . . . . . . . . . . . . . . . . . . . . . . 5\n4.1.1 Experimental results . . . . . . . . . . . . . . . . . . . 6 4.1.2 Choice of solutions . . . . . . . . . . . . . . . . . . . . 19\n4.2 Second Order Operator . . . . . . . . . . . . . . . . . . . . . 20 4.2.1 Experimental results . . . . . . . . . . . . . . . . . . . 21 4.2.2 Choice of solutions . . . . . . . . . . . . . . . . . . . . 32 4.3 Sampling-step \u03c4 , Parameter \u03b8 and number of Impulses . . . . 33"}, {"heading": "5 First application on ANNs 37", "text": "5.1 One dimension functions . . . . . . . . . . . . . . . . . . . . . 38 5.2 Two dimensions functions . . . . . . . . . . . . . . . . . . . . 38 5.3 Vowels Classifications . . . . . . . . . . . . . . . . . . . . . . 39"}, {"heading": "6 Conclusions 40", "text": ""}, {"heading": "7 Appendix 41", "text": "7.1 General solution and coefficients . . . . . . . . . . . . . . . . 41 7.2 From solution to parameters . . . . . . . . . . . . . . . . . . . 43\n7.2.1 First Order . . . . . . . . . . . . . . . . . . . . . . . . 43 7.2.2 Second Order . . . . . . . . . . . . . . . . . . . . . . . 44\n7.3 From continuos to discrete model . . . . . . . . . . . . . . . . 45"}, {"heading": "1 Introduction", "text": "Many real world phenomena could be interpreted in an on-line scenario within Machine Learning theory. In long-life learning problems, various approaches have been developed to deal with the large amounts of data and the exploitation of their time correlation. Usually, some difficulties arise in the storage of data and in going after the intrinsic information coming from data during time. These aspects could suggest a more natural approach to learning, in a joint theory among Mechanics, Variational Calculus and Statistics. However, we postpone a deeply analysis of these ideas at a theoretical level, focusing on a first practical implementation, to create a connection between this new ideas and some applications on existing structures, which could be useful in these preliminary steps.\nWe briefly introduce basics ideas of this theory, first formulated in [1]. The concept of dissipation is well formulated in [2],whereas an summing up of this report and an experimental analysis on standard benchmark can be find in [3]. A further theoretical abstraction applied to similar environment is proposed in [4], In this document, we will give a slightly theoretical formulation in order to allow us to go straight to the practical implementation issues. We study the meaning of the model parameters on artificial problems, using a linear function, and then we try some experiments with simple Neural Networks.\nAn important result is the fact that we can choose arbitrarily the memory of our system by a parameter, avoiding the hardware memory storage problems. Indeed, the trend of the system is influenced by the information coming from each example during an adjustable interval of time."}, {"heading": "2 Formulation of the problem", "text": "We study the case in which we want to learn a function f that aims to represent the behavior of a features representation u of the spatio-temporal domains D. If we assume the temporal domain T = [0,\u221e) and X \u2286 RD then we have D = T \u00d7X and u : D \u2192 Rd so that f has input u and depends on a set of weights W, like for example if f is described by an Artificial Neural Network. The problem consits on learning the parameters W under some assumption, i.e. by minimizing a cost functional L composed by a penalty term and a regularization one. Since the weights have to be learned as time goes by, we have that W depends on time and we write f = f(u(t,x),W(t)). In the classical approach the penalty term impose a\ncoherence w.r.t. the Training Set, whereas the regularization term impose the norm of the parameters to be small, so as to f be a smooth function. If we want the process of learning itself to be smooth, we could requires some kind of regularization in the changing of W during time. We shall see in next sections how this idea could be studied in a Physics-like approach."}, {"heading": "3 Construction of Cost Functional", "text": "In a classical learning problem we have to minimize a cost functional L w.r.t. W(t). The functional is composed by a penalty term and a regularization one. The penalty term is calculated on every supervised example, over a training set P={(uk, f\u0304k)} l k=1, by a loss function V =f(u(t,x),W(t)) which is the summation over P :\nV (u(t,x),W(t)) = l \u2211\nk=1\nV ( f(u(t,x),W(t)) , f\u0304k )\nwhere V could be for example the quadratic function V (f, f\u0304k)= 1 2(f \u2212 f\u0304k)\n2\n( where f means f(u(t,x),W(t)) from now on ). Since the examples are presented in time, if tk is the instant of time in which the couple (uk, f\u0304k) is provide, we can write V as\nV (u(t,x),W(t)) = l \u2211\nk=1\nV ( f , f\u0304k ) \u00b7H(t\u2212 tk).\nIn a physics-like approach we can look at the term V as the potential energy, so as the total energy of the system L is composed by K+V where K is the kinetic energy . Then in our formulation we write\nL=K+\u03b3V (1)\nwhere \u03b3 represent a regularization parameter (which includes the classical case when \u03b3=1 ). We can write K as:\nK = m \u2211\ni=1\n\u00b5i\u03c9\u0307 2 i\nwhere m is the total number of weights in W, and \u00b5i represent the mass of each weight, (i.e. an additional parameters which can be use to choose the\ninertia of each weight). Since \u03c9\u0307i= d dt\u03c9i we can replace D= d dt with a general differential operator T . Now we can finally write our cost functional as\nS\u03b3 =\n\u222b te\n0 \u03c8(t) \u00b7 L dt. (2)\nwhere \u03c8 is a suitable dissipation function, that we take as \u03c8(t)=e\u03b8t."}, {"heading": "4 First Application", "text": ""}, {"heading": "4.1 First Order Operator", "text": "In our first application of this theoretical framework we analyze the simple case in which f is a linear function of one single real variable f :R \u2192 R and f = yu+b where u=u(t, x(t)))=x(t) . In this framework we want to learn the two weights y, b. The problem can be formulated as\ny\u2217 = argmin y\u2208R\n\u222b te\n0 \u03c8(t) \u00b7 L dt\nand analogous formula hold for b. We start with the case T =\u03b10+\u03b11D. By the application of the EuleroLagrange equation we have to solve the second order linear differential equation:\ny\u0308 + \u03b8y\u0307 + \u03b2y \u2212 \u03b3\n\u00b5\u03b121\nl \u2211\nk=1\n(ukyk + bk \u2212 f\u0304k)uk \u00b7 \u03b4(t\u2212 tk) = 0 (3)\nwhere \u03b2 = \u03b10\u03b11\u03b8\u2212\u03b120\n\u03b121 . The solution is then composed by a term given\nfrom the homogeneous solution yo(t) plus a term given from the impulsive response yF (t). Then we have1:\ny(t) = yo(t) + yF (t) = yo(t) + \u03b3\n\u00b5\u03b121\nl \u2211\nk=1\n\u03b6k \u00b7 g(t\u2212 tk) (4)\nwhere we posed \u03b6k = (ukyk + bk \u2212 f\u0304k)uk. For the bias b we have an analogous solution except for the element \u03b6k, which represent \u2202V \u2202y and in the correspondent formula for b is \u03b6k=ukyk+bk\u2212f\u0304k. For the stability of the system during time, we have to impose the RouthHurwitz conditions, which lead to the relation \u03b8 > \u03b10/\u03b11.\n1see section 7 for details about practical calculation"}, {"heading": "4.1.1 Experimental results", "text": "A first implementation (in MatLab) of our theoretical results is in the simple case in which we want to approximate a linear function in an interval [a, b] = [\u22121, 1] of the real axis. We assume that the examples on the training set are equally spaced in time by a factor \u03c4 with t0 = 0. This allow us to use a discretization of (4) for the computing of the evolution of the system, as you can see in Section(7.3). We also consider an equally-spaced subdivision of our interval that we cover forward and backward, i.e. we move from a to b and viceversa, so as to guarantee time correlation among the examples. We assign to every point uk a target f\u0304k = 2\u00b7uk\u22121, so we desired y(t)\u2192 2 and b(t)\u2192\u22121 after some epochs. For start, we feed the system with a total supervised training set. We studied some results on this first implementation w.r.t. the parameters \u03b8, \u03b10, \u03b11, \u03b3, \u00b5, \u03c4 .\nIf we start our study for a simple case we have a behavior as shown in Fig.1 . We set \u03b3=\u22121 , \u00b5=1 , \u03b8=5 , \u03b10 =1 , \u03b11 =1 , y(0) = y\n\u2032(0)= b(0)= b\u2032(0) = 0. We repeat the training set for a total of 40 iterations. From the top to the bottom of the figure we can find :\n\u2022 the plot of the impulse response g(red line)\n\u2022 the plot of y(blue line) and b(green line)\n\u2022 the plot of the last 20% of update of the weights\n\u2022 the plot of the last 10% of update of the weights\nEach plot have the same scaling referred to the time t, so as in the graphs is reported the evolution of the system w.r.t. real time (we can think in seconds). The width of each plot depends on both the number of iterations and the parameter \u03c4 , but each plot has the same scale in seconds. As we can see in Fig.1 the weights have an initial oscillation and then assume a cyclic behavior with a smaller amplitude oscillation when the system is a steady state, near the desired value 2,\u22121 respectively.\nIn the remainder of this section we studied some variations produced by each parameters on the behavior of the weights.\nParameter \u03b3\nIn our formulation of the theory this parameters only determine the sign between the two termsK and V , so we only explore the set {\u22121, 1} for this one. Because of some correlation with the gradient descent, we expect our weights to have a divergent trend when \u03b3 = 1. Our experimental results confirm this, as we can see in Fig.2, where we can notice the trend of the weights after just 5 iterations.\nWe tried to vary the other parameters in order to make the weights converge to the desired value also in the case \u03b3=1. We obtained converge by imposing a strong regularization by the differential operator\n(\u03b10 , \u03b11 > 10), but is difficult to find a correlation between the final values and the desired ones(Fig.3 ). The same result ca be achieved with \u00b5 > 30 or \u03b8 > 120 and also by increasing the parameter \u03c4 . All these adjustments on the other parameters represent the imposition of a strong regularization on the system, which drive all the weights to zero.\nBecause of this preliminary results, we drop the parameter \u03b3 from now on as we fix \u03b3=\u22121.\nParameter \u03c4\nThis parameters represent the time-sampling step of the system. Under our practical assumptions also the time-spacing of the examples. It represent a crucial parameters of the system and we will study it\nafter the second order operator. For now we fix \u03c4=0.01\nInitial Conditions\nBecause of the asymptotic behavior of the solution of (3), it is reasonable to assume that different Initial Conditions do not produce relevant changes in the weights at the end of optimization. Indeed in Fig.4 and Fig.5 we can see that the final values of the weights are in the same range of Fig.1, but the initial oscillation is different, due to the different starting points and derivatives. We can also see that for very different initial conditions, the time that the algorithm spent on reach the steady state is almost the same, maybe because the terms \u03b6h, which reflect the gradient, balance these differences. As in the previous case, from now on we drop the specification of initial conditions assuming them null.\nParameter \u03b8\nThis parameter come from the exponent in the dissipation term \u03c8(t)= e\u03b8t and influence the solution direct in the structure of the functions g(t) and yo(t) ( see section7). If we decrease \u03b8=2, we have that g(t) \u2192 0 slowly(Fig.6), the initial transient phase is longer and oscillation as a greater amplitude. On the opposite, if we set \u03b8 = 10 we have the reverse effect. In the steady state, we can observe that as \u03b8 increases, the average value of the weights decreases(y(t)\u223c1.956 for \u03b8=2, y(t)\u223c 1.835 for \u03b8 = 5, y(t) \u223c 1.665 for \u03b8 = 10 ), so as we can think to a regularization effect.\nParameter \u03b10, \u03b11\nAlso these parameters affect the solution of (3). They are expected to act as a regularization parameters, since they appear directly in the construction of term K. A larger value of \u03b10 should lead to a weights with a smaller magnitude. \u03b11 should impose a smaller derivatives, i.e. a smooth variations of the weights during time. These hypothesis are confirmed by Fig.8, Fig.9, Fig.10. In Fig.9, we can notice that a larger \u03b11 gives not only the expected smoothness, but also a stronger regularization effect w.r.t. \u03b10 in Fig.8.\nParameter \u00b5\n\u00b5 is the correspondent of the mass in physics, so it should represent the inertia of the weights, i.e. how we want to allow to the system to change them at each step (w.r.t. the penalty term V ). Furthermore, we can see in (4) that it can be use to represent a sort of learning rate (to follow a parallel with gradient descent again). This is confirmed in Fig.11 and Fig.12 where a smaller value of \u00b5 gives more importance to the optimization than to the regularization.\nIf we try to grow up with \u00b5, we find that this increment maintain this behavior (unbalancing towards regularization). On the other hand, if we choose a \u00b5 \u2264 0.4, the system diverges(Fig.13)."}, {"heading": "4.1.2 Choice of solutions", "text": "All this parameter contribute to model our system, but we have to choose how. Since we would like that the system promptly reacts to the stimuli, we want an Impulsive Response that reach its maximum as fast as possible. Moreover, its useful that the system is width enough to see all the examples more times before to forget them. Once we have chosen a suitable \u03b8 (w.r.t. the Training Set, see Section 4.3), we can model the parameters \u03b1j so as to satisfy these request. Often is more convenient to choose the directly the solutions to build our system (see Section 7.2). In Fig.14 we can see that if we chose a solution close to 0, and then the other close to \u03b8 (since \u03b8=\u2212(\u03bb1 + \u03bb2)) we have a longer g (green plot), whereas we go in the other direction if the solutions are similar (blue plot)."}, {"heading": "4.2 Second Order Operator", "text": "Under the practical assumptions of the previous section, we study the implementation of the case in which the term K is composed by the second order linear differential operator T =\u03b10+\u03b11D+\u03b12D\n2. The Eulero-Lagrange equation lead this time to a fourth order linear differential equation:\nD4y+\u03b23D 3y+\u03b22D 2y+\u03b21Dy+\u03b20y+ \u03b3\n\u00b5\u03b122\nl \u2211\nk=1\n(ukyk+bk\u2212 f\u0304k)uk \u00b7\u03b4(t\u2212tk) = 0\n(5) where:\n\u03b20 = \u03b10\u03b12\u03b82\u2212\u03b10\u03b11\u03b8+\u03b120\na22\n\u03b21 = \u03b11\u03b12\u03b82+(2\u03b10\u03b12\u2212\u03b121)\u03b8\n\u03b122\n\u03b22 = \u03b122\u03b8 2+\u03b11\u03b12\u03b8+2\u03b10\u03b12\u2212\u03b121 \u03b122 \u03b23 = 2\u03b8\n(6)\nThis time the Routh-Hurwitz conditions requires:\n\u03b2i > 0 , i = 0, ..., 3 \u03b23\u03b22 > \u03b21\n\u03b23\u03b22\u03b21 > \u03b2 2 1 + \u03b2 2 3\u03b20\n(7)\nThe updating formula is then2:\ny(t) = yo(t)\u2212 \u03b3\n\u00b5\u03b122 \u00b7\nl \u2211\nk\n\u03b6h \u00b7 g(t\u2212 tk) (8)\nwhere we can notice a sign flip before the second term w.r.t. (4) due to the even order of T . This make us expect (because of the parallel with gradient descent again) that this time our system is stable for an opposite sign of \u03b3 w.r.t. the first order operator studied in section 4.1.1. In practice, we can observe a more complicated behavior due to the kind of the solutions of (5)."}, {"heading": "4.2.1 Experimental results", "text": "This time we start by observing that there are different possibilities in the kind of the solutions of (5):\n(1)Four distinct real solution\nWe start with the case \u03b8 = 4 , \u03b10 = 0.8 , \u03b11 = 1.6 , \u03b12 = 0.8. As we can see in Fig.15 the system is divergent for \u03b3 =\u22121. Also in the case \u03b3=1 in Fig.16 we have divergence, but we can notice a different oscillation of the weights. We can see in (8) that the second term is multiplied by the factor \u03b3/(\u00b5\u03b122). Since \u03b12 < 1 we can apply some of the considerations done in Section 4.1.1 about the parameter \u00b5. This divergence is maybe due to a too higher balancing on the gradients term, indeed if we set \u00b5=4 we have convergence to the desired values Fig. 17. We can obtain convergence by increasing \u00b5 also when \u03b3=\u22121 Fig. 18 but with futile values of the weights.\n2again see section 7 for details about practical calculation\nWe have this kind of solution also for 2.2 \u2264 \u03b8 \u2264 4 and for 6.6 \u2264 \u03b8 \u2264 7.2, with the same value of \u03b1j . When \u03b8=2.2 we need a bigger value of \u00b5 to achieve convergence Fig. 19. This is because the impulse response has a bigger maximum than before, and then also the coefficients which multiply the gradients are bigger, so that we need a bigger balancing \u00b5.\nWhen \u03b8 = 6.8 these first conclusion are confirmed in Fig.20, where \u00b5=1 is enough for convergence. This is because the solutions to (5) are different, but we can comparing again the impulse response that assume a smaller value than before.\n(2) Four real solution, 1 with multiplicity 2\nAlso in this case the system diverge when \u03b3 = \u22121. When \u03b3 = 1 we report the case in Fig.21 where we can see a behavior similar to case (1).\n(3)Two distinct real solutions, two conjugate complex solution\nWe can obtain this solutions for example for these settings of the parameters:\n\u03b8 \u03b10 \u03b11 \u03b12\n5.75 1 2 1\n7 1 2 1 1 1 3 2.25 2.25 1 3 2.25\nWhen we have the complex solution with real part smaller than the real solutions , part of the Impulse Response without oscillation (the one coming from the real solutions) disappears before the other one, and if the imaginary part is big enough we have an oscillation that lead to a more complex behavior and then to instability. We can\nchoose (see Section 7.2) \u03bb1,2 =\u22120.1 \u00b1 i , \u03bb3 =\u22121.2 , \u03bb4 =\u22121 and we have the situation in Fig.22, where a big regularization is required to convergence.\n(4)One real solution with multiplicity 2 , two conjugate complex solution\nWe can obtain this solutions for example when:\n\u03b8 \u03b10 \u03b11 \u03b12 8.2 0.2 1.2 1.8\n18.4 0.2 1.2 1.8\nand we have an analogous situation of (3).\n(5)Four complex solution, two conjugate pairs\nWe have both the case in which we have two complex conjugate pairs and the case with a complex conjugate pair with multiplicity two. They have a similar behavior depending on the magnitude of the real and imaginary parts. The real parts influence the memory of the system, whereas the imaginary parts the frequency of the sinusoidal oscillation. In each case is possible to find a value of \u00b5 which allow convergence, but often to values near to 0 with an high-oscillatory trend similar to the one reported in Fig.22, since it is due to the sinusoidal nature of the solution.\nFrom this study on the solutions is clear that the parameter \u00b5 decides again the balancing between regularization and fitting, as observed for the first order operator. The behavior w.r.t. Initial Conditions is again the same as we can see in Fig. 24. Since we have a fourth-order differential equation we need to know the values of the first n \u2212 1 = 3 derivatives of the solution yo(t) in t= 0. We indicate I.C. with the vectors y0 = [yo(0) , yo(1)(0) , yo(2)(0) , yo(3)(0)] and b0=[b o(0) , bo(1)(0) , bo(2)(0) , bo(3)(0)]."}, {"heading": "4.2.2 Choice of solutions", "text": "Like in the First order case, we are allowed to choose a suitable set of solutions and then find the parameters for our model (Section 7.2). In this case we have four solutions related to \u03b8 by the relation 2\u03b8=\u2212(\u03bb1+\u03bb2+\u03bb3+ \u03bb4). Also in this case we need a solution close to 0 to allow memorization. It is also useful not to choose another one or two little (w.r.t. \u03b8) solutions since this makes the g grow too much (Fig.25)."}, {"heading": "4.3 Sampling-step \u03c4 , Parameter \u03b8 and number of Impulses", "text": "In our first experiments we consider a totally supervised Training set, where the examples are equally spaced both in time and space. The first example comes at t1=\u03c4 , then the first supervision came at 3 2\u03c4 (see Section 7.3) and the system receives an impulse. The next example comes \u03c4 seconds after the first and so on. Since the memory is related to the saturation time of impulse response, the learning process of the system embraces all the examples appearing in the interval of time before this saturation. This means that the system has to be build so as the saturation time comes after the whole Training Set has been seen more times. Further more, since the functional in (2) contain the term e\u03b8t, the parameter \u03b8 has to be such that e\u03b8t0 = 1 is not too much smaller than e\u03b8tl (tl instant at which the last example ul comes). Another important characteristic to take in account is the delay the Impulse Response. If the supervisions (and then the impulses themselves) are too frequent, there is an accumulation of this delay that could cause instability. For these reasons it is important to study the behavior of the system w.r.t. the rate between \u03b8 and \u03c4 , with some adjustment allowed by\nthe other parameters. In this section its convenient to restrict the analysis to the second order differential operator by managing the solutions of the differential equations (5). The parameter \u03b8 is directly related to the roots \u03bb by the relation\n2\u03b8=\u2212(\u03bb1 + \u03bb2 + \u03bb3 + \u03bb4).\nThis allow us to choose a suitable \u03b8 for our model by the solutions, then find the other parameters of the model to optimize the behavior (see Section 7.2). One solution close to 0 gives memory to our system. So we choose \u03bb1 small enough to give sufficient memory w.r.t. data, from now on we fix \u03bb1=\u22121\u00d7 10\n\u22128. We split \u03b8\u2212\u03bb1 roughly equally among the others solutions, since in this way we have a quicker response with a smaller maximum (see Section 7.2), we assume they are respectively the 60,65 and 75% of 2\u03b8. Because of this choice on the parameters, in the following figures we plot at the top both the behavior of function g near the origin and its global trend. We also pose \u03b7= \u03b3/\u00b5. At the bottom we plot the last five iteration on the Training Set, to better see how the oscillation at the steady state depends on the data set. In the label we specify SO to indicate we are dealing with the second order operator.\nAs a first experiment we try to better understand the oscillatory behavior when the system reach the steady state. For start we choose our models. We start with the same Training set with l=20 and \u03c4=0.01, \u03b8=1 produce e\u03b8tl = 1.22, using the parameters reported in Fig.26 and referring to this configuration as the low dissipation one.\nWhen we increase the parameter \u03c4 = 0.1, also the period T and the oscillation period have the same increment, as shown in Fig.27.\nNow we show the behavior of the system when we reproduce a situation similar to the classic Stochastic Gradient Descent. Since the memory of the system is very long w.r.t. data (because of \u03bb1), the function g is almost constant once it reach its maximum. If the examples are far enough to allow the system to respond between two of them (i.e. g reach its maximum), we reply the gradient descent algorithm. Each examples modify the weights with a term related to the gradient calculated at the previous instant of time. As already said, the period both depends on \u03c4 and \u03b8, so that we can arbitrarily fix \u03b8 = 1 and enlarge \u03c4 = 40 so as to obtain the desired configuration with e\u03b8tl =10173. The outcome of this high dissipation setting is showed in Fig.28. In the remainder of this Section we can see at the top of the figures g again, then the behavior of the two weights y (blue) and b (green) after the first iteration on the Training Set, the total trend and the last 5 iterations at the bottom."}, {"heading": "5 First application on ANNs", "text": "As first simple practical application we try some experiment in the optimization of a simply ANN. We use a network with one hidden layer and an output layer, the identity as output function and the rectifier function:\nf(x) =\n{\nx if x>0 0 otherwise\nas activation function. In this model we have simply to extend the updating formulas for the weights y, b to the weights of the two layers. In this first application we try different setting of the parameters, with different number of units and for both the first and the second order differential operator. In the next list of experiments, we refer to some results obtained by use \u03b8=1 and 20 of units in the hidden layer, \u03c4 varying so as to guarantee a good\nvalue of e\u03b8\u03c4l, \u03b7 has been changed to guarantee the best fitting, in the second order differential operator case."}, {"heading": "5.1 One dimension functions", "text": "We first attack the practical model of the first sections, i.e. a regression task on a Set containing 100 points uk \u2208 [\u22121, 1], which are sorted and equally spaced. All the points are labeled with the target yk=2\u00b7uk \u2212 1, but only 10 points give supervision. We have the MSE= 1.77\u00b710\u22123 after 2\u00b7104 iterations. In Fig.29 we can see the trend of MSE in other 2\u00b7105 epochs if we turn off the supervisions (only labeled points for MSE evaluation).\nWe then try the same parameters for a classification task on the same set. We assign the target class true (f\u0304k=[1 0]\n\u2032) to the points in [\u22120.5, 0.5] and the class false (f\u0304k=[0 1]\n\u2032) to the others. Again we use only 10 points for supervision. After 5\u00b7104 iterations we have MSE= 0.03 and Accuracy= 0.97. Again we try to turn of the supervisions and go on with agent for other 2\u00b7105 epochs. Both MSE and accuracy remain almost the same."}, {"heading": "5.2 Two dimensions functions", "text": "We choose our point in [\u22121, 1]\u00d7 [\u22121, 1] . We use two different trajectories to cover the Training set, a spiral and a flower. We obtain the points of the\nspiral as:\nu(t) =\n{\n(t/100) cos(t) (t/100) sin(t)\nwhereas the flower trajectory is obtained by\nu(t) =\n{\ncos(10t) \u00b7 cos(t) cos(10t) \u00b7 sin(t)\nWe take 100 supervised points coming from each trajectories with t= 1, ..., 100 (26 and 40 for the class true, respectively for the flower and spiral trajectory). The points in { (x, y)\u2208R2 : |x|+ |y| \u2264 0.5 }\nrepresent the class true, the others are false. We divide our experiments in two different phases. In the first phase, we train the network with the supervised points for 105 iterations on the set obtained with one trajectory. In the second phase (validation) we check the performance of the system after some epochs without supervisions. We evaluate the performance on the two sets coming from different trajectories and on set obtained by an equally spaced grid of [\u22120.5, 0.5]\u00d7[\u22120.5, 0.5] containing 100 examples, equally split in true and false label. The results are in Table 1."}, {"heading": "5.3 Vowels Classifications", "text": "We record few tracks with the sequential pronunciation of the five Italian vowels. We process the files with Matlab and take the auditory spectra coefficients coming from RASTA PLP algorithm. We obtain a sampling of the tracks with points in a 40 dimensions features space. Set 1 is obtained from a 20 seconds track with sequential pronunciation of the vowels(2053 samples, 700 labeled). Set 2 (2144 samples) is obtained from a 20 seconds track with sequential pronunciation of the vowels repeated in time (600 labeled\npoints). Set 3 is obtained from 5 tracks, each containing the pronunciation of a vowel (14934 labeled points). We carry on the experiments with the same approach of the previous section. In the first phase we train the net with a few supervised samples. Again, in the second phase we turn off the supervisions and study the performance of the system as time goes by. After some epochs, we evaluate the agent on each set. The results are in Table 2."}, {"heading": "6 Conclusions", "text": "As already said, the studied applications are not the perfect suit of our theory. However, the positive results showed could strengthen our hypothesis and help to better understand the meaning of the different aspects. This made us look for a deeper analysis from many theoretical points of view. We are talking about study others differential operators, cost functionals and forms of the function f . Simultaneously, we would like to investigate the behavior of the current model in applications in which a manifold regularization in time plays an fundamental role, as in Computer Vision problems."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 General solution and coefficients", "text": "In this section we report some practical calculations and assumptions to solve the differential equation of our theoretical framework. The notation is finalized to a practical general implementation.\nWe can write the general form of characteristic polynomial as:\n\u03b2n\u03bb n + ...+ \u03b21\u03bb+ \u03b20 = 0 (9)\nThen we have a set of J solution \u03bbj each with they multiplicity rj. The Laplace Transform lead to:\nG(s) = 1\n\u2211n q=0 \u03b2q\u03bb\nq =\nJ \u2211\nj=1\nrj \u2211\ni=1\ncji (s\u2212 \u03bbj) ri (10)\nWhere cji are constants such that:\nJ \u2211\nj=1\nrj \u2211\ni=1\ncji(s\u2212 \u03bbj) rj\u2212i(\nJ \u220f\nk=1\nk 6=i\n(s\u2212 \u03bbk) rk) = 1 (11)\nFor practical issue we pose\n\u039bj,j = [ \u03bb1 \u00b7 \u00b7 \u00b7 \u03bb1 \u00b7 \u00b7 \u00b7 \u03bbJ \u00b7 \u00b7 \u00b7 \u03bbJ \u03bbj \u00b7 \u00b7 \u00b7 \u03bbj ] Rj,j = [ 1 \u00b7 \u00b7 \u00b7 r1 \u00b7 \u00b7 \u00b7 1 \u00b7 \u00b7 \u00b7 rJ 1 \u00b7 \u00b7 \u00b7 rj ] \u039bj,q = [ \u03bb1 \u00b7 \u00b7 \u00b7 \u03bb1 \u00b7 \u00b7 \u00b7 \u03bbJ \u00b7 \u00b7 \u00b7 \u03bbJ \u03bbj \u00b7 \u00b7 \u00b7 \u03bbq ] Rj,q = [ 1 \u00b7 \u00b7 \u00b7 r1 \u00b7 \u00b7 \u00b7 1 \u00b7 \u00b7 \u00b7 rJ 1 \u00b7 \u00b7 \u00b7 rq ] , q \u2264 j In = [ 1 \u00b7 \u00b7 \u00b7 n ] (12) To simplify the notation we pose R=RJ,J , \u039b=\u039bJ,J . If we carry out the summation in (10)(with the new notation), pose nj = n\u2212Rj we find that each cji multiply a factor:\ncji\n\n \nnj \u2211\nk=0\nsnj\u2212k\n\n \n\u2211\ni\u2208Cnj,k(Inj )\n(\ninj \u220f\np=i1\n\u2212\u039bj,ip )\n\n \n\n  = cji\nnj \u2211\nk=0\nsnj\u2212kAl,j+i , l = nj\u2212k+1\n(13)\nthen we can determine C = [c11 \u00b7 \u00b7 \u00b7 c1r1 \u00b7 \u00b7 \u00b7 cJ1 \u00b7 \u00b7 \u00b7 cJrJ ] \u2032 from the system AC = b , b = [10 \u00b7 \u00b7 \u00b7 0]\u2032 and A\u2208Rn,n with:\nAl,j+i =\n{\n\u2211 i\u2208Cnj ,k(Inj ) ( \u220finj p=i1 \u2212\u039bj,ip ) if 1 \u2264 l \u2264 nj + 1 , k=nj + 1\u2212 l 0 nj + 1 < l \u2264 n (14)\nThe general solution is of the form:\ng(t) =\nJ \u2211\nj=1\nCjt Rj\u22121e\u039bjt (15)\nWhen we have a complex solution \u03bbj = \u03b1 + i\u03b2, also its conjugate \u03bbp = \u03bb\u0304j = \u03b1 \u2212 i\u03b2 is present and also the relatives constants are such that Cp = C\u0304j = \u03b1c \u2212 i\u03b2c. We have in the solution :\n\u00b7 \u00b7 \u00b7+ Cj \u00b7 e \u03b1t (cos \u03b2t+ i sin \u03b2t) + Cp \u00b7 e \u03b1t (cos(\u2212\u03b2t) + i sin(\u2212\u03b2t)) + \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7+ Cj \u00b7 e \u03b1t (cos \u03b2t+ i sin \u03b2t) + Cp \u00b7 e \u03b1t (cos(\u03b2t)\u2212 i sin(\u03b2t)) + \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 + (Cj +Cp) \u00b7 e \u03b1t (cos \u03b2t) + (Cj \u2212 Cp) \u00b7 e \u03b1t (i sin \u03b2t) + \u00b7 \u00b7 \u00b7 (16)\nand since\n(Cj + Cp) = \u03b1c + i\u03b2c + \u03b1c \u2212 i\u03b2c\n(Cj \u2212 Cp) = \u03b1c + i\u03b2c \u2212 \u03b1c + i\u03b2c\nthe (16) becomes\n\u00b7 \u00b7 \u00b7+ 2\u03b1c \u00b7 e \u03b1t (cos \u03b2t) + 2i\u03b2c \u00b7 e \u03b1t (i sin \u03b2t) + \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7 + 2\u03b1c \u00b7 e \u03b1t (cos \u03b2t)\u2212 2\u03b2c \u00b7 e \u03b1t (sin\u03b2t) + \u00b7 \u00b7 \u00b7 (17)\nsince this is the contribution of two solution with the same real and imaginary parts, we can write (15) as\ng(t) =\nJ \u2211\nj=1\ntRj\u22121e\u211c(\u039bj )t (\u211c(Cj)(cos(\u2111(\u039bj)t))\u2212\u2111(Cj)(sin(\u2111(\u039bj)t))) (18)\nalso the solution to the homogeneous equation is of the form\nyo(t) = J \u2211\nj=1\nKjt Rj\u22121e\u039bjt (19)\nwhere Kj are determined by imposing the Initial Conditions given for yo(t), that is Y0 = [y o(0) \u00b7 \u00b7 \u00b7 yo(n\u22121)(0)]\u2032. Since\nyo(d)(0) =\nJ \u2211\nj=1\n(d+ 2\u2212Rj) +Kj\u039b\n(d+1\u2212Rj) + j (20)\nwe can find K = [K1 \u00b7 \u00b7 \u00b7Kn] \u2032 by solving the system MK = Y0 where:\nMvj = (v + 1\u2212Rj) +\u039b\n(v\u2212Rj ) + j (21)\nand exactly as in the case of g(t) we can write\ny(t) = J \u2211\nj=1\ntRj\u22121e\u211c(\u039bj)t (\u211c(Kj)(cos(\u2111(\u039bj)t))\u2212\u2111(Kj)(sin(\u2111(\u039bj)t))) (22)"}, {"heading": "7.2 From solution to parameters", "text": "In section 4.2 we saw that for the second order case convergence depends not only on \u03b3, but also on the kind of the solutions of the characteristic polynomial, i.e. on \u03b2j . Moreover, the most important parameter of our model is \u03b8, which allow to choose the memory width of the system. This memory is related to the function \u03c8(t)=e\u03b8t, which represent the weight that the model assigns to each samples as the time goes by. The smaller is \u03b8( but always > 0) the bigger is the memory of our model. We are interested in find suitable values of \u03b1j which allow convergence when \u03b8 is small. In practice, we can build our model only by the solutions of the characteristic poly, since the parameters influence the updating formulas (4),(8) only by the last \u03b1j, which can be absorbed in the term \u00b5. Then we can choose directly suitable solutions, and verify that they are related to meaningful values of parameters. So, starting from the solutions (chosen in a way to have the desired \u03b8), is easy to find the coefficients of the characteristic poly and then find the rate between the \u03b1j that generate the model."}, {"heading": "7.2.1 First Order", "text": "If we have the solutions \u03bb1, \u03bb2 the characteristic poly of (3) is\n\u03bb2 + \u03b8\u03bb+ \u03b2 = (\u03bb\u2212 \u03bb1) (\u03bb\u2212 \u03bb2) .\nSo the value of \u03b8 is given by \u03b8 = \u2212 (\u03bb2 + \u03bb1) and the rate \u03b10 \u03b11\nis computable from \u03b2. That is, if we pose \u03bd = \u03b10\u03b11 , we can find suitable value of \u03b1j from the solutions of\n\u03bd2 \u2212 \u03b8\u03bd + \u03b2 = 0. (23)"}, {"heading": "7.2.2 Second Order", "text": "In this case the characteristic poly is \u03bb4 + \u03b23\u03bb 3 + \u03b22\u03bb 2 + \u03b21\u03bb + \u03b20 = 0. The coefficient \u03b23 is still given by the opposite of the summation among the solutions, and since \u03b23=2\u03b8 it is still easy to set \u03b8. To find the rates among \u03b1j is convenient to work with \u03bd0 =\n\u03b10 \u03b12 and \u03bd1 = \u03b11 \u03b12 so that:\n\u03b20= \u03b10\u03b12\u03b8\n2 \u2212 \u03b10\u03b11\u03b8 + \u03b1 2 0\na22 =\u03bd0\u03b8\n2 \u2212 \u03bd0\u03bd1\u03b8 + \u03bd 2 0 (24)\n\u03b21= \u03b11\u03b12\u03b8\n2 + (2\u03b10\u03b12 \u2212 \u03b1 2 1)\u03b8\n\u03b122 =\u03bd1\u03b8\n2 + 2\u03bd0\u03b8 \u2212 \u03bd 2 1\u03b8 (25)\n\u03b22= \u03b122\u03b8 2 + \u03b11\u03b12\u03b8 + 2\u03b10\u03b12 \u2212 \u03b1 2 1\n\u03b122 =\u03b82 + \u03bd1\u03b8 + 2\u03bd0 \u2212 \u03bd 2 1 (26)\nFrom the equation (26) we can find the relation among the coefficients:\n\u03b21= \u03b23\u03b22 2 \u2212 \u03b233 8\n(27)\nfrom equation (25) we find\n\u03bd0= 1\n2\u03b8\n(\n\u03b21 + \u03bd 2 1\u03b8 \u2212 \u03bd1\u03b8 2 )\n(28)\nand from (24) we have\n\u03bd41 \u2212 4\u03b8\u03bd 3 1 + \u03bd 2 1\n( 5\u03b82 + 2\u03b21/\u03b8 ) + \u03bd1 ( \u22122\u03b83 \u2212 4\u03b21 ) + ( 2\u03b8\u03b21 + \u03b2 2 1/\u03b8 2 \u2212 4\u03b20 )\n=0. (29)\nOnce we find the solutions of (29) we can choose a suitable one and find \u03bd0, and then have an idea of which \u03b1j generate our model. For practical reasons, since \u03b20 = \u03bb1 \u03bb2 \u03bb3 \u03bb4, by using (24) we can find \u03b11 by posing \u03b10=\u03b12=1 in (24):\n\u03b11=\u03bd1= \u03bd0 \u03b8 2 + \u03bd20 \u2212 \u03b20 \u03bd0 \u03b8 . (30)\nIn our study on the model parameters, we see that is convenient to have one solution close to zero (which guarantees memory to the system). We can choose \u03bb1=1/a (where a is a parameters which represent the memory of the system, since the saturation time is proportional to a). Since the modulus of the other solutions determine the shape of the Impulse response, is convenient to write the solutions as:\n\u03bb1 = c1 \u03b8=1/a \u03bb2 = c2 \u03b8 \u03bb3 = c3 \u03b8 \u03bb4 = c4 \u03b8\n(31)\nwhere \u2211\ncj=2 and c2 , c3 , c4 as to be similar among them (but not too much to avoid numerical error) to give a quick Impulsive Response.\nNow we are allowed to choose suitable solutions w.r.t. the model and then find the value of \u03b3/\u00b5 which guarantee convergence and the best fitting performance, both for first and second order."}, {"heading": "7.3 From continuos to discrete model", "text": "In the practical implementation is not convenient to use the continuos updating formulas (4),(8), since we have to store too much value of the gradient, that is, the wider is memory of the system, the bigger is the number of elements that we have to remember. For this reason, is convenient to use a discretization of the system. Indeed, when we have a linear differential equation of order bigger than one, we can transform it in a system of the same order with only linear differential equations of order one. In our case in\nD4y + \u03b23D 3y + \u03b22D 2y + \u03b21Dy + \u03b20y + \u03b7\nl \u2211\nk=1\n\u03b6k \u00b7 \u03b4(t \u2212 tk)=0\nwe can substitute y0= y, y1=Dy, ... and posing u(t)= \u03b7 \u2211l\nk=1 \u03b6k \u00b7 \u03b4(t \u2212 tk) so that to have:\n\n  \n  \ny1 = y \u2032 0 y2 = y \u2032 1 y3 = y \u2032 2 y4 = \u2212\u03b23y3 \u2212 \u03b22y2 \u2212 \u03b21y1 \u2212 \u03b20y0 \u2212 u(t)\n(32)\nand then we have the system\ny\u0307 =\n\n   y1 y2 y3 y4\n\n   =\n\n   0 1 0 0 0 0 1 0 0 0 0 1\n\u2212\u03b20 \u2212\u03b21 \u2212\u03b22 \u2212\u03b23\n\n  \n\n   y0 y1 y2 y3\n\n   +Bu = Ay +Bu (33)\nwhere\nA =\n\n   0 1 0 0 0 0 1 0 0 0 0 1\n\u2212\u03b20 \u2212\u03b21 \u2212\u03b22 \u2212\u03b23\n\n   B =\n\n   0 0 0\n\u22121\n\n  \nfrom the Lagrange formula we have\ny(t)=eA(t\u2212t0)y(t0) +\n\u222b t\nt0\neA(t\u2212s) \u00b7Bu(s)ds. (34)\nWhen we consider an equally spaced discretization of the time of width \u03c4 , a general instant of time is t = \u03c4K and if we assume t0=0, the general evolution of the system can be computed by\ny[K] = y(\u03c4K) = eA\u03c4Ky[0] +\n\u222b \u03c4K\n0 eA(\u03c4K\u2212s) \u00b7Bu(s)ds (35)\nand at the next step we have\ny[K + 1] = eA\u03c4(K+1)y[0] +\n\u222b \u03c4(K+1)\n0 eA(\u03c4(K+1)\u2212s) \u00b7Bu(s)ds\n= eA\u03c4 ( eA\u03c4Ky[0] + \u222b \u03c4K\n0 eA(\u03c4K\u2212s) \u00b7Bu(s)ds\n)\n+\n+\n\u222b \u03c4(K+1)\n\u03c4K eA[\u03c4(K+1)\u2212s] \u00b7Bu(s)ds\n= eA\u03c4y[K] +\n\u222b \u03c4(K+1)\n\u03c4K eA[\u03c4(K+1)\u2212s] \u00b7Bu(s)ds\nsince u(t) is composed by a summation of impulses we have a summation of integrals in the second term. If we assume that each impulse (i.e. each\nsupervision) is provided in the middle of two step, we have that only the integrals referred to the last impulse (the one provided at h=\u03c4K + \u03c4/2) is different from 0 :\n\u222b \u03c4(K+1)\n\u03c4K eA(\u03c4(K+1)\u2212s) \u00b7Bu(s)ds = \u03b7\nl \u2211\nk=1\n\u222b \u03c4(K+1)\n\u03c4K eA[\u03c4(K+1)\u2212s] \u00b7B \u03b6k \u00b7 \u03b4(s \u2212 tk)\n= \u03b7\n\u222b \u03c4(K+1)\n\u03c4K eA[\u03c4(K+1)\u2212s] \u00b7B \u03b6h \u00b7 \u03b4[s \u2212 (\u03c4K + \u03c4/2)]\n= \u03b7 eA[\u03c4(K+1)\u2212(\u03c4K+\u03c4/2)] \u00b7B \u03b6h = \u03b7 eA\u03c4/2 \u00b7B \u03b6h\nthat is y[K + 1] = eA\u03c4y[K] + eA\u03c4/2 \u00b7B \u03b7 \u03b6h. (36)"}], "references": [{"title": "Variational foundations of online backpropagation", "author": ["Salvatore Frandina", "Marco Gori", "Marco Lippi", "Marco Maggini", "Stefano Melacci"], "venue": "In Artificial Neural Networks and Machine Learning - ICANN 2013 - 23rd International Conference on Artificial Neural Networks, Sofia, Bulgaria,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The principle of least cognitive action", "author": ["Alessandro Betti", "Marco Gori"], "venue": "Theoretical Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Neural network training as a dissipative process", "author": ["Marco Gori", "Marco Maggini", "Alessandro Rossi"], "venue": "Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "On-line Learning on Temporal Manifolds, pages 321\u2013333", "author": ["Marco Maggini", "Alessandro Rossi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "We briefly introduce basics ideas of this theory, first formulated in [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "The concept of dissipation is well formulated in [2],whereas an summing up of this report and an experimental analysis on standard benchmark can be find in [3].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "The concept of dissipation is well formulated in [2],whereas an summing up of this report and an experimental analysis on standard benchmark can be find in [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "A further theoretical abstraction applied to similar environment is proposed in [4], In this document, we will give a slightly theoretical formulation in order to allow us to go straight to the practical implementation issues.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "We assign the target class true (f\u0304k=[1 0] ) to the points in [\u22120.", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "5] and the class false (f\u0304k=[0 1] ) to the others.", "startOffset": 28, "endOffset": 33}], "year": 2017, "abstractText": null, "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}