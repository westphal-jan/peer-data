{"id": "1701.02946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Cross-lingual RST Discourse Parsing", "abstract": "adaptive discourse parsing is an inherent integral part of communication understanding information flow and argumentative metadata structure in informal documents. most previous research had has simply focused firmly on diver inducing data and evaluating models from the synthetic english rst syntax discourse linguistic treebank. however, discourse construct treebanks modeled for other languages normally exist, including spanish, german, basque, dutch romani and brazilian while portuguese. the same treebanks share the basic same underlying linguistic theory, many but differ but slightly more in themselves the way documents are annotated. specifically in this 1998 paper, collectively we present ( a ) a new algebraic discourse terminology parser which it is simpler, bigger yet competitive ( significantly put better on ie 2 / 3 metrics ) to state of'the art for structural english, ( b ) a harmonization proposal of improved discourse semantic treebanks adopted across languages, enabling behind us often to present ( alternatively c ) along what seem to call the mean best ends of our knowledge, are the first experiments on cross - lingual discourse parsing.", "histories": [["v1", "Wed, 11 Jan 2017 12:16:25 GMT  (35kb,D)", "http://arxiv.org/abs/1701.02946v1", "To be published in EACL 2017, 13 pages"]], "COMMENTS": "To be published in EACL 2017, 13 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chlo\\'e braud", "maximin coavoux", "anders s{\\o}gaard"], "accepted": false, "id": "1701.02946"}, "pdf": {"name": "1701.02946.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual RST Discourse Parsing", "authors": ["Chlo\u00e9 Braud", "Anders S\u00f8gaard"], "emails": ["braud@di.ku.dk", "soegaard@di.ku.dk", "maximin.coavoux@etu.univ-paris-diderot.fr"], "sections": [{"heading": "1 Introduction", "text": "Documents can be analyzed as sequences of hierarchical discourse structures. Discourse structures describe the organization of documents in terms of discourse or rhetorical relations. For instance, the three discourse units below can be represented by the tree in Figure 1, where a relation COMPARISON holds between the segments 1 and 2, and a relation ATTRIBUTION links the segment covering the units 1 and 2, and the segment 3.1\n1 Consumer spending in Britain rose 0.1% in the third quarter from the second quarter\n2 and was up 3.8% from a year ago,\n3 the Central Statistical Office estimated.\n1\u201cNS\u201d and \u201cNN\u201d in Figure 1 describe the nuclearity of the segments, see Section 3.\nRhetorical Structure Theory (RST) (Mann and Thompson, 1988) is a prominent linguistic theory of discourse structures, in which texts are analyzed as constituency trees, such as the one in Figure 1. This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Li et al., 2014; Ji and Eisenstein, 2014). Such parsers have proven to be useful for various downstream applications (Daume\u0301 III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015).\nThere are discourse treebanks for other languages than English, including Spanish, German, Basque, Dutch, and Brazilian Portuguese. However, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al., 2015).\nMoreover, all discourse corpora are limited in size, since annotation is complex and time consuming. This data sparsity makes learning hard, especially considering that discourse parsing involves several complex and interacting factors, ranging from syntax and semantics, to pragmat-\nar X\niv :1\n70 1.\n02 94\n6v 1\n[ cs\n.C L\n] 1\n1 Ja\nn 20\n17\nics. We thus propose to harmonize existing corpora in order to leverage information by combining datasets in different languages.\nContributions In this paper, we propose a new discourse parser that is significantly better than existing parsers for English on 2/3 standard metrics. Our parser relies on fewer features than previous work and is arguably algorithmically simpler. Moreover, we present the first end-to-end statistical discourse parsers for other languages than English (6 languages, in total). We also present the first experiments in cross-lingual discourse parsers, showing that discourse parsing is possible even when no or very little labeled data is available for the language of interest. We do so by harmonizing available discourse treebanks, enabling us to apply models across languages. We make the code and preprocessing scripts available for download at https://bitbucket.org/ chloebt/discourse."}, {"heading": "2 Related Work", "text": "The first text-level discourse parsers were developed for English, relying mainly on hand-crafted rules and heuristics (Marcu, 2000a; Carlson et al., 2001). Hernault et al. (2010, HILDA) greedily use SVM classifiers to make attachment and labeling decisions, building up a discourse tree. Joty et al. (2012, TSP) build a two-stage parsing system, training separate sequential models (CRF) for the intra- and the inter-sentential levels. These models jointly learn the relation and the structure, and a CKY-like algorithm is used to find the optimal tree. Feng and Hirst (2014) use CRFs only as local models for the inter- and intra-sententials levels. For Brazilian Portuguese, for example, the first system, called DiZer (Pardo and Nunes, 2008; Maziero et al., 2011), was also rule-based, but there has been some work on using classification of intra-sentential relations (Maziero et al., 2015).\nRecently studies have focused on building good representations of the data. Feng and Hirst (2012) introduced linguistic features, mostly syntactic and contextual ones. Li et al. (2014) used a recursive neural network that builds a representation for each clause based on the syntactic tree, and then apply two classifiers as in Hernault et al. (2010). This leads to the best performing system for unlabeled structure (85.0 in F1). The system presented by Ji and Eisenstein (2014, DPLP) jointly learns the representation and the task: a large mar-\ngin classifier is used to learn the actions of a shiftreduce parser, optimizing at the same time the loss of the parser and a projection matrix that maps the bag-of-word representation of the discourse units into a new vector space. This system, however, only slightly outperforms the original bag-of-word representation. DPLP is the best performing discourse parser for labeled structure, 71.13 in F1 for nuclearity and 61.63% for relation.\nOur system is similar to these last approaches in learning a representation using a neural network. However, we found that good performance can already be obtained without using all the words in the discourse units, resulting in a parser that is faster and easier to adapt, as demonstrated in our multilingual experiments, see Section 7."}, {"heading": "3 RST framework", "text": "Discourse analysis In building a discourse structure, the text is first segmented into elementary discourse units (EDU), mostly clauses. EDUs are the smallest discourse units (DUs). Discourse relations are then used to build DUs, recursively. A non-elementary DU is called a complex discourse unit (CDU). The structure of a document is the set of linked DUs. In this paper, we focus on the Rhetorical Structure Theory (RST), a theoretical framework proposed by Mann and Thompson (1988).\nNuclearity A DU is either a nucleus or a satellite, the nucleus being the most important part of the relation (i.e. of the text), while the satellite contains additional, less important information. In general, this feature depends on the relation: a relation can be either mono-nuclear (with a scheme nucleus-satellite or satellite-nucleus depending on the relative order of the spans), or multi-nuclear. Some relations can be either monoor multi-nuclear, such as consequence or evaluation in the RST-DT.\nBinary trees In the original RST framework, each relation is associated with an application scheme that defines the nuclearity of the DUs (mono- or multi-nuclear relation), and the number of DUs linked. Among the six schemes, two correspond to a link between more than two DUs, either a nucleus shared between two mono-nuclear relations (e.g. motivation and enablement) or a relation linking several nuclei (e.g. list). Marcu (1997) proposed to simplify the representation to\naThe test set contains 84 documents doubly annotated, we report figures for annotator A.\nbinary trees, and all discourse parsers are built on a binary representation."}, {"heading": "4 Data", "text": "We test our discourse parser on six languages, using available RST corpora harmonized as described in Section 4.2. Information about the datasets are summarized in Table 1."}, {"heading": "4.1 RST corpora", "text": "English The RST Discourse Treebank (Carlson and Marcu, 2001), from now on En-DT, is the most widely used corpus to build discourse parsers. It contains 385 documents in English from the Wall Street Journal. The relation set contains 56 relations (ignoring nuclearity and embedding information2). The inter-annotator agreement scores are 88.70 for the unlabeled structure (score \u201cSpan\u201d), 77.72 for the structure with nuclearity (\u201cNuclearity\u201d) and 65.75 with relations (\u201cRelation\u201d).3\nBrazilian Portuguese We merged all the corpora annotated for Brazilian Portuguese, as in (Maziero et al., 2015), to form the Pt-DT. The largest corpus is CST-News4 (Cardoso et al., 2011), it is composed of 140 documents from the news domain annotated with 31 relations. Authors report agreement scores corresponding to nuclearity (0.78 in F1) and relations (0.66).\nThe other corpora are: Summ-it5 (Collovini et al., 2007) \u2013 50 texts from science articles\n2In this corpus, the embedded relations are annotated with a specific label (suffix \u201c-e\u201d) that we removed.\n3See Section 6 for a description of these metrics. 4http://nilc.icmc.usp.br/CSTNews/\nlogin/?next=/CSTNews/ 5http://www.inf.pucrs.br/ontolp/ downloads-ontolpplugin.php\nin a newspaper, annotated with 29 relations; Rhetalho6 (Pardo and Seno, 2005) \u2013 40 texts from the computer science and news domains, annotated with 23 relations; and CorpusTCC6 (Pardo and Nunes, 2003; Pardo and Nunes, 2004) \u2013 100 introductions of scientific texts in computer science, annotated with 31 relations.\nSpanish The Spanish RST DT7 (da Cunha et al., 2011), from now on Es-DT, contains 267 texts written by specialists on different topics (e.g. astrophysics, economy, law, linguistics) The relation set contains 29 relations. The authors report interannotator agreement of 86% in precision for the unlabeled structure, 82.46% for the structure with nuclearity and 76.81% with relations.\nGerman The Postdam Commentary Corpus 2.08 (Stede, 2004; Stede and Neumann, 2014), from now on De-DT, contains newspaper commentaries annotated at several levels. A part of this corpus (MAZ) contains 175 documents annotated within the RST framework using 30 relations.9\nDutch The corpus for Dutch (Vliet et al., 2011; Redeker et al., 2012), from now on Nl-DT, contains 80 documents from expository (encyclopedias and science news website) and persuasive (fund-raising letters and commercial advertisements) genres, annotated with 31 relations. The authors report an agreement of 0.83 for discourse spans, 0.77 for nuclearity and 0.70 for relations.\n6http://conteudo.icmc.usp.br/pessoas/ taspardo/Projects.htm\n7http://corpus.iingen.unam.mx/rst/ index_en.html\n8http://angcl.ling.uni-potsdam.de/ resources/pcc.html\n9We systematically ignore the first segment of each document, the title, that is not linked to the rest of the text.\nBasque The Basque RST DT10 (Iruskieta et al., 2013), from now on Eu-DT, contains 88 abstracts from three specialized domains \u2013 medicine, terminology and science \u2013, annotated with 31 relations. The inter-annotator agreement is 81.67% for the identification of the CDU (Iruskieta et al., 2015), and 61.47% for the identification of the relations.\nOther corpora To the best of our knowledge, the only two non English corpora not included are the one annotated for Tamil (Subalalitha and Parthasarathi, 2012) that we were unable to find, and the (intra-sentential) one developed for Chinese (Wu et al., 2016), for which we were unable to produce RST trees since annotation does not contain nuclearity indications.\nFor English, there are corpora annotated for other domains than the one covered by the EnDT. We however leave out-of-domain evaluation for future work: it requires to decide how to use a corpus annotated only at the sentence level (SFU review corpus)11, or a corpus annotated with genre specific relations (Subba and Di Eugenio, 2009)."}, {"heading": "4.2 Harmonization of the datasets", "text": "Recent discourse parsers built on the En-DT are based on pre-processed data: the corpus contains only binary trees, with the large label set mapped to 18 coarse-grained classes. In this section, we describe this pre-processing step for all corpora used. Discourse corpora have been released under three different file formats: dis (En-DT), lisp (Rhetalho and CorpusTCC) and rs3 (all remaining corpora). The first two ones are bracketed format, the third one is an XML encoding. In all cases, the trees encoded do not look like the one in Figure 1: the relations are annotated on the daughter nodes, on the satellite for mono-nuclear relations, or on all the nuclei for multi-nuclear relations. Moreover, in the rs3 format, the nuclearity of the segments is not directly annotated, it has to be retrieved using the type of the relation (indicated at the beginning of each file) and the previous principle. Our pre-processing step leads to corpora with bracketed files representing directly the RST trees (as in Figure 1) with stand-off annotation of the text of the EDUs.\nNote that, even if harmonized, the corpora are not parallel, making it hard to use them to study\n10http://ixa2.si.ehu.es/diskurtsoa/en/ 11https://www.sfu.ca/\u02dcmtaboada/\nresearch/SFU_Review_Corpus.html\nlanguage variations for the discourse level. Some preliminary work exists on this question (Iruskieta et al., 2015).\nPre-processing Some documents (format rs3) contain several roots or empty segments. We were generally able to remove useless units, that is units that are not linked to other ones within the tree, except for one document in the CST corpus (two roots, both linked to other units).\nAnother issue concerns unordered EDUs: the structure annotated contains nodes spanning non adjacent EDUs. In general, we were able to correct these cases, but we failed to automatically produce trees spanning only adjacent EDUs for three documents in the Eu-DT, and one document in the De-DT.\nBinarization All the corpora contain non-binary trees that we map to binary ones. In the En-DT, common cases of non-binarity are nodes whose daughters all hold the same multi-nuclear relation \u2013 indicating that this relation spans multiple DUs, e.g. list.12 In rare cases, the children are two satellites and a nucleus \u2013 indicating that the nucleus is shared by the satellites. These configurations are the ones described in (Marcu, 1997) (see Section 3), and choosing right or left-branching leads to a similar interpretation. For the En-DT, rightbranching is the chosen strategy since (Soricut and Marcu, 2003).\nWe found more diverse cases in the other corpora, and, for some of them, right-branching is impossible. It is the case when the daughters are one nucleus (annotated with \u201cSpan\u201d, only indicating that this node spans several EDUs) and more than two satellites holding different relations \u2013 i.e. the nucleus is shared by all the relations. More precisely, the issue arises when the last two children are satellites. Using right-branching, we end with a node with two satellites as daughters, and thus a ill-formed tree. In order to keep as often as possible the \u201cright-branching by default\u201d strategy, we first do a right-branching and then a leftbranching: beginning with four children \u2013 S1-Ri, N2-Span, S3-Rj and S4-Rk, indicating the relations Ri(S1, N2), Rj(N2, S3) and Rk(N2, S4)13 \u2013 , we end up with the tree in Figure 2. Finally, we used a right-branching in all cases, except when the two last children are satellites.\n12Recall that in the original format, the relation is not annotated on the parent node but on the children.\n13S being a satellite, N a nucleus and R a relation.\nLabel set harmonization We map all the relations used in the corpora to the 18 coarse grained classes (Carlson and Marcu, 2001) used to build the most recent discourse parsers on the En-DT.14\nThe mapping for the En-DT is given in (Carlson and Marcu, 2001). For all the other corpora, we first map all the relations that exist in this mapping (i.e. used in the En-DT annotation scheme) to their corresponding classes. We end with 18 problematic relations, that is relations that were not used when annotating the En-DT.\nAmong them, 10 can be mapped easily, because they directly correspond to a class \u2013 explanation is mapped to the class EXPLANATION, elaboration to ELABORATION, joint to JOINT \u2013, because they were just renamed \u2013 reformulation is mapped to the class RESTATEMENT and solutionhood (same as problem-solution) to TOPICCOMMENT \u2013, or because they correspond to a more-fine grained formulation of existing relations \u2013 entity-elaboration is mapped to ELABORATION and the 4 volitional/non-volitional cause and result are mapped to the class CAUSE, corresponding to the relations cause and result in the En-DT.\nFor the remaining relations, we looked at the definition of the relations15 to decide on a mapping. Note that this label mapping is made quite easy by the fact that all the corpora were annotated following the same underlying theory \u2013 they thus use relations defined using similar criteria \u2013, and that we are using a coarse-grained classification \u2013 we thus do not need to decide whether a relation is equivalent to another one, but rather whether it fits the properties of the other relations within a specific class. Label mappings for corpora annotated following different frameworks are still\n14The full mapping is provided in Appendix A. 15http://www.sfu.ca/rst/01intro/\ndefinitions.html\ndiscussed (Roze, 2013; Benamara and Taboada, 2015).\nWe decided on the following mapping, considering the properties of the relations and the classes: parenthetical \u2013 used to give \u201cadditional details\u201d \u2013 is mapped to ELABORATION, conjunction \u2013 similar to a list with only two elements \u2013 to JOINT, justify \u2013 similar to Explanationargumentative \u2013 and motivation \u2013 quite similar to reason and grouped with evidence in (Benamara and Taboada, 2015) \u2013 to EXPLANATION, preparation \u2013 presenting preliminary information, increasing the readiness to read the nucleus \u2013 to BACKGROUND, and unconditional and unless \u2013 linked to condition \u2013 to CONDITION.\nFinally, note that this mapping does not lead to having the same relation set for all the corpora, and that the relation distribution could vary among the datasets."}, {"heading": "5 Discourse Parser", "text": "Our discourse parser builds discourse structures from segmented texts, we did not implement discourse segmenters for each language. Discourse segmenters only exist for English (Hernault et al., 2010) (95, 0% in F1), Brazilian Portuguese (Pardo and Nunes, 2008) (56.8%) and Spanish (da Cunha et al., 2010; da Cunha et al., 2012) (80%). Discourse segmenters can be built quite easily relying only on manual rules as it is the case for the Spanish and Portuguese ones, especially considering that segmentation has generally been made coarser in the corpora built after the En-DT (Vliet et al., 2011). While improving this first step is crucial, we focus on the harder step of tree building."}, {"heading": "5.1 Description of the Parser", "text": "We used the syntactic parser described in Coavoux and Crabbe\u0301 (2016), in the static oracle setting. We chose this parser because it can take pre-trained embeddings as input and, more importantly, because it was designed for morphologically rich languages and thus takes as input not only tokens and POS tags, but any token attribute that is then mapped to a real-valued vector, which allows the use of complex features.\nThe parser is a transition-based constituent parser that uses a lexicalized shift-reduce transition system (Sagae and Lavie, 2005). The transition system is based on two data structures \u2013 a stack (S) stores partial trees and a queue (B) con-\ntains the unparsed DUs. A parsing configuration is a couple \u3008S,B\u3009. In the initial configuration, S is empty and B contains the whole document. The parser iteratively applies actions to the current configuration, in order to derive new configurations until it reaches a final state, i.e. a parsing configuration where B is empty and S contains a single element (the root of the tree).\nThe actions are defined as follows:\n\u2022 SHIFT pops an EDU from B and pushes it onto S.\n\u2022 REDUCE-R-X and REDUCE-L-X pop two DUs from S, push a new CDU with the label X on S and assign its nucleus (Left or Right).\nScoring System As in Chen and Manning (2014), at each parsing step, the parser scores actions with a feed-forward neural network. The input of the network is a sequence of typed symbols extracted from the top elements of S and B. The symbols are typically discourse relations or attributes of their nucleus EDU (e.g. first word of EDU, see Section 5.3).\nThe first layer of the network projects these symbols onto an embedding space (each type of symbol has its own embedding matrix). The following two layers are non-linear layers with a ReLU activation. The output of the network is a probability distribution over possible actions computed by a softmax layer.\nTo generate a set of training examples {a(i), c(i)}Ni=1, we used the static oracle to extract the gold sequence of actions and configurations for each tree in the corpus. The objective function of the parser is the negative log-likelihood of gold actions given corresponding configurations:\nL(\u03b8) = \u2212 N\u2211 i=1 logP (a(i)|c(i);\u03b8)\nwhere \u03b8 is the set of all parameters, including embedding matrices.\nWe optimized this objective with the averaged stochastic gradient descent algorithm (Polyak and Juditsky, 1992). At inference time, we used beamsearch to find the best-scoring tree."}, {"heading": "5.2 Cross-lingual Discourse Parsing", "text": "Our first experiments are strictly monolingual, and they are intended to give state-of-the-art performance in a fully supervised setting. We consider\nthat we need at least 100 documents to build a monolingual model, since we already keep around 65 documents for test and development.\nWe then evaluate multi-source transfer methods, considering one language as the target and the others as sources. More precisely, we will evaluate two settings: (1) training and optimizing only on the available source data; (2) training on all available data, including target ones if any, and optimizing on the development set of the target language. Setting (1) provides performance when no data are available at all in the target language, while (2) aims at evaluating if one can expect improvements by simply combining all the available data.\nWhen combining the corpora, we cannot ignore lexical information as it has been done for syntactic parsing with delexicalized models (McDonald et al., 2011). Discourse parsing is a semantic task, at least when it comes to predict a rhetorical relation between two spans of text, and information from words have proven to be crucial (Rutherford and Xue, 2014; Braud and Denis, 2015). We thus include word features using bilingual dictionaries \u2013 i.e. translating the words used as features into a single language (English) \u2013, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing. More precisely, we used the cross-lingual word representations presented in (Levy et al., 2017) that allow multi-source learning and have proven useful for POS tagging but also more semantic-oriented tasks, such as dependency parsing and document classification."}, {"heading": "5.3 Features", "text": "As in previous studies, we used features representing the two EDUs on the top of the stack and the EDU on the queue. If the stack contains CDUs, we use the nuclearity principle to choose the head EDU, converting multi-nuclear relations into nucleus-satellite ones as done since (Sagae, 2009). However, we found that using these information also for the left and right children of the two CDUs on the top of the stack, and adding as a feature the representation built for these two CDUs lead to important improvements.\nLexical features We use the first three words and the last word along with their POS, features that have proven useful for discourse (Pitler et al., 2009), and the words in the head set (Sagae, 2009)\n\u2013 i.e. words whose head in the dependency graph is not in the EDU \u2013, here limited to the first three.16 This head set contains the head of the sentence (in general, the main event), or words linked to the main clause when the segment does not contain the head (especially, discourse connectives that are subordinating or coordinating conjunctions could be found there). The words are the boundaries could also contain discourse connectives, adverbs or temporal expressions that could be relevant for discourse structure. Note however that these feature have been built for English, and they could be less useful for other languages. We leave the question of investigating their utility linked to word order differences for future work.\nNote that we do not use all the words in the EDUs as features, contrary to (Li et al., 2014; Ji and Eisenstein, 2014). Our only word features are the words in the head set and at the boundaries, thus 7 words per EDU. When using word embeddings, we concatenate the vectors for each word, each of d dimensions, keeping the same order to build a vector of 7d dimensions (e.g., the first word of the EDU corresponds to the first d dimensions, the second has values between d and 2d).\nPosition and length Other features are used to represent the position of the EDU in the document and its length in tokens. We use thresholds to distinguish between very long (length l > 25 tokens), long (l > 15), short (l > 5) and very short (l \u2264 5) EDUs. We also distinguish between the \u201cfirst\u201d and the \u201clast\u201d EDU in the document, and use also a threshold on the ratio s =(position of the EDU divided by the total number of EDUs) to separate EDUs at the beginning (s < 0.25), in the first middle (0.25 \u2264 s < 0.5), in the second middle (0.5 \u2264 s < 0.75) or in the end (s >= 0.75).\nPosition of the head We add a boolean feature indicating if the head of the sentence is in the current EDU or outside.\nNumber/date/percent/money We also use 4 indicators of the presence of a date, a number, an amount of money and a percentage, features that have proven to be useful for discourse (Pitler et al., 2009). We build these features using simple regular expressions."}, {"heading": "6 Experiment settings", "text": "Data For the En-DT, we follow previous works in using the official test set of 38 documents. For the Es-DT, we report results on the test set A.17 For all the other corpora, we randomly choose 38 documents to make a test set, and either use the remaining documents as development set (NlDT and Eu-DT), or split them into a development set of 25 documents, the remaining being used as training set (En-DT, Es-DT, Pt-DT and De-DT).\nAll the results given are based on a gold segmentation of the documents.\nEach dataset is parsed using UDPipe,18 thus tokenizing, splitting into sentences and annotating each document based on the Universal Dependency scheme (Nivre et al., 2016).\nThe word features for the non-English datasets are translated using available bilingual Wiktionaries19 without disambiguation, the coverage of each dictionary is given in Table 2. We also look for a translation of the lemma (and of the stems for the languages for which a stemmer20 was available) as a backup strategy. When no translation is found, we keep the original token.\nThe word embeddings used were built on the EuroParl corpus (Levy et al., 2017). We keep only the 50 first dimensions of the vectors representing the words, our preliminary experiments suggesting no significant differences against keeping the whole 200 dimensions. Unknown words are represented by the average vector of all word vectors. For Basque, we had no access to these embeddings, we thus only report results using bilingual Wiktionaries.\n16Having more than three tokens in the head set is rare. 17We found similar performance on the other test set. 18http://ufal.mff.cuni.cz/udpipe 19https://en.wiktionary.org/wiki/User:\nMatthias_Buchmeier 20https://pypi.python.org/pypi/ snowballstemmer\nParameter tuning In our experiments we optimized on the development set the following parameters: the learning rate \u2208 {0.01, 0.02, 0.03}, the learning rate decay constant \u2208 {10\u22125, 10\u22126, 10\u22127, 0}, the number of iterations \u2208 [1 \u2212 20], and the size of the beam \u2208 {1, 2, 4, 8, 16, 32}. We fixed the number N of hidden layers to 2 and the size of the hidden layers H to 128 after experimenting on the En-DT (with N \u2208 {1, 2, 3} and H \u2208 {64, 128}).\nWe fixed the size of the vectors for each feature to 50 for word features,21 16 for POS, 6 for position, 4 for length, and 2 for other features.\nMetrics Following (Marcu, 2000b) and most subsequent work, output trees are evaluated against gold trees in terms of how similar they bracket the EDUs (Span), how often they agree about nuclei when predicting a true bracket (Nuclearity), and in terms of the relation label, i.e., the overlap between the shared brackets between predicted and gold trees (Relation).22 These scores are analogous to labeled and unlabeled syntactic parser evaluation metrics.\nBaseline Since we do not have state-of-the-art results for most of the languages, we provide results for a simple most frequent baseline (System MFS) that labels all nodes with the most frequent relation in the training or development set \u2013 that is NN-JOINT for De-DT and Es-DT, and NS-ELABORATION for the others \u2013, and build the structure by right-branching."}, {"heading": "7 Results", "text": "Monolingual experiments Monolingual experiments are aimed at evaluating performance for languages having a large annotated corpus (at least 100 documents). Our results are summarized in Table 3. Our parser is competitive with state-ofthe-art systems for English (first line in Table 3), with even better performance for unlabeled structure (85.04%) and structure labeled with nuclearity (72.29%). These results show that using all the words in the units (Ji and Eisenstein, 2014; Li et al., 2014), is not as useful as using more contextual information, that is taking more DUs into account (left and right children of the CDUs in the stack). However, the slight drop for Relation shows that\n21When using embeddings, the final vector is of size 350. 22We use the evaluation script provided at https://\ngithub.com/jiyfeng/DPLP.\nwe probably miss some lexical information, or that we need to choose a more effective combination scheme than concatenation. We plan to use bi-LSTM encoders (Hochreiter and Schmidhuber, 1997) to construct fixed-length representations of EDUs.\nFor the other languages, performance are still high for unlabeled structure, but far lower for labeled structure except for Spanish. For this language, the quite high performance obtained were unexpected, since the corpus is far much smaller than the Portuguese one. One possible explanation is that the Portuguese corpus is in fact a mix of different corpora, with varied domains, and possibly changes in annotation choices. On the other hand, the low results for German show the sparsity issue since it is the language for which we have the fewest annotations (\u201c#CDU\u201d, see Table 1).\nCross-lingual experiments When only relying on data from different languages (\u201cCross\u201d in Table 3), we observe a large drop in performance compared to monolingual systems. The sourceonly discourse parsers still have fairly high performance for unlabeled structure (around 70% or higher), the scores being especially low for relation identification. This could indicate that our representation does not generalize well. But it also comes from differences among the corpora. For example, only the En-DT and the Pt-DT use the relation ATTRIBUTION. This leads to a large drop in performance associated with this relation, when one of these corpora is not in the training data, especially for the source-only system for the En-DT (from 93% in F1 to 30%). On the other hand, on the En-DT, we observe improvement for other relations either largely represented in all the corpora (e.g. JOINT +3%), or under-represented in the EnDT (e.g. CONDITION +3%).\nWhen combining corpora for source and target languages (\u201c+ dev.\u201d in Table 3), we obtain our best performing system for English, with all scores improved compared to our best monolingual system (+0.8 for Nuclearity and +1.3 for Relation). Otherwise scores are similar to the monolingual case.\nFinally, for languages without training set (NlDT and Eu-DT), this strategy allows us to build parsers outperforming our simple baseline (MFS) by around 11\u221213% for Span, 8\u221215% for Nuclearity and 6\u221211% for Relation. Having at least some annotated data to make a development set allows improvements against only using corpora in other\naScores reported from (Li et al., 2014), and DPLP (Ji and Eisenstein, 2014). bFor Brazilian Portuguese, inter-annotator agreement scores are only available for the CST-news corpus ; For Spanish, only\nprecision scores are reported ; For Basque, the scores reported are different (Iruskieta et al., 2015).\nlanguages (around +3% for the Nl-DT and the EuDT for Relation). On the other hand, we probably overfit our development data for the Eu-DT, since better results were obtained for unlabeled structure (+2%) and structure with nuclearity (+2.5%) using only data in other languages.\nWord embeddings Using word embeddings (\u201c+emb\u201d in Table 3) for monolingual systems often leads to an important drop in performance, especially for Relation (from \u22121.1 to \u22124.2%). This demonstrates that these embeddings do not provide the large range of information needed for relation identification, a task inherently semantic. We believe however that the results are not too low to prevent for interesting applications. It is noteworthy that the English parser with embeddings is still better than the systems proposed in (Hernault et al., 2010; Joty et al., 2013).\nFor cross-lingual experiments, the bilingual dictionaries perform generally better than embeddings (except for Pt-DT and De-DT for sourceonly systems), demonstrating again that we need representations more tailored to the task to leverage all relevant lexical information."}, {"heading": "8 Conclusion", "text": "We introduced a new discourse parser that obtains state-of-the-art performance for English. We harmonized discourse treebanks for several languages, enabling us to present results for five other languages for which available corpora are smaller, including the first cross-lingual discourse parsing\nresults in the literature."}, {"heading": "Acknowledgements", "text": "We thank the three anonymous reviewers for their comments. Chloe\u0301 Braud and Anders S\u00f8gaard were funded by the ERC Starting Grant LOWLANDS No. 313695."}], "references": [{"title": "Mapping different rhetorical relation annotations: A proposal", "author": ["Benamara", "Taboada2015] Farah Benamara", "Maite Taboada"], "venue": "In Proceedings of Starsem", "citeRegEx": "Benamara et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Benamara et al\\.", "year": 2015}, {"title": "Better document-level sentiment analysis from RST discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Bhatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Braud", "Denis2015] Chlo\u00e9 Braud", "Pascal Denis"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Braud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braud et al\\.", "year": 2015}, {"title": "Finding the write stuff: automatic identification of discourse structure in student essays", "author": ["Daniel Marcu", "Kevin Knight"], "venue": "IEEE Intelligent Systems: Special Issue on Advances in Natural Language Processing,", "citeRegEx": "Burstein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Burstein et al\\.", "year": 2003}, {"title": "CSTNews - a discourse", "author": ["Erick G. Maziero", "Mara Luca Castro Jorge", "Eloize R.M. Seno", "Ariani Di Felippo", "Lucia Helena Machado Rino", "Maria das Gracas Volpe Nunes", "Thiago A.S. Pardo"], "venue": null, "citeRegEx": "Cardoso et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cardoso et al\\.", "year": 2011}, {"title": "Discourse tagging reference manual", "author": ["Carlson", "Marcu2001] Lynn Carlson", "Daniel Marcu"], "venue": "Technical report,", "citeRegEx": "Carlson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "Building a discoursetagged corpus in the framework of Rhetorical Structure Theory", "author": ["Carlson et al.2001] Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski"], "venue": "In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue", "citeRegEx": "Carlson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "A fast and accurate dependency parser using neural networks. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Neural greedy constituent parsing with dynamic oracles", "author": ["Coavoux", "Crabb\u00e92016] Maximin Coavoux", "Benoit Crabb\u00e9"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Coavoux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Coavoux et al\\.", "year": 2016}, {"title": "Summ-it: Um corpus anotado com informa\u00e7oes discursivas visandoa sumariza\u00e7ao autom\u00e1tica", "author": ["Thiago I Carbonel", "Juliana Thiesen Fuchs", "Jorge C\u00e9sar Coelho", "L\u00facia Rino", "Renata Vieira"], "venue": "Proceedings of TIL", "citeRegEx": "Collovini et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Collovini et al\\.", "year": 2007}, {"title": "DiSeg: Un segmentador discursivo autom\u00e1tico para el espa\u00f1ol", "author": ["Eric SanJuan", "Juan-Manuel Torres-Moreno", "Marina Lloberas", "Irene Castell\u00f3n"], "venue": "Procesamiento del lenguaje natural,", "citeRegEx": "Cunha et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cunha et al\\.", "year": 2010}, {"title": "On the development of the RST Spanish Treebank", "author": ["Juan-Manuel Torres-Moreno", "Gerardo Sierra"], "venue": "In Proceedings of the Fifth Linguistic Annotation Workshop,", "citeRegEx": "Cunha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cunha et al\\.", "year": 2011}, {"title": "DiSeg 1.0: The first system for Spanish discourse segmentation", "author": ["Eric SanJuan", "Juan-Manuel Torres-Moreno", "Marina Lloberes", "Irene Castell\u00f3n"], "venue": "Expert Syst. Appl.,", "citeRegEx": "Cunha et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cunha et al\\.", "year": 2012}, {"title": "A noisy-channel model for document compression", "author": ["III Daum\u00e9", "III Marcu2009] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proceedings of ACL", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "Text-level discourse parsing with rich linguistic features", "author": ["Feng", "Hirst2012] Vanessa Wei Feng", "Graeme Hirst"], "venue": "In Proceedings of ACL", "citeRegEx": "Feng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2012}, {"title": "A linear-time bottom-up discourse parser with constraints and post-editing", "author": ["Feng", "Hirst2014] Vanessa Wei Feng", "Graeme Hirst"], "venue": "In Proceedings of ACL", "citeRegEx": "Feng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2014}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proceedings of ACLIJCNLP", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "HILDA: A discourse parser using support vector machine classification", "author": ["Helmut Prendinger", "David A. duVerle", "Mitsuru Ishizuka"], "venue": "Dialogue and Discourse,", "citeRegEx": "Hernault et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hernault et al\\.", "year": 2010}, {"title": "Evaluating multiple aspects of coherence in student essays", "author": ["Jill Burstein", "Daniel Marcu", "Claudia Gentile"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Higgins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2004}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The RST Basque Treebank: an online search interface to check rhetorical relations", "author": ["Mar\u0131\u0301a J. Aranzabe", "Arantza Diaz de Ilarraza", "Itziar Gonzalez-Dios", "Mikel Lersundi", "Oier Lopez de la Calle"], "venue": null, "citeRegEx": "Iruskieta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iruskieta et al\\.", "year": 2013}, {"title": "A qualitative comparison method for rhetorical structures: identifying different discourse structures in multilingual corpora", "author": ["Iria da Cunha", "Maite Taboada"], "venue": "Proceedings of LREC", "citeRegEx": "Iruskieta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iruskieta et al\\.", "year": 2015}, {"title": "Representation learning for text-level discourse parsing", "author": ["Ji", "Eisenstein2014] Yangfeng Ji", "Jacob Eisenstein"], "venue": "In Proceedings of ACL", "citeRegEx": "Ji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "A novel discriminative framework for sentence-level discourse analysis", "author": ["Joty et al.2012] Shafiq R. Joty", "Giuseppe Carenini", "Raymond T. Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Joty et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joty et al\\.", "year": 2012}, {"title": "Combining intra- and multi-sentential rhetorical parsing for document-level discourse analysis", "author": ["Joty et al.2013] Shafiq R. Joty", "Giuseppe Carenini", "Raymond T. Ng", "Yashar Mehdad"], "venue": "Proceedings of ACL", "citeRegEx": "Joty et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Joty et al\\.", "year": 2013}, {"title": "A strong baseline for learning cross-lingual word embeddings from sentence alignments", "author": ["Levy et al.2017] Omer Levy", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "In Proceedings of EACL", "citeRegEx": "Levy et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2017}, {"title": "Recursive deep models for discourse parsing", "author": ["Li et al.2014] Jiwei Li", "Rumeng Li", "Eduard H. Hovy"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Discourse indicators for content selection in summarization", "author": ["Louis et al.2010] Annie Louis", "Aravind Joshi", "Ani Nenkova"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Rhetorical Structure Theory: Toward a functional theory of text organization", "author": ["Mann", "Thompson1988] William C. Mann", "Sandra A. Thompson"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "From discourse structures to text summaries", "author": ["Daniel Marcu"], "venue": "In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,", "citeRegEx": "Marcu.,? \\Q1997\\E", "shortCiteRegEx": "Marcu.", "year": 1997}, {"title": "The rhetorical parsing of unrestricted texts: A surface-based approach", "author": ["Daniel Marcu"], "venue": "Computational Linguistics", "citeRegEx": "Marcu.,? \\Q2000\\E", "shortCiteRegEx": "Marcu.", "year": 2000}, {"title": "The Theory and Practice of Discourse Parsing and Summarization", "author": ["Daniel Marcu"], "venue": null, "citeRegEx": "Marcu.,? \\Q2000\\E", "shortCiteRegEx": "Marcu.", "year": 2000}, {"title": "DiZer 2.0-an adaptable on-line discourse parser", "author": ["Thiago A.S. Pardo", "Iria da Cunha", "Juan-Manuel Torres-Moreno", "Eric SanJuan"], "venue": "In Proceedings of 3rd RST Brazilian Meeting,", "citeRegEx": "Maziero et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maziero et al\\.", "year": 2011}, {"title": "Adaptation of discourse parsing models for Portuguese language", "author": ["Graeme Hirst", "Thiago A.S. Pardo"], "venue": "In Proceedings of the Brazilian Conference on Intelligent Systems (BRACIS)", "citeRegEx": "Maziero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maziero et al\\.", "year": 2015}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proceedings of EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Universal dependencies 1.3", "author": ["matsu", "Larraitz Uria", "Gertjan van Noord", "Viktor Varga", "Veronika Vincze", "Jing Xian Wang", "Jonathan North Washington", "Zden\u011bk \u017dabokrtsk\u00fd", "Daniel Zeman", "Hanzhi Zhu"], "venue": null, "citeRegEx": "matsu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "matsu et al\\.", "year": 2016}, {"title": "A constru\u00e7\u00e3o de um corpus de textos cient\u0131\u0301ficos em Portugu\u00eas do Brasil e sua marca\u00e7\u00e3o ret\u00f3rica", "author": ["Pardo", "Nunes2003] Thiago A.S. Pardo", "Maria das Gra\u00e7as Volpe Nunes"], "venue": null, "citeRegEx": "Pardo et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pardo et al\\.", "year": 2003}, {"title": "Rela\u00e7\u00f5es ret\u00f3ricas e seus marcadores superficiais: An\u00e1lise de um corpus de textos cient\u0131\u0301ficos em Portugu\u00eas do Brasil. Relat\u00f3rio T\u00e9cnico NILC", "author": ["Pardo", "Nunes2004] Thiago A.S. Pardo", "Maria das Gra\u00e7as Volpe Nunes"], "venue": null, "citeRegEx": "Pardo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pardo et al\\.", "year": 2004}, {"title": "On the development and evaluation of a Brazilian Portuguese discourse parser", "author": ["Pardo", "Nunes2008] Thiago A.S. Pardo", "Maria das Gra\u00e7as Volpe Nunes"], "venue": "Revista de Informa\u0301tica Teo\u0301rica e Aplicada,", "citeRegEx": "Pardo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pardo et al\\.", "year": 2008}, {"title": "Rhetalho: Um corpus de refer\u0142ncia anotado retoricamente", "author": ["Pardo", "Seno2005] Thiago A.S. Pardo", "Eloize R.M. Seno"], "venue": "In Proceedings of Encontro de Corpora", "citeRegEx": "Pardo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pardo et al\\.", "year": 2005}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["Pitler et al.2009] Emily Pitler", "Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of ACL-IJCNLP", "citeRegEx": "Pitler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Polyak", "Juditsky1992] Boris T. Polyak", "Anatoli B. Juditsky"], "venue": "SIAM J. Control Optim.,", "citeRegEx": "Polyak et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Polyak et al\\.", "year": 1992}, {"title": "Multi-layer discourse annotation of a dutch text corpus", "author": ["Ildik Berzlnovich", "Nynke van der Vliet", "Gosse Bouma", "Markus Egg"], "venue": "Proceedings of LREC", "citeRegEx": "Redeker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Redeker et al\\.", "year": 2012}, {"title": "Vers une alg\u00e8bre des relations de discours", "author": ["Charlotte Roze"], "venue": "Ph.D. thesis, Universite\u0301 Paris-Diderot", "citeRegEx": "Roze.,? \\Q2013\\E", "shortCiteRegEx": "Roze.", "year": 2013}, {"title": "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns", "author": ["Rutherford", "Xue2014] Attapol Rutherford", "Nianwen Xue"], "venue": "In Proceedings of EACL", "citeRegEx": "Rutherford et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rutherford et al\\.", "year": 2014}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Sagae", "Lavie2005] Kenji Sagae", "Alon Lavie"], "venue": "In Proceedings of the Ninth International Workshop on Parsing Technology,", "citeRegEx": "Sagae et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2005}, {"title": "Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing", "author": ["Kenji Sagae"], "venue": "In Proceedings of IWPT", "citeRegEx": "Sagae.,? \\Q2009\\E", "shortCiteRegEx": "Sagae.", "year": 2009}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["Soricut", "Marcu2003] Radu Soricut", "Daniel Marcu"], "venue": "In Proceedings of NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2003}, {"title": "Discourse chunking and its application to sentence compression", "author": ["Sporleder", "Lapata2005] Caroline Sporleder", "Mirella Lapata"], "venue": "In Proceedings of HLT/EMNLP", "citeRegEx": "Sporleder et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sporleder et al\\.", "year": 2005}, {"title": "Potsdam commentary corpus 2.0: Annotation for discourse research", "author": ["Stede", "Neumann2014] Manfred Stede", "Arne Neumann"], "venue": "In Proceedings of LREC", "citeRegEx": "Stede et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stede et al\\.", "year": 2014}, {"title": "The potsdam commentary corpus", "author": ["Manfred Stede"], "venue": "In Proceedings of the ACL Workshop on Discourse Annotation", "citeRegEx": "Stede.,? \\Q2004\\E", "shortCiteRegEx": "Stede.", "year": 2004}, {"title": "An approach to discourse parsing using sangati and Rhetorical Structure Theory", "author": ["Subalalitha", "Ranjani Parthasarathi"], "venue": "In Proceedings of the Workshop on Machine Translation and Parsing in Indian Lan-", "citeRegEx": "Subalalitha et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Subalalitha et al\\.", "year": 2012}, {"title": "An effective discourse parser that uses rich linguistic information", "author": ["Subba", "Di Eugenio2009] Rajen Subba", "Barbara Di Eugenio"], "venue": "In Proceedings of ACL-HLT", "citeRegEx": "Subba et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Subba et al\\.", "year": 2009}, {"title": "Applications of rhetorical structure theory", "author": ["Taboada", "Mann2006] Maite Taboada", "William C. Mann"], "venue": "Discourse Studies,", "citeRegEx": "Taboada et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Taboada et al\\.", "year": 2006}, {"title": "Hybrid text summarization: Combining external relevance measures with structural analysis", "author": ["Martin Van den Berg", "Livia Polanyi", "Chris Culy"], "venue": "In Proceedings of the ACL Workshop Text Summarization", "citeRegEx": "Thione et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Thione et al\\.", "year": 2004}, {"title": "Building a discourse-annotated Dutch text corpus", "author": ["Ildik\u00f3 Berzlnovich", "Gosse Bouma", "Markus Egg", "Gisela Redeker"], "venue": null, "citeRegEx": "Vliet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vliet et al\\.", "year": 2011}, {"title": "A new ranking method for Chinese discourse tree building", "author": ["Wu et al.2016] Yunfang Wu", "Fuqiang Wan", "Yifeng Xu", "Xueqiang L\u00fc"], "venue": "Acta Scientiarum Naturalium Universitatis Pekinensis,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "This theory guided the annotation of the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English, from which several textlevel discourse parsers have been induced (Hernault et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 3, "context": "Such parsers have proven to be useful for various downstream applications (Daum\u00e9 III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015).", "startOffset": 74, "endOffset": 260}, {"referenceID": 18, "context": "Such parsers have proven to be useful for various downstream applications (Daum\u00e9 III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015).", "startOffset": 74, "endOffset": 260}, {"referenceID": 54, "context": "Such parsers have proven to be useful for various downstream applications (Daum\u00e9 III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015).", "startOffset": 74, "endOffset": 260}, {"referenceID": 27, "context": "Such parsers have proven to be useful for various downstream applications (Daum\u00e9 III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015).", "startOffset": 74, "endOffset": 260}, {"referenceID": 1, "context": "Such parsers have proven to be useful for various downstream applications (Daum\u00e9 III and Marcu, 2009; Burstein et al., 2003; Higgins et al., 2004; Thione et al., 2004; Sporleder and Lapata, 2005; Taboada and Mann, 2006; Louis et al., 2010; Bhatia et al., 2015).", "startOffset": 74, "endOffset": 260}, {"referenceID": 32, "context": "ever, most research experimenting with these languages has focused on rule-based systems (Pardo and Nunes, 2008; Maziero et al., 2011) or has been limited to intra-sentential relations (Maziero et al.", "startOffset": 89, "endOffset": 134}, {"referenceID": 33, "context": ", 2011) or has been limited to intra-sentential relations (Maziero et al., 2015).", "startOffset": 58, "endOffset": 80}, {"referenceID": 5, "context": "rules and heuristics (Marcu, 2000a; Carlson et al., 2001).", "startOffset": 21, "endOffset": 57}, {"referenceID": 32, "context": "For Brazilian Portuguese, for example, the first system, called DiZer (Pardo and Nunes, 2008; Maziero et al., 2011), was also rule-based, but there has been some work on using classification of intra-sentential relations (Maziero et al.", "startOffset": 70, "endOffset": 115}, {"referenceID": 33, "context": ", 2011), was also rule-based, but there has been some work on using classification of intra-sentential relations (Maziero et al., 2015).", "startOffset": 113, "endOffset": 135}, {"referenceID": 25, "context": "Li et al. (2014) used a recursive neural network that builds a representation for each clause based on the syntactic tree, and then apply two classifiers as in Hernault et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 17, "context": "(2014) used a recursive neural network that builds a representation for each clause based on the syntactic tree, and then apply two classifiers as in Hernault et al. (2010). This leads to the best performing system for unlabeled structure (85.", "startOffset": 150, "endOffset": 173}, {"referenceID": 29, "context": "Marcu (1997) proposed to simplify the representation to", "startOffset": 0, "endOffset": 13}, {"referenceID": 33, "context": "Brazilian Portuguese We merged all the corpora annotated for Brazilian Portuguese, as in (Maziero et al., 2015), to form the Pt-DT.", "startOffset": 89, "endOffset": 111}, {"referenceID": 4, "context": "The largest corpus is CST-News4 (Cardoso et al., 2011), it is composed of 140 documents from the news domain annotated with 31 relations.", "startOffset": 32, "endOffset": 54}, {"referenceID": 9, "context": "The other corpora are: Summ-it5 (Collovini et al., 2007) \u2013 50 texts from science articles", "startOffset": 32, "endOffset": 56}, {"referenceID": 50, "context": "08 (Stede, 2004; Stede and Neumann, 2014), from now on De-DT, contains newspaper commentaries annotated at several levels.", "startOffset": 3, "endOffset": 41}, {"referenceID": 21, "context": "67% for the identification of the CDU (Iruskieta et al., 2015), and 61.", "startOffset": 38, "endOffset": 62}, {"referenceID": 56, "context": "Other corpora To the best of our knowledge, the only two non English corpora not included are the one annotated for Tamil (Subalalitha and Parthasarathi, 2012) that we were unable to find, and the (intra-sentential) one developed for Chinese (Wu et al., 2016), for which we were unable to produce RST trees since annotation does not contain nuclearity indications.", "startOffset": 242, "endOffset": 259}, {"referenceID": 21, "context": "preliminary work exists on this question (Iruskieta et al., 2015).", "startOffset": 41, "endOffset": 65}, {"referenceID": 29, "context": "are the ones described in (Marcu, 1997) (see Section 3), and choosing right or left-branching leads to a similar interpretation.", "startOffset": 26, "endOffset": 39}, {"referenceID": 17, "context": "Discourse segmenters only exist for English (Hernault et al., 2010) (95, 0% in F1), Brazilian Portuguese (Pardo and Nunes, 2008) (56.", "startOffset": 44, "endOffset": 67}, {"referenceID": 55, "context": "coarser in the corpora built after the En-DT (Vliet et al., 2011).", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "translating the words used as features into a single language (English) \u2013, or through crosslingual word embeddings as proposed in (Guo et al., 2015) for dependency parsing.", "startOffset": 130, "endOffset": 148}, {"referenceID": 25, "context": "tations presented in (Levy et al., 2017) that allow multi-source learning and have proven useful for POS tagging but also more semantic-oriented tasks, such as dependency parsing and document classification.", "startOffset": 21, "endOffset": 40}, {"referenceID": 46, "context": "If the stack contains CDUs, we use the nuclearity principle to choose the head EDU, converting multi-nuclear relations into nucleus-satellite ones as done since (Sagae, 2009).", "startOffset": 161, "endOffset": 174}, {"referenceID": 40, "context": "Lexical features We use the first three words and the last word along with their POS, features that have proven useful for discourse (Pitler et al., 2009), and the words in the head set (Sagae, 2009)", "startOffset": 133, "endOffset": 154}, {"referenceID": 46, "context": ", 2009), and the words in the head set (Sagae, 2009)", "startOffset": 39, "endOffset": 52}, {"referenceID": 26, "context": "Note that we do not use all the words in the EDUs as features, contrary to (Li et al., 2014; Ji and Eisenstein, 2014).", "startOffset": 75, "endOffset": 117}, {"referenceID": 40, "context": "Number/date/percent/money We also use 4 indicators of the presence of a date, a number, an amount of money and a percentage, features that have proven to be useful for discourse (Pitler et al., 2009).", "startOffset": 178, "endOffset": 199}, {"referenceID": 25, "context": "The word embeddings used were built on the EuroParl corpus (Levy et al., 2017).", "startOffset": 59, "endOffset": 78}, {"referenceID": 26, "context": "These results show that using all the words in the units (Ji and Eisenstein, 2014; Li et al., 2014), is not as useful as using more contextual information, that is taking more DUs into account (left and right children of the CDUs in the stack).", "startOffset": 57, "endOffset": 99}, {"referenceID": 26, "context": "Scores reported from (Li et al., 2014), and DPLP (Ji and Eisenstein, 2014).", "startOffset": 21, "endOffset": 38}, {"referenceID": 21, "context": "For Brazilian Portuguese, inter-annotator agreement scores are only available for the CST-news corpus ; For Spanish, only precision scores are reported ; For Basque, the scores reported are different (Iruskieta et al., 2015).", "startOffset": 200, "endOffset": 224}, {"referenceID": 17, "context": "It is noteworthy that the English parser with embeddings is still better than the systems proposed in (Hernault et al., 2010; Joty et al., 2013).", "startOffset": 102, "endOffset": 144}, {"referenceID": 24, "context": "It is noteworthy that the English parser with embeddings is still better than the systems proposed in (Hernault et al., 2010; Joty et al., 2013).", "startOffset": 102, "endOffset": 144}], "year": 2017, "abstractText": "Discourse parsing is an integral part of understanding information flow and argumentative structure in documents. Most previous research has focused on inducing and evaluating models from the English RST Discourse Treebank. However, discourse treebanks for other languages exist, including Spanish, German, Basque, Dutch and Brazilian Portuguese. The treebanks share the same underlying linguistic theory, but differ slightly in the way documents are annotated. In this paper, we present (a) a new discourse parser which is simpler, yet competitive (significantly better on 2/3 metrics) to state of the art for English, (b) a harmonization of discourse treebanks across languages, enabling us to present (c) what to the best of our knowledge are the first experiments on crosslingual discourse parsing.", "creator": "LaTeX with hyperref package"}}}