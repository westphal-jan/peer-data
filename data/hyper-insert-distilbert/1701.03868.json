{"id": "1701.03868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Minimally Naturalistic Artificial Intelligence", "abstract": "the rapid advancement trend of machine - learning management techniques gradually has re - energized research ideas into demonstrating general functional artificial intelligence. while the idea framework of domain - agnostic meta - learning is appealing, this historically emerging field must then come fresh to terms with its relationship directly to human cognition and the statistics and structure of the tasks this humans perform. the crucial position critique of this article is that as only by aligning our rational agents'abilities and environments with those descriptions of humans do here we stand a chance shot at essentially developing general procedural artificial intelligence ( and gai ). considering a broad reading of below the critically famous'hence no free learning lunch'chess theorem project is that there is no universally intrinsic optimal inductive bias _ or, equivalently, incomplete bias - free learning is impossible. this follows from regarding the purported fact that there are sometimes an comparatively infinite number of sufficient ways to individually extrapolate this data, even any sort of which might be the minimum one used frequently by the desired data generating environment ; an inductive bias user prefers some of these extrapolations to others, learning which lowers behavioral performance in environments using these adversarial design extrapolations. we alternately may posit that the optimal gai is the one that maximally wholly exploits the statistics and of its simplest environment partly to create likewise its alternative inductive filter bias ; accepting the interesting fact continues that this singular agent is highly guaranteed \" to apparently be extremely and sub - optimal fit for utilizing some alternative environments. | this trade - room off appears perfectly benign when sufficiently thinking about overlooking the environment as if being the physical universe, } as performance judged on any fictive universe is obviously also irrelevant. \u2026 but, we should expect a distinctly sharper inductive filtering bias if we therefore further constrain our environment. indeed,... we implicitly cannot do - so by defining corresponding gai strategies in terms deprived of automatically accomplishing constraints that so humans'consider possibly useful. } one obviously common version of this claim is need the for'common - sense constrained reasoning ', which evidently implicitly appeals to the analogous statistics of understanding physical infinite universe abilities as similarly perceived exactly by interacting humans.", "histories": [["v1", "Sat, 14 Jan 2017 01:57:31 GMT  (8kb)", "http://arxiv.org/abs/1701.03868v1", "Accepted into the NIPS 2016 Workshop on Machine Intelligence (M.A.I.N.)"]], "COMMENTS": "Accepted into the NIPS 2016 Workshop on Machine Intelligence (M.A.I.N.)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["steven stenberg hansen"], "accepted": false, "id": "1701.03868"}, "pdf": {"name": "1701.03868.pdf", "metadata": {"source": "CRF", "title": "Minimally Naturalistic Artificial Intelligence", "authors": ["Steven Hansen"], "emails": ["sshansen@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n03 86\n8v 1\n[ cs\n.A I]\n1 4\nJa n\n20 17\nMinimally Naturalistic Artificial Intelligence\nSteven Hansen Department of Psychology\nStanfored University Stanford, CA 94303\nsshansen@stanford.edu"}, {"heading": "1 Introduction", "text": "The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents\u2019 abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI).\nA broad reading of the famous \u201cNo Free Lunch\u201d theorem[1] is that there is no universally optimal inductive bias or, equivalently, bias-free learning is impossible. This follows from the fact that there are an infinite number of ways to extrapolate data, any of which might be the one used by the data generating environment; an inductive bias prefers some of these extrapolations to others, which lowers performance in environments using these adversarial extrapolations. We may posit that the optimal GAI is the one that maximally exploits the statistics of its environment to create its inductive bias; accepting the fact that this agent is guaranteed to be extremely sub-optimal for some alternative environments. This trade-off appears benign when thinking about the environment as being the physical universe, as performance on any fictive universe is obviously irrelevant. But, we should expect a sharper inductive bias if we further constrain our environment. Indeed, we implicitly do so by defining GAI in terms of accomplishing that humans consider useful. One common version of this is need the for \u201ccommon-sense reasoning\u201d, which implicitly appeals to the statistics of physical universe as perceived by humans [2]."}, {"heading": "2 The CommAI Environment", "text": "The CommAI environment[3] is both too broad and too narrow. It is too broad in that the tasks need not bear in relation to those encountered by humans, and are untethered to the statistics that lie therein. For example, imagine that a CommAI agent is trying to follow written driving direction to reach allocation in a simulated car. Most of the time the teacher gives standard instructions akin to those of a human. But sometimes, directions are given under the assumption that the world is governed by non-Euclidean geometry (e.g. \"drive until these parallel streets intersect\"). An optimal agent should spend time trying to infer the geometry governing the teacher\u2019s directions. But such an agent is woefully sub-optimal in the real world, as directions given under these beliefs are far too rare to warrant such exploratory behavior. The variation in the CommAI environment doesn\u2019t match the variation in the real environment.\nThis criticism appears to be addressed by appealing to meta-learning or learning-to-learn. For example, by first exploring the space of possible teachers, and then forming an exploration strategy based on the empirical variation. But this merely pushes the problem to a higher level of abstraction. We can imagine an environment whereby the type of variation in the set of teachers does not align with reality, such as teachers\u2019 whose geometric believes change by the day of the week. An optimal agent would thus have to track the statistics of the teachers\u2019 beliefs conditioned by the day of the week in\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nThe CommAI environment also too narrow, in that it throws out naturalistic structure in the input and output representations in the pursuit of simplicity. While the bit stream passed between the the teacher and the agent can send image data, its one dimensional nature means stripping it of its two-dimensional structure. While this could be recovered by the agent, such a problem is nontrivial and would likely slow progress on its ultimate task, which is problematic since one of the environment\u2019s primary goals is to strip away extraneous preprocessing in order to focus efforts on general learning principles. The authors attempt to address this criticism by the suggested use of object detection systems to convert image data into a format with a similarity structure akin to linguistic data, but this ignores the fact that much of the recent progress in machine learning has been due to systems that adapt their representations to the ultimate objective function, foregoing the modularization necessitated by an object detection system [4].\nNone of this is to say that the CommaAI environment won\u2019t be a useful tool in the pursuit of general artificial intelligence. Indeed, the high barrier to entry imposed by vision-based environments is probably the reason why more work isn\u2019t done on higher level tasks like instruction taking. Separating these projects may accelerate progress in both areas, as the reasons behind unsuccessful modeling attempts can be more readily diagnosed. However, we must be cognizant that we\u2019ve made this split for pragmatic reasons rather than philosophical ones."}, {"heading": "3 The Case of Reward Inference", "text": "A concrete case where generic and minimally naturalistic approaches to artificial intelligence diverge is the problem of goal inference. While reward is in some sense the simplest possible feedback, there are many cases where even a reward signal is hard to come by[5]. Indeed, social learning appears to be one of these cases[6]. Instead, we must frame our agent\u2019s motivations in terms of a latent intention that the teacher has in mind, but can only communicate indirectly (e.g. sending reward, verbal instructions). The most generic way to handle such a situation is by taking the reward signal at face value and treat other indications of latent intention as additional state information. This was the approach taken in the work of Branavan et al[7] (though they did not have to handle the issue of imprecise rewards). In trying to solve a video-game (Civilization 2) via model-based reinforcement learning, they utilized the written instructions in the game manual to augment the state-space and focus the search.\nA more psychologically informed approach can be seen in the work of Ullman et al, where the task is to explain the behavior of other agents in the environment. One could take the generic approach and try and find a mapping between observations of the other agents to the correct explanation the behavior of other agents was understood by inverse planning on a model of each agent\u2019s behavior. Not only did this approach predict human performance on this task, related work has shown that this approach is also a computationally viable way to handle imprecise reward signals[6].\nHaving an agent model its teacher is very difficult, as the agent is likely to only observe the teacher is circumscribed, unrepresentative situations (e.g. when the teacher is about to teach something). However, if the teacher is sufficiently similar to the agent, the agent can use a model of its own behavior as a stand-in for a teacher model. One of the overarching assumptions of GAI is that we want to be the ones giving instructions; we want to be the teacher. The growing body of work on human learning suggests that in order to do so tractably, we should make a deliberate effort to make the inductive biases of our agent as close to ours as possible by training in naturalistic situations. The extent to which this constraint is weighed against others, such as the computational resources required for simulating an environment, is an open question, but some minimal commitment to naturalism is necessary."}], "references": [{"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "IEEE transactions on evolutionary computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "The St. Thomas common sense symposium: designing architectures for human-level intelligence", "author": ["M.L. Minsky", "P. Singh", "A. Sloman"], "venue": "AI Magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "A roadmap towards machine intelligence. arXiv preprint arXiv:1511.08130", "author": ["T. Mikolov", "A. Joulin", "M. Baroni"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "S. Petersen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R. Loftin", "B. Peng", "J. MacGlashan", "M.L. Littman", "M.E. Taylor", "J. Huang", "D.L. Roberts"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Help or hinder: Bayesian models of social goal inference. In Advances in neural information processing systems (pp. 1874-1882)", "author": ["T. Ullman", "C. Baker", "O. Macindoe", "O. Evans", "N. Goodman", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning to win by reading manuals in a Monte- Carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 (pp. 268-277)", "author": ["S.R.K. Branavan", "D. Silver", "Barzilay", "June"], "venue": "Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A broad reading of the famous \u201cNo Free Lunch\u201d theorem[1] is that there is no universally optimal inductive bias or, equivalently, bias-free learning is impossible.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "One common version of this is need the for \u201ccommon-sense reasoning\u201d, which implicitly appeals to the statistics of physical universe as perceived by humans [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "The CommAI environment[3] is both too broad and too narrow.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "The authors attempt to address this criticism by the suggested use of object detection systems to convert image data into a format with a similarity structure akin to linguistic data, but this ignores the fact that much of the recent progress in machine learning has been due to systems that adapt their representations to the ultimate objective function, foregoing the modularization necessitated by an object detection system [4].", "startOffset": 428, "endOffset": 431}, {"referenceID": 4, "context": "While reward is in some sense the simplest possible feedback, there are many cases where even a reward signal is hard to come by[5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "Indeed, social learning appears to be one of these cases[6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "This was the approach taken in the work of Branavan et al[7] (though they did not have to handle the issue of imprecise rewards).", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "Not only did this approach predict human performance on this task, related work has shown that this approach is also a computationally viable way to handle imprecise reward signals[6].", "startOffset": 180, "endOffset": 183}], "year": 2017, "abstractText": "The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents\u2019 abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI).", "creator": "LaTeX with hyperref package"}}}