{"id": "1701.04079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Agent-Agnostic Human-in-the-Loop Reinforcement Learning", "abstract": "providing automated reinforcement learning agents with expert advice can dramatically improve various managerial aspects of embodied learning. comprehensive prior process work has developed teaching style protocols that enable external agents to learn efficiently in complex environments ; many of these validation methods tailor the teacher's key guidance to agents interfering with acquiring a remote particular external representation class or inherent underlying learning retrieval scheme, further offering effective but specialized critical teaching procedures. in this work, we explore protocol rehearsal programs, developing an agent - agnostic schema for human - in - operation the - loop reinforcement practice learning. our ambitious goal is to extensively incorporate the broadly beneficial coping properties outlined of a experienced human teacher into communicating reinforcement learning without making strong assumptions about the inner biological workings of resisting the agent. we already show architects how to represent simultaneously existing approaches such as dynamic action pruning, enhance reward rewards shaping, and rigorous training results in simulation as special emerging cases aspects of our implementing schema and conduct preliminary experiments using on multiple simple domains.", "histories": [["v1", "Sun, 15 Jan 2017 17:14:40 GMT  (2091kb,D)", "http://arxiv.org/abs/1701.04079v1", "Presented at the NIPS Workshop on the Future of Interactive Learning Machines, 2016"]], "COMMENTS": "Presented at the NIPS Workshop on the Future of Interactive Learning Machines, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david abel", "john salvatier", "reas stuhlm\\\"uller", "owain evans"], "accepted": false, "id": "1701.04079"}, "pdf": {"name": "1701.04079.pdf", "metadata": {"source": "CRF", "title": "Agent-Agnostic Human-in-the-Loop Reinforcement Learning", "authors": ["David Abel", "John Salvatier"], "emails": ["david_abel@brown.edu", "jsalvatier@gmail.com", "andreas@stuhlmueller.org", "owaine@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "A central goal of Reinforcement Learning (RL) is to design agents that learn in a fully autonomous way. An engineer designs a reward function, input/output channels, and a learning algorithm. Then, apart from debugging, the engineer need not intervene during the actual learning process. Yet fully autonomous learning is often infeasible due to the complexity of real-world problems, the difficulty of specifying reward functions, and the presence of potentially dangerous outcomes that constrain exploration.\nConsider a robot learning to perform household chores. Human engineers create a curriculum, moving the agent between simulation, practice environments, and real house environments. Over time, they may tweak reward functions, heuristics, sensors, and state or action representations. They may intervene directly in real-world training to prevent the robot damaging itself, destroying valuable goods, or harming people it interacts with.\nIn this example, humans do not just design the learning agent: they are also in the loop of the agent\u2019s learning process, as is typical for many learning systems. Self-driving cars learn with humans ready to intervene in dangerous situations. Facebook\u2019s algorithm for recommending trending news stories has humans filtering out inappropriate content [1]. In both examples, the agent\u2019s environment is complex, non-stationary, and there are a wide range of damaging outcomes (like a traffic accident). As RL is applied to increasingly complex real-world problems, such interactive guidance will be critical to the success of these systems.\nPresented at the 2016 NIPS Future of Interactive Learning Machines Workshop, Barcelona, Spain.\nar X\niv :1\n70 1.\n04 07\n9v 1\n[ cs\n.L G\n] 1\n5 Ja\nPrior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10]. Often, the human\u2019s role is to pass along knowledge about relevant quantities of the RL problem, like Q-values, action optimality, or the true reward for a particular state-action pair. This way, the person can bias exploration, prevent catastrophic outcomes, and accelerate learning.\nMost existing work develops agent-specific protocols for human interaction. That is, protocols for human interaction or advice that are designed for a specific RL algorithm (such as Q-learning). For instance, Griffith et al. [17] investigate the power of policy advice for a Bayesian Q-Learner. Other works assume that the states of the MDP take a particular representation, or that the action space is discrete or finite. Making explicit assumptions about the agent\u2019s learning process can enable more powerful teaching protocols that leverage insights about the learning algorithm or representation."}, {"heading": "1.1 Our contribution: agent-agnostic guidance of RL algorithms", "text": "Our goal is to develop a framework for human-agent interaction that is (a) agent-agnostic and (b) can capture a wide range of ways a human can help an RL agent. Such a setting is informative of the structure of general teaching paradigms, the relationship and interplay of pre-existing teaching methods, and suggestive of new teaching methodologies, which we discuss in Section 6. Additionally, approaching human-in-the-loop RL from a maximally general standpoint can help illustrate the relationship between the requisite power of a teacher and the teacher\u2019s effectiveness on learning. For instance, we demonstrate sufficient conditions on a teacher\u2019s knowledge about an environment that enable effective1 action pruning of an arbitrary agent. Results of this form can again be informative to the general structure of teaching RL agents.\nWe make two simplifying assumptions. First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40]. Second, we note that conducting experiments with an actual human in the loop creates a huge amount of work for a human, and can slow down training to an unacceptable degree. For this reason, we focus on programatic instantiations of humans-in-the-loop; a person informed about the task (MDP) in question will write a program to facilitate various teaching protocols.\nThere are obvious disadvantages to agent-agnostic protocols. The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21]. Likewise, the human cannot provide optimally informative advice to the agent as they don\u2019t know the agent\u2019s prior knowledge, exploration technique, representation, or learning method.\nConversely, agent-specific protocols may perform well for one type of algorithm or environment, but poorly on others. In many cases, without further hand-engineering, agent-specific protocols can\u2019t be adapted to a variety of agent-types. When researchers tackle challenging RL problems, they tend to explore a large space of algorithms with important structural differences: some are model-based vs. model-free, some approximate the optimal policy, others a value function, and so on. It takes substantial effort to adapt an advice protocol to each such algorithm. Moreover, as advice protocols and learning algorithms become more complex, greater modularity will help limit design complexity.\nIn our framework, the interaction between the person guiding the learning process, the agent, and the environment is formalized as a protocol program. This program controls the channels between the agent and the environment based on human input, pictured in Figure 1. This gives the teacher extensive control over the agent: in an extreme case, the agent can be prevented from interacting with the real environment entirely and only interact with a simulation. At the same time, we require that the human only interact with the agent during learning through the protocol program\u2014both agent and environment are a black box to the human.\n1By \u201ceffective\u201c we mean: pruning bad actions while never pruning an optimal action. See Remark 3 (below)."}, {"heading": "2 Framework", "text": "Any system for RL with a human in the loop has to coordinate three components:\n1. The environment is an MDP and is specified by a tuple M = (S,A, T ,R, \u03b3), where S is the state space, A is the action space, T : S \u00d7 A \u00d7 S 7\u2192 [0, 1], denotes the transition function, a probability distribution on states given a state and action,R : S \u00d7A 7\u2192 R is the reward function, and \u03b3 is the discount factor.\n2. The agent is a (stateful, potentially stochastic) function L : S \u00d7R \u2192 A.\n3. The human can receive and send advice information of flexible type, say Xin and Xout, so, we will treat the human as a (stateful, potentially stochastic) function H : Xin \u2192 Xout. For example, Xin might contain the history of actions, states, and rewards so far, and a new proposed action a\u2032, and Xout might be an action as well, either equivalent to a\u2032 (if accepted) or different (if rejected). We assume that the human knows in general terms how their responses will be used and is making a good-faith effort to be helpful.\nThe interaction between the environment, the agent, and a human advisor sets up a mechanism design problem: how can we design an interface that orchestrates the interaction between these components such that the combined system maximizes the expected sum of \u03b3-discounted rewards from the environment? In other words, how can we write a protocol program P : S \u00d7R \u2192 A that can take the place of a given agent L, but that achieves higher rewards by making efficient use of information gained through sub-calls to L and H?\nBy formalizing existing and new techniques as programs, we facilitate understanding and comparison of these techniques within a common framework. By abstracting from particular agents and environments, we may better understand the mechanisms underlying effective teaching for Reinforcement Learning by developing portable and modular teaching methods."}, {"heading": "3 Capturing Existing Advice Schemes", "text": "Naturally, protocol programs cannot capture all advice protocols. Any protocol that depends on prior knowledge of the agent\u2019s learning algorithm, representation, priors, or hyperparameters is ruled out. Despite this constraint, the framework can capture a range of existing protocols where a human-in-the-loop guides an agent.\nFigure 1 shows that the human can manipulate the actions (A) sent to the environment, the agent\u2019s observed states (S), and observed rewards (R). This points to the following combinatorial set of protocol families in which the human manipulates one or more of these components to influence learning:\n{S,A,R, (S,A), (S,R), (A,R), (S,A,R)}\nThe first three elements of the set correspond to state manipulation, action pruning, and reward shaping protocol families.2 The remaining elements represent families of teaching schemes that modify multiple elements of the agent\u2019s learning; these protocols may introduce powerful interplay between the different components, which hope future work will explore.\nWe now demonstrate simple ways in which protocol programs instantiate typical methods for intervening in an agent\u2019s learning process.\nAlgorithm 1 Agent in control (standard) 1: procedure AGENTCONTROL(s, r) 2: return L(s, r) 3: end procedure\nAlgorithm 2 Human in control 1: procedure HUMANCONTROL(s, r) 2: return H(s, r) 3: end procedure\nAlgorithm 3 Action pruning 1: \u2206\u2190 H.\u2206 . To Prune: S \u00d7A 7\u2192 {0, 1} 2: procedure PRUNEACTIONS(s, r) 3: a = L(s, r) 4: while \u2206(s, a) do . If Needs Pruning 5: r = H[(s, a)] 6: a = L(s, r) 7: end while 8: return a 9: end procedure\nAlgorithm 4 Reward manipulation 1: procedure MANIPULATEREWARD(s, r) 2: r = H(s, r) 3: return L(s, r) 4: end procedure\nAlgorithm 5 Training in simulation 1: M\u2217 = (S,A, T \u2217,R\u2217, \u03b3) . Simulation 2: \u03b7 = [] . History: array of (S \u00d7R\u00d7A) 3: procedure TRAININSIMULATION(s, r) 4: s = s 5: r = r 6: while H(\u03b7) 6= \u201cagent is ready\u201d do 7: a = L(s, r) 8: append (s, r, a) to \u03b7 9: r \u223c R\u2217(s, a) 10: s \u223c T \u2217(s, a) 11: end while 12: return L(s, r) 13: end procedure\nFigure 2: Many schemes for human guidance of RL algorithms can be expressed as protocol programs. These programs have the same interface as the agent L, but can be safer or more efficient learners by making use of human advice H ."}, {"heading": "3.1 Reward shaping", "text": "Section 2 defined the reward functionR as part of the MDP M . However, while humans generally don\u2019t design the environment, we do design reward functions. Usually the reward function is handcoded prior to learning and must accurately assign reward values to any state the agent might reach. An alternative is to have a human generate the rewards interactively: the human observes the state and action and returns a scalar to the agent. This setup has been explored in work on TAMER [23]. A similar setup (with an agent-specific protocol) was applied to robotics by Daniel et al. [6]. It is straightforward to represent rewards that are generated interactively (or online) using protocol programs.\nWe now turn to other protocols in which the human manipulates rewards. These protocols assume a fixed reward functionR that is part of the MDP M ."}, {"heading": "3.1.1 Reward shaping and Q-value initialization", "text": "In Reward Shaping protocols, the human engineer changes the rewards given by some fixed reward function in order to influence an agent\u2019s learning. Ng et al. [32] introduced potential-based shaping, which shapes rewards without changing an MDP\u2019s optimal policy. In particular, each reward received by the environment is augmented by a shaping function:\nF (s, a, s\u2032) = \u03b3\u03c6(s\u2032)\u2212 \u03c6(s), (1)\nso the agent actually receives r = F (s, a, s\u2032) + R(s, a). Wiewiora et al. [48] showed potential shaping to be equivalent (for Q-learners) to a subset Q-value initialization under some assumptions.\n2State manipulation can correspond to abstraction or training in simulation\nFurther, Devlin and Kudenko [8] propose dynamic potential shaping functions that change over time. That is, the shaping function F also takes as two time parameters, t and t\u2032, such that:\nF (s, t, s\u2032, t\u2032) = \u03b3\u03c6(s\u2032, t\u2032)\u2212 \u03c6(s, t) (2)\nWhere t\u2032 > t. Their main result is that dynamic shaping functions of this form also guarantee optimal policy invariance. Similarly, Wiewiora et al. [48] extend potential shaping to potential-based advice functions, which identifies a similar class of shaping functions on (s, a) pairs.\nIn Section 4, we show that our Framework captures reward shaping, and consequently, a limited notion of Q-value initialization."}, {"heading": "3.2 Training in Simulation", "text": "It is common practice to train an agent in simulation and transfer it to the real world once it performs well enough. Algorithm 5 (Figure 2) shows how to represent the process of training in simulation as a protocol program. We let M represent the real-world decision problem and let M\u2217 be a simulator for M that is included in the protocol program. Initially the protocol program has the agent L interact with M\u2217 while the human observes the interaction. When the human decides the agent is ready, the protocol program has L interact with M instead."}, {"heading": "3.3 Action Pruning", "text": "Action pruning is a technique for dynamically removing actions from the MDP to reduce the branching factor of the search space. Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2]. In Section 5, we apply action-pruning to prevent catastrophic outcomes during exploration, a problem explored by Lipton et al. [27], Garcia and Fernandez [14, 15], Hans et al. [18], Moldovan and Abbeel [31].\nProtocol programs allow action pruning to be carried out interactively. Instead of having to decide which actions to prune prior to learning, the human can wait to observe the states that are actually encountered by the agent, which may be valuable in cases where the human has limited knowledge of the environment or the agent\u2019s learning ability. In Section 4, we exhibit an agent-agnostic protocol for interactively pruning actions that preserves the optimal policy while removing some bad actions.\nOur pruning protocol is illustrated in a gridworld with lava pits (Figure 3). The agent is represented by a gray circle, \u201cG\u201d is a goal state that provides reward +1, and the red cells are lava pits with reward \u2212200. All white cells provide reward 0.\nAt each time step, the human checks whether the agent moves into a lava pit. If it does not (as in moving DOWN from state 34), the agent continues as normal. If it does (as in moving RIGHT from state 33), the human bypasses sending any action to the true MDP (preventing movement right) and sends the agent a next state of 33. The agent doesn\u2019t actually fall in the lava but the human sends them a reward r \u2264 \u2212200. After this negative reward, the agent is less likely to try the action again. For the protocol program, see Algorithm 3 in Figure 2.\nNote that the agent receives no explicit signal that their attempted catastrophic action was blocked by the human. They observe a big negative reward and a self-loop but no information about whether the human or environment generated their observation."}, {"heading": "3.4 Manipulating state representation", "text": "The agent\u2019s state representation can have a significant influence on its learning. Suppose the states of MDP M consist of a number of features, defining a state vector s. The human engineer can specify a mapping \u03c6 such that the agent always receives \u03c6(s) = s\u0304 in place\nof this vector s. Such mappings are used to specify high-level features of state that are important for learning, or to dynamically ignore confusing features from the agent.\nThis transformation of the state vector is normally fixed before learning. A protocol program can allow the human to provide processed states or high-level features interactively. By the time the human stops providing features, the agent might have learned to generate them on its own (as in Learning with Privileged Information [45, 35]).\nOther methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13]. Using a state abstraction function, agents compress representations of their environments, enabling deeper planning and lower sample complexity. Any state aggregation function can be implemented by a protocol program, perhaps dynamically induced through interaction with a teacher."}, {"heading": "4 Theory", "text": "Here we illustrate some simple ways in which our proposed agent-agnostic interaction scheme captures other existing agent-agnostic protocols. The following results all concern Tabular MDPs, but are intended to offer intuition for high-dimensional or continuous environments as well."}, {"heading": "4.1 Reward Shaping", "text": "First we observe that protocol programs can precisely capture methods for shaping reward functions.\nRemark 1: For any reward shaping function F , including potential-based shaping, potential-based advice, and dynamic potential-based advice, there is a protocol that produces the same rewards.\nTo construct such a protocol for a given F , simply let the reward output by the protocol, r, take on the value F (s) + r at each time step. That is, in Algorithm 4, simply define H(s, r) = F (s) + r."}, {"heading": "4.2 Action Pruning", "text": "We now show that there is a simple class of protocol programs that carry out action pruning of a certain form.\nRemark 2: There is a protocol for pruning actions in the following sense: for any set of state action pairs sa \u2282 S \u00d7A, the protocol ensures that, for each pair (si, aj) \u2208 sa, action aj is never executed in the MDP in state si.\nThe protocol is as described in Section 3.3 and shown in Algorithm 3. The premise is this: in all cases where the agent executes an action that should be pruned, the protocol gives the agent low reward and forces the agent to self-loop.\nKnowing which actions to prune is itself a challenging problem. Often, it is natural to assume that the human guiding the learner knows something about the environment of interest (such as where high rewards or catastrophes lie), but may not know every detail of the problem. Thus, we consider a case in which the human has partial (but useful) knowledge about the problem of interest, represented as an approximate Q-function. The next remark shows there is a protocol based on approximate knowledge with two properties: (1) it never prunes an optimal action, (2) it limits the magnitude of the agent\u2019s worst mistake:\nRemark 3: Assuming the protocol designer has a \u03b2-optimal Q function:\n||Q\u2217(s, a)\u2212QH(s, a)||\u221e \u2264 \u03b2 (3)\nthere exists a protocol that never prunes an optimal action, but prunes all actions so that the agent\u2019s mistakes are never more than 4\u03b2 below optimal. That is, for all times t:\nV Lt(st) \u2265 V \u2217(st)\u2212 4\u03b2, (4)\nwhere Lt is the agent\u2019s policy after t timesteps.\nProof of Remark 3. The protocol designer has a \u03b2-approximate Q function, denoted QH , defined as above. Consider the state-specific action pruning function H(s):\nH(s) = { a \u2208 A | QH(s, a) \u2265 max\na\u2032 QH(s, a\n\u2032)\u2212 2\u03b2 }\n(5)\nThe protocol prunes all actions not in H(s) according to the self-loop method described above. This protocol induces a pruned Bellman Equation over available actions, H(s), in each state:\nVH(s) = max a\u2208H(s)\n( R(s, a) + \u03b3\n\u2211 s\u2032 T (s, a, s\u2032)VH(s\u2032)\n) (6)\nLet a\u2217 denote the true optimal action: a\u2217 = arg maxa\u2032 Q\u2217(s, a\u2032). To preserve the optimal policy, we need a\u2217 \u2208 H(s), for each state. Note that a\u2217 6\u2208 H(s) when:\nQH(s, a \u2217) < max\na\u2032 QH(s, a\n\u2032)\u2212 2\u03b2 (7)\nBut by definition of QH(s, a):\n|QH(s, a\u2217)\u2212max a QH(s, a)| \u2264 2\u03b2 (8)\nThus, a\u2217 \u2208 H(s) can never occur. Furthermore, observe that H(s) retains all actions a for which:\nQH(s, a) \u2265 max a\u2032 QH(s, a \u2032)\u2212 2\u03b2, (9)\nholds. Thus, in the worst case, the following two hold:\n1. The optimal action estimate is \u03b2 too low: QH(s, a\u2217) = Q\u2217(s, a\u2217)\u2212 \u03b2\n2. The action with the lowest value, abad, is \u03b2 too high: QH(s, abad) = Q\u2217(s, abad) + \u03b2\nFrom Equation 9, observe that the minimal Q\u2217(s, abad) such that abad \u2208 H(s) is:\nQ\u2217(s, abad) + \u03b2 \u2265 Q\u2217(s, a\u2217)\u2212 \u03b2 \u2212 2\u03b2 \u2234 Q\u2217(s, abad) \u2265 Q\u2217(s, a\u2217)\u2212 4\u03b2\nThus, this pruning protocol never prunes an optimal action, but prunes all actions worse then 4\u03b2 below a\u2217 in value. We conclude that the agent may never execute an action 4\u03b2 below optimal."}, {"heading": "5 Experiments", "text": "This section applies our action pruning protocols (Section 3.3 and Remarks 2 and 3 above) to concrete RL problems. In Experiment 1, action pruning is used to prevent the agent from trying catastrophic actions, i.e. to achieve safe exploration. In Experiment 2, action pruning is used to accelerate learning."}, {"heading": "5.1 Protocol for Preventing Catastrophes", "text": "Human-in-the-loop RL can help prevent disastrous outcomes that result from ignorance of the environment\u2019s dynamics or of the reward function. Our goal for this experiment is to prevent the agent from taking catastrophic actions. These are real world actions so costly that we want the agent to never take the action3. This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].\nSection 3.3 describes our protocol program for preventing catastrophes in finite MDPs using action pruning. There are two important elements of this program:\n1. When the agent tries a catastrophic action a in state s, the agent is blocked from executing the action in the real world, and the agent receives state and reward: (s, rbad), where rbad is an extreme negative reward.\n3We allow an RL agent to take sub-optimal actions while learning. Catastrophic actions are not allowed because their cost is orders of magnitude worse than non-catastrophic actions.\nFigure 4: Preventing Catastrophic Speeds\n2. This (s, a) is stored so that the protocol program can automate the human\u2019s intervention, which could allow the human to stop monitoring after all catastrophes have been stored.\nThis protocol prevents catastrophic actions while preserving the optimal policy and having only minimal side-effects on the agent\u2019s learning. We can extend this protocol to environments with high-dimensional state spaces. Element (1) above remains the same. But (2) must be modified: preventing future catastrophes requires generalization across catastrophic actions (as there will be infinitely many such actions). We discuss this setting in Appendix A."}, {"heading": "5.2 Experiment 1: Preventing Catastrophes in a Pong-like Game", "text": "Our protocol for preventing catastrophes is intended for use in a real-world environment. Here we provide a preliminary test of our protocol in a simple video game.\nOur protocol treats the RL agent as a black box. To this end, we applied our protocol to an opensource implementation of the state-of-the-art RL algorithm \u201cTrust Region Policy Optimization\u201d from Duan et al. [11]. The environment was Catcher, a simplified version of Pong with non-visual state representation. Since there are no catastrophic actions in Catcher, we modified the game to give a large negative reward when the paddle\u2019s speed exceeds a speed limit. We compare the performance of an agent who is assisted by the protocol (\u201cPruned\u201d) and so is blocked from the catastrophic actions4 to the performance of a normal RL agent (\u201cNot Pruned\u201d).\nFigure 4 shows the agent\u2019s mean performance (\u00b11SD over 16 trials) over the course of learning. We see that the agent with protocol support (\u201cPruned\u201d) performed much better overall. This is unsurprising, as it was blocked from ever doing a catastrophic action. The gap in mean performance is large early on but diminishes as the \u201cNot Pruned\u201d agent learns to avoid high speeds. By the end (i.e. after 400,000 actions), \u201cNot Pruned\u201d is close to \u201cPruned\u201d in mean performance but its total returns over the whole period are around 5 times worse. While the \u201cPruned\u201d agent observes incongruous state transitions due to being blocked by our protocol, Figure 4 suggests these observations do not have negative side effects on learning."}, {"heading": "5.3 Protocol for Accelerating Learning", "text": "We also conducted a simple experiment in the Taxi domain from Dietterich [9]. The Taxi problem is a more complex version of grid world: each problem instances consists of a taxi and some number of passengers. The agent directs the taxi to each passenger, picks the passenger up, and brings them to their destination and drops them off.\n4 We did not use an actual human in the loop. Instead the agent was blocked by a protocol program that checked whether each action would exceed the speed limit. This is essentially the protocol outlined in Appendix A but with the classifier trained offline to recognize catastrophes. Future work will test similar protocols using actual humans. (In this experiment a human can easily recognize catastrophic actions by reading the agent\u2019s speed directly from the game state.)\nWe use Taxi to evaluate the effect of our action pruning protocol for accelerating learning in discrete MDPs. There is a natural procedure for pruning suboptimal actions that dramatically reduces the size of the reachable state space: if the taxi is carrying a passenger but is not at the passenger\u2019s destination, we prune the dropoff action by returning the agent back to its current state with -0.01 reward. This prevents the agent from exploring a large portion of the state space, thus accelerating learning."}, {"heading": "5.4 Experiment 2: Accelerated Learning in Taxi", "text": "We evaluated Q-learning [47] and R-MAX [5] with and without action pruning in a simple 10\u00d7 10 instance with one passenger. The taxi starts at (1, 1), the passenger at (4, 3) with destination (2, 2). We ran standard Q-Learning with \u03b5-greedy exploration with \u03b5 = 0.2 and with R-MAX using a planning horizon of four. Results are displayed in Figure 5.\nOur results suggest that the action pruning protocol simplifies the problem for a Q-Learner and dramatically so for R-Max. In the allotted number of episodes, we see that pruning substantially improves the overall cumulative reward achieved; in the case of R-MAX, the agent is able to effectively solve the problem after a small number of episodes. Further, the results suggests that the agentagnostic method of pruning is effective without having any internal access to the agent\u2019s code."}, {"heading": "6 Conclusion", "text": "We presented an agent-agnostic method for giving guidance to Reinforcement Learning agents. Protocol programs written in this framework apply to any possible RL agent, so sophisticated schemes for human-agent interaction can be designed in a modular fashion without the need for adaptation to different RL algorithms. We presented some simple theoretical results that relate our method to existing schemes for interactive RL and illustrated the power of action pruning in two toy domains.\nA promising avenue for future work are dynamic state manipulation protocols, which can guide an agent\u2019s learning process by incrementally obscuring confusing features, highlighting relevant features, or simply reducing the dimensionality of the representation. Additionally, future work might investigate whether certain types of value initialization protocols can be captured by protocol programs, such as the optimistic initialization for arbitrary domains developed by Machado et al. [29]. Moreover, the full combinatoric space of learning protocols is suggestive of teaching paradigms that have yet to be explored. We hypothesize that there are powerful teaching methods that take advantage of the interplay between state manipulation, action pruning, and reward shaping. A further challenge is to extend the formalism to account for the interplay between multiple agents, in both competitive and cooperative settings.\nAdditionally, in our experiments, all protocols are explicitly programmed in advance. In the future, we\u2019d like to experiment with dynamic protocols with a human in the loop during the learning process.\nLastly, an alternate perspective on the framework is that of a centaur system: a joint Human-AI decision maker [41]. Under this view, the human trains and queries the AI dynamically in cases where the human needs help. In the future, we\u2019d like to establish and investigate formalisms relevant to the centaur view of the framework."}, {"heading": "Acknowledgments", "text": "This work was supported by Future of Life Institute grant 2015-144846 and by the Future of Humanity Institute (Oxford). We thank Shimon Whiteson, James MacGlashan, and D. Ellis Herskowitz for helpful conversations."}, {"heading": "A Protocol program for preventing catastrophes in high-dimensional state spaces", "text": "We provide an informal overview of the protocol program for avoiding catastrophes. We focus on the differences between the high-dimensional case and the finite case described in Section 5.1. In the finite case, pruned actions are stored in a table. When the human is satisfied that all catastrophic actions are in the table, the human\u2019s monitoring of the agent can be fully automated by the protocol program. The human may need to be in the loop until the agent has attempted each catastrophic action once \u2013 after that the human can \u201cretire\u201d.\nIn the infinite case, we replace this look-up table with a supervised classification algorithm. All visited state-actions are stored and labeled (\u201ccatastrophic\u201d or \u201cnot catastrophic\u201d) based on whether the human decides to block them. Once this labeled set is large enough to serve as a training set, the human trains the classifier and tests performance on held-out instances. If the classifier passes the test, the human can be replaced by the classifier. Otherwise the data-gathering process continues until the training set is large enough for the classifier to pass the test.\nIf the class of catastrophic actions is learnable by the classifier, this protocol prevents all catastrophes and has minimal side-effects on the agent\u2019s learning. However, there are limitations of the protocol that will be the subject of future work:\n\u2022 The human may need to monitor the agent for a very long time to provide sufficient training data. One possible remedy is for the human to augment the training set by adding synthetically generated states to it. For example, the human might add noise to genuine states without altering their labels. Alternatively, extra training data could be sampled from an accurate generative model.\n\u2022 Some catastrophic outcomes have a \u201clocal\u201d cause that is easy to block. If a car moves very slowly, then it can avoid hitting an obstacle by braking at the last second. But if a car has lots of momentum, it cannot be slowed down quickly enough. In such cases a human in-the-loop would have to recognize the danger some time before the actual catastrophe would take place.\n\u2022 To prevent catastrophes from ever taking place, the classifier needs to correctly identify every catastrophic action. This requires strong guarantees about the generalization performance of the classifier. Yet the distribution on state-action instances is non-stationary (violating the usual i.i.d assumption)."}], "references": [{"title": "Goal-based action priors", "author": ["David Abel", "David Ellis Hershkowitz", "Gabriel Barth-Maron", "Stephen Brawner", "Kevin O\u2019Farrell", "James MacGlashan", "Stefanie Tellex"], "venue": "In ICAPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Near optimal behavior via approximate state abstraction", "author": ["David Abel", "D Ellis Hershkowitz", "Michael L. Littman"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Interactive teaching strategies for agent training", "author": ["Ofra Amir", "Ece Kamar", "Andrey Kolobov", "Barbara Grosz"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Active reward learning", "author": ["Christian Daniel", "Malte Viering", "Jan Metz", "Oliver Kroemer", "Jan Peters"], "venue": "In Proceedings of Robotics Science & Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Model reduction techniques for computing approximately optimal solutions for markov decision processes", "author": ["Thomas Dean", "Robert Givan", "Sonia Leach"], "venue": "In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Dynamic potential-based reward shaping", "author": ["Sam Devlin", "Daniel Kudenko"], "venue": "Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Integrating guidance into relational reinforcement learning", "author": ["Kurt Driessens", "Sa\u0161o D\u017eeroski"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Approximate equivalence of Markov decision processes", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Methods for computing state similarity in markov decision processes", "author": ["Norman Ferns", "Pablo Samuel Castro", "Doina Precup", "Prakash Panangaden"], "venue": "Proceedings of the 22nd conference on Uncertainty in artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Safe reinforcement learning in high-risk tasks through policy improvement", "author": ["Javier Garcia", "Fernando Fernandez"], "venue": "IEEE SSCI 2011: Symposium Series on Computational Intelligence", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Safe exploration of state and action spaces in reinforcement learning", "author": ["Javier Garcia", "Fernando Fernandez"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A Comprehensive Survey on Safe Reinforcement Learning", "author": ["Javier Garcia", "Fernando Fernandez"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning", "author": ["Shane Griffith", "Kaushik Subramanian", "J Scholz"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Safe Exploration for Reinforcement Learning", "author": ["Alexander Hans", "Daniel Schneega\u00df", "Anton Maximilian Sch\u00e4fer", "Steffen Udluft"], "venue": "Proceedings of the 16th European Symposium on Artificial Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Reinforcement learning for mixed open-loop and closed-loop control", "author": ["Eric A Hansen", "Andrew G Barto", "Shlomo Zilberstein"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "State abstraction discovery from irrelevant state variables", "author": ["Nicholas K Jong", "Peter Stone"], "venue": "In IJCAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Reinforcement Learning Via Practice and Critique Advice", "author": ["Kshitij Judah", "Saikat Roy", "Alan Fern", "Thomas G Dietterich"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Interactively shaping agents via human reinforcement: The tamer framework", "author": ["W Bradley Knox", "Peter Stone"], "venue": "In Proceedings of the fifth international conference on Knowledge capture,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Augmenting reinforcement learning with human feedback", "author": ["W Bradley Knox", "Peter Stone"], "venue": "Proceedings of the ICML Workshop on New Developments in Imitation Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Beyond Rewards : Learning from Richer Supervision", "author": ["K.V.N. Pradyot"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Towards a unified theory of state abstraction for mdps", "author": ["Lihong Li", "Thomas J Walsh", "Michael L Littman"], "venue": "In ISAIM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Combating reinforcement learning\u2019s sisyphean curse with intrinsic fear", "author": ["Zachary C Lipton", "Jianfeng Gao", "Lihong Li", "Jianshu Chen", "Li Deng"], "venue": "arXiv preprint arXiv:1611.01211,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning something from nothing: Leveraging implicit human feedback strategies", "author": ["Robert Loftin", "Bei Peng", "James MacGlashan", "Michael L Littman", "Matthew E Taylor", "Jie Huang", "David L Roberts"], "venue": "In Robot and Human Interactive Communication,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Domain-Independent Optimistic Initialization for Reinforcement Learning", "author": ["Marlos C. Machado", "Sriram Srinivasan", "Michael Bowling"], "venue": "AAAI Workshop on Learning for General Competency in Video Games,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Creating advice-taking reinforcement learners", "author": ["Richard Maclin", "Jude W. Shavlik"], "venue": "Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Safe Exploration in Markov Decision Processes", "author": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "venue": "Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Adaptive aggregation for reinforcement learning in average reward Markov decision processes", "author": ["Ronald Ortner"], "venue": "Annals of Operations Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Alternating optimisation and quadrature for robust reinforcement learning", "author": ["Supratik Paul", "Kamil Ciosek", "Michael A Osborne", "Shimon Whiteson"], "venue": "arXiv preprint arXiv:1605.07496,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "On the Theory of Learnining with Privileged Information", "author": ["Dmitry Pechyony", "Vladimir Vapnik"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A need for speed: Adapting agent action speed to improve task learning from non-expert humans", "author": ["Bei Peng", "James MacGlashan", "Robert Loftin", "Michael L Littman", "David L Roberts", "Matthew E Taylor"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "What good are actions? accelerating learning using learned action priors", "author": ["Benjamin Rosman", "Subramanian Ramamoorthy"], "venue": "In Development and Learning and Epigenetic Robotics (ICDL),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Improving action selection in mdp\u2019s via knowledge transfer", "author": ["A.A. Sherstov", "P. Stone"], "venue": "In Proceedings of the 20th national conference on Artificial Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1998}, {"title": "Virtual humans as centaurs: Melding real and virtual", "author": ["William R Swartout"], "venue": "In International Conference on Virtual, Augmented and Mixed Reality,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance", "author": ["Andrea Lockerd Thomaz", "Cynthia Breazeal"], "venue": "Aaai,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Teaching on a budget: Agents advising agents in reinforcement learning", "author": ["Lisa Torrey", "Matthew Taylor"], "venue": "In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Help an agent out: Student/teacher learning in sequential decision tasks", "author": ["Lisa Torrey", "Matthew E. Taylor"], "venue": "Proceedings of the Adaptive and Learning Agents Workshop", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["Vladimir Vapnik", "Akshay Vashist"], "venue": "Neural Networks,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Blending Autonomous Exploration and Apprenticeship Learning", "author": ["Thomas J. Walsh", "Daniel Hewlett", "Clayton T Morrison"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Principled methods for advising reinforcement learning agents", "author": ["Eric Wiewiora", "Garrison Cottrell", "Charles Elkan"], "venue": "In ICML,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2003}], "referenceMentions": [{"referenceID": 30, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 22, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 26, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 34, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 40, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 45, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 19, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 15, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 23, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 41, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 21, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 44, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 42, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 28, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 8, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 15, "context": "[17] investigate the power of policy advice for a Bayesian Q-Learner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40].", "startOffset": 142, "endOffset": 154}, {"referenceID": 20, "context": "First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40].", "startOffset": 142, "endOffset": 154}, {"referenceID": 38, "context": "First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40].", "startOffset": 142, "endOffset": 154}, {"referenceID": 2, "context": "The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21].", "startOffset": 107, "endOffset": 110}, {"referenceID": 15, "context": "The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21].", "startOffset": 230, "endOffset": 238}, {"referenceID": 19, "context": "The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21].", "startOffset": 230, "endOffset": 238}, {"referenceID": 21, "context": "This setup has been explored in work on TAMER [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[32] introduced potential-based shaping, which shapes rewards without changing an MDP\u2019s optimal policy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[48] showed potential shaping to be equivalent (for Q-learners) to a subset Q-value initialization under some assumptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Further, Devlin and Kudenko [8] propose dynamic potential shaping functions that change over time.", "startOffset": 28, "endOffset": 31}, {"referenceID": 45, "context": "[48] extend potential shaping to potential-based advice functions, which identifies a similar class of shaping functions on (s, a) pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 17, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 36, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 0, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 25, "context": "[27], Garcia and Fernandez [14, 15], Hans et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[27], Garcia and Fernandez [14, 15], Hans et al.", "startOffset": 27, "endOffset": 35}, {"referenceID": 13, "context": "[27], Garcia and Fernandez [14, 15], Hans et al.", "startOffset": 27, "endOffset": 35}, {"referenceID": 16, "context": "[18], Moldovan and Abbeel [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[18], Moldovan and Abbeel [31].", "startOffset": 26, "endOffset": 30}, {"referenceID": 43, "context": "By the time the human stops providing features, the agent might have learned to generate them on its own (as in Learning with Privileged Information [45, 35]).", "startOffset": 149, "endOffset": 157}, {"referenceID": 33, "context": "By the time the human stops providing features, the agent might have learned to generate them on its own (as in Learning with Privileged Information [45, 35]).", "startOffset": 149, "endOffset": 157}, {"referenceID": 24, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 31, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 10, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 18, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 5, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 1, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 11, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 14, "context": "This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].", "startOffset": 76, "endOffset": 84}, {"referenceID": 29, "context": "This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].", "startOffset": 76, "endOffset": 84}, {"referenceID": 32, "context": "This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We also conducted a simple experiment in the Taxi domain from Dietterich [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "4 Experiment 2: Accelerated Learning in Taxi We evaluated Q-learning [47] and R-MAX [5] with and without action pruning in a simple 10\u00d7 10 instance with one passenger.", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Lastly, an alternate perspective on the framework is that of a centaur system: a joint Human-AI decision maker [41].", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher\u2019s guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.", "creator": "LaTeX with hyperref package"}}}