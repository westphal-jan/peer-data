{"id": "1701.04222", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2017", "title": "Achieving Privacy in the Adversarial Multi-Armed Bandit", "abstract": "essentially in this this particular paper, we improve the previously best known regret bound r to _ achieve $ \\ epsilon $ - [ differential relative privacy } in oblivious adversarial bandits from $ \\ mathcal { o } { ( t ^ { for 2 / and 3 } / \\ epsilon ) } $ to $ \\ ch mathcal { dd o } { ( \\ sqrt { t } \\ ff ln t / \\ epsilon ) } $. this trend is probably achieved exclusively by combining a topological laplace retrieval mechanism with exp3. visually we show that strictly though exp3 filter is essentially already theoretically differentially globally private, it leaks a linear amount rate of information in $ t $. here however, indeed we ourselves can improve again this inherent privacy algorithm by relying on examining its intrinsic finite exponential sampling mechanism inherently for securely selecting actions. ultimately this reasoning allows expects us immediately to reach $ \\ \u2032 mathcal { < o } { ( \\ sqrt { \\ pu ln t } ) } $ - dp, establishing with itself a regret of $ \\ _ mathcal { theta o } { ( t ^ { approximately 2 / 3 } ) } $ that mistakenly holds fairness against quite an adaptive external adversary, as an earlier improvement from the 2013 best known winner of $ \\ mathcal { epsilon o } { ( for t ^ { ( 3 / 4 } ) } $. this one is principally done by secretly using purely an earlier algorithm that run run exp3 in forming a distinct mini - byte batch binary loop. finally, we run experiments algorithm that never clearly helped demonstrate the approximate validity complexity of our practical theoretical pattern analysis.", "histories": [["v1", "Mon, 16 Jan 2017 10:04:05 GMT  (442kb,D)", "http://arxiv.org/abs/1701.04222v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CR", "authors": ["aristide charles yedia tossou", "christos dimitrakakis"], "accepted": true, "id": "1701.04222"}, "pdf": {"name": "1701.04222.pdf", "metadata": {"source": "CRF", "title": "Achieving Privacy in the Adversarial Multi-Armed Bandit", "authors": ["Aristide C. Y. Tossou", "Christos Dimitrakakis"], "emails": ["aristide@chalmers.se", "christos.dimitrakakis@gmail.com"], "sections": [{"heading": null, "text": "\u221a T lnT/ ). This is achieved\nby combining a Laplace Mechanism with EXP3. We show that though EXP3 is already differentially private, it leaks a linear amount of information in T . However, we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions. This allows us to reach O( \u221a lnT )-DP, with a regret of O(T 2/3) that holds against an adaptive adversary, an improvement from the best known of O(T 3/4). This is done by using an algorithm that run EXP3 in a mini-batch loop. Finally, we run experiments that clearly demonstrate the validity of our theoretical analysis."}, {"heading": "1 Introduction", "text": "We consider multi-armed bandit problems in the adversarial setting whereby an agent selects one from a number of alternatives (called arms) at each round and receives a gain that depends on its choice. The agent\u2019s goal is to maximize its total gain over time. There are two main settings for the bandit problem. In the stochastic one, the gains of each arm are generated i.i.d by some unknown probability law. In the adversarial setting, which is the focus of this paper, the gains are generated adversarially. We are interested in finding algorithms with a total gain over T rounds not much smaller than that of an oracle with additional knowledge about the problem. In both settings, algorithms that achieve the optimal (problemindependent) regret bound ofO( \u221a T ) are known (Auer, CesaBianchi, and Fischer 2002; Burnetas and Katehakis 1996; Pandey and Olston 2006; Thompson 1933; Auer et al. 2003; Auer 2002; Agrawal and Goyal 2012).\nThis problem is a model for many applications where there is a need for trading-off exploration and exploitation. This is so because, whenever we make a choice, we only observe the gain generated by that choice, and not the gains that we could have obtained otherwise. An example is clinical trials, where arms correspond to different treatments or tests, and the goal is to maximize the number of cured patients over time while being uncertain about the effects of treatments. Other problems, such as search engine advertisement and\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nmovie recommendations can be formalized similarly (Pandey and Olston 2006).\nPrivacy can be a serious issue in the bandit setting (c.f. (Jain, Kothari, and Thakurta 2012; Thakurta and Smith 2013; Mishra and Thakurta 2015; Zhao et al. 2014)). For example, in clinical trials, we may want to detect and publish results about the best drug without leaking sensitive information, such as the patient\u2019s health condition and genome. Differential privacy (Dwork 2006) formally bounds the amount of information that a third party can learn no matter their power or side information.\nDifferential privacy has been used before in the stochastic setting (Tossou and Dimitrakakis 2016; Mishra and Thakurta 2015; Jain, Kothari, and Thakurta 2012) where the authors obtain optimal algorithms up to logarithmic factors. In the adversarial setting, (Thakurta and Smith 2013) adapts an algorithm called Follow The Approximate Leader to make it private and obtain a regret bound of O(T 2/3). In this work, we show that a number of simple algorithms can satisfy privacy guarantees, while achieving nearly optimal regret (up to logarithmic factors) that scales naturally with the level of privacy desired.\nOur work is also of independent interest for non-private multi-armed bandit algorithms, as there are competitive with the current state of the art against switching-cost adversaries (where we recover the optimal bound). Finally, we provide rigorous empirical results against a variety of adversaries.\nThe following section gives the main background and notations. Section 3.1 describes meta-algorithms that perturb the gain sequence to achieve privacy, while Section 3.2 explains how to leverage the privacy inherent in the EXP3 algorithm by modifying the way gains are used. Section 4 compares our algorithms with EXP3 in a variety of settings. The full proofs of all our main results are in the full version.\n2 Preliminaries"}, {"heading": "2.1 The Multi-Armed Bandit problem", "text": "Formally, a bandit game is defined between an adversary and an agent as follows: there is a set of K arms A, and at each round t, the agent plays an arm It \u2208 A. Given the choice It, the adversary grants the agent a gain gIt,t \u2208 [0, 1]. The agent only observes the gain of arm It, and not that of any other arms. The goal of this agent is to maximize its total gain\nar X\niv :1\n70 1.\n04 22\n2v 1\n[ cs\n.L G\n] 1\n6 Ja\nn 20\n17\nafter T rounds, \u2211T t=1 gIt,t. A randomized bandit algorithm \u039b : (A\u00d7 [0, 1])\u2217 \u2192 D(A) maps every arm-gain history to a distribution over the next arm to take.\nThe nature of the adversary, and specifically, how the gains are generated, determines the nature of the game. For the stochastic adversary (Thompson 1933; Auer, Cesa-Bianchi, and Fischer 2002), the gain obtained at round t is generated i.i.d from a distribution PIt . The more general fully oblivious adversary (Audibert and Bubeck 2010) generates the gains independently at round t but not necessarily identically from a distribution PIt,t. Finally, we have the oblivious adversary (Auer et al. 2003) whose only constraint is to generate the gain gIt,t as a function of the current action It only, i.e. ignoring previous actions and gains.\nWhile focusing on oblivious adversaries, we discovered that by targeting differential privacy we can also compete against the stronger m-bounded memory adaptive adversary (Cesa-Bianchi, Dekel, and Shamir 2013; Merhav et al. 2002; Dekel, Tewari, and Arora 2012) who can use up to the last m gains. The oblivious adversary is a special case with m = 0. Another special case of this adversary is the one with switching costs, who penalises the agent whenever he switches arms, by giving the lowest possible gain of 0 (here m = 1).\nRegret. Relying on the cumulative gain of an agent to evaluate its performance can be misleading. Indeed, consider the case where an adversary gives a zero gain for all arms at every round. The cumulative gain of the agent would look bad but no other agents could have done better. This is why one compares the gap between the agent\u2019s cumulative gain and the one obtained by some hypothetical agent, called oracle, with additional information or computational power. This gap is called the regret.\nThere are also variants of the oracle that are considered in the literature. The most common variant is the fixed oracle, which always plays the best fixed arm in hindsight. The regret R against this oracle is :\nR = max i=1,...K T\u2211 t=1 gi,t \u2212 T\u2211 t=1 gIt,t\nIn practice, we either prove a high probability bound onR or an expected value ER with:\nER = E [ max\ni=1,...K T\u2211 t=1 gi,t \u2212 T\u2211 t=1 gIt,t ] where the expectation is taken with respect to the random choices of both the agent and adversary. There are other oracles like the shifting oracle but those are out of scope of this paper.\nEXP3. The Exponential-weight for Exploration and Exploitation (EXP3 (Auer et al. 2003)) algorithm achieves the optimal bound (up to logarithmic factors) of O( \u221a TK lnK) for the weak regret (i.e. the expected regret compared to the fixed oracle) against an oblivious adversary. EXP3 simply maintains an estimate G\u0303i,t for the cumulative gain of arm i up to round t with G\u0303i,t = \u2211t s=1 gi,t pi,t 1It=i where\npi,t = (1\u2212 \u03b3) exp (\u03b3/KG\u0303i,t)\u2211K i=1 exp (\u03b3/KG\u0303i,t) + \u03b3 K (2.1) with \u03b3 a well defined constant. Finally, EXP3 plays one action randomly according to the probability distribution pt = {p1,t, . . . pK,t} with pi,t as defined above."}, {"heading": "2.2 Differential Privacy", "text": "The following definition (from (Tossou and Dimitrakakis 2016)) specifies what is meant when we called a bandit algorithm differentially private at a single round t: Definition 2.1 (Single round ( , \u03b4)-differentially private bandit algorithm). A randomized bandit algorithm \u039b is ( , \u03b4)differentially private at round t, if for all sequence g1:t\u22121 and g\u20321:t\u22121 that differs in at most one round, we have for any action subset S \u2286 A: P\u039b(It \u2208 S | g1:t\u22121) \u2264 \u03b4 + P\u039b(It \u2208 S | g\u20321:t\u22121)e , (2.2) where P\u039b denotes the probability distribution specified by the algorithm and g1:t\u22121 = {g1, . . . gt\u22121} with gs the gains of all arms at round s. When \u03b4 = 0, the algorithm is said to be -differential private.\nThe and \u03b4 parameters quantify the amount of privacy loss. Lower ( ,\u03b4) indicate higher privacy and consequently we will also refer to ( ,\u03b4) as the privacy loss. Definition 2.1 means that the output of the bandit algorithm at round t is almost insensible to any single change in the gains sequence. This implies that whether or not we remove a single round, replace the gains, the bandit algorithm will still play almost the same action. Assuming the gains at round t are linked to a user private data (for example his cancer status or the advertisement he clicked), the definition preserves the privacy of that user against any third parties looking at the output. This is the case because the choices or the participation of that user would not almost affect the output. Equation (2.2) specifies how much the output is affected by a single user.\nWe would like Definition 2.1 to hold for all rounds, so as to protect the privacy of all users. If it does for some ( , \u03b4), then we say the algorithm has per-round or instantaneous privacy loss ( , \u03b4). Such an algorithm also has a cumulative privacy loss of at most ( \u2032, \u03b4\u2032) with \u2032 = T and \u03b4\u2032 = \u03b4T after T steps. Our goal is to design bandit algorithm such that their cumulative privacy loss ( \u2032, \u03b4\u2032) are as low as possible while achieving simultaneously a very low regret. In practice, we would like \u2032 and the regret to be sub-linear while \u03b4\u2032 should be a very small quantity. Definition 2.2 formalizes clearly the meaning of this cumulative privacy loss and for ease of presentation, we will ignore the term \u201dcumulative\u201d when referring to it. Definition 2.2 (( , \u03b4)-differentially private bandit algorithm). A randomized bandit algorithm \u039b is ( , \u03b4)-differentially private up to round t, if for all g1:t\u22121 and g\u20321:t\u22121 that differs in at most one round, we have for any action subset S \u2286 At:\nP\u039b(I1:t \u2208 S | g1:t\u22121) \u2264 \u03b4 + P\u039b(I1:t \u2208 S | g\u20321:t\u22121)e , (2.3)\nwhere P\u039b and g are as defined in Definition 2.1.\nMost of the time, we will refer to Definition 2.2 and whenever we need to use Definition 2.1, this will be made explicit.\nThe simplest mechanism to achieve differential privacy for a function is to add Laplace noise of scale proportional to its sensitivity. The sensitivity is the maximum amount by which the value of the function can change if we change a single element in the inputs sequence. For example, if the input is a stream of numbers in [0, 1] and the function their sum, we can add Laplace noise of scale 1 to each number and achieve -differential privacy with an error of O( \u221a T/ ) in the sum. However, (Chan, Shi, and Song 2010) introduced Hybrid Mechanism, which achieves -differential privacy with only poly-logarithmic error (with respect to the true sum). The idea is to group the stream of numbers in a binary tree and only add a Laplace noise at the nodes of the tree.\nAs demonstrated above, the main challenge with differential privacy is thus to trade-off optimally privacy and utility.\nNotation. In this paper, i will be used as an index for an arbitrary arm in [1,K], while k will be used to indicate an optimal arm and It is the arm played by an agent at round t. We use gi,t to indicate the gain of the i-th arm at round t. R\u039b(T ) is the regret of the algorithm \u039b after T rounds. The index and T are dropped when it is clear from the context. Unless otherwise specified, the regret is defined for oblivious adversaries against the fixed oracle. We use \u201dx \u223c P \u201d to denote that x is generated from distribution P . Lap(\u03bb) is used to denote the Laplace distribution with scale \u03bb while Bern (p) denotes the Bernoulli distribution with parameter p."}, {"heading": "3 Algorithms and Analysis", "text": ""}, {"heading": "3.1 DP-\u039b-Lap: Differential privacy through additional noise", "text": "We start by showing that the obvious technique to achieve a given -differential privacy in adversarial bandits already beat the state-of-the art. The main idea is to use any base bandit algorithm \u039b as input and add a Laplace noise of scale 1 to each gain before \u039b observes it. This technique gives -DP differential privacy as the gains are bounded in [0, 1] and the noises are added i.i.d at each round.\nHowever, bandits algorithms require bounded gains while the noisy gains are not. The trick is to ignore rounds where the noisy gains fall outside an interval of the form [\u2212b, b+1]. We pick the threshold b such that, with high probability, the noisy gains will be inside the interval [\u2212b, b+ 1]. More precisely, b can be chosen such that with high probability, the number of rounds ignored is lower than the upper bound R\u039b on the regret of \u039b. Given that in the standard bandit problem, the gains are bounded in [0, 1], the gains at accepted rounds are rescaled back to [0, 1].\nTheorem 3.2 shows that all these operations still preserve -DP while Theorem 3.1 demonstrates that the upper bound on the expected regret of DP-\u039b-Lap adds some small additional terms to R\u039b. To illustrate how small those additional terms are, we instantiate DP-\u039b-Lap with the EXP3 algorithm. This leads to a mechanism called DP-EXP3-Lap described in Algorithm 1. With a carefully chosen threshold b, corollary 3.1 implies that the additional terms are such that the\nexpected regret of DP-EXP3-Lap is O( \u221a T lnT/ ) which is optimal in T up to some logarithmic factors. This result is a significant improvement over the best known bound so far of O(T 2/3/ ) from (Thakurta and Smith 2013) and solves simultaneously the challenge (whether or not one can get -DP mechanism with optimal regret) posed by the authors.\nAlgorithm 1 DP-EXP3-Lap Let G\u0303i = 0 for all arms and b = lnT , \u03b3 = \u221a K lnK (e\u22121)T\nfor each round t = 1, \u00b7 \u00b7 \u00b7 , T do Compute the probability distribution p over the arms\nwith p = (p1,t, \u00b7 \u00b7 \u00b7 pK,t) and pi,t as in eq (2.1). Draw an arm It from the probability distribution p. Receive the reward gIt,t Let the noisy gain be g\u2032It,t = gIt,t +NIt,t with NIt,t \u223c Lap( 1 ) if g\u2032It,t \u2208 [\u2212b, b+ 1] then\nScale g\u2032It,t to [0, 1] Update the estimated cumulative gain of arm It:\nG\u0303It = G\u0303It + g\u2032It,t pIt,t\nend if end for\nTheorem 3.1. If DP-\u039b-Lap is run with input a base bandit algorithm \u039b, the noisy reward g\u2032It,t of the true reward gIt,t set to g\u2032It,t = gIt,t +NIt,t withNIt,t \u223c Lap( 1 ), the acceptance interval set to [\u2212b, b + 1] with the scaling of the rewards g\u2032It outside [0, 1] done using g \u2032 It,t = g\u2032It,t+b\n2b+1 ; then the regret RDP-\u039b-Lap of DP-\u039b-Lap satisfies:\nERDP-\u039b-Lap \u2264 ERscaled\u039b + 2TK exp(\u2212 b) + \u221a 32T\n(3.1)\nwhere Rscaled\u039b is the upper bound on the regret of \u039b when the rewards are scaled from [\u2212b, b+ 1] to [0, 1]\nProof Sketch. We observed that DP-\u039b-Lap is an instance of \u039b run with the noisy rewards g\u2032 instead of g. This means Rscaled\u039b is an upper bound of the regret L on g\n\u2032. Then, we derived a lower bound onL showing how close it is toRDP-\u039b-Lap. This allows us to conclude.\nCorollary 3.1. If DP-\u039b-Lap is run with EXP3 as its base algorithm and b = lnT , then its expected regret ERDP-EXP3-Lap satisfies\nERDP-EXP3-Lap \u2264 4 lnT \u221a (e\u2212 1)TK lnK\n+ 2K +\n\u221a 32T\nProof. The proof comes by combining the regret of EXP3 (Auer et al. 2003) with Theorem 3.1\nTheorem 3.2. DP-\u039b-Lap is -differentially private up to round T .\nProof Sketch. Combining the privacy of Laplace Mechanism with the parallel composition (McSherry 2009) and postprocessing theorems (Dwork and Roth 2013) concludes the proof."}, {"heading": "3.2 Leveraging the inherent privacy of EXP3", "text": "On the differential privacy of EXP3 (Dwork and Roth 2013) shows that a variation of EXP3 for the full-information setting (where the agent observes the gain of all arms at any round regardless of what he played) is already differentially private. Their results imply that one can achieve the optimal regret with only a sub-logarithmic privacy loss (O( \u221a\n128 log T )) after T rounds. We start this section by showing a similar result for EXP3 in Theorem 3.3. Indeed, we show that EXP3 is already differentially private but with a per-round privacy loss of 2. 1 Our results imply that EXP3 can achieve the optimal regret albeit with a linear privacy loss of O(2T )-DP after T rounds. This is a huge gap compared with the full-information setting and underlines the significance of our result in section 3.1 where we describe a concrete algorithm demonstrating that the optimal regret can be achieved with only a logarithmic privacy loss after T rounds.\nTheorem 3.3. The EXP3 algorithm is:\nmin { 2T, T \u00b7 ln K(1\u2212 \u03b3) + \u03b3\n\u03b3 , 2(1\u2212 \u03b3)T + 2\n\u221a 2 lnT\nT } differentially private up to round T .\nIn practice, we also want EXP3 to have a sub-linear regret. This implies that \u03b3 << 1 and EXP3 is simply 2T -DP over T rounds.\nProof Sketch. The first two terms in the theorem come from the observation that EXP3 is a combination of two mechanisms: the Exponential Mechanism (McSherry and Talwar 2007) and a randomized response. The last term comes from the observation that with probability \u03b3 we enjoy a perfect 0-DP. Then, we use Chernoff to bound with high probability the number of times we suffer a non-zero privacy loss.\nWe will now show that the privacy of EXP3 itself may be improved without any additional noise, and with only a moderate impact on the regret.\nOn the privacy of a EXP3 wrapper algorithm The previous paragraph leads to the conclusion that it is impossible to obtain a sub-linear privacy loss with a sub-linear regret while using the original EXP3. Here, we will prove that an existing technique is already achieving this goal. The algorithm which we called EXP3\u03c4 is from (Dekel, Tewari, and Arora 2012). It groups the rounds into disjoint intervals of fixed size \u03c4 where the j\u2019th interval starts on round (j \u2212 1)\u03c4 + 1 and ends on round j\u03c4 . At the beginning of interval j, EXP3\u03c4 receives\n1Assuming we want a sub-linear regret. See Theorem 3.3\nan action from EXP3 and plays it for \u03c4 rounds. During that time, EXP3 does not observe any feedback. At the end of the interval, EXP3\u03c4 feeds EXP3 with a single gain, the average gain received during the interval.\nTheorem 3.4 borrowed from (Dekel, Tewari, and Arora 2012) specifies the upper bound on the regret EXP3\u03c4 . It is remarkable that this bound holds against the m-memory bounded adaptive adversary. While in theorem 3.5, we show the privacy loss enjoyed by this algorithm, one gets a better intuition of how good those results are from corollary 3.2 and 3.3. Indeed, we can observe that EXP3\u03c4 achieves a sub-logarithmic privacy loss of O( \u221a lnT ) with a regret of O(T 2/3) against a special case of the m-memory bounded adaptive adversary called the switching costs adversary for which m = 1. This is the optimal regret bound (in the sense that there is a matching lower bound (Dekel et al. 2014)). This means that in some sense we are getting privacy for free against this adversary.\nTheorem 3.4 (Regret of EXP3\u03c4 (Dekel, Tewari, and Arora 2012)). The expected regret of EXP3\u03c4 is upper bounded by:\n\u221a 7T\u03c4K lnK + Tm\n\u03c4 + \u03c4\nagainst the m-memory bounded adaptive adversary for any m < \u03c4 . Theorem 3.5 (Privacy loss of EXP3\u03c4 ). EXP3\u03c4 is( 4T \u03c43 + \u221a 8 ln(1/\u03b4\u2032) T\u03c43 , \u03b4 \u2032 ) -DP up to round T .\nProof. The sensitivity of each gain is now 1\u03c4 as we are using the average. Combined with theorem (3.3), it means the perround privacy loss is 2T\u03c4 . Given that EXP3 only observes T \u03c4 rounds, using the advanced composition theorem (Dwork, Rothblum, and Vadhan 2010) (Theorem III.3) concludes the final privacy loss over T rounds.\nCorollary 3.2. EXP3\u03c4 run with \u03c4 = (7K logK)\u22121/3T 1/3 is ( , \u03b4\u2032) differentially private up to round T with \u03b4\u2032 = T\u22122, = 28K lnK + \u221a 112K lnK lnT . Its expected regret against the switching costs adversary is upper bounded by 2(7K lnK)1/3T 2/3 + (7K logK)\u22121/3T 1/3.\nProof. The proof is immediate by replacing \u03c4 and \u03b4\u2032 in Theorem 3.4 and 3.5 and the fact that for the switching costs adversary, m = 1.\nCorollary 3.3. EXP3\u03c4 run with \u03c4 = ( 4T +2T ln 1\u03b4 2 )1/3 is ( , \u03b4) differentially private and its expected regret against the switching costs adversary is upper bounded by:\nO ( T 2/3 \u221a K lnK (\u221a ln 1\u03b4 )1/3)\n4 Experiments We tested DP-EXP3-Lap, EXP3\u03c4 together with the nonprivate EXP3 against a few different adversaries. The privacy parameter of DP-EXP3-Lap is set as defined in corollary 3.2. This is done so that the regret of DP-EXP3-Lap and\nEXP3\u03c4 are compared with the same privacy level. All the other parameters of DP-EXP3-Lap are taken as defined in corollary 3.1 while the parameters of EXP3\u03c4 are taken as defined in corollary 3.2.\nFor all experiments, the horizon is T = 218 and the number of arms is K = 4. We performed 720 independent trials and reported the median-of-means estimator2 of the cumulative regret. It partitions the trials into a0 equal groups and return the median of the sample means of each group. Proposition 4.1 is a well known result (also in (Hsu and Sabato 2013; Lerasle and Oliveira 2011)) giving the accuracy of this estimator. Its convergence is O(\u03c3/ \u221a N), with exponential probability tails, even though the random variable x may have heavytails. In comparison, the empirical mean can not provide such guarantee for any \u03c3 > 0 and confidence in [0, 1/(2e)] (Catoni 2012). Proposition 4.1. Let x be a random variable with mean \u00b5 and variance \u03c32 <\u221e. Assume that we have N independent sample of x and let \u00b5\u0302 be the median-of-means computed using a0 groups. With probability at least 1 \u2212 e\u2212a0/4.5, \u00b5\u0302 satisfies |\u00b5\u0302\u2212 \u00b5| \u2264 \u03c3 \u221a 6a0/N .\nWe set the number of groups to a0 = 24, so that the confidence interval holds w.p. at least 0.995.\nWe also reported the deviation of each algorithm using the Gini\u2019s Mean Difference (GMD hereafter) (Gini and Pearson 1912). GMD computes the deviation as \u2211N j=1(2j \u2212 N \u2212 1)x(j) with x(j) the j-th order statistics of the sample (that is x(1) \u2264 x(2) \u2264 . . . \u2264 x(N)). As shown in (Yitzhaki and others 2003; David 1968), the GMD provides a superior approximation of the true deviation than the standard one. To account for the fact that the cumulative regret of our algorithms might not follow a symmetric distribution, we computed the GMD separately for the values above and below the median-of-means.\nAt round t, we computed the cumulative regret against the fixed oracle who plays the best arm assuming that the end of the game is at t. The oracle uses the actual sequence of gains to decide his best arm. For a given trial, we make sure that all algorithms are playing the same game by generating the gains for all possible pair of round-arm before the game starts.\nDeterministic adversary. As shown by (Audibert and Bubeck 2010), the expected regret of any agent against an oblivious adversary can not be worse than that against the worst case deterministic adversary. In this experiment, arm 2 is the best and gives 1 for every even round. To trick the players into picking the wrong arms, the first arm always gives 0.38 whereas the third gives 1 for every round multiple of 3. The remaining arms always give 0. As shown by the figure, this simple adversary is already powerful enough to make the algorithms attain their upper bound.\n2Used heavily in the streaming literature (Alon, Matias, and Szegedy 1996)\nStochastic adversary This adversary draws the gains of the first arm i.i.d from Bern (0.55) whereas all other gains are drawn i.i.d from Bern (0.5).\nFully oblivious adversary. For the best arm k, it first draws a number p uniformly in [0.5, 0.5 + 2 \u00b7 \u03b5] and generates the gain gk,t \u223c Bern (p). For all other arms, p is drawn from [0.5\u2212\u03b5, 0.5+\u03b5]. This process is repeated at every round. In our experiments, \u03b5 = 0.05\nAn oblivious adversary. This adversary is identical to the fully oblivious one for every round multiple of 200. Between two multiples of 200 the last gain of the arm is given.\nThe Switching costs adversary This adversary (defined at Figure 1 in (Dekel et al. 2014)) defines a stochastic processes (including simple Gaussian random walk as special case) for generating the gains. It was used to prove that any algorithm against this adversary must incur a regret of O(T 2/3).\nDiscussion Figure 1 shows our results against a variety of adversaries, with respect to a fixed oracle. Overall, the performance (in term of regret) of DP-EXP3-Lap is very competitive against that of EXP3 while providing a significant better privacy. This means that DP-EXP3-Lap allows us to get privacy for free in the bandit setting against an adversary not more powerful than the oblivious one.\nThe performance of EXP3\u03c4 is worse than that of DPEXP3-Lap against an oblivious adversary or one less powerful. However, the situation is completely reversed against the more powerful switching cost adversary. In that setting, EXP3\u03c4 outperforms both EXP3 and DP-EXP3-Lap confirming the theoretical analysis. We can see EXP3\u03c4 as the algorithm providing us privacy for free against switching cost adversary and adaptive m-bounded memory one in general.\n5 Conclusion We have provided the first results on differentially private adversarial multi-armed bandits, which are optimal up to logarithmic factors. One open question is how differential privacy affects regret in the full reinforcement learning problem. At this point in time, the only known results in the MDP setting obtain differentially private algorithms for Monte Carlo policy evaluation (Balle, Gomrokchi, and Precup 2016). While this implies that it is possible to obtain policy iteration algorithms, it is unclear how to extend this to the full online reinforcement learning problem.\nAcknowledgements. This research was supported by the SNSF grants \u201cAdaptive control with approximate Bayesian computation and differential privacy\u201d and \u201cSwiss Sense Synergy\u201d, by the Marie Curie Actions (REA 608743), the Future of Life Institute \u201cMechanism Design for AI Architectures\u201d and the CNRS Specific Action on Security."}, {"heading": "In Proceedings of the 2010 IEEE 51st Annual Symposium on", "text": "Foundations of Computer Science, FOCS \u201910, 51\u201360.\n[Dwork 2006] Dwork, C. 2006. Differential privacy. In ICALP, 1\u201312. Springer.\n[Gini and Pearson 1912] Gini, C., and Pearson, K. 1912. Variabilita\u0300 e mutabilita\u0300: contributo allo studio delle distribuzioni e delle relazioni statistiche. Fascicolo 1. tipografia di Paolo Cuppini.\n[Hsu and Sabato 2013] Hsu, D., and Sabato, S. 2013. Loss minimization and parameter estimation with heavy tails. arXiv preprint arXiv:1307.1827.\n[Jain, Kothari, and Thakurta 2012] Jain, P.; Kothari, P.; and Thakurta, A. 2012. Differentially private online learning. In Mannor, S.; Srebro, N.; and Williamson, R. C., eds., COLT 2012, volume 23, 24.1\u201324.34.\n[Lerasle and Oliveira 2011] Lerasle, M., and Oliveira, R. I. 2011. Robust empirical mean estimators. arXiv preprint arXiv:1112.3914.\n[McSherry and Talwar 2007] McSherry, F., and Talwar, K. 2007. Mechanism design via differential privacy. In Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201907, 94\u2013103. Washington, DC, USA: IEEE Computer Society.\n[McSherry 2009] McSherry, F. D. 2009. Privacy integrated queries: An extensible platform for privacy-preserving data analysis. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201909, 19\u201330. New York, NY, USA: ACM.\n[Merhav et al. 2002] Merhav, N.; Ordentlich, E.; Seroussi, G.; and Weinberger, M. J. 2002. On sequential strategies for loss functions with memory. IEEE Trans. Information Theory 48(7):1947\u20131958.\n[Mishra and Thakurta 2015] Mishra, N., and Thakurta, A. 2015. (nearly) optimal differentially private stochastic multiarm bandits. Proceedings of the 31th UAI.\n[Pandey and Olston 2006] Pandey, S., and Olston, C. 2006. Handling advertisements of unknown quality in search advertising. In Scho\u0308lkopf, B.; Platt, J. C.; and Hoffman, T., eds., Twentieth NIPS, 1065\u20131072.\n[Thakurta and Smith 2013] Thakurta, A. G., and Smith, A. D. 2013. (nearly) optimal algorithms for private online learning in full-information and bandit settings. In NIPS, 2733\u20132741.\n[Thompson 1933] Thompson, W. 1933. On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of two Samples. Biometrika 25(3-4):285\u2013294.\n[Tossou and Dimitrakakis 2016] Tossou, A. C. Y., and Dimitrakakis, C. 2016. Algorithms for differentially private multi-armed bandits. In AAAI, 2087\u20132093. AAAI Press.\n[Yitzhaki and others 2003] Yitzhaki, S., et al. 2003. Gini\u2019s mean difference: A superior measure of variability for nonnormal distributions. Metron 61(2):285\u2013316.\n[Zhao et al. 2014] Zhao, J.; Jung, T.; Wang, Y.; and Li, X. 2014. Achieving differential privacy of data disclosure in the smart grid. In 2014 IEEE Conference on Computer Communications, INFOCOM 2014, 504\u2013512."}], "references": [{"title": "N", "author": ["S. Agrawal", "Goyal"], "venue": "2012. Analysis of thompson sampling for the multi-armed bandit problem. In COLT", "citeRegEx": "Agrawal and Goyal 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "The space complexity of approximating the frequency moments", "author": ["Matias Alon", "N. Szegedy 1996] Alon", "Y. Matias", "M. Szegedy"], "venue": null, "citeRegEx": "Alon et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1996}, {"title": "and Bubeck", "author": ["Audibert", "J.-Y."], "venue": "S.", "citeRegEx": "Audibert and Bubeck 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "R", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "Schapire"], "venue": "E.", "citeRegEx": "Auer et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Finite time analysis of the multiarmed bandit problem. Machine Learning 47(2/3):235\u2013256", "author": ["Cesa-Bianchi Auer", "P. Fischer 2002] Auer", "N. CesaBianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "D", "author": ["B. Balle", "M. Gomrokchi", "Precup"], "venue": "2016. Differentially private policy evaluation. In ICML", "citeRegEx": "Balle. Gomrokchi. and Precup 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["A.N. Burnetas", "Katehakis"], "venue": "N.", "citeRegEx": "Burnetas and Katehakis 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Online learning with switching costs and other adaptive adversaries", "author": ["Dekel Cesa-Bianchi", "N. Shamir 2013] Cesa-Bianchi", "O. Dekel", "O. Shamir"], "venue": null, "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2013}, {"title": "T", "author": ["Chan"], "venue": "H.; Shi, E.; and Song, D.", "citeRegEx": "Chan. Shi. and Song 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Miscellanea: Gini\u2019s mean difference rediscovered", "author": ["H. David 1968] David"], "venue": null, "citeRegEx": "David,? \\Q1968\\E", "shortCiteRegEx": "David", "year": 1968}, {"title": "Bandits with switching costs: T2/3 regret", "author": ["Dekel"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Dekel,? \\Q2014\\E", "shortCiteRegEx": "Dekel", "year": 2014}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["Tewari Dekel", "O. Arora 2012] Dekel", "A. Tewari", "R. Arora"], "venue": "In ICML. icml.cc / Omnipress", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "and Roth", "author": ["C. Dwork"], "venue": "A.", "citeRegEx": "Dwork and Roth 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["Dwork, C.", "Rothblum"], "venue": "N.; and Vadhan, S.", "citeRegEx": "Dwork. Rothblum. and Vadhan 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Pearson", "author": ["C. Gini"], "venue": "K.", "citeRegEx": "Gini and Pearson 1912", "shortCiteRegEx": null, "year": 1912}, {"title": "and Sabato", "author": ["D. Hsu"], "venue": "S.", "citeRegEx": "Hsu and Sabato 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Differentially private online learning", "author": ["Kothari Jain", "P. Thakurta 2012] Jain", "P. Kothari", "A. Thakurta"], "venue": "COLT 2012,", "citeRegEx": "Jain et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "R", "author": ["M. Lerasle", "Oliveira"], "venue": "I.", "citeRegEx": "Lerasle and Oliveira 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Talwar", "author": ["F. McSherry"], "venue": "K.", "citeRegEx": "McSherry and Talwar 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "F", "author": ["McSherry"], "venue": "D.", "citeRegEx": "McSherry 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "M", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "Weinberger"], "venue": "J.", "citeRegEx": "Merhav et al. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Thakurta", "author": ["N. Mishra"], "venue": "A.", "citeRegEx": "Mishra and Thakurta 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Olston", "author": ["S. Pandey"], "venue": "C.", "citeRegEx": "Pandey and Olston 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["A.G. Thakurta", "Smith"], "venue": "D.", "citeRegEx": "Thakurta and Smith 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Dimitrakakis", "author": ["A.C.Y. Tossou"], "venue": "C.", "citeRegEx": "Tossou and Dimitrakakis 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Gini\u2019s mean difference: A superior measure of variability for nonnormal distributions. Metron 61(2):285\u2013316", "author": ["Yitzhaki", "S others 2003] Yitzhaki"], "venue": null, "citeRegEx": "Yitzhaki and Yitzhaki,? \\Q2003\\E", "shortCiteRegEx": "Yitzhaki and Yitzhaki", "year": 2003}, {"title": "Achieving differential privacy of data disclosure in the smart grid", "author": ["Zhao"], "venue": "IEEE Conference on Computer Communications,", "citeRegEx": "Zhao,? \\Q2014\\E", "shortCiteRegEx": "Zhao", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we improve the previously best known regret bound to achieve -differential privacy in oblivious adversarial bandits from O(T / ) to O( \u221a T lnT/ ). This is achieved by combining a Laplace Mechanism with EXP3. We show that though EXP3 is already differentially private, it leaks a linear amount of information in T . However, we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions. This allows us to reach O( \u221a lnT )-DP, with a regret of O(T ) that holds against an adaptive adversary, an improvement from the best known of O(T ). This is done by using an algorithm that run EXP3 in a mini-batch loop. Finally, we run experiments that clearly demonstrate the validity of our theoretical analysis.", "creator": "LaTeX with hyperref package"}}}