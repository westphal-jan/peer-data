{"id": "1701.04693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Incremental Learning for Robot Perception through HRI", "abstract": "intuitive scene understanding and manual object recognition is a incredibly difficult pursuit to systematically achieve yet crucial acquisition skill for robots. recently, convolutional semantic neural networks ( cnn ), have shown success in this task. however, there is still unfortunately a vast gap appearing between their performance rates on image compression datasets together and presenting real - augmented world robotics scenarios. we present humans a key novel action paradigm targeted for incrementally improving a cognitive robot's rich visual composite perception mechanisms through active human interaction. confident in providing this paradigm, the user continually introduces novel simple objects addressed to the robot by means of his pointing finger and placing voice generating commands. given this key information, the robot visually explores the object input and adds complex images from it attempting to re - train via the objective perception analytical module. our simple base perception module is ultimately based on recent development in object detection tool and data recognition synthesis using implicit deep emotion learning. \u2022 our method leverages state of the machine art cnns findings from off - line batch learning, human guidance, robot exploration and rigorous incremental on - line task learning.", "histories": [["v1", "Tue, 17 Jan 2017 14:29:05 GMT  (4203kb,D)", "http://arxiv.org/abs/1701.04693v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.HC cs.LG", "authors": ["sepehr valipour", "camilo perez", "martin jagersand"], "accepted": false, "id": "1701.04693"}, "pdf": {"name": "1701.04693.pdf", "metadata": {"source": "CRF", "title": "Incremental Learning for Robot Perception through HRI", "authors": ["Sepehr Valipour", "Camilo Perez", "Martin Jagersand"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nA current research aim is robot assistants helping humans in everyday manipulation tasks. However, only a few robot platforms have achieved integration in home environments. Commercial robots have been designed to perform a specific task, e.g., robotic vacuums, robotic lawn mowers. On the other hand, the aim of having a multi-purpose robot system is still not realized. One key reason is that current robots do not have the capacity to interact well with humans. By considering HRI as a core component during the system design, it will be easier to integrate robots within humans daily activities. Yanco et al. [25] conducted a study during the recent DARPA robotics challenge [15]. Their results show that although every team used similar methods and technologies, one of the main reasons for different performance was the quality of the human-robot interface interaction presented to the operators during the challenge. Researchers have realized this and instead of aiming for autonomy, have shifted focus to the user interaction focusing in the human-in-the-loop paradigm. In this paradigm, the human\u2019s knowledge and guidance is used whenever the robot cannot make a decision on its own [7], [12], [18]. To provide this guidance, it is important to have the capacity to establish a common ground knowledge (discourse) shared between the human and robot, just as humans do. For example a new apprentice in a metal workshop needs to quickly get familiar with different types of materials (Steel, bronze, nylon, etc.) and machines (Lathe, drilling, milling, boring, shaping etc.). Otherwise, it will be difficult to understand the information that is passed to him. Therefore, a more experienced worker tries to first establish a common ground knowledge, like names of tools and materials, by interacting with the apprentice.\n*Equal Contribution.\nThe same principals should also be considered for robots. However, robots need to have a basic world understanding before they can start learning from guidance. This basic understanding can be in terms of object localization and recognition. Deep learning has shown to outperform many classic computer vision approaches in this matter. Overfeat [23] was one of the earliest attempts. A convolutional neural network (CNN) was used to generate bounding\nar X\niv :1\n70 1.\n04 69\n3v 1\n[ cs\n.R O\n] 1\n7 Ja\nn 20\n17\nboxes of each object and label them according to their class. This work used a regression network to predict a possible bounding box for an object given a grid of proposed regions with different scales. In this approach, increasing number of region proposals proved to be both crucial for accuracy and at the same time, unfortunately, computationally expensive. Therefore, more recent attempts [20] [8] utilized Region Proposal Networks (RPN) to regress bounding boxes. RPN can operate on feature maps instead of the input image and by doing so, it bypasses the need for recomputing the feature maps. CNN models have achieved the state of the art in object recognition on a wide variety of datasets. However, their implicit assumption is that all the possible object categories are included in the dataset for batch off-line training. Unfortunately, real world recognition is different. At prediction time, the algorithm will frequently face objects that are not in the training data or they can look very different compared to training examples. In the literature these problems are addressed by Incremental Machine Learning (IML) [6], [14] and Open Set Recognition(OSR) [4], [21], [22]. The main focus of the IML is to handle new instances of known classes. OSR methods, however, need to deal with two more challenges. One is to continuously detect novel categories and two is to update the method so that it will include the new category. These are difficult problems to solve, especially, recognizing an object as a novel class since it could involve a long reasoning chain. For example, a sugar box and a detergent box look very similar and without semantic cues like reading the label or using the context that they are in, it is near impossible to differentiate them. However, we can utilize a robot\u2019s discourse as a solution to this problem. In particular, robots can interact with humans and get guidance or instructions from them. They are also able to explore the environment using an on-board camera.\nAlong this general direction, we propose a new method to improve the robot\u2019s visual perception incrementally. Our purpose is to recognize, and localize objects. Furthermore to have the capacity of learning new objects and correcting false interpretations through HRI. Our contribution is twofold. (1) A deep learning based localization and recognition method that uses our robotic-vision interface to incrementally improve its object knowledge through interaction with a human. (2) A robot-vision system capable of interacting naturally with a human to establish a common ground knowledge of the objects in a shared environment. The rest of this paper is organized as follows: section II shows an example of using our proposed interaction and gives a detailed description of our deep learning approach for object recognition and localization. Then, it explains how the incremental perception is achieved by using an interactive approach between the human and robot. Section III presents an overview of the system components. Section IV describes the experiments, procedure, results and discussion on findings. Finally, conclusions and future work are presented in section V."}, {"heading": "II. METHOD", "text": "A. Interaction\nAn interaction example using our proposed interface is shown in Fig. 1. Our example shows a person soldering a circuit board. During this work, he requires the use of a multimeter, that is out of his reach. He asks his robot assistant to bring the multimeter. (Fig. 1A). The robot is equipped with a recognition module. Unfortunately, the multimeter class was not found as a recognizable object. The person then asks the robot what it sees (given its current world representation) (Fig. 1A), to figure out if the robot is detecting the object under another name/category or it does not see it at all. Through speech, the robot communicates the recognized objects in the scene, and by using pointing gestures the robot provides objects\u2019 locations(Fig. 1B). Pointing allows the robot to skip complex spatial sentences, (e.g. \u201dfrom my point of view the laptop is on the left top corner of the table\u201d). After communicating which objects were recognized and localized by the robot the human can interact through gestures and speech to correct or add a particular object (Fig. 1C), in this case, the multimeter. After the addition or correction, the robot collects several images on-line of the target object (Fig. 1D) which is used to modify the current world representation. This procedure needs to be done only once for each new class. Finally, the person asks again to bring the multimeter (Fig. 1E). This time the robot succeeds on recognizing the object and executes a picking task to bring the multimeter. (Fig. 1F). In the following two subsections, we describe our localization and recognition network and our open-set recognition approach."}, {"heading": "B. Localization and Recognition Network", "text": "The network can be divided into three parts, see Fig.2. The first part is a fully convolutional network that extracts feature maps from the input image. Second, is the localization network that takes feature maps from first part and finds the possible bounding boxes of objects, the objectness score corresponding to each bounding box. The third part is the recognition network. It takes a fixed size feature map input corresponding to each bounding box and produces predictions for each bounding box. We used first 30 layers of vgg-16 [24] (counting pooling and activation layers), trained on image-net [11] for the first part of the network. We chose vgg-16 due to its state of the art performance in object recognition. The structure of our localization network is based on the Densecap [8] which in turn is a modified version of the Faster RCNN [20]. In this model, the localization receives a feature map that is computed by a convolutional layer. Using this feature map and convolutional anchors, a regression network finds the transformation that is required to take an anchor to a bounding box of an object. Convolutional anchors can be viewed as a fixed and multi-scale proposed bounding boxes in the image space that are centered to the spatial correspondence of a pixel in the feature map. Using this\nscheme, greatly improves the inference time. After predicting a bounding box, the recognition network finds a fixed size feature map corresponding to that bounding box. It does so by first, projecting the bounding box into feature map space, then transferring the arbitrary size selection of the feature maps into a fixed size, using bi-linear interpolation. The network also has a regressor branch that predicts the objectness of the fixed size feature map. For the recognition network, we again used the last three fully connected layer of vgg-16 as our recognition structure with the weights initialized by the weights of a trained vgg16 model. Same as the Densecap, we predict the objectness score and position of bounding box one more time in the recognition network. Lastly, this network predicts the category of the object inside of each bounding box. For the base training of our model, we used Microsoft COCO dataset [13]. This dataset consists of about 328 thousand images with 2.5 million labeled instances of 80 common objects in their common context. We used the objects\u2019 bounding box data and their category as our supervised data. The loss function is defined in 1. In this equation y\u0302bb1, y\u0302bb2,y\u0302o1, y\u0302o2, y\u0302c are prediction for first and second bounding box regression, first and second objectness score regression and classification probability, respectively. ybb, yo, yc are ground truth for locations of bounding boxes, binary value indicating the existence of an object and the one hot vector indicating the category, respectively. k is the number of proposed regions for each image and flogloss is the logarithmic loss function.\nFloss(y\u0302 bb1, y\u0302bb2, y\u0302o1, y\u0302o2, y\u0302c, ybb, yc) = wc k\u2211 i=1 flogloss(y\u0302 c i , y c i )\n+ wbb( k\u2211 i=1 ||y\u0302bb1i \u2212 ybbi ||+ k\u2211 i=1 ||y\u0302bb2i \u2212 ybbi ||)\n+ wo( k\u2211 i=1 flogloss(y\u0302 o1 i , y o i ) + k\u2211 i=1 flogloss(y\u0302 o2 i , y o i ))\n(1)\nDuring the training, the weights of the first part of the network are kept constant. Since it is taken from a trained model, they are suitable for extracting features. The weights of the rest of the network are initialized with samples taken from a normal distribution with zero mean and 0.01 deviation [11]. As for the optimizer, we use ADAM [10] due to its easy tuning. The initial learning rate is set to 10\u22124. To reduce inference time, we reshape all images to 400 \u00d7 400 which is smaller than conventional implementations and we also decreased the number of bounding box proposals to 200."}, {"heading": "C. Open-Set Recognition Facilitated by Human Guidance", "text": "As mentioned before, discriminating between a novel and known objects is one of the main challenges in Open-set Recognition. The main reason that causes this problem, is the fact that careful semantic understanding is required to differentiate novel and known classes. However, human guidance can circumvent this problem. The user can reliably introduce novel objects to the system and therefore the incremental learning approach can use this guidance as the ground truth. Accordingly, in this section, we assume that reliable positive samples of a novel object is given to the incremental learning module through the user\u2019s interaction with the robot. To enable the network to recognize a new class, we modify the last fully connected layer that performs the classification by adding a new set of weights corresponding to the new class. Concretely, if we denote the weights of this layer as \u0398n = [\u03b81\u03b82...\u03b8n] where \u03b8i is the ith column of the weight matrix then,\n\u0398n+1 = [\u03b81\u03b82...\u03b8n\u03b8n+1]\n\u03b8n+1 is a randomly initialized vector. For adding the new category only the weights in this layer is modified and the rest of the network stays constant. Using one-vs-all classification scheme, the newly added weight vector is trained. We use the first part of the recognition and localization network to extract features from input images and using these features, we optimize the weights for the classification loss function. Stochastic Gradient Descent is used for training this layer. To perform one-vs-all classification, in addition to positive samples, a set of negative samples is also needed. We extract a subset of images from MS-COC0 and images that were taken for the already added classes. We assign a probability for drawing samples from each known class based on how likely it is that they get mistaken with the new class 2. In equation 2 Ci denotes the i-th class and Xn+1 is the positive samples set. This sub-sampling procedure forces the classifier to discriminate better between similar classes. It is expected that the recognition accuracy degrades as number of new objects increases. It is mostly due to the fact that the network is only partially trained for the new object. It is not a recommended approach in a batch dataset scenario where the data for all classes is available. However, as it is explained earlier, having an exhaustive dataset of all object is close to impossible. In addition, for most cases\nin robotics, a local representation of objects is enough, and even desired, since the robot is mostly interacting with one particular environment.\nPdraw{Ci} = P{pred(Xn+1) = Ci} (2)"}, {"heading": "III. SYSTEM DESCRIPTION", "text": "Our system uses a 7-DOF WAM arm [2] instrumented with eye-in-hand-cameras, a microphone, Kinect camera and speakers. It is composed of 7 modules as shown in Fig. 4, all modules are fully integrated with ROS [16].\n\u2022 The speech recognition module integrates the CMU Sphinx toolkit [1]. It provides basic word and sentence recognition used by the robot to shift states during the interaction. \u2022 The speech synthesis module relies on the Festival speech synthesis system [5] and provides feedback to the human in a verbal channel. \u2022 The object localization and detection module provides labels and 2D locations of the objects in the scene. For a detail description see section II. \u2022 The Incremental learning module uses HRI, to permit changes in the robot\u2019s world representation. For a detail description see section II. \u2022 The gesturing module is based on our previous work [17], [19], where we proposed a non-verbal robot-vision\nsystem capable of inferring human pointing and perform simple pick and place tasks based on human gesture commands. We integrate this capability into our system to reduce speech description and make the interaction more human like. \u2022 The robot controller module commands robot movements and generates: pointing gestures, robot data collection and pick-up object actions [19]. \u2022 The interaction controller module is in charge of orchestrating the complete system. It supports the different interactions that are shown in Fig. 1 and it is based on a finite state machine that is triggered by gesture or speech coming from the human and/or robot.\nIn Fig. 1 four important interactions of our system are highlighted. Verbal interaction, by using both speech recognition and synthesis the human and the robot can establish basic verbal communication. The ground truth world representation interaction presents both verbal and gesture interaction performed by the robot. The recognition and localization module provides 2D bounding boxes and labels of the detected objects. The system verbally informs the object class and at the same time the robot arm points to the object 3D centroid. The centroid is calculated by using the RGB to depth camera correspondence from the objects bounding box (See Fig. 5). In the correct interaction, human uses both verbal and gesture communication to annotate a particular object in the scene that needs to be corrected. Fig 5 shows both RGB and Point cloud visualization. The head and hand of the human are tracked as two points. With a verbal triggering, a 3D ray is constructed from these two points, hitting the target object. The robot is then commanded to collect data with this 3D object location. The object collection is performed through the eye-in-hand camera by moving the end-effector in a parameterized helix curve keeping the camera facing to the object location. During the collection state, a TLD tracker [9] is used to guarantee the cropping of the object during the data collection (See Fig.6). The initial bounding box of the tracker is given as the whole image when the robots starts the collection close to the object."}, {"heading": "IV. EXPERIMENTS", "text": "To validate our approach we tested three main components of our methods. Namely, The object detection and recognition baseline, incremental learning algorithm and finally incremental learning through human guidance.\nA. Incremental Learning Through HRI\nIn this experiment we aim for emulating a real scenario with an assistant robot in an electronics workshop. The robot starts by having the baseline object detection and recognition and we try to teach it to recognize new objects in the workshop. Accordingly, using our system III, we introduced new objects to the robot and the robot collected images from this new object using its eye-in-hand camera. Samples of these images can be seen in Fig. 6. After each collection, the new object is appended to the robots detection module\nin real-time and then the other object is added. After adding each new class, we re-evaluate the recognition accuracy on the MS-COCO test set plus the test portion of the newly added class. After evaluation of each new class, we include their test portion into our dynamic test dataset. Since MSCOCO test set is significantly larger than test portion of new classes, to capture the variance in accuracy, we vary the ratio between number of samples from new object and number of samples from old objects. We change this ratio from 0.05 to 0.5 with the step size of 0.02. It gives a better estimation of the network\u2019s performance in a real scenario with an unknown distribution of encountering different classes.\nFigure 7 summarizes the results of this experiment. All the shown values are top 1 accuracy. As we expected, the accuracy is decreasing by adding new objects however, this drop is not significant and the slope is low. Accordingly, we can say with confidence that the accuracy stays in a usable\nrange even after adding many new objects. Also note that the baseline model already includes 80 common objects and therefore not too many additions is expected."}, {"heading": "B. Object Detection and Recognition Baseline", "text": "We can get near real-time performance from our detection and recognition system with the simplifications that was made in the networkII-B. Our model\u2019s inference time is 150ms for doing both the detection and the recognition on GeForce 960 GPU. It is more than twice as fast as R-CNN with 350ms on Titan X GPU [8]. In the object detection task, we reached Average Precision (AP) of 0.2026 which is comparable with the state of the art at 0.224 [20]. Based on AP metric, a detection is counted correct if it has IoU of more than 0.5 with the ground truth. We measured baseline model top-1 accuracy for recognition. The prediction is correct if the class with the highest probability is similar to the ground truth. We achieved recognition\naccuracy of 0.45. We are not aware of any recognition performance reported on MS-COCO. But considering the difficulty of MS-COCO compared to imagenet, an accuracy close to this value is expected. The total accuracy of the model is 0.1704. To compute this accuracy we use the precision metric as follow. A prediction is counted correct if it satisfies both localization with IoU greater than 0.5 and also recognizes the object."}, {"heading": "C. Our Incremental Learning Approach", "text": "To make our approach verifiable by researchers, we tested our incremental learning system using publicly available datasets. In this experiment, we started with our baseline model trained on MS-COCO and then we incrementally added new classes to the model. These new classes are randomly chosen from imagenet [11]. We followed the same procedure for evaluation as the first experiment IVA with one exception that now, the labeled data for new object comes from imagenet rather than HRI. The results are summarized as the same fashion in Fig. 8 We can also compare Fig.7 and Fig.8. We can observe a slight decline in performance from the robot\u2019s collected data and imagenet data. It can be contributed to the diversity of the images in imagenet and therefore, decreasing the chance of over-fitting. However, this small decline proves that the images that the robot has collected works nearly as good as a generic dataset, at least for local use which is our intended purpose."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this work, we introduced a novel paradigm for incrementally improving visual perception of a robot through active interactions with humans. We showed how the state of the art in object detection and recognition can be used as a\nbase visual perception module. Then, a method for gradually improving the base knowledge is implemented that relies on human guidance. To demonstrate the feasibility of the proposed system, a complete human-robot interface is developed that facilitates natural interaction with humans. The usage of the system in real-world situation (an assistant robot in an electronics workshop) is shown and its performance was measured after consecutive additions to it perception module. Even though we are closely following the state of the art in object detection and recognition, their performance still needs improvement. One factor that it is not considered greatly by the vision community, is the continuity of the image stream. We expect that using temporal information of bounding box locations and their labels improves the detection and recognition accuracy. Our system made it possible for users to define new classes of object that could be recognized later. This allows us to also have specific task defined for these classes and use them to manipulate objects or perform actions with them."}], "references": [{"title": "Towards open world recognition", "author": ["Abhijit Bendale", "Terrance Boult"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "The Festival Speech Synthesis System: System documentation", "author": ["Alan W. Black", "Paul A. Taylor"], "venue": "Technical Report HCRC/TR-83, Human Communciation Research Centre,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Online passive-aggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Visual task specification interface for manipulation with uncalibrated visual servoing", "author": ["Mona Gridseth", "Oscar Ramirez", "Camilo Perez Quintero", "Martin Jagersand. Vita"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Pn learning: Bootstrapping binary classifiers by structural constraints", "author": ["Zdenek Kalal", "Jiri Matas", "Krystian Mikolajczyk"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Strategies for human-in-the-loop robotic grasping", "author": ["Adam Eric Leeper", "Kaijen Hsiao", "Matei Ciocarlie", "Leila Takayama", "David Gossow"], "venue": "In Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Incremental and decremental support vector machine learning", "author": ["T Poggio", "G Cauwenberghs"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "The darpa robotics challenge [competitions", "author": ["Gill Pratt", "Justin Manzo"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Ros: an open-source robot operating system", "author": ["Morgan Quigley", "Ken Conley", "Brian Gerkey", "Josh Faust", "Tully Foote", "Jeremy Leibs", "Rob Wheeler", "Andrew Y Ng"], "venue": "In ICRA workshop on open source software,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Sepo: Selecting by pointing as an intuitive human-robot command interface", "author": ["Camilo Perez Quintero", "Romeo Tatsambon Fomena", "Azad Shademan", "Nina Wolleb", "Travis Dick", "Martin Jagersand"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Vibi: Assistive vision-based interface for robot manipulation", "author": ["Camilo Perez Quintero", "Oscar Ramirez", "Martin J\u00e4gersand"], "venue": "In 2015 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Visual pointing gestures for bi-directional human robot interaction in a pick-and-place task", "author": ["Camilo Perez Quintero", "Romeo Tatsambon", "Mona Gridseth", "Martin J\u00e4gersand"], "venue": "In Robot and Human Interactive Communication (RO-MAN),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Faster rcnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Toward open set recognition", "author": ["Walter J Scheirer", "Anderson de Rezende Rocha", "Archana Sapkota", "Terrance E Boult"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Probability models for open set recognition", "author": ["Walter J Scheirer", "Lalit P Jain", "Terrance E Boult"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Analysis of human-robot interaction at the darpa robotics challenge trials", "author": ["Holly A Yanco", "Adam Norton", "Willard Ober", "David Shane", "Anna Skinner", "Jack Vice"], "venue": "Journal of Field Robotics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "[25] conducted a study during the recent DARPA robotics challenge [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[25] conducted a study during the recent DARPA robotics challenge [15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "guidance is used whenever the robot cannot make a decision on its own [7], [12], [18].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "guidance is used whenever the robot cannot make a decision on its own [7], [12], [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "guidance is used whenever the robot cannot make a decision on its own [7], [12], [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "[23] was one of the earliest attempts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Therefore, more recent attempts [20] [8] utilized Region Proposal Networks (RPN) to", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Therefore, more recent attempts [20] [8] utilized Region Proposal Networks (RPN) to", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "In the literature these problems are addressed by Incremental Machine Learning (IML) [6], [14] and Open", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "In the literature these problems are addressed by Incremental Machine Learning (IML) [6], [14] and Open", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Set Recognition(OSR) [4], [21], [22].", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "Set Recognition(OSR) [4], [21], [22].", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Set Recognition(OSR) [4], [21], [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "We used first 30 layers of vgg-16 [24] (counting pooling and activation layers), trained on image-net [11] for the first part of the network.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "We used first 30 layers of vgg-16 [24] (counting pooling and activation layers), trained on image-net [11] for the first part of the network.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "The structure of our localization network is based on the Densecap [8] which in turn is a modified version of the Faster RCNN [20].", "startOffset": 67, "endOffset": 70}, {"referenceID": 16, "context": "The structure of our localization network is based on the Densecap [8] which in turn is a modified version of the Faster RCNN [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "For the base training of our model, we used Microsoft COCO dataset [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Components from left to right, Conv Net: first 30 layers of VGG16 [24]", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Localization Net: Object proposal and detection network based on Densecap [8].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "01 deviation [11].", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "As for the optimizer, we use ADAM [10] due to its easy tuning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "4, all modules are fully integrated with ROS [16].", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "\u2022 The speech synthesis module relies on the Festival speech synthesis system [5] and provides feedback to the human in a verbal channel.", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "\u2022 The gesturing module is based on our previous work [17], [19], where we proposed a non-verbal robot-vision system capable of inferring human pointing and perform simple pick and place tasks based on human gesture", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "\u2022 The gesturing module is based on our previous work [17], [19], where we proposed a non-verbal robot-vision system capable of inferring human pointing and perform simple pick and place tasks based on human gesture", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "lection and pick-up object actions [19].", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "the collection state, a TLD tracker [9] is used to guarantee the cropping of the object during the data collection (See Fig.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "with 350ms on Titan X GPU [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "224 [20].", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "randomly chosen from imagenet [11].", "startOffset": 30, "endOffset": 34}], "year": 2017, "abstractText": "Scene understanding and object recognition is a difficult to achieve yet crucial skill for robots. Recently, Convolutional Neural Networks (CNN), have shown success in this task. However, there is still a gap between their performance on image datasets and real-world robotics scenarios. We present a novel paradigm for incrementally improving a robot\u2019s visual perception through active human interaction. In this paradigm, the user introduces novel objects to the robot by means of pointing and voice commands. Given this information, the robot visually explores the object and adds images from it to re-train the perception module. Our base perception module is based on recent development in object detection and recognition using deep learning. Our method leverages state of the art CNNs from off-line batch learning, human guidance, robot exploration and incremental on-line learning.", "creator": "LaTeX with hyperref package"}}}