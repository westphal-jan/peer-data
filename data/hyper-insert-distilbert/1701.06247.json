{"id": "1701.06247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "A Multichannel Convolutional Neural Network For Cross-language Dialog State Tracking", "abstract": "concluding the fifth dialog state recognition tracking validation challenge ( dstc5 ) introduces a new cross - language dialog conversion state tracking scenario, where visually the participants are asked roughly to build their homogeneous trackers based substantially on the english translit training corpus, while regularly evaluating calculating them manually with the unlabeled chinese corpus. although the resultant computer - generated software translations for both english turkish and chinese japanese corpus they are generally provided in the dataset, however these coded translations contain processing errors and careless chinese use usage of them can nonetheless easily hurt improving the performance of applying the built immersion trackers. to address potentially this logistical problem, sometimes we propose combining a multichannel convolutional neural networks ( cnn ) architecture, which in which we treat english standard and thai chinese / language as different input channels of one single language cnn model. in following the 2001 evaluation manual of dstc5, we formally found that such strong multichannel architecture adaptations can effectively improve the robustness against translation errors. well additionally, maybe our optimal method for developing dstc5 is relatively purely machine named learning models based nowadays and ideally requires ultimately no prior knowledge about the closest target arabic language. we consider this a desirable property for building choosing a tracker pattern in the cross - language testing context, as maybe not presently every computational developer will be solely familiar with working both languages.", "histories": [["v1", "Mon, 23 Jan 2017 01:36:10 GMT  (179kb)", "http://arxiv.org/abs/1701.06247v1", "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken Language Technology (SLT 2016)"]], "COMMENTS": "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken Language Technology (SLT 2016)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["hongjie shi", "takashi ushio", "mitsuru endo", "katsuyoshi yamagami", "noriaki horii"], "accepted": false, "id": "1701.06247"}, "pdf": {"name": "1701.06247.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hongjie Shi", "Takashi Ushio", "Mitsuru Endo", "Katsuyoshi Yamagami"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n06 24\n7v 1\n[ cs\n.C L\n] 2\n3 Ja\nn 20\n17 Published as a conference paper at IEEE SLT 2016\nIndex Terms\u2014 Convolutional neural networks, multichannel architecture, dialog state tracking, dialog systems"}, {"heading": "1. INTRODUCTION", "text": "Dialog state tracking is one of key sub-tasks of dialog management, whose goal is to transfer human utterances into a slot-value representation (dialog state) that is easy for computer to process and track the information that appeared in the dialog. To provide a common testbed for this task, the series of Dialog State Tracking Challenges (DSTC) was initiated [1]. This challenge has already been held for four times, during which it provided a very valuable shared recourse for the research in this field and helped to improve the state-of-theart. Since the forth challenge (DSTC4 2015), the target of dialog state tracking has been shifted from human-machine dialog to human-human dialog, which significantly increases the difficulty of the dialog state tracking task because of the variety and ambiguity in the human-human dialog. One lesson we learned from DSTC4 is the difficulty of building a high performance tracker for human-human dialog with very limited training corpus, no matter whether using machine learning or\nhand-crafted rule-based approaches [2, 3].This is a very unfavorable situation because building hand-annotated training corpus is very expensive, time-consuming and requires human experts. Not to mention the collection of a new corpus for each language other than English if we need to build trackers for a new language.\nThe DSTC5 proposed a new challenge based on using the rapidly advancing machine translation (MT) technology, one may be able to adapt the built tracker to a new language with limited training or development corpus in that language. We find this idea very attractive because not only it can reduce the cost of new language adaptation, but also it provides the possibility of building a tracker with cross-language corpus. For example it can be very useful for developing the tourist information systems because one may have corpus collected from different language speakers (i.e. tourists from different countries): for each language, the amount of corpus may be very limited, but together it can be large enough for a good training. On the other hand, although the machine translation technology has achieved great progress recently, the translation quality is still not satisfactory [4]. A conventional monolingual tracker trained on the computer-generated translations may lead to an imperfect model, and it can only accept the translations from other languages as input which will also degrade the performance.\nTo address these problems, we propose a model that can be trained with different languages at the same time, and use both original utterances and their translations as input source for the dialog state tracking. In such way, we can avoid building the tracker only based on computer-generated translations, and maximize the use of all possible input languages to increase the robustness to translation errors. This paper is organized as follows. Sect.2 briefly describes the dataset and the dialog state tracking problem; Sect.3 presents an overview of our method and explains in detail our multichannel CNN model. Sect.4 presents the evaluation results with analysis and discussion. Sect.5 concludes our work and proposes future improvements."}, {"heading": "2. DATASET AND PROBLEM DESCRIPTION", "text": "The fifth Dialog State Tracking Challenge (DSTC5) uses the whole dataset (including train/dev/test datasets) from the\nCopyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken Language Technology (SLT 2016).\nDSTC4 as the training dataset. This dataset contains 35 dialog sessions on tourist information for Singapore collected from English speakers. Besides the training dataset, a development set which includes 2 dialog sessions collected from Chinese speakers is provided for testing and tuning the trackers\u2019 cross-language performance before the final evaluation. Both the training and development sets are labelled with the dialog state tags and come with 5-best hypothesis of English or Chinese translations by a machine translation systems. In the evaluation phase of the challenge, a Test set including 8 unlabeled Chinese dialogs is distributed to each participant, and all prediction results submitted by each participant are evaluated by comparing with the true labels. The test dataset also includes 5-best English translations which are generated by the same machine translation system as the training/development dataset.\nThe dialog state in this challenge is defined by the same ontology used in DSTC4, which contains 5 topic branches with different slot sets (Table2). These topic-slot combinations indicate the most important information mentioned in that topic, for example the \u2018CUISINE\u2019 slot under the topic of \u2018FOOD\u2019 refers to the cuisine types, while the \u2018STATION\u2019 slot for the topic of \u2018TRANSPORTATION\u2019 refers to the train stations. In total there are 30 such topic-slot combinations, and all possible values for each topic-slot are given as a list in the ontology. The main task of DSTC5 is to predict the proper value(s) for each slot given the current utterance and its topic, with all dialog history prior to that turn (e.g. Table1)."}, {"heading": "3. METHOD", "text": "In DSTC4 we proposed a method which is based on the convolutional neural networks originally proposed by Kim [5]. By this method we were able to achieve the best performance for tracking the INFO slot. The CNN model we used in this method was modified from the origin by adding a structure of multi-topic convolutional layer, so that it can better handle the information presented in different dialog topics. This model is characterized by its high performance for limited training data, because it can be trained across various topics. More details about this multi-topic model can be found in [2].\nIn DSTC5 the training data is 75% more than DSTC4, therefore the situation of limited training data is improved. In order to focus more on the new cross-language problem and keep our method simple, instead of using the more complex multi-topic model we proposed last time, we trained individual CNN model for each slot-topic combination. That is, for example the \u2018INFO\u2019 slot in the topic of \u2018FOOD\u2019 and the same \u2018INFO\u2019 slot in the topic of \u2018SHOPPING\u2019 will be trained by two independent models. This is the major difference from the last time, where we trained one single model for all topics. With this new scheme, we can set the hyperparameters in each model for each slot/topic to be exactly the same, so that our method is scalable, universally applicable and easy to tune. Fig 1 is a simple diagram illustrating our method."}, {"heading": "3.1. Motivation", "text": "The biggest challenge of DSTC5 is that the training and test corpora are originally collected in different languages. Since\nboth computer-generated Chinese and English translation are provided in the training and test dataset respectively, one straightforward approach is to train a model with English corpus and use it for the English translation in the test data. Alternatively, a model trained on Chinese translations in the training dataset can be used for Chinese utterances in the test data. However, both methods will waste the originally collected utterances either in the training or the test data. In order to fully utilize the corpus resource in both English and Chinese languages, we proposed the following multichannel model which can be regarded as a combination of both English and Chinese models."}, {"heading": "3.2. Model architecture", "text": "Our model is inspired by the multichannel convolutional neural networks commonly used in the image processing [6]. Instead of RGB channels used for color images, we apply each input channel to each different language source. In this model, the input of each channel is a two dimensional matrix, each row of which is the embedding vector of the corresponding word:\ns =\n\n   \n\u2014 w1 \u2014 \u2014 w2 \u2014\n... \u2014 wn \u2014\n\n    , (1)\nwhere wi \u2208 Rk is the embedding vector for the i-th word in the input text. This 2-dimensional array s is a matrix representation of the input text. We used three different word embedding in our model \u2014 two for Chinese and one for English. The details of these embedding will be explained later in the Sect. 3.3. For each channel, a feature map h \u2208 Rn\u2212d+1 is obtained by convolving a trainable filter m \u2208 Rd\u00d7k with the embedding matrix s \u2208 Rn\u00d7k using the following equation:\nh = f(m \u2217 s+ b). (2)\nHere f is a non-linear activation function1; \u2217 is the convolution operator and b = (b, . . . , b) \u2208 Rn is a bias term. The maximum value of this feature map h\u0302 = max{h} is then selected by the max-pooling layer. This is the process how one filter extracts one most important feature from the input matrix. In this model, multiple filters are used in each channel to extract multiple features. These features then form the pooling layer which are passed to a fully connected layer for prediction.\nThe idea of multichannel model is to connect those extracted features from different channels before the final output, so that the model can use richer information obtained from different channels. The fully connect layer in multichannel model follows the equation:\ny = S ( w \u00b7 (h\u0302ch1 \u2295 h\u0302ch2 \u2295 h\u0302ch3) + b ) , (3)\n1We used rectified linear unit (ReLU) for this activation function.\nwhere S is the sigmoid function; \u2295 is the concatenation operator and h\u0302chn = (h\u03021, . . . , h\u0302m) is the penultimate layer of the n-th channel.\nNotice that in the original paper of Kim, a multi-channel architecture has also been proposed. The main difference between our model and their model is that we use different sets of filters for each channel, while in their model the same filter set are applied to all channels. The reason for this modification is that the word embedding for different languages can\nvary greatly, for example the same (or nearly the same) embedding vector in different language models may correspond to irrelevant words with very different meanings. Using different sets of filters ensures that proper features can be extracted in each channel no matter how the word embedding varies among different languages."}, {"heading": "3.3. Embedding models", "text": "The word2vec [7] is one of the most common methods for producing word embeddings. In DSTC5, we applied this method and trained three different models with different training corpus. The details of these models are listed as below:\n1. English word model: 200-dimension word2vec model trained on English Wikipedia, with all text split by space and all letters lowercased. This model contains 253854 English words.\n2. Chinese word model: 200-dimension word2vec model trained on Chinese Wikipedia, with all text split by word boundary using \u2018jieba\u2019 module2. This model contains 457806 Chinese words and 53743 English words appeared in the Chinese Wikipedia.\n3. Chinese character model: 200-dimension word2vec model trained on Chinese Wikipedia with all text split into single Chinese character. This model contains 12145 Chinese characters and 53743 English words appeared in the Chinese Wikipedia.\nThe reason why we trained two models for Chinese language is because identifying word boundaries in Chinese is not a trivial task. For Chinese, the smallest element with meaning (word) varies from one single Chinese character to several concatenated Chinese characters, and the task for Chinese word splitting usually involves parsing the sentence and the state-of-the-art method still cannot achieve perfect accuracy. For this reason the Chinese word model may contain incorrect vocabularies and is not capable of handling unseen Chinese character combinations. On the other hand, the Chinese character model does not rely on word segmentation so that the model is error-free, and also it can easily deal with unseen words. However, since the Chinese character model ignores the word boundaries, the resulting embedding vector may not be able to reflect the precise meaning of each word."}, {"heading": "4. RESULTS", "text": "The results of the proposed method along with the scores of other teams are shown in the Table 3. Our multichannel CNN model achieves the best score among all 9 teams: the result of entry-3 outperforms the second best team by 50%\n2https://github.com/fxsjy/jieba\n(0.0956/0.0635) in Accuracy and 15% (0.4519/0.3945) in Fmeasure with the sub-dialog evaluation. Our submitted five entries are the results of 5 different hyperparameters settings which are determined by a rough grid search3, and those settings are summarized in Table 4. Compared these results with each other, one can easily tell that among these hyperparameters the dropout rate is a key factor. The dropout is known as a technique for reducing overfitting in neural networks [9], and in our case reducing the dropout rate always improves the Precision while degrading the Recall score. One explanation for this is that an over-fitted model only outputs the same labels for the data which are very similar to the training data, and therefore decreases its generalization to unseen data. On the other hand, further decreasing the dropout rate does not improve the overall performance, whose results and parameter settings are also shown in the table as \u2018Additional Expt. #5&6\u2019."}, {"heading": "4.1. Multichannel model & single channel model & model combination", "text": "To investigate by how much the proposed multichannel architecture contributes to these results, we compared the performance between the multichannel and ordinary single channel CNN models. For this comparison, we trained three different monolingual single channel CNN models using each of the embedding models mentioned in Section3.3. These models used the same parameter setting as \u2018multichannel #3\u2019 in the Table 4, and were trained only on the 1-best machine translation results. Fig.4 shows the comparing results: the Chinese character model achieves the best overall accuracy among single channel models, while the multichannel model outperforms all three single channel models.\nIn the earlier DSTC, a simple model combination technique has been used to further improve the predictive performance, where the final output is computed by averaging the scores output by different models [10]. We also applied this method to combine the output from all three single channel models, and the result is also shown in the Fig.4. This simple model combination method does not perform as good as the multichannel model, but considering its simplicity, we still consider it as a good alternative to improve the performance."}, {"heading": "4.2. Discussion", "text": "We think the above results can be partially explained from the point of view of ensemble learning. In a multichannel model, each channel provides a different view of the data, and an\nexample is described using different feature sets that provide different, complementary information about the instance. The fully connected layer in the multichannel model further provides an optimization to use this information for the prediction, and therefore the resulting model can in principle better deal with the translation errors appeared in different channels.\nTable 5 is one of the examples that demonstrate this idea. In this particular sub-dialog segment, none of the 3 single channel models is able to output the correct labels, while the multichannel model gives the correct prediction. As seen in this example, the model combination behaves like a simple voting, which means it only picks up the labels that are supported by majority of the single channel models. The multichannel model, on the other hand, is able to selectively choose which language source to trust more for each particular slot or value. As a result, the label of \u2018Walking\u2019 is correctly predicted despite it only appearing in the English model\u2019s output, while the \u2018Exhibit\u2019 label is correctly rejected even though it is supported by two single channel models out of three.\nHowever the real situation is more complex. When we look at the overall predictive accuracy for each slot (Fig.5), we can find that the performance for each model varies on slots. We consider this is due to the ambiguity caused by machine translation which varies on different subjects. For example, as a time expression in English, 96% of the word \u201cevening\u201d and 43% of the word \u201cnight\u201d are translated into\nthe same Chinese word \u201cwan shang\u201d. Although this Chinese word does have both meanings of \u201cevening\u201d and \u201cnight\u201d in Chinese, there are more precise Chinese terms representing each word. This English to Chinese translation ambiguity immediately increases the difficulty of identifying the values of EVENING and NIGHT in Chinese, which leads to the poor performance of the Chinese model in the slot of TIME.\nAnother problem is that the translation quality often varies by reversing the translation direction, due to the difference in inflections, word order and grammars [11]. Since the training corpus only contains one translation direction (English to Chinese), the multichannel model is by no means optimized for the reverse translation direction. This may cause the multichannel model to have bias on certain channels, and it can explain why in certain slots the model combination that treats each channel equally works better. A more sophisticated way to train our multichannel model should be firstly training the model with one translation direction and then fine-tuning the model with the other. Unfortunately this is difficult in DSTC5 because the development dataset that can be used for the fine tuning is too limited."}, {"heading": "5. CONCLUSION", "text": "We proposed a multichannel convolutional neural network, in which we treat multiple languages as different input channels. This multichannel model is found to be robust against the translation errors and outperforms any of the single channel models. Furthermore, our method does not require prior knowledge about new languages, and therefore can be easily applied to available corpus resources of different languages. This not only can reduce the cost for the adaption to a new language, but also offers the possibility to build multilingual dialog state trackers with large-scale cross-language corpora.\nIn this work we applied three different embedding models, while there is one more we did not try \u2014 the English character model. There are several character-aware language models proposed recently, which are superior in dealing with subword information, rare words and misspelling [12, 13]. We believe that integrating them into the multichannel model is a promising research direction.\nOn the other hand, since our method is purely machine learning based, it cannot handle unseen labels in the test data. This is a very important issue especially for a large ontology, because of the difficulty in obtaining large training corpus that covers all concepts. To overcome this disadvantage, future work should include combining machine learning with other approaches, such as hand-craft rules, data argumentation and so on."}, {"heading": "6. REFERENCES", "text": "[1] Seokhwan Kim, Luis Fernando D\u2019Haro, Rafael E. Banchs, Jason Williams, Matthew Henderson, and\nKoichiro Yoshino, \u201cThe Fifth Dialog State Tracking Challenge,\u201d in Proceedings of the 2016 IEEE Workshop on Spoken Language Technology (SLT), 2016.\n[2] Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi Yamagami, and Noriaki Horii, \u201cConvolutional neural networks for multi-topic dialog state tracking,\u201d Proceedings of the 7th International Workshop on Spoken Dialogue Systems (IWSDS), 2016.\n[3] Franck Dernoncourt, Ji Young Lee, Trung H Bui, and Hung H Bui, \u201cRobust dialog state tracking for large ontologies,\u201d arXiv preprint arXiv:1605.02130, 2016.\n[4] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[5] Yoon Kim, \u201cConvolutional neural networks for sentence classification,\u201d arXiv preprint arXiv:1408.5882, 2014.\n[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems, 2012, pp. 1097\u20131105.\n[7] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean, \u201cEfficient estimation of word representations in vector space,\u201d arXiv preprint arXiv:1301.3781, 2013.\n[8] Ye Zhang and Byron Wallace, \u201cA sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification,\u201d arXiv preprint arXiv:1510.03820, 2015.\n[9] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv preprint arXiv:1207.0580, 2012.\n[10] Matthew Henderson, Blaise Thomson, and Steve Young, \u201cWord-based dialog state tracking with recurrent neural networks,\u201d in Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 2014, pp. 292\u2013299.\n[11] David Vilar, Jia Xu, Luis Fernando dHaro, and Hermann Ney, \u201cError analysis of statistical machine translation output,\u201d in Proceedings of LREC, 2006, pp. 697\u2013702.\n[12] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush, \u201cCharacter-aware neural language models,\u201d arXiv preprint arXiv:1508.06615, 2015.\n[13] Xiang Zhang, Junbo Zhao, and Yann LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657."}], "references": [{"title": "The Fifth Dialog State Tracking Challenge", "author": ["Seokhwan Kim", "Luis Fernando D\u2019Haro", "Rafael E. Banchs", "Jason Williams", "Matthew Henderson", "Koichiro Yoshino"], "venue": "Proceedings of the 2016 IEEE Workshop on Spoken Language Technology (SLT), 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural networks for multi-topic dialog state tracking", "author": ["Hongjie Shi", "Takashi Ushio", "Mitsuru Endo", "Katsuyoshi Yamagami", "Noriaki Horii"], "venue": "Proceedings of the 7th International Workshop on Spoken Dialogue Systems (IWSDS), 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust dialog state tracking for large ontologies", "author": ["Franck Dernoncourt", "Ji Young Lee", "Trung H Bui", "Hung H Bui"], "venue": "arXiv preprint arXiv:1605.02130, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 2014, pp. 292\u2013299.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Error analysis of statistical machine translation output", "author": ["David Vilar", "Jia Xu", "Luis Fernando dHaro", "Hermann Ney"], "venue": "Proceedings of LREC, 2006, pp. 697\u2013702.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To provide a common testbed for this task, the series of Dialog State Tracking Challenges (DSTC) was initiated [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "One lesson we learned from DSTC4 is the difficulty of building a high performance tracker for human-human dialog with very limited training corpus, no matter whether using machine learning or hand-crafted rule-based approaches [2, 3].", "startOffset": 227, "endOffset": 233}, {"referenceID": 2, "context": "One lesson we learned from DSTC4 is the difficulty of building a high performance tracker for human-human dialog with very limited training corpus, no matter whether using machine learning or hand-crafted rule-based approaches [2, 3].", "startOffset": 227, "endOffset": 233}, {"referenceID": 3, "context": "On the other hand, although the machine translation technology has achieved great progress recently, the translation quality is still not satisfactory [4].", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "In DSTC4 we proposed a method which is based on the convolutional neural networks originally proposed by Kim [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "More details about this multi-topic model can be found in [2].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "Our model is inspired by the multichannel convolutional neural networks commonly used in the image processing [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "The word2vec [7] is one of the most common methods for producing word embeddings.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "The dropout is known as a technique for reducing overfitting in neural networks [9], and in our case reducing the dropout rate always improves the Precision while degrading the Recall score.", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "3A guide for setting these hyperparameters can be found in [8] 4These are the evaluation results using \u2018Schedule 2\u2019 described in the challenge handbook [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "3A guide for setting these hyperparameters can be found in [8] 4These are the evaluation results using \u2018Schedule 2\u2019 described in the challenge handbook [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "In the earlier DSTC, a simple model combination technique has been used to further improve the predictive performance, where the final output is computed by averaging the scores output by different models [10].", "startOffset": 205, "endOffset": 209}, {"referenceID": 10, "context": "Another problem is that the translation quality often varies by reversing the translation direction, due to the difference in inflections, word order and grammars [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "There are several character-aware language models proposed recently, which are superior in dealing with subword information, rare words and misspelling [12, 13].", "startOffset": 152, "endOffset": 160}, {"referenceID": 12, "context": "There are several character-aware language models proposed recently, which are superior in dealing with subword information, rare words and misspelling [12, 13].", "startOffset": 152, "endOffset": 160}], "year": 2017, "abstractText": "The fifth Dialog State Tracking Challenge (DSTC5) introduces a new cross-language dialog state tracking scenario, where the participants are asked to build their trackers based on the English training corpus, while evaluating them with the unlabeled Chinese corpus. Although the computer-generated translations for both English and Chinese corpus are provided in the dataset, these translations contain errors and careless use of them can easily hurt the performance of the built trackers. To address this problem, we propose a multichannel Convolutional Neural Networks (CNN) architecture, in which we treat English and Chinese language as different input channels of one single CNN model. In the evaluation of DSTC5, we found that such multichannel architecture can effectively improve the robustness against translation errors. Additionally, our method for DSTC5 is purely machine learning based and requires no prior knowledge about the target language. We consider this a desirable property for building a tracker in the cross-language context, as not every developer will be familiar with both languages.", "creator": "LaTeX with hyperref package"}}}