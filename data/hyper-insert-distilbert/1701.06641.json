{"id": "1701.06641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "Perceptually Optimized Image Rendering", "abstract": "we develop precisely a generalized framework for rendering photographic images, besides taking into account display spectral limitations, purely so as to optimize perceptual processing similarity between the approximate rendered 2d image scales and selecting the partially original scene. internally we formulate this program as a vertically constrained optimization problem, in which formulation we minimize uniformly a cumulative measure of horizontal perceptual temporal dissimilarity, fitting the normalized laplacian pyramid spatial distance ( nlpd ), to which probably mimics the early stage transformations of the human centered visual system. when rendering images acquired with higher dynamic cost range than beyond that projection of the resultant display, we subsequently find that the purely optimized solution boosts the contrast of underlying low - energy contrast features without introducing either significant artifacts, inevitably yielding results of offering comparable visual quality opposite to current state - of - the art photography methods with no manual numerical intervention model or intrinsic parameter settings. unfortunately we sometimes also formally examine a useful variety requirements of feasible other image display selection constraints, including strict limitations laid on minimum lighting luminance ( particularly black point ), mean dark luminance ( as has a useful proxy algorithm for energy \u2010 consumption ), and quantized mean luminance levels ( halftoning ). applying finally, equations we similarly show concluded that the method may be used to efficiently enhance details and resolve contrast functions of images degraded uniformly by optical scattering ( e. ex g. fog ).", "histories": [["v1", "Mon, 23 Jan 2017 21:38:52 GMT  (3851kb,D)", "http://arxiv.org/abs/1701.06641v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.GR", "authors": ["valero laparra", "alex berardino", "johannes ball\\'e", "eero p simoncelli"], "accepted": false, "id": "1701.06641"}, "pdf": {"name": "1701.06641.pdf", "metadata": {"source": "CRF", "title": "Perceptually Optimized Image Rendering", "authors": ["Valero Laparra", "Alex Berardino", "Johannes Ball\u00e9", "Eero P. Simoncelli"], "emails": ["valero.laparra@uv.es"], "sections": [{"heading": null, "text": "We develop a framework for rendering photographic images, taking into account display limitations, so as to optimize perceptual similarity between the rendered image and the original scene. We formulate this as a constrained optimization problem, in which we minimize a measure of perceptual dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics the early stage transformations of the human visual system. When rendering images acquired with higher dynamic range than that of the display, we find that the optimized solution boosts the contrast of low-contrast features without introducing significant artifacts, yielding results of comparable visual quality to current state-of-the art methods with no manual intervention or parameter settings. We also examine a variety of other display constraints, including limitations on minimum luminance (black point), mean luminance (as a proxy for energy consumption), and quantized luminance levels (halftoning). Finally, we show that the method may be used to enhance details and contrast of images degraded by optical scattering (e.g., fog)."}, {"heading": "1 Introduction", "text": "A general goal in designing a pipeline for the capture and display of photographic images is to remain as faithful to the original source as possible, minimizing distortions introduced by the sensor, coding, transmission, or display processes. If images are meant for presentation to human observers, distortion should be measured accordingly, penalizing errors that are most visually noticeable and/or disturbing, while permitting those that are perceptually unnoticeable. This strategy is most evident in the handling of color, in which both sensors and displays are designed so as to capture and render the three-dimensional subspace of wavelengths relevant for human trichromatic visual representation, while allowing significant distortion outside of this subspace.\nArguably the most significant limitations of current sen-\nsors and displays are with regard to dynamic range. Early sensors were restricted to capturing a limited luminance range, and were unable to adequately capture the majority of realistic natural scenes, which contain luminances spanning many orders of magnitude. In contrast, the human visual system is capable of sensing fixed scenes with a range of over 5 orders of magnitude in real time, and up to 8 orders of magnitude when the effects of extended temporal adaptation mechanisms are incorporated [12] (see Fig. 1). The dynamic range of sensors has steadily improved, and current sensors (often augmented with software solutions that fuse images captured at different exposures) are capable of acquiring images with luminance ranges approximating those of human vision. Despite this, even our best display devices are limited to a significantly lower dynamic range than these sensors can capture.\nThe simplest solution to the problem of displaying high dynamic range (HDR) images on a low dynamic range (LDR) rendering device is to linearly rescale the luminance values recorded by the sensor into the display\u2019s reproducible range of luminances. This, however, produces images that look nothing like the original scene - typically all of the low-luminance information is lost. A variety of \"tone-mapping\" methods have been proposed to solve this problem by nonlinearly remapping the intensities of the original image into the output range, in a way that remains faithful to the visual appearance of the original scene [29]. Most of these are based on heuristics, and require manual adjustment of parameters for best results. In addition, many display applications introduce constraints other than global luminance range, such as discrete luminance levels (i.e. halftoning), average energy consumption, as well as interactions between pixel values over space or time. Separate methods have been developed for solving each of these problems.\nHere, we formulate the general problem of perceptuallyaccurate display rendering as a constrained optimization problem, optimizing the rendered image to minimize perceptual differences from the original, subject to any constraints imposed by the display (Fig. 1). The formulation relies on four ingredients: knowledge of the conditions of\nar X\niv :1\n70 1.\n06 64\n1v 1\n[ cs\n.C V\n] 2\n3 Ja\nn 20\nimage acquisition, knowledge of the display constraints, a definition of perceptual similarity between images, and a method for optimizing the image to be rendered. In the next section, we combine these ingredients in a simple optimization problem. Because we choose a model of perceptual similarity that is continuous and differentiable, it can be efficiently solved by first-order constrained optimization techniques. We show that the solution is well defined and general, and therefore represents a framework for solving a wide class of rendering problems. Here, we restrict ourselves to grayscale images (without chromatic components) since the original perceptual model was designed for achromatic images. We show one result per experiment, more images can be found at our web page.1\nIn section 3, we optimize images captured under differing acquisition conditions for rendering on the same display. We start with calibrated images, where the original scene luminances are known. We also deal with the more common scenario in which the exact luminances of the original scene are unknown (the tone mapping problem). In this scenario, we have to make some educated guesses about the physical conditions of the original scene. We demonstrate the effect that different starting assumptions have on the optimized images. Moreover, we take advantage of these effects to solve other image processing problems, such as detail enhancement and haze removal, by manipulating these source assumptions. For each of these tasks, we compare the results with state-of-the-art algorithms designed to solve each specific case. In section 4,\n1 http://www.cns.nyu.edu/~lcv/perceptualRendering/\nwe optimize images to be displayed under differing display restrictions, including luminance limited displays, energy limited displays, and displays restricted to a small set of output values. Finally, we analyze the effect that each component of our perceptual measure has on the quality of our optimized images."}, {"heading": "2 Optimal rendering framework", "text": "Optimally rendering an image, I, on a given display means displaying it in such a way that it remains faithful to the human perception of the original scene, S. We formalize this as a constrained optimization problem:\nI\u0302 = argmin I\nD(S, I), s.t. I \u2208 C, (1)\nwhere D(\u00b7, \u00b7) is a measure of human perceptual dissimilarity, and C is the set of all images that can be rendered on the display. This formulation can express many well-known rendering problems, such as tone mapping or dithering, which differ only in the specification of C. In general, the optimization problem expressed in Eq. (1) cannot be solved analytically, and thus we will not obtain an explicit function to compute I\u0302, given S and C. Rather than assume a functional form for this mapping, we choose a perceptual measure that is differentiable with respect to I, and use modern high-dimensional optimization tools to numerically solve for the optimal I\u0302. Specifically, we descend the objective function, alternating between minimizing the perceptual distance, and projecting the image back onto the constraint set. Specific formulations for different example problems can be found online1.\nWe follow a principled, two-step approach to quantify perceptual distance. Rather than defining a perceptual distance directly (as in SSIM [30], for example), we first define a nonlinear perceptual transform f(\u00b7), which approximates the computations performed within the early stages of the human visual system. We apply this to both the original scene luminances, S, and the rendered image, I, and then measure the distance between f(S) and f(I). A perceptual distance measure constructed this way is symmetric and yields a value zero for identical images, but may not satisfy the remaining mathematical requirements of a metric. Specifically, it might also yield zero for non-identical images (if the transformation discards information), and may not satisfy the triangle inequality.2 Despite this, we refer to it casually as a metric throughout the paper.\nFigure 2 illustrates the components of the perceptual transform, for which we use the Normalized Laplacian Pyramid (NLP), a multi-scale nonlinear representation that mimics the operations of the retina and lateral geniculate nucleus in the human visual system. We have previously shown that distances measured between two images\n2I.e., it may technically be a semimetric or a premetric.\nin the perceptual space defined by the NLP are highly correlated with human judgments [15]. Here, we adapt this model to operate directly on luminances (rather than values that have been gamma-adjusted for a particular display), which allows use of the same units when defining constraints on acquisition and display. Luminances are first transformed elementwise using a power law, which approximate the transformation of light to voltage in retinal photoreceptors:\nx = S\u03b3 (2)\nThis initial nonlinear transformation is followed by a recursive partition into frequency channels, as in the Laplacian Pyramid [3]:\nx(k+1) = D ( L(x(k)) ) ,\nz(k) = x(k) \u2212L ( U(x(k+1)) ) , (3)\nwhere D(\u00b7) and U(\u00b7) indicate down/up-samping by a factor of two, respectively (figure 2). For the filtering operation L, we apply a spatially separable 5-tap filter, (0.05, 0.25, 0.4, 0.25, 0.05), as originally specified in [3].\nWithin each channel, each coefficient is divided by a weighted local sum of the element-wise amplitudes (absolute values) plus a constant:\ny(k) = z(k) ( \u03c3 + P |z(k)| ) . (4)\nwhere P indicates convolution with a filter, and indicates pointwise division. This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4]. The NLP coefficients of all channels y(k)\ncombined represent the response of the perceptual transform:\nf(S) = { y(k); k = 1, . . . , Nk } . (5)\nFigure 3 illustrates the construction of the metric employed in the perceptual space. We compute the L\u03b1-norm of the differences between NLP coefficients within each frequency channel (that is, we raise the absolute value of each coefficient difference to the power \u03b1, sum over the entire channel, and take the \u03b1th root). These values are then combined across channels using an L\u03b2-norm:\nD(S, I) =   1 Nk Nk\u2211\nk=1\n( 1\nN (k) c\nNc\u2211\ni=1\n\u2223\u2223y(k)i \u2212 y\u0303 (k) i\n\u2223\u2223\u03b1 ) \u03b2 \u03b1   1 \u03b2 , (6)\nwhere y\u0303(k)i indicates the subbands arising from the displayed image I, i.e. f(I) = { y\u0303(k); k = 1, . . . , Nk } . A similar summation model has been employed in previous perceptual quality metrics [32, 16].\nAll parameters of the perceptual transform and metric were optimized to best explain human perceptual ratings of distorted images in a public database [24]. Specifically, we chose parameters to maximize the correlation between the mean opinion scores from the human observers and the distance given by out metric. We fixed the parameters prior to using the perceptual model for the rendering results presented below. The front-end nonlinearity used an exponent \u03b3 = 12.6 . Unlike in [15], we set the normalization parameters to be identical for all bandpass channels (assuming scale-invariance), but allowed a different set for the lowpass channel. For bandpass channels, the additive constant was \u03c3 = 0.17, and the local weighting functions P were filters with 5\u00d7 5 support, with values\nP =  \n0.04 0.04 0.05 0.04 0.04 0.04 0.03 0.04 0.03 0.04 0.05 0.04 0.05 0.04 0.05 0.04 0.03 0.04 0.03 0.04 0.04 0.04 0.05 0.04 0.04\n  .\nThe parameters for the lowpass channel were P = 1 and \u03c3 = 4.86. Optimized exponents for the metric were \u03b1 = 2.0 and \u03b2 = 0.6. Appendix B shows that the performance of this extended version of the NLP metric surpasses that of state-of-the-art image quality metrics."}, {"heading": "3 Varying image acquisition conditions", "text": "We performed a set of experiments which demonstrate the flexibility of our optimization framework over different image acquisition conditions. We begin with calibrated images, for which we know the the exact luminance values (cd/m2) in the original scene. We then move on to uncalibrated images, for which we need to make an assumption about the luminance values in the original scene. Finally, we close this section by demonstrating that we can manipulate our assumptions about the original scene luminances to solve other rendering problems, such as haze removal and artificial detail enhancement.\nEach example requires us to minimize the perceptual distance with respect to the rendered image I, subject to the display constraints. For this, we use the Adaptive Moment Estimation (Adam) algorithm [13]. The derivative of the perceptual distance with respect to I is described in appendix A. The computation time scales linearly with\nthe size of the image. When optimized on a Tesla K40 GPU card, optimization took approximately 1 second for 10, 000 pixels (i.e. an image of 1000 \u00d7 1000 requires less than 2 minutes). Note that all image results presented here are inteded for viewing on a display with luminance range from 5 to 300 cd/m2, and a gamma exponent of 2.2."}, {"heading": "3.1 Rendering calibrated HDR luminances", "text": "We begin with an image S obtained from an HDR imaging device such that we know the true luminance values of all pixels. The image used here has been extracted from the database of Mark Fairchild [6], and its luminance range is Smin = 0.78 to Smax = 16200 cd/m2. Suppose further that we wish to display this image using a device with a luminance range of Imin = 5 to Imax = 300 cd/m2 (typical for many computer monitors), and that this range is far less than that of the image. We solve for the perceptually optimal rendered image:\nI\u0302 = argmin I\nD(S, I), s.t. \u2200i : Imin \u2264 Ii \u2264 Imax. (7)\nResults are shown in Fig. 4. We compare the original image intensities, linearly rescaled to fit within the luminance range [Imin, Imax], our perceptually optimized image I\u0302, and an image tone-mapped using a recent state-of-the\nart method by Paris et. al. [21]. For the latter, we have used the default parameters recommended by the authors for tone mapping of HDR images: \u03b1 = 1, \u03b2 = 0, and \u03c3 = log 2.5.\nLinearly rescaling the values creates a rendered image where most of the the details cannot be seen or differentiated. The algorithm by Paris et. al. [21] mitigates this problem, rendering an image that reveals detail in both dark and bright regions. Nevertheless, the solution appears less detailed and lower in contrast than the image computed using our method. This is mostly because the Paris algorithm does not take into account the display luminance range. Although the algorithm (and most tone-mapping algorithms) has additional parameters that can be adjusted, it is not obvious to a naive user how to select these parameters based on the display properties. In contrast, our solution is fully automatic (assuming the luminance values of the source image and the range of the display are known), albeit at the expense of significantly more computation."}, {"heading": "3.2 Rendering LDR images with an image acquisition model", "text": "Our method can also be used to improve the appearance of images acquired with a conventional low dynamic range (LDR) digital camera that has been calibrated to allow recovery of luminance values (in cd/m2) from recorded pixel values, R. For most modern digital cameras, the acquisition luminance range is still generally much larger than the display range, and in any case is unlikely to match. Thus, we need to solve the following optimization problem analogous to the previous section:\nI\u0302 = argmin I\nD(S, I), s.t. \u2200i : Imin \u2264 Ii \u2264 Imax (8)\nwhere S = g(R),\nwhere g is the mapping from recorded pixel values to estimated scene luminances.\nResults for two example grayscale images from the McGill database [20] are shown in Fig. 5. For each image, we again compare the original image intensities, linearly rescaled to fit within the luminance range [Imin, Imax], to our perceptually optimized image I\u0302, and a tone-mapped\nimage computed using the Paris et. al method. [21]. For the latter, we have again used the parameters recommended by the authors for tone mapping of HDR images: \u03b1 = 1, \u03b2 = 0, and \u03c3 = log 2.5. Our method again offers an advantage, producing higher contrast and more detailed results. The improvement here is perhaps even more noticeable than in the HDR case, for which the Paris et. al. algorithm was developed."}, {"heading": "3.3 Rendering uncalibrated HDR images", "text": "Unlike the situation in section 3.1, the typical scenario for images acquired from HDR cameras is that they are uncalibrated. That means that we have access to measurements L that are linearly related to actual luminances, but we do not have access to the scaling parameters (for instance, they might be normalized values, lying between 0 and 1). To apply our method, the measurements need to be linearly rescaled to luminance values, which amounts to knowing, estimating, or guessing the minimum and the maximum luminance in the original scene (Smin and Smax,\nrespectively). One can often use an educated guess for those values given the content of the image \u2013 for instance, the luminance of a filament of a clear incandescent lamp is roughly 106 cd/m2. As in the previous experiments, we solve the resulting optimization problem:\nI\u0302 = argmin I\nD(S, I), s.t. \u2200i : Imin \u2264 Ii \u2264 Imax (9)\nwhere S = (Smax \u2212 Smin) \u00b7L+ Smin.\nFigure 6 shows the results for the classical HDR image \u201cMemorial\u201d for different chosen values of Smax (and a fixed value of Smin = 0.01). The proposed method converges on an image which exhibits enhanced contrast in all the regions, preserving the details, but also preserving the relative contrast and luminance between regions. This is particularly evident in high luminance regions (for instance the bright window behind the altar, or the round window in the top of the dome), where both the perceived details and luminance intensity is effectively portrayed.\nAs we increase the maximum luminance chosen for the original scene (while fixing the display restrictions), our\nalgorithm further amplifies the contrast of details in the image. This makes sense from a perceptual point of view. If the original scene was brighter, an observer would be able to perceive more details within the scene. Therefore the method has to artificially enhance these details to mimic the appearance of the original scene. In the next two sections we take advantage of this behavior."}, {"heading": "3.4 Artificial detail enhancement and haze removal", "text": "We showed in the preceding sections that using the knowledge we have about the image acquisition process helps greatly in automatically recovering images that are optimized to look like the original scene, given the display constraints. In some cases, however, detail visibility in the scene might be unsatisfactory. Intuitively, photographers know that the amount of detail visible in a scene depends on the amount of available light. If the image has already been acquired, it is of course not possible to alter the\nlight sources. However, since the scene luminances scale linearly with the intensity of the light sources, our method allows us to simulate more light post hoc, by linearly rescaling the scene luminances S.\nFigure 7 shows the results of modifying our choice of Smax (choosing as in the previous experiment Smin = 0.01). Note that with increasing values of Smax, details become more visible. For results from the Paris et al. algorithm (shown here as well), we have again employed the parameters proposed in their paper for the detail enhancement problem: \u03b1 = 0.25, \u03b2 = 1, and \u03c3 = 0.3.\nSurprisingly, this same method of detail enhancement can also be used for the problem of haze removal. In a hazy scene, the local contrast has effectively been reduced (roughly speaking, but adding a constant level of scattered light) which makes detail more difficult to discern. In this experiment, we choose also Smin = 0.01 (we find that results are fairly robust to the selection of this parameter) and Smax = 104. Figure 8 compares the performance of our method with both a classical ([10]) and a\nstate-of-the-art method ([7]). Our algorithm converges on an image that greatly enhances the details of the original hazy image, boosting the contrast and reducing the perception of haze within the image. Although the other two methods are specifically designed for this particular problem, our method obtains a similar result without modification. In this case we used the parameters Smin = 5 and Smax = 10 4."}, {"heading": "4 Varying display constraints", "text": "While examining the effects of various image acquisition scenarios in the previous section, we assumed only that the display luminance is bounded. The upper bound is a\nnatural constraint for any real display. The lower bound is also relevant for a wide range of practical display devices, and arises from reflected ambient light and scatter within the display. In this section, we examine the effect of each of these constraints independently, along with a few more complex constraints.\nFigure 9 shows the results for different maximum (Imax) and minimum (Imin) luminance bounds. Our method enhances local contrast, whereas a rescaling can only manipulate contrast globally. For a wide range of display characteristics, optimizing the image to minimize the NLP distance reduces distortion in the rendered images, and increases the visibility of perceptually relevant features."}, {"heading": "4.1 Rendering with limited energy consumption", "text": "The proposed framework allows us to seamlessly introduce arbitrary display constraints. For example, we can optimize the tradeoff between image quality and energy consumption. We assume that energy consumption is proportional to luminance, as for instance in organic light-emitting diode (OLED) displays used in cell phones. Thus, we keep the luminance within a lower and upper bound, while also keeping the mean luminance constant:\nI\u0302 = argmin I\nD(S, I), s.t. \u2200i : Imin \u2264 Ii \u2264 Imax (10)\nand 1\nNi\n\u2211\ni\nIi = Imean\nFigure 10 shows images optimized for different mean luminance compared to images linearly rescaled to achieve the same target mean luminance. It is clear that our optimized images retain more detail from the original scene. Figure 11 shows two plots where the visual appearance and the energy consumption are compared for both methods. Optimizing the images yields a clear benefit in terms of the tradeoff between consumed energy versus perceptual distortion."}, {"heading": "4.2 Rendering with a discrete set of gray levels (dithering)", "text": "Most displays have a limited number of available gray levels. In the extreme case this can be as few as two (e.g., black-and-white printers, e-ink devices, etc). Here, we illustrate that the proposed method is flexible enough to produce good results even under such extreme constraints. The optimization problem is the same as before, but here, we restrict the pixel values to be taken from a discrete set:\nI\u0302 = argmin I D(S, I), s.t. \u2200i : Ii \u2208 {Imin, . . . , Imax}. (11)\nThe discrete nature of the optimization problem prevents us from using a gradient-based method for optimization. Instead, we use a greedy error-diffusion algorithm, analogous to the classic Floyd\u2013Steinberg method. We first initialize the image to the continuous solution obtained for a continuous range of luminances, as in previous experiments. Then, we iteratively select the discrete value for each pixel of the image in raster-scan order, each time picking the discrete value that minimizes the NLP distance of the intermediate result to the original scene.\nFigure 12 shows the results for images rendered using two and four gray levels. In low contrast regions, our method is seen to preserve significantly more detail than the Floyd\u2013Steinberg method. In addition, the Floyd\u2013 Steinberg algorithm tends to return artificially imposed patterns in long flat regions, which can be seen in the dark regions of the penguin\u2019s wings. Our method, however, does not generate these artificial patterns."}, {"heading": "5 Contribution of perceptual metric components", "text": "In order to provide intuition regarding the effect of each of the primary components of the NLP, we optimized images for rendering while removing one of three components of the transform: the initial pointwise nonlinearity (set \u03b3 = 1), the multi-scale decomposition (set Nk = 1), and divisive normalization (set P = 0 and \u03c3 = 1). Figure 13 shows results for each manipulation. Note that we did not refit each of the partial transforms to predict human perceptual judgments; therefore, these results should be seen as a way to understand the importance of each computation, and not as a quantitative comparison of image quality assessment performance (see details in appendix B).\nEach of the three images differs noticeably from the one optimized with the full transform. Without the initial pointwise nonlinearity, the algorithm produces images in which low to medium luminance patches of an image are misrepresented. The high luminance areas are detailed but some parts with medium or low luminance are almost flat. Without the multi-scale decomposition, the algorithm produces images in which extremely high\nand extremely low frequencies are well preserved, but intermediate frequencies are underrepresented, and some cases nearly disappear. Without the normalization, the algorithm converges to images that saturate at the luminance boundary constraints of the display. Normalization preserves the relative luminance changes between coefficients while allowing the absolute luminance to be modified. This allows the rendered image pixel intensities to be proportional to the relative energy in each local region. Moreover, this ensures that regions with similar content scale in a similar way."}, {"heading": "6 Discussion", "text": "We\u2019ve described a method of directly optimizing rendered images, so as to minimize their perceptual difference from the original scene from which they were derived. Perceptual optimization of tone mapping algorithms is not\na new concept. For example, Tumblin and Rushmeier\u2019s seminal paper on tone mapping states: \u201cAccurate display methods should compensate for all light dependent changes in the way we see\u201d [29]. The authors propose optimizing a tone mapping operator (i.e. the transformation from HDR to LDR) to best match the appearance between the displayed image and the scene. Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene. These methods depend critically on the particular parametric function they use as a tone mapping operator, which restricts the space of possible solutions. Thus, a given functional form may not allow the optimal solution to be found, or may only work satisfactorily for a particular type of rendering constraint.\nNowadays, tone mapping methods often do not make explicit use of perceptual metrics (see [5] for a nice review), but rather provide the user with a small set of free\nparameters to hand-control the mapping from the scene to the displayed image. These methods are conceptually simpler than ours, and some of them can produce high quality results in controlled situations (see for instance [21]). Nevertheless, their parameter are often difficult to interpret (and thus, to set), and the restriction to particular functional forms may limit their applicability to different rendering problems.\nIn contrast, by directly optimizing the rendered image itself, our method is free to take into account different display constraints, without requiring manual selection of an appropriate form for each situation. The downside of this approach is computational cost: optimization over the high-dimensional space of rendered images is expensive, and although both hardware and software continue to improve, it will presumably always be significantly more expensive than optimizing a small set of parameters for a fixed transformation. But even if the computational costs prevent the use of this method in a given application, the results can still serve as a benchmark for what is possible, thus facilitating the development of alternative methods.\nWe\u2019ve employed a perceptual metric based on an abstraction of the physiological transformations implemented in the early stages of the visual system. The metric is an extension of the NLP distance presented in [15]. We have show that this metric is consistent with human perception, exhibiting correlation to human quality ratings that is similar to or better than models specifically designed to assess perceptual quality (see appendix B). It is continuous and differentiable, with well-behaved gradients, making it easy to incorporate into optimization procedures. As a case in point, it has also been employed to optimize an image compression algorithm [1].\nAlthough our framework may be applied to any display problem, it depends heavily on both the HVS model employed, and the method used to solve the constrained\noptimization problem. There is room for improvement in both of these elements. Optimization has undergone dramatic changes in the past decade, and methods for handling nonconvex and even discrete problems have become more reliable and efficient. In particular, it should be possible to improve the halftoning solution, for which we used a simple greedy method analogous to Floyd-Steinberg [9].\nOur use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]). For example, the NLP can likely be improved by including relationships between channels, which could help to control artifacts such as halos that sometimes appear around edges with large contrast. In addition, the NLP model should be extended to operate on color images. This can be done following the same procedure that we used for the achromatic model: taking inspiration from human physiology to define the functional form, and fitting the parameters using human psychophysical data. It should also be beneficial to extend the model to include another stage of processing corresponding to primary visual cortex, and containing oriented, multi-scale, derivative filters."}, {"heading": "A Derivative of the distance with respect to the rendered image", "text": "Here we provide, for the interested reader, the derivative of the perceptual distance D(S, I) with respect to the rendered image I. The distance is given by:\nD(S, I) =   1 Nk Nk\u2211\nk=1\n( 1\nN (k) c\nNc\u2211\ni=1\n( d (k) i\n)\u03b1 ) \u03b2 \u03b1   1 \u03b2\nHere, we summarized the distance between the transformed images, y = f(S) and y\u0303 = f(I), as:\nd (k) i = \u2223\u2223y(k)i \u2212 y\u0303 (k) i \u2223\u2223\nThe general equation is:\n\u2202D(S, I)\n\u2202Ij =\n1 \u03b2 D(S, I)1\u2212\u03b2 \u2202 \u2202Ij   1 Nk Nk\u2211\nk=1\n( 1\nN (k) c\nNc\u2211\ni=1\nd (k)\u03b1\ni\n) \u03b2 \u03b1  \nFrom here we only apply the chain rule to expand the full equation. The second term in the equation above is:\n\u2202\n\u2202Ij   1 Nk Nk\u2211\nk=1\n( 1\nN (k) c\nNc\u2211\ni=1\nd (k)\u03b1\ni\n) \u03b2 \u03b1   =\n\u03b2\n\u03b1Nk\nNk\u2211\nk=1\n( 1\nN (k) c\nNc\u2211\ni=1\nd (k)\u03b1\ni\n) \u03b2 \u03b1 \u22121\n\u2202\n\u2202Ij\n[ 1\nN (k) c\nNc\u2211\ni=1\nd (k)\u03b1\ni\n]\nAnd we can put the second term in the equation above in function of the derivative of the difference:\n\u2202\n\u2202Ij\n[ 1\nN (k) c\nNc\u2211\ni=1\nd (k)\u03b1\ni\n] = \u03b1\nN (k) c\nNc\u2211\ni=1\nd (k)(\u03b1\u22121) i\n\u2202d (k) i\n\u2202Ij\nBy putting all terms together we have the derivative \u2202D(S,I) \u2202Ij as a function of the derivative of the difference:\nD(S, I)1\u2212\u03b2 1\nNk\nNk\u2211\nk=1\n1\nN (k)\n\u03b2 \u03b1\nc\n( Nc\u2211\ni=1\nd (k)\u03b1\ni\n) \u03b2 \u03b1 \u22121 Nc\u2211\ni=1\nd (k)(\u03b1\u22121) i\n\u2202d (k) i\n\u2202Ij\nAnd now expanding the derivative of the difference:\n\u2202d (k) i\n\u2202Ij = sgn(y\n(k) i \u2212 y\u0303 (k) i )\n\u2202y (k) i\n\u2202Ij\n\u2202y (k) i\n\u2202Ij =\n( \u2202y\n(k) i \u2202z(k)\n)( \u2202z(k)\n\u2202xj )( \u2202xj \u2202Ij )\nThe derivative for the first term is:\n\u2202y (k) i \u2202zi(k) = \u03c3 + Pi|z| \u2212 Pii sgn(zi)zi (\u03c3 + Pi|z|)2\n\u2202y (k) i \u2202zl(k) = \u2212Pil sgn(zl)zi (\u03c3 + Pi|z|)2 , l 6= i\nWhere sgn(zi) is the sign of zi. The second term is:\n\u2202z(k)\n\u2202xj = Q\n(k) (:,j)\nWhere Q is the matrix of the linear transformation performed by the Laplacian Pyramid, z = Qx. The third term is:\n\u2202xj (k)\n\u2202Ij =\n1 \u03b3 I ( 1 \u03b3 \u22121) j\nB IQA performance of Normalized Laplacian Pyramid\nPerceptual image quality assessment (IQA), as a means of comparing results obtained by different methods, has become an important topic in image processing. Although the best method of evaluating IQA is through explicit measurement of human responses, this is a difficult and costly undertaking. An objective measure of perceptual quality alleviates this difficulty. If the measure is differentiable and well-behaved, additional advantage arises from using it to optimize the perceptual performance of algorithms.\nThe most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19]. Table 1 presents correlation results against five databases of human mean opinion scores: four were measured using low dynamic range displays, and one was targeted at HDR displays. All results are obtained using the achromatic images in the databases (we have not yet extended the NLP metric to handle color), and we also include results for several widely employed IQA methods. The table shows results for two types of correlation: The Pearson correlation, which measures linear predictability of the human responses, and the Spearman correlation, which is concerned only with the ranking of the responses, and thus more robust to (monotonic) nonlinear distortions. Note that the latter measure is perhaps too flexible, since the nonlinear relationship between the MOS and the predicted value can be different for each database, it is often reported when evaluating IQA methods, and we include it here for completeness.\nOur results indicate that the proposed metric behaves well for both low and high dynamic range images. Note that the parameters of our metric were adjusted using the TID 2008 [24] database, the VDP 2.2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27]. The Pearson (linear) correlation of our proposed metric is clearly better for four of five datasets (including the HDR dataset), and the Spearman (nonlinear) correlation is equal to or better than all the other metrics for all the datasets. We conclude that our proposed NLP metric is competitive with the current state-of-the-art in IQA. In addition, the NLP metric is the only one that has shown to be easily differentiable for incorporation into convex optimization procedures. For example, the application of SSIM to optimization procedures is not straightforward and it involves some modifications of the metric [2]."}, {"heading": "Acknowledgments", "text": "JB and EPS are supported by the Howard Hughes Medical Institute. VL is supported by the APOSTD/2014/095 Generalitat Valenciana grant (Spain) and Analog Devices, Inc. AB is supported by the NEI Visual Neuroscience Training Program, T32 EY007136. We want to thank the comments of many people that have helped during the development of this work, in particular Jes\u00fas Malo, Javier Calpe, Pau Segu\u00ed, Jorge P\u00e9rez, Marcelo Bertalm\u00edo, Ted Adelson, Alejandro P\u00e1rraga, Xim Cerd\u00e1, Sylvian Paris, Mark Fairchild, and all the people from LCV."}], "references": [{"title": "End-to-end optimization of nonlinear transform codes for perceptual quality", "author": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "venue": "Picture Coding Symposium,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "On the mathematical properties of the structural similarity index", "author": ["Dominique Brunet", "Edward R. Vrscay", "Zhou Wang"], "venue": "Trans. Img. Proc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The Laplacian pyramid as a compact image code", "author": ["Peter J. Burt", "Edward H. Adelson"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1983}, {"title": "Normalization as a canonical neural computation", "author": ["M. Carandini", "D.J. Heeger"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Which tone-mapping operator is the best? a comparative study of perceptual quality", "author": ["Xim Cerd\u00e1-Company", "C. Alejandro P\u00e1rraga", "Xavier Otazu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Dehazing using color-lines", "author": ["Raanan Fattal"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A model of visual adaptation for realistic image synthesis", "author": ["James A. Ferwerda", "Sumanta N. Pattanaik", "Peter Shirley", "Donald P. Greenberg"], "venue": "In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "An Adaptive Algorithm for Spatial Greyscale", "author": ["Robert W. Floyd", "Louis Steinberg"], "venue": "Proceedings of the Society for Information Display,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1976}, {"title": "Single image haze removal using dark channel prior", "author": ["Kaiming He", "Jian Sun", "Xiaoou Tang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Normalization of cell responses in cat striate cortex", "author": ["D.J. Heeger"], "venue": "Journal of Modern Optics Vis. Neurosci.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "arXiv e-prints,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Subjective quality assessment database of HDR images compressed with JPEG XT", "author": ["Pavel Korshunov", "Philippe Hanhart", "Thomas Richter", "Alessandro Artusi", "Rafal Mantiuk", "Touradj Ebrahimi"], "venue": "In 7th International Workshop on Quality of Multimedia Experience (QoMEX),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Perceptual image quality assessment using a normalized laplacian pyramid", "author": ["V Laparra", "J Ball\u00e9", "A Berardino", "EP Simoncelli"], "venue": "Proc. IS&T Int\u2019l Symposium on Electronic Imaging, Conf. on Human Vision and Electronic Imaging,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Divisive normalization image quality metric revisited", "author": ["V. Laparra", "J. Mu\u00f1oz Mar\u00ed", "J. Malo"], "venue": "JOSA A,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Categorical image quality (csiq) database", "author": ["EC Larson", "DM Chandler"], "venue": "http://vision.okstate.edu/csiq,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Display adaptive tone mapping", "author": ["Rafa\u0142Mantiuk", "Scott Daly", "Louis Kerofsky"], "venue": "ACM Trans. Graph.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Hdr-vdp-2.2: a calibrated method for objective quality prediction of highdynamic range and standard images", "author": ["Manish Narwaria", "Rafal K. Mantiuk", "Matthieu Perreira Da Silva", "Patrick Le Callet"], "venue": "J. Electronic Imaging,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A biologically inspired algorithm for the recovery of shading and reflectance", "author": ["A. Olmos", "F.A.A. Kingdom"], "venue": "images. Perception,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Local laplacian filters: edge-aware image processing with a laplacian pyramid", "author": ["Sylvain Paris", "Samuel W. Hasinoff", "Jan Kautz"], "venue": "ACM Trans. Graph.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A multiscale model of adaptation and spatial vision for realistic image display", "author": ["Sumanta N. Pattanaik", "James A. Ferwerda", "Mark D. Fairchild", "Donald P. Greenberg"], "venue": "In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Time-dependent visual adaptation for fast realistic image display", "author": ["Sumanta N. Pattanaik", "Jack Tumblin", "Hector Yee", "Donald P. Greenberg"], "venue": "In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "TID2008 - a database for evaluation of full-reference visual quality assessment metrics", "author": ["N. Ponomarenko", "V. Lukin", "A. Zelensky", "K. Egiazarian", "M. Carli", "F. Battisti"], "venue": "Advances of Modern Radioelectronics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Image database tid2013: Peculiarities, results and perspectives", "author": ["Nikolay N. Ponomarenko", "Lina Jin", "Oleg Ieremeiev", "Vladimir V. Lukin", "Karen O. Egiazarian", "Jaakko Astola", "Benoit Vozel", "Kacem Chehdi", "Marco Carli", "Federica Battisti", "C.-C. Jay Kuo"], "venue": "Sig. Proc.: Image Comm.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Natural signal statistics and sensory gain control", "author": ["O. Schwartz", "E.P. Simoncelli"], "venue": "Nat. Neurosci.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "A statistical evaluation of recent full reference image quality assessment algorithms", "author": ["Hamid R. Sheikh", "Muhammad F. Sabir", "Alan C. Bovik"], "venue": "IEEE Transactions on Image Processing (TIP),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Two methods for display of high contrast images", "author": ["Jack Tumblin", "Jessica K. Hodgins", "Brian K. Guenter"], "venue": "ACM Trans. Graph.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Tone reproduction for realistic images", "author": ["Jack Tumblin", "Holly Rushmeier"], "venue": "IEEE Comput. Graph. Appl.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1993}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Zhou Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "Trans. Img. Proc.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["ZhouWang", "Eero P. Simoncelli", "Alan C. Bovik"], "venue": "In in Proc. IEEE Asilomar Conf. on Signals, Systems, and Computers,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "A technique for visual optimization of DCT quantization matrices for individual images. Society for Information Display", "author": ["Andrew B. Watson"], "venue": "Digest of Technical Papers", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}], "referenceMentions": [{"referenceID": 26, "context": "A variety of \"tone-mapping\" methods have been proposed to solve this problem by nonlinearly remapping the intensities of the original image into the output range, in a way that remains faithful to the visual appearance of the original scene [29].", "startOffset": 241, "endOffset": 245}, {"referenceID": 27, "context": "Rather than defining a perceptual distance directly (as in SSIM [30], for example), we first define a nonlinear perceptual transform f(\u00b7), which approximates the computations performed within the early stages of the human visual system.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Figure 2: Perceptual transform, constructed as a Normalized Laplacian Pyramid (NLP) [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "The transformed luminance image is then decomposed into frequency channels, using the recursive implementation of the Laplacian Pyramid [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "in the perceptual space defined by the NLP are highly correlated with human judgments [15].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "This initial nonlinear transformation is followed by a recursive partition into frequency channels, as in the Laplacian Pyramid [3]:", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "05), as originally specified in [3].", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4].", "startOffset": 147, "endOffset": 158}, {"referenceID": 23, "context": "This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4].", "startOffset": 147, "endOffset": 158}, {"referenceID": 3, "context": "This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4].", "startOffset": 147, "endOffset": 158}, {"referenceID": 12, "context": "Figure 3: Summation model, using the Normalized Laplacian Pyramid [15] as perceptual transform f (see Fig.", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "A similar summation model has been employed in previous perceptual quality metrics [32, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 13, "context": "A similar summation model has been employed in previous perceptual quality metrics [32, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "All parameters of the perceptual transform and metric were optimized to best explain human perceptual ratings of distorted images in a public database [24].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Unlike in [15], we set the normalization parameters to be identical for all bandpass channels (assuming scale-invariance), but allowed a different set for the lowpass channel.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "[21] NLP", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Center: result obtained from a state-of-the-art tone mapping algorithm [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "For this, we use the Adaptive Moment Estimation (Adam) algorithm [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "[21] NLP", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] mitigates this problem, rendering an image that reveals detail in both dark and bright regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Results for two example grayscale images from the McGill database [20] are shown in Fig.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "[21] result.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21]", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Top center: image processed using [21].", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10]", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Fattal [7] NLP, Smax = 104", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "algorithm [10].", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "Bottom left: image processed using Fattal algorithm [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "Figure 8 compares the performance of our method with both a classical ([10]) and a", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "state-of-the-art method ([7]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "Center column: Floyd\u2013Steinberg method [9].", "startOffset": 38, "endOffset": 41}, {"referenceID": 26, "context": "For example, Tumblin and Rushmeier\u2019s seminal paper on tone mapping states: \u201cAccurate display methods should compensate for all light dependent changes in the way we see\u201d [29].", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 19, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 25, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 20, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 15, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 4, "context": "Nowadays, tone mapping methods often do not make explicit use of perceptual metrics (see [5] for a nice review), but rather provide the user with a small set of free", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "These methods are conceptually simpler than ours, and some of them can produce high quality results in controlled situations (see for instance [21]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "The metric is an extension of the NLP distance presented in [15].", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "As a case in point, it has also been employed to optimize an image compression algorithm [1].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "In particular, it should be possible to improve the halftoning solution, for which we used a simple greedy method analogous to Floyd-Steinberg [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 27, "context": "Our use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]).", "startOffset": 191, "endOffset": 203}, {"referenceID": 28, "context": "Our use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]).", "startOffset": 191, "endOffset": 203}, {"referenceID": 16, "context": "Our use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]).", "startOffset": 191, "endOffset": 203}, {"referenceID": 27, "context": "The most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19].", "startOffset": 150, "endOffset": 162}, {"referenceID": 28, "context": "The most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19].", "startOffset": 150, "endOffset": 162}, {"referenceID": 16, "context": "The most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19].", "startOffset": 150, "endOffset": 162}, {"referenceID": 21, "context": "Note that the parameters of our metric were adjusted using the TID 2008 [24] database, the VDP 2.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "For example, the application of SSIM to optimization procedures is not straightforward and it involves some modifications of the metric [2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 21, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "We develop a framework for rendering photographic images, taking into account display limitations, so as to optimize perceptual similarity between the rendered image and the original scene. We formulate this as a constrained optimization problem, in which we minimize a measure of perceptual dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics the early stage transformations of the human visual system. When rendering images acquired with higher dynamic range than that of the display, we find that the optimized solution boosts the contrast of low-contrast features without introducing significant artifacts, yielding results of comparable visual quality to current state-of-the art methods with no manual intervention or parameter settings. We also examine a variety of other display constraints, including limitations on minimum luminance (black point), mean luminance (as a proxy for energy consumption), and quantized luminance levels (halftoning). Finally, we show that the method may be used to enhance details and contrast of images degraded by optical scattering (e.g., fog).", "creator": "LaTeX with hyperref package"}}}