{"id": "1701.06725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "A Contextual Bandit Approach for Stream-Based Active Learning", "abstract": "contextual bandit algorithms - - considering a finite class of empirical multi - armed bandit algorithms presently that exploit effectively the intervening contextual information - - have been shown potentially to be effective techniques in independently solving sequential decision making management problems under zero uncertainty. a common assumption explicitly adopted in the theoretical literature is that accepting the realized ( presumably ground kiran truth ) reward by taking the selected naive action is observed by the immediate learner acting at no cost, which, however however, is not itself realistic value in many clearly practical scenarios. when observing the apparent ground deception truth reward is costly, a key design challenge for exposing the learner is how simply to judiciously only acquire the meaningful ground truth by equally assessing the benefits and actual costs needed in order to balance net learning efficiency and learning cost. hence from the information enhancement theoretic perspective, addressing a or perhaps other even very more interesting serious question is how incredibly much efficiency might be lost considerably due to this operational cost. therefore in this key paper, so we could design a novel contextual filter bandit - based rm learning algorithm and endow it with the active learning capability. maybe the critical key feature of aiding our algorithm program is that then in part addition to sending a query to an annotator for reading the ground truth, previously prior information encoded about the ground truth cost learned by the learner is sent together, likewise thereby reducing the query cost. we prove insight that by therefore carefully consideration choosing choosing the target algorithm parameters, the learning benefit regret increase of the proposed agile algorithm often achieves with the same order as that of conventional contextual bandit master algorithms in two cost - effectively free novice scenarios, further implying effectiveness that, surprisingly, cost due to essentially acquiring the ground truth does not increase the learning regret gain in the long - off run. improving our analysis routinely shows that prior input information about the ground truth rate plays strategically a critical additional role in not improving the system performance positively in scenarios where decreased active learning performance is fundamentally necessary.", "histories": [["v1", "Tue, 24 Jan 2017 04:12:25 GMT  (173kb,D)", "http://arxiv.org/abs/1701.06725v1", "arXiv admin note: text overlap witharXiv:1607.03182"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1607.03182", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["linqi song", "jie xu"], "accepted": false, "id": "1701.06725"}, "pdf": {"name": "1701.06725.pdf", "metadata": {"source": "CRF", "title": "A Contextual Bandit Approach for Stream-Based Active Learning", "authors": ["Linqi Song", "Jie Xu"], "emails": ["songlinqi@ucla.edu", "jiexu@miami.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nContextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5]. In a typical setting, a task arrives to the system with certain contextual information (e.g. incoming user\u2019s age, gender, search and purchase history etc. in online content recommendation), then the system pulls an arm from a possibly very large arm space (e.g. recommend a piece of online content from a large content pool). A reward is later realized depending on the context value and the selected arm. The objective of a learner (or a learning algorithm) is to make arm selection decisions based on the history of context-armreward realizations to minimize the learning regret (i.e. the gap of achievable reward compared with certain benchmarks). A common assumption made in the literature is that the reward of each task is observed by the learner at no cost,\nthereby allowing the learner to fully and freely utilize this information. While this assumption holds true in some application scenarios, it hardly captures the reality in many others in which observing the ground truth reward requires substantial manpower, time, energy and/or other resources. For instance, to calculate the reward in stream mining systems, human experts are needed to manually annotate the ground truth labels of the mining tasks. Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7]. Figure 1 illustrates the considered active learning scenario.\nIn this paper, we design a learning algorithm, called Contextual Bandits with Active Learning (CB-AL), that accomplishes the aforementioned task. We prove that CB-AL is orderoptimal in terms of the learning regret, which matches that of conventional contextual bandits in cost-free scenarios. The key to achieving the optimal regret order by our proposed algorithm is that the query about the ground truth reward is sent to the annotator together with some prior information about this reward. Although the learner does not directly observe the reward realization by selecting an arm, it is learning the distribution of the reward as it learns the optimal arm to pull, and this statistical information can be utilized to reduce the cost of acquiring the ground truth reward by an annotator. This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].\nOur algorithm is able to effectively deal with large context and arm spaces. To this end, our algorithm divides time into epochs and the context/arm spaces are adaptively partitioned across epochs. The partitions become finer and finer as the\nar X\niv :1\n70 1.\n06 72\n5v 1\n[ cs\n.L G\n] 2\n4 Ja\nn 20\n17\nepoch grows. Within each epoch, our algorithm first explores various arm clusters (defined for each arm subspace) to learn their reward estimates for each context cluster (defined for each context subspace) and removes suboptimal arm clusters during the course of learning. When the remaining arm clusters are learned to be optimal or near-optimal, the algorithm enters an exploitation phase in which the arm cluster removal operation stops and acquiring the ground truth rewards is no longer needed for the remaining time slots in the current epoch, thereby maximizing the reward and minimizing the query cost concurrently. To optimize the overall long term performance and minimize the long-term learning regret, our algorithm carefully designs control functions that determine what arm clusters are optimal, near-optimal and suboptimal. The remainder of this paper is organized as follows. Section II formulates the problem and defines the learning regret. Section III describes our algorithm whose regret performance is analyzed in Section IV. Section V provides illustrative numerical results followed by conclusions in Section VI."}, {"heading": "II. PROBLEM FORMULATION", "text": ""}, {"heading": "A. System Model", "text": "We consider a discrete time system where time is divided into slots t = 1, 2, .... The arm space is a bounded space K with covering dimension dK . The context space is a bounded space X with covering dimension dX . For any context x \u2208 X , the reward of selecting arm k \u2208 K is r(x, k) \u2208 [0, 1], which is sampled according to some underlying but unknown distribution f(x, k). The expected value of r(x, k) is denoted as \u00b5(x, k), which is unknown too. We assume that the reward value space is [0, 1] for the ease of exposition but this assumption can be relaxed to account for any bounded interval. In the conventional contextual bandits setting, the following events occur in sequence in each time slot t: (1) A context xt \u2208 X arrives; (2) An arm kt \u2208 K is selected; (3) The (groundtruth) reward r(xt, kt) is generated according to f(xt, kt) and is observed by the learner at no cost as feedback, which provides information for future arm selections. In our considered setting, r(xt, kt) is not observed for free. Instead, there is a cost associated with requesting the ground truth reward. Therefore, there is a need to actively and judiciously decide when to request the ground truth reward to balance the learning efficiency and the cost minimization. Thus, in addition to deciding which arm kt to choose, the learner also has to decide whether or not to query the ground truth reward at a cost, denoted by qt \u2208 {0, 1}, where qt = 1 stands for requesting and qt = 0 stands for not requesting. We consider that the query cost is not fixed, but a function of the prior information about the ground truth, which is updated as learning goes on. The intuition is that if the prior information is more informative, then the query cost should be smaller. In particular, we define the prior information about the reward r(xt, kt) as a tuple (at, bt, \u03b4t), which represents that the expected reward \u00b5(xt, kt) is in the region [at, bt] with probability at least 1\u2212 \u03b4t. The query cost is then defined as a\nconvex increasing function of the confidence interval bt \u2212 at and the significance level \u03b4t of the following form:\nct = c[(bt \u2212 at)\u03b21 + \u03b7\u03b4\u03b22t ] (1)\nwhere c > 0, \u03b21 \u2265 1, \u03b22 \u2265 1, \u03b7 > 0 are constant parameters. Therefore, a larger confidence interval bt \u2212 at and a smaller confidence level 1\u2212\u03b4t result in a higher query cost. We choose this form of query cost because it captures the reality to a large extent and also is amenable to our subsequent analysis. Let r\u0302t denote the observed reward in time slot t, which is r\u0302t = r(xt, kt) if qt = 1, and r\u0302t = \u2205 if qt = 0. Given the context arrival process, the selected arm sequence and the observed reward sequence, the history by time slot t is defined as\nht\u22121 = {(x1, k1, r\u03021), ..., (xt\u22121, kt\u22121, r\u0302t\u22121))}, \u2200t > 1 (2)\nand h0 = \u2205 for t = 1. The set of all possible histories is denoted by H. An algorithm \u03c0 is a mapping \u03c0 : H \u00d7 X \u2192 K\u00d7{0, 1}, which selects an arm and decides whether or not to query given the history and the current context. For the ease of exposition, we separately write \u03c0tK = \u03c0K(h\nt\u22121, xt) and \u03c0tq = \u03c0q(h\nt\u22121, xt) for the arm selection component and the query decision component of the algorithm, respectively."}, {"heading": "B. Learning Regret", "text": "We use the total expected payoff (i.e. the reward minus the query cost) to describe the performance of an algorithm \u03c0. The total expected payoff up to time slot T is thus\nU\u03c0(T ) = E T\u2211\nt=1\n[r(xt, kt)\u2212 ctqt] (3)\nwhere the expectation is taken over the context arrival process and the reward distributions. We compare an algorithm with the static-best oracle policy \u03c0\u2217 which knows the reward distributions a priori. Therefore, in each time slot t, the oracle policy selects the arm k\u2217t = argmaxk \u00b5(xt, kt) that maximizes the expected reward. Clearly, since the oracle knows the reward distributions, there is no need for it to query the ground truth to learn about them. Therefore, q\u2217t = 0, \u2200t. The learning regret of an algorithm \u03c0 is defined as\nR\u03c0(T ) = U\u03c0\u2217(T )\u2212 U\u03c0(T ) (4)\nAs a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm. This assumption is formalized as follows.\nAssumption 1. For any two contexts x, x\u2032 \u2208 X and two arms k, k\u2032 \u2208 K, the expected rewards satisfy\n|\u00b5(x, k)\u2212 \u00b5(x\u2032, k)| \u2264 LX\u2016x\u2212 x\u2032\u2016 (5) |\u00b5(x, k)\u2212 \u00b5(x, k\u2032)| \u2264 LK\u2016k \u2212 k\u2032\u2016 (6)\nwhere LX , LK are the Lipschitz constants for the context space and the arm space, respectively."}, {"heading": "III. THE ALGORITHM", "text": "In this section, we describe the proposed contextual bandits\nalgorithm with active learning (CB-AL)."}, {"heading": "A. Useful Notions", "text": "First, we introduce some useful notions for the algorithm. Context/Arm Space Partition. Time slots are grouped into epochs. The i-th epoch lasts for Ti = 2i time slots. At the beginning of each epoch, the context space and the arm space are partitioned into small subspaces. A context (arm) space is called a context (arm) cluster. The context/arm space partition is kept unchanged throughout the entire epoch. Formally, the partition of the context space for epoch i is denoted by PX(i) = {X1,X2, ...,XMi} consisting of Mi subspaces. Similarly, the partition of the arm space for epoch i is denoted by PK(i) = {K1,K2, ...,KNi} consisting of Ni subspaces. The radius of a context cluster Xm is half of the maximum distance between any two context points in the cluster, i.e.\n\u03c1Xm = 0.5 sup x,x\u2032\u2208Xm\n\u2016x\u2212 x\u2032\u2016 (7)\nThe radius of an arm cluster is defined similarly. The context/arm space partitioning is performed such that the context/arm clusters satisfy \u2200m = {1, ...,Mi}, \u03c1Xm,i = T\u2212\u03b1i , \u03c1X,i and \u2200n = {1, ..., Ni}, \u03c1Kn,i = T\u2212\u03b1i , \u03c1K,i, where \u03b1 \u2208 (0, 1). Active Arm Cluster. At the beginning of each epoch, all arm clusters according to the arm space partitioning are set to be active. We denote the set of active arm clusters with respect to context cluster Xm in epoch i by Am(i). The active arm cluster set will be updated as time goes by according to the learning outcome. Some arm clusters will be learned to be suboptimal and hence will be de-activated (i.e. be removed from Xm) and will not be selected by the algorithm in the remaining time slots of the current epoch. Round. A round sm(i) is defined for each context cluster Xm in each epoch i, which consists a number of |Am(i)| time slots. Thus, in each round sm(i), each active arm cluster in Am(i) is selected once. Therefore, even in the same epoch, the length of a round sm(i) may change due to the updating of the active arm cluster set Am(i). Control Functions. There are two important control functions in our algorithm. The first control function, denoted by Di(i, sm(i)), is used to de-active arm clusters, with respect to each context cluster Xm, that are learned to be suboptimal depending on the epoch index i and the round index sm(i). In particular, Di(i, sm(i)) has the form\nD1(i, sm(i)) = \u01eb(i) + [2D(sm(i)) + 2LX\u03c1X,i + 2LK\u03c1K,i]\nwhere \u01eb(i) = LT\u2212\u03b1i is a small positive value for epoch i,\nand D(sm(i)) = \u221a ln(2T 1+\u03b3i )/2sm(i). Here L = L(c) > 4LX + 4LK and \u03b3 \u2208 (0, 1) are constants. The second control function, denoted by D2(i, sm(i)), is used to determine when to stop querying the ground truth reward. When the stopping condition is satisfied, the algorithm stops de-activating arm clusters and enters a pure exploitation\nphase for the remaining time slots of the current epoch for the context cluster Xm. In particular, D2(i, sm(i)) has the form\nD2(i, sm(i)) = 2\u01eb(i)\u2212 [2D(sm(i)) + 2LX\u03c1X,i + 2LK\u03c1K,i]\nSample Mean Reward. The sample mean reward of an arm cluster Kn with respect to a context cluster Xm by round sm(i) in epoch i is denoted by r\u0304m,n(sm(i)). The sample mean reward of the empirical best arm cluster is r\u0304\u2217m(sm(i)) = maxKn\u2208Am(i) r\u0304m,n(sm(i))."}, {"heading": "B. The Algorithm", "text": "Now, we describe the proposed CB-AL algorithm, whose pseudo-code is provided in Algorithm 1. Figure 2 provides an illustration of the algorithm. The algorithm operates in epochs. At the beginning of each epoch, the context/arm space partitions are determined. As aforementioned, the radii of the context and arm spaces become smaller as the epoch grows and therefore, the partitions of the spaces become finer and finer. All arm clusters for any context cluster are set to be active at the beginning of each epoch. In each time slot t, a context xt arrives and the algorithm finds the context cluster Xm \u2208 Pm(i) that it belongs to. Depending on which phase the algorithm is in (with respect to the context cluster Xm), different operations are carried out as follows: Exploration. The goal of the algorithm in the exploration phase is to explore various active arm clusters to learn their performance for Xm. Arm clusters that are learned to be suboptimal will be de-activated over time, thereby improving the learning efficiency and system performance. In each round sm(i), the algorithm selects an active arm cluster Kn \u2208 Am(i) that has not been selected in the current round for Xm. If all active arm clusters have been selected in the current round, then the current round sm(i) ends and a new round begins. Then the algorithm arbitrarily selects any arm kt in the selected arm cluster Kn. In the exploration phase, a query is always sent, namely qt = 1, together with prior information (at, bt, \u03b4t). The prior information is computed as follows: for round sm(i) > 1, the prior information is\nat = r\u0304m,n \u2212 2LX\u03c1X,i \u2212 2LK\u03c1K,i \u2212 2D(sm(i)\u2212 1) (8) bt = r\u0304m,n + 2LX\u03c1X,i + 2LK\u03c1K,i + 2D(sm(i)\u2212 1) (9) \u03b4t = T \u2212(1+\u03b3) i (10)\nAlgorithm 1 Contextual Bandits with Active Learning 1: for epoch i = 0, 1, 2, ... do 2: Initialization: Create context and arm space partitioning PX(i) and PK(i). Set Am(i) = PK(i), \u2200m. Set Stopm =\n0, \u2200m. Set sm(i) = 1, \u2200m. Set r\u0304m,n = 0, \u2200m,n. 3: for time slot t = 2i to 2i+1 \u2212 1 do 4: Observe the context xt and find Xm such that xt \u2208 Xm. 5: switch Stopm do 6: case 0 \u22b2 Exploration 7: Select Kn \u2208 Am(i) that has not been selected in round sm(i) and select any kt \u2208 Kn. 8: Choose qt = 1 and send the prior information (at, bt, \u03b4t) to the annotator 9: (A query cost ct is incurred, and the reward r\u0302t = r(xt, kt) is observed.) 10: Update r\u0304m,n(sm(i)). 11: if round sm(i) has finished then 12: For any Kn \u2208 Am(i) such that \u2206m,n(sm(i)) \u2265 D1(i, sm(i)), remove Kn from Am(i). 13: If for all Kn \u2208 Am(i), \u2206m,n(sm(i)) \u2264 D2(i, sm(i)), then set Stopm = 1. 14: Update sm(i)\u2190 sm(i) + 1. 15: end if 16: case 1 \u22b2 Exploitation 17: Select any Kn \u2208 Am(i) and any kt \u2208 Kn. 18: Choose qt = 0. 19: (The reward r(xt, kt) is generated, but cannot be observed.) 20: end for 21: end for\nFor sm(i) = 1, the prior information is at = 0, bt = 1, \u03b4 = 0. Once the ground truth reward r(xt, kt) is obtained from the annotator, the sample mean r\u0304m,n is updated as follows\nr\u0304m,n \u2190 (r\u0304m,n \u00b7 (sm(i)\u2212 1) + r(xt, kt))/sm(i) (11) At the end of a round sm(i), the algorithm de-activates suboptimal arm clusters if necessary. Specifically, the algorithm first finds the empirically best arm cluster for Xm and calculates the sample mean reward difference between the empirically best arm cluster and any other active arm cluster, denoted by \u2206m,n(sm(i)) , r\u0304\u2217m(sm(i))\u2212 r\u0304m,n(sm(i)), \u2200Kn \u2208 Am(sm(i)). Then it compares \u2206m,n(sm(i)) with the current value of the control function D1(i, sm(i)). If the sample mean reward difference is greater than or equal to this value, then the corresponding arm cluster is suboptimal with high probability and hence is de-activated. Moreover, if the remaining active arm clusters have sufficiently similar sample mean reward estimates, then the algorithm stops the de-activation process in the remaining time slots of the current epoch and enters the exploitation phase. Specifically, if the reward difference for any active cluster is smaller or equal to D2(i, sm(i)), then the de-activation process stops. We denote by Sim the number of rounds taken when the stopping condition is satisfied. Exploitation. The goal of the algorithm in the exploitation phase is to exploit the best arms to maximize the reward. Since in the exploitation phase, the remaining active arm clusters are the optimal arm cluster or near-optimal arm cluster for the corresponding context cluster with high probability, the algorithm simply arbitrarily selects an active arm cluster Kn \u2208 Am(i) and then arbitrarily selects an arm from Kn. Notably,\nthe algorithm no longer requests for the ground truth reward, i.e. qt = 0, for all time slots in the exploitation phase."}, {"heading": "IV. REGRET ANALYSIS", "text": "To analyze the regret, we first introduce some notions. \u2022 Cluster reward. We define the expected reward of selecting an arm cluster Kn for context cluster Xm as \u00b5(m,n) = maxx\u2208Xm,k\u2208Kn \u00b5(x, k). The reward of the optimal arm cluster with respect to the context cluster Xm is thus \u00b5\u2217m = maxn \u00b5(m,n). Furthermore, we define the reward difference as \u2206m,n = \u00b5\u2217m \u2212 \u00b5(m,n). \u2022 \u01eb-optimal arm cluster. We define the \u01eb-optimal arm clusters with respect to the context cluster Xm as the arm clusters Kn that satisfy \u00b5(m,n) \u2265 \u00b5\u2217m \u2212 \u01eb. Similarly, the \u01eb-suboptimal arm clusters are those that satisfy \u00b5(m,n) < \u00b5\u2217m \u2212 \u01eb. \u2022 Normal event and abnormal event. A normal event Nm,n(sm(i)) is an event such that the reward of selecting arm cluster Kn for context cluster Xm in round sm(i) satisfies |r\u0304m,n(sm(i)) \u2212 E[rm,n(sm(i))]| \u2264 D(sm(i)). A abnormal event N cm,n(sm(i)) is an event such that |r\u0304m,n(sm(i)) \u2212 E[rm,n(sm(i))]| > D(sm(i)). Further, we denote Ni,m,n as the event that no abnormal event N cm,n(sm(i)) occurs with respect to arm cluster Kn and context cluster Xm for the entire epoch i. To analyze the regret, we first provide the following lemmas.\nLemma 1. An abnormal event for arm cluster Kn in epoch i occurs with probability at most \u03b4(i) = T\u2212\u03b3i .\nProof. According to the definition of abnormal event and the Chernoff-Hoeffding bound, the probability that an abnormal\nevent for an arm cluster occurs in round sm(i) can be bounded by\nPr{[Nm,n(sm(i))]C} \u2264 2e\u22122[D(sm(i))] 2sm(i) \u2264 1\nT 1+\u03b3i . (12)\nHence, the probability that an abnormal event for an arm cluster Kn in epoch i occurs with at most\n\u2211 m Pr{[Ni,m,n]C} \u2264 \u2211 m Sim\u2211 sm(i)=1 Pr{[Nm,n(sm(i))]C} \u2264\u2211 m Sim\u2211 sm(i)=1 1 T 1+\u03b3i \u2264 1 T\u03b3i .\n(13)\nLemma 2. (a) With probability at least 1 \u2212Ni\u03b4(i), an \u01eb(i)optimal arm clusters are not de-activated for context cluster Xm in epoch i. (b) With probability at least 1 \u2212 Ni\u03b4(i), the active set Am(i) in the exploitation phase contains at most 2\u01eb(i)-optimal arm clusters for context cluster Xm in epoch i. Proof. If the normal event occurs, for any deactivated arm clusters Kn, we have:\nr\u0304\u2217m(sm(i))\u2212 r\u0304m,n(sm(i)) = (\u00b5\u2217m \u2212 \u00b5(m,n)) + (r\u0304\u2217m(sm(i))\u2212 \u00b5\u2217m) + (\u00b5(m,n)\u2212 r\u0304m,n(sm(i))) \u2264 \u2206m,n + 2D(sm(i)) + 2LX\u03c1X,i + 2LK\u03c1K,i,\n(14)\nwhere the inequality follows from that r\u0304\u2217m(sm(i)) \u2212 \u00b5\u2217m \u2264 D(sm(i)) and \u00b5(m,n) \u2212 r\u0304m,n(sm(i)) \u2264 D(sm(i)) + 2LX\u03c1X,i + 2LK\u03c1K,i. Combining with the deactivating rule, we have \u2206m,n > \u01eb(i). If the normal event occurs, for any reserved active arm cluster Kn, we also have: r\u0304\u2217m(S i m)\u2212 r\u0304m,n(Sim)\n= (\u00b5\u2217m \u2212 \u00b5(m,n)) + (r\u0304\u2217m(Sim)\u2212 \u00b5\u2217m) + (\u00b5(m,n)\u2212 r\u0304m,n(Sim))\n\u2265 \u2206m,n \u2212 2(D(Sim) + LX\u03c1X,i + LK\u03c1K,i), (15)\nwhere the inequality follows from that \u00b5\u2217m \u2212 r\u0304\u2217m(Sim) \u2264 D(Sim) + 2LX\u03c1X,i + 2LK\u03c1K,i and r\u0304m,n(Sim) \u2212 \u00b5(m,n) \u2264 D(sm(i)). Combining with the stopping rule, we have \u2206m,n \u2264 2\u01eb(i). Since the normal event occurs with probability at least 1\u2212Ni\u03b4(i), the results follow. Now we are ready to prove the regret of CB-AL.\nTheorem 1. The regret of the CB-AL algorithm can be upperbounded by R(T ) = O(T dX+dK+1 dX+dK+2 ).\nProof. To bound the regret, we first consider the regret caused in epoch i, denoted by Ri. This regret can be decomposed into four terms: the regret Rai caused by abnormal events, the regret Rni caused by 2\u01eb(i)-optimal arm cluster selection and the inaccuracy of clusters, the regret Rsi caused by 2\u01eb(i)suboptimal arm cluster selection when no abnormal events occur, and the query cost Rqi . We have\nRi \u2264 Rai +Rni +Rsi +Rqi . (16)\nLet us denote by Ti the number of time slots in epoch i, denote by Ti,m the number of context arrivals in context cluster Xm in epoch i, and denote by Ti,m,n the number of query requests for arm cluster Kn in context cluster Xm in epoch i. We set \u03b1 = 1dA+dX+2 and \u03b3 = dA+1 dA+dX+2\n. For the first term Rai in (16), when an abnormal event happens, the regret is at most Ti. According to Lemma 1 abnormal events happens with probability at most \u03b4(i) for arm cluster Kn in epoch i. Therefore, the regret Rai in (16) can be expressed as:\nRai \u2264 Ni\u2211\nn=1\n\u03b4(i)Ti \u2264 Ni\u03b4(i)Ti. (17)\nFor the second term Rni in (16), the regret of 2\u01eb(i)-optimal arm cluster selection at each time slot is at most 2\u01eb(i), and the regret of inaccuracy of clusters at each time slot is at most 2LX\u03c1X,i + 2LK\u03c1K,i. Therefore, the regret Rni can be expressed as:\nRni \u2264 2i+1\u22121\u2211 t=2i (2\u01eb(i) + 2LX\u03c1X,i + 2LK\u03c1K,i) = 2(\u01eb(i) + LX\u03c1X,i + LK\u03c1K,i)Ti.\n(18)\nFor the third term Rsi in (16), when the normal event occurs, according to Lemma 2, 2\u01eb(i)-suboptimal arm cluster can only be selected in the exploration phases. Hence, the regret Rsi can be expressed as:\nRsi \u2264 E \u2211\n\u2206m,n>2\u01eb(i) 2i+1\u22121\u2211 t=2i\n\u2206m,nI{xt \u2208 Xm, \u03c0tK \u2208 Kn, \u03c0tq = 1,Ni,m,n}. (19)\nAccording to the deactivating rule, for normal events, if the following is satisfied:\n\u2206m,n\u22122D(s)\u22122LX\u03c1X,i\u22122LK\u03c1K,i \u2265 r\u0304\u2217m(s)\u2212r\u0304m,n(s) \u2265 D1(i, s), (20) then the arm cluster is deactivated. Hence, the rounds of exploring arm cluster Kn, Ti,m,n with \u2206m,n > 2\u01eb(i), can be bounded by\nTi,m,n \u2264 8 ln(2T 1+\u03b3i )\n[\u2206m,n \u2212 (\u01eb(i) + 4LX\u03c1X,i + 4LK\u03c1K,i)]2 . (21)\nTherefore, the regret Rsi can be bounded by\nRsi \u2264 E \u2211\n\u2206m,n>2\u01eb(i)\n\u2206m,nTi,m,n\n\u2264 E \u2211 \u2206m,n>2\u01eb(i) ( 8 ln(2T 1+\u03b3i ) \u2206m,n\u2212(\u01eb(i)+4LX\u03c1X,i+4LK\u03c1K,i)\n+ 8(\u01eb(i)+4LX\u03c1X,i+4LK\u03c1K,i) ln(2T\n1+\u03b3 i )\n[\u2206m,n\u2212(\u01eb(i)+4LX\u03c1X,i+4LK\u03c1K,i)]2 )\n\u2264 8MiNi ln(2T 1+\u03b3 i )\n2\u01eb(i)\u2212(\u01eb(i)+4LX\u03c1X,i+4LK\u03c1K,i)\n+ 8MiNi(\u01eb(i)+4LX\u03c1X,i+4LK\u03c1K,i) ln(2T\n1+\u03b3 i )\n[2\u01eb(i)\u2212(\u01eb(i)+4LX\u03c1X,i+4LK\u03c1K,i)]2 \u2264 C1MiNi ln(2T 1+\u03b3i )T\u03b1i ,\n(22)\nwhere C1 = 16L(L\u22124LX\u22124LK)2 is a constant. For the fourth term Rqi in (16), we first consider the query cost Rq,1i when the abnormal event occurs. In this case, since\nthe maximum query cost per slot is 2c, the query cost can be bounded by\nRq,1i \u2264 Ni2c\u03b4(i)Ti. (23)\nNext, we consider the query cost Rq,2i in the case that only normal events occur. This can be bounded by\nRq,2i \u2264 E \u2211 m,n 2i+1\u22121\u2211 t=2i ctI{xt \u2208 Xm, \u03c0tK \u2208 Kn, \u03c0tq = 1} \u2264 E \u2211 m,n c+ E \u2211 m,n Sim\u2211 s=2 [c(4LX\u03c1X,i + 4LK\u03c1K,i + 4D(s\u2212 1))\u03b21\n+ c\u03b7T \u2212(1+\u03b3)\u03b22 i ]\n\u2264 cMiNi + cMiNi Sim\u2211 s=2 (8LX\u03c1X,i+8LK\u03c1K,i) \u03b21+(8D(s\u22121))\u03b21 2\n+ c\u03b7MiNi Sim\u2211 s=2 T \u2212(1+\u03b3)\u03b22 i ]\n\u2264 cMiNi + cMiNi23\u03b21\u22121(LX\u03c1X,i + LK\u03c1K,i)\u03b21Sim\n+cMiNi2 3\u03b21\u22121 Sim\u2211 s=2 [ln(2T \u03b3+1 i )] \u03b21/2 2\u03b21/2(s\u22121)\u03b21/2 + c\u03b7MiNiT \u2212(1+\u03b3)\u03b22+1 i ,\n(24) where the third inequality is due to the Jensen\u2019s inequality. If 1 \u2264 \u03b21 < 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5\u03b21/2\u22121 [ln(2T \u03b3+1 i )] \u03b21/2(Sim) 1\u2212\u03b21/2\n1\u2212\u03b21/2 , due to the divergent series \u2211T t=1 t\n\u2212y \u2264 T (1\u2212y)/(1 \u2212 y) for 0 < y < 1 [11]. If \u03b21 \u2265 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5\u03b21/2\u22121[ln(2T \u03b3+1i )] \u03b21/2(lnSim)\n\u03b21/2, due to the series\u2211T\u22121 t=1 t\n\u2212y \u2264 lnT for y \u2265 1. We can also have the bound of Sim (for all m) due to the fact that when D1(i, S i m) \u2264 D2(i, S i m), the stopping rule is satisfied. Hence, Sim can be bounded by the minimum s, such that D1(i, s) \u2264 D2(i, s). This shows:\nSim \u2264 8T 2\u03b1i ln(2T \u03b3+1 i )\n(L\u2212 4LX \u2212 4LK)2 . (25)\nThus, we can bound Rq,2i by\nRq,2i \u2264 { C2MiNiT \u03b1(2\u2212\u03b21) i ln(2T \u03b3+1 i ), if 1 \u2264 \u03b21 < 2\nC3MiNi[ln(2T \u03b3+1 i )] \u03b21 , if \u03b21 \u2265 2, (26)\nwhere C2 = c(1 + \u03b7) + c23\u03b21+5(LX+LK) \u03b21\n(L\u22124LX\u22124LK)2 + c26\u2212\u03b21/2\n(2\u2212\u03b21)(L\u22124LX\u22124LK)2 is a constant, C3 = c(1 + \u03b7 + 25\u03b21/2\u22121) + c2 3\u03b21+5(LX+LK) \u03b21\n(L\u22124LX\u22124LA)2 is a constant. According to the definition of covering dimensions [12], the maximum number of arm clusters can be bounded by Ni \u2264 CA\u03c1\u2212dAK,i in epoch i, and the maximum number of context clusters can be bounded by Mi \u2264 CX\u03c1\u2212dXX,i in epoch i, where CA, CX are covering constants for the arm space and\nthe context space. Hence, the regret can be bounded by\nR(T ) \u2264 E log2T\u2211 i=0 Ri \u2264 log2T\u2211 i=0 (\u03b4(i)Ni + 2c\u03b4(i)Ni + 2\u01eb(i) + 2LX\u03c1X,i + 2LK\u03c1K,i)Ti +E log2T\u2211 i=0 O(1) ln(2T 1+\u03b3i )T \u03b1 i \u2264 O(1)T dX+dA+1\ndX+dA+2 ln(T ). (27)\nTherefore, the result of Theorem 1 follows.\nWe further show a lower bound for the CB-AL algorithm. Since the proposed algorithm incurs the query cost when it requests a ground truth, the lower bound of the regret cannot be lower than that of the conventional contextual MAB setting where no query cost is incurred [1].\nTheorem 2. The regret of the CB-AL algorithm can be lowerbounded by R(T ) = \u2126(T dX+dK+1 dX+dK+2 ).\nTheorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3]."}, {"heading": "V. NUMERICAL RESULTS", "text": "We conduct illustrative experiments using synthetic data with 2-dimensional contexts and 2-dimensional arms. In our first experiment, we compare the performance of our proposed CB-AL algorithm with the Contextual Bandit algorithm and the Contextual Bandit Active Learning without considering prior information (CB-AL without prior information). The result is shown in Fig. 3. As we can see, the proposed CB-AL algorithm performs better, in terms of payoff, than the conventional bandit algorithm (by 16%) and the CBAL without prior information (by 13%) by the end of the experiment duration. In our second experiment, we show the payoffs achieved by our proposed CB-AL algorithm when the query cost varies (by changing cost parameters c in e.q. (1)). We show the result in Fig 4. We can see that as c increases from 0.1 to 1, the achieved payoff decreases by 15% for T = 10000 and by 8% for T = 20000."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we developed a contextual bandits learning algorithm with active learning capability. The active learning cost is reduced by providing prior information about the reward realization to the annotator. The algorithm maintains and updates partitions of the context and arm spaces, and operates between exploration and exploitation phases. Through precise control of the partitioning process and when to request the ground truth of the reward, the algorithm gracefully balances the accuracy of learning and the cost incurred by active learning. We prove that the regret of the proposed algorithm achieves the same order as that of conventional contextual bandits algorithms in cost-free scenarios."}], "references": [{"title": "and M", "author": ["T. Lu", "D. P\u00e1l"], "venue": "P\u00e1l, \u201cContextual multi-armed bandits,\u201d in Artificial Intelligence and Statistics Conference (AISTATS)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The epoch-greedy algorithm for multiarmed bandits with side information,", "author": ["J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Contextual bandits with similarity information.", "author": ["A. Slivkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "and M", "author": ["L. Song", "W. Hsu", "J. Xu"], "venue": "van der Schaar, \u201cUsing contextual learning to improve diagnostic accuracy: Application in breast cancer screening,\u201d IEEE Journal of Biomedical and Health Informatics, vol. 20, no. 3, pp. 902\u2013914", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "and M", "author": ["J. Xu", "T. Xing"], "venue": "van der Schaar, \u201cPersonalized course sequence recommendations,\u201d IEEE Transactions on Signal Processing, vol. 64, no. 20, pp. 5340\u20135352", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning literature survey,", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "and M", "author": ["D.A. Cohn", "Z. Ghahramani"], "venue": "I. Jordan, \u201cActive learning with statistical models,\u201d Journal of Artificial Intelligence Research, vol. 4, no. 1, pp. 129\u2013145", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical active learning algorithms,", "author": ["M.-F.F. Balcan", "V. Feldman"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Nigamy, \u201cEmploying em and pool-based active learning for text classification,", "author": ["K.A.K. McCallumzy"], "venue": "in Proc. International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Analysis of a greedy active learning strategy.", "author": ["S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "An approximate formula for a partial sum of the divergent p-series,", "author": ["E. Chlebus"], "venue": "Applied Mathematics Letters, vol. 22,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Lectures on analysis on metric spaces", "author": ["J. Heinonen"], "venue": "Springer Science & Business Media", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 282, "endOffset": 285}, {"referenceID": 4, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 313, "endOffset": 316}, {"referenceID": 5, "context": "Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7].", "startOffset": 249, "endOffset": 252}, {"referenceID": 6, "context": "Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7].", "startOffset": 252, "endOffset": 255}, {"referenceID": 7, "context": "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "For any context x \u2208 X , the reward of selecting arm k \u2208 K is r(x, k) \u2208 [0, 1], which is sampled according to some underlying but unknown distribution f(x, k).", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "We assume that the reward value space is [0, 1] for the ease of exposition but this assumption can be relaxed to account for any bounded interval.", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "As a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "As a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm.", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "If 1 \u2264 \u03b21 < 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5\u03b21/2\u22121 [ln(2T \u03b3+1 i )] 1(S m) 1\u2212\u03b21/2 1\u2212\u03b21/2 , due to the divergent series \u2211T t=1 t \u2212y \u2264 T (1\u2212y)/(1 \u2212 y) for 0 < y < 1 [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 11, "context": "According to the definition of covering dimensions [12], the maximum number of arm clusters can be bounded by Ni \u2264 CA\u03c1A K,i in epoch i, and the maximum number of context clusters can be bounded by Mi \u2264 CX\u03c1X X,i in epoch i, where CA, CX are covering constants for the arm space and the context space.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "Since the proposed algorithm incurs the query cost when it requests a ground truth, the lower bound of the regret cannot be lower than that of the conventional contextual MAB setting where no query cost is incurred [1].", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 2, "context": "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].", "startOffset": 177, "endOffset": 180}], "year": 2017, "abstractText": "Contextual bandit algorithms \u2013 a class of multiarmed bandit algorithms that exploit the contextual information \u2013 have been shown to be effective in solving sequential decision making problems under uncertainty. A common assumption adopted in the literature is that the realized (ground truth) reward by taking the selected action is observed by the learner at no cost, which, however, is not realistic in many practical scenarios. When observing the ground truth reward is costly, a key challenge for the learner is how to judiciously acquire the ground truth by assessing the benefits and costs in order to balance learning efficiency and learning cost. From the information theoretic perspective, a perhaps even more interesting question is how much efficiency might be lost due to this cost. In this paper, we design a novel contextual bandit-based learning algorithm and endow it with the active learning capability. The key feature of our algorithm is that in addition to sending a query to an annotator for the ground truth, prior information about the ground truth learned by the learner is sent together, thereby reducing the query cost. We prove that by carefully choosing the algorithm parameters, the learning regret of the proposed algorithm achieves the same order as that of conventional contextual bandit algorithms in cost-free scenarios, implying that, surprisingly, cost due to acquiring the ground truth does not increase the learning regret in the long-run. Our analysis shows that prior information about the ground truth plays a critical role in improving the system performance in scenarios where active learning is necessary.", "creator": "LaTeX with hyperref package"}}}