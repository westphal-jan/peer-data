{"id": "1701.08343", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2017", "title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices", "abstract": "in receiving a recent conference reading paper, we likewise have reported a unique rhythm algorithm transcription reasoning method based only on employing a merged - vocals output hidden markov model ( hmm ) module that explicitly describes significantly the multiple - voice structure behaviour of performed polyphonic drone music. additionally this regression model solves a major application problem of choosing conventional methods that could not properly describe the nature structure of incorporating multiple voices as desirable in composing polyrhythmic orchestral scores worldwide or in the statistical phenomenon of loose synchrony between voices. in computing this paper / we currently present a complete description of sounding the next proposed computation model and develop an inference technique, which is valid for any merged - amplifier output software hmms for instances which output probabilities depend on past viewing events. we also examine the influence models of meeting the echo architecture assumptions and parameters of the method in terms of specific accuracies extraction of rhythm rhythm transcription and associated voice item separation and perform comparative evaluations with six important other algorithms. compare using midi maestro recordings of classical piano pieces, though we found conclusions that the proposed model ultimately outperformed those other conventional methods by more than nearly 12 precision points scored in discovering the accuracy for polyrhythmic music performances conducted and performed almost as notably good as yet the best algorithms one for non - polyrhythmic performances. this reveals the universal state - of - consciousness the - ball art comparative methods principle of rhythm transcription using for the first scientific time found in the literature. publicly available source application codes are also provided free for future comparisons.", "histories": [["v1", "Sun, 29 Jan 2017 01:25:57 GMT  (656kb,D)", "http://arxiv.org/abs/1701.08343v1", "13 pages, 13 figures, version accepted to IEEE/ACM TASLP"]], "COMMENTS": "13 pages, 13 figures, version accepted to IEEE/ACM TASLP", "reviews": [], "SUBJECTS": "cs.AI cs.SD", "authors": ["eita nakamura", "kazuyoshi yoshii", "shigeki sagayama"], "accepted": false, "id": "1701.08343"}, "pdf": {"name": "1701.08343.pdf", "metadata": {"source": "CRF", "title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices", "authors": ["Eita Nakamura", "Kazuyoshi Yoshii", "Shigeki Sagayama"], "emails": ["enakamura@sap.ist.i.kyoto-u.ac.jp,", "yoshii@kuis.kyoto-u.ac.jp).", "sagayama@meiji.ac.jp)."], "sections": [{"heading": null, "text": "Index Terms\u2014Rhythm transcription, statistical music language model, model for polyphonic music scores, hidden Markov models, music performance model.\nI. INTRODUCTION\nMUSIC transcription is one of the most challengingproblems in music information processing. To obtain music scores, we need to extract pitch information from music audio signals. Recently pitch analysis for polyphonic (e.g. piano) music has been receiving much attention [1], [2]. To solve the other part of the transcription problem, many studies have been devoted to so-called rhythm transcription, that is, the problem of recognising quantised note lengths (or note values) of the musical notes in MIDI performances [3]\u2013[19].\nSince early studies in the 1980s, various methods have been proposed for rhythm transcription. As we explain in detail in Sec. II, the general trend has shifted to using machine learning techniques to capture what natural music scores are and how music performances fluctuate in time. One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20]. In spite of its importance and about 30 years of history, however, little comparative\nE. Nakamura and K. Yoshii are with the Graduate School of Informatics, Kyoto University, Kyoto 606-8501, Japan (e-mail: enakamura@sap.ist.i.kyoto-u.ac.jp, yoshii@kuis.kyoto-u.ac.jp). E. Nakamura is supported by the JSPS research fellowship (PD).\nS. Sagayama is with the Graduate School of Advanced Mathematical Sciences, Meiji University, Nakano, Tokyo 164-8525, Japan (e-mail: sagayama@meiji.ac.jp).\nevaluations on rhythm transcription have been reported in the literature and the state-of-the-art method has not been known.\nRhythm transcription also raises challenging problems of representing and modelling scores and performances for polyphonic music. This is because a polyphonic score has multilayer structure, where concurrently sounding notes are grouped into several streams, or voices1. As explained in Sec. II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances. Therefore solutions to explicitly describe the multiple-voice structure must be sought.\nFrom this point of view, in a recent conference [19], we reported a statistical model that can describe the multiplevoice structure of polyphonic music. The model is based on the merged-output HMM [23], [24], which describes polyphonic performances as merged outputs from multiple component HMMs, called voice HMMs, each of which describes the generative process of music scores and performances of one voice (Fig. 1). It was confirmed that the model outperformed conventional HMM-based methods for transcribing polyrhythmic performances.\nThe purpose of this paper is to discuss in detail the mergedoutput HMM and its inference technique. Due to the large size of the state space and the complex dependencies between variables, the standard Viterbi algorithm or its refined version [23] cannot be applied and a new inference technique is\n1In this paper, a \u2018voice\u2019 means a unit stream of musical notes that can contain chords. The score in Fig. 2, for example, has two voices corresponding to the left and right hands.\nar X\niv :1\n70 1.\n08 34\n3v 1\n[ cs\n.A I]\n2 9\nJa n\n20 17\nnecessary. This problem typically arises when a voice HMM is an autoregressive HMM, which is commonly used as music score/performance models where output probabilities of events (e.g. pitch, time, etc.) depend on past events. Using a trick of introducing an auxiliary variable to trace the history of output symbols similarly as in Ref. [24], we develop an inference technique that can work in a practical computer environment and could be applied for any merged-output HMMs with autoregressive voice HMMs.\nWe provide a complete description of the proposed model and examine the influence of its architecture and parameters. First, we explain details omitted in the previous paper including the description of the chord model and a switch of a coupling parameter between voice HMMs depending on pitch contexts. The effects are examined in terms of accuracies. Second, the determination of model parameters based on supervised learning is discussed and the influence of parameters of the performance model is investigated. Finally, a feature of the proposed method is its simultaneous voice separation and rhythm recognition. We examine this effect by evaluating accuracies of both voice separation and rhythm recognition and comparing with a cascading algorithm that performs voice separation first and then recognises rhythm.\nAnother contribution of this paper is to present results of systematic comparative evaluations to find the state-ofthe-art method. In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref. [19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25]. An evaluation measure for rhythm transcription, which is briefly sketched in Ref. [19], is explained in full detail together with its calculation algorithm.\nWe make public the source codes for the best models found (the proposed model and other two HMMs) as well as the evaluation tool to enable future comparisons [26]. We hope that these materials would encourage researchers interested in music transcription and symbolic music processing."}, {"heading": "II. RELATED WORK", "text": "Previous studies on rhythm transcription are reviewed in this section. The purpose is two-fold: First, we describe the historical development of models for rhythm transcription, some of which form bases of our model and some are subjects of our comparative evaluation. Second, we review how polyphony has been treated in previous studies in rhythm transcription and related fields and explain in details the motivations for explicitly modelling multiple voices. Part of discussions in Secs. II-B and II-C and the figures are quoted from Ref. [19] to make this section more informative and self-contained."}, {"heading": "A. Early Studies", "text": "Until the late 1990s, studies on rhythm transcription used models describing the process of quantising note durations and/or recognising the metre structure. Longuet-Higgins [3] developed a method for estimating the note values and the\nmetre structure simultaneously by recursively subdividing a time interval into two or three almost equally spaced parts that are likely to begin at note onsets. A similar method of dividing a time interval using template grids and an error function of onsets and inter-onset intervals (IOIs) has also been proposed [4]. Methods using preference rules for the ratios of quantised note durations have been developed by Chowning et al. [5] and Temperley et al. [6]. Desain et al. [7] proposed a connectionist approach that iteratively converts note durations so that adjacent durations tend to have simple integral ratios.\nDespite some successful results, these methods have limitations in principle. First, they use little or no information about sequential regularities of note values in music scores. Since there are many logically possible sequences of note values, such sequential regularities are important clues to finding the one that is most likely to appear in actual music scores. Second, tendencies of temporal fluctuations in human performances are described only roughly. In particular, the chord clustering\u2014that is, the identification of notes whose onsets are exactly simultaneous in the score\u2014is handled with thresholding or is not treated at all. Finally, the parameters of most of those algorithms are tuned manually and optimisation methods have not been developed. This means that one cannot utilise a data set of music scores and performances to learn the parameters, or only inefficient optimisation methods like grid search can be applied."}, {"heading": "B. Statistical Methods", "text": "Since around the year 2000, it has become popular to use statistical models, which enable us to utilise the statistical nature of music scores and performances. Usually two models, one describing the probability of a score (score model) and the other describing the probability of a performance given a score (performance model), are combined as a Bayesian model, and rhythm transcription can be formulated as maximum a posteriori estimation. Below we review representative models for rhythm transcription. We here consider only monophonic performances; polyphonic extensions are described in Sec. II-C.\nIn one class of HMMs for rhythm transcription, which we call note HMMs, a score is represented as a sequence of note values and described with a Markov model (Fig. 2) [8], [9].\nTo describe the temporal fluctuations in performances, one introduces a latent variable corresponding to a (local) tempo that is also described with a Markov model. An observed duration is described as a product of the note value and the tempo that is exposed to noise of onset times.\nIn another class of HMMs, which we call metrical HMMs, a different description is used for the score model [11]\u2013[13]. Instead of a Markov model of note values, a Markov process on a grid space representing beat positions of a unit interval, such as a bar, is considered (Fig. 2). The note values are given as differences between successive beat positions. Incorporation of the metre structure is an advantage of metrical HMMs.\nPCFG models have also been proposed [15], [16]. As in [3], a time interval in a score is recursively divided into shorter intervals until those corresponding to note values are obtained, and probabilities describe what particular divisions are likely. As an advantage, modifications of rhythms by inserting (splitting) notes can be naturally described with these models."}, {"heading": "C. Polyphonic Extensions", "text": "The note HMM has been extended to handle polyphonic performances [10]. This is done by representing a polyphonic score as a linear sequence of chords or, more precisely, note clusters consisting of one or more notes. Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29]. Chordal notes can be represented as self-transitions in the score model (Fig. 2) and their IOIs can be described with a probability distribution with a peak at zero. Polyphonic extension of metrical HMMs is possible in the same way.\nAlthough this simplified representation of polyphonic scores is logically possible, there are instances in which score and performance models based on this representation cannot describe the nature of polyphonic music appropriately. First, complex polyphonic scores such as polyrhythmic scores are forced to have unrealistically small probabilities. This is because such scores consist of rare rhythms in the simplified representation even if the component voices have common rhythms (Fig. 3). Second, the phenomenon of loose synchrony between voices (e.g. two hands in piano performances [21]), called voice asynchrony, cannot be described. For example,\ni (1) 1 i (1) 2 i (1) 3 i (1) 4i (1) 0\ni (2) 0 i (2) 1 i (2) 2 i (2) 3\nx1 x2 x3 x4 x5 x6 x7\nHMM 1\nHMM 2\nOutputs\ns\ni(1)\ni(2)\n1/2 i (1) 0 i (2) 0\n1\ni (1) 1 i (2) 0\n2\ni (1) 1 i (2) 1\n1\ni (1) 2 i (2) 1\n1\ni (1) 3 i (2) 1\n2\ni (1) 3 i (2) 2\n2\ni (1) 3 i (2) 3\n1\ni (1) 4 i (2) 3\nState representation of merged-output HMM\nFig. 4. A schematic illustration of the merged-output HMM. The symbols\ni (1) 0 and i (2) 0 represent auxiliary states to define the initial transitions.\nthe importance of incorporating the multiple-voice structure in the presence of voice asynchrony is well investigated in studies on score-performance matching [21], [22].\nTo describe the multiple-voice structure of polyphonic scores, an extension of the PCFG model called 2D PCFG model has been proposed [16], [25]. This model describes, in addition to the divisions of a time interval, duplications of intervals into two voices. Unfortunately, a tractable inference algorithm could not be obtained for the model, and the correct voice information had to be provided for evaluations. In a recent report, Takamune et al. [17] state that this problem is solved using a generalised LR parser. However, as we shall see in Sec. IV-B, their algorithm often fails to output results and the computational cost is quite high."}, {"heading": "D. Merged-Output HMM", "text": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24]. In the model, each voice is described with an HMM, called a voice HMM, and the total polyphonic music signal is represented as merged outputs from multiple voice HMMs (Fig. 4).\nMathematically the model is described as follows. Let us consider the case of two voices indexed by a variable s = 1, 2, and let i(s) denote the state variable, let \u03c0s(i\u2032, i) = P (i|i\u2032, s) denote the transition probability and let \u03c6s(x; i) = P (x|i, s) denote the output probability of each voice HMM (for some output symbol x). For each instance n, one voice sn is chosen by a Bernoulli process as sn \u223c Ber(\u03b11, \u03b12) where Ber is the Bernoulli distribution and its probability parameter \u03b1sn represents how likely the n-th output is generated from the HMM of voice sn. The chosen voice HMM then makes a state transition and outputs xn while the other voice HMM stays at the current state. The whole process is described as an HMM with a state space indexed by k = (s, i(1), i(2)) and the transition and output probabilities (in the non-interacting case [23]) are given as\nP (kn = k|kn\u22121 = k\u2032) = \u03b1s\u03c0s(i \u2032(s), i(s)) ( \u03b4s1\u03b4i\u2032(2)i(2) + \u03b4s2\u03b4i\u2032(1)i(1) ) , (1) P (xn|kn = k) = \u03c6s(xn; i(s)) (2)\nwhere \u03b4 is Kronecker\u2019s delta. A merged-output HMM with more than two voices can be constructed similarly.\nAs discussed in Ref. [19], the merged-output HMM can be seen as a variant of factorial HMM [30] in its most general sense. Unlike the standard factorial HMM, only one of the voice HMMs makes a state transition and outputs a symbol at each instant. Owing to this property the sequential regularity within each voice can be described efficiently in the mergedoutput HMM, even when notes in one voice are interrupted (in the time order) by notes of other voices. Accordingly necessary inference algorithms are also different as we will see in Sec. III-C."}, {"heading": "III. PROPOSED METHOD", "text": "We present a complete description of a rhythm transcription method based on merged-output HMM [19] that describes polyphonic performances with multiple-voice structure. The generative model is presented in Sec. III-A, the determination of model parameters is discussed in Sec. III-B and its inference algorithm that simultaneously yields rhythm transcription and voice separation is derived in Sec. III-C."}, {"heading": "A. Model Formulation", "text": "A merged-output HMM for rhythm transcription proposed in Ref. [19] is reviewed here with additional details. First, the description of the chord model is given, which was explained as a \u2018self-transition\u2019 in the note-value state space. Since selftransition is also used to represent repeated note values of two note clusters, it should be treated with care and we introduce a two-level hierarchical Markov model to solve the problem. Second, a refinement of switching the probability of choosing voice HMMs is given, which was not mentioned previously but necessary to improve the accuracy of voice separation.\nIn the following, a music score is specified by multiple sequences, corresponding to voices, of pitches and note values and a MIDI performance signal is specified by a sequence of pitches and onset times. In this paper we only consider note onsets and thus note length and IOI mean the same thing.\n1) Model for Each Voice: A voice HMM is constructed based on the note HMM [9], which is extended to explicitly model pitches in order to appropriately describe voices. If there are no chords, a score note is specified by a pair of pitch and note value. Note that to define N note lengths we need N+1 note onsets and thus N+1 score notes should be considered. Let N+1 be the number of score notes in one voice and let rn denote the note value of the n-th note. If there are no chords, the note values r = (rn)N+1n=1 are generated by a Markov chain with the following probabilities:\nP (r1) = \u03c0 ini(r1), (3)\nP (rn+1|rn) = \u03c0(rn, rn+1) (n = 1, . . . , N) (4) where \u03c0ini is the initial probability and \u03c0 is the (stationary) transition probability.\nTo describe chords, we extend the above Markov model to a two-level hierarchical Markov model with state variables (r, g). The variable r represents the note value of a note cluster and g indicates whether the next note onset belongs to the same\nnote cluster or not: If gn = 0 the n-th and (n+1)-th notes are in a note cluster and if gn = 1, they belong to different note clusters. The variable g also takes the values \u2018in\u2019 and \u2018out\u2019 to define the initial and exiting probabilities. The internal Markov model has the topology illustrated in Fig. 5 and is described with the following transition probabilities (\u03c1(r)g,g\u2032 = P (g \u2032|g; r)):\n\u03c1 (r) in,0 = \u03b2r, \u03c1 (r) in,1 = 1\u2212 \u03b2r, (5)\n\u03c1 (r) 0,0 = \u03b3r, \u03c1 (r) 0,1 = 1\u2212 \u03b3r, \u03c1 (r) 0,out = 0, (6) \u03c1 (r) 1,0 = \u03c1 (r) 1,1 = 0, \u03c1 (r) 1,out = 1 (7)\nwhere \u03b2r and \u03b3r are parameters controlling the number of notes in a note cluster. Denoting w = (r, g) and w\u2032 = (r\u2032, g\u2032), the transition probability of the hierarchical model is given as\n\u03be(w\u2032, w) := P (w|w\u2032) = \u03c1(r \u2032)\ng\u2032,out\u03c0(r \u2032, r)\u03c1 (r) in,g + \u03b4rr\u2032\u03c1 (r) g\u2032,g. (8)\nThe initial probability is given as \u03beini(w1) := P (w1) = \u03c0ini(r1)\u03c1 (r1) in,g1 . We notate w = (wn)N+1n=1 = (rn, gn) N+1 n=1 .\nTo describe the temporal fluctuations, we introduce a tempo variable, denoted by vn, that describes the local (inverse) tempo for the time interval between the n-th and (n+1)-th note onsets [28], [31]. To represent the variation of tempos, we put a Gaussian Markov process on the logarithm of the tempo variable un = ln vn as\nP (u1) = N(u1;uini, \u03c3 2 v,ini), (9)\nP (un+1|un) = N(un+1;un, \u03c32v) (10) where N( \u00b7 ;\u00b5,\u03a3) denotes the normal distribution with mean \u00b5 and variance \u03a3, and uini, \u03c3v,ini and \u03c3v are parameters. The parameter \u03c3v describes the amount of tempo changes. If the n-th and (n+1)-th notes belong to a note cluster (i.e. gn = 0), their IOI approximately obeys an exponential distribution [28] and the probability of the onset time of the (n+1)-th note, denoted by tn+1, is then given as\nP (tn+1|tn, vn, rn, gn = 0) = Exp(tn+1;\u03bb) (11) where Exp denotes the exponential distribution and \u03bb is the scale parameter, which controls the asynchrony of note onsets in a note cluster. Otherwise, tn+1 \u2212 tn has a duration corresponding to note value rn and the probability is described with a normal distribution as\nP (tn+1|tn, vn, rn, gn = 1) = N(tn+1; tn + rnvn, \u03c32t ). (12) Intuitively the parameter \u03c3t describes the amount of onset-time\nfluctuations due to human motor noise when a performer keeps a tempo. We do not put a distribution on the onset time of the first note t1 because we formulate the model to be invariant under time translations and this value would not affect any results of inference. We notate v = (vn)Nn=1 and t = (tn) N+1 n=1 .\nFinally we describe the generation of pitches p = (pn)N+1n=0 as a Markov chain (we introduce an auxiliary symbol p0 for later convenience). The probabilities are\nP (p1) = \u03b8(p0, p1), (13) P (pn|pn\u22121) = \u03b8(pn\u22121, pn) (n = 2, . . . , N+1) (14) where \u03b8(p\u2032, p) denotes the (stationary) transition probability and if p\u2032 = p0 it denotes the initial probability.\nThe above model can be summarised as an autoregressive HMM with hidden states (r, g,v) and outputs (p, t) (Fig. 6), which will be a voice HMM. Although so far the probabilities of pitches are independent of other variables, they will be significant once multiple voice HMMs are merged and the posterior probabilities are inferred.\n2) Model for Multiple Voices: We combine the multiple voice HMMs in Sec. III-A1 using the framework of mergedoutput HMMs (Sec. II-D). Since in piano performances, which are our main focus, polyrhythm and voice asynchrony usually involve the two hands, we consider a model with two voices, leaving a note that it is not difficult to formalise a model with more than two voices. In what follows, voices are indexed by a variable s = 1, 2, corresponding to the left and right hand in practice. All the variables and parameters are now considered for each voice and thus r(s) = (r(s)n )Ns+1n=1 is the sequence of note values in voice s, \u03c0s(r\u2032, r) their transition probability, etc. Simply speaking, the sequence of merged outputs is obtained by gathering the outputs of the voice HMMs and sorting them according to onset times. To derive inference algorithms that are computationally tractable, however, we should formulate a model that outputs notes incrementally in the order of observations. This can be done by introducing stochastic variables s = (sn)N+1n=1 , which indicate that the nth observed note belongs to voice sn and follow the probability sn \u223c Ber(\u03b11, \u03b12). The parameter \u03b1sn represents how likely the n-th note is generated from the HMM of voice sn.\nThe variable sn is determined in advance to the pitch, note value or onset time of the corresponding note in the generative process (which is described below). For rhythm transcription,\nhowever, dependence of the parameter \u03b1s on features of the given input (MIDI performance) can be introduced to improve the accuracy of voice separation. As such a feature, we use contexts of pitch that reflects the constraint on pitch intervals that can be simultaneously played by one hand. Defining phighn and plown as the highest and lowest pitch that is sounding simultaneously (but not necessarily having a simultaneous onset) with pn, we switch the value of \u03b1sn depending on whether pn \u2212 plown > 15 or not and whether phighn \u2212 pn > 15 or not (total of four cases), reflecting the fact that a pitch interval larger than 15 semitones is rarely played with one hand at a time. The effect of using this context-dependent \u03b1s is examined in Sec. IV-C.\nIf voice sn is chosen, then the HMM of voice sn outputs a note, and the hidden state of the other voice HMM is unchanged. Such a model can be described with an HMM with a state space labelled by k = (s, p(1), w(1), t(1), p(2), w(2), t(2), v). Here we have a single tempo variable v that is shared by the two voices in order to assure loose synchrony between them. The transition probability P (kn=k|kn\u22121=k\u2032), for n \u2265 2, is given as\n\u03b1sP (v|v\u2032)As(w(s), p(s), t(s)|w\u2032(s), p\u2032(s), t\u2032(s); v\u2032) \u00b7 [ \u03b4s1\u03b4w\u2032(2)w(2)\u03b4p\u2032(2)p(2)\u03b4(t \u2032(2) \u2212 t(2)) + (1\u2194 2) ]\n(15)\nwhere the expression \u2018(1\u21942)\u2019 means that the previous term is repeated with 1 and 2 interchanged and we have defined\nAs(w (s), p(s), t(s)|w\u2032(s), p\u2032(s), t\u2032(s); v\u2032)\n= \u03bes(w \u2032(s), w(s))\u03b8s(p \u2032(s), p(s))P (t(s)|t\u2032(s), v\u2032, w\u2032(s)) (16) and \u03b4 denotes Kronecker\u2019s delta for discrete variables and Dirac\u2019s delta function for continuous variables. The probability P (v|v\u2032) is defined in Eq. (10), and P (t(s)|t\u2032(s), v\u2032, w\u2032(s)) is defined in Eqs. (11) and (12). For note values the initial probability is given as P (r(s)1 ) = \u03c0 ini s (r (s) 1 ), and for pitches the initial probability is given in Eq. (13). The first onset times t(1)1 and t(2)1 do not have distributions, as explained in Sec. III-A1, and we practically set t(1)1 = t (2) 1 = t1 (the first observed onset time). Finally the output of the model is given as\npn = p (sn) n , tn = t (sn) n , (17)\nand thus the complete-data probability is written as\nP (k,p, t) =\nN+1\u220f\nn=1\nP (kn|kn\u22121)\u03b4pnp(sn)n \u03b4(tn \u2212 t (sn) n ). (18)\nHere N = N1 + N2 denotes the total number of score notes, and the following notations are used: v = (vn)Nn=1, p = (pn) N+1 n=1 , t = (tn) N+1 n=1 , and k = (kn) N+1 n=1 . Note that whereas p and t are observed quantities, p(1),p(2), t(1) and t(2) are not because we cannot directly observe the voice information encrypted in s. The graphical representation of the model is illustrated in Fig. 7."}, {"heading": "B. Model Parameters and Their Determination", "text": "We here summarise model parameters, explain how they can be determined from data and describe some reasonable constraints to improve the efficiency of parameter learning.\nInit\nOnset time\nPitch\nTempo\nNote value (voice 2)\ns1=1 s2=2 s3=1 s4=1 s5=2\nv1 v2 v3 v4\nt1 t2 t3 t4 t5\np1 p2 p3 p4 p5\nInit(1)\nInit(2)\nNote value (voice 1)\nInit(1)\nInit(2)\nVoice\nr\u0303 (2) 1\nr\u0303 (1) 1 r\u0303 (1) 2\nr\u0303 (2) 2\ng\u0303 (2) 1\ng\u0303 (1) 1 g\u0303 (1) 2\ng\u0303 (2) 2\nFig. 7. Graphical representation of the proposed merged-output HMM when the voice information is fixed. The variables with a tilde (r\u0303(s)n and g\u0303 (s) n ) represent note values for each voice without redundancies (see Sec. III-C). See also the caption of Fig. 6. Here we have independent initial distributions for note values and pitches of different voice HMMs.\nLet Np and Nr be the number of pitches and note values, which are set as 88 and 15 in our implementation in Sec. IV.\nThe score model for each voice HMM has the following parameters: \u03c0ini(r) [Nr], \u03c0(r, r\u2032) [N2r ], \u03b2r [Nr], \u03b3r [Nr], \u03b8(p0, p) [Np] and \u03b8(p, p\u2032) [N2p ], where the number in square brackets indicate the number of parameters. (The number of independent parameters may reduce because of normalisation conditions, which we shall not care here for simplicity.) These parameters can be determined with a data set of music scores with voice indications. For piano pieces, the two staffs in the grand staff notation can be used for the two voice HMMs. After representing notes in each voice as a sequence of note clusters as in Fig. 3, \u03c0ini(r) and \u03c0(r, r\u2032) can be obtained in a standard way. Determining \u03b8(p0, p) and \u03b8(p, p\u2032) is also straightforward. To determine the parameters \u03b2r and \u03b3r, we first define the frequency of note clusters containing m notes with note value r as f (r)m . Since \u03b2r is the proportion of note clusters containing more than one notes, it is given by\n\u03b2r = \u221e\u2211\nm=2\nf (r)m\n/ \u221e\u2211\nm\u2032=1\nf (r) m\u2032 . (19)\nThe \u03b3r can be obtained by matching the expected staying time at state with g = 0 (Fig. 5) as follows: \u221e\u2211 m=2 mf (r) m\n\u221e\u2211 m\u2032=2 f (r) m\u2032\n= \u3008m\u3009m\u22652 = \u221e\u2211\nm=2\nm\u03b3m\u22122r (1\u2212 \u03b3r) = 2\u2212 \u03b3r 1\u2212 \u03b3r .\nIn practice, the transition probability \u03b8(p, p\u2032) and the initial probabilities \u03b8(p0, p) and \u03c0ini(r) are often subject to the sparseness problem since the first one has a rather large number of parameters and for the last two only one sample from each piece can be used. To overcome this problem, we can reduce the number of parameters in the following way, which is used in our implementation. First, we approximate \u03b8(p, p\u2032) as a function of the interval p\u2032 \u2212 p, which reduces the number of parameters from N2p to 2Np. Second, we can approximate \u03b8(p0, p) by a gaussian function as\n\u03b8(p0, p) \u221d N(p;\u00b5p, \u03c32p). (20)\nFinally, for \u03c0ini(r), the stationary (unigram) probability obtained from \u03c0(r, r\u2032) can be used. Note that the pitch probabilities are only used to improve voice separation and their precise values do not much influence the results of rhythm transcription. Likewise the initial probabilities do not influence the results for most notes due to the Markov property.\nThe performance model has the following five parameters: \u03c3v , \u03c3iniv , vini(= exp(uini)), \u03c3t and \u03bb. These can be determined from performance data, for example, MIDI recordings of piano performances whose notes are matched to the corresponding notes in the scores. Among these the initial values, \u03c3iniv and vini(= exp(uini)) are most difficult to determine from data but again have limited influence as a prior for global tempo, which is supposed to be musically less important (see discussion in Sec. IV-A). In our implementation, they are simply set by hand. The method for determining the other parameters based on a principle of minimal prediction error is discussed in a previous study [28] and will not be repeated here.\nAn additional parameter for the merged-output HMM is \u03b1s, which is generally obtained by simply counting the number of notes in each voice or can be approximated simply by \u03b11 = \u03b12 = 1/2. In our implementation, we obtain four \u03b1s\u2019s depending on the context as described in Sec. III-A2, which is also straightforward.\nC. Inference Algorithm\nTo obtain the result of rhythm transcription using the model just described, we must estimate the most probable hidden state sequence k\u0302 = argmaxkP (k|p, t) given the observations (p, t). This gives us the voice information s\u0302 and the estimated note values r\u0302(1) and r\u0302(2). Let w\u0303(s) = (w\u0303(s)n )Nsn=1 = (r\u0303 (s) n , g\u0303 (s) n ) Ns n=1 be the reduced sequence of note values for voice s, which is obtained by, for all n, deleting the n-th element with sn 6= s in w\u0302(s). Then the score time \u03c4 (s)n of the n-th note onset in voice s is given by\n\u03c4 (s)n =\nn\u2212\u03b4ss1\u2211\nm=1\ng\u0303(s)m r\u0303 (s) m . (21)\nThe inference algorithm of merged-output HMM has been discussed previously [24]. Since a merged-output HMM can be seen as an HMM with a product state space, the Viterbi algorithm [20] can be applied for inference in principle. It was shown that owing to the specific form of transition probability matrix as in Eq. (1), the computational complexity for one Viterbi update can be reduced from O(4N21N22 ) to O(2N1N2(N1 +N2)) where Ns is the size of the state space of the s-th voice HMM. However, since the state space of the model in Sec. III-A2 involves both discrete and continuous variables, an exact inference in this way is difficult.\nTo solve this, we discretise the tempo variable, which practically has little influence when the step size is sufficiently small since tempo is restricted in a certain range in conventional music and vn always has uncertainty of O(\u03c3t/r(sn)n\u22121). Discretisation of tempo variables has also been used for audio-toscore alignment [32] and beat tracking [33]. Other continuous variables t, t(1) and t(2) can take only values of observed onset times and thus can, in effect, be treated as discrete\nvariables. Unfortunately the direct use of the Viterbi algorithm is impractical even with this discretisation. Let us roughly estimate the computational cost to see this. Let Np, Nw and Nv be the sizes of the state space for pitch, note value and tempo. Since onset times t(s)n could take N values, the size of the state space for kn is O(2N2pN2wN2Nv), and the computational cost for one Viterbi update is C = O(4N4pN4wN4N2v ). A rough estimation (Np \u223c 100, Nw \u223c 30, N \u223c 300, Nv \u223c 50) yields C \u223c 1028, which is intractable. Even after using the constraints of the transition probabilities in Eq. (15), we have C = O(4N3pN3wN3N2v ) \u223c 1022, which is still intractable.\nWe can avoid this intractable computational cost and derive an efficient inference algorithm by appropriately relating the hidden variables (p(1),p(2), t(1), t(2)) to observed quantities (p, t). We first introduce a variable hn = 1, 2, \u00b7 \u00b7 \u00b7 , which is defined as the smallest h \u2265 1 satisfying sn 6= sn\u2212h for each n. We find the following relations:\nhn =\n{ hn\u22121 + 1, sn = sn\u22121;\n1, sn 6= sn\u22121, (22)\n(p(s)n , t (s) n ) =\n{ (pn, tn), s = sn;\n(pn\u2212hn , tn\u2212hn), s 6= sn. (23)\nThis means that (p(1),p(2), t(1), t(2)) are determined if we are given (s,h,p, t) and the effective number of variables is reduced by using h. With this change of variables, we find\nP (k,p, t) = P (s,w(1),w(2),v,p(1),p(2), t(1), t(2),p, t)\n= P (s,h,w(1),w(2),v,p, t) = \u220f\nn\n{ \u03b1snP (vn|vn\u22121) [ \u03b4sn1\u03b4w(2)n w(2)n\u22121 + (1\u2194 2) ]\n\u00b7 [ \u03b4snsn\u22121\u03b4hn(hn\u22121+1)A same n + (1\u2212 \u03b4snsn\u22121)\u03b4hn1Adiffn ]} ,\nAsamen = Asn(w (sn) n , pn, tn|w (sn) n\u22121, pn\u22121, tn\u22121; vn\u22121),\nAdiffn = Asn(w (sn) n , pn, tn|w (sn) n\u22121, pn\u0303, tn\u0303; vn\u22121) (24)\nwhere n\u0303 in the last line should be replaced by n\u2212 hn\u22121 \u2212 1. We can now apply the Viterbi algorithm on the state space (s,h,w(1),w(2),v). Noting that the maximum possible value of hn is N and using the constraints of the transition probabilities, one finds that C = O(4NN3wN2v )(\u223c 1011), which is significantly smaller than the previous values. Note that so far no ad-hoc approximations have been introduced to reduce the computational complexity. Practically, we can set a smaller maximal value Nh(< N) of hn to obtain approximate optimisation, which further reduces the computational cost to O(4NhN3wN2v ). The number Nh can be regarded as the maximum number of succeeding notes played by one hand without being interrupted by the other hand. The choice of Nh and its dependency is discussed in Sec. IV-C1."}, {"heading": "IV. EVALUATION", "text": ""}, {"heading": "A. Methodology for Systematic Evaluation", "text": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.e. the number of necessary operations to correct an estimated result) are used as evaluation measures. These studies used the shift\noperation, which changes the score time of a particular note or equivalently, changes a note value, to count the number of note-wise rhythmic errors. Musically speaking, on the other hand, the relative note values are more important than the absolute note values, and the tempo error should also be considered. This is because there is arbitrariness in choosing the unit of note values: For example, a quarter note played in a tempo of 60 BPM has the same duration as a half note played in a tempo of 120 BPM. Since results of rhythm transcription often contain note values that are uniformly scaled from the correct values, which should not be considered as completely incorrect estimations [8], [34], we must take into account the scaling operation as well as the shift operation.\nAs shown in the example in Fig. 8, there can be local scaling operations and shift operations, and a reasonable definition of the editing cost is the least number No of operations consisting of Nsc scaling operations and Nsh shift operations (No = Nsc + Nsh). As explained in detail in the appendix, this rhythm correction cost can be calculated by a dynamic programming similarly as the Levenshtein distance. Definition and calculation of the rhythm correction cost in the polyphonic case are also discussed there. We use the rhythm correction rate R = No/N as an evaluation measure."}, {"heading": "B. Comparisons with Other Methods", "text": "We first present results of comparative evaluations. The purpose is to find out the state-of-the-art method of rhythm transcription and its relation to the proposed model. Among previous methods described in Sec. II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17]. The first five are relatively frequently cited and the last one is theoretically important as it provides an alternative way of statistically modelling multiple-voice structure (Sec. II-C).\n1) Setup: Two data sets of MIDI recordings of classical piano pieces were used. One (polyrhythmic data set) consisted of 30 performances of different (excerpts of) pieces that contained 2 against 3 or 3 against 4 polyrhythmic passages, and the other (non-polyrhythmic data set) consisted of 30 performances of different (excerpts of) pieces that did not contain polyrhythmic passages. Pieces by various composers, ranging from J. S. Bach to Debussy, were chosen and the players were also various: Some of the performances were\ntaken from the PEDB database [35], a few were performances we recorded, and the rests were taken from collections in public domain websites2.\nFor the proposed method, all normal, dotted, and triplet note values ranging from the whole note to the 32nd note were used as candidate note values. Parameters for the score model, \u03c0(r, r\u2032), \u03b2r, \u03b3r and \u03b8(p, p\u2032), and the value of \u03b1s were learned from a data set of classical piano pieces that had no overlap with the test data2. We set (\u00b5p, \u03c3p) = (54, 12) for the lefthand voice HMM and (70, 12) for the right-hand voice HMM (see Sec. III-B). Values for the parameters for the performance model, \u03c3v , \u03c3t and \u03bb, were taken from a previous study [28] (which used performance data different from ours). The used values were \u03c3\u0304v = 3.32 \u00d7 10\u22122, \u03c3\u0304t = 0.02 s and \u03bb\u0304 = 0.0101 s. For the tempo variable, we discretised vn into 50 values logarithmically equally spaced in the range of 0.3 to 1.5 s per quarter note (corresponding to 200 BPM and 40 BPM). We set vini as the central value of the range (0.671 s per quarter note or 89.4 BPM) and \u03c3iniv = 3\u03c3\u0304v . Nh was chosen as 30, which will be explained later (Sec. IV-C1).\nFor other methods, we used the default values provided in the source codes, except for the note HMM and the metrical HMM, which are closely related to our method. For these models, the parameters of the score models were also trained with the same score data set and the performance model was the same as that for the proposed model. The metrical HMM was build and learned for two cases, duple metres (2/4, 4/4, etc.) and triple metres (3/4, 6/4, etc.), and one of these models were chosen for each performance according to the likelihood.\nFor Melisma Analyzers, results of the metre analysis were used and the estimated tactus was scaled to a quarter note to use the results as rhythm transcriptions. For Connectionist Quantizer, which accepts only monophonic inputs, chord clustering was performed beforehand with a threshold of 35 ms on the IOIs of chordal notes. The algorithm was run for 100 iterations for each performance. Because this algorithm outputs note lengths in units of 10 ms without indications for tactus, the most frequent note length was taken as the quarter note value.\n2) Results: The distributions of rhythm correction rates, their averages and standard deviations are shown in Fig. 9. For clear illustration, the results for Connectionist Quantizer, which was much worse than the others, were omitted: The average (standard deviation, first, third quantiles) was 53.7% (18.5%, 43.8%, 67.3%) for the polyrhythmic data and 38.9% (13.9%, 28.2%, 47.3%) for the non-polyrhythmic data.\nAs shown in Table I, some performances were not properly processed by the 2D PCFG model and Melisma Analyzers. For the 2D PCFG model, because it took much time in processing some performances (executions lasted more than a week for some performances), every performance was run for at least 24 hours and only performances for which execution ended were treated as processed cases. Among 29 (out of 60) performances for which execution ended, 12 performances did not receive any results (because the parser did not succeed in accepting the performances) and those were also treated as\n2The list of used pieces is available on our web page [26].\n\u2018unprocessed\u2019 cases. To compare the results in the presence of these unprocessed cases, we calculated for each of the algorithms and for successfully processed performances the differences in rhythm corrections rates relative to the proposed model. Their average and standard error (corresponding to 1\u03c3 deviation in the t-test) are shown in Table I.\nFor the polyrhythmic data, it is clear that the proposed model outperformed the other methods, by more than 12 points in the accuracies and more than 3\u03c3 deviations in the\nstatistical significances. Among the other algorithms, the note HMM and metrical HMM had similar accuracies and were second best, and Connectionist Quantizer was the worst. These results quantitatively confirmed that modelling multiple-voice structure is indeed effective for polyrhythmic performances. Contrary to our expectation, the result for the 2D PCFG model was second worst for these data. This might be because that the algorithm using pruning cannot always find the optimal result and the model parameters have not been trained from data, both of which are difficult to ameliorate currently but could possibly be improved in the future. The results show that the statistical models with (almost fully) learned parameters (the proposed model, the note HMM and the metrical HMM) had better accuracies than the other statistical models with partly learned parameters or without parameter learning (the 2D PCFG model and Melisma Analyzer version 2) and other methods. A typical example of polyrhythmic performance that is almost correctly recognised by the proposed model but not by other methods is shown in Fig. 10 3. One finds that the 3Other examples and sound files are accessible in our demonstration web page [26].\n3 against 4 polyrhythm was properly recognised only by the proposed model (cf. Fig. 3).\nFor the non-polyrhythmic data, Connectionist Quantizer again had a far worst result and the differences among the other methods were much smaller (within 2 points) compared to the polyrhythmic case. The note HMM and metrical HMM had similar and best accuracies and the proposed model was the third best. The difference in the average values between the proposed model and the note HMM or the metrical HMM was less than 1 point and the statistical significance was 1.6\u03c3 and 1.3\u03c3, respectively. Presumably, the main reason that the note and metrical HMMs worked better is that the rhythmic pattern in the reduced sequence of note clusters is often simpler than that of melody/chords in each voice in the non-polyrhythmic case because of the principle of complementary rhythm [36]. In particular, notes/chords in a voice can have tied note values that are not contained in our candidate list (e.g. a quarter note + 16th note value for the last note of the first bar in the upper staff in the example of Fig. 11), which can also appear as a result of incorrect voice separation.\nIt is observed that the transcription by the merged-output HMM can produce desynchronised cumulative note values in different voices (e.g., the quarter note E[5 in the upper voice in Fig. 11 has a time span different from that of the corresponding notes in the lower voice). This is due to the lack of constraints to assure the matching of these cumulative note values and the simplification of independent voice HMMs. For the note HMM and the proposed model, there were grammatically wrong sequences of note values, for example, triplets that appear in single or two notes without completing a unit of beat (e.g. the triplet notes in the left-hand part in Fig. 11). Further improvements are desired by incorporating such constraints and interactions between voices into the model."}, {"heading": "C. Examining the Proposed Method", "text": "We here examine the proposed method in more details. 1) Dependency on Nh: In Sec. III-C we introduced a cutoff Nh in the inference algorithm to reduce the computational cost. In a previous study [24] that discussed the same cutoff, it has been empirically confirmed that Nh = 50 yields almost exact results for piano performances. Since it is difficult for our model to run the exact algorithm corresponding to Nh \u2192\u221e, we compared results of varying Nh up to 50 to investigate its dependency.\nAs shown in Fig. 12, the results were similar for Nh \u2265 20 and were exactly same for Nh \u2265 30. Based on this result, we used the value Nh = 30 for all other evaluations in this paper. Note that the sufficient value of Nh for exact inference may depend on data and that smaller values with sub-optimal estimations could yield better accuracies (as the case of Nh = 20 for our algorithm and data).\n2) Effect of the Chord Model: As explained in Sec. III-A, we propose a two-level hierarchical HMM for the description of chords, replacing self-transitions used in previous studies [10], [19]. To examine its effect in terms of accuracies, we directly compared the two cases implemented in the mergedoutput HMM. Since in the former case a self-transition is also used to describe repeated note values of two note clusters, post-processing using the onset-time output probabilities was performed on the results of Viterbi decoding to determine whether a self-transition describe chordal notes or not.\nThe average rhythm correction rate by the chord model using self-transitions was 15.69% for the polyrhythmic data and 9.12% for the non-polyrhythmic data. By comparing with the values in Fig. 9, our chord model was slightly better and the differences are 0.87 \u00b1 0.46 and 0.68 \u00b1 2.47 (statistical significance 1.9\u03c3 and 0.3\u03c3) for the two data sets. These results indicate that our chord model is not only conceptually simple but also seems to improve the accuracy slightly.\n3) Effect of Joint Estimation of Voice and Note Values and Voice Separation Accuracy: A feature of our method is the simultaneous estimation of voice and note values. An alternative approach is to use a cascading algorithm that performs voice separation first and then estimates note values using the estimated voices. To examine the effectiveness of the joint estimation approach, we implemented a cascading algorithm consisting of voice separation using only the pitch part of the model in Sec. III-A2 and rhythm transcription using two note HMMs with coupled tempos and compared it with the proposed method.\nThe average rhythm correction rate by the cascading algorithm was 16.89% for the polyrhythmic data and 9.67% for the non-polyrhythmic data. By comparing with the values in Fig. 9, we see that the proposed method was slightly better and the differences are 1.88\u00b11.64 and 1.42\u00b10.47 (statistical significance 1.1\u03c3 and 3.0\u03c3) for the two data sets. These results indicate the effectiveness of the joint estimation approach of the proposed method while the cascading algorithm may have practical importance because of its smaller computational cost.\nWe also measured the accuracy of voice separation (into two hands). The accuracy with the proposed model was 94.2% for the polyrhythmic data and 88.0% for the non-polyrhythmic data and with the cascading algorithm it was 93.8% and 92.5%. This indicates firstly that a similar (or higher) accuracy can be obtained by using only the pitch information and secondly that a higher accuracy of voice separation does not necessarily lead to a better rhythm recognition accuracy.\n4) Influence of the Model Parameters: In our implementation, parameters of the tempo variables (mainly \u03c3v , \u03c3t and \u03bb) were not optimised but adjusted to values measured in a completely different experiment [28]. Since these parameters play important roles of describing \u2018naturalness\u2019 of temporal\nfluctuations in music performance, we performed experiments to examine their influence.\nFig. 13 shows the results of measuring average rhythm correction rates for varying \u03c3v , \u03c3t and \u03bb around the value used for our implementation. When one parameter was varied, the other parameters were fixed to the original values. Results for the note HMM are also shown as references. We see that overall (with some exceptions) the parameters were optimal around the original values, which implies the universality of these parameters. For both models, we found values with a better accuracy (at least for one of the data sets) than the original values, suggesting the possibility of further optimisations.\nWe see relatively large influence of \u03c3t on the merged-output HMM and \u03bb on the note HMM. This can be explained by the fact that compared to the note HMM, the merged-output HMM must handle a more number of inter-note-cluster durations and a less number of chordal notes because of the presence of two\nvoices. Accordingly the \u03c3t, which controls the fluctuation of inter-note-cluster durations, has more chances and the \u03bb, which controls the asynchrony of chordal notes, has less chances to influence the results of the merged-output HMM.\nFinally we examined the effect of context-dependent \u03b1s described in Sec. III-A2. For this purpose we simply run the proposed method with uniformly distributed \u03b1s (\u03b11 = \u03b12 = 1/2). The average rhythm correction rate was slightly worse (17.96 \u00b1 2.49) for the polyrhythmic data and slightly better (7.76\u00b11.33) for the non-polyrhythmic data. On the other hand, the accuracy of voice separation was 30.4% (50.0%) for the polyrhythmic (non-polyrhythmic) data, which is much worse. The results confirm that the context-dependent \u03b1s is important to improve voice separation and provide yet another example that a more precise voice separation does not necessarily induce better rhythm recognition accuracy."}, {"heading": "V. CONCLUSION", "text": "We have described and examined a rhythm transcription method based on a merged-output HMM of polyphonic symbolic performance. This model has an internal structure consisting of multiple HMMs to solve the long-standing problem of properly describing the multiple-voice structure of polyphonic music. With the inference method derived in this paper, the algorithm can perform voice separation and notevalue recognition simultaneously. The technique of deriving inference algorithms with reduced computational cost can be applied to other merged-output HMMs with autoregressive voice HMMs, which are expectedly effective models of polyphonic music where the multiple-voice structure is significant.\nBy examining the proposed method, we also confirmed that simultaneously inferring the voice and rhythm information improved the accuracy of rhythm transcription compared to a cascading approach, even though it did not necessarily improve the accuracy of voice separation. On the other hand, transcribed results sometimes contained unwanted asynchrony between notes in different voices that have almost simultaneous notes onset times. This is because the model describes no information about the absolute onset time and there are no strong interactions between voices other than the shared tempo. The use of merged-output HMMs with interacting voice HMMs [23] could provide a solution in principle, but how to describe synchrony of global score times while retaining computational tractability is a remaining problem.\nWith evaluations comparing seven rhythm transcription methods, we found that the proposed method performed significantly better than others for polyrhythmic performances. For non-polyrhythmic performances, we found that the note HMM and metrical HMM had the best accuracies and the proposed method was almost as good as (but slightly worse than) these methods. These results revealed the state-of-theart methods for rhythm transcription that were different for the two kinds of data. While practically running two or more methods simultaneously and choosing the best result can be effective, developing a unified method that yields best results for both kinds of data is desired. Solving the above problem of unwanted asynchrony would be one key and constructing a model with variable number of voices would be another."}, {"heading": "APPENDIX A CALCULATION OF THE RHYTHM CORRECTION COST", "text": "Let us formulate the rhythm correction cost introduced in Sec. IV-A and derive an algorithm for calculating it. We first consider the monophonic case. Let rtruen and r est n be the correct and estimated note value of the n-th note length in the performance input (n = 1, . . . , N ). We consider a sequence of pairs (dn, en) of scaling factor dn and shift interval en for n = 1, . . . , N . To recover (rtruen ) N n=1 from (r est n ) N n=1 with the scaling and shift operations, we must have\nrtruen = dnr est n + en (25)\nfor n = 1, \u00b7 \u00b7 \u00b7 , N . The number of scaling operations and that of shift operations are formally defined as Nsc = #{n|dn+1 6= dn} and Nsh = #{n|en 6= 0}. The minimum number of editing operations No is determined by minimising Nsc +Nsh for all sequences (dn, en)Nn=1 satisfying Eq. (25). This is a special case of a generalised rhythm correction cost, which can be defined similarly as the minimum of wscNsc +wshNsh for some non-negative real numbers wsc and wsh.\nLet us now present a dynamic programming to calculate the rhythm correction cost No. We describe a general algorithm valid for any values of wsc and wsh. The algorithm can be derived in the same form as the Viterbi algorithm for HMMs. We define the \u2018state space\u2019 \u2126 as the set of all possible scaling operations, which can be constructed by taking ratios of all possible note values. The space \u2126 is finite since the set of note values is finite. The scaling cost (analogous to transition probability) Csc : \u2126\u00d7 \u2126\u2192 R is defined as\nCsc(dn\u22121, dn) =\n{ 0, if dn = dn\u22121;\nwsc, otherwise. (26)\nFor the initial value d1 the cost is defined as Csc(d1) = 0 if d1 = 1 and wsc otherwise. To describe whether a shift operation is necessary for the n-th note value after scaling, the shift cost (analogous to output probability) Csh : \u2126 \u2192 R is defined as\nCsh(dn) =\n{ 0, if rtruen = dnr est n ;\nwsh, otherwise. (27)\nDefining the total cost as\nC(d1, . . . , dN ) =\nN\u2211\nn=1\n(Csc(dn\u22121, dn) + Csh(dn)) (28)\n(we understand Csc(d0, d1) as Csc(d1)), we have the relation\nNo = min d1,...,dN C(d1, . . . , dN ). (29)\nThe right-hand side of Eq.(29) can be calculated by the Viterbi algorithm [20] with computational complexity O((#\u2126)2N) < O(N4rN) where Nr is the number of note-value types.\nNote that the above formulation is already valid in the presence of chords. Chordal notes are represented as notes with rn = 0. The error in clustering a chord, i.e., rtruen = 0 but restn 6= 0 or vice versa, can be corrected by a shift operation.\nWhen there are separated multiple voices, we can apply shift operations on each note in each voice and scaling operations on all voices simultaneously. If the estimated score time duration between the first notes of any two voices is different\nfrom that in the correct score, it must be corrected as well. The rhythm correction cost for multiple voices can be calculated by the same manner as above using as (restn ) N n=1 a sequence of by merging all \u03c4 (s)n+1 \u2212 \u03c4 (s) n for all s and 1 \u2264 n \u2264 Ns and \u03c4 (s) 1 for all s > 1 in the order of onset time, where \u03c4 (s) n is the score time of n-th note onset in voice s as defined in Eq. (21)."}, {"heading": "ACKNOWLEDGMENT", "text": "We are grateful to David Temperley, Norihiro Takamune and Henkjan Honing for providing their source codes. The author EN thanks Hiroaki Tanaka for useful discussions on mergedoutput HMM and Yoshiaki Bando for his help with running computer programs. This work is partially supported by JSPS KAKENHI Nos. 24220006, 26240025, 26280089, 26700020, 15K16054, 16H01744 and 16J05486, and JST CrestMuse, OngaCREST and OngaACCEL projects."}], "references": [{"title": "Automatic Music Transcription: Challenges and Future Directions", "author": ["E. Benetos", "S. Dixon", "D. Giannoulis", "H. Kirchhoff", "A. Klapuri"], "venue": "J. Intelligent Information Systems, vol. 41, no. 3, pp. 407\u2013434, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Mental Processes: Studies in Cognitive Science", "author": ["H. Longuet-Higgins"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Transcribe: A Comprehensive Autotranscription Program", "author": ["J. Pressing", "P. Lawrence"], "venue": "Proc. ICMC, pp. 343\u2013345, 1993.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Intelligent Systems for the Analysis of Digitized Acoustic Signals", "author": ["J. Chowning", "L. Rush", "B. Mont-Reynaud", "C. Chafe", "W. Schloss", "J. Smith"], "venue": "Tech. Rep. CCRMA, STAN-M-15, 1984.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Modeling Meter and Harmony: A Preference-Rule Approach", "author": ["D. Temperley", "D. Sleator"], "venue": "Comp. Mus. J., vol. 23, no. 1, pp. 10\u201327, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "The Quantization of Musical Time: A Connectionist Approach", "author": ["P. Desain", "H. Honing"], "venue": "Comp. Mus. J., vol. 13, no. 3, pp. 56\u201366, 1989.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Musical Rhythm Recognition Using Hidden Markov Model (in Japanese)", "author": ["T. Otsuki", "N. Saitou", "M. Nakai", "H. Shimodaira", "S. Sagayama"], "venue": "J. Information Processing Society of Japan, vol. 43, no. 2, pp. 245\u2013255, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden Markov Model for Automatic Transcription of MIDI Signals", "author": ["H. Takeda", "T. Otsuki", "N. Saito", "M. Nakai", "H. Shimodaira", "S. Sagayama"], "venue": "Proc. MMSP, pp. 428\u2013431, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Rhythm and Tempo Analysis Toward Automatic Music Transcription", "author": ["H. Takeda", "T. Nishimoto", "S. Sagayama"], "venue": "Proc. ICASSP, vol. 4, pp. 1317\u20131320, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated Rhythm Transcription", "author": ["C. Raphael"], "venue": "Proc. ISMIR, pp. 99\u2013 107, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "A Learning-Based Quantization: Unsupervised Estimation of the Model Parameters", "author": ["M. Hamanaka", "M. Goto", "H. Asoh", "N. Otsu"], "venue": "Proc. ICMC, pp. 369\u2013372, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization", "author": ["A. Cemgil", "B. Kappen"], "venue": "J. Artificial Intelligence Res., vol. 18, no. 1, pp. 45\u201381, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "A Unified Probabilistic Model for Polyphonic Music Analysis", "author": ["D. Temperley"], "venue": "J. New Music Res., vol. 38, no. 1, pp. 3\u201318, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving Metrical Grammar with Grammar Expansion", "author": ["M. Tanji", "D. Ando", "H. Iba"], "venue": "Proc. AI 2008 (Springer LNAI 5360), pp. 180\u2013 191, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic Model of Two-Dimensional Rhythm Tree Structure Representation for Automatic Transcription of Polyphonic MIDI Signals", "author": ["M. Tsuchiya", "K. Ochiai", "H. Kameoka", "S. Sagayama"], "venue": "Proc. APSIPA, pp. 1\u20136, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic Transcription from MIDI Signals of Music Performance Using 2-Dimensional LR Parser (in Japanese)", "author": ["N. Takamune", "H. Kameoka", "S. Sagayama"], "venue": "Tech. Rep. SIGMUS, vol. 2014-MUS-104, no. 7, pp. 1\u20136, 2014.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. YY, 2017  13", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Rhythm Transcription of MIDI Performances Based on Hierarchical Bayesian Modelling of Repetition and Modification of Musical Note Patterns", "author": ["E. Nakamura", "K. Itoyama", "K. Yoshii"], "venue": "Proc. EUSIPCO, pp. 1946\u2013 1950, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1946}, {"title": "Rhythm Transcription of Polyphonic MIDI Performances Based on a Merged-Output HMM for Multiple Voices", "author": ["E. Nakamura", "K. Yoshii", "S. Sagayama"], "venue": "Proc. SMC, pp. 338\u2013343, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "author": ["L. Rabiner"], "venue": "Proc. IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "Data Processing in Music Performance Research: Using Structural Information to Improve Score- Performance Matching", "author": ["H. Heijink", "L. Windsor", "P. Desain"], "venue": "Behavior Research Methods, Instruments, & Computers, vol. 32, no. 4, pp. 546\u2013554, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Improved Score-Performance Matching Using Both Structural and Temporal Information from MIDI Recordings", "author": ["B. Gingras", "S. McAdams"], "venue": "J. New Music Res., vol. 40, no. 1, pp. 43\u201357, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Merged-Output Hidden Markov Model for Score Following of MIDI Performance with Ornaments, Desynchronized Voices, Repeats and Skips", "author": ["E. Nakamura", "Y. Saito", "N. Ono", "S. Sagayama"], "venue": "Proc. Joint ICMC|SMC 2014, pp. 1185\u20131192, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Merged-Output HMM for Piano Fingering of Both Hands", "author": ["E. Nakamura", "N. Ono", "S. Sagayama"], "venue": "Proc. ISMIR, pp. 531\u2013536, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Context-Free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms", "author": ["H. Kameoka", "K. Ochiai", "M. Nakano", "M. Tsuchiya", "S. Sagayama"], "venue": "Proc. ISMIR, pp. 307\u2013312, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Demo Page of Polyphonic Rhythm Transcription", "author": ["E. Nakamura", "K. Yoshii", "S. Sagayama"], "venue": "[Online]. Available: http:// anonymous4721029.github.io/demo.html, Accessed on:", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Representation and Discovery of Vertical Patterns in Music", "author": ["D. Conklin"], "venue": "A. Smaill et al. (eds.), Music and Artificial Intelligence, Lecture Notes in Artificial Intelligence, Springer, pp. 32\u201342, 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments", "author": ["E. Nakamura", "N. Ono", "S. Sagayama", "K. Watanabe"], "venue": "J. New Music Res., vol. 44, no. 4, pp. 287\u2013304, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A Coupled Duration-Focused Architecture for Realtime Music to Score Alignment", "author": ["A. Cont"], "venue": "IEEE Trans. on PAMI, vol. 32, no. 6, pp. 974\u2013987, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorial Hidden Markov Models", "author": ["Z. Ghahramani", "M. Jordan"], "venue": "Machine Learning, vol. 29, pp. 245\u2013273, 1997.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Automatic Segmentation of Acoustic Musical Signals Using Hidden Markov Models", "author": ["C. Raphael"], "venue": "IEEE Trans. on PAMI, vol. 21, no. 4, pp. 360\u2013370, 1999.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "A Conditional Random Field Framework for Robust and Scalable Audio-to-Score Matching", "author": ["C. Joder", "S. Essid", "G. Richard"], "venue": "IEEE Trans. on ASLP, vol. 19, no. 8, pp. 2385\u20132397, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring Metrical Structure in Music Using Particle Filters", "author": ["F. Krebs", "A. Holzapfel", "A.T. Cemgil", "G. Widmer"], "venue": "IEEE Trans. on ASLP, vol. 23, no. 5, pp. 817\u2013827, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "On Tempo Tracking: Tempogram Representation and Kalman Filtering", "author": ["A.T. Cemgil", "B. Kappen", "P. Desain", "H. Honing"], "venue": "J. New Music Res., vol. 29, no. 4, pp. 259\u2013273, 2000.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "A New Music Database Describing Deviation Information of Performance Expressions", "author": ["M. Hashida", "T. Matsui", "H. Katayose"], "venue": "Proc. ISMIR, pp. 489\u2013494, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "piano) music has been receiving much attention [1], [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "To solve the other part of the transcription problem, many studies have been devoted to so-called rhythm transcription, that is, the problem of recognising quantised note lengths (or note values) of the musical notes in MIDI performances [3]\u2013[19].", "startOffset": 238, "endOffset": 241}, {"referenceID": 17, "context": "To solve the other part of the transcription problem, many studies have been devoted to so-called rhythm transcription, that is, the problem of recognising quantised note lengths (or note values) of the musical notes in MIDI performances [3]\u2013[19].", "startOffset": 242, "endOffset": 246}, {"referenceID": 6, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 57, "endOffset": 60}, {"referenceID": 11, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances.", "startOffset": 240, "endOffset": 244}, {"referenceID": 20, "context": "II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances.", "startOffset": 246, "endOffset": 250}, {"referenceID": 17, "context": "From this point of view, in a recent conference [19], we reported a statistical model that can describe the multiplevoice structure of polyphonic music.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "The model is based on the merged-output HMM [23], [24], which describes polyphonic performances as merged outputs from multiple component HMMs, called voice HMMs, each of which describes the generative process of music scores and performances of one voice (Fig.", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "The model is based on the merged-output HMM [23], [24], which describes polyphonic performances as merged outputs from multiple component HMMs, called voice HMMs, each of which describes the generative process of music scores and performances of one voice (Fig.", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "Due to the large size of the state space and the complex dependencies between variables, the standard Viterbi algorithm or its refined version [23] cannot be applied and a new inference technique is", "startOffset": 143, "endOffset": 147}, {"referenceID": 22, "context": "[24], we develop an inference technique that can work in a practical computer environment and could be applied for any merged-output HMMs with autoregressive voice HMMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 170, "endOffset": 173}, {"referenceID": 12, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 188, "endOffset": 192}, {"referenceID": 14, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 267, "endOffset": 271}, {"referenceID": 15, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 273, "endOffset": 277}, {"referenceID": 23, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 279, "endOffset": 283}, {"referenceID": 17, "context": "[19], is explained in full detail together with its calculation algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We make public the source codes for the best models found (the proposed model and other two HMMs) as well as the evaluation tool to enable future comparisons [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "[19] to make this section more informative and self-contained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Longuet-Higgins [3] developed a method for estimating the note values and the j \u0153 j \u0153 \u0153 \u0153 Note model", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "A similar method of dividing a time interval using template grids and an error function of onsets and inter-onset intervals (IOIs) has also been proposed [4].", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "[5] and Temperley et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] proposed a connectionist approach that iteratively converts note durations so that adjacent durations tend to have simple integral ratios.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "2) [8], [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "2) [8], [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "In another class of HMMs, which we call metrical HMMs, a different description is used for the score model [11]\u2013[13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "In another class of HMMs, which we call metrical HMMs, a different description is used for the score model [11]\u2013[13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "PCFG models have also been proposed [15], [16].", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "PCFG models have also been proposed [15], [16].", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "As in [3], a time interval in a score is recursively divided into shorter intervals until those corresponding to note values are obtained, and probabilities describe what particular divisions are likely.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "The note HMM has been extended to handle polyphonic performances [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29].", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "two hands in piano performances [21]), called voice asynchrony, cannot be described.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "the importance of incorporating the multiple-voice structure in the presence of voice asynchrony is well investigated in studies on score-performance matching [21], [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "the importance of incorporating the multiple-voice structure in the presence of voice asynchrony is well investigated in studies on score-performance matching [21], [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "To describe the multiple-voice structure of polyphonic scores, an extension of the PCFG model called 2D PCFG model has been proposed [16], [25].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "To describe the multiple-voice structure of polyphonic scores, an extension of the PCFG model called 2D PCFG model has been proposed [16], [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "[17] state that this problem is solved using a generalised LR parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 195, "endOffset": 199}, {"referenceID": 22, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 201, "endOffset": 205}, {"referenceID": 21, "context": "The whole process is described as an HMM with a state space indexed by k = (s, i, i) and the transition and output probabilities (in the non-interacting case [23]) are given as", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "[19], the merged-output HMM can be seen as a variant of factorial HMM [30] in its most general sense.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[19], the merged-output HMM can be seen as a variant of factorial HMM [30] in its most general sense.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We present a complete description of a rhythm transcription method based on merged-output HMM [19] that describes polyphonic performances with multiple-voice structure.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "[19] is reviewed here with additional details.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "1) Model for Each Voice: A voice HMM is constructed based on the note HMM [9], which is extended to explicitly model pitches in order to appropriately describe voices.", "startOffset": 74, "endOffset": 77}, {"referenceID": 26, "context": "To describe the temporal fluctuations, we introduce a tempo variable, denoted by vn, that describes the local (inverse) tempo for the time interval between the n-th and (n+1)-th note onsets [28], [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 29, "context": "To describe the temporal fluctuations, we introduce a tempo variable, denoted by vn, that describes the local (inverse) tempo for the time interval between the n-th and (n+1)-th note onsets [28], [31].", "startOffset": 196, "endOffset": 200}, {"referenceID": 26, "context": "gn = 0), their IOI approximately obeys an exponential distribution [28] and the probability of the onset time of the (n+1)-th note, denoted by tn+1, is then given as", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "The method for determining the other parameters based on a principle of minimal prediction error is discussed in a previous study [28] and will not be repeated here.", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "The inference algorithm of merged-output HMM has been discussed previously [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Since a merged-output HMM can be seen as an HMM with a product state space, the Viterbi algorithm [20] can be applied for inference in principle.", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "Discretisation of tempo variables has also been used for audio-toscore alignment [32] and beat tracking [33].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "Discretisation of tempo variables has also been used for audio-toscore alignment [32] and beat tracking [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "Since results of rhythm transcription often contain note values that are uniformly scaled from the correct values, which should not be considered as completely incorrect estimations [8], [34], we must take into account the scaling operation as well as the shift operation.", "startOffset": 182, "endOffset": 185}, {"referenceID": 32, "context": "Since results of rhythm transcription often contain note values that are uniformly scaled from the correct values, which should not be considered as completely incorrect estimations [8], [34], we must take into account the scaling operation as well as the shift operation.", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 203, "endOffset": 207}, {"referenceID": 33, "context": "taken from the PEDB database [35], a few were performances we recorded, and the rests were taken from collections in public domain websites2.", "startOffset": 29, "endOffset": 33}, {"referenceID": 26, "context": "Values for the parameters for the performance model, \u03c3v , \u03c3t and \u03bb, were taken from a previous study [28] (which used performance data different from ours).", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "2The list of used pieces is available on our web page [26].", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 42, "endOffset": 45}, {"referenceID": 15, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 42, "endOffset": 45}, {"referenceID": 15, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "Data set Model Unprocessed R \u2212Rprop [%] Polyrhythmic Note HMM [9] 0 12.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "8 Metrical HMM [11] 0 13.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "1 2-dim PCFG [17] 18 23.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "9 Melisma ver 1 [6] 9 17.", "startOffset": 16, "endOffset": 19}, {"referenceID": 12, "context": "7 Melisma ver 2 [14] 4 21.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "3 Connectionist [7] 0 38.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Non-polyrhythmic Note HMM [9] 0 \u22120.", "startOffset": 26, "endOffset": 29}, {"referenceID": 9, "context": "50 Metrical HMM [11] 0 \u22120.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "61 2-dim PCFG [17] 25 1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "64 Melisma ver 1 [6] 4 1.", "startOffset": 17, "endOffset": 20}, {"referenceID": 12, "context": "12 Melisma ver 2 [14] 11 \u22120.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "33 Connectionist [7] 0 30.", "startOffset": 17, "endOffset": 20}, {"referenceID": 24, "context": "3Other examples and sound files are accessible in our demonstration web page [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "In a previous study [24] that discussed the same cutoff, it has been empirically confirmed that Nh = 50 yields almost exact results for piano performances.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "III-A, we propose a two-level hierarchical HMM for the description of chords, replacing self-transitions used in previous studies [10], [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "III-A, we propose a two-level hierarchical HMM for the description of chords, replacing self-transitions used in previous studies [10], [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "4) Influence of the Model Parameters: In our implementation, parameters of the tempo variables (mainly \u03c3v , \u03c3t and \u03bb) were not optimised but adjusted to values measured in a completely different experiment [28].", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "The use of merged-output HMMs with interacting voice HMMs [23] could provide a solution in principle, but how to describe synchrony of global score times while retaining computational tractability is a remaining problem.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "(29) can be calculated by the Viterbi algorithm [20] with computational complexity O((#\u03a9)N) < O(N rN) where Nr is the number of note-value types.", "startOffset": 48, "endOffset": 52}], "year": 2017, "abstractText": "In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model (HMM) that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-ofthe-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.", "creator": "LaTeX with hyperref package"}}}