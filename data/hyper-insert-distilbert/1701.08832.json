{"id": "1701.08832", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Expert Level control of Ramp Metering based on Multi-task Deep Reinforcement Learning", "abstract": "... this whole article strongly shows how the then recent breakthroughs in reinforcement theory learning ( rl ) abilities that evolved have thus enabled robots to gradually learn where to play analog arcade simulator video games, simulated walk circuits or assemble colored bricks, can widely be used to help perform other tasks below that are currently held at the core thrust of engineering cyberphysical systems. we routinely present the first use of proprietary rl engines for investigating the parameter control of existing systems extensively modeled by previously discretized non - intrinsic linear partial differential programming equations ( pdes ). and devise via a novel algorithm to uniquely use new non - parametric control techniques for large structured multi - agent systems. we show how neural network based rl enables addressing the parameter control problem of discretized behavior pdes whose parameters overall are unknown, random, and time - varying. we simultaneously introduce to an algorithm of mutual polynomial weight regularization ( cp mwr ) which completely alleviates the fundamental curse level of dimensionality of multi - area agent control schemes instead by maintaining sharing experience between particular agents locally while giving each multiple agent individual the slightest opportunity to critically specialize its action policy so as to essentially tailor it globally to the local parameters of the part of the system it frequently is located in.", "histories": [["v1", "Mon, 30 Jan 2017 21:27:14 GMT  (1029kb,D)", "http://arxiv.org/abs/1701.08832v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["francois belletti", "daniel haziza", "gabriel gomes", "alexandre m bayen"], "accepted": false, "id": "1701.08832"}, "pdf": {"name": "1701.08832.pdf", "metadata": {"source": "CRF", "title": "Expert Level control of Ramp Metering based on Multi-task Deep Reinforcement Learning", "authors": ["Francois Belletti", "Daniel Haziza", "Gabriel Gomes", "Alexandre M. Bayen"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Deep learning, reinforcement learning, deep reinforcement learning, continuous control, transportation systems, macroscopic traffic models, Partial Differential Equations.\nI. INTRODUCTION\nIN the United States alone, the cost of direct and indirectconsequences of traffic congestion was estimated to 124 billions USD in 2013, this cost taking the form of time spent by commuters in traffic jam, air pollution, accidents, etc. It represents almost 1% of the country\u2019s GDP, and is expected to grow by 50% within the next 15 years. Dealing with this issue is becoming a priority of government agencies, as the\nU.S. Department of Transportation Budget rose to almost 100 billions USD in 2016. In this context, any improvement to travel times on highways can lead to tremendous nationwide and worldwide improvements for the economy and the environment.\nMaintaining and building road infrastructure, as well as urban planning are the most obvious ways to adapt the traffic network to the ever growing demand for mobility. However, changing the network architecture can only occur seldom and at an expensive cost.\nControl of traffic flow is an alternate approach to addressing this issue as it aims at using existing infrastructure more efficiently and adapt it dynamically to the demand. In this article, we introduce new techniques for traffic control based on advances in Reinforcement Learning (RL) and Neural Networks. As opposed to most commonly used approaches in traffic control, we want to achieve control in a model-free fashion, meaning that we do not assume any prior knowledge of a model or the parameters of its dynamics, and thus do not need to rely on the expensive and time consuming model calibration procedures.\nRecent developments in Reinforcement Learning (RL) have enabled machines to learn how to play video games with no other information than the screen display [1], remarkably beat champion human players at Go with the AlphaGo program [2], or complete various tasks including locomotion and simple problem solving [3], [4], [5]. The advent of policy gradient-based optimization algorithms enabling RL for highly-dimensional continuous control systems as can be found in [6], [7], has generalized model-free control to systems that were characteristically challenging for Q-learning. Qlearning approaches, although successful in [1] suffer from a curse of dimensionality in continuous control if they rely on discretizing the action space.\nIn this article, RL trains a traffic management policy able to control the metering of highway on-ramps. The current state of the art Ramp-Metering policy ALINEA [8] controls incoming flow in order to reach an optimal density locally. This optimal density depends on the model used and has to be manually specified to have an optimal control. Recently, nonparametric approaches based on Reinforcement Learning such as [9] or [10] have been proposed to achieve ramp-metering, but face two main limitations. These methods are not scalable\nar X\niv :1\n70 1.\n08 83\n2v 1\n[ cs\n.A I]\n3 0\nJa n\n20 17\n2 beyond a few on-ramps, and limit traffic management to onramp control.\nWe introduce a way to learn an optimal control policy with numerous agents, and demonstrate the flexibility of our approach by applying it to different scenarios. The following contributions of our work are presented in this article:\n\u2022 We introduce a framework to use RL as a generic optimization scheme that enables the control of discretized Partial Differential Equations (PDEs) in a robust and non-parametric manner. To our knowledge, this is the first use of RL for control of PDEs discretized by finite differencing. Discretized non-linear PDEs are notoriously difficult to control if the difference scheme used is nonsmooth and non-continuous, which is usually required to capture nonlinear and non-smooth features of the solution (as is the case here). \u2022 In the case of PDEs used to model traffic, we demonstrate on different examples an extensive control over boundary conditions as well as inner domain for the first time in a non-parametric way. We showcase the robustness of the approach and its ability to learn from real-world examples by assessing its performance after an extremely noisy training phase with stochastic variations in the underlying PDE parameters of the model used \u2022 We introduce an algorithm to train Neural Networks that we denote Mutual Weights Regularization (MWR) which enables the sharing of experience between agents and specialization of each agent thanks to Multi Task Learning [11]. MWR is a Neural Network training approach that allows Reinforcement Learning to train a policy in a multi-agent environment without being hampered by a curse of dimensionality with respect to the number of agents. Applied to the actual traffic control problem of \u201cRamp metering\u201d, our model-free approach achieves a control of a comparable level to the currently used and model-dependent implementation of ALINEA which constitutes the state art and was in our case calibrated by an world-wide renowned traffic engineer.\nWe first present the PDEs used to simulate traffic and introduce the generic PDE discretization scheme. A first simulator based on a Godunov scheme [12] is used to demonstrate the efficiency of our approach on multiples situations. The Berkeley Advanced Transportation Simulator (BeATS), a state of the art macroscopic simulator implementing a particular instantiation of the Godunov scheme sometimes referred to as the Cell Transmission Model [13] and used in traffic research [14] is also introduce, as we use it for our final benchmark. Traffic control is presented in the form of a Reinforcement Learning problem and we present the MWR algorithm to mitigate the issues arising from the high dimensionality of the problem. We eventually present the results we achieve, and compare our algorithm to the pre-existing state of the art techniques. This state of practice reference, an ALINEA control scheme calibrated by traffic engineers at California PATH can be considered representative of state of the art expert performance. To the best of our knowledge, we are the first to present a non-parametric scalable method calibrated\nwith RL that performs as well as the pre-existing parametric solutions provided by traffic engineering experts."}, {"heading": "II. MODELS AND METHODOLOGY", "text": ""}, {"heading": "A. Highway traffic PDE", "text": "A highway vehicle density may be modeled by a Partial Differential Equation (PDE) of the following form:\n\u2202t\u03c1+ \u2202xF (\u03c1) = 0. (LWR PDE)\nFor a given uniform vehicle density \u03c1, F (\u03c1) is the flow of the vehicles (in vehicles per time unit); F is the fundamental diagram, its maximum is called the critical density, and corresponds to the optimal density to maximize the flow. F usually has the following typical shape:\n\u2022 When the density \u03c1 is lower than the critical density \u03c1crit, the vehicle flow increases with the density \u2022 When the density exceeds this value, congestion appears, and adding more vehicles actually reduces the flow"}, {"heading": "B. Control of PDEs with reinforcement learning", "text": "Non-parametric control of PDEs takes the form in the present article of a Markov Decision Process (MDP) which is formally introduced below in III-A. The PDE in its discretized form devises a transition probability P between two different states of the system. Solving a MDP is a known procedure if the transition probability P is known beforehand, using techniques such as Dynamic or Linear Programming. If P is unknown, it is more challenging. However, it is more appealing a procedure as devising P for discretized PDEs is generally intractable and requires estimates of the parameters of the system. Also, as we operate in a continuous action space we will not consider Q-learning based approaches which are typically challenged by high-dimensionality in that setting. Therefore, policy gradient algorithms present a compelling opportunity as model dynamics are sampled from the simulation trials in algorithms such as [15], [16], [17], [18] and no prior knowledge of the model is necessary to train the policy. This creates a model-independent paradigm which abstracts out the model and makes the approach generic.\n3"}, {"heading": "C. Simulation", "text": "The experimental method we followed consists of two steps. A first one uses a coarser less realistic discretization scheme for the LWR PDE named after its inventor: Godunov [12]. We provide more details about this scheme in Appendix A. This step serves the purpose of a proof of concept that discretized PDEs can be controlled by a neural net trained using a RL scheme in completely different situations and with different objectives. Our first contribution is to show that policy gradient algorithms achieve that aim.\nIn our second step we use a more accurate cell transition model [13] scheme entailed in the state of the art BeATs simulator in order to show that the procedure we present is still valid in a more realistic setting. We also change the tasks we assigned to the control scheme in order to precisely account for the actual needs of traffic management systems used in production. We show that training our neural net based policy by policy gradient methods achieves comparable performance with the state of the art ALINEA control scheme [8] although the former is non-parametric when the latter requires a calibration of traffic related parameters. In both cases, the neural net manages to implicitly learn the intrinsic properties of the road segment under consideration and provide a good control policy."}, {"heading": "III. CONTROLLING CYBERPHYSICAL SYSTEMS WITH NEURAL NETWORKS TRAINED BY REINFORCEMENT LEARNING", "text": "In order to be able to control a complex cyberphysical system a non-parametric manner, we adopt a Reinforcement Learning formulation."}, {"heading": "A. Reinforcement Learning formulation", "text": "RL is concerned with solving a finite-horizon discounted Markov Decision Process (MDP). A MDP is defined by a tuple (S,A, P,R, P0, \u03b3, T ). The set of states is denoted S and will typically be Rd in our instance where d is the number of finite differences cells as in [12]. The action space A will correspond to a vector in Rd which represents the vehicular flow that the actuator lets enter the freeway which corresponds in the present case to the weak boundary condition implemented in the form of a ghost cell. The transition probability P : S\u00d7A\u00d7S \u2192 R+ is determined by the freeway traffic simulator we use i.e. the Godunov discretization scheme and the stochastic queue arrival model devised, discussed below. Random events such as perturbations to the input flow of vehicles or accidents affect the otherwise deterministic dynamics of the discretized system. The transition probability P is affected by these events and there likelihood but does not need to be known analytically for the system to operate nor be estimated. This is one of the key advantages offered by Reinforcement Learning over other approaches. The real valued reward function R is for the practitioner to define which implies that the same training algorithm can be used to achieve different objectives. The initial state distribution is denoted P0, the discount factor \u03b3 and the horizon T . Generically, RL\nconsists in finding the policy \u03c0\u03b8 : S\u00d7A\u2192 R+ that maximizes the expected discounted reward\nE (\u03b7(\u03c0)) where \u03b7(\u03c0) := Tmax\u2211 t=0 \u03b3trt\nWe denote \u03c4 = (s0, a0, . . .) the representation of a trajectory defined by the probability distributions s0 \u223c P0, at \u223c \u03c0(at|st), the state transition probability st+1 \u223c P (st+1|st, at) and the reward distributioon rt \u223c P (rt|st, at). We will consider a stochastic policy which defines a probability distribution of at conditional to st (or the observation of the state at time t) parametrized by \u03b8. This creates a stochastic regularization of the objective to maximize and enables to computation of the gradient of a policy with respect to its parameters in spite of the dynamics of the system under consideration not being differentiable, continuous, or even known.\n1) Reinforcement Learning based control of discretized PDEs: The recent developments in RL featured in [19], [7], [20] guarantee convergence properties similar to those of standard control methods and therefore strongly motivates their usage for the control of PDEs. On the contrary, since they are being model-independent, they are intrinsically robust to varying parameters and are able to track parameter slippage. This leads us to consider them as the new generation of generic control schemes. We show how the use of RL on discretized PDEs enables the extension of schemes to systems featuring random dynamics, unknown parameters and regime changes, hence surpassing parametric control schemes. The MPC approach in [21] and the adjoint method based technique of [22] both rely on the definition of a cost function which needs to be minimized. For PDEs such as the LWR PDE, one typically maximizes throughput, minimizes delay, or a functional of the state (for example encompassing energy emissions). A RL approach will therefore focus on maximizing a decreasing function of that cost which will be our accumulated reward. This is standard practice to encode an operational objective.\n2) State and action space.: Consider a discretized approximation of the solution y to Eq. (3) (see appendix) by the Godunov scheme described in appendix. The solution domain is [0, T ] \u00d7 [0, L], the discretization resolutions \u2206t and \u2206x satisfying \u2206t \u2264 c\u2206x are chosen to meet well posedness requirements (Courant-Friedrichs-Lewy condition where c is the maximum characteristic speed of Eq. (3)). The solution to the equation is approximated by a piecewise constant solution computed at the discrete time-space points {0,\u2206t, . . . , T \u2212\u2206t, T} \u00d7 {0,\u2206x, . . . , L\u2212\u2206x, L}. The action space for this system consists of incoming flow at the discretized elements {0,\u2206t, . . . , T \u2212\u2206t, T} \u00d7 {0}, and generally belongs to a bounded domain [0 . . . umaxi,j ]. The policy will control this vector of incoming flows at each time step.\n4"}, {"heading": "B. Neural Networks for Parametrized stochastic policies.", "text": "In this paragraph we show how to construct an actuator based on a Neural Network.\n1) Parametrized stochastic policies.: A vast family of stochastic policies are available for us to use so as to choose an action conditionally to an observation of the state. A common paradigm is to create a regression operator, typically a Neural Network, which is going to determine the values of the parameters of a probability distribution over the actions based on the space observation. We practically use a Multilayer Perceptron that determines the mean and covariance of a Gaussian distribution over the action space. The action the policy undertakes is sampled from this parametrized distribution and will manage to maximize its expected rewards provided a reliable training algorithm is used."}, {"heading": "C. Neural Networks", "text": "The policy we train are implemented as Artificial Neural Networks, containing Artificial Neural wired together.\n1) Artificial Neural Model: For p \u2208 N, an Artificial Neural computes an output y \u2208 R from an input vector X \u2208 Rp with the following formula:\ny = \u03c6(WX + b)\nWhere \u03c6 : R \u2192 R is called the activation function which responds to the outcome of an affine transformation of its input space parametrized by W \u2208 Rp and b \u2208 R.\n2) Networks: A group of q artificial neurons can be linked to a single input X \u2208 Rp to create an output vector Y \u2208 Rq . This forms a neural layer. When several layers are stacked, with the output of one being the input of the next one, one can speak of an Artificial Neural Network, whose input is the input of the first layer, and output is the output of the last layer.\nThe general organization and architecture of such a network may vary depending on usages, the type of input data to process. In the setting of computer vision, convolutional neural networks famously achieved human level image classification thanks to the invariance by translation and rotation of the convolution masks they progressively learnt [23].\nBack propagation In order to train Neural Networks backpropagation [24] is a key algorithm that performs a stochastic gradient descent on a non-convex function [25]. Approaches to train such a Neural Network for control in the Q-learning framework has been adopted in [1] and were successful in a discrete control setting. With continuous control, a different family of methods is generally used that encourage the policy entailed in the network parameters to take actions that are on average advantageous and discourage actions that have an expected negative reward."}, {"heading": "D. Training algorithms in a RL context", "text": "Modern training algorithms for continuous control stochastic policies can be divided in policy gradient-based approaches\nand non-gradient-based approaches. The former family encompasses first order methods such as REINFORCE [7] which we will denote Vanilla Policy Gradient (VPG), approximated second order methods based on the use of natural gradient descent [15], local line search methods such as Trust Region Policy Optimization (TRPO) [20], LBGFs inspired methods such as Penalized Policy Optimization (PPO) [5] and gradient free approaches such as the cross-entropy method [26]. The performance of these algorithms have been thoroughly compared in [5] where the natural gradient based method Truncated Natural Policy Gradient (TNPG) and TRPO generally outperformed other approaches. In our numerical experiments we find that when the statistical patterns at the stochastic boundary conditions are stationary enough, all approaches perform conveniently. However, TNPG and TRPO outperform other methods when regime changes occur.\n1) REINFORCE: The Reinforce algorithm [7] has been used to train our policy to maximize E (\u03b7(\u03c0)).\nWe consider a parametric policy and we denote by \u03b8 its parameters. In our case, the policy is a neural network parametrized by its weights. The input layer is filled with the environment observation, and the output layer contains the action probability distribution.\nFor a \u2208 A, and s \u2208 S, we note \u03c0\u03b8(a|s) the probability to take action a while being under state s, and following the policy parametrized by \u03b8.\nIn practice, we use the following equality to compute a gradient average across multiple trajectories:\n\u2202\u03b8E (\u03b7(\u03c0\u03b8)) = E (( Tmax\u2211 t=0 \u2202\u03b8 log (\u03c0\u03b8 (at|st)) ) Tmax\u2211 t=0 \u03b3trt ) The right side term can be approximated by simply running enough simulations with the given policy according to the law of large numbers.\nOnce we obtain the gradient \u2202\u03b8E (\u03b7(\u03c0\u03b8)) we can perform a gradient ascent on the parameters to incrementally improve our policy.\n2) Architecture and choices: Beside these theoretical considerations and algorithms, the network architecture choice has a crucial impact on the training of the policy. If the layers\n5 Agent\nAgent\nAgent\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nA, b\nL2 regularization to generalize decision\nmaking in topmost layers\nSpecialization\nSpecialization\nSpecialization\nMutual Weight Regularization\nForward pass: computation of regularization term\nBack-propagation: send regularization gradient\nFig. 3: Sharing experience across agents while allowing for specialization.\nare not adapted to the input data, the training algorithm will converge to a bad local minimum, or may not converge at all.\nIn our settings, the observation consists of a n \u00d7 3 array, where n is the number of discretization cells of the highway in the simulator, and every cell contains 3 information: \u2022 The vehicle density scaled to have a median value of 0,\nand a standard deviation of 1 in average \u2022 A boolean value indicating the presence of an off-ramp \u2022 A logarithmic scaled value indicating the number of\nvehicle waiting in the on-ramp queue if there is any, 0 otherwise.\nAs this data is spatially structured, we chose to process it with convolutionnal neural layers the core idea being to handle data in a spatially invariant way. Local features are created as a function of these local values independently of the highway location. A pipeline of 3 convolutionnal neural network layers are stacked to create local features. A last layer on the top of these convolution takes the action which consists in deciding how many vehicle can enter at the highway respective onramp. This practically achieve by ramp metering traffic lights. This algorithm is illustrated in Figure 3.\n3) Sharing information while allowing specialization among agents: the Mutual Weight Regularization algorithm: The features used for decision making on the low level layers of the network are created with a convolutionnal neural network, which exploits the spatial invariance of the problem.\nThere are two possible situations for the last layer: \u2022 Parameters sharing for all on-ramp policies. This results\ninto having the exact same policy for all agents. This\nshould not be the case because of local specificity of the highway, such as a reduced number of lanes, or a different speed limit for instance. \u2022 Every on-ramp has its dedicated set of parameters. It allows more flexibility and different control for every onramp, but dramatically increases the number of parameters, does not share learning between agents, and finally does not converge to a good policy\nThe novel approach we introduce and call Mutual Weight Regularization (MWR) is between these 2 extremes. It acknowledges the fact that experiences and feedback should be shared between agents, to mitigate the combinatorial explosion when the number of agents scales, and still allows some agent specific modifications to adapt to local variations.\nLet us consider:\n\u2022 m \u2208 N the number of features computed per cell. \u2022 i \u2208 R where R is the set of cells linked to a controllable\non-ramp. \u2022 X \u2208 Mn,m(R) the output of the convolutionnal layers\nand we note X[i] the features of the agent i.\nWe introduce W0,W1, . . . ,Wn \u2208 Rm the distinct parameters of every agent, and define, for i \u2208 R, yi as\nyi = \u3008Wi , X[i]\u3009\nThe decision for the average of the distribution of actions of a given agent will be determined by a non-linear transforms of yi. Similarly for the variance of this Gaussian stochastic policy distribution [27].\nThe MWR methods consist in adding a regularization term to the global gradient used for the gradient ascent:\n\u2202\u03b8E \u03b7(\u03c0\u03b8)\u2212 \u03b1 2 n\u2211 j=1 (Wj \u2212W0)2  = (1)\n\u2202\u03b8E (\u03b7(\u03c0\u03b8))\u2212 \u03b1 n\u2211 j=1 (Wj \u2212W0) \u2202\u03b8E (Wj \u2212W0) (2)\nWhere the hyperparameter \u03b1 defines the strength of the regularization and therefore how much mutual information is shared between agents in the gradient descent. Note that:\n\u2022 \u03b1 = 0 is equivalent to having independant policies for every on-ramp. \u2022 \u03b1 = \u221e is equivalent to having a shared policy making algorithm for every on-ramp (shares weights). \u2022 W0 is not actually used for control computation, but is rather a reference weight."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Proof of concept on different scenario cases", "text": "In this first experimentation set, we demonstrate that RL can be used to control PDEs in a robust and generic way. The same training procedure converges to a successful policy for two very different tasks. In this section, simulations are run using a simple Godunov discretization scheme for the LWR PDE (Eq. (3)), see appendix.\n6 1) Highway outflow control: Traffic management scenario. We consider a 5 mile section of I80W starting from the metering toll plaza and ending within San Francisco (see Figure 5. The flow is metered at the toll plaza at a rate shown in Figure 5 (iii) (i.e. a vehicle rate). The Godunov scheme is implemented with 20 cells and simulated over a time span enabling several bridge crossings. The inflow integrates random arrival rates for inbound traffic, which consists of a sinusoid perturbed by random noise. The state is vehicular density (i.e. \u03c1(t, x) as defined by the LWR PDE, vehicles per unit length) from which flow F (\u03c1(t, x)) (numbers of vehicles per unit time at x) can be computed.\nAction space and environment We consider the following operational scenario. The number of vehicles upstream from the meter (see Figure 5 (iii)) is randomized for each simulation in order to reproduce the diversity of freeway flow dynamics as they actually occur. At every time step, the observation forward propagated in the policy Neural Network is the value of current time step in time unit. An action undertaken is also a positive scalar representing the number of cars permitted to enter the highway per time unit. We train the policy to reproduce a pattern z(t) on the outflow with the following reward\nR (st, at) = \u2212 (\u03c1(t, L)\u2212 z(t))2 .\nThe choice of this reward function encodes our intention to replicate the function z(t) on the downstream boundary condition (located at spatial offset L). In particular, what is remarkable here is that this is the only way the environment provides information to the policy about the state of the freeway. Indeed, we only provide the current time step t as state observation to our policy along with the reward associated with the result obtained after a simulation roll out.\nLearning in the presence of disruptive perturbations The discretized model we use is well-suited for representing traffic accidents. The state update mechanism may be randomly altered by accidents which drastically change the maximum flow the freeway can carry at local points in space. Accidents can be simulated by locally decreasing the maximum flow speed in a given discretization cell for a given time interval. A key goal of our work is alleviating the impact of accidents while simultaneously tracking operational objectives (e.g. desired outflow of the bridge into the city), and achieving it with a robust and generic method is a tremendous breakthrough for urban planning. See the appendix for a presentation of the accident scenario.\nLearning algorithms and convergence to an effective policy In Figure 5, we analyze the results of the control scheme learnt by a given policy consisting of two fully connected hidden layers of size 32. The policy controls the inflow of cars (boundary control). We choose an arrival rate sufficiently high on average to provide the controller with sufficient numbers of vehicles to match the prescribed downstream conditions (note obviously in congestion this is always the case). The results prove that in spite of the problem being non-differentiable, non-smooth, non-linear and perturbed drastically by unpredictable accidents and random\ninput queues, the policy converges to a control scheme that manages to replicate the objective density. The learning phase uses different policy update methods such as [15], [6], [20] and benchmarked in [5]. In this benchmark, among gradient based methods, TNPG, TRPO and PPO seem to outperform the simpler REINFORCE method which only leverages first order gradient information. In Figure 6 we show how PPO, TRPO and REINFORCE are all reliable in this instance and converge to more effective policies than TNPG. It is also noteworthy that PPO converges faster to a plateau of rewards.\n2) Inner domain control: Reward shaping, in the form of assigning a target density and penalizing the L2 distance between the observed density and the objective enables us to reproduce an arbitrary image with the solution density in the solution domain only by controlling it on the boundaries. The results in Figure 7 demonstrate the ability of the method we present to train a policy to extensively control the values of a solution to the discretized PDE we study in its solution domain. The action space here is much higher dimensional as ramps are present all along the freeway that can let cars in at a sequence spatial offsets each separated by 3 cells. An off ramp split model handles the vehicle leaving the freeways. In spite of the increased dimensionality, a neural net with tree fully connected hidden layers of size 64 trained by the TRPO method converges to a policy capable of reproducing a target solution in the interior domain as shown in Figure 7, whereas TNPG failed in this instance to converge to an efficient policy. From a practitioner\u2019s perspective, this example is very powerful, it shows the ability to generate arbitrary congestion patterns based on metering along the freeway. From a PDE control standpoint, this is even more powerful, as direct state actuation is a very hard problem in manufacturing and has a lot of applications with PDEs such as the (nonlinear) heat equation, Hamilton-Jacobi equations, and several others."}, {"heading": "B. Optimal ramp metering control", "text": "In order to demonstrate the applicability of this novel method to real world cases, we consider the Ramp-Metering problem in a 20 miles (33 km) long section of the 210 Eastbound freeway in southern California as illustrated in Figure 8. For this simulation, we use the BeATS simulator calibrated by traffic experts based on real-world data. Every simulation run lasts for 4 hours, after a 30 minutes warmup period to initialize the freeway. The simulation starts at 12pm and the traffic peak happens between 3pm and 4pm, as the demand curve reaches a maximum (Fig. 14).\nTwo reinforcement learning algorithms and the ALINEA control scheme are benchmarked against the baseline scenario in which no control occurs at all: \u2022 NoRM, baseline: The baseline without any ramp meter-\ning. Cars instantly enter the freeway when they reach an onramp, and if the freeway has enough capacity \u2022 NoMWR, standard deep reinforcement learning: The Reinforcement Learning based policy we introduce, trained with shared weights for the last layer \u2022 MWR, novel approach to training: The same policy as NoMWR, but trained with MWR.\n7\n\u2022 Alinea, parametric control: The state of the art reference algorithm, using with model and parameters the exact same values used in the simulator it is being benchmarked on.\n1) Reinforcement Learning problem: In this scenario, the agent takes an action every 32 seconds. An action is a R29 vector with the ramp-metering rates for the 29 on-ramps on the highway section, in vehicle per time unit. The reward we collect at every time step is the total outflow in the last 32 seconds (in number of vehicle per time unit).\nThe highway is discretized in 167 cells of 200 meters each to generate an observation vector in R167\u00d73. For every cell, the following 3 data values are provided to the network:\n\u2022 Density in vehicle per space unit, normalized \u2022 A boolean value indicating the presence of an off-ramp\non this cell \u2022 The number of cars waiting in this cell\u2019s onramp queue\n8 Freeway input flow\ntime\nWest bound flow\nOn-ramp input flow\ntime\nFreeway output flow\ntime\nRamp meters\nFig. 8: Setting simulated by BeATS in our experiment\nApproach Score in veh.hr Score in veh.mile (lower is better) (higher is better) ALINEA 10514 644522 MWR 10575 644334 NoMWR 10617 643605 NoRM 11085 639709\nTABLE I: Aggregated scores of the different control strategies over the congested period. Reinforcement learning enables a non parametric control scheme whose performance is similar to that of the parametric ALINEA scheme and MWR, the new learning algorithm we introduce, for improves this performance.\n(logarithmically scaled), or 0 if there is no onramp It is worth mentioning that the Reinforcement Learning policies are trained in a stochastic way to ensure that a generic policy is learned. This is done by introducing some noise in the actions taken by the Neural Network: the ramp metering actually applied is sampled from a Gaussian distribution centered on the network output. This strategy, along with the use of shared learning techniques over the 29 onramps, globally prevents overfitting issues.\n2) Numerical results: After the training, we compared the results of our approach to existing algorithm on several criteria. The average speed is globally increased as expected (Fig. 9). We also report the Total Vehicle Miles in Fig. 13, along with the Total Vehicle Hour (Fig. 12) that assess the performance of our approach with a single score. In both cases, the MWR training approach provides a significative performance increase regular parameters sharing (NoMWR), and almost reaches the performance of the reference and state of the art parametric method Alinea."}, {"heading": "V. CONCLUSION", "text": "We have demonstrated how neural RL substantially improves upon the state-of-the-art in the field of control of discretized PDE. It enables reliable non-parametric control while offering theoretical guarantees similar to that of classic parametric control techniques. In particular, neural RL can be applied without an explicit model of system dynamics, and instead only requires the ability to simulate the system under consideration. Through our experimental evaluation, we demonstrated that neural RL approach can be used to control discretized macroscopic vehicular traffic equations by their boundary conditions in spite of accidents drastically perturbing the system. Achieving such robustness is a significant breakthrough in the field of control of cyberphysical systems. Specific to the practice of transportation, the results are a major disruption as they enable us to beat current controllers by\nFig. 10: RM methods reduce by 20% the number of vehicles in the freeway during congestion peak times. Our RL learning trained with MWR maintains the number of vehicles in the simulation to a level similar to what Alinea performs.\nFig. 11: Vehicle flux\n9\nperforming adaptive control, without the need for model calibration. By eliminating the need for calibration, our method addresses one of the critical challenges and dominant causes of controller failure making our approach particularly promising in the field of traffic management. We also introduced a novel algorithm, MWR, to achieve multi-agent control and leverage trial and error experiences across different agents while at the same time allowing each agent to learn how to tailor its behavior to its localization in the large cyberphysical system under study."}, {"heading": "APPENDIX A GODUNOV DISCRETIZATION SCHEME", "text": "Because of the presence of discontinuities in their solutions, the benchmark PDE we consider is formulated in the weak sense. We consider an open set \u2126 in R, and a measurable function \u03c1 = \u03c1(t, x) is a distributional solution to the system of conservation laws\n\u2202t\u03c1+ \u2202xF (\u03c1) = 0. (3)\nif, for every C1 function \u03c6 defined over \u2126 with compact support, one has\u222b\nt\u2208R,x\u2208\u2126 [\u03c1\u2202t\u03c6+ F (\u03c1) \u2202x\u03c6] dxdt = 0. (4)\nThe operator F in Eq. (3) will be referred to as flux function, also called the \u201cFundamental diagram\u201d in transportation engineering. The operator F defines entirely the dynamics at stake and therefore is often domain specific. Given an initial condition\n\u03c1(0, x) = \u03c1\u0304(x) (5)\nwhere \u03c1\u0304(x) is locally integrable, y : [0, T ] \u00d7 R \u2192 R is a distributional solution to the Cauchy problem defined by Eq. (4) and Eq. (5) if\u222b t\u2208[0, T ],x\u2208\u2126 [\u03c1\u2202t\u03c6+ F (\u03c1) \u2202x\u03c6] dxdt+ \u222b x\u2208\u2126 [\u03c1\u0304(x)\u03c6(0, x)] dx = 0. (6) for every C1 function \u03c6 with compact support contained in ]\u2212\u221e, T ] \u00d7 R. If y is a continuous function from [0, T ] into the set of locally integrable on \u2126, solves the Cauchy problem above in the distribution sense, \u03c1 is referred to as a weak solution to the Cauchy problem.\nThe Dirichlet problem corresponding to a boundary condition (as done later in the article) can be formulated in a similar manner and is left out of the article for brevity. Such a definition of weak solutions is not sufficient to guarantee their being admissible solutions.The entropy condition guarantees the uniqueness to the problem and continuous dependence with respect to the initial data (derivation also left out of the article for brevity).\nThe Godunov\u2019s scheme computes an approximate weak solution \u03c1\u0303 to the Dirichlet problem Eq. (4), Eq. (5) with the following recursive equation and Godunov flux G:\n\u03c1\u0303(n\u2206t+ \u2206t, i\u2206x) =\n\u03c1\u0303(n\u2206t, i\u2206x)\u2212 \u2206t\n\u2206x G (\u03c1\u0303(n\u2206t, i\u2206x\u2212\u2206x), \u03c1\u0303(n\u2206t, i\u2206x+ \u2206x))\n10\nG (\u03c1\u0303l, \u03c1\u0303r) =F (\u03c1\u0303l) if \u03c1\u0303l > \u03c1\u0303r and (F (\u03c1\u0303r)\u2212 F (\u03c1\u0303l))/(\u03c1\u0303r \u2212 \u03c1\u0303l) > 0, F (\u03c1\u0303r) if \u03c1\u0303l > \u03c1\u0303r and (F (\u03c1\u0303r)\u2212 F (\u03c1\u0303l))/(\u03c1\u0303r \u2212 \u03c1\u0303l) < 0, F (\u03c1\u0303l) if \u03c1\u0303l < \u03c1\u0303r and DF (\u03c1\u0303l) > 0,\nF (\u03c1\u0303l) if \u03c1\u0303l < \u03c1\u0303r and DF (\u03c1\u0303r) < 0, DF\u22121(0) otherwise .\nThe Godunov scheme is second order accurate in space. Unfortunately, like most numerical schemes, it is non-differentiable because of the presence of the \u201cif-then-else\u201d statements in its explicit form. Another problematic aspect related with computing numerical weak entropy solutions with most numerical schemes (incl. Godunov) is their relying on a numerical evaluation of F which often takes the form of a parametrized function. The estimation of these parameters is often difficult and it is practically intractable to assess the impact of the parameter uncertainty on the solutions because of the nonlinearity, non-smoothness and non-differentiability of the schemes."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["L.L.K.G.H. Kalchbrenner", "Sutskever"], "venue": "Nature, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["C. Finn", "X.Y. Tan", "Y. Duan", "T. Darrell", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1509.06113, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Guided policy search", "author": ["S. Levine", "V. Koltun"], "venue": "International Conference on Machine Learning (ICML 2013), 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Alinea local ramp metering: Summary of field results", "author": ["M. Papageorgiou", "H. Hadj-Salem", "F. Middelham"], "venue": "Transportation Research Record: Journal of the Transportation Research Board, no. 1603, pp. 90\u201398, 1997.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Freeway ramp-metering control based on reinforcement learning", "author": ["A. Fares", "W. Gomaa"], "venue": "11th IEEE International Conference on Control & Automation (ICCA). IEEE, 2014, pp. 1226\u20131231.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Decentralized coordinated optimal ramp metering using multi-agent reinforcement learning", "author": ["K. Rezaee"], "venue": "Ph.D. dissertation, University of Toronto, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Learning to learn. Springer, 1998, pp. 95\u2013133.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "A difference scheme for numerical solution of discontinuous solution of hydrodynamic equations", "author": ["S.K. Godunov"], "venue": "Math. Sbornik, vol. 47, pp. 271\u2013306, 1969.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1969}, {"title": "Freeway traffic flow simulation using the link node cell transmission model", "author": ["A. Muralidharan", "G. Dervisoglu", "R. Horowitz"], "venue": "2009 American Control Conference. IEEE, 2009, pp. 2916\u20132921.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Prediction on traveltime distribution for freeways using online expectation maximization algorithm", "author": ["N. Wan", "G. Gomes", "A. Vahidi", "R. Horowitz"], "venue": "Transportation Research Board 93rd Annual Meeting, 2013, pp. 14\u20133221.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A natural policy gradient.", "author": ["S. Kakade"], "venue": "in NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Policy gradient methods for robotics", "author": ["J. Peters", "S. Schaal"], "venue": "Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference on. IEEE, 2006, pp. 2219\u20132225.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning of non-parametric control policies with high-dimensional state features.", "author": ["H. Van Hoof", "J. Peters", "G. Neumann"], "venue": "in AISTATS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["N. Heess", "G. Wayne", "D. Silver", "T. Lillicrap", "T. Erez", "Y. Tassa"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2926\u2013 2934.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1071\u20131079.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1502.05477, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Model predictive control for ramp metering of motorway traffic: A case study", "author": ["T. Bellemans", "B. De Schutter", "B. De Moor"], "venue": "Control Engineering Practice, vol. 14, no. 7, pp. 757\u2013767, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Adjoint-based optimization on a network of discretized scalar conservation laws with applications to coordinated ramp metering", "author": ["J. Reilly", "S. Samaranayake", "M.L. Delle Monache", "W. Krichene", "P. Goatin", "A.M. Bayen"], "venue": "Journal of optimization theory and applications, vol. 167, no. 2, pp. 733\u2013760, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in neural information processing systems. Citeseer, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Learning tetris using the noisy cross-entropy method", "author": ["I. Szita", "A. L\u00f6rincz"], "venue": "Neural computation, vol. 18, no. 12, pp. 2936\u20132941, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Recent developments in Reinforcement Learning (RL) have enabled machines to learn how to play video games with no other information than the screen display [1], remarkably beat champion human players at Go with the AlphaGo program [2], or complete various tasks including locomotion and simple problem solving [3], [4], [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "Recent developments in Reinforcement Learning (RL) have enabled machines to learn how to play video games with no other information than the screen display [1], remarkably beat champion human players at Go with the AlphaGo program [2], or complete various tasks including locomotion and simple problem solving [3], [4], [5].", "startOffset": 231, "endOffset": 234}, {"referenceID": 2, "context": "Recent developments in Reinforcement Learning (RL) have enabled machines to learn how to play video games with no other information than the screen display [1], remarkably beat champion human players at Go with the AlphaGo program [2], or complete various tasks including locomotion and simple problem solving [3], [4], [5].", "startOffset": 310, "endOffset": 313}, {"referenceID": 3, "context": "Recent developments in Reinforcement Learning (RL) have enabled machines to learn how to play video games with no other information than the screen display [1], remarkably beat champion human players at Go with the AlphaGo program [2], or complete various tasks including locomotion and simple problem solving [3], [4], [5].", "startOffset": 315, "endOffset": 318}, {"referenceID": 4, "context": "Recent developments in Reinforcement Learning (RL) have enabled machines to learn how to play video games with no other information than the screen display [1], remarkably beat champion human players at Go with the AlphaGo program [2], or complete various tasks including locomotion and simple problem solving [3], [4], [5].", "startOffset": 320, "endOffset": 323}, {"referenceID": 5, "context": "The advent of policy gradient-based optimization algorithms enabling RL for highly-dimensional continuous control systems as can be found in [6], [7], has generalized model-free control to systems that were characteristically challenging for Q-learning.", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "The advent of policy gradient-based optimization algorithms enabling RL for highly-dimensional continuous control systems as can be found in [6], [7], has generalized model-free control to systems that were characteristically challenging for Q-learning.", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "Qlearning approaches, although successful in [1] suffer from a curse of dimensionality in continuous control if they rely on discretizing the action space.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "The current state of the art Ramp-Metering policy ALINEA [8] controls incoming flow in order to reach an optimal density locally.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Recently, nonparametric approaches based on Reinforcement Learning such as [9] or [10] have been proposed to achieve ramp-metering, but face two main limitations.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Recently, nonparametric approaches based on Reinforcement Learning such as [9] or [10] have been proposed to achieve ramp-metering, but face two main limitations.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "We showcase the robustness of the approach and its ability to learn from real-world examples by assessing its performance after an extremely noisy training phase with stochastic variations in the underlying PDE parameters of the model used \u2022 We introduce an algorithm to train Neural Networks that we denote Mutual Weights Regularization (MWR) which enables the sharing of experience between agents and specialization of each agent thanks to Multi Task Learning [11].", "startOffset": 462, "endOffset": 466}, {"referenceID": 11, "context": "A first simulator based on a Godunov scheme [12] is used to demonstrate the efficiency of our approach on multiples situations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "The Berkeley Advanced Transportation Simulator (BeATS), a state of the art macroscopic simulator implementing a particular instantiation of the Godunov scheme sometimes referred to as the Cell Transmission Model [13] and used in traffic research [14] is also introduce, as we use it for our final benchmark.", "startOffset": 212, "endOffset": 216}, {"referenceID": 13, "context": "The Berkeley Advanced Transportation Simulator (BeATS), a state of the art macroscopic simulator implementing a particular instantiation of the Godunov scheme sometimes referred to as the Cell Transmission Model [13] and used in traffic research [14] is also introduce, as we use it for our final benchmark.", "startOffset": 246, "endOffset": 250}, {"referenceID": 14, "context": "Therefore, policy gradient algorithms present a compelling opportunity as model dynamics are sampled from the simulation trials in algorithms such as [15], [16], [17], [18] and no prior knowledge of the model is necessary to train the policy.", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "Therefore, policy gradient algorithms present a compelling opportunity as model dynamics are sampled from the simulation trials in algorithms such as [15], [16], [17], [18] and no prior knowledge of the model is necessary to train the policy.", "startOffset": 156, "endOffset": 160}, {"referenceID": 16, "context": "Therefore, policy gradient algorithms present a compelling opportunity as model dynamics are sampled from the simulation trials in algorithms such as [15], [16], [17], [18] and no prior knowledge of the model is necessary to train the policy.", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "Therefore, policy gradient algorithms present a compelling opportunity as model dynamics are sampled from the simulation trials in algorithms such as [15], [16], [17], [18] and no prior knowledge of the model is necessary to train the policy.", "startOffset": 168, "endOffset": 172}, {"referenceID": 11, "context": "A first one uses a coarser less realistic discretization scheme for the LWR PDE named after its inventor: Godunov [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "In our second step we use a more accurate cell transition model [13] scheme entailed in the state of the art BeATs simulator in order to show that the procedure we present is still valid in a more realistic setting.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "We show that training our neural net based policy by policy gradient methods achieves comparable performance with the state of the art ALINEA control scheme [8] although the former is non-parametric when the latter requires a calibration of traffic related parameters.", "startOffset": 157, "endOffset": 160}, {"referenceID": 11, "context": "The set of states is denoted S and will typically be R in our instance where d is the number of finite differences cells as in [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "PDEs: The recent developments in RL featured in [19], [7], [20] guarantee convergence properties similar to those of standard control methods and therefore strongly motivates their usage for the control of PDEs.", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "PDEs: The recent developments in RL featured in [19], [7], [20] guarantee convergence properties similar to those of standard control methods and therefore strongly motivates their usage for the control of PDEs.", "startOffset": 54, "endOffset": 57}, {"referenceID": 19, "context": "PDEs: The recent developments in RL featured in [19], [7], [20] guarantee convergence properties similar to those of standard control methods and therefore strongly motivates their usage for the control of PDEs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "The MPC approach in [21] and the adjoint method based technique of [22] both rely on the definition of a cost function which needs to be minimized.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "The MPC approach in [21] and the adjoint method based technique of [22] both rely on the definition of a cost function which needs to be minimized.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "In the setting of computer vision, convolutional neural networks famously achieved human level image classification thanks to the invariance by translation and rotation of the convolution masks they progressively learnt [23].", "startOffset": 220, "endOffset": 224}, {"referenceID": 23, "context": "Back propagation In order to train Neural Networks backpropagation [24] is a key algorithm that performs a stochastic gradient descent on a non-convex function [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "Back propagation In order to train Neural Networks backpropagation [24] is a key algorithm that performs a stochastic gradient descent on a non-convex function [25].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "Approaches to train such a Neural Network for control in the Q-learning framework has been adopted in [1] and were successful in a discrete control setting.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "The former family encompasses first order methods such as REINFORCE [7] which we will denote Vanilla Policy Gradient (VPG), approximated second order methods based on the use of natural gradient descent [15], local line search methods such as Trust Region Policy Optimization (TRPO) [20], LBGFs inspired methods such as Penalized Policy Optimization (PPO) [5] and gradient free approaches such as the cross-entropy method [26].", "startOffset": 68, "endOffset": 71}, {"referenceID": 14, "context": "The former family encompasses first order methods such as REINFORCE [7] which we will denote Vanilla Policy Gradient (VPG), approximated second order methods based on the use of natural gradient descent [15], local line search methods such as Trust Region Policy Optimization (TRPO) [20], LBGFs inspired methods such as Penalized Policy Optimization (PPO) [5] and gradient free approaches such as the cross-entropy method [26].", "startOffset": 203, "endOffset": 207}, {"referenceID": 19, "context": "The former family encompasses first order methods such as REINFORCE [7] which we will denote Vanilla Policy Gradient (VPG), approximated second order methods based on the use of natural gradient descent [15], local line search methods such as Trust Region Policy Optimization (TRPO) [20], LBGFs inspired methods such as Penalized Policy Optimization (PPO) [5] and gradient free approaches such as the cross-entropy method [26].", "startOffset": 283, "endOffset": 287}, {"referenceID": 4, "context": "The former family encompasses first order methods such as REINFORCE [7] which we will denote Vanilla Policy Gradient (VPG), approximated second order methods based on the use of natural gradient descent [15], local line search methods such as Trust Region Policy Optimization (TRPO) [20], LBGFs inspired methods such as Penalized Policy Optimization (PPO) [5] and gradient free approaches such as the cross-entropy method [26].", "startOffset": 356, "endOffset": 359}, {"referenceID": 25, "context": "The former family encompasses first order methods such as REINFORCE [7] which we will denote Vanilla Policy Gradient (VPG), approximated second order methods based on the use of natural gradient descent [15], local line search methods such as Trust Region Policy Optimization (TRPO) [20], LBGFs inspired methods such as Penalized Policy Optimization (PPO) [5] and gradient free approaches such as the cross-entropy method [26].", "startOffset": 422, "endOffset": 426}, {"referenceID": 4, "context": "The performance of these algorithms have been thoroughly compared in [5] where the natural gradient based method Truncated Natural Policy Gradient (TNPG) and TRPO generally outperformed other approaches.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "1) REINFORCE: The Reinforce algorithm [7] has been used to train our policy to maximize E (\u03b7(\u03c0)).", "startOffset": 38, "endOffset": 41}, {"referenceID": 14, "context": "The learning phase uses different policy update methods such as [15], [6], [20] and benchmarked in [5].", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "The learning phase uses different policy update methods such as [15], [6], [20] and benchmarked in [5].", "startOffset": 70, "endOffset": 73}, {"referenceID": 19, "context": "The learning phase uses different policy update methods such as [15], [6], [20] and benchmarked in [5].", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "The learning phase uses different policy update methods such as [15], [6], [20] and benchmarked in [5].", "startOffset": 99, "endOffset": 102}], "year": 2017, "abstractText": "This article shows how the recent breakthroughs in Reinforcement Learning (RL) that have enabled robots to learn to play arcade video games, walk or assemble colored bricks, can be used to perform other tasks that are currently at the core of engineering cyberphysical systems. We present the first use of RL for the control of systems modeled by discretized non-linear Partial Differential Equations (PDEs) and devise a novel algorithm to use non-parametric control techniques for large multi-agent systems. Cyberphysical systems (e.g., hydraulic channels, transportation systems, the energy grid, electromagnetic systems) are commonly modeled by PDEs which historically have been a reliable way to enable engineering applications in these domains. However, it is known that the control of these PDE models is notoriously difficult. We show how neural network based RL enables the control of discretized PDEs whose parameters are unknown, random, and time-varying. We introduce an algorithm of Mutual Weight Regularization (MWR) which alleviates the curse of dimensionality of multi-agent control schemes by sharing experience between agents while giving each agent the opportunity to specialize its action policy so as to tailor it to the local parameters of the part of the system it is located in. A discretized PDE such as the scalar Lighthill-Whitham-Richards (LWR) PDE can indeed be considered as a macroscopic freeway traffic simulator and which presents the most salient challenges for learning to control large cyberphysical system with multiple agents. We consider two different discretization procedures and show the opportunities offered by applying deep reinforcement for continuous control on both. Using a neural RL PDE controller on a traffic flow simulation based on a Godunov discretization of the of the San Francisco Bay Bridge we are able to achieve precise adaptive metering without model calibration thereby beating the state of the art in traffic metering. Furthermore, with the more accurate BeATS simulator we manage to achieve a control performance on par with ALINEA, a state of the art parametric control scheme, and show how using MWR improves the learning procedure.", "creator": "LaTeX with hyperref package"}}}