{"id": "1702.00178", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2017", "title": "On the Futility of Learning Complex Frame-Level Language Models for Chord Recognition", "abstract": "chord recognition systems consistently use temporal regression models to analyze post - auditory process frame - wise chord preditions from acoustic models. traditionally, first - order models utilizing such techniques as hidden markov models were used for this tracking task, with recent wikipedia works suggesting students to apply recurrent neural networks instead. due to their ability presumably to learn longer - term seasonal dependencies, these models are intentionally supposed also to exclusively learn and teach to completely apply classical musical knowledge, instead because of preferring just smoothing with the acoustic output of the acoustic model. shown in this paper, we repeatedly argue that learning complex temporal models performed at the exact level of nonlinear audio frames adjustment is futile on principle, merely and that non - markovian models likely don't perform better either than on their first - world order counterparts. tentatively we support pursuing our musical argument conclusion through three acoustic experiments relied on recovering the mcgill billboard dataset. the diagram first shows two show experiments 1 ) that when automatically learning complex temporal models at the frame estimation level, improvements in sparse chord sequence modelling are marginal ; back and back 2 ) that these improvements don't help translate noticeably when applied within a full chord recognition system. the subsequent third, still noisy rather sensitive preliminary linguistics experiment gives possible first episode indications that knowing the firms use values of different complex sequential rehearsal models for chord prediction used at higher temporal levels here might be becoming more promising.", "histories": [["v1", "Wed, 1 Feb 2017 09:44:44 GMT  (46kb,D)", "http://arxiv.org/abs/1702.00178v1", null], ["v2", "Fri, 31 Mar 2017 11:24:42 GMT  (69kb,D)", "http://arxiv.org/abs/1702.00178v2", "Published at AES Conference on Semantic Audio 2017"]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["filip korzeniowski", "gerhard widmer"], "accepted": false, "id": "1702.00178"}, "pdf": {"name": "1702.00178.pdf", "metadata": {"source": "CRF", "title": "On the Futility of Learning Complex Frame-Level Language Models for Chord Recognition", "authors": ["Filip Korzeniowski", "Gerhard Widmer"], "emails": ["filip.korzeniowski@jku.at"], "sections": [{"heading": null, "text": "Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising."}, {"heading": "1 Introduction", "text": "Computational systems that extract high-level information from signals face two key problems: 1) how to extract meaningful information from noisy sources,\nand 2) how to process and combine this information into sensible output. For example, in domains such as speech recognition, these translate to acoustic modelling (how to predict phonemes from audio) and language modelling (how to connect these phonemes into words and sentences).\nA similar structure can be observed in many systems related to music processing, such as chord recognition. Chord recognition systems aim at recognising and transcribing musical chords from audio, highly descriptive harmonic features of music that form the basis of a myriad applications (e.g. key estimation, cover song detection, or directly to automatically provide lead sheets for musicians, to name a few). We refer the reader to [1, 2] for an overview.\nChord recognition systems comprise two main parts: an acoustic model that extracts frame-wise harmonic features and predicts chord label distributions for each time frame, and a temporal model that connects consecutive predictions and outputs a sequence of chord labels for a piece of audio.\nThe temporal model provides coherence to the possibly volatile predictions of the acoustic model. It also permits the introduction of higher-level musical knowledge\u2014we know from music theory that certain chord progressions are more likely than others\u2014to further improve the obtained chord labels.\nA number of works (e.g. [3, 4, 5]) implemented hand-designed temporal models for chord recognition. These models are usually first-order Dynamic Bayesian Networks that operate at the beat or timeframe level. They are designed to incorporate musical\nar X\niv :1\n70 2.\n00 17\n8v 1\n[ cs\n.S D\n] 1\nF eb\nknowledge, with parameters set by hand or trained from data.\nA different approach is to learn temporal models fully from data, without any imposed structure. Here, it common to use simple Hidden Markov Models [6] or Conditional Random Fields [7] with states corresponding to chord labels.\nHowever, recent research showed that first-order models are unable to encode musical knowledge and focus on ensuring stability between consecutive predictions [8, 2] (i.e. they only smooth the output sequence).\nThis result bears little surprise: firstly, many common chord patterns in pop, jazz, or classical music span more than two chords, and thus cannot be adequately modelled by first-order models; secondly, models that operate at the frame level by definition only predict the chord symbol of the next frame (typically 10ms away), which most of the time will be the same as the current chord symbol.\nTo overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models1 for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11]. RNNs are capable of modelling relations in temporal sequences that go beyond simple firstorder connections. Their great modelling capacity is, however, limited by the difficulty to optimise their parameters: exploding gradients make training instable, while vanishing gradients hinder learning long-term dependencies from data.\nIn this paper, we argue that (neglecting the aforementioned problems) adopting and training complex temporal models on a time-frame basis is futile on principle. We support this claim by experimentally showing that they do not outperform first-order models (for which we know they are unable to capture musical knowledge) as part of a complete chord recognition system, and perform only negligibly better at modelling chord label sequences. We thus conclude that, despite their greater modelling capacity, the input representation (musical symbols on a time-\n1Note of our use of the term \u201ctemporal model\u201d instead of \u201clanguage model\u201d as used in many publications\u2014the reason for this distinction will hopefully become clear at the end of this paper.\nframe basis) prohibits learning and applying knowledge about musical structure\u2014the language models hence resort to what their simpler first-order siblings are also capable of: smoothing the predictions.\nIn the following, we describe three experiments: in the first, we judge two temporal models directly by their capacity to model frame-level chord sequences; in the second, we deploy the temporal models within a full-fledged chord recognition pipeline; finally, in the third, we learn a language model at chord level to show that RNNs are able to learn musical structure if used at a higher hierarchical level. Based on the results, we then draw conclusions for future research on temporal and language modelling in the context of music processing."}, {"heading": "2 Experiment 1: Chord Se-", "text": "quence Modelling\nIn this experiment, we want to quantify the modelling power of temporal models directly. A temporal model predicts the next chord symbol in a sequence given the ones already observed. Since we are dealing with frame-level data and adopt a frame rate of 10 fps, a chord sequence consists of 10 chord symbols per second. More formally, given a chord sequence y = (y1, . . . , yK), a model M outputs a probability distribution PM (yk | y1, . . . , yk\u22121) for each yk. From this, we can compute the probability of the chord sequence\nPM (y) = PM (y1) \u00b7\u03a0Kk=2PM (yk | y1, . . . , yk\u22121) . (1)\nTo measure how well a model M predicts the chord sequences in a dataset, we compute the average logprobability that it assigns to the sequences y of a dataset Y:\nL(M,Y) = 1 NY \u2211 y\u2208Y log (PM (y)) , (2)\nwhere NY is the total number of chords symbols in the dataset."}, {"heading": "2.1 Temporal Models", "text": "We compare two temporal models in this experiment: A first-order Markov Chain and a RNN with LongShort Term Memory (LSTM) units.\nFor the Markov Chain, PM (yk | y1, . . . , yk\u22121) in Eq. 1 is simplified to PM (yk | yk\u22121) = Ayk,yk\u22121 due to the Markov property, and PM (y1) = \u03c0y1 . Both \u03c0 and A can be estimated by counting the corresponding occurrences in the train set.\nFor the LSTM-RNN, we follow closely the design, parametrisation and training procedure proposed in [10], and we refer the reader to their paper for details. The input to the network at time step k is the chord symbol yk\u22121 in one-hot encoding, the output is the probability distribution PM (yk | y1, . . . , yk\u22121) used in Eq. 1. (For k = 1, we input a \u201cno-chord\u201d symbol and the network computes P (y1).) As loss we use the categorical cross-entropy between the output distribution and the one-hot encoding of the target chord symbol.\nWe use 2 layers of 100 LSTM units each, and add skip-connections such that the input is connected to both hidden layers and to the output. We train the network using stochastic gradient descent with momentum for a maximum of 200 epochs (the network usually converges earlier) with the learning rate decreasing linearly from 0.001 to 0. As in [10], we show the network sequences of 100 symbols (corresponding to 10 seconds) during training. We experimented with longer sequences (up to 50 seconds), but results did not improve (i.e. the network did not profit from longer contexts). Finally, to improve generalisation, we augment the train data by randomly shifting the key of the sequences each time they are shown during training."}, {"heading": "2.2 Data", "text": "We evaluate the models on the McGill Billboard dataset [12]. We filter out duplicate songs to prevent train/test overlap and use songs with ids smaller than 1000 for training, and the remaining for testing. Filtering duplicate songs reduces the number of pieces from 890 to 742, of which 571 are used for training and validation, and 171 for testing."}, {"heading": "2.3 Results", "text": "Table 1 shows the resulting avg. log-probabilities of the models on the test set. Additionally to L(M,Y), we report Ls(M,Y) and Lc(M,Y). These numbers represent the average log-probability the model assigns to chord symbols in the dataset when the current symbol is the same as the previous one and, when it changed, respectively. They are computed similarly to L(M,Y), but the product in Eq. 1 only captures k where yk = yk\u22121 or yk 6= yk\u22121, respectively.\nThey permit us to reason about how well a model will smooth the predictions when the chord is stable, and how well it can predict chords when they change (this is where \u201cmusical knowledge\u201d could come into play).\nWe can see that the RNN performs only slightly better than the Markov Chain, despite its higher modelling capacity. This improvement roots in better predictions when the chord changes (-5.22 for the RNN vs. -5.42 for the MC). This might indicate that the RNN is able to model musical knowledge better than the MC after all. However, this advantage is minuscule and comes seldom into play: the correct chord has an avg. probability of 0.0044 with the RNN vs. 0.0054 with the MC2, and the number of positions at which the chord symbol changes, compared to where it stays the same, is low.\nIn the next experiment, we evaluate if the marginal improvement provided by the RNN translates into better chord recognition accuracy when deployed in a fully-fledged system.\n2Both are worse than random, because both would still favour self-transitions"}, {"heading": "3 Experiment 2: Frame-Level", "text": "Chord Recognition\nIn this experiment, we want to evaluate the temporal models in the context of a complete chord recognition framework. The task is to predict for each audio frame the correct chord symbol. We restrict ourselves to major and minor chords, resulting in 24 (12 root notes\u00d7 {major,minor}) chord symbols and a special \u201cno chord\u201d symbol, or 25 classes. Following [2], we map all ground truth chord labels with a minor 3rd as their first interval to minor chords, and all others to major chords.\nOur chord-recognition pipeline comprises spectrogram computation, an automatically learned feature extractor and chord predictor, and finally the temporal model. The first two stages are based on our previous work [7, 13]. We extract a log-filtered and log-scaled spectrogram between 65 and 2100 Hz at 10 frames per second, and feed spectral patches of 1.5s into one of three acoustic models: a logistic regression classifier (LogReg), a deep neural network (DNN) with 3 fully connected hidden layers of 256 rectifier units, and a convolutional neural network (ConvNet) with the exact architecture we presented in [7].\nEach acoustic model yields frame-level chord predictions, which are then processed by one of three different temporal models."}, {"heading": "3.1 Temporal Models", "text": "We test three temporal models of increasing complexity. The simplest one is Majority Voting within a context of 1.3s, The others are the very same we used in the previous experiment.\nConnecting the Markov Chain temporal model to the predictions of the acoustic model results in a Hidden Markov Model (HMM). The output chord sequence is decoded using the Viterbi algorithm.\nTo connect the RNN temporal model to the predictions of the acoustic model, we apply the hashed beam search algorithm, as introduced in [10], with a beam width of 25, hash length of 3 symbols and a maximum of 4 solutions per hash bin. The algorithm only approximately decodes the chord sequence (no efficient\nand exact algorithms exist, because the output of the network depends on all previous inputs)."}, {"heading": "3.2 Results", "text": "Table 2 shows the Weighted Chord Symbol Recall (WCSR) of major and minor chords for all combinations of acoustic and temporal models. The WCSR is defined as R = tc/ta, where tc is the total time where the prediction corresponds to the annotation, and ta is the total duration of annotations of the respective chord classes (major and minor chords and the \u201cno-chord\u201d class, in our case). We used the implementation provided in the \u201cmir eval\u201d library [14].\nThe results show that the complex RNN temporal model does not outperform the simpler first-order HMM. They improve compared to not using a temporal model at all, and to a simple majority vote.\nThe results suggest that the RNN temporal model does not display its (marginal) advantage in chord sequence modelling when deployed within the complete chord recognition system. We assume the reasons to be 1) that the improvement was small in first place, and 2) that exact inference is not computationally feasible for this model, and we have to resort to approximate decoding using beam search."}, {"heading": "4 Experiment 3: Modelling", "text": "Chord-level Label Sequences\nIn the final experiment, we want to support our argument that the RNN does not learn musical structure because of the hierarchical level (time frames) it is\napplied on. To this end, we conduct an experiment similar to the first one\u2014an RNN is required to predict the next chord symbol in the sequence. However, this time the sequence is not sampled at frame level, but at chord level (i.e. no matter how long a certain chord is played, it is reduced to a single instance in the sequence).\nThe results confirm that in such a scenario, the RNN outperforms the Markov Chain significantly (Avg. Log-P. of -1.62 vs. -2.28). Additionally, we observe that the RNN does not only learn static dependencies between consecutive chords; it is also able to adapt to a song and recognise chord progressions seen previously in this song without any on-line training. This resembles the way humans would expect the chord progressions not to change much during a part (e.g. the chorus) of a song, and come back later when a part is repeated. Figure 1 shows exemplary results from the test data."}, {"heading": "5 Conclusion", "text": "We argued that learning complex temporal models for chord recognition 3 on a time-frame basis is futile. The experiments we carried out support our argument. The first experiment focused on how well a complex temporal model can learn to predict chord sequences compared to a simple first-order one. We saw that the complex model, despite its substantially greater modelling capacity, performed only marginally better. The second experiment showed that, when deployed within a chord recognition system, the RNN temporal model did not outperform the first-order HMM. Its slightly better capability to model frame-level chord sequences was probably counteracted by the approximate nature of the inference algorithm. Finally, in the third experiment, we showed preliminary results that when deployed at a higher hierarchical level than time frames, RNNs are indeed capable of learning musical structure beyond first-order transitions.\n3We expect similar results for other music-related tasks.\nWhy are complex temporal models like RNNs unable to model frame-level chord sequences? We believe the following two circumstances to be the causes: 1) transitions are dominated by self-transitions, i.e. models need to predict self-transitions as well as possible to achieve good predictive results on the data, and 2) predicting chord changes \u201cblindly\u201d (i.e. without knowledge when the change might occur) competes with 1) via the normalisation constraint of probability distributions.\nInferring when a chord changes proves difficult if the model can only consider the (frame-level) chord sequence. There are simply too many uncertainties (e.g. the tempo and harmonic rhythm of a song, timing deviations, etc.) that are hard to estimate. However, the models also do not have access to features computed from the input signal, which might help in judging whether a chord change is imminent or not. Thus, the models are blind to the time of a chord change, which makes them focus on predicting self-transitions, as we outlined in the previous paragraph.\nWe know from other domains such as natural language modelling that RNNs are capable of learning state-of-the-art language models [15]. We thus argue that the reason they underperform in our setting is the frame-wise nature of the input data. For future research, we propose to focus on language models instead of frame-level temporal models for chord recognition. By \u201clanguage model\u201d we mean a model at a higher hierarchical level than the temporal models explored in this paper (hence the distinction in name)\u2014like the model used in the final experiment described in this paper. Such language models can then be used in sequence classification framework such as sequence transduction [16] or segmental recurrent neural networks [17].\nMusic theory teaches that cadences play an important role in the harmonic structure of music, but many current state-of-the-art chord recognition systems (including our own) ignore this. Learning powerful language models at sensible hierarchical levels bears the potential to further improve the accuracy of chord recognition systems, which remained stagnant in recent years."}, {"heading": "Acknowledgements", "text": "This work is supported by the European Research Council (ERC) under the EU\u2019s Horizon 2020 Framework Programme (ERC Grant Agreement number 670035, project \u201cCon Espressione\u201d). The Tesla K40 used for this research was donated by the NVIDIA Corporation."}], "references": [{"title": "Automatic Chord Estimation from Audio: A Review of the State of the Art", "author": ["M. McVicar", "R. Santos-Rodriguez", "Y. Ni", "T.D. Bie"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(2), pp. 556\u2013575, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On the Relative Importance of Individual Components of Chord Recognition Systems", "author": ["T. Cho", "J.P. Bello"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(2), pp. 477\u2013492, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An End-to-End Machine Learning System for Harmonic Analysis of Music", "author": ["Y. Ni", "M. McVicar", "R. Santos-Rodriguez", "T. De Bie"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 20(6), pp. 1771\u20131783, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining Musicological Knowledge About Chords and Keys in a Simultaneous Chord and Local Key Estimation System", "author": ["J. Pauwels", "Martens", "J.-P."], "venue": "Journal of New Music Research, 43(3), pp. 318\u2013330, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Simultaneous Estimation of Chords and Musical Context From Audio", "author": ["M. Mauch", "S. Dixon"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 18(6), pp. 1280\u20131289, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring Common Variations in State of the Art Chord Recognition Systems", "author": ["T. Cho", "R.J. Weiss", "J.P. Bello"], "venue": "Proceedings of the Sound and Music Computing Conference (SMC), Barcelona, Spain, 2010. 6", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A Fully Convolutional Deep Auditory Model for Musical Chord Recognition", "author": ["F. Korzeniowski", "G. Widmer"], "venue": "Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP), Salerno, Italy, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Chord Recognition Using Duration- Explicit Hidden Markov Models", "author": ["R. Chen", "W. Shen", "A. Srinivasamurthy", "P. Chordia"], "venue": "Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR), Porto, Portugal, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio Chord Recognition with Recurrent Neural Networks", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR), Curitiba, Brazil, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio Chord Recognition with a Hybrid Recurrent Neural Network", "author": ["S. Sigtia", "N. Boulanger-Lewandowski", "S. Dixon"], "venue": "16th International Society for Music Information Retrieval Conference (ISMIR), M\u00e1laga, Spain, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "An Endto-End Neural Network for Polyphonic Piano Music Transcription", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(5), pp. 927\u2013939, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.", "author": ["J.A. Burgoyne", "J. Wild", "I. Fujinaga"], "venue": "Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR), Miami, USA,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor", "author": ["F. Korzeniowski", "G. Widmer"], "venue": "Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR), New York, USA, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Mir eval: A Transparent Implementation of Common MIR Metrics,", "author": ["C. Raffel", "B. McFee", "E.J. Humphrey", "J. Salamon", "O. Nieto", "D. Liang", "D.P.W. Ellis"], "venue": "Proceedings of the 15th International Conference on Music Information Retrieval (ISMIR), Taipei, Taiwan,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Recurrent Neural Network Based Language Model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "Proceedings of INTERSPEECH 2010, Makuhari, Japan, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence Transduction with Recurrent Neural Networks", "author": ["A. Graves"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), Edinburgh, Scotland, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental Recurrent Neural Networks for End-to-End Speech Recognition", "author": ["L. Lu", "L. Kong", "C. Dyer", "N.A. Smith", "S. Renals"], "venue": "Proceedings of INTERSPEECH 2016, San Francisco, USA, 2016. 7", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "We refer the reader to [1, 2] for an overview.", "startOffset": 23, "endOffset": 29}, {"referenceID": 1, "context": "We refer the reader to [1, 2] for an overview.", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "[3, 4, 5]) implemented hand-designed temporal models for chord recognition.", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[3, 4, 5]) implemented hand-designed temporal models for chord recognition.", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[3, 4, 5]) implemented hand-designed temporal models for chord recognition.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "Here, it common to use simple Hidden Markov Models [6] or Conditional Random Fields [7] with states corresponding to chord labels.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "Here, it common to use simple Hidden Markov Models [6] or Conditional Random Fields [7] with states corresponding to chord labels.", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "However, recent research showed that first-order models are unable to encode musical knowledge and focus on ensuring stability between consecutive predictions [8, 2] (i.", "startOffset": 159, "endOffset": 165}, {"referenceID": 1, "context": "However, recent research showed that first-order models are unable to encode musical knowledge and focus on ensuring stability between consecutive predictions [8, 2] (i.", "startOffset": 159, "endOffset": 165}, {"referenceID": 8, "context": "To overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11].", "startOffset": 176, "endOffset": 183}, {"referenceID": 9, "context": "To overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11].", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "To overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11].", "startOffset": 205, "endOffset": 209}, {"referenceID": 9, "context": "For the LSTM-RNN, we follow closely the design, parametrisation and training procedure proposed in [10], and we refer the reader to their paper for details.", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "As in [10], we show the network sequences of 100 symbols (corresponding to 10 seconds) during training.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "We evaluate the models on the McGill Billboard dataset [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "Following [2], we map all ground truth chord labels with a minor 3rd as their first interval to minor chords, and all", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "The first two stages are based on our previous work [7, 13].", "startOffset": 52, "endOffset": 59}, {"referenceID": 12, "context": "The first two stages are based on our previous work [7, 13].", "startOffset": 52, "endOffset": 59}, {"referenceID": 6, "context": "5s into one of three acoustic models: a logistic regression classifier (LogReg), a deep neural network (DNN) with 3 fully connected hidden layers of 256 rectifier units, and a convolutional neural network (ConvNet) with the exact architecture we presented in [7].", "startOffset": 259, "endOffset": 262}, {"referenceID": 9, "context": "To connect the RNN temporal model to the predictions of the acoustic model, we apply the hashed beam search algorithm, as introduced in [10], with a beam width of 25, hash length of 3 symbols and a maximum of 4 solutions per hash bin.", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "We used the implementation provided in the \u201cmir eval\u201d library [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "We know from other domains such as natural language modelling that RNNs are capable of learning state-of-the-art language models [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Such language models can then be used in sequence classification framework such as sequence transduction [16] or segmental recurrent neural networks [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Such language models can then be used in sequence classification framework such as sequence transduction [16] or segmental recurrent neural networks [17].", "startOffset": 149, "endOffset": 153}], "year": 2017, "abstractText": "Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising.", "creator": "LaTeX with hyperref package"}}}