{"id": "1702.01776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Multi-task memory networks for category-specific aspect and opinion terms co-extraction", "abstract": "in utilizing aspect - based sentiment classification analysis, most existing methods either regularly focus on aspect / preferred opinion terms extraction or aspect + terms categorization. \u2014 however, replacing each task by itself only provides partial linguistic information critically to end decision users. to progressively generate thereby more detailed documentation and structured opinion analysis, we firmly propose implementing a finer - fibre grained problem, which we now call inferior category - specific aspect assignment and complementary opinion terms extraction. fundamentally this primary problem involves emphasizing the successful identification of aspect and adverse opinion terms evaluated within each sentence, valued as far well separately as the categorization of removing the identified terms. to accomplish this end, whereas we propose an end - to - end multi - task attention model, defined where each task assignment corresponds smoothly to appropriate aspect / opinion usage terms extraction for a specific identified category. simultaneously our consistency model benefits from concurrently exploring even the commonalities and cultural relationships among different tasks to adequately address purely the data sparsity awareness issue. moreover we systematically demonstrate its state - of - the - information art review performance on three benchmark referenced datasets.", "histories": [["v1", "Mon, 6 Feb 2017 19:55:51 GMT  (901kb)", "http://arxiv.org/abs/1702.01776v1", null], ["v2", "Mon, 5 Jun 2017 06:39:37 GMT  (297kb)", "http://arxiv.org/abs/1702.01776v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenya wang", "sinno jialin pan", "daniel dahlmeier"], "accepted": false, "id": "1702.01776"}, "pdf": {"name": "1702.01776.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier"], "emails": ["sinnopan}@ntu.edu.sg,", "d.dahlmeier@sap.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n01 77\n6v 1\n[ cs\n.C L\n] 6\nF eb\n2 01\n7"}, {"heading": "1 Introduction", "text": "Aspect-based sentiment analysis deals with tokenlevel predictions, aiming to provide fine-grained information. Under this branch, most work has been proposed for aspect/opinion terms extraction (Hu and Liu, 2004; Qiu et al., 2011; Wang et al., 2016), where an aspect term refers to a word or a phrase describing some feature of an entity, and an opinion term refers to the expression carrying subjective emotions. For example, in the sentence \u201cThe soup is served with nice portion, the service is prompt\u201d, soup, portion and service are aspect terms, while nice and prompt are opinion terms. However, the above task simply extracts target terms without classifying them into different\ncategories, which should be more useful for generating structured aspect-based opinion summaries.\nOn the other hand, some work focused on categorization of aspect terms. Given an extracted aspect term, early work (Carenini et al., 2005; Yu et al., 2011) applied lexicon and taxonomybased methods to classify it to a category according to some distance measures. Semi-supervised methods have also been proposed for this problem (Zhai et al., 2010, 2011). However, this task requires the aspect terms to be extracted beforehand. Although topic models (Guo et al., 2009; Titov and McDonald, 2008a) can achieve both grouping and extraction at the same time, they mainly focused on grouping and can only identify general and coarse-grained aspect terms.\nTo address the limitations of existing methods and provide a more useful end-to-end analysis, we introduce a finer-grained task called category-specific aspect and opinion terms extraction, where the aspect/opinion terms need to be extracted and classified to a category from a pre-defined set. Consider the previous example, our objective is to extract and classify soup and portion as aspect terms under the \u201cDRINKS#STYLE OPTION\u201d category, and service as an aspect term under the \u201cSERVICE#GENERAL\u201d category, similar for the opinion terms nice and prompt.\nThe proposed task is much more challenging because when considering different categories, the data could become extremely sparse, i.e. certain categories might only contain very few reviews. Moreover, it requires to achieve both extraction and categorization at the same time which significantly increases the difficulty compared to both tasks. Under our problem setting, existing methods are not directly applicable. Even if we modify the extraction methods to incorporate category information, the result may still be unpromising, be-\ncause they fail to utilize inter-category correlations which can help address data sparsity issue. At this point, we propose a multi-task deep learning method to exploit commonalities and correlations among different tasks, where each task is defined as aspects/opinions extraction for a specific category. Inspired by (Wang et al., 2017), we model each task with coupled multi-layer attentions to extract the relations between aspect terms and opinion terms within each category. The multilayer coupled attentions for each task/category are jointly learned in a multi-task learning manner. In summary, the contributions are two-fold: 1) We offer an end-to-end deep multi-task learning model to accomplish a finer-grained sentiment analysis task; 2) We demonstrate its state-of-theart performance on three benchmark datasets."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Fine-grained Sentiment Analysis", "text": "For the task of aspect/opinion terms extraction, there are mainly four approaches. The first approach aims to exploit syntactic dependency relations among aspect terms and opinion terms for information extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011). A second approach models the extraction of target terms as a supervised sequence labeling problem with exhaustive human-engineered features (Jin and Ho, 2009; Li et al., 2010; Jakob and Gurevych, 2010). To reduce the effort of feature engineering, a third approach aims to utilize deep learning to learn high-level features automatically (Liu et al., 2015a; Yin et al., 2016; Wang et al., 2016, 2017). However, all the above approaches only focus on opinion/aspect terms extraction without categorizing them into specific categories. A forth approach adopts topic models (Titov and McDonald, 2008b; Lu et al., 2009; Zhao et al., 2010; Chen et al., 2014) or clustering techniques (Su et al., 2008; Yu et al., 2011; Chen et al., 2016) to group potential aspect terms into different clusters (not explicit categories). Though this approach can automatically group opinion/aspect terms into different clusters or topics, it fails to explicitly classify a term into one of a set of user predefined categories.\nFor the task of aspect categorization, most existing methods assume the aspect terms be extracted in advance, and aim to predict their corresponding categories (Carenini et al., 2005; Yu et al., 2011;\nZhai et al., 2010, 2011). To apply these methods to solve the problem studied in this paper, one needs to first use some aspect/opinion terms extraction method to identify aspect/opinion terms as a preprocessing step. In such a pipeline solution, error can be propagated across steps."}, {"heading": "2.2 Deep Multi-task Learning", "text": "Multi-task learning aims to improve generalization for each individual task by exploiting relatedness among different tasks (Caruana, 1997). One common assumption in multi-task learning is that parameters for different tasks lie in a low-dimensional subspace (Argyriou et al., 2008; Kumar and III, 2012) which is achieved either by imposing low-rank constraint or matrix factorization. Through factorization, the model of each task becomes a linear combination of a small set of latent tasks. Following this idea, a multi-linear model was proposed in (Romera-Paredes et al., 2013) to deal with multi-modal tasks with multiple indexes. This tensor factorization idea also promotes a deep multitask learning model (Yang et al., 2016) where the parameters in different layers of a CNN for different tasks form a tensor that could be factorized across tasks. Moreover, many deep learning models have been introduced for multi-task learning (Liu et al., 2015b; Misra et al., 2016) with an aim to learn shared hidden representation that are regularized from different tasks. Our proposed deep multi-task learning model is specially designed to suit in sentiment analysis and is expected to be more effective for the proposed finergrained sentiment analysis compared with the general deep multi-task learning methods."}, {"heading": "3 Preliminary", "text": "The base classifier used in our deep multi-task learning model is the coupled multi-layer attentions (CMLA) (Wang et al., 2017), which is proposed for aspect-opinion terms co-extraction. The basic component of CMLA is a pair of coupled attentions composing of an aspect attention and an opinion attention, which are interactively learned. The idea behind the coupled attentions is to exploit the relations among the aspect terms and opinion terms for double propagation (Qiu et al., 2011; Wang et al., 2016). Through the multi-layer design, both direct and indirect relations among terms can be captured.\nSpecifically, given a sentence with pre-trained word embeddings {xi}\u2019s, Gated Recurrent Unit (GRU) (Cho et al., 2014) is applied on top of xi to obtain input feature representations H = {h1, ..., hni}. The architecture for the first layer of CMLA is shown in Figure 1, where an aspect prototype vector ua and an opinion prototype vector up are first initialized for guiding the attention to select aspect and opinion terms. During learning, transformed hidden representations rai and r p i are computed from the aspect attention fa and the opinion attention fp, respectively, for each hi through its interaction with two prototype vectors:\nfa(hi, u a, up) = tanh([h\u22a4i G aua : h\u22a4i D aup]),(1) fp(hi, u a, up) = tanh([h\u22a4i G pua : h\u22a4i D pup]),(2)\nwhere [:] denotes concatenation of vectors. Ga, Gp \u2208RK\u00d7d\u00d7d and Da,Dp\u2208RK\u00d7d\u00d7d are 3- dimensional tensors composed of K bi-linear interaction matrices where each matrix models one type of implicit relation between hi and ua (or up). The core idea of coupled learning is reflected in the way that both aspect and opinion prototypes are used to compute each attention. Then rai and rpi are obtained with GRU, given f a(hi, u a, up) and fp(hi, ua, up) as input, respectively, to incorporate context information. Let\u2019s denote\n(rai , r p i ) = g(hi, u a, up;G,D, \u03b8GRU ), (3)\nwhere G = {Ga, Gp}, D = {Da,Dp}, and \u03b8GRU = { \u03b8 a GRU , \u03b8 p GRU} includes transformation matrices in GRU computation. Additionally, an attention score eai (or e p i ) is computed to reflect the relevance of input hi for aspect (or opinion) attention, and to compute sentence representation oa for aspect (or op for opinion). We illustrate the computation for aspect attention as follows1,\neai = v a\u22a4 \u00b7 rai , and o a =\nni \u2211\ni=1\n\u03b1ai hi, (4)\nwhere \u03b1ai =exp(e a i )/ \u2211n j exp(e a j ), and v a, which is to be learned, transforms vector rai to attention score eai . Intuitively, o\na is dominated by the input feature vectors {hi}\u2019s with higher attention scores, which indicate more probable aspect/opinion terms. This will help to produce better prototype vector in the next layer, which in turn\n1Computation for opinion attention is similar\nis used to interact with {hi}\u2019s in the next layer:\nuat+1 = tanh(V a \u00b7 uat ) + o a t , (5)\nwhere t is the index of a layer. In this way, uat+1 incorporates most probable aspect terms to select the other non-obvious target tokens in the next layer. The final prediction for aspects is made on the sum of hidden vectors rai,t for each layer t, similar for opinions. The objective for training is to minimize the sum of cross-entropy losses of each token for both aspect prediction and opinion prediction."}, {"heading": "4 Problem Statement and Motivation", "text": "Let C = {1, 2, ..., C} denote a predefined set of C categories, where c \u2208 C is an entity/attribute type, e.g., \u201cDRINK#QUALITY\u201d in the restaurant domain. For any review sentence zi = {wi1, ..., wini} consisting of a sequence of ni tokens, we are given a collection of all the explicit aspect terms and opinion terms appearing in zi, as well as their corresponding categories, denoted by Ai = {(ai1, yai1), ..., (ail, y a il)} for aspects, and by Pi = {(pi1, y p i1), ..., (pim, y p im))} for opinions, where yaij, y p ij \u2208 C. Our goal is to learn a predictive model f that takes zi as input and generates Ai and Pi as outputs. Moreover, as an aspect or opinion term can be a phrase, we use the BIO encoding scheme to define labels. Specifically, for each category c, we have five labels {BAc, IAc,BPc, IPc,Oc}, where BAc and IAc refer to beginning of aspect and inside of aspect, respectively, for category c, similar for BPc and IPc for opinions, and Oc refers to none of aspect or opinion terms for category c.\nTo solve this problem using existing aspect/opinion terms extraction methods, e.g, CMLA, one straightforward solution is to train a CMLA model for each category c, and combine the results of all CMLA models to generate final\npredictions. However, for each fine-grained category, aspect and opinion terms become extremely sparse, which makes it difficult to learn a precise model for each category if trained independently. To address this issue, we propose to model the problem in a multi-task learning manner, where term extraction for each category is considered as an individual task, and the goal is to jointly learn all the tasks by exploiting the commonalities and similarities among them. We hypothesize that some commonalities exist in the syntactic interactions among the tokens for different categories, and there are subtle relations among the categories that indicate their similarities to facilitate sharing. Moreover, global information on the overall categories of a sentence could be incorporated to assist token-level predictions.\nThese considerations are reflected in the following 3 components: 1) Coupled Attentions with Shared Tensors, which aims to model the commonalities in syntactic relations by sharing the tensor parameters {G,D} among CMLA models, 2) Context-aware Feature Sharing, which aims to share features among tasks through constructing context-aware task similarity matrices, and 3) Auxiliary Task, which creates an auxiliary task to predict overall sentence-level category labels for helping token-level prediction tasks.\nIn the sequel, we name our proposed model as Multi-task Coupled Attentions (MTCA), whose overall architecture is shown in Figure 2. Specifically, MTCA takes the learning of CMLA for each category c as a task Tc, and a sequence of word embeddings {xi}\u2019s as the input. Through the three components, each Tc produces a task-specific feature representation r\u0303ci for each word, as well as a task-specific context representation o\u0303c for the sequence (i.e., sentence). Backpropagation is conducted on both the token-level prediction tasks and the auxiliary sentence-level prediction task to update the feature representations. In the following section, we present the 3 components in detail."}, {"heading": "5 Proposed Methodology", "text": ""}, {"heading": "5.1 Coupled Attentions with Shared Tensors", "text": "Similar to Section 3, for each task Tc, we generate a pair of embeddings uc = {uac , u p c}2 to capture the distributed representations of aspect prototype and opinion prototype, respectively, for category\n2We randomly initialize all category embeddings uc from a uniform distribution: uc \u223c U [\u22120.2, 0.2] \u2208 Rd.\nc. Follow (3), we aim to obtain\nrci = ( r (c,a) i , r (c,p) i ) =g(hi, u a c , u p c ;Gc,Dc, \u03b8GRU )\nfor each task Tc. Here different Tc corresponds to different tensors {Gc,Dc} where Gc={Gac , G p c} and Dc= {Dac ,D p c} model the complex token interactions. We assume that these relations are similar across categories, and propose to learn a lowrank shared information among {Gc}\u2019s ({Dc}\u2019s) through tensor factorization. Specifically, let Ga \u2208 R\nC\u00d7K\u00d7d\u00d7d be the concatenation of all the {Gac}\u2019s and denote by Gak = G a [\u00b7,k,\u00b7,\u00b7] \u2208 R\nC\u00d7d\u00d7d the collection of k-th bi-linear interaction matrices across C tasks for aspect attention. The same also applies for opinion attention. Factorization is performed on each Gak and G p k, respectively, through\nGak[c,\u00b7,\u00b7]=Z a k [c,\u00b7]G a k , and G p k[c,\u00b7,\u00b7] =Zpk [c,\u00b7]G p k , (6)\nwhere Gak ,G p k\u2208R C\u2032\u00d7d\u00d7d are shared factors among all the tasks with C \u2032<C , while Zak , Z p k \u2208R C\u00d7C\u2032 with each row Zak [c,\u00b7] and Z p k [c,\u00b7]\nbeing specific factors for Tc. The matrices of shared factors can be considered as C \u2032 latent basis interactions, where the original k-th bi-linear relation matrix Gak[c,\u00b7,\u00b7] (Gpk[c,\u00b7,\u00b7]) for Tc is the linear combination of those latent basis interactions. In this way, we reduce the parameter dimensions by enforcing sharing within a small number of latent interactions. The same approach also applies to the tensors {Dc}\u2019s."}, {"heading": "5.2 Context-aware Feature Sharing", "text": "Besides syntactic relations, we further explore similarities between tasks or categories to learn\nmore powerful features for different task. For example, \u201cFOOD#PRICE\u201d is more similar to \u201cDRINK#PRICE\u201d than \u201cSERVICE#GENERAL\u201d because the first two categories may share some common aspect/opinion terms, such as expensive. By representing each task or category in a form of distributed vector, we can directly compute their similarities to facilitate knowledge sharing. In this component, we aim to update features r\u0303ci from r c i by integrating task relatedness. To do that, we construct context-aware category representations which capture not only the general category information, but also the context it lies in. Follow (4), we obtain overall context representations oc = {o a c , o p c} for Tc through attention scores:\noac =\nni \u2211\ni=1\n\u03b1 (c,a) i hi, and o p c =\nni \u2211\ni=1\n\u03b1 (c,p) i hi, (7)\nwhere \u03b1(c,a)i and \u03b1 (c,p) i are normalized scores that represent the relevance of hi for aspect and opinion attentions in Tc, respectively. As a result, oac and opc will capture the features of the most likely aspect and opinion terms for category c within the sentence, as indicated by higher attention scores. Given oc, the context-aware category representation u\u0303c = {u\u0303ac , u\u0303 p c} is generated through:\nu\u0303ac =tanh(V\u0303 auac ) + o a c , u\u0303 p c=tanh(V\u0303 pupc) + o p c\nwhere V\u0303 a, V\u0303 p \u2208 Rd\u00d7d are transformation matrices. Through this operation, u\u0303c incorporates both general category features uc and context-specific category features oc for the input sentence. Let U\u0303a, U\u0303p \u2208 RC\u00d7d denote the matrices consisting of u\u0303ac , u\u0303 p c as a row vector, respectively, the task similarity matrices in terms of aspects and opinions, Sa and Sp, can be computed as follows,\nSa = q(U\u0303aU\u0303a\u22a4), and Sp = q(U\u0303pU\u0303p\u22a4), (8)\nwhere q(\u00b7) is the softmax function carried in a rowwise manner so that the similarity scores between Tc and any Tc\u2032 sum up to 1. The similarity matrices Sa and Sp are then used to refine feature representation for each task by incorporating feature representations from related tasks:\nr\u0303 (c,a) i =\nC \u2211\nc\u2032=1\nSacc\u2032r (c\u2032,a) i , r\u0303 (c,p) i =\nC \u2211\nc\u2032=1\nSpcc\u2032r (c\u2032,p) i ,\no\u0303ac =\nC \u2211\nc\u2032=1\nSacc\u2032o a c\u2032 , and o\u0303 p c=\nC \u2211\nc\u2032=1\nSpcc\u2032o p c\u2032 ,\nWe denote by r\u0303ci = {r\u0303 (c,a) i , r\u0303 (c,p) i } the new feature representations for the i-th token and by o\u0303c= {o\u0303ac , o\u0303 p c} the new feature representations for the overall context for the aspect attention and the opinion attention of task Tc, respectively.\nNote that the feature sharing among different tasks is context-aware because U\u0303a and U\u0303p are context-aware category representations. This means that different sentences might indicate different task similarities. For example, when cheap is presented, it might increase the similarities between \u201cFOOD#PRICES\u201d and \u201cRESTAURANT#PRICES\u201d. As a result, r\u0303(c,a)i for Tc could incorporate more information from Tc\u2032 if Tc\u2032 has higher similarity score indicated by Sacc\u2032."}, {"heading": "5.3 Multiple Layers", "text": "To capture non-obvious relations among aspect and opinion terms, we construct multiple layers of attentions. Given a sentence, each layer t produces its own {r\u0303ci,t} C c=1 for the i-th token, and {o\u0303c,t} C c=1 for the whole sentence. Note that different layers takes the same H but different prototype vectors uc,t as the input, where uac,t (similar for u p c,t) is updated through:\nuac,t = tanh(V auac,t\u22121) + o\u0303 a c,t\u22121. (9)\nThe final feature representations are obtained as the sum of features from each layer:\n\u03b3 (c,a) i =\nT \u2211\nt=1\nr\u0303 (c,a) i,t , \u03b3 (c,p) i =\nT \u2211\nt=1\nr\u0303 (c,p) i,t (10)\n\u03b2ac =\nT \u2211\nt=1\no\u0303ac,t, and \u03b2 p c =\nT \u2211\nt=1\no\u0303pc,t, (11)\nwhere \u03b3(c,a)i and \u03b3 (c,p) i are the final representations for i-th token, while \u03b2ac and \u03b2 p c are the final representations for the input sentence for task Tc."}, {"heading": "5.4 Auxiliary Task", "text": "To better address the data sparsity issue, we aim to use additional global information on categories in the sentence level. Consider the following motivating example, if we know the sentence \u201cThe soup is served with nice portion, the service is prompt\u201d has aspect/opinion terms from category \u201cDRINKS#STYLE OPTIONS\u201d and \u201cSERVICE#GENERAL\u201d, we can infer that some words in the sentence should belong to one of these two categories. To make use of this information, we\nconstruct an auxiliary task to predict the categories of a sentence. Note that in training data, sentencelevel labels can be obtained by integrating tokens\u2019 labels. Therefore, besides the token loss for our target token-level prediction tasks, we also need to define sentence loss for the auxiliary task. The learning of the target task and auxiliary task are not independent. On one hand, the global sentence information helps the attentions to select categoryrelevant tokens. On the other hand, if the attentions are able to attend to target terms, the output context representation will filter out irrelevant noise, which helps the overall sentence prediction.\nDenote by f csen the classifier for the sentencelevel prediction task for category c, and by ftok the classifier for target task, which are defined as,\ny\u0302c = f c sen(\u03b2 a c , \u03b2 p c ) = q(Wc[\u03b2 a c : \u03b2 p c ]), (12) y\u0302 (c,a) i = ftok(\u03b3 (c,a) i ;W a) = q(W a\u03b3 (c,a) i ), (13) y\u0302 (c,p) i = ftok(\u03b3 (c,p) i ;W p) = q(W p\u03b3 (c,p) i ), (14)\nwhere q(\u00b7) is the softmax function, y\u0302c \u2208 R2 indicates the probability of the sentence belonging to category c, and y\u0302(c,a)i , y\u0302 (c,p) i \u2208 R\n3 denoting the probabilities of being BAc (BPc), IAc (IPc) or Oc for category c. We use shared W a (W p) for token predictions because \u03b3(c,a)i (\u03b3 (c,p) i ) encodes the extent of interaction with prototype vectors that are not task-specific. The final label y\u0302ci for i-th token for category c is produced by comparing the largest value in y\u0302(c,a)i and y\u0302 (c,p) i . If both of them are O, then the final label is O. If only one of them is O, we pick the other one as the label. Otherwise, the final label is the one with the largest value. By incorporating the loss of the auxiliary task, the objective for our model can be written as:\nmin ftok,fcsen\n\u2211\nc\ne(yc, f c sen(\u03b2 a c , \u03b2 p c ))\n+\u03bb \u2211\nc\nni \u2211\ni=1\n\u2211\nm\u2208{a,p}\ne (\ny (c,m) i , ftok\n(\n\u03b3 (c,m) i ;W\nm ))\n,\nwhere e is the cross-entropy loss, yc \u2208 {0, 1} indicates whether category c is presented for the input sentence, y(c,m)i \u2208 R\n3 is a one-hot vector representing ground-truth label for i-th token, and \u03bb is a trade-off parameter for the two types of losses."}, {"heading": "5.5 Training", "text": "As shown in Figure 2, the training is carried by propagating the errors from top to bottom in an\nend-to-end manner. Specifically, we first obtain the loss from both the token-level prediction as well as the sentence-level prediction, which are backpropagated to r\u0303ci,t and o\u0303c,t in each layer, respectively for each Tc. These together will update Sa, Sp, which combines with the error from o\u0303c,t to update oc,t. Then the error from both r\u0303 c,t i and oc,t will be used to update all the parameters for the coupled attentions until the word embeddings."}, {"heading": "6 Experiments", "text": "The experiments are conducted on three benchmark datasets from subtask 1 in SemEval Challenge 2015 task 12 (Pontiki et al., 2015), SemEval Challenge 2016 task 5 (Pontiki et al., 2016), and SemEval Challenge 2014 task 4 (Pontiki et al., 2014), which are denoted by S1, S2, and S3, respectively. Note that S1 and S2 are reviews in restaurant domain and S3 is in laptop domain3. We use term-level aspect-opinion annotations provided by (Wang et al., 2017) for S1 and S3, and manually annotate opinion terms for S2. To facilitate our experiment, we also make additional annotations on category labels for target terms, except for the aspect term categories for S1 and S2 which are provided by SemEval. The statistics of each dataset is shown in Table 1 where text and tuple represent the number of sentences and the number of tuples consisting of an aspect term and its corresponding category label, respectively. Each sentence may contain multiple aspect terms with more than one categories. The aspect categories are shown in Table 2. For S1 and S2, an aspect category is defined as the combination of an entity and an attribute, e.g., \u201cFOOD#PRICES\u201d. There are in total 12 categories. For S3, an aspect category is an entity.\n3We filter out some categories with a few target terms. The remaining categories are shown in Table 2.\nFollow (Wang et al., 2017), we first obtain word embeddings by applying word2vec4 on Yelp Challenge dataset5 and electronic domain in Amazon reviews (McAuley et al., 2015) for restaurant and laptop datasets, respectively. We set the dimension of word embeddings to be 150 and the dimension after GRU transformation to be 50 for all the three datasets. We use two layers as in CMLA for experiments. For each layer, the number of bilinear interactions for the 3-dimensional tensors is 20 (K=20), and tensor factorization operates with C \u2032=5 for S1 and S2, and C \u2032=8 for S3. We apply partial dropout at 0.5 to chosen parameters to avoid overfitting. The training is carried with rmsprop with the initial value at 0.001 and decayed with rate 0.9. The trade-off parameter \u03bb is set to be 1.0. All the hyper-parameters are chosen according to cross-validation."}, {"heading": "6.1 Experimental Results", "text": "We conduct comparison experiments with the following baseline models: NLANG: The best system for both SemEval-15 and SemEval-16 for the proposed task. IHS-RD, XRCE: The second best systems for SemEval-15 and SemEval-16, respectively. RNCRF+: We modify RNCRF (Wang et al., 2016), which is for aspect-opinion terms extraction, by defining finer-grained categories as labels. CMLA+: Similar to RNCRF+, we modify CMLA (Wang et al., 2017) by defining finergrained categories as labels. CMLA++: CMLA is used to extract all the aspect and opinion terms, and then a category classification layer is added to classify the extracted terms.\nWe report the results from top performing systems in the Challenges for S1 and S2. There are no reported results for S3 as the original task is different from ours. Note that original task for S1 and S2 includes two slots: slot 1 for aspect terms extraction and slot 2 for aspect category prediction. Moreover, SemEval made the combination of slot 1 and slot 2 as an additional task that corresponds to the problem we study. However, most of the reported models did not provide feasible methods for the joint prediction of aspect terms and corresponding categories, instead, they trained the model for slot 1 first and then combined with slot 2. This may fail to capture the relations be-\n4https://radimrehurek.com/gensim/models/word2vec.html 5http://www.yelp.com/dataset challenge\ntween target terms and their categories. In order to show the advantage of our model, we modify the existing state-of-the-art deep models for aspect/opinion terms extraction to suit our problem settings. Since RNCRF and CMLA both exploit the correlations between aspect terms and opinion terms, which have been shown to be effective for extraction task, a simple idea is to increase the number of classes to incorporate different categories, e.g., BA becomes {BAc}\u2019s for different category c. By increasing the number of classes, the only change to the original model is the dimension of classification matrix. As a result, the modified model should be able to capture the correlations between target terms based on their categories. On the other hand, we also construct another baseline model (denoted by CMLA++) based on CMLA by separating the task into 2 steps. The first step is the same as CMLA for extracting target terms. Then the second step performs category prediction only on the extracted terms.\nThe comparison results are shown in Table 3. It can be seen that MTCA achieves the state-of-theart performances in category-specific aspect and opinion terms extraction (ASC and OPC). And there is a large gap between the results of MTCA and the other baseline models on S1 and S2. This is because RNCRF+ and CMLA+ can only propagate information between target terms within each category, but fail to explore the relations and commonalities among different categories. The other model CMLA++ performs even poorer, because the training is separated into different stages, similar to the top systems in SemEval Challenges. This separation results in the failure of propagating information from category prediction to target term extraction. The result proves the effectiveness of MTCA for learning shared information among different tasks, as well as the addition of global information to assist extraction. The improvement for S3 is not significant, which might indicate that the category correlations are not obvious in laptop domain, as can be seen in Table 2. Only entity labels make different categories distinct from each other.\nMoreover, we also report the results on target terms extraction (AS and OP) by accumulating the aspect/opinion terms that are assigned at least one category by MTCA. It can be seen that MTCA still achieves comparable performances even if the data becomes sparser when adding the category\ninformation. On the contrary, the results for RNCRF+, CMLA+ and CMLA++ are obtained using the original models that ignore category labels.\nAs have been discussed in the previous sections, the multi-task attention network explores the commonalities and relations among tasks through both tensor sharing and feature sharing, as well as enhances prediction results by incorporating auxiliary labels. To test the effect of each component, we conduct comparison experiments for different combinations of these components as shown in Table 4, where C1, C2 and C3 represents separate component for multi-task tensor sharing, contextaware feature sharing and auxiliary task, respectively. Note that C2+C3* refers to the use of same tensor across all the tasks. Clearly, The inclusion of either feature sharing (C2+C3) or tensor sharing (C1+C3) improves the results compared to independent training (C3). Furthermore, tensor sharing proves to be more beneficial than feature sharing. This shows that the commonalities in terms of token interactions are more obvious for different tasks. Moreover, either independent tensors (C2+C3) or the same tensor (C2+C3*) across tasks does not perform well. This indicates that it is still crucial to explore both the uniqueness and commonality of all the tasks, which are preserved in our proposed model. By comparing the results between C1+C2+C3 and C1+C2, we can see how auxiliary task contributes to the final prediction (2.49% and 4.05% increase for ASC in S1 and S2 respectively). This shows that the global information in the sentence level could enforce the correct predictions of each token within the sentence by reiterating the category information.\nTo show the robustness of our model, we test\nit with different dimension of factorization (C \u2032). The results on S1 are shown in Figure 3. We also provide some examples in Figure 4 to show what the attentions learn for different categories. The first column shows the largest attention scores for both aspects and opinions in each identified category. We use different colors to denote different categories and list the identified categories in the second column. Clearly, MTCA is able to attend important tokens for different categories for extraction. Moreover, MTCA is able to identify the case when specific terms belong to more than one categories, which are indicated with multiple colors in the figure."}, {"heading": "7 Conclusion", "text": "In this work, we introduce a finer-grained task involving the predictions of both aspect/opinion terms and their corresponding aspect categories, and offer a novel multi-task deep learning model, MTCA, to solve the problem. The model is able to exploit syntactic commonalities and task simi-\nlarities through attention mechanism. In the end, we demonstrate the effectiveness of our model on three benchmark datasets."}], "references": [{"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil."], "venue": "Mach. Learn. 73(3):243\u2013272.", "citeRegEx": "Argyriou et al\\.,? 2008", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "K-cap", "author": ["Giuseppe Carenini", "Raymond T. Ng", "Ed Zwart."], "venue": "pages 11\u201318.", "citeRegEx": "Carenini et al\\.,? 2005", "shortCiteRegEx": "Carenini et al\\.", "year": 2005}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Mach. Learn. 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Clustering for simultaneous extraction of aspects and features from reviews", "author": ["Lu Chen", "Justin Martineau", "Doreen Cheng", "Amit P. Sheth."], "venue": "NAACLHLT. pages 789\u2013799.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Aspect extraction with automated prior knowledge learning", "author": ["Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu."], "venue": "ACL. pages 347\u2013358.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP. pages", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Product feature categorization with multilevel latent semantic association", "author": ["Honglei Guo", "Huijia Zhu", "Zhili Guo", "XiaoXun Zhang", "Zhong Su."], "venue": "CIKM. pages 1087\u20131096.", "citeRegEx": "Guo et al\\.,? 2009", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "KDD. pages 168\u2013177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Extracting opinion targets in a single- and cross-domain setting with conditional random fields", "author": ["Niklas Jakob", "Iryna Gurevych."], "venue": "EMNLP. pages 1035\u20131045.", "citeRegEx": "Jakob and Gurevych.,? 2010", "shortCiteRegEx": "Jakob and Gurevych.", "year": 2010}, {"title": "A novel lexicalized hmm-based learning framework for web opinion mining", "author": ["Wei Jin", "Hung Hay Ho."], "venue": "ICML. pages 465\u2013472.", "citeRegEx": "Jin and Ho.,? 2009", "shortCiteRegEx": "Jin and Ho.", "year": 2009}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daum III."], "venue": "ICML.", "citeRegEx": "Kumar and III.,? 2012", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Structure-aware review mining and summarization", "author": ["Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Ying-Ju Xia", "Shu Zhang", "Hao Yu."], "venue": "COLING. pages 653\u2013661.", "citeRegEx": "Li et al\\.,? 2010", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["Pengfei Liu", "Shafiq Joty", "Helen Meng."], "venue": "EMNLP. pages 1433\u20131443.", "citeRegEx": "Liu et al\\.,? 2015a", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang."], "venue": "NAACL. pages 912\u2013921.", "citeRegEx": "Liu et al\\.,? 2015b", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Rated aspect summarization of short comments", "author": ["Yue Lu", "ChengXiang Zhai", "Neel Sundaresan."], "venue": "WWW. pages 131\u2013140.", "citeRegEx": "Lu et al\\.,? 2009", "shortCiteRegEx": "Lu et al\\.", "year": 2009}, {"title": "Image-based recommendations on styles and substitutes", "author": ["Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton van den Hengel."], "venue": "SIGIR. pages 43\u201352.", "citeRegEx": "McAuley et al\\.,? 2015", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Cross-stitch networks for multi-task learning", "author": ["Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert"], "venue": null, "citeRegEx": "Misra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "SemEval-2016 task 5: Aspect based sentiment analysis", "author": ["talia Loukachevitch", "Evgeny Kotelnikov", "Nuria Bel", "Salud Mara Jimnez-Zafra", "Glen Eryiit"], "venue": "SemEval", "citeRegEx": "Loukachevitch et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Loukachevitch et al\\.", "year": 2016}, {"title": "SemEval-2015 task 12: Aspect based sentiment analysis", "author": ["Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos."], "venue": "SemEval 2015. pages 486\u2013495.", "citeRegEx": "Pontiki et al\\.,? 2015", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "Semeval-2014 task 4: Aspect based sentiment analysis", "author": ["Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar."], "venue": "SemEval. pages 27\u201335.", "citeRegEx": "Pontiki et al\\.,? 2014", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "Extracting product features and opinions from reviews", "author": ["Ana-Maria Popescu", "Oren Etzioni."], "venue": "EMNLP. pages 339\u2013346.", "citeRegEx": "Popescu and Etzioni.,? 2005", "shortCiteRegEx": "Popescu and Etzioni.", "year": 2005}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen."], "venue": "Comput. Linguist. 37(1):9\u201327.", "citeRegEx": "Qiu et al\\.,? 2011", "shortCiteRegEx": "Qiu et al\\.", "year": 2011}, {"title": "Multilinear multitask learning", "author": ["Bernardino Romera-Paredes", "Hane Aung", "Nadia Bianchi-Berthouze", "Massimiliano Pontil."], "venue": "ICML (3). volume 28 of JMLR Workshop and Conference Proceedings, pages 1444\u20131452.", "citeRegEx": "Romera.Paredes et al\\.,? 2013", "shortCiteRegEx": "Romera.Paredes et al\\.", "year": 2013}, {"title": "Hidden sentiment association in chinese web opinion mining", "author": ["Qi Su", "Xinying Xu", "Honglei Guo", "Zhili Guo", "Xian Wu", "Xiaoxun Zhang", "Bin Swen", "Zhong Su."], "venue": "WWW. pages 959\u2013968.", "citeRegEx": "Su et al\\.,? 2008", "shortCiteRegEx": "Su et al\\.", "year": 2008}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["Ivan Titov", "Ryan McDonald."], "venue": "www. pages 111\u2013120.", "citeRegEx": "Titov and McDonald.,? 2008a", "shortCiteRegEx": "Titov and McDonald.", "year": 2008}, {"title": "A joint model of text and aspect ratings for sentiment summarization", "author": ["Ivan Titov", "Ryan T. McDonald."], "venue": "ACL. pages 308\u2013316.", "citeRegEx": "Titov and McDonald.,? 2008b", "shortCiteRegEx": "Titov and McDonald.", "year": 2008}, {"title": "Recursive neural conditional random fields for aspect-based sentiment analysis", "author": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao."], "venue": "EMNLP.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Coupled multi-layer tensor network for co-extraction of aspect and opinion terms", "author": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao."], "venue": "AAAI.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Phrase dependency parsing for opinion mining", "author": ["Yuanbin Wu", "Qi Zhang", "Xuanjing Huang", "Lide Wu."], "venue": "EMNLP. pages 1533\u20131541.", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "NAACL. pages 1480\u20131489.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Unsupervised word and dependency path embeddings for aspect term extraction", "author": ["Yichun Yin", "Furu Wei", "Li Dong", "Kaimeng Xu", "Ming Zhang", "Ming Zhou."], "venue": "IJCAI.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Aspect ranking: Identifying important product aspects from online consumer reviews", "author": ["Jianxing Yu", "Zheng-Jun Zha", "Meng Wang", "TatSeng Chua."], "venue": "ACL.", "citeRegEx": "Yu et al\\.,? 2011", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Grouping product features using semi-supervised learning with soft-constraints", "author": ["Zhongwu Zhai", "Bing Liu", "Hua Xu", "Peifa Jia."], "venue": "COLING. pages 1272\u20131280.", "citeRegEx": "Zhai et al\\.,? 2010", "shortCiteRegEx": "Zhai et al\\.", "year": 2010}, {"title": "Clustering product features for opinion mining", "author": ["Zhongwu Zhai", "Bing Liu", "Hua Xu", "Peifa Jia."], "venue": "WSDM. pages 347\u2013354.", "citeRegEx": "Zhai et al\\.,? 2011", "shortCiteRegEx": "Zhai et al\\.", "year": 2011}, {"title": "Jointly modeling aspects and opinions with a maxent-lda hybrid", "author": ["Wayne Xin Zhao", "Jing Jiang", "Hongfei Yan", "Xiaoming Li."], "venue": "EMNLP. pages 56\u201365.", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}, {"title": "Movie review mining and summarization", "author": ["Li Zhuang", "Feng Jing", "Xiao-Yan Zhu."], "venue": "CIKM. pages 43\u201350.", "citeRegEx": "Zhuang et al\\.,? 2006", "shortCiteRegEx": "Zhuang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 7, "context": "Under this branch, most work has been proposed for aspect/opinion terms extraction (Hu and Liu, 2004; Qiu et al., 2011; Wang et al., 2016), where an aspect term refers to a word or a phrase describing some feature of an entity, and an opinion term refers to the expression carrying subjective emotions.", "startOffset": 83, "endOffset": 138}, {"referenceID": 21, "context": "Under this branch, most work has been proposed for aspect/opinion terms extraction (Hu and Liu, 2004; Qiu et al., 2011; Wang et al., 2016), where an aspect term refers to a word or a phrase describing some feature of an entity, and an opinion term refers to the expression carrying subjective emotions.", "startOffset": 83, "endOffset": 138}, {"referenceID": 26, "context": "Under this branch, most work has been proposed for aspect/opinion terms extraction (Hu and Liu, 2004; Qiu et al., 2011; Wang et al., 2016), where an aspect term refers to a word or a phrase describing some feature of an entity, and an opinion term refers to the expression carrying subjective emotions.", "startOffset": 83, "endOffset": 138}, {"referenceID": 1, "context": "Given an extracted aspect term, early work (Carenini et al., 2005; Yu et al., 2011) applied lexicon and taxonomybased methods to classify it to a category according to some distance measures.", "startOffset": 43, "endOffset": 83}, {"referenceID": 31, "context": "Given an extracted aspect term, early work (Carenini et al., 2005; Yu et al., 2011) applied lexicon and taxonomybased methods to classify it to a category according to some distance measures.", "startOffset": 43, "endOffset": 83}, {"referenceID": 6, "context": "Although topic models (Guo et al., 2009; Titov and McDonald, 2008a) can achieve both grouping and extraction at the same time, they mainly focused on grouping and can only identify general and coarse-grained aspect terms.", "startOffset": 22, "endOffset": 67}, {"referenceID": 24, "context": "Although topic models (Guo et al., 2009; Titov and McDonald, 2008a) can achieve both grouping and extraction at the same time, they mainly focused on grouping and can only identify general and coarse-grained aspect terms.", "startOffset": 22, "endOffset": 67}, {"referenceID": 27, "context": "Inspired by (Wang et al., 2017), we model each task with coupled multi-layer attentions to extract the relations between aspect terms and opinion terms within each category.", "startOffset": 12, "endOffset": 31}, {"referenceID": 7, "context": "The first approach aims to exploit syntactic dependency relations among aspect terms and opinion terms for information extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011).", "startOffset": 130, "endOffset": 231}, {"referenceID": 20, "context": "The first approach aims to exploit syntactic dependency relations among aspect terms and opinion terms for information extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011).", "startOffset": 130, "endOffset": 231}, {"referenceID": 35, "context": "The first approach aims to exploit syntactic dependency relations among aspect terms and opinion terms for information extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011).", "startOffset": 130, "endOffset": 231}, {"referenceID": 28, "context": "The first approach aims to exploit syntactic dependency relations among aspect terms and opinion terms for information extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011).", "startOffset": 130, "endOffset": 231}, {"referenceID": 21, "context": "The first approach aims to exploit syntactic dependency relations among aspect terms and opinion terms for information extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Wu et al., 2009; Qiu et al., 2011).", "startOffset": 130, "endOffset": 231}, {"referenceID": 9, "context": "A second approach models the extraction of target terms as a supervised sequence labeling problem with exhaustive human-engineered features (Jin and Ho, 2009; Li et al., 2010; Jakob and Gurevych, 2010).", "startOffset": 140, "endOffset": 201}, {"referenceID": 11, "context": "A second approach models the extraction of target terms as a supervised sequence labeling problem with exhaustive human-engineered features (Jin and Ho, 2009; Li et al., 2010; Jakob and Gurevych, 2010).", "startOffset": 140, "endOffset": 201}, {"referenceID": 8, "context": "A second approach models the extraction of target terms as a supervised sequence labeling problem with exhaustive human-engineered features (Jin and Ho, 2009; Li et al., 2010; Jakob and Gurevych, 2010).", "startOffset": 140, "endOffset": 201}, {"referenceID": 12, "context": "To reduce the effort of feature engineering, a third approach aims to utilize deep learning to learn high-level features automatically (Liu et al., 2015a; Yin et al., 2016; Wang et al., 2016, 2017).", "startOffset": 135, "endOffset": 197}, {"referenceID": 30, "context": "To reduce the effort of feature engineering, a third approach aims to utilize deep learning to learn high-level features automatically (Liu et al., 2015a; Yin et al., 2016; Wang et al., 2016, 2017).", "startOffset": 135, "endOffset": 197}, {"referenceID": 25, "context": "A forth approach adopts topic models (Titov and McDonald, 2008b; Lu et al., 2009; Zhao et al., 2010; Chen et al., 2014) or clustering techniques (Su et al.", "startOffset": 37, "endOffset": 119}, {"referenceID": 14, "context": "A forth approach adopts topic models (Titov and McDonald, 2008b; Lu et al., 2009; Zhao et al., 2010; Chen et al., 2014) or clustering techniques (Su et al.", "startOffset": 37, "endOffset": 119}, {"referenceID": 34, "context": "A forth approach adopts topic models (Titov and McDonald, 2008b; Lu et al., 2009; Zhao et al., 2010; Chen et al., 2014) or clustering techniques (Su et al.", "startOffset": 37, "endOffset": 119}, {"referenceID": 4, "context": "A forth approach adopts topic models (Titov and McDonald, 2008b; Lu et al., 2009; Zhao et al., 2010; Chen et al., 2014) or clustering techniques (Su et al.", "startOffset": 37, "endOffset": 119}, {"referenceID": 23, "context": ", 2014) or clustering techniques (Su et al., 2008; Yu et al., 2011; Chen et al., 2016) to group potential aspect terms into different clusters (not explicit categories).", "startOffset": 33, "endOffset": 86}, {"referenceID": 31, "context": ", 2014) or clustering techniques (Su et al., 2008; Yu et al., 2011; Chen et al., 2016) to group potential aspect terms into different clusters (not explicit categories).", "startOffset": 33, "endOffset": 86}, {"referenceID": 3, "context": ", 2014) or clustering techniques (Su et al., 2008; Yu et al., 2011; Chen et al., 2016) to group potential aspect terms into different clusters (not explicit categories).", "startOffset": 33, "endOffset": 86}, {"referenceID": 1, "context": "For the task of aspect categorization, most existing methods assume the aspect terms be extracted in advance, and aim to predict their corresponding categories (Carenini et al., 2005; Yu et al., 2011; Zhai et al., 2010, 2011).", "startOffset": 160, "endOffset": 225}, {"referenceID": 31, "context": "For the task of aspect categorization, most existing methods assume the aspect terms be extracted in advance, and aim to predict their corresponding categories (Carenini et al., 2005; Yu et al., 2011; Zhai et al., 2010, 2011).", "startOffset": 160, "endOffset": 225}, {"referenceID": 2, "context": "Multi-task learning aims to improve generalization for each individual task by exploiting relatedness among different tasks (Caruana, 1997).", "startOffset": 124, "endOffset": 139}, {"referenceID": 0, "context": "One common assumption in multi-task learning is that parameters for different tasks lie in a low-dimensional subspace (Argyriou et al., 2008; Kumar and III, 2012) which is achieved either by imposing low-rank constraint or matrix factorization.", "startOffset": 118, "endOffset": 162}, {"referenceID": 10, "context": "One common assumption in multi-task learning is that parameters for different tasks lie in a low-dimensional subspace (Argyriou et al., 2008; Kumar and III, 2012) which is achieved either by imposing low-rank constraint or matrix factorization.", "startOffset": 118, "endOffset": 162}, {"referenceID": 22, "context": "Following this idea, a multi-linear model was proposed in (Romera-Paredes et al., 2013) to deal with multi-modal tasks with multiple indexes.", "startOffset": 58, "endOffset": 87}, {"referenceID": 29, "context": "This tensor factorization idea also promotes a deep multitask learning model (Yang et al., 2016) where the parameters in different layers of a CNN for different tasks form a tensor that could be factorized across tasks.", "startOffset": 77, "endOffset": 96}, {"referenceID": 13, "context": "Moreover, many deep learning models have been introduced for multi-task learning (Liu et al., 2015b; Misra et al., 2016) with an aim to learn shared hidden representation that are regularized from different tasks.", "startOffset": 81, "endOffset": 120}, {"referenceID": 16, "context": "Moreover, many deep learning models have been introduced for multi-task learning (Liu et al., 2015b; Misra et al., 2016) with an aim to learn shared hidden representation that are regularized from different tasks.", "startOffset": 81, "endOffset": 120}, {"referenceID": 27, "context": "The base classifier used in our deep multi-task learning model is the coupled multi-layer attentions (CMLA) (Wang et al., 2017), which is proposed for aspect-opinion terms co-extraction.", "startOffset": 108, "endOffset": 127}, {"referenceID": 21, "context": "The idea behind the coupled attentions is to exploit the relations among the aspect terms and opinion terms for double propagation (Qiu et al., 2011; Wang et al., 2016).", "startOffset": 131, "endOffset": 168}, {"referenceID": 26, "context": "The idea behind the coupled attentions is to exploit the relations among the aspect terms and opinion terms for double propagation (Qiu et al., 2011; Wang et al., 2016).", "startOffset": 131, "endOffset": 168}, {"referenceID": 5, "context": "Specifically, given a sentence with pre-trained word embeddings {xi}\u2019s, Gated Recurrent Unit (GRU) (Cho et al., 2014) is applied on top of xi to obtain input feature representations H = {h1, .", "startOffset": 99, "endOffset": 117}, {"referenceID": 18, "context": "The experiments are conducted on three benchmark datasets from subtask 1 in SemEval Challenge 2015 task 12 (Pontiki et al., 2015), SemEval Challenge 2016 task 5 (Pontiki et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 19, "context": ", 2016), and SemEval Challenge 2014 task 4 (Pontiki et al., 2014), which are denoted by S1, S2, and S3, respectively.", "startOffset": 43, "endOffset": 65}, {"referenceID": 27, "context": "We use term-level aspect-opinion annotations provided by (Wang et al., 2017) for S1 and S3, and manually annotate opinion terms for S2.", "startOffset": 57, "endOffset": 76}, {"referenceID": 27, "context": "Follow (Wang et al., 2017), we first obtain word embeddings by applying word2vec4 on Yelp Challenge dataset5 and electronic domain in Amazon reviews (McAuley et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 15, "context": ", 2017), we first obtain word embeddings by applying word2vec4 on Yelp Challenge dataset5 and electronic domain in Amazon reviews (McAuley et al., 2015) for restaurant and laptop datasets, respectively.", "startOffset": 130, "endOffset": 152}, {"referenceID": 26, "context": "RNCRF+: We modify RNCRF (Wang et al., 2016), which is for aspect-opinion terms extraction, by defining finer-grained categories as labels.", "startOffset": 24, "endOffset": 43}, {"referenceID": 27, "context": "CMLA+: Similar to RNCRF+, we modify CMLA (Wang et al., 2017) by defining finergrained categories as labels.", "startOffset": 41, "endOffset": 60}], "year": 2017, "abstractText": "In aspect-based sentiment analysis, most existing methods either focus on aspect/opinion terms extraction or aspect terms categorization. However, each task by itself only provides partial information to end users. To generate more detailed and structured opinion analysis, we propose a finer-grained problem, which we call category-specific aspect and opinion terms extraction. This problem involves the identification of aspect and opinion terms within each sentence, as well as the categorization of the identified terms. To this end, we propose an end-to-end multitask attention model, where each task corresponds to aspect/opinion terms extraction for a specific category. Our model benefits from exploring the commonalities and relationships among different tasks to address the data sparsity issue. We demonstrate its state-of-the-art performance on three benchmark datasets.", "creator": "LaTeX with hyperref package"}}}