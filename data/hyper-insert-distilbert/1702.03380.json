{"id": "1702.03380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2017", "title": "Training Deep Neural Networks via Optimization Over Graphs", "abstract": "precisely in this work, we first propose to train studying a real deep neural deformation network by distributed signal optimization over a restricted graph. alternatively two specific nonlinear functions are considered : are the continuous rectified dynamic linear unit ( relu ) and a linear unit with both arbitrary lower dir and nonlinear upper bandwidth cutoffs ( dcutlu ). the aforementioned problem reformulation over finding a graph is realized usually by finding explicitly unsuccessfully representing locally relu or matrix dcutlu transforms using a set mixture of local slack optimization variables. we then apply the generic alternating direction displacement method of multipliers ( admm ) to firstly update automatically the weights parameter of the new network layerwise slope by solving subproblems rather of overcoming the reformulated problem. empirical results instead suggest that by proper parameter selection, the admm - based acceleration method thus converges considerably faster than pure gradient elastic descent method.", "histories": [["v1", "Sat, 11 Feb 2017 04:02:40 GMT  (889kb)", "https://arxiv.org/abs/1702.03380v1", "5 pages"], ["v2", "Sat, 17 Jun 2017 11:18:48 GMT  (973kb)", "http://arxiv.org/abs/1702.03380v2", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["guoqiang zhang", "w bastiaan kleijn"], "accepted": false, "id": "1702.03380"}, "pdf": {"name": "1702.03380.pdf", "metadata": {"source": "CRF", "title": "Training Deep Neural Networks via Optimization Over Graphs", "authors": ["Guoqiang Zhang", "Bastiaan Kleijn"], "emails": ["guoqiang.zhang@uts.edu.au", "aan.kleijn@ecs.vuw.ac.nz"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n03 38\n0v 2\n[ cs\n.L G\n] 1\n7 Ju\nn 20\n17 1\nIndex Terms\u2014Deep learning, DNN, optimization, ADMM.\nI. INTRODUCTION\nIn the last decade, research on deep learning has made remarkable progress both in theoretical understanding and in practical applications (see [1] for an overview). Deep learning interprets data at multiple levels of abstraction, realized in a computational model with multiple processing layers. Each layer is composed of a set of simple nonlinear processing units (referred to as neurons), which aims to transform the input into progressively more abstract representations [2], [3]. With the composition of multiple processing layers, the model is able to produce data representations that are required by various applications.\nIn the literature, different types of deep neural networks (DNNs) have been proposed and applied in different applications. For instance, feedforward neural networks have been successfully applied in speech recognition [4], [5]. Convolutional neural networks (CNNs) are popular in computer vision [6], [7]. Recurrent neural networks (RNNs) have proven to be effective for mapping sequential inputs and outputs [8], [9].\nThe common procedure for training a deep neural network is to iteratively adjust its parameters (referred to as weights) such that the network approximates the input-output relations with increasing accuracy, referred to as supervised learning.\nThe traditional supervised learning approach treats a neural network as a large complex model [1] rather than decomposing it as a combination of many small nonlinear models. The standard procedure, stochastic gradient descent (SGD), is to back-propagate gradients from the top layer down to the bottom layer on a mini-batch and then adjusts the weights accordingly. In recent years, various advanced methods have\nG. Zhang is with the center of Audio, Acoustics and Vibration (CAAV), School of Computing and Communications, University of Technology, Sydney, Australia. Email: guoqiang.zhang@uts.edu.au W. B. Kleijn is with is the school of Engineering and Computer Science, Victoria University of Wellington, New Zealand. Email: bastiaan.kleijn@ecs.vuw.ac.nz\nbeen proposed to use the gradient information smartly for either fast convergence or automatic parameter adjustment, such as Adam [10], AdaGrad [11] and RMSprop [12].\nIn recent years, a new supervised learning paradigm has been proposed that decomposes a neural network as a combination of many small nonlinear models. In [13], the authors firstly proposed to decouple the nested structure of DNNs by introducing a set of auxiliary variables and a set of equality constraints. However, computation of the gradient is still required in their work to tackle the nonlinear functions of the neurons. The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15]. However, [14] needs to perform a computation at each and every neuron to be able to characterize its nonlinear operation. The Bregman iteration is used in [14] to produce stable algorithmic convergence.\nIn this paper, we propose to train a deep neural network by reformulating the problem as an optimization over a factor graph G = (V , C) [16], [17]. Every node r \u2208 V carries a convex function of its node variable while every factor c \u2208 C carries a nonlinear equality constraint in terms of the node variables connected to the factor. Our graphic formulation is able to handle rectified linear units (ReLUs) (see [18], [19]) and linear units with both upper and lower cutoffs (DCutLUs) at the layer-level. In particular, the ReLUs or DCutLUs are represented in terms of a set of slack variables, which lead to the equality constraints in the factor graph.\nWe apply ADMM to solve the graph-based problem. Differently from [14] which has to perform computations at the neuron-level, our proposed method is able to perform computations at the layer-level like the SGD and Adam. Experimental results on the MNIST dataset demonstrate that the new training method is less sensitive to overfitting than the SGD and Adam methods. Further, the performance of the new method on the test data is better than the SGD and Adam, which may be due to the flexibility of ADMM."}, {"heading": "II. ON TRAINING A DEEP NEURAL NETWORK", "text": "Suppose we have a sequence of m training samples, represented by an input matrix D \u2208 Rm\u00d7nin and an output matrix O \u2208 Rm\u00d7nout , where the q\u2019th row-vectors ofD and O form an input-output pair. Given (D,O), we consider training a deep neural network with the weights {(Wi, bi)|i = 1, . . . , N} ofN layers, where for each i, Wi \u2208 R ni\u22121\u00d7ni is a weight matrix and bi \u2208 R 1\u00d7ni a bias vector. To match the network with the training samples, we let n0 = nin and nN = nout. The objective is to find the proper weights {(Wi, bi)} so that the\n2 network maps the input D to the output O as accurately as possible. Let us now define the operation of the individual layers. We use Vi to denote the output of layer i, i \u2264 N . We let e be a (column) vector of ones. Vi, i \u2264 N \u2212 1, is obtained by performing (element-wise) nonlinear operation on the matrix product Vi\u22121Wi + ebi, denoted as Vi = hi(Vi\u22121Wi + ebi).\nThe popular forms for the nonlinear function hi are sigmoid, tanh and ReLU [1]. It is found in [19] that ReLU leads to fast convergence using SGD as compared to sigmoid and tanh. We consider ReLU and DCutLU in this paper. Formally, we define hi in the form of DCutLU as\nVi = min(max(Vi\u22121Wi + ebi, l), u) i \u2264 N \u2212 1, (1)\nwhere the max and min operators are element-wise, and l and u are the lower and upper threshold, respectively. ReLU is a special case of DCutLU by letting (l, u) = (0,\u221e). The procedure of training the above neural network can be formulated as\nmin {Vi,Wi,bi}\n[\nfN (VN ;O) +\nN \u2211\ni=1\ngi(Wi, bi)\n]\n, (2)\nwhere fN measures the difference between the output VN and the ground truth O, gi is a penalty function on (Wi, bi), and {Vi,Wi, bi} satisfies (1) and\nVN = VN\u22121WN + ebN . (3)"}, {"heading": "III. PROBLEM REFORMULATION ONTO A GRAPH", "text": "In this section, we reformulate (2)-(3) as an optimization over a factor graph. We first represent the nonlinear function (1) by introducing a set of slack variables. Specifically, (1) can be rewritten as\nXi = Vi\u22121Wi + ebi (4)\nXi + Yi = max(Vi\u22121Wi + ebi, l) (5)\nXi + Yi + Zi = Vi = min(Xi + Yi, u), (6)\nwhere for each i \u2208 {1, . . . , N \u2212 1}, we introduced three slack matrices Xi, Yi and Zi to characterize the effect of the upper and lower cutoffs of the function at u and l. Next, we argue that the min and max operators in (5)-(6) can be expressed in terms of constraints on (Xi, Yi, Zi). To do so, we introduce two index sets for each layer i:\n\u2126li = {(q, j)|xi,qj < l} (7) \u2126ui = {(q, j)|xi,qj > u}, (8)\nwhere xi,qj is the (q, j) element of Xi. At the moment, one can think of \u2126li and \u2126 u i as two sets that are preset already, imposing constraints on Xi. We will explain later how to update (\u2126li,\u2126 u i ) iteratively. Given a set \u2126, we let P\u2126(X) denote the subset of the elements of X specified by the indices of \u2126. The max operator in (5) can be characterized as\nP\u2126l i (Xi) < l (9a)\nP\u2126li(Xi) + P\u2126li(Yi) = l (9b)\nP\u2126\u0304li(Xi) \u2265 l (9c)\nP\u2126\u0304li(Yi) = 0, (9d)\nFig. 1. Problem reformulation over a factor graph G = (V, C). \u25e6 is a node in V and is a factor in C. \u22b2 represents constant inputs to the graph, where X0 = V0 = D is the data input.\nwhere \u2126\u0304 denotes the complement of \u2126. By inspection of (5) and (6), we conclude that Yi and Zi are decoupled given Xi. The min operator in (6) can thus be characterized as\nP\u2126u i (Xi) > u (10a)\nP\u2126u i (Xi) + P\u2126u i (Zi) = u (10b)\nP\u2126\u0304ui (Xi) \u2264 u (10c) P\u2126\u0304ui (Zi) = 0. (10d)\nTo briefly summarize, we use the constraints (9) and (10) to replace the min and max operations in (5)-(6). Based on the above analysis, the training problem (2)-(3) can be reformulated as\nmin {Wi,bi Xi,Yi,Zi} fN (XN;O)+\nN \u2211\ni=1\ngi(Wi,bi)+\nN\u22121 \u2211\ni=1\nfi(Xi,Yi,Zi|\u2126 l i,\u2126 u i )(11)\ns. t. Xi=(Xi\u22121+Yi\u22121+Zi\u22121)Wi+ebi \u2200i=1, . . . ,N, (12)\nwhere (X0, Y0,Z0) = (V0,0,0) and each fi(Xi, Yi,Zi|\u2126 l i,\u2126 u i ) can be taken as a summation of indicator functions, each defined by one constraint in (9)-(10), given by\nfi(Xi,Yi, Zi|\u2126 l i,\u2126 u i )=\n[\n1P \u2126l i (Xi)<l+1P\u2126\u0304l i (Yi)=0+1P\u2126\u0304l i (Xi)\u2265l\n+ 1P \u2126l i (Xi)+P\u2126l i (Yi)=l + 1P\u2126u i (Xi)>u + 1P\u2126\u0304u i (Zi)=0\n+ 1P\u2126\u0304u i (Xi)\u2264u + 1P\u2126u i (Xi)+P\u2126u i (Zi)=u\n]\n, (13)\nwhere the indicator function 1(\u00b7) equals to 0 when its constraint is satisfied and equals to +\u221e otherwise. Eqn. (11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig. 1 for demonstration).\nRemark 1. If the ReLU is chosen for layer i of the network, one can simply ignore Zi and \u2126 u i and let l = 0 in (11)-(13)."}, {"heading": "IV. DISTRIBUTED OPTIMIZATION OVER A GRAPH", "text": "We note that (11)-(13) is a nonconvex optimization because of the nonlinear equality constraints (12). We solve (11)- (13) in an iterative fashion using ADMM by solving convex subproblems. It is worth noting that ADMM has already been successfully applied for solving nonnegative matrix factorization (NMF) [21], which is nonconvex."}, {"heading": "A. Augmented Lagrangian function", "text": "To apply ADMM, we introduce a Lagrange multiplier \u039bi for the ith equality constraint in (12). We build an augmented\n3\nLagrangian function as\nL{\u03c1i}({Xi, Yi, Zi, bi,Wi,\u039bi}, XN |{\u2126 l i,\u2126 u i })\n= fN (XN;O)+ N \u2211\ni=1\ngi(Wi,bi)+ N\u22121 \u2211\ni=1\nfi(Xi,Yi,Zi|\u2126 l i,\u2126 u i )\n+ N \u2211\ni=1\npi,\u03c1i((Xi\u22121,Yi\u22121,Zi\u22121),Xi, (Wi,bi),\u039bi), (14)\nwhere for each i = 1, . . . , N , pi,\u03c1i(\u00b7 \u00b7 \u00b7 ) is defined as\npi,\u03c1i(\u00b7 \u00b7 \u00b7 ) = [\u03c1i\n2 \u2016Xi\u2212(Xi\u22121+Yi\u22121+Zi\u22121)Wi\u2212ebi\u2016\n2\n+\u3008\u039bi, Xi\u2212(Xi\u22121+Yi\u22121+Zi\u22121)Wi\u2212ebi\u3009 ] , (15)\nwhere \u03c1i > 0, (X0, Y0, Z0) = (V0, 0, 0), and \u3008\u00b7, \u00b7\u3009 denotes dot product. We note that differently from the single learning rate of SGD, each layer i possesses a positive parameter \u03c1i, which can be treated as a layer-oriented learning rate.\nOur objective now is to reach a saddle point of the Lagrangian function L{\u03c1i} by minimizing over {Xi, Yi, Zi, bi,Wi, } \u222a XN and maximizing over {\u039bi}. A saddle point would satisfy the equality constraints (12)."}, {"heading": "B. Blockwise parameter updating using ADMM", "text": "We now consider optimizing the Lagrangian function L{\u03c1i}. We follow a similar updating procedure as the SGD and Adam methods [10]. That is, at each iteration, we initialize all the variables and index sets of L{\u03c1i} by feeding D to the network through the forward computation. We then update all the variables of L{\u03c1i} blockwise through backward computation. Differently from SGD which computes gradient directly, the variables of L{\u03c1i} are updated by solving smallsize optimization problems.\nSuppose we finished updating variables of layer i + 1 and would like to update (Xi, Yi, Zi), (Wi, bi) and \u039bi of layer i. We first simplify L{\u03c1i} by removing irrelevant components,\nLi((Xi, Yi, Zi), (Wi, bi),\u039bi|\u2126 u i ,\u2126 l i)\n= pi+1,\u03c1i+1((Xi,Yi,Zi),X\u0302 new i+1 , (W\u0302i+1, b\u0302i+1),\u039b\u0302 new i+1 )\n+ pi,\u03c1i((X\u0302i\u22121,Y\u0302i\u22121,Z\u0302i\u22121),Xi, (Wi,bi),\u039bi) + gi(Wi, bi)\nwhere X\u0302newi+1 and \u039b\u0302 new i+1 are the new estimate obtained from the computation at layer i + 1. By following the ADMM updating procedure, we first compute (X\u0302newi , Y\u0302 new i , Z\u0302 new i ) by optimizing Li with (W\u0302i, b\u0302i) and \u039b\u0302i fixed. We then compute \u039b\u0302newi using X\u0302 new i through dual ascent. Finally, we compute (W\u0302newi ,b\u0302 new i ) by optimizing Li with (X\u0302 new i , Y\u0302 new i , Z\u0302 new i ) and \u039b\u0302newi fixed. See Table I for the updating procedure. For the top layer i = N , the function LN takes the form:\nLN(XN , (WN , bN),\u039bN ) = fN (XN ;O) + gN (WN , bN )\n+ pN,\u03c1N ((X\u0302N\u22121,Y\u0302N\u22121,Z\u0302N\u22121),XN, (WN,bN),\u039bN),\nwhere there is no YN and ZN . For this layer, only X\u0302 new N is computed by optimizing LN in Table I.\nRemark 2. In (16), (W\u0302i+1, b\u0302i+1) is used instead of (W\u0302newi+1 , b\u0302 new i+1 ), which is found to be much more stable through experiments."}, {"heading": "C. Handling of the indicator function", "text": "The function fi(\u00b7) in (16) is composed of a set of indicator functions, which makes it difficult to compute (X\u0302newi , Y\u0302 new i , Z\u0302 new i ) in Table I. To facilitate the computation, we introduce the auxiliary variables (Xci , Y c i , Z c i ) to replace (Xi, Yi, Zi) in fi(Xi, Yi, Zi|\u2126 l i,\u2126 u i ) with the constraints Xci = X c i , Y c i = Yi and Z c i = Zi. We then apply ADMM again to handle the three equality constraints. To do so, we build a new augmented Lagrangian as\nLi,\u03b2i((Xi, Yi, Zi), (\u0393 x i ,\u0393 y i ,\u0393 z i ), (X c i , Y c i , Z c i ))\n= pi+1,\u03c1i+1((Xi,Yi,Zi),X\u0302 new i+1 , (W\u0302i+1, b\u0302i+1),\u039b\u0302 new i+1 )\n+ pi,\u03c1i((X\u0302i\u22121,Y\u0302i\u22121,Z\u0302i\u22121),Xi, (Wi,bi),\u039bi) + gi(Wi, bi)\n+fi(X c i ,Y c i ,Z c i |\u2126 l i,\u2126 u i )+\n\u03b2i\n2 \u2016Xi\u2212X\nc i \u2016 2 +\u3008\u0393xi , Xi\u2212X c i \u3009\n+ \u03b2i\n2 \u2016Yi\u2212Y\nc i \u2016 2+\u3008\u0393yi , Yi\u2212Y c i \u3009+\n\u03b2i 2 \u2016Zi\u2212Z c i \u2016 2+\u3008\u0393zi , Zi\u2212Z c i \u3009,\nwhere {\u0393xi ,\u0393 y i ,\u0393 z i } are the Lagrange multipliers, and \u03b2i > 0 which has a similar role as \u03c1i in L{\u03c1i}. We update the three sets of variables (Xi, Yi, Zi), (\u0393 x i ,\u0393 y i ,\u0393 z i ) and (X c i , Y c i , Z c i ) one after another (see Table II). To reduce the computational time, we only update the above variables once instead of multiple iterations. To briefly summarize, at each iteration, the proposed algorithm performs both forward and backward computations. The forward computation initializes all variables and index sets\n4\nwhile the backward computation updates all the variables and the network weights. The algorithm has a set of learning rates {\u03c1i} \u222a {\u03b2i}, which provides great flexibility to fine-tune the algorithm to have fast convergence (See the first experiment of Section V about the parameter setup)."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In the simulation, we considered the handwritten-digit recognition problem by using MNIST with the standard division of the training (60000 samples) and test (10000 samples) datasets. In doing so, we built a DNN of three layers (N = 3), where the first and second hidden layer consists of 500 and 600 neurons, respectively. The output function was chosen as the summation of the individual cross-entropy functions ([22]). The function gi(Wi, bi) was chosen as 0.1 2 \u2016(Wi, bi)\u2016 2. The mini-batch size was set as 3000. The entire training dataset thus consisted of 20 minibatches.\nWe note that the cross-entropy function makes it difficult to compute X\u0302newN analytically in Table I. When updating the above variable at each iteration, we approximate each crossentropy term by a quadratic function around the most recent estimate, where the quadratic coefficient is set to 0.05 and the linear coefficient is set to the gradient.\nWe evaluated the proposed method (referred to ADMM) with two proof-of-concept experiments. In the first experiment, we tested ADMM, SGD and Adam [10] using only the ReLUs. In the second experiment, we studied how the learning rates {\u03c1i} \u222a {\u03b2i} affect the convergence speed of ADMM for both ReLUs and DCutLUs.\n1) Comparison with the state-of-the-art: In addition to ADMM, we also evaluated SGD and Adam [10], where Adam represents the state-of-the-art training method. The goal is to study the convergence properties of the proposed algorithm. The learning rate of SGD was chosen as 0.3 (producing stable and fast convergence among {0.1, 0.2, 0.3, 0.4}). Adam was implemented by following [10] directly. When running SGD and Adam, the gradient of ReLU at zero is set to 0. Finally the learning rates of ADMMwere set as \u03c13 = 0.05, \u03c12 = \u03b22 = 0.1 and \u03c11 = \u03b21 = 0.2. The basic principle is to set the learning rates \u03c1i and \u03b2i of layer i slightly larger than \u03c1i+1 and \u03b2i+1 of layer i+ 1. The experimental results are displayed in Fig. 2 (a). It is seen that the performance gap of ADMM between the test data and training data is relatively stable compared to that of Adam and SGD. Furthermore, ADMM performs better than Adam and SGD on the test data, where the recognition accuracy for the test data at the last iteration is: 98.41(ADMM), 98.23(Adam) and 97.98(SGD). The better performance of ADMM might be due to the introduction of layer-oriented learning rates {\u03c1i, \u03b2i}. The computational time of the three methods was measured on an Apple MacBook Pro and is summarized in Table III. In general, ADMM is somewhat more expensive than SGD and Adam because it consumes more memory due to the auxiliary variables and involves solving a set of small-size optimization problems per min-batch.\n2) Effect of different learning rates on convergence speed: In this experiment, we studied how the learning rates {\u03c1i, \u03b2i}\nTABLE III AVERAGE EXECUTION TIMES (PER MINI-BATCH) AND THEIR STANDARD DEVIATIONS FOR THE FOUR METHODS.\nSGD (ReLU) Adam (ReLU) ADMM (ReLU) ADMM (DCutLU)\nave. (second) 0.2257 0.2398 0.9373 1.446\nstd 0.0444 0.0416 0.0851 0.0949\n0 50 100\niterates over entire dataset\n10 -3\n10 -2\n10 -1\nc ro\ns s -e\nn tr\no p\ny\nSGD-train SGD-test Adam-train Adam-test ADMM-train ADMM-test\n10 -1\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\nit e\nra te\ns o\nv e\nr e\nn ti re\nd a\nta s\ne t\nADMM-ReLU ADMM-DCutLU\nlearning rate ( for all i)\n(a) (b)\nFig. 2. Performance comparison. Subplot (a) displays the performance of SGD, Adam and ADMM using only ReLUs. Subplot (b) shows the number of iterations over entire training dataset needed to reach a threshold (0.05) of average cross-entropy for each learning rate of ADMM.\naffect the convergence speed of ADMM for both ReLUs and DCutLUs (where (l, u) = (0, 1)). To simplifying the evaluation, we let all \u03c1i and \u03b2i to be the same per experiment. For each learning rate, we counted the number of iterations over entire training dataset until the average cross-entropy reaches 0.05.\nThe convergence results are displayed in Fig. 2 (b). It is seen that the learning rate indeed affects the convergence speed. Further, it is observed that ReLU needs significantly fewer iterations than DCutLU. Table III also shows that the computational time of ReLU is lower than that of DCutLU. This suggests that ReLU is a better choice in practice.\nRemark 3. At the moment, the convergence of the proposed method is only demonstrated by experiments. We leave the theoretical convergence analysis for future investigation."}, {"heading": "VI. CONCLUSIONS", "text": "We have proposed a new algorithm for training a DNN by performing optimization over a factor graph. The key step is to explicitly represent the ReLUs or DCutLUs by a set of slack variables, which enables layer-level computation rather than neuron-level computation as in [14]. Experimental results indicate that the new algorithm is less sensitive to overfitting than two references. One future research direction is to adjust the learning rates {\u03c1i} and {\u03b2i} of the new algorithm automatically, which likely will lead to good convergence speed for various learning problems.\n5"}], "references": [{"title": "Deep Learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436\u2013444, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1311.2901v3, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding Neural Networks Through Deep Visualization", "author": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "venue": "arXiv preprint arXiv:1506.06579v1, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic Modeling Using Deep Belief Networks", "author": ["A.-R. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Trans. Audio Speech Lang. Process, pp. 14\u201322, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-Dependent Pre-Trained Deep Neural Networks for Large Vocabulary Speech Recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio Speech Lang. Process, vol. 20, pp. 33\u201342, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Nips, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep Face: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proc. Conference on Computer Vision and Pattern Recognition, 2014, pp. 1701\u20131708.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "Q. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems 27, 2014, pp. 3104\u20133112.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for Stochastic Optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "arXiv preprint arXiv:1412.6980v9, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed Optimization of Deeply Nested Systems", "author": ["M. Carreira-Perpinan", "W. Wang"], "venue": "arXiv:1212.5921 [cs.LG], 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Training Nueral Networks Without Gradients: A Scalable ADMM Approach", "author": ["G. Taylor", "R. Burmeister", "Z. Xu", "B. Singh", "A. Patel", "T. Goldstein"], "venue": "Proc. IEEE Int. Conf. Machine Learning, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to Dual Decomposition for Inference", "author": ["D. Sontag", "A. Globerson", "T. Jaakkola"], "venue": "Optimization for Machine Learning. MIT Press, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "An Alternating Direction Method for Dual MAP LP Relaxation", "author": ["O. Meshi", "A. Globerson"], "venue": "ECML, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Con- ference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 315\u2013323.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Foundations and Trends in Machine Learning, vol. 1(1-2), pp. 1\u2013305, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative Matrix Factorization Using ADMM: Algorithm and Convergence Analysis", "author": ["D. Hajinezhad", "T.-H. Chang", "X. Wang", "Q. Shi", "M. Hong"], "venue": "Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 4742\u20134746.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In the last decade, research on deep learning has made remarkable progress both in theoretical understanding and in practical applications (see [1] for an overview).", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "Each layer is composed of a set of simple nonlinear processing units (referred to as neurons), which aims to transform the input into progressively more abstract representations [2], [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "Each layer is composed of a set of simple nonlinear processing units (referred to as neurons), which aims to transform the input into progressively more abstract representations [2], [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "For instance, feedforward neural networks have been successfully applied in speech recognition [4], [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "For instance, feedforward neural networks have been successfully applied in speech recognition [4], [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "Convolutional neural networks (CNNs) are popular in computer vision [6], [7].", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "Convolutional neural networks (CNNs) are popular in computer vision [6], [7].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "Recurrent neural networks (RNNs) have proven to be effective for mapping sequential inputs and outputs [8], [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs) have proven to be effective for mapping sequential inputs and outputs [8], [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "The traditional supervised learning approach treats a neural network as a large complex model [1] rather than decomposing it as a combination of many small nonlinear models.", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "nz been proposed to use the gradient information smartly for either fast convergence or automatic parameter adjustment, such as Adam [10], AdaGrad [11] and RMSprop [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "nz been proposed to use the gradient information smartly for either fast convergence or automatic parameter adjustment, such as Adam [10], AdaGrad [11] and RMSprop [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "In [13], the authors firstly proposed to decouple the nested structure of DNNs by introducing a set of auxiliary variables and a set of equality constraints.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15].", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "However, [14] needs to perform a computation at each and every neuron to be able to characterize its nonlinear operation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "The Bregman iteration is used in [14] to produce stable algorithmic convergence.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "In this paper, we propose to train a deep neural network by reformulating the problem as an optimization over a factor graph G = (V , C) [16], [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "In this paper, we propose to train a deep neural network by reformulating the problem as an optimization over a factor graph G = (V , C) [16], [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Our graphic formulation is able to handle rectified linear units (ReLUs) (see [18], [19]) and linear units with both upper and lower cutoffs (DCutLUs) at the layer-level.", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "Our graphic formulation is able to handle rectified linear units (ReLUs) (see [18], [19]) and linear units with both upper and lower cutoffs (DCutLUs) at the layer-level.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Differently from [14] which has to perform computations at the neuron-level, our proposed method is able to perform computations at the layer-level like the SGD and Adam.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The popular forms for the nonlinear function hi are sigmoid, tanh and ReLU [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 17, "context": "It is found in [19] that ReLU leads to fast convergence using SGD as compared to sigmoid and tanh.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "(11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "(11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "(11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "It is worth noting that ADMM has already been successfully applied for solving nonnegative matrix factorization (NMF) [21], which is nonconvex.", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "We follow a similar updating procedure as the SGD and Adam methods [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "In the first experiment, we tested ADMM, SGD and Adam [10] using only the ReLUs.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "1) Comparison with the state-of-the-art: In addition to ADMM, we also evaluated SGD and Adam [10], where Adam represents the state-of-the-art training method.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "Adam was implemented by following [10] directly.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "The key step is to explicitly represent the ReLUs or DCutLUs by a set of slack variables, which enables layer-level computation rather than neuron-level computation as in [14].", "startOffset": 171, "endOffset": 175}], "year": 2017, "abstractText": "In this work, we propose to train a deep neural network by distributed optimization over a graph. Two nonlinear functions are considered: the rectified linear unit (ReLU) and a linear unit with both lower and upper cutoffs (DCutLU). The problem reformulation over a graph is realized by explicitly representing ReLU or DCutLU using a set of slack variables. We then apply the alternating direction method of multipliers (ADMM) to update the weights of the network layerwise by solving subproblems of the reformulated problem. Empirical results suggest that the ADMM-based method is less sensitive to overfitting than the stochastic gradient descent (SGD) and Adam methods.", "creator": "LaTeX with hyperref package"}}}