{"id": "1702.04638", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "A Spacetime Approach to Generalized Cognitive Reasoning in Multi-scale Learning", "abstract": "in modern based machine generated learning, analytic pattern recognition replaces realtime semantic reasoning. eventually the mapping proceeding from input to output is clearly learned intentionally with fixed semantics by training query outcomes or deliberately. this is an expensive generic and static prediction approach which vastly depends heavily on the availability benefit of selecting a locally very simple particular synthetic kind of manually prior raining data resources to make complicated inferences better in a single primary step. conventional semantic communication network solving approaches, on both the other hand, rely base multi - step reasoning on modal logics and specifically handcrafted ontologies, logic which are { \\ em ad hoc }, politically expensive to best construct, contradictory and fragile approaches to inconsistency. both approaches may be successfully enhanced significantly by defining a hybrid optimization approach, which completely immediately separates reasoning from pattern recognition. secondly in utilizing this report, conversely a refined quasi - linguistic reasoning approach to knowledge representation is discussed, loosely motivated further by constraint spacetime tree structure. physically tokenized patterns from diverse sources are integrated to build overall a lightly constrained and normally approximately locally scale - free network. this image is then be visually parsed accurately with very easily simple recursive algorithms utilized to precisely generate ` brainstorming'sets of reasoned functional knowledge.", "histories": [["v1", "Sun, 12 Feb 2017 14:58:45 GMT  (114kb,D)", "http://arxiv.org/abs/1702.04638v1", null], ["v2", "Tue, 1 Aug 2017 11:32:42 GMT  (3786kb,D)", "http://arxiv.org/abs/1702.04638v2", "Typos corrected and new examples added to appendix"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["mark burgess"], "accepted": false, "id": "1702.04638"}, "pdf": {"name": "1702.04638.pdf", "metadata": {"source": "CRF", "title": "A Spacetime Approach to Generalized Cognitive Reasoning in Multi-scale Learning", "authors": ["Mark Burgess"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nReasoning has long been associated with formal logic in computer science, but it may be argued that reasoning is only a subset of a wider class of narratives, which may be told by joining together assertions. Logic\u2019s goal is to maximize the certainty of a narrative derived from prior concepts, by transforming them according to a constrained and largely deterministic set of rules, in which each step selects a unique possibility. However, it is only of value in a very limited set of circumstances. In other cases, where unique certainty is not available or practical, a more expansive kind of \u2018brainstorming\u2019 is needed for problem solving: one that allows multiple possibilities to remain open for selection at a later time.\nBrainstorming is the first stage of a process of reasoning by \u2018whittling\u2019. The idea is to create a large hypothesis set, for subsequent reduction into something more focused. The whittling is often emergent and iterative. Reasoning, in this interpretation, is a cognitive process, which allows an observer to integrate experiences, perhaps not evidentially linked, or even provably true, in order to form a set of speculative hypotheses. By process of contextual elimination, this converges, iteratively, to a much smaller set, or a final answer, using a variety of criteria from freshness to relevance and importance. In this dynamical approach,\ncriteria like semantic and dynamic stability of the stories are now more important than logical idealizations like \u2018truth\u2019 or ad hoc determinations of \u2018correctness\u2019 [1].\nNarration, or storytelling, is an underestimated aspect of intelligent behaviour [2]\u2013[4]. It is how learning individuals explain things in terms of prior experience, and make sense of the world. To form a story, we have to know the difference between correlation and causation: correlations do not have direction, i.e. no arrow of progression, which may be used to accumulate narrative storyline. Hence, correlation can only offer short-range explanations, with uncertainty that grows with the number of claims for similarity. Combining directed associations into a reasoned argument is a different problem altogether, one that requires the propagation of semantics from step to step.\nHow we combine concepts into stories for small sets is one thing; applying expansive reasoning to systems with vast numbers of sensors, sporting very different characteristics, is a whole new challenge. Imagine a scenario like the monitoring of a massive array of smart connected systems, e.g. an Internet of Things, smart buildings and cities, etc. Data of very different kinds are generated at many different scales, and from a vast number of different sources. How could we begin to interpret phenomena across different scales? What concepts do we need? The challenge of making sense of such data, at scale, urgently calls for a broader view of artificial reasoning, based on improved semantic labelling of collected data. This is how we might scale the meaningful interpretation of data. But it is not simply a case of collecting more and more data. The monitoring of sensory data would be incomplete without the ability to reason about observations. Reasoning requires the definition of a set of concepts, tied to operational goals.\nThis work summarizes a model of invariant storytelling, based on spacetime inference [5]\u2013[7]. The approach reduces the computational complexity for story inference by orders of magnitude. It describes how semantic associations may be collected from arbitrary sensors and inputs, and how a knowledge representation can learn context-dependent interpretations of\nar X\niv :1\n70 2.\n04 63\n8v 1\n[ cs\n.A I]\n1 2\nFe b\n20 17\nthose inputs, in order to later generate explanatory narrative automatically. It summarizes the practical and implementational aspects of the promise theoretic notion of \u2018semantic spacetime\u2019, and implements a number of experiments based on the model.\nPromise theory [8] motivates a graph theoretical approach to semantic scaling [6], which is based on the observation that cognitive relationships ultimately derive from elementary spacetime relationships. A set of recursive structures leads to a partially deterministic set of connected paths, each of which represents reasoned \u2018stories\u2019, with explanatory content. In this work, I show how to apply these structures, tentatively, to simple cases."}, {"heading": "II. THE SEMANTIC SPACETIME (PROMISE) MODEL", "text": "Promise theory frames elementary questions about intent: how it scales by cooperative interaction into extensive spatial networks, and how its properties may express spacetime-like variations to mirror a representation of environment. Based on the considerations in [7], we expect a number of information processing stages in a cognitive learning system (figure 1):\n1) A sensory apparatus for inputting exterior information. 2) A stage that replaces sensory patterns with semantic tokens, i.e. naming. 3) A stage that associates semantics i.e. meaning to the named tokens, by associating them as clusters called concepts, associated in predictable and qualifying ways. 4) An introspective story-generating stage, which is able to \u2018think\u2019 about or activate interior concepts, and feed the resulting stream of consciousness back into the post-tokenization stages (3 onwards), along side sensory inputs.\nAlthough we are not used to thinking of systems (physical or software) explained as narratives, any organized arrangement may be understood in terms of stories, so our goal is completely general. Most discussions of socalled Artificial Intelligence (AI) discuss stages 1 and 2, this work deals mainly with stages 3 and onwards, which I believe are orthogonal. These latter stages have much in common with linguistics [7]; indeed, once one can map invariant patterns into a finite alphabet of tokens, every reasoning problem maps to a linguistic problem.\nA simple prototype system, to explore the underlying principles, has been reconstructed from the earlier cognitive approaches applied to pervasive computing management in CFEngine [9], [10], and further modernized and extended in the Cellibrium project [11]."}, {"heading": "III. SEPARATION OF LEARNING SCALES", "text": "Data from only a single sensory episode, localized in time or space, are of limited value, and do not typically lead to claims of deep knowledge. It is by revisiting experiences, i.e. by the iterative process of observing\nand learning that we build trust in a stable \u2018invariant\u2019 representation of knowledge (see figure 2).\nLearning happens over a hierarchy of timescales [7]. Fundamental concepts are evolved by stabilizing \u2018genetic\u2019 adaptations through long term selection in a group; then there are concepts that build on this stability and use them to frame newly learned concepts, which congeal over generations, and are adapted and passed on socially (we may call this domain knowledge). This includes specialized sensors that tokenize, or dimensionally reduce, information intensive data into compressed conceptual representations [7]. Finally, there are concepts, which build on the foregoing, that are formed over shorter timescales by the observation and introspection of a single observer (see later figure 16). The latter is what we normally think of as learning; however, it is important to remember that it builds on a stable set of preconditions that have evolved\nprior to the current learning episode. We would expect consistent knowledge to be represented by eigenstates of a memory network.\nIn approximate order of aggregation, learning may be viewed at these scales:\n\u2022 Slow (evolutionary) training (unsupervised adaptive learning): Random variations are whittled away into a set of behaviours and memories encoded in a long-term memory, and providing the seeds and the framing into which newer ideas are be formed through realtime recombination. The hypothesis here is that such ideas are naturally associated with the the most invariant aspects of space and time, leading to four basic kinds of conceptual association, as well as the basic ability to compress extensive spacetime datasets into simple tokenized concepts [7]. In a software system, this is represented by hardcoded semantics, and specialized sensors, designed to discriminate pre-understood concepts, by virtue of their adaptation (e.g. sensors for hot/cold, light/dark, movement/no movement, face/no face). \u2022 Fast (direct) cognitive assessment or context (unsupervised learning and recall): Context changes quickly, and some concepts and associations are formed at this timescale. Each independent observer inherits the adaptations learned by previous generations and can build on this basis, assembling context from sensory experiences, and assessing as emotional state by pattern recognition. The sensory channel will eventually be supplemented by an introspective channel (see last item). The effect of being simultaneously in mind, by observation or thought, leads to aggregate clusters, or composite concepts, by co-activation, which can be then named as new concepts. In software, this represents monitoring and classification of data. It might be represented by facial or handwriting recognition algorithms for training, etc. \u2022 Slow (indirect) training (supervised linguistic learning): Given a lexicon of tokens (i.e. a language, however primitive), concepts inherited in a compressed form may be passed on from one individual to another, e.g. by word of mouth, or as text in a book. There is no longer need for direct experience. A set of concepts may thus be remembered in a knowledge bank, such as generational or societal memory, and be handed down as domain expertise. This form of knowledge acts as a second level of boundary conditions for framing and seeding new concepts. The fast cognitive knowledge is useless without having these seeds and constraints provide basic conceptual anchors. In life, the analogy would be the existence of domain knowledge that frames our current awareness of a situation.\n\u2022 Fast (recurrent) introspection (emergent adaptation): Once a knowledge representation contains a sufficiency of concepts and memory representations to be able to form stories without new outside stimulus, a system can think about them and associate freely, leading to further new concepts. This would be essentially talking to oneself, as it would naturally share the same linguistic representation as the exterior sharing.\nAt each stage, the key question is how concepts are addressed and recalled, based on a mixture exterior and interior stimulus. This is what we mean by the naming of concepts [5]1. This is discussed at length in [7].\nFinally, no memory system would be complete without processes of annealing and garbage collection, to even out and clear away clusters and memories that do not become so important that they dominate over older ones."}, {"heading": "IV. ENCODING AND RETRIEVING CONCEPTUAL ASSOCIATIONS", "text": "Data are singular events, with no a priori inferable interpretation. The meaning of a single data point can only be promised by its source. A knowledge system has to be able to integrate the diversity of such experiences into a stable aggregation, channelling perceived conflicts into contextualized differences, thus preserving them without neutralizing them. In a sense, diversity is a prerequisite for knowledge, and integration leads to a scaled form of hashing.\nA. Direct primary cognition\nCognition leads to an association of tokenized concepts through a process illustrated schematically in figure 3. This prism separates data into a spectrum of four basic associative types, known as the irreducible types (see table I). It mirrors ideas in linguistics on the classification of nouns. [12]. The semantics of cognition are very different from the semantics of experimental observation in science. Ensemble measurement is about eliminating \u2018self\u2019 from experience. In cognition, observer subjectivity is a key aspect that cannot be disregarded2.\nB. Indirect secondary cognition (empathy)\nIn machine learning, one tries to fit bulk recorded experience, in all its detail, directly into a singular neural network, simulating direct first-hand experience by\n1This suggests that language would co-evolve with a conceptual representation of knowledge, and that utterances, i.e. statements and stories would grow in sophistication along side the knowledge evolved in a society of such representations or brains.\n2A lot of attention has been given over to the algebraic aspects of measurement in physics, e.g. quantum mechanics, but surprisingly little attention has been given to the semantics of data, and cognitive perception, except in the case of experimental error [13]\u2013[15]. The extent to which measurement disturbs the system during the act of measurement affects what can be promised about a measurement. Measurables may have the property of \u2018compatibility\u2019 [16], meaning that measurement of one does not influence the measurement of the other.\n(NOT)\nbrute force reconstruction, then adapting its hardwired nature to match presumed semantics. However, humans are also able to learn from books and stories passed on verbally: we can learn from simplified, tokenized representations of knowledge, in which we do not have to experience every sensation and emotion as the source did. Even when learning to read books, neural network techniques would need to read every book into a network in order to train it, rather than using a short summary of what each book was about. As humans, we are able to scale knowledge acquisition through summarization, on trust, without verifying directly from a source. In order to scale machine learning, machines must also be able to do this.\nWhen training a network by second hand teaching alone (e.g. from books rather than from practice or experience), we lose realistic sensory or emotional context, which forms part of our access key to recover the memories. Thus a model cannot simulate reality from a memory standpoint. Some aspects of context can be simulated or described second-hand, but a realistic emotional state will not normally match actual first-hand experience. The ability of a second-hand agent to empathize with the concepts and sensations of a primary agent will therefore play a major role in the ability of pass on knowledge3. On the other hand, the compression of such a complex lookup key, into a more compact address, allows to reorder concepts into new categories, and to generalize them, overlooking irrelevances based on context.\nCognition therefore ultimately leads to associations between tokenized (or dimensionally reduced) concept representations, through a whittling process, described in figure 3. This \u2018prism\u2019 separates data into a spectrum of associative types, which theory predicts to represent different spacetime characteristics, called the irreducible types [7] (see figure I). The conceptual\n3This introspective simulation of emotional context may well be the evolutionary purpose of empathy, given the enormous value of saving in communication and computational processing.\nprism bears some similarity with the staged structures of neural networks; however, neural networks are relatively expensive compression algorithms, that are not well suited to the latter \u2018expansive\u2019 stages of reasoning, where the bulk of input data would make reasoning too slow.\nC. Tuple form of semantic graph\nLet us assume that the result of a learning process is a set of tuples of the form:\n(C1, \u03c4, A+, C2, A\u2212, \u03c7 \u2217) (1)\nwhere C1, C2 are dimensionally reduced concept tokens, A+, A\u2212 are forward and backward associations of type \u03c4 \u2208 {1, 2, 3, 4} and \u2212\u03c4 respectively, and \u03c7\u2217 context labels. These tuples represent links or edges in a graph \u0393(C,A) whose nodes C are conceptual tokens, and whose edges A are associations. This summarizes an associative relationship implied by an observation. We can also add a weight or relative strength for each tuple and a timestamp for when it was last updated (which in a full knowledge equilibrium, with garbage collection, would be equivalent). All of these elements are strings. For example, highlighting the main features of the tuple, we may represent a simple qualitative description as a tuple:\ndoctor concept 4 ST type\npromises forward association identity credentials next-concept\nis promised by reverse association\npatient doctor registration context\nIn other words, we know that a doctor promises to have identity credentials, and we are thinking about patient-doctor registration. This association is of type 4, on the prism. We can apply the same approach to turn numerical data into qualitative linguistic representations in this format, so\nX = 47 (2)\nmight translate into:\nX, concept\n4 ST type\nhas value, forward association\nidentity credentials next-concept\nis the value of by, reverse association\nvariable assignment context\nIn other words, the token X has a value of 37, and we are thinking about variable assignment.\nWhat is noteworthy about the tuples is the absence of data types for the tokens, translating in an untyped graph. This is not a conventional data representation; rather it is a symbolic (purely linguistic) representation. This must be so, since semantics can only be extended by reference to other concepts, and these must be rooted somewhere in language. All concepts are thus symbolic strings, and their interpretation lies in the way they are associated.\nGiven such a tuple, nodes and edges are created for C1, C2, \u03c7\n\u2217 and a root node Call directed associative edges are added for:\nC1 A+,\u03c7\n\u2217\n\u2212\u2212\u2212\u2212\u21c1 C2 (3)\nC2 A\u2212,\u2212\u03c7\u2217\u2212\u2212\u2212\u2212\u2212\u21c1 C1 (4) C1 ST 3\u2212\u2212\u2212\u21c1 \u03c7\u2217 (5) C2 ST 3\u2212\u2212\u2212\u21c1 \u03c7\u2217 (6) \u03c7\u2217 ST \u22123\u2212\u2212\u2212\u2212\u21c1 C1 (7) \u03c7\u2217 ST \u22123\u2212\u2212\u2212\u2212\u21c1 C2 (8) \u03c7\u2217 ST 3\u2212\u2212\u2212\u21c1 Call (9)\n(10)\nWith this linkage, we can determine which associations Ai are relevant to which contexts \u03c7j and also which concepts Ck may be activated by contexts \u03c7j . The encoding of concepts in this way, linked by associations, forms a recursive structure, which is described below. This is not regular in the manner of a Cayley tree: the result is closer to a \u2018semantic small worlds\u2019 graph [17]\u2013[19]."}, {"heading": "V. NAMING OF CONCEPTS AND THE STRUCTURE OF", "text": ""}, {"heading": "A NAME", "text": "Semantic interpretation is essentially about the naming of things, in a representation meaningful to the observer. A name is a pattern (behavioural or symbolic) that represents a concept. Names are most useful if they are shared between multiple agents, so that concepts can be communicated and committed to shared (societal) memory, and be used to explain (not merely recognize) phenomena. Thus, the way in which we name observed patterns is of vital importance to their interpretation by other agents. This includes numerical names such as coordinate tuples, e.g. (1, 2, 3, 4), (x, y, z), etc.\nWithout a consistent pattern representation for concepts (which we may define to be a language) there could not be recall of memory, and thus concepts would be useless and irretrievable. Thus language is a necessary condition for semantic interpretation. In neural network approaches, the dimensional reduction of a number of inputs to a small number of outputs is only meaningful when one can clearly and unambiguously identify the output channels with named concepts. This too is a simple language transformation4.\nA. Associations and their aliases in context\nAs argued in [7], the fundamental basis for conceptual association has its origins in the structure of spacetime, represented by four types of relationship \u03c4 (being close in space and time, being in a bounded area, etc). We then give specific associative meaning A(\u03c4) to these different spacetime coincidences by naming associations according to abstracted concepts. Concepts and associations are nodes and edges of a graph:\nC1 A(\u03c4)|\u03c7\u2212\u2212\u2212\u2212\u21c1C2 (11)\nwhere the edge of the graph depends on a fundamental spacetime type, whose qualitative description is given by one of the four irreducible associations documented in table I, and their negatives.\nThe conditional parameter \u03c7 represents a context set of terms, under which this edge is active. Since context changes, by definition, as fast as we can think, or the environment can be perceived, the elementary tokens associated with context must be expected to be as large in number as the number of invariant sensory perceptions of the observer, and the most rapidly changing symbols in the knowledge representation. For this reason, we could expect primary context characterizations to be hardwired, i.e. built into the functional nature of the agent, by specialized eyes and ears, face cells and place cells, and so on [20]. Secondary characteristics could be learned over time, provided they remain simple enough to be activated quickly. It is unlikely that context would be expressed by complex concepts. However, it could be that this is the role of emotions: sensations as complex as sensory perception, but without strong rational linkage [10].\nMultiple aliases, or alternative interpretations, of the four spatial relationships are possible, indeed they are encouraged in effective communication for expressivity and qualification. For example, type 1 (proximity) may be interpreted as adjacency, approximately equal to, close to, next to, etc. Type 2 (linear sequential order) may represent time, or unidirectional ordering, causation, dependency, etc. Type 3 (containment) may represent membership in a group, generalization of a collection of concepts, location inside or outside a\n4Note that this suggests that language is not a rule-determined system, but a pattern-based one, i.e. that any rules we identify are post-hoc rather than generative.\nperimeter, etc. However, we should also be cautious that informal association of linguistic metaphor also leads to confusions about the appropriate classification of meaning under the irreducible types, as interpretation by metaphor is fluid in human language [21].\nAssociations must retain their specific interpretation on every link, but also the generic type, which affects the way we parse the structure and hence reason about the associations. Meaningful narrative is usually driven mainly by type 2 (causally ordered sequences of happenings or reasoned steps), embellished by type 4 discriminating properties and lateral reasoning of types 1 and 3.\nB. Scaling of semantics Semantically enhanced concepts may be derived from the aggregation of primitive concepts, by associative clustering. Each cluster may act as a new compound agent, through its representative hub (whose name represents the aggregation), by making new associative links. The hub acts as a gateway to the qualified concept\u2019s associative network, much as a router is a gateway between a subnet and the larger Internet. Each concept is therefore an agent, at some scale of aggregation, with a unique name representing its meaning. In addition, the names of contextualized associations, within a cluster, and without, add the specificity of concepts expressed.\nBased on trial and error experience, it seems that the most comprehensible or \u2018natural\u2019 linguistic naming strategy is one that follows the scaling of agency, i.e. aggregation, as described in [6]. With a scaled approach, one combines the names of interior promises to yield a compound name for a collective concept, and exterior promises then express the associations\nbetween the composite concepts [6]. In linguistics this is called compounding. The scaling is compatible with patterns observed in cognitive linguistics [22], indicating that language, as we understand it, forms a good network structure for concept representation5.\nFrom a cognitive perspective, the way we turn data into concepts depends on the spacetime boundary conditions imposed by exterior environment. The semantics of data are different depending on whether samples originate from from a single source or from a collection (statistical ensemble) of sources. In information culture, people sometimes talk about pets versus cattle, meaning unique instances with names and labels, versus herds without distinct identities. In data terms, these are singletons and ensembles.\nC. The role of approximation in scaling invariance\nIn a batch experiment, where multiple samples contribute to a collective impression, all data points within an ensemble are considered to be equivalent, and with homogeneous semantics. In other words, we choose (as a matter of policy) to overlook any differences, and map them all to the same name. If a dataset is a stream of scalar values qi, then every qi would be considered interchangeable. The indices i may be relabelled and the results will be preserved. The properties that are derived from the dataset must therefore be invariant under relabellings of the array index i. This is how invariant representations are formed. Such invariant representations are crucial to avoid an explosion of contextualization, which would be expensive and would render concepts non-reusable.\n5It should probably be considered a tautology that linguistic structure and conceptual networking are complementary.\nIn order to learn from the past, a cognitive agent must transform ephemeral samples into invariant concepts efficiently.\nproperty\nFor each of the irreducible spacetime association types, there is an interpretation of the meaning of aggregation, which promotes invariance:\n1) Proximity clustering or approximation (spacelike or semantic convergence): When named concepts are close together either in the real external world, or in the interior representation, then they form a labelled proximity cluster. The nature of the proximity may be physical, literal, or semantic depending on the label. The encoding is the same in all cases, up to a label. See figure 6. 2) Causal aggregation (timelike convergent dynamics): When dependencies come together to a head, so that a single outcome may be a causal determinant of multiple sources. This is a definition of the arrow of time. See figure 7. 3) Symmetry aggregation (membership or spacelike homogeneity): When concepts are joined to a single hub that represents their collective identity, by a \u2018contains/contained by\u2019 relationship. See figure 8.\n4) Qualification by attribution (semantic qualification): When the name of a concept is qualified by a context (sometimes called a namespace), and possibly a role (like verb, noun etc), For example, the multiple interpretations of a word like \u201ctar\u201d\n\u2022 Tar as a noun in the context of waterproofing \u2022 Tar as a noun in the context of computing \u2022 Tar as a verb in the context of painting \u2022 Tar as a verb in the context of characteriza-\ntion\nmust be distinguished using the hub construction for attribute association. This construction is a generic (pattern) form of what we call a schema or datatype in computing. It is recognizable without any specially arranged protocol, just from the observed associations. See figure 9.\nThe scaling of conceptual agency, in these representations, is a direct application of the promise theory [6], summarized here according to the four irreducible spacetime attributes elucidated in [7]. The parsing algorithm can now be be remarkably simple because the associations are organized around the most elementary spacetime concepts, or distinction in location, order, or expressed properties."}, {"heading": "VI. THE MATROID NAMING CONSTRUCTION", "text": "In earlier work [5], I showed how a promise view of structure motivated an identification of concepts as spanning sets, or matroids over a labelled graph. The most convenient representation of these is to bind all related clusters to a hub node, whose name is the\ncollective name of the cluster6. This offers a scalable approach to spanning concepts [5], with upwardly extensible uniqueness. By associating a collection of associated concepts with a single point of address, we throw a kind of perimeter around the spokes and rim emanating from the hub to form a single new concept. Such clusters may overlap without limit, thus this is not a mutually exclusive branching process [3]7.\nUnlike a hashing approach to unique naming, uniqueness can be assured by combinatoric compounding of names. The collection of named concepts in a cluster is accessed through a single node hub which acts as a gateway for the aggregation of parts. The name of such a matroid hub (or compound name) represents the name of the composite concept, formed by the internal association of interior concepts.\nA. Nominal compounds and orphans\nConcepts, whether primitive or derived, may therefore be thought of as \u2018lexical fragments\u2019, \u2018syntax fragments\u2019, or generally language fragment, formed by recursive scaling. A compound is, in essence, a long name, which contains evidence of the concepts it depends on, and which is linked recursively to the atomic concepts (see figure 11). It is also possible that the compound name forgets its association to\n6This is broadly analogous to the way we name a compound data schema object in computing. The hub has the name of the object in its entirety, and the members have the names of the associated items. The association types are analogous to the data types. Where the analogy breaks down is that matroid hubs can name arbitrary collections of data, they do not form mutually exclusive, non-overlapping memory segments.\n7Concepts may be aggregated without limit for qualification, but they cannot be subdivided without limit. Thus taxonomies and ontologies as branching processes are not scalable to adaptation.\nthe concepts from which it was derived, and survives independently through the frequency and associative density of its use.\nThe study of compound terminology is a major topic in linguistics, which addresses the conceptual origins of names we use in language. For example, the leg of a table can be referred to by a single compound word or phrase \u2018tableleg\u2019, in which the compound has the role of \u2018leg\u2019 (by analogy to an animal leg) and a contextual attribute \u2018table\u2019 (see figures 11 and 10).(\nleg role\u2190\u2212\u2212 (table-leg) qualifier\u2212\u2212\u2212\u2212\u2212\u2192 table )\n(12)\nThis example is straightforward, and fairly unambiguous. The name reflects the components directly with a conventional ordering, leading to a simple rule to construct a compound identifier. However, many compound concepts do not directly reflect their origins, or in fact forget their origins over time, e.g window (from Norwegian \u2018vind\u2019 and \u2018auge\u2019, meaning eye for the wind, clearly before the invention of glass). A window is neither a wind, nor an eye, yet the word serves its purpose with a whiff of the metaphysical. In other cases, there is no linguistic term for a new qualification, yet we need to distinguish ambiguous concepts with qualifying context, e.g. consider \u2018doctor\u2019. A doctor may refer to a medical doctor, a general practitioner (GP), a surgeon, or someone with a PhD, etc. We could simply find unique names for all of these, but this approach does not easily scale; hence, it is normal to describe \u2018namespaces\u2019 as contextual constraints. This can be done by forming a compound name as a phrase \u2018doctor in the role of GP\u2019 for instance (see figure 10).\nIt is now somewhat ambiguous what is the role and what is the qualifier in this construction. It could be argued that \u2018GP\u2019 is the role and doctor is redundant, or that \u2018doctor\u2019 is the role and \u2018GP\u2019 is the qualifier. The distinction is somewhat arbitrary from a computational viewpoint (this is the danger of \u2018schema\u2019 thinking),\nbut it is sometimes helpful to be able to distinguish a privileged component that describes the behaviour or function of the term. Through the various experiments conducted during the course of this work, from topic maps [23] to the present model, it seems that the appropriate place for agent role in a network lies in the associations between pairs of concepts, rather than in the concepts themselves. If one tries to \u2018type\u2019 concepts (in a typology, taxonomy, ontology, meronomy (partonomoy), etc) one prevents the free association of ideas by analogy that is so central to human reasoning. Thus the classic idea of a \u2018data type\u2019 for concepts in a semantic network appears to be simply wrong because it leads to an over-constrained network with too much uniqueness. The resulting network is too sparse to allow percolation and hence propagation of storyline [7], [24].\nFinally, there is the central problem: how do we think up names to concept clusters? The naming of clusters seems analogous to the matter of forming nominal compounds (see figure 11). One approach is to try to abandon linguistic meaning and use some form of unique hash. This approach is used in natural language processing [25] with some claimed success. However, hashing is a one-way transformation, which is unsuitable for our associative map. To create an interactive system of knowledge, we need to be able to read back concepts in a form parsable by humans.\nThe matroid hubs, with their compound names, are binding points, or confluences of association act, which as semantic correlators, i.e. authoritative anchor points, expressing uniqueness. If we follow through this approach consistently, then conceptual specificity increases upwards with the level of aggregation, rather\nthan downwards with the depth of branching, as in a taxonomic hierarchies (i.e. branching processes). This leads to a bottom-up approach to conceptualization, which is entirely compatible with the modern genetic understanding of phylogeny and phenotype expressed through the aggregation of genetic flavours. This is also motivated by a promise-oriented interpretation. Mirroring the proposed usage approach of language formation in cognitive linguistics [26]. It is a chemistry of semantic atoms, mixed into distinct molecular combinations, with differentiated functional meaning.\nB. Concepts, associations and the scaling hierarchy\nThe end result of a cognitive process is to tokenize complex data inputs into tokens (concepts), which summarize the data in an invariant representation; this might be approximate, but it is proportionately immune to variations, and to link these together by association. In promise theory language, concept tokens are agents (nodes in a graph), and associations (graph edges) are promised by these agents.\nThere is a ladder from sensing to conceptualization, which proceeds by aggregation (over time and space). This involves work, and thus it takes increasing time to process complex associative concepts. Concepts are clusters of agents. The basis of interpretational semantics lies in the following observations:\nLaw 1 (Coincidence and concurrence): That which occurs together (concurrently, coincidentally, i.e. locally, or in the same context) is associated. Thus context belongs to associations, not to concepts. Measurements at different locations and at different times are not necessarily causally related, but may be correlated. The naming of the association is undetermined.\nLaw 2 (Semantic stability): Averages stabilize semantics by decoupling from temporal and spatial approximate regions. Data sampled and combined from different sources (locations) should be treated as ensemble averages, in a single concurrent experiment. The meaning of time and location are lost in these averages, and semantics are stabilized by this decoupling.\nLaw 3 (Scaled agency): Semantics (interpretations) arise from the naming of associated forms and patterns, in space and time, and across multiple scales. Names assigned become invariant concepts.\nLaw 4 (Constant semantics during observation): Measurements are comparable if they have the same semantics. In cognition, observations are not comparable in an experimental sense, unless we deliberately overlook certain information. Thus semantic stability depends on what information we throw away. The last two are circularly self consistent.\nC. The learned context channel\nDuring activation, inputs might activate an elementary context token, and in turn, several such atoms might activate an association, somewhat in the manner of a crude neural network (see figure 3). The relative activation level of an association is thus the integral of\nthe weights of its incoming edges. A triggering policy could also be employed for generality and tuning; however, we can only simulate this spatial interference process on von Neumann computers..\nContext is used to frame the circumstances under which associations and concepts are identified. Striking the balance between a context that strives for a coordinatized precision and one that is maximally invariant (for repeated \u2018context-free\u2019 use) is the central challenge of building an addressing system for semantics and knowledge [7]. In the implementation of Cellibrium, it is thus assumed that:\n\u2022 Context is attached to associations, not concepts. A co-activation of an associative bond needs only be a set that activates both ends, but the specific members may differ at each end by evolution of contextual awareness of the cognitive agent. \u2022 Limiting the amount of nominal information in context is a priority, else the memory requirements for knowledge would be divergent. \u2022 All associations belong to one of the four spacetime association types [7]:\n1) An approximate similar location in space. 2) A common cause or outcome, convergence\nof path in past or future. 3) A common enclosure or membership. 4) A shared property, leading to implicit mem-\nbership (e.g. all kinds of doctor in figure 11).\nIn aggregation by symmetry, all these collapse to type 3 in practice, i.e. grouping of indistinguishable members [7]. \u2022 Context describes something analogous to recent browsing history:\n1) The intent that led to the association being made (interior state).\n2) The emotional and rational state of the agent\u2019s recent thinking (interior). 3) The unintended circumstances in which the agent finds itself when making the association (exterior state).\nIn other words, anything we happen to be thinking about or involved in can be a context for knowledge relevance (see figure 13).\nD. Interior and exterior associations of concept clusters\nWhat is expressed externally by sensors plays a different role than what is expressed internally by introspection, as we saw in section VI. Interior and exterior properties continue to play a role in distinguishing. Figure 13 illustrates this. Both interior and exterior contexts may play a role in addressing, i.e. labelling and retrieving, concepts (see section VII-B).\nThere remains the question how we should use context to activate or deactivate story pathways. In an artificial system, especially one made for a domain specific purpose, the information content, or semantic complexity, inherent in context annotation during training might be too small to give a reasonable chance of discrimination of relevance in real-world circumstances. Unless there is a sufficient density of discriminators, we will either fail to find possible pathways (lateral thinking) or we will include too many, losing the point of context as a refinement criterion.\nWe might rank pathways by an activation score for how each node in a story overlaps with the current context. Alternatively, we might rank an entire story along its path. The latter would bias in favour of long\nstories8.\nE. Examples of contextualization\nThe structure of the following examples helps to show why context is so important, and why it is important to aggregate clusters into concept-property matroids and situation-context groups. This is a settheoretic interpretation. Contextual information is both interior (explanatory of subjects) and exterior (causal and situational or correlated). The following cases help to exemplify the issue: \u2022 The doctor (in the role of surgeon) depends\non surgical gloves in the context of (operating room,examining patients). \u2022 Bruce Wayne (in the role of Batman) is a member of super-heroes (in the role of nightclub). \u2022 Tidal is generalized by (is a kind of) company when (thinking about music, streaming music) \u2022 Tidal in the role of gravitational effect does NOT have the role of company. \u2022 Tidal is NOT caused by the moon in the context of tidal as a company. \u2022 Tidal has the role of an effect when thinking about physics, gravity. \u2022 Gravity depends on mass in the context of physics. \u2022 Gravity is the name of a book (a book called gravity) in the context of physics. \u2022 Temperature expresses very hot in the context of measuring server. \u2022 Hot contains very hot in context (all contexts). \u2022 Very hot contains \u201850 degrees C\u2019 in the context\nof measuring server.\n8If we could find a normalized \u2018probability\u2019 to sum per unit story length, then we could build a score analogous to the Shannon entropy, in which associations were symbols in an associative alphabet. This remains for future investigation.\n\u2022 Temperature expresses hot in the context of (measuring server, 306, datacentre, NYC).\n\u2022 Temperature expresses hot in the context of June. These singular context names, while meaningful to readers with a broad cultural knowledge, are too simple to allow meaningful discrimination during machine processing. We need to understand how to extend context labels for efficient addressing of semantic relevance.\nF. Context design and compound concepts\nBecause of the recursive nature of concepts, the promise model of scaled agency in [6] suggests that it will be helpful to distinguish between interior and exterior context of a concept. Interior context refers to qualifying associations positioned \u2018behind\u2019 hubs, which help to support and qualify the interpretation of a concept. Exterior context refers to the set of concepts that are active in the recent train of thought of the total cognitive system, as it interfaces with the world on the other end of its sensory apparatus (see table II).\nInterior memory context acts as a kind of \u2018type\u2019 or \u2018role\u2019 interpretation for concepts that have been selected or deselected by an exterior memory context, whereas interior awareness context represents the feedback of the system talking to itself about past experiences and thoughts. In all cases, the functional treatment of memory context may be handled by an invariant algorithm, with great computational simplicity. Context is not just about the addition of descriptive words during learning, but also about linking those compound states to all the sub-concepts on which they depend too. By following this approach, the average degree of connectivity in the graph increases superlinearly, making non-trivial stories increasingly likely, while also providing switches by which to exclude pathways based on later context during retrieval.\nThe recursion in the graph corresponds to a recursive linguistic structure that can be illustrated as follows. Consider the following unary association, containing subgraphs, labelled by their irreducible ST-type.\n(the cat which (is black (4), eats fish (2))) expresses(4) (happy purr) in the context of (living room)\nThe parenthetic recursions are all interior properties, and the explanations of situation are exterior (bold-\nface). We end up with useful statements of the general structure:\n( (concept name, interior recursive properties) (association type, alias)\n(concept name, interior properties) in (exterior context)\n)\nThis suggests that an API, something like the one shown in figure 14, can help to automate the structural regularity during the documentation of concepts and associations, i.e. during semantic learning. Notice how, by using the matroid hubs and compound concepts to reduce the dimensionality of conceptual activations into a single concept, we also achieve an economy of scale, with attendant computational simplification9."}, {"heading": "VII. RETRIEVAL OF \u2018KNOWLEDGE\u2019", "text": "Once encoded, retrieval of associations is achieved by reparsing the representation in figure 3, subject to an independently maintained context. Sensory context is a set of somewhat elementary tokens, which activates certain concepts and associations. Exterior context may be exchanged for slower, but more stable introspection, in which derived concepts are fed back into the context by \u2018thinking about\u2019 a particular topic. Tokens can be followed individually or activated in parallel to search for the consequences of a particular concept in the current context. Implementing this context channel will be the most challenging aspect of building a cognitive knowledge representation. Only the first stages will be addressed in this report.\n9It is interesting to speculate whether this observation might help to explain the common use of compound words and special terminology, like acronyms, in human language.\nA. Conceptual pathways Retrieval of explanations and associative reasoning can be open ended or bounded (see figure 15); we want to: \u2022 Explain how to get from a starting concept and a\nfinal concept (bounded but not unique). \u2022 Explore where we can go from some a starting\nconcept (unbounded brainstorming). \u2022 Explain the different ways one can get to a final\nconcept (inverse brainstorming). The first of these is somewhat analogous to a quantum path integral [1]. Any one or several of the possible pathways might be in play between concepts in a given context. For example, without sufficient context to know the interpretations of \u2018Oslo\u2019 and \u2018America\u2019, both these unary associations may be in play simultaneously during a reasoning process: \u2022 Oslo is located in America \u2022 Oslo is NOT located in America.\nIn logic, this would be considered impossible, yet both of these pathways may be equally relevant (we should\navoid the expression \u2018true\u2019) if the context is unable to select only one. A knowledge system cannot even know that these seem potentially contradictory without a sufficient density of linkage to qualifying context. We must be quite careful not to over-constrain reasoning by projecting human expectations onto a process too early. It is likely that concepts such as true and false are emergent rather than prescriptive.\nB. Context in retrieval\nAs we trace along a story, from concept to concept, four semantic component sets may be associated with with each step in a story (see figure 16). \u2022 The current concept in a story (a fixed boundary\ncondition). \u2022 The concepts associated with the current concept\n(directed links to possibilities). \u2022 The current awareness context or \u2018state of mind\u2019\nof the observer (current relevance). \u2022 The awareness context at the time of learning\n(encoded in interior and exterior associations). How these four sets activate one another is key to how stories may be generated \u2018deterministically\u2019. The current concept is a natural part of the state of mind of an agent, and thus it belongs to the interior cognitive context as much as any sensory inputs. The cognitive context of an agent divides into interior and exterior parts. Interior context is what the agent is thinking about, independently of its sensory inputs. The exterior context is its sensory channels. Thus some context information is naturally inward-looking, or introspective, while other information is outwardlooking (grounding in the realities at different times). It is interior awareness context that determines the relevance of forward pathways, because state of mind is the calibrator of intent. Exterior awareness context is about framing the current experience, grounding it by experience or feeling. Thus, the two cognitive \u2018awareness\u2019 channels have different semantic roles [7].\nAs context evolves, so does the focus of consciousness, or \u2018what the system is thinking about\u2019 i.e. the current concept or set of concepts in a story frame. Interior associative context plays the role of functional type, and is entirely encoded in the recursive compounding structures of the knowledge representation. So when we retrieve knowledge, this kind of context is purely qualifying or explanatory of the usage. The major links to concepts will be to the fully qualified hub nodes, and the interior context will be hidden unless explicitly examined, at any given level of recursion.\nIt is exterior associations that link concepts to one another in the knowledge representation, so they determine which concepts lead to other concepts. However, those exterior associations are judged for relevance by comparing the context in which they were learned with the current context or state of mind of the agent. This leads to a complex causation: concepts overlap through association, and context overlaps with pairs of\nconcepts, and perhaps beyond. We therefore have two channels evolving in parallel: a short term memory of context and a long term memory of qualified concepts [7].\nC. Evolution of awareness context Measuring the relevance of forward pathways remains an unsolved problem. How should the current context evolve then as we traverse a story. How quickly should context be forgotten? When to increase the scope of context and when to say that our evolving thoughts have drifted off-topic is an highly subtle determination, which may not lend itself to algorithmic or pattern based solution. It might well be that relevance needs a bank of multiple pathways interfering across the entire length of a story path. When we apply software to the problem, it is natural to treat each leg of a story as a Markov process, each step independent of the last, with some activation context adding transverse memory. This is probably too simplistic, but for this work, that must be the limit of ambition.\nAs a first approximation, we may take the lexical overlap of the two context sets (past and present) as an estimation of relevance. Using this, we can sort stories link by link to judge the relevance of each twist and turn. The relevance of the total story cannot be calculated without a particular end-topic. That goes beyond the scope of these notes.\nWhen machine learning is applied to document processing, story fragments are treated as patterns to be recognized, and the relevant forward pathways are determined based on statistical ensemble support rather than semantic relevance [27], [28].\nThere are many issues with trying to match relevance based on context in a small system. Present context may not match the context at the time of learning, because there are too few degrees of freedom for coupling contexts together. It may turn out that we have to rely on more universal invariant context markers like emotional states (fear, happiness, etc) to play a role in activation and selection. Although these are poorly understood, they may play a central role in joining together ideas that cannot easily be joined by explanations of semantics.\nAs usual, it is important to remember that semantics are secondary to what dynamics enable, thus causal propagation of meaning cannot be assumed across different timescales (see figure 16). We may expect the timescales of learning and retrieval to become separated eventually, with retrieval times much longer than training times, yet we must still recall and modify stories that were learnt previously. This gives some clues about the encoding. In particular it tells us that all causal information needs to be encoded in exterior associations, not by relying on context to light up answers directly, like a primary database key.\nIf excessive specificity can be the enemy of retrieval, then specificity is important during the encoding phase of knowledge. For example, it is not the general\nconcept of birds that depends on the concept of flight, but rather a fully qualified instance of a bird. In other words, we have to be careful not to confuse what is general with what is specific, at any level of recursion, as this can lead to manifest errors of reasoning, such as \u2018all birds depend on flight\u2019. Matching of conceptual scales is yet another problem that must be deferred for further study in future work."}, {"heading": "VIII. THE EXPERIMENTS", "text": "At the time of writing, a simple proof of concept implementation of the foregoing cognitive system has been implemented. Only the simplest level of interior/exterior context conditioning was used to activate and screen paths, owing to the complexity of simulating such a feedback system in software. The initial goal was to illustrate the principles.\nA. Data sources\nFour kinds of data source were imported into a semantic graph, in the hope of first generating stories by brainstorming. Even this simple first step has application for causal analysis, qualitative hypothesis testing, etc. The data sources were: \u2022 Computer monitoring examples: data from the\nCellibrium CGNgine agent (a CFEngine \u2018computer immunology\u2019 derivative, with embedded machine learning at the pattern level) were output in realtime, as in a production datacentre environment or IoT scenario. As an intentional system, the intended policy could also be encoded and connected to the activities of the agent, describing desired state. Thus data observations, intentionally measured from the actual state of the system, and converted into invariant forms, with an intended interpretation relative determined by policy goals and constraints. See figure 18. Policy documentation files, describing intent, were incorporated, such as the intended meanings\nof software functions, with links to documentation, as well as the coding of particular policy rules. The source code of the software agent could be scanned to extract error messages given in a particular context10. Domain knowledge about the design and intent of the software itself was included manually, as well as remarks about the world of computer datacentres, servers, and operating systems. \u2022 Software system example: Data scanned from the nesting structure and dependencies of software, packaged in process containers, file bundles, and linked binaries, probed using intended composition rules (source: Makefiles, container specifications, package compositions, and binary linker data from ldd, etc). See figures 19 and 20. \u2022 Doctor online registration wizard: Domain knowledge from an imaginary online scenario, where a patient is trying to register for a doctors appointment in a new city. Concepts and associations are input directly from a wizard overview of knowledge of intent about the smart distributed service, and the software it uses, its dependencies, and workflows etc., (source: authors of the system). See figure 21. \u2022 Big picture semantic index example: Domain knowledge contributed from possibly multiple sources giving a cognitive overview of an entire environment or experience. See figure 22.\nB. Machine learning\nMachine learning appears at two distinct levels: 1) The sensory data collectors sample the exterior\nsources at some (relatively frequent) timescale, in accordance with Nyquist\u2019s theorem. Each collector, with its independent semantics, transmutes sample data into a separate invariant representation and learns its significant patterns and states (for monitoring, see [29]). From this Bayesian statistical description, a lexical name representation is selected from a small set invariant states. 2) Associations are updated on a regular timescale, and each associative link acts as an independent Bayesian learning process (i.e. a Bayesian network), giving the links in the semantic network weights relating to their importance and frequency of visitation. This applies at each recursive scale of concept aggregation.\nThe intentional aspects of any system, which was designed or evolved for a niche purpose, are central to the encoding of its semantics. If such information is not captured at the source, it is simply lost. Users of the software may later reinterpret this intent, in\n10Current programming languages do not make it easy to export the contextual intent behind code or user messages. In future, such an innovation will become essential to the scaled monitoring of a software society.\nthe light of current assumptions, but the intended meaning ultimately originates from a source which matches two promises: the \u2018promise/intent offered\u2019 and \u2018promise/intent accepted\u2019 (like lock and key). The receiver always has the last word in interpretation however [8].\nC. Software implementation\nPrototype code for creating and parsing the knowledge representation, is shared under the Cellibrium [11] project. This employs the simplest possible proof of concept, avoiding programming concerns in favour of pedagogy. Concepts are represented as directory names, each containing subdirectories for the 4+1 association types; then, in turn, a file of association annotations, including timestamps and weights for Bayesian updating. Although filesystem limitations make this approach unsuitable in the long run, it made the coding of a prototype pleasantly free of dependencies and obscure APIs. Everything could be handled with fast, efficient POSIX system calls11. All concepts have directories under some root node, which contain subdirectories corresponding to \u00b1 the four spacetime association types. Under these lie files, which document the concepts that may be reached by following that type of association:\nroot/ root/StartConcept/ root/StartConcept/1-4/ root/StartConcept/1-4/NextConcepts*\nThe \u2018next concepts\u2019 are files containing the specific annotations, timestamps, and learning weights of the associations between the starting concept and the chosen next concept.\nTo frame context, a separate control channel is used, mainly for implementation convenience. The context association is a spacetime containment type (ST3), but its functional role is different to other groupings. Contexts work in exactly the same way as other concepts, with composites and sub-clusters. However, contexts arise ad hoc and thus their clusters do not always have lasting meaning as composite concepts. As running catalogues of what is currently going on, somewhat analogous to a browser history, so relevant words pertaining to the agent state were simply strung together in no particular order. This is analogous to the original CFEngine classes [9], where context was a collection of class strings that were probed from the environment by specialized software sensors. Combinations of these classes could be constructed as quasilogical expressions (acting as hubs) to activate certain concepts (in this case configurations). The basic atoms were drawn from system variables.\n11It would seem as though a graph database would be the ideal candidate for representing such data, but thus far, the input and query languages for such databases are clumsy and ill-suited to this kind of structure.\nIn more general cases, at a higher level of abstraction, context must include the \u2018state of mind\u2019 of the cognitive system over the recent past (again, think of browser history), which encapsulates intent as well as environmental impulses. These will be expressible in terms of higher level concepts12"}, {"heading": "IX. RESULTS", "text": "A. Assessment\nAssessing the success of the prototype \u2018brainstorming\u2019 experiments is not an easy matter, given the qualitative nature of the results. This is not so much a weakness of the experiment, as the nature of the problem. Although it is possible to manufacture artificial metrics by the collection of numerical data, e.g. about numbers of proposed stories, or conceptual overlap with context along a path, etc, these measures are themselves ad hoc and largely unhelpful, as they do not correlate easily with the subjective sense of \u2018a ha!\u2019 one has when finding a helpful outcome. So, we are left with a kind of \u2018Turing test\u2019 assessment approach, i.e. \u2018it looks ok\u2019 to a human. Later, once one applies the approach to a specific problem, where target goals can be incorporated from the beginning, we should be able to do better in establishing consistent criteria. I shall not address this further in this report.\nB. Data\nData sets used consisted of hundreds to thousands of concept tuples, collected from a variety of sources. These numbers are quite small for a realistic application, but suffice to test some comparative aspects of the approach. Association data were generated by annotating system monitoring in realtime, from software outputs, documentation scans, and by manual knowledge documentation of hints.\nIn early trials, realtime monitoring data were fed into a knowledge representation in a way that retained a lot of specific information, to see if this might stabilize into a comprehensible emergent pattern. However, this experiment quickly revealed that the potential burden of remembering the detailed semantics of every moment.\nIf each new measurement set leads to a different context, i.e. making time itself the generator of context, the number of concepts (or amount of data) would be prohibitively large (not for a computer to remember, but for any human or algorithm to make sense of). One cannot escape the fact that our human brains have builtin limitations.\nSince we are dealing with semantic interpretations, we cannot simply say \u2018big data\u2019, and treat every knowledge problem as a forensic investigation, because\n12Framing and limiting a context is a very hard pedagogical problem. As a semantic spacetime, a knowledge representation is somewhat similar to a higher dimensional field theory with an ultraviolet cutoff, as information is discrete and finite. A story is like a transition function, or S-matrix element, whose vertex rules are based on the causal associations and their propagation rules.\nwe cannot guarantee that a focused set of concepts will emerge from the data. That is a different problem than having an expert on hand who is already au fait with the necessary knowledge. Thus, we must look for invariant representations of big data, cumulatively over time, that may be interpreted by only a small number of (possibly composite) concepts, for the sake of comprehension. Fortunately, studies have shown that meaningful patterns from system monitoring behaviours are few in number, compared to a bulk of non-meaningful noise [30].\nC. Structure\nThroughout early experiments, the output of a brainstorming episode, based on a given starting concept, tended to lead to either no stories or too many stories. The reasons for this depended on both the structure of data and the selection criteria. Initial attempts to find quality stories were hampered simply by an absence of data. The first trials, using data types to represent contexts, were relatively unsuccessful. The input of knowledge took a huge manual effort to curate, both in terms of syntactic and semantic burdens placed on the author; then, even if someone could be persuaded to undertake encoding, it would be fragile and underconnected. Very little unexpected emergence was possible, as the typing made data overconstrained. The sparseness and fragility of the typed structure meant that concepts could not easily be discovered by free association, and most searches resulted in no results.\nTopic map inspired structures [23] created mainly led to trivial stories, so the project was set aside for\nalmost 4 years. Following this hiatus, the models based on topic maps were abandoned [3], and ideas from the promise theoretic approach [5] were revived.\nThe removal of \u2018types\u2019 as discriminators, and their replacement with context-free lexical terms, led to a large increase in the density of stories. It is difficult to put numerical measures to the improvement, because a radical restructuring of many aspects was involved in this shift, and the results are very dependent on the particular characteristics of example data, which were not preserved over the intervening years between the difference, although an attempt was made to reconstruct similar data. At best one could say that there was a qualitative improvement in the results.\nThe earliest attempts at discovering emergent connections, were implemented by my collaborator Alva Couch [4], [31], and used a shortest path approach to selecting a unique \u2018route\u2019 between two concepts: initial and final boundary conditions. This is slightly different from full brainstorming, because it is already more constrained by having a definite target concept.\nUltimately, I abandoned the shortest path approach as it creates a tension between quantitative metrics and dynamical selection criteria rather than semantic relationships. The idea that a single path, based on its length, could be a correct answer to the exclusion of others does not well motivated; although, it is plausible that, given a set of semantically valid paths from A to B, the shortest one might have interesting qualities. The advantages of fixing boundary conditions at the start and the end of a story were compelling, but it is not always so clear what we are looking for when\nexploring causes and interpretations.\nD. Computational complexity\nThe derivation of the four irreducible spacetime association types brought a significant simplification to the algorithm for exploring locales. Initially a linear search was needed to identify matching concepts from a number of type \u2018namespaces\u2019. Then associations could be followed, but the question of whether the associations could propagate or not was not encoded directly into the association, requiring another lookup, or so-called inference rule. The possible complexity was of the order of the product of the length of the inference table with the number of outgoing associations (the out-degree of the graph). Thus the pre-classification according to the spacetime \u2018compass types\u2019 led to a reduction in the order of magnitude of the computational complexity by eliminating the need for the inference table. The propagation of these types in inferences is defined by the spacetime classifications themselves [7], and no matching is required.\nA further reduction in the degree of \u2018grammatical complexity\u2019 may come from the aggregation of multiple concepts into aggregate classes. If associations can be made to larger umbrella concepts, rather than concepts, which are too specific, then the cost of recursion can be spared. Thus, what may seem to be a context-free expression, linguistically, may be sufficiently represented by a regular language expression. See [5], [7] for a discussion of the grammatical complexity of spacetime associations.\nThe addition of context, for either positive or negative matching, is the most intensive computation remaining; however, this observation seems critical to establishing semantic relevance of stories. Context, as modelled in the original CFEngine representation [9], [32], and its derived Cellibrium implementation [11], is a non-ordered linear list composed of many atomic characterizations. Each evaluation of context requires parsing the entire list for matches with a similar nonordered expression composed of the same atoms. The complexity could be of the order of the product of the current context lookup and the learned context lookup, for each association. Supposing that each could be of logarithmic order, by hashing, this is much better than a model without the encoding of spacetime types. The size of a context cannot be pre-determined, because it fluctuates over short times, and grows during long-term learning.\nEach link in a story has a complexity cost, associated with the relevance of the link. This was measured by the size of the overlap between current context and learned context (analogous to the mutual information transfer [33], [34]). In the CFEngine model, context was defined as a \u2018logical boolean expression\u2019, which enabled contextual relevance to be reduced even further to a single expression evaluation, whose overlap was generally quite small, i.e. only the length of a very\nspecific boolean expression [32]. There are many ways to optimize such contextual matches, using aggregate classifications and caching, largely because each execution of a CFEngine context is a frozen moment in time. The nature of a cognitive system, on the other hand, is that context is continuously changing, so it will always be more expensive to compute.\nE. Sensemaking of stories from structure\nHaving enough brainstorming paths to analyze, in the output of a search, was an advantage of the new spacetime approach to knowledge representation. Two problems remained however, whose remnants may still be seen in the figures:\n\u2022 Stories were often unreadable and bizarre, as even valid lexical phrases coughed up along pathways of the graph had little meaning without the original intent of the associations and names preserved and represented. The loss of intent seemed to lie at the heart of this. Intent acts itself as a form of context. \u2022 Often, stories did not terminate, without an artificial length cutoff, in spite of loop detection methods on a per-story basis, as the simplistic recursion allowed stories to contain one another as sub-parts by the free association. This suggested that success may lie in the use of context to constrain stories and bring focus to the results.\nLater, when repeated visitation of a concept was forbidden across all stories (as a global constraint), the stories did become finite; however, now an argument could be made that the search was now overconstrained, and ad hoc choices might exclude certain combinatoric stories from being visited at all13.\nIn fact, the non-termination of stories is another indication that the graph is well connected, even with the small data sets used, but they were also an artefact of the open-endedness of the brainstorming process. Without a clear goal either in terms of context or specific concept, there is no deterministic criterion for running out of combinatoric options.\nThe recursive scaling of ideas anchors ideas to context in a more powerful way of attaching intended context than typing. This is basically analogous to a transition to a schemaless database. After the introduction of the recursive hub structure, and the automation API, in figure 14, the story density was largely unchanged, but the readability of the stories was greatly improved, allowing the kinds of outputs shown in the figures. The structure led naturally to the separation of context into interior and exterior memory channels as\n13The original approach used by CFEngine was to simulate the \u2018neuronal dead-time\u2019 into concepts, eliminating loops by virtue of concepts having fired already [39]. This worked in CFEngine, because of its simplistic linear processing approach. In a network with multidimensional causation, the approach could easily prevent important answers from emerging rather than protect against repetition.\nmentioned in section VI-F. The interior memory context, in particular, brought immediate comprehension, even when the strict linguistic form was contrived. This, of course, is more a testament to human linguistic acumen than to the skill of a very simple algorithm, but it indicates that the essence of language is captured by a recursive identification of irreducible types.\nF. Outputs\nFigures 18-22 show typical samples of outputs from a brainstorming. The actual outputs are much longer than these. This print format is not well suited to show the data. The left hand margin shows the spacetime association types (ST1-ST4) followed to make the association, and the indentation shows recursion depth along a single path.\nIn figure 18, the data are taken from the CGNgine software, using several data transmutors to scan both the source code, and the system policy. The occurrence of an obscure error message in a system output can immediately be tied to the likely sources and points of expression, allowing a user to trace the source all the way back to their intended policy, and through the software layers.\nFigure 19 shows data acquired by a software build pipeline. It shows how deployed software depends on its containerized packaging, software dependencies, and host operating system base, all the way down to the location of the deployment. This chain of dependencies may be of use to developers, operations, and risk analysis personnel, to name a few. Further analysis based on the specific semantics could be automated further. Simply gathering this information into a single place, in a readable format is no small task.\nFigure 20 shows a more straightforward recursive decomposition of library dependencies discovered by using the Linux ldd command. The deeply nested and extensive list of cross dependencies is quite impossible\nfor any human to comprehend, but is straightforward to encode for analysis.\nFigure 21 shows a facsimile of the distributed system used by patients to register with a new doctor in Norway, as curated from the perspective of a domain expert. The procedure involves going to a website, but first one must have a number of credentials from third party sites, totally unrelated to the public service. This kind of constellation of independent services, integrated into a meaningful whole would be handled by some kind of wizard software if it were a task for a single computer. In the web era, there is no single agency with the responsibility to integrate this information. A cognitive system, which monitors services in a smart city, could do this, for instance.\nFigure 22 shows some more playful examples of domain knowledge, concerning the purpose of different services, in different contexts. A lexical token like \u2018tidal\u2019 might refer to many different things in different contexts. Here the brainstorming is able to distinguish these cases using the recursive structure of the matroid hubs. In a more sophisticated rendition of the brain dump, one would be able to select specific interpretations from these structures without risk of conflict. Whether the source of the semantics is manual curation or trained natural language processing, it does alter the key benefit of the approach used here: namely that it is orders of magnitude cheaper to build and to use than an approach based on repeated use of \u2018big data\u2019. Such approaches have been used to discover vector congruences and emergent functional semantics, but only with extensive training [35]\u2013[38]. Such approaches could still be used as inputs to the present approach, in the absence of something more practical.\nG. Inadequate context\nDuring the development of this approach, the notion of context was subject to a lot of confusion and misdi-\nrection. The way logic-based approaches to semantic modelling have tried to use context (as data types) turned out to be quite wrong: there exists no objective version of knowledge that can determine a fixed typehierarchy or preferred ontology. Knowledge is very much in the eye of the beholder, and every observer forms an individual ontology [3]. Context is not an objective categorization, it is a running state of mind (like a browser search history that records our intent). As long as we try to treat a knowledge representation as an objective static boundary condition on knowledge, rather than as a private subjective interpretation, we will run into difficulties.\nIn this present approach, motivated by spacetime classification of semantics, and which thus far ignores any preferred ordering by link weights, semantic stability is encouraged simply by a structural aggregation of concepts, analogous to human language. So, even if there are random variations, using promises like \u2018A is near B\u2019 effectively allows us to collapse disconnected or multiple stories into a single broader line story. What is important to reasoning is that there lies a causal set of paths, that may be integrated with the specific\nboundary conditions from start to end14."}, {"heading": "X. SUMMARY AND REMARKS", "text": "The recent focus of work in artificial intelligence (AI) has been on the training of artificial neural networks to recognize increasingly complicated patterns, like voice, faces and handwriting. Trained networks mimic human cognitive sensory systems, usually one by one. Neural networks are trained by attempting to replay a lifetime of experiences at high-speed into a learning process. The extent to which such a network accumulated, and made predictive, depends on the extent to which its spatial structure can mimic a representation of the semantics one happens to be looking for, with only a single trigger [7]. However, it is impossible to measure the certainty of even this simple kind of reasoning, indicating that an entirely separate stage of processing by a more deterministic representation is needed for reasoning that goes beyond a single inference. That is where the present work fits in. The recognition of the lexicon of basic patterns is the first part of sensory perception, the second part is associative reasoning based on its tokens.\n14This is tantalizingly reminiscent of a quantum path integral, for reasons the reader may ponder without suggestion from me.\nIn this report, the idea is not to mimic human faculties but to integrate them, across multiple scales of experience, allowing us to to add selected semantic, data of high quality, incrementally from any source to a single model with very few limitations. The learning described here is unsupervised, in the AI sense, but still curated by a surrounding \u2018society\u2019 of expert interactions, so it is supervised by an emergent framework of domain expertise. No learning can happen without some external selection process, just as no child can be programmed with life skills without others to raise it. The boundary between raw exterior and curated interior knowledge is what cognition enables: to be able to represent and pass on knowledge is a compressed (tokenized) form for consumption by others, without the need to undergo every experience, blow by blow, and in person.\nThese experiments are very interesting, but there are\nplenty of issues to address in future work. Finding how to constrain stories and bring focus to their trains of thought is the major problem. With the use of a spacetime inspired structure, stories are no longer as scarce as when trying to use an overconstrained typed approach. Lateral reasoning is now possible, and relevance is maintained by representing context. The treatment of context in this note is weak, but there is much room for improvement. In [10], I speculated on the role of emotional state in determining context. Emotional weight plays a significant role in cognition for setting context and interpretation priorities. From the viewpoint in [7], emotions seem to play the role of very coarse aggregate \u2018concepts\u2019 stimulated by sensory/introspective inputs. Emotional context is thus a systemic assessment of some \u2018agent\u2019 based on a number of contextual factors. It is a threshold judgement, based on policy:\nhost$ ./stories -t 2 -s \"doctor service\" |more Found story subject: \"doctor service\" Stories of type 2 only\nPOSITIVE NEGATIVE\nINTERIOR CONTENTED DISTRESSED\nEXTERIOR LIKE DISLIKE\nPREDICTION OPTIMISTIC PESSIMISTIC\nStates like distress are easy to measure in a finite system. When a threshold of behaviour is reached and a system is unable to keep its promises, e.g. the thrashing to empty a queue. Emotions could be semanticized versions of these major conditions.\nStates like good, bad, danger, happy, sad, etc are components of what we would think of as emotional states. These give clear meanings to other measurements: how we respond to a context (state) and the associations we make during good and bad times affect how we recall concepts later. If we feel strongly about something (good or bad) this translates into an importance rank of an association. We want to sum the recurrence scores of the concepts to label their importance. It is possible that we might even be able to make all of these by combining the simultaneous\nactivation of senses with the concepts of good and bad: e.g.\nGood/bad \u2229 person \u2192 love/hate (13) Good/bad \u2229 senses \u2192 happy/sad (14)\nGood/bad \u2229 comparison \u2192 true/false (15)\nThis remains a topic for future exploration. I hope to return to all these issues, in the context of applications, in future work.\nAcknowledgement: I am deeply indebted to Steve Pepper for in depth discussions about linguistics and knowledge representations. I am also grateful to Nikos Anerousis, Anup Kalia, Maja Vukovic, and Jin Xiao for helpful conversations."}], "references": [{"title": "In Search of Certainty: the science of our information infrastructure", "author": ["M. Burgess"], "venue": "Xtaxis Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "New Research on Knowledge Management Models and Methods., chapter What\u2019s wrong with knowledge management", "author": ["M. Burgess"], "venue": "The emergence of ontology. InTech,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Compass and direction in topic maps", "author": ["A. Couch", "M. Burgess"], "venue": "(Oslo University College preprint)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Spacetimes with semantics (i)", "author": ["M. Burgess"], "venue": "http://arxiv.org/abs/1411.5563", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Spacetimes with semantics (ii)", "author": ["M. Burgess"], "venue": "http://arxiv.org/abs/1505.01716", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Spacetimes with semantics (iii)", "author": ["M. Burgess"], "venue": "http://arxiv.org/abs/1608.02193", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Promise Theory: Principles and Applications", "author": ["J.A. Bergstra", "M. Burgess"], "venue": "\u03c7tAxis Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A site configuration engine", "author": ["M. Burgess"], "venue": "Computing systems (MIT Press: Cambridge MA), 8:309", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Computer immunology", "author": ["M. Burgess"], "venue": "Proceedings of the Twelth Systems Administration Conference (LISA XII) (USENIX Association: Berkeley, CA), page 283", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Cellibrium project", "author": ["M. Burgess"], "venue": "https://github.com/markburgess/Cellibrium", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A taxonomy, dataset, and classifier for automatic noun compound interpretation", "author": ["Stephen Tratz", "Eduard Hovy"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "editors", "author": ["M. Curd", "J.A. Cover"], "venue": "Philosophy of Science: The Central Issues. Norton", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Errors of Observation and their Treatment", "author": ["J. Topping"], "venue": "Chapman and Hall", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1972}, {"title": "Measurement and Uncertainty (Methods and Applications)", "author": ["R.H. Dieck"], "venue": "Instrument, Systems and Automation Society, Triangle Park, North Carolina, third edition edition", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Quantum Kinematics and Dynamics", "author": ["J. Schwinger"], "venue": "Addison Wesley", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1970}, {"title": "Introduction to Graph Theory (2nd Edition)", "author": ["D.B. West"], "venue": "(Prenctice Hall, Upper Saddle River)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "The Theory of Graphs", "author": ["C. Berge"], "venue": "(Dover, New York)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Small Worlds", "author": ["D.J. Watts"], "venue": "(Princeton University Press, Princeton)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Place cells", "author": ["M.B. Moser", "D.C. Rowland", "E.I. Moser"], "venue": "grid cells, and memory. Perspectives in Biology", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The Unfolding of Language", "author": ["G. Deutsche"], "venue": "Academic Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Cognitive Grammar", "author": ["R.W. Langacker"], "venue": "A Basic Introduction. Oxford, Oxford", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Encyclopedia of Library and Information Sciences 4th Ed", "author": ["S. Pepper"], "venue": "chapter Topic Maps. CRC Press, ISBN 9780849397127", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A graph theoretical model of computer security: from file access to social engineering", "author": ["M. Burgess", "G. Canright", "K. Eng\u00f8"], "venue": "International Journal of Information Security, 3:70\u201385", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Semantic hashing using tags and topic modeling", "author": ["Qifan Wang", "Dan Zhang", "Luo Si"], "venue": "In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Constructing a Language: A Usage-Based Theory of Language Acquisition", "author": ["M. Tomasello"], "venue": "Harvard University Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Lifelong learning for sentiment classification", "author": ["Z. Chen", "N. Ma", "B. Liu"], "venue": "Proceedings of the 53 Annual meeting of the Association of Computational Linguistics, pages 26\u201331", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Topic modeling using topics from many domains", "author": ["Z. Chen", "B. Liu"], "venue": "lifelong learning and big data. In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic anomaly detection in distributed computer networks", "author": ["M. Burgess"], "venue": "Science of Computer Programming, 60(1):1\u201326", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Measuring host normality", "author": ["M. Burgess", "H. Haugerud", "T. Reitan", "S. Straumsnes"], "venue": "ACM Transactions on Computing Systems, 20:125\u2013160", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Human-understandable inference of causal relationships", "author": ["A. Couch", "M. Burgess"], "venue": "Proc. 1st International Workshop on Knowledge Management for Future Services and Networks, Osaka, Japan", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "A tiny overview of cfengine: convergent maintenance agent", "author": ["M. Burgess"], "venue": "Proceedings of the 1st International Workshop on Multi-Agent and Robotic Systems, MARS/ICINCO", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "The mathematical theory of communication", "author": ["C.E. Shannon", "W. Weaver"], "venue": "University of Illinois Press, Urbana", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1949}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "(J.Wiley & Sons., New York)", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1991}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, page 171180", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G Zweig"], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Looking for hyponyms in vector space", "author": ["Marek Rei", "Ted Briscoe"], "venue": "In CoNLL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Reasoning about linguistic regularities in word embeddings using matrix manifolds", "author": ["Sridhar Mahadevan", "Sarath Chandar"], "venue": "CoRR, abs/1507.07636,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Adaptive locks for frequently scheduled tasks with unpredictable runtimes", "author": ["M. Burgess", "D. Skipitaris"], "venue": "Proceedings of the Eleventh Systems Administration Conference (LISA XI) (USENIX Association: Berkeley, CA), page 113", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "In this dynamical approach, criteria like semantic and dynamic stability of the stories are now more important than logical idealizations like \u2018truth\u2019 or ad hoc determinations of \u2018correctness\u2019 [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 2, "context": "Narration, or storytelling, is an underestimated aspect of intelligent behaviour [2]\u2013[4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "This work summarizes a model of invariant storytelling, based on spacetime inference [5]\u2013[7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "This work summarizes a model of invariant storytelling, based on spacetime inference [5]\u2013[7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "Promise theory [8] motivates a graph theoretical approach to semantic scaling [6], which is based on the observation that cognitive relationships ultimately derive from elementary spacetime relationships.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "Promise theory [8] motivates a graph theoretical approach to semantic scaling [6], which is based on the observation that cognitive relationships ultimately derive from elementary spacetime relationships.", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "Based on the considerations in [7], we expect a number of information processing stages in a cognitive learning system (figure 1):", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "These latter stages have much in common with linguistics [7]; indeed, once one can map invariant patterns into a finite alphabet of tokens, every reasoning problem maps to a linguistic problem.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "A simple prototype system, to explore the underlying principles, has been reconstructed from the earlier cognitive approaches applied to pervasive computing management in CFEngine [9], [10], and further modernized and extended in the Cellibrium project [11].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "A simple prototype system, to explore the underlying principles, has been reconstructed from the earlier cognitive approaches applied to pervasive computing management in CFEngine [9], [10], and further modernized and extended in the Cellibrium project [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 9, "context": "A simple prototype system, to explore the underlying principles, has been reconstructed from the earlier cognitive approaches applied to pervasive computing management in CFEngine [9], [10], and further modernized and extended in the Cellibrium project [11].", "startOffset": 253, "endOffset": 257}, {"referenceID": 5, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "This includes specialized sensors that tokenize, or dimensionally reduce, information intensive data into compressed conceptual representations [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "The hypothesis here is that such ideas are naturally associated with the the most invariant aspects of space and time, leading to four basic kinds of conceptual association, as well as the basic ability to compress extensive spacetime datasets into simple tokenized concepts [7].", "startOffset": 275, "endOffset": 278}, {"referenceID": 3, "context": "This is what we mean by the naming of concepts [5]1.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "This is discussed at length in [7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "quantum mechanics, but surprisingly little attention has been given to the semantics of data, and cognitive perception, except in the case of experimental error [13]\u2013[15].", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "quantum mechanics, but surprisingly little attention has been given to the semantics of data, and cognitive perception, except in the case of experimental error [13]\u2013[15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "Measurables may have the property of \u2018compatibility\u2019 [16], meaning that measurement of one does not influence the measurement of the other.", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "This \u2018prism\u2019 separates data into a spectrum of associative types, which theory predicts to represent different spacetime characteristics, called the irreducible types [7] (see figure I).", "startOffset": 167, "endOffset": 170}, {"referenceID": 15, "context": "This is not regular in the manner of a Cayley tree: the result is closer to a \u2018semantic small worlds\u2019 graph [17]\u2013[19].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "This is not regular in the manner of a Cayley tree: the result is closer to a \u2018semantic small worlds\u2019 graph [17]\u2013[19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "As argued in [7], the fundamental basis for conceptual association has its origins in the structure of spacetime, represented by four types of relationship \u03c4 (being close in space and time, being in a bounded area, etc).", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "built into the functional nature of the agent, by specialized eyes and ears, face cells and place cells, and so on [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "However, it could be that this is the role of emotions: sensations as complex as sensory perception, but without strong rational linkage [10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 5, "context": "TABLE I: Examples of the four irreducible association types, characterized by their spacetime origins, from [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "However, we should also be cautious that informal association of linguistic metaphor also leads to confusions about the appropriate classification of meaning under the irreducible types, as interpretation by metaphor is fluid in human language [21].", "startOffset": 244, "endOffset": 248}, {"referenceID": 4, "context": "aggregation, as described in [6].", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "With a scaled approach, one combines the names of interior promises to yield a compound name for a collective concept, and exterior promises then express the associations between the composite concepts [6].", "startOffset": 202, "endOffset": 205}, {"referenceID": 20, "context": "The scaling is compatible with patterns observed in cognitive linguistics [22], indicating that language, as we understand it, forms a good network structure for concept representation5.", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "If we group aggregated properties by a linking them centrally to a singular basis agent [5], [7], then attribute expression is effectively the inverse of set membership, and the basis agent alone represents the collective ensemble.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "If we group aggregated properties by a linking them centrally to a singular basis agent [5], [7], then attribute expression is effectively the inverse of set membership, and the basis agent alone represents the collective ensemble.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "The scaling of conceptual agency, in these representations, is a direct application of the promise theory [6], summarized here according to the four irreducible spacetime attributes elucidated in [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "The scaling of conceptual agency, in these representations, is a direct application of the promise theory [6], summarized here according to the four irreducible spacetime attributes elucidated in [7].", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "In earlier work [5], I showed how a promise view of structure motivated an identification of concepts as spanning sets, or matroids over a labelled graph.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "This offers a scalable approach to spanning concepts [5], with upwardly extensible uniqueness.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Such clusters may overlap without limit, thus this is not a mutually exclusive branching process [3]7.", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "Through the various experiments conducted during the course of this work, from topic maps [23] to the present model, it seems that the appropriate place for agent role in a network lies in the associations between pairs of concepts, rather than in the concepts themselves.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The resulting network is too sparse to allow percolation and hence propagation of storyline [7], [24].", "startOffset": 92, "endOffset": 95}, {"referenceID": 22, "context": "The resulting network is too sparse to allow percolation and hence propagation of storyline [7], [24].", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "This approach is used in natural language processing [25] with some claimed success.", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "Mirroring the proposed usage approach of language formation in cognitive linguistics [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "Striking the balance between a context that strives for a coordinatized precision and one that is maximally invariant (for repeated \u2018context-free\u2019 use) is the central challenge of building an addressing system for semantics and knowledge [7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 5, "context": "\u2022 All associations belong to one of the four spacetime association types [7]:", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "grouping of indistinguishable members [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Because of the recursive nature of concepts, the promise model of scaled agency in [6] suggests that it will be helpful to distinguish between interior and exterior context of a concept.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "The first of these is somewhat analogous to a quantum path integral [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "Thus, the two cognitive \u2018awareness\u2019 channels have different semantic roles [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "We therefore have two channels evolving in parallel: a short term memory of context and a long term memory of qualified concepts [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 25, "context": "When machine learning is applied to document processing, story fragments are treated as patterns to be recognized, and the relevant forward pathways are determined based on statistical ensemble support rather than semantic relevance [27], [28].", "startOffset": 233, "endOffset": 237}, {"referenceID": 26, "context": "When machine learning is applied to document processing, story fragments are treated as patterns to be recognized, and the relevant forward pathways are determined based on statistical ensemble support rather than semantic relevance [27], [28].", "startOffset": 239, "endOffset": 243}, {"referenceID": 27, "context": "Each collector, with its independent semantics, transmutes sample data into a separate invariant representation and learns its significant patterns and states (for monitoring, see [29]).", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": "The receiver always has the last word in interpretation however [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "Prototype code for creating and parsing the knowledge representation, is shared under the Cellibrium [11] project.", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "This is analogous to the original CFEngine classes [9], where context was a collection of class strings that were probed from the environment by specialized software sensors.", "startOffset": 51, "endOffset": 54}, {"referenceID": 28, "context": "Fortunately, studies have shown that meaningful patterns from system monitoring behaviours are few in number, compared to a bulk of non-meaningful noise [30].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "Topic map inspired structures [23] created mainly led to trivial stories, so the project was set aside for almost 4 years.", "startOffset": 30, "endOffset": 34}, {"referenceID": 1, "context": "Following this hiatus, the models based on topic maps were abandoned [3], and ideas from the promise theoretic approach [5] were revived.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Following this hiatus, the models based on topic maps were abandoned [3], and ideas from the promise theoretic approach [5] were revived.", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "The earliest attempts at discovering emergent connections, were implemented by my collaborator Alva Couch [4], [31], and used a shortest path approach to selecting a unique \u2018route\u2019 between two concepts: initial and final boundary conditions.", "startOffset": 106, "endOffset": 109}, {"referenceID": 29, "context": "The earliest attempts at discovering emergent connections, were implemented by my collaborator Alva Couch [4], [31], and used a shortest path approach to selecting a unique \u2018route\u2019 between two concepts: initial and final boundary conditions.", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "The propagation of these types in inferences is defined by the spacetime classifications themselves [7], and no matching is required.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "See [5], [7] for a discussion of the grammatical complexity of spacetime associations.", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "See [5], [7] for a discussion of the grammatical complexity of spacetime associations.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Context, as modelled in the original CFEngine representation [9], [32], and its derived Cellibrium implementation [11], is a non-ordered linear list composed of many atomic characterizations.", "startOffset": 61, "endOffset": 64}, {"referenceID": 30, "context": "Context, as modelled in the original CFEngine representation [9], [32], and its derived Cellibrium implementation [11], is a non-ordered linear list composed of many atomic characterizations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Context, as modelled in the original CFEngine representation [9], [32], and its derived Cellibrium implementation [11], is a non-ordered linear list composed of many atomic characterizations.", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "This was measured by the size of the overlap between current context and learned context (analogous to the mutual information transfer [33], [34]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "This was measured by the size of the overlap between current context and learned context (analogous to the mutual information transfer [33], [34]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 30, "context": "only the length of a very specific boolean expression [32].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "13The original approach used by CFEngine was to simulate the \u2018neuronal dead-time\u2019 into concepts, eliminating loops by virtue of concepts having fired already [39].", "startOffset": 158, "endOffset": 162}, {"referenceID": 33, "context": "Such approaches have been used to discover vector congruences and emergent functional semantics, but only with extensive training [35]\u2013[38].", "startOffset": 130, "endOffset": 134}, {"referenceID": 36, "context": "Such approaches have been used to discover vector congruences and emergent functional semantics, but only with extensive training [35]\u2013[38].", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": "Knowledge is very much in the eye of the beholder, and every observer forms an individual ontology [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "The extent to which such a network accumulated, and made predictive, depends on the extent to which its spatial structure can mimic a representation of the semantics one happens to be looking for, with only a single trigger [7].", "startOffset": 224, "endOffset": 227}, {"referenceID": 8, "context": "In [10], I speculated on the role of emotional state in determining context.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "From the viewpoint in [7], emotions seem to play the role of very coarse aggregate \u2018concepts\u2019 stimulated by sensory/introspective inputs.", "startOffset": 22, "endOffset": 25}], "year": 2017, "abstractText": "In modern machine learning, pattern recognition replaces realtime semantic reasoning. The mapping from input to output is learned with fixed semantics by training outcomes deliberately. This is an expensive and static approach which depends heavily on the availability of a very particular kind of prior training data to make inferences in a single step. Conventional semantic network approaches, on the other hand, base multi-step reasoning on modal logics and handcrafted ontologies, which are ad hoc, expensive to construct, and fragile to inconsistency. Both approaches may be enhanced by a hybrid approach, which completely separates reasoning from pattern recognition. In this report, a quasi-linguistic approach to knowledge representation is discussed, motivated by spacetime structure. Tokenized patterns from diverse sources are integrated to build a lightly constrained and approximately scale-free network. This is then be parsed with very simple recursive algorithms to generate \u2018brainstorming\u2019 sets of reasoned knowledge.", "creator": "TeX"}}}