{"id": "1702.06709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings", "abstract": "fine - tasting grained entity metadata type classification ( * fetc ) is the task activity of quickly classifying across an entity labeled mention to match a broad defined set of tag types. distant supervision paradigm use is extensively now used especially to generate training document data targeted for this task. colloquially however, generated training data always assigns often same set usage of labels to every mention typical of witnessing an entity without considering it its local context. existing fetc sensor systems have two major drawbacks : assuming entity training sensor data to respectively be noise free subjects and continual use of hand crafted features. increasingly our collaborative work overcomes both drawbacks. we thereby propose a neural input network transformation model paradigm that repeatedly jointly learns entity thing mentions and emphasizes their verbal context representation to eliminate use testing of hand crafted features. our systemic model treats training data assessed as noisy contexts and uses proposed non - semantic parametric variant detection of fixed hinge sensory loss function. experiments show that substantially the proposed bi model outperforms matched previous state - trial of - the - scale art neural methods observed on two corresponding publicly available sensor datasets, namely info figer ( gold ) and / bbn with an average relative improvement of 2. 69 % more in our micro - analytical f1 score. knowledge learnt driven by our meta model constituents on one dataset can be transferred gradually to join other datasets while using same model processes or other fetc systems. these approaches of transferring knowledge further improve the performance of respective semantic models.", "histories": [["v1", "Wed, 22 Feb 2017 08:59:37 GMT  (958kb,D)", "http://arxiv.org/abs/1702.06709v1", "11 pages, 5 figures, accepted at EACL 2017 conference"]], "COMMENTS": "11 pages, 5 figures, accepted at EACL 2017 conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["abhishek", "ashish anand", "amit awekar"], "accepted": false, "id": "1702.06709"}, "pdf": {"name": "1702.06709.pdf", "metadata": {"source": "CRF", "title": "Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings", "authors": ["Abhishek", "Ashish Anand", "Amit Awekar"], "emails": ["awekar}@iitg.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "Entity type classification is the task for assigning types or labels such as organization, location to entity mentions in a document. This classification is useful for many natural language processing (NLP) tasks such as relation extraction (Mintz et al., 2009), machine translation (Koehn et al.,\n2007), question answering (Lin et al., 2012) and knowledge base construction (Dong et al., 2014).\nThere has been considerable amount of work on Named Entity Recognition (NER) (Collins and Singer, 1999; Tjong Kim Sang and De Meulder, 2003; Ratinov and Roth, 2009; Manning et al., 2014), which classifies entity mentions into a small set of mutually exclusive types, such as Person, Location, Organization and Misc. However, these types are not enough for some NLP applications such as relation extraction, knowledge base construction (KBC) and question answering. In relation extraction and KBC, knowing fine-grained types for entities can significantly increase the performance of the relation extractor (Ling and Weld, 2012; Koch et al., 2014; Mitchell et al., 2015) since this helps in filtering out candidate relation types that do not follow the type constrain. Fine-grained entity types provide additional information while matching questions to its potential answers and significantly improves performance (Dong et al., 2015). For example, Li and Roth (2002) rank questions based on their expected answer types (will the answer be food, vehicle or disease).\nTypically, FETC systems use over hundred labels, arranged in a hierarchical structure. An important aspect of FETC is that based on local context, two different mentions of same entity can have different labels. We illustrate this through an example in Figure 1. All three sentences S1, S2, and S3 mention same entity Barack Obama. However, looking at the context, we can infer that S1 mentions Obama as a person/author, S2 mentions Obama only as a person, and S3 mentions Obama as a person/politician.\nAvailable training data for FETC has noisy labels. Creating manually annotated training data for FETC is time consuming, expensive, and error prone. Note that, a human annotator will\nar X\niv :1\n70 2.\n06 70\n9v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nhave to assign a subset of correct labels from a set of around hundred labels for each entity mention in the corpus. Existing FETC systems use distant supervision paradigm (Craven and Kumlien, 1999) to automatically generate training data. Distant supervision maps each entity in the corpus to knowledge bases such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007), YAGO (Suchanek et al., 2007). This method assigns same set of labels to all mentions of an entity across the corpus. For example, Barack Obama is a person, politician, lawyer, and author. If a knowledge base has these four matching labels for Barack Obama, then distant supervision assigns all of them to every mention of Barack Obama. Training data generated with distant supervision will fail to distinguish between mentions of Barack Obama in sentences S1, S2, and S3.\nExisting FETC systems have one or both of following drawbacks:\n1. Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)\n2. Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)\nWe have observed that for real world datasets, more than twenty five percent of training data has noisy labels. First drawback propagates this noise in training data to the FETC model. To extract hand crafted features various NLP tools are used. Since errors inevitably exist in such tools, the second drawback propagates errors of these tools to FETC model.\nWe propose a neural network based model to overcome the two drawbacks of existing FETC systems. First, we separate training data into clean and noisy partitions using the same method as in AFET system (Ren et al., 2016). For these parti-\ntions, we use simple yet effective non-parametric variant of hinge loss function while training. To avoid use of hand crafted features, we learn representations for given entity mention and its context.\nAdditionally, we investigate effectiveness of using transfer learning (Pratt, 1993) for FETC task both at feature and model level. We show that feature level transfer learning can be used to improve performance of other FETC system such as AFET, by up to 4.5% in micro-F1 score. Similarly, model level transfer learning can be used to improve performance of the same model using different dataset by up to 3.8% in micro-F1 score.\nOur contributions can be summarized as follows:\n1. We propose a simple neural network model that learns representations for entity mention and its context, and incorporate noisy label information using a variant of non-parametric hinge loss function. Experimental results on two publicly available datasets demonstrate the effectiveness of proposed model, with an average relative improvement of 2.69% in micro-F1 score.\n2. We investigate the use of feature level and model level transfer-learning strategies in the domain of the FETC task. The proposed transfer learning strategies further improve the state-of-the-art on BBN dataset by 3.8% in micro-F1 score."}, {"heading": "2 Related Work", "text": "Ling et al. (2012) proposed the first system for FETC task, which used 112 overlapping labels. They used linear classifier perceptron for multilabel classification. Yosef et al. (2012) used multiple binary SVM classifiers in a hierarchy, to classify an entity mention to a set of 505 types. While the initial work assumed that all labels present in a training dataset for an entity mention are correct, Gillick et al. (2014) introduced context dependent FETC and proposed a set of heuristics for pruning labels that might not be relevant given the entity mention\u2019s local context. Yogatama et al. (2015) proposed an embedding based model where userdefined features and labels were embedded into a low dimensional feature space to facilitate information sharing among labels.\nShimaoka et al. (2016) proposed an attentive neural network model that used LSTMs to encode entity mention\u2019s context and used an atten-\n(a) \u03b1 models label-label correlation. Higher the \u03b1, lower is the margin between noncorrelation labels. (b) During inference, labels above this threshold are predicted as positive.\nFigure 2: Effect of change of parameters on AFET\u2019s performance.\ntion mechanism to allow the model to focus on relevant expressions in the entity mention\u2019s context. However, the model assumed that all labels obtained via distant supervision are correct. In contrast, our model does not assume that all labels are correct. To learn entity representation, we propose a scheme which is simpler yet more effective.\nMost recently, Ren et al. (2016) have proposed AFET, an FETC system. AFET separates the loss function for clean and noisy entity mentions. AFET uses label-label correlation information obtained by given data in its parametric loss function (model parameter \u03b1). During inference, AFET uses a threshold to separate positive types from negative types (similarity threshold parameter d). However, AFET\u2019s loss function is sensitive to change in parameters, which are data dependent. Figure 2 shows the effect of parameter \u03b1 and d, on AFET performance evaluated on different datasets. In contrast, our model uses a simple yet effective variant of hinge loss function. This function does not need to tune the similarity threshold.\nTransfer learning is well applied to many NLP applications, such as cross-domain document classification (Shi et al., 2010), multi-lingual word clustering (Ta\u0308ckstro\u0308m et al., 2012) and sentiment classification (Mou et al., 2016). Initialization of word vectors with pre-trained word vectors in neural network models can be considered as one of the best example of transfer learning in NLP. Wang et al. (2015) provide a broad overview of transfer learning techniques used for language processing."}, {"heading": "3 The Proposed Model", "text": ""}, {"heading": "3.1 Problem description", "text": "Our task is to automatically classify type information of entity mentions present in natural language\nsentences. Figure 3 shows a general overview of our proposed approach. Input: The input to the model is a training and testing corpus consisting of a set of sentences on which entity mentions have been identified. In training corpus, every entity mention will have corresponding labels according to a given hierarchy. Formally, a training corpus Dtrain consists of a set of sentences, S = {si}Ni=1. Each sentence si will have one or more entity mentions denoted by mij,k, where j and k denotes indices of start and end tokens, respectively. SetM consists of all the entity mentions mij,k. For every entity mention mij,k, there will be a corresponding label vector lij,k \u2208 {0, 1}K , which is a binary vector, where lij,kt = 1 if t\nth type is true otherwise it will be zero. K denotes the total number of labels in a given hierarchy \u03a8. Testing corpus Dtest will only contain sentences and entity mentions. Output: For entity mentions in testing corpus Dtest, predict their corresponding labels."}, {"heading": "3.2 Training set partition", "text": "Similar to AFET, we partition the mention setM of training corpus Dtrain into two parts, a setMc, consisting only of clean entity mentions and a set Mn, consisting only of noisy entity mentions. An entity mention mij,k is said to be clean if its labels lij,k belong to only a single path (not necessary to be leaf) in the hierarchy \u03a8, that is its labels are not ambiguous; otherwise, it is noisy. For example, as per hierarchy given in figure 1, an entity mention with labels person, artist and politician will be considered as noisy, whereas entity mention with labels person, artist and actor will be considered as clean."}, {"heading": "3.3 Feature representations", "text": "Mention representation: This representation captures information about entity mention\u2019s morphology and orthography. We decompose an entity mention into character sequence, and use a vanilla LSTM encoder (Hochreiter and Schmidhuber, 1997) to encode character sequences to a fixed dimensional vector. Formally, for entity mention mij,k, we decompose it into a sequence of character tokens cij,k1 , c i j,k2\n, . . . ,cij,k|mi j,k | , where |mij,k|\ndenotes the total number of characters present in the entity mention. For entity mention containing multiple tokens, we join these tokens with a space in between tokens. Every character will have corresponding vector representation in a lookup table for characters. The character sequence is then fed one by one to a LSTM encoder, and the final output is used as a feature representation for entity mention mij,k. We denote this process by a function Fm : M \u2192 RDm , where Dm is the number of dimensions for mention representation. The whole process is illustrated in figure 4 (Mention representation). Context representation: This representation captures information about the context surrounding the entity mention. Context representation is further divided into two parts, left and right context representation. The left context consists of a sequence of tokens within a sentence from the start of a sentence till the last token of entity mention. The right context consists of a sequence\nof tokens from the start of entity mention till the end of a sentence. We use bi-directional LSTM encoders (Graves et al., 2013) to encode token level sequences of both context to a fixed dimensional vector. Formally, for an entity mentionmij,k present in a sentence si, decompose si into a sequence of tokens si1, s i 2, . . . , s i k for the left context, and sij , s i j+1, . . . , s i |si| for the right context, where |si| denotes the number of tokens in the sentence. Every token will have a corresponding vector representation in a lookup tables for token. The token sequence is then fed one by one to a bi-directional LSTM encoder, and the final output will be used as feature representation. We denote this whole process by function Flc : (M,S) \u2192 RDlc for computing left context and Frc : (M,S) \u2192 RDrc for computing right context. Dlc andDrc are the number of dimensions for the left context and the right context representation, respectively. The whole process is illustrated in figure 4 (Left and right context representation).\nThe context representation described above is slightly different from what was proposed in (Shimaoka et al., 2016), here we include entity mention tokens within both left and right context, to explicitly encode context relative to an entity mention.\nIn the end, we concatenate entity mention and its context representation into a single Df dimensional vector, where Df = Dm + Dlc + Drc. This complete process is denoted by a function\nF : (M,S)\u2192 RDf given by:\nF (mij,k, s i) = Fm(m i j,k)\u2295 Flc(mij,k, si)\u2295 Frc(mij,k, si) (1) where \u2295 denotes vector concatenation. For brevity, we will now omit the use of subscript j, k from mij,k and l i j,k, and will use f\ni to denote feature representation for entity mention and its context obtained via equation 1."}, {"heading": "3.4 Feature and label embeddings", "text": "Similar to Yogatama et al. (2015) and Ren et al. (2016), we embed feature representations and labels in a same dimensional space such that an object is embedded closer to the objects that share similar types than the objects that do not. Formally, we are trying to learn linear mapping functions \u03c6M : RDf \u2192 RDe and \u03c6L : RDK \u2192 RDe , where De is the size of embedding space. These mappings are given by:\n\u03c6M(f i) = f i T U ; \u03c6L(l i t) = l iT t V (2)\nwhere, U \u2208 RDf\u00d7De and V \u2208 RDK\u00d7De are projection matrices for features representations and type labels respectively and lit is one-hot vector representation for label t. We assign a score to each label type t and feature vector as a dot product of their embeddings. Formally, we denote a score as:\ns(f i, lit) = \u03c6M(f i) \u00b7 \u03c6L(lit) (3)"}, {"heading": "3.5 Optimization", "text": "We use two different loss functions to model clean and noisy entity mentions. For clean entity mentions, we use a hinge loss function. The intuition is simple: maintain a margin, centered at zero, between positive and negative type scores. The scores are computed by similarity between an entity mention and label types (eq. 3). Hinge loss function has two advantages. First, it intuitively seprates positive and negative labels during inference. Second, it is independent of data dependent parameter. Formally, for a given entity mention mi and its label li we compute the associated loss as given by:\nLc(m i, li) = \u2211 t\u2208\u03b3 max(0, 1\u2212 s(mi, lit))\n+ \u2211 t\u2208\u03b3\u0304 max(0, 1 + s(mi, lit)) (4)\nwhere \u03b3 and \u03b3\u0304 are set of indices that have positive and negative labels respectively.\nFor noisy entity mentions, we propose a variant of a hinge loss where, like Lc, score for all negative labels should go below \u22121. However, for positive labels, as we don\u2019t know which labels are relevant to entity mention\u2019s local context, we propose that the maximum score from the set of given positive labels should be greater than one. This maintains a margin between all negative types and the most relevant positive type. Formally, noisy label loss, Ln is defined as:\nLn(m i, li) = \u2211 t\u2208\u03b3\u0304 max(0, 1 + s(mi, lit))\n+ max(0, 1\u2212 s(mi, lit\u2217)); t\u2217 = arg max\nt\u2208\u03b3 s(mi, lit) (5)\nAgain, using this loss function makes it intuitive to set a threshold of zero during inference.\nThese loss functions are different from the loss functions used in (Yogatama et al., 2015; Ren et al., 2016) in a way that, we make strict absolute criteria to distinguish between positive and negative labels. Whereas in (Yogatama et al., 2015; Ren et al., 2016) positive labels should have a higher score than negative labels. As their scoring is relative, the final result varies on the threshold used to separate positive and negative labels.\nTo train the partitioned dataset together, we formulate the joint objective problem as:\nmin \u03b8 O = \u2211 m\u2208Mc Lc(m, l) + \u2211 m\u2208Mn Ln(m, l) (6)\nwhere \u03b8 is the collection of all model parameters that needs to be learned. To jointly optimize the objective O, we use Adam (Kingma and Ba, 2014), a stochastic gradient-based optimization algorithm."}, {"heading": "3.6 Inference", "text": "For every entity mention in setM from Dtest, we perform a top-down search in the given type hierarchy \u03a8, and estimate the correct type path \u03a8\u2217. Starting from the tree root, we recursively compute the best type among node\u2019s children by computing its score with obtained feature representations. We select the node that has maximum score among other nodes. We continue this process till a leaf node is encountered or the score associated\nwith a node falls below an absolute threshold zero. The thresold is fixed across all datasets used."}, {"heading": "3.7 Transfer learning", "text": "We want to investigate, whether the feature representations learnt for an entity mention are useful. We study what contribution these feature representations make to an existing feature engineering based method such as AFET. We learn the proposed model on one training dataset, namely Wiki dataset, which has the highest number of entity mentions among other datasets and use this model to generate representations that is F (mij,k, s\ni) for another training and testing data. These representations, which are Df dimensional vectors, are used as feature for an existing state-of-the-art model, AFET, in place of the hand-crafted features that were originally used. AFET model is then trained using these feature representations. We call this as feature level transfer learning. On the other hand, we also evaluate model level transfer learning, where we initialize weights of LSTM encoders for a new dataset with the weights learnt from the model trained on another dataset, namely Wiki dataset."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets used", "text": "We evaluate the proposed model on three publicly available datasets, provided in a pre-processed tokenized format by Ren et al. (2016). Statistics of the datasets used in this work are shown in Table 1. The details of the datasets are as follows: Wiki/FIGER(GOLD): The training data consists of Wikipedia sentences and was automatically generated in distant supervision paradigm, by mapping hyperlinks in Wikipedia articles to Freebase. The test data, mainly consisting of sentences from news reports, was manually annotated as described in (Ling and Weld, 2012). OntoNotes: OntoNotes dataset consists of sentences from newswire documents present in OntoNotes text corpus (Weischedel et al., 2013). DBpedia spotlight (Daiber et al., 2013) was used to automatically link entity mention in sentences to Freebase. For this corpus, manually annotated test data was shared by Gillick et al. (2014). BBN: BBN dataset consists of sentences from Wall Street Journal articles and is completely manually annotated (Weischedel and Brunstein, 2005). Please refer to (Ren et al., 2016) for more details\nof the datasets."}, {"heading": "4.2 Evaluation settings", "text": ""}, {"heading": "4.2.1 Baselines", "text": "We compared the proposed model with state-of-the-art entity classification methods2: (1) FIGER (Ling and Weld, 2012); (2) HYENA (Yosef et al., 2012); (3) AFETNoCo (Ren et al., 2016): AFET without data based label-label correlation modeled in loss function; (4) AFET-CoH (Ren et al., 2016): AFET with hierarchy based label-label correlation modeled in loss function; (5) AFET (Ren et al., 2016); (6) Attentive (Shimaoka et al., 2016): An attentive neural network based model.\nWe compare these baselines with variants of our proposed model: (1) our: complete model; (2) our-AllC assuming all mentions are clean; (3) our-NoM without mention representation."}, {"heading": "4.2.2 Experimental setup", "text": "We use Accuracy or Strict-F1 score, Macroaveraged F1 score, and Micro-averaged F1 score as metrics for evaluation. Existing methods for FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016). We removed entity mentions that do not have any label in training as well as test set. We also remove entity mentions that have spurious indices (i.e entity mention length of 0).3 For all the three datasets, we randomly sampled 10% of the test set, and use it as a development set, on which we tune model parameters. The remaining 90% is used for final evaluation. For all our experiments, we train each model using same hyperparameters five times and report their performance in terms of micro-F1 score on the development set as\n1We considered an entity mention as pronominal, if all of its tokens have POS tag as pronoun.\n2Whenever possible, the baselines result are reported from (Ren et al., 2016), otherwise we re-implemented baseline methods based on description available in corresponding papers.\n3The code to replicate the work is available at https: //github.com/abhipec/fnet\nshown in Figure 5. On Wiki dataset, we observed a large variance in performance as compared to other two datasets. This might be because of the fact that Wiki dataset has a very small development set. From each of these five runs, we pick the best performing model based on the development set and report its result on the test set. Hyperparameter setting: All the neural network based models in this paper used 300 dimensional pre-trained word embeddings distributed by Pennington et al. (2014). The hidden-layer size of word level bi-directional LSTM was 100, and that of character level LSTM was 200. Vectors for character embeddings were randomly initialized and were of size 200. We use dropout with the probability of 0.5 on the output of LSTM encoders. The embedding dimension used was 500. We use Adam (Kingma and Ba, 2014) as optimization method with learning rate of 0.0005-0.001 and mini-batch size in the range of 800 to 1500. The proposed model and some of the baselines were implemented using TensorFlow4 framework."}, {"heading": "4.3 Transfer learning", "text": "In feature level transfer learning, we use the best performing proposed model trained on Wiki dataset to generate representations that is Df dimensional vector for every entity mention present in the train, development, and test set of the BBN and the OntoNotes dataset. Figure 4 illustrates an example for the encoding process. Then we use these representations as a feature vector in place of the user-defined features and train the AFET\n4http://tensorflow.org/\nmodel. Its hyper-parameters were tuned on the development set. These results are shown in table 2 as feature level transfer-learning.\nIn model level transfer learning, we use the learnt weights of LSTM encoders from the best performing proposed model trained on Wiki dataset and initialize the LSTM encoders of the same model with these weights while training on BBN and OntoNotes datasets. These results are shown in table 2 as model level transfer learning."}, {"heading": "4.4 Performance comparison and analysis", "text": "Table 2 shows the results of the proposed method, its variants and the baseline methods. Comparison with other feature learning methods: The proposed model and its variants (ourAllC, our-NoM) perform better than the existing feature learning method by Shimaoka et al. (2016) (Attentive), consistently on all datasets. This indicates benefits of the proposed representation scheme and joint learning of representation and label embedding. Comparison with feature engineering methods: The proposed model performs better than the existing feature engineered methods (FIGER, HYENA, AFET-NoCo, AFET-CoH) consistently across all datasets on Micro-F1 and MacroF1 evaluation metrics. These methods do not model label-label correlation based on data. In comparison with AFET, the proposed model outperforms AFET on Wiki and BBN dataset in terms of Micro-F1 evaluation metric. This indicates benefits of feature learning as well as data driven label-label correlation. We do a type-wise perfor-\nmance comparison on OntoNotes dataset in subsection 4.5. Comparison with variants of our model: The proposed model performs better on all dataset as compared to our-AllC in terms of micro-F1 score. However, we find the performance difference on Wiki and OntoNotes dataset is not statistically significant. We investigated it further and found that across all three datasets, there exist only few entity types for which more than 85% of entity mentions are noisy. These types consist of approximately 3- 4% of test set, and our model fails on these types (zero micro-F1 score). However, our-AllC performs relatively well on these types. Examples of such types are: /building, /person/political figure, /GPE/STATE PROVINCE. This indicates two limitations of the proposed model. First, the separating of clean and noisy mentions based on the hierarchy has its own inherent limitation of assuming labels within a path are correct. Second, our model learns better if more clean examples are available at the cost of not learning very noisy types. We will try to address these limitations in our future work. Compared with our-NoM, the proposed model performs slightly better across all datasets in terms of micro-F1 score. Feature level transfer learning analysis: We observed 4.5% performance increase in microF1 score of AFET on BBN dataset, after replacing hand-crafted features with feature representations generated by the proposed model. This indicates usefulness of the learnt feature representations. However, if we repeat the same process with OntoNotes dataset, there is only a subtle change in performance. This is majorly because of the data distribution of OntoNotes dataset is different from\nthat of Wiki dataset. This issue is discussed in the next subsection. Model level transfer learning analysis: In model level transfer learning, sharing knowledge from similar dataset (Wiki to BBN) increases the performance by 3.8% in terms of micro-F1 score. However, sharing knowledge from Wiki to OntoNotes dataset slightly increases the performance by 0.4% in terms of micro-F1 score."}, {"heading": "4.5 Case analysis: OntoNotes dataset", "text": "We observed three things; (i) all models perform relatively poor on OntoNotes dataset compared to their performance on other two datasets; (ii) the proposed model outperforms other models including AFET on the other two datasets, but gave worse performance on OntoNotes dataset; (iii) the two variants of transfer learning significantly improve performance of the proposed model on the BBN dataset but resulted in only a subtle performance change on OntoNotes dataset.\nStatistics of the dataset (Table 1) indicates that presence of pronominal or other kinds of mentions are relatively higher in OntoNotes (6.78% in test set) than the other two datasets (0% in test set). Examples of such mentions are 100 people, It, the director, etc. Table 3 shows 20 randomly sampled entity mentions from test set of OntoNotes datasets. Some of these mentions are very generic and likely to be dependent on\n*These results are from (Ren et al., 2016) that also uses 10% of the test set as development set and the remaining for evaluation.\n\u2021We used the publicly available code distributed by Ren et al. (2016).\n\u2020All of these results are on exact same train, development and test set.\nprevious sentences. As all the methods use features solely based on the current sentence, they fail to transfer cross-sentence boundary knowledge. Removing pronominal mentions from test set increases the performance of all feature learning methods by around 3%.\nNext we analyse where the proposed model is failing as compared to AFET. For this, we look at type-wise performance for the top-10 most frequent types in the OntoNotes test dataset. Results are shown in Table 4. Compared to AFET, the proposed model performs better in all types except other in the top-10 frequent types. The other type, which is dominant in test set (42.6% of entity mentions are of type other) and is a collection of multiple broad subtypes such as product, event, art, living thing, food. Performance of AFET significantly drops (AFET-NoCo) when data-driven label-label correlation is ignored, which indicates that modeling data-driven correlation helps. However, as shown in Figure 2a, the use of label-label correlation depends on appropriate values of parameters which vary from one dataset to another."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a neural network based model for the task of fine-grained entity classification. The proposed model learns representations for entity mention, its context and incorporate label noise information in a variant of non-parametric hinge loss function. Experiments show that the proposed model outperforms existing state-of-the-art models on two publicly available datasets without explicitly tuning data dependent parameters.\nOur analysis indicates the following observations. First, OntoNotes dataset has a different distribution of entity mentions compared with other two datasets. Second, if data distribution is similar, then transfer learning is very helpful. Third, incorporating data-driven label-label correlation helps in the case of labels of mixed types. Fourth, there is an inherent limitation in assuming all labels to be clean if they belong to the same path of the hierarchy. Fifth, the proposed model fails to learn label types that are very noisy.\nFuture work could analyse the effect of label noise reduction techniques on the proposed model, revisiting the definition of clean and noisy labels and modeling label-label correlation in a principled way that is not dependent on dataset specific parameters."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their invaluable and insightful comments. Abhishek is supported by MHRD fellowship, Government of India. We acknowledge the use of computing resources made available from the Board of Research in Nuclear Science (BRNS), Dept. of Atomic Energy (DAE), Govt. of India sponsered project (No.2013/13/8-BRNS/10026) by Dr. Aryabartta Sahu at Department of Computer Science and Engineering, IIT Guwahati."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": "In Proceedings of the 6th International The Semantic Web and 2nd Asian", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Unsupervised models for named entity classification", "author": ["Collins", "Singer1999] Michael Collins", "Yoram Singer"], "venue": "Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,", "citeRegEx": "Collins et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Collins et al\\.", "year": 1999}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven", "Kumlien1999] Mark Craven", "Johan Kumlien"], "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,", "citeRegEx": "Craven et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Craven et al\\.", "year": 1999}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "In Proceedings of the 9th International Conference on Semantic Systems,", "citeRegEx": "Daiber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Dong et al.2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "A hybrid neural model for type classification of entity mentions", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Hong Sun", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Context-dependent fine-grained entity type tagging", "author": ["Gillick et al.2014] Dan Gillick", "Nevena Lazic", "Kuzman Ganchev", "Jesse Kirchner", "David Huynh"], "venue": "arXiv preprint arXiv:1412.1820", "citeRegEx": "Gillick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Type-aware distantly supervised relation extraction with linked arguments", "author": ["Koch et al.2014] Mitchell Koch", "John Gilmer", "Stephen Soderland", "Daniel S. Weld"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Koch et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koch et al\\.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1,", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "No noun phrase left behind: Detecting and typing unlinkable entities", "author": ["Lin et al.2012] Thomas Lin", "Mausam", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Neverending learning", "author": ["E. Platanios", "A. Ritter", "M. Samadi", "B. Settles", "R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling."], "venue": "Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Platanios et al\\.,? 2015", "shortCiteRegEx": "Platanios et al\\.", "year": 2015}, {"title": "Distilling word embeddings: An encoding approach", "author": ["Mou et al.2016] Lili Mou", "Ran Jia", "Yan Xu", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Discriminabilitybased transfer between neural networks", "author": ["Lorien Y. Pratt"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Pratt.,? \\Q1993\\E", "shortCiteRegEx": "Pratt.", "year": 1993}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Afet: Automatic fine-grained entity typing by hierarchical partial-label embedding", "author": ["Ren et al.2016] Xiang Ren", "Wenqi He", "Meng Qu", "Lifu Huang", "Heng Ji", "Jiawei Han"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Ren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Cross language text classification by model translation and semi-supervised learning", "author": ["Shi et al.2010] Lei Shi", "Rada Mihalcea", "Mingjun Tian"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "An attentive neural architecture for fine-grained entity type classification", "author": ["Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "venue": "In Proceedings of the 5th Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Shimaoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shimaoka et al\\.", "year": 2016}, {"title": "Yago: A core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": null, "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Transfer learning for speech and language processing", "author": ["Wang", "Zheng2015] Dong Wang", "Thomas Fang Zheng"], "venue": "In 2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "BBN Pronoun Coreference and Entity Type Corpus LDC2005T33", "author": ["Weischedel", "Brunstein2005] Ralph Weischedel", "Ada Brunstein"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Weischedel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weischedel et al\\.", "year": 2005}, {"title": "Embedding methods for fine grained entity type classification", "author": ["Daniel Gillick", "Nevena Lazic"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "HYENA: Hierarchical type classification for entity names", "author": ["Sandro Bauer", "Johannes Hoffart", "Marc Spaniol", "Gerhard Weikum"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Yosef et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yosef et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "This classification is useful for many natural language processing (NLP) tasks such as relation extraction (Mintz et al., 2009), machine translation (Koehn et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 14, "context": ", 2007), question answering (Lin et al., 2012) and knowledge base construction (Dong et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 5, "context": ", 2012) and knowledge base construction (Dong et al., 2014).", "startOffset": 40, "endOffset": 59}, {"referenceID": 11, "context": "tor (Ling and Weld, 2012; Koch et al., 2014; Mitchell et al., 2015) since this helps in filtering out candidate relation types that do not follow the type constrain.", "startOffset": 4, "endOffset": 67}, {"referenceID": 6, "context": "to its potential answers and significantly improves performance (Dong et al., 2015).", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "to its potential answers and significantly improves performance (Dong et al., 2015). For example, Li and Roth (2002) rank questions based on their expected answer types (will the answer be food, vehicle or disease).", "startOffset": 65, "endOffset": 117}, {"referenceID": 1, "context": "pus to knowledge bases such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al.", "startOffset": 40, "endOffset": 64}, {"referenceID": 0, "context": ", 2008), DBpedia (Auer et al., 2007), YAGO (Suchanek et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 26, "context": ", 2007), YAGO (Suchanek et al., 2007).", "startOffset": 14, "endOffset": 37}, {"referenceID": 32, "context": "Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)", "startOffset": 40, "endOffset": 127}, {"referenceID": 31, "context": "Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)", "startOffset": 40, "endOffset": 127}, {"referenceID": 25, "context": "Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)", "startOffset": 40, "endOffset": 127}, {"referenceID": 32, "context": "Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)", "startOffset": 29, "endOffset": 111}, {"referenceID": 31, "context": "Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)", "startOffset": 29, "endOffset": 111}, {"referenceID": 23, "context": "Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)", "startOffset": 29, "endOffset": 111}, {"referenceID": 23, "context": "AFET system (Ren et al., 2016).", "startOffset": 12, "endOffset": 30}, {"referenceID": 21, "context": "ing transfer learning (Pratt, 1993) for FETC task both at feature and model level.", "startOffset": 22, "endOffset": 35}, {"referenceID": 7, "context": "While the initial work assumed that all labels present in a training dataset for an entity mention are correct, Gillick et al. (2014) introduced context dependent FETC and proposed a set of heuristics for pruning labels that might not be relevant given the entity mention\u2019s local context.", "startOffset": 112, "endOffset": 134}, {"referenceID": 7, "context": "While the initial work assumed that all labels present in a training dataset for an entity mention are correct, Gillick et al. (2014) introduced context dependent FETC and proposed a set of heuristics for pruning labels that might not be relevant given the entity mention\u2019s local context. Yogatama et al. (2015)", "startOffset": 112, "endOffset": 312}, {"referenceID": 23, "context": "Most recently, Ren et al. (2016) have proposed AFET, an FETC system.", "startOffset": 15, "endOffset": 33}, {"referenceID": 24, "context": "Transfer learning is well applied to many NLP applications, such as cross-domain document classification (Shi et al., 2010), multi-lingual word clustering (T\u00e4ckstr\u00f6m et al.", "startOffset": 105, "endOffset": 123}, {"referenceID": 27, "context": ", 2010), multi-lingual word clustering (T\u00e4ckstr\u00f6m et al., 2012) and sentiment classification (Mou et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 19, "context": ", 2012) and sentiment classification (Mou et al., 2016).", "startOffset": 37, "endOffset": 55}, {"referenceID": 19, "context": ", 2012) and sentiment classification (Mou et al., 2016). Initialization of word vectors with pre-trained word vectors in neural network models can be considered as one of the best example of transfer learning in NLP. Wang et al. (2015) provide a broad overview of transfer learning techniques used for language processing.", "startOffset": 38, "endOffset": 236}, {"referenceID": 8, "context": "We use bi-directional LSTM encoders (Graves et al., 2013) to encode token level sequences of both context to a fixed dimensional vector.", "startOffset": 36, "endOffset": 57}, {"referenceID": 25, "context": "The context representation described above is slightly different from what was proposed in (Shimaoka et al., 2016), here we include entity men-", "startOffset": 91, "endOffset": 114}, {"referenceID": 30, "context": "Similar to Yogatama et al. (2015) and Ren et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 23, "context": "(2015) and Ren et al. (2016), we embed feature representations and labels in a same dimensional space such that an", "startOffset": 11, "endOffset": 29}, {"referenceID": 31, "context": "Whereas in (Yogatama et al., 2015; Ren et al., 2016) positive labels should have a higher score than negative labels.", "startOffset": 11, "endOffset": 52}, {"referenceID": 23, "context": "Whereas in (Yogatama et al., 2015; Ren et al., 2016) positive labels should have a higher score than negative labels.", "startOffset": 11, "endOffset": 52}, {"referenceID": 4, "context": "DBpedia spotlight (Daiber et al., 2013) was used to automatically link entity mention in sentences to Freebase.", "startOffset": 18, "endOffset": 39}, {"referenceID": 21, "context": "available datasets, provided in a pre-processed tokenized format by Ren et al. (2016). Statistics of the datasets used in this work are shown in Table 1.", "startOffset": 68, "endOffset": 86}, {"referenceID": 4, "context": "DBpedia spotlight (Daiber et al., 2013) was used to automatically link entity mention in sentences to Freebase. For this corpus, manually annotated test data was shared by Gillick et al. (2014). BBN: BBN dataset consists of sentences from Wall Street Journal articles and is completely manually annotated (Weischedel and Brunstein, 2005).", "startOffset": 19, "endOffset": 194}, {"referenceID": 23, "context": "Please refer to (Ren et al., 2016) for more details Datasets Wiki/FIGER(GOLD) OntoNotes BBN", "startOffset": 16, "endOffset": 34}, {"referenceID": 32, "context": "state-of-the-art entity classification methods2: (1) FIGER (Ling and Weld, 2012); (2) HYENA (Yosef et al., 2012); (3) AFETNoCo (Ren et al.", "startOffset": 92, "endOffset": 112}, {"referenceID": 23, "context": ", 2012); (3) AFETNoCo (Ren et al., 2016): AFET without data based label-label correlation modeled in loss function; (4) AFET-CoH (Ren et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 23, "context": ", 2016): AFET without data based label-label correlation modeled in loss function; (4) AFET-CoH (Ren et al., 2016):", "startOffset": 96, "endOffset": 114}, {"referenceID": 23, "context": "AFET with hierarchy based label-label correlation modeled in loss function; (5) AFET (Ren et al., 2016); (6) Attentive (Shimaoka et al.", "startOffset": 85, "endOffset": 103}, {"referenceID": 25, "context": ", 2016); (6) Attentive (Shimaoka et al., 2016): An attentive neural network based model.", "startOffset": 23, "endOffset": 46}, {"referenceID": 31, "context": "FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016).", "startOffset": 23, "endOffset": 108}, {"referenceID": 25, "context": "FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016).", "startOffset": 23, "endOffset": 108}, {"referenceID": 23, "context": "FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016).", "startOffset": 23, "endOffset": 108}, {"referenceID": 23, "context": "Whenever possible, the baselines result are reported from (Ren et al., 2016), otherwise we re-implemented baseline methods based on description available in corresponding papers.", "startOffset": 58, "endOffset": 76}, {"referenceID": 25, "context": "Comparison with other feature learning methods: The proposed model and its variants (ourAllC, our-NoM) perform better than the existing feature learning method by Shimaoka et al. (2016) (Attentive), consistently on all datasets.", "startOffset": 163, "endOffset": 186}, {"referenceID": 32, "context": "612 HYENA* (Yosef et al., 2012) 0.", "startOffset": 11, "endOffset": 31}, {"referenceID": 23, "context": "587 AFET-NoCo* (Ren et al., 2016) 0.", "startOffset": 15, "endOffset": 33}, {"referenceID": 23, "context": "716 AFET-CoH* (Ren et al., 2016) 0.", "startOffset": 14, "endOffset": 32}, {"referenceID": 23, "context": "712 AFET* (Ren et al., 2016) 0.", "startOffset": 10, "endOffset": 28}, {"referenceID": 23, "context": "735 AFET\u2020\u2021 (Ren et al., 2016) 0.", "startOffset": 11, "endOffset": 29}, {"referenceID": 25, "context": "747 Attentive\u2020 (Shimaoka et al., 2016) 0.", "startOffset": 15, "endOffset": 38}, {"referenceID": 23, "context": "These results are from (Ren et al., 2016) that also uses 10% of the test set as development set and the remaining for evaluation.", "startOffset": 23, "endOffset": 41}, {"referenceID": 23, "context": "These results are from (Ren et al., 2016) that also uses 10% of the test set as development set and the remaining for evaluation. \u2021We used the publicly available code distributed by Ren et al. (2016). \u2020All of these results are on exact same train, development and test set.", "startOffset": 24, "endOffset": 200}], "year": 2017, "abstractText": "Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous stateof-the-art methods on two publicly available datasets, namely FIGER(GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.", "creator": "LaTeX with hyperref package"}}}