{"id": "1702.07121", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Consistent On-Line Off-Policy Evaluation", "abstract": "the problem solving of on - mode line off - point policy subjective evaluation ( ope ) perception has been most actively studied toward in fact the last present decade due to basically its immediate importance both as solely a stand - cause alone problem and as a dedicated module in introducing a policy improvement solution scheme. however, downstream most temporal difference ( rd td ) value based solutions ignore substantially the discrepancy similarities between the stationary peak distribution of accurately the behavior and defined target optimal policies and its effect on the desired convergence limit when hash function approximation is applied. in following this paper we propose the consistent off - limits policy temporal difference ( cop - result td ( $ \\ lambda $, $ \\ beta $ ) ) algorithm that concurrently addresses precisely this convergence issue itself and reduces indirectly this reduction bias over at varying some specific computational expense. generally we show importantly that correlated cop - td ( $ \\ [UNK] lambda $, $ \\ beta $ ) configurations can be designed simultaneously to converge to the same default value that would have been likely obtained initially by computation using on - policy derivative td ( $ \\ @ lambda $ ) proceeded with applying the target policy. subsequently, the proposed scheme leads systematically to a commercially related and promising semantic heuristic trajectory we call log - cop - function td ( $ \\ lambda $, $ \\ beta $ ). substantially both coupled algorithms independently have attributed favorable respective empirical results to the current natural state strategies of since the 20th art on - line digital ope algorithms. finally, our formulation will sheds down some new light again on the recently proposed practical emphatic td learning.", "histories": [["v1", "Thu, 23 Feb 2017 07:44:43 GMT  (427kb)", "http://arxiv.org/abs/1702.07121v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["assaf hallak", "shie mannor"], "accepted": true, "id": "1702.07121"}, "pdf": {"name": "1702.07121.pdf", "metadata": {"source": "META", "title": "Consistent On-Line Off-Policy Evaluation", "authors": ["Assaf Hallak", "Shie Mannor"], "emails": ["<ifogph@gmail.com>,", "<shie@ee.technion.ac.il>."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 12\n1v 1\n[ st\nat .M\nL ]\n2 3\nFe b\n20 17\n(OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(\u03bb, \u03b2)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD(\u03bb, \u03b2) can be designed to converge to the same value that would have been obtained by using on-policy TD(\u03bb) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call logCOP-TD(\u03bb, \u03b2). Both algorithms have favorable empirical results to the current state of the art online OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning."}, {"heading": "1. Introduction", "text": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013). We consider the problem of offpolicy evaluation (OPE) \u2013 assessing the performance of a complex strategy without applying it. An OPE formulation is often considered in domains with limited sampling capability. For example, marketing and recommender systems (Theocharous & Hallak, 2013; Theocharous et al., 2015) directly relate policies to revenue. A more extreme example is drug administration, as there are only few patients in the testing population, and sub-optimal policies\n1The Technion, Haifa, Israel. Correspondence to: Assaf Hallak <ifogph@gmail.com>, Shie Mannor <shie@ee.technion.ac.il>.\ncan have life threatening effects (Hochberg et al., 2016). OPE can also be useful as a module for policy optimization in a policy improvement scheme (Thomas et al., 2015a).\nIn this paper, we consider the OPE problem in an on-line setup where each new sample is immediately used to update our current value estimate of some previously unseen policy. We propose and analyze a new algorithm called COP-TD(\u03bb,\u03b2) for estimating the value of the target policy; COP-TD(\u03bb,\u03b2) has the following properties:\n1. Easy to understand and implement on-line.\n2. Allows closing the gap to consistency such that the\nlimit point is the same that would have been obtained by on-policy learning with the target policy.\n3. Empirically comparable to state-of-the art algorithms.\nOur algorithm resembles (Sutton et al., 2015)\u2019s Emphatic TD that was extended by (Hallak et al., 2015) to the general parametric form ETD(\u03bb,\u03b2). We clarify the connection between the algorithms and compare them empirically. Finally, we introduce an additional related heuristic called Log-COP-TD(\u03bb,\u03b2) and motivate it."}, {"heading": "2. Notations and Background", "text": "We consider the standard discounted Markov Decision Process (MDP) formulation (Bertsekas & Tsitsiklis, 1996) with a single long trajectory. Let M = (S,A,P ,R, \u03b6, \u03b3) be an MDP where S is the finite state space and A is the finite action space. The parameter P sets the transition probabilities Pr(s\u2032|s, a) given the previous state s \u2208 S and action a \u2208 A, where the first state is determined by the distribution \u03b6. The parameter R sets the reward distribution r(s, a) obtained by taking action a in state s and \u03b3 is the discount factor specifying the exponential reduction in reward with time.\nThe process advances as follows: A state s0 is sampled according to the distribution \u03b6(s). Then, at each time step t starting from t = 0 the agent draws an action at according to the stochastic behavior policy \u00b5(a|st), a reward rt . = r(st, at) is accumulated by the agent, and the next state st+1 is sampled using the transition probability Pr(s\u2032|st, at).\nThe expected discounted accumulated reward starting from a specific state and choosing an action by some policy \u03c0 is called the value function, which is also known to satisfy the Bellman equation in a vector form:\nV \u03c0(s) = E\u03c0\n[ \u221e\u2211\nt=0\n\u03b3trt \u2223\u2223\u2223 s0 = s ] , T\u03c0V . = R\u03c0 + \u03b3P\u03c0V,\nwhere [R\u03c0]s . = E\u03c0 [r(s, \u03c0(s))] and [P\u03c0]s,s\u2032 . = E\u03c0 [Pr(s \u2032|s, \u03c0(s))] are the policy induced reward vector and transition probability matrix respectively; T\u03c0 is called the Bellman operator. The problem of estimating V \u03c0(s) from samples is called policy evaluation. If the target policy \u03c0 is different than the behavior policy \u00b5 which generated the samples, the problem is called off-policy evaluation (OPE). The TD(\u03bb) (Sutton, 1988) algorithm is a standard solution to on-line on-policy evaluation: Each time step the temporal difference error updates the current value function estimate, such that eventually the stochastic approximation process will converge to the true value function. The standard form of TD(\u03bb) is given by:\nR (n) t,st =\nn\u22121\u2211\ni=0\n\u03b3irt+i + \u03b3 nV\u0302t(st+n),\nR\u03bbt,st =(1\u2212 \u03bb) \u221e\u2211\nn=0\n\u03bbnR(n+1)st ,\nV\u0302t+1(st) =V\u0302t(st) + \u03b1t ( R\u03bbt,st \u2212 V\u0302t(st) ) ,\n(1)\nwhere \u03b1t is the step size. The value R (n) t,st is an estimate of the current state\u2019s V (st), looking forward n steps, and R\u03bbt,st is an exponentially weighted average of all of these estimates going forward till infinity. Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al. (2014), Sutton et al. (2015) for off-policy TD(\u03bb)). The underlying operator of TD(\u03bb) is given by:\nT \u03bb\u03c0 V = (1 \u2212 \u03bb)\n\u221e\u2211\nn=0\n\u03bbn\n( n\u2211\ni=0\n\u03b3iP i\u03c0R\u03c0 + \u03b3 n+1Pn+1\u03c0 V\n)\n= (1 \u2212 \u03bb)(I \u2212 \u03bbT\u03c0) \u22121T\u03c0V,\nand is a \u03b3(1\u2212\u03bb) 1\u2212\u03bb\u03b3 -contraction (Bertsekas, 2012).\nWe denote by d\u00b5(s) the stationary distribution over states induced by taking the policy \u00b5 and mark D\u00b5 = diag(d\u00b5). Since we are concerned with the behavior at infinite horizon, we assume \u03b6(s) = d\u00b5(s). In addition, we assume that the MDP is ergodic for the two specified policies \u00b5, \u03c0 so \u2200s \u2208 S : d\u00b5(s) > 0, d\u03c0(s) > 0 and that the OPE problem is proper \u2013 \u03c0(a|s) > 0 \u21d2 \u00b5(a|s) > 0.\nWhen the state space is too large to hold V \u03c0(s), a linear function approximation scheme is used: V \u03c0(s) \u2248 \u03b8\u22a4\u03c0 \u03c6(s), where \u03b8 is the optimized weight vector and \u03c6(s) is the feature vector of state s composed of k features. We denote by \u03a6 \u2208 RS,k the matrix whose lines consist of the feature vectors for each state and assume its columns are linearly independent.\nTD(\u03bb) can be adjusted to find the fixed point of \u03a0d\u03c0T \u03bb \u03c0 where \u03a0d\u03c0 is the projection to the subspace spanned by the features with respect to the d\u03c0-weighted norm (Sutton & Barto, 1998):\nR (n) t,st =\nn\u22121\u2211\ni=0\n\u03b3irt+i + \u03b3 n\u03b8\u22a4t \u03c6(st+n),\nR\u03bbt,st =(1\u2212 \u03bb)\n\u221e\u2211\nn=0\n\u03bbnR(n+1)st ,\n\u03b8t+1 =\u03b8t + \u03b1t ( R\u03bbt,st \u2212 \u03b8 \u22a4 t \u03c6(st) ) \u03c6(st).\nFinally, we define OPE-related quantities:\n\u03c1t . = \u03c0(at|st)\n\u00b5(at|st) , \u0393nt\n. =\nn\u22121\u220f\ni=0\n\u03c1t\u22121\u2212i, \u03c1d(s) . = d\u03c0(s)\nd\u00b5(s) ,\n(2)\nwe call \u03c1d the covariate shift ratio (as denoted under different settings by (Hachiya et al., 2012)).\nWe summarize the assumptions used in the proofs:\n1. Under both policies the induced Markov chain is er-\ngodic.\n2. The first state s0 is distributed according to the stationary distribution of the behavior policy d\u00b5(s).\n3. The problem is proper: \u03c0(a|s) > 0 \u21d2 \u00b5(a|s) > 0.\n4. The feature matrix \u03a6 has full rank k.\nAssumption 1 is commonly used for convergence theorems as it verifies the value function is well defined on all states regardless of the initial sampled state. Assumption 2 can be relaxed since we are concerned with the long-term properties of the algorithm past its mixing time \u2013 we require it for clarity of the proofs. Assumption 3 is required so the importance sampling ratios will be well defined. Assumption 4 guarantees the optimal \u03b8 is unique which greatly simplifies the proofs."}, {"heading": "3. Previous Work", "text": "We can roughly categorize previous OPE algorithms to two main families. Gradient based methods that perform\nstochastic gradient descent on error terms they want to minimize. These include GTD (Sutton et al., 2009a), GTD2, TDC (Sutton et al., 2009b) and HTD (White & White, 2016). The main disadvantages of gradient based methods are (A) they usually update an additional error correcting term, which means another time-step parameter needs to be controlled; and (B) they rely on estimating non-trivial terms, an estimate that tends to converge slowly. The other family uses importance sampling (IS) methods that correct the gains between on-policy and off-policy updates using the IS-ratios \u03c1t\u2019s. Among these are full IS (Precup et al., 2001) and ETD(\u03bb,\u03b2) (Sutton et al., 2015). These methods are characterized by the bias-variance trade-off they resort to \u2013 navigating between biased convergent values (or even divergent), and very slow convergence stemming from the high variance of IS correcting factors (the \u03c1t products). There are also a few algorithms that fall between the two, for example TO-GTD (van Hasselt et al., 2014) and WISTD(\u03bb) (Mahmood & Sutton, 2015).\nA comparison of these algorithms in terms of convergence rate, synergy with function approximation and more is available in (White & White, 2016; Geist & Scherrer, 2014). We focus in this paper on the limit point of the convergence. For most of the aforementioned algorithms, the process was shown to converge almost surely to the fixed point of the projected Bellman operator \u03a0dT\u03c0 where d is some stationary distribution (usually d\u00b5), however the d in question was never1 d\u03c0 as we would have obtained from running on-policy TD with the target policy. The algorithm achieving the closest result is ETD(\u03bb,\u03b2) which replaced d with f = ( I \u2212 \u03b2P\u22a4\u03c0 )\u22121 d\u00b5, where \u03b2 trades-off some of the process\u2019 variance with the bias in the limit point. Hence, our main contribution is a consistent algorithm which can converge to the same value that would have been obtained by running an on-policy scheme with the same policy."}, {"heading": "4. Motivation", "text": "Here we provide a motivating example showing that even in simple cases with \u201cclose\u201d behavior and target policies, the two induced stationary distributions can differ greatly. Choosing a specific linear parameterization further emphasizes the difference between applying on-policy TD with the target policy, and applying inconsistent off-policy TD.\nAssume a chain MDP with numbered states 1, 2, ..|S|, where from each state s you can either move left to state s \u2212 1, or right to state s + 1. If you\u2019ve reached the beginning or the end of the chain (states 1 or |S|) then taking a step further does not affect your location. Assume the behavior policy moves left with probability 0.5+ \u01eb, while the\n1Except full IS, however its variance is too high to be applicable in practice.\ntarget policy moves right with probability 0.5+ \u01eb. It is easy to see that the stationary distributions are given by:\nd\u00b5(s) \u221d\n( 0.5\u2212 \u01eb\n0.5 + \u01eb\n)s , d\u03c0(s) \u221d ( 0.5 + \u01eb\n0.5\u2212 \u01eb\n)s .\nFor instance, if we have a length 100 chain with \u01eb = 0.01, for the rightmost state we have d\u00b5(|S|) \u2248 8 \u00b7 10\u22124, d\u03c0(|S|) \u2248 0.04. Let\u2019s set the reward to be 1 for the right half of the chain, so the target policy is better since it spends more time in the right half. The value of the target policy in the edges of the chain for \u03b3 = 0.99 is V \u03c0(1) = 0.21, V \u03c0(100) = 99.97.\nNow what happens if we try to approximate the value function using one constant feature \u03c6(s) \u2261 1? The fixed point of \u03a0d\u00b5T\u03c0 is \u03b8 = 11.92, while the fixed point of \u03a0d\u03c0T\u03c0 is \u03b8 = 88.08 \u2013 a substantial difference. The reason for this difference lies in the emphasis each projection puts on the states: according to \u03a0d\u00b5 , the important states are in the left half of the chain \u2013 these with low value function, and therefore the value estimation of all states is low. However, according to \u03a0d\u03c0 the important states are concentrated on the right part of the chain since the target policy will visit these more often. Hence, the estimation error is emphasized on the right part of the chain and the value estimation is higher. When we wish to estimate the value of the target policy, we want to know what will happen if we deploy it instead of the behavior policy, thus taking the fixed point of \u03a0d\u03c0T\u03c0 better represents the off-policy evaluation solution.\n5. COP-TD(\u03bb, \u03b2)\nMost off-policy algorithms multiply the TD summand of TD(\u03bb) with some value that depends on the history and the current state. For example, full IS-TD by (Precup et al., 2001) examines the ratio between the probabilities of the trajectory under both policies:\nP\u03c0(s0, a0, s1, . . . , st, at) P\u00b5(s0, a0, s1, . . . , st, at) =\nt\u220f\nm=0\n\u03c1m = \u0393 t t\u03c1t. (3)\nIn problems with a long horizon, or these that start from the stationary distribution, we suggest using the time-invariant covariate shift \u03c1d multiplied by the current \u03c1t. The intuition is the following: We would prefer using the probabilities ratio given in Equation 3, but it has very high variance, and after many time steps we might as well look at the stationary distribution ratio instead. This direction leads us to the following update equations:\n\u03b8t+1 =\n\u03b8t + \u03b1t\u03c1d(st)\u03c1t ( rt + \u03b8 \u22a4 t (\u03b3\u03c6(st+1)\u2212 \u03c6(st)) ) \u03c6(st).\n(4)\nLemma 1. If the \u03b1t satisfy \u2211\u221e t=0 \u03b1t = \u221e, \u2211\u221e t=0 \u03b1 2 t < \u221e then the process described by Eq. (4) converges almost surely to the fixed point of \u03a0\u03c0T\u03c0V = V .\nThe proof follows the ODEmethod (Kushner & Yin, 2003) similarly to Tsitsiklis & Van Roy (1997) (see the appendix for more details).\nSince \u03c1d(s) is generally unknown, it is estimated using an additional stochastic approximation process. In order to do so, we note the following Lemma:\nLemma 2. Let \u03c1\u0302d be an unbiased estimate of \u03c1d, and for every n = 0, 1, . . . , t define \u0393\u0303nt . = \u03c1\u0302d(st\u2212n)\u0393 n t . Then:\nE\u00b5 [ \u0393\u0303nt |st ] = \u03c1d(st).\nFor any state st there are t \u2192 \u221e such quantities {\u0393\u0303 n t } t n=0, where we propose to weight them similarly to TD(\u03bb):\n\u0393\u0303\u03b2t = (1 \u2212 \u03b2) \u221e\u2211\nn=0\n\u03b2n\u0393\u0303n+1t . (5)\nNote that \u03c1d(s), unlike V (s), is restricted to a close set since its d\u00b5-weighted linear combination is equal to 1 and all of its entries are non-negative; We denote this d\u00b5weighted simplex by\u2206d\u00b5 , and let\u03a0\u2206d\u00b5 be the (non-linear) projection to this set with respect to the Euclidean norm (\u03a0\u2206d\u00b5 can be calculated efficiently, (Chen & Ye, 2011)). Now, we can devise a TD algorithmwhich estimates \u03c1d and uses it to find \u03b8, which we call COP-TD(0, \u03b2) (Consistent Off-Policy TD).\nAlgorithm 1 COP-TD(0,\u03b2), Input: \u03b80, \u03c1\u0302d,0,\n1: Init: F0 = 0, n \u03b2 0 = 1, N(s) = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: N(st) = N(st) + 1, \u2200s \u2208 S : d\u0302\u00b5(s) = N(s) t 6: n\u03b2t = \u03b2n \u03b2 t + 1 7: Update \u0393nt \u2019s weighted average: 8: Ft = \u03c1t\u22121(\u03b2Ft\u22121 + est\u22121) 9: Update & project by \u03c1d\u2019s TD error:\n10: \u03b4dt = F\u22a4t \u03c1\u0302d,t\nn\u03b2t\ufe38 \ufe37\ufe37 \ufe38 \u2192\u0393\u0303\u03b2t\n\u2212\u03c1\u0302d,t(st)\n11: \u03c1\u0302d,t+1 = \u03a0\u2206d\u0302\u00b5\n( \u03c1\u0302d,t + \u03b1 d t \u03b4 d t est )\n12: Off-policy TD(0): 13: \u03b4t = rt + \u03b8 \u22a4 t (\u03b3\u03c6(st+1)\u2212 \u03c6(st)) 14: \u03b8t+1 = \u03b8t + \u03b1t\u03c1\u0302d,t+1(st)\u03c1t\u03b4t\u03c6(st) 15: end for\nSimilarly to the Bellman operator for TD-learning, we de-\nfine the underlying COP-operator Y and its \u03b2 extension:\nY u = D\u22121\u00b5 P \u22a4 \u03c0 D\u00b5u,\nY \u03b2u = (1\u2212 \u03b2)D\u22121\u00b5 P \u22a4 \u03c0 (I \u2212 \u03b2P \u22a4 \u03c0 ) \u22121D\u00b5u. (6)\nThe following Lemma may give some intuition on the convergence of the \u03c1d estimation process:\nLemma 3. Under the ergodicity assumption, denote the eigenvalues of P\u03c0 by 0 \u2264 \u00b7 \u00b7 \u00b7 \u2264 |\u03be2| < \u03be1 = 1. Then Y \u03b2 is amaxi6=1 (1\u2212\u03b2)|\u03bei| |1\u2212\u03b2\u03bei| < 1-contraction in the L2-norm on the orthogonal subspace to \u03c1d, and \u03c1d is a fixed point of Y \u03b2 .\nThe technical proof is given in the appendix. Theorem 1. If the step sizes satisfy \u2211 t \u03b1t = \u2211 t \u03b1 d t =\n\u221e, \u2211\nt(\u03b1 2 t + (\u03b1 d t ) 2) < \u221e, \u03b1t \u03b1dt \u2192 0, t\u03b1dt \u2192 0, and\nE [ (\u03b2n\u0393nt ) 2|st ]\n\u2264 C for some constant C and every t and n, then after applying COP-TD(0, \u03b2), \u03c1\u0302d,t converges to \u03c1d almost surely, and \u03b8t converges to the fixed point of \u03a0\u03c0T\u03c0V .\nNotice that COP-TD(0, \u03b2) given in Alg. 1 is infeasible in problems with large state spaces since \u03c1d \u2208 R |S|. Like TD(\u03bb), we can introduce linear function approximation: represent \u03c1d(s) \u2248 \u03b8 \u22a4 \u03c1 \u03c6\u03c1(s) where \u03b8\u03c1 is a weight vector and \u03c6\u03c1(s) is the off-policy feature vector and adjust the algorithm accordingly. For \u03c1\u0302d to still be contained in the set \u2206d\u00b5 , we pose the requirement on the feature vectors: \u03c6\u03c1(s) \u2208 R k +, and \u2211 s d\u00b5(s)\u03b8 \u22a4 \u03c1 \u03c6\u03c1(s) = 1 ( noted as\nthe simplex projection \u03a0\u2206E\u00b5[\u03c6\u03c1(s)] ) . In practice, the latter\nrequirement can be approximated: \u2211\ns d\u00b5(s)\u03b8 \u22a4 \u03c1 \u03c6\u03c1(s) \u2248\n1 t \u03b8\u22a4\u03c1 \u2211\nt \u03c6\u03c1(st) = 1 resulting in an extension of the previously applied d\u00b5 estimation (step 5 in COP-TD(0, \u03b2)). We provide the full details in Algorithm 2, which also incorporates non-zero \u03bb ( similarly to ETD(\u03bb,\u03b2) ) . Theorem 2. If the step sizes satisfy \u2211 t \u03b1t = \u2211 t \u03b1 d t =\n\u221e, \u2211\nt(\u03b1 2 t + (\u03b1 d t ) 2) < \u221e, \u03b1t \u03b1dt \u2192 0, t\u03b1dt \u2192 0, and\nE [ (\u03b2n\u0393nt ) 2|st ] \u2264 C for some constant C and every t, n, then after applying COP-TD(0, \u03b2) with function approximation satisfying \u03c6\u03c1(s) \u2208 R k +, \u03c1\u0302d,t converges to the fixed point of \u03a0\u2206E\u00b5[\u03c6\u03c1]\u03a0\u03c6\u03c1Y \u03b2 denoted by \u03c1COPd almost surely, and if \u03b8t converges it is to the fixed point of \u03a0d\u00b5\u25e6\u03c1COPd T\u03c0V , where \u25e6 is a coordinate-wise product of vectors.\nThe proof is given in the appendix and also follows the ODE method. Notice that a theorem is only given for \u03bb = 0, convergence results for general \u03bb should follow the work by Yu (2015).\nA possible criticism on COP-TD(0,\u03b2) is that it is not actually consistent, since in order to be consistent the original state space has to be small, in which case every off-policy algorithm is consistent as well. Still, the dependence on another set of features allows to trade-off accuracy with\nAlgorithm 2 COP-TD(\u03bb,\u03b2) with Function Approximation, Input: \u03b80, \u03b8\u03c1,0\n1: Init: F0 = 0, n \u03b2 0 = 1, N\u03c6 = 0, e0 = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: n\u03b2t = \u03b2n \u03b2 t + 1, N\u03c6 = N\u03c6 + \u03c6\u03c1(st), d\u0302\u03c6\u03c1 = N\u03c6 t 6: Update \u0393nt \u2019s weighted average: 7: Ft = \u03c1t\u22121(\u03b2Ft\u22121 + \u03c6\u03c1(st\u22121)) 8: Update & project by \u03c1d\u2019s TD error: 9: \u03b4dt = \u03b8 \u22a4 \u03c1,t\u22121 ( Ft\nn \u03b2 t\n\u2212 \u03c6\u03c1(st) )\n10: \u03b8\u03c1,t+1 = \u03a0\u2206 d\u0302\u03c6\u03c1\n( \u03b8\u03c1,t + \u03b1 d t \u03b4 d t \u03c6\u03c1(st) )\n11: Off-policy TD(\u03bb): 12: Mt = \u03bb+ (1\u2212 \u03bb)\u03b8 \u22a4 \u03c1,t+1\u03c6\u03c1(st) 13: et = \u03c1t (\u03bb\u03b3et +Mt\u03c6(st+1)) 14: \u03b4t = rt + \u03b8 \u22a4 t (\u03b3\u03c6(st+1)\u2212 \u03c6(st)) 15: \u03b8t+1 = \u03b8t + \u03b1t\u03b4tet 16: end for\ncomputational power in estimating \u03c1d and subsequently V . Moreover, smart feature selection may further reduce this gap, and COP-TD(0, \u03b2) is still the first algorithm addressing this issue. We conclude with linking the error in \u03c1d\u2019s estimate with the difference in the resulting \u03b8, which suggests that a well estimated \u03c1d results in consistency: Corollary 1. Let 0 < \u01eb < 1. If (1 \u2212 \u01eb)\u03c1d \u2264 \u03c1 COP d \u2264 (1 + \u01eb)\u03c1d, then the fixed point of COP-TD(0,\u03b2) with function approximation \u03b8COP satisfies the following, where \u2016 \u00b7 \u2016\u221e is the L\u221e induced norm:\n\u2016\u03b8\u2217 \u2212 \u03b8COP\u2016\u221e \u2264\n\u01eb\u2016A\u22121\u03c0 \u03a6 \u22a4\u2016\u221e ( Rmax + (1 + \u03b3)\u2016\u03a6\u2016\u221e\u2016\u03b8 COP\u2016\u221e ) ,\n(7)\nwhereA\u03c0 = \u03a6 \u22a4D\u03c0(I\u2212\u03b3P\u03c0)\u03a6, and \u03b8 \u2217 sets the fixed point of the operator \u03a0d\u03c0T\u03c0V ."}, {"heading": "5.1. Relation to ETD(\u03bb, \u03b2)", "text": "Recently, Sutton et al. (2015) had suggested an algorithm for off-policy evaluation called Emphatic TD. Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(\u03bb, \u03b2), which was shown to perform extremely well empirically by White & White (2016). ETD(0, \u03b2) can be represented as:\nFt = (1\u2212 \u03b2)\n\u221e\u2211\nn=0\n\u03b2n\u0393nt ,\nV\u0302t+1(st) = V\u0302t(st) + \u03b1tFt\u03c1t ( rt + \u03b8 \u22a4 t (\u03b3\u03c6(st+1)\u2212 \u03c6(st)) ) .\n(8)\nAs mentioned before, ETD(\u03bb, \u03b2) converges to the fixed\npoint of \u03a0fT \u03bb \u03c0 (Yu, 2015), where f = E [Ft|st] = (I \u2212 \u03b2P\u03c0) \u22121d\u00b5. Error bounds can be achieved by showing that the operator \u03a0fT \u03bb \u03c0 is a contraction under certain requirements on \u03b2 and that the variance of Ft is directly related to \u03b2 as well (Hallak et al., 2015) (and thus affects the convergence rate of the process).\nWhen comparing ETD(\u03bb,\u03b2)\u2019s form to COP-TD(\u03bb,\u03b2)\u2019s, instead of spending memory and time resources on a state/feature-dependent Ft, ETD(\u03bb,\u03b2) uses a one-variable approximation. The resulting Ft is in fact a one-step estimate of \u03c1d, starting from \u03c1\u0302d(s) \u2261 1 (see Equations 15, 8), up to a minor difference: F ETDt = \u03b2F COP-TD t + 1 (which following our logic adds bias to the estimate 2).\nUnlike ETD(\u03bb, \u03b2), COP-TD(\u03bb,\u03b2)\u2019s effectiveness depends on the available resources. The number of features \u03c6\u03c1(s) can be adjusted accordingly to provide the most affordable approximation. The added cost is fine-tuning another stepsize, though \u03b2\u2019s effect is less prominent."}, {"heading": "6. The Logarithm Approach for Handling Long Products", "text": "We now present a heuristic algorithm which works similarly to COP-TD(\u03bb, \u03b2). Before presenting the algorithm, we explain the motivation behind it."}, {"heading": "6.1. Statistical Interpretation of TD(\u03bb)", "text": "Konidaris et al. (2011) suggested a statistical interpretation of TD(\u03bb). They show that under several assumptions the TD(\u03bb) estimate R\u03bbst is the maximum likelihood estimator of V (st) given R n st : (1) Each Rnst is an unbiased estimator of V (st); (2) The random variables R n st are independent and specifically uncorrelated; (3) The random variablesRnst are jointly normally distributed; and (4) The variance of each Rnst is proportional to \u03bb n.\nUnder Assumptions 1-3 the maximum likelihood estimator of V (s) given its previous estimate can be represented as a linear convex combination of Rnst with weights:\nwn =\n[ Var ( R (n) st )]\u22121\n\u2211\u221e m=0 [ Var ( R (m) st )]\u22121 . (9)\nSubsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed. In a follow-up paper by Thomas et al. (2015b), the second assumption was also removed and the weights were instead given as: wn =\n2We have conducted several experiments with an altered ETD and indeed obtained better results compared with the original, these experiments are outside the scope of the paper.\n1\u22a4cov(Rst)en 1\u22a4cov(Rst)1 , where the covariancematrix can be estimated from the data, or otherwise learned through some parametric form.\nWhile both the approximated variance and learned covariance matrix solutions improve performance on several benchmarks, the first uses a rather crude approximation, and the second solution is both state-dependent and based on noisy estimates of the covariance matrix. In addition, there aren\u2019t efficient on-line implementations since all past weights should be recalculated to match a new sample. Still, the suggested statistical justification is a valuable tool in assessing the similar role of \u03b2 in ETD(\u03bb, \u03b2)."}, {"heading": "6.2. Variance Weighted \u0393nt", "text": "As was shown by Konidaris et al. (2011), we can use statedependent weights instead of \u03b2 exponents to obtain better estimates. The second moments are given explicitly as follows3: E [ (\u0393nt ) 2 |st ] = d\u22a4\u00b5 P\u0303 n\u22121est\nd\u00b5(st) , where [ P\u0303 ] s,s\u2032 =\n\u2211 a\u2208A \u03c02(a|s) \u00b5(a|s) P (s \u2032|s, a).\nThese can be estimated for each state separately. Notice that the variances increase exponentially depending on the largest eigenvalue of P\u0303 (as Assumption 4 dictates), but this is merely an asymptotic behavior and may be relevant only when the weights are already negligible. Hence, implementing this solution on-line should not be a problem with the varying weights, as generally only the first few of these are non-zero. While this solution is impractical in problems with large state spaces parameterizing or approximating these variances (similarly to Thomas et al. (2015b)) could improve performance in specific applications."}, {"heading": "6.3. Log-COP-TD(\u03bb, \u03b2)", "text": "Assumption 3 in the previous section is that the sampled estimators (R(n),\u0393nt ) are normally distributed. For on policy TD(\u03bb), this assumption might seem not too harsh as the estimators R(n) represent growing sums of random variables. However, in our case the estimators \u0393nt are growing products of random variables. To correct this issue we can define new estimators using a logarithm on each \u0393\u0303nt :\nlog [\u03c1d(st)] = log\n[ E [ \u03c1\u0302d(st\u2212m) t\u22121\u220f\nk=t\u2212m\n\u03c1k \u2223\u2223 st ]]\n\u2248 log [\u03c1\u0302d(st\u2212m)] + t\u22121\u2211\nk=t\u2212m\nE [log [\u03c1k] |st] .\n(10)\n3The covariances can be expressed analytically as well, for clarity we drop this immediate result.\nThis approximation is crude \u2013 we could add terms reducing the error through Taylor expansion, but these would be complicated to deal with. Hence, we can relate to this method mainly as a well-motivated heuristic.\nNotice that this formulation resembles the standard MDP formulation, only with the corresponding \u201dreward\u201d terms log[\u03c1t] going backward instead of forward, and no discount factor. Unfortunately, without a discount factor we cannot expect the estimated value to converge, so we propose using an artificial one \u03b3log. We can incorporate function approximation for this formulation as well. Unlike COP-TD(\u03bb, \u03b2), we can choose the features and weights as we wish with no restriction, besides the linear constraint on the resulting \u03c1d through the weight vector \u03b8\u03c1. This can be approximately enforced by normalizing \u03b8\u03c1 using X t . = 1 t \u2211 t exp(\u03b8 \u22a4 \u03c1,t\u03c6(st)) (which should equal 1 if we were exactly correct). We call the resulting algorithm LogCOP-TD(\u03bb,\u03b2).\nAlgorithm 3 Log-COP-TD(\u03bb,\u03b2) with Function Approximation, Input: \u03b80,\u03b8\u03c1,0\n1: Init: F0 = 0, n0(\u03b2) = 1, N(s) = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: n\u03b2t = \u03b2n \u03b2 t + 1, N\u03c6 = \u03b3log(\u03b2N\u03c6 +\n\u03c6\u03c1(st)), X = X + exp(\u03b8 \u22a4 \u03c1,t\u03c6(st))\n6: Update log(\u0393nt )\u2019s weighted average: 7: Ft = \u03b2\u03b3logFt\u22121 + n \u03b2 t log[\u03c1(st\u22121)] 8: Update & project by log(\u03c1d)\u2019s TD error: 9: \u03b4dt = Ft\nn \u03b2 t\n+ \u03b8\u22a4\u03c1,t\n( N\u03c6\nn \u03b2 t\n\u2212 \u03c6\u03c1(st) )\n10: \u03b8\u03c1,t+1 = \u03b8\u03c1,t + \u03b1 d t \u03b4 d t \u03c6\u03c1(st) 11: Off-policy TD(\u03bb): 12: Mt = \u03bb+ (1\u2212 \u03bb) exp ( \u03b8\u22a4\u03c1,t+1\u03c6\u03c1(st) ) /(X/t) 13: et = \u03c1t (\u03bb\u03b3et +Mt\u03c6(st+1)) 14: \u03b4t = rt + \u03b8 \u22a4 t (\u03b3\u03c6(st+1)\u2212 \u03c6(st)) 15: \u03b8t+1 = \u03b8t + \u03b1t\u03b4tet 16: end for"}, {"heading": "6.4. Using the Original Features", "text": "An interesting phenomenon occurs when the behavior and target policies employ a feature based Boltzmann distribution for choosing the actions: \u00b5(a|s) = exp ( \u03b8\u22a4a,\u00b5\u03c6(s) ) ,\nand \u03c0(a|s) = exp ( \u03b8\u22a4a,\u03c0\u03c6(s) ) , where a constant feature is added to remove the (possibly different) normalizing constant. Thus, log(\u03c1t) = (\u03b8a,\u03c0 \u2212 \u03b8a,\u00b5) \u22a4\u03c6(st), and LogCOP-TD(\u03bb,\u03b2) obtains a parametric form that depends on the original features instead of a different set."}, {"heading": "6.5. Approximation Hardness", "text": "As we propose to use linear function approximation for \u03c1d(s) and log (\u03c1d(s)) one cannot help but wonder how hard it is to approximate these quantities, especially compared to the value function. The comparison between V (s) and \u03c1d(s) is problematic for several reasons:\n1. The ultimate goal is estimating V \u03c0(s), approximation errors in \u03c1d(s) are second order terms.\n2. The value function V \u03c0(s) depends on the policyinduced reward function and transition probability\nmatrix, while \u03c1d(s) depends on the stationary distributions induced by both policies. Since each depends on at least one distinct factor - we can expect different setups to result in varied approximation hardness. For example, if the reward function has a poor approximation then so will V \u03c0(s), while extremely different behavior and target policies can cause \u03c1d(s) to behave erratically.\n3. Subsequently, the choice of features for approximat-\ning V \u03c0(s) and \u03c1d(s) can differ significantly depending on the problem at hand.\nIf we would still like to compare V \u03c0(s) and \u03c1d(s), we could think of extreme examples:\n\u2022 When \u03c0 = \u00b5, \u03c1d(s) \u2261 1, when R(s) \u2261 0 then V \u03c0(s) \u2261 0.\n\u2022 In the chain MDP example in Section 4 we saw that \u03c1d(s) is an exponential function of the location in the chain. Setting reward in one end to 1 will result in an exponential form for V \u03c0(s) as well. Subsequently, in the chain MDP example approximating log (\u03c1d(s)) is easier than \u03c1d(s) as we obtain a linear function of the position; This is not the general case."}, {"heading": "7. Experiments", "text": "We have performed 3 types of experiments. Our first batch of experiments (Figure 1) demonstrates the accuracy of predicting \u03c1d by both COP-TD(\u03bb, \u03b2) and Log-COP-TD(\u03bb, \u03b2). We show two types of setups in which visualization of \u03c1d is relatively clear - the chain MDP example mentioned in Section 4 and the mountain car domain (Sutton & Barto, 1998) in which the state is determined by only two continuous variables - the car\u2019s position and speed. The parameters \u03bb and \u03b2 exhibited low sensitivity in these tasks so they were simply set to 0, we show the estimated \u03c1d after 106 iterations. For the chain MDP (top two plots, notice the logarithmic scale) we first approximate \u03c1d without any function approximation (top-left) and we can see COP-TD\nmanages to converge to the correct value while Log-COPTD is much less exact. When we use linear feature space (constant parameter and position) Log-COP-TD captures the true behavior of \u03c1d much better as expected. The two lower plots show the error (in color) in \u03c1d estimated for the mountain car with a pure exploration behavior policy vs. a target policy oriented at moving right. The z-axis is the same for both plots and it describes a much more accurate estimate of \u03c1d obtained through simulations. The features used were local state aggregation. We can see that both algorithms succeed similarly on the position-speed pairs which are sampled often due to the behavior policy and the mountain. When looking at more rarely observed states, the estimate becomes worse for both algorithms, though Log-COP-TD seems to be better performing on the spike at position > 0.\nNext we test the sensitivity of COP-TD(\u03bb, \u03b2) and LogCOP-TD(\u03bb,\u03b2) to the parameters \u03b2 and \u03b3log (Figure 2) on two distinct toy examples - the chain MDP introduced before but with only 30 states with the position-linear features, and a random MDP with 32 states, 2 actions and a 5-bit binary feature vector along with a free parameter (this compact representation was suggested by White & White (2016) to approximate real world problems). The policies on the chain MDP were taken as described before, and on the random MDP a state independent 0.75/0.25 probability to choose an action by the behavior/target policy. As we can see, larger values of \u03b2 cause noisier estimations in the randomMDP for COP-TD(\u03bb, \u03b2), but has little effect in other venues. As for \u03b3log - we can see that if it is too large or too small the error behaves sub-optimally, as expected for the crude approximation of Equation 10. In conclusion, unlike ETD(\u03bb, \u03b2), Log/COP-TD(\u03bb, \u03b2) are much less effected by \u03b2, though \u03b3log should be tuned to improve results.\nOur final experiment (Figure 3) compares our algorithms to ETD(\u03bb, \u03b2) and GTD(\u03bb, \u03b2) over 4 setups: chain MDP with 100 states with right half rewards 1 with linear features, a 2 action random MDP with 256 states and binary features, acrobot (3 actions) and cart-pole balancing (21 actions) (Sutton & Barto, 1998) with reset at success and state aggregation to 100 states. In all problems we used the same features for \u03c1d and V\n\u03c0(s) estimation, \u03b3 = 0.99, constant step size 0.05 for the TD process and results were averaged over 10 trajectories, other parameters (\u03bb, \u03b2, other step sizes, \u03b3log) were swiped over to find the best ones. To reduce figure clutter we have not included standard deviations though the noisy averages still reflect the variance in the process. Our method of comparison on the first 2 setups estimates the value function using the suggested algorithm, and finds the d\u03c0 weighted average of the error between V and the on-policy fixed point \u03a0\u03c0TV\u03c0:\n\u2016V\u0302 \u2212\u03a0\u03c0TV\u03c0\u2016 2 d\u03c0\n= \u2211\ns\nd\u03c0(s) [ (\u03b8\u2217 \u2212 \u03b8\u0302)\u22a4\u03c6(s) ]2 , (11)\nwhere \u03b8\u2217 is the optimal \u03b8 obtained by on-policy TD using the target policy. On the latter continuous state problems we applied on-line TD on a different trajectory following the target policy, used the resulting \u03b8 value as ground truth and taken the sum of squared errors with respect to it. The behavior and target policies for the chain MDP and random MDP are as specified before. For the acrobot problem the behavior policy is uniform over the 3 actions and the target policy chooses between these with probabilities (16 , 1 3 , 1 2 ). For the cart-pole the action space is divided to 21 actions from -1 to 1 equally, the behavior policy chooses among these uniformly while the target policy is 1.5 times more\nprone to choosing a positive action than a negative one.\nThe experiments show that COP-TD(\u03bb, \u03b2) and Log-COPTD(\u03bb, \u03b2) have comparable performance to ETD(\u03bb, \u03b2) where at least one is better in every setup. The advantage in the new algorithms is especially seen in the chain MDP corresponding to a large discrepancy between the stationary distribution of the behavior and target policy. GTD(\u03bb) is consistently worse on the tested setups, this might be due to the large difference between the chosen behavior and target policies which affects GTD(\u03bb) the most."}, {"heading": "8. Conclusion", "text": "Research on off-policy evaluation has flourished in the last decade. While a plethora of algorithms were suggested so far, ETD(\u03bb, \u03b2) by Hallak et al. (2015) has perhaps the simplest formulation and theoretical properties. Unfortunately, ETD(\u03bb, \u03b2) does not converge to the same point achieved by on-line TD when linear function approximation is applied.\nWe address this issue with COP-TD(\u03bb,\u03b2) and proved it can achieve consistency when used with a correct set of features, or at least allow trading-off some of the bias by adding or removing features. Despite requiring a new set of features and calibrating an additional update function, COP-TD(\u03bb,\u03b2)\u2019s performance does not depend as much on \u03b2 as ETD(\u03bb,\u03b2), and shows promising empirical results.\nWe offer a connection to the statistical interpretation of TD(\u03bb) that motivates our entire formulation. This interpretation leads to two additional approaches: (a) weight the \u0393nt using estimated variances instead of \u03b2 exponents and (b) approximating log[\u03c1d] instead of \u03c1d; both approaches deserve consideration when facing a real application."}, {"heading": "9.1. Proof of Lemma 1", "text": "If the step sizes \u03b1t hold \u2211\u221e t=0 \u03b1t = \u221e, \u2211\u221e t=0 \u03b1 2 t < \u221e then the process described by Equation 4 converges almost surely to the fixed point of \u03a0\u03c0T\u03c0V = V .\nProof. Similarly to on-policy TD, we define A and b, the fixed point is the solution to A\u03b8 = b. First we find A and show\nstability:\nA = lim t\u2192\u221e E\u00b5\n[ \u03c1t\u03c1d(st)\u03c6t(\u03c6t \u2212 \u03b3\u03c6t+1) \u22a4 ]\n= \u2211\ns\nd\u00b5(s)\u03c1d(s)E\u00b5 [ \u03c1k\u03c6k(\u03c6k \u2212 \u03b3\u03c6k+1) \u22a4|sk = s ]\n= \u2211\ns\nd\u03c0(s)E\u03c0 [ \u03c1k\u03c6k(\u03c6k \u2212 \u03b3\u03c6k+1) \u22a4|sk = s ]\n= \u03a6\u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6.\n(12)\nThis is exactly the same A we would have obtained from TD(0) and it is negative definite (see (Sutton et al., 2015)). Similarly we can find b:\nb = lim t\u2192\u221e E\u00b5\n[ \u03c1t\u03c1d(st)\u03c6tr \u22a4 t |sk = s ]\n= \u2211\ns\nd\u00b5(s)\u03c1d(s)E\u00b5 [\u03c1k\u03c6krk|sk = s]\n= \u2211\ns\nd\u03c0(s)E\u03c0 [\u03c6krk|sk = s]\n= \u03a6\u22a4D\u03c0R\u03c0,\n(13)\nand we obtained the same b as on-policy TD(0) with \u03c0.\nNow we consider the noise of this off-policy TD, which is exactly the same noise as the on-policy TD only multiplied by \u03c1t\u03c1d(st) - as long as the noise term of the ODE formulation (Kushner & Yin, 2003) is still bounded, the proof is exactly the same. According to Assumption 1, we know that \u03c1d is lower and upper bounded. By Assumption 3 we also know that \u03c1t is lower and upper bounded. Therefore the noise of the new process is bounded and the same a.s. convergence applies as on-policy TD(0) (Tsitsiklis & Van Roy, 1997). Since A, b are the same as on-policy TD(0) for the target policy \u03c0, the convergence is to the same fixed point."}, {"heading": "9.2. Proof of Lemma 2", "text": "Let \u03c1\u0302d be an unbiased estimate of \u03c1d, and for every n = 0, 1, . . . , t define \u0393\u0303 n t . = \u03c1\u0302d(st\u2212n)\u0393 n t . Then:\nE\u00b5 [ \u0393\u0303nt |st ] = \u03c1d(st).\nProof. For any function on the state space u(s):\nE\u00b5 [\u0393 n t u(st\u2212n)|st] =\n\u2211\n(si) t\u22121 i=t\u2212n\nPr \u00b5 ((si)\nt\u22121 i=t\u2212n |st)\u0393 n t u(st\u2212n)\n= \u2211\n(si) t\u22121 i=t\u2212n\nPr\u00b5((si) t\u22121 i=t\u2212n , st)\nPr\u00b5(st) \u0393nt u(st\u2212n)\n= \u2211\n(si) t\u22121 i=t\u2212n\nPr\u00b5(st\u2212n) Pr\u03c0((si) t\u22121 i=t\u2212n , st|st\u2212n)\nPr\u00b5(st) u(st\u2212n)\n= \u2211\nst\u2212n\nPr\u00b5(st\u2212n) Pr\u03c0(st|st\u2212n)\nPr\u00b5(st) u(st\u2212n)\n=u\u22a4D\u00b5P n \u03c0 D \u22121 \u00b5 est ,\n(14)\nwhere est is the unit vector of state st. So, for an unbiased estimate of \u03c1d denoted \u03c1\u0302d we can define and derive:\n\u0393\u0303nt . =\u03c1\u0302d(st\u2212n)\u0393 n t = \u03c1\u0302d(st\u2212n)\nn\u22121\u220f\ni=0\n\u03c1t\u2212i\u22121,\n\u21d2 E\u00b5 [ \u0393\u0303nt |st ] = E [\u03c1\u0302d] \u22a4 D\u00b5P n \u03c0 D \u22121 \u00b5 est\n= \u03c1\u22a4d D\u00b5P n \u03c0 D \u22121 \u00b5 est = d\u22a4\u03c0 P n \u03c0 D \u22121 \u00b5 est = d\u22a4\u03c0D \u22121 \u00b5 est = \u03c1d(st).\n(15)"}, {"heading": "9.3. Proof of Lemma 3", "text": "Under the ergodicity assumption, denote the eigenvalues of P\u03c0 by 0 \u2264 \u00b7 \u00b7 \u00b7 \u2264 |\u03be2| < \u03be1 = 1. Then Y \u03b2 is a maxi6=1 (1\u2212\u03b2)|\u03bei| |1\u2212\u03b2\u03bei| -contraction in the L2-norm on the orthogonal subspace to \u03c1d, and \u03c1d is a fixed point of Y \u03b2 .\nProof. We first show that \u03c1d is a fixed point of Y \u03b2 :\nY \u03b2\u03c1d = (1\u2212 \u03b2)D \u22121 \u00b5 P \u22a4 \u03c0 (I \u2212 \u03b2P \u22a4 \u03c0 ) \u22121D\u00b5 ( D\u22121\u00b5 d\u03c0 )\n= (1\u2212 \u03b2)D\u22121\u00b5 P \u22a4 \u03c0 (I \u2212 \u03b2P \u22a4 \u03c0 ) \u22121d\u03c0 = (1\u2212 \u03b2)D\u22121\u00b5 (1\u2212 \u03b2) \u22121d\u03c0 = D\u22121\u00b5 d\u03c0 = \u03c1d\n(16)\nDue to similarity, the eigenvalues of Y \u03b2 are the same as these of (1\u2212\u03b2)P\u22a4\u03c0 (I\u2212\u03b2P \u22a4 \u03c0 ) \u22121 which is a stochastic matrix with eigenvalues (\n(1\u2212\u03b2)\u03bei 1\u2212\u03b2\u03bei )|S| i=1 , where for i = 1 we obtain the eigenvalue 1. Now for every vector orthogonal to \u03c1d denoted u,\nthe first eigenvalue has no effect on its spectral decomposition, which means that \u2016Y \u03b2u\u2016 \u2264 maxi6=1 (1\u2212\u03b2)|\u03bei| |1\u2212\u03b2\u03bei| \u2016u\u2016."}, {"heading": "9.4. Proof of Theorem 1", "text": "If the step sizes satisfy \u2211 t \u03b1t = \u2211 t \u03b1 d t = \u221e, \u2211 t(\u03b1 2 t + (\u03b1 d t )\n2) < \u221e, \u03b1t \u03b1dt\n\u2192 0, t\u03b1dt \u2192 0, and E [ (\u03b2n\u0393nt ) 2|st ] \u2264 C for\nsome constant C and every t, n, then after applying COP-TD(0, \u03b2), \u03c1\u0302d,t converges to \u03c1d almost surely, and \u03b8t converges to the fixed point of \u03a0\u03c0T\u03c0V .\nProof. We use a three timescales stochastic approximation analysis. The fastest process is d\u0302\u00b5(s)which converges naturally with time-step O(1\nt ):\nd\u0302\u00b5,t+1 = 1\nt+ 1\nt\u2211\nk=0\nesk = 1\nt+ 1 (td\u0302\u00b5,t + est) = d\u0302\u00b5,t +\n1\nt+ 1 (est \u2212 d\u0302\u00b5,t). (17)\nThe process d\u0302\u00b5,t converges almost surely to d\u00b5 by the strong law of large numbers. Our next process is \u03c1\u0302d,t, which we will show converges a.s. to \u03c1d with d\u0302\u00b5(s) = d\u00b5(s):\nLemma 4. The process:\nFt = \u03c1t\u22121(\u03b2Ft\u22121 + est\u22121), n(\u03b2) = \u03b2n(\u03b2) + 1\n\u03c1\u0302d,t+1(st) = \u03a0\u2206d\u00b5 ( \u03c1\u0302d,t(st) + \u03b1 d t ( F\u22a4t \u03c1\u0302d,t n(\u03b2) \u2212 \u03c1\u0302d,t(st) )) (18)\nConverges almost surely to \u03c1d.\nProof. We follow the notation from (Schuss & Borkar, 2009). We first specify the stochastic approximation using h(x) andMn+1:\nh(\u03c1\u0302d,t) =E [ F\u22a4t \u03c1\u0302d,t n(\u03b2) |st ] \u2212 \u03c1\u0302d,t(st) = estY \u03b2 \u03c1\u0302d,t \u2212 est \u03c1\u0302d,t = est(Y \u03b2 \u2212 I)\u03c1\u0302d,t,\nMn+1 = F\u22a4t \u03c1\u0302d,t n(\u03b2) \u2212 E [ F\u22a4t \u03c1\u0302d,t n(\u03b2) |st ] = F\u22a4t \u03c1\u0302d,t n(\u03b2) \u2212 estY \u03b2 \u03c1\u0302d,t.\n(19)\nNow there are several conditions that must follow - condition on the step sizes, conditions on the Martingale and conditions on the projection. If all of these are met than the process converegs to the fixed point of the projected operator \u03c1d.\nThe step size conditions follow by the theorem\u2019s assumption. Now we move on to the Martingale conditions. Obviously E [Mn+1|st] = 0. In order for E [ \u2016Mn+1\u2016 2|st ] to be bounded a.s., we use the assumption E [ (\u03b2n\u0393n)2 ] \u2264 C.\nSince Ft is the leading factor in E [ \u2016Mn+1\u2016 2|st ] (the others are naturally bounded depending quadratically on \u03c1\u0302d), and Ft = \u0393\u0303 \u03b2 = (1\u2212 \u03b2) \u2211\u221e n=0 \u03b2 n\u0393\u0303n+1t , the upper bound follows.\nNow let\u2019s consider the projection where we follow the discussion in (Schuss & Borkar, 2009), Section 5.4. Notice that h is Lipschitz and the eigenvalues around the fixed point are non-negative, therefore there\u2019s a stable invariant solution set. In addition, the projection is to a closed convex set \u2206d\u00b5 , so \u03c1\u0302d,t is bounded. Hence, our goal is to show that the projection is Lipschitz and that we can ignore its non-smooth boundary.\nThe projection to the simplex zeros some coordinates and decreases a constant from the other coordinates. If indeed it zeros some coordinates - we are at a problematic area of the space since the projection is not Frechet differentiable there (we are on the boundary of the set). However, because \u03c1d(s) > 0 (Assumption 1), the unprojected ODE repels \u03c1\u0302d from these problematic boundaries, and we can assume that after enough the steps the projection is simply a projection to the affine subspace \u2211\ns d\u00b5(s)u(s) = 1. In that case the projection is given by: \u03a0\u2206d\u00b5u = (I \u2212 1 \u2016d\u00b5\u20162 d\u00b5d \u22a4 \u00b5 )(u \u2212 1) + 1\nand its Frechet derivative is \u03a0\u0304\u2206d\u00b5u = (I \u2212 1 \u2016d\u00b5\u20162 d\u00b5d \u22a4 \u00b5 )u. This derivative is Lipschitz continuous which means that its composition with h is also Lipschitz .\nHence, the process converges to the solution set of \u03a0\u0304\u2206d\u00b5h(x) = 0 for \u03a0\u2206d\u00b5x = x. Under these constraints the only fixed point can be \u03c1d (intersection of the c \u00b7 \u03c1d line with the weighted simplex set \u2206d\u00b5).\nFinally, treating the \u03b8t process assuming \u03c1\u0302d,t already converged to \u03c1d, leaves us with Lemma 1. Since each process depends only on the previous ones, it is enough to show they converge independently as long as the step sizes satisfy the rate constraints."}, {"heading": "9.5. Proof of Theorem 2", "text": "If the step sizes hold \u2211 t \u03b1t = \u2211 t \u03b1 d t = \u221e, \u2211 t(\u03b1 2 t + (\u03b1 d t )\n2) < \u221e, \u03b1t \u03b1dt\n\u2192 0, t\u03b1dt \u2192 0, and E [ (\u03b2n\u0393nt ) 2|st ] \u2264 C for some\nconstant C and every t, n, then after applying COP-TD(0, \u03b2) with function approximation satisfying \u03c6\u03c1(s) \u2208 R k +, \u03c1\u0302d,t converges to the fixed point of \u03a0\u2206E\u00b5[\u03c6\u03c1]\u03a0\u03c6\u03c1Y \u03b2 denoted by \u03c1COPd almost surely, and if \u03b8t converges it is to the fixed point of \u03a0d\u00b5\u25e6\u03c1COPd T\u03c0V .\nProof. Similarly to the proof of Theorem , we can analyze the system on 3-time scales. The fastest process is d\u0302\u03c6\u03c1 which converges naturally with time-step O(1\nt ):\nd\u0302\u03c6\u03c1,t+1 = 1\nt+ 1\nt\u2211\nk=0\n\u03c6\u03c1(sk) = 1\nt+ 1 (td\u0302\u03c6\u03c1,t + \u03c6\u03c1(sk)) = d\u0302\u03c6\u03c1,t +\n1\nt+ 1 (\u03c6\u03c1(sk)\u2212 d\u0302\u03c6\u03c1,t). (20)\nThe process d\u0302\u03c6\u03c1 converges almost surely to E\u00b5[\u03c6\u03c1(s)] by the strong law of large numbers.\nWe now show that \u03c1\u0302d,t converges to \u03c1 COP d . The proof follows the same lines as that of Theorem 1, where two things changed: (a) The estimated value \u03c1\u0302d,t is now contained in a linear subspace spanned by \u03a6\u03c1, and (b) the projection changed as well from the d\u00b5 simplex to the E\u00b5[\u03c6\u03c1(s)] simplex.\nFirst we represent the corresponding A and b of the projected \u03c1d ODE as follows (we assume \u03b2 = 0, but the results are similar for general \u03b2):\nA = \u03a6\u22a4\u03c1 D\u00b5(D \u22121 \u00b5 P \u22a4 \u03c0 D\u00b5 \u2212 I)\u03a6\u03c1 = \u03a6 \u22a4 \u03c1 (P \u22a4 \u03c0 \u2212 I)D\u00b5\u03a6\u03c1, b = 0. (21)\nWe can now verify that a solution to Ax = b also holds the Projected COP equation: \u03a0 \u03c6\u03c1 d\u00b5 Y\u03a6\u03c1\u03b8\u03c1 = \u03a6\u03b8\u03c1 by multiplying it from the left by \u03a6\u22a4\u03c1 D\u00b5:\n\u03a6\u22a4\u03c1 D\u00b5\n[ \u03a0\n\u03c6\u03c1 d\u00b5 Y\u03a6\u03c1\u03b8\u03c1 \u2212 \u03a6\u03c1\u03b8\u03c1 ] = \u03a6\u22a4\u03c1 D\u00b5 [ \u03a6\u03c1 ( \u03a6\u22a4\u03c1 D\u00b5\u03a6\u03c1 )\u22121 \u03a6\u22a4\u03c1 D\u00b5 ( D\u22121\u00b5 P\u03c0D\u00b5 ) \u03a6\u03c1\u03b8\u03c1 \u2212 \u03a6\u03c1\u03b8\u03c1 ]\n= \u03a6\u22a4\u03c1 P\u03c0D\u00b5\u03a6\u03c1\u03b8\u03c1 \u2212 \u03a6 \u22a4 \u03c1 D\u00b5\u03a6\u03c1\u03b8\u03c1 = \u03a6\u22a4\u03c1 (P\u03c0 \u2212 I)D\u00b5\u03a6\u03c1\n(22)\nLet\u2019s look on the new projection \u03a0\u2206E\u00b5[\u03c6\u03c1] . Since we demanded \u03c6\u03c1(s) \u2208 R k +, this set is close and bounded, so the convergence is guaranteed.\nIn order for the new projection\u03a0\u2206E\u00b5[\u03c6\u03c1] to be Frechet differentiable, we should verify it still avoids the boundaries meaning \u03b8\u03c1 > 0 coordinate-wise. However, evenwere this not true, we could simply throw away one of the features and get a smaller problem that does hold this condition, keeping the projection Frechet differentiable similarly to before. Subsequently \u03c1\u0302d,t converges to the fixed point of \u03a0\u2206E\u00b5[\u03c6\u03c1]\u03a0\u03c6\u03c1Y \u03b2 denoted by \u03c1COPd almost surely.\nMoving on to the last process, we get the following equations:\nA = lim t\u2192\u221e E\u00b5\n[ \u03c1t\u03c1 COP d (st)\u03c6t(\u03c6t \u2212 \u03b3\u03c6t+1) \u22a4 ] = \u03a6\u22a4D\u00b5diag(\u03c1 COP d )(I \u2212 \u03b3P\u03c0)\u03a6,\nb = lim t\u2192\u221e E\u00b5\n[ \u03c1t\u03c1 COP d (st)\u03c6tr \u22a4 t |sk = s ] = \u03a6\u22a4D\u03c0diag(\u03c1 COP d )R\u03c0 ,\n(23)\nleading us to the known convergence solution (if indeed the process converge, which is not necessarily true) which is the fixed point of \u03a0d\u00b5\u25e6\u03c1COPd T\u03c0V ."}, {"heading": "9.6. Proof of Corollary 1", "text": "Let 0 < \u01eb < 1. If (1 \u2212 \u01eb)\u03c1d \u2264 \u03c1 COP d \u2264 (1 + \u01eb)\u03c1d, then the fixed point of COP-TD(0,\u03b2) with function approximation \u03b8 COP satisfies the following, where \u2016 \u00b7 \u2016\u221e is the L\u221e induced norm:\n\u2016\u03b8\u2217 \u2212 \u03b8COP\u2016\u221e \u2264 \u01eb\u2016A \u22121 \u03c0 \u03a6 \u22a4\u2016\u221e ( Rmax + (1 + \u03b3)\u2016\u03a6\u2016\u221e\u2016\u03b8 COP\u2016\u221e ) , (24)\nwhere A\u03c0 = \u03a6 \u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6, and \u03b8 \u2217 sets the fixed point of the operator \u03a0d\u03c0T\u03c0V .\nProof. If (1\u2212 \u01eb)\u03c1d \u2264 \u03c1 COP d \u2264 (1 + \u01eb)\u03c1d, then we know that the weights of the projection hold:\n(1\u2212 \u01eb)d\u03c0 \u2264 d\u03c0\u0303 . = d\u00b5 \u25e6 \u03c1 COP d \u2264 (1\u2212 \u01eb)d\u03c0. (25)\nWe write the solution equations for both weight vectors\n(\u03a6\u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6)\u03b8 \u2217 = \u03a6\u22a4D\u03c0R (\u03a6\u22a4D\u03c0\u0303(I \u2212 \u03b3P\u03c0)\u03a6)\u03b8 COP = \u03a6\u22a4D\u03c0\u0303R\n(26)\nNow we subtract both equations and add and subtract (\u03a6\u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6)\u03b8 COP:\n(\u03a6\u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6)(\u03b8 \u2217 \u2212 \u03b8COP) = \u03a6\u22a4(D\u03c0 \u2212D\u03c0\u0303)R + (\u03a6 \u22a4(D\u03c0\u0303 \u2212D\u03c0)(I \u2212 \u03b3P\u03c0)\u03a6)\u03b8 COP (27)\nNow we take L\u221e norm on both sides, and use induced matrix sub-multiplicative property:\n\u2016\u03b8\u2217 \u2212 \u03b8COP\u2016\u221e =\u2016(\u03a6 \u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6) \u22121 ( \u03a6\u22a4(D\u03c0 \u2212D\u03c0\u0303)R+ (\u03a6 \u22a4(D\u03c0\u0303 \u2212D\u03c0)(I \u2212 \u03b3P\u03c0)\u03a6)\u03b8 COP ) \u2016\u221e\n\u2264\u2016(\u03a6\u22a4D\u03c0(I \u2212 \u03b3P\u03c0)\u03a6) \u22121\u2016\u03a6\u22a4\u2016\u221e ( (D\u03c0 \u2212D\u03c0\u0303)R\u2016\u221e + \u2016((D\u03c0\u0303 \u2212D\u03c0)(I \u2212 \u03b3P\u03c0)\u03a6)\u03b8 COP\u2016\u221e ) \u2264\u2016A\u22121\u03c0 \u03a6 \u22a4\u2016\u221e ( \u2016D\u03c0 \u2212D\u03c0\u0303\u2016\u221e\u2016R\u2016\u221e + \u2016D\u03c0 \u2212D\u03c0\u0303\u2016\u221e\u2016I \u2212 \u03b3P\u03c0\u2016\u221e\u2016\u03a6\u2016\u221e\u2016\u03b8 COP\u2016\u221e ) \u2264\u01eb\u2016A\u22121\u03c0 \u03a6 \u22a4\u2016\u221e ( Rmax + (1 + \u03b3)\u2016\u03a6\u2016\u221e\u2016\u03b8 COP\u2016\u221e )\n(28)"}, {"heading": "9.7. More details on the experiments", "text": "Experiments for Figure 1: 100 states chain MDP, with probability 0.51 to move left / right for the behavior / target policy. Results were taken after T = 1e6 iterations. For COP-TD we used \u03b2 = 0 and a constant step size 0.5. For Log-COP-TD we used \u03b2 = 0, \u03b3log = 0.9999 and constant step size 0.5. The experiment was conducted 10 times and the standard deviation is given as shading in the graph.\nFor the mountain car experimentwe used the simulator given by https://jamh-web.appspot.com/download.htm. The state aggregation was obtained by running kmeans with 100 clusters and taking the centers as representative states. The behavior policy was taken to be uniform over the 3 possible actions (-1, 0, 1), and the target policy chose these actions with probabilities (1/6,1/3,1/2) regardless of the state. COP-TD was applied with \u03b2 = 0 and constant step size 0.01, and Log-COP-TD was applied with \u03b2 = 0, \u03b3log = 0.9 and constant step size 0.01. Both algorithms ran for T = 1e6 iterations before the estimated \u03c1d was taken.\nExperiments for Figure 2: 100 states chain MDP, with probability 0.51 to move left / right for the behavior / target policy. For both algorithms constant step size 0.5. For Log-COP-TD we used \u03b2 = 0, \u03b3log = 0.9999 when sweeping on the other parameter. All experiments were conducted 10 times and we show the average result.\nIn the randomized MDP with 32 states we used uniform distribution over transition probabilities for two possible actions, and the target policy had p = 0.75 to choose one action where the behavior policy had p = 0.75 to choose the other action. All experiments were conducted 10 times and we show the average result.\nExperiments for Figure 3 were obtained by running COP-TD, Log-COP-TD, ETD and GTD over 4 setups. The distinct parameters of each algorithm were swiped over to find the best value: COP-TD\u2019s step size and \u03b2, Log-COP-TD\u2019s steps size, \u03b2 and \u03b3log, ETD\u2019s \u03b2 and GTD\u2019s step size. The step size of the main process and \u03bb were taken to be the same for all algorithms: step size = 0.05 and \u03bb = 0 (obtained also by sweeping over possible values). The simulators for acrobot and pole-balancing were taken from https://jamh-web.appspot.com/download.htm."}], "references": [{"title": "Application of reinforcement learning to routing in distributed wireless networks: a review", "author": ["Al-Rawi", "Hasan AA", "Ng", "Ming Ann", "Yau", "KokLim Alvin"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Al.Rawi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Al.Rawi et al\\.", "year": 2015}, {"title": "Applying reinforcement learning towards automating resource allocation and application scalability in the cloud", "author": ["Barrett", "Enda", "Howley", "Duggan", "Jim"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "Barrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2013}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "Projection onto a simplex", "author": ["Chen", "Yunmei", "Ye", "Xiaojing"], "venue": "arXiv preprint arXiv:1101.6081,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["Geist", "Matthieu", "Scherrer", "Bruno"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Geist et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Geist et al\\.", "year": 2014}, {"title": "Importance-weighted least-squares probabilistic classifier for covariate shift adaptation with application to human activity", "author": ["Hachiya", "Hirotaka", "Sugiyama", "Masashi", "Ueda", "Naonori"], "venue": "recognition. Neurocomputing,", "citeRegEx": "Hachiya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hachiya et al\\.", "year": 2012}, {"title": "Generalized emphatic temporal difference learning: Bias-variance analysis", "author": ["Hallak", "Assaf", "Tamar", "Aviv", "Munos", "Remi", "Mannor", "Shie"], "venue": "arXiv preprint arXiv:1509.05172,", "citeRegEx": "Hallak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Kober", "Jens", "Bagnell", "J Andrew", "Peters", "Jan"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Kober et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Td-gamma: Re-evaluating complex backups in temporal difference learning", "author": ["Konidaris", "George", "Niekum", "Scott", "Thomas", "Philip S"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Stochastic approximation and recursive algorithms and applications, volume 35", "author": ["Kushner", "Harold", "Yin", "G George"], "venue": "Springer Science & Business Media,", "citeRegEx": "Kushner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 2003}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["Mahmood", "A Rupam", "Sutton", "Richard S"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Mahmood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["Precup", "Doina", "Sutton", "Richard S", "Dasgupta", "Sanjoy"], "venue": "In ICML,", "citeRegEx": "Precup et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Stochastic approximation: A dynamical systems viewpoint", "author": ["Schuss", "Zeev", "Borkar", "Vivek S"], "venue": null, "citeRegEx": "Schuss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schuss et al\\.", "year": 2009}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "An emphatic approach to the problem of off-policy temporaldifference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "A new q (lambda) with interim forward view and monte carlo equivalence", "author": ["Sutton", "Rich", "Mahmood", "Ashique R", "Precup", "Doina", "Hasselt", "Hado V"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "A convergent o(n) temporal-difference algorithm for offpolicy learning with linear function approximation", "author": ["Sutton", "Richard S", "Maei", "Hamid R", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Lifetime value marketing using reinforcement learning", "author": ["Theocharous", "Georgios", "Hallak", "Assaf"], "venue": "RLDM", "citeRegEx": "Theocharous et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Theocharous et al\\.", "year": 2013}, {"title": "High confidence policy improvement", "author": ["Thomas", "Philip", "Theocharous", "Georgios", "Ghavamzadeh", "Mohammad"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "Policy evaluation using the omega-return", "author": ["Thomas", "Philip S", "Niekum", "Scott", "Theocharous", "Georgios", "Konidaris", "George"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["Tsitsiklis", "John N", "Van Roy", "Benjamin"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Off-policy td (\u03bb) with a true online equivalence", "author": ["van Hasselt", "Hado", "Mahmood", "A Rupam", "Sutton", "Richard S"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, Quebec City,", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "Investigating practical, linear temporal difference learning", "author": ["White", "Adam", "Martha"], "venue": "arXiv preprint arXiv:1602.08771,", "citeRegEx": "White et al\\.,? \\Q2016\\E", "shortCiteRegEx": "White et al\\.", "year": 2016}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "In COLT,", "citeRegEx": "Yu,? \\Q2015\\E", "shortCiteRegEx": "Yu", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).", "startOffset": 119, "endOffset": 183}, {"referenceID": 0, "context": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).", "startOffset": 119, "endOffset": 183}, {"referenceID": 1, "context": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).", "startOffset": 119, "endOffset": 183}, {"referenceID": 14, "context": "Our algorithm resembles (Sutton et al., 2015)\u2019s Emphatic TD that was extended by (Hallak et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 6, "context": ", 2015)\u2019s Emphatic TD that was extended by (Hallak et al., 2015) to the general parametric form ETD(\u03bb,\u03b2).", "startOffset": 43, "endOffset": 64}, {"referenceID": 2, "context": "Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al.", "startOffset": 195, "endOffset": 225}, {"referenceID": 2, "context": "Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al. (2014), Sutton et al.", "startOffset": 195, "endOffset": 269}, {"referenceID": 2, "context": "Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al. (2014), Sutton et al. (2015) for off-policy TD(\u03bb)).", "startOffset": 195, "endOffset": 291}, {"referenceID": 2, "context": "and is a \u03b3(1\u2212\u03bb) 1\u2212\u03bb\u03b3 -contraction (Bertsekas, 2012).", "startOffset": 34, "endOffset": 51}, {"referenceID": 5, "context": "we call \u03c1d the covariate shift ratio (as denoted under different settings by (Hachiya et al., 2012)).", "startOffset": 77, "endOffset": 99}, {"referenceID": 11, "context": "Among these are full IS (Precup et al., 2001) and ETD(\u03bb,\u03b2) (Sutton et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 14, "context": ", 2001) and ETD(\u03bb,\u03b2) (Sutton et al., 2015).", "startOffset": 21, "endOffset": 42}, {"referenceID": 11, "context": "For example, full IS-TD by (Precup et al., 2001) examines the ratio between the probabilities of the trajectory under both policies:", "startOffset": 27, "endOffset": 48}, {"referenceID": 24, "context": "Notice that a theorem is only given for \u03bb = 0, convergence results for general \u03bb should follow the work by Yu (2015). A possible criticism on COP-TD(0,\u03b2) is that it is not actually consistent, since in order to be consistent the original state space has to be small, in which case every off-policy algorithm is consistent as well.", "startOffset": 107, "endOffset": 117}, {"referenceID": 13, "context": "Recently, Sutton et al. (2015) had suggested an algorithm for off-policy evaluation called Emphatic TD.", "startOffset": 10, "endOffset": 31}, {"referenceID": 6, "context": "Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(\u03bb, \u03b2), which was shown to perform extremely well empirically by White & White (2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 6, "context": "Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(\u03bb, \u03b2), which was shown to perform extremely well empirically by White & White (2016). ETD(0, \u03b2) can be represented as:", "startOffset": 41, "endOffset": 163}, {"referenceID": 24, "context": "As mentioned before, ETD(\u03bb, \u03b2) converges to the fixed point of \u03a0fT \u03bb \u03c0 (Yu, 2015), where f = E [Ft|st] = (I \u2212 \u03b2P\u03c0) d\u03bc.", "startOffset": 71, "endOffset": 81}, {"referenceID": 6, "context": "Error bounds can be achieved by showing that the operator \u03a0fT \u03bb \u03c0 is a contraction under certain requirements on \u03b2 and that the variance of Ft is directly related to \u03b2 as well (Hallak et al., 2015) (and thus affects the convergence rate of the process).", "startOffset": 176, "endOffset": 197}, {"referenceID": 8, "context": "Subsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed.", "startOffset": 17, "endOffset": 41}, {"referenceID": 8, "context": "Subsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed. In a follow-up paper by Thomas et al. (2015b), the second assumption was also removed and the weights were instead given as: wn = We have conducted several experiments with an altered ETD and indeed obtained better results compared with the original, these experiments are outside the scope of the paper.", "startOffset": 17, "endOffset": 182}, {"referenceID": 8, "context": "Variance Weighted \u0393t As was shown by Konidaris et al. (2011), we can use statedependent weights instead of \u03b2 exponents to obtain better estimates.", "startOffset": 37, "endOffset": 61}, {"referenceID": 19, "context": "While this solution is impractical in problems with large state spaces parameterizing or approximating these variances (similarly to Thomas et al. (2015b)) could improve performance in specific applications.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "While a plethora of algorithms were suggested so far, ETD(\u03bb, \u03b2) by Hallak et al. (2015) has perhaps the simplest formulation and theoretical properties.", "startOffset": 67, "endOffset": 88}], "year": 2017, "abstractText": "The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(\u03bb, \u03b2)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD(\u03bb, \u03b2) can be designed to converge to the same value that would have been obtained by using on-policy TD(\u03bb) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call logCOP-TD(\u03bb, \u03b2). Both algorithms have favorable empirical results to the current state of the art online OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.", "creator": "LaTeX with hyperref package"}}}