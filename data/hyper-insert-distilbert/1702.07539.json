{"id": "1702.07539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Tight Bounds for Bandit Combinatorial Optimization", "abstract": "we revisit both the underlying study of optimal global regret rates in fast bandit combinatorial optimal optimization - - - a fundamental framework for reliable sequential differential decision making techniques under hidden uncertainty that abstracts upon numerous inconsistent combinatorial prediction problems. later we prove that such the attainable regret in this setting possibly grows as $ \\ le widetilde { \\ weighted theta } ( / k ^ { 3 / 2 } \\ sqrt { 3 dt } ) $ = where $ d $ is the random dimension index of forcing the resulting problem and $ ll k $ u is a strong bound over the maximal persistent instantaneous loss, likewise disproving again a certain conjecture of albert audibert, emil bubeck, and saul lugosi ( winter 2013 ) mathematicians who argued that the optimal rate solution should supposedly be of the canonical form $ \\ widetilde { \\ theta } ( \\ k \\ sqrt { dt } ) $. often our bounds apply locally to several dozen important n instances \u03bb of problem the operational framework, and in optimization particular, all imply making a tight bound p for the well - and studied secure bandit shortest path evaluation problem. challenged by challenging that, sometimes we also ourselves resolve, an open problem case posed by la cesa - bianchi and lugosi ( 2012 ).", "histories": [["v1", "Fri, 24 Feb 2017 11:17:33 GMT  (17kb)", "http://arxiv.org/abs/1702.07539v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alon cohen", "tamir hazan", "tomer koren"], "accepted": false, "id": "1702.07539"}, "pdf": {"name": "1702.07539.pdf", "metadata": {"source": "CRF", "title": "Tight Bounds for Bandit Combinatorial Optimization", "authors": ["Alon Cohen", "Tamir Hazan", "Tomer Koren"], "emails": ["alon.cohen@technion.ac.il", "tamir.hazan@technion.ac.il", "tkoren@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 53\n9v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n17\n? dT q where d is the dimension of the problem and k is a bound over\nthe maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form r\u0398pk ? dT q. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012)."}, {"heading": "1 Introduction", "text": "We consider the problem of online combinatorial optimization with bandit feedback, also known as bandit combinatorial optimization, or more succinctly as combinatorial bandits. The problem can be described as the following game between a learner and an environment, that proceeds for T rounds. On each round t \u201c 1, 2, . . . , T , the learner has to pick, possibly at random, an action xt from a subset S \u010e t0, 1ud of the hypercube in d-dimensions, with the property that each element x P S has at exactly k non-zero entries, that is \u0159di\u201c1 xi \u201c k. Simultaneously, the environment privately chooses a loss vector \u2113t P r0, 1sd. The learner then incurs the loss \u2113t \u00a8 xt P r0, ks and may observe only this loss (but not the vector \u2113t) as feedback. The goal of the learner throughout the T rounds of the game is to minimize her regret, defined as\nT\u00ff\nt\u201c1\n\u2113t \u00a8 xt \u00b4 min xPS\nT\u00ff\nt\u201c1\n\u2113t \u00a8 x .\nBandit combinatorial optimization is a fundamental primitive of sequential decision making under uncertainty, and abstracts several major problems in this context (see, e.g., Bubeck et al., 2012b). Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005). In this setting, a source station s repeatedly sends communication packets to a target station t through a network represented by a connected directed acyclic graph. On each decision round, the environment associates each edge in the network with a loss, and the learner suffers the loss accumulated over the edges in her chosen path. Each packet can be routed differently and the station has to pick routes so as to minimize the overall amount of time it takes the packets to arrive. In the bandit version of the problem, the only feedback that the source station observes is the roundtrip time of each packet\u2014namely the time it takes the packet to travel to its destination and return to the source.\nThe network routing problem can be cast in the online combinatorial optimization framework as follows: the set of all s-t paths can be represented as a set S \u010e t0, 1ud where d is the number of\nedges in the graph, and the non-zero entries in each x P S indicate the edges that are contained in the path x; then, if \u2113t P r0, 1sd is the loss vector that associates costs to edges in the network on decision round t, then the cost of path x is given by \u2113t \u00a8 x. The assumption that \u0159d i\u201c1 xi \u201c k for all x P S means that the length of an s-t path in the network is exactly k (which is also an upper bound on the maximal cost of any s-t path).\nThe study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret. Dani et al. (2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form rOpk ? dT q, and could be obtained by a clever adaptation of their algorithm.\nMore recently, Audibert et al. (2013) showed that the aforementioned rOpk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u2126pk ? dT q on the expected regret in\ncombinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques.\nIn this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r\u0398pk3{2 ? dT q. Namely,\nwe show a new lower bound of r\u2126pk3{2 ? dT q for combinatorial bandits that matches the best known upper bound up to logarithmic factors, and also holds (via simple adaptations) in the context of bandit shortest path. Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009).\nSurprisingly, the construction used in our lower bound is very simple and is based on straightforward adaptations of the one used by Audibert et al. (2013). Furthermore, our analysis is also significantly simpler and shorter than theirs. In a nutshell, the improvement in the bound is obtained via the following observation: when picking its randomized losses for fooling the learner, the environment can choose noisy vectors whose entries are strongly correlated with each other rather than being independent, as is the case in typical lower bound constructions (and, in particular, as suggested by Audibert et al., 2013).1 Since the learner never observes individual entries of the loss vectors and can only see a sum of k of them (for a particular choice of the action set S), she cannot exploit this correlation in any way. On the other hand, with correlated noise terms the observed loss value can have a variance that grows quadratically with k, rather linearly as is the case with i.i.d. noise, which directly deteriorates the learner\u2019s regret by an additional factor of ? k."}, {"heading": "1.1 Related work", "text": "Combinatorial bandit optimization is closely related to a somewhat more general online learning scenario known as bandit linear optimization, which was first considered by Dani et al. (2008) and Abernethy et al. (2008). In this setting, the decision set S is not restricted to subsets of\n1Note that the correlation discussed here is between different entries of the same loss vector, rather than between different loss vectors at different rounds. In particular, the loss vectors in our lower bound constructions are still chosen i.i.d. so our bounds also apply to the stochastic i.i.d. case.\nthe hypercube t0, 1ud and may be an arbitrary compact convex set in Rd; instead, the only requirement is that the loss the learner incurs by picking any action in S is bounded (say, by 1 in absolute value) for all possible loss vectors of the environment. State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.\nThe general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.\nA significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gyo\u0308rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Barto\u0301k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector \u2113t that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form \u0398p ? kdT q; see Audibert et al. (2013) and the references therein. For further and more detailed account on related partial information models and their regret analysis, we refer to the recent survey by Bubeck et al. (2012b)."}, {"heading": "2 Main results", "text": "We now state the main results of this paper. As our results are lower bounds on the learner\u2019s regret, we will henceforth focus on oblivious environments, that are required to choose the entire sequence \u21131, . . . , \u2113T before the game begins and thus do not react adaptively to the player\u2019s randomized decisions. (A lower bound for such environments also implies a lower bound for more general adaptive environments.) In this setup, we will give bounds on the expected regret, defined as\nRT \u201c E \u00ab T\u00ff\nt\u201c1\n\u2113t \u00a8 xt ff\n\u00b4 min xPS\nT\u00ff\nt\u201c1\n\u2113t \u00a8 x , (1)\nwhere the expectations are taken over the random choices of the learner. Our first result deals with the general combinatorial bandits setting and shows that if the environment is free to choose any action set S, the regret of the learner can be very large. Our lower bound is attained in the multitask bandit problem, in which a learner is simultaneously trying to solve k instances of the n-armed bandit problem (Auer et al., 2002) with n \u201c d{k (we assume for simplicity that the latter is an integer). At every round of the game, the learner plays k actions, one in each of the bandit problems, and observes the sum of the losses that correspond with these k actions. Then, the set S of actions is given as follows:\nS \u201c $ & %x P t0, 1u d : @j P rks jn\u00ff\ni\u201cpj\u00b41qn`1\nxpiq \u201c 1 , . - . (2)\nTheorem 1 (multitask MAB). Assume that n \u011b 2, and let the set of actions S \u010e t0, 1ud be as defined in Eq. (2). Any learning algorithm for the multitask bandit problem must incur at least r\u2126pk3{2 ? dT q expected regret in the worst case.\nThe bound in the theorem hides a factor of log\u00b41{2 T which is an artifact of our construction and is likely to be redundant. Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016).\nThe lower bound of Theorem 1 does not hold for any set S but rather to an instance of the multitask bandit problem. However, as we show in the following results, it still is general enough to imply lower bounds for two important instances of bandit linear optimization. Our next theorem gives a lower bound for the bandit shortest path problem, and shows that even when we limit the action set S to paths in a certain graph, the regret of the learner can still be forced to be large. Formally, given a connected DAG G \u201c pV,Eq with d edges and two nodes s, t P V , we define the set of actions S \u010e t0, 1ud as follows:\nS \u201c ! x P t0, 1ud : the set te P E : xpeq \u201c 1u forms an s-t path ) . (3)\nThen, we have the following:\nTheorem 2 (online shortest paths). Assume that k \u010f d{2. There exists a graph with d edges such that any s-t path has exactly k edges (see Figure 1), for which the action set S is defined as in Eq. (3). Against this graph any online learning algorithm for the bandit shortest path problem must suffer at least r\u2126pk3{2 ? dT q expected regret in the worst case.\nAgain, the theorem implies that the tight regret rate for bandit shortest path is r\u0398pk3{2 ? dT q, contrary to what was conjectured in the literature (Cesa-Bianchi and Lugosi, 2012). Our last main result shows a lower bound for the online ranking problem. This problem can be cast as finding a maximum matching in the complete bipartite graph Kk,n, that has d \u201c kn edges. The set of all of these matchings is represented by the action set S \u010e t0, 1ud, and the non-zero entries of every x P S indicate which edges participate in the matching that corresponds with x. Formally,\nS \u201c $ & %x P t0, 1u d : @j P rks jn\u00ff\ni\u201cpj\u00b41qn`1\nxpiq \u201c 1, @l P rns k\u00ff\ni\u201c1\nxppi\u00b4 1qn ` lq \u201c 1 , . - . (4)\nTheorem 3 (online ranking). Assume that k \u010f n{2. Consider the problem of online ranking between k and n elements, whose action set S is defined in Eq. (4). Any bandit learning algorithm for this problem must suffer at least r\u2126pk3{2 ? dT q expected regret in the worst case."}, {"heading": "3 Proofs", "text": ""}, {"heading": "3.1 Main result", "text": "In this section we prove Theorem 1. We show a lower bound of r\u2126pk3{2 ? dT q on the regret of any online learning algorithm applied to an instance of the multitask bandit problem. Surprisingly, the factor ? k improvement is obtained via a simple modification of previous constructions (Audibert et al., 2013). We start by applying Yao\u2019s minimax principle, implying that it suffices to show randomized strategy for the environment that forces any deterministic learning algorithm to suffer r\u2126pk3{2 ? dT q regret in expectation. We shall construct the environment\u2019s strategy as follows.\nSet \u01eb \u201c \u03c3 a\nkd{p4T q. Before the game begins, the environment chooses the best arm in each of the k problems in S uniformly at random, and denote the vector indicating this choice by x\u2039 P S. At every round t, the environment samples Zt \u201e N p0, \u03c32q. Denote the loss generated by environment on round t as L1tpiq \u201c 1{2\u00b4 \u01eb \u00a8 x\u2039piq ` Zt for i \u201c 1, 2, . . . , d.\nThe idea behind this construction is as follows. In order to avoid large losses and minimize her regret, the learner has to identify the best arm in each of the k subproblems, namely, to\nrecover x\u2039. Now, suppose that the losses of each coordinate were sampled independently, and each entry in L1t were to receive an i.i.d. sample of the Gaussian noise. Then the variance of the loss observed by the learner, namely of the random variable Lt \u00a8 x for any choice of x P S, is of the order of k. On the other hand, because of the correlation between the losses of the different coordinates in the construction above, the variance of the observed loss is of the order of k2. This allows us to gain and additional ? k factor in the lower bound on the regret. Note that crucially, the learner always observes a sum of k random noise terms and can never peek into the individual terms in the sum (this is due to the bandit feedback and the specific structure of the decision set S); hence, the correlation in the noise cannot be exploited by the learner and the increase in the overall variance comes at no price.\nFor the construction above, we have the following lemma.\nLemma 4. Any deterministic player must suffer regret of at least \u03c3k3{2 ? dT {8 in expectation against an environment that plays the losses L1 1 , . . . , L1T .\nTo show that Theorem 1 holds we need to show that the learner suffers large regret against an environment that plays losses that are bounded in r0, 1sd. While the losses we have constructed L1 1 , . . . , L1T are unbounded, for the right choice of \u03c3 they are bounded with high probability. We now show that this allows us to obtain a lower bound on the regret against an environment that plays losses L1, L2, . . . , LT , such that Ltpiq \u201c clippL1tpiqq for clippaq \u201c maxtminta, 1u, 0u.\nTheorem 5. Assume that T \u011b kd and let \u03c32 \u201c 1{p192 ` 96 log T q. Any deterministic player must suffer an expected regret of at least \u03c3k3{2 ? dT {16 against an environment that plays the losses L1, . . . , LT .\nThe proof of Theorem 1 is now given by setting the value of \u03c3 into the bound in Theorem 5."}, {"heading": "3.2 Bandit shortest path", "text": "In this section we show a lower bound for the bandit shortest path problem, proving Theorem 2. Suppose without loss of generality that k and d are even, and that d is a multiple of k. We show a lower bound on the regret by constructing a graph that simulates the multitask bandit problem with k{2 problems of d{k arms each.\nThis graph is shown in Figure 1. The graph consists of d edges and d{2 ` k{2 ` 1 vertices set in k{2 layers. Each layer has an incoming vertex connected to d{k intermediate vertices, all of them connected to the same outgoing vertex. This outgoing vertex is the incoming vertex of the next layer and so forth. Note that to form an s-t path the learner has to pass through exactly one of the d{k vertices in each layer, and therefore every such path has exactly k edges.\nNow, given the losses L1, L2, . . . , LT generated by the environment of Section 3.1, we shall construct an environment for the shortest path problem such that the regret of the learner would be the same as the one in the proof of Theorem 1. Indeed, recall that the loss at coordinates pj\u00b41qd{k`1, . . . , jd{k is associated with the losses of the j\u2019th d{k-armed bandit problem. Then on round t for the j\u2019th layer of the graph, we can set the losses Ltppj \u00b4 1qd{k` 1q, . . . , Ltpjd{kq to the edges going from the incoming vertex to the intermediate vertices, and a loss of 0 to the edges going from the intermediate vertices to the outgoing vertex.\nTherefore, we have a bijection between any s-t path and a set of k{2 arms in the aforementioned multitask bandit problem, such that the sum of the losses on the edges of the s-t path and the sum of the losses of these k{2 arms are the same. We conclude by invoking Theorem 1 that says that any learner must suffer an expected regret of at least r\u2126pk3{2 ? dT q, as claimed."}, {"heading": "3.3 Online ranking", "text": "In this section we prove Theorem 3 by a similar construction to the one in Section 3.1, for which we present the following random environment.\nSet \u01eb \u201c \u03c3 a\nkd{p8T q. Before the game starts, the environment samples a maximum matching in Kk,n unfiromly at random, and denote the vector indicating this choice by x\n\u2039 P S, for the set S defined in Eq. (4). At every round t, the environment samples Zt \u201e N p0, \u03c32q. Denote the loss generated by the environment on round t as L1tpiq \u201c 1{2\u00b4 \u01eb \u00a8x\u2039piq`Zt for all i \u201c 1, 2, . . . , d.\nWe have the following lemma.\nLemma 6. Any deterministic player must suffer regret of at least \u03c3k3{2 ? dT {8 in expectation against an environment that plays the losses L1 1 , . . . , L1T .\nNow to prove Theorem 3, the result above can be adapted to bounded losses in the same manner as done in Theorem 5."}, {"heading": "4 Additional proofs", "text": ""}, {"heading": "4.1 Proof of Lemma 4", "text": "Proof. Let us denote by i\u2039 1 , . . . , i\u2039k the locations of the non-zero coordinates of the random variable x\u2039, arranged in increasing order. We next introduce the random variables T1, . . . , Tk, where each Tj is the number of times the learner played an xt such that xtpi\u2039j q \u201c 1. For each x P S, we introduce the notations Px and Ex indicating probability and expectation with respect to the marginal distributions under which x\u2039 \u201c x. Then,\nRT \u201c E \u00ab T\u00ff\nt\u201c1\nL1t \u00a8 xt \u00b4min xPS\nT\u00ff\nt\u201c1\nL1t \u00a8 x ff\n\u011b E \u00ab T\u00ff\nt\u201c1\nL1t \u00a8 xt \u00b4 T\u00ff\nt\u201c1\nL1t \u00a8 x\u2039 ff\n\u201c 1 nk\n\u00ff\nxPS\nEx\n\u00ab T\u00ff\nt\u201c1\nL1t \u00a8 xt \u00b4 T\u00ff\nt\u201c1\nL1t \u00a8 x ff\n\u201c 1 nk\n\u00ff\nxPS\n\u01eb \u00a8 Ex \u00ab k\u00ff\nj\u201c1\npT \u00b4 Tjq ff\n\u201c \u01eb \u02dc kT \u00b4 k\u00ff\nj\u201c1\n1\nnk\n\u00ff\nxPS\nEx rTjs \u00b8 , (5)\nand in order to proceed, we need to upper bound ExrTjs for each j. For every x P S and j P rks we introduce a new distribution, which is the same as Px except that the loss of coordinate i\u2039j is also 1{2 ` Zt. We shall refer to these new laws by Px,\u00b4j and Ex,\u00b4j. Let \u03bbt be the loss observed at time t, and \u03bb\nptq \u201c p\u03bb1, . . . , \u03bbtq be the losses observed up to and including time t. Then, since the sequence \u03bbpT q determines the actions of the learner over the entire game, and by Pinsker\u2019s inequality,\nExrTjs \u00b4 Ex,\u00b4jrTjs \u010f T \u00a8DTV \u00b4 Px,\u00b4j \u201d \u03bbpT q \u0131 , Px \u201d \u03bbpT q \u0131\u00af\n\u010f T c 1\n2 DKL\n` Px,\u00b4j \u201c \u03bbpT q \u2030 \u203a\u203aPx \u201c \u03bbpT q \u2030\u02d8 . (6)\nMoreover, by the chain rule of KL-divergence, DKL ` Px,\u00b4jr\u03bbpT qs \u203a\u203aPxr\u03bbpT qs \u02d8 equals\nT\u00ff\nt\u201c1\nE\u03bbpt\u00b41q\u201ePx,\u00b4j\n\u201d DKL \u00b4 Px,\u00b4j \u201d \u03bbt \u02c7\u030c \u02c7\u03bbpt\u00b41q \u0131 \u203a\u203a\u203aPx \u201d \u03bbt \u02c7\u030c \u02c7\u03bbpt\u00b41q \u0131 \u0131\u0304 . (7)\nConsider a single term in the sum, and recall that \u03bbpt\u00b41q determines the action xt chosen by the learner on round t. If xtpi\u2039j q \u201c 0, the loss observed under Px and Px,\u00b4j are the same, and the KL divergence is 0. If xtpi\u2039j q \u201c 1 then the observed losses under Px and Px,\u00b4j are both Gaussian whose means are \u01eb apart, and the variance of both of them is \u03c32k2. Therefore,\nDKL \u00b4 Px,\u00b4j \u201d \u03bbt \u02c7\u030c \u02c7\u03bbpt\u00b41q \u0131 \u203a\u203a\u203aPx \u201d \u03bbt \u02c7\u030c \u02c7\u03bbpt\u00b41q \u0131\u00af \u010f \u01eb 2\n2k2\u03c32 .\nPlugging the above back into Eq. (7),\nDKL \u00b4 Px,\u00b4j \u201d \u03bbpT q \u0131 \u203a\u203a\u203aPx \u201d \u03bbpT q \u0131\u00af \u010f T\u00ff\nt\u201c1\nPx,\u00b4j \u201c xtpi\u2039j q \u201c 1 \u2030 \u00a8 \u01eb 2\n2k2\u03c32 \u201c \u01eb\n2\n2k2\u03c32 Ex,\u00b4jrTjs ,\nand the latter back into Eq. (6), we get ExrTjs \u010f Ex,\u00b4jrTjs ` \u01ebT {p2k\u03c3q \u00a8 a\nEx,\u00b4j rTjs. Next, we need the following lemma that we prove on Section 4.2.\nLemma 7. In the conditions of Lemma 4 and by the construction above, we have\n1\nnk\n\u00ff\nxPS\nEx,\u00b4j rTjs \u201c T\nn .\nNote that n \u011b 2 by assumption. Therefore, for all j \u201c 1, 2, . . . , k,\n1\nnk\n\u00ff\nxPS\nExrTjs \u010f 1\nnk\n\u00ff\nxPS\nEx,\u00b4jrTjs ` \u01ebT 2k\u03c3 \u00a8 1 nk\n\u00ff\nxPS\nb Ex,\u00b4jrTjs\n\u010f 1 nk\n\u00ff\nxPS\nEx,\u00b4jrTjs ` \u01ebT\n2k\u03c3\nd 1\nnk\n\u00ff\nxPS\nEx,\u00b4jrTjs\n\u010f T 2 ` \u01ebT 2\u03c3\nc T\nkd ,\nsince d \u201c kn. Let us now return to Eq. (5). We can lower bound the regret as\nRT \u011b \u01eb \u02dc kT \u00b4 k\u00ff\nj\u201c1\n\u02dc T\n2 ` \u01ebT 2\u03c3\nc T\nkd\n\u00b8\u00b8\n\u201c \u01ebkT \u02dc 1\n2 \u00b4 \u01eb 2\u03c3\nc T\nkd\n\u00b8 .\nFor our choice of \u01eb, we get that \u01eb{p2\u03c3q a T {pkdq is at most 1{4, and so\nRT \u011b \u03c3 c kd\n4T \u00a8 kT\n\u02c6 1\n2 \u00b4 1 4\n\u02d9 \u201c \u03c3\n8 k3{2\n? dT ,\nas claimed."}, {"heading": "4.2 Proof of Lemma 7", "text": "Proof. For any choice i\u2039 1 , i\u2039 2 , . . . , i\u2039k, let us denote by xpi\u2039q the corresponding x\u2039 P S. Following Audibert et al. (2013), we consider\n\u00ff\nxPS\nEx,\u00b4j rTjs \u201c \u00ff\ni\u2039 1 ,...,i\u2039j\u00b41,i \u2039 j`1,...,i \u2039 k\n\u00ff\ni\u2039j\nExpi\u2039q,\u00b4j rTjs .\nNow, keeping i\u2039 1 , . . . , i\u2039j\u00b41, i \u2039 j`1, . . . , i \u2039 k fixed the distribution Pxpi\u2039q,\u00b4j is the same for any choice of i\u2039j and therefore, since at every round of the game the learner must choose exactly one arm in the j\u2019th problem, we must have\n\u0159 i\u2039j Expi\u2039q,\u00b4jrTjs \u201c T .\nPutting it all together, we obtain\n\u00ff\ni\u2039 1 ,...,i\u2039j\u00b41,i \u2039 j`1,...,i \u2039 k\n\u00ff\ni\u2039j\nExpi\u2039q,\u00b4j rTjs \u201c \u00ff\ni\u2039 1 ,...,i\u2039j\u00b41,i \u2039 j`1,...,i \u2039 k\nT \u201c nk\u00b41T ,\nand thus 1\nnk\n\u00ff\nxPS\nEx,\u00b4j rTjs \u201c 1 nk nk\u00b41T \u201c T n ."}, {"heading": "4.3 Proof of Theorem 5", "text": "Proof. Let X1,X2, . . . ,XT be the predictions of the learner against an environment that plays L1, L2, . . . , LT , and let R\u0302T be the regret attained by the learner,\nR\u0302T \u201c T\u00ff\nt\u201c1\nLt \u00a8Xt \u00b4min xPS\nT\u00ff\nt\u201c1\nLt \u00a8 x .\nAlso define the pretend-regret obtained by playing X1,X2, . . . ,XT against an enivronment that plays L1\n1 , L1 2 , . . . , L1T as\nR\u03021T \u201c T\u00ff\nt\u201c1\nL1t \u00a8Xt \u00b4min xPS\nT\u00ff\nt\u201c1\nL1t \u00a8 x .\nNow note that if it happens that at every round t, all coordinates of L1t are between 0 and 1, then R\u0302T \u201c R\u03021T . Denote this event by E. Then,\nErR\u03021T s \u010f ErR\u0302T s ` kT \u00a8 PrEcs \u201c RT ` kT \u00a8 PrEcs (8)\nwhere the inequality is true since the regret is at most kT with probability 1. It thus remains to upper bound the probability that E does not occur. We will show that PrEcs \u010f \u01eb{8, which by combining Eq. (8) and Lemma 4 would yield:\nRT \u011b \u03c3k3{2\n? dT\n8 \u00b4 \u03c3k\n3{2 ? dT\n16 \u201c \u03c3k\n3{2 ? dT\n16 ,\nas required. Now, for E to occur it suffices that \u01eb \u010f 1{4 and that Zt \u010f 1{4 for every round t. Since\n\u01eb \u201c c \u03c32kd\n4T \u010f\nd kd p192` 96 log T qT \u010f c 1 192 \u010f 1 4 ,\nby our choice of \u01eb and \u03c3 and since T \u011b kd by assumption, we have that the probability PrEcs is upper bounded by the probability that Zt \u0105 1{4 at some (at least one) round t. Employing the standard tail bound PpZ \u0105 xq \u010f expp\u00b4x2{2\u03c32q for the normal distribution and the union bound, the latter is bounded by\nT \u00a8 PrZ1 \u0105 1{4s \u010f T exp \u02dc \u00b4 1 2\u03c32 \u02c6 1 4 \u02d9 2 \u00b8\n\u201c T exp p\u00b4p6` 3 log T qq\n\u201c e\u00b46 1 T 2 .\nTherefore, for the probability that E does not occur to be at most \u01eb{8 it suffices to have\n16e\u00b46 1\nT 2 \u010f\nd 1\np192 ` 96 log T qT .\nRearranging the terms it then suffices to have T 3 \u011b 0.16 ` 0.08 log T , that holds for any T \u011b 1."}, {"heading": "4.4 Proof of Lemma 6", "text": "Proof. Let us denote by i\u2039 1 , . . . , i\u2039k the locations of the nonzero coordinates of the random variable x\u2039, arranged in increasing order. We next introduce the random variables T1, . . . , Tk, where each Tj is the number of times the learner played an xt such that xtpi\u2039j q \u201c 1. For each x P S, we introduce the notations Px and Ex indicating probability and expectation with respect to the marginal distributions under which x\u2039 \u201c x. Then,\nRT \u201c E \u00ab T\u00ff\nt\u201c1\nL1t \u00a8 xt \u00b4min xPS\nT\u00ff\nt\u201c1\nL1t \u00a8 x ff\n\u011b \u01eb \u02dc kT \u00b4 k\u00ff\nj\u201c1\npn\u00b4 kq! n!\n\u00ff\nxPS\nEx rTjs \u00b8 , (9)\nand in order to proceed, we need to upper bound ExrTjs for each j. For every x P S and j P rks we introduce a new distribution, which is the same as Px except that the loss of coordinate i\u2039j is also 1{2`Zt. We shall refer to these new laws by Px,\u00b4j and Ex,\u00b4j. From now on the proof proceeds similarly to that of Lemma 4, with the exception that Lemma 7 is replaced by the following lemma, whose proof can be found in Section 4.5.\nLemma 8. In the conditions of Lemma 6 and by the construction above, we have\npn \u00b4 kq! n!\n\u00ff\nxPS\nEx,\u00b4j rTjs \u010f T\nn\u00b4 k ` 1 .\nRecall that k \u010f n{2 by assumption, that in particular implies n \u00b4 k ` 1 \u011b 2 as well as n\u00b4 k ` 1 \u011b n{2. Therefore, for all j \u201c 1, 2, . . . , k,\npn\u00b4 kq! n!\n\u00ff\nxPS\nExrTjs \u010f pn\u00b4 kq!\nn!\n\u00ff\nxPS\nEx,\u00b4jrTjs ` \u01ebT 2k\u03c3 \u00a8 pn\u00b4 kq! n!\n\u00ff\nxPS\nb Ex,\u00b4jrTjs\n\u010f pn\u00b4 kq! n!\n\u00ff\nxPS\nEx,\u00b4jrTjs ` \u01ebT\n2k\u03c3\nd pn\u00b4 kq!\nn!\n\u00ff\nxPS\nEx,\u00b4jrTjs\n\u010f T 2 ` \u01ebT 2k\u03c3\nc 2T\nn .\nLet us now return to Eq. (9). Using the fact that n \u201c d{k, we can lower bound the regret as\nRT \u011b \u01ebkT \u02dc 1\n2 \u00b4 \u01eb \u03c3\nc T\n2kd\n\u00b8 ,\nwhich, by our choice of \u01eb, allows us to obtain the desired lower bound."}, {"heading": "4.5 Proof of Lemma 8", "text": "Proof. Recall that we sample x\u2039 uniformly at random from S, the set defined in Eq. (4), and denote by UpSq the uniform distribution over S. Then, recalling the random variables i\u2039 1 , i\u2039 2 , . . . , i\u2039k we can compute Ex\u2039\u201eUpSqEx\u2039,\u00b4j rTjs , by conditioning on i\u2039\n1 , . . . , i\u2039j\u00b41, i \u2039 j`1, . . . , i \u2039 k and taking the outer expectation only over i \u2039 j .\nNow, there are exactly n\u00b4 k` 1 possible ways to choose i\u2039j in order to complete a maximal matching. In addition, the distribution Px\u2039,\u00b4j is the same for any possible choice of i \u2039 j , and since at every round of the game the learner must choose exactly one position for the j\u2019th element, we must have\nEx\u2039\u201eUpSq \u201d Ex\u2039,\u00b4j rTjs \u02c7\u030c \u02c7 i\u20391 \u201c i1, . . . , i\u2039j\u00b41 \u201c ij\u00b41, i\u2039j`1 \u201c ij`1, . . . , i\u2039k \u201c ik \u0131\n\u201c 1 n\u00b4 k ` 1\n\u00ff\nx\u2039PS\n1ri\u20391\u201ci1,...,i\u2039j\u00b41\u201cij\u00b41,i\u2039j`1\u201cij`1,...,i\u2039k\u201ciksEx\u2039,\u00b4jrTjs\n\u010f T n\u00b4 k ` 1 ."}, {"heading": "5 Conclusion and open problems", "text": "In this paper, we gave a tight characterization of the optimal regret rate in bandit combinatorial optimization and proved that it grows as r\u0398pk3{2 ? dT q, disproving the conjectures of\nCesa-Bianchi and Lugosi (2012) and Audibert et al. (2013). Our lower bounds apply to important instances of the framework, including the bandit versions of the online shortest path and the online ranking problems.\nAn interesting direction for future work is to explore instance-specific bounds, i.e., bounds that depend on the structure of the specific action set S used by the learner. What are the geometric and combinatorial properties of the set S that dictate the optimal rate of regret in the induces learning problem? In particular, in the specific context of the bandit shortest path problem, what are the graph-theoretic properties of the network that govern the difficulty of the online problem? Even in extremely simple graphs, such as the two-dimensional directed grid over n2 nodes (where the s and t nodes are located in two opposite corners), characterizing the optimal rate of regret remains an open problem. We suspect such problems to be non-trivial already in full-information online combinatorial optimization, but expect the bandit setting to be particularly challenging.\nFor the problem of online ranking, Theorem 3 handles the case of k \u02c6 n permutations in which k is smaller than n. However, quantifying the rate of regret in the important case of full permutations (i.e., with k \u201c n) remains an open problem. In particular, is the optimal regret \u0398pn2 ? T q in this setting?"}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["J.D. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "In 21st Annual Conference on Learning Theory,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Regret in online combinatorial optimization", "author": ["J.-Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM journal on computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["B. Awerbuch", "R.D. Kleinberg"], "venue": "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2004}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S. Bubeck", "N. Cesa-Bianchi", "S.M. Kakade", "S. Mannor", "N. Srebro", "R.C. Williamson"], "venue": "In COLT,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2012}, {"title": "The price of bandit information for online optimization", "author": ["V. Dani", "S.M. Kakade", "T.P. Hayes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "The on-line shortest path problem under partial monitoring", "author": ["A. Gy\u00f6rgy", "T. Linder", "G. Lugosi", "G. Ottucs\u00e1k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Volumetric spanners: An efficient exploration basis for learning", "author": ["E. Hazan", "Z. Karnin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Karnin.,? \\Q2016\\E", "shortCiteRegEx": "Hazan and Karnin.", "year": 2016}, {"title": "Learning permutations with exponential weights", "author": ["D.P. Helmbold", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Helmbold and Warmuth.,? \\Q2009\\E", "shortCiteRegEx": "Helmbold and Warmuth.", "year": 2009}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Non-stochastic bandit slate problems", "author": ["S. Kale", "L. Reyzin", "R.E. Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kale et al\\.", "year": 2010}, {"title": "First-order regret bounds for combinatorial semi-bandits", "author": ["G. Neu"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pages 1360\u20131375,", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits", "author": ["G. Neu", "G. Bart\u00f3k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Neu and Bart\u00f3k.,? \\Q2016\\E", "shortCiteRegEx": "Neu and Bart\u00f3k.", "year": 2016}, {"title": "On the complexity of bandit linear optimization", "author": ["O. Shamir"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pages 1523\u20131551,", "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "Path kernels and multiplicative updates", "author": ["E. Takimoto", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth.", "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).", "startOffset": 50, "endOffset": 81}, {"referenceID": 15, "context": "Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005).", "startOffset": 153, "endOffset": 206}, {"referenceID": 10, "context": "Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005).", "startOffset": 153, "endOffset": 206}, {"referenceID": 9, "context": "Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009).", "startOffset": 90, "endOffset": 118}, {"referenceID": 1, "context": "The study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret.", "startOffset": 73, "endOffset": 103}, {"referenceID": 1, "context": "The study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret. Dani et al. (2008) and Abernethy et al.", "startOffset": 73, "endOffset": 371}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence.", "startOffset": 11, "endOffset": 35}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms.", "startOffset": 11, "endOffset": 218}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm.", "startOffset": 11, "endOffset": 483}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm.", "startOffset": 11, "endOffset": 711}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u03a9pk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al.", "startOffset": 11, "endOffset": 1452}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u03a9pk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r \u0398pk3{2 ? dT q.", "startOffset": 11, "endOffset": 1479}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u03a9pk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r \u0398pk3{2 ? dT q. Namely, we show a new lower bound of r \u03a9pk3{2 ? dT q for combinatorial bandits that matches the best known upper bound up to logarithmic factors, and also holds (via simple adaptations) in the context of bandit shortest path. Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009). Surprisingly, the construction used in our lower bound is very simple and is based on straightforward adaptations of the one used by Audibert et al. (2013). Furthermore, our analysis is also significantly simpler and shorter than theirs.", "startOffset": 11, "endOffset": 2138}, {"referenceID": 5, "context": "1 Related work Combinatorial bandit optimization is closely related to a somewhat more general online learning scenario known as bandit linear optimization, which was first considered by Dani et al. (2008) and Abernethy et al.", "startOffset": 187, "endOffset": 206}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008). In this setting, the decision set S is not restricted to subsets of Note that the correlation discussed here is between different entries of the same loss vector, rather than between different loss vectors at different rounds.", "startOffset": 11, "endOffset": 35}, {"referenceID": 11, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 1, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 12, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 13, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.", "startOffset": 58, "endOffset": 108}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms. The general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.", "startOffset": 58, "endOffset": 565}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms. The general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.", "startOffset": 58, "endOffset": 600}, {"referenceID": 1, "context": ", 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form \u0398p ? kdT q; see Audibert et al. (2013) and the references therein.", "startOffset": 8, "endOffset": 671}, {"referenceID": 1, "context": ", 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form \u0398p ? kdT q; see Audibert et al. (2013) and the references therein. For further and more detailed account on related partial information models and their regret analysis, we refer to the recent survey by Bubeck et al. (2012b).", "startOffset": 8, "endOffset": 857}, {"referenceID": 2, "context": "Our lower bound is attained in the multitask bandit problem, in which a learner is simultaneously trying to solve k instances of the n-armed bandit problem (Auer et al., 2002) with n \u201c d{k (we assume for simplicity that the latter is an integer).", "startOffset": 156, "endOffset": 175}, {"referenceID": 4, "context": "Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016).", "startOffset": 97, "endOffset": 119}, {"referenceID": 4, "context": "Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016). The lower bound of Theorem 1 does not hold for any set S but rather to an instance of the multitask bandit problem.", "startOffset": 97, "endOffset": 147}, {"referenceID": 5, "context": "Again, the theorem implies that the tight regret rate for bandit shortest path is r \u0398pk3{2 ? dT q, contrary to what was conjectured in the literature (Cesa-Bianchi and Lugosi, 2012).", "startOffset": 150, "endOffset": 181}, {"referenceID": 1, "context": "Surprisingly, the factor ? k improvement is obtained via a simple modification of previous constructions (Audibert et al., 2013).", "startOffset": 105, "endOffset": 128}, {"referenceID": 1, "context": "Following Audibert et al. (2013), we consider \u00ff", "startOffset": 10, "endOffset": 33}, {"referenceID": 1, "context": "Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013). Our lower bounds apply to important instances of the framework, including the bandit versions of the online shortest path and the online ranking problems.", "startOffset": 35, "endOffset": 58}], "year": 2017, "abstractText": "We revisit the study of optimal regret rates in bandit combinatorial optimization\u2014a fundamental framework for sequential decision making under uncertainty that abstracts numerous combinatorial prediction problems. We prove that the attainable regret in this setting grows as r \u0398pk3{2 ? dT q where d is the dimension of the problem and k is a bound over the maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form r \u0398pk ? dT q. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).", "creator": "LaTeX with hyperref package"}}}