{"id": "1702.07793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Residual Convolutional CTC Networks for Automatic Speech Recognition", "abstract": "effective deep learning filtering approaches have been widely used in both automatic cellular speech frequency recognition ( asr ) and additionally they have achieved a significant accuracy improvement. among especially, convolutional neural node networks ( and cnns ) practices have continually been revisited in studying asr than recently. however, most cnns used once in existing different work areas have generated less protocols than only 10 layers which may traditionally not be deep just enough to capture all human speech signal acquisition information. whilst in analyzing this paper, we currently propose establishing a seemingly novel deep and wide hierarchical cnn architecture then denoted as single rcnn - 256 ctc, which has abundant residual connections data and predicted connectionist temporal flow classification ( ctc ) random loss function. managing rcnn - ctc simultaneously is an end - complete to - part end system which can rapidly exploit temporal coding and spectral structures of identical speech signals simultaneously. essentially furthermore, we ultimately introduce just a ctc - controller based system combination, following which there is different complexity from the best conventional hierarchical frame - wise senone - based one. since the basic kernel subsystems ( adopted only in enabling the digital combination are different types and thus thereby mutually generally complementary to each like other. theoretical experimental results far show that including our proposed single system rcnn - ctc successfully can economically achieve the lowest word error margin rate ( above wer ) relying on between wsj and tencent with chat together data sets, compared to several highly widely used mixed neural compute network replacement systems than in traditional asr. in addition, the proposed system combination product can probably offer a continuous further error reduction on these two data sets, resulting in relative wer reductions of $ = 14. 91 \\ % $ and $ 6. 52 \\ % $ on : wsj dev93 and tencent share chat data sets : respectively.", "histories": [["v1", "Fri, 24 Feb 2017 22:49:13 GMT  (126kb,D)", "http://arxiv.org/abs/1702.07793v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yisen wang", "xuejiao deng", "songbai pu", "zhiheng huang"], "accepted": false, "id": "1702.07793"}, "pdf": {"name": "1702.07793.pdf", "metadata": {"source": "META", "title": "Residual Convolutional CTC Networks for Automatic Speech Recognition", "authors": ["Yisen Wang", "Xuejiao Deng", "Songbai Pu", "Zhiheng Huang"], "emails": ["WANGYS14@MAILS.TSINGHUA.EDU.CN", "SOPHIADENG@TENCENT.COM", "JOHNSONPU@TENCENT.COM", "ZHIHHUANG@TENCENT.COM"], "sections": [{"heading": null, "text": "Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an endto-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of 14.91% and 6.52% on WSJ dev93 and Tencent Chat data sets respectively.\n\u2217 Equal contribution."}, {"heading": "1. Introduction", "text": "Automatic Speech Recognition (ASR) is designed to automatically transcribe human speech into text. In the past several years, deep learning (Yu & Deng, 2014) has been successfully applied in ASR to boost the recognition accuracy. Very recently, CNN becomes an attractive model in ASR, which transforms speech signals into feature maps as used in computer vision (LeCun & Bengio, 1998). Compared to other deep learning architectures, CNN has several advantages: 1) CNN is suited to exploit local correlations of human speech signals in both time and frequency dimensions. 2) CNN has the capacity to exploit translational invariance in signals.\nMost of previous applications of CNN in ASR only used a few convolutional layers. One typical architecture usually contains several convolutional layers, followed by a number of recurrent layers and fully-connected feedforward layers. These CNN structures are often less than 10 layers1, which may not be deep enough to capture all the information of human speech signals, especially for long sequences. As a result, their WERs may be adversely affected. Also, the convergence speed is too slow for training this type of architecture for acoustic models in practice.\nTraditional acoustic model training is based on frame-wise cross entropy loss (CE), which requires pre-generated and aligned frame labels by hidden Markov model/Gaussian mixture model (HMM/GMM) paradigm. To simplify this process, Graves et al. (2006) introduced CTC objective function to infer speech-label alignments automatically without any intermediate process, leading to an end-to-end system for ASR. CTC technique has shown promising results in Deep Speech (Hannun et al., 2014; Amodei et al., 2015) and EESEN (Miao et al., 2015).\nMotivated by the above observations, a residual convolu-\n1One exception is LACE (Yu et al., 2016) which has about 20 layers, but it does not utilize CTC as proposed in this paper.\nar X\niv :1\n70 2.\n07 79\n3v 1\n[ cs\n.C L\n] 2\n4 Fe\nb 20\ntional neural networks architecture along with CTC loss system, denoted as RCNN-CTC, is proposed in this paper to boost the performance of ASR. RCNN-CTC has the following three advantages: 1) It is a CNN-based system which operates on both time and frequency dimensions. RCNN-CTC can model temporal as well as spectral local correlations and gain translational invariance in speech signals. 2) Its network architecture can be very deep (more than 40 layers) to obtain more expressive power and better generalization capacity through residual connections between layers, as inspired by Residual Networks (ResNets) (He et al., 2016). 3) RCNN-CTC can also be trained in an end-to-end manner thanks to the CTC loss. In addition to the proposed RCNN-CTC, we propose a CTCbased system combination to further enhance the recognition accuracy. The proposed combination is different from the conventional frame-wise senone-based one due to the fact that the former produces peak phone/label distribution while the latter produces frame-wise senone distribution. The basic subsystems adopted in our combination are RCNN-CTC, Bidirectional Long Short Term Memory (BLSTM) (Sak et al., 2014) and Convolutional Long short term memory Deep Neural Network (CLDNN) (Sainath et al., 2015). They have heterogeneous structures and are mutually complementary in producing transcriptions (see Section 4). Note that the CTC-based system combination may be difficult as the output of each basic subsystem is not frame-aligned and the scores are not well calibrated, thus the results cannot be simply averaged. We implement a series of procedures of time normalization, alignment and voting to address the above issue.\nIn summary, our contributions in this paper are threefolds: 1) We propose a residual convolutional neural networks architecture paired with CTC loss (RCNN-CTC) for ASR task. Such a deep and wide network has not been applied to ASR before in our knowledge; 2) We propose a novel CTC-based system combination, which can obtain significant reduction on WER in our experiments; 3) Empirically, our proposed single system RCNN-CTC can achieve lower WERs compared with other widely used neural network ASR systems on WSJ and Tencent Chat data sets. In addition, the proposed system combination can further reduce the WERs on these two data sets."}, {"heading": "2. Related Work", "text": "In the last few years, Recurrent Neural Networks (RNNs) have been widely used for sequential modeling due to its capability of modeling long history (Mikolov et al., 2010). As a sequential task, Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and Bidirectional LSTM (BLSTM) (Graves & Schmidhuber, 2005) have also been successfully applied to ASR, and they ad-\ndressed the drawbacks of RNN, such as the gradient vanishing problem. However, a disadvantage of LSTM is that it needs to store multiple gating neural responses at each time-step and unfold the time steps during training and test stages, which results in a computational bottleneck for long sequences, i.e., thousands of frames in ASR. CNN was introduced into ASR to alleviate the computational problem. In early work, only a few CNN layers were typically used. For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers. Amodei et al. (2015) used three convolutional layers as the feature preprocessing layers. Palaz et al. (2015) showed that CNN-based speech recognition which uses raw speech as input can be more robust. To the end, deep CNN (about 10 convolutional layers) showed great performance in noisy speech recognition (Qian & Woodland, 2016; Sercu & Goel, 2016).\nRecently, ResNet (He et al., 2016) has been shown to achieve compelling convergence and high accuracy in computer vision, which attributes to its identity mapping as the skip connection in residual blocks. Successful attempts along this line in ASR have also been reported very recently. Zhang et al. (2016a) proposed a deep convolutional network with batch normalization (BN), residual connections and convolutional LSTM structure. Convolutional LSTM uses convolutions to replace the inner products within LSTM units. Residual connections are used to train very deep network, and BN normalizes each layer\u2019s inputs to reduce internal covariance shift. The above techniques are employed to add more computation depth to the model while reducing the number of parameters at the same time. Another network architecture was proposed in (Zhang et al., 2016b), i.e., deep recurrent convolutional network with deep residual learning. They implemented several recurrent layers at the bottom, followed by deep full convolutional layers with 3 \u00d7 3 filters (but no pooling layer). Besides, they built four residual blocks among the CNN layers, with each residual block containing layers with the same number of feature maps to avoid extra parameters. Residual LSTM architecture was proposed in (Kim et al., 2017). In addition to the inherent shortcut paths between LSTM memory cells, they employed additional spatial shortcut paths between layer outputs. They showed that the residual LSTM architecture provided a large gain from increasing depth. However, these models still suffer from the computational bottleneck, due to the components of LSTM in their network architectures.\nYu et al. (2016) proposed another deep CNN with layerwise context expansion and location-based attention architecture (LACE). The layer-wise context expansion and location-based attention mechanism are implemented by element-wise matrix product and convolution operations without max-pooling or average-pooling. Moreover, they\nemployed four residual blocks, each having an identical structure, which is similar to ResNet. It is worth pointing out that they did not employ CTC loss. Consequently, LACE depends on the tedious label alignment process and cannot facilitate an end-to-end training framework."}, {"heading": "3. Residual Convolutional CTC Networks", "text": "As stated above, CNN and CTC both own excellent characteristics for ASR task, but the combination of these two components is not fully explored. In this paper, we propose a novel residual convolutional CTC networks architecture, namely RCNN-CTC, which is very deep (more than 40 layers) to get full value of CNN, residual connections and CTC."}, {"heading": "3.1. Residual CNN", "text": "Generally speaking, deep CNNs can improve generalization and outperform shallow networks. However, they tend to be more difficult to train and slower to converge. Residual Networks (ResNets) (He et al., 2016) have been proposed recently to ease the training of very deep CNNs. ResNet is composed of a number of stacked residual blocks, and each block contains direct links between the lower layer outputs and the higher layer inputs. The residual block (described in Figure 1) is defined as:\ny = F(x,Wi) + x, (1)\nwhere x and y are the input and output of the layers considered, and F is the stacked nonlinear layers mapping function. Note that identity shortcut connections of x do not add extra parameters and computational complexity. With the presence of residual connections, ResNet can improve the convergence speed in training. ResNet can also enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.\nRecently, Zagoruyko & Komodakis (2016) showed that\nwide residual networks (WRNs) are superior over the commonly used narrow and very deep counterparts (original ResNets), which widens the convolutional layers by adding more feature maps in each residual block. Note that more feature maps mean more computation. In order to get a trade-off between performance and computational complexity, we adopt the network architecture with width = 2, i.e., our network is 2 times wider of original ResNets architecture. The details of the proposed RCNNCTC network architecture is shown in Table 1. In particular, we use a large 41 \u00d7 11 filter with 32 feature maps and width = 1 as conv1, followed by 4 groups (each with size N , width = 2) of residual blocks defined in Figure 1, namely ResBlock1, ResBlock2, ResBlock3 and ResBlock4 (N = 5 and 2 for Tencent Chat and WSJ data respectively, due to the fact that the former data is larger than the latter).\nIn general, convolutions require a context window, thus conv1 is set by considering the input feature dimension and the empirical window size. We also employ batch normalization (BN) (Ioffe & Szegedy, 2015) technique in RCNNCTC, which is used for normalizing each layers input to reduce internal covariance shift. BN speeds up training and acts as a regularizer. The standard formulation of BN for CNN can be readily applied here, and we do not need the sequence-wise normalization of RNN (Amodei et al., 2015). Moreover, strided convolutions are an essential element of CNN. For RCNN-CTC applying striding is also a natural way to reduce the computational cost on time and frequency dimensions. We find that RCNN-CTC\u2019s performance is sensitive to the stride on the time dimension but\nnot on the frequency dimension. Unlike ResNets used in computer vision where ResBlock2 and ResBlock3 need to be set deeper than ResBlock1 and ResBlock4 to describe the shape or skeleton, each ResBlock has almost the same importance in ASR (i.e., N is identical for each ResBlock). In summary, our proposed RCNN-CTC has a deeper and wider network architecture, compared to the existing CNN-based systems in ASR."}, {"heading": "3.2. CTC", "text": "Traditional acoustic model training is based on frame-level labels with cross-entropy criterion (CE), which requires a tedious label alignment procedure. Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al., 2006) to automatically learn the alignments between speech frames and their label sequences, leading to an end-to-end training.\nTo align the network outputs with the label sequences, an intermediate representation of CTC path is introduced in (Graves et al., 2006). The label sequence z can then be mapped to its corresponding CTC paths. It is a one-tomany mapping because multiple CTC paths can correspond to the same label sequence. For example, both \u201cA A \u03c6 \u03c6 B C \u03c6\u201d and \u201c\u03c6 A A B \u03c6 C C\u201d are mapped to label sequence \u201cA B C\u201d, where \u03c6 is the blank symbol. We denote the set of CTC paths for z as \u03a6(z). The likelihood of z can thus be evaluated as a sum of the probabilities of its CTC paths:\nP (z|X) = \u2211\np\u2208\u03a6(z)\nP (p|X), (2)\nwhere X is the utterance consisting of speech frames and p is a CTC path. Given this distribution, we can derive the objective function of sequence labeling lnP (z|X). Since the objective function is differentiable, we can backpropagate these errors and further update the network parameters."}, {"heading": "4. CTC-based System Combination", "text": "With regard to the conventional system combination, its performance improvement is little, due to the slight difference among subsystems. Therefore, we propose a system combination method which takes the diversity and complementary among subsystems into account. As a result, our proposed system combination can obtain an absolute WER reduction of 1% on WSJ and Tencent Chat data sets."}, {"heading": "4.1. Subsystems Selection", "text": "Our selection of subsystems is guided by the following principles: compared to the transcription (ground truth) G, we first figure out the correct part/words Ci in the decoding text of each subsystem i. We then search for the combina-\ntion of subsystems and compute their union set of correct words U = \u22c3 i Ci. We define a maximal correct word rate (MCWR) as the selection criterion:\nMCWR = \u2211 w\u2208G I(w \u2208 U) |G| , (3)\nwhere I(\u00b7) is the indicator function that takes 1 if (\u00b7) is true and 0 otherwise, and |G| is the length of ground truth G.\nOur goal is to select the combination which achieves the highest MCWR while using a minimal number of subsystems at the same time. Through this method, we can use the least cost to find subsystems which are mutually complementary. This also provides a guideline to choose the combination which has a balance between recognition accuracy and combination costs. In our experiments, a small held-out data of WSJ is used to search for an optimal system combination via MCWR metric. Therefore, the following three subsystems2 are selected: 1) The proposed RCNN-CTC in Section 3; 2) BLSTM (Sak et al., 2014) which consists of several bidirectional LSTM layers; and 3) CLDNN (Sainath et al., 2015), which consists of convolutional layers, LSTM layers and DNN layers. Figure 2 demonstrates these subsystems network architectures. Due to the MCWR metric and their heterogeneous structures, they may be mutually complementary to each other, which\n2On the held-out data, two subsystems cannot achieve an acceptable MCWR (0.95) but three subsystems have already obtained a very high MCWR (0.98), while four subsystems\u2019 MCWR (0.98) is almost the same to the three ones.\nis confirmed in our experiments.\nAn illustrative example in WSJ data set is given below to explain the complementary of subsystems. Here, the ground truth is: CONTACTS STILL INSIDE OWENS CORNING HELP TOO and the output sentences of the three subsystems are:\n1. CONTACTS STILL INSIDE OWNS CORNING HELPED TOO\n2. CONTACT STILL INSIDE OWENS CORNING HELPED\n3. CONTACT STILL INSIDE OWNS CORNING HELP TOO\nThe incorrect words in the output of each subsystem are marked underline. We can see that each subsystem has its own defect (i.e., incorrect words). But the incorrect words are different among the three subsystems, and they can be mutually corrected to a certain extent. For example, compared to the ground truth word \u201cCONTACTS\u201d, the word \u201cCONTACT\u201d is incorrect in subsystem 2 and 3 while it is correct in subsystem 1. The hope is that via system combination, we can leverage multiple systems to have more correct words.\nIn summary, we select three different types of subsystems for combination, including CNN-based subsystems (RCNN-CTC), LSTM-based subsystems (BLSTM) and their mixture (CLDNN). We argue that CNN can have a global view on a long utterance via hierarchical feature abstraction from bottom up, while LSTM can capture the sequence information contained in long sentences. The system combination can thus realize both advantages."}, {"heading": "4.2. Challenges", "text": "Our proposed single system RCNN-CTC uses a CTC output. The system combination is thus CTC based and different from the frame-wise CE-based one, since the peak responses of CTC in each subsystem may mismatch. Besides, the output likelihood of each subsystem is not at the same scale of time, which confuses the decoding process of the Weighted Finite-State Transducer (WFST) used in our experiments. Inspired by ROVER (Fiscus, 1997), we propose our CTC-based system combination method (Figure 3) as follows. For each subsystem, after decoding with the WFST graph (TLG), 1-best hypothesis3 with confidence score is prepared for the following processes. Alignment and composition are applied to the hypotheses of various subsystems to generate a single composite word transition\n3We have tested top N (N > 1) hypotheses and found that the results are no better, see Section 5.4 for details.\nnetwork (WTN). Once the WTN is generated, we select the best scoring word from each branching path by a voting scheme to produce a new hypothesis."}, {"heading": "4.3. Alignment", "text": "Time Normalization. With regard to each subsystem, after searching the best lattice path, we get a hypothesis sequence with each item involving a label, confidence score, starting time and duration time, which may not be at the same scale due to the CTC decoding. Therefore, we need to unify the time length and rescale starting/duration time to the same scale before constructing a WTN.\nWTN Construction. After time normalization, we can align and combine the hypotheses sequences into a single composite WTN. In particular, one of the sequences is chosen as the base WTN (WTN-BASE), and other sequences are added to WTN-BASE word by word. Comparing the word in the sequence and the corresponding word in WTN-BASE, we adopt different operations for different conditions. 1) Correction. A branching point is created and the word transition arc is added to WTN-BASE; 2) Substitution. A branching point is created and the word transition arc is added to WTN-BASE; 3) Deletion. A branching point is created and the BLANK transition arc is added to WTN-BASE; 4) Insertion. A sub-WTN is created and inserted between the adjacent nodes in WTN-BASE to record the fact. Following the above procedure, we iteratively combine the lattice words until the final composite WTN is generated. Considering the example in section 4.1, if we select the output of subsystem 1 as WTN-BASE, the first word in WTN-BASE is \u201cCONTACTS\u201d while it is \u201cCONTACT\u201d in subsystem 2 and 3. This satisfies the substitution condition, we thus create a branching point and add the word transition arc of \u201cCONTACT\u201d to WTN-BASE. The rest words are processed in a similar way until the final single composite WTN is generated in Figure 4."}, {"heading": "4.4. Voting", "text": "Once the composite WTN has been generated, a voting module is employed to select the best scoring word sequence by searching the WTN. According to ROVER, there are three voting schemes, i.e., voting by 1) frequency of occurrence, 2) frequency of occurrence and average word confidence, and 3) frequency of occurrence and maximum confidence. Generally, the third voting scheme, i.e., frequency of occurrence and maximum confidence, usually reports the best results (Fiscus, 1997), which is thus adopted in our system combination. As for the choice of confidence score, we use the minimum Bayes risk score (Xu et al., 2011) to serve as maximum confidence."}, {"heading": "5. Experiments", "text": "We analyze the performance of our proposed RCNN-CTC and CTC-based system combination on a benchmark data set, Wall Street Journal (WSJ), and a large mobile chat data set, Tencent Chat from Tencent company. Tencent Chat data set contains about 2.3 million utterances which account for 1400 hours speech data."}, {"heading": "5.1. Experimental Setup", "text": "For WSJ data set, we use the standard configuration si284 for training, eval92 for validation and dev93 for test. Our input features are 40 dimensional filterbank features with delta and delta-delta configuration. The features are normalized via mean subtraction and variance normalization on the speaker basis.\nFor Tencent Chat data set, we use about 1400 hours internal speech data for training and an independent 2000 utterances for test. Our input features are 40 dimensional filterbank combined with 3 dimensional pitch features, and are normalized by per utterance mean and variance as there is no speaker information.\nWe use the Kaldi recipe (Povey et al., 2011) to prepare the dictionary for WSJ and Tencent Chat data sets. It in fact uses CMU dictionary and Sequitur G2P to prepare phone sequences for both English and Chinese words. Finally, we have 118 phones served as acoustic model output labels.\nOur decoding follows the WFST-based approach in EESEN\n(Miao et al., 2015). As for the language model, we apply the WSJ pruned trigram language model with expanded lexicon (Povey et al., 2011) in the ARPA format on WSJ data set. For Tencent Chat data set, we use 5-gram language model trained with about 6 billion tokens (120K vocabulary) corpus from an internal data set. All the networks use phone-based training by stochastic gradient descent optimization (SGD). The learning rates are initialized to be in the range of 4 \u00d7 10\u22125 to 1 \u00d7 10\u22124, and are exponentially decayed by a factor of 0.1 after every 10 epochs during training."}, {"heading": "5.2. Results on WSJ data set", "text": "We compare our proposed single system RCNN-CTC with several commonly used neural network baseline systems in ASR, i.e., BLSTM (Sak et al., 2014), CLDNN (Sainath et al., 2015) and VGG (Simonyan & Zisserman, 2014).\nBLSTM is implemented according to (Miao et al., 2015), which uses 4 bidirectional LSTM layers. At each layer, both the forward and the backward layers comprise 320 hidden units. CLDNN is implemented following (Amodei et al., 2015), which contains 3 convolutional layers, 3 bidirectional LSTM layers and 2 fully-connected layers. The kernel sizes of the three convolutional layers are (11, 21), (11, 11), (3, 3), and the strides are (3,2), (1,2), (1,1) respectively. Batch normalization and Relu activation function are also employed. Each LSTM layer consists of 896 hidden units and 2 fully-connected layers have 896 and 74 units respectively. VGG is implemented according to (Yu et al., 2016), which has 14 layers. First, there are 3 convolutional layers with small 3\u00d7 3 filters and 96 feature maps, followed by a max-pooling layer. Then, 4 convolutional layers with 192 feature maps and 4 convolutional layers with 384 feature maps are added, all using 3\u00d7 3 filters and max-pooling at the end. With regard to RCNN-CTC, we adopt the parameters in Table 1 with N = 2.\nTable 2 compares our proposed single system RCNN-CTC with baseline systems on WSJ data set. For all systems trained with CTC loss, we can observe that RCNN-CTC obtains WER of 5.35% and 8.99% on eval92 and dev93 respectively4 , which slightly outperforms BLSTM, VGG and CLDNN. We speculate the slight gain may be limited to the small data size of WSJ, as the proposed RCNNCTC cannot demonstrate its full system strength. We will observe much larger gain of RCNN-CTC vs. other systems when a larger Chat data set is used in Section 5.3. Moreover, we show additional results in Table 2 where we compare systems trained with CTC and CE. Here, we only take BLSTM system as an example. The results are similar for other systems. For BLSTM+CE, we use GMMHMM system (Rabiner, 1989) to generate the label alignment to train. The GMM-HMM system is trained with the maximum likelihood (ML) criterion and refined with the boosted maximum-mutual-information (BMMI) sequencediscriminative training criterion. As can be seen from the last two rows of Table 2, BLSTM+CTC slightly outperforms BLSTM+CE, whereas the former can be trained in an end-to-end manner while the latter requires label alignment.\nWe next proceed with the system combination experiments on WSJ data set. For a fair comparison, we only consider combinations of three subsystems6 among the four: RCNN-CTC, VGG-CTC, CLDNN-CTC and BLSTM-CTC. Table 3 shows all four possible combinations and their WER on eval92 and dev93 respectively. It is worth pointing out that the WERs of subsystem may not be a useful metric for selecting subsystems for combination.\n4Lower WER results on WSJ data set were reported in Kaldi Speech Recognition project5, however, these results were achieved using additional techniques including speaker-adaptive features, splice context for data preparation, and iVector for instantaneous adaptation.\n5https://github.com/kaldi-asr/kaldi 6As mentioned in Section 4.1, on a held-out data set, two subsystems cannot achieve an acceptable MCWR (0.95) but three subsystems have already obtained a very high MCWR (0.98), while four subsystems\u2019 MCWR (0.98) is almost the same to the three ones.\nInstead, it is the complementary among subsystems that really matters. For example, RCNN-CTC, VGG and CLDNN are top 3 single systems with regard to WER in Table 2, while their system combination results are 4.70%/8.04% on eval92 and dev93 respectively, which is the worst in Table 3. While our system combination has the lowest WER of 4.29%/7.65%, which indicate the effectiveness of MCWR subsystem selection method. The fact that both top 2 system combinations including RCNN-CTC suggests its supremacy over other systems. Moreover, we notice that the WERs of combined systems are all lower than the single system results in Table 2, indicating that system combination can always boost the recognition accuracy. Note that our proposed system combination achieves an absolute WER drop of 1.06% and 1.34% (or relative drop of 19.81% and 14.91%) on eval92 and dev93 respectively compared to the best single system RCNN-CTC."}, {"heading": "5.3. Results on Tencent Chat data set", "text": "In the following, we explore the performance of RCNNCTC and system combination on a large Chat data set. Here, we only demonstrate the results trained with CTC loss to avoid tedious label alignment work in CE. Baseline systems are the same to those in Section 5.2, but some network parameters are slightly adjusted. CLDNN uses the same network architecture, but the kernel sizes of the three convolutional layers are (11, 11), (5, 5), (3, 3), and the strides are (3,1), (1,1), (1,1) respectively. As for RCNNCTC, we again adopt the parameters in Table 1, where the difference is N = 5. With regard to BLSTM and VGG, parameters of these systems are the same as in Section 5.2.\nTable 4 summarizes the WERs of single systems on Tencent Chat data set. Compared to VGG, CLDNN and BLSTM, RCNN-CTC performs the best and obtains an absolute WER reduction of 0.77%, 0.68% and 0.77%, or relative WER reduction of 5.12%, 4.55% and 5.12% respectively. Furthermore, Table 4 confirms the advantages of deep CNN architecture for ASR tasks on large data sets. RCNN-CTC and VGG are both CNN-based systems, while RCNN-CTC has residual connections as described in Section 3, which allow it to have very deep network\ndepth (RCNN-CTC 40 layers vs. VGG 14 layers) and thus achieve higher accuracy.\nSimilar to the experiments on WSJ data set, we also carry out a series of experiments on Tencent Chat data set to further assess the proposed system combination. We again consider combinations of three subsystems only, with WER of all combinations are collectively listed in Table 5. Similar to the WSJ system combination, the combination of RCNN+BLSTM+CLDNN outperforms others, due to the maximal complementary of these three subsystems described by MCWR. As can be noticed, top 2 system combinations also both choose RCNN-CTC as one base subsystem, which reveals its superb capacity in ASR. WERR is the relative WER reduction of each combination with respect to the best single system RCNN-CTC in Table 4. Our proposed system combination can achieve WER of 13.33%, which accounts for an absolute WER drop of 0.93% or relative drop of 6.52% compared to RCNN-CTC.\nIn summary, the experimental results are representative to reveal the effectiveness of our proposed single system RCNN-CTC and CTC-based system combination."}, {"heading": "5.4. Analysis and Discussion", "text": "Choice of 1-best vs. N-best in system combination. As stated in Section 4, we choose 1-best hypothesis for combination, because we find that N-best is no better than 1- best in our experiments, as shown in Table 6. Here N-best\n(N=10) distinct hypotheses of each subsystem are prepared for combination. Firstly, if we use the voting scheme in Section 4.4, i.e., maximal confidence score voting, choosing N-best does not offer any further benefits. Although N-best hypotheses make the WTN contain more branchings and words choices, maximal confidence score voting almost gets the same result as with 1-best hypothesis. The first two rows of Table 6 verify the above conclusions. Moreover, we conduct another experiment using frequency of occurrence as voting score for N-best subsystems combination. We find that the results are close to 1- best on WSJ data set but slightly worse on Tencet Chat data set. This is because that one subsystem\u2019s error may repeat many times in N-best hypotheses, which distorts the following frequency-based voting. Furthermore, considering the computational cost of N-best hypotheses, 1-best from each subsystem with maximal confidence score may be preferred."}, {"heading": "6. Conclusions", "text": "In this paper, we proposed a novel residual convolutional neural networks architecture trained with CTC loss (RCNN-CTC) for ASR. We argued that CNN is suited to exploit local correlations of human speech signals in both time and frequency dimensions, and has the capacity to exploit translational invariance in signals. In our proposed RCNN-CTC, we employ a wide and deep CNN architecture (more than 40 layers) with residual connections, which owns more expressive power and better generalization capacity. RCNN-CTC can be trained in an end-to-end manner thanks to the adoption of CTC loss, which effectively avoids the tedious frame alignment process. Furthermore, we proposed a CTC-based system combination via subsystems selection, alignment and voting procedures. Experiments on WSJ and Tencent Chat data sets show that, among widely used neural network systems in ASR, RCNN-CTC obtains the lowest WER. In addition, significant WER reductions are further obtained via our proposed system combination. For example compared to RCNN-CTC, the proposed system combination further results in relative WER reductions of 14.9% and 6.52% on WSJ dev93 and Tencent Chat data sets respectively."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Deng", "Li", "Penn", "Gerald", "Yu", "Dong"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Amodei", "Dario", "Anubhai", "Rishita", "Battenberg", "Eric", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Chen", "Jingdong", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover)", "author": ["Fiscus", "Jonathan G"], "venue": "In Proceedings of 1997 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU", "citeRegEx": "Fiscus and G.,? \\Q1997\\E", "shortCiteRegEx": "Fiscus and G.", "year": 1997}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino J", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 23th International Conference on Machine Learning (ICML", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Residual lstm: Design of a deep recurrent architecture for distant speech recognition", "author": ["Kim", "Jaeyoung", "El-Khamy", "Mostafa", "Lee", "Jungwon"], "venue": "arXiv preprint arXiv:1701.03360,", "citeRegEx": "Kim et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "In The handbook of Brain Theory and Neural Networks,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Miao", "Yajie", "Gowayyed", "Mohammad", "Metze", "Florian"], "venue": "In Proceedings of 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Karafi\u00e1t", "Martin", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Proceedings of 11th Annual Conference of the International Speech Communication Association (Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["Palaz", "Dimitri", "Magimai-Doss", "Mathew", "Collobert", "Ronan"], "venue": "In Proceedings of 16th Annual Conference of the International Speech Communication Association (Interspeech", "citeRegEx": "Palaz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2015}, {"title": "The kaldi speech recognition toolkit", "author": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz"], "venue": "In Proceedings of 2011 IEEE workshop on Automatic Speech", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Very deep convolutional neural networks for robust speech recognition", "author": ["Qian", "Yanmin", "Woodland", "Philip C"], "venue": "arXiv preprint arXiv:1610.00277,", "citeRegEx": "Qian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2016}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Rabiner", "Lawrence R"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner and R.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner and R.", "year": 1989}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Sainath", "Tara N", "Weiss", "Ron J", "Senior", "Andrew", "Wilson", "Kevin W", "Vinyals", "Oriol"], "venue": "In Proceedings of 16th Annual Conference of the International Speech Communication Association (Interspeech 2015),", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "In Proceedings of 15th Annual Conference of the International Speech Communication Association (Interspeech", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Advances in very deep convolutional neural networks for lvcsr", "author": ["Sercu", "Tom", "Goel", "Vaibhava"], "venue": "arXiv preprint arXiv:1604.01792,", "citeRegEx": "Sercu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sercu et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Minimum bayes risk decoding and system combination based on a recursion for edit distance", "author": ["Xu", "Haihua", "Povey", "Daniel", "Mangu", "Lidia", "Zhu", "Jie"], "venue": "Computer Speech & Language,", "citeRegEx": "Xu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Automatic Speech Recognition: A Deep Learning Approach", "author": ["Yu", "Dong", "Deng", "Li"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Zhang", "Yu", "Chan", "William", "Jaitly", "Navdeep"], "venue": "arXiv preprint arXiv:1610.03022,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Deep recurrent convolutional neural network: Improving performance for speech recognition", "author": ["Zhang", "Zewang", "Sun", "Zheng", "Liu", "Jiaqi", "Chen", "Jingwen", "Huo", "Zhao", "Xiao"], "venue": "arXiv preprint arXiv:1611.07174,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "CTC technique has shown promising results in Deep Speech (Hannun et al., 2014; Amodei et al., 2015) and EESEN (Miao et al.", "startOffset": 57, "endOffset": 99}, {"referenceID": 1, "context": "CTC technique has shown promising results in Deep Speech (Hannun et al., 2014; Amodei et al., 2015) and EESEN (Miao et al.", "startOffset": 57, "endOffset": 99}, {"referenceID": 11, "context": ", 2015) and EESEN (Miao et al., 2015).", "startOffset": 18, "endOffset": 37}, {"referenceID": 2, "context": "To simplify this process, Graves et al. (2006) introduced CTC objective function to infer speech-label alignments automatically without any intermediate process, leading to an end-to-end system for ASR.", "startOffset": 26, "endOffset": 47}, {"referenceID": 6, "context": "2) Its network architecture can be very deep (more than 40 layers) to obtain more expressive power and better generalization capacity through residual connections between layers, as inspired by Residual Networks (ResNets) (He et al., 2016).", "startOffset": 222, "endOffset": 239}, {"referenceID": 18, "context": "The basic subsystems adopted in our combination are RCNN-CTC, Bidirectional Long Short Term Memory (BLSTM) (Sak et al., 2014) and Convolutional Long short term memory Deep Neural Network (CLDNN) (Sainath et al.", "startOffset": 107, "endOffset": 125}, {"referenceID": 17, "context": ", 2014) and Convolutional Long short term memory Deep Neural Network (CLDNN) (Sainath et al., 2015).", "startOffset": 77, "endOffset": 99}, {"referenceID": 12, "context": "In the last few years, Recurrent Neural Networks (RNNs) have been widely used for sequential modeling due to its capability of modeling long history (Mikolov et al., 2010).", "startOffset": 149, "endOffset": 171}, {"referenceID": 0, "context": "For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers.", "startOffset": 13, "endOffset": 39}, {"referenceID": 0, "context": "For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers. Amodei et al. (2015) used three convolutional layers as the feature preprocessing layers.", "startOffset": 13, "endOffset": 141}, {"referenceID": 0, "context": "For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers. Amodei et al. (2015) used three convolutional layers as the feature preprocessing layers. Palaz et al. (2015) showed that CNN-based speech recognition which uses raw speech as input can be more robust.", "startOffset": 13, "endOffset": 230}, {"referenceID": 6, "context": "Recently, ResNet (He et al., 2016) has been shown to achieve compelling convergence and high accuracy in computer vision, which attributes to its identity mapping as the skip connection in residual blocks.", "startOffset": 17, "endOffset": 34}, {"referenceID": 9, "context": "Residual LSTM architecture was proposed in (Kim et al., 2017).", "startOffset": 43, "endOffset": 61}, {"referenceID": 6, "context": "Recently, ResNet (He et al., 2016) has been shown to achieve compelling convergence and high accuracy in computer vision, which attributes to its identity mapping as the skip connection in residual blocks. Successful attempts along this line in ASR have also been reported very recently. Zhang et al. (2016a) proposed a deep convolutional network with batch normalization (BN), residual connections and convolutional LSTM structure.", "startOffset": 18, "endOffset": 309}, {"referenceID": 6, "context": "Residual Networks (ResNets) (He et al., 2016) have been proposed recently to ease the training of very deep CNNs.", "startOffset": 28, "endOffset": 45}, {"referenceID": 1, "context": "The standard formulation of BN for CNN can be readily applied here, and we do not need the sequence-wise normalization of RNN (Amodei et al., 2015).", "startOffset": 126, "endOffset": 147}, {"referenceID": 5, "context": "Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al.", "startOffset": 10, "endOffset": 71}, {"referenceID": 1, "context": "Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al.", "startOffset": 10, "endOffset": 71}, {"referenceID": 11, "context": "Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al.", "startOffset": 10, "endOffset": 71}, {"referenceID": 4, "context": ", 2015), we adopt the CTC objective (Graves et al., 2006) to automatically learn the alignments between speech frames and their label sequences, leading to an end-to-end training.", "startOffset": 36, "endOffset": 57}, {"referenceID": 4, "context": "To align the network outputs with the label sequences, an intermediate representation of CTC path is introduced in (Graves et al., 2006).", "startOffset": 115, "endOffset": 136}, {"referenceID": 18, "context": "Therefore, the following three subsystems2 are selected: 1) The proposed RCNN-CTC in Section 3; 2) BLSTM (Sak et al., 2014) which consists of several bidirectional LSTM layers; and 3) CLDNN (Sainath et al.", "startOffset": 105, "endOffset": 123}, {"referenceID": 17, "context": ", 2014) which consists of several bidirectional LSTM layers; and 3) CLDNN (Sainath et al., 2015), which consists of convolutional layers, LSTM layers and DNN layers.", "startOffset": 74, "endOffset": 96}, {"referenceID": 21, "context": "As for the choice of confidence score, we use the minimum Bayes risk score (Xu et al., 2011) to serve as maximum confidence.", "startOffset": 75, "endOffset": 92}, {"referenceID": 14, "context": "We use the Kaldi recipe (Povey et al., 2011) to prepare the dictionary for WSJ and Tencent Chat data sets.", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": "(Miao et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "As for the language model, we apply the WSJ pruned trigram language model with expanded lexicon (Povey et al., 2011) in the ARPA format on WSJ data set.", "startOffset": 96, "endOffset": 116}, {"referenceID": 18, "context": ", BLSTM (Sak et al., 2014), CLDNN (Sainath et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 17, "context": ", 2014), CLDNN (Sainath et al., 2015) and VGG (Simonyan & Zisserman, 2014).", "startOffset": 15, "endOffset": 37}, {"referenceID": 11, "context": "BLSTM is implemented according to (Miao et al., 2015), which uses 4 bidirectional LSTM layers.", "startOffset": 34, "endOffset": 53}, {"referenceID": 1, "context": "CLDNN is implemented following (Amodei et al., 2015), which contains 3 convolutional layers, 3 bidirectional LSTM layers and 2 fully-connected layers.", "startOffset": 31, "endOffset": 52}], "year": 2017, "abstractText": "Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an endto-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of 14.91% and 6.52% on WSJ dev93 and Tencent Chat data sets respectively. \u2217 Equal contribution.", "creator": "LaTeX with hyperref package"}}}