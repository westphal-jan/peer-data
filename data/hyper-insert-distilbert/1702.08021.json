{"id": "1702.08021", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Friends and Enemies of Clinton and Trump: Using Context for Detecting Stance in Political Tweets", "abstract": "stance detection, the task of only identifying matching the speaker's public opinion bias towards a very particular target, always has attracted the special attention here of policy researchers. this paper uniquely describes today a basically novel alternative approach for detecting stance events in twitter. we instead define a set of features in this order jointly to consider the context surrounding a respective target angle of interest with the final aim of primarily training a model for predicting the stance towards resolving the mentioned two targets. in quite particular, perhaps we initially are interested initially in investigating political debates in streaming social media. for emphasizing this main reason considering we initially evaluated our approach focusing simultaneously on two targets approved of the proposed semeval - 2016 initiative task6 on detecting covert stance in tweets, which are principally related to the predicted political campaign for the early 2016 u. s. senior presidential secondary elections : hillary clinton vs. donald trump. additionally for viewing the continued sake of psychological comparison with the ongoing state of in the art, we repeatedly evaluated our model against the dataset released reported in in the semeval - 2016 task 6 national shared online task card competition. seeing our results outperform the best used ones obtained by ranking participating teams, and show showing that finding information about enemies supporters and friends of particular politicians help in detecting stance towards them.", "histories": [["v1", "Sun, 26 Feb 2017 11:22:41 GMT  (64kb)", "http://arxiv.org/abs/1702.08021v1", "To appear in MICAI 2016 LNAI Proceedings"]], "COMMENTS": "To appear in MICAI 2016 LNAI Proceedings", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mirko lai", "delia iraz\\'u hern\\'andez far\\'ias", "viviana patti", "paolo rosso"], "accepted": false, "id": "1702.08021"}, "pdf": {"name": "1702.08021.pdf", "metadata": {"source": "CRF", "title": "Friends and Enemies of Clinton and Trump: Using Context for Detecting Stance in Political Tweets", "authors": ["Mirko Lai", "Paolo Rosso"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n08 02\n1v 1\n[ cs\n.C L\n] 2\n6 Fe"}, {"heading": "1 Introduction", "text": "Social media provide a way for expressing opinions about different topics. From this kind of user-generated content it is possible to discover relevant information under several perspectives. A wide range of research has been carried out in order to exploit the vast amount of data generated in social media. One of the most interesting research areas concerns to investigate how people expose their feelings, evaluations, attitudes and emotions. These kinds of aspects are the subject of interest of Sentiment Analysis (SA) [1].\nDetermining the subjective value of a piece of text is the most general task of SA. Recently, the interest on studying finer-grained and different facets of sentiment in texts has derived in areas such as Aspect based sentiment analysis [2] and Stance Detection (SD) [3], which is the focus of our work. Identifying the speaker\u2019s opinion towards a particular target is the main goal of SD. It is not enough to recognize whether or not a text is positive/negative/neutral but it is necessary to infer the point of view of the tweeter towards a particular target.\nStance detection could not only provide useful information for improving the performance of SA but it could also help to better understand the way in which\npeople communicate ideas in order to highlight their point of view towards a particular target entity. This is particularly interesting when the target entity is controversial issue (e.g., political reforms [4,5]) or a polarizing person (e.g., candidates in political elections). Therefore, detecting stance in social media could become a helpful tool for various sectors of society, such as journalism, companies and government, having politics as an especially good application domain. Several efforts have been made in order to investigate different aspects related to social media and politics [6]. We are interested in political debates in social media, particularly in the interaction between polarized communities. We consider that being able to detect stance in user-generated content could provide useful insights to discover novel information about social network structures. Political debate texts coming from social media where people discuss their different points of view offer an attractive information source.\nThis year, for the first time a shared task on stance detection in tweets was organized [3]. Two of the targets considered in order to evaluate stance detection systems were: Hillary Clinton and Donald Trump3. Both targets have been the focus of different research, for instance in [7] the authors studied their speeches during the 2016 political campaign. In such way, studying these targets is an attracting topic of research due to the impact of the use of social media during the political campaign for the 2016 U.S. Presidential elections.\nOur approach to detect stance in tweets relies mainly on the context of the targets of interest: Hillary Clinton and Donald Trump. Besides, we also took advantage of widely used features in SA.\nThe paper is organized as follows. Section 2 introduces the first shared task on Twitter stance detection. Section 3 describes our method to detect stance by exploiting different features. Section 4 describes the evaluation and results. Finally, Section 5 draws some conclusions."}, {"heading": "2 Detecting Stance on Tweets", "text": "The SemEval-2016 Task 6: Detecting Stance in Tweets4 was the first shared task on detecting stance from tweets. Mohammad et. al in [3] describe the task as: Given a tweet text and a target entity (person, organization, movement, policy, etc.), automatic natural language systems must determine whether the tweeter is in favor of the target, against the given target, or whether inference is likely.\nLet us to introduce the following example5:"}, {"heading": "Support #independent #BernieSanders because he\u2019s not a liar. #POTUS #libcrib #democrats #tlot #republicans #WakeUpAmerica #SemST", "text": "The target of interest is \u201cHillary Clinton\u201d. Here, the tweeter expresses a positive opinion towards an adversary of the target. Consequently the annotator\n3 They are the candidates who won the Party Presidential Primaries for the Democratic and Republican parties, respectively. 4 http://alt.qcri.org/semeval2016/task6/ 5 This tweet was extracted from the training set of SemEval-2016 Task 6.\ninferred that the tweeter expresses a negative opinion towards the target. As can be noticed, this tweet does not contain any explicit clue to find the target.\nFor evaluating the task, the organizers annotated near to 5,000 English tweets for stance towards six commonly known targets in the United States: \u201cAtheism\u201d, \u201cClimate Change is a Real Concern\u201d, \u201cFeminism Movement\u201d, \u201cHillary Clinton\u201d, \u201cLegalization of Abortion\u201d, and \u201cDonald Trump\u201d (Stance Dataset, henceforth). A set of hashtags widely used by people when tweeting about these targets was compiled; then it was used to retrieve tweets according three categories: in-favor hashtags, against hashtags and stance-ambiguous hashtags. The tweets were manually annotated by crowdsourcing. More details about the Stance Dataset can be found in [3].\nThe participants in the SemEval-2016 Task 6 were required to classify tweettarget pairs into exactly one of three classes: Favor : It can be inferred from the tweet that the tweeter supports the target (e.g., directly or indirectly by supporting someone/something, by opposing or criticizing someone/something opposed to the target, or by echoing the stance of somebody else); Against : It can be inferred from the tweet that the tweeter is against the target (e.g., directly or indirectly by opposing or criticizing someone/something, by supporting someone/something opposed to the target, or by echoing the stance of somebody else); and Neither : None of the above.\nThe SemEval-2016 Task 6 was divided into two subtasks:\n\u2013 Task A. Supervised Framework. The participating systems were asked to perform stance detection towards the following targets: \u201cAtheism\u201d, \u201cClimate Change is a Real Concern\u201d, \u201cFeminism Movement\u201d, \u201cHillary Clinton\u201d, and \u201cLegalization of Abortion\u201d. For evaluation the organizer provided a training (2,914 tweets) and test (1,249 tweets) sets. \u2013 Task B. Weakly Supervised Framework. The task was detecting stance towards one target \u201cDonald Trump\u201d in 707 tweets. For this task the participants were not provided with any training data about this target.\nNineteen teams participated in Task A while only nine competed in Task B. It is important to highlight that only two systems were evaluated specifically on Task B. Figure 1 shows a brief summary of the systems. Further information about the systems in the task can be found in [8]6.\nBoth tasks were addressed in similar ways. Most teams exploited standard text classification features such as n-grams and word embedding vectors. Besides, some SA features from well-known lexical resources, such as EmoLex [9], MPQA [10], Hu and Liu [11] and NRC Hashtag [12], were used to detect stance in tweets. Furthermore, some teams decided to take advantage of additional data by harvesting Twitter using stance-bearing hashtags in order to have more stance tweets. It is important to highlight that the best system in Task A (MITRE) did use this alternative. A similar approach was adopted by the three best ranked systems on Task B (pkudblab, LitisMind, and INF-UFRGS). For what concerns\n6 Notice that not all the reports describing systems and approaches of teams participating at SemEval-2016 Task 6 are available in [8].\nto Task B, in order to deal with the lack of training data, some systems attempted to generalize the supervised data from task A in different ways such as defining rules or by exploiting multi-stage classifiers."}, {"heading": "3 Our approach", "text": "We are proposing a supervised approach for stance detection 7. Our work is focused on detecting stance towards Hillary Clinton and Donald Trump that are currently contesting the political campaign for the 2016 U.S. Presidential election. An important aspect to mention concerns to the fact that when the Stance Dataset was built the two targets were still participating to the Party Presidential Primaries for the Democratic and Republican parties, respectively. We address the stance detection in tweets, casting it as a classification task. A set of features that comprises different aspects was exploited. The most novel one refers to the extraction of context-related information regarding to the target of interest. Our hypothesis is that domain knowledge could provide useful information to improve the performance of SD systems. For instance, in order to correctly identify stance in a tweet as the one mentioned in Section 2, it is needed to recognize that Bernie Sander was an adversary of Hillary Clinton during the Party Presidential Primaries of the Democratic party. Attempting to capture information related to domain knowledge, we define two concepts: \u201cenemies\u201d and \u201cfriends\u201d. These concepts are used for denoting the entities related to the target. By using the terms \u201cenemies\u201d and \u201cfriends\u201d, we are trying to infer that when a tweeter is against an \u201cenemy\u201d/\u201cfriend\u201d of the target, then the tweeter is in favor/against towards the target and, on the other hand, when a tweeter is in favor towards an \u201cenemy\u201d/\u201cfriend\u201d of the target, then the tweeter is against/in favor towards the target. Figure 1 shows an example of the relationships between the \u201cfriends\u201d and \u201cenemies\u201d according to their political party, in this case the target of interest is Hillary Clinton. Three groups of features were considered: sentiment, structural, and contextbased.\n7 https://github.com/mirkolai/Friends-and-Enemies-of-Clinton-and-Trump"}, {"heading": "Sentiment-based Features", "text": "We shared the idea that stance detection is strongly related to sentiment analysis [3,16]. As far as we know, there are not sentiment analysis lexica retrieved specifically in the political domain8; thus, in order to take advantage of sentiment features it is possible to exploit the wide range of resources available for English. We used a set of four lexica to cover different facets of affect ranging from prior polarity of words to fine-grained emotional information:\n\u2013 AFINN. It is an affective lexicon of 2,477 English words manually labeled with a polarity value between -5 to +5. AFINN was collected by Finn A\u030arup Nielsen [28]. We consider one feature from AFINN: the sum of the polarity of the words present in each tweet.\n\u2013 Hu&Liu (HL). It includes about 6,800 positive and negative words. We calculate the difference between the positive and negative words in a tweet as a feature.\n\u2013 LIWC. The Linguistic Inquiry and Word Counts (LIWC) [29] is a dictionary that contains about 4,500 entries distributed in 64 categories that can be further used to analyse psycholinguistic features in texts. We calculate the difference between PosEmo (with 405 entries) and NegEmo (with 500 entries) categories in a tweet as a feature.\n\u2013 DAL. The Dictionary of Affect in Language (DAL) contains 8,742 English words; it was developed by Whissell [30]. Each word is rated in a three-point scale into three dimensions: Pleasantness (It refers to the degree of pleasure produced by words), Activation (It refers the degree of response that humans have under an emotional state) and Imagery (It refers to how difficult to form a mental picture of a given word is). We consider six features, i.e. the sum and the mean of the rates of the words present in the tweet for each one of the three dimensions."}, {"heading": "Structural Features", "text": "We also explore structural characteristics of tweets because we believe that could be useful to detect stance. We experimented with several kinds of structural features, however only the most relevant ones were included in the final approach:\n\u2013 Hashtags. The frequency of hashtags present in each tweet.\n\u2013 Mentions. The frequency of screen names (often called mentions) in each tweet.\n\u2013 Punctuation marks (punct marks). We consider a set of 6 different features: the frequency of exclamation marks, question marks, periods, commas, semicolons, and finally the sum of all the punctuation marks mentioned before.\n8 For example, the term vote is strongly related to politics, but it is not present in commonly used SA lexica such as: AFINN, Hu&Liu, and LIWC."}, {"heading": "Context-based Features", "text": "Our hypothesis is that the context-based features should capture some domainrelated information. An overall perspective of the context surrounding a target can be acquired by the relationships that exist between the target and other entities in its domain. As mentioned before we are interested in investigating Political debates: for this reason we selected as targets of interest politicians such as Hillary Clinton and Donald Trump. We manually created a list of entities related to the Party Presidential Primaries for the Democratic and Republican parties from Wikipedia9. We exploited 6 types of context-based features considering different kinds of relationships between the target and the entities around the target:\n\u2013 Target of interest mentioned by name (targetByName): This feature captures the presence of the target of interest in the tweet in hand. #StopHillary2016 HillaryClinton if there was a woman with integrity and honesty I would vote for such as woman president, NO. The list of tokens used to check the presence of the target of interest are: hillaryclinton, hillary, clinton, and hill for Hillary Clinton; while for Donald Trump are realdonaldtrump, donald, and trump.\n\u2013 Target of interest mentioned by pronoun (targetByPronoun): This feature allows to identify those cases when the target of interest is mentioned by using a pronoun. In the following example, knowing that the target of the tweet is Hillary Clinton, it is possible to exploit the pronoun \u201cshe\u201d to capture the presence of the target in hand. HomeOfUncleSam ScotsFyre RWNutjob1 SA Hartdegen She\u2019s too old to understand the internet...that she can be fact checked. Two pronouns were considered for each one of the targets of interest: she and her for Hillary Clinton, while he and his for Donald Trump.\n\u2013 Target\u2019s party (targetParty): As people involved in politics, our targets belong to a political party. Using this feature we identify if the stance against (or in favor) towards the target of interest was expressed mentioning the name of the party instead of the target. In the following example the tweeter expresses a negative opinion toward Hillary Clinton party. It\u2019s a miracle, suddenly #Democrats don\u2019t mind having someone who voted for war. In this case we consider the tokens dem, democratic, democrat, democrats, progressive in order to check the entity party for Hillary Clinton, while we consider the tokens republican, republicans, and conservative for Donald Trump.\n\u2013 Party colleague opposite (targetPartyColleagues): We also considered the case where the party colleagues of the target of interest are mentioned\n9 Articles: Democratic Party presidential primaries, 2016 and Republican Party presidential candidates, 2016\nto express an opinion towards it. We use the name and the surnames of the candidates for the Party Presidential Primaries for both Democratic and Republican parties. In the example, Hillary Clinton\u2019s party colleagues are mentioned. msnbc Lawrence JoeBiden SenSanders we love Joe and Bernie\u2013but they ARE too OLD\u2013they would end up a #OneTerm President #SemST The list of names used for Hillary Clinton is: bernie, sanders, martin, o\u2019malley, lincoln, chafee, webb, lawrence, and lessig; while for Donald Trump is: ted, cruz, marco, rubio, john, kasich, ben, carson, jeb, bush, rand, paul, mike, huckabee, carly, fiorina, chris, christie, rick, santorum, gilmore, rick, perry, scott, walker, bobby, jindal, lindsey, graham, george, pataki.\n\u2013 Target\u2019s oppositors party (targetsOppositors): This feature captures the presence of oppositors belonging to the rival party of target of interest\u2019s. In the following example a positive opinion is expressed towards two candidates from the Republican party. Thus, the tweet is against Hillary Clinton. PhilGlutting megadreamin Thank you so much for RT and FAV!!! #WakeUpAmerica #Rubio2016 #Cruz2016 #SemST We use the Donald Trump\u2019s tokens lists targetParty and targetPartyColleagues in order to create Hillary Clinton\u2019s targetsOppositors tokens list, while we use Hillary Clinton\u2019s tokens lists targetParty and targetPartyColleagues in order to create Donald Trump\u2019s targetsOppositors tokens list.\n\u2013 Nobody (nobody): This feature allows to catch those cases where any of the above described entities are mentioned in a tweet. In the following example the term Ambassador refers to Chris Stevens, who served as the U.S. Ambassador to Libya and who was killed at Bengasi in 2012. The diplomat is related to Hillary Clinton in a situation not related with the election campaign10."}, {"heading": "I don\u2019t want to be appointed to an Ambassador post.", "text": "The example also shows how difficult is to infer the stance without a deep knowledge of the context.\nAfter the evaluation of participating systems, the organizers of Semeval-2016 Task 6 annotated the Stance Datataset for sentiment and target in order to explore the relationship between sentiment and stance [31,3]11. In particular, tweets were manually annotated by using two additional labels: Sentiment and Opinion Towards, used to mark the overall sentiment polarity of the tweet and information about the fact that opinion is expressed directly towards the target, respectively:\n\u2013 Sentiment. It can be positive, negative, neutral or none. \u2013 Opinion target. It can take three different values: (1) if a tweet expresses\nan opinion about the target; (2) if a tweet expresses an opinion related to an aspect of the target or related to something that is not the target; and (3) if there is not opinion expressed.\n10 https://en.wikipedia.org/wiki/J._Christopher_Stevens 11 Notice that this is the first publicly available Twitter dataset annotated with both\nstance and sentiment.\nWe decided to exploit such new labels, by enriching our model with corresponding labeled-based features, with the aim to experiment with both context and sentiment information provided by human annotators."}, {"heading": "4 Evaluation", "text": "We experimented with a set of tweets belonging to Hillary Clinton and Donald Trump from the Stance Dataset, the Table 2 shows the distribution of tweets annotated with stance in the training and the test set for our targets of interest.\nWe evaluated our approach by using the same measure defined in [3] in order to compare our results with those participating in the task. We trained a Gaussian Naive Bayes classifier [32] implemented in Scikit-learn Python library12 to built a model for identifying stance in tweets. We adopted two experimental setting: a) experiment1. It means to the use of the Sentiment-based, Structural and Context-based features; b) experiment2. It refers to the use of all the features described in Section 3 including the labeled-based ones. Besides, we experimented using different feature combinations in order to identify which kinds of features could be more relevant for stance detection.\nTables 3 and 4 present the best results obtained for Hillary Clinton in the experiment1 and experiment2, respectively. Moreover, those obtained by using\n12 http://scikit-learn.org/\nthe same set of features for Donald Trump are shown. From the results can be noted that the F1-score in \u201cagainst\u201d class is higher than in \u201cfavor\u201d. Interestingly, the opposite happens for Donald Trump. The results in Table 4 are higher than those from Table 3. Table 5 shows the best results for Donald Trump using for both experiment1 and experiment2.\nAs can be noted the context-based features seem to be so relevant for both targets. Besides, it is important to highlight that the best result for each target was not achieved by the same set of features. This is maybe not surprising, if we consider the different political campaign marketing strategies of the two candidates, which can influence also the communication of candidates\u2019 oppositors and supporters, both in terms of language register used and addressed topics. For the sake of comparison with the state of the art, we present the results obtained by the three best ranked systems at SemEval-2016 Task 6. We only include the results concerning to Hillary Clinton and Donald Trump. Both the F-measure average and the rank position of each system are included in the Table 6. We also show our best results for the two targets using both experimental settings as well as the position in the official ranking in the shared task.\nOur approach achieves strongly competitive results. We ranked in the first position for both Task A and Task B using the experiment2 setting considering Hillary Clinton and Donald Trump results. For what concerns to the experiment1 we ranked in the third position for Task A and the second one for Task B.\nThe obtained results outperform the baselines proposed in [3]13. Besides, our outcomes outrank those obtained by submissions from all teams participating in the shared task (both task A and B). Overall, the results for Hillary Clinton are higher than those for Donald Trump. This was in someway expected, due to the lack of a training set of tweets concerning the target Donald Trump."}, {"heading": "5 Conclusions", "text": "In this paper we have shown that including context-related information is crucial in order to improve the performance of stance detection systems. Experiments confirms that stance detection is highly dependent on the domain knowledge of the target in hand. Our approach relies on the presence of entities related to a target in order to try to extract the opinion expressed towards it. Besides, our proposal allows to infer the stance in both cases when the target is explicitly mentioned and also when it is not. The results obtained by exploiting contextrelated features outperforms those from the best ranked systems in the SemEval2016 Task 6.\nLet us highlight that we are not using either n-grams or any word-based representation, but our approach mainly relies on the context of the target in hand. We plan to investigate the performance of our approach in different domains. Exploiting semantic resources in order to catch additional context information is also an interesting line for future research. Also user\u2019s information and her social network structure could be useful. For what concerns to the sentiment-related features, overall results confirm that these kinds of features help in identifying the stance towards a particular target. We exploited different sentiment-related features, ranging from those extracted from affective resources to manually assigned polarity labels.\n13 The authors experimented with n-grams, char-grams and majority class to establish the baselines for the task.\nA further interesting matter of future work could be explore also the stance w.r.t. different aspects of a political target entity. This means to perform a sort of aspect-based sentiment analysis in a political domain, e.g., a tweeter can be in favor of Hillary for aspects related to \u201cHealth\u201d, but not for other aspects.\nFinally, we think that it could be also interesting to investigate how to fruitfully combine information about stance and information about the presence of figurative devices in tweets, such as irony and sarcasm [33,34], since the use of such devices is very frequent in political debates also in social media and detecting irony and sarcasm have been considered as one of the biggest challenges for sentiment analysis."}, {"heading": "Acknowledgments", "text": "The National Council for Science and Technology (CONACyT Mexico) has funded the research work of Delia Irazu\u0301 Herna\u0301ndez Far\u0301\u0131as (218109/313683). The work of Paolo Rosso has been partially funded by the SomEMBED TIN201571147-C2-1-P MINECO research project and by the Generalitat Valenciana under the grant ALMAMATER (PrometeoII/2014/030). The work of Viviana Patti was partially carried out at the Universitat Polite\u0300cnica de Vale\u0300ncia within the framework of a fellowship of the University of Turin co-funded by Fondazione CRT (World Wide Style Program 2)."}], "references": [{"title": "Sentiment analysis and opinion mining", "author": ["B. Liu"], "venue": "Synthesis Lectures on Human Language Technologies 5", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Semeval-2016 task 5: Aspect based sentiment analysis", "author": ["M. Pontiki", "D. Galanis", "H. Papageorgiou", "I. Androutsopoulos", "S. Manandhar", "M. AL-Smadi", "M. Al-Ayyoub", "Y. Zhao", "B. Qin", "O. De Clercq", "V. Hoste", "M. Apidianaki", "X. Tannier", "N. Loukachevitch", "E. Kotelnikov", "N. Bel", "S.M. Jim\u00e9nez-Zafra", "G. Eryi\u011fit"], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval2016), ACL", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "SemEval-2016 Task 6: Detecting Stance in Tweets", "author": ["S. Mohammad", "S. Kiritchenko", "P. Sobhani", "X. Zhu", "C. Cherry"], "venue": "Bethard et al. [8].", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Tweeting and being ironic in the debate about a political reform: the french annotated corpus twitter-mariagepourtous", "author": ["C. Bosco", "M. Lai", "V. Patti", "D. Virone"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), ELRA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Annotating Sentiment and Irony in the Online Italian Political Debate on #labuonascuola", "author": ["Marco Stranisci", "Cristina Bosco", "Delia Iraz\u00fa Hern\u00e1ndez Fa\u0155\u0131as", "Viviana Patti"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), ELRA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Twitter Predicting the 2012 US Presidential Election?: Lessons Learned from an Unconscious Value Co-Creation Platform", "author": ["M. Maldonado", "V. Sierra"], "venue": "Journal of Organizational and End User Computing (JOEUC) 28", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "A readability analysis of campaign speeches from the 2016 US presidential campaign", "author": ["E. Schumacher", "M. Eskenazi"], "venue": "CoRR abs/1603.05739", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)", "author": ["Steven Bethard", "Marine Carpuat", "Daniel Cer", "David Jurgens", "Preslav Nakov", "Torsten Zesch", "eds."], "venue": "Association for Computational Linguistics, San Diego, California", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Crowdsourcing a word\u2013emotion association lexicon", "author": ["S.M. Mohammad", "P.D. Turney"], "venue": "Computational Intelligence 29", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognizing Contextual Polarity in Phraselevel Sentiment Analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of the Conference on HLT and Empirical Methods in Natural Language Processing, ACL", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining and Summarizing Customer Reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD \u201904, Seattle, WA, USA, ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets", "author": ["S. Mohammad", "S. Kiritchenko", "X. Zhu"], "venue": "Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection", "author": ["G. Zarrella", "A. Marsh"], "venue": "Bethard et al. [8].", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "pkudblab at SemEval-2016 Task 6 : A Specific Convolutional Neural Network System for Effective Stance detection", "author": ["W. Wei", "X. Zhang", "X. Liu", "W. Chen", "T. Wang"], "venue": "Bethard et al. [8].", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "TakeLab at SemEval-2016 Task 6: Stance Classification in Tweets Using a Genetic Algorithm Based Ensemble", "author": ["M. Tutek", "I. Sekulic", "P. Gombar", "I. Paljak", "F. Culinovic", "F. Boltuzic", "M. Karan", "D. Alagi\u0107", "J. \u0160najder"], "venue": "Bethard et al. [8].", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "ECNU at SemEval 2016 Task 6: Relevant or Not? Supportive or Not? A Two-step Learning System for Automatic Detecting Stance in Tweets", "author": ["Z. Zhang", "M. Lan"], "venue": "Bethard et al. [8].", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "CU-GWU Perspective at SemEval-2016 Task 6: Ideological Stance Detection in Informal Text", "author": ["H. Elfardy", "M. Diab"], "venue": "Bethard et al. [8].", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "IUCL at SemEval2016 task 6: An Ensemble Model for Stance Detection in Twitter", "author": ["C. Liu", "W. Li", "B. Demarest", "Y. Chen", "S. Couture", "D. Dakota", "N. Haduong", "N. Kaufman", "A. Lamont", "M. Pancholi", "K. Steimel", "S. K\u00fcbler"], "venue": "Bethard et al. [8].", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepstance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs", "author": ["P. Vijayaraghavan", "I. Sysoev", "S. Vosoughi", "D. Roy"], "venue": "Bethard et al. [8].", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "UWB at SemEval-2016 Task 6: Stance Detection", "author": ["P. Krejzl", "J. Steinberger"], "venue": "Bethard et al. [8].", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "IDI@NTNU at SemEval-2016 Task 6: Detecting Stance in Tweets Using Shallow Features and GloVe Vectors for Word Representation", "author": ["H. B\u00f8hler", "P. Asla", "E. Marsi", "R. S\u00e6tre"], "venue": "Bethard et al. [8].", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Tohoku at SemEval-2016 Task 6: Feature-based Model versus Convolutional Neural Network for Stance Detection", "author": ["Y. Igarashi", "H. Komatsu", "S. Kobayashi", "N. Okazaki", "K. Inui"], "venue": "Bethard et al. [8].", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "ltl.uni-due at SemEval-2016 Task 6: Stance Detection in Social Media Using Stacked Classifiers", "author": ["M. Wojatzki", "T. Zesch"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "JU NLP at SemEval-2016 Task 6: Detecting Stance in Tweets using Support Vector Machines", "author": ["B.G. Patra", "D. Das", "S. Bandyopadhyay"], "venue": "Bethard et al. [8].", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "NLDS-UCSC at SemEval-2016 Task 6: A Semi-Supervised Approach to Detecting Stance in Tweets", "author": ["A. Misra", "B. Ecker", "T. Handleman", "N. Hahn", "M. Walker"], "venue": "Bethard et al. [8].", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "INF-UFRGS-OPINION-MINING at SemEval-2016 Task 6: Automatic Generation of a Training Corpus for Unsupervised Identification of Stance in Tweets", "author": ["M. Dias", "K. Becker"], "venue": "Bethard et al. [8].", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "USFD at SemEval-2016 Task 6: AnyTarget Stance Detection on Twitter with Autoencoders", "author": ["I. Augenstein", "A. Vlachos", "K. Bontcheva"], "venue": "Bethard et al. [8].", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "A new ANEW: Evaluation of a word list for sentiment analysis in microblogs", "author": ["Finn \u00c5rup Nielsen"], "venue": "CoRR abs/1103.2903", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Linguistic inquiry and word count: LIWC 2001", "author": ["J.W. Pennebaker", "M.E. Francis", "R.J. Booth"], "venue": "Lawrence Erlbaum Associates, Mahwah,NJ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Using the Revised Dictionary of Affect in Language to Quantify the Emotional Undertones of Samples of Natural Language", "author": ["C. Whissell"], "venue": "Psychological Reports 105", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A dataset for detecting stance in tweets", "author": ["S. Mohammad", "S. Kiritchenko", "P. Sobhani", "X. Zhu", "C. Cherry"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), ELRA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Updating Formulae and a Pairwise Algorithm for Computing Sample Variances", "author": ["T.F. Chan", "G.H. Golub", "R.J. LeVeque"], "venue": "Technical report, Stanford University, Stanford, CA, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1979}, {"title": "Irony detection in twitter: The role of affective content", "author": ["D.I. Hern\u00e1ndez Fa\u0155\u0131as", "V. Patti", "P. Rosso"], "venue": "ACM Trans. Internet Technol. 16", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Figurative messages and affect in twitter: Differences between #irony, #sarcasm and #not", "author": ["E. Sulis", "D.I. Hern\u00e1ndez Fa\u0155\u0131as", "P. Rosso", "V. Patti", "G. Ruffo"], "venue": "Knowledge-Based Systems 108", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "These kinds of aspects are the subject of interest of Sentiment Analysis (SA) [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Recently, the interest on studying finer-grained and different facets of sentiment in texts has derived in areas such as Aspect based sentiment analysis [2] and Stance Detection (SD) [3], which is the focus of our work.", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "Recently, the interest on studying finer-grained and different facets of sentiment in texts has derived in areas such as Aspect based sentiment analysis [2] and Stance Detection (SD) [3], which is the focus of our work.", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": ", political reforms [4,5]) or a polarizing person (e.", "startOffset": 20, "endOffset": 25}, {"referenceID": 4, "context": ", political reforms [4,5]) or a polarizing person (e.", "startOffset": 20, "endOffset": 25}, {"referenceID": 5, "context": "Several efforts have been made in order to investigate different aspects related to social media and politics [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "This year, for the first time a shared task on stance detection in tweets was organized [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Both targets have been the focus of different research, for instance in [7] the authors studied their speeches during the 2016 political campaign.", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "al in [3] describe the task as: Given a tweet text and a target entity (person, organization, movement, policy, etc.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "More details about the Stance Dataset can be found in [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "Further information about the systems in the task can be found in [8].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Besides, some SA features from well-known lexical resources, such as EmoLex [9], MPQA [10], Hu and Liu [11] and NRC Hashtag [12], were used to detect stance in tweets.", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "Besides, some SA features from well-known lexical resources, such as EmoLex [9], MPQA [10], Hu and Liu [11] and NRC Hashtag [12], were used to detect stance in tweets.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "Besides, some SA features from well-known lexical resources, such as EmoLex [9], MPQA [10], Hu and Liu [11] and NRC Hashtag [12], were used to detect stance in tweets.", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "Besides, some SA features from well-known lexical resources, such as EmoLex [9], MPQA [10], Hu and Liu [11] and NRC Hashtag [12], were used to detect stance in tweets.", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "6 Notice that not all the reports describing systems and approaches of teams participating at SemEval-2016 Task 6 are available in [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 12, "context": "MITRE [13] Overall approach: Recurrent neural networks.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "pkudblab [14] Overall approach: Convolutional neural network.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "TakeLab [15] Overall approach: An ensamble of learning algorithms (such as SVM, random forest) fine-tuned using a genetic algorithm.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "ECNU [16] Overall approach: A pipeline-based procedure involving relevance and orientation detection.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "CU-GWU [17] Overall approach: Classification using SVM Task A External resources: N-grams, Stanford\u2019s SA system and LIWC.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "IUCL-RF [18] Overall approach: Classification algorithms (SVM, random forest, gradient boosting decision trees) and an ensamble classifier (TiMBL).", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "DeepStance [19] Overall approach: A set of naive bayes classifiers using deep learning.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "UWB [20] Overall approach: Maximum entropy classifier.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "IDI@NTNU [21] Overall approach: A soft voting classifier approach (naive bayes and logistic regression).", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "Tohoku [22] Overall approach: Two methods: a feature based approach and a neural network based approach.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "uni-due [23] Overall approach: Multidimensional classification problem Tasks A and B External resources: N-grams, punctuation marks, negation, nouns.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "JU NLP [24] Overall approach: Classification using SVM Task A External resources: N-Gram and sentiment analysis resources such as: SentiWordNet, EmoLex and NRC Hashtag Emotion Lexicon.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "nldsucsc [25] Overall approach: Classification using SVM, J48 and naive bayes.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "INF UFRGS [26] Overall approach: Set of rules together with SVM.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "USFD [27] Overall approach: Classification using logistic regression.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "We shared the idea that stance detection is strongly related to sentiment analysis [3,16].", "startOffset": 83, "endOffset": 89}, {"referenceID": 15, "context": "We shared the idea that stance detection is strongly related to sentiment analysis [3,16].", "startOffset": 83, "endOffset": 89}, {"referenceID": 27, "context": "AFINN was collected by Finn \u00c5rup Nielsen [28].", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "The Linguistic Inquiry and Word Counts (LIWC) [29] is a dictionary that contains about 4,500 entries distributed in 64 categories that can be further used to analyse psycholinguistic features in texts.", "startOffset": 46, "endOffset": 50}, {"referenceID": 29, "context": "The Dictionary of Affect in Language (DAL) contains 8,742 English words; it was developed by Whissell [30].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "After the evaluation of participating systems, the organizers of Semeval-2016 Task 6 annotated the Stance Datataset for sentiment and target in order to explore the relationship between sentiment and stance [31,3].", "startOffset": 207, "endOffset": 213}, {"referenceID": 2, "context": "After the evaluation of participating systems, the organizers of Semeval-2016 Task 6 annotated the Stance Datataset for sentiment and target in order to explore the relationship between sentiment and stance [31,3].", "startOffset": 207, "endOffset": 213}, {"referenceID": 2, "context": "We evaluated our approach by using the same measure defined in [3] in order to compare our results with those participating in the task.", "startOffset": 63, "endOffset": 66}, {"referenceID": 31, "context": "We trained a Gaussian Naive Bayes classifier [32] implemented in Scikit-learn Python library to built a model for identifying stance in tweets.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "The obtained results outperform the baselines proposed in [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 32, "context": "Finally, we think that it could be also interesting to investigate how to fruitfully combine information about stance and information about the presence of figurative devices in tweets, such as irony and sarcasm [33,34], since the use of such devices is very frequent in political debates also in social media and detecting irony and sarcasm have been considered as one of the biggest challenges for sentiment analysis.", "startOffset": 212, "endOffset": 219}, {"referenceID": 33, "context": "Finally, we think that it could be also interesting to investigate how to fruitfully combine information about stance and information about the presence of figurative devices in tweets, such as irony and sarcasm [33,34], since the use of such devices is very frequent in political debates also in social media and detecting irony and sarcasm have been considered as one of the biggest challenges for sentiment analysis.", "startOffset": 212, "endOffset": 219}], "year": 2017, "abstractText": "Stance detection, the task of identifying the speaker\u2019s opinion towards a particular target, has attracted the attention of researchers. This paper describes a novel approach for detecting stance in Twitter. We define a set of features in order to consider the context surrounding a target of interest with the final aim of training a model for predicting the stance towards the mentioned targets. In particular, we are interested in investigating political debates in social media. For this reason we evaluated our approach focusing on two targets of the SemEval-2016 Task 6 on Detecting stance in tweets, which are related to the political campaign for the 2016 U.S. presidential elections: Hillary Clinton vs. Donald Trump. For the sake of comparison with the state of the art, we evaluated our model against the dataset released in the SemEval-2016 Task 6 shared task competition. Our results outperform the best ones obtained by participating teams, and show that information about enemies and friends of politicians help in detecting stance towards them.", "creator": "LaTeX with hyperref package"}}}