{"id": "1702.08303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "abstract": "multi - task learning ( ng mtl ) in between deep neural networks solved for describing nlp has recently received upon increasing geographic interest specifically due to some compelling empirical benefits, including its attractive potential to actually efficiently regularize underlying models and conversely to reduce the need for labeled data. while it has brought significant improvements in all a worldwide number varieties of nlp tasks, mainly mixed results have been reported, and little is known here about the potential conditions known under which mtl communication leads to gains directly in nlp. this paper sheds light focus on emphasizing the already specific task relations that consolidation can now lead collectively to gains done from arbitrary mtl models interpreted over single - digit task setups.", "histories": [["v1", "Mon, 27 Feb 2017 14:37:21 GMT  (99kb,D)", "http://arxiv.org/abs/1702.08303v1", "Accepted for publication at EACL 2017"]], "COMMENTS": "Accepted for publication at EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["joachim bingel", "anders s{\\o}gaard"], "accepted": false, "id": "1702.08303"}, "pdf": {"name": "1702.08303.pdf", "metadata": {"source": "CRF", "title": "Identifying beneficial task relations for multi-task learning in deep neural networks", "authors": ["Joachim Bingel", "Anders S\u00f8gaard"], "emails": ["bingel@di.ku.dk", "soegaard@di.ku.dk"], "sections": [{"heading": "1 Introduction", "text": "Multi-task learning is receiving increasing interest in both academia and industry, with the potential to reduce the need for labeled data, and to enable the induction of more robust models. The main driver has been empirical results pushing state of the art in various tasks, but preliminary theoretical findings guarantee that multi-task learning works under various conditions. Some approaches to multi-task learning are, for example, known to work when the tasks share optimal hypothesis classes (Baxter, 2000) or are drawn from related sample generating distributions (BenDavid and Borberly, 2003).\nIn NLP, multi-task learning typically involves very heterogeneous tasks. However, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; S\u00f8gaard and Goldberg, 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), and theoretical guarantees no longer apply. The question what task relations guarantee gains or make gains likely in NLP remains open.\n\u2217Both authors contributed to the paper in equal parts.\nContributions This paper presents a systematic study of when and why MTL works in the context of sequence labeling with deep recurrent neural networks. We follow previous work (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017) in studying the set-up where hyperparameters from the single task architectures are reused in the multi-task setup (no additional tuning), which makes predicting gains feasible. Running MTL experiments on 90 task configurations and comparing their performance to single-task setups, we identify data characteristics and patterns in single-task learning that predict task synergies in deep neural networks. Both the LSTM code used for our singletask and multi-task models, as well as the script we used for the analysis of these, are available at github.com/jbingel/eacl2017_mtl."}, {"heading": "2 Related work", "text": "In the context of structured prediction in NLP, there has been very little work on the conditions under which MTL works. Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data. Mart\u0131\u0301nez Alonso and Plank (2017) present a study on different task combinations with dedicated main and auxiliary tasks. Their findings suggest, among others, that success depends on how uniformly the auxiliary task labels are distributed.\nMou et al. (2016) investigate multi-task learning and its relation to transfer learning, and under which conditions these work between a set of sentence classification tasks. Their main finding with respect to multi-task learning is that success\nar X\niv :1\n70 2.\n08 30\n3v 1\n[ cs\n.C L\n] 2\n7 Fe\nb 20\n17\ndepends largely on \u201chow similar in semantics the source and target datasets are\u201d, and that it generally bears close resemblance to transfer learning in the effect it has on model performance."}, {"heading": "3 Multi-task Learning", "text": "While there are many approaches to multi-task learning, hard parameter sharing in deep neural networks (Caruana, 1993) has become extremely popular in recent years. Its greatest advantages over other methods include (i) that it is known to be an efficient regularizer, theoretically (Baxter, 2000), as well as in practice (S\u00f8gaard and Goldberg, 2016); and (ii) that it is easy to implement.\nThe basic idea in hard parameter sharing in deep neural networks is that the different tasks share some of the hidden layers, such that these learn a joint representation for multiple tasks. Another conceptualization is to think of this as regularizing our target model by doing model interpolation with auxiliary models in a dynamic fashion.\nMulti-task linear models have typically been presented as matrix regularizers. The parameters of each task-specific model makes up a row in a matrix, and multi-task learning is enforced by defining a joint regularization term over this matrix. One such approach would be to define the joint loss as the sum of losses and the sum of the singular values of the matrix. The most common approach is to regularize learning by the sum of the distances of the task-specific models to the model mean. This is called mean-constrained learning. Hard parameter sharing can be seen as a very crude form of mean-constrained learning, in which parts of all models (typically the hidden layers) are enforced to be identical to the mean.\nSince we are only forcing parts of the models to be identical, each task-specific model is still left with wiggle room to model heterogeneous tasks, but the expressivity is very limited, as evidenced by the inability of such networks to fit random noise (S\u00f8gaard and Goldberg, 2016)."}, {"heading": "3.1 Models", "text": "Recent work on multi-task learning of NLP models has focused on sequence labeling with recurrent neural networks (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as\nwell (Luong et al., 2016). Our multi-task learning architecture is similar to the former, with a bi-directional LSTM as a single hidden layer of 100 dimensions that is shared across all tasks. The inputs to this hidden layer are 100-dimensional word vectors that are initialized with pretrained GloVe embeddings, but updated during training. The embedding parameters are also shared. The model then generates predictions from the biLSTM through task-specific dense projections. Our model is symmetric in the sense that it does not distinguish between main and auxiliary tasks.\nIn our MTL setup, a training step consists of uniformly drawing a training task, then sampling a random batch of 32 examples from the task\u2019s training data. Every training step thus works on exactly one task, and optimizes the task-specific projection and the shared parameters using Adadelta. As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017)."}, {"heading": "3.2 Tasks", "text": "In our experiments below, we consider the following ten NLP tasks, with one dataset for each task. Characteristics of the datasets that we use are summarized in Table 1.\n1. CCG Tagging (CCG) is a sequence tagging problem that assigns a logical type to every token. We use the standard splits for CCG super-tagging from the CCGBank (Hockenmaier and Steedman, 2007).\n2. Chunking (CHU) identifies continuous spans of tokens that form syntactic units such as noun phrases or verb phrases. We use the standard splits for syntactic chunking from the English Penn Treebank (Marcus et al., 1993).\n3. Sentence Compression (COM) We use the publicly available subset of the Google Compression dataset (Filippova and Altun, 2013), which has token-level annotations of word deletions.\n4. Semantic frames (FNT) We use FrameNet 1.5 for jointly predicting target words that trigger frames, and deciding on the correct frame in context.\n5. POS tagging (POS) We use a dataset of tweets annotated for Universal part-of-speech tags (Petrov et al., 2011).\n6. Hyperlink Prediction (HYP) We use the hypertext corpus from Spitkovsky et al. (2010) and predict what sequences of words have been bracketed with hyperlinks.\n7. Keyphrase Detection (KEY) This task amounts to detecting keyphrases in scientific publications. We use the SemEval 2017 Task 10 dataset.\n8. MWE Detection (MWE) We use the Streusle corpus (Schneider and Smith, 2015) to learn to identify multi-word expressions (on my own, cope with).\n9. Super-sense tagging (SEM) We use the standard splits for the Semcor dataset, predicting coarse-grained semantic types of nouns and verbs (super-senses).\n10. Super-sense Tagging (STR) As for the MWE task, we use the Streusle corpus, jointly predicting brackets and coarse-grained semantic types of the multi-word expressions."}, {"heading": "4 Experiments", "text": "We train single-task bi-LSTMs for each of the ten tasks, as well as one multi-task model for each of\nthe pairs between the tasks, yielding 90 directed pairs of the form \u3008Tmain, {Tmain, Taux}\u3009. The single-task models are trained for 25,000 batches, while multi-task models are trained for 50,000 batches to account for the uniform drawing of the two tasks at every iteration in the multi-task setup. The relative gains and losses from MTL over the single-task models (see Table 1) are presented in Figure 1, showing improvements in 40 out of 90 cases. We see that chunking and high-level semantic tagging generally contribute most to other tasks, while hyperlinks do not significantly improve any other task. On the receiving end, we see that multiword and hyperlink detection seem to profit most from several auxiliary tasks. Symbiotic relationships are formed, e.g., by POS and CCG-tagging, or MWE and compression.\nWe now investigate whether we can predict gains from MTL given features of the tasks and single-task learning characteristics. We will use\nthe induced meta-learning for analyzing what such characteristics are predictive of gains.\nSpecifically, for each task considered, we extract a number of dataset-inherent features (see Table 2) as well as features that we derive from the learning curve of the respective single-task model. For the curve gradients, we compute the gradients of the loss curve at 10, 20, 30, 50 and 70 percent of the 25,000 batches. For the fitted log-curve parameters, we fit a logarithmic function to the loss curve values, where the function is of the form: L(i) = a \u00b7 ln(c \u00b7i+d)+b. We include the fitted parameters a and c as features that describe the steepness of the learning curve. In total, both the main and the auxiliary task are described by 14 features. Since we also compute the main/auxiliary ratios of these values, each of our 90 data points is described by 42 features that we normalize to the [0, 1] interval. We binarize the results presented in Figure 1 and use logistic regression to predict benefits or detriments of MTL setups based on the features computed above.1"}, {"heading": "4.1 Results", "text": "The mean performance of 100 runs of randomized five-fold cross-validation of our logistic regression\n1An experiment in which we tried to predict the magnitude of the losses and gains with linear regression yielded inconclusive results.\nmodel for different feature combinations is listed in Table 3. The first observation is that there is a strong signal in our meta-learning features. In almost four in five cases, we can predict the outcome of the MTL experiment from the data and the single task experiments, which gives validity to our feature analysis. We also see that the features derived from the single task inductions are the most important. In fact, using only data-inherent features, the F1 score of the positive class is worse than the majority baseline."}, {"heading": "4.2 Analysis", "text": "Table 4 lists the coefficients for all 42 features. We find that features describing the learning curves for the main and auxiliary tasks are the best predictors of MTL gains. The ratios of the learning curve features seem less predictive, and the gradients around 20-30% seem most important, after the area where the curve typically flattens a bit (around 10%). Interestingly, however, these gradients correlate in opposite ways for the main and auxiliary tasks. The pattern is that if the main tasks have flattening learning curves (small negative gradients) in the 20-30% percentile, but the auxiliary task curves are still relatively steep, MTL is more likely to work. In other words, multi-task gains are more likely for target tasks that quickly plateau with non-plateauing auxiliary tasks. We speculate the reason for this is that multi-task learning can help target tasks that get stuck early in local minima, especially if the auxiliary task does not always get stuck fast.\nOther features that are predictive include the number of labels in the main task, as well as the label entropy of the auxiliary task. The latter supports the hypothesis put forward by Mart\u0131\u0301nez Alonso and Plank (2017) (see Related work). Note, however, that this may be a side effect of tasks with more uniform label distributions being easier to learn. The out-of-vocabulary rate for the target task also was predictive, which\nmakes sense as the embedding parameters are also updated when learning from the auxiliary data.\nLess predictive features include JensenShannon divergences, which is surprising, since multi-task learning is often treated as a transfer learning algorithm (S\u00f8gaard and Goldberg, 2016). It is also surprising to see that size differences between the datasets are not very predictive."}, {"heading": "5 Conclusion and Future Work", "text": "We present the first systematic study of when MTL works in the context of common NLP tasks, when single task parameter settings are also applied for multi-task learning. Key findings include that MTL gains are predictable from dataset characteristics and features extracted from the single-task inductions. We also show that the most predictive features relate to the single-task learning curves, suggesting that MTL, when successful, often helps target tasks out of local minima. We also observed that label entropy in the auxiliary task was also a good predictor, lending some support to the hypothesis in Mart\u0131\u0301nez Alonso and Plank (2017); but there was little evidence that dataset balance is a reliable predictor, unlike what previous work has suggested.\nIn future work, we aim to extend our experiments to a setting where we optimize hyperparameters for the single- and multi-task models individually, which will give us a more reliable picture of the effect to be expected from multi-task learning in the wild. Generally, further conclusions could be drawn from settings where the joint models do not treat the two tasks as equals, but instead give more importance to the main task, for instance through a non-uniform drawing of the task considered at each training iteration, or through an adaptation of the learning rates. We are also interested in extending this work to additional NLP tasks, including tasks that go beyond sequence labeling such as language modeling or sequence-tosequence problems."}, {"heading": "Acknowledgments", "text": "For valuable comments, we would like to thank Dirk Hovy, Yoav Goldberg, the attendants at the second author\u2019s invited talk at the Danish Society for Statistics, as well as the anonymous reviewers. This research was partially funded by the ERC Starting Grant LOWLANDS No. 313695, as well as by Trygfonden."}], "references": [{"title": "A model of inductive bias learning", "author": ["Jonathan Baxter"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter.,? \\Q2000\\E", "shortCiteRegEx": "Baxter.", "year": 2000}, {"title": "Multitask learning for mental health conditions with limited social media data", "author": ["Benton et al.2017] Adrian Benton", "Margaret Mitchell", "Dirk Hovy"], "venue": null, "citeRegEx": "Benton et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Benton et al\\.", "year": 2017}, {"title": "Improving historical spelling normalization with bi-directional lstms and multitask learning", "author": ["Bollman", "S\u00f8gaard2016] Marcel Bollman", "Anders S\u00f8gaard"], "venue": "In COLING", "citeRegEx": "Bollman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bollman et al\\.", "year": 2016}, {"title": "Multi-view and multi-task training of rst discourse parser", "author": ["Braud et al.2016] Chloe Braud", "Barbara Plank", "Anders S\u00f8gaard"], "venue": "In COLING", "citeRegEx": "Braud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Braud et al\\.", "year": 2016}, {"title": "Multitask learning: a knowledge-based source of inductive bias", "author": ["Rich Caruana"], "venue": "In ICML", "citeRegEx": "Caruana.,? \\Q1993\\E", "shortCiteRegEx": "Caruana.", "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Filippova", "Altun2013] Katja Filippova", "Yasemin Altun"], "venue": "In EMNLP,", "citeRegEx": "Filippova et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2013}, {"title": "Ccgbank: A corpus of ccg derivations and dependency structures extracted from the penn treebank", "author": ["Hockenmaier", "Steedman2007] Julia Hockenmaier", "Mark Steedman"], "venue": "Comput. Linguist.,", "citeRegEx": "Hockenmaier et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hockenmaier et al\\.", "year": 2007}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Klerke et al.2016] Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Klerke et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In ICLR", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["Mary Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Multitask learning for semantic sequence prediction under varying data conditions", "author": ["Mart\u0131\u0301nez Alonso", "Barbara Plank"], "venue": null, "citeRegEx": "Alonso et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2017}, {"title": "How transferable are neural networks in nlp applications? arXiv preprint arXiv:1603.06111", "author": ["Mou et al.2016] Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": null, "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2011] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "CoRR abs/1104.2086", "citeRegEx": "Petrov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Keystroke dynamics as signal for shallow syntactic parsing", "author": ["Barbara Plank"], "venue": "In COLING", "citeRegEx": "Plank.,? \\Q2016\\E", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "A corpus and model integrating multiword expressions and supersenses", "author": ["Schneider", "Smith2015] Nathan Schneider", "Noah A Smith"], "venue": "Proc. of NAACL-HLT", "citeRegEx": "Schneider et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 2015}, {"title": "Deep multitask learning with low level tasks supervised at lower layers", "author": ["S\u00f8gaard", "Goldberg2016] Anders S\u00f8gaard", "Yoav Goldberg"], "venue": null, "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2016}, {"title": "Profiting from mark-up: Hyper-text annotations for guided parsing", "author": ["Daniel Jurafsky", "Hiyan Alshawi"], "venue": null, "citeRegEx": "Spitkovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Some approaches to multi-task learning are, for example, known to work when the tasks share optimal hypothesis classes (Baxter, 2000) or are drawn from related sample generating distributions (BenDavid and Borberly, 2003).", "startOffset": 119, "endOffset": 133}, {"referenceID": 9, "context": "However, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; S\u00f8gaard and Goldberg, 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), and theoretical guarantees no longer apply.", "startOffset": 53, "endOffset": 94}, {"referenceID": 8, "context": "However, while great improvements have been reported (Luong et al., 2016; Klerke et al., 2016), results are also often mixed (Collobert and Weston, 2008; S\u00f8gaard and Goldberg, 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), and theoretical guarantees no longer apply.", "startOffset": 53, "endOffset": 94}, {"referenceID": 8, "context": "We follow previous work (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017) in studying the set-up where hyperparameters from the single task architectures are reused in the multi-task setup (no additional tuning), which makes predicting gains feasible.", "startOffset": 24, "endOffset": 167}, {"referenceID": 14, "context": "We follow previous work (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017) in studying the set-up where hyperparameters from the single task architectures are reused in the multi-task setup (no additional tuning), which makes predicting gains feasible.", "startOffset": 24, "endOffset": 167}, {"referenceID": 3, "context": "We follow previous work (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017) in studying the set-up where hyperparameters from the single task architectures are reused in the multi-task setup (no additional tuning), which makes predicting gains feasible.", "startOffset": 24, "endOffset": 167}, {"referenceID": 8, "context": "Luong et al. (2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "(2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data.", "startOffset": 100, "endOffset": 121}, {"referenceID": 1, "context": "(2016) suggest that it is important that the auxiliary data does not outsize the target data, while Benton et al. (2017) suggest that multi-task learning is particularly effective when we only have access to small amounts of target data. Mart\u0131\u0301nez Alonso and Plank (2017) present a study on different task combinations with dedicated main and auxiliary tasks.", "startOffset": 100, "endOffset": 272}, {"referenceID": 4, "context": "While there are many approaches to multi-task learning, hard parameter sharing in deep neural networks (Caruana, 1993) has become extremely popular in recent years.", "startOffset": 103, "endOffset": 118}, {"referenceID": 0, "context": "Its greatest advantages over other methods include (i) that it is known to be an efficient regularizer, theoretically (Baxter, 2000), as well as in practice (S\u00f8gaard and Goldberg, 2016); and (ii) that it is easy to implement.", "startOffset": 118, "endOffset": 132}, {"referenceID": 8, "context": "Recent work on multi-task learning of NLP models has focused on sequence labeling with recurrent neural networks (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as well (Luong et al.", "startOffset": 113, "endOffset": 256}, {"referenceID": 14, "context": "Recent work on multi-task learning of NLP models has focused on sequence labeling with recurrent neural networks (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as well (Luong et al.", "startOffset": 113, "endOffset": 256}, {"referenceID": 3, "context": "Recent work on multi-task learning of NLP models has focused on sequence labeling with recurrent neural networks (Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as well (Luong et al.", "startOffset": 113, "endOffset": 256}, {"referenceID": 9, "context": ", 2016; Mart\u0131\u0301nez Alonso and Plank, 2017), although sequence-to-sequence models have been shown to profit from MTL as well (Luong et al., 2016).", "startOffset": 123, "endOffset": 143}, {"referenceID": 8, "context": "As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 223, "endOffset": 394}, {"referenceID": 14, "context": "As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 223, "endOffset": 394}, {"referenceID": 3, "context": "As already mentioned, we keep hyper-parameters fixed across single-task and multi-task settings, making our results only applicable to the scenario where one wants to know whether MTL works in the current parameter setting (Collobert and Weston, 2008; Klerke et al., 2016; S\u00f8gaard and Goldberg, 2016; Bollman and S\u00f8gaard, 2016; Plank, 2016; Braud et al., 2016; Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 223, "endOffset": 394}, {"referenceID": 10, "context": "We use the standard splits for syntactic chunking from the English Penn Treebank (Marcus et al., 1993).", "startOffset": 81, "endOffset": 102}, {"referenceID": 13, "context": "POS tagging (POS) We use a dataset of tweets annotated for Universal part-of-speech tags (Petrov et al., 2011).", "startOffset": 89, "endOffset": 110}, {"referenceID": 17, "context": "Hyperlink Prediction (HYP) We use the hypertext corpus from Spitkovsky et al. (2010) and predict what sequences of words have been bracketed with hyperlinks.", "startOffset": 60, "endOffset": 85}, {"referenceID": 14, "context": "The latter supports the hypothesis put forward by Mart\u0131\u0301nez Alonso and Plank (2017) (see Related work).", "startOffset": 71, "endOffset": 84}, {"referenceID": 14, "context": "We also observed that label entropy in the auxiliary task was also a good predictor, lending some support to the hypothesis in Mart\u0131\u0301nez Alonso and Plank (2017); but there was little evidence that dataset balance is a reliable predictor, unlike what previous work has suggested.", "startOffset": 148, "endOffset": 161}], "year": 2017, "abstractText": "Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.", "creator": "LaTeX with hyperref package"}}}