{"id": "1702.08648", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Auto-clustering Output Layer: Automatic Learning of Latent Annotations in Neural Networks", "abstract": "in presented this final paper, we will propose a novel method to enrich specifically the representation provided to the output layer of fully feedforward neural networks in adopt the form of an auto - clustering coupled output transformation layer ( mv acol ) which enables the network to naturally structurally create sub - structured clusters under the externally provided simultaneous main class la - bels. ; in addition, a dynamic novel regularization term is introduced which allows extended acol communication to encourage the associated neural network driver to uniquely reveal its own purely explicit clustering processing objective. simultaneously while defining the widespread underlying process difficulty of theoretically finding the subclasses combined is initially completely unsupervised, some semi - supervised learning converge is also possible based on the clearly provided domain classification objective. the results show findings that acol convergence can eventually achieve yielding a 99. 2 % clustering accuracy, for the semi - concurrent supervised case when partial class labels as are presented and induce a near 96 % information accuracy objective for the unsupervised clustering case. these convergence findings should represent towards a significant paradigm new shift now especially appropriate when it comes second to significantly harnessing the tremendous power of deep cognitive networks inherently for predicting primary and secondary clustering applications in achieving large datasets.", "histories": [["v1", "Tue, 28 Feb 2017 05:21:31 GMT  (11293kb,D)", "http://arxiv.org/abs/1702.08648v1", "Submitted to ICML 2017"], ["v2", "Wed, 9 Aug 2017 00:02:45 GMT  (1920kb,D)", "http://arxiv.org/abs/1702.08648v2", "Submitted to IEEE Transactions on Neural Networks and Learning Systems, 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ozsel kilinc", "ismail uysal"], "accepted": false, "id": "1702.08648"}, "pdf": {"name": "1702.08648.pdf", "metadata": {"source": "META", "title": "Deep Clustering using Auto-Clustering Output Layer", "authors": ["Ozsel Kilinc", "Ismail Uysal"], "emails": ["<ozsel@mail.usf.edu>."], "sections": [{"heading": "1. Introduction", "text": "Artificial neural networks, when first introduced as universal estimators, created wide spread enthusiasm especially with innovative and bio-inspired learning methodologies such as error back-propagation (Rumelhart et al., 1985). However, their potential wasn\u2019t fully realized until greater availability of computational power and massive datasets to help train deep structures. Deep learning mimics the operational and organizational behavior of the human brain, which works through abstraction (Arel et al., 2010) and relies on higher-level representations of features embedded in the data.\nDeep learning has provided a powerful framework for supervised learning and has changed traditional machine learning methods transformatively over the last several\n1University of South Florida, Tampa, US. Correspondence to: Ozsel Kilinc <ozsel@mail.usf.edu>.\nSubmitted on February 24, 2017. Under review by the International Conference on Machine Learning (ICML) 2017.\nyears. For instance, the convolutional neural networks (ConvNet) has been applied to computer vision problems with remarkable success (Krizhevsky et al., 2012). On the other hand, for tasks involving sequential input such as speech and text, recurrent neural networks (RNN) are widely in use since they provide better prediction for the future elements thanks to the architecture implicitly keeping the information about past elements, such as long-shortterm-memory (LSTM) networks (LeCun et al., 2015), (Hochreiter & Schmidhuber, 1997). In recent years, researchers have shown that even basic modifications to these deep learning architectures may yield significant increases in classification performance. The dropout method which prevents overfitting is based on the idea of dropping out a neuron with all its connections to introduce noise which helps regularize the network (Srivastava et al., 2014).\nHowever, other data challenges, such as generating new samples, cannot be solved by supervised learning. One may also wish to use a large dataset of unlabeled samples. These kinds of tasks may require unsupervised or semisupervised learning. Algorithms designed for unsupervised problems haven\u2019t been as successful as those designed for supervised applications. Hence, today\u2019s deep learning research mainly focuses on approaches to solve these kinds of problems (Goodfellow et al., 2016).\nRepresentation learning (Bengio et al., 2013) provides a way to perform unsupervised and semi-supervised learning. One of the applications of representation learning is unsupervised feature learning where unsupervised training is used as a pre-training stage for initializing the hidden layer parameters before they are fine-tuned by some form of supervised training using backpropogation (La\u0308ngkvist et al., 2014). It is shown that unsupervised pre-training improves learning by preventing overfitting as it provides better regularization by guiding the learning towards basins of attraction of minima (Erhan et al., 2010). While unsupervised feature learning can be dated back to the invention of Principle Component Analysis (PCA) in 1901 (Pearson, 1901), ongoing studies mostly focus on architectures such as Deep Boltzmann Machines (DBM) (Salakhutdinov & Hinton, 2009; Hinton, 2010) Deep Belief Networks (DBN) (Hinton et al., 2006) and autoencoder variants (Coates & Ng, 2011; Coates et al., 2011). ar X iv :1\n70 2.\n08 64\n8v 1\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17\nClustering the data into groups of related examples is one general example of unsupervised learning applications. Most de facto clustering algorithms, such as k-means, hierarchical clustering, self-organizing maps (Kohonen, 1990), go back to the 1980s. These algorithms also try to find the best representation of the data. k-means, for example, provides one-hot coded sparse representations.\nIn this paper, we introduce a novel method to utilize the computational potential of deep neural networks to find the subclasses of provided class labels, which can be used for clustering purposes. The proposed method enables the use of deep learning models - originally proposed for supervised classification - for clustering problems, thus coining the phrase deep clustering."}, {"heading": "2. Auto-Clustering Output Layer", "text": "Auto-clustering output layer (ACOL) is a method that allows feedforward neural networks to perform a secondary task - unsupervised clustering - while the primary task - supervised classification - is being carried out. In other words, while networks are being trained to accomplish the classification objective defined through provided class labels, ACOL enables them to find subclasses within these classes. Since this objective does not involve any direct information about subclasses, subclass exploration is considered to be an unsupervised task. Hence, throughout this paper, they will be called as the clusters of provided class labels. On the other hand, the overall learning procedure can be considered either as semi-supervised or fully unsupervised depending on the defined classification objective as will be described in more detail in the subsequent sections.\nFor clarification, let us assume that we have a dataset of letters and digits where the overall task is complete categorization of each digit and letter. In order to simplify this task, an expert divides this dataset into two classes and labels each sample as either a letter or a digit. Then, we introduce this dataset to an ACOL modified network as a two-class classification problem. While being trained to categorize each sample either as a letter or as a digit (provided classes) the network also learns to distinguish digits from each other, and letters as well (clusters). Since provided labels involve no explicit information about the difference between the samples of digit 0 and the samples of digit 1, their separation is performed as an unsupervised task. As we use expert supervision to help overall categorization (is it a letter or a digit?), the entire procedure can be considered semi-supervised. In another realization, instead of using the expert\u2019s help, if we simply double the dataset by applying a transformation to all samples, e.g. horizontal flipping, and label each sample either as original or as flipped, this new augmented dataset can be provided to the\nnetwork as a two-class classification problem. In this case, the entire procedure can be considered fully unsupervised learning as we do not use any relevant supervision for the overall categorization task other than pseudo-class labels (original vs. flipped).\nUnlike other existing neural network models used for clustering such as self-organizing maps (SOM) and adaptive resonance theory (ART) (Carpenter et al., 1991), ACOL does not apply competitive learning. Instead, it implements exactly the same error correction learning that is applied for classification problems, i.e. backpropagation with gradient descent. Besides, ACOL does not provide any explicit clustering objective. Rather, it allows the networks to reveal their own clustering objective while they are being trained for the provided classification objective. Hence, choosing this objective will be crucial in order to obtain accurate clustering, as discussed in the sections below.\nAs will be explained in following sections, ACOL involves an output layer modification that defines multiple softmax nodes for each class and a regularization method to motivate the network to make these duplicated softmax nodes specialized for each individual sample but equally-active for entire dataset."}, {"heading": "2.1. Output Layer Modification", "text": "Neural networks define a family of functions parameterized by weights and biases which define the relation between inputs and outputs. In multi-class categorization tasks, outputs correspond to class labels, hence in a typical output layer structure there exists an individual output node for each class. An activation function, such as softmax is then used to calculate normalized exponentials to convert the previous hidden layer\u2019s activations, i.e. scores, into probabilities.\nUnlike traditional output layer structure, ACOL defines more than one softmax node per class. Outputs of softmax nodes that belong to the same class are then combined in a sequent pooling layer for the final prediction. Training is performed in the configuration shown in Figure 1. This might look like a classifier with redundant softmax nodes. However, duplicated softmax nodes of each class are specialized during training phase such that each softmax node represent an individual cluster of a class.\nAfter training phase is completed, network is simply truncated by completely disconnecting the pooling layer as shown in Figure 2 and the rest of the network with trained weights performs the clustering task for the samples of each class.\nACOL does not change feedforward and backpropagation mechanisms of the network drastically. During feedforward operation of the network, pooling layer calculates\nfinal class predictions through cluster probabilities. For i = 1, ..., N where N is the number of classes, the probability for given sample belongs to ith class, s\u0303i becomes\nf(zi,1, zi,2, ..., zi,K) = s\u0303i =\nK\u2211 ezi,k\nN\u2211 K\u2211 ezj,k\n(1)\nin case of average pooling. And when max pooling is used, it becomes\nf(zi,1, zi,2, ..., zi,K) = s\u0303i = max k\u2208K\nezi,k\nN\u2211 K\u2211 ezj,k\n(2)\nwhere K is the number of clusters per class and zi,k corresponds to the activation of kth softmax of ith class.\nSummation and max operators do not affect backpropagation in terms of derivatives and ACOL behaves in a similar fashion to traditional output layer. However, labels are now implicitly applied to multiple softmax nodes each representing an individual cluster under the same class. In other words, even if the labels are provided as one-hot encoded vector at the output, due to the pooling layer, it turns into K-hot encoded vector at the augmented softmax layer. K\nsoftmax nodes simultaneously receive the error between the label and the prediction and then backpropagate it towards the previous hidden layers.\nThis structure carries the ability to create clusters as ACOL introducesN(K\u22121)NL\u22121 extra trainable weights between the previous hidden layer and itself, where NL\u22121 is the number of nodes at that hidden layer. Each softmax node is connected to the previous hidden layer through NL\u22121 nonshared weights. Due to random initialization, these weights may ultimately converge to different values at the end of training and duplicated softmax nodes may be specialized for only a subset of the samples of that class. However, this mechanism is completely uncontrollable. Furthermore, during the weight updates, if any one of the duplicated softmax nodes get activated to generate significantly lower error, through the pooling layer, this will also eliminate the backpropagated error to other K \u2212 1 softmax nodes of that class. Therefore, not only the one reducing the error, but all K duplicated softmax nodes stop backpropagating the error to previous layers. That is to say, without any additional mechanism, there is no actual competition between the duplicated softmax nodes of a class. Dominating one is prone to be more dominating and react to whole samples of that class.\nApplying dropout to the hidden layer nodes does not provide any improvement; however, dropping out the softmax nodes significantly reduces the problem of dominating clusters. It ensures that duplicated softmax nodes get almost equal overall activation for the entire dataset, i.e. at the end of training, clusters involve almost equal numbers of samples. However, entire mechanism still relies only on randomness. Since softmax nodes are randomly being dropped out now, clusters significantly vary even after a single weight update, particularly when dropping out ratio is high. Besides, this may cause more than one softmax nodes to get highly activated for the same samples of the class. More specifically, in the absence of the highest activated softmax node, another softmax node among the duplicated ones becomes the highest activated one and updates its weights to reduce the error. Eventually, at the end of the training these softmax nodes become equivalent, i.e. they are not specialized, hence the distribution of samples over these cluster will not be clear.\nBriefly, applying dropout to the augmented softmax layer regularizes overall activations obtained by softmax nodes and makes them equally-active for entire dataset; however, it also introduces an additional level of randomness and moreover, it does not provide any obvious specialization among duplicated softmax nodes. In order to fulfill our two requirements, i.e. specialized but equally-active softmax nodes, through a more deterministic way ending up with similar clustering schemes for every other trial, we define\nregularization terms for the activities coming to augmented softmax layer from the preceding hidden layer. These terms are added to cost function to regularize the weights between these two layers such that each NL\u22121 weights connecting a softmax node to the previous layer are tuned up in a way that balances both requirements."}, {"heading": "2.2. Activity Regularization", "text": "Consider a neural network with L hidden layers where l denotes the individual index for each hidden layer such that l \u2208 {1, ..., L}. Let Z(l) and Y(l) denote the input and output matrices of the nodes at layer l, respectively. Y(0) = X is the input and Y(L) = Y the output of the entire network. W(l) and b(l) are the weights and biases of layer l, respectively.\nThen, Z(L) corresponds to the m \u00d7 n input matrix of the augmented softmax layer such that\nZ(L) = Y(L\u22121)W(L) + b(L) (3)\nand Y = Y(L) = f(Z(L)) (4)\nwhere m is the number of samples, n is the number of all softmax nodes such that n = NK and f(.) is one of the activation functions defined in equations (1) and (2). For the sake of simplicity, hereafter we refer to Z(L) as Z.\nZ =  Z11 Z12 . . . Z1n Z21 Z22 . . . Z2n ... ... . . . ...\nZm1 Zm2 . . . Zmn\n (5)\nCell Zij of Z corresponds to the activation created at the jth softmax node by the ith sample. As our first requirement, we want to specialize each softmax node such that for each sample of the dataset, only one softmax gets positively activated and the rest becomes either zero or negative. That is to say, there need to be only one positive value on each row of Z. Provided classification objective already assures this condition for (N \u2212 1)K entries by converging them to negative values as the sample doesn\u2019t belong to theseN\u22121 classes. Hence, our concern becomes to eliminate multiple positive values in remaining K entries in each row of Z. As we are only interested in positively activated softmax nodes, we can work on the positive part of Z such that\nZ\u0304 := (max(0, Zij))d\u00d7n (6)\nNow, let us define U as\nU := Z\u0304T Z\u0304 (7)\nthen, U becomes a n\u00d7 n symmetric matrix such that\nU =  m\u2211 Z\u0304i1Z\u0304i1 m\u2211 Z\u0304i1Z\u0304i2 . . . m\u2211 Z\u0304i1Z\u0304in m\u2211 Z\u0304i2Z\u0304i1 m\u2211 Z\u0304i2Z\u0304i2 . . . m\u2211 Z\u0304i2Z\u0304in ... ... . . . ...\nm\u2211 Z\u0304inZ\u0304i1 m\u2211 Z\u0304inZ\u0304i2 . . . m\u2211 Z\u0304inZ\u0304in\n (8)\nOff-diagonal entries of U correspond to summation of multiplied activities of any two softmax nodes for each individual sample. In other words, they indicate how much these two nodes get activated together. And zero value means that they are perfectly specialized for different samples of the dataset as these nodes never get activated together. If we define coactivity, \u03b1c as the summation of all off-diagonal entires such that\nCoactivity = \u03b1c := \u2211 i 6=j Uij (9)\nthen we can add this term into the cost function to minimize it in order to obtain specialized softmax nodes. However, coactivity is also minimized when a single cluster collects all samples of a class. Since remaining K \u2212 1 duplicated softmax nodes of that class get no positive activation, all off-diagonal entries of U become zero. Hence, without any secondary term assuring all duplicated softmax nodes are equally activated for entire dataset, coactivity only supports the dominating cluster problem.\nLet v be a 1\u00d7 n vector such that v := [ m\u2211 Z\u0304i1 m\u2211 Z\u0304i2 . . . m\u2211 Z\u0304in ] (10)\nwhere each entry corresponds to overall positive activation of a softmax node for the entire dataset. Now, we want to define the second regularization term to equalize all entries of v.\nIf we define V as V := vT v (11)\nV becomes a n\u00d7 n symmetric matrix such that\nV =  m\u2211 Z\u0304i1 m\u2211 Z\u0304i1 m\u2211 Z\u0304i1 m\u2211 Z\u0304i2 . . . m\u2211 Z\u0304i1 m\u2211 Z\u0304in m\u2211 Z\u0304i2 m\u2211 Z\u0304i1 m\u2211 Z\u0304i2 m\u2211 Z\u0304i2 . . . m\u2211 Z\u0304i2 m\u2211\nZ\u0304in . . . . . . . . . . .\n. m\u2211 Z\u0304in m\u2211 Z\u0304i1 m\u2211 Z\u0304in m\u2211 Z\u0304i2 . . . m\u2211 Z\u0304in m\u2211 Z\u0304in  (12) Maximizing the sum of off-diagonal entries of V could satisfy this requirement, but it would be an unbounded problem. Besides, adjusting the coefficient of this term would be troublesome. Since weights are initialized with values very close to zero, they create very low activations at softmax nodes, which results in U V at the beginning of\ntraining. However, as activations start to increase, this ratio changes drastically during first few epochs. Hence, using such a term would require adaptive coefficient adjustment in order to maintain both requirements at the same time.\nInstead, we can define a normalized term Balance, \u03b2 such that\nBalance = \u03b2 :=\n\u2211 i 6=j Vij\n(n\u2212 1) \u2211 i=j Vij (13)\nwhich takes values between 0 and 1 and becomes 1 if and only if all entries of v are equal to each other. Now we can add (1 \u2212 \u03b2) to cost function to minimize it. When minimized, it assures that all duplicated softmax nodes are perfectly equal-activated for the entire dataset.\nUsing a normalized term, balance, together with an unnormalized term, coactivity, would still require rigorous coefficient adjustment at the beginning of training due to the drastic change of U that affects coactivity. Instead, we can normalize it in the same manner, and use this normalized term instead of coactivity such that\nAffinity = \u03b1 :=\n\u2211 i 6=j Uij\n(n\u2212 1) \u2211 i=j Uij (14)\nand the overall regularization term becomes\nregularization = c1\u03b1+ c2(1\u2212 \u03b2) (15)\nAffinity and balance work in harmony and provide effortless start-off without any fine adjustment of coefficients. And ultimately, they guide the learning towards basin of attraction where both of them are stabilized and wellsatisfied. At the subsequent epochs of training, as its denominator also increases, affinity reaches very small values, i.e. \u03b1 \u2248 0.01, and its effect vanishes before coactivity goes to zero. At this point, we can switch to minimize coactivity, which is indeed our direct metric to measure how well softmax node are specialized. Since we are now in safe region, thanks to affinity and balance, minimizing coactivity will no longer be a trouble. And through a feasible trade off with balance, we can obtain better specialized softmax nodes. The overall regularization term turns into\nregularization = c1\u03b1+ c2(1\u2212 \u03b2) + c3\u03b1c (16)\nwhere c3 > 0 if \u03b1 \u2248 0.01.\nAs one last improvement, L2 norm can be used to penalize the overall activity increase. Otherwise, optimization might prefer to increase all activations to minimize the regularization term, which particularly diminishes affinity. Besides, higher activations might result in loss of generalization.\nRecall that Frobenius norm for Z, ||Z||F is analogous to the L2 norm of a vector. Hence, the overall regularization term then becomes\nregularization = c1\u03b1+c2(1\u2212\u03b2)+c3\u03b1c+c4||Z||2F (17)\nThe effects of each of these terms will be explored in detail in the next section."}, {"heading": "3. Analysis and Results", "text": "In order to observe the effects of defined regularization terms and clustering performance of ACOL on a wellknown benchmark dataset, we have created clustering problems on MNIST (LeCun et al., 1998) for both semisupervised and fully unsupervised type of uses."}, {"heading": "3.1. MNIST - Semi-supervised - 2 Class", "text": "As an example to semi-supervised type of use, we have relabeled MNIST to create two class problem as follows:\nt\u2217i = { 0 if ti < 5 1 otherwise , \u2200ti \u2208 t\nwhere t is the original label vector for the entire dataset, ti is the original label for sample i such that 0 \u2264 ti \u2264 9 and t\u2217i is its new label. As the classification objective, now we provide whether a digit is greater than 5 or not. And unsupervised part of this problem then becomes \u201cclustering digits 0, 1, 2, 3, 4\u201d and \u201cclustering digits 5, 6, 7, 8, 9\u201d. Before clustering performance, we will first show the effects of three regularization terms that we have introduced."}, {"heading": "3.1.1. EFFECTS OF REGULARIZATION TERMS", "text": "As we have stated before, defined regularization terms do not specify a clustering objective. Instead, they reveal hidden clustering potential of the networks and regularize it in a way that we become able to observe clusters. Hence, we need to be sure that they work as we have proposed and satisfy our two requirements; specialized and equally active softmax nodes.\nIn order to observe how affinity and coactivity work, after training a network with clustering coefficient K = 5, i.e. the number of duplicated softmax nodes per class, we test it only by using the samples that are grouped into Cluster-1 of Class-1 and then observe the activations of all 5 duplicated softmax nodes of Class-1. Figure 3 shows the distributions of these activations obtained for these 6588 samples. One can say that the softmax node representing the Cluster-1 is specialized for these samples as it is the only one getting positive activations while other 4 softmax nodes representing other clusters of Class-1 are negatively activated.\nThis proves that affinity and coactivity successfully fulfill the specialized softmax nodes requirement.\nThen, in order to see whether balance satisfies the equally active softmax nodes requirement or not, we test the same network using all 30596 samples of Class-1 and observe the activations of all 5 duplicated softmax nodes. Figure 4 shows the distributions of these activations. It can be seen that each softmax is active for a subset of the samples of Class-1 (with positive mean activation values between 4.76 and 6.17) and idle for other 4 subsets (with negative mean activation values between -1.27 and -2.19), therefore balance successfully fulfills the equally active softmax nodes requirement. At this point it is important to point that the main purpose of using balance term is not dividing the dataset into clusters with exactly the same number of samples, but it is to prevent dominating softmax node problem and assure that each softmax is active for a subset of samples. In this problem, since there are almost equal number of samples in the expected clusters, we may able to obtain perfectly equal-activated softmax nodes. However, for the problems in which each cluster is expected to have different numbers of samples, this might not be the case. Hence, it is good to interpret \u201cthe equally active\u201d softmax nodes requirement in this sense.\nFigure 5 shows how affinity and balance are settled during weight updates in the first epoch. For this problem, they have already been stabilized by the end of first epoch. Depending on the difficulty of the problem, this might not be the case and stabilization period might continue for many epochs. This figure also shows the drastic fluctuations of coactivity and explains why trying to directly minimize it during first few epochs of the training might be trouble-\nsome. In order to observe the effect of switching to coactivity, at the end of second epoch, we have copied the weights of the network to another one which is identical except for c3 > 0, and continued to train both of them and observed their difference shown in Figure 6. For c3 = 0, when affinity reaches very small values, it stops minimizing coactivity. We can further minimize coactivity with c3 > 0 which may also affect balance. In this example, balance recovers back to its previous level. However, depending on clustering coefficient K, this change may be so forceful that the network cannot recover back. But this trade-off may be acceptable as long as we do not lose any cluster."}, {"heading": "3.1.2. CLUSTERING PERFORMANCE", "text": "For MNIST two class problem, we have obtained clustering performance for two different K settings i.e. K = 5 and K = 10 using ConvNet in following configuration: 32x3x3 - 32x3x3 - MP2x2 - 25% Dropout - 64x3x3 - 64x3x3 - MP2x2 - 25% Dropout - Feedforward 2048 - 50% Dropout - ACOL with untrainable average pooling. We couldn\u2019t observe any significant difference when using ACOL with average or max pooling and with trainable or untrainable weights. All results presented in this paper are obtained using untrainable average pooling layer for ACOL.\nDepending on the initialization of weights, networks may end up with different clustering schemes. In order to understand the range of these schemes, for both K settings, we have repeated the training a sufficient number of times. Also, to observe the effect of switching to minimize coactivity, we have copied the network after the sufficient number of epochs for affinity to go below 0.03, and continued to train the new network with c3 = 0.0003, (chosen depending on coactivity value at that epoch to satisfy c3\u03b1c \u2248 1)\nwhile training the original one with c3 = 0. For both K values, we have set c1 = c2 = 1 and c4 = 0.000001.\nWe have also applied k-means clustering algorithm initialized using k-means++ (Arthur & Vassilvitskii, 2007) to this problem with same K settings. Table 1 summarizes the results for both ACOL and k-means forK = 5 case. There is no single criterion measuring the quality of clusterings, but for this problem since we already know the ground-truth for labels, we measure the clustering accuracy by dividing the number of samples of the major label to the number of all samples grouped into that cluster. Results of k-means hardly vary between re-trainings and it almost ends up with the same clustering scheme. On the other hand, ACOL results differ depending on the initialization of weights even when all regularization terms are well-satisfied, which is an expected and also desired behavior of the proposed clustering method. As no explicit clustering objective defined, networks extract their own objective which may differ depending on initialization and end up with different clustering schemes. Each clustering may correspond well to some property of the real world so that different schemes enable us to explore the hidden patterns in the data. For this problem, in terms of clustering accuracy, even in the worst case ACOL outperforms k-means and, in the best case, it approaches to classification accuracy. For these two extreme cases, samples that are grouped together in each cluster are averaged and obtained mean images of all clusters are visualized in Figure 7 which tells us two schemes disagree on grouping the samples of digits 5 and 8. Besides, since both clustering schemes are completely observable on testing set whose samples are never introduced to the network during training, one can say that clusterings are learned behaviors and they can be generalized to never-seen-before data.\nFigure 8 shows how clustering accuracy changes during training and the effect of switching to minimize coactivity. Range between minimum and maximum observations and average of all re-trainings are also provided on this figure. It can be seen that switching to minimize coactivity improves clustering accuracy both on average and at extrema.\nFor K = 10 case, clustering accuracies are summarized in Table 2. Now, ACOL also tries to split each digit into two, which, in return, decreases the maximum observed\nerror, i.e. increases the worst case accuracy. Also, this increased functionality slightly decreases the best case accuracy. Mean images of all clusters are visualized in Figure 9 for the most accurate clustering scheme. One can observe how two clusters of each digit are diversified in terms of visually identifiable image properties such as digit tilt, roundness, etc. Besides, now it is more obvious that clustering schemes are learned behaviors and generalized to never-seen-before data.\nWhen K = 10, accuracy improvement by switching to minimize coactivity becomes even more significant. Figure 10 shows that both at extrema and on average, clustering accuracy spikes at the epoch when c3 is applied. As a difference between K = 5 and K = 10 cases, since the decrement of affinity slows down with the higher number of clusters, c3 is applied at 20th epoch when K = 10. Applying c3 too soon may result in accuracy decrease instead of improvement - which can be controlled by checking affinity."}, {"heading": "3.2. MNIST - Fully Unsupervised", "text": "For fully unsupervised type of use, entire dataset is duplicated by 8 times to create a made-up classes and corresponding labels, then to be called pseudo-classes and pseudo-labels, such that\nX\u2217 :=  X(0) X(1) ...\nX(7)\n =  g0(X) g1(X)\n... g7(X)\n and t\u2217 := \n0 1 ... 7  (18) where X\u2217 is the duplicated pseudo-dataset and t\u2217 is the corresponding pseudo-labels, gi(.) is the ith transformation to generate the samples of ith pseudo-class , X(i). We have used original dataset, its horizontal flip, and 90o, 180o, 270o rotated versions of both of them as 8 pseudo-classes. Since we do not use any human annotation for categorization of digits and also, provided classification objective through pseudo-classes and pseudo-labels do not involve any explicit information about it, overall process is considered to be an unsupervised learning. Choosing transformations for the best clustering accuracy is still a research question and out of the scope of this paper. Our intuition is that transformations should challenge the network in order to force it to focus on the details of input. For example, distinguishing a digit 0 from its other 7 transformations is a difficult task, as a result we have obtained good clustering performance. However, shifting the images to generate pseudo-classes creates an easier classification task, which results in bad clustering performance.\nWe have used the same ConvNet with same coefficients,\nchosen K = 20, and also switched to minimize coactivity during training. In Figure 11, mean images of clusters are visualized for the samples of original dataset, which is provided to NN as X(0). When K is chosen as 20, at the end of training 4 of these softmax nodes become empty as they never get the maximum activation for any of the samples. k-means is also applied to this problem with K = 20. Table 3 summarizes clustering performances for ACOL and k-means. Clustering accuracy for ACOL is calculated only for its performance on clustering the samples of X(0). As a future idea, one might think to also use the clustering results for other 7 pseudo-classes, in order to obtain a collective clustering decision, which is intuitively likely to be more accurate.\nTable 3. Clustering accuracies for fully unsupervised case when K = 20.\nTrain Test k-means 71.00 71.51 ACOL 92.75 93.20\nACOL-With Active Labeling 95.98 96.59\nFigure 11. Visualization of cluster means obtained for MNIST unsupervised problem.\nWe can further improve the clustering accuracy by defining a secondary classification objective whose classes and labels are provided depending on the clustering obtained in the first training, i.e. active labeling. More specifically, for this problem, 16 clusters become the classes of the second training and another network is then trained using this new objective withK = 4. This second network tries to find the diverse samples in each one of 16 classes. One can observe the mean images of clusters in Figure 12 and notice that some of wrong clustered 4-9 digits and 3-5 digits are now separated and in return, clustering accuracy is improved as can be seen in Table 3."}, {"heading": "4. Conclusion", "text": "In this paper we propose a modification at the output layer of a neural network to enable unsupervised clustering of\nsamples within provided class labels. We introduce and analyze a new regularization term to enable and control this transformation by including the distribution of neuron activations (affinity, balance and coactivity) in the cost function used for supervised training of the modified neural network. We also propose the methodology to utilize deep neural networks in a completely unsupervised learning framework with a paradigm shift we call deep clustering. The results on the MNIST dataset clearly demonstrate that not only the samples are clustered with visually similar characteristics during semi-supervised learning but the improvement in unsupervised performance is very significant compared to traditional clustering algorithms."}], "references": [{"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["Arel", "Itamar", "Rose", "Derek C", "Karnowski", "Thomas P"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "Arel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Arel et al\\.", "year": 2010}, {"title": "k-means++: the advantages of careful seeding", "author": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron C", "Vincent", "Pascal"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Fuzzy ART: fast stable learning and categorization of analog patterns by an adaptive resonance system", "author": ["Carpenter", "Gail A", "Grossberg", "Stephen", "Rosen", "David B"], "venue": "Neural Networks,", "citeRegEx": "Carpenter et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Carpenter et al\\.", "year": 1991}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Hinton and Geoffrey.,? \\Q2010\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The self-organizing map", "author": ["Kohonen", "Teuvo"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Kohonen and Teuvo.,? \\Q1990\\E", "shortCiteRegEx": "Kohonen and Teuvo.", "year": 1990}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["L\u00e4ngkvist", "Martin", "Karlsson", "Lars", "Loutfi", "Amy"], "venue": "Pattern Recognition Letters,", "citeRegEx": "L\u00e4ngkvist et al\\.,? \\Q2014\\E", "shortCiteRegEx": "L\u00e4ngkvist et al\\.", "year": 2014}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Principal components analysis", "author": ["Pearson", "Karl"], "venue": "The London, Edinburgh and Dublin Philosophical Magazine and Journal,", "citeRegEx": "Pearson and Karl.,? \\Q1901\\E", "shortCiteRegEx": "Pearson and Karl.", "year": 1901}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}], "referenceMentions": [{"referenceID": 14, "context": "Artificial neural networks, when first introduced as universal estimators, created wide spread enthusiasm especially with innovative and bio-inspired learning methodologies such as error back-propagation (Rumelhart et al., 1985).", "startOffset": 204, "endOffset": 228}, {"referenceID": 0, "context": "Deep learning mimics the operational and organizational behavior of the human brain, which works through abstraction (Arel et al., 2010) and relies on higher-level representations of features embedded in the data.", "startOffset": 117, "endOffset": 136}, {"referenceID": 10, "context": "For instance, the convolutional neural networks (ConvNet) has been applied to computer vision problems with remarkable success (Krizhevsky et al., 2012).", "startOffset": 127, "endOffset": 152}, {"referenceID": 2, "context": "Representation learning (Bengio et al., 2013) provides a way to perform unsupervised and semi-supervised learning.", "startOffset": 24, "endOffset": 45}, {"referenceID": 11, "context": "One of the applications of representation learning is unsupervised feature learning where unsupervised training is used as a pre-training stage for initializing the hidden layer parameters before they are fine-tuned by some form of supervised training using backpropogation (L\u00e4ngkvist et al., 2014).", "startOffset": 274, "endOffset": 298}, {"referenceID": 7, "context": "While unsupervised feature learning can be dated back to the invention of Principle Component Analysis (PCA) in 1901 (Pearson, 1901), ongoing studies mostly focus on architectures such as Deep Boltzmann Machines (DBM) (Salakhutdinov & Hinton, 2009; Hinton, 2010) Deep Belief Networks (DBN) (Hinton et al., 2006) and autoencoder variants (Coates & Ng, 2011; Coates et al.", "startOffset": 290, "endOffset": 311}, {"referenceID": 4, "context": ", 2006) and autoencoder variants (Coates & Ng, 2011; Coates et al., 2011).", "startOffset": 33, "endOffset": 73}, {"referenceID": 3, "context": "Unlike other existing neural network models used for clustering such as self-organizing maps (SOM) and adaptive resonance theory (ART) (Carpenter et al., 1991), ACOL does not apply competitive learning.", "startOffset": 135, "endOffset": 159}, {"referenceID": 12, "context": "In order to observe the effects of defined regularization terms and clustering performance of ACOL on a wellknown benchmark dataset, we have created clustering problems on MNIST (LeCun et al., 1998) for both semisupervised and fully unsupervised type of uses.", "startOffset": 178, "endOffset": 198}], "year": 2017, "abstractText": "In this paper, we propose a novel method to enrich the representation provided to the output layer of feedforward neural networks in the form of an auto-clustering output layer (ACOL) which enables the network to naturally create sub-clusters under the provided main class labels. In addition, a novel regularization term is introduced which allows ACOL to encourage the neural network to reveal its own explicit clustering objective. While the underlying process of finding the subclasses is completely unsupervised, semi-supervised learning is also possible based on the provided classification objective. The results show that ACOL can achieve a 99.2% clustering accuracy for the semi-supervised case when partial class labels are presented and a 96% accuracy for the unsupervised clustering case. These findings represent a paradigm shift especially when it comes to harnessing the power of deep networks for primary and secondary clustering applications in large datasets.", "creator": "LaTeX with hyperref package"}}}