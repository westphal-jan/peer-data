{"id": "1703.00538", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients", "abstract": "background : electronic health services record ( ee ehr ) notes today contain and abundant complicated medical jargon that can often be difficult for older patients unaware to digitally comprehend. one way suggested to help patients is to reduce accurate information document overload and help educate them focus on medical terms conditions that matter most to them.", "histories": [["v1", "Wed, 1 Mar 2017 22:37:02 GMT  (1318kb)", "http://arxiv.org/abs/1703.00538v1", null], ["v2", "Sat, 25 Mar 2017 21:34:10 GMT  (1319kb)", "http://arxiv.org/abs/1703.00538v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jinying chen", "hong yu"], "accepted": false, "id": "1703.00538"}, "pdf": {"name": "1703.00538.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients", "authors": ["Jinying Chen", "Hong Yu"], "emails": [], "sections": [{"heading": null, "text": "Results: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. Both\nperformance scores significantly exceeded the corresponding scores from the four single rankers (P<.001). FIT also outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter.\nConclusions: FIT can automatically identify EHR terms important to patients. It may help develop future interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications. Keywords: electronic health record; natural language processing; information extraction; unsupervised ensemble ranking"}, {"heading": "1. Introduction", "text": "Greater patient involvement has the potential to improve patient-centered care to build better and safer health services. In a nationwide effort to achieve this goal [1\u2013 3], online patient portals have been widely adopted by health systems in the U.S. [3,4]. Patient portals give patients secure access to their personal health information, including structured information from EHRs (e.g., laboratory test results and medication lists). In addition, the OpenNotes initiative [5] and the Blue Button movement [6] allow patients to access their full EHR notes through the portals, with early evidence showing improved medical comprehension, healthcare management, and outcomes [7\u20139]. However, patients may face great challenges when reading full EHR notes due to the lack of medical training, which would compromise the benefits gleaned from this approach. Since EHR notes are written by physicians for the purpose of record keeping and communication among healthcare providers, they are typically long and contain abundant medical jargon difficult for average people. Previous studies showed that EHRs were written at an 8th-12th grade reading level [10\u201313], which is above the average adult patient\u2019s reading level of 7th to 8th grade in the U.S. [14\u201318]. In addition, 36% adult Americans have limited health literacy [19]. Limited health literacy has been identified as one of the major barriers to patient portal use (which includes interpreting information from EHRs) [20\u201322]. One way to help patients to comprehend their EHR notes is to reduce information overload and help them first understand medical terms that matter most to them. This approach is motivated by two reasons. First, medical terms have been shown to be obstacles for patients [23\u201328]. Second, EHR notes incorporate a comprehensive description of patients\u2019 medical courses, part of which (e.g., technical details about surgery procedures or echocardiogram results) may not directly address patients\u2019 immediate concerns. The approach of explaining all the jargon in their notes at once may likely overwhelm the patients and may be unnecessary in the first place. Therefore, we help patients focus on EHR terms most important to them.\nPersonalized interventions can then be developed to support patient EHR comprehension. Figure 1 shows an excerpt from a typical EHR note from our evaluation data which was annotated by physicians. Although there are many medical terms in this piece of text (here we only highlighted a subset of terms identified by MetaMap [29] for illustration purpose), physicians identified only five terms most important for patients to know, i.e., \u201cpancreatic neoplasm\u201d, \u201cWhipple procedure\u201d, \u201cpancreatectomy\u201d, \u201csplenectomy\u201d, and \u201cinsulin-dependent diabetic\u201d. Note that physicians do not mark many unfamiliar medical terms, e.g., \u201cTinnitus\u201d, \u201cCA 19-9\u201d, and \u201cbile duct stricture\u201d, suggesting that they do not rank terms based on their difficulty levels.\nThe goal of this work is to develop FIT, a robust unsupervised NLP system, to automate the process of identifying EHR(patient)-specific important terms. This task is challenging because FIT does not use supervision from labeled data. In addition, existing unsupervised methods based on general principles of term importance (such as term frequency and topic coherence) are not sufficient to solve this problem (details in the Discussion section). We address this challenge by proposing a new unsupervised ensemble ranking method which adapts the biased\nrandom walk algorithm for information fusion to integrate evidences of term importance from heterogeneous information resources. Our work is an important step towards building generalizable NLP systems to facilitate personalized interventions to improve patient EHR comprehension. Our contributions are multi-folds. Firstly, we developed FIT, a state-of-the-art unsupervised NLP system for ranking and identifying EHR terms important to patients. Secondly, we explored novel domain adaptation techniques and features, including developing a patient vocabulary model using distant supervision. Thirdly, we developed a new ensemble ranking model as the core of FIT and empirically showed that it was robust and flexible in fusing information from multiple resources. These properties, along with the model\u2019s unsupervised nature, ease FIT\u2019s extension to other domains and applications (e.g., document retrieval and opinion extraction). FIT\u2019s output can also be used by other NLP applications, including summarization and question answering."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. NLP Systems Facilitating Concept-level EHR Comprehension", "text": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12]. On the issue of determining which medical terms to simplify, previous work explored frequency-based and/or context-based approaches to check if a term is unfamiliar to the average patients or if it has simpler synonyms [11,30,31]. Such work targets medical jargon and treats all the jargon terms equally important. Our approach is different because it targets important medical terms (which are not equivalent to medical jargon, as discussed in the Introduction section) and is patient-centered (i.e., it finds important terms for each EHR note of individual patients). It is worth noting that our approach is complementary to previous work. For example, in a real-world application, we can display the lay definitions for all the medical jargon in a patient\u2019s EHR note, and then highlight those terms which FIT predicts to be most important to this patient."}, {"heading": "2.2. Unsupervised Single-Document Keyphrase Extraction", "text": "Our work is related to but different from single-document keyphrase extraction (KE), the goal of which is to identify terms representing important concepts and topics in a document. KE targets topics which the writers wanted to convey when\nwriting the documents. Our problem is more challenging because physicians wrote EHR notes for physician-physician communication and, therefore, features extracted from EHR notes may not be sufficient to guide an automated system to find topics and terms important to patients. Previous work in unsupervised KE has explored various techniques, including language modeling, topic-clustering, graph-based ranking and simultaneous learning of keyphrases and key sentences [33]. Among them, graph-based methods such as TextRank [34] and its variations are the state-of-the-arts [33]. We adapted SingleRank [35] (an extension of TextRank) to clinical domain and used it as a baseline as well as an input for the ensemble ranking approaches. KE in the biomedical domain has been limitedly explored in literature articles and in using domain-specific methods and features [36\u201339]. For example, Li et al. [36] developed KIP to extract keyphrases from medical articles. KIP used MeSH (Medical Subject Headings) as the knowledge base to compute a score to reflect a phrase\u2019s domain specificity. It assigned each candidate phrase a rank score by multiplying its within-document term frequency and domain-specificity score. Different from the aforementioned approaches, we used unsupervised ensemble ranking to combine multiple information resources for the ranking.\nFigure 2. Overview of FIT and its evaluation."}, {"heading": "3. The FIT System", "text": "Figure 2 shows the workflow of running and evaluating FIT. FIT operates in three steps: identifies candidate terms, collects information about term importance from multiple resources, and ranks terms. In the first step, FIT applies MetaMap [29], a concept detection tool that automatically maps biomedical text to the Unified Medical Language System (UMLS) concepts, to find medical terms as candidate terms. The remaining pipeline of FIT is detailed below."}, {"heading": "3.1 Four Single Views of Term Importance", "text": ""}, {"heading": "3.1.1 Patient Use of Medical Concepts", "text": "In this view, we consider medical terms frequently used by patients to be important to patients (w.r.t. patient centeredness). One way to quantify terms used by patients is to collect and analyze patients\u2019 queries on the internet.\nThe consumer health vocabulary (CHV) is collection of terms used by patients. It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346]. It contains 152,338 terms, most of which are consumer health terms [44\u201346]. Zeng et al. [45] mapped these consumer health terms to the UMLS concepts by a semi-automatic approach. As the result of this work, the CHV encompasses lay terms as well as corresponding medical terms. Although the CHV is a comprehensive patient-centered resource, its coverage is limited. Therefore, we used the CHV to develop a distant-supervision vocabulary model to rank the patient-centeredness of a medical term. Our model made an assumption that medical terms that occur in both EHRs and the CHV are important to patients for comprehending their EHRs because they are medical synonyms of terms initially used by patients to query online health forums. We built the model on support vector machines (SVMs). The training examples include 37,732 terms identified from 6K EHR notes by MetaMap. We followed [30] (i.e., CHV familiarity score \u2264 0.6) to identify 9,994 medical terms that occur in both the CHV and these EHR terms, which we labeled as positive examples. The remaining 27,738 terms were labeled as negative. We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53]. We trained a neural language model to learn word embeddings by using the Word2Vec software [54,55]. We trained Word2Vec on a combined text corpus (over 3G words) of English Wikipedia, articles from PubMed Open Access and 99K EHR notes from the Pittsburg corpus1. We set the training parameters based on the study of Pyysalo et al. [56]. We represented multi-word terms with the mean of individual word vectors. In this work, we used 200-dimension word vectors, with each dimension normalized to [0,1]. We used the RBF-kernel SVM algorithm implemented by LibSVM [57] to learn feature weights and used LibSVM\u2019s probabilistic outputs [58,59] to rank candidate terms in each EHR note."}, {"heading": "3.1.2 Document-level Term Salience", "text": "1 Chapman W., University of Pittsburgh NLP Repository. Using this data requires a license.\nFIT uses TF*IDF to represent the salience of a candidate term to an individual EHR note. TF*IDF [60] is widely used to measure the salience of a term to a document d in a corpus D, as defined in (1). The more frequent the term appears in the document and the less frequent it appears in other documents, the more important it is to this document.\n\ud835\udc47\ud835\udc39 \u2217 \ud835\udc3c\ud835\udc37\ud835\udc39 (\ud835\udc61, \ud835\udc51, \ud835\udc37) = \ud835\udc47\ud835\udc39(\ud835\udc61, \ud835\udc51) \u2217 \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc61, \ud835\udc37) \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc61, \ud835\udc37) = \ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc41\n|{\ud835\udc51|\ud835\udc61\u2208\ud835\udc51}| (1)\nHere, t is a term; d is a document; \ud835\udc47\ud835\udc39(\ud835\udc61, \ud835\udc51) is the frequency of t in d; \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc61, \ud835\udc37) is the inverse document frequency of t in corpus D; N is the total number of documents in corpus D. We used as D the 6K EHR notes collected from the same domains where the evaluation data was collected."}, {"heading": "3.1.3 Word-occurrence Based Term Relatedness", "text": "We used SingleRank [35] to represent this view. In our case, each EHR note is an undirected, unweighted graph in which words are vertices and are connected if they co-occur within a context window of 10. The rank score of a word is calculated recursively by (2),\n\ud835\udc46(\ud835\udc63\ud835\udc56) = (1 \u2212 \ud835\udc51)\u00d7 1\n\ud835\udc41 + \ud835\udc51 \u00d7 \u2211 (\n\ud835\udf14\ud835\udc57\ud835\udc56\n\u2211 \ud835\udf14\ud835\udc57\ud835\udc58\ud835\udc63\ud835\udc58\u2208\ud835\udc34\ud835\udc51\ud835\udc57(\ud835\udc63\ud835\udc57)\n\ud835\udc46(\ud835\udc63\ud835\udc57))\ud835\udc63\ud835\udc57\u2208\ud835\udc34\ud835\udc51\ud835\udc57(\ud835\udc63\ud835\udc56) (2)\nwhere \ud835\udc63\ud835\udc56 is a word, \ud835\udc34\ud835\udc51\ud835\udc57(\ud835\udc63\ud835\udc56) is a set that contains all neighbors of \ud835\udc63\ud835\udc56 , d is the damping factor which is set to 0.85 [61], \ud835\udf14\ud835\udc57\ud835\udc56 is the edge weight which equals the number of co-occurrences of \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57 within a context window of 10. \ud835\udf14\ud835\udc57\ud835\udc56\n\u2211 \ud835\udf14\ud835\udc57\ud835\udc58\ud835\udc63\ud835\udc58\u2208\ud835\udc34\ud835\udc51\ud835\udc57(\ud835\udc63\ud835\udc57)\nis the\nprobability of reaching \ud835\udc63\ud835\udc56 by \ud835\udc63\ud835\udc57 .\nThe rank score of a candidate term is the sum of the rank scores of individual words contained in this term."}, {"heading": "3.1.4 Topic Coherence", "text": "In this view, the importance of a candidate term to an EHR note is measured by the topic coherence between the term and the note. We compute topic coherence \ud835\udc43(\ud835\udc61|\ud835\udc52) by (3) and (4),\n\ud835\udc43(\ud835\udc64|\ud835\udc52) = \u2211 \ud835\udc43(\ud835\udc64|\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc50\ud835\udc56)\ud835\udc43(\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc50\ud835\udc56|\ud835\udc52) \ud835\udc3e \ud835\udc56=1 (3)\n\ud835\udc43(\ud835\udc61|\ud835\udc52) = \u2211 \ud835\udc43(\ud835\udc64|\ud835\udc52)\ud835\udc64\u2208\ud835\udc61 (4)\nwhere P(t|e) is the probability of a candidate term conditioned on an EHR note e; P(w|e) is the probability of a word w conditioned on e; \ud835\udc43(\ud835\udc64|\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc50\ud835\udc56) and \ud835\udc43(\ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc50\ud835\udc56|\ud835\udc52) are word-topic and topic-EHR note distributions estimated by the topic model; K is the number of topics used in topic modeling. We used the Latent Dirichlet Allocation algorithm implemented by MALLET [62] for topic modeling. We trained the topic model on the same 6K EHR notes which were used to compute TF*IDF and set K to 200 after testing different K\u2019s on the training data."}, {"heading": "3.2 Additional Information about Term Importance", "text": ""}, {"heading": "3.2.1 Term Unfamiliarity", "text": "Since familiar terms may be too common to be important to patients, we made an assumption that unfamiliar terms are in general more important than familiar ones and thus should be ranked higher. We used CHV familiarity scores to distinguish between unfamiliar terms and familiar terms. The CHV assigns familiarity scores to 58% (88,189 out of 152,338) of its terms for extended usability. CHV familiarity scores estimate the likelihood that a medical term can be understood by an average reader [63] and take values between 0 and 1 (with 1 being most familiar and 0 being least familiar). The CHV provides different types of familiarity scores [30]. Following [30], we used the combined score and a score threshold 0.6 to detect unfamiliar and familiar terms (familiarity score \u22640.6 are unfamiliar terms; >0.6 are familiar terms). Since not every EHR term has a CHV familiarity score, the available information about term unfamiliarity is partial. Note that our approach is different from ranking terms by term familiarity scores for two reasons: (1) we only use two familiarity levels; and (2) the most unfamiliar terms (with the lowest familiarity score in the CHV) are not necessarily the most important terms."}, {"heading": "3.2.2 Semantic Types of Medical Concepts", "text": "We found that the distribution of UMLS semantic types over medical concepts in the CHV was skewed. This suggests that medical concepts frequently used by patients may concentrate on certain semantic types (which we called CHV-preferred semantic types). We therefore made an assumption that medical terms with CHVpreferred semantic types are more important to patients than terms with other semantic types and thus should be ranked higher. We used a heuristic, i.e., frequency > 1,000, to select 12 CHV-preferred semantic types (see Appendix A). Note that the evidence of term importance from CHV-preferred semantic types is\ncoarse-grained because it only divides candidate terms into two groups: likely important and unlikely important, according to their semantic types."}, {"heading": "3.3 Combining Heterogeneous Information Resources by Random Walk", "text": "Our goal is to rank EHR terms based on their importance to patients. As described previously, we have collected information from multiple resources which represent term importance from different perspectives. However, we do not know which perspective is more relevant to the patient perspective. Since these information resources are complimentary to each other, an ensemble model that utilizes all of them is expected to be more robust than any model that uses only a single resource. We used unsupervised ensemble learning and built FIT on an adapted random walk algorithm derived from PageRank [61]. The adaption is to allow transition from term \ud835\udc61\ud835\udc57 to term \ud835\udc61\ud835\udc56 during random walk if and only if term \ud835\udc61\ud835\udc57 is ranked lower than term \ud835\udc61\ud835\udc56 based on comprehensive information from all available resources (i.e., single rankers). Specifically, for each EHR note, we generate a directed graph where candidate terms from the note are treated as vertices. We form an edge from term \ud835\udc61\ud835\udc57 to term \ud835\udc61\ud835\udc56 if and only if both the rank relation \ud835\udc45(\ud835\udc61\ud835\udc57 \u2192 \ud835\udc61\ud835\udc56) defined in (5) and the edge weight defined in (6) are greater than 0.\n\ud835\udc45(\ud835\udc61\ud835\udc57 \u2192 \ud835\udc61\ud835\udc56) = |{\ud835\udc5f|\ud835\udc5f(\ud835\udc61\ud835\udc56) < \ud835\udc5f(\ud835\udc61\ud835\udc57)}| \u2212 |{\ud835\udc5f|\ud835\udc5f(\ud835\udc61\ud835\udc57) < \ud835\udc5f(\ud835\udc61\ud835\udc56)}| (5)\n\ud835\udf14\ud835\udc57,\ud835\udc56 = \ud835\udc45(\ud835\udc61\ud835\udc56 \u2192 \ud835\udc61\ud835\udc57) \u00d7 \u2211 ( 1\n\ud835\udc5f(\ud835\udc61\ud835\udc56) \ud835\udc5f \u2212\n1\n\ud835\udc5f(\ud835\udc61\ud835\udc57) ) (6)\nHere, r is any single ranker that ranks both \ud835\udc61\ud835\udc56 and \ud835\udc61\ud835\udc57; r(t) is the rank assigned to term t by ranker r; |{\ud835\udc5f|\ud835\udc5f(\ud835\udc61\ud835\udc56) < \ud835\udc5f(\ud835\udc61\ud835\udc57)}| is the number of single rankers that rank \ud835\udc61\ud835\udc56 higher than \ud835\udc61\ud835\udc57 .\nBecause some candidate terms do not have familiarity scores, the single ranker that uses this information cannot rank all terms. We estimated r(t) for this ranker by using all candidate terms that have familiarity scores. Our random walk algorithm estimates the importance of each term by counting the support from its neighboring terms (i.e., any term that has an edge pointing to this term) recursively. Neighbors of higher importance and with higher probabilities to reach a term contribute more to the term\u2019s importance. Mathematically, we used the same equation as defined in (2) to update the importance score of each term, where the word \ud835\udc63\ud835\udc56 (\ud835\udc63\ud835\udc57) is replaced by the term \ud835\udc61\ud835\udc56 (\ud835\udc61\ud835\udc57).\nThe aforementioned algorithm uses only rank orders. Rank scores, when available, are also useful for ensemble ranking [64,65]. We therefore extended our model, by using the framework of a biased random walk model, to incorporate rank scores, as defined in (7),\n\ud835\udc46(\ud835\udc61\ud835\udc56) = (1 \u2212 \ud835\udc51)\u00d7\ud835\udc5d\ud835\udc56 + \ud835\udc51 \u00d7 \u2211 ( \ud835\udf14\ud835\udc57\ud835\udc56\n\u2211 \ud835\udf14\ud835\udc57\ud835\udc58\ud835\udc63\ud835\udc58\u2208\ud835\udc34\ud835\udc51\ud835\udc57(\ud835\udc61\ud835\udc57)\n\ud835\udc46(\ud835\udc61\ud835\udc57))\ud835\udc63\ud835\udc57\u2208\ud835\udc34\ud835\udc51\ud835\udc57(\ud835\udc61\ud835\udc56) (7)\nwhere we replace the constant 1/N in (2) by \ud835\udc5d\ud835\udc56, the probability of the random jump to \ud835\udc61\ud835\udc56. We calculate \ud835\udc5d\ud835\udc56 by (8),\n\ud835\udc5d\ud835\udc56 = 1\n\ud835\udc4d \u00d7 \u2211 \ud835\udc60\ud835\udc5f(\ud835\udc61\ud835\udc56)\ud835\udc5f (8)\nwhere \ud835\udc4d is a normalization constant that ensures \u2211 \ud835\udc5d\ud835\udc56 = 1 \ud835\udc41 \ud835\udc56=1 ; r is a single ranker that outputs rank scores for all candidate terms; \ud835\udc60\ud835\udc5f(\ud835\udc61\ud835\udc56) is \ud835\udc61\ud835\udc56\u2019s normalized rank score as assigned by r. We use the standard zero-one normalization method (i.e.,\n\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc65\u2212\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\n\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc65\u2212\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc56\ud835\udc5b ) to normalize rank scores. Note that the parameter d in (7) serves a different purpose than the original damping factor and is used to weight the contributions of rank orders and rank scores for the ranking. We set d to 0.5."}, {"heading": "4. Experimental Settings", "text": ""}, {"heading": "4.1 Evaluation Set", "text": "Our evaluation set is a collection of 90 representative EHR discharge summaries and progress notes from UMass Memorial Hospital outpatient clinics. To maximize the representativeness, we selected notes from patients with six different but common primary clinical diagnoses: cancer, COPD, diabetes, heart failure, hypertension, and liver failure. We de-identified the notes and then asked physicians to identify, for each note, at least 5 most important medical terms which the patients should know in order to better understand their health and treatment courses. We asked physicians to do the annotation because this task requires a full comprehension of EHR notes which is beyond the capacity of the average patients [11\u201313,30]. We developed an annotation guideline (see Appendix B) to instruct physicians to annotate the notes from the patient perspective. For each note, we obtained annotations from two physicians and used the agreement from both physicians as the gold-standard. The annotation agreement (micro average) on the 90 notes is 0.51 Cohen\u2019s Kappa. In total, the physicians have identified 793 important medical terms (9\u00b15 terms per note), which covers a wide range of topics including disease, syndrome, medication, and diagnostic and therapeutic procedures. Table 1 summarizes the statistics of the evaluation set.\nTable 1. Statistics of the evaluation set.\nEvaluation set Number of notes 90 Number of words per EHR note (mean \u00b1 std) 816\u00b1133 Number of candidate terms identified by MetaMap per EHR note (mean \u00b1 std) 250\u00b142\nNumber of important medical terms identified by physicians per EHR note (mean \u00b1 std)\n9\u00b15"}, {"heading": "4.2 Baseline Systems", "text": "We used four single-view rankers (details in section 3.1), called Patient Vocabulary Model, TF*IDF, Adapted SingleRank, and Topic Coherence respectively, as baseline systems to test the effect of ensemble ranking. In addition, we implemented three benchmark unsupervised ensemble ranking methods, CombSum [64], Condorcet-fuse [66], and Reciprocal rank fusion [67], as strong baselines. The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65]."}, {"heading": "4.2.1 CombSum", "text": "CombSum[64] is a rank-score-based ensemble method, which calculates the rank score of a candidate term t by summing t\u2019s rank scores received from single rankers, as calculated by (9),\n\ud835\udc36\ud835\udc5c\ud835\udc5a\ud835\udc4f\ud835\udc46\ud835\udc62\ud835\udc5a (\ud835\udc61) = \u2211 \ud835\udc60\ud835\udc5f(\ud835\udc61){\ud835\udc5f|\ud835\udc5f\u2208\ud835\udc45} (9)\nwhere R is the set of single rankers to ensemble; \ud835\udc60\ud835\udc5f(\ud835\udc61) is the rank score of t given by a single ranker r."}, {"heading": "4.2.2 Condorcet Fuse", "text": "Condorcet Fuse[66] sorts candidate terms by pairwise rank relation \ud835\udc45(\ud835\udc61\ud835\udc57 \u2192 \ud835\udc61\ud835\udc56) as defined in (5). Specifically, it ranks \ud835\udc61\ud835\udc56 higher than \ud835\udc61\ud835\udc57 if \ud835\udc45(\ud835\udc61\ud835\udc57 \u2192 \ud835\udc61\ud835\udc56) > 0. We implemented the Condorcet Fuse ranker using the quick sort algorithm by following [66].\n4.2.3 Reciprocal Rank Fusion (RRF)\nReciprocal Rank Fusion[67] calculates the rank score of a candidate term t by using t\u2019s ranks assigned by single rankers, as defined in (10),\n\ud835\udc45\ud835\udc45\ud835\udc39(\ud835\udc61) = \u2211 1\n\ud835\udc58+\ud835\udc5f(\ud835\udc61){\ud835\udc5f|\ud835\udc5f\u2208\ud835\udc45} (10)\nwhere R is the set of single rankers to ensemble; r(t) is the rank of t given by a single ranker r; k is a parameter used to mitigate the impact of high ranks of t assigned by potential outlier systems. We set k to 60 by following [67]."}, {"heading": "4.3 Evaluation Metrics", "text": "Precision/Recall/F-score at n: the averaged precision, recall and F-score at ranks 5 and 10 respectively (abbreviated as P5, R5, F5, P10, R10, and F10). These metrics measure system performances for top ranks and are widely used to evaluate KE systems. We computed these metrics by using all the gold-standard important terms (including those that would never be included in the stage of candidate term extraction) as positive examples. Area Under ROC Curve (AUC-ROC): AUC-ROC is a metrics widely used for evaluating ranking outputs. It computes the area under a ROC curve, which plots the true positive rate (y-coordinate) against the false positive rate (x-coordinate) at various threshold settings. When evaluating a system, we compute its AUC-ROC for each EHR note in the evaluation set and report the averaged value. AUC-ROC measures the performance of the global ranking. Because both candidate term extraction and ranking affect the quality of global ranking, we report two AUC-ROC metrics: AUCROCranking and AUC-ROCKE. AUC-ROCranking is computed on the candidate terms extracted by a system. Thereby, if a gold-standard important term is missed in candidate term extraction, it will not affect the system\u2019s AUC-ROCranking. AUC-ROCKE is computed by using all the gold-standard important terms as positive examples and measures the combined performance of candidate term extraction and ranking.\nIn evaluation, we use relaxed string match to determine true positives as exact match is known to underestimate performance as perceived by human judges [73]. Specifically, we treat a term from the system output as a true positive if it either exactly matches or subsumes (e.g., \u201cnon-Hodgkin lymphoma\u201d subsumes \u201clymphoma\u201d) a gold-standard important term. We allow \u201csubsume\u201d but not \u201cpart-of\u201d match in relaxed string match, as previous work found that the former aligned well with human judges but the latter did not [74]. For example, a part of an important term may be too general to be important, e.g., \"disease\" in \"Crohn's disease\" and \"iron\" in \"iron deficiency\"."}, {"heading": "5. Results", "text": "5.1 Candidate Term Extraction\nOn average, FIT extracts 250 candidate terms per EHR note in the evaluation set, which match 89% gold-standard (physician annotated terms)."}, {"heading": "5.2 Evaluation Results", "text": "Our results (in Table 2) show that FIT performs significantly better than any singleview ranker (see Table 2 for P values).\nIn addition, as shown in Table 3, FIT outperforms the three ensemble methods for all metrics except having a tie with CombSum on AUC-ROCKE. The performance difference between FIT and each baseline ensemble method is statistically significant for some metrics (see Table 3 for P values).\nCombSum 0.302 0.202 0.225 0.253 (P=.03) 0.335 0.266 (P=.01) 0.884 0.813\nCondorcetFuse 0.264 0.168 0.191 0.218 (P<.001) 0.277 (P<.001) 0.225 (P<.001) 0.819 (P<.001) 0.753 (P<.001)\nReciprocal Rank Fusion 0.313 0.208 0.230 0.249 (P=.004) 0.322 (P=.02) 0.260 (P=.01) 0.878 (P=.04)\n0.807 (P=.04)\nFigure 3 shows the ranks which different NLP systems respectively assign to the five important medical terms identified by physicians in the EHR excerpt in Figure 1, where the ranks assigned by FIT are higher than those assigned by the baseline systems for most cases. The original data for Figure 3 is shown in Appendix C.\n5.3 Impact of Parameter d on FIT\u2019s Performance Figure 4 plots FIT\u2019s performance (F5, F10, AUC-ROCranking, and AUC-ROCKE) for different values of the parameter d used in Eq. (7) (the value of each data point is provided in Appendix D). As shown in Figure 4a, when d>0, the F5 score is relatively stable; while the F10 score first increases, reaching a peak point at d=0.2, and then decreases. In addition, when d>0, the F5 and F10 scores at different d values are consistently higher than the respective F5 and F10 scores at d=0 (FIT equals\nCombSum at d=0). As shown in Figure 4b, the two AUC-ROC scores have the same trend, reaching the highest values at d=0.1 and slightly decreasing for bigger d\u2019s.\n5.4 Effects of Domain-Specific Information on FIT\u2019s Performance\nWe compared the performances of an implementation of FIT that uses information only from TF*IDF, TextRank, and Topic Coherence (called FIT-generic) and the other three implementations that respectively add information from each of the three domain-specific knowledge resources (i.e., Patient Vocabulary Model, term unfamiliarity, and UMLS semantic types) on FIT-generic.\nThe results (Table 4) show that adding each type of domain-specific information to FIT-generic consistently improves its performance for all metrics. The system differences are significant for AUC-ROC measures (P<.001)."}, {"heading": "6. Discussion", "text": ""}, {"heading": "6.1 Principle Results", "text": "Automated ranking of EHR terms based on their importance to patients is challenging because EHR notes contain abundant medical terms, among which only a small portion (4% in our case) were judged to be important by physicians. Unsupervised ranking of EHR terms is even more challenging for two reasons. First, the ranker has no supervision from annotated data. Second, and probably most important, many factors affect the importance of a term. As a result, no single criterion or type of information is sufficient to achieve adequate results in ranking candidate terms (see Rows 2-4 in Table 2). Our system FIT overcomes this problem by integrating multiple complimentary information resources. It achieves 0.885 AUC-ROC in ranking candidate terms and 0.813 AUC-ROC when counting errors from candidate term identification. This performance level is adequate, especially for unsupervised systems."}, {"heading": "6.2 FIT vs. Single Rankers", "text": "Unsupervised ensemble ranking methods typically assume that the inputs from the single rankers are reasonably good and are sufficiently different and complimentary\nto each other [65,68]. When this assumption is violated, an ensemble ranker is not guaranteed to outperform the (best) single rankers [66,68,70,75]. Our results (Table 2) show that FIT works well and outperforms each single ranker significantly for all the metrics, suggesting that our design of FIT and selection of single rankers are appropriate and effective. Among the four single rankers, Topic Coherence performs best (Row 5 in Table 2). Topic modeling has been used to extend SingleRank to improve the state-of-the-arts in unsupervised KE [76,77]. Our results suggest that Topic Coherence, as a standalone ranker, can provide good-quality input for ensemble ranking for NLP tasks similar to KE. SingleRank is among the state-of-the-arts and TF*IDF is frequently used as a strong baseline in unsupervised KE from scientific literature and news [33]. Both models heavily rely on word/term frequency. However, in our data, 56% important medical terms occur only once in any individual EHR note, which partially explains why the two methods are less effective for EHRs. Unlike other single rankers, Patient Vocabulary Model was introduced specifically to incorporate domain(task)-specific knowledge into FIT. Our results show that its performance (Row 2 in Table 2) is comparable to other single rankers and is the second best on the AUC-ROC scores. In addition, as shown in Table 4 (Row 2 vs. Row 1), adding this model on a generic FIT system (FIT-generic) that uses only general information about term importance improves performance for all metrics. These results verify the validity of our assumption behind this model, i.e., medical terms that occur in both EHRs and the CHV are important to patients for comprehending their EHRs."}, {"heading": "6.3 FIT vs. Baseline Ensemble Rankers", "text": "CondorcetFuse, Reciprocal Rank Fusion, and CombSum respectively use pairwise rank order relations, rank orders, and rank scores for ensemble ranking. Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75]. Furthermore, in real-world tasks, it is likely that we have only certain types of information (e.g., pairwise rank order relations from product review) or have information from heterogeneous resources (e.g., in our case). Therefore, it is desired to have a robust ensemble ranker that can utilize different information resources flexibly. FIT, by its design and as confirmed by our experiments, has the desired properties. For example, as shown in Table 4, FIT not only can use the complete information about rank scores and orders given by Patient Vocabulary Model to improve its performance (Row 2 vs. Row 1), but also can use the partial information about rank orders inferred from term unfamiliarity (Row 3 vs. Row 1). In addition, FIT\u2019s\nperformance is relatively stable at different d values (Figure 4), confirming its robustness. These properties make FIT easily generalizable to new domains and other ranking problems."}, {"heading": "6.4 Error Analysis and Future Work", "text": "We manually examined 22 notes, on which FIT has either zero recall at rank 10 or low AUC-ROCKE (<0.700). We identified three types of errors. First, we used relaxed string match for evaluation but did not allow \u201cpart-of\u201d match (for the reason discussed in section 4.3). However, in some cases, this approach underestimates the performance. For example, FIT counted it as a mistake if MetaMap recognized \u201cinvasive\u201d and \u201ccarcinoma of the colon cancer\u201d but not \u201cinvasive carcinoma of the colon cancer\u201d, the gold-standard term. Second, FIT depends on MetaMap, which makes mistakes. It failed to identify certain abbreviations as medical terms, e.g., A1c (a lab test for blood glucose), MRCP (Magnetic resonance cholangiopancreatography), CPPD (calcium pyrophosphate deposition disease), and TSH (a lab test for thyroid stimulating hormone). In future work, we may collect a list of common clinical abbreviations by mining a large EHR corpus and uses this list to enhance medical term identification. Third, physicians sometimes judged common disease names (e.g., \u201chypertension\u201d, \u201cdiabetes\u201d, and \u201ccoronary artery disease\u201d) as important when they are the major diagnoses of a patient. These terms were frequently missed from FIT\u2019s top-10 because they were ranked low by TF*IDF (due to their high document frequencies) and the patient vocabulary model (which was trained to rank unfamiliar terms in both EHRs and the CHV high)."}, {"heading": "7. Conclusions", "text": "We have presented FIT, an unsupervised ensemble ranking system for identifying medical terms important to patients from individual EHR notes. FIT can combine heterogeneous information resources. It achieves promising results and outperforms benchmark unsupervised ensemble methods in ranking EHR terms. Our work is an important step towards empowering patients to comprehend their own EHR notes to improve quality of care. By using unsupervised learning and robust information fusion techniques, FIT can be readily applied to other domains and applications.\nConflicts of Interest None declared.\nAbbreviations AUC-ROC: Area Under ROC Curve CHV: consumer health vocabulary EHR: electronic health record FIT: Finding Important Terms for patients KE: keyphrase extraction MeSH: Medical Subject Headings NLP: natural language processing SVM: support vector machines UMLS: Unified Medical Language System"}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Investigator Initiated Research 1I01HX001457-01 from the Health Services Research & Development Program of the United States (U.S.) Department of Veterans Affairs. We also acknowledge the support from the Institutional National Research Service Award (T32) 5T32HL120823-02 from the National Institutes of Health (NIH). The content is solely the responsibility of the authors and does not necessarily represent the official views of the US Department of Veterans Affairs, NIH or the United States Government. We thank the UMassMed annotation team, including Elaine Freund, Victoria Wang, Andrew Hsu, Barinder Hansra and Sonali Harchandani, for creating the evaluation corpus and thank Weisong Liu for technical support in collecting EHR notes. We also thank the anonymous reviewers for their constructive comments and suggestions."}, {"heading": "Appendix A. Twelve semantic types used to prioritize important medical terms in EHR notes", "text": "UMLS semantic type Example EHR terms Pharmacologic Substance Advair, Budesonide, insulin, NSAIDs, Spironolactone Disease or syndrome autoimmune hemolytic anemia, gastroesophageal reflux, pancytopenia, Sjogren's syndrome, osteoporosis Organic chemical Atenolol, Vincristine, Warfarin, Wellbutrin, Zocor Finding alopecia, hematuria, hypertension, NSTEMI (Non-ST-elevation myocardial infarction), retinopathy Therapeutic or preventive procedure chemotherapy, dialysis, immunosuppression, kidney transplantation, pancreatectomy\nAmino acid, peptide, or protein2\nbasal insulin, Rituxan, Neupogen, Synthroid, hemoglobin A1C, HPL (human placental lactogen)\nBody Part, Organ, or Organ Component\nadrenal glands, coronary arteries, cranial nerves, lymph nodes, thyroid nodule\nSign or Symptom lower extremity edema, sciatica, scleral icterus, syncopal episodes, vertigo Medical Device Foley catheter, defibrillator, insulin pump, pacemaker, pedometer Neoplastic process dermoid, large B cell lymphoma, pancreatic neoplasm, thyroid nodule Injury or Poisoning bruising, distal radial fracture, exposure to asbestos, spinal compression fractures, Methotrexate toxicity Laboratory Procedure hepatitis B serology, LFTs, lipid panel, sedimentation rate, urinalysis Pathologic function atrial fibrillation, autonomic dysfunction, BPH (benign prostatic hyperplasia), microscopic hematuria, systolic dysfunction Diagnostic procedure thyroid ultrasound, echocardiogram, endoscopy, biopsy, cardiac\ncatheterization\n2 EHR terms in this topic split into two subtopics: medicine (denoted by their ingredients) and laboratory measure."}, {"heading": "Appendix B. Guidelines for annotating medical terms important to patients in EHR notes", "text": "1. Goal/task: identifying at least five most important medical terms per EHR note which the patients need to know in order to better understand their EHR notes In general, the goal can be achieved by selecting the minimum number of medical terms, which if the patients know, they will have a significant understanding of their clinical diseases and symptoms without being overwhelmed. We provide operational rules in Section 2 to help achieve this goal. 2. Selection criteria (1) Include terms that represent the main concept of each EHR note Note: The most important medical terms that patients should know shall be straight-forward clinical knowledge, rather than complex clinical knowledge that may confuse patients or may need additional explanation (2) Include terms that are related to the main concepts identified in (1) and can help patients\u2019 comprehension of the most important clinical concepts in their EHR notes Note: These related terms also shall be straight-forward clinical knowledge, rather than complex clinical knowledge that may confuse patients or may need additional explanation"}, {"heading": "Appendix C. Ranks assigned by different NLP systems to the five important medical terms in the EHR excerpt in Figure 1", "text": "Systems insulin-\ndependent\ndiabetic\npancreatectomy pancreatic\nneoplasm\nsplenectomy Whipple\nprocedure\nFIT 58 3 4 22 32\nPatient\nVocabulary\nModel\n147 50 44 14 64\nTF*IDF 182 2 25 52 31\nAdapted\nSingleRank\n25 51 7 111 60\nTopic\nCoherence\n8 132 23 133 106\nCombSum 76 11 6 19 72\nCondorcetFuse 69 93 10 59 97\nReciprocal\nRank Fusion\n39 20 4 51 52\nAppendix D. Effect of d on FIT\u2019s ranking performance\nd P5 R5 F5 P10 R10 F10 AUCROCranking AUCROCKE\n0 (CombSum) 0.302 0.202 0.225 0.253 0.335 0.266 0.884 0.813\n0.1 0.318 0.206 0.232 0.281 0.366 0.293 0.887 0.815\n0.2 0.318 0.208 0.232 0.287 0.365 0.296 0.887 0.815\n0.3 0.316 0.209 0.232 0.283 0.362 0.293 0.886 0.814\n0.4 0.318 0.208 0.233 0.283 0.364 0.293 0.885 0.813\n0.5 0.320 0.209 0.234 0.281 0.361 0.291 0.885 0.813\n0.6 0.320 0.208 0.233 0.278 0.355 0.287 0.884 0.812\n0.7 0.322 0.209 0.234 0.280 0.357 0.289 0.883 0.811\n0.8 0.322 0.209 0.234 0.278 0.351 0.286 0.883 0.811\n0.9 0.322 0.209 0.234 0.276 0.348 0.283 0.882 0.810\n1.0 0.324 0.209 0.235 0.274 0.346 0.282 0.882 0.810"}], "references": [{"title": "Health care and the American Recovery and Reinvestment Act", "author": ["R. Steinbrook"], "venue": "N. Engl. J. Med. 360 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "The Medicare Electronic Health Record Incentive Program: provider performance on core and menu measures", "author": ["A. Wright", "J. Feblowitz", "L. Samal", "A.B. McCoy", "D.F. Sittig"], "venue": "Health Serv. Res. 49 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Patient Portals and Patient Engagement: A State of the Science Review", "author": ["T. Irizarry", "A. DeVito Dabbs", "C.R. Curran"], "venue": "J. Med. Internet Res. 17 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Inviting Patients to Read Their Doctors\u2019 Notes: A Quasi-experimental Study and a Look Ahead", "author": ["T. Delbanco", "J. Walker", "S.K. Bell", "J.D. Darer", "J.G. Elmore", "N. Farag", "H.J. Feldman", "R. Mejilla", "L. Ngo", "J.D. Ralston", "S.E. Ross", "N. Trivedi", "E. Vodicka", "S.G. Leveille"], "venue": "Ann. Intern. Med. 157 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating patient access to Electronic Health Records: results from a survey of veterans", "author": ["K.M. Nazi", "T.P. Hogan", "D.K. McInnes", "S.S. Woods", "G. Graham"], "venue": "Med. Care. 51 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Patient experiences with full electronic access to health records and clinical notes through the My HealtheVet Personal Health Record Pilot: qualitative study", "author": ["S.S. Woods", "E. Schwartz", "A. Tuepker", "N.A. Press", "K.M. Nazi", "C.L. Turvey", "W.P. Nichol"], "venue": "J. Med. Internet Res. 15 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Text characteristics of clinical reports and their implications for the readability of personal health records", "author": ["Q. Zeng-Treitler", "H. Kim", "S. Goryachev", "A. Keselman", "L. Slaughter", "C.A. Smith"], "venue": "Stud. Health Technol. Inform. 129 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "A Semantic and Syntactic Text Simplification Tool for Health Content", "author": ["S. Kandula", "D. Curtis", "Q. Zeng-Treitler"], "venue": "AMIA. Annu. Symp. Proc. 2010 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving patients\u2019 electronic health record comprehension with NoteAid", "author": ["B. Polepalli Ramesh", "T. Houston", "C. Brandt", "H. Fang", "H. Yu"], "venue": "Stud. Health Technol. Inform. 192 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Opportunities to improve clinical summaries for patients at hospital discharge", "author": ["E. Sarzynski", "H. Hashmi", "J. Subramanian", "L. Fitzpatrick", "M. Polverento", "M. Simmons", "K. Brooks", "C. Given"], "venue": "BMJ Qual. Saf. ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching patients with low literacy skills", "author": ["C.C. Doak", "L.G. Doak", "J.H. Root"], "venue": "AJN Am. J. Nurs. 96 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Improving comprehension for cancer patients with low literacy skills: strategies for clinicians", "author": ["C.C. Doak", "L.G. Doak", "G.H. Friedell", "C.D. Meade"], "venue": "CA. Cancer J. Clin. 48 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Readability assessment of internet-based consumer health information", "author": ["T.M. Walsh", "T.A. Volsko"], "venue": "Respir. Care. 53 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Readability of patient education materials on the American Orthopaedic Society for Sports Medicine website", "author": ["A.E.M. Eltorai", "A. Han", "J. Truntzer", "A.H. Daniels"], "venue": "Phys. Sportsmed. 42 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Readability of Written Materials for CKD Patients: A Systematic Review", "author": ["S. Morony", "M. Flynn", "K.J. McCaffery", "J. Jansen", "A.C. Webster"], "venue": "Am. J. Kidney Dis. Off. J. Natl. Kidney Found. 65 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The literacy divide: health literacy and the use of an internet-based patient portal in an integrated health system-results from the diabetes study of northern California (DISTANCE)", "author": ["U. Sarkar", "A.J. Karter", "J.Y. Liu", "N.E. Adler", "R. Nguyen", "A. Lopez", "D. Schillinger"], "venue": "J. Health Commun. 15 Suppl 2 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Consumers\u2019 perceptions of patient-accessible electronic medical records", "author": ["C. Zarcadoolas", "W.L. Vaughon", "S.J. Czaja", "J. Levy", "M.L. Rockoff"], "venue": "J. Med. Internet Res. 15 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Barriers and Facilitators to Online Portal Use Among Patients and Caregivers in a Safety Net Health Care System: A Qualitative Study", "author": ["L. Tieu", "U. Sarkar", "D. Schillinger", "J.D. Ralston", "N. Ratanawongsa", "R. Pasick", "C.R. Lyles"], "venue": "J. Med. Internet Res. 17 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "C", "author": ["C. Pyper", "J. Amery", "M. Watson"], "venue": "Crook, Patients\u2019 experiences when accessing their on-line electronic patient records in primary care., Br J Gen Pr. 54 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Lay understanding of terms used in cancer consultations", "author": ["K. Chapman", "C. Abraham", "V. Jenkins", "L. Fallowfield"], "venue": "Psychooncology. 12 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Medical communication: do our patients understand", "author": ["E.B. Lerner", "D.V. Jehle", "D.M. Janicke", "R.M. Moscati"], "venue": "Am. J. Emerg. Med. 18 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Patient on-line access to medical records in general practice", "author": ["R.B. Jones", "S.M. McGhee", "D. McGhee"], "venue": "Health Bull. (Edinb.). 50 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "V", "author": ["M. Baldry", "C. Cheal", "B. Fisher", "M. Gillett"], "venue": "Huet, Giving patients their own records in general practice: experience of patients and staff., Br. Med. J. Clin. Res. Ed. 292 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1986}, {"title": "An overview of MetaMap: historical perspective and recent advances", "author": ["A.R. Aronson", "F.-M. Lang"], "venue": "J. Am. Med. Inform. Assoc. 17 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Making texts in electronic health records comprehensible to consumers: a prototype translator", "author": ["Q. Zeng-Treitler", "S. Goryachev", "H. Kim", "A. Keselman", "D. Rosendale"], "venue": "AMIA Annu. Symp. Proc. AMIA Symp. AMIA Symp. ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Medical text simplification using synonym replacement: Adapting assessment of word difficulty to a compounding language", "author": ["E. Abrahamsson", "T. Forni", "M. Skeppstedt", "M. Kvist"], "venue": "in: Proc. 3rd Workshop Predict. Improv. Text Readability Target Read. Popul. EACL, Association for Computational Linguistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Methods for Linking EHR Notes to Education Materials", "author": ["J. Zheng", "H. Yu"], "venue": "AMIA Jt. Summits Transl. Sci. Proc. AMIA Summit Transl. Sci. 2015 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic keyphrase extraction: A survey of the state of the art", "author": ["K.S. Hasan", "V. Ng"], "venue": "in: Proc 52nd Annu. Meet. Assoc. Comput. Linguist. ACL, Citeseer,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "TextRank: Bringing order into texts, in: Association for Computational Linguistics", "author": ["R. Mihalcea", "P. Tarau"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Single document keyphrase extraction using neighborhood knowledge", "author": ["X. Wan", "J. Xiao"], "venue": "in: Proc. 23rd Natl. Conf. Artif. Intell., AAAI Press,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Identifying important concepts from medical documents", "author": ["Q. Li", "Y.-F.B. Wu"], "venue": "J. Biomed. Inform. 39 ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain-independent automatic keyphrase indexing with small training sets", "author": ["O. Medelyan", "I.H. Witten"], "venue": "J. Am. Soc. Inf. Sci. Technol. 59 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic keyphrase extraction from medical documents", "author": ["K. Sarkar"], "venue": "in: Int. Conf. Pattern Recognit. Mach. Intell.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "A Hybrid Approach to Extract Keyphrases from Medical Documents", "author": ["K. Sarkar"], "venue": "Int. J. Comput. Appl. 63 ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Terminology issues in user access to Web-based medical information", "author": ["A.T. McCray", "R.F. Loane", "A.C. Browne", "A.K. Bangalore"], "venue": "Proc. AMIA Symp. ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "Patient and clinician vocabulary: how different are they", "author": ["Q. Zeng", "S. Kogan", "N. Ash", "R.A. Greenes"], "venue": "Stud. Health Technol. Inform. 84 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2001}, {"title": "Evaluation of Controlled Vocabulary Resources for Development of a Consumer Entry Vocabulary for Diabetes", "author": ["T.B. Patrick", "H.K. Monga", "M.C. Sievert", "J.H. Hall", "D.R. Longo"], "venue": "J. Med. Internet Res. 3 ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "others", "author": ["Q. Zeng", "S. Kogan", "N. Ash", "R.A. Greenes", "A.A. Boxwala"], "venue": "Characteristics of consumer terminology for health information retrieval, Methods Inf. Med. 41 ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2002}, {"title": "Exploring medical expressions used by consumers and the media: an emerging view of consumer health vocabularies", "author": ["T. Tse", "D. Soergel"], "venue": "Proc. AMIA Symp. ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2003}, {"title": "Identifying consumer-friendly display (CFD) names for health concepts", "author": ["Q.T. Zeng", "T. Tse", "J. Crowell", "G. Divita", "L. Roth", "A.C. Browne"], "venue": "in: Proc. AMIA Annu. Symp.,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2005}, {"title": "Consumer health concepts that do not map to the UMLS: where do they fit", "author": ["A. Keselman", "C.A. Smith", "G. Divita", "H. Kim", "A.C. Browne", "G. Leroy", "Q. Zeng- Treitler"], "venue": "J. Am. Med. Inform. Assoc. JAMIA. 15 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating word representation features in biomedical named entity recognition tasks", "author": ["B. Tang", "H. Cao", "X. Wang", "Q. Chen", "H. Xu"], "venue": "BioMed Res. Int. 2014 ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Effects of Semantic Features on Machine Learning-Based Drug Name Recognition Systems: Word Embeddings vs", "author": ["S. Liu", "B. Tang", "Q. Chen", "X. Wang"], "venue": "Manually Constructed Dictionaries, Information. 6 ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Identifying adverse drug event information in clinical notes with distributional semantic representations of context", "author": ["A. Henriksson", "M. Kvist", "H. Dalianis", "M. Duneld"], "venue": "J. Biomed. Inform. 57 ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional RNN for Medical Event Detection in Electronic Health Records", "author": ["A.N. Jagannatha", "H. Yu"], "venue": "in: Proc. NAACL-HLT 2016,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Mining and Ranking Biomedical Synonym Candidates from Wikipedia", "author": ["A.N. Jagannatha", "J. Chen", "H. Yu"], "venue": "in: Proc. Sixth Int. Workshop Health Text Min. Inf. Anal. Louhi,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Clinical abbreviation disambiguation using neural word embeddings", "author": ["Y. Wu", "J. Xu", "Y. Zhang", "H. Xu"], "venue": "Proc. BioNLP 2015 Workshop Biomed. Nat. Lang. Process. ACL-IJCNLP 2015. ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "author": ["Y. Liu", "T. Ge", "K.S. Mathews", "H. Ji", "D.L. McGuinness"], "venue": "Proc. 2015 Workshop Biomed. Nat. Lang. Process. ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "in: Adv. Neural Inf. Process. Syst.,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ArXiv13013781 Cs. ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparative analysis of five protein-protein interaction corpora", "author": ["S. Pyysalo", "A. Airola", "J. Heimonen", "J. Bj\u00f6rne", "F. Ginter", "T. Salakoski"], "venue": "BMC Bioinformatics. 9 Suppl 3 ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intell. Syst. Technol. TIST. 2 ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J. Platt"], "venue": "Adv. Large Margin Classif. 10 ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1999}, {"title": "A note on Platt\u2019s probabilistic outputs for support vector machines", "author": ["H.-T. Lin", "C.-J. Lin", "R.C. Weng"], "venue": "Mach. Learn. 68 ", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K. Sparck Jones"], "venue": "J. Doc. 28 ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1972}, {"title": "Reprint of: The anatomy of a large-scale hypertextual web search engine", "author": ["S. Brin", "L. Page"], "venue": "Comput. Netw. 56 ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Kachites, MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu", "author": ["McCallum", "Andrew"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2002}, {"title": "A text corpora-based estimation of the familiarity of health terminology", "author": ["Q. Zeng", "E. Kim", "J. Crowell", "T. Tse"], "venue": "in: Proc. 6th Int. Conf. Biol. Med. Data Anal., Springer,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2005}, {"title": "Combining multiple evidence from different properties of weighting schemes", "author": ["J.H. Lee"], "venue": "in: Proc. 18th Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 1995}, {"title": "Collaborative ranking: A case study on entity linking", "author": ["Z. Chen", "H. Ji"], "venue": "in: Proc. Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Condorcet fusion for improved retrieval", "author": ["M. Montague", "J.A. Aslam"], "venue": "in: Proc. Elev. Int. Conf. Inf. Knowl. Manag.,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2002}, {"title": "Buettcher, Reciprocal rank fusion outperforms condorcet and individual rank learning methods", "author": ["G.V. Cormack", "S.C.L. Clarke"], "venue": "in: Proc. 32nd Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2009}, {"title": "A unified model for metasearch, pooling, and system evaluation", "author": ["J.A. Aslam", "V. Pavlu", "R. Savell"], "venue": "in: Proc. Twelfth Int. Conf. Inf. Knowl. Manag., ACM,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2003}, {"title": "New learning methods for supervised and unsupervised preference aggregation", "author": ["M. Volkovs", "R.S. Zemel"], "venue": "J. Mach. Learn. Res", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "Ranking model selection and fusion for effective microblog search", "author": ["Z. Wei", "W. Gao", "T. El-Ganainy", "W. Magdy", "K.-F. Wong"], "venue": "in: Proc. First Int. Workshop Soc. Media Retr. Anal.,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2014}, {"title": "Condorcet fusion for blog opinion retrieval", "author": ["S. Wu", "X. Zeng"], "venue": "in: 2012 23rd Int. Workshop Database Expert Syst. Appl.,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2012}, {"title": "iRANK: A rank-learn-combine framework for unsupervised ensemble ranking", "author": ["F. Wei", "W. Li", "S. Liu"], "venue": "J. Am. Soc. Inf. Sci. Technol. 61 ", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning algorithm for keyphrase extraction", "author": ["P.D. Turney"], "venue": "Inf. Retr. ", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2000}, {"title": "Approximate Matching for Evaluating Keyphrase Extraction", "author": ["T. Zesch", "I. Gurevych"], "venue": "in: RANLP,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2009}, {"title": "A robust approach to optimizing multi-source information for enhancing genomics retrieval performance", "author": ["Q. Hu", "J.X. Huang", "J. Miao"], "venue": "BMC Bioinformatics. 12 Suppl 5 ", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic keyphrase extraction via topic decomposition", "author": ["Z. Liu", "W. Huang", "Y. Zheng", "M. Sun"], "venue": "in: Proc. 2010 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "In a nationwide effort to achieve this goal [1\u2013 3], online patient portals have been widely adopted by health systems in the U.", "startOffset": 44, "endOffset": 50}, {"referenceID": 1, "context": "[3,4].", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[3,4].", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "In addition, the OpenNotes initiative [5] and the Blue Button movement [6] allow patients to access their full EHR notes through the portals, with early evidence showing improved medical comprehension, healthcare management, and outcomes [7\u20139].", "startOffset": 238, "endOffset": 243}, {"referenceID": 4, "context": "In addition, the OpenNotes initiative [5] and the Blue Button movement [6] allow patients to access their full EHR notes through the portals, with early evidence showing improved medical comprehension, healthcare management, and outcomes [7\u20139].", "startOffset": 238, "endOffset": 243}, {"referenceID": 5, "context": "In addition, the OpenNotes initiative [5] and the Blue Button movement [6] allow patients to access their full EHR notes through the portals, with early evidence showing improved medical comprehension, healthcare management, and outcomes [7\u20139].", "startOffset": 238, "endOffset": 243}, {"referenceID": 6, "context": "Previous studies showed that EHRs were written at an 8th-12th grade reading level [10\u201313], which is above the average adult patient\u2019s reading level of 7th to 8th grade in the U.", "startOffset": 82, "endOffset": 89}, {"referenceID": 7, "context": "Previous studies showed that EHRs were written at an 8th-12th grade reading level [10\u201313], which is above the average adult patient\u2019s reading level of 7th to 8th grade in the U.", "startOffset": 82, "endOffset": 89}, {"referenceID": 8, "context": "Previous studies showed that EHRs were written at an 8th-12th grade reading level [10\u201313], which is above the average adult patient\u2019s reading level of 7th to 8th grade in the U.", "startOffset": 82, "endOffset": 89}, {"referenceID": 9, "context": "Previous studies showed that EHRs were written at an 8th-12th grade reading level [10\u201313], which is above the average adult patient\u2019s reading level of 7th to 8th grade in the U.", "startOffset": 82, "endOffset": 89}, {"referenceID": 10, "context": "[14\u201318].", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[14\u201318].", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[14\u201318].", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "[14\u201318].", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[14\u201318].", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "Limited health literacy has been identified as one of the major barriers to patient portal use (which includes interpreting information from EHRs) [20\u201322].", "startOffset": 147, "endOffset": 154}, {"referenceID": 16, "context": "Limited health literacy has been identified as one of the major barriers to patient portal use (which includes interpreting information from EHRs) [20\u201322].", "startOffset": 147, "endOffset": 154}, {"referenceID": 17, "context": "Limited health literacy has been identified as one of the major barriers to patient portal use (which includes interpreting information from EHRs) [20\u201322].", "startOffset": 147, "endOffset": 154}, {"referenceID": 18, "context": "First, medical terms have been shown to be obstacles for patients [23\u201328].", "startOffset": 66, "endOffset": 73}, {"referenceID": 19, "context": "First, medical terms have been shown to be obstacles for patients [23\u201328].", "startOffset": 66, "endOffset": 73}, {"referenceID": 20, "context": "First, medical terms have been shown to be obstacles for patients [23\u201328].", "startOffset": 66, "endOffset": 73}, {"referenceID": 21, "context": "First, medical terms have been shown to be obstacles for patients [23\u201328].", "startOffset": 66, "endOffset": 73}, {"referenceID": 22, "context": "First, medical terms have been shown to be obstacles for patients [23\u201328].", "startOffset": 66, "endOffset": 73}, {"referenceID": 23, "context": "Although there are many medical terms in this piece of text (here we only highlighted a subset of terms identified by MetaMap [29] for illustration purpose), physicians identified only five terms most important for patients to know, i.", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 69, "endOffset": 79}, {"referenceID": 24, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 69, "endOffset": 79}, {"referenceID": 25, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 69, "endOffset": 79}, {"referenceID": 8, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 26, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 7, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 234, "endOffset": 241}, {"referenceID": 8, "context": "There has been active research on linking medical terms to lay terms [11,30,31] and also on linking them to consumer-oriented definitions [12] and educational materials [32], and showing improved comprehension with such interventions [11,12].", "startOffset": 234, "endOffset": 241}, {"referenceID": 7, "context": "On the issue of determining which medical terms to simplify, previous work explored frequency-based and/or context-based approaches to check if a term is unfamiliar to the average patients or if it has simpler synonyms [11,30,31].", "startOffset": 219, "endOffset": 229}, {"referenceID": 24, "context": "On the issue of determining which medical terms to simplify, previous work explored frequency-based and/or context-based approaches to check if a term is unfamiliar to the average patients or if it has simpler synonyms [11,30,31].", "startOffset": 219, "endOffset": 229}, {"referenceID": 25, "context": "On the issue of determining which medical terms to simplify, previous work explored frequency-based and/or context-based approaches to check if a term is unfamiliar to the average patients or if it has simpler synonyms [11,30,31].", "startOffset": 219, "endOffset": 229}, {"referenceID": 27, "context": "Previous work in unsupervised KE has explored various techniques, including language modeling, topic-clustering, graph-based ranking and simultaneous learning of keyphrases and key sentences [33].", "startOffset": 191, "endOffset": 195}, {"referenceID": 28, "context": "Among them, graph-based methods such as TextRank [34] and its variations are the state-of-the-arts [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "Among them, graph-based methods such as TextRank [34] and its variations are the state-of-the-arts [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "We adapted SingleRank [35] (an extension of TextRank) to clinical domain and used it as a baseline as well as an input for the ensemble ranking approaches.", "startOffset": 22, "endOffset": 26}, {"referenceID": 30, "context": "KE in the biomedical domain has been limitedly explored in literature articles and in using domain-specific methods and features [36\u201339].", "startOffset": 129, "endOffset": 136}, {"referenceID": 31, "context": "KE in the biomedical domain has been limitedly explored in literature articles and in using domain-specific methods and features [36\u201339].", "startOffset": 129, "endOffset": 136}, {"referenceID": 32, "context": "KE in the biomedical domain has been limitedly explored in literature articles and in using domain-specific methods and features [36\u201339].", "startOffset": 129, "endOffset": 136}, {"referenceID": 33, "context": "KE in the biomedical domain has been limitedly explored in literature articles and in using domain-specific methods and features [36\u201339].", "startOffset": 129, "endOffset": 136}, {"referenceID": 30, "context": "[36] developed KIP to extract keyphrases from medical articles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "In the first step, FIT applies MetaMap [29], a concept detection tool that automatically maps biomedical text to the Unified Medical Language System (UMLS) concepts, to find medical terms as candidate terms.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 35, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 36, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 37, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 38, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 39, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 40, "context": "It is a rich resource that incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus and postings in health-focused online discussion forums [40\u201346].", "startOffset": 189, "endOffset": 196}, {"referenceID": 38, "context": "It contains 152,338 terms, most of which are consumer health terms [44\u201346].", "startOffset": 67, "endOffset": 74}, {"referenceID": 39, "context": "It contains 152,338 terms, most of which are consumer health terms [44\u201346].", "startOffset": 67, "endOffset": 74}, {"referenceID": 40, "context": "It contains 152,338 terms, most of which are consumer health terms [44\u201346].", "startOffset": 67, "endOffset": 74}, {"referenceID": 39, "context": "[45] mapped these consumer health terms to the UMLS concepts by a semi-automatic approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We followed [30] (i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 41, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 240, "endOffset": 247}, {"referenceID": 42, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 240, "endOffset": 247}, {"referenceID": 43, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 278, "endOffset": 285}, {"referenceID": 44, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 278, "endOffset": 285}, {"referenceID": 45, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 315, "endOffset": 319}, {"referenceID": 46, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 363, "endOffset": 370}, {"referenceID": 47, "context": "We used word embeddings as learning features because word embedding has emerged as a powerful technique for word representation and has been successfully used in biomedical and clinical NLP tasks such as biomedical named entity recognition [47,48], adverse drug event detection [49,50], ranking biomedical synonyms [51], and disambiguating clinical abbreviations [52,53].", "startOffset": 363, "endOffset": 370}, {"referenceID": 48, "context": "We trained a neural language model to learn word embeddings by using the Word2Vec software [54,55].", "startOffset": 91, "endOffset": 98}, {"referenceID": 49, "context": "We trained a neural language model to learn word embeddings by using the Word2Vec software [54,55].", "startOffset": 91, "endOffset": 98}, {"referenceID": 50, "context": "[56].", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "We used the RBF-kernel SVM algorithm implemented by LibSVM [57] to learn feature weights and used LibSVM\u2019s probabilistic outputs [58,59] to rank candidate terms in each EHR note.", "startOffset": 59, "endOffset": 63}, {"referenceID": 52, "context": "We used the RBF-kernel SVM algorithm implemented by LibSVM [57] to learn feature weights and used LibSVM\u2019s probabilistic outputs [58,59] to rank candidate terms in each EHR note.", "startOffset": 129, "endOffset": 136}, {"referenceID": 53, "context": "We used the RBF-kernel SVM algorithm implemented by LibSVM [57] to learn feature weights and used LibSVM\u2019s probabilistic outputs [58,59] to rank candidate terms in each EHR note.", "startOffset": 129, "endOffset": 136}, {"referenceID": 54, "context": "TF*IDF [60] is widely used to measure the salience of a term to a document d in a corpus D, as defined in (1).", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "We used SingleRank [35] to represent this view.", "startOffset": 19, "endOffset": 23}, {"referenceID": 55, "context": "85 [61], \u03c9ji is the edge weight which equals the number of co-occurrences of vi and vj within a context window of 10.", "startOffset": 3, "endOffset": 7}, {"referenceID": 56, "context": "We used the Latent Dirichlet Allocation algorithm implemented by MALLET [62] for topic modeling.", "startOffset": 72, "endOffset": 76}, {"referenceID": 57, "context": "CHV familiarity scores estimate the likelihood that a medical term can be understood by an average reader [63] and take values between 0 and 1 (with 1 being most familiar and 0 being least familiar).", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "The CHV provides different types of familiarity scores [30].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "Following [30], we used the combined score and a score threshold 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 55, "context": "We used unsupervised ensemble learning and built FIT on an adapted random walk algorithm derived from PageRank [61].", "startOffset": 111, "endOffset": 115}, {"referenceID": 58, "context": "Rank scores, when available, are also useful for ensemble ranking [64,65].", "startOffset": 66, "endOffset": 73}, {"referenceID": 59, "context": "Rank scores, when available, are also useful for ensemble ranking [64,65].", "startOffset": 66, "endOffset": 73}, {"referenceID": 7, "context": "We asked physicians to do the annotation because this task requires a full comprehension of EHR notes which is beyond the capacity of the average patients [11\u201313,30].", "startOffset": 155, "endOffset": 165}, {"referenceID": 8, "context": "We asked physicians to do the annotation because this task requires a full comprehension of EHR notes which is beyond the capacity of the average patients [11\u201313,30].", "startOffset": 155, "endOffset": 165}, {"referenceID": 9, "context": "We asked physicians to do the annotation because this task requires a full comprehension of EHR notes which is beyond the capacity of the average patients [11\u201313,30].", "startOffset": 155, "endOffset": 165}, {"referenceID": 24, "context": "We asked physicians to do the annotation because this task requires a full comprehension of EHR notes which is beyond the capacity of the average patients [11\u201313,30].", "startOffset": 155, "endOffset": 165}, {"referenceID": 58, "context": "In addition, we implemented three benchmark unsupervised ensemble ranking methods, CombSum [64], Condorcet-fuse [66], and Reciprocal rank fusion [67], as strong baselines.", "startOffset": 91, "endOffset": 95}, {"referenceID": 60, "context": "In addition, we implemented three benchmark unsupervised ensemble ranking methods, CombSum [64], Condorcet-fuse [66], and Reciprocal rank fusion [67], as strong baselines.", "startOffset": 112, "endOffset": 116}, {"referenceID": 61, "context": "In addition, we implemented three benchmark unsupervised ensemble ranking methods, CombSum [64], Condorcet-fuse [66], and Reciprocal rank fusion [67], as strong baselines.", "startOffset": 145, "endOffset": 149}, {"referenceID": 58, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 98, "endOffset": 108}, {"referenceID": 60, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 98, "endOffset": 108}, {"referenceID": 61, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 98, "endOffset": 108}, {"referenceID": 62, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 98, "endOffset": 108}, {"referenceID": 63, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 98, "endOffset": 108}, {"referenceID": 64, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 129, "endOffset": 133}, {"referenceID": 65, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 154, "endOffset": 158}, {"referenceID": 66, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 174, "endOffset": 178}, {"referenceID": 59, "context": "The three methods have been widely in information retrieval and NLP, including document retrieval [64,66\u201369], web blog retrieval [70], opinion extraction [71], summarization [72], and entity linking [65].", "startOffset": 199, "endOffset": 203}, {"referenceID": 58, "context": "CombSum[64] is a rank-score-based ensemble method, which calculates the rank score of a candidate term t by summing t\u2019s rank scores received from single rankers, as calculated by (9),", "startOffset": 7, "endOffset": 11}, {"referenceID": 60, "context": "Condorcet Fuse[66] sorts candidate terms by pairwise rank relation R(tj \u2192 ti) as defined in (5).", "startOffset": 14, "endOffset": 18}, {"referenceID": 60, "context": "We implemented the Condorcet Fuse ranker using the quick sort algorithm by following [66].", "startOffset": 85, "endOffset": 89}, {"referenceID": 61, "context": "Reciprocal Rank Fusion[67] calculates the rank score of a candidate term t by using t\u2019s ranks assigned by single rankers, as defined in (10),", "startOffset": 22, "endOffset": 26}, {"referenceID": 61, "context": "We set k to 60 by following [67].", "startOffset": 28, "endOffset": 32}, {"referenceID": 67, "context": "In evaluation, we use relaxed string match to determine true positives as exact match is known to underestimate performance as perceived by human judges [73].", "startOffset": 153, "endOffset": 157}, {"referenceID": 68, "context": "We allow \u201csubsume\u201d but not \u201cpart-of\u201d match in relaxed string match, as previous work found that the former aligned well with human judges but the latter did not [74].", "startOffset": 161, "endOffset": 165}, {"referenceID": 59, "context": "to each other [65,68].", "startOffset": 14, "endOffset": 21}, {"referenceID": 62, "context": "to each other [65,68].", "startOffset": 14, "endOffset": 21}, {"referenceID": 60, "context": "When this assumption is violated, an ensemble ranker is not guaranteed to outperform the (best) single rankers [66,68,70,75].", "startOffset": 111, "endOffset": 124}, {"referenceID": 62, "context": "When this assumption is violated, an ensemble ranker is not guaranteed to outperform the (best) single rankers [66,68,70,75].", "startOffset": 111, "endOffset": 124}, {"referenceID": 64, "context": "When this assumption is violated, an ensemble ranker is not guaranteed to outperform the (best) single rankers [66,68,70,75].", "startOffset": 111, "endOffset": 124}, {"referenceID": 69, "context": "When this assumption is violated, an ensemble ranker is not guaranteed to outperform the (best) single rankers [66,68,70,75].", "startOffset": 111, "endOffset": 124}, {"referenceID": 70, "context": "Topic modeling has been used to extend SingleRank to improve the state-of-the-arts in unsupervised KE [76,77].", "startOffset": 102, "endOffset": 109}, {"referenceID": 27, "context": "SingleRank is among the state-of-the-arts and TF*IDF is frequently used as a strong baseline in unsupervised KE from scientific literature and news [33].", "startOffset": 148, "endOffset": 152}, {"referenceID": 60, "context": "Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75].", "startOffset": 149, "endOffset": 168}, {"referenceID": 62, "context": "Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75].", "startOffset": 149, "endOffset": 168}, {"referenceID": 61, "context": "Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75].", "startOffset": 149, "endOffset": 168}, {"referenceID": 63, "context": "Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75].", "startOffset": 149, "endOffset": 168}, {"referenceID": 64, "context": "Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75].", "startOffset": 149, "endOffset": 168}, {"referenceID": 69, "context": "Previous work shows that the performances of the three methods vary for different tasks and datasets and, therefore, there is no a guaranteed winner [66,68,67,69,70,75].", "startOffset": 149, "endOffset": 168}], "year": 2017, "abstractText": "Background: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, EHR notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them. Targeted education can then be developed to improve patient EHR comprehension and the quality of care. Objective: The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients. Methods: We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views (rankers) for term importance: patient use of medical concepts, document-level term salience, word-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms\u2019 unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and used the four single-view rankers as baselines. In addition, we implemented three benchmark unsupervised ensemble ranking methods as strong baselines. Results: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. Both performance scores significantly exceeded the corresponding scores from the four single rankers (P<.001). FIT also outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter. Conclusions: FIT can automatically identify EHR terms important to patients. It may help develop future interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications.", "creator": "Microsoft\u00ae Word 2016"}}}