{"id": "1703.00788", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Robust Adaptive Stochastic Gradient Method for Deep Learning", "abstract": "stochastic function gradient adjustment algorithms are the longest main domain focus clusters of large - tree scale optimization behavior problems and led ones to obtain important successes significantly in the very recent advancement of the deep learning algorithms. the convergence of sgd efficiently depends intensely on the careful choice of learning rate constraints and limits the amount of the noise in conducting stochastic estimates of the gradients. in this implementation paper, here we propose an eponymous adaptive learning rate learning algorithm, which utilizes some stochastic curvature information of the loss function specifications for those automatically sampled tuning collectively the learning recovery rates. the information about analyzing the spatial element - wise differential curvature of the loss function estimation is estimated roughly from scanning the local statistics of only the sampled stochastic lower first order gradients. we further propose investigating a new variance dimension reduction technique available to speed backwards up the convergence. in our experiments experimenting with dynamic deep neural curve networks, we gradually obtained better performance compared respectively to initially the popular stochastic gradient algorithms.", "histories": [["v1", "Thu, 2 Mar 2017 14:03:48 GMT  (655kb,D)", "http://arxiv.org/abs/1703.00788v1", "IJCNN 2017 Accepted Paper, An extension of our paper, \"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\""]], "COMMENTS": "IJCNN 2017 Accepted Paper, An extension of our paper, \"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\"", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["caglar gulcehre", "jose sotelo", "marcin moczulski", "yoshua bengio"], "accepted": false, "id": "1703.00788"}, "pdf": {"name": "1703.00788.pdf", "metadata": {"source": "CRF", "title": "A Robust Adaptive Stochastic Gradient Method for Deep Learning", "authors": ["Caglar Gulcehre", "Jose Sotelo", "Marcin Moczulski", "Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nWe develop an automatic stochastic gradient algorithm which reduces the burden of extensive hyper-parameter search for the optimizer. Our proposed algorithm exploits a lower variance estimator of curvature of the cost function and uses it to obtain an automatically tuned adaptive learning rate for each parameter.\nIn deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4]. A fundamental advantage of using such approximation is that inverting such approximation can be a trivial and cheap operation. However generally, for neural networks, the inverse of the diagonal Hessian is usually a bad approximation of the diagonal of the inverse of Hessian. For example, obtaining a diagonal approximation of Hessian are the Gauss-Newton matrix [5] or by finite differences [6]. Such estimations may however be very sensitive to the noise coming from the Monte-Carlo estimates of the gradients. [3] suggested a reliable way to estimate the local curvature in the stochastic setting by keeping track of the variance and average of the gradients.\nWe propose a different approach: instead of using a diagonal estimate of Hessian, to estimate curvature along the direction of the gradient and we apply a new variance reduction technique to compute it reliably. By using root mean square statistics, the variance of gradients are reduced adaptively\n* denotes equal contribution. 1This paper is an extension/update of our previous paper [1].\nwith a simple transformation. We keep track of the estimation of curvature using a technique similar to that proposed by [3], which uses the variability of the expected loss. Standard adaptive learning rate algorithms only scale the gradients, but regular Newton-like second order methods, can perform more complicate transformations, e.g. rotating the gradient vector. Newton and quasi-newton methods can also be invariant to affine transformations in the parameter space. AdaSecant algorithm is basically a stochastic rank-1 quasi-Newton method. But in comparison with other adaptive learning algorithms, instead of just scaling the gradient of each parameter, AdaSecant can also perform an affine transformation on them."}, {"heading": "II. DIRECTIONAL SECANT APPROXIMATION", "text": "Directional Newton is a method proposed for solving equations with multiple variables[7]. The advantage of directional Newton method proposed in[7], compared to Newton\u2019s method is that, it does not require a matrix inversion and still maintains a quadratic rate of convergence.\nIn this paper, we develop a second-order directional Newton method for nonlinear optimization. Step-size tk of update \u2206k for step k can be written as if it was a diagonal matrix:\n\u2206k = \u2212tk \u2207\u03b8f(\u03b8k), (1) = \u2212diag(tk)\u2207\u03b8f(\u03b8k), (2) = \u2212diag(dk)(diag(Hdk))\u22121\u2207\u03b8f(\u03b8k). (3)\nwhere \u03b8k is the parameter vector at update k, f is the objective function and dk is a unit vector of direction that the optimization algorithm should follow. Denoting by hi = \u2207\u03b8 \u2202f(\u03b8 k) \u2202\u03b8i\nthe ith row of the Hessian matrix H and by \u2207\u03b8if(\u03b8 k) the ith element of the gradient vector at update k, a reformulation of Equation 1 for each diagonal element of the step-size diag(tk) is:\n\u2206ki = \u2212tki\u2207\u03b8i f(\u03b8 k), (4)\n= \u2212dki \u2207\u03b8i f(\u03b8 k)\nhki d k\n. (5)\nso effectively\ntki = dki\nhki d k . (6)\nar X\niv :1\n70 3.\n00 78\n8v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n7\nWe can approximate the per-parameter learning rate tki following [8] using finite differences:\ntki = dki\nhki d k , (7)\n= lim |\u2206ki |\u21920 \u2206ki \u2207\u03b8i f(\u03b8 k + \u2206k)\u2212\u2207\u03b8i f(\u03b8 k) , for every i. (8)\nLet us note that alternatively one might use the R-op to compute the Hessian-vector product for the denominator in Equation 7 [9].\nTo choose a good direction dk in the stochastic setting, we use block-normalized gradient vector that the parameters of each layer is considered as a block and for each weight matrix Wik and bias vector b i k for \u03b8 = { Wik,b i k } i=1\u00b7\u00b7\u00b7k at\neach layer i and update k, dk = [ dk W0k dk b0k \u00b7 \u00b7 \u00b7dk blk ] for a neural network with l layers. The update step is defined as \u2206ki = t k i d k i . The per-parameter learning rate tki can be estimated with the finite difference approximation,\ntki \u2248 \u2206ki\n\u2207\u03b8i f(\u03b8 k + \u2206k)\u2212\u2207\u03b8i f(\u03b8\nk) , (9)\nsince, in the vicinity of the quadratic local minima,\n\u2207\u03b8f(\u03b8k + \u2206k)\u2212\u2207\u03b8f(\u03b8k) \u2248 Hk\u2206k, (10)\nWe can therefore recover tk as\ntk = diag(\u2206k)(diag(Hk\u2206k))\u22121. (11)\nThe directional secant method basically scales the gradient of each parameter with the curvature along the direction of the gradient vector and it is numerically stable."}, {"heading": "III. RELATIONSHIP TO THE DIAGONAL APPROXIMATION", "text": ""}, {"heading": "TO THE HESSIAN", "text": "Our secant approximation of the gradients are also very closely tied to diagonal approximation of the Hessian matrix. Considering that ith diagonal entry of the Hessian matrix can be denoted as, Hii =\n\u22022f(\u03b8) \u2202\u03b82i\n. By using the finite differences, it is possible to approximate this with as in Equation 12,\nHii = lim |\u2206|\u21920 \u2207\u03b8i f(\u03b8 + \u2206)\u2212\u2207\u03b8i f(\u03b8) \u2206i , (12)\nAssuming that the diagonal of the Hessian is denoted with A matrix, we can see the equivalence:\nA \u2248 diag(\u2207\u03b8f(\u03b8 + \u2206)\u2212\u2207\u03b8f(\u03b8)) diag(\u2206)\u22121. (13)\nThe Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches.\nIV. VARIANCE REDUCTION FOR ROBUST STOCHASTIC GRADIENT DESCENT\nVariance reduction techniques for stochastic gradient estimators have been well-studied in the machine learning literature. Both [10] and [11] proposed new ways of dealing with this problem. In this paper, we proposed a new variance reduction technique for stochastic gradient descent that relies only on basic statistics related to the gradient. Let gi refer to the ith element of the gradient vector g with respect to the parameters \u03b8 and E[\u00b7] be an expectation taken over minibatches and different trajectories of parameters.\nWe propose to apply the following transformation to reduce the variance of the stochastic gradients:\ng\u0303i = gi + \u03b3iE[gi]\n1 + \u03b3i , (14)\nwhere \u03b3i is strictly a positive real number. Let us note that:\nE[g\u0303i] = E[gi] and Var(g\u0303i) = 1\n(1 + \u03b3i)2 Var(gi). (15)\nThe variance is reduced by a factor of (1 + \u03b3i)2 compared to Var(gi).\nIn practice we do not have access to E[gi], therefore a biased estimator gi based on past values of gi will be used instead. We can rewrite the g\u0303i as:\ng\u0303i = 1\n1 + \u03b3i gi + (1\u2212\n1\n1 + \u03b3i )E[gi], (16)\nAfter substitution \u03b2i = 11+\u03b3i , we will have:\ng\u0303i = \u03b2igi + (1\u2212 \u03b2i)E[gi]. (17)\nBy adapting \u03b3i or \u03b2i, it is possible to control the influence of high variance, unbiased gi and low variance, biased gi on g\u0303i. Denoting by g\u2032 the stochastic gradient obtained on the next minibatch, the \u03b3i that well balances those two influences is the one that keeps the g\u0303i as close as possible to the true gradient E[g\u2032i] with g \u2032 i being the only sample of E[g \u2032 i] available. We try to find a regularized \u03b2i, in order to obtain a smoother estimate of it and this yields us to more more stable estimates of \u03b2i. \u03bb is the regularization coefficient for \u03b2.\narg min \u03b2i\nE[||g\u0303i \u2212 g\u2032i||22] + \u03bb(\u03b2i)2. (18)\nIt can be shown that this a convex problem in \u03b2i with a closedform solution (details in appendix) and we can obtain the \u03b3i from it:\n\u03b3i = E[(gi \u2212 g\u2032i)(gi \u2212 E[gi])]\nE[(gi \u2212 E[gi])(gi\u2032 \u2212 E[gi]))] + \u03bb , (19)\nAs a result, to estimate \u03b3 for each dimension, we keep track of a estimation of E[(gi\u2212g \u2032 i)(gi\u2212E[gi])]\nE[(gi\u2212E[gi])(g\u2032i\u2212E[gi]))]+\u03bb during training.\nThe necessary and sufficient condition here, for the variance reduction is to keep \u03b3 positive, to achieve a positive estimate of \u03b3 we used the root mean square statistics for the expectations."}, {"heading": "V. BLOCKWISE GRADIENT NORMALIZATION", "text": "It is very well-known that the repeated application of the non-linearities can cause the gradients to vanish [12], [13]. Thus, in order to tackle this problem, we normalize the gradients coming into each block/layer to have norm 1. Assuming the normalized gradient can be denoted with g\u0303, it can be computed as, g\u0303 = g||E[g]||2 . We estimate, E[g] via moving averages.\nBlockwise gradient normalization of the gradient adds noise to the gradients, but in practice we did not observe any negative impact of it. We conjecture that this is due to the angle between the stochastic gradient and the block-normalized gradient still being less than 90 degrees."}, {"heading": "VI. ADAPTIVE STEP-SIZE IN STOCHASTIC CASE", "text": "In the stochastic gradient case, the step-size of the directional secant can be computed by using an expectation over the minibatches:\nEk[ti] = Ek[ \u2206ki\n\u2207\u03b8i f(\u03b8 k + \u2206k)\u2212\u2207\u03b8i f(\u03b8\nk) ]. (20)\nThe Ek[\u00b7] that is used to compute the secant update, is taken over the minibatches at the past values of the parameters.\nComputing the expectation in Equation20 was numerically unstable in stochastic setting. We decided to use a more stable second order Taylor approximation of Equation 20 around ( \u221a Ek[(\u03b1ki ) 2], \u221a Ek[(\u2206ki ) 2]), with \u03b1ki = \u2207\u03b8i f(\u03b8\nk + \u2206k) \u2212 \u2207\u03b8i f(\u03b8 k). Assuming \u221a Ek[(\u03b1ki ) 2] \u2248 Ek[\u03b1ki ] and\u221a\nEk[(\u2206ki ) 2] \u2248 Ek[\u2206ki ] we obtain always non-negative ap-\nproximation of Ek[ti]: Ek[ti] \u2248 \u221a Ek[(\u2206ki ) 2]\u221a\nEk[(\u03b1ki ) 2] \u2212 Cov(\u03b1\nk i ,\u2206 k i )\nEk[(\u03b1ki ) 2]\n. (21)\nIn our experiments, we used a simpler approximation, which in practice worked as well as formulations in Equation21:\nEk[ti] \u2248 \u221a Ek[(\u2206ki ) 2]\u221a\nEk[(\u03b1ki ) 2] \u2212 Ek[\u03b1\nk i \u2206 k i ]\nEk[(\u03b1ki ) 2] . (22)"}, {"heading": "VII. ALGORITHMIC DETAILS", "text": ""}, {"heading": "A. Approximate Variability", "text": "To compute the moving averages as also adopted by [3], we used an algorithm to dynamically decide the time constant based on the step size being taken. As a result algorithm that we used will give bigger weights to the updates that have large step-size and smaller weights to the updates that have smaller step-size.\nBy assuming that \u2206\u0304i[k] \u2248 E[\u2206i]k, the moving average update rule for \u2206\u0304i[k] can be written as,\n\u2206\u03042i [k] = (1 \u2212 \u03c4\u22121i [k])\u2206\u0304 2 i [k \u2212 1] + \u03c4\u22121i [k](t k i g\u0303 k i ), (23)\nand,\n\u2206\u0304i[k] = \u221a \u2206\u03042i [k]. (24)\nThis rule for each update assigns a different weight to each element of the gradient vector . At each iteration a scalar\nmultiplication with \u03c4\u22121i is performed and \u03c4i is adapted using the following equation:\n\u03c4i[k] = (1 \u2212 E[\u2206i]\n2 k\u22121\nE[(\u2206i)2]k\u22121 )\u03c4i[k \u2212 1] + 1 . (25)"}, {"heading": "B. Outlier Gradient Detection", "text": "Our algorithm is very similar to [6], but instead of incrementing \u03c4i[t+1] when an outlier is detected, the time-constant is reset to 2.2. Note that when \u03c4i[t + 1] \u2248 2, this assigns approximately the same amount of weight to the current and the average of previous observations. This mechanism made learning more stable, because without it outlier gradients saturate \u03c4i to a large value.\nC. Variance Reduction\nThe correction parameters \u03b3i (Equation19) allows for a finegrained variance reduction for each parameter independently. The noise in the stochastic gradient methods can have advantages both in terms of generalization and optimization. It introduces an exploration and exploitation trade-off, which can be controlled by upper bounding the values of \u03b3i with a value \u03c1i, so that thresholded \u03b3\u2032i = min(\u03c1i, \u03b3i).\nWe block-wise normalized the gradients of each weight matrix and bias vectors in g to compute the g\u0303 as described in Section II. That makes AdaSecant scale-invariant, thus more robust to the scale of the inputs and the number of the layers of the network. We observed empirically that it was easier to train very deep neural networks with block normalized gradient descent. In our experiments, we fixed \u03bb to 1e\u2212 5.\nVIII. IMPROVING CONVERGENCE\nClassical convergence results for SGD are based on the conditions: \u2211\ni (\u03b7(i))2 <\u221e and \u2211 i \u03b7(i) =\u221e (26)\nsuch that the learning rate \u03b7(i) should decrease [14]. Due to the noise in the estimation of adaptive step-sizes for AdaSecant, the convergence would not be guaranteed. To ensure it, we developed a new variant of Adagrad [15] with thresholding, such that each scaling factor is lower bounded by 1. Assuming aki is the accumulated norm of all past gradients for i th parameter at update k, it is thresholded from below ensuring that the algorithm will converge:\naki = \u221a\u221a\u221a\u221a k\u2211 j=0 (gji ) 2, (27)\nand \u03c1ki = maximum(1, a k i ), (28)\ngiving\n\u2206ki = 1\n\u03c1i \u03b7ki g\u0303 k i . (29)\nIn the initial stages of training, accumulated norm of the perparameter gradients can be less than 1. If the accumulated\nper-parameter norm of a gradient is less than 1, Adagrad will augment the learning-rate determined by AdaSecant for that update, i.e. \u03b7 k i\n\u03c1ki > \u03b7ki where \u03b7 k i = Ek[t k i ] is the per-parameter\nlearning rate determined by AdaSecant. This behavior tends to create unstabilities during the training with AdaSecant. Our modification of the Adagrad algorithm is to ensure that, it will reduce the learning rate determined by the AdaSecant algorithm at each update, i.e. \u03b7 k i\n\u03c1ki \u2264 \u03b7ki and the learning rate\nwill be bounded. At the beginning of the training, parameters of a neural network can get 0-valued gradients, e.g. in the existence of dropout and ReLU units. However this phenomena can cause the per-parameter learning rate scaled by Adagrad to be unbounded.\nIn Algorithm 1, we provide a simple pseudo-code of the AdaSecant algorithm.\nAlgorithm 1: AdaSecant: minibatch-AdaSecant for adaptive learning rates with variance reduction\nrepeat draw n samples, compute the gradients g(j) where g(j) \u2208 Rn for each minibatch j, g(j) is computed as, 1n \u2211n k=1\u2207 (k) \u03b8 f(\u03b8)\nestimate E[g] via moving averages. block-wise normalize gradients of each weight matrix\nand bias vector for parameter i \u2208 {1, . . . , n} do\ncompute the correction term by using, \u03b3ki = E[(gi\u2212g\u2032i)(gi\u2212E[gi])]k E[(gi\u2212E[gi])(g\u2032i\u2212E[gi]))]k compute corrected gradients g\u0303i = gi+\u03b3iE[gi]\n1+\u03b3i\nif |g(j)i \u2212 E[gi]| > 2 \u221a E[(gi)2]\u2212 (E[gi])2 or \u2223\u2223\u2223\u03b1(j)i \u2212 E[\u03b1i]\u2223\u2223\u2223 >\n2 \u221a\nE[(\u03b1i)2]\u2212 (E[\u03b1i])2 then reset the memory size for outliers \u03c4i \u2190 2.2\nend\nupdate moving averages according to Equation 23\nestimate learning rate\n\u03b7 (j) i \u2190\n\u221a Ek[(\u2206 (k) i )\n2]\u221a Ek[(\u03b1ki ) 2] \u2212 Ek[\u03b1 k i \u2206 k i ] Ek[(\u03b1ki ) 2]\nupdate memory size as in Equation 25\nupdate parameter \u03b8ji \u2190 \u03b8 j\u22121 i \u2212 \u03b7 (j) i \u00b7 g\u0303 (j) i\nend until stopping criterion is met;"}, {"heading": "IX. EXPERIMENTS", "text": "We have run experiments on character-level PTB with GRU units, on MNIST with Maxout Networks [16] and on handwriting synthesis using the IAM-OnDB dataset [17]. We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning\nrate). AdaSecant performs as well or better as carefully tuned algorithms for all these different tasks."}, {"heading": "A. Ablation Study", "text": "In this section, we decompose the different parts of the algorithm to measure the effect they have in the performance. For this comparison, we trained a model to learn handwriting synthesis on IAM-OnDB dataset. Our model follows closely the architecture introduced in [18] with two modifications. First, we use one recurrent layer of size 400 instead of three. Second, we use GRU [21] units instead of LSTM [22] units. Also, we use a different symbol for each of the 87 different characters in the dataset. The code for this experiment is available online.2\nWe tested different configurations that included taking away the use of Variance Reduction (VR), Adagrad (AG), Block Normalization (BN), and Outlier Detection (OD). Also, we compared against ADAM [20] with different learning rates in Figure 1. There, we observe that adasecant performs as well as Adam with a carefully tuned learning rate.\nIn Figure 2, we disable each of the four components of the algorithm. We find that BN provides a small, but constant advantage in performance. OD is also important for the algorithm. Disabling OD makes training more noisy and unstable and gives worse results. Disabling VR also makes training unstable. AG has the least effect in the performance of the algorithm. Furthermore, disabling more than one component makes training even more unstable in the majority of scenarios. A summary of the results is available in Table I. In all cases, we use early stopping on the validation log-loss. Furthermore, we present the train log-loss corresponding to\n2https://github.com/sotelo/scribe\nthe best validation loss as well. Let us note that the log-loss is computed per data point."}, {"heading": "B. PTB Character-level LM", "text": "We have run experiments with GRU-RNN[21] on PTB dataset for character-level language modeling over the subset defined in [23]. On this task, we use 400 GRU units with minibatch size of 20. We train the model over the sequences of length 150. For AdaSecant, we have not run any hyperparmeter search, but for Adam we run a hyperparameter search for the learning rate and gradient clipping. The learning rates are sampled from log-uniform distribution between 1e\u22121 and 6e\u22125. Gradient clipping threshold is sampled uniformly between 1.2 to 20. We have evaluated 20 different pairs of randomlysampled learning rates and gradient clipping thresholds. The rest of the hyper-parameters are fixed to their default values. We use the model with the best validation error for Adam. For AdaSecant algorithm, we fix all the hyperparameters to their\ndefault values. The learning curves for the both algorithms are shown in Figure 3."}, {"heading": "C. MNIST with Maxout Networks", "text": "The results are summarized in Figure 4 and we show that AdaSecant converges as fast or faster than other techniques, including the use of hand-tuned global learning rate and momentum for SGD, RMSprop, and Adagrad. In our experiments with AdaSecant algorithm, adaptive momentum term \u03b3ki was clipped at 1.8. In 2-layer Maxout network experiments for SGD-momentum experiments, we used the best hyper-parameters reported by [16], for RMSProp and Adagrad, we crossvalidated learning rate for 15 different learning rates sampled uniformly from the log-space. We crossvalidated 30 different pairs of momentum and learning rate for SGD+momentum, for RMSProp and Adagrad, we\ncrossvalidated 15 different learning rates sampled them from log-space uniformly for deep maxout experiments."}, {"heading": "X. CONCLUSION", "text": "We described a new stochastic gradient algorithm with adaptive learning rates that is fairly insensitive to the tuning of the hyper-parameters and doesn\u2019t require tuning of learning rates. Furthermore, the variance reduction technique we proposed improves the convergence when the stochastic gradients have high variance. Our algorithm performs as well or better than other popular, carefully-tuned stochastic gradient algorithms. We also present a comprehensive ablation study where we show the effects and importance of each of the elements of our algorithm. As future work, we should try to find theoretical convergence properties of the algorithm to understand it better analytically."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Que\u0301bec. This work has been partially supported by NSERC, CIFAR, and Canada Research Chairs, Project TIN2013-41751, grant 2014-SGR-221. Jose Sotelo also thanks the Consejo Nacional de Ciencia y Tecnolog\u0131\u0301a (CONACyT) as well as the Secretar\u0131\u0301a de Educacio\u0301n Pu\u0301blica (SEP) for their support. We would like to thank Tom Schaul for the valuable discussions. We also thank Kyunghyun Cho and Orhan Firat for proof-reading and giving feedbacks on the paper."}, {"heading": "A. Derivation of Equation 18", "text": "\u2202E[(\u03b2igi + (1\u2212 \u03b2i)E[gi]\u2212 g\u2032i)2] \u2202\u03b2i + \u03bb\u03b22i = 0\nE[(\u03b2igi + (1\u2212 \u03b2i)E[gi]\u2212 g\u2032i) \u2202(\u03b2igi + (1\u2212 \u03b2i)E[gi]\u2212 g\u2032i)\n\u2202\u03b2i ] + \u03bb\u03b2i = 0\nE[(\u03b2igi + (1\u2212 \u03b2i)E[gi]\u2212 g\u2032i)(gi \u2212 E[gi])] + \u03bb\u03b2i = 0\nE[(\u03b2igi(gi \u2212 E[gi]) + (1\u2212 \u03b2i)E[gi](gi \u2212 E[gi]) \u2212 g\u2032i(gi \u2212 E[gi])] + \u03bb\u03b2i = 0\n\u03b2i = E[(gi \u2212 E[gi])(g\u2032i \u2212 E[gi])]\nE[(gi \u2212 E[gi])(gi \u2212 E[gi])] + \u03bb\n= E[(gi \u2212 E[gi])(g\u2032i \u2212 E[gi])]\nVar(gi) + \u03bb"}, {"heading": "B. Further Experimental Details", "text": "In Figure 5, we analyzed the effect of using different minibatch sizes for AdaSecant and compared its convergence with Adadelta in wall-clock time. For minibatch size 100 AdaSecant was able to reach the almost same training negative log-likelihood as Adadelta after the same amount of time, but its convergence took much longer. With minibatches of size 500 AdaSecant was able to converge faster in wallclock time to a better local minima."}, {"heading": "C. More decomposition experiments", "text": "We have run experiments with the different combinations of the components of the algorithm. We show those results on handwriting synthesis with IAM-OnDB dataset. The results can be observed from Figure 6, Figure 7, Figure 8, and Figure 9 deactivating the components leads to a more unstable training curve in the majority of scenarios."}], "references": [{"title": "Improving the convergence of backpropagation learning with second order methods", "author": ["S. Becker", "Y. Le Cun"], "venue": "Proceedings of the 1988 connectionist models summer school. San Matteo, CA: Morgan Kaufmann, 1988, pp. 29\u201337.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1206.1106, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic learning rate maximization by on-line estimation of the hessians eigenvectors", "author": ["Y. LeCun", "P.Y. Simard", "B. Pearlmutter"], "venue": "Advances in neural information processing systems, vol. 5, pp. 156\u2013163, 1993.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade. Springer, 2012, pp. 9\u201348.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients", "author": ["T. Schaul", "Y. LeCun"], "venue": "arXiv preprint arXiv:1301.3764, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Directional newton methods in n variables", "author": ["Y. Levin", "A. Ben-Israel"], "venue": "Mathematics of Computation, vol. 71, no. 237, pp. 251\u2013262, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Directional secant method for nonlinear equations", "author": ["H.-B. An", "Z.-Z. Bai"], "venue": "Journal of computational and applied mathematics, vol. 175, no. 2, pp. 291\u2013304, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast curvature matrix-vector products for second-  order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural computation, vol. 14, no. 7, pp. 1723\u2013 1738, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["C. Wang", "X. Chen", "A. Smola", "E. Xing"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 181\u2013189.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 315\u2013323.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pp. 400\u2013407, 1951.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1951}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Iam-ondb - an on-line english sentence database acquired from handwritten text on a whiteboard.\u201d in ICDAR", "author": ["M. Liwicki", "H. Bunke"], "venue": "IEEE Computer Society,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1162/neco.1997.9.8.1735", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "H. Le", "S. Kombrink", "J. Cernocky"], "venue": "preprint, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "arXiv preprint arXiv:1308.4214, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde- Farley", "J. Chorowski", "Y. Bengio"], "venue": "ArXiv e-prints, jun 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4].", "startOffset": 316, "endOffset": 319}, {"referenceID": 1, "context": "In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4].", "startOffset": 321, "endOffset": 324}, {"referenceID": 2, "context": "In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4].", "startOffset": 326, "endOffset": 329}, {"referenceID": 3, "context": "For example, obtaining a diagonal approximation of Hessian are the Gauss-Newton matrix [5] or by finite differences [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "For example, obtaining a diagonal approximation of Hessian are the Gauss-Newton matrix [5] or by finite differences [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "[3] suggested a reliable way to estimate the local curvature in the stochastic setting by keeping track of the variance and average of the gradients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "We keep track of the estimation of curvature using a technique similar to that proposed by [3], which uses the variability of the expected loss.", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "Directional Newton is a method proposed for solving equations with multiple variables[7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "The advantage of directional Newton method proposed in[7], compared to Newton\u2019s method is that, it does not require a matrix inversion and still maintains a quadratic rate of convergence.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "We can approximate the per-parameter learning rate ti following [8] using finite differences:", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "Let us note that alternatively one might use the R-op to compute the Hessian-vector product for the denominator in Equation 7 [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Both [10] and [11] proposed new ways of dealing with this problem.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Both [10] and [11] proposed new ways of dealing with this problem.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "BLOCKWISE GRADIENT NORMALIZATION It is very well-known that the repeated application of the non-linearities can cause the gradients to vanish [12], [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "BLOCKWISE GRADIENT NORMALIZATION It is very well-known that the repeated application of the non-linearities can cause the gradients to vanish [12], [13].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "Approximate Variability To compute the moving averages as also adopted by [3], we used an algorithm to dynamically decide the time constant based on the step size being taken.", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "Our algorithm is very similar to [6], but instead of incrementing \u03c4i[t+1] when an outlier is detected, the time-constant is reset to 2.", "startOffset": 33, "endOffset": 36}, {"referenceID": 12, "context": "such that the learning rate \u03b7 should decrease [14].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "To ensure it, we developed a new variant of Adagrad [15] with thresholding, such that each scaling factor is lower bounded by 1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "EXPERIMENTS We have run experiments on character-level PTB with GRU units, on MNIST with Maxout Networks [16] and on handwriting synthesis using the IAM-OnDB dataset [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "EXPERIMENTS We have run experiments on character-level PTB with GRU units, on MNIST with Maxout Networks [16] and on handwriting synthesis using the IAM-OnDB dataset [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning rate).", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning rate).", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning rate).", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "Our model follows closely the architecture introduced in [18] with two modifications.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "Second, we use GRU [21] units instead of LSTM [22] units.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "Second, we use GRU [21] units instead of LSTM [22] units.", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "Also, we compared against ADAM [20] with different learning rates in Figure 1.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "We have run experiments with GRU-RNN[21] on PTB dataset for character-level language modeling over the subset defined in [23].", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "We have run experiments with GRU-RNN[21] on PTB dataset for character-level language modeling over the subset defined in [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "In 2-layer Maxout network experiments for SGD-momentum experiments, we used the best hyper-parameters reported by [16], for RMSProp and Adagrad, we crossvalidated learning rate for 15 different learning rates sampled uniformly from the log-space.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms. 1", "creator": "LaTeX with hyperref package"}}}