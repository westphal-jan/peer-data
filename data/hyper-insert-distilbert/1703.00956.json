{"id": "1703.00956", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning", "abstract": "representation versus learning system and option discovery capability are two of us the biggest challenges in reinforcement network learning ( en rl ). proto - rl classification is a well known approach utilized for discrete representation learning, in mdps. the naive representations prior learned with combining this computation framework are explicitly called projective proto - actor value functions ( pvfs ). in parallel this paper show we address the option discovery problem by showing how pvfs now implicitly physically define selected options. we do it exactly by introducing eigenpurposes, intrinsic dependent reward functions derived from assuming the learned representations. identifying the options currently discovered then from projective eigenpurposes traverse the 2d principal directions mode of achieving the continuous state space. they significantly are useful for multiple linear tasks each because they primarily are independent of the implementing agents'exact intentions. moreover, by capturing mostly the finite diffusion process elements of a random walk, 2 different solution options do act at different minute time length scales, making them helpful for computational exploration strategies. furthermore we jointly demonstrate features transformations of eigenpurposes in traditional tabular task domains as well as in atari 2600 games.", "histories": [["v1", "Thu, 2 Mar 2017 21:31:29 GMT  (3600kb,D)", "https://arxiv.org/abs/1703.00956v1", "Version submitted to the 34th International Conference on Machine Learning (ICML)"], ["v2", "Fri, 16 Jun 2017 02:52:21 GMT  (3684kb,D)", "http://arxiv.org/abs/1703.00956v2", "Appearing in the Proceedings of the 34th International Conference on Machine Learning (ICML)"]], "COMMENTS": "Version submitted to the 34th International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["marlos c machado", "marc g bellemare", "michael h bowling"], "accepted": true, "id": "1703.00956"}, "pdf": {"name": "1703.00956.pdf", "metadata": {"source": "META", "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning", "authors": ["Marlos C. Machado", "Marc G. Bellemare", "Michael Bowling"], "emails": ["<machado@ualberta.ca>."], "sections": [{"heading": "1. Introduction", "text": "Two important challenges in reinforcement learning (RL) are the problems of representation learning and of automatic discovery of skills. Proto-value functions (PVFs) are a well-known solution for the problem of representation learning (Mahadevan, 2005; Mahadevan & Maggioni, 2007); while the problem of skill discovery is generally posed under the options framework (Sutton et al., 1999; Precup, 2000), which models skills as options.\nIn this paper, we tie together representation learning and option discovery by showing how PVFs implicitly define options. One of our main contributions is to introduce the concepts of eigenpurpose and eigenbehavior. Eigenpurposes are intrinsic reward functions that incentivize the agent to traverse the state space by following the principal directions of the learned representation. Each intrinsic reward function leads to a different eigenbehavior, which is the optimal policy for that reward function. In this paper we\n1University of Alberta 2Google DeepMind. Correspondence to: Marlos C. Machado <machado@ualberta.ca>.\nAppearing in the Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.\nintroduce an algorithm for option discovery that leverages these ideas. The options we discover are task-independent because, as PVFs, the eigenpurposes are obtained without any information about the environment\u2019s reward structure. We first present these ideas in the tabular case and then show how they can be generalized to the function approximation case.\nExploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; S\u0327ims\u0327ek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016). In this paper, we provide evidence that not all options capable of accelerating planning are useful for exploration. We show that options traditionally used in the literature to speed up planning hinder the agents\u2019 performance if used for random exploration during learning. Our options have two important properties that allow them to improve exploration: (i) they operate at different time scales, and (ii) they can be easily sequenced. Having options that operate at different time scales allows agents to make finely timed actions while also decreasing the likelihood the agent will explore only a small portion of the state space. Moreover, because our options are defined across the whole state space, multiple options are available in every state, which allows them to be easily sequenced."}, {"heading": "2. Background", "text": "We generally indicate random variables by capital letters (e.g.,Rt), vectors by bold letters (e.g., \u03b8), functions by lowercase letters (e.g., v), and sets by calligraphic font (e.g., S)."}, {"heading": "2.1. Reinforcement Learning", "text": "In the RL framework (Sutton & Barto, 1998), an agent aims to maximize cumulative reward by taking actions in an environment. These actions affect the agent\u2019s next state and the rewards it experiences. We use the MDP formalism throughout this paper. An MDP is a 5-tuple \u3008S,A, r, p, \u03b3\u3009. At time t the agent is in state st \u2208 S where it takes action at \u2208 A that leads to the next state st+1 \u2208 S according to the transition probability kernel p(s\u2032|s, a), which encodes Pr(St+1 = s\n\u2032|St = s,At = a). The agent also observes a reward Rt+1 \u223c r(s, a). The agent\u2019s goal is to learn a policy \u00b5 : S \u00d7 A \u2192 [0, 1] that maximizes the expected\nar X\niv :1\n70 3.\n00 95\n6v 2\n[ cs\n.L G\n] 1\n6 Ju\nn 20\n17\ndiscounted returnGt . = Ep,\u00b5 [\u2211\u221e k=0 \u03b3 kRt+k+1|st ] , where \u03b3 \u2208 [0, 1) is the discount factor.\nIt is common to use the policy improvement theorem (Bellman, 1957) when learning to maximize Gt. One technique is to alternate between solving the Bellman equations for the action-value function q\u00b5k(s, a),\nq\u00b5k(s, a) . = E\u00b5k,p [ Gt|St = s,At = a ] = \u2211 s\u2032,r p(s\u2032, r|s, a) [ r + \u03b3 \u2211 a\u2032 \u00b5k(a \u2032|s\u2032)q\u00b5k(s\u2032, a\u2032)\n] and making the next policy, \u00b5k+1, greedy w.r.t. q\u00b5k ,\n\u00b5k+1 . = arg max\na\u2208A q\u00b5k(s, a),\nuntil converging to an optimal policy \u00b5\u2217.\nSometimes it is not feasible to learn a value for each stateaction pair due to the size of the state space. Generally, this is addressed by parameterizing q\u00b5(s, a) with a set of weights \u03b8 \u2208 Rn such that q\u00b5(s, a) \u2248 q\u00b5(s, a,\u03b8). It is common to approximate q\u00b5 through a linear function, i.e., q\u00b5(s, a,\u03b8) = \u03b8\n>\u03c6(s, a), where \u03c6(s, a) denotes a linear feature representation of state s when taking action a."}, {"heading": "2.2. The Options Framework", "text": "The options framework extends RL by introducing temporally extended actions called skills or options. An option \u03c9 is a 3-tuple \u03c9 = \u3008I, \u03c0, T \u3009 where I \u2208 S denotes the option\u2019s initiation set, \u03c0 : A\u00d7S\u2192 [0, 1] denotes the option\u2019s policy, and T \u2208 S denotes the option\u2019s termination set. After the agent decides to follow option \u03c9 from a state in I, actions are selected according to \u03c0 until the agent reaches a state in T . Intuitively, options are higher-level actions that extend over several time steps, generalizing MDPs to semiMarkov decision processes (SMDPs) (Puterman, 1994).\nTraditionally, options capable of moving agents to bottleneck states are sought after. Bottleneck states are those states that connect different densely connected regions of the state space (e.g., doorways) (S\u0327ims\u0327ek & Barto, 2004; Solway et al., 2014). They have been shown to be very efficient for planning as these states are the states most frequently visited when considering the shortest distance between any two states in an MDP (Solway et al., 2014)."}, {"heading": "2.3. Proto-Value Functions", "text": "Proto-value functions (PVFs) are learned representations that capture large-scale temporal properties of an environment (Mahadevan, 2005; Mahadevan & Maggioni, 2007). They are obtained by diagonalizing a diffusion model, which is constructed from the MDP\u2019s transition matrix. A diffusion model captures information flow on a graph, and\nit is commonly defined by the combinatorial graph Laplacian matrix L = D \u2212 A, where A is the graph\u2019s adjacency matrix and D the diagonal matrix whose entries are the row sums of A. Notice that the adjacency matrix A easily generalizes to a weight matrix W . PVFs are defined to be the eigenvectors obtained after the eigendecomposition of L. Different diffusion models can be used to generate PVFs, such as the normalized graph Laplacian L = D\u2212 1 2 (D \u2212A)D\u2212 12 , which we use in this paper."}, {"heading": "3. Option Discovery through the Laplacian", "text": "PVFs capture the large-scale geometry of the environment, such as symmetries and bottlenecks. They are task independent, in the sense that they do not use information related to reward functions. Moreover, they are defined over the whole state space since each eigenvector induces a realvalued mapping over each state. We can imagine that options with these properties should also be useful. In this section we show how to use PVFs to discover options.\nLet us start with an example. Consider the traditional 4- room domain depicted in Figure 1c. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. Ideally, we would like to discover options that move the agent from room to room. Thus, we should be able to automatically distinguish between the different rooms in the environment. This is exactly what PVFs do, as depicted in Figure 2 (left). Instead of interpreting a PVF as a basis function, we can interpret the PVF in our example as a desire to reach the highest point of the plot, corresponding to the centre of the room. Because the sign of an eigenvector is arbitrary, a PVF can also be interpreted as a desire to reach the lowest point of the plot, corresponding to the opposite room. In this paper we use the eigenvectors in both directions (i.e., both signs).\nAn eigenpurpose formalizes the interpretation above by defining an intrinsic reward function. We can see it as defining a purpose for the agent, that is, to maximize the discounted sum of these rewards.\nDefinition 3.1 (Eigenpurpose). An eigenpurpose is the intrinsic reward function rei (s, s\n\u2032) of a proto-value function e \u2208 R|S| such that\nrei (s, s \u2032) = e>(\u03c6(s\u2032)\u2212 \u03c6(s)), (1)\nwhere \u03c6(x) denotes the feature representation of state x.\nNotice that an eigenpurpose, in the tabular case, can be written as rei (s, s \u2032) = e[s\u2032]\u2212 e[s].\nWe can now define a new MDP to learn the option associated with the purpose,Mei = \u3008S,A\u222a{\u22a5}, rei , p, \u03b3\u3009, where\nthe reward function is defined as in (1) and the action set is augmented by the action terminate (\u22a5), which allows the agent to leave Mei without any cost. The state space and the transition probability kernel remain unchanged from the original problem. The discount rate can be chosen arbitrarily, although it impacts the timescale the option encodes.\nWith Mei we define a new state-value function ve\u03c0(s), for policy \u03c0, as the expected value of the cumulative discounted intrinsic reward if the agent starts in state s and follows policy \u03c0 until termination. Similarly, we define a new action-value function qe\u03c0(s, a) as the expected value of the cumulative discounted intrinsic reward if the agent starts in state s, takes action a, and then follows policy \u03c0 until termination. We can also describe the optimal value function for any eigenpurpose obtained through e:\nve\u2217(s) = max \u03c0 ve\u03c0(s) and q e \u2217(s, a) = max \u03c0 qe\u03c0(s, a).\nThese definitions naturally lead us to eigenbehaviors.\nDefinition 3.2 (Eigenbehavior). An eigenbehavior is a policy \u03c7e : S \u2192 A that is optimal with respect to the eigenpurpose rei , i.e., \u03c7 e(s) = arg maxa\u2208A q e \u2217(s, a).\nFinding the optimal policy \u03c0e\u2217 now becomes a traditional RL problem, with a different reward function. Importantly, this reward function tends to be dense, avoiding challenging situations due to exploration issues. In this paper we use policy iteration to solve for an optimal policy.\nIf each eigenpurpose defines an option, its corresponding eigenbehavior is the option\u2019s policy. Thus, we need to define the option\u2019s initiation and termination set. An option should be available in every state where it is possible to achieve its purpose, and to terminate when it is achieved.\nWhen defining the MDP to learn the option, we augmented the agent\u2019s action set with the terminate action, allowing the agent to interrupt the option anytime. We want options to terminate when the agent achieves its purpose, i.e., when it is unable to accumulate further positive intrinsic rewards. With the defined reward function, this happens when the agent reaches the state with largest value in the eigenpurpose (or a local maximum when \u03b3 < 1). Any subsequent reward will be negative. We are able to formalize this con-\ndition by defining q\u03c7(s,\u22a5) . = 0 for all \u03c7e. When the terminate action is selected, control is returned to the higher level policy (Dietterich, 2000). An option following a policy \u03c7e terminates when qe\u03c7(s, a) \u2264 0 for all a \u2208 A. We define the initiation set to be all states in which there exists an action a \u2208 A such that qe\u03c7(s, a) > 0. Thus, the option\u2019s policy is \u03c0e(s) = arg maxa\u2208A\u222a{\u22a5} q e \u03c0(s, a). We refer to the options discovered with our approach as eigenoptions. The eigenoption corresponding to the example at the beginning of this section is depicted in Figure 2 (right).\nFor any eigenoption, there is always at least one state in which it terminates, as we now show.\nTheorem 3.1 (Option\u2019s Termination). Consider an eigenoption o = \u3008Io, \u03c0o, To\u3009 and \u03b3 < 1. Then, in an MDP with finite state space, To is nonempty.\nProof. We can write the Bellman equation in the matrix form: v = r+\u03b3Tv, where v is a finite column vector with one entry per state encoding its value function. From (1) we have r = Tw\u2212w with w = \u03c6(s)>e, where e denotes the eigenpurpose of interest. Therefore:\nv + w = Tw + \u03b3Tv\n= (1\u2212 \u03b3)Tw + \u03b3T (v + w) = (1\u2212 \u03b3)(I \u2212 \u03b3T )\u22121Tw.\n||v + w||\u221e = (1\u2212 \u03b3)||(I \u2212 \u03b3T )\u22121Tw||\u221e ||v + w||\u221e \u2264 (1\u2212 \u03b3)||(I \u2212 \u03b3T )\u22121T ||\u221e||w||\u221e\n||v + w||\u221e \u2264 (1\u2212 \u03b3) 1\n(1\u2212 \u03b3) ||w||\u221e\n||v + w||\u221e \u2264 ||w||\u221e\nWe can shift w by any finite constant without changing the reward, i.e., Tw\u2212w = T (w+\u03b4)\u2212(w+\u03b4) because T1\u03b4 = 1\u03b4 since \u2211 j Ti,j = 1. Hence, we can assume w\u2265 0. Let s\u2217 = arg maxsws\u2217 , so that ws\u2217 = ||w||\u221e. Clearly vs\u2217 \u2264 0, otherwise ||v + w||\u221e \u2265 |vs\u2217 + ws\u2217 | = vs\u2217 + ws\u2217 > ws\u2217 = ||w||\u221e, arriving at a contradiction.\nThis result is applicable in both the tabular and linear function approximation case. An algorithm that does not rely on knowing the underlying graph is provided in Section 5."}, {"heading": "4. Empirical Evaluation", "text": "We used three MDPs in our empirical study (c.f. Figure 1): an open room, an I-Maze, and the 4-room domain. Their transitions are deterministic and gray squares denote walls. Agents have access to four actions: up, down, right, and left. When an action that would have taken the agent into a wall is chosen, the agent\u2019s state does not change. We demonstrate three aspects of our framework:1\n\u2022 How the eigenoptions present specific purposes. Interestingly, options leading to bottlenecks are not the first ones we discover.\n\u2022 How eigenoptions improve exploration by reducing the expected number of steps required to navigate between any two states.\n\u2022 How eigenoptions help agents to accumulate reward faster. We show how few options may hurt the agents\u2019 performance while enough options speed up learning."}, {"heading": "4.1. Discovered Options", "text": "In the PVF theory, the \u201csmoothest\u201d eigenvectors, corresponding to the smallest eigenvalues, are preferred (Mahadevan & Maggioni, 2007). The same intuition applies to eigenoptions, with the eigenpurposes corresponding to the smallest eigenvalues being preferred. Figures 3, 4, and 5 depict the first eigenoptions discovered in the three domains used for evaluation.\nEigenoptions do not necessarily look for bottleneck states,\n1Python code can be found at: https://github.com/mcmachado/options\nallowing us to apply our algorithm in many environments in which there are no obvious, or meaningful, bottlenecks. We discover meaningful options in these environments, such as walking down a corridor, or going to the corners of an open room. Interestingly, doorways are not the first options we discover in the 4-room domain (the fifth eigenoption is the first to terminate at the entrance of a doorway). In the next sections we provide empirical evidence that eigenoptions are useful, and often more so than bottleneck options."}, {"heading": "4.2. Exploration", "text": "A major challenge for agents to explore an environment is to be decisive, avoiding the dithering commonly observed in random walks (Machado & Bowling, 2016; Osband et al., 2016). Options provide such decisiveness by operating in a higher level of abstraction. Agents performing a random walk, when equipped with options, are expected to cover larger distances in the state space, navigating back and forth between subgoals instead of dithering around the starting state. However, options need to satisfy two conditions to improve exploration: (1) they have to be available in several parts of the state space, ensuring the agent always has access to many different options; and (2) they have to operate at different time scales. For instance, in the 4-room domain, it is unlikely an agent randomly selects enough primitive actions leading it to a corner if all options move the agent between doorways. An important result in this section is to show that it is very unlikely for an agent to explore the whole environment if it keeps going back and forth between similar high-level goals.\nEigenoptions satisfy both conditions. As demonstrated in Section 4.1, eigenoptions are often defined in the whole state space, allowing sequencing. Moreover, PVFs can be seen as a \u201cfrequency\u201d basis, with different PVFs being associated with different frequencies (Mahadevan & Maggioni, 2007). The corresponding eigenoptions also operate\nat different frequencies, with the length of a trajectory until termination varying. This behavior can be seen when comparing the second and fourth eigenoptions in the 10 \u00d7 10 grid (Figure 3). The fourth eigenoption terminates, on expectation, twice as often as the second eigenoption.\nIn this section we show that eigenoptions improve exploration. We do so by introducing a new metric, which we call diffusion time. Diffusion time encodes the expected number of steps required to navigate between two states randomly chosen in the MDP while following a random walk. A small expected number of steps implies that it is more likely that the agent will reach all states with a random walk. We discuss how this metric can be computed in the Appendix.\nFigure 6 depicts, for our the three environments, the diffusion time with options and the diffusion time using only primitive actions. We add options incrementally in order of increasing eigenvalue when computing the diffusion time for different sets of options.\nThe first options added hurt exploration, but when enough options are added, exploration is greatly improved when compared to a random walk using only primitive actions. The fact that few options hurt exploration may be surprising at first, based on the fact that few useful options are generally sought after in the literature. However, this is a major difference between using options for planning and for learning. In planning, options shortcut the agents\u2019 trajectories, pruning the search space. All other actions are still taken into consideration. When exploring, a uniformly random policy over options and primitive actions skews where\nagents spend their time. Options that are much longer than primitive actions reduce the likelihood that an agent will deviate much from the options\u2019 trajectories, since sampling an option may undo dozens of primitive actions. This biasing is often observed when fewer options are available.\nThe discussion above can be made clearer with an example. In the 4-room domain, if the only options available are those leading the agent to doorways (c.f. Appendix), it is less likely the agent will reach the outer corners. To do so the agent would have to select enough consecutive primitive actions without sampling an option. Also, it is very likely agents will be always moving between rooms, never really exploring inside a room. These issues are mitigated with eigenoptions. The first eigenoptions lead agents to individual rooms, but other eigenoptions operate in different time scales, allowing agents to explore different parts of rooms.\nFigure 6d supports the intuition that options leading to bottleneck states are not sufficient, by themselves, for exploration. It shows how the diffusion time in the 4-room domain is increased when only bottleneck options are used. As in the PVF literature, the ideal number of options to be used by an agent can be seen as a model selection problem."}, {"heading": "4.3. Accumulating Rewards", "text": "We now illustrate the usefulness of our options when the agent\u2019s goal is to accumulate reward. We also study the impact of an increasing number of options in such a task. In these experiments, the agent starts at the bottom left cor-\nner and its goal is to reach the top right corner. The agent observes a reward of 0 until the goal is reached, when it observes a reward of +1. We used Q-Learning (Watkins & Dayan, 1992) (\u03b1 = 0.1, \u03b3 = 0.9) to learn a policy over primitive actions. The behavior policy chooses uniformly over primitive actions and options, following them until termination. Figure 7 depicts, after learning for a given number of episodes, the average over 100 trials of the agents\u2019 final performance. Episodes were 100 time steps long, and we learned for 250 episodes in the 10 \u00d7 10 grid and in the I-Maze, and for 500 episodes in the 4-room domain.\nIn most scenarios eigenoptions improve performance. As in the previous section, exceptions occur when only a few options are added to the agent\u2019s action set. The best results were obtained using 64 options. Despite being an additional parameter, our results show that the agent\u2019s performance is fairly robust across different numbers of options.\nEigenoptions are task-independent by construction. Additional results in the appendix show how the same set of eigenoptions is able to speed-up learning in different tasks. In the appendix we also compare eigenoptions to random options, that is, options that use a random state as subgoal."}, {"heading": "5. Approximate Option Discovery", "text": "So far we have assumed that agents have access to the adjacency matrix representing the underlying MDP. However, in practical settings this is generally not true. In fact, the number of states in these settings is often so large that agents rarely visit the same state twice. These problems are generally tackled with sample-based methods and some sort of function approximation.\nIn this section we propose a sample-based approach for option discovery that asymptotically discovers eigenoptions. We then extend this algorithm to linear function approximation. We provide anecdotal evidence in Atari 2600 games that this relatively na\u0131\u0308ve sample-based approach to function approximation discovers purposeful options."}, {"heading": "5.1. Sample-based Option Discovery", "text": "In the online setting, agents must sample trajectories. Naturally, one can sample trajectories until one is able to perfectly construct the MDP\u2019s adjacency matrix, as suggested by Mahadevan & Maggioni (2007). However, this approach does not easily extend to linear function approximation. In this section we provide an approach that does not build the adjacency matrix allowing us to extend the concept of eigenpurposes to linear function approximation.\nIn our algorithm, a sample transition is added to a matrix T if it was not previously encountered. The transition is added as the difference between the current and previous observations, i.e., \u03c6(s\u2032) \u2212 \u03c6(s). In the tabular case we define \u03c6(s) to be the one-hot encoding of state s. Once enough transitions have been sampled, we perform a singular value decomposition on the matrix T such that T = U\u03a3V >. We use the columns of V , which correspond to the right-eigenvectors of T , to generate the eigenpurposes. The intrinsic reward and the termination criterion for an eigenbehavior are the same as before.\nMatrix T is known as the incidence matrix. If all transitions in the graph are sampled once, for tabular representations, this algorithm discovers the same options we obtain with the combinatorial Laplacian. The theorem below states the equivalence between the obtained eigenpurposes.\nTheorem 5.1. Consider the SVD of T = UT\u03a3TV >T , with each row of T consisting of the difference between observations, i.e., \u03c6(s\u2032)\u2212\u03c6(s). In the tabular case, if all transitions in the MDP have been sampled once, the orthonormal eigenvectors of L are the columns of V >T .\nProof. Given the SVD decomposition of a matrix A = U\u03a3V >, the columns of V are the eigenvectors of A>A (Strang, 2005). We know that T>T = 2L, where L = D \u2212 W (Lemma 5.1, c.f. Appendix). Thus, the columns of VT are the eigenvectors of T>T , which can be rewritten as 2(D \u2212W ). Therefore, the columns of VT are also the eigenvectors of L.\nThere is a trade-off between reconstructing the adjacency matrix and constructing the incidence matrix. In MDPs in which states are sparsely connected, such as the I-Maze, the latter is preferred since it has fewer transitions than states. However, what makes this result interesting is the fact that our algorithm can be easily generalized to linear function approximation."}, {"heading": "5.2. Function Approximation", "text": "An adjacency matrix is not very useful when the agent has access only to features of the state. However, we can use the intuition about the incidence matrix to propose an algorithm compatible with linear function approximation.\nIn fact, to apply the algorithm proposed in the previous section, we just need to define what constitutes a new transition. We define two vectors, t and t\u2032, to be identical if and only if t\u2212 t\u2032 = 0. We then use a set data structure to avoid duplicates when storing \u03c6(s\u2032)\u2212\u03c6(s). This is a na\u0131\u0308ve approach, but it provides encouraging evidence eigenoptions generalize to linear function approximation. We expect more involved methods to perform even better.\nWe tested our method in the ALE (Bellemare et al., 2013). The agent\u2019s representation consists of the emulator\u2019s RAM state (1,024 bits). The final incidence matrix in which we ran the SVD had 25,000 rows, which we sampled uniformly from the set of observed transitions. We provide further details of the experimental setup in the appendix.\nIn the tabular case we start selecting eigenpurposes generated by the eigenvectors with smallest eigenvalue, because these are the \u201csmoothest\u201d ones. However, it is not clear such intuition holds here because we are in the function approximation setting and the matrix of transitions does not contain all possible transitions. Therefore, we analyzed, for each game, all 1,024 discovered options.\nWe approximate these options greedily (\u03b3 = 0) with the ALE emulator\u2019s look-ahead. The next action a\u2032 for an eigenpurpose e is selected as arg maxb\u2208A \u222b s\u2032 p(s\u2032|s, b) rei (s, s\u2032).\nEven with such a myopic action selection mechanism we\nwere able to obtain options that clearly demonstrate intent. In FREEWAY, a game in which a chicken is expected to cross the road while avoiding cars, we observe options in which the agent clearly wants to reach a specific lane in the street. Figure 8 (left) depicts where the chicken tends to be when the option is executed. On the right we see a histogram representing the chicken\u2019s height during an episode. We can clearly see how the chicken\u2019s height varies for different options, and how a random walk over primitive actions (rand) does not explore the environment properly. Remarkably, option #445 scores 28 points at the end of the episode, without ever explicitly taking the reward signal into consideration. This performance is very close to those obtained by state-of-the-art algorithms.\nIn MONTEZUMA\u2019S REVENGE, a game in which the agent needs to navigate through a room to pickup a key so it can open a door, we also observe the agent having the clear intent of reaching particular positions on the screen, such as staircases, ropes and doors (Figure 9). Interestingly, the options we discover are very similar to those handcrafted by Kulkarni et al. (2016) when evaluating the usefulness of options to tackle such a game. A video of the highlighted options can be found online.2"}, {"heading": "6. Related Work", "text": "Most algorithms for option discovery can be seen as topdown approaches. Agents use trajectories leading to informative rewards3 as a starting point, decomposing and refining them into options. There are many approaches based on this principle, such as methods that use the observed rewards to generate intrinsic rewards leading to new value functions (e.g., McGovern & Barto, 2001; Menache et al., 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.g., Mankowitz et al., 2016; Vezhnevets et al., 2016; Bacon et al., 2017), or to do\n2https://youtu.be/2BVicx4CDWA 3We define an informative reward to be the signal that informs the agent it has reached a goal. For example, when trying to escape from a maze, we consider 0 to be an informative reward if the agent observes rewards of value \u22121 in every time step it is inside the maze. A different example is a positive reward observed by an agent that typically observes rewards of value 0.\nprobabilistic inference (Daniel et al., 2016). However, such approaches are not applicable in large state spaces with sparse rewards. If informative rewards are unlikely to be found by an agent using only primitive actions, requiring long or specific sequences of actions, options are equally unlikely to be discovered.\nOur algorithm can be seen as a bottom-up approach, in which options are constructed before the agent observes any informative reward. These options are composed to generate the desired policy. Options discovered this way tend to be independent of an agent\u2019s intention, and are potentially useful in many different tasks (Gregor et al., 2016). Such options can also be seen as being useful for exploration by allowing agents to commit to a behavior for an extended period of time (Machado & Bowling, 2016). Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (S\u0327ims\u0327ek & Barto, 2004; S\u0327ims\u0327ek et al., 2005; S\u0327ims\u0327ek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016). Interestingly, S\u0327ims\u0327ek et al. (2005) and Lakshminarayanan et al. (2016) also use the graph Laplacian in their algorithm, but to identify bottleneck states.\nBaranes & Oudeyer (2013) and Moulin-Frier & Oudeyer (2013) show how one can build policies to explicitly assist agents to explore the environment. The proposed algorithms self-generate subgoals in order to maximize learning progress. The policies built can be seen as options. Recently, Solway et al. (2014) proved that \u201coptimal hierarchy minimizes the geometric mean number of trial-and-error attempts necessary for the agent to discover the optimal policy for any selected task (...)\u201d. Our experiments confirm this result, although we propose diffusion time as a different metric to evaluate how options improve exploration.\nThe idea of discovering options by learning to control parts of the environment is also related to our work. Eigenpurposes encode different rates of change in the agents representation of the world, while the corresponding options aim at maximizing such change. Others have also proposed ways to discover options based on the idea of learning to control the environment. Hengst (2002), for instance, proposes an algorithm that explicitly models changes in the variables that form the agent\u2019s representation. Recently, Gregor et al. (2016) proposed an algorithm in which agents discover options by maximizing a notion of empowerment (Salge et al., 2014), where the agent aims at getting to states with a maximal set of available intrinsic options.\nContinual Curiosity driven Skill Acquisition (CCSA) (Kompella et al., In Press) is the closest approach to ours. CCSA also discovers skills that maximize an intrinsic reward obtained by some extracted representation. While we use PVFs, CCSA uses Incremental Slow Feature Analysis\n(SFA) (Kompella et al., 2011) to define the intrinsic reward function. Sprekeler (2011) has shown that, given a specific choice of adjacency function, PVFs are equivalent to SFA (Wiskott & Sejnowski, 2002). SFA becomes an approximation of PVFs if the function space used in the SFA does not allow arbitrary mappings from the observed data to an embedding. Our method differs in how we define the initiation and termination sets, as well as in the objective being maximized. CCSA acquires skills that produce a large variation in the slow-feature outputs, leading to options that seek for bottlenecks. Our approach does not seek for bottlenecks, focusing on traversing different directions of the learned representation."}, {"heading": "7. Conclusion", "text": "Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks. On the other hand, the wrong hierarchy can hinder the agents\u2019 learning process, moving the agent away from desired goal states. Current algorithms for option discovery often depend on an initial informative reward signal, which may not be readily available in large MDPs. In this paper, we introduced an approach that is effective in different environments, for a multitude of tasks.\nOur algorithm uses the graph Laplacian, being directly related to the concept of proto-value functions. The learned representation informs the agent what are meaningful options to be sought after. The discovered options can be seen as traversing each one of the dimensions in the learned representation. We believe successful algorithms in the future will be able to simultaneously discover representations and options. Agents will use their learned representation to discover options, which will be used to further explore the environment, improving the agent\u2019s representation.\nInterestingly, the options first discovered by our approach do not necessarily find bottlenecks, which are commonly sought after. In this paper we showed how bottleneck options can hinder exploration strategies if naively added to the agent\u2019s action set, and how the options we discover can help an agent to explore. Also, we have shown how the discovered options can be used to accumulate reward in a multitude of tasks, leveraging their exploratory properties.\nThere are several exciting avenues for future work. As noted, SFA can be seen as an approximation to PVFs. It would be interesting to compare such an approach to eigenoptions. It would also be interesting to see if the options we discover can be generated incrementally and with incomplete graphs. Finally, one can also imagine extensions to the proposed algorithm where a hierarchy of options is built."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Will Dabney, Re\u0301mi Munos and Csaba Szepesva\u0301ri for useful discussions. This work was supported by grants from Alberta Innovates Technology Futures and the Alberta Machine Intelligence Institute (Amii). Computing resources were provided by Compute Canada through CalculQue\u0301bec."}, {"heading": "Appendix: Supplementary Material", "text": "This supplementary material contains details omitted from the main text due to space constraints. The list of contents is below:\n\u2022 Supporting lemmas and their respective proofs, as well as a more detailed proof of Theorem 3.1;\n\u2022 Description of how to easily compute the diffusion time in tabular MDPs;\n\u2022 The options leading to bottleneck states (doorways) we used in our experiments;\n\u2022 Performance comparisons between eigenoptions and options generated to reach randomly selected states;\n\u2022 Demonstration of the applicability of eigenoptions in multiple tasks with a new set of experiments;\n\u2022 Further details on the empirical setting used in the Arcade Learning Environment."}, {"heading": "A. Lemmas and Proofs", "text": "Lemma 11.1. Suppose (I +A) is a non-singular matrix, with ||A|| \u2264 1. We have:\n||(I +A)\u22121|| \u2264 1 1\u2212 ||A|| .\nProof. 4\n(I +A)(I +A)\u22121 = I\nI(I +A)\u22121 +A(I +A)\u22121 = I\n(I +A)\u22121 = I \u2212A(I +A)\u22121\n||(I +A)\u22121|| = ||I \u2212A(I +A)\u22121|| \u2264 ||I||+ ||A(I +A)\u22121|| because ||A+B|| \u2264 ||A||+ ||B|| \u2264 1 + ||A||||(I +A)\u22121|| because ||AB|| \u2264 ||A|| \u00b7 ||B||\n||(I +A)\u22121|| \u2212 ||A||||(I +A)\u22121|| \u2264 1 (1\u2212 ||A||)||(I +A)\u22121|| \u2264 1\n||(I +A)\u22121|| \u2264 1 1\u2212 ||A||\nif ||A|| \u2264 1.\nLemma 11.2. The induced infinity norm of (I \u2212 \u03b3T )\u22121T is bounded by\n||(I \u2212 \u03b3T )\u22121T ||\u221e \u2264 1\n(1\u2212 \u03b3) .\nProof.\n||(I \u2212 \u03b3T )\u22121T ||\u221e \u2264 ||(I \u2212 \u03b3T )\u22121||\u221e||T ||\u221e because ||AB||\u221e \u2264 ||A||\u221e \u00b7 ||B||\u221e\n||(I \u2212 \u03b3T )\u22121T ||\u221e \u2264 1\n1\u2212 || \u2212 \u03b3T ||\u221e ||T ||\u221e Lemma 3.1\n||(I \u2212 \u03b3T )\u22121T ||\u221e \u2264 1\n1\u2212 \u03b3||T ||\u221e ||T ||\u221e because ||\u03bbB|| = |\u03bb|||B||\n||(I \u2212 \u03b3T )\u22121T ||\u221e \u2264 1\n(1\u2212 \u03b3)\n4Our proof follows closely the proof of Parnell in lecture notes available at http://www-solar.mcs.st-and.ac.uk/ \u02dcclare/Lectures/num-analysis.html.\nTheorem 11.1 (Option\u2019s Termination). Consider an eigenoption o = \u3008Io, \u03c0o, To\u3009 and \u03b3 < 1. Then, in an MDP with finite state space, To is nonempty.\nProof. This proof is more detailed than the one presented in the main paper. We can write the Bellman equation in the matrix form: v = r + \u03b3Tv, where v is a finite column vector with one entry per state encoding its value function. From equation (1) in the main paper we have r = Tw \u2212w with w = \u03c6(s)>e, where e denotes the eigenpurpose of interest. Therefore:\nv = Tw \u2212w + \u03b3Tv v + w = Tw + \u03b3Tv\n= Tw + \u03b3Tv + \u03b3Tw \u2212 \u03b3Tw = (1\u2212 \u03b3)Tw + \u03b3T (v + w)\nv + w \u2212 \u03b3T (v + w) = (1\u2212 \u03b3)Tw (I \u2212 \u03b3T )(v + w) = (1\u2212 \u03b3)Tw\nv + w = (1\u2212 \u03b3)(I \u2212 \u03b3T )\u22121Tw (I \u2212 \u03b3T )\u22121 is guaranteed to be nonsigular because ||T || \u2264 1, where ||T || = sup\nv:||v||\u221e=1 ||Tv||\u221e. By\nNeumann series we have (I \u2212 \u03b3T )\u22121 = \u221e\u2211 n=0 \u03b3nTn\n||v + w||\u221e = (1\u2212 \u03b3)||(I \u2212 \u03b3T )\u22121Tw||\u221e using the induced norm ||v + w||\u221e \u2264 (1\u2212 \u03b3)||(I \u2212 \u03b3T )\u22121T ||\u221e||w||\u221e because ||Ax|| \u2264 ||A|| \u00b7 ||x||\n||v + w||\u221e \u2264 (1\u2212 \u03b3) 1\n(1\u2212 \u03b3) ||w||\u221e Lemma 3.2\n||v + w||\u221e \u2264 ||w||\u221e\nWe can shift w by any finite constant without changing the reward, i.e. Tw \u2212w = T (w+\u03b4)\u2212(w+\u03b4) because T1\u03b4 = 1\u03b4 since \u2211 j Ti,j = 1. Therefore, we can assume w \u2265 0. Let s\u2217 = arg maxsws\u2217 , so that ws\u2217 = ||w||\u221e. Clearly vs\u2217 \u2264 0, otherwise ||v + w||\u221e \u2265 |vs\u2217 + ws\u2217 | = vs\u2217 + ws\u2217 > ws\u2217 = ||w||\u221e, arriving at a contradiction.\nLemma 12.1. In the tabular case, if all transitions in the MDP have been sampled once, T>T = 2L.\nProof. Let tij and ttij denote the entries in the i-th row and j-th column of matrices T and T>T . We can write ttij as:\nttij = \u2211 k tik \u00d7 tjk. (2)\nIn the tabular case, tij has three possible values:\n\u2022 tij = +1, meaning that the agent arrived in state j at time step i,\n\u2022 tij = \u22121, meaning that the agent left state j at time step i,\n\u2022 tij = 0, meaning that the agent did not arrive nor leave state j at time step i.\nWe decompose T>T in two matrices, K and Z, such that T>T = K+Z. Here Z is a diagonal matrix such that zii = ttii, for all i; and K contains all elements from T>T that lie outside the main diagonal.\nWhen computing the elements of Z we have i = j. Thus zii = \u2211 k t 2 ik. Because we square all elements, we are in fact summing over all transitions leaving (\u221212) and arriving (12) in state i, counting the node\u2019s degree twice. Thus, Z = 2D.\nWhen not computing the elements in the main diagonal, for the element ttij , we add all transitions that leave state i arriving in state j (\u22121 \u00d7 1), and those that leave state j arriving in state i (1 \u00d7 \u22121). We assume each transition has been sampled once, thus:\nttij =\n{ \u22122, if the transition between states i and j exists,\n0, otherwise.\nTherefore, we have K = \u22122W and T>T = K + Z = 2(D \u2212W )."}, {"heading": "B. Diffusion Time Computation", "text": "In the main paper we introduced diffusion time as a new metric to evaluate exploration, but we did not discuss how it can be computed. Diffusion time encodes the expected number of time steps required to navigate between any two states in the MDP when following a random walk. In tabular domains, we can easily compute the diffusion time with dynamic programming. To do so we define a new MDP such that the value function of a state s, under a uniform random policy, encodes the expected number of steps required to navigate between state s and a chosen goal state. We can then compute the expected number of steps between any two states by averaging, for each possible goal, the value of all other states.\nThe MDP in which the value function of state s encodes the expected number of time steps from s to a goal state has \u03b3 = 1 and a reward function where the agent observes +1 at every time step in which it is not in the goal state. Policy evaluation in this case encodes the expected number of time steps the agent will take before arriving to the goal state. To compute the diffusion time we iterate over all possible states, defining them as terminal states, and averaging the value function of the other states in that MDP."}, {"heading": "C. Options Leading to Doorways in the 4-room Domain", "text": "Figure 10 depicts the four options we refer to in Section 4 as the options leading to bootleneck states, i.e., doorways. Each option is defined in a room and it moves the agent toward the closest doorway. These options were inspired by Solway et al. (2014)\u2019s discussion about the optimal options discovered by their algorithm."}, {"heading": "D. Comparison to Random Options", "text": "In this section we show the importance of using information about diffusion in the environment to define the option\u2019s purposes. This information impacts the sequence of subgoal locations the options\u2019 seek after, as well as the time scales they operate at. The ordering in which the eigenoptions are discovered and the different time scales they operate at can have a major impact on the agents\u2019 performance.\nWe demonstrate the importance of using the environment\u2019s diffusion information by comparing our approach to random options, a simple baseline that does not use such information. This baseline defines an option to be the policy, defined in the whole state space, that terminates in a randomly selected state of the environment. We performed our experiments in the tabular case because it is not clear how we can extend this baseline to settings in which states cannot be enumerated.\nFigure 11a depicts the diffusion time (c.f. Section B) of random options and eigenoptions in the 4-room domain. We used the same method described in Section 4.2 to obtain the eigenoptions\u2019 performance. For the random options results, we added them incrementally to the agent\u2019s action set until having added all possible options. We repeated this process 24 times to verify the impact of adding random options in a different order. Each blue line represents the performance of one\nof the evaluated sequences. The results clearly show that eigenoptions do more than going to a randomly selected state. Most of the obtained sequences of random options fail to reduce the agent\u2019s diffusion time. They increase it by several orders of magnitude (notice the y-axis is in logarithmic scale) until having enough options available to the point that the graph is almost fully connected, that is, when the agent basically has an option leading it to each possible state in the MDP.\nFigure 11b was generated following the protocol described in Section 4.3. It depicts the learning curve of agents equipped with eigenoptions and of agents equipped with random options. As before, the blue lines indicate the agent\u2019s performance in individual runs. We can see that no individual run is competitive to eigenoptions. When fewer options are used (not shown), the variance across individual runs is even larger, depending on whether one of the random options terminates near the goal state. In some runs the agent never even learns to reach the goal. Therefore, as in the diffusion time, on average, random options are not competitive to eigenoptions, demonstrating the importance of the diffusion model we use."}, {"heading": "D. Empirical Evaluation of the Agent\u2019s Performance in Multiple Tasks", "text": "In Section 4 we argued that eigenoptions are useful for multiple tasks, based on results showing that eigenoptions allow us to find and to accumulated rewards faster. Here we explicit demonstrate the uselfuness of eigenoptions to multiple tasks. We evaluate the agents\u2019 performance for different starting and goal states in the 4-room domain. As in Section 4.3, we use Q-Learning (\u03b1 = 0.1, \u03b3 = 0.9) to learn a policy over primitive actions. The behavior policy chooses uniformly over primitive actions and options, following them until termination. Episodes were 100 time steps long, and we learned for 250 episodes. For clarity, we zoom in the plots on the interval in which agents are still learning.\nFigure 14 depicts, after learning for a pre-determined number of episodes, the average over 100 trials of the agents\u2019 final performance, as well as the starting (S) and goal (G) states. Based on our previous results, we fixed the number of used eigenoptions to 64 (32 options and their negations). In this set of experiments we also compare our approach to traditional bottleneck options (Figure 10).\nThe obtained results show that switching the positions of the starting and goal states have no effect in the performance of our algorithm. Also, in almost all settings, the agents augmented by eigenoptions outperfom those equipped only with primitive actions. The comparison between eigenoptions and options that look for bottleneck states is more subtle. As expected, agents equipped with eigenoptions outperform agents equipped with options leading to bottleneck states in settings in which the goal state is far from the doorways, as discussed in the main paper. In scenarios where the goal state is closer to bottleneck states, the options leading to doorways are more competitive. Importantly, this analysis is based on the results when using 64 eigenoptions, which may not encode all options required to go to a specific region of the state space."}, {"heading": "E. Experimental Setup in the Arcade Learning Environment", "text": "We defined six different starting states in each Atari 2600 game, letting the agent take random actions from that point until termination. The agent follows a pre-determined sequence of actions leading it to each starting state. We store the observed transitions leading the agent to the start states as well as those obtained from the random actions. In the main paper we provided results for FREEWAY and MONTEZUMA\u2019S REVENGE. In this section we also provide results for MS PAC-MAN. The starting states for all three games are depicted in Figure 12.\nThe agent plays rounds of six episodes, with each episode starting from a different start state, until it observes at least 25,000 new transitions. The final incidence matrix in which we ran the SVD had 25,000 rows, which we sampled uniformly from the set of observed transitions. The agent used the deterministic version of the Arcade Learning Environment (ALE), the games\u2019 minimal action set and, a frame skip of 1.\nWe used three games to evaluate the options we discover in the sample-based setting with linear function approximation. We discussed the results for FREEWAY and MONTEZUMA\u2019S REVENGE in the main paper. The results we obtained in MS. PAC-MAN are similar to those we already discussed. MS. PAC-MAN is a game in which the agent needs to navigate through a maze eating pellets while avoiding ghosts. As in the other games, the agent has the clear intent of reaching particular positions in the screen, such as corners and intersections. Figure 4 depicts the positions in which agents tend to spend most of their time on. A video of the highlighted options can be found online.5\n5https://youtu.be/2BVicx4CDWA"}], "references": [{"title": "On the Bottleneck Concept for Options Discovery: Theoretical Underpinnings and Extension in Continuous State Spaces", "author": ["Bacon", "Pierre-Luc"], "venue": null, "citeRegEx": "Bacon and Pierre.Luc.,? \\Q2013\\E", "shortCiteRegEx": "Bacon and Pierre.Luc.", "year": 2013}, {"title": "The option-critic architecture", "author": ["Bacon", "Pierre-Luc", "Harb", "Jean", "Precup", "Doina"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Active learning of inverse models with intrinsically motivated goal exploration in robots", "author": ["Baranes", "Adrien", "Oudeyer", "Pierre-Yves"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Baranes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baranes et al\\.", "year": 2013}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Dynamic Programming", "author": ["Bellman", "Richard E"], "venue": null, "citeRegEx": "Bellman and E.,? \\Q1957\\E", "shortCiteRegEx": "Bellman and E.", "year": 1957}, {"title": "Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2004\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2004}, {"title": "Skill Characterization Based on Betweenness", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2008}, {"title": "Identifying Useful Subgoals in Reinforcement Learning by Local Graph Partitioning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Wolfe", "Alicia P", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Probabilistic Inference for Determining Options in Reinforcement Learning", "author": ["Daniel", "Christian", "van Hoof", "Herke", "Peters", "Jan", "Neumann", "Gerhard"], "venue": "Machine Learning,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition", "author": ["Dietterich", "Thomas G"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Discovering Hierarchy in Reinforcement Learning with HEXQ", "author": ["Hengst", "Bernhard"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "Incremental Slow Feature Analysis", "author": ["Kompella", "Varun Raj", "Luciw", "Matthew D", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Kompella et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kompella et al\\.", "year": 2011}, {"title": "Continual Curiosity-Driven Skill Acquisition from High-Dimensional Video Inputs for Humanoid Robots", "author": ["Kompella", "Varun Raj", "Stollenga", "Marijn", "Luciw", "Matthew", "Schmidhuber", "Juergen"], "venue": "Artificial Intelligence,", "citeRegEx": "Kompella et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kompella et al\\.", "year": 2015}, {"title": "Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining", "author": ["Konidaris", "George", "Barto", "Andrew"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Konidaris et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2009}, {"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik R", "Saeedi", "Ardavan", "Tenenbaum", "Joshua B"], "venue": "ArXiv e-prints,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering", "author": ["Lakshminarayanan", "Aravind", "Krishnamurthy", "Ramnandan", "Kumar", "Peeyush", "Ravindran", "Balaraman"], "venue": "CoRR, abs/1605.05359,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Learning Purposeful Behaviour in the Absence of Rewards", "author": ["Machado", "Marlos C", "Bowling", "Michael"], "venue": "CoRR, abs/1410.4604,", "citeRegEx": "Machado et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Machado et al\\.", "year": 2016}, {"title": "Proto-Value Functions: Developmental Reinforcement Learning", "author": ["Mahadevan", "Sridhar"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mahadevan and Sridhar.,? \\Q2005\\E", "shortCiteRegEx": "Mahadevan and Sridhar.", "year": 2005}, {"title": "Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes", "author": ["Mahadevan", "Sridhar", "Maggioni", "Mauro"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Mahadevan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2007}, {"title": "Adaptive Skills Adaptive Partitions (ASAP)", "author": ["Mankowitz", "Daniel J", "Mann", "Timothy Arthur", "Mannor", "Shie"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mankowitz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2016}, {"title": "Dynamic Abstraction in Reinforcement Learning via Clustering", "author": ["Mannor", "Shie", "Menache", "Ishai", "Hoze", "Amit", "Klein", "Uri"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "QCut - Dynamic Discovery of Sub-goals in Reinforcement Learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Proceedings of the European Conference on Machine Learning (ECML),", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Control of Memory, Active Perception, and Action in Minecraft", "author": ["Oh", "Junhyuk", "Chockalingam", "Valliappa", "Singh", "Satinder P", "Lee", "Honglak"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Generalization and Exploration via Randomized Value Functions", "author": ["Osband", "Ian", "Roy", "Benjamin Van", "Wen", "Zheng"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Temporal Abstraction in Reinforcement Learning", "author": ["Precup", "Doina"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Precup and Doina.,? \\Q2000\\E", "shortCiteRegEx": "Precup and Doina.", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q1994\\E", "shortCiteRegEx": "Puterman and L.", "year": 1994}, {"title": "Empowerment \u2013 An Introduction", "author": ["Salge", "Christoph", "Glackin", "Cornelius", "Polani", "Daniel"], "venue": "In Guided SelfOrganization: Inception,", "citeRegEx": "Salge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salge et al\\.", "year": 2014}, {"title": "Optimal Behavioral Hierarchy", "author": ["Solway", "Alec", "Diuk", "Carlos", "C\u00f3rdova", "Natalia", "Yee", "Debbie", "Barto", "Andrew G", "Niv", "Yael", "Botvinick", "Matthew M"], "venue": "PLOS Computational Biology,", "citeRegEx": "Solway et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solway et al\\.", "year": 2014}, {"title": "On the Relation of Slow Feature Analysis and Laplacian Eigenmaps", "author": ["Sprekeler", "Henning"], "venue": "Neural Computation,", "citeRegEx": "Sprekeler and Henning.,? \\Q2011\\E", "shortCiteRegEx": "Sprekeler and Henning.", "year": 2011}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Szepesv\u00e1ri", "Csaba"], "venue": null, "citeRegEx": "Szepesv\u00e1ri and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Csaba.", "year": 2010}, {"title": "Technical Note: Q-Learning", "author": ["Watkins", "Christopher J.C. H", "Dayan", "Peter"], "venue": "Machine Learning,", "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Perron Cluster Analysis and Its Connection to Graph Partitioning for Noisy Data", "author": ["Weber", "Marcus", "Rungsarityotin", "Wasinee", "Schliep", "Alexander"], "venue": "Technical Report 04-39, ZIB,", "citeRegEx": "Weber et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weber et al\\.", "year": 2004}, {"title": "Slow Feature Analysis: Unsupervised Learning of Invariances", "author": ["Wiskott", "Laurenz", "Sejnowski", "Terrence J"], "venue": "Neural Computation,", "citeRegEx": "Wiskott et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wiskott et al\\.", "year": 2002}, {"title": "Options Leading to Doorways in the 4-room Domain Figure 10 depicts the four options we refer to in Section 4 as the options leading to bootleneck states, i.e., doorways. Each option is defined in a room and it moves the agent toward the closest doorway", "author": ["C. MDP"], "venue": "These options were inspired by Solway et al", "citeRegEx": "MDP.,? \\Q2014\\E", "shortCiteRegEx": "MDP.", "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Proto-value functions (PVFs) are a well-known solution for the problem of representation learning (Mahadevan, 2005; Mahadevan & Maggioni, 2007); while the problem of skill discovery is generally posed under the options framework (Sutton et al., 1999; Precup, 2000), which models skills as options.", "startOffset": 229, "endOffset": 264}, {"referenceID": 7, "context": "Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; \u015eim\u015fek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).", "startOffset": 141, "endOffset": 230}, {"referenceID": 28, "context": "Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; \u015eim\u015fek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).", "startOffset": 141, "endOffset": 230}, {"referenceID": 14, "context": "Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; \u015eim\u015fek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).", "startOffset": 141, "endOffset": 230}, {"referenceID": 28, "context": ", doorways) (\u015eim\u015fek & Barto, 2004; Solway et al., 2014).", "startOffset": 12, "endOffset": 55}, {"referenceID": 28, "context": "They have been shown to be very efficient for planning as these states are the states most frequently visited when considering the shortest distance between any two states in an MDP (Solway et al., 2014).", "startOffset": 182, "endOffset": 203}, {"referenceID": 24, "context": "A major challenge for agents to explore an environment is to be decisive, avoiding the dithering commonly observed in random walks (Machado & Bowling, 2016; Osband et al., 2016).", "startOffset": 131, "endOffset": 177}, {"referenceID": 36, "context": "Naturally, one can sample trajectories until one is able to perfectly construct the MDP\u2019s adjacency matrix, as suggested by Mahadevan & Maggioni (2007). However, this approach does not easily extend to linear function approximation.", "startOffset": 84, "endOffset": 152}, {"referenceID": 3, "context": "We tested our method in the ALE (Bellemare et al., 2013).", "startOffset": 32, "endOffset": 56}, {"referenceID": 14, "context": "Interestingly, the options we discover are very similar to those handcrafted by Kulkarni et al. (2016) when evaluating the usefulness of options to tackle such a game.", "startOffset": 80, "endOffset": 103}, {"referenceID": 22, "context": "There are many approaches based on this principle, such as methods that use the observed rewards to generate intrinsic rewards leading to new value functions (e.g., McGovern & Barto, 2001; Menache et al., 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.", "startOffset": 158, "endOffset": 235}, {"referenceID": 1, "context": ", 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.g., Mankowitz et al., 2016; Vezhnevets et al., 2016; Bacon et al., 2017), or to do https://youtu.", "startOffset": 92, "endOffset": 167}, {"referenceID": 8, "context": "probabilistic inference (Daniel et al., 2016).", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": "Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (\u015eim\u015fek & Barto, 2004; \u015eim\u015fek et al., 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al.", "startOffset": 130, "endOffset": 195}, {"referenceID": 20, "context": ", 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016).", "startOffset": 55, "endOffset": 120}, {"referenceID": 15, "context": ", 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016).", "startOffset": 55, "endOffset": 120}, {"referenceID": 5, "context": "Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (\u015eim\u015fek & Barto, 2004; \u015eim\u015fek et al., 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016). Interestingly, \u015eim\u015fek et al. (2005) and Lakshminarayanan et al.", "startOffset": 153, "endOffset": 324}, {"referenceID": 5, "context": "Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (\u015eim\u015fek & Barto, 2004; \u015eim\u015fek et al., 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016). Interestingly, \u015eim\u015fek et al. (2005) and Lakshminarayanan et al. (2016) also use the graph Laplacian in their algorithm, but to identify bottleneck states.", "startOffset": 153, "endOffset": 359}, {"referenceID": 28, "context": "Recently, Solway et al. (2014) proved that \u201coptimal hierarchy minimizes the geometric mean number of trial-and-error attempts necessary for the agent to discover the optimal policy for any selected task (.", "startOffset": 10, "endOffset": 31}, {"referenceID": 27, "context": "(2016) proposed an algorithm in which agents discover options by maximizing a notion of empowerment (Salge et al., 2014), where the agent aims at getting to states with a maximal set of available intrinsic options.", "startOffset": 100, "endOffset": 120}, {"referenceID": 11, "context": "While we use PVFs, CCSA uses Incremental Slow Feature Analysis (SFA) (Kompella et al., 2011) to define the intrinsic reward function.", "startOffset": 69, "endOffset": 92}, {"referenceID": 11, "context": "Continual Curiosity driven Skill Acquisition (CCSA) (Kompella et al., In Press) is the closest approach to ours. CCSA also discovers skills that maximize an intrinsic reward obtained by some extracted representation. While we use PVFs, CCSA uses Incremental Slow Feature Analysis (SFA) (Kompella et al., 2011) to define the intrinsic reward function. Sprekeler (2011) has shown that, given a specific choice of adjacency function, PVFs are equivalent to SFA (Wiskott & Sejnowski, 2002).", "startOffset": 53, "endOffset": 368}, {"referenceID": 31, "context": "Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks.", "startOffset": 91, "endOffset": 133}, {"referenceID": 28, "context": "Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks.", "startOffset": 91, "endOffset": 133}], "year": 2017, "abstractText": "Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment\u2019s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.", "creator": "LaTeX with hyperref package"}}}