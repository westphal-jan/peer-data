{"id": "1703.01024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Exponential Moving Average Model in Parallel Speech Recognition Training", "abstract": "hence as input training data rapid global growth, no large - context scale online parallel training work with mixed multi - gpus cluster model is found widely poorly applied internally in the dynamic neural activity network implementation model learning currently. we first present on a new approach formula that applies fully exponential moving average method in adopting large - bandwidth scale parallel training of neural network model. it is theoretically a crucial non - interference strategy that how the simple exponential moving average spatial model is not broadcasted to distributed workers to update their internal local models after model parameter synchronization in the training performance process, often and nonetheless it is effectively implemented as the final outcome model of the training cognitive system. fully - connected feed - forward neural networks ( dnns ) methods and some deep unidirectional long data short - continuous term memory ( lstm ) recurrent neural communications networks ( rnns ) investigators are successfully trained with this proposed synthetic method for using large compound vocabulary using continuous speech flow recognition on shenma oracle voice file search data computed in mandarin. normally the character error rate ( cer ) of naive mandarin speech recognition software further easily degrades than state - of - form the - : art approaches consists of parallel training.", "histories": [["v1", "Fri, 3 Mar 2017 03:14:28 GMT  (119kb)", "http://arxiv.org/abs/1703.01024v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu tian", "jun zhang", "zejun ma", "yi he", "juan wei"], "accepted": false, "id": "1703.01024"}, "pdf": {"name": "1703.01024.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wj80290}@alibaba-inc.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n01 02\n4v 1\n[ cs\n.C L\n] 3\nM ar\n2 01\nwith multi-GPUs cluster is widely applied in the neural network model learning currently. We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training."}, {"heading": "1. INTRODUCTION", "text": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6]. Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10]. It is significant that training with larger dataset could improve recognition accuracy. As a matter of fact, larger dataset does mean more training samples and more model parameters, and it is high time consumption to train neural networks with only one computing unit. Therefore, parallel training with multi-GPUs is essential, but it leads to slower convergence. For multi-GPUs training, the key problem is how to accelerate convergence and get further improvement.\nMini-batch based stochastic gradient descent (SGD) is the most prevalent method in neural network training procedure. Several methods are proposed based on it, and achieving encouraging performance for parallel training.\nAsynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy. Besides, synchronous SGD is another positive effort, where the parameter server waits for every workers to finish their computation and send their local models to it, and then it sends updated model back to all workers [13]. Synchronous SGD converges well in parallel training with data parallelism, and is also easy to be implemented.\nModel averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15]. Compared with single GPU training, it achieves linear speedup, but the accuracy decreases. Moreover, blockwise model-updating filter (BMUF) provides another linear speedup approach with multi-GPUs on the basis of model averaging. It can achieve improvement or nodegradation of recognition performance compared with minibatch SGD on single GPU [16].\nIt is demonstrated that the performance of moving average of the parameters obtained by SGD is as good as that of the parameters which minimize the empirical cost, and moving average parameters can be used as the estimator of them, if there are enough training samples [17]. One pass learning is then proposed, and it is the combination of learning rate schedule and averaged SGD using moving average [18]. When the moving average model outperforms the model aggregated with model averaging, the moving average model is broadcasted to update local workers. Since the learning rate of one pass learning is difficult to be adjusted, it is challenging to train different models in different domains.\nIn this paper, we propose a new approach which applies exponential moving average (EMA) directly in large-scale synchronous-based parallel training. It is a kind of noninterference method that the EMA model is not broadcasted, after the parameters of each worker are synchronized. It is applied as the final model of the training. The exponential moving average method in parallel training will be described in Section 2. Neural network models are successfully trained for LVCSR, using this method. The experiments and results are presented in Section 3, followed by the conclusion in Section 4."}, {"heading": "2. EXPONENTIAL MOVING AVERAGE MODEL", "text": "In recognition applications, the parameters \u03b8 of neural network is trained for classification. It\u2019s also an optimization problem:\nargmin \u03b8\n1\nt\nt\u2211\ni=1\n(L(f\u03b8(x), y))\nWhere t is the number of data points, (x, y) is the input data and correspondent target, L is the loss function, and f\u03b8 denotes the network. Let \u03b8\u2217 be the parameters that minimize the empirical cost. Large scale recognition training needs to deal with the optimization problem with billions of training data, and makes it hard to find the \u03b8\u2217. SGD and its variants are presented promising learning results for large scale optimization problem, and become the most popular methods of deep learning."}, {"heading": "2.1. Model averaging and block-wise model updating filter", "text": "In order to reduce the time cost of training, data parallelism is implemented. The full training dataset is partitioned into N splits without overlapping, and they are distributed to N GPUs.\nEach GPU optimizes local model in parallel with one split of training dataset. After a mini-batch training, the global model is needed to update, and it is computed with model averaging or BMUF, and consequently broadcasted to GPUs to update their local models. For model averaging method, all local models are synchronized and averaged, and then aggregated model \u03b8\u0304(t) is sent back to GPUs [14, 15]. For BMUF method, the global model \u03b8g(t) is employed, instead of \u03b8\u0304(t) in model averaging method. The synchronization and updating process of \u03b8g(t) in BMUF as follows:\n\u03b8\u0304(t) = 1\nN\nN\u2211\ni=1\n\u03b8i\nG(t) = \u03b8\u0304(t)\u2212 \u03b8g(t\u2212 1)\n\u2206(t) = \u03b7t\u2206(t\u2212 1) + \u03b6tG(t)\nWhere G(t) denotes model update, and \u2206(t) is the globalmodel update. There are two parameters in BMUF, block momentum \u03b7, and block learning rate \u03b6. Then, the global model is updated as\n\u03b8g(t) = \u03b8g(t\u2212 1) + \u2206(t)\nConsequently, \u03b8g(t) is broadcasted to all GPUs to update their local models.\nIt is worth noting that when block momentum and block learning rate are set as 0 and 1, BMUF becomes model averaging. We treat model averaging and BMUF as model averaging based methods."}, {"heading": "2.2. Moving Average and Exponential Moving Average", "text": "Averaged SGD is proposed to further accelerate the convergence speed of SGD. Averaged SGD leverages the moving average (MA) \u03b8\u0304 as the estimator of \u03b8\u2217 [17]:\n\u03b8\u0304t = 1\nt\nt\u2211\n\u03c4=1\n\u03b8\u03c4\nWhere \u03b8\u03c4 is computed by model averaging or BMUF. It is shown that \u03b8\u0304t can well converge to \u03b8 \u2217, with the large enough training dataset in single GPU training. It can be considered as a non-interference strategy that \u03b8\u0304t does not participate the main optimization process, and only takes effect after the end of entire optimization. However, for the parallel training implementation, each \u03b8\u03c4 is computed by model averaging and BMUF with multiple models, and moving average model \u03b8\u0304t does not well converge, compared with single GPU training.\nModel averaging based methods are employed in parallel training of large scale dataset, because of their faster convergence, and especially no-degradation implementation of BMUF. But combination of model averaged based methods and moving average does not match the expectation of further enhance performance and it is presented as\n\u03b8\u0304gt = 1\nt\nt\u2211\n\u03c4=1\n\u03b8g\u03c4\nThe weight of each \u03b8gt is equal in moving average method regardless the effect of temporal order. But t closer to the end of training achieve higher accuracy in the model averaging based approach, and thus it should be with more proportion in final \u03b8\u0304g. As a result, exponential moving average(EMA) is appropriate, which the weight for each older parameters decrease exponentially, and never reaching zero. After moving average based methods, the EMA parameters are updated recursively as\n\u03b8\u0302gt = \u03b1\u03b8\u0302gt\u22121 + (1\u2212 \u03b1)\u03b8gt\nHere \u03b1 represents the degree of weight decrease, and called exponential updating rate. EMA is also a non-interference training strategy that is implemented easily, as the updated model is not broadcasted. Therefore, there is no need to add extra learning rate updating approach, as it can be appended to existing training procedure directly."}, {"heading": "3. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "3.1. Training Data", "text": "In order to present the performance of our proposed method, we trained acoustic model for LVCSR. A large quantity of labeled data is needed for training a more accurate acoustic model. We collect the 17000 hours labeled data from Shenma voice search, which is one of the most popular mobile search\nengines in China. The dataset is created from anonymous online users\u2019 search queries in Mandarin, and all audio file\u2019s sampling rate is 16kHz, recorded by mobile phones. This dataset consists of many different conditions, such as diverse noise even low signal-to-noise, babble, dialects, accents, hesitation and so on. The dataset is divided into training set, validation set and test set, and the quantity of them is shown in Table 1. The three sets are split according to speakers, in order to avoid utterances of same speaker appearing in three sets simultaneously. The overfitting can be prevented in time, if there is a apparent gap between the frame error rate (FER) of training and validation set."}, {"heading": "3.2. Experimental setup", "text": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20]. Shenma voice search is a streaming service that intermediate recognition results displayed while users are still speaking. So as for online recognition in real time, we prefer unidirectional LSTM model rather than bidirectional one. Thus, the parallel training procedure is unidirectional LSTM-based.\nA 28-dimensional filter bank feature is extracted for each frame, and is concatenated with first and second order difference as the final input of the network. The architecture we trained consists of two LSTM layers with sigmoid activation function, followed by a full-connection layer. The out layer is a softmax layer with 11088 hidden markov model (HMM) tied-states as output classes, the loss function is cross-entropy (CE). The performance metric of the system in Mandarin is reported with character error rate (CER). The alignment of frame-level ground truth is obtained by GMM-HMM system. Mini-batched SGD is utilized with momentum trick and the network is trained for a total of 4 epochs. The block learning rate and block momentum of BMUF are set as 1 and 0.9. 5-gram language model is leveraged in decoder, and the vocabulary size is as large as 760000.\nEMA method is proposed for parallel training problem. In our training system, it is employed on the MPI-based HPC cluster where 8 GPUs are used to train neural networkmodels. Each GPU processes non-overlap subset split from the entire large scale dataset in parallel.\nLocal models from distributed workers synchronize with each other in decentralized way. In the traditional model av-\neraging and BMUF method, a parameter server waits for all workers to send their local models, aggregate them, and send the updated model to all workers. Computing resource of workers is wasted until aggregation of the parameter server done. Decentralized method makes full use of computing resource. There is no centralized parameter server, and peer to peer communication is used to transmit local models between workers. Local model \u03b8i of i-th worker in N workers cluster is split to N pieces \u03b8i,j j = 1 \u00b7 \u00b7 \u00b7N , and send to corresponding worker. In the aggregation phase, j-th worker computed N splits of model \u03b8i,j i = 1 \u00b7 \u00b7 \u00b7N and send updated model \u03b8\u0304gj back to workers. As a result, all workers participate in aggregation and no computing resource is dissipated. It is significant to promote training efficiency, when the size of neural network model is too large. The EMA model is also updated additionally, but not broadcasting it.\nBesides, frame stacking leads to reduce the computation and training time dramatically [21]. Frames are stacked so as that the network sees multiple frames at a time. The super frame after stacking is the input feature of the network and it contains abundant information. As a result, 3 frames are stacked without overlapping in the training procedure."}, {"heading": "3.3. Results", "text": "The test set including about 9000 samples contains various real world conditions. It simulates the majority of user scenarios, and could well evaluate the performance of a trained model. BMUF based approach, which has no worse performance than the single-GPU training procedure, is the baseline of experiments. The results of MA and EMA methods on the basis of BMUF are presented, and we call them MA-based methods.\nSince the EMA is a non-interference method, the performance can not be evaluated with the real-time FER. Therefore, the FER on validation sets are computed after every epoch. In order to present the decoding performance of MAbased methods, we extract 4 temporary models from each epoch to visualize the degradation of CER. FER curves of LSTM models trained with BMUF, MA and EMA methods are shown in Figure 1. It is significant that the frame accuracy of MA-based methods are higher than those of BMUF. Frame accuracies between MA-based methods have slight difference. Though the EMA only perform better than MA slightly on FER, there is an obvious difference between the CER of EMA and MA, as shown in Figure 2 which illustrates CER curves of different models after decoding. It demonstrates that decoding result of EMA is always much better than that of BMUF, but that of MA fluctuates greatly, and even higher than that of BMUF sometimes. From the Table 2 which shows the CER of final models trained from three methods, the superiority of EMA over the others can be also observed. EMA method achieves about relative 3.9% CER reduction on test set, while MA method only achieves relative 2.1% CER\nreduction.\nMoreover, the CER of final DNN models with 8 layers are also presented in Table 2, and the CER of EMA method decreases relative 8.4% compared with baseline. Therefore, more accuracy models are trained with large-scale parallel training using EMA method, and it is more stable than MA method."}, {"heading": "4. CONCLUSION", "text": "The exponential moving average method is proposed in this paper for multi-GPUs cluster parallel training with almost linear speedup. It is demonstrated that unidirectional LSTM and DNN models trained with EMA method have better decoding results than that of BMUF and traditional moving average methods for large vocabulary continues speech recognition in Mandarin. Our future work includes 1) Employing this method to CNN, Connectionist Temporal Classification\n(CTC), attention-based neural networks and other hybrid deep neural network architecture; 2) Extending this method from frame-wise discriminative training to sequence discriminative training such as maximum mutual information (MMI) and segmental Minimum Bayes-Risk (sMBR); 3) Develop more approaches for parallel training with better performance."}, {"heading": "5. REFERENCES", "text": "[1] Alex Graves and Navdeep Jaitly, \u201cTowards end-to-end\nspeech recognition with recurrent neural networks.,\u201d in ICML, 2014, vol. 14, pp. 1764\u20131772.\n[2] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-\nhamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.\n[3] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl\nCase, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al., \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d arXiv preprint arXiv:1512.02595, 2015.\n[4] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems, 2012, pp. 1097\u20131105.\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.\n[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.\n[7] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\nAbdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[8] Alex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645\u20136649.\n[9] Hasim Sak, AndrewW Senior, and Franc\u0327oise Beaufays,\n\u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling.,\u201d in Interspeech, 2014, pp. 338\u2013342.\n[10] Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,\nSanjeev Khudanpur, and James Glass, \u201cHighway long short-term memory rnns for distant speech recognition,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5755\u20135759.\n[11] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,\nMatthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al., \u201cLarge scale distributed deep networks,\u201d in Advances in neural information processing systems, 2012, pp. 1223\u20131231.\n[12] Shanshan Zhang, Ce Zhang, Zhao You, Rong Zheng,\nand Bo Xu, \u201cAsynchronous stochastic gradient descent for dnn training,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663.\n[13] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal\nJozefowicz, \u201cRevisiting distributed synchronous sgd,\u201d arXiv preprint arXiv:1604.00981, 2016.\n[14] Ryan McDonald, Keith Hall, and Gideon Mann, \u201cDis-\ntributed training strategies for the structured perceptron,\u201d in Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.\n[15] Martin Zinkevich, Markus Weimer, Lihong Li, and\nAlex J Smola, \u201cParallelized stochastic gradient descent,\u201d in Advances in neural information processing systems, 2010, pp. 2595\u20132603.\n[16] Kai Chen and Qiang Huo, \u201cScalable training of deep\nlearning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.\n[17] Boris T Polyak and Anatoli B Juditsky, \u201cAcceleration of\nstochastic approximation by averaging,\u201d SIAM Journal\non Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992.\n[18] Wei Xu, \u201cTowards optimal one pass large scale learn-\ning with averaged stochastic gradient descent,\u201d arXiv preprint arXiv:1107.2490, 2011.\n[19] Michiel Hermans and Benjamin Schrauwen, \u201cTraining\nand analysing deep recurrent neural networks,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.\n[20] Has\u0327im Sak, Fe\u0301lix de Chaumont Quitry, Tara Sainath,\nKanishka Rao, et al., \u201cAcoustic modelling with cd-ctcsmbr lstm rnns,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.\n[21] Has\u0327im Sak, Andrew Senior, Kanishka Rao, and\nFranc\u0327oise Beaufays, \u201cFast and accurate recurrent neural network acoustic models for speech recognition,\u201d arXiv preprint arXiv:1507.06947, 2015."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "ICML, 2014, vol. 14, pp. 1764\u20131772.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four  research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "AndrewW Senior", "Fran\u00e7oise Beaufays"], "venue": "Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway long short-term memory rnns for distant speech recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yaco", "Sanjeev Khudanpur", "James Glass"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5755\u20135759.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2012, pp. 1223\u20131231.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous stochastic gradient descent for dnn training", "author": ["Shanshan Zhang", "Ce Zhang", "Zhao You", "Rong Zheng", "Bo Xu"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Revisiting distributed synchronous sgd", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola"], "venue": "Advances in neural information processing systems, 2010, pp. 2595\u20132603.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering", "author": ["Kai Chen", "Qiang Huo"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal  on Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["Wei Xu"], "venue": "arXiv preprint arXiv:1107.2490, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Acoustic modelling with cd-ctcsmbr lstm rnns", "author": ["Ha\u015fim Sak", "F\u00e9lix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 142, "endOffset": 151}, {"referenceID": 1, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 142, "endOffset": 151}, {"referenceID": 2, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 142, "endOffset": 151}, {"referenceID": 3, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 4, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 6, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 7, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 8, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 9, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 10, "context": "Asynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy.", "startOffset": 41, "endOffset": 49}, {"referenceID": 11, "context": "Asynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy.", "startOffset": 41, "endOffset": 49}, {"referenceID": 12, "context": "Besides, synchronous SGD is another positive effort, where the parameter server waits for every workers to finish their computation and send their local models to it, and then it sends updated model back to all workers [13].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "Model averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15].", "startOffset": 137, "endOffset": 145}, {"referenceID": 14, "context": "Model averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15].", "startOffset": 137, "endOffset": 145}, {"referenceID": 15, "context": "It can achieve improvement or nodegradation of recognition performance compared with minibatch SGD on single GPU [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "It is demonstrated that the performance of moving average of the parameters obtained by SGD is as good as that of the parameters which minimize the empirical cost, and moving average parameters can be used as the estimator of them, if there are enough training samples [17].", "startOffset": 269, "endOffset": 273}, {"referenceID": 17, "context": "One pass learning is then proposed, and it is the combination of learning rate schedule and averaged SGD using moving average [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "For model averaging method, all local models are synchronized and averaged, and then aggregated model \u03b8\u0304(t) is sent back to GPUs [14, 15].", "startOffset": 129, "endOffset": 137}, {"referenceID": 14, "context": "For model averaging method, all local models are synchronized and averaged, and then aggregated model \u03b8\u0304(t) is sent back to GPUs [14, 15].", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": "Averaged SGD leverages the moving average (MA) \u03b8\u0304 as the estimator of \u03b8\u2217 [17]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 19, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 20, "context": "Besides, frame stacking leads to reduce the computation and training time dramatically [21].", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning currently. We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training.", "creator": "LaTeX with hyperref package"}}}