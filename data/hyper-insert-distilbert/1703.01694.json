{"id": "1703.01694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Word forms - not just their lengths- are optimized for efficient communication", "abstract": "by the simplest inverse relationship between the length of copying a translated word and the frequency of its initial use, first identified by g. k. moritz zipf in 1935, perhaps is a classic systematic empirical law construct that holds across a wide range of human sign languages. we demonstrate indirectly that length is one prominent aspect of a substantial much more general property component of words : measures how truly distinctive artifacts they are present with respect proportion to other words utilized in a language. speech distinctiveness correlation plays just a potential critical role commonly in recognizing words especially in performing fluent spanish speech, robust in that it likely reflects the strength necessary of potential competitors strategy when selecting the preferred best recognizable candidate for executing an perceived ambiguous signal. phonological information - content, basically a measure index of a word's average string comprehension probability under a dynamic statistical model of a speech language's sound or character sequences, concisely captures cellular distinctiveness. examining large - number scale corpora from 13 spoken languages, we firstly find that distinctiveness significantly outperforms word length as a dynamic predictor of audio frequency. this similar finding apparently provides evidence compelling that listeners'processing data constraints shape fine - grained aspects of actual word sense forms across adjacent languages.", "histories": [["v1", "Mon, 6 Mar 2017 00:38:51 GMT  (3231kb,D)", "https://arxiv.org/abs/1703.01694v1", "14 pages, 7 figures"], ["v2", "Wed, 31 May 2017 18:40:08 GMT  (3447kb,D)", "http://arxiv.org/abs/1703.01694v2", "16 pages, 8 figures"]], "COMMENTS": "14 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stephan c meylan", "thomas l griffiths"], "accepted": false, "id": "1703.01694"}, "pdf": {"name": "1703.01694.pdf", "metadata": {"source": "CRF", "title": "Word forms\u2014not just their lengths\u2014are optimized for efficient communication", "authors": ["Stephan C. Meylan", "Thomas L. Griffiths"], "emails": [], "sections": [{"heading": null, "text": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133]. How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136]. Prominent among these regularities is the wellknown relationship between word length and frequency: across languages, frequently-used words tend to be short [7]. In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8]. While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.\nHere, we propose a generalization of Zipf\u2019s analysis and present two possible listener-focused explanations. We show that a word\u2019s frequency is inversely related to its distinctiveness\u2014how easily it can be identified as the intended message for a given speech signal. While speakers prefer easier-to-produce less distinctive forms, they are constrained by listeners\u2019 need for sufficiently distinctive forms to differentiate each word from others, especially if a speaker\u2019s intended word has higher-frequency competitors. If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d). We define a statistical measure of distinctiveness that succinctly captures the diagnosticity of a word form by assessing the aggregate strength of competitors in the language. We then show that distinctiveness should be inversely related to frequency, if languages are constructed to equalize error rates for low and high frequency words.\nImportantly, distinctiveness subsumes Zipf\u2019s observation regarding the relationship of length and frequency as a special case. Length is a na\u0308\u0131ve approximation of the distinctiveness of a word form insofar as longer strings are simply less probable. We demonstrate that a more comprehensive measure of distinctiveness that takes into account the sound-to-sound (phoneme-to-phoneme or letter-to-letter) sequences in a language accounts for significantly more frequency-related variance\nar X\niv :1\n70 3.\n01 69\n4v 2\n[ cs\n.C L\n] 3\n1 M\nay 2\nthan does length across a broad sample of natural languages. This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16]."}, {"heading": "Model", "text": "We define a probabilistic language model to characterize the distinctiveness of word forms in terms of their constituent sound-to-sound transitions. This model can be fit using a large written sample of a particular language. The starting point for the model is formalizing the task of the listener as a rational statistical inference."}, {"heading": "Bayesian inference and distinctiveness", "text": "Upon hearing a string of sounds s, a listener has to infer what word w was intended by the speaker. This can be formulated as a problem of Bayesian inference. The listener should calculate a posterior distribution P (w|s) over words based on the sounds. Applying Bayes\u2019 rule, this is given by\nP (w|s) = P (s|w)P (w) P (s)\n(1)\nwhere P (s|w) is the probability of hearing s if w is the intended word, P (w) is the prior probability of the word w intended by the speaker, and P (s) is the probability of hearing s.\nAssuming that sounds are produced faithfully, such that P (sw|w) is close to 1 for a particular string sw for each w and close to 0 otherwise, we obtain the approximation\nP (w|sw) \u2248 P (w)\nP (sw) (2)\nwhich expresses the probability that the word w is correctly identified as a function of its normalized frequency, P (w), and the probability of the string sw in the language, P (sw). We define the distinctiveness of a word to be inversely related to P (sw): intuitively, a word that shares the same sound sequences with many other words is necessarily less distinctive.\nFollowing this logic, we use the phonological information content (PIC) of a word to measure the distinctiveness of its phoneme-to-phoneme transitions [17], or its approximation in character-tocharacter transitions. Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word. To obtain a compact representation of the probabilities of various phone or character sequences in a language we estimate an n-phone or n-character model (analogous to an n-gram model over words [21] but computed over individual phonemes or characters) from a corpus sample. To support a stronger test of the relationship between distinctiveness and frequency, we avoid the circularity that more probable words necessarily contain more probable sequences by estimating the transition probabilities using the type inventory (unique words) in the language. For the smaller datasets (those from the OPUS corpus) we also present the results for a model with token-weighted phonological transitions, though we note that interpreting the correlations obtained from the weighted model is challenging given this circularity.\nUnder this model, the phonological information content PIC(w) of a word is defined as:\nPIC(w) = \u2212 logP (sw) (3) = \u2212 logP (l1, . . . , l|sw|) for l \u2208 sw (4)\n= \u2212 |sw|\u2211 i=1 logP (li|li\u2212(n\u22121), . . . , li\u22121). (5)\nwhere l are the phonemes or letters that comprise the sequence sw, |sw| is the length of sw, and n is the order of the n-phoneme or n-character model (n=5 in the current analyses to avoid overfitting). PIC does not explicitly account for the morphemic components of a word (sub-word meaningful units, like the English prefix un-), rather the the relative prevalence of morphemes is reflected in the character-to-character or phoneme-to-phoneme transition statistics."}, {"heading": "Length and distinctiveness", "text": "Intuitively, short words have more similar competitors and are hence easier to confuse with other words, while long words have fewer neighbors (Fig. 1). Per Zipf\u2019s formulation, a signal which is too short may be ambiguous, and a listener is less likely to infer a speaker\u2019s intended meaning [8]. We argue that it is not length per se which drives this effect, but rather the contribution of length to distinctiveness.\nWord length is an important determinant of string probability: under a probabilistic treatment, a string that is one sound longer is a longer sequence of events, and hence of equal or lesser probability. In fact, length is strictly proportional to the probability (or log probability) of a string under a \u201cmonkeys-on-typewriters\u201d model of letter-to-letter transitions such as that proposed by Mandelbrot [23] and further explored by Miller [10].\nUnder this naive statistical model, symbols (sounds, or written characters approximating those sounds) are equiprobable and independent: P (li) = 1/v, where v is the number of symbols in the symbol set of the language (its alphabet or phoneme inventory, for example 26 letters in English). If words were composed of equiprobable, independently-drawn symbols, PIC(w) would simply be |sw| log v.\nHowever, this approximation fails to capture regularities in the lexical substructure observed in natural languages in two obvious ways. First, symbols are not equiprobable: across the word types in the English lexicon, w is substantially less common than e. Second, the sound symbols are not statistically independent: the sound t in English is followed more frequently by i or e and very rarely\u2014if ever\u2014by b or g. People have rich knowledge of the relative prominence of these sequences in their respective languages\u2014just as they have rich knowledge of inter-word statistical dependencies\u2014 and can call upon this knowledge in spoken word recognition [24, 25]. Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].\nWhen the probability of sub-word sequences is taken into account, sequences of the same length can vary markedly in distinctiveness: while xylophone and something are both nine letters long, the latter is comprised of significantly less common subsequences. When phonological information content is computed with a more accurate model of the structure of words, it differs from length because it captures deviations from independence (see Table 1)."}, {"heading": "Benefits of distinctiveness", "text": "Conceiving of words in terms of their distinctiveness has several benefits compared to length. First, distinctiveness (as measured by PIC) is a much more fine-grained measure of word form complexity, and generates predictions contrary to word length in many cases. A shorter word may contain a relatively low probability phoneme sequence (e.g., depth, 4 phonemes/5 letters, PIC = 17.86 bits under a type-weighted model from the Google Books 2012 English corpus), while a longer word may contain a higher probability, less informative sequence (e.g., ground, 5 phonemes/6 letters, PIC = 12.85 bits under that same model).\nSecond, PIC is closely related to metrics of lexical neighborhood density used in psycholinguistic models of spoken word recognition. Neighborhood density reflects how many words have a similar form to a given word; while proposals vary on how to best measure neighborhood density, they share the intuition that words with more similar word forms (or \u201cneighbors\u201d) are harder to recognize because there are more competitors consistent with a given signal. PIC is formally very similar to frequency-weighted neighborhood density [30], however it measures the number of competitors at each successive phone\u2014a feature consistent with empirical results suggesting incremental phoneme-by-phoneme processing in some cases [29]. PIC thus constitutes a more detailed measure of neighborhood density than the canonical measure of Coltheart\u2019s N [31], the number of words within a edit distance of one of the target word. In particular, PIC is sensitive to competition effects from hearing the partial word form: while there are approximately 40 possible candidates upon hearing /Ti/ in thesis in a large sample of English, neighborhood density as assessed by Coltheart\u2019s N is much lower (the only competitors by this criterion are theses and Theseus). We return to the question of whether treating spoken word recognition as a purely sequential prediction task is an appropriate simplifying assumption in the Discussion."}, {"heading": "A relationship between distinctiveness and frequency", "text": "We provide two listener-centric explanations as to why there might be an inverse relationship between distinctiveness and frequency. First, distinctiveness provides a way to measure the effort that listeners expend in comprehending speakers; total comprehension effort is minimized when the most frequent words are the least distinctive. Zipf\u2019s original explanation for the inverse relationship be-\ntween the frequency of a word and its length was based on the idea that languages are shaped by the desire of speakers to expend the least effort in producing words. If longer strings are more effortful to produce, it makes sense that they should be associated with less frequent words. However, we can imagine a similar argument being applied on the part of listeners: that languages are shaped by the desire of listeners to minimize the effort they expend in comprehending speakers. Distinctiveness more accurately indexes comprehension effort, above and beyond word length.\nSecond, an inverse relationship between distinctiveness and frequency can also be derived from an invariance argument. Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334]. A similar invariance principle that might be relevant at the level of the words themselves is Uniform Recognizability: that the probability any word is successfully recognized is approximately the same. Under Equation 2, the probability of successfully recognizing w from its associated string sw is P (w)/P (sw). Making this constant across words means that we should expect P (w) and P (sw) to be directly related, and hence P (w) and PIC(w) to be negatively correlated.\nTo evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38]. We limit our analysis to languages with phonemic scripts. Following Piantadosi et al. [19], we compute correlations over the 25,000 most frequent types in each language; following that same study we additionally compute the correlation between PIC and average incontext information content (trigram surprisal) over the same set of words."}, {"heading": "Results", "text": "We investigate the correlation between distinctiveness and frequency in large corpus samples (43m to 266b words) in 13 languages, across three large-scale datasets. In each case we compute the frequency and in-context information content (mean trigram surprisal, or negative mean log conditional probability under a trigram model) for each word, and measure its PIC under an n-phoneme and n-character model estimated from distinct words in the language (for additional details, see Methods). For the OPUS datasets, we also compute PIC under the analogous token-weighted models. If length is a reduced-resolution approximation of distinctiveness, then we expect an even stronger relationship between distinctiveness\u2014as measured by PIC\u2014and frequency across languages."}, {"heading": "Cross-Linguistic Results", "text": "Following the methodology adopted in [19], we examine the correlation between word-level predictors (frequency and in-context information content) and metric of structural form (word length or PIC) for the 25,000 most frequent types in each language. Unlike [19], we limit our analysis to indictionary types, thereby excluding person names, place names, and acronyms from the analysis. We obtain a systematically stronger negative correlation between frequency and distinctiveness, as measured by type-weighted model PIC, than frequency and word length (Fig. 3). Even holding word length constant, distinctiveness explains substantial additional variance in word frequency (Fig. 2). Building the model from phonemic transcriptions, this pattern holds in 11 of 11 languages in the Google 1T datasets, 6 of 7 languages from Google Books 2012, and all 13 languages from the 2013 OPUS corpus. Building the model from raw characters\u2014as an approximation of the phonological form\u2014this pattern holds in all cases. In many cases (10 of 11 languages in Google 1T, 3 of 7 in Google Books 2012, and 1 of 13 in OPUS) the partial correlation of PIC and frequency\u2014with word length partialed out\u2014is higher than the simple correlation of frequency and word length.\nThe obtained correlations are even stronger than those recently obtained between word length and average in-context information content [19].\nPIC computed under the token-weighted model demonstrates an even stronger correlation with frequency across the languages in the sample (Fig. 4). We caution against an overly strong interpretation of this result, however, in that this correlation is not significantly higher than that obtained when phonemic content is shuffled among word forms (maintaining length for each word) and PIC subsequently recomputed. In contrast, the correlation between frequency and PIC computed using a type-weighted model under the permuted dataset is no stronger than the correlation between frequency and length.\nA similar pattern of results emerges regardless of whether the type-weighted model is computed over characters or phonemes; the sole exception is the Russian corpus from Google Books 2012 where word length is a stronger predictor when the model is computed over phonemes. However, this dataset is an outlier in two notable ways. Russian shows the lowest correlation between frequencies obtained from Google Books and OPUS (Pearson\u2019s r = .48), as well as the lowest correlation between PIC estimates derived from Google Books and those derived from OPUS (Pearson\u2019s r = .63). Across languages, models built over phonemes and character transitions provide proportional estimates of PIC (Pearson\u2019s r between .789 and .919 across languages, median = .874). While more research is required to extend these findings beyond Germanic, Romance, and Slavic languages, Hebrew provides an important test of whether this relationship holds in languages with extensive nonconcatenative in addition to affixal morphology."}, {"heading": "Relationship to Preceding Context", "text": "While Piantadosi et al. [19] found that taking into account contextual predictability / information content (in the form of mean trigram surprisal in a dataset) better predicts word length than using a\nsimple frequency measure (operationalized as unigram surprisal, or negative log probability), we find a qualitatively different pattern of results for PIC. In the analysis above, we find that the correlation between frequency and PIC is higher than the correlation between mean trigram surprisal and PIC in all cases (Fig. 5, A). The correlation between frequency (negative log unigram probability) and PIC is also greater than the correlation between mean trigram surprisal and word length in all but one case (Fig. 5, B). We find substantially attenuated support for the principal claim in Piantadosi et al. [19], in that we find unigram frequencies better predict word length than does mean trigram surprisal (Fig. 5, C). This discrepancy, which we explore further in the Discussion, seems to reflect refinements in the list of lexical items analyzed, improvements in the data preparation and analysis methodology in the current work, and underlying issues in the construct validity of average information content for word forms in languages with rich morphology."}, {"heading": "Discussion", "text": "The relationship between word length and frequency is one of the most robust empirical findings regarding the structure of natural languages. Treating word recognition in terms of Bayesian inference, we show that the requirement for successful recognition not only motivates the relationship between frequency and length, but generates even stronger predictions regarding the relationship between frequency and the probability of the phoneme sequences (and their approximation in terms of character sequences) that make up word forms. Analyses of large-scale corpora from 13 languages across three datasets (including the linguistic content of web pages, books, and movie subtitles) substantiate these claims. In this section we consider some of the implications of these results, as well as caveats and future directions for research.\nThe approach that we have taken in this paper potentially provides a link between the psycholin-\nguistic processes at work in producing and perceiving speech and the factors that shape human languages. Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341]. On longer timescales, variants with more probable forms may become the dominant form in the language, e.g., English lunch displaced luncheon in the 19th and 20th centuries [42] (see Mahowald et al. [43] for additional examples).\nPIC provides a metric by which phonemic and phonetic lexical variants may be compared while capturing variation beyond that of word length. Whereas word length can only reflect deletion and epenthesis (the insertion of phonemic material), PIC is sensitive to changes that maintain the same length, including assimilation (a sound becoming more similar to a neighboring sound), dissimilation (a sound becoming less similar to a neighboring sound), lenition (a consonant taking on more vowel-like qualities), and metathesis (transposition of proximal sounds). The change from Middle English aks to ask, for example, is reflected in a change in PIC, though both have the same length. Paradigmatic sound changes operating across many words\u2014or indeed an entire language (e.g. phonemic mergers)\u2014can also be characterized in terms of changes in PIC, similar to entropy-based estimates of functional load [44, 45].\nThe formulation of the relationship between word form distinctiveness and frequency that we present above also makes several testable predictions regarding the relationship between frequency, length, PIC, and how long a word has been in a language. First, we expect words that have recently entered a language such as neologisms and loanwords from other languages to exhibit a relatively weak relationship between frequency and their structural form, regardless of whether that structural form is characterized in terms of either PIC or length. In the case of neologisms, semantic transparency of the inputs initially determines structural form; in the case of loanwords, the it is heavily influenced by the phonological form in the language from which the word is borrowed. Second, we expect a word\u2019s structural form comes to better reflect frequency over the course of time, in that changes that improve communicative efficiency can proceed gradually, in part because variation is constrained by the need to maintain intelligibility within a population of speakers. Third, frequency should demonstrate a stronger correlation with PIC than with length on shorter timescales, in that PIC is sensitive to more subtle changes in the word form than the change in length. For example, deletion of a phoneme often comes only after an intermediate stage where that\nphoneme is weakened; PIC quantitatively reflects the intermediate weakening, whereas word length changes when the phoneme is deleted entirely.\nIn order to test these hypotheses, we obtain the date of first appearance for 31,027 English types from the Oxford English Dictionary. We stratify these dates by century, assigning pre-700 (corresponding to proto-Old English) words to the 7th century, and compute Spearman\u2019s rank correlation coefficient against current word frequency for the set of words corresponding to each century. The correlations we obtain between PIC and frequency are stronger than those between length and frequency for words entering English between the 13th and 20th century (Fig. 6). This pattern of results provides qualitative support for all three hypotheses: an overall weaker relationship between frequency and structural form among recent words, an increasingly strong relationship for words that have been in the language for a longer interval, and a stronger relationship between PIC and frequency for those words that have entered the language more recently. With additional datasets we hope to characterize how changes in frequency relate to specific changes in word form.\nIn contrast to the original formulation by Zipf which focused on minimization of speaker effort, in-context predictability as presented in Piantadosi et al. [19] and word form distinctiveness both implicate listener-oriented pressures in the relationship between frequency and word form. The two proposals make contrasting predictions regarding what factors are most relevant to generalizing the relationship, however. In-context predictability generalizes the notion of a listener\u2019s prior beliefs regarding the probability of a given word, replacing frequency with predictability. Distinctiveness provides a more detailed characterization of word form complexity. In principle, both proposals could be true: the strongest relationship could be between in-context predictability and word form distinctiveness, though we do not find empirical support for this in the datasets analyzed here. Instead, as noted in the results we find in the current work substantively attenuated support for the relationship between word length and average in-context predictability (Fig. 5). Here we further investigate the source and significance of this discrepancy with the findings of Piantadosi et al. [19].\nUsing the same list of words and in-context information content estimates from Piantadosi et al. [19], we classify each word into one of the three categories: those found in the relevant dictionary (by testing for membership in the corresponding GNU Aspell dictionary for each language), those found in English (by testing for membership in the English Aspell dictionary for words in non-English languages), and label the remainder as out-of-dictionary. We designate those words found both in the language-specific dictionary as well as English, e.g. Spanish pan, as in-dictionary items. The proportion of items found in each category for each language, as well as example classifications from Spanish, are presented in the inset in Fig. 8, center.\nUnder this analysis, within-group correlations for all three groups are substantially lower than the aggregate correlations reported in Piantadosi et al. [19] (Fig. 7). Instead, the high global correlation between mean in-context information content and word length emerges from the inclusion of out-ofdictionary items and (for languages other than English) items from English, which are both more predictable and shorter than in-dictionary words (Figure 8, right). The global correlations between frequency and word length found in that study were depressed by out-of-dictionary items, which were shorter\u2014yet oddly less frequent\u2014than in-dictionary terms (Figure 8, left).\nBecause [19] represented word forms as closest ASCII equivalents (e.g., manana for Spanish man\u0303ana), some words cannot be found in the relevant dictionary. This means that for the above analysis some lexical items that should have been classified as in-dictionary have been classified as out-of-dictionary, with unknown implications for the within-group correlations. Because there is no way to systematically restore these word forms, we compare the correlation between in-context information content (mean trigram surprisal, which they also refer to as information content) and word length, as well as unigram surprisal (negative log normalized frequency) and word length using estimates from the main analysis.\nUsing 25,000 in-dictionary words\u2014and respecting the original character encoding\u2014we find that frequency is a better predictor of word length than in-context information content in most languages among those words that appear in the corresponding language\u2019s dictionary. Key exceptions, however,\nare consistent with the findings of Piantadosi et al. [19]: English and French in the Google 1T and English and German in Google Books 2012 show a stronger correlation of mean trigram surprisal and word length (Fig. 5, C). This may reflect that strong correlations only emerge with unbiased estimates of in-context information content, which may in turn only be obtained in very large datasets: English Google 1T and Books 2012 corpora are at least five times larger than the next largest dataset. In the case of Google 1T, this correlation is higher than that of frequency and PIC, leaving open the possibility that word length better reflects in-context information content while PIC more strongly reflects frequency.\nWhile the heterogeneous pattern of results in which structural form is more closely linked to frequency in some languages and to in-context information content in others may reflect genuine cross-linguistic differences, in the current section we argue that computing in-context information content using n-gram models may be theoretically unsound in morphologically-rich languages. Consider for example that whereas an n-gram model for English would have entries for a handful of forms for the verb sell (e.g., sell, sells, sold, selling), an n-gram model for Spanish, a language with much richer morophology for verbs, needs to have many more entries for the corresponding verb vender owing to the combinatorial space of possible conjugations and object clitics (\u2248160 in Google Books 2012). Aggregating across inflected forms, the forms of vender and sell are approximately equally common, yet estimates of mean in-context surprisal may are more likely to be biased in Spanish given the sparsity of certain forms (e.g., ve\u0301ndeselas, corresponding to English imperative you sell them that). Second, if an analysis is conducted on the top n most frequent word forms, high frequency lemmas take up more of the list because they are spread across a larger number of inflected entries. Depending on what parts of speech have high morphological complexity, substantive differences may emerge in the composition of the vocabulary under analysis across languages. Future work will need to investigate how morphological complexity interacts with in-context information content across languages, and whether different tokenization (segmentation) processes should be used, for example determining whether object clitics should be treated as separate words in Spanish. For now, we note that we obtain the same pattern of results for PIC and frequency when we restrict our analysis to morphologically simple word forms for those languages with morphological annotation in the CELEX database [46] (English, German, and Dutch).\nWe make two key simplifications in the current work with respect to the measurement of PIC: we use a relatively simple, purely sequential representation of the phonemes that comprise a word and we compute PIC with respect to the true preceding context within each word. Using a richer\nhierarchical representation of word form structure may account for regularities than an n-phoneme model cannot, leading to a modest improvement in predicting a held-out set of words [47]. Treating spoken word recognition as sequential recognition of phonemes may also discount regularities in morophological structure, though in many cases such regularities are captured implicitly in the transition probabilities among phonemes. Adapting more elaborate models of lexical structure\u2014 both morphological and phonological\u2014may lead to improved estimates of word form probability in a language.\nWith respect to the second simplification, listeners do not have access to the true identity of preceding phonemes, and instead likely marginalize over a distribution of preceding as well as following sounds within a word in the process of word recognition [48]. This may mean that listeners have comparatively peaked estimates of the preceding sounds only in the case of longer words,, such that the relationship between frequency and PIC as computed here is weaker for short words. In future work we intend to investigate whether a measure of uncertainty that takes this into account can explain additional variance in word frequency.\nData Issues\nCorpus analyses of the scale used here necessarily contain some degree of noisy data. The choice of texts, methods for identifying content (i.e., excluding page numbers, tables, publisher notes etc.), optical character recognition procedure, and tokenization all influence the obtained dataset. Additional data preparation decisions such as text encoding, treatment of punctuation, and treatment of upper and lower case forms can all have pronounced effects on lexical statistics.\nDespite these challenges, these datasets constitute the best resource available for computing average in-context information content for lower frequency words, given that their huge size provides better evidence of the range of contexts in which these words appear. In the course of the analysis\nwe found two notable issues in both the Google Books 2012 and Google 1T datasets; we include a brief note on each here to alert others using this or similar corpora.\nFirst, we found extensive evidence of issues with word segmentation in the Hebrew corpus of Google Books 2012 (for details of the rule-based tokenization procedure see Lin et al. [49]). We find a high proportion of bound morphemes (\u05dc\u05e9, \u05ea\u05d0, \u05dc, \u05e9, \u05d1, \u05d5, \u05d4) listed as separate lexical items in the raw data, in total accounting for approximately 20% of the total unigram probability mass.\nSecond, the web-based nature of the Google 1T corpora strongly influences certain average incontext surprisal estimates. For example, \u201cRomanian\u201d has very low trigram surprisal in the Google 1T corpus because it is overwhelmingly encountered in language lists or lists of currencies (22% of instances appear in the context \u201cPolish Portuguese Romanian\u201d). Furthermore, the statistical properties of the everyday language environment of speakers may differ substantially from the corpora examined here. Nonetheless, these corpora constitute the best approximation to naturalistic language use of sufficient size to support lexicon-wide analysis, particularly the calculation of mean information content for relatively rare words. As more corpora are made available, we hope to extend the analyses presented above to a more diverse set of languages."}, {"heading": "Conclusion", "text": "The canonical inverse relationship between word length and frequency is a special case of an even broader relationship between word distinctiveness and frequency. Distinctiveness plays a crucial role in word recognition, capturing the strength of competing targets for a speech signal. Rational analysis reveals limitations on word forms: speakers can simplify and shorten words, but they are limited by listener\u2019s requirements for distinctive word forms for successful recognition."}, {"heading": "Methods", "text": ""}, {"heading": "Datasets for Frequency and Surprisal Estimates", "text": "The Google Web 1T datasets were downloaded from the Linguistic Data Consortium [35, 36]; the Google Books 2012 datasets were downloaded from storage.googleapis.com/books/ngrams/books/ datasetsv2.html [37], and OPUS (2013) from opensubtitles.org [38]. All n-grams with punctuationonly words were discarded, and punctuation appearing with other text, with the exception of apostrophes, was removed. We make the simplifying assumption that the tokenized orthographic forms correspond to psychologically salient words in the lexicon of speakers; while this assumption does not hold for all forms (e.g. German compound nouns), it holds for the vast majority of word forms in the analysis (see also our analysis of morphologically-simple forms in the Discussion, which also addresses this question). All characters were converted to lowercase using the relevant POSIX locale; US English and European Portuguese were used for English and Portuguese, respectively. In the case of Google Books 2012, records with part-of-speech tags were discarded, along with records from earlier than 1800. UTF-8 encoding was maintained throughout for all languages and datasets. Hebrew strings were represented with right-normalized forms. Counts were stored using ZS, a specialized file format for efficient retrieval of n-gram counts [50]."}, {"heading": "Estimating Sentential Information Content", "text": "Following Piantadosi et al. (2013) we analyze a word list constructed from the 25,000 most frequent words in each dataset. Token frequencies were computed from the 2013 release of the the OPUS subtitle corpus. For each language, the list of unique types were filtered by those words recognized by the UNIX utility Aspell for the relevant locale. After filtering, we computed the negative log unigram probability (proportional to log normalized frequency) for each word w along with the negative mean\nlog trigram probability across contexts, following [19], \u2212 1N \u2211N\ni=1 logP (W = w|C = ci), where ci is the context for the ith occurrence of w and N is the frequency of w in the dataset."}, {"heading": "Estimating Phonological Information Content", "text": "For the type-weighted models, a five-character transition model was estimated for each language using the 25,000 most frequent in-dictionary words also appearing in the corresponding OPUS subtitle corpus. For the token-weighted models (OPUS only), a five-character transition model was estimated using all in-dictionary tokens in the corresponding OPUS subtitle corpus. In both the tokenand type- weighted cases, we also produced a five-phone transition models for all languages with the exception of Hebrew using IPA transcriptions from an automatic speech synthesizer, eSpeak. Using IPA representations for words accounts for language-specific variations in orthographic conventions. For example, written Spanish includes accents only when the placement of prosodic stress cannot be deduced from more general rules in the language. Using an IPA transcription avoids the need for developing language-specific decisions, for example deciding whether \u2018a\u2019 vs. \u2018a\u0301\u2019 should be merged or kept as separate orthographic variants in Spanish. Loan words and acronyms can greatly affect the obtained transition probabilities, especially when the transitions observed in the type-weighted model (e.g., if the transitions in \u201cOkeechobee,\u201d \u201cman\u0303ana,\u201d and \u201cACLU\u201d are as heavily weighted in a phonotactic model of English as the transitions in \u201cthey\u201d and \u201cwill\u201d). To minimize these effects, we use only non-capitalized types present in Aspell dictionaries to build sound and character transition models for each language (with the exception of German, in which common nouns are capitalized). To avoid overfitting among higher order sequences, phone and character transition probabilities for the type-weighted models were computed with modified Kneser-Ney smoothing [51] with interpolation on orders 3, 4, and 5 using the SRILM toolkit [52]. Good-Turing smoothing was used for the token-weighted models. Each word\u2019s phonological probability was calculated as the product of the probabilities of each symbol given the preceding symbol string, including a start symbol ? , e.g., P (the) = P (t|?) \u00d7 P (h| ? t) \u00d7 P (e| ? th). The probability of the end symbol was omitted in that this appreciably inflates the PIC of short words under a type-weighted model (\u201cto\u201d is an unlikely/high-surprisal sequence because there are many words that begin with \u201cto\u201d)."}, {"heading": "Code availability", "text": "Our library for the rapid cleaning, manipulation, summarization, and querying of n-gram data is available at github.com/smeylan/ngrawk. Jupyter notebooks for the analyses presented here are available at github.com/smeylan/pic-analysis."}, {"heading": "Acknowledgements", "text": "This material is based upon work supported by the US National Science Foundation Graduate Research Fellowship under grant no. DGE-1106400 and NSF grant no. SMA-1228541. Special thanks to Steven Piantadosi for sharing materials and lexical information content estimates, helpful commentary on early drafts from Terry Regier, and members of the Computational Cognitive Science Lab at UC Berkeley for valuable discussion."}], "references": [{"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["JH Greenberg"], "venue": "JH Greenberg, editor, Universals of Human Language, pages 73\u2013113. MIT Press, Cambridge, MA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1963}, {"title": "The myth of language universals: language diversity and its importance for cognitive science", "author": ["N Evans", "SC Levinson"], "venue": "Behav Brain Sci, 32(5):429\u2013448", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale evidence of dependency length minimization in 37 languages", "author": ["R Futrell", "K Mahowald", "E Gibson"], "venue": "Proc. Natl. Acad. Sci. U.S.A., 112(33):10336\u201310341", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The faculty of language: what is it", "author": ["MD Hauser", "N Chomsky", "WT Fitch"], "venue": "who has it, and how did it evolve? Science, 298(5598):1569\u20131579", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Kinship categories across languages reflect general communicative principles", "author": ["C Kemp", "T Regier"], "venue": "Science, 336(6084): 1049\u20131054", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Language learners restructure their input to facilitate efficient communication", "author": ["M Fedzechkina", "TF Jaeger", "EL Newport"], "venue": "Proc. Natl. Acad. Sci. U.S.A., 109(44):17897\u201317902", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The Psychobiology of Language", "author": ["GK Zipf"], "venue": "Houghton-Mifflin", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1935}, {"title": "Human Behaviour and the Principle of Least-Effort", "author": ["G Zipf"], "venue": "Addison-Wesley, Cambridge, MA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1949}, {"title": "The Statistical Study of Literary Vocabulary", "author": ["G.U. Yule"], "venue": "Cambridge University Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1944}, {"title": "Some effects of intermittent silence", "author": ["GA Miller"], "venue": "American Journal of Psychology, 70:311\u2013314", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1957}, {"title": "Least effort and the origins of scaling in human language", "author": ["Ramon Ferrer i Cancho", "Ricard V. Sol"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Power laws for monkeys typing randomly: the case of unequal probabilities", "author": ["B. Conrad", "M. Mitzenmacher"], "venue": "IEEE Transactions on Information Theory, 50(7):1403\u20131414", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Zipf\u2019s word frequency law in natural language: A critical review and future directions", "author": ["S.T. Piantadosi"], "venue": "Psychonomic Bulletin & Review, 21(5):1112\u20131130", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Shortlist B: a Bayesian model of continuous speech recognition", "author": ["D Norris", "JM McQueen"], "venue": "Psychol Rev,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Probability and surprisal in auditory comprehension of morphologically complex", "author": ["LW Balling", "RH Baayen"], "venue": "words. Cognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A Performance Theory of Order and Constituency", "author": ["JA Hawkins"], "venue": "Cambridge University Press, Cambrdige, UK", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Using information content to predict phone deletion", "author": ["U Cohen Priva"], "venue": "Proceedings of the 27th West Coast Conference on Formal Linguistics, pages 90\u201398, Somerville, MA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Expectation-based syntactic comprehension", "author": ["R Levy"], "venue": "Cognition, 106(3):1126\u20131177", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Word lengths are optimized for efficient communication", "author": ["ST Piantadosi", "H Tily", "E Gibson"], "venue": "Proc. Natl. Acad. Sci. U.S.A., 108(9):3526\u20139", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "The effect of word predictability on reading time is logarithmic", "author": ["NJ Smith", "R Levy"], "venue": "Cognition, 128(3):302\u2013319", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["CD Manning", "H Sch\u00fctze"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "CLEARPOND: cross-linguistic easy-access resource for phonological and orthographic neighborhood densities", "author": ["V. Marian", "J. Bartolotti", "S. Chabal", "A. Shook"], "venue": "PLoS ONE, 7(8):e43230", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple games of strategy occurring in communication through natural languages", "author": ["B Mandelbrot"], "venue": "Transaction of the IRE Professional Group on Information Theory PGIT, 3(3):124\u2013137", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1954}, {"title": "Probabilistic phonotactics and neighborhood activation in spoken word recognition", "author": ["MS Vitevitch", "PA Luce"], "venue": "J Mem Lang, 40(3):374\u2013408", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Phonotactics", "author": ["PA Luce", "NR Large"], "venue": "density, and entropy in spoken word recognition. Lang Cognitive Proc, 16(5-6): 565\u2013581", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Processing interactions and lexical access during word recognition in continuous speech", "author": ["WD Marslen-Wilson", "A Welsh"], "venue": "Cognitive Psychology, 10(1):29 \u2013 63", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1978}, {"title": "Functional parallelism in spoken word-recognition", "author": ["WD Marslen-Wilson"], "venue": "Cognition, 25(1-2):71\u2013102", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1987}, {"title": "The locus of the effects of sentential-semantic context in spoken-word processing", "author": ["P Zwitserlood"], "venue": "Cognition, 32(1): 25\u201364", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1989}, {"title": "Eye movements as a window into real-time spoken language comprehension in natural contexts", "author": ["KM Eberhard", "MJ Spivey-Knowlton", "JC Sedivy", "MK Tanenhaus"], "venue": "J Psycholinguist Res, 24(6):409\u2013436", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Recognizing spoken words: The neighborhood activation model", "author": ["PA Luce", "DB Pisoni"], "venue": "Ear Hear, 19(1):1\u201336", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Access to the internal lexicon", "author": ["M Coltheart", "E Davelaar", "JT Jonasson", "D Besner"], "venue": "S Dornic, editor, Attention and Performance VI, pages 535\u2013555. Lawrence Erlbaum Associates", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1977}, {"title": "Entropy rate constancy in text", "author": ["D Genzel", "E Charniak"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 199\u2013206", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "The smooth signal redundancy hypothesis: a functional explanation for relationships between redundancy", "author": ["M Aylett", "A Turk"], "venue": "prosodic prominence, and duration in spontaneous speech. Lang Speech, 47(1):31\u201356", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Speakers optimize information density through syntactic reduction", "author": ["R Levy", "TF Jaeger"], "venue": "B Sch\u00f6lkopf, J Platt, and T Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 849\u2013856, Cambridge, MA", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Web 1T 5-gram", "author": ["T Brants", "A Franz"], "venue": "10 European Languages Version 1 LDC2009T25", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Web 1T 5-gram Version 1 LDC2006T13", "author": ["T Brants", "A Franz"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Quantitative analysis of culture using millions of digitized", "author": ["J Michel"], "venue": "books. Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Parallel data", "author": ["J Tiedemann"], "venue": "tools and interfaces in OPUS. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei", "author": ["M Aylett", "A Turk"], "venue": "The Journal of the Acoustical Society of America, 119(5):3048\u20133058", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Predictability effects on durations of content and function words in conversational english", "author": ["A Bell", "JM Brenier", "M Gregory", "C Girand", "D Jurafsky"], "venue": "Journal of Memory and Language, 60(1):92\u2013111", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Why reduce? phonological neighborhood density and phonetic reduction in spontaneous speech", "author": ["S Gahl", "Y Yao", "K Johnson"], "venue": "Journal of Memory and Language, 66(4):789\u2013806", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Info/information theory: speakers choose shorter words in predictive contexts", "author": ["K. Mahowald", "E. Fedorenko", "S.T. Piantadosi", "E. Gibson"], "venue": "Cognition,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Quantifying the functional load of phonemic oppositions", "author": ["D Surendran", "P Niyogi"], "venue": "distinctive features, and suprasegmentals. Amsterdam Studies in the Theory and History of Linguistic Science Series 4, 279:43", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "High functional load inhibits phonological contrast loss: A corpus study", "author": ["A Wedel", "A Kaplan", "S Jackson"], "venue": "Cognition, 128(2):179\u2013186", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "The CELEX lexical database", "author": ["HR Baayen", "R Piepenbrock", "L Gulikers"], "venue": "Release 2 (CD-ROM)", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1995}, {"title": "Making long-distance relationships work: Quantifying lexical competition with hidden markov models", "author": ["J Strand", "D Liben-Nowell"], "venue": "Journal of Memory and Language, 90:88 \u2013 102", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Yuri Lin", "Jean-Baptiste Michel", "Erez Lieberman Aiden", "Jon Orwant", "Will Brockman", "Slav Petrov"], "venue": "In Proceedings of the ACL 2012 system demonstrations,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["SF Chen", "J Goodman"], "venue": "Comput Speech & Lang, 13(4):359\u2013393", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Proceedings of icslp", "author": ["A Stolcke"], "venue": "volume 2, pages 901\u2013904", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133].", "startOffset": 93, "endOffset": 98}, {"referenceID": 1, "context": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133].", "startOffset": 93, "endOffset": 98}, {"referenceID": 2, "context": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133].", "startOffset": 93, "endOffset": 98}, {"referenceID": 1, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 3, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 4, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 5, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 6, "context": "Prominent among these regularities is the wellknown relationship between word length and frequency: across languages, frequently-used words tend to be short [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8].", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 8, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 9, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 10, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 11, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 12, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 13, "context": "If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d).", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d).", "startOffset": 53, "endOffset": 61}, {"referenceID": 2, "context": "This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16].", "startOffset": 175, "endOffset": 185}, {"referenceID": 5, "context": "This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16].", "startOffset": 175, "endOffset": 185}, {"referenceID": 15, "context": "This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16].", "startOffset": 175, "endOffset": 185}, {"referenceID": 16, "context": "Following this logic, we use the phonological information content (PIC) of a word to measure the distinctiveness of its phoneme-to-phoneme transitions [17], or its approximation in character-tocharacter transitions.", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word.", "startOffset": 151, "endOffset": 158}, {"referenceID": 18, "context": "Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word.", "startOffset": 151, "endOffset": 158}, {"referenceID": 19, "context": "Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word.", "startOffset": 151, "endOffset": 158}, {"referenceID": 20, "context": "To obtain a compact representation of the probabilities of various phone or character sequences in a language we estimate an n-phone or n-character model (analogous to an n-gram model over words [21] but computed over individual phonemes or characters) from a corpus sample.", "startOffset": 195, "endOffset": 199}, {"referenceID": 21, "context": "Data from 27,751 words in the English Clearpond dataset [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Per Zipf\u2019s formulation, a signal which is too short may be ambiguous, and a listener is less likely to infer a speaker\u2019s intended meaning [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 22, "context": "In fact, length is strictly proportional to the probability (or log probability) of a string under a \u201cmonkeys-on-typewriters\u201d model of letter-to-letter transitions such as that proposed by Mandelbrot [23] and further explored by Miller [10].", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "In fact, length is strictly proportional to the probability (or log probability) of a string under a \u201cmonkeys-on-typewriters\u201d model of letter-to-letter transitions such as that proposed by Mandelbrot [23] and further explored by Miller [10].", "startOffset": 236, "endOffset": 240}, {"referenceID": 23, "context": "People have rich knowledge of the relative prominence of these sequences in their respective languages\u2014just as they have rich knowledge of inter-word statistical dependencies\u2014 and can call upon this knowledge in spoken word recognition [24, 25].", "startOffset": 236, "endOffset": 244}, {"referenceID": 24, "context": "People have rich knowledge of the relative prominence of these sequences in their respective languages\u2014just as they have rich knowledge of inter-word statistical dependencies\u2014 and can call upon this knowledge in spoken word recognition [24, 25].", "startOffset": 236, "endOffset": 244}, {"referenceID": 25, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 26, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 27, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 28, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 29, "context": "PIC is formally very similar to frequency-weighted neighborhood density [30], however it measures the number of competitors at each successive phone\u2014a feature consistent with empirical results suggesting incremental phoneme-by-phoneme processing in some cases [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "PIC is formally very similar to frequency-weighted neighborhood density [30], however it measures the number of competitors at each successive phone\u2014a feature consistent with empirical results suggesting incremental phoneme-by-phoneme processing in some cases [29].", "startOffset": 260, "endOffset": 264}, {"referenceID": 30, "context": "PIC thus constitutes a more detailed measure of neighborhood density than the canonical measure of Coltheart\u2019s N [31], the number of words within a edit distance of one of the target word.", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 31, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 32, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 33, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 34, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 198, "endOffset": 206}, {"referenceID": 35, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 198, "endOffset": 206}, {"referenceID": 36, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 256, "endOffset": 260}, {"referenceID": 37, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 302, "endOffset": 306}, {"referenceID": 18, "context": "[19], we compute correlations over the 25,000 most frequent types in each language; following that same study we additionally compute the correlation between PIC and average incontext information content (trigram surprisal) over the same set of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Cross-Linguistic Results Following the methodology adopted in [19], we examine the correlation between word-level predictors (frequency and in-context information content) and metric of structural form (word length or PIC) for the 25,000 most frequent types in each language.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Unlike [19], we limit our analysis to indictionary types, thereby excluding person names, place names, and acronyms from the analysis.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "The obtained correlations are even stronger than those recently obtained between word length and average in-context information content [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "[19] found that taking into account contextual predictability / information content (in the form of mean trigram surprisal in a dataset) better predicts word length than using a", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], in that we find unigram frequencies better predict word length than does mean trigram surprisal (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341].", "startOffset": 241, "endOffset": 248}, {"referenceID": 39, "context": "Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341].", "startOffset": 241, "endOffset": 248}, {"referenceID": 40, "context": "Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341].", "startOffset": 241, "endOffset": 248}, {"referenceID": 41, "context": "[43] for additional examples).", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "phonemic mergers)\u2014can also be characterized in terms of changes in PIC, similar to entropy-based estimates of functional load [44, 45].", "startOffset": 126, "endOffset": 134}, {"referenceID": 43, "context": "phonemic mergers)\u2014can also be characterized in terms of changes in PIC, similar to entropy-based estimates of functional load [44, 45].", "startOffset": 126, "endOffset": 134}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] and word form distinctiveness both implicate listener-oriented pressures in the relationship between frequency and word form.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], we classify each word into one of the three categories: those found in the relevant dictionary (by testing for membership in the corresponding GNU Aspell dictionary for each language), those found in English (by testing for membership in the English Aspell dictionary for words in non-English languages), and label the remainder as out-of-dictionary.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Because [19] represented word forms as closest ASCII equivalents (e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "[19] found higher global correlations between word length and in-context predictability (information content) as measured by mean trigram surprisal (blue bars in panel 1) than between word length and frequency (unigram surprisal), red bars in panel 1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19]: English and French in the Google 1T and English and German in Google Books 2012 show a stronger correlation of mean trigram surprisal and word length (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "For now, we note that we obtain the same pattern of results for PIC and frequency when we restrict our analysis to morphologically simple word forms for those languages with morphological annotation in the CELEX database [46] (English, German, and Dutch).", "startOffset": 221, "endOffset": 225}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "With respect to the second simplification, listeners do not have access to the true identity of preceding phonemes, and instead likely marginalize over a distribution of preceding as well as following sounds within a word in the process of word recognition [48].", "startOffset": 257, "endOffset": 261}, {"referenceID": 46, "context": "[49]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Methods Datasets for Frequency and Surprisal Estimates The Google Web 1T datasets were downloaded from the Linguistic Data Consortium [35, 36]; the Google Books 2012 datasets were downloaded from storage.", "startOffset": 134, "endOffset": 142}, {"referenceID": 35, "context": "Methods Datasets for Frequency and Surprisal Estimates The Google Web 1T datasets were downloaded from the Linguistic Data Consortium [35, 36]; the Google Books 2012 datasets were downloaded from storage.", "startOffset": 134, "endOffset": 142}, {"referenceID": 36, "context": "html [37], and OPUS (2013) from opensubtitles.", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "org [38].", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "log trigram probability across contexts, following [19], \u2212 1 N \u2211N i=1 logP (W = w|C = ci), where ci is the context for the ith occurrence of w and N is the frequency of w in the dataset.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "To avoid overfitting among higher order sequences, phone and character transition probabilities for the type-weighted models were computed with modified Kneser-Ney smoothing [51] with interpolation on orders 3, 4, and 5 using the SRILM toolkit [52].", "startOffset": 174, "endOffset": 178}, {"referenceID": 48, "context": "To avoid overfitting among higher order sequences, phone and character transition probabilities for the type-weighted models were computed with modified Kneser-Ney smoothing [51] with interpolation on orders 3, 4, and 5 using the SRILM toolkit [52].", "startOffset": 244, "endOffset": 248}], "year": 2017, "abstractText": "The inverse relationship between the length of a word and the frequency of its use, first identified by G.K. Zipf in 1935, is a classic empirical law that holds across a wide range of human languages. We demonstrate that length is one aspect of a much more general property of words: how distinctive they are with respect to other words in a language. Distinctiveness plays a critical role in recognizing words in fluent speech, in that it reflects the strength of potential competitors when selecting the best candidate for an ambiguous signal. Phonological information content, a measure of a word\u2019s string probability under a statistical model of a language\u2019s sound or character sequences, concisely captures distinctiveness. Examining largescale corpora from 13 languages, we find that distinctiveness significantly outperforms word length as a predictor of frequency. This finding provides evidence that listeners\u2019 processing constraints shape fine-grained aspects of word forms across languages. Despite their apparent diversity, natural languages display striking structural regularities [1\u20133]. How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136]. Prominent among these regularities is the wellknown relationship between word length and frequency: across languages, frequently-used words tend to be short [7]. In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8]. While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages. Here, we propose a generalization of Zipf\u2019s analysis and present two possible listener-focused explanations. We show that a word\u2019s frequency is inversely related to its distinctiveness\u2014how easily it can be identified as the intended message for a given speech signal. While speakers prefer easier-to-produce less distinctive forms, they are constrained by listeners\u2019 need for sufficiently distinctive forms to differentiate each word from others, especially if a speaker\u2019s intended word has higher-frequency competitors. If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d). We define a statistical measure of distinctiveness that succinctly captures the diagnosticity of a word form by assessing the aggregate strength of competitors in the language. We then show that distinctiveness should be inversely related to frequency, if languages are constructed to equalize error rates for low and high frequency words. Importantly, distinctiveness subsumes Zipf\u2019s observation regarding the relationship of length and frequency as a special case. Length is a n\u00e4\u0131ve approximation of the distinctiveness of a word form insofar as longer strings are simply less probable. We demonstrate that a more comprehensive measure of distinctiveness that takes into account the sound-to-sound (phoneme-to-phoneme or letter-to-letter) sequences in a language accounts for significantly more frequency-related variance 1 ar X iv :1 70 3. 01 69 4v 2 [ cs .C L ] 3 1 M ay 2 01 7 than does length across a broad sample of natural languages. This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16]. Model We define a probabilistic language model to characterize the distinctiveness of word forms in terms of their constituent sound-to-sound transitions. This model can be fit using a large written sample of a particular language. The starting point for the model is formalizing the task of the listener as a rational statistical inference. Bayesian inference and distinctiveness Upon hearing a string of sounds s, a listener has to infer what word w was intended by the speaker. This can be formulated as a problem of Bayesian inference. The listener should calculate a posterior distribution P (w|s) over words based on the sounds. Applying Bayes\u2019 rule, this is given by P (w|s) = P (s|w)P (w) P (s) (1) where P (s|w) is the probability of hearing s if w is the intended word, P (w) is the prior probability of the word w intended by the speaker, and P (s) is the probability of hearing s. Assuming that sounds are produced faithfully, such that P (sw|w) is close to 1 for a particular string sw for each w and close to 0 otherwise, we obtain the approximation P (w|sw) \u2248 P (w) P (sw) (2) which expresses the probability that the word w is correctly identified as a function of its normalized frequency, P (w), and the probability of the string sw in the language, P (sw). We define the distinctiveness of a word to be inversely related to P (sw): intuitively, a word that shares the same sound sequences with many other words is necessarily less distinctive. Following this logic, we use the phonological information content (PIC) of a word to measure the distinctiveness of its phoneme-to-phoneme transitions [17], or its approximation in character-tocharacter transitions. Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word. To obtain a compact representation of the probabilities of various phone or character sequences in a language we estimate an n-phone or n-character model (analogous to an n-gram model over words [21] but computed over individual phonemes or characters) from a corpus sample. To support a stronger test of the relationship between distinctiveness and frequency, we avoid the circularity that more probable words necessarily contain more probable sequences by estimating the transition probabilities using the type inventory (unique words) in the language. For the smaller datasets (those from the OPUS corpus) we also present the results for a model with token-weighted phonological transitions, though we note that interpreting the correlations obtained from the weighted model is challenging given this circularity. Under this model, the phonological information content PIC(w) of a word is defined as: PIC(w) = \u2212 logP (sw) (3) = \u2212 logP (l1, . . . , l|sw|) for l \u2208 sw (4)", "creator": "LaTeX with hyperref package"}}}