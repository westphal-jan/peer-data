{"id": "1703.01726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "A Novel Comprehensive Approach for Estimating Concept Semantic Similarity in WordNet", "abstract": "computation of semantic similarity standards between concepts is supposedly an important foundation for thus many research works. this paper focuses on ic consortium computing calculation methods and ic mesh measures, \u201c which estimate twice the semantic similarities between concepts \u2014 by precisely exploiting commonly the generalized topological parameters components of the data taxonomy. especially based further on periodically analyzing similar representative ic computing methods adequately and typical semantic similarity measures, lastly we propose naming a new comparative hybrid ic computing method. starting through repeatedly adopting the parameter dhyp and lch, we utilize adopting the newer new unique ic computing method and propose a novel \" comprehensive measure of acceptable semantic common similarity between concepts. an experiment clearly based off on academic wordnet \" blog is a \" comparing taxonomy has really been designed to test various representative measures and increase our measure focus on benchmark dataset r & amp ; team g, and the results show that our similarity measure can obviously improve the similarity ratio accuracy. we even evaluate against the simplest proposed approach algorithms by comparing the overall correlation coefficients between five measures and the artificial data. the overall results show that perhaps our proposal outperforms twice the previous measures.", "histories": [["v1", "Mon, 6 Mar 2017 05:07:12 GMT  (642kb)", "http://arxiv.org/abs/1703.01726v1", "11pages, 2 tables"]], "COMMENTS": "11pages, 2 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiao-gang zhang", "shou-qian sun", "ke-jun zhang"], "accepted": false, "id": "1703.01726"}, "pdf": {"name": "1703.01726.pdf", "metadata": {"source": "CRF", "title": "A Novel Comprehensive Approach for Estimating Concept Semantic Similarity in WordNet", "authors": ["Xiao-gang Zhang", "Shou-qian Sun", "Ke-jun Zhang"], "emails": ["11121062@zju.edu.cn"], "sections": [{"heading": null, "text": "many research works. This paper focuses on IC (information content) computing methods and IC measures, which estimate the semantic similarities between concepts by exploiting the topological parameters of the taxonomy. Based on analyzing representative IC computing methods and typical semantic similarity measures, we propose a new hybrid IC computing method. Through adopting the parameter dhyp and lch, we utilize the new IC computing method and propose a novel comprehensive measure of semantic similarity between concepts. An experiment based on WordNet \u201cis a\u201d taxonomy has been designed to test representative measures and our measure on benchmark dataset R&G, and the results show that our measure can obviously improve the similarity accuracy. We evaluate the proposed approach by comparing the correlation coefficients between five measures (the proposed approach, four other similarity methods) and the artificial data. The results show that our proposal outperforms the previous measures.\nKeywords: semantic similarity\uff1binformation content\uff1bcorrelation coefficient\uff1bWordNet taxonomy"}, {"heading": "1 Instruction", "text": "Computation of semantic similarity has already become the precondition for some research in\nvarious fields, including natural language processing, artificial intelligence, knowledge management and information retrieval [1]. The computation of concept semantic similarities is the fundamental for estimating textual semantic similarities because the concept is the smallest unit of semantic computing and the basis of information resource matching [2]. Utilizing the uniqueness of ontology concept and linguistic independence, polysemy and synonym of the concept can be effectively eliminated [3].\nWordNet and the Wikipedia Category Graph (WCG) are both reference ontology in the\ncomputation of concept semantic similarity. WordNet is a universal semantic lexicon and common ontology, which developed by the Cognitive Science Laboratory of Princeton University [4]. Because of its versatility and rigorous semantic organization, WordNet has been implemented as\n Corresponding author Correspondence to: No 705 South Hongqiao Road, Tarim University, Alar 843300, China. Tel.: +8615199601605. E-mail address: 11121062@zju.edu.cn (X.G. Zhang).\nthe underlying reference ontology in various tasks of natural language processing, such as machine translation, word discrimination, keyword retrieval, text mapping, information extracting and so on. The WCG is the other resources in some works, including in works of Hadj Taieb et al. [5, 6] and Zesch [7]. The WCG is different from WordNet because it is proposed by volunteers, and the categories of WCG do not include specifying the type in semantic relations. In the paper, we adopt WordNet as the reference ontology [1].\nIn this paper, we propose a new comprehensive approach based on taxonomical parameters\nwhich are extracted from WordNet \u201cis a\u201d taxonomy, the taxonomical parameters including subsumer and implicating hyponyms, the depth ratio and the deepest common hypernym between the two concepts concerned by the semantic similarity task [1]. We utilize the proposed approach for computing the semantic similarities between words.\nThe rest of the paper is organized as follows. In part 2, we will analyze some related works,\nincluding typical semantic similarity measures and representative IC computing models. In part 3, we will improve existed IC model, propose our semantic similarity approach and apply this measure to benchmark dataset. In part 4, we will design an evaluation metrics to evaluate semantic similarity measures. In part 5, we will make an experiment to estimate the semantic similarities and compare the correlation coefficient of several similarity measures and artificial discrimination. In part 6, we will discuss the results of experiment and the contrast of the correlation. In part 7, we will take a conclusion to this paper and make a plan for future works. 2 Related works\nNowadays, some scholars at home and abroad have carried on extensive exploration and\nresearch on the concept similarity computation, and proposed many semantic similarity methods. Representative methods included IC-based measures, distance-based measures, feature-based measures and hybrid measures. The measure of IC-based computed concept similarity by examining the information content contained in the word pairs [8]. The measure of distance-based calculated concept similarity by the semantic distance (the number of edges linking two concepts) between words, and then transformed the distance into similarity value [9]. The measure of feature-based estimated the semantic similarities between words according to the structural feature of taxonomy, which included nodes and edges [10]. Hybrid measures computed the similarities between words by merging the advantages of other measures conceived [11].\nIn this paper, we focus on IC-based semantic similarity measures [12\u201315], which include two\nparts: computing IC method and IC-based similarity measures [10]."}, {"heading": "2.1 Estimating the IC of a concept", "text": "Computing IC is the key of computing IC-based similarity. Method of computing IC is\nusually divided into two categories according to the different calculating objects, one based on statistical information and the other based on ontology intrinsic structure."}, {"heading": "2.1.1 The IC model based on statistical information", "text": "This method computed the value of IC by counting the probability of a concept in a given\ncorpus. In this kind of method, the most typical model is the model proposed by Resnik. He put forward that the frequency of concept could be estimated by the term frequency appearing in Brown Corpus [15]. Resnik proposed the equation as follows [12]:\nIC( c ) log( p( c ))  (1)\nHere, c is a concept node, and p(c) is the probability that \ud835\udc50 appears in a given corpus. From equation (1), we can see that the more frequency of concept appeared, the less message of the concept transferred. Each term appeared in the corpus was counted as an occurrence rate of concept which included the term in ontology taxonomy. Then, Freq(c) is computed as follows:\nWord( c ) Freq( c ) Count( )      (2)\nHere, Word(c) is a set of words subsumed by c, and Count(\u03c9) represents the frequency of the word \u03c9 appeared in the corpus. Then, p(c) is computed as follows [12]:\nFreq( c ) p( c )\nN  (3)\nWherein, N is the total number of terms appeared.\nIn general, the advantages of this method are high efficiency and suitable for large-scale data\nprocessing. The shortcomings are subject to external interference and inaccurate value."}, {"heading": "2.1.2 The IC model based ontology intrinsic structure", "text": "Compared with the model based on statistical information, this kind of method calculated the\nvalue of IC by the ontology intrinsic structure regardless of the external factor, but this kind of method asked the ontology taxonomy has been organized with a meaningful way.\nSeco et al. [16] were the first one computed IC with ontology hierarchical structure. They\ndiscovered that the IC of concept subsumed more child nodes was fewer and the IC of each of its leaf nodes was larger in a classification tree. The calculating method of IC value is as follows [17]:\nLog(| hypo( c )| 1)\nIC( c ) 1 Log(max_ nodes )\n   (4)\nWhere hypo(c) is the count of child nodes of node c, max_nodes represents the maximum number of the concepts in the classification tree. It can be seen from (4) that the IC was only related to the intrinsic hierarchical structure, and the IC value of node c could be computed by the number of hyponym of node c.\nLater, David Sanchez et al. [18] proposed a new model, which adopted subsumers of leaf\nnode to calculate the value of IC. The equation was as follows:\nDavid\ncommonness( c ) IC ( c ) log\ncommonness( root )\n     \n  (5)\nWhere the function commonness(c) equals \u2211commonness(n), and commonness(n) equals 1/subsumers(n). Wherein n is a leaf node and one of hyponym of node c, subsumers(n) returns the number of nodes from the root to node n along the path of taxonomy.\nFrom above we concluded that the method based on ontology intrinsic structure only relied\non the hierarchical structure without any external information, so it was more accurate than statistical method."}, {"heading": "2.2 IC-based semantic similarity measures", "text": "Typical semantic similarity measures include Resnik\u2019s [12], Jiang and Conrath\u2019s [13] and\nLin\u2019s measure [14] etc.\nResnik [12] was the first one computed semantic similarity through the intrinsic structure in\nontology, which judged the similarity of a pair of concepts by the amount of sharing information. Therefore he regarded the Most Specific Common Abstraction (MSCA) that subsumes both concepts as the semantic similarities of the two concepts. The model is as follows [12]:\nR 1 2 1 2 1 2sim ( c ,c ) lg p( lso( c ,c )) IC( lso( c ,c ))   (6)\nHere, lso(c1,c2) stand for the MSCA of c1 and c2 in taxonomy.\nJiang & Conrath [13] computed the semantic distance through the IC sum of two concepts\nsubtracting the IC of their MSCA. The measure is as follows:\nJC 1 2 1 2 1 2dist ( c ,c ) IC( c ) IC( c ) 2 IC( lso( c ,c ))    (7)\nAfter a linear transformation, the equation (7) could be transformed as follows [16].\n1 2 1 2 J &c 1 2\nIC( c ) IC( c ) 2 IC( lso( c ,c )) sim ( c ,c ) 1\n2\n      \n  (8)\nLin [14] had a same understanding as Resnik on semantic similarity, and he believed that the\nsimilarity of two concepts could be measured by the ratio of common information and total information. The model proposed by him is as follows [14]:\n1 2 Lin 1 2\n1 2\n2 IC( lso( c ,c )) sim ( c ,c )\nIC( c ) IC( c )\n \n (9)\nBased on stated above, it is noted that, the IC-based similarity measures, the most critical\nissue was how to get the IC value of concept exactly and how to introduce IC into the measures."}, {"heading": "2.3 Path and depth based measures", "text": "Except for IC-based measures, there are some other representative semantic similarity\nmeasures, including Rada\u2019s [18], Wu & Palmer\u2019s [19] and Leacock & Chodorow\u2019s [20] etc.\nRada et al. [18] stated that the length of the minimum path of two concepts quantified their\nsemantic distance. Namely, the similarity between words can be calculated by the minimum path distance linking their corresponding nodes of \u201cis-a\u201d links of ontology. A simple measure to calculate their semantic distance defined by [18] is:\n1 2 1 2( , ) min ( , )rad i idis c c path c c (10)\nWu & Palmer\u2019s measure [19] is a typical method based on the shortest path. Their method\nadopted depth and length to compute the similarity. The corresponding calculating equation is as follows [19]:\n1 2 W &P 1 2\n1 2 1 2\n2 depth( lso( c ,c )) sim ( c ,c )\nlen( c ,c ) 2 depth( lso( c ,c ))\n \n  (11)\nWhere the function depth(ci) is the depth of ci. len(c1,c2) stand for the shortest path distance\nbetween c1 and c2. lso(c1,c2) represent the MSCA of c1 and c2.\nLater, Leacock & Chodorow [20] proposed a non-linear calculating model, which included\ntwo parameters. One is the number of nodes between two concepts (including itself), and the other is the maximum depth of the classification tree. The calculating equation is as follows [20]:\n1 2 L&C 1 2\nc WordNet\nlen( c ,c ) sim ( c ,c ) log\n2 max depth( c )    (12)\nFor a fixed classification tree, we can see in the equation (12), if the path distance between two concepts was further, the semantic similarity was smaller. 3 A new hybrid measure of concept semantic similarity\nAs stated section 2, IC-based semantic similarity measures [12\u201315] include two parts: IC\ncomputing method and IC-based similarity measures [10]. Computing IC is the key of similarity computation of IC-based, so in this section we focus on two parts: improving IC model and proposing a new semantic similarity measure."}, {"heading": "3.1 Rewriting the IC model by introducing information theory", "text": "As Rada et al. [18] stated, the length of the minimum path of two concepts quantified their\nsemantic distance.\nrad 1 2 1 2dis ( c ,c ) min_ path( c ,c ) (13)\nAccording to information theory of the paper [9], the deeper the concept is, the greater the\ninformation content is. As stated in [21], differential information of a content comparing to another could be quantified by the IC of the concept alone subtracting the public parts of two information content. The calculating equation is as follows [9]:\n1 2 1 2\n1 2 2 1\n1 1 2 2 1 2\n1 2 1 2\n| min_ path( c ,c )| length( c ,c )\n( IC( c ) IC( c )) ( IC( c ) IC( c )) ( IC( c ) IC( lso( c ,c ))) ( IC( c ) IC( lso( c ,c ))) IC( c ) IC( c ) 2 IC( lso( c ,c ))\n\n           \n(14)\nWhere the function IC(lso(c1,c2)) are the IC of the MSCAof c1 and c2. As stated above, a conceptual relative depth is the minimum distance between the concept and root, and root is the MSCA between root node and any node, namely IC(lso(c, root))=IC(root). As the root includes any concept, namely IC(root)=0, the depth of concept c can be approximated as follows [22]:\ndepth( c ) min_ path( c,root )\nIC( c ) IC( root ) 2 IC( lso( c,root )) IC( c ) IC( root ) IC( c )\n\n      \n(15)\nIn the same way, the equation (5) could be rewritten as following:\nnew\ncommonness( c ) IC ( c ) log log( subsumers( c ))\ncommonness( root )\n     \n  (16)\nWhere the function commonness(c) equals \u2211commonness(n), and commonness(n) equals 1/subsumers(n), in which n is a leaf node and one of hyponym of c. The function subsumers(c)\nequals \u2211subsumers(n), and subsumers(n) returns the number of node from the root to node n along the path of taxonomy, namely the number of direct hypernym of ci (including ci itself). Because the function commonness(root) equals 1/subsumers(root) and subsumers(root) equals 1, therefore the function commonness (root) equals 1.\nThe model of equation (16) introduced the information theory and included intrinsic\nparameters the number of child-nodes of concept c and the depth of concept c in taxonomy, so this IC method belonged to a hybrid computing IC method. In addition, this IC model does not be interfered by external factors, therefore in theory it can achieve better stability."}, {"heading": "3.2 A comprehensive semantic similarity approach between words", "text": "Before propose our similarity approach, we define four definitions of semantic similarity as\nfollows. Definition 1. Hypernyms hyp(c)={ci\u2208V, c\u2208V| c<ci}\u222a{c} . In this equation, ci stands for the nodes along the path from the root to node c in the classification tree, V is the set of concepts of the classification tree. Definition 2. Directhypernyms dhyp(c)={ci\u2208V,ci\u2192c}, ci is the direct hypernym of c (including c itself) , V is a set of concepts of the classification tree. Definition 3. Lowest Common Hypernym lch(c1, c2) the most specific common abstraction that subsumes both concepts c1 and c2. Definition 4. Max Depth cmax_depth represents the depth of the deepest node in the classification tree.\nBased on the equation (16) and Leacock & Chodorow similarity measure (12), we propose a\ncomprehensive semantic similarity approach which took into account the information theory and the taxonomy structure. The new semantic similarity approach is as follows:\nmax_ depth\nnew 1 2\n1 2 1 2\n2 log( dhyp( c )) sim ( c ,c ) log\nlog( dhyp( c )) log( dhyp( c )) 2 log( dhyp( lch( c ,c )))\n \n   (17)\nIn theory, our measure belonged to the comprehensive measure because our approach\nintegrates characters of IC-based measure and feature-based measure. Our approach owns three advantages. Firstly, this method does not interfere by external factors because it is based on ontology intrinsic structure. Secondly, this approach reduced the count of parameter because it only included one parameter (dhyp and lch). Thirdly, this approach simplified the difficulty of operation by converting the minimum distance between c1 and c2 to direct hypernym of the most specific common abstraction of c1 and c2. In part 4, we will design an experiment to compare the semantic similarity measures in a bench mark dataset."}, {"heading": "4 Semantic similarity measures evaluation", "text": "As Adhikari et al. stated [3], the first step of finding semantic similarity is designing a good\nIC computing model and the second step is using the IC computing model in an efficient similarity measure. In order to evaluate the proposed IC model and our new similarity measure, we will compare our measure and four typical semantic similarity measures mentioned in section 2."}, {"heading": "4.1 Data source and concept selection", "text": "In this paper, we compute the semantic similarity degree between words by the ontology of\nWordNet 3.0 version, and make an experiment on Rubenstein & Goodenough (R&G) benchmark dataset [23].\nWordNet 3.0 [4] was organized in a taxonomical way and included more than ten thousands\nof English concepts. In WordNet 3.0 each word was described by a set of concepts that express the possible meanings of the concerned word. The taxonomy \u201cis-a\u201d was mainly semantic relations and was used to compute the semantic similarity degree between words, which were more important in semantic computing. In WordNet 3.0, because noun reached the 75 percent, so our measure used the nominal \u201cis a\u201d taxonomies of WordNet in this paper.\nR&G dataset included 63 word pairs, which were judged by 51 professional people. We\nchose 30 word-pairs, and the range of similarity was from irrelevant to identity. According to the similarity degree between words, artificial scoring range was in [0.0-4.0].\nWe computed the semantic similarity in WordNet 3.0 by an accepted website, which includes\nJiang & Conrath\u2019s, Resnik\u2019s, Lin\u2019s, Wu & Palmer\u2019s, Leacock & Chodorow\u2019s measures and so on [24]. Considering the situation that each word corresponds to a number of concepts in WordNet and R&G dataset only include words, so we need to transform seeking concept into seeking word. We assumed word w1 owns m concepts and w2 owns n concepts. When calculating the similarities of w1 and w2, we could get m\u00d7n similarity values. Wherein, we adopted the largest value of a word as the concept semantic similarity. We gave a specific model for seeking conceptual similarities as follows:\n1 2 1i 2 j ( i , j )\nsim( w ,w ) max[ sim( c ,c )] (16)\nHere, c1i is the concept of w1, and c2j is the concept of w2. 4.2 Evaluation metrics\nSeeking the correlation coefficient of similarity measure and artificial data is an important\nbench mark for evaluating similarity measure. We evaluated our approach by the equation (17) (two-sided 0.05 level Pearson correlation measurement) [6, 25]:\nn\ni ii 1 xy\nn 2 2 i ii 1\n( x x )( y y ) r\n( x x ) ( y y )\n\n\n  \n \n  (17)\nHere, X represents similarity value computed by a similarity measure in R&G and Y represents similarity value derived from artificial data in R&G, and Y is used in the benchmark data to evaluate similarity measures. X equals (x1,x2,\u2026xn) and Y equals (y1,y2,\u2026yn). (xi-x) is the difference between xi and mean of xi, and xi represents each term of set X. Similarly, (yi-y) is the difference between yi and mean of yi, and yi represents each term of set Y. The correlation coefficient rxy is in [1, -1]. 5 Experimental and evaluation results\nWe design an experiment to test similarity scores of 30 term pairs on Wu & Palmer\u2019s, Jiang\n& Conrath\u2019s, Leacock & Chodorow\u2019s, artificial data, Lin\u2019s and our measure.\ndata Palmer\u2019s Conrath\u2019s Chodorow\u2019s\nautograph-shore 0.0600 0.3077 0.0000 0.0000 1.3863 0.2188\nnoon-string 0.0800 0.3529 0.0653 0.0923 1.2040 0.3815 glass-magician 0.1100 0.5333 0.0604 0.1421 1.6094 0.3852 automobile-wizard 0.1100 0.4545 0.0738 0.1682 1.1239 0.4930\nmound-stove 0.1400 0.6667 0.0681 0.3143 1.7430 0.5976\ncoast-forest 0.4200 0.6154 0.0628 0.1181 1.8971 0.6056 boy-rooster 0.4400 0.5600 0.0727 0.2094 1.2040 0.6849 cushion-jewel 0.4500 0.6667 0.0694 0.2572 1.7430 0.7610\ncoast-hill 0.8700 0.7143 0.2187 0.7286 2.0794 0.7622 boy-sage 0.9600 0.6667 0.0680 0.2057 1.8971 0.9472 mound-shore 0.9700 0.7143 0.1672 0.6724 2.0794 0.8045 automobile-cushion 0.9700 0.6364 0.0894 0.3812 1.5404 0.7208\ncrane-rooster 1.4100 0.7586 0.0000 0.0000 1.6094 0.9923 hill-woodland 1.4800 0.6154 0.0592 0.1218 1.8971 0.6056\nbrother-lad 1.6600 0.7143 0.0830 0.2400 2.0794 1.0136 crane-implement 1.6800 0.7778 0.0784 0.3327 2.0794 0.9729 magician-oracle 1.8200 0.6250 0.0588 0.1828 1.7430 0.8980\nsage-wizard 2.4600 0.1667 0.0580 0.1809 1.8971 0.9472 oracle-sage 2.6100 0.7059 0.1083 0.5885 1.8971 1.0076\nbrother-monk 2.8200 0.9565 0.0689 0.2079 2.9957 0.8367\nimplement-tool 2.9500 0.9412 0.8484 0.9146 2.9957 1.5718\nbird-crane 2.9700 0.8800 0.0000 0.0000 2.3026 1.3402 bird-cock 3.0500 0.9565 0.2681 0.7881 2.9957 1.7568 hill-mound 3.2900 1.0000 0.4931 1.0000 3.6889 1.1924 cord-string 3.4100 0.9412 0.6553 0.9188 2.9957 1.0576 midday-noon 3.4200 1.0000 3.5685 1.0000 3.6889 1.4007 glass-tumbler 3.4500 0.5882 0.0626 0.1858 1.6094 1.1306\nserf-slave 3.4600 0.8000 0.0000 0.0000 2.3026 0.9776 cemetery-graveyard 3.8800 1.0000 1.0000 1.0000 3.6889 1.3395\nmagician-wizard 3.5000 1.0000 0.0640 1.0000 3.6889 1.1110\nrange 3.8200 0.6923 1.0000 1.0000 2.5650 1.5380\nIn this study, in the same settings we have evaluated representative measurements generated by machine, and compared them against human ratings performed on semantic similarities between words.\nWe evaluated our approach by the equation (17) (two-sided 0.05 level Pearson correlation\nmeasurement). Testing results of Poisson correlation coefficient showed as follows:\nLeacock & Chodorow Measure 0.792\nOur Measure 0.823\nIn Table 2, Poisson correlation coefficient represents the correlation score between each machine generated measure and artificial data. 6 Discussions\nThere were four aspects of our work have to be addressed. Firstly, we rewrote the IC model\nwhich we took into account the taxonomy structure and information theory, and converted the old computing IC method (based ontology intrinsic structure) to a new hybrid computing IC method. Based on the new IC model, we proposed a novel semantic similarity approach, which computed the similarities between words based on the comprehensive factors instead of relying on the path and depth. In this similarity approach, we adopted the parameters lch and dhyp, which validated the estimation of semantic similarity degree between words.\nSecondly, the results of Table 1 show that our quantification approach owned better\nperformance than other methods in set of 30 term pairs of R&G dataset. This performance is very desirable because WordNet is a common ontology and the intrinsic IC models of ontology-based own better independence than domain corpora. In fact, the intrinsic IC models are efficient and easily applicable to different domains because the IC models of corpora-based are hampered by corpora used [1].\nThirdly, the parameter range is an important benchmark for the dispersion degree of\nmeasures. In Table 1, the last row showed that the range of our measure reachs 1.5380, and this meant that the dispersion degree of our measure is better than Wu & Palmer\u2019s, Jiang & Conrath\u2019s, Lin\u2019s measures. The dispersion degree of our measure is lower than Leacock & Chodorow\u2019s measure (W&P measure range=0.6923, L&C measure range=2.5652, Lin measure range=1.0000, J&C measure r=1.0000). This is due to the smallest similarity of word reached the 1.2040 in Leacock & Chodorow\u2019s measure, but the smallest similarity of word equaled 0.2188 in proposed measure (Leacock & Chodorow measure \u201cnoon-string\u201d =1.2040, our measure \u201cautograph-shore\u201d =0.2188).\nThe last point, based on evaluation metrics equation (17), if correlation score was bigger,\ntesting measure and artificial data are closer related. In Table 2, the measures based on path and depth owned good correlations than pure IC-based measures (W&P measure r=0.678, L&C measure r=0.792; Lin measure r=0.543, J&C measure r=0.389). When we introduced our hybrid IC model, the correlation coefficient of our measure reached 0.823. This means the fitting degree of our measure is better than others. 7 conclusion and future works\nIn this paper, our works included three aspects. Firstly, after analyzing representative IC\nmodels and typical semantic similarity measures, we proposed an improved computing IC model. Our model has been considered information theory and intrinsic ontology structure factors (hypernym, hyponym, depth, node number), which owned significant weight on computing accurately information content of concepts. Secondly, based on the improved IC model we put forward a new comprehensive measure for estimating concept semantic similarity. Thirdly, we\ntested our approach in set of 30 term pairs of R&G benchmark dataset and compared the correlation coefficient between existed measures, our measure and artificial data. The results showed our measure was effective. In future, we will improve the proposed approach by considering the more spatial structure of ontology and proof-test this approach in some widely datasets. Acknowledgments The work in this paper was supported by Chinese National Natural Science Foundation (Grant No. 61562072).\nReferences\uff1a [1] Hadj Taieb, M. A., Ben Aouicha, M., Ben Hamadou, A.. Ontology- based approach for measuring semantic\nsimilarity. Engineering Applications of Artificial Intelligence, 2014, 36(C): 238\u2013261.\n[2] Gruber T R. A translation approach to portable ontology specifications. Knowledge Acquisition, 1993, 5(2):\n199-220.\n[3] Adhikari A, Singh S, Mondal D, et al. A novel information theoretic framework for finding semantic similarity\nin WordNet [J]. arXiv preprint arXiv:1607.05422, 2016.\n[4] Http://wordnet.princeton.edu\n[5] Hadj Taieb, M. A., Ben Aouicha, M., and Ben Hamadou, A. Computing semantic relatedness using wikipedia\nfeatures. Knowledge-Based Systems, 2013, 50: 260-278.\n[6] Aouicha, M. B., Taieb, M. A.. Computing semantic similarity between biomedical concepts using new\ninformation content approach. Journal of Biomedical Informatics, 2015, 59(1): 258\u2013275.\n[7] Zesch T. Study of semantic relatedness of words using collaboratively constructed semantic resources [D].\nTechnische Universit\u00e4t, 2010.\n[8] Lu W, Cai Y, Che X, et al. Joint semantic similarity assessment with raw corpus and structured ontology for\nsemantic-oriented service discovery[J]. Personal and Ubiquitous Computing, 2016, 20(3): 311-323.\n[9] Pirr\u00f3, Giuseppe, and J. Euzenat. A feature and information theoretic framework for semantic similarity and\nrelatedness. The Semantic Web \u2013 ISWC 2010. Springer Berlin Heidelberg, 2010:615-630.\n[10] S\u00e1nchez, D., Sol\u00e9-Ribalta, A., Batet, M., and Serratosa, F. Enabling semantic similarity estimation\nacrossmultiple ontologies: An evaluation in the biomedical domain. J. of Biomedical Informatics 45, 2012, 1\n(2):141-155.\n[11] Zhang Y, Shang L, Huang L, et al. A hybrid similarity measure method for patent portfolio analysis [J].\nJournal of Informetrics, 2016, 10(4): 1108-1130.\n[12] Resnik, Philip. Using information content to evaluate semantic similarity in a taxonomy. International Joint\nConference on Artificial Intelligence Morgan Kaufmann Publishers Inc. 1995:448-453.\n[13] Conrath D W. Semantic similarity based on corpus statistics and lexical taxonomy[J]. arXiv preprint\ncmp-lg/9709008, 1997.\n[14] Lin, D. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International\nConference on Machine Learning (San Francisco, CA, USA, 1998), ICML '98, Morgan Kaufmann Publishers Inc.,\n1998:296-304.\n[15] Http://www.helsinki.fdvarieng/CoRD/corpora/BROWN/.\n[16] Seco N, Veale T, Hayes J. An intrinsic information content metric for semantic similarity in\nWordNet[C]//Proceedings of the 16th European conference on artificial intelligence. IOS Press, 2004: 1089-1090.\n[17] S\u00e1nchez, D. A new model to compute the information content of concepts from taxonomic\nknowledge. International Journal on Semantic Web & Information Systems, 2012, 8(2):34-50.\n[18] Rada R, Mili H, Bicknell E, et al. Development and application of a metric on semantic nets [J]. IEEE\nTransactions on Systems Man & Cybernetics, 1989, 19(1):17-30.\n[19] Wu, Zhibiao, and M. Palmer. Verb semantics and lexical selection. ACL Proceedings of Annual Meeting on\nAssociation for Computational Linguistics, 1995:133--138.\n[20] Leacock C, Chodorow M. Combining local context and WordNet similarity for word sense identification [J].\nWordNet: An electronic lexical database, 1998, 49(2): 265-283.\n[21] David S\u00e1nchez, Montserrat Batet. Semantic similarity estimation in the biomedical domain: An\nontology-based information-theoretic perspective. Journal of Biomedical Informatics, 2011, 44(5):749\u2013759.\n[22] S\u00e1nchez D, Batet M, Isern D. Ontology-based information content computation [J]. Knowledge-Based\nSystems, 2011, 24(2):297-303.\n[23] Rubenstein H, Goodenough J B. Contextual correlates of synonymy [J]. Communications of the ACM, 1965,\n8(10):627-633.\n[24] Http://marimba.d.umn.edu/cgi-bin/similarity/similarity.cgi?version=yes.\n[25] Http://en.wikipedia.org/wiki/Correlation coefficients."}], "references": [{"title": "Ontology- based approach for measuring semantic similarity", "author": ["M.A. Hadj Taieb", "M. Ben Aouicha", "Ben Hamadou"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A translation approach to portable ontology specifications", "author": ["R. Gruber T"], "venue": "Knowledge Acquisition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "A novel information theoretic framework for finding semantic similarity in WordNet [J", "author": ["A Adhikari", "S Singh", "D Mondal"], "venue": "arXiv preprint arXiv:1607.05422,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Computing semantic relatedness using wikipedia features", "author": ["M.A. Hadj Taieb", "M. Ben Aouicha", "A. Ben Hamadou"], "venue": "Knowledge-Based Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Computing semantic similarity between biomedical concepts using new information content approach", "author": ["M.B. Aouicha", "Taieb", "M. A"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Study of semantic relatedness of words using collaboratively constructed semantic resources [D", "author": ["T. Zesch"], "venue": "Technische Universita\u0308t,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Joint semantic similarity assessment with raw corpus and structured ontology for semantic-oriented service discovery[J", "author": ["W Lu", "Y Cai", "X Che"], "venue": "Personal and Ubiquitous Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "A feature and information theoretic framework for semantic similarity and relatedness", "author": ["Pirr\u00f3", "Giuseppe", "J. Euzenat"], "venue": "The Semantic Web \u2013 ISWC", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Enabling semantic similarity estimation acrossmultiple ontologies: An evaluation in the biomedical domain", "author": ["D. S\u00e1nchez", "A. Sol\u00e9-Ribalta", "M. Batet", "F. Serratosa"], "venue": "J. of Biomedical Informatics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A hybrid similarity measure method for patent portfolio analysis [J", "author": ["Y Zhang", "L Shang", "L Huang"], "venue": "Journal of Informetrics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["Resnik", "Philip"], "venue": "International Joint Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy[J", "author": ["W. Conrath D"], "venue": "arXiv preprint cmp-lg/9709008,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning (San Francisco, CA, USA,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "An intrinsic information content metric for semantic similarity in WordNet[C]//Proceedings of the 16th European conference on artificial intelligence", "author": ["N Seco", "T Veale", "J. Hayes"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "A new model to compute the information content of concepts from taxonomic knowledge", "author": ["D. S\u00e1nchez"], "venue": "International Journal on Semantic Web & Information Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Development and application of a metric on semantic nets [J", "author": ["R Rada", "H Mili", "E Bicknell"], "venue": "IEEE Transactions on Systems Man & Cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "Verb semantics and lexical selection", "author": ["Wu", "Zhibiao", "M. Palmer"], "venue": "ACL Proceedings of Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["C Leacock", "M. Chodorow"], "venue": "[J]. WordNet: An electronic lexical database,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Semantic similarity estimation in the biomedical domain: An ontology-based information-theoretic perspective", "author": ["David S\u00e1nchez", "Montserrat Batet"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Ontology-based information content computation [J", "author": ["D S\u00e1nchez", "M Batet", "D. Isern"], "venue": "Knowledge-Based Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Contextual correlates of synonymy [J", "author": ["H Rubenstein", "B. Goodenough J"], "venue": "Communications of the ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1965}], "referenceMentions": [{"referenceID": 0, "context": "1 Instruction Computation of semantic similarity has already become the precondition for some research in various fields, including natural language processing, artificial intelligence, knowledge management and information retrieval [1].", "startOffset": 233, "endOffset": 236}, {"referenceID": 1, "context": "The computation of concept semantic similarities is the fundamental for estimating textual semantic similarities because the concept is the smallest unit of semantic computing and the basis of information resource matching [2].", "startOffset": 223, "endOffset": 226}, {"referenceID": 2, "context": "Utilizing the uniqueness of ontology concept and linguistic independence, polysemy and synonym of the concept can be effectively eliminated [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "[5, 6] and Zesch [7].", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[5, 6] and Zesch [7].", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[5, 6] and Zesch [7].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "In the paper, we adopt WordNet as the reference ontology [1].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "In this paper, we propose a new comprehensive approach based on taxonomical parameters which are extracted from WordNet \u201cis a\u201d taxonomy, the taxonomical parameters including subsumer and implicating hyponyms, the depth ratio and the deepest common hypernym between the two concepts concerned by the semantic similarity task [1].", "startOffset": 324, "endOffset": 327}, {"referenceID": 6, "context": "The measure of IC-based computed concept similarity by examining the information content contained in the word pairs [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "The measure of distance-based calculated concept similarity by the semantic distance (the number of edges linking two concepts) between words, and then transformed the distance into similarity value [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 8, "context": "The measure of feature-based estimated the semantic similarities between words according to the structural feature of taxonomy, which included nodes and edges [10].", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "Hybrid measures computed the similarities between words by merging the advantages of other measures conceived [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "In this paper, we focus on IC-based semantic similarity measures [12\u201315], which include two parts: computing IC method and IC-based similarity measures [10].", "startOffset": 65, "endOffset": 72}, {"referenceID": 11, "context": "In this paper, we focus on IC-based semantic similarity measures [12\u201315], which include two parts: computing IC method and IC-based similarity measures [10].", "startOffset": 65, "endOffset": 72}, {"referenceID": 12, "context": "In this paper, we focus on IC-based semantic similarity measures [12\u201315], which include two parts: computing IC method and IC-based similarity measures [10].", "startOffset": 65, "endOffset": 72}, {"referenceID": 8, "context": "In this paper, we focus on IC-based semantic similarity measures [12\u201315], which include two parts: computing IC method and IC-based similarity measures [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Resnik proposed the equation as follows [12]:", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "Then, p(c) is computed as follows [12]: Freq( c ) p( c ) N \uf03d (3)", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "[16] were the first one computed IC with ontology hierarchical structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The calculating method of IC value is as follows [17]:", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "[18] proposed a new model, which adopted subsumers of leaf node to calculate the value of IC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "2 IC-based semantic similarity measures Typical semantic similarity measures include Resnik\u2019s [12], Jiang and Conrath\u2019s [13] and Lin\u2019s measure [14] etc.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "2 IC-based semantic similarity measures Typical semantic similarity measures include Resnik\u2019s [12], Jiang and Conrath\u2019s [13] and Lin\u2019s measure [14] etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "2 IC-based semantic similarity measures Typical semantic similarity measures include Resnik\u2019s [12], Jiang and Conrath\u2019s [13] and Lin\u2019s measure [14] etc.", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "Resnik [12] was the first one computed semantic similarity through the intrinsic structure in ontology, which judged the similarity of a pair of concepts by the amount of sharing information.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "The model is as follows [12]:", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "Jiang & Conrath [13] computed the semantic distance through the IC sum of two concepts subtracting the IC of their MSCA.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "After a linear transformation, the equation (7) could be transformed as follows [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Lin [14] had a same understanding as Resnik on semantic similarity, and he believed that the similarity of two concepts could be measured by the ratio of common information and total information.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "The model proposed by him is as follows [14]:", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "3 Path and depth based measures Except for IC-based measures, there are some other representative semantic similarity measures, including Rada\u2019s [18], Wu & Palmer\u2019s [19] and Leacock & Chodorow\u2019s [20] etc.", "startOffset": 145, "endOffset": 149}, {"referenceID": 16, "context": "3 Path and depth based measures Except for IC-based measures, there are some other representative semantic similarity measures, including Rada\u2019s [18], Wu & Palmer\u2019s [19] and Leacock & Chodorow\u2019s [20] etc.", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "3 Path and depth based measures Except for IC-based measures, there are some other representative semantic similarity measures, including Rada\u2019s [18], Wu & Palmer\u2019s [19] and Leacock & Chodorow\u2019s [20] etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "[18] stated that the length of the minimum path of two concepts quantified their semantic distance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "A simple measure to calculate their semantic distance defined by [18] is:", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "Wu & Palmer\u2019s measure [19] is a typical method based on the shortest path.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "The corresponding calculating equation is as follows [19]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "Later, Leacock & Chodorow [20] proposed a non-linear calculating model, which included two parameters.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "The calculating equation is as follows [20]:", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "3 A new hybrid measure of concept semantic similarity As stated section 2, IC-based semantic similarity measures [12\u201315] include two parts: IC computing method and IC-based similarity measures [10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 11, "context": "3 A new hybrid measure of concept semantic similarity As stated section 2, IC-based semantic similarity measures [12\u201315] include two parts: IC computing method and IC-based similarity measures [10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 12, "context": "3 A new hybrid measure of concept semantic similarity As stated section 2, IC-based semantic similarity measures [12\u201315] include two parts: IC computing method and IC-based similarity measures [10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 8, "context": "3 A new hybrid measure of concept semantic similarity As stated section 2, IC-based semantic similarity measures [12\u201315] include two parts: IC computing method and IC-based similarity measures [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 15, "context": "[18] stated, the length of the minimum path of two concepts quantified their semantic distance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "According to information theory of the paper [9], the deeper the concept is, the greater the information content is.", "startOffset": 45, "endOffset": 48}, {"referenceID": 18, "context": "As stated in [21], differential information of a content comparing to another could be quantified by the IC of the concept alone subtracting the public parts of two information content.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "The calculating equation is as follows [9]:", "startOffset": 39, "endOffset": 42}, {"referenceID": 19, "context": "As the root includes any concept, namely IC(root)=0, the depth of concept c can be approximated as follows [22]:", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "stated [3], the first step of finding semantic similarity is designing a good IC computing model and the second step is using the IC computing model in an efficient similarity measure.", "startOffset": 7, "endOffset": 10}, {"referenceID": 20, "context": "0 version, and make an experiment on Rubenstein & Goodenough (R&G) benchmark dataset [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "05 level Pearson correlation measurement) [6, 25]:", "startOffset": 42, "endOffset": 49}, {"referenceID": 0, "context": "The correlation coefficient rxy is in [1, -1].", "startOffset": 38, "endOffset": 45}, {"referenceID": 0, "context": "In fact, the intrinsic IC models are efficient and easily applicable to different domains because the IC models of corpora-based are hampered by corpora used [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "[1] Hadj Taieb, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Gruber T R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Adhikari A, Singh S, Mondal D, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Hadj Taieb, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Aouicha, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Zesch T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Lu W, Cai Y, Che X, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Pirr\u00f3, Giuseppe, and J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] S\u00e1nchez, D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Zhang Y, Shang L, Huang L, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Resnik, Philip.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Conrath D W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Lin, D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Seco N, Veale T, Hayes J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] S\u00e1nchez, D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Rada R, Mili H, Bicknell E, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Wu, Zhibiao, and M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Leacock C, Chodorow M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] David S\u00e1nchez, Montserrat Batet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] S\u00e1nchez D, Batet M, Isern D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Rubenstein H, Goodenough J B.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Computation of semantic similarity between concepts is an important foundation for many research works. This paper focuses on IC (information content) computing methods and IC measures, which estimate the semantic similarities between concepts by exploiting the topological parameters of the taxonomy. Based on analyzing representative IC computing methods and typical semantic similarity measures, we propose a new hybrid IC computing method. Through adopting the parameter dhyp and lch, we utilize the new IC computing method and propose a novel comprehensive measure of semantic similarity between concepts. An experiment based on WordNet \u201cis a\u201d taxonomy has been designed to test representative measures and our measure on benchmark dataset R&G, and the results show that our measure can obviously improve the similarity accuracy. We evaluate the proposed approach by comparing the correlation coefficients between five measures (the proposed approach, four other similarity methods) and the artificial data. The results show that our proposal outperforms the previous measures.", "creator": "Microsoft\u00ae Word 2010"}}}