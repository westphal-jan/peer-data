{"id": "1703.01775", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Building a Regular Decision Boundary with Deep Networks", "abstract": "in this work, collectively we therefore build a generic architecture of systematically convolutional engineered neural output networks to discover empirical properties dependent of artificial neural networks. our true first interesting contribution model is to introduce us a state - of - < the - \u2013 art framework that itself depends slightly upon few hyper - parameters and frequencies to study hence the network when examined we vary them. it has no max pooling, no biases, only returns 13 vector layers, is purely convolutional and continuously yields up margins to 95. 4 % and 79. 6 % accuracy characteristics respectively displayed on surveys cifar10 100 and cifar100. we basically show that the nonlinearity property of a hypothetical deep network does realize not desperately need to be too continuous, not non _ expansive or uniform point - sum wise, to achieve good spectral performance. we show signs that increasing bandwidth the width of our sensor network permits being competitive with very costly deep spread networks. our successful second contribution methodology is performing an analysis studies of the contraction and separation induced properties problem of this fragmented network. indeed, a 1 - fold nearest to neighbor classifier strategy applied on underlying deep packet features progressively improves with local depth, where which probably indicates that keeping the filter representation is progressively more regular. ] besides, we defined and analyzed five local support node vectors that separate classes locally. all completed our experiments are extremely reproducible and code is privately available publicly online, based on tensorflow.", "histories": [["v1", "Mon, 6 Mar 2017 09:21:35 GMT  (76kb)", "http://arxiv.org/abs/1703.01775v1", "CVPR 2017, 8 pages"]], "COMMENTS": "CVPR 2017, 8 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["edouard oyallon"], "accepted": false, "id": "1703.01775"}, "pdf": {"name": "1703.01775.pdf", "metadata": {"source": "CRF", "title": "Building a Regular Decision Boundary with Deep Networks", "authors": ["Edouard Oyallon"], "emails": ["edouard.oyallon@ens.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n01 77\n5v 1\n[ cs\n.C V\n] 6\nM ar\nIn this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow."}, {"heading": "1. Introduction", "text": "Classification in high dimension requires building a representation that reduces a lot of variability while being discriminative. For example, in the case of images, there are geometric variabilities such as affine roto-translation, scaling changes, color changes, lighting, or intra-class variabilities such as style. Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9]. The creation of those invariants corresponds to a contraction of feature space, while separating the different classes. ConvolutionalNeural Networks (CNNs) consist of cascades of convolutional operators and non-linearities, and lead to state-of-the-art results on benchmarks where enough data are available [15, 11].\nGeometric variabilities can be successfully handled by prior representations such as scattering transforms [23, 4, 20, 19, 25]. They use cascades of wavelet transform and complex modulus to build a representation which is covariant to the action of several groups of variabilities. Invariants to those variabilities are built by linear averaging that does not require to be learned, which corresponds to a contraction of the space along the orbits of those groups. Such priors can be derived from the physical laws that permit generating natural signals, such as the euclidean group. They lead to state-of-the-art results when all the groups of variabilities of the problem are understood, since in this case one only has a problem of variance. However, in the case of complex images datasets such as CIFAR10, CIFAR100 or ImageNet, understanding the nature of the non-geometrical invariants that are built remains an open question.\nSeveral works address the problem to understand theoretically CNNs, but most of them assume the existence of a low-dimensional structure [26]. As in [22], we chose instead to have an empirical analysis. Our objective is to determine general properties which were not predictable by the theory and to characterize them rigorously. Are there restrictions on the property of a non-linearity? How can one estimate a decision boundary? Can we find layerwise properties built by the deep cascade? Is there a layerwise dimensionality reduction?\nIn a classification task, one builds an estimator of the class for a test set using the samples of a training dataset. The intrinsic dimension of the classes is a quite low-dimensional structure in comparison with the original dimension of the signals, thus a classification task requires estimating a (often non-linear) projection onto a lowdimensional space. Observe that building this estimator is equal to estimating the boundary of classification of this task. It can be done in several ways that are related to each others. First, one can estimate the group of symmetry of the classification task as suggested in [20, 4]. Those papers suggest that a deep network could potentially build a linear approximation of the group of symmetry of a classifica-\n1\ntion task. For example, in the case of a CNN, all the layers are covariant with the action of translation which is a linear sub-group of symmetry of a classification problem, and this variability should be reduced. However, the class should not collapse when an averaging in translation is performed since they would become indistiguishable and lead to classification errors, thus [20] proposes to introduce a notion of support vectors. They are vectors of different classes, which prevent the collapsing of different classes by indicating to the algorithm that it should carefully separate the different classes at those points. It means the deep network should not contract this part of the space, and they should build a classification boundary. Secondly, one could use the smoothness of the data. It means for example building an intermediate representation of the data that projects them into a lower dimensional structure, for which building estimators of the class is simpler. For instance, it can be potentially done by linearizing high dimensional symmetries and then applying a projection.\nUnderstanding the nature of the classification boundary in the deep learning framework is especially hard, because the non-linear modules used are increasingly more complex, while improving progressively the benchmarks. For example, max pooling [14], spatial transformers [13], local contrast normalization [15], attention networks [8], resnet [11], make the mathematical analysis even more difficult, since the way they are combined is mainly defined by a complex engineering process of trial and error based on the final accuracy of the network. However, [27] showed that a simple cascade of convolutions and ReLUs is enough to achieve good performances on standard datasets. The question of simplicity of a deep network is raised: what does simple mean? How simple can a deep network be, while leading to state-of-the-arts results?\nSection 2 describes our architecture that depends upon few hyper parameters but leads to excellent numerical performances. Secondly, we discuss the variation of the hyper parameters of our architectures. Then, Section 3 shows that the representation built by a deep network is progressively more regular. Finally, we introduce a notion of local support vectors which avoid the collapsing of classes, in Section 4. All experiments are reproducible using TensorFlow, via a software that is available online at: https://github.com/edouardoyallon/ deep_separation_contraction/."}, {"heading": "2. A sandbox to understand deep networks", "text": "We build a class of CNNs that depends on two hyperparameters: its width and a non-linearity. We demonstrate that this framework is flexible and simple. First, we describe the setting that permits our network to reach the state of the art. Then, we vary those two hyper-parameters and observe counter-intuitive properties: a non-linearity does\nnot need to be contractive, nor point wise, and a wide deep network generalizes better than a tight one."}, {"heading": "2.1. A spare pipeline for evaluation", "text": "We describe the architecture that we used during all our experiment, with the datasets CIFAR10 and CIFAR100. It will depend only on K \u2208 N, which is the width of our network, and \u03c1 a non-linear function. Our deep network consists of the cascade of 13 convolutional layers Wn with non-linearity \u03c1. The spatial support of the kernel is 3 \u00d7 3, and except for the first layer, the number of input and output layers is fixed equal to K . The output of the final convolutional layer is linearly and globally spatially averaged byA, and then reduced to the number of classes of the problem by a projection L. We did not learn any biases in the convolutional layers, however we subtract the mean Exn from our feature maps, which is estimated on all the dataset via the standard batch normalization technique [12]. In this case we are in a similar setting as [21], which proves that if Wn is unitary then for any depths the network preserves the energy of the input signal and is non-expansive. For computational speed-up, we apply a spatial stride of 2 at the output of the layers 6 and 10. Figure 1 describes our network, which can be formally summarized for an input x, via x0 = x, and:\nxn+1 = \u03c1Wn(xn \u2212 Exn)\nWe trained our network via a SGD with momentum 0.9 to minimize the standard negative cross-entropy. We used a batch size of 128, and the training lasts 120 000 iterations. We used an initial learning rate of 0.25, that we divided by two every 10 000 iterations. To avoid overfitting, we apply 4 regularizations. First, a weight decay of 0.0002 that corresponds to a l2 regularization. Then, we used dropout every two layers, starting at the second layer, that randomly sets 40% of the coefficients to 0: this is our main trick to achieve good performances. Thirdly, we used spatial batch normalization regularization that is supposed to remove instabilities during the training, as developed in [12]. Finally, we applied standard random flipping and cropping techniques as data augmentation. Observe that we did not use any bias, simply removing the mean and did not use any non-linear pooling. Our architecture is thus kept as simple as possible, as in [27] but it only depends only on a few hyper parameters: its width and the non-linearity. Without any contrary mentions, we used \u03c1 = ReLU since it has heuristically been shown to achieve better performances. The first layer will always have a ReLU non-linearity.\nCIFAR10 and CIFAR100 are two datasets of colored images of size 32 \u00d7 32. The training set consists of 50 000 images, that are separated into 10 and 100 balanced classes respectively for CIFAR10 and CIFAR100. The testing set consists in 10 000 images. Those datasets are preprocessed using a standard procedure of whitening.\nThe number of parameters used by our network with CIFAR10 is 9 \u00d7 (3K + 12K2) + 10K . To get our best accuracy, we used K = 512 which corresponds roughly to 28M parameters, that lead to 95.4% and 79.6% accuracies on CIFAR10 and CIFAR100 respectively, which is an excellent performance according to Table 1. Thus, we are in a state-of-the-art setting to perform an analysis of the features learned."}, {"heading": "2.2. Weakening the non-linearity", "text": "Contraction phenomenon is a necessary step to explain the tremendous dimensionality reduction of the space that occurs. A network cannot be purely linear, since some classification problems are not linearly separated: indeed a linear operator can only contract along straight lines. Should \u03c1 also be a contracting operator? We study specifically the\npointwise non linearity \u03c1 in a CNN and its necessary conditions to reach good classification accuracy."}, {"heading": "2.2.1 Unneccesity to contract via \u03c1", "text": "Since the AlexNet [15], non-linearity is often chosen to be a ReLU(x) = max(0, x). This is a non-expansive function, e.g. |ReLU(x)\u2212ReLU(y)| \u2264 |x\u2212 y|, and also continuous. Consequently, the cascade of linear operators of norm less than 1 and this non-linearity is non-expansive which is a convenient property to the reduce or maintain the volume of the data.\nModulus non-linearity in complex network have been also suggested to remove the phase of a signal, which corresponds in several frameworks to a translation variability [19, 3] . For instance, if the linear operator consists in a wavelet transform with appropriate mother wavelet [18], then the spectrum of each convolution with a wavelet is localized in Fourier. This implies that a small enough translation in the spatial domainwill also result in a phase multiplication in the spatial domain. Applying a modulus removes this variability. As a classical result of signal theory, observe also that an averaged rectified signal is approximately equal to the average of its complex envelope [18]. Consequently, cascaded with an average pooling, a ReLU and a modulus might have the same use.\nExperimentally, it is possible to build a deep network that leads to 89.0% accuracy on CIFAR10, with K = 256, with the non-linearity chosen as:\n\u03c1(x) = sign(x)( \u221a |x|+ 0.1)\nIn the neighborhood of 0, this non-linearity is not continuous in 0, has an arbitrary large derivative, and preserves the sign of the signal. It shows that continuity property, lipschitz property or removing the phase of the signals are not necessary conditions to obtain a good accuracy. It suggests that more refinement in the mathematical analysis of \u03c1 is required."}, {"heading": "2.2.2 Degree of non-linearity", "text": "In this subsection, we try to weaken the traditional property of pointwise non-linearity. Indeed, being non-linear is essential to ensure that the different classes can be separated, however the recent work on ResNet [11] suggests that it is not necessary to apply a pointwise non-linearity, thanks to identity mapping that can be interpreted as the the concatenation of a linear block (the identity) and a non-linear block. In this case, a non-linearity is applied only on a half of the feature maps. We investigate the question to understand if this property generalizes to our architecture by introducing a ReLU with a degree k K of non-linearity that we apply to a\nfeature map x(u, l), where u is the spatial variable and l the index of the feature map, defined by:\nReLUKk (x)(u, l) ,\n{\nReLU(x(u, l)), if l \u2264 k\nx(u, l), otherwise\nIn the case k = 0, we have an almost linear network (there is the ReLU non-linearity at the first layer), and when k = K , it is a standard deepnetwork with point-wise nonlinearity. Figure 2 reports the numerical accuracy when we vary k, fixing K equal to 32 or 128. A linear deep network performs poorly, leading to an accuracy of roughly 70% on CIFAR10. We see that there is a plateau when k K \u2265 0.6 = k0 K , and that the maximum accuracy is not necessary obtained for k = K . Our networks could reach 89.8% and 94.4% classification accuracy respectively for K = 32 andK = 128.\nThis is an opportunity to reinterpret the non-linearity. Let \u03c4 be a cyclic translation of {1, ...,K}, e.g. \u03c4([1, ...,K]) = [K, 1, ...,K \u2212 1], such that we define: \u03c4(x)(u, l) , x(u, \u03c4(l)). In this case, \u03c4 is a linear operator that translates cyclically the channels of a feature map. Observe that:\nReLUKk x = \u03c4 \u25e6 ReLU K 1 \u25e6 ...\u03c4 \u25e6 ReLU K 1\n\ufe38 \ufe37\ufe37 \ufe38\nk times\nx\nIn this setting, one might interpret a CNN with depth N and width K as a CNN of depth NK and width K , since it is also a cascade of NK ReLUK1 non-linearities and {\u03c4,Wn}n linear operators. In this work, \u03c4 is fixed, yet it\nmight be learned as well. It means also that if k < K , by increasing the number of layers, a CNN using a ReLU nonlinearity can be rewritten with a ReLUkK non-linearity. For k < k0, we tried to increase the depth of the deep network to recover its best accuracy since there will be as much nonlinearity as in the case k = K , and we know there exists an optimal solution. However, our network was not able to perform as well, which implies that there is an issue with the optimization. Restricting the non-linearity application to only one feature map could help future analysis, since it gives explicitly the coefficients that exhibits non-linear effects. Finally, the only hyper parameter that remains is the number of feature maps K of the layers, that we study in the next final subsection."}, {"heading": "2.3. Increasing the width", "text": "In this section, we show that increasing K increases the classification accuracy of the network. [29] reports also this observation, which is not obvious since increasing K increases by K2 the number of parameters and could lead to a severe overfitting. Besides, since a final dimensionality reduction must occur at the last layer, one could expect that the intermediate layers might have a small number of feature maps. Figure 3 and 4 reports the numerical accuracy respectively on CIFAR10 and CIFAR100, with respect to K . Setting K = 512 leads to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100, while K = 16 leads to 79.8% and 30.6% accuracy on CIFAR10 and CIFAR100 respectively. It is not clear if the reason of this improvement is the optimization or if it is a structural reason. Nevertheless, it indicates that increasing the number of feature maps is a simple way to improve the network accuracy."}, {"heading": "3. Contracting the space", "text": "Regularity of a representation with respect to the classes is necessary to classify high-dimensional samples. Regularity means here that a supervised classifier builds a covering of the full data space with training samples via \u03b5-balls that is small in term of volume or number of balls, yet that it still generalizes well [28]. The issue is that it is hard to track this measure. For example, assume the data lay on a 2D or 3D manifold and are smooth with respect to the classes, then it is possible to build a classifier which is locally affine using manifold learning techniques. In particular, this implies that the euclidean metric is locally meaningful. We take a second example: when a nearest neighbor obtains good generalization properties on a test set. In this case, our problem is regular since it means again that locally the euclidean metric is meaningful and that the representation of the training set is mostly covering the data space. We show that supervised deep networks progressively build a representation where euclidean distance becomes more meaningful. Indeed, we numerically demonstrate that the performance of local classifiers, e.g. which assign a class by giving more important weights to points of the training set that are in a neighborhood of the testing sample, progressively improves with depth.\nIn this section and the following, in order to save computation time, we used the network previously introduced with K = 32 and \u03c1 = ReLU. Numerically, we however tried several parameters as a sanity check that our conclusion generalizes to any values ofK , to avoid a loss in generality. With K = 32, the accuracy of the network is 88.0%. IncreasingK to 512 should approximatively increase all the reported results by 7 absolute percents of accuracy.\nTranslation is one of the symmetries of the image classi-\nfication problem, thus it is necessary to remove this variability, even if the features are not extracted at the final layer. In the following, we perform our experiments using x\u0304 , Ax \u2208 R32. We have the following \u01eb separation property, thanks to non-expansivity of averaging operators:\n\u2016x\u2212 y\u2016 \u2265 \u2016x\u0304\u2212 y\u0304\u2016 \u2265 \u01eb (1)\nIt means that a separation by \u01eb of the averaged signals implies a separation by at least \u01eb of any translated versions of the original signals. We denote the features at depth n of the training set by X\u0304ntrain = {x\u0304n, x \u2208 training} and the features of depth n of the testing set by X\u0304ntest = {x\u0304n, x \u2208 testing}. For a given x, xn or x\u0304n, we write its class y(x), y(xn) or y(x\u0304n) since there is no confusion possible."}, {"heading": "3.1. A progressively more regular representation", "text": "Brutal contraction via a linear projection of the space would not preserve the distances between different classes. Yet, with a cascade of linear and non linear operators, it would be possible to progressively contract the space without losing in discriminability [21]. Several works [30, 22] reported that linear separability of deep features increases with depth. It might (but this is not the only solution) indicate that intra-class variabilities are progressively linearized [20], until the last layer, such that a final linear projector can build invariants. Following this approach, we start by applying a Gaussian SVM at each depth n, which is a discriminative locally linear classifier, with a fixed bandwith. Indeed, observe here that the case of an infinite bandwidth corresponds to a linear SVM. We train it on the standardized features X\u0304ntrain corresponding to the training set at depth n and test it on X\u0304ntest, via a Gaussian SVM with bandwith equal to the average l2 norm of the points of the training set. We only cross-validate once the regularization parameter at one layer and then kept the parameters of the SVM constant. Figure 5 reports that the accuracy of this classifier increases at regular step with depth, which confirms the features become more separable.\nIn fact, we prove this Gaussian SVM acts as a local classifier. A 1 nearest neighbor (1-NN) classifier is a naive and simple non-parametric classifier for high-dimensional signals, that simply assigns to a point the class of its closest neighbor. It can be interpreted as a local classifier with adaptive bandwith [5]. It is unbiased, yet it bears a lot of variance. Besides, resampling the data will affect a lot the classification results. We train a 1-NN on X\u0304ntrain and test it on X\u0304ntest. We denote by a x (k) the result of the k-th closest neighbor of a point, that is distinct of itself. We observe in Figure 5 that a 1-NN trained on X\u0304ntrain and tested on X\u0304 n test performs nearly as well as a Gaussian SVM. Besides, the progression of this classifier is almost linear with respect to the depth. It means that the representation built by a deep network is in fact progressively more regular, which\nexplainswhy the Gaussian SVM accuracy progressively improves."}, {"heading": "3.2. A progressive reduction of the space", "text": "We want to quantify the contraction of the space performed by our CNN. In this subsection, we show that the samples of a same class define a structure that progressively becomes low-dimensional. We investigate the question to understand if the progressive improvement of the 1-NN is due to a dimensionality reduction. First, we check wether a linear dimensionality reduction is implemented by our CNN. To this end, we apply a PCA on the features X\u0304ntrain belonging to the same class at each depth n. As a normalization, the features at each depth n were globally standardized. In other words, the data at each depth are in the l2 balls of radius 32. Remember that in our case, x\u0304n \u2208 R 32. Figure 6 represents the cumulated variances of the K = 32 principal component axis of a given class at different depth n. The obtained diagram and conclusions are not specific to this class. The accumulated variance indicates the proportion of energy that is explained by the first axis. The slope of a curve is an indicator of the dimension of the classes: as a plateau is reached, the last components are not useful to represent the class for classifiers based on l2 distances. The first observation is that the variance seems to be uniformly reduced with depth. However, certain plots of successive depths are almost indisguishable: it indicates that almost no variance reduction has been performed. Except for the last layer, the decay of the 20 last eigenvalues is slow: this is not surprising since nothing indicates that the dimension should be reduced and small variance coefficients might be important for the classification task. The very last layer exhibits a large variance reduction and is low-dimensional, yet this\nis logical since by construction, the final features should be linearly separable and be in space of dimension 10.\nWe then focus on the contraction of the intra-class distances. As a cascade of non-expansive operators, a deep network is also non-expansive up to a multiplicative constant. In particular, the intra-class distances should be smaller. In Subsection 3.1, we observed that the euclidean metric was in fact meaningful: this could indicate low-dimensional manifold structure. Under this hypothesis, in a similar fashion as [10], we study the average intra-class distances. As a normalization, the features X\u0304ntrain at depth n are standardized over the dataset. On CIFAR10 (where each of the 10 classes has 5000 samples) we compute an estimation of the average distances of the intra-class samples of the features X\u0304ntrain at depth n for the class c:\n1\n50002\n\u2211\nx\u0304n\u2208X\u0304 n\ntrain\ny(xn)=c\n\u2211\nx\u0304\u2032n\u2208X\u0304 n\ntrain\ny(x\u2032 n )=c\n\u2016x\u0304n \u2212 x\u0304\u2032n\u2016\nFigure 7 reports this value for different classes c and different depths n. One sees that the intra-class distances do not strictly decrease with depth, except on the last layer, which must be low-dimensional since the features are, up to projection, in a space of size 10. This is due to two phenomena: the normalization procedure whose choice can drastically change the final results and the averaging. Indeed, let us assume here that \u2016Wn\u2016 \u2264 1, then if x, x\u0303 are in the same class, \u2016xn+1 \u2212 x\u0303n+1\u2016 \u2264 \u2016xn \u2212 x\u0303n\u2016, but this does not imply that \u2016x\u0304n+1 \u2212 x\u0304\u2032n+1\u2016 \u2264 \u2016x\u0304n \u2212 x\u0304\u2032n\u2016, since the averaging is a projection and could break the distance inequality.\nThose two experiments indicate we need to refine our measurement of contraction to explain the progressive and\nconstant improvement of a 1-NN. Specifically, one should estimate the local intrinsic dimension: this is not possible since we do not have enough available samples in each neighborhood [7]."}, {"heading": "4. Local support vectors for building a variable", "text": "bandwidth classification boundary\nOur objective is to quantify at each depth n, the regularity of the representation constructed by a deep net in order to understand the progressive contraction of the space. In other words, we need to build a measure of this regularity. The contraction of the space is global, but we know from below that neighbors are meaningful: we woud like to explain how they separate the different classes. We thus introduce a notion of local support vectors. In the case of a SVM, a support vector corresponds to samples of the training set that delimit different classes, by interpolating a hyperplane between them [6]. It means that a support vectors permits to avoid the collapsing of the boundary classification [20]. But in our case, we do not have enough samples to estimate the exact boundaries. Local support vectors corresponds to support vectors defined by a local neighborhood. In other words, at depth n, the set of support vectors is defined as \u0393n = {x\u0304n, y(x\u0304 (1) n ) 6= y(x\u0304n)} \u2282 X\u0304 n train which is the set of nearest neighbors that have a different class. In this section for a finite set X , we denote its cardinality by |X |."}, {"heading": "4.1. Margin", "text": "In this subsection, we numerically observe a margin between support vectors. In [26], bounds on the margin are obtainedwith hypothesis of low dimensional structures, and\nthis might be restrictive according to the analysis above. A margin at depth n is defined as:\n\u03b3n = inf y(x\u0304 (1) n ) 6=y(x\u0304n) \u2016x\u0304(1)n \u2212 x\u0304n\u2016 \u2265 0\nSince our data are in finite number this quantity is always different from 0, but we need to measure if it is significant. We thus compare the distributions of distances of nearest neighbors belonging to the same class An = {\u2016x\u0304 (1) n \u2212 x\u0304n\u2016, x\u0304n 6\u2208 \u0393n} and the distributions of the distances between support vectorsBn = {\u2016x\u0304 (1) n \u2212 x\u0304n\u2016, x\u0304n \u2208 \u0393n}. The features have been normalized by a standardization. Figure 8 represents the cumulative distributions of An and Bn for different depths n. We recall that a cumulative distribution of a finite set A \u2282 R is defined as: A(t) = |{x\u2264t,x\u2208A}||A| . One observes that in a neighborhood of 0,An(t) is roughly the translation of Bn(t) by 0.5. It indicates there is a significant difference, showing \u03b3n is actually meaningful. Consequently there exists a margin between the spatially averaged samples of different classes, which means by Equation (1) that this margin exists between the samples themselves and their orbits by the action of translations."}, {"heading": "4.2. Complexity of the classification boundary", "text": "Estimating a local intrisic dimension is difficult when few samples per neighborhood are available, but the classes of the neighbors of the samples of X\u0304train are known. In this subsection, we build a measure of the complexity of\nthe classification boundary based on neighbors. This permits evaluating both separation and contraction properties. It can be viewed as a weak estimation of the intrisic dimension [2], even if the manifold hypothesis might not hold. We compute an estimate of the efficiency of a k-NN to correctly find the label of a local support vector. To this end, we define by recurrence at depth n and for a given k \u2208 N, which is a number of neighbors, \u0393kn via \u0393 1 n = \u0393n and:\n\u0393k+1n = { x\u0304n \u2208 \u0393 k n, |{y(x\u0304 (l) n ) 6= y(x\u0304n), l \u2264 k + 1}| > k\n2\n}\nIn other words, \u0393kn is the set of points at depth n that are not well-classified by l-NNs using majority vote, for l \u2264 k. By construction,\u0393k+1n \u2282 \u0393 k n which implies that |\u0393 k+1 n | \u2264 |\u0393 k n|. Since the number of samples is finite, this sequence converges to the number of samples of the training set that can not be identified by their nearest neighbors. The decay and the amplitude of |\u0393kn| is an indicator of the regularity of the classification boundary. Recall that for a deep network, the 1-NN classifier has better generalization properties with deeper features. A small value of |\u0393kn| indicates that a few samples are necessary to build the classification boundary (contraction), and at a given depth n, if |\u0393kn| decreases quickly to its constant value, it means a few neighbors are required to build the decision boundary (separation). Figure 9 indicates that the classification boundary is uniformly more regular with depth, in term of number of local support vectors and number of neighbors required to estimate the correct class. This measure has the advantage of being simple to compute, yet this analysis must be refined in a future work."}, {"heading": "5. Conclusion", "text": "In this work, we simplified a standard deep network that still reaches good accuracies on CIFAR10 and CIFAR100. We studied the influence of different hyperparameters such as the non-linearity and the number of feature maps. We demonstrate that the performance of a nearest neighbors classifier applied at different depth increases and that this classifier is almost as discriminative as a Gaussian SVM. Finally, we defined local support vectors that allow us to build a measure of the contraction and separation properties of the built representation. They could permit to potentially improving the classification accuracy, by refining the boundary classification of a CNN in their neighborhood.\nWe have built a class of deep networks that uses only pointwise non-linearities and convolutions and that should help for future analysis. In a context where the use of deep networks is increasing impressively, understanding the nature of the intrisic regularity they exploit is mandatory. Solving this problem will help finding theoretical guarantees on deep networks for applications and must be the topic of future research."}, {"heading": "Acknowledgement", "text": "I would like to thank Mathieu Andreux, Toma\u0301s Angles, Bogdan Cirstea, Michael Eickenberg and Ste\u0301phane Mallat for helpful discussions and comments. This work is funded by the ERC grant InvariantClass 320959 and via a grant for PhD Students of the Conseil re\u0301gional d\u2019Ile-de-France (RDM-IdF)."}], "references": [{"title": "Understanding deep features with computer-generated imagery", "author": ["M. Aubry", "B.C. Russell"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2875\u20132883,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Intrinsic dimension identification via graph-theoretic methods", "author": ["M. Brito", "A. Quiroz", "J.E. Yukich"], "venue": "Journal of Multivariate Analysis, 116:263\u2013277,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1872\u20131886,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning stable group invariant representations with convolutional networks", "author": ["J. Bruna", "A. Szlam", "Y. LeCun"], "venue": "arXiv preprint arXiv:1301.3537,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Location-adaptive density estimation and nearest-neighbor distance", "author": ["P. Burman", "D. Nolan"], "venue": "Journal of multivariate analysis, 40(1):132\u2013157,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Estimating local intrinsic dimension with k-nearest neighbor graphs", "author": ["J.A. Costa", "A. Girotra", "A. Hero"], "venue": "IEEE/SP 13th Workshop on Statistical Signal Processing, 2005, pages 417\u2013422. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["M. Denil", "L. Bazzani", "H. Larochelle", "N. de Freitas"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Manitest: Are classifiers really invariant", "author": ["A. Fawzi", "P. Frossard"], "venue": "arXiv preprint arXiv:1507.06535,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["R. Giryes", "G. Sapiro", "A.M. Bronstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "Y. Lecun"], "venue": "In 2009 IEEE 12th International Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["K. Lenc", "A. Vedaldi"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 991\u2013999,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Sgdr: Stochastic gradient descent with restarts", "author": ["I. Loshchilov", "F. Hutter"], "venue": "arXiv preprint arXiv:1608.03983,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Communications on Pure and Applied Mathematics, 65(10):1331\u20131398,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "Phil. Trans. R. Soc. A, 374(2065):20150203,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning by scattering", "author": ["S. Mallat", "I. Waldspurger"], "venue": "arXiv preprint arXiv:1306.5532,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring how deep neural networks form phonemic categories", "author": ["T. Nagamine", "M.L. Seltzer", "N. Mesgarani"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep roto-translation scattering for object classification", "author": ["E. Oyallon", "S. Mallat"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2865\u2013 2873,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Rotation, scaling and deformation invariant scattering for texture discrimination", "author": ["L. Sifre", "S. Mallat"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1233\u20131240,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust large margin deep neural networks", "author": ["J. Sokolic", "R. Giryes", "G. Sapiro", "M.R. Rodrigues"], "venue": "arXiv preprint arXiv:1605.08254,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods of pattern recognition", "author": ["V.N. Vapnik"], "venue": "The nature of statistical learning theory, pages 123\u2013180. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Wide Residual Networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pages 818\u2013833. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Residual networks of residual networks: Multilevel residual networks", "author": ["K. Zhang", "M. Sun", "T.X. Han", "X. Yuan", "L. Guo", "T. Liu"], "venue": "arXiv preprint arXiv:1608.02908,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 62, "endOffset": 69}, {"referenceID": 23, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 89, "endOffset": 101}, {"referenceID": 21, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 89, "endOffset": 101}, {"referenceID": 29, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 89, "endOffset": 101}, {"referenceID": 2, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 166, "endOffset": 176}, {"referenceID": 19, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 166, "endOffset": 176}, {"referenceID": 8, "context": "Deep networks have been shown to be covariant to such actions [1, 16], to linearize them [24, 22, 30] and combing those strategies permit building invariants to them [3, 20, 9].", "startOffset": 166, "endOffset": 176}, {"referenceID": 14, "context": "ConvolutionalNeural Networks (CNNs) consist of cascades of convolutional operators and non-linearities, and lead to state-of-the-art results on benchmarks where enough data are available [15, 11].", "startOffset": 187, "endOffset": 195}, {"referenceID": 10, "context": "ConvolutionalNeural Networks (CNNs) consist of cascades of convolutional operators and non-linearities, and lead to state-of-the-art results on benchmarks where enough data are available [15, 11].", "startOffset": 187, "endOffset": 195}, {"referenceID": 22, "context": "Geometric variabilities can be successfully handled by prior representations such as scattering transforms [23, 4, 20, 19, 25].", "startOffset": 107, "endOffset": 126}, {"referenceID": 3, "context": "Geometric variabilities can be successfully handled by prior representations such as scattering transforms [23, 4, 20, 19, 25].", "startOffset": 107, "endOffset": 126}, {"referenceID": 19, "context": "Geometric variabilities can be successfully handled by prior representations such as scattering transforms [23, 4, 20, 19, 25].", "startOffset": 107, "endOffset": 126}, {"referenceID": 18, "context": "Geometric variabilities can be successfully handled by prior representations such as scattering transforms [23, 4, 20, 19, 25].", "startOffset": 107, "endOffset": 126}, {"referenceID": 24, "context": "Geometric variabilities can be successfully handled by prior representations such as scattering transforms [23, 4, 20, 19, 25].", "startOffset": 107, "endOffset": 126}, {"referenceID": 25, "context": "Several works address the problem to understand theoretically CNNs, but most of them assume the existence of a low-dimensional structure [26].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "As in [22], we chose instead to have an empirical analysis.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "First, one can estimate the group of symmetry of the classification task as suggested in [20, 4].", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "First, one can estimate the group of symmetry of the classification task as suggested in [20, 4].", "startOffset": 89, "endOffset": 96}, {"referenceID": 19, "context": "However, the class should not collapse when an averaging in translation is performed since they would become indistiguishable and lead to classification errors, thus [20] proposes to introduce a notion of support vectors.", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "For example, max pooling [14], spatial transformers [13], local contrast normalization [15], attention networks [8], resnet [11], make the mathematical analysis even more difficult, since the way they are combined is mainly defined by a complex engineering process of trial and error based on the final accuracy of the network.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "For example, max pooling [14], spatial transformers [13], local contrast normalization [15], attention networks [8], resnet [11], make the mathematical analysis even more difficult, since the way they are combined is mainly defined by a complex engineering process of trial and error based on the final accuracy of the network.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "For example, max pooling [14], spatial transformers [13], local contrast normalization [15], attention networks [8], resnet [11], make the mathematical analysis even more difficult, since the way they are combined is mainly defined by a complex engineering process of trial and error based on the final accuracy of the network.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "For example, max pooling [14], spatial transformers [13], local contrast normalization [15], attention networks [8], resnet [11], make the mathematical analysis even more difficult, since the way they are combined is mainly defined by a complex engineering process of trial and error based on the final accuracy of the network.", "startOffset": 112, "endOffset": 115}, {"referenceID": 10, "context": "For example, max pooling [14], spatial transformers [13], local contrast normalization [15], attention networks [8], resnet [11], make the mathematical analysis even more difficult, since the way they are combined is mainly defined by a complex engineering process of trial and error based on the final accuracy of the network.", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "However, [27] showed that a simple cascade of convolutions and ReLUs is enough to achieve good performances on standard datasets.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "We did not learn any biases in the convolutional layers, however we subtract the mean Exn from our feature maps, which is estimated on all the dataset via the standard batch normalization technique [12].", "startOffset": 198, "endOffset": 202}, {"referenceID": 20, "context": "In this case we are in a similar setting as [21], which proves that if Wn is unitary then for any depths the network preserves the energy of the input signal and is non-expansive.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Thirdly, we used spatial batch normalization regularization that is supposed to remove instabilities during the training, as developed in [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 26, "context": "Our architecture is thus kept as simple as possible, as in [27] but it only depends only on a few hyper parameters: its width and the non-linearity.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "6 SGDR [17] 28 150M 96.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "3 RoR [31] 58 13M 96.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "3 WResNet [29] 28 37M 95.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "0 All-CNN [27] 9 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Since the AlexNet [15], non-linearity is often chosen to be a ReLU(x) = max(0, x).", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "Modulus non-linearity in complex network have been also suggested to remove the phase of a signal, which corresponds in several frameworks to a translation variability [19, 3] .", "startOffset": 168, "endOffset": 175}, {"referenceID": 2, "context": "Modulus non-linearity in complex network have been also suggested to remove the phase of a signal, which corresponds in several frameworks to a translation variability [19, 3] .", "startOffset": 168, "endOffset": 175}, {"referenceID": 17, "context": "For instance, if the linear operator consists in a wavelet transform with appropriate mother wavelet [18], then the spectrum of each convolution with a wavelet is localized in Fourier.", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "As a classical result of signal theory, observe also that an averaged rectified signal is approximately equal to the average of its complex envelope [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "Indeed, being non-linear is essential to ensure that the different classes can be separated, however the recent work on ResNet [11] suggests that it is not necessary to apply a pointwise non-linearity, thanks to identity mapping that can be interpreted as the the concatenation of a linear block (the identity) and a non-linear block.", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "[29] reports also this observation, which is not obvious since increasing K increases by K the number of parameters and could lead to a severe overfitting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Regularity means here that a supervised classifier builds a covering of the full data space with training samples via \u03b5-balls that is small in term of volume or number of balls, yet that it still generalizes well [28].", "startOffset": 213, "endOffset": 217}, {"referenceID": 20, "context": "Yet, with a cascade of linear and non linear operators, it would be possible to progressively contract the space without losing in discriminability [21].", "startOffset": 148, "endOffset": 152}, {"referenceID": 29, "context": "Several works [30, 22] reported that linear separability of deep features increases with depth.", "startOffset": 14, "endOffset": 22}, {"referenceID": 21, "context": "Several works [30, 22] reported that linear separability of deep features increases with depth.", "startOffset": 14, "endOffset": 22}, {"referenceID": 19, "context": "It might (but this is not the only solution) indicate that intra-class variabilities are progressively linearized [20], until the last layer, such that a final linear projector can build invariants.", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "It can be interpreted as a local classifier with adaptive bandwith [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "Under this hypothesis, in a similar fashion as [10], we study the average intra-class distances.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Specifically, one should estimate the local intrinsic dimension: this is not possible since we do not have enough available samples in each neighborhood [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "In the case of a SVM, a support vector corresponds to samples of the training set that delimit different classes, by interpolating a hyperplane between them [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 19, "context": "It means that a support vectors permits to avoid the collapsing of the boundary classification [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "In [26], bounds on the margin are obtainedwith hypothesis of low dimensional structures, and 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "It can be viewed as a weak estimation of the intrisic dimension [2], even if the manifold hypothesis might not hold.", "startOffset": 64, "endOffset": 67}], "year": 2017, "abstractText": "In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow.", "creator": "LaTeX with hyperref package"}}}