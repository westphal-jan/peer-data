{"id": "1703.02905", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Learning a Unified Control Policy for Safe Falling", "abstract": "being able self to independently fall safely continuously is theoretically a necessary learned motor skill for humanoids performing successfully highly effective dynamic tasks, addressing such techniques as downhill running and jumping. we propose a new method to learn a policy that minimizes executing the maximal impulse during analyzing the fall. the optimization approach solves for learning both a tandem discrete tag contact planning problem tasks and achieve a continuous optimal control problem. once trained, the policy can specifically compute the optimal next goal contacting body part ( and e. g. left foot, fetal right centre foot, calves or hands ), contact location and timing, steering and the independently required joint reward actuation. we essentially represent the same policy as a mixture complexity of actor - person critic complex neural behavioral network, among which uniformly consists of n control policies detecting and managing the corresponding value functions. taking each unique pair of participating actor - critic is alternately associated with one each of several the n possible contacting system body interface parts. during automatic execution, the policy path corresponding ideally to the third highest actual value reduction function normally will more be effectively executed together while lowering the associated body part will be approaching the next maximal contact with the ground. with this mixture regardless of actor - critic architecture, the same discrete contact sequence target planning approach is solved through the selection of the best critics objectives while instead the overall continuous access control problem is solved by the optimization algorithms of selecting actors. we furthermore show that our policy choice can help achieve comparable, sometimes even higher, rewards than a different recursive search possible of employing the action space algorithm using dynamic programming, while enjoying 50 to 400 times of speed gain during online execution.", "histories": [["v1", "Wed, 8 Mar 2017 16:38:21 GMT  (2688kb,D)", "https://arxiv.org/abs/1703.02905v1", null], ["v2", "Thu, 20 Apr 2017 15:17:35 GMT  (2688kb,D)", "http://arxiv.org/abs/1703.02905v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["visak cv kumar", "sehoon ha", "c karen liu"], "accepted": false, "id": "1703.02905"}, "pdf": {"name": "1703.02905.pdf", "metadata": {"source": "CRF", "title": "Learning a Unified Control Policy for Safe Falling", "authors": ["Visak C.V. Kumar", "Sehoon Ha", "C. Karen Liu"], "emails": ["visak3@gatech.edu,", "karenliu@cc.gatech.edu", "sehoon.ha@disneyresearch.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe advent of reinforcement learning in recent years has expanded the capability of agents to perform highly dynamic motor tasks, such as running or jumping. However, most policy optimization methods for dynamic tasks have been demonstrated in simulation only. To deploy the recent RL algorithms on full-body humanoids in the real world, one must overcome the challenge of preventing detrimental falls during the initial exploration phase of the learning algorithm. As such, the first policy a humanoid must acquire before learning other dynamic tasks is perhaps the skill of safe fall.\nAs shown in Ha and Liu [1], the falling problem can be formulated as a Markov Decision Process (MDP) that solves for a contact sequence with the ground such that the damage incurred during the fall is minimized. Their work provides an unifying view over various falling strategies designed for specific scenarios; instead of switching among a collection of individual strategies, a single unified algorithm can plan for appropriate responses to a wide variety of falls. However, using dynamic programming to solve a MDP, as proposed in [1], is computationally demanding and infeasible to deploy to the real world. Consequently, the controller can only act in an open-loop fashion without any ability to re-plan during\n1 Visak C.V. Kumar and C. Karen Liu is with the School of Interactive Computing, Georgia Institute of Technology, GA 30308. visak3@gatech.edu, karenliu@cc.gatech.edu\n2 Sehoon Ha is with Disney Research (The work was done at Georgia Institute of Technology). sehoon.ha@disneyresearch.com\nthe course of the fall. Such a controller is extremely brittle when executing a long sequence of contacts.\nOne promising direction toward a fast and robust falling controller is to learn a policy through reinforcement learning. Indeed, the MDP formulation of the falling problem [1] makes it innately suitable for algorithms that take advantage of the recursive Bellman equation. While a rich body of literature in RL can potentially be applicable, our problem is unique in that the action space consists of both discrete and continuous variables. That is, the discrete problem of contact sequence planning and the continuous problem of joint torque optimization need to be solved simultaneously.\nWe proposes a new policy optimization method to learn the appropriate actions for minimizing the damage of a humanoid fall. Our algorithm learns a policy to minimize the maximal impulse occurred during the fall. The actions in this problem include discrete decisions on the sequence of body parts used to contact the ground (e.g. contact the ground with left foot, right foot, and then both hands) and continuous decisions on location and timing of the contacts, as well as the joint torques of the robot. Our algorithm is based on CACLA [2], [3] which solves an actor-critic controller for continuous state and action spaces. We adapt their actor-critic model to solve for both discrete and continuous actions using the architecture, MACE, proposed by Peng et al. [4].\nOur algorithm trains n control policies (actors) and the corresponding value functions (critics) in a single interconnected neural network (Figure 3). Each policy and its corresponding value function are associated with a candidate contacting body part, which is designed or preferred to be a contact point with the ground, such as hands, feet, or knees. During policy execution, the network is queried every time the robot establishes a new contact with the ground, allowing the robot to re-plan throughout the fall. Based on the current state of the robot, the actor corresponding to the critic with the highest value will be executed while the associated body part will be the next contact with the ground. With this mixture of actor-critic architecture, the discrete contact sequence planning is solved through the selection of the best critics while the continuous control problem is solved by the optimization of actors.\nWe demonstrate that our algorithm reliably reduces the maximal impulse induced by a fall on both simulated humanoids and on the actual hardware. Different contact strategies emerge as the initial conditions vary. We also show that the algorithm can run in real time (as opposed to 1-10 seconds reported by [1]) once the policy is trained. Comparing to the actions computed by Ha and Liu [1], our policy produces better rewards on average with only 0.25%\nar X\niv :1\n70 3.\n02 90\n5v 2\n[ cs\n.R O\n] 2\n0 A\npr 2\n01 7\nto 2% of computation time."}, {"heading": "II. RELATED WORK", "text": "Researchers have proposed various motion planning and optimal control algorithms to reduce the damage of humanoid falls. One possible approach is to design a few falling motion sequences for a set of expected scenarios. When a fall is detected, the sequence designed for the most similar falling scenario is executed [5], [6]. This approach, albeit simple, can be a practical solution in an environment in which the types of falls can be well anticipated. To handle more dynamic environments, a number of researchers cast the falling problem to an optimization which minimizes the damage of the fall. To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1]. In spite of the effort to reduce the computation, most of the optimization-based techniques are still too slow for real-time applications, with the exception of the work done by Goswami et al. [11], who proposed to compute the optimal stepping location to change the falling direction. In contrast, our work takes the approach of policy learning using deep reinforcement learning techniques. Once trained, the policy is capable of handling various situations with real-time computation.\nOur work is also built upon recent advancement in deep reinforcement learning (DRL). Although the network architecture used in this work is not necessarily \u201ddeep\u201d, we borrow many key ideas from the DRL literature to enable training a large network with 278976 variables. The ideas of \u201dexperience replay\u201d and \u201dtarget network\u201d from Mnih et al. [12] are crucial to the efficiency and stability of our learning process, despite that the original work (DQN) is designed for learning Atari video games from pixels with the assumption that the action space is discrete. Lilicrap et al. [13] later combined the ideas of DQN and the deterministic policy gradient (DPG) [14] to learn actor-critic networks for continuous action space and demonstrated that end-to-end (vision perception to actuation) policies for dynamic tasks can be efficiently trained.\nThe approach of actor-critic learning has been around for many decades [15]. The main idea is to simultaneously learn the state-action value function (the Q-function) and the policy function, such that the intractable optimization of the Q-function over continuous action space can be avoided. van Hasselt and Wiering introduced CACLA [2], [3] that represents an actor-critic approach using neural networks. Our work adopts the update scheme for the value function and the policy networks proposed in CACLA. Comparing to the recent work using actor-critic networks [13], [16], the main difference of CACLA (and our work as well) lies in the update scheme for the actor networks. That is, CACLA updates the actor by matching the action samples while other methods follow the gradients of the accumulated reward function. Our work is mostly related to the MACE algorithm introduced by Peng et al. [4]. We adopt their\nnetwork architecture and the learning algorithm but for a different purpose: instead of using multiple actor-critic pairs to switch between experts, we exploit this architecture to solve an MDP with a mixture of discrete and continuous action variables."}, {"heading": "III. PRELIMINARIES", "text": "For completeness we briefly review the MDP formulation of the falling problem proposed by Ha and Liu [1]. The objective of the problem is to find a sequence of actions such that the maximal impulse during the fall is minimized. The value function of such an optimization can be expressed as:\nV (s) = min a max(g(s,a), V (f(s,a))), (1)\nwhere s \u2208 S and a \u2208 A are the state space and the action. The function g(s,a) : S \u00d7 A 7\u2192 R evaluates the impulse induced by taking the action a at the state s. The transition function, s\u2032 = f(s,a), computes the next state s\u2032 by integrating the dynamic equations of the system. To solve the falling problem efficiently, Ha and Liu [1] proposed an abstract model to approximate the motion of the humanoid (Figure 1(a)). The abstract model consists of an inverted pendulum with a massless telescopic rod that connects the center of mass (COM) and the center of pressure in the sagittal plane. Furthermore, a second massless telescopic rod, called a \u201dstopper\u201d, is attached to the COM and used to approximate the reactive limb motion in human falls. Multiple abstract models are used to represent a sequence of contacts. As the current stopper hits the ground, a new abstract model is initialized with the pivot locating at the tip of the current stopper. This abstract model can be used to represent any contact point on the robot, but in practice the robot has only a limited number of preferred contacting body parts, such as feet or hands. As a result, there exists a finite set of contact sequences achievable by a given robot.\nUsing the abstract models to formulate the MDP, the state at each impact moment is defined as s =\n{c1, t, r1, \u03b81, r\u03071, \u03b8\u03071}, where c1 denotes the index of the contacting body part, t denotes the elapsed time from the beginning of the fall until this impact occurs, and other parameters describe the position and the velocity of the pendulum at the impact moment detailed in Figure 1(a). An action a = {c2, \u03b82,\u2206, r\u0307d1} describes the index of the contacting body part used as the stopper (c2), the position of the stopper at the next impact moment (\u03b82), the elapse time from the previous impact moment to the next impact moment (\u2206), and the desired speed of the pendulum length during the current contact r\u0307d1 .\nDetermining the best action from a given state is a 4D search problem with a mixture of continuous and discrete action variables. Ha and Liu [1] discretized the continuous action parameters and exploited the monotonic nature of the falling problem to accelerate the dynamic programming. However, the computation of the algorithm is still far from real-time (1 \u2212 10 seconds computation time after the robot receives an external force) and impractical to deploy in the real world."}, {"heading": "IV. METHOD", "text": "We present a new approach to the falling problem using a reinforcement learning framework. Our goal is to learn a control policy capable of computing optimal contact sequence, contact timing and locations, and joint actuation in real time. The overall policy consists of two components: abstract-level policy and joint-level policy. The abstract-level determines the optimal action on the abstract model, while the joint-level policy maps the abstract action to the full body space of the robot and produces joint actuation.\nFigure 2 illustrates the workflow of the overall policy. We first define a function p : X 7\u2192 S which maps a state of robot (x \u2208 X ) to a state of abstract model (s \u2208 S). The mapping can be easily done because the full set of joint position and velocity contains all the necessary information to compute s. As the robot receives an external force initially, the state of the robot is projected to S and fed into the abstractlevel policy. The action (a) computed by the abstract-level policy is passed into the joint-level policy to compute the corresponding joint torques (\u03c4 ). If no new contact is detected after executing \u03c4 , the new state of the robot will be fed back into the joint-level policy and a new \u03c4 will be computed (the lower feedback loop in Figure 2). If the robot encounters a new contact, we re-evaluate the contact plan by querying the\nabstract-level policy again (the upper feedback loop in Figure 2)."}, {"heading": "A. Abstract-level Policy", "text": "To overcome the challenge of optimizing over both discrete and continuous action variables, we introduce a new policy representation based on a neural network architecture inspired by MACE [4]. The policy takes as input the state of the abstract model and outputs the action, as well as the next contacting body part. The state space is defined as s = {c1, r1, \u03b81, r\u03071, \u03b8\u03071}, where the total elapsed time t is removed from the state space previously defined by Ha and Liu [1]. As a side benefit of a feedback policy, we no longer need to keep track of the total elapsed time. For the action space, we remove the discrete action variable, c2 so that the new action is a continuous vector in R3: a = {\u03b82,\u2206, r\u0307d1}. The network combines n pairs of actor-critic subnets, each of which is associated with a contacting body part. Each actor, \u03a0i(s) : R5 7\u2192 R3, represents a control policy given that the i-th contacting body part is chosen to be the next contact. Each critic, Vi(s) : R5 7\u2192 R, represents the value function that evaluates the return (long-term reward) of using the ith contacting body part as the next contact and taking the action computed by \u03a0i(s) as the next action. We fuse all n actor-critic pairs in one single network with a shared input layer (Figure 3).\nAt each impact moment when a new contact is established, the network evaluates all the Vi(s), 1 \u2264 i \u2264 n, and selects the policy corresponding to the highest critic. This arrangement allows us to train n experts, each specializes to control the robot when a particular contacting body part is selected to be the next contact. As a result, we cast the discrete contact planning into the problem of expert selection, while simultaneously optimizing the policy in continuous space.\n1) Reward: Since our goal is to minimize the maximal impulse, we define the reward function as:\nr(s,a) = 1\n1 + j , (2)\nwhere j is the impulse induced by the contact. Suppose the COM of the pendulum and the tip of the stopper are (x1, y1)\nand (x2, y2) respectively at the impact moment, the impulse can be computed as:\nj = \u2212 y\u0307 \u2212 2\n1 M + 1 I (x2 \u2212 x1)2\n, (3)\nwhere M is the mass and I is the inertia of the abstract model (see details in [1]).\nWith this definition of the reward function, the objective of the optimization is to maximize the minimal reward during the fall.\n2) Learning Algorithm: Algorithm 1 illustrates the process of learning the abstract-level policy. We represent the policy using a neural network consisting n pairs of actorcritic subnets with a shared input layer (Figure 3). Each critic has two hidden layers with 32 neurons each. The first hidden layer is shared among all the critics. Each actor has 3 hidden layers with 128 neurons each. All the hidden layers use tanh as the activation functions. We define weights and biases of the network as \u03b8, which is the unknown vector we attempt to learn.\nThe algorithm starts off with generating an initial set of high-reward experiences, each of which is represented as a tuple: \u03c4 = (s,a, s\u2032, r, c), where the parameters are the starting state, action, next state, reward, and the next contacting body part. To ensure that these tuples have high reward, we use the dynamic-programming-based algorithm described in [1] (referred as DP thereafter) to simulate a large amount of rollouts from various initial states and collect tuples. Filling the training buffer with these high-reward experiences accelerates the learning process significantly. In addition, the high-reward tuples generated by DP can guide the abstract-level policy to learn \u201dachievable actions\u201d when executed on a full body robot. Without the guidance of these tuples, the network might learn actions that increase the return but unachievable under robot\u2019s kinematic constraints.\nIn addition to the initial experiences, the learning algorithm continues to explore the action space and collect new experiences during the course of learning. At each iteration, we simulate K(= 10) rollouts starting at a random initial state sampled from a Gaussian distribution N0 and terminating when the abstract model comes to a halt. A new tuple is generated whenever the abstract model establishes a new contact with the ground. The exploration is done by stochastically selecting the critic and adding noise in the chosen actor. We follow the same Boltzmann exploration scheme as in [4] to select the actor \u03a0i(s) based on the probability defined by the predicted output of the critics:\nPi(s) = eVi(s|\u03b8)/Tt\u2211 j e Vj(s|\u03b8)/Tt , (4)\nwhere Tt(= 5) is the temperature parameter, decreasing linearly to zero in the first 250 iterations. While the actor corresponding to the critic with the highest value is most likely to be chosen, the learning algorithm occasionally explores other actor-critic pairs, essentially trying other possible contacting body parts to be the next contact. Once an actor is selected, we add a zero-mean Gaussian noise to the\noutput of the actor. The covariance of the Gaussian is a userdefined parameter.\nAfter K rollouts are simulated and tuples are collected, the algorithm proceeds to update the critics and actors. In critic update, a minibatch is first sampled from the training buffer with m(= 32) tuples, \u03c4i = (si,ai, s\u2032i, ri, ci). We use the temporal difference to update the chosen critic similar to [12]:\nyi = min(ri, \u03b3maxj V\u0302j(s \u2032 i)) (5) \u03b8 \u2190 \u03b8 + \u03b1 \u2211 i(yi \u2212 Vci(s))\u2207\u03b8Vci(s)\nwhere \u03b8 is updated by following the negative gradient of the loss function \u2211 i(yi \u2212 Vci(s))2 with the learning rate \u03b1(= 0.0001). The discount factor is set to \u03b3 = 0.9. Note that we also adopt the idea of target network from [12] to compute the target, yi, for the critic update. We denote the target networks as V\u0302 (s).\nThe actor update is based on supervised learning where the policy is optimized to best match the experiences: min\u03b8 \u2211 i \u2016ai \u2212 \u03a0ci(si)\u20162. We use the positive temporal difference to decide whether matching a particular tuple is advantageous:\ny = maxj Vj(si) (6)\ny\u2032 = min(ri, \u03b3maxj V\u0302j(s \u2032 i))\nif y\u2032 > y, \u03b8 \u2190 \u03b8 + \u03b1(\u2207\u03b8\u03a0ci(s))T (ai \u2212\u03a0ci(s))."}, {"heading": "B. Joint-level Policy", "text": "The goal of the joint-level policy is to execute the action computed by the abstract-level policy in the joint space of the robot. Recall that the abstract-level policy outputs an action vector a = {\u03b82,\u2206, r\u0307d1} and the next contacting body part c. Together with the current state s = {c1, r1, \u03b81, r\u03071, \u03b8\u03071}, we can define a triangle formed by the pivot of the pendulum (v1), the mass point (v2), and the tip of the stopper (v3) (Figure 1(b)). The shape of the triangle at the next impact moment can be determined by \u03b82, \u03b81 + \u2206\u03b8\u03071, and r1 + \u2206r\u0307d1 . The latter two terms are the predicted \u03b81 and the predicted r1 at the next impact moment. We then transform the triangle into the coordinate frame of the robot by aligning v1 with the current contact point of the robot and rotating the triangle such that v3 touches the ground. We define the coordinates of v2 as the target of the robot\u2019s COM and the coordinates of v3 as the target of the next contacting body part, c. We then solve the target pose to match the two targets using inverse kinematics. Once the target pose is solved, we use a PD control to track the target pose (the lower feedback loop in Figure 2) until the next impact moment occurs."}, {"heading": "V. EXPERIMENTS", "text": "We validate our policy in both simulation and on physical hardware. The testing platform is a small humanoid, BioloidGP [17] with a height of 34.6 cm, a weight of 1.6 kg, and 16 actuated degrees of freedom. We also compare the results from our policy against those calculated by the dynamic-programming (DP) based method proposed by Ha\nAlgorithm 1 Learning abstract-level policy 1: Randomly initialize \u03b8 2: Initialize training buffer with tuples from DP 3: while not done do 4: EXPLORATION: 5: for k = 1 \u00b7 \u00b7 \u00b7K do 6: s \u223c N0 7: while s.\u03b8\u03071 \u2265 0 do 8: c \u2190 Select actor stochastically using Equation 4\n9: a \u2190 \u03a0c(s) +Nt 10: Apply a and simulate until next impact moment\n11: s\u2032 \u2190 Current state of abstract model 12: r \u2190 r(s,a) 13: Add tuple \u03c4 \u2190 (s,a, s\u2032, r, c) in training buffer 14: s\u2190 s\u2032 15: end while 16: end for\n17: UPDATE CRITIC: 18: Sample a minibatch m tuples {\u03c4i = (si,ai, s\u2032i, ri, ci)}\n19: yi \u2190 min(ri, \u03b3maxj V\u0302j(s\u2032i)) for each \u03c4i 20: \u03b8 \u2190 \u03b8 + \u03b1 \u2211 i(yi \u2212 Vci(s))\u2207\u03b8Vci(s)\n21: UPDATE ACTOR: 22: Sample a minibatch m tuples {\u03c4i = (si,ai, s\u2032i, ri, ci)}\n23: yi = maxj Vj(s) 24: y\u2032i \u2190 min(ri, \u03b3maxj V\u0302j(s\u2032i)) 25: if y\u2032i > yi then 26: \u03b8 \u2190 \u03b8 + \u03b1(\u2207\u03b8\u03a0ci(s))T (ai \u2212\u03a0ci(s)) 27: end if 28: end while\nand Liu [1]. Because DP conducts a full search in the action space online, which is 50 to 400 times slower than our policy, in theory DP should produce better solutions than ours. Thus, the goal of the comparison is to show that our policy produces comparable solutions as DP while enjoying the speed gain by two orders of magnitude.\nWe implement and train the proposed network architecture using PyCaffe [18] on Ubuntu Linux. For simulation, we use a Python binding [19] of an open source physics engine, DART [20]."}, {"heading": "A. Learning of Abstract-Level Policy", "text": "In our experiment, we construct a network with 8 pairs of actor-critic to represent 8 possible contacting body parts: right toe, right heel, left toe, left heel, knees, elbows, hands, and head. During training, we first generate 5000 tuples from DP to initialize the training buffer. The learning process takes 1000 iterations, approximately 4 hours on 6 cores of 3.3GHz Intel i7 processor. Figure 4 shows the average reward of 10 randomly selected test cases over iterations. Once the policy is trained, a single query of the policy network takes approximately 0.8 milliseconds followed by 25 milliseconds of the inverse kinematics routine. The total of 25.8 milliseconds computation time is a drastic improvement from DP which takes 1 to 10 seconds of computation time.\nWe compare the rewards of the trained policy with those of DP by running 1000 test cases starting from randomly sampled initial states. The results are shown in Figure 5, a histogram of rewards for the 1000 tests computed by our policy and by DP. Our policy achieves not only comparable rewards, it actually outperforms DP in 64 % of the test cases. The average reward of our policy is 0.8093, comparing to 0.7784 of DP. In terms of impulse, the average of maximum impulse produced by our policy is 0.2540, which shows a 15 % improvement from 0.2997 produced by DP.\nTheoretically, DP, which searches the entire action space for the given initial state, should be the upper bound of the reward our policy can ever achieve. In practice, the discretization of the state and action spaces in DP might result in suboptimal plans. In contrast, our policy exploits the mixture of actor and critic network to optimize continuous action variables without discretization. The more precise continuous optimization often results in more optimal contact\nlocation or timing. In some cases, it also results in different contact sequences being selected (45 out of 641 test cases where our policy outperforms DP)."}, {"heading": "B. Different falling strategies", "text": "With different initial conditions, various falling strategies emerge as the solution computed by our policy. Table I showcases three distinctive falling strategies from the test cases. The table shows the initial pose, the external push, the resulting contact sequence, the impulse due to each contact (the number in the parenthesis), as well as the maximal impulse for each test case. Starting with a two-feet stance, the robot uses knees and hands to stop a fall. If the initial state is leaning forward and unbalanced, the robot directly uses its hands to catch itself. If the robot starts with a one-foot stance, it is easer to use the swing foot followed by the hands to stop a fall. The robot motion sequences can be visualized in Figure 6 and the supplementary video. For each case, we compare our policy against DP and a naive controller which simply tracks the initial pose (referred as Unplanned). Both our policy and DP significantly reduce the maximum impulse comparing to Unplanned. In the cases where our\npolicy outperforms DP, the improvement can be achieved by different contact timing (One-foot case), better target poses (Unbalanced case), or different contact sequences (Two-feet case, Figure 7)."}, {"heading": "C. Hardware Results", "text": "Finally, we compare the falling strategy generated by our policy against the unplanned motion on the hardware of BioloidGP. Due to the lack of on-board sensing capability, BioloidGP cannot take advantage of the feedback aspect of\nour policy. Nevertheless, we can still use this platform to demonstrate the falling strategy generated by our policy and compare it against an unplanned motion.\nWe first match the initial pose of the simulated BioloidGP with the real one and push the simulated BioloidGP from the back by 3 N and 5 N, assuming that the pushes we applied to the robot by hand are about the same. We then apply our policy on the simulated BioloidGP to obtain a sequence of target poses. In the hardware experiment, we program BioloidGP to track these poses once a fall is detected. During the falls, we measure the acceleration of the head using an external IMU. Figure 8 shows the results of two different falls. In the first case, the robot is pushed with a force of 3N and is initialized with both the feet on the ground and an upright position, the robot uses its knees first and then the hands to control the fall. The maximal acceleration from our policy is 2.9 G while that from an unplanned motion is 5.7 G, showing a 49% of improvement. In the second case, the robot is pushed with a force of 5N starting with one foot on the ground, the falling strategy for this includes using the left-heel first then the hands to control the fall. The maximal acceleration from our policy is 2.3 G while that from an unplanned motion is 6.4 G, showing a 64% of improvement."}, {"heading": "VI. CONCLUSION", "text": "We proposed a new policy optimization method to learn the appropriate actions for minimizing the damage of a\nhumanoid fall. Unlike most optimal control problems, the action space of our problem consists of both discrete and continuous variables. To address this issue, our algorithm trains n control policies (actors) and the corresponding value functions (critics) in an actor-critic network. Each actor-critic pair is associated with a candidate contacting body part. When the robot establishes a new contact with the ground, the policy corresponding to the highest value function will be executed while the associated body part will be the next contact. As a result of this mixture of actor-critic architecture, we cast the discrete contact planning into the problem of expert selection, while optimizing the policy in continuous space. We show that our algorithm reliably reduces the maximal impulse of a variety of falls. Comparing to the previous work [1] that employs an expensive dynamic programming method during online execution, our policy can reach better reward and only takes 0.25% to 2% of computation time on average.\nOne limitation of this work is the assumption that humanoid falls primarily lie on the sagittal plane. This limitation is due to our choice of the simplified model, which reduces computation time but only models planar motions. This assumption can be easily challenged when considering real-world falling scenarios, such as those described in [10], [11]. One possible solution to handling more general falls is to employ a more complex model similar to the inertialoaded inverted pendulum proposed by [11].\nAnother possible future work direction is to learn control policies directly in the full-body joint space, bypassing the need of an abstract model and the restrictions come with it. This allows us to consider more detailed features of the robot during training, such as full body dynamics or precise collision shapes. Given the increasingly more powerful policy learning algorithms for deep reinforcement learning [21], [22], motor skill learning with a large number of variables, as is the case with falling, becomes a feasible option."}], "references": [{"title": "Multiple Contact Planning for Minimizing Damage of Humanoid Falls", "author": ["S. Ha", "C.K. Liu"], "venue": "IEEE IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning in Continuous Action Spaces", "author": ["H. Van Hasselt", "M.A. Wiering"], "venue": "no. Adprl, pp. 272\u2013279, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement Learning in Continuous State and Action Spaces", "author": ["H. Van Hasselt"], "venue": "Reinforcement Learning, pp. 207\u2014-251, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Terrain-Adaptive Locomotion Skills using Deep Reinforcement Learning", "author": ["X.B. Peng", "G. Berseth", "M. van de Panne"], "venue": "ACM Transactions on Graphics, vol. 35, no. 4, pp. 1\u201310, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "UKEMI: falling motion control to minimize damage to biped humanoid robot", "author": ["K. Fujiwara", "F. Kanehiro", "S. Kajita", "K. Kaneko", "K. Yokoi", "H. Hirukawa"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, vol. 3, no. October, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Fall Detection and Management in Biped Humanoid Robots", "author": ["J. Ruiz-del solar", "S. Member", "J. Moya", "I. Parra-tsunekawa"], "venue": "Management, vol. 12, no. April, pp. 3323\u20133328, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards an optimal falling motion for a humanoid robot", "author": ["K. Fujiwara", "S. Kajita", "K. Harada", "K. Kaneko", "M. Morisawa", "F. Kanehiro", "S. Nakaoka", "H. Hirukawa"], "venue": "Proceedings of the 2006 6th IEEE- RAS International Conference on Humanoid Robots, HUMANOIDS, pp. 524\u2013529, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "An Optimal planning of falling motions of a humanoid robot", "author": ["\u2014\u2014"], "venue": "IEEE International Conference on Intelligent Robots and Systems, no. Table I, pp. 456\u2013462, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Whole-body trajectory optimization for humanoid falling", "author": ["J. Wang", "E. Whitman", "M. Stilman"], "venue": ". . . Control Conference (ACC), . . . , pp. 4837\u20134842, 2012. [Online]. Available: http://ieeexplore.ieee. org/xpls/abs{ }all.jsp?arnumber=6315177", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Tripod fall: Concept and experiments of a novel approach to humanoid robot fall damage reduction", "author": ["S.K. Yun", "A. Goswami"], "venue": "Proceedings - IEEE International Conference on Robotics and Automation, pp. 2799\u20132805, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Direction-changing fall control of humanoid robots: theory and experiments", "author": ["A. Goswami", "S.-k. Yun", "U. Nagarajan", "S.-H. Lee", "K. Yin", "S. Kalyanakrishnan"], "venue": "Autonomous Robots, vol. 36, no. 3, pp. 199\u2013223, jul 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. a. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971, pp. 1\u201314, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deterministic Policy Gradient Algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "Proceedings of the 31st International Conference on Machine Learning, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Reinforcement Learning in Parameterized Action Space", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv, pp. 1\u201312, 2016. [Online]. Available: http://arxiv.org/abs/1511.04143", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "As shown in Ha and Liu [1], the falling problem can be formulated as a Markov Decision Process (MDP) that solves for a contact sequence with the ground such that the damage incurred during the fall is minimized.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "[1], is computationally demanding and infeasible to deploy to the real world.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Indeed, the MDP formulation of the falling problem [1] makes it innately suitable for algorithms that take advantage of the recursive Bellman equation.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Our algorithm is based on CACLA [2], [3] which solves an actor-critic controller for continuous state and action spaces.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Our algorithm is based on CACLA [2], [3] which solves an actor-critic controller for continuous state and action spaces.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We also show that the algorithm can run in real time (as opposed to 1-10 seconds reported by [1]) once the policy is trained.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Comparing to the actions computed by Ha and Liu [1], our policy produces better rewards on average with only 0.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "When a fall is detected, the sequence designed for the most similar falling scenario is executed [5], [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "When a fall is detected, the sequence designed for the most similar falling scenario is executed [5], [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 9, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 241, "endOffset": 244}, {"referenceID": 10, "context": "[11], who proposed to compute the optimal stepping location to change the falling direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] are crucial to the efficiency and stability of our learning process, despite that the original work (DQN) is designed for learning Atari video games from pixels with the assumption that the action space is discrete.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] later combined the ideas of DQN and the deterministic policy gradient (DPG) [14] to learn actor-critic networks for continuous action space and demonstrated that end-to-end (vision perception to actuation) policies for dynamic tasks can be efficiently trained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] later combined the ideas of DQN and the deterministic policy gradient (DPG) [14] to learn actor-critic networks for continuous action space and demonstrated that end-to-end (vision perception to actuation) policies for dynamic tasks can be efficiently trained.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "van Hasselt and Wiering introduced CACLA [2], [3] that represents an actor-critic approach using neural networks.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "van Hasselt and Wiering introduced CACLA [2], [3] that represents an actor-critic approach using neural networks.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "Comparing to the recent work using actor-critic networks [13], [16], the main difference of CACLA (and our work as well) lies in the update scheme for the actor networks.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Comparing to the recent work using actor-critic networks [13], [16], the main difference of CACLA (and our work as well) lies in the update scheme for the actor networks.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "For completeness we briefly review the MDP formulation of the falling problem proposed by Ha and Liu [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "To solve the falling problem efficiently, Ha and Liu [1] proposed an abstract model to approximate the motion of the humanoid (Figure 1(a)).", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Ha and Liu [1] discretized the continuous action parameters and exploited the monotonic nature of the falling problem to accelerate the dynamic programming.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "To overcome the challenge of optimizing over both discrete and continuous action variables, we introduce a new policy representation based on a neural network architecture inspired by MACE [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "The state space is defined as s = {c1, r1, \u03b81, \u1e591, \u03b8\u03071}, where the total elapsed time t is removed from the state space previously defined by Ha and Liu [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "where M is the mass and I is the inertia of the abstract model (see details in [1]).", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "To ensure that these tuples have high reward, we use the dynamic-programming-based algorithm described in [1] (referred as DP thereafter) to simulate a large amount of rollouts from various initial states and collect tuples.", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "We follow the same Boltzmann exploration scheme as in [4] to select the actor \u03a0i(s) based on the probability defined by the predicted output of the critics:", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "We use the temporal difference to update the chosen critic similar to [12]:", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Note that we also adopt the idea of target network from [12] to compute the target, yi, for the critic update.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "and Liu [1].", "startOffset": 8, "endOffset": 11}, {"referenceID": 15, "context": "We implement and train the proposed network architecture using PyCaffe [18] on Ubuntu Linux.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Comparing to the previous work [1] that employs an expensive dynamic programming method during online execution, our policy can reach better reward and only takes 0.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "This assumption can be easily challenged when considering real-world falling scenarios, such as those described in [10], [11].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "This assumption can be easily challenged when considering real-world falling scenarios, such as those described in [10], [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "One possible solution to handling more general falls is to employ a more complex model similar to the inertialoaded inverted pendulum proposed by [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "Given the increasingly more powerful policy learning algorithms for deep reinforcement learning [21], [22], motor skill learning with a large number of variables, as is the case with falling, becomes a feasible option.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Given the increasingly more powerful policy learning algorithms for deep reinforcement learning [21], [22], motor skill learning with a large number of variables, as is the case with falling, becomes a feasible option.", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "Being able to fall safely is a necessary motor skill for humanoids performing highly dynamic tasks, such as running and jumping. We propose a new method to learn a policy that minimizes the maximal impulse during the fall. The optimization solves for both a discrete contact planning problem and a continuous optimal control problem. Once trained, the policy can compute the optimal next contacting body part (e.g. left foot, right foot, or hands), contact location and timing, and the required joint actuation. We represent the policy as a mixture of actor-critic neural network, which consists of n control policies and the corresponding value functions. Each pair of actor-critic is associated with one of the n possible contacting body parts. During execution, the policy corresponding to the highest value function will be executed while the associated body part will be the next contact with the ground. With this mixture of actor-critic architecture, the discrete contact sequence planning is solved through the selection of the best critics while the continuous control problem is solved by the optimization of actors. We show that our policy can achieve comparable, sometimes even higher, rewards than a recursive search of the action space using dynamic programming, while enjoying 50 to 400 times of speed gain during online execution.", "creator": "LaTeX with hyperref package"}}}