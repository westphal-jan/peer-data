{"id": "1703.03111", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "Statistical Cost Sharing", "abstract": "we study with the simplest cost edge sharing option problem algorithm for cooperative games arising in situations primarily where the cost loss function $ c $ np is not available via arbitrary oracle automated queries, but must, instead be repeatedly derived digitally from data, informally represented as tuples $ ( s, - c ( s ) ) $, for different bargaining subsets $ s $ respectively of involved players. suppose we informally formalize this approach, on which since we increasingly call statistical simulations cost sharing, further and carefully consider considering the quantitative computation of the core dollar and the shapley return value, when the tuples are drawn from some distribution $ \\ mathcal { d } $.", "histories": [["v1", "Thu, 9 Mar 2017 02:50:49 GMT  (355kb,D)", "http://arxiv.org/abs/1703.03111v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["eric balkanski", "umar syed", "sergei vassilvitskii"], "accepted": true, "id": "1703.03111"}, "pdf": {"name": "1703.03111.pdf", "metadata": {"source": "CRF", "title": "Statistical Cost Sharing", "authors": ["Eric Balkanski", "Umar Syed", "Sergei Vassilvitskii"], "emails": ["ericbalkanski@g.harvard.edu.", "usyed@google.com.", "sergeiv@google.com."], "sections": [{"heading": null, "text": "Previous work by Balcan et al. [8] in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions. We expand on their work and give an algorithm that computes such cost shares for any function with a non-empty core. We complement these results by proving an inapproximability lower bound for a weaker relaxation.\nWe then turn our attention to the Shapley value. We first show that when cost functions come from the family of submodular functions with bounded curvature, \u03ba, the Shapley value can be approximated from samples up to a \u221a 1\u2212 \u03ba factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value. We show that these can always be approximated arbitrarily well for general functions over any distribution D.\n\u2217School of Engineering and Applied Sciences, Harvard University, ericbalkanski@g.harvard.edu. \u2020Google NYC, usyed@google.com. \u2021Google NYC, sergeiv@google.com.\nar X\niv :1\n70 3.\n03 11\n1v 1\n[ cs\n.G T\n] 9\nM ar\n2 01\n7"}, {"heading": "1 Introduction", "text": "The cost sharing problem asks for an equitable way to split the cost of a service among all of the participants. Formally, there is a cost function defined over all subsets of a ground set of elements (or players) and the objective is to fairly divide the cost of the full set among the participants. Cost sharing is central to cooperative game theory, and there is a rich literature developing the key concepts and principles to reason about this topic. Two popular cost sharing concepts are the core [23], where no group of players has an incentive to deviate, and the Shapley value [39], which is the unique vector of cost shares satisfying four natural axioms.\nWhile both the core and the Shapley value are easy to define, computing them poses additional challenges. One obstacle is that the computation of the cost shares requires knowledge of costs in myriad different scenarios. For example, computing the exact Shapley value requires one to look at the marginal contribution of a player over all possible subsets. Recent work [30] shows that one can find approximate Shapley values for a restricted subset of cost functions by looking at the costs for polynomially many specifically chosen examples. In practice, however, another roadblock emerges: one cannot simply query for the cost of a hypothetical scenario. Rather, the costs for scenarios that have not occurred are simply unknown. We share the opinion of Balcan et al. [8] that the main difficulty with using cost sharing methods in concrete applications is the information needed to compute them.\nConcretely, consider the following cost sharing applications.\nAttributing Battery Consumption on Mobile Devices. A modern mobile phone or tablet is typically running a number of distinct apps concurrently. In addition to foreground processes, a lot of activity may be happening in the background: email clients may be fetching new mail, GPS may be active for geofencing applications, messaging apps are polling for new notifications, and so on. All of these activities consume power; the question is how much of the total battery consumption should be attributed to each app? This problem is non-trivial because the operating system induces cooperation between apps to save battery power. For example there is no need to activate the GPS sensor twice if two different apps request the current location almost simultaneously.\nMoneyball and Player Ratings The impact of an individual player on the overall performance of the team typically depends on the other players currently playing. One can infer the total benefit from the players on the field (or on the court) from metrics like number of points scored, time of possession, etc., the question here is how to allocate this impact to the individuals. Recently many such metrics have been proposed (for example plus/minus ratio in hockey, wins above replacement in baseball.), our goal here is to find scores compatible with cooperative game theory concepts.\nUnderstanding Black Box Learning Deep neural networks are prototypical examples of black box learning, and it is almost impossible to tease out the contribution of a particular feature to the final output. Particularly, in situations where the features are binary, cooperative game theory gives a formal way to analyze and derive these contributions. While one can evaluate the objective function on any subset of features, deep networks are notorious for performing poorly on certain out of sample examples [25, 41], which may lead to misleading conclusions when using traditional cost sharing methods.\nWe model these cost sharing questions as follows. LetN be the set of possible players (apps or features), and for a subset S \u2286 N , let C(S) denote the cost of S. This cost represents the total power consumed over a standard period of time, or the number of points scored by the team, and so on. We are given ordered\npairs (S1, C(S1)), (S2, C(S2)), . . . , (Sm, C(Sm)), where each Si \u2286 N is drawn independently from some distribution D. The problem of STATISTICAL COST SHARING asks to look for reasonable cost sharing strategies in this setting."}, {"heading": "1.1 Our results", "text": "We build on the approach from Balcan et al. [8], which studied STATISTICAL COST SHARING in the context of the core, and assume that only partial data about the cost function is observed. The authors showed that cost shares that are likely to respect the core property can be obtained for certain restricted classes of functions. Our main result is an algorithm that generalizes these results for all games where the core is non-empty and we derive sample complexity bounds showing exactly the number of samples required to compute cost shares (Theorems 3 and 5).1 While the main approach of Balcan et al. [8] relied on first learning the cost function and then computing cost shares, we show how to proceed directly, computing cost shares without explicitly learning a good estimate of the cost function. We also show that approximately satisfying the core with probability one is impossible in general (Theorem 6).\nWe then focus on the Shapley value, which has never been studied in the STATISTICAL COST SHARING context. We introduce a new cost sharing method called data-dependent Shapley value which is the unique solution (Theorem 10) satisfying four natural axioms resembling the Shapley axioms (Definition 8), and which can be approximated arbitrarily well from samples for any bounded function and any distribution (Theorem 11). Regarding the traditional Shapley value, we obtain a tight \u221a 1\u2212 \u03ba multiplicative approximation for submodular functions with bounded curvature \u03ba over the uniform distribution (Theorems 7 and 8), but show that they cannot be approximated by a bounded factor in general, even for the restricted class of coverage functions, which are learnable, over the uniform distribution (Theorem 9)."}, {"heading": "1.2 Related work", "text": "There are two avenues of work which we build upon. The first is the notion of cost sharing in cooperative games, first introduced by Von Neumann and Morgenstern [42]. We consider the Shapley value and the core, two popular solution concepts for cost-sharing in cooperative games. The Shapley value [39] is studied in algorithmic mechanism design [1, 10, 20, 33]. For applications of the Shapley value, see the surveys by Roth [36] and Winter [44]. A naive computation of the Shapley value of a cooperative game would take exponential time; recently, methods for efficiently approximating the Shapley value have been suggested [2, 19, 30, 31] for some restricted settings.\nThe core, introduced by Gillies [23], is another well-studied solution concept for cooperative games. Bondareva [13] and Shapley [38] characterized when the core is non-empty. The core has been studied in the context of multiple combinatorial games, such as facility location [24] and maximum flow [16]. In cases with no solutions in the core or when it is computationally hard to find one, the balance property has been relaxed to hold approximately [17, 26]. In applications where players submit bids, cross-monotone cost sharing, a concept stronger than the core that satisfies the group strategy proofness property, has attracted a lot of attention [26, 29, 34, 35]. We note that these applications are sufficiently different from the ones we are studying in this work.\nThe second is the recent work in econometrics and computational economics that aims to estimate critical concepts directly from a limited data set, and reason about the sample complexity of the computational problems. Specifically, in all of the above papers, the algorithm must be able to query or compute C(S) for\n1Concurrently and independently of our work, Balcan et al. [9] proved a polynomial sample complexity bound for this problem of computing cost shares that are likely to respect the core property, for all functions with a non-empty core.\nan arbitrary set S \u2286 N . In our work, we are instead given a collection of samples from some distribution; importantly the algorithm does not know C(S) for sets S that were not sampled. This approach was first introduced by Balcan et al. [8], who showed how to compute an approximate core for some families of games. Their main technique is to first learn the cost function C from samples and then to use the learned function to compute cost shares. The authors also showed that there exist games that are not PAC-learnable but that have an approximate core that can be computed.\nOutside of cooperative game theory, this data-driven approach has attracted renewed focus. In auction design, a line of work [14, 15, 18, 32] has studied revenue maximization from samples instead of being given a Bayesian prior. In the inductive clustering setting, the algorithm is only given a small random subset of the data set it wishes to cluster [4, 6]. More closely related to our work is the problem of optimization from samples [11, 12] where, the goal is to approximate maxS\u2208M C(S) for some constraint M \u2286 2N and C : 2N \u2192 R from samples for some class of combinatorial functions."}, {"heading": "2 Preliminaries", "text": "A cooperative game is defined by an ordered pair (N,C), whereN is the ground set of elements, also called players, and C : 2N \u2192 R\u22650 is the cost function mapping each coalition S \u2286 N to its cost, C(S). The ground set of size n = |N | is called the grand coalition and we denote the elements by N = {1, . . . , n} = [n]. We assume that C(\u2205) = 0, C(S) \u2265 0 for all S \u2286 N , and that maxS C(S) is bounded by a polynomial in n, which are standard assumptions. We will slightly abuse notation and use C(i) instead of C({i}) for i \u2208 N when it is clear from the context.\nWe recall three specific classes of functions. Submodular functions exhibit the property of diminishing returns: CS(i) \u2265 CT (i) for all S \u2286 T \u2286 N and i \u2208 N where CS(i) is the marginal contribution of element i to set S, i.e., CS(i) = C(S \u222a {i})\u2212 C(S). Coverage functions are the canonical example of submodular functions. A function is coverage if it can be written as C(S) = | \u222ai\u2208S Ti| where Ti \u2286 U for some universe U . Finally, we also consider the simple class of additive functions that are such that C(S) = \u2211 i\u2208S C(i).\nA cost allocation is a vector \u03c8 \u2208 Rn where \u03c8i is the share of element i. We call a cost allocation \u03c8 balanced if \u2211 i\u2208N \u03c8i = C(N). Given a cooperative game (N,C) the goal in the cost sharing literature is to find \u201cdesirable\u201d balanced cost allocations. Most proposals take an axiomatic approach, defining a set of axioms that a cost allocation should satisfy. These lead to the concepts of Shapley value and the core, which we define next. A useful tool to describe and compute these cost sharing concepts is permutations. We denote by \u03c3 a uniformly random permutation of N and by S\u03c3<i the players before i in permutation \u03c3."}, {"heading": "2.1 The core", "text": "The core is a balanced cost allocation where no player has an incentive to deviate from the grand coalition\u2014 for any subset of players the sum of their shares does not cover their collective cost.\nDefinition 1. A cost allocation \u03c8 is in the core of function C if the following properties are satisfied: \u2022 Balance: \u2211\ni\u2208N \u03c8i = C(N), \u2022 Core property: for all S \u2286 N , \u2211\ni\u2208S \u03c8i \u2264 C(S).\nThe core is a natural cost sharing concept. For example, in the battery blame scenario it translates to the following assurance: No matter what other apps are running concurrently, an app is never blamed for more battery consumption than if it were running alone. Given that app developers are typically business\ncompetitors, and that a mobile device\u2019s battery is a very scarce resource, such a guarantee can rather neatly avoid a great deal of finger-pointing. Unfortunately, for a given cost function C the core may not exist (we say the core is empty), or there may be multiple (or even infinitely many) cost allocations in the core. For submodular functions C, the core is guaranteed to exist and one allocation in the core can be computed in polynomial time. Specifically, for any permutation \u03c3, the cost allocation \u03c8 such that \u03c8i = C(S\u03c3<i \u222a{i})\u2212 C(S\u03c3<i) is in the core."}, {"heading": "2.2 The Shapley value", "text": "The Shapley value provides an alternative cost sharing method. For a game (N,C) we denote it by \u03c6C , dropping the superscript when it is clear from the context. While the Shapley value may not satisfy the core property, they satisfy the following four equally natural axioms:\n\u2022 Balance: \u2211\ni\u2208N \u03c6i = C(N).\n\u2022 Symmetry: For all i, j \u2208 N , if C(S \u222a {i}) = C(S \u222a {j}) for all S \u2286 N \\ {i, j} then \u03c6i = \u03c6j .\n\u2022 Zero element: For all i \u2208 N , if C(S \u222a {i}) = C(S) for all S \u2286 N then \u03c6i = 0.\n\u2022 Additivity: For two games (N,C1) and (N,C2) with the same players, but different cost functions C1 and C2, let \u03c61 and \u03c62 be the respective cost allocations. Consider a new game (N,C1 + C2), and let \u03c6\u2032 be the cost allocation for this game. Then for all elements, i \u2208 N , \u03c6\u2032i = \u03c61i + \u03c62i .\nEach of these axioms is natural: balance ensures that the cost of the grand coalition is distributed among all of the players. Symmetry states that two identical players should have equal shares. Zero element verifies that a player that adds zero cost to any coalition should have zero share. Finally, additivity just confirms that costs combine in a linear manner. It is surprising that the set of cost allocations that satisfies all four axioms is unique. Moreover, the Shapley value \u03c6 can be written as the following summation:\n\u03c6i = E \u03c3\n[C(S\u03c3<i \u222a {i})\u2212 C(S\u03c3<i)] = \u2211\nS\u2286N\\{i}\n|S|!(n\u2212 |S| \u2212 1)! n! (C(S \u222a {i})\u2212 C(S)).\nThis expression is the expected marginal contribution C(S \u222a {i}) \u2212 C(S) of i over a set of players S who arrived before i in a random permutation of N . As the summation is over exponentially many terms, the Shapley value generally cannot be computed exactly in polynomial time. However, several sampling approaches have been suggested to approximate the Shapley value for specific classes of functions [2, 19, 30, 31]."}, {"heading": "2.3 Statistical cost sharing", "text": "With the sole exception of [8], previous work in cost-sharing critically assumes that the algorithm is given oracle access to C, i.e., it can query, or determine, the cost C(S) for any S \u2286 N . In this paper, we aim to (approximately) compute the Shapley value and other cost allocations from samples, without oracle access to C, and with a number of samples that is polynomial in n.\nDefinition 2. Consider a cooperative game with players N and cost function C. In the STATISTICAL COST SHARING problem we are given pairs (S1, C(S1)), (S2, C(S2)), . . . , (Sm, C(Sm)) where each Si is drawn i.i.d. from a distribution D over 2N . The goal is to find a cost allocation \u03c8 \u2208 Rn.\nIn what follows we will often refer to an individual (S,C(S)) pair as a sample. It is tempting to reduce STATISTICAL COST SHARING to classical cost sharing by simply collecting enough samples to use known algorithms. For example, Liben-Nowell et al. [30] showed how to approximate the Shapley value with polynomially many queries C(S). However, if the distribution D is not aligned with these specific queries, which is the case for the uniform distribution, emulating these algorithms in our setting requires exponentially many samples. Balcan et al. [8] showed how to instead first learn an approximation to C from the given samples and then compute cost shares for the learned function, but their results hold only for a limited number of games and cost functions C. We show that a more powerful approach is to compute cost shares directly from the data, without explicitly learning the cost function first."}, {"heading": "2.4 Warm up: linear functions and product distributions", "text": "As a simple example, we consider the special case of additive functions with C(i) \u2265 1/ poly(n) and on bounded product distributions D.2 In this setting, the core property and the Shapley value can be approximated arbitrarily well. It is easy to verify that the cost allocation \u03c8 such that \u03c8i = C(i) is in the core and that it is the Shapley value.\nTo compute these cost shares, we estimate the expected marginal contribution of an element i to a random set, i.e., vDi := ES\u223cD|i 6\u2208S [C(S \u222a {i}) \u2212 C(S)]. Note that in the case of additive functions, vDi = C(i). In addition,\nvDi = E S\u223cD|i 6\u2208S [C(S \u222a {i})\u2212 C(S)] = E S\u223cD|i\u2208S [C(S)]\u2212 E S\u223cD|i 6\u2208S [C(S)],\nwhen D is a product distribution. Thus, this value can be estimated arbitrarily well by looking at the difference in cost between the average value of the samples containing i and the average of those not containing i. The analysis is a simple concentration bound and is deferred to the appendix.\nLemma 1. Let C be an additive function with C(i) \u2265 1/ poly(n) for all i. Then, given poly(n, 1/\u03b4, 1/ ) samples, we can compute an estimate v\u0303i such that with probability (1\u2212 \u03b4):\n(1\u2212 )C(i) < v\u0303i < (1 + )C(i).\nThus an algorithm which computes \u03c8i = v\u0303i approximates the Shapley value and the core property arbitrarily well (the formal definitions of approximating the core and the Shapley value are deferred to the respective sections devoted to those concepts)."}, {"heading": "3 Approximating the Core from Samples", "text": "In this section, we consider the problem of finding cost allocations from samples that satisfy relaxations of the core. A natural approach to this problem is to first learn the underlying model, C, from the data and to then compute a cost allocation for the learned function. As shown in [8], this approach works if C is PAC-learnable, but there exist functions C that are not PAC-learnable and for which a cost allocation that approximately satisfies the core can still be computed. The main result of this section shows that a cost allocation that approximates the core property can be computed for any function with a non-empty core. Moreover, we show that the number of samples from D needed to accurately learn the core is low.\n2A bounded product distribution has marginal probabilities bounded below and above by 1/ poly(n) and 1\u2212 1/ poly(n).\nThe approach is to directly compute a cost allocation, which empirically satisfies the core property, i.e., it satisfies the core property on all of the samples. We then argue that the same cost shares will likely satisfy the core property on newly drawn samples as well. This difference between the empirical performance of a function and its expected performance is known as the generalization error of a function and its analysis is central to theoretical machine learning. Intuitively, the generalization error is small when the number of samples, m, used to train the function is large, and the function itself is relatively simple. Two of the most common tools that formally capture these notions are the VC-dimension and the m-sample Rademacher complexity of a function class. We will use both of these to highlight different trade-offs in computing statistical cost shares.\nWe begin by defining three notions of approximate core: the probably stable [8], approximately stable, and probably approximately stable cores.\nDefinition 3. Given \u03b4, > 0, a cost allocation \u03c8 such that \u2211\ni\u2208N \u03c8i = C(N) is in\n\u2022 the probably stable core [8] if, for all D,\nPr S\u223cD [\u2211 i\u2208S \u03c8i \u2264 C(S) ] \u2265 1\u2212 \u03b4,\n\u2022 the approximately stable core over D if for all S \u2286 N , (1\u2212 ) \u00b7 \u2211 i\u2208S \u03c8i \u2264 C(S),\n\u2022 the probably approximately stable core if, for all D,\nPr S\u223cD\n[ (1\u2212 ) \u00b7\n\u2211 i\u2208S \u03c8i \u2264 C(S)\n] \u2265 1\u2212 \u03b4.\nThe algorithms we consider compute cost shares in polynomial time. The hardness results are information theoretic and are not due to running time limitations.\nDefinition 4. Cost shares \u03c8 are computable for the class of functions C over distribution D, if for all C \u2208 C and any \u2206, \u03b4, > 0, given C(N) and m = poly(n, 1/\u2206, 1/\u03b4, 1/ ) samples (Sj , C(Sj)) with each Sj drawn i.i.d. from distribution D, there exists an algorithm that computes \u03c8 with probability at least 1 \u2212 \u2206 over both the samples and the choices of the algorithm. If the algorithm has poly(m) running time, then the cost shares \u03c8 are efficiently computable.\nFinally, we will refer to the number of samples m required to compute approximate cores as the sample complexity of the algorithm.\nOur Results. We give algorithms that efficiently compute cost shares in the probably stable core for functions with a non-empty (traditional) core with a simple approach using the VC-dimension (Section 3.1), the algorithm has sample complexity linear in n. With a more complex analysis and using the Rademacher complexity, we obtain efficiently computable cost shares in the probably approximately stable core with an improved sample complexity dependence of log n but with an additional dependence on the spread of the function C (Section 3.2). Finally, we show that cost shares in the approximately stable core are not computable even for the uniform distribution and the well-behaved class of monotone submodular functions (Section 3.3)."}, {"heading": "3.1 Cost shares in the probably stable core are efficiently computable", "text": "Balcan et al. [8] showed that several families of functions D have a core that is probably stable. These families include network flow, threshold task, and induced subgraph games which are all well-known classes of cooperative games; and the class of monotone simple games which are games that take values in {0, 1}. We generalize their result so that it is not constrained to specific classes of functions, and show how to compute a probably stable core for any game with a non-empty core.\nTechnically, we use the VC-dimension of the class of halfspaces to show that the performance on the samples generalizes well to the performance on the distribution D. We review the definition of the VCdimension in Appendix C and only state VC-dimension results needed for our purposes. We first state the generalization error obtained for a class of functions with VC-dimension d.\nTheorem 1 ([37], Theorem 6.8). LetH be a hypothesis class of functions from a domain X to {\u22121, 1} and f : X 7\u2192 {\u22121, 1} be some \u201ccorrect\u201d function. Assume that H has VC-dimension d. Then, there is an absolute constant c such that with m \u2265 c(d+ log(1/\u2206))/\u03b42 i.i.d. samples x1, . . . ,xm \u223c D,\u2223\u2223\u2223\u2223\u2223 Prx\u223cD [h(x) 6= f(x)]\u2212 1m m\u2211 i=1 1h(xi)6=f(xi)\n\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b4 for all h \u2208 H, with probability 1\u2212\u2206 over the samples.\nWe use a special case of the class of halfspaces, for which we know the VC-dimension.\nTheorem 2 ([37], Theorem 9.2). The class of functions {x 7\u2192 sign(w\u1d40x) : w \u2208 Rn} has VC-dimension n.\nWe first define a class of functions that contains the core, and prove that it has low VC-dimension. Given a sample S, define xS such that xSi = 1i\u2208S for i \u2208 [n] and xSn+1 = C(S). Note that if the core property is satisfied for sample S, then sign (\u2211n i=1 \u03c8ix S i \u2212 xSn+1 ) = \u22121 . We now bound the VC-dimension of this hypothesis class of functions induced by cost allocations \u03c8.\nCorollary 1. The class of functionsHcore = {x 7\u2192 sign( \u2211n i=1 \u03c8ixi \u2212 xn+1) : \u03c8 \u2208 Rn, \u2211\ni \u03c8i = C(N)} has VC-dimension at most n+ 1.\nProof. We combine the observation that {x 7\u2192 sign( \u2211n i=1wixi \u2212 xn+1) : w \u2208 Rn, \u2211\niwi = C(N)} \u2286 {x 7\u2192 sign(w\u1d40x) : w \u2208 Rn+1} with the well-known fact that the VC-dimension of H\u2032 is at most the VC-dimension ofH forH\u2032 \u2286 H.\nIt remains to show how to optimize over functions in this class.\nTheorem 3. The class of functions with a non-empty core has cost shares in the probably stable core that are efficiently computable. The sample complexity is\nO\n( n+ log(1/\u2206)\n\u03b42\n) .\nProof. Let \u03c8 be a cost allocation which satisfies both the core property on all the samples and the balance property, i.e., \u2211 i\u2208S \u03c8i \u2264 C(S) for all samples S and \u2211 i\u2208N \u03c8i = C(N). Note that such a cost allocation exists since we assume that C has a non-empty core. Given the set of samples, it can be computed with a simple linear program. We argue that \u03c8 is probably stable.\nDefine h(x) = sign (\u2211n\ni=1 \u03c8ix S i \u2212 xSn+1\n) and f(x) = \u22121 for all x. Since the core property is satisfied\non all the samples, 1m \u2211m i=1 1h(x)6=f(x) = 0. Thus, by Theorem 1,\nPr S\u223cD [\u2211 i\u2208S \u03c8i \u2264 C(S) ] = 1\u2212 Pr xS :S\u223cD [ sign ( n\u2211 i=1 \u03c8ix S i \u2212 xSn+1 ) 6= \u22121 ] = 1\u2212 Pr\nxS :S\u223cD\n[ h ( xS ) 6= f ( xS )]\n= 1\u2212 \u2223\u2223\u2223\u2223\u2223 PrxS :S\u223cD [h (xS) 6= f (xS)]\u2212 1m m\u2211 i=1 1h(xS)6=f(xS) \u2223\u2223\u2223\u2223\u2223 \u2265 1\u2212 \u03b4\nwith O((n+ log(1/\u2206))/\u03b42) samples."}, {"heading": "3.2 Logarithmic sample complexity for probably approximately stable cores", "text": "We show that the sample complexity dependence on n can be improved from linear to logarithmic. However, this improvement comes at a cost. We now find a probably approximately stable core instead of probably stable core, and the sample complexity depends on the spread of the function C, defined as maxS C(S)minS 6=\u2205 C(S) . This approach assumes that minS 6=\u2205C(S) > 0. We start with an overview.\n1. As previously, we find a cost allocation which satisfies the core property on all samples. However, we restrict this search to cost allocations with bounded `1-norm. Such a cost allocation can be found efficiently since the space of such cost allocations is convex.\n2. The analysis begins by bounding the `1-norm of any vector in the core (Lemma 2). Combined with the assumption that the core is non-empty, this implies that a cost allocation \u03c8 satisfying the previous conditions exists.\n3. Let [x]+ denote the function x 7\u2192 max(x, 0). Consider the following \u201closs\u201d function:[\u2211 i\u2208S \u03c8i C(S) \u2212 1 ]\n+\nThis loss function is convenient since it is equal to 0 if and only if the core property is satisfied for S and it is 1-Lipschitz, which is used in the next step.\n4. Next, we bound the difference between the empirical loss and the expected loss for all \u03c8 with a known result using the Rademacher complexity of linear predictors with low `1 norm over \u03c1-Lipschitz loss functions (Theorem 4).\n5. Finally, given \u03c8 which approximately satisfies the core property in expectation, we show that \u03c8 is in the probably approximately stable core by Markov\u2019s inequality (Lemma 3).\nWe review the definition of the Rademacher complexity in Appendix C. For our purposes, the following result which follows from the Rademacher complexity of linear classes is sufficient.\nTheorem 4 ([37], Theorem 26.15). Suppose that D is a distribution over X \u00d7 R such that with probability 1 we have that \u2016x\u2016\u221e \u2264 R. Let H = {w \u2208 Rd : \u2016w\u20161 \u2264 B} and let ` : H \u00d7 (X \u00d7 R) \u2192 R be a loss function of the form\n`(w, (x, y)) = \u03c6(w\u1d40x, y)\nsuch that for all y \u2208 R, a 7\u2192 \u03c6(a, y) is an \u03c1-Lipschitz function and such that maxa\u2208[\u2212BR,BR] |\u03c6(a, y)| \u2264 c. Then, for all w \u2208 H and any \u2206 \u2208 (0, 1), with probability of at least 1 \u2212 \u2206 over m i.i.d. samples (x1, y1), . . . , (xm, ym) from D,\nE (x,y)\u223cD [`(w, (x, y))] \u2264 1 m m\u2211 i=1 `(w, (xi, yi)) + 2\u03c1BR\n\u221a 2 log(2d)\nm + c\n\u221a 2 log(2/\u2206)\nm .\nWe first bound the `1 norm of vectors in the core to bound the space of linear functions that we search over.\nLemma 2. Assume that \u03c8 is a vector in the core, then \u2016\u03c8\u20161 \u2264 2 maxS |C(S)|.\nProof. Fix some vector \u03c8 in the core. LetA be the set of elements i such that \u03c8i \u2265 0 andB be the remaining elements. First note that \u2211\ni\u2208A \u03c8i \u2264 C(A) \u2264 max S |C(S)|\nwhere the first inequality is by the core property. Next, note that 0 \u2264 C(N) = \u2211 i\u2208A \u03c8i + \u2211 i\u2208B \u03c8i \u2264 max S |C(S)|+ \u2211 i\u2208B \u03c8i\nwhere the equality is by the balance property, so \u2211\ni\u2208B \u03c8i \u2265 \u2212maxS |C(S)|. Thus, \u2016\u03c8\u20161 = \u2211 i\u2208A \u03c8i \u2212 \u2211 i\u2208B \u03c8i \u2264 max S |C(S)|+ max S |C(S)|.\nWe can thus focus on bounded cost allocations \u03c8 \u2208 H where H := { \u03c8 : \u03c8 \u2208 Rn, \u2016\u03c8\u20161 \u2264 2 max\nS |C(S)|\n} .\nThe next lemma shows that if the core property approximately holds in expectation, then it is likely to approximately hold.\nLemma 3. For any 0 < , \u03b4 < 1 and cost allocation \u03c8,\nE S\u223cD\n[[\u2211 i\u2208S \u03c8i C(S) \u2212 1 ]\n+\n] \u2264 \u03b4\n1\u2212 \u21d2 Pr S\u223cD\n[ (1\u2212 )\n\u2211 i\u2208S \u03c8i \u2264 C(S)\n] \u2265 1\u2212 \u03b4.\nProof. For any a > 0 and nonnegative random variable X , by Markov\u2019s inequality we have Pr[X \u2264 a] \u2265 1\u2212 E[X]/a. By letting a = /(1\u2212 ), X = [ ( \u2211 i\u2208S \u03c8i)/C(S)\u2212 1 ] +\n, and observing that[\u2211 i\u2208S \u03c8i C(S) \u2212 1 ]\n+\n\u2264 1\u2212\n\u21d2 \u2211 i\u2208S \u03c8i\nC(S) \u2212 1 \u2264 1\u2212 \u21d2 (1\u2212 ) \u2211 i\u2208S \u03c8i \u2264 C(S),\nwe obtain PrS\u223cD [ (1\u2212 ) \u2211 i\u2208S \u03c8i \u2264 C(S) ] \u2265 1\u2212 \u03b4.\nCombining Theorem 4, Lemma 2, and Lemma 3, the dependence of the sample complexity on n is improved.\nTheorem 5. The class of functions with a non-empty core has cost shares in the probably approximately stable core that are efficiently computable with sample complexity(\n1\u2212 \u03b4\n)2 ( 128\u03c4(C)2 log(2n) + 8\u03c4(C)2 log(2/\u2206) ) = O (( \u03c4(C)\n\u03b4\n)2 (log n+ log(1/\u2206)) ) .\nwhere \u03c4(C) = maxS C(S)minS 6=\u2205 C(S) is the spread of C.\nProof. Fix C \u2208 C. Suppose we are givenm samples fromD. We pick \u03c8? \u2208 H such that core property holds on all the samples and such that the balance property holds ( \u2211 i\u2208N \u03c8i = C(N)). This cost allocation \u03c8\n? can be found efficiently since the collection of such \u03c8 is a convex set. By the assumption that C has at least one vector in the core and by Lemma 2, such a \u03c8? exists. Given S \u223c D, define xS such that xSi = 1i\u2208S/C(S). Fix y = 1. Define the loss function ` as follows,\n` ( \u03c8, ( xS , y )) := [ \u03c8\u1d40xS \u2212 y ] + =\n[\u2211 i\u2208S \u03c8i C(S) \u2212 1 ]\n+\nWe wish to use Theorem 4 with R = 1/minS 6=\u2205 |C(S)|, B = 2 maxS |C(S)|, \u03c6(a, y) = [a\u2212 y]+, \u03c1 = 1, and c = \u03c4(C). We verify that all the conditions hold. First note that without loss of generality, samples where S = \u2205 can be ignored, so \u2225\u2225xS\u2225\u2225\u221e \u2264 1/minS 6=\u2205 |C(S)| for all S. Next, \u2016\u03c8\u20161 \u2264 2 maxS |C(S)| for \u03c8 \u2208 H by definition of H. The loss function ` is of the form `(\u03c8, (x, y)) = \u03c6(\u03c8\u1d40x, y) = [\u03c8\u1d40x\u2212 y]+ such that a 7\u2192 \u03c6(a, y) = [a\u2212 y]+ is an 1-Lipschitz function and such that maxa\u2208[\u2212BR,BR] |\u03c6(a, y)| \u2264 2 maxS |C(S)|/minS 6=\u2205 |C(S)| = 2\u03c4(C). In addition, note that\n1\nm m\u2211 i=1 ` ( \u03c8?, ( xSi , 1 )) = 0\nsince the core property holds on all the samples. Thus, by Theorem 4,\nE S\u223cD\n[\u2211 i\u2208S \u03c8 ? i C(S) \u2212 1 ]\n+\n= E xS :S\u223cD\n[ ` ( \u03c8?, ( xS , 1 ))] \u2264 4\u03c4(C)\n\u221a 2 log(2n)\nm + \u03c4(C)\n\u221a 2 log(2/\u2206)\nm .\nChoose any , \u03b4 > 0. If the number of samples m is chosen as in the statement of the theorem, then the righthand side of the above inequality will be less than \u03b41\u2212 . Thus by Lemma 3,\nPr S\u223cD\n[ (1\u2212 )\n\u2211 i\u2208S \u03c8i \u2264 C(S)\n] \u2265 1\u2212 \u03b4,\nwhich completes the proof."}, {"heading": "3.3 The approximately stable core is not computable", "text": "Since we obtained a probably approximately stable core, a natural question is if it is possible to compute cost allocations that are approximately stable over natural distributions. The answer is negative in general: even for the restricted class of monotone submodular functions, which always have a solution in the core, the core cannot be approximated from samples, even over the uniform distribution.\nTheorem 6. Cost shares \u03c8 in the (1/2 + )-approximately stable core, i.e., such that for all S,( 1\n2 + ) \u00b7 \u2211 i\u2208S \u03c8i \u2264 C(S),\ncannot be computed for monotone submodular functions over the uniform distribution, for any constant > 0.\nProof. The ground set of elements is partitioned in \u22121 sets A1, . . . A \u22121 of size n for some small constant > 0. Let C = {CAi : i \u2208 [ \u22121]} where\nCA(S) = |(N \\A) \u2229 S|+ min ( |A \u2229 S|, (1 + ) n\n2\n) .\nThe expected number of elements of Ai in a sample S from the uniform distribution is |Ai|/2 = n/2, so by the Chernoff bound\nPr [ |Ai \u2229 S| \u2265 (1 + ) n\n2\n] \u2264 e\u2212 2n 6 ,\nThus, by a union bound, CAi(S) = |S| over all i and all samples S with probability 1 \u2212 O(e\u2212n) and we henceforth assume this is the case. It is therefore impossible to learn any information about the partition A1, . . . A \u22121 from samples. Any cost allocation \u03c8 computed by an algorithm given samples from CAi is thus independent of i.\nNext, consider such a cost allocation \u03c8 independent of i satisfying the balance property. There exists Ai such that \u2211 j\u2208Ai \u03c8j > (1\u2212 )n since \u2211 j\u2208N \u03c8j = C(N) > (1\u2212 )n by the balance property. In addition, CAi(Ai) = (1 + ) n/2. We obtain\u2211 j\u2208Ai \u03c8j > (1\u2212 )n = (1\u2212 ) (1 + ) 2CAi(Ai).\nThus, the core property is violated by a 1/2 + \u2032 factor for set Ai and function CAi , and for any constant \u2032 > 0 by picking sufficiently small."}, {"heading": "4 Approximating the Shapley Value from Samples", "text": "We turn our attention to the STATISTICAL COST SHARING problem in the context of the Shapley value. Since the properties (axioms) of the Shapley value are over elements and not sets, there is no simple relaxation of the Shapley value where the properties hold \u201cprobably\u201d over D as we had for the core. However, since the Shapley value exists and is unique for all functions, a natural relaxation is to simply approximate this value from samples.\nWe begin by observing that there exists a distribution such that Shapley value can be approximated arbitrarily well from samples. However, in this paper, we are motivated by applications where the algorithm\ndoes not control the distribution over the samples, but where the samples are drawn from some \u201cnatural\u201d distribution. Thus, the distributions we consider in this section are the uniform distribution, and more generally product distributions, which are the standard distributions studied in the learning literature for combinatorial functions [5, 7, 21, 22]. It is easy to see that we need some restrictions on the distribution D (for example, if the empty set if drawn with probability one, the Shapley value cannot be approximated).\nIn the case of submodular functions with bounded curvature, we prove a tight approximation bound in terms of the curvature when samples are drawn from bounded product distributions. However, we show that the Shapley value cannot be approximated from samples even for coverage functions (which are a special case of submodular functions) and the uniform distribution. Since coverage functions are learnable from samples, this implies the counter-intuitive observation that learnability does not imply that the Shapley value is approximable from samples. We begin by formally defining \u03b1-approximability of the Shapley value in the statistical setting.\nDefinition 5. An algorithm \u03b1-approximates, \u03b1 \u2208 (0, 1], the Shapley value of a family of cost functions C over distribution D, if, for all C \u2208 C and all \u03b4 > 0, given poly(n, 1/\u03b4, 1/1\u2212\u03b1) samples from D, it computes Shapley value estimates \u03c6\u0303C such that for\n\u2022 positive bounded Shapley value, if \u03c6i \u2265 1/poly(n), then \u03b1\u03c6i \u2264 \u03c6\u0303i \u2264 1\u03b1\u03c6i;\n\u2022 negative bounded Shapley value, if \u03c6i \u2264 \u22121/ poly(n), then 1\u03b1\u03c6i \u2264 \u03c6\u0303i \u2264 \u03b1\u03c6i;\n\u2022 small Shapley value, if |\u03c6i| < 1/ poly(n), then |\u03c6i \u2212 \u03c6\u0303i| = o(1) .\nfor all i \u2208 N with probability at least 1\u2212 \u03b4 over both the samples and the choices made by the algorithm.\nControlling D. We begin by noting that there exists a distribution D such that the Shapley value can be approximated arbitrarily well for bounded functions. Other sampling methods have previously been suggested ([2, 19, 30, 31]), but the samples (S,C(S)) used in these methods are not i.i.d. and the value query model is assumed.\nDefinition 6. The Shapley distribution Dsh is the distribution which first picks a size j \u2208 {0, . . . , n} uniformly at random and then draws a uniformly random set of size j.\nLet Sji and S j \u2212i be the collections of samples of size j containing element i and not containing it respectively. Define avg(S) := ( \u2211\nS\u2208S C(S))/|S| to be the average value of the samples in S . Consider the following cost allocation:\n\u03c6\u0303i = n\u2211 j=1 avg(Sji )\u2212 n\u22121\u2211 j=0 avg(Sj\u2212i)\nWhen the distribution is the Shapley distributionDsh, the expected value of this cost allocation is the Shapley value and concentration bounds kick in.\nProposition 1. The Shapley value is (1 \u2212 )-approximable, for any constant > 0, over the Shapley distribution Dsh.\nProof. Recall the definition of the Shapley value \u03c6 and observe that\n\u03c6i = E \u03c3\n[C(S\u03c3<i \u222a {i})\u2212 C(S\u03c3<i)]\n= E S:|S|\u223cU({1,...,n}),i\u2208S [C(S)]\u2212 E S:|S|\u223cU({0,...,n\u22121}),i 6\u2208S [C(S)] = n\u2211 j=1 E[avg(Sji )]\u2212 n\u22121\u2211 j=0 E[avg(Sj\u2212i)]\n= E[\u03c6\u0303i]\nNext, observe that by standard concentration bounds and a sufficiently large polynomial number of samples drawn from the Shapley distribution Dsh, m/poly(n) samples are in Sji and S j \u2212i for all j and i. Then, by Hoeffding\u2019s inequality,\nPr [\u2223\u2223\u2223avg(Sji )\u2212 E[avg(Sji )]\u2223\u2223\u2223 \u2265 |\u03c6i|2n ] \u2264 2e\u2212 m( |\u03c6i|) 2 poly(n)\nBy a union bound, we have\nPr \u03c6\u0303i \u2212 \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 j=1 E[avg(Sji )]\u2212 n\u22121\u2211 j=0 E[avg(Sj\u2212i)] \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 |\u03c6i|  \u2264 2e\u2212m( |\u03c6i|)2poly(n) .\nWe get that either (1 \u2212 )\u03c6i \u2264 \u03c6\u0303i \u2264 (1 + )\u03c6i if \u03c6i > 0 or (1 + )\u03c6i \u2264 \u03c6\u0303i \u2264 (1 \u2212 )\u03c6i if \u03c6i < 0 with probability 1\u2212 2e\u2212 m 2\npoly(n) , if |\u03c6i| \u2265 1/poly(n). If |\u03c6i| = o(1/ poly(n)), we bound the first inequality by /2n instead of |\u03c6i|/2n and obtain |\u03c6i\u2212 \u03c6\u0303i| =\no(1) with probability 1\u2212 2e\u2212 m 2 poly(n) .\nA known method to estimate an expected value according to some distribution while given samples from another distribution is called importance sampling. Importance sampling reweighs samples according to their probabilities of being sampled. Although the above method achieves accurate estimates with a sufficiently large number of samples, the number of samples required may be exponential, see Theorem 9."}, {"heading": "4.1 Submodular functions with bounded curvature", "text": "We consider submodular functions with bounded curvature, a common assumption in the submodular maximization literature [27, 28, 40, 43]. We show that the Shapley value of these functions is approximable from samples, for which we derive a tight bound. Intuitively, the curvature of a submodular function bounds by how much the marginal contribution of an element can decrease. This property is useful since the Shapley value of an element can be written as a weighted sum of its marginal contributions over all sets.\nDefinition 7. A monotone submodular function C has curvature \u03ba \u2208 [0, 1] if CN\\{i}(i) \u2265 (1\u2212 \u03ba)C(i) for all i \u2208 N . This curvature is bounded if \u03ba < 1.\nAn immediate consequence of this definition is thatCS(i) \u2265 (1\u2212\u03ba)CT (i) for all S, T such that i 6\u2208 S\u222aT by monotonicity and submodularity. The main idea for the approximation is that the expected marginal contribution of an element i to a random set approximates the Shapley value of i by the curvature property. We use the same tool v\u0303i to estimate expected marginal contributions vi = ES\u223cD|i 6\u2208S [CS(i)] as for additive functions. Recall that v\u0303i = avg(Si) \u2212 avg(S\u2212i) is the difference between the average value of samples containing i and the average value of samples not containing i.\nTheorem 7. Monotone submodular functions with bounded curvature \u03ba have Shapley value that is \u221a\n1\u2212 \u03ba\u2212 approximable from samples over the uniform distribution and 1 \u2212 \u03ba \u2212 approximable over any bounded product distribution for any constant > 0.\nFirst, the Shapley value of monotone functions is non-negative since marginal contributions are nonnegative by monotonicity. Next, if a Shapley value \u03c6i is \u201csmall\u201d (o(1/poly(n))), then CS(i) is small for all S by the curvature property, implying that vi = ES\u223cD|i 6\u2208S [CS(i)] is small as well. By Lemma 11, a generalization of Lemma 1 showing that v\u0303i is a good estimate of vi, and with = 1/n, |vi \u2212 v\u0303i| = o(1). With \u03c6\u0303i = v\u0303i, we then obtain |\u03c6\u0303i \u2212 \u03c6i| = o(1).\nThe interesting case is positive bounded Shapley value \u03c6i, which we assume for the rest of the analysis. We first show a 1\u2212 \u03ba approximation for product distributions, which is a straightforward application of the curvature property combined with Lemma 11. Consider the algorithm which computes \u03c6\u0303i = v\u0303i. Note that\n\u03c6i = E \u03c3 [CA\u03c3<i(i)] \u2265 (1\u2212 \u03ba)vi > 1\u2212 \u03ba 1 + v\u0303i > (1\u2212 \u03ba\u2212 )v\u0303i\nwhere the first inequality is by curvature and the second by Lemma 11. Similarly, for the other direction, \u03c6i \u2264 vi/(1 \u2212 \u03ba) < v\u0303i/(1 \u2212 \u03ba \u2212 ). The \u221a 1\u2212 \u03ba result is the main technical component of this proof. We begin with a technical overview.\n1. Denote the uniform distribution over all sets of size j by Uj . Lemma 4 shows that the expected marginal contribution ES\u223cUj |i 6\u2208S [CS(i)] of i to a uniformly random set S of size j is decreasing in j, which is by submodularity.\n2. Consider L := (1\u2212 )n/2, H := (1 + )n/2. Lemma 5 shows that (1 + ) \u00b7 ES\u223cUL|i 6\u2208S [CS(i)] \u2265 vi and (1\u2212 ) \u00b7ES\u223cUH |i 6\u2208S [CS(i)] \u2264 vi, which is because a uniformly random set S is likely to have size between L and H and by submodularity. 3. Combining these two lemmas, roughly half of the terms (when j \u2264 L) in the summation \u03c6i = ( \u2211n\u22121\nj=0 ES\u223cUj |i 6\u2208S [CS(i)])/n are greater than vi and the other half (when j \u2265 H) of the terms are smaller. This is the main observation for the improvement from 1\u2212 \u03ba.\n4. The above and curvature imply that (1/2 + (1\u2212 \u03ba)/2)vi \u2264 \u03c6i \u2264 (1/2 + 1/(2(1\u2212 \u03ba))vi.\n5. By scaling vi to obtain the best approximation possible with the previous inequality, we obtain a\u221a 1\u2212 \u03ba approximation.\nLet \u03c6\u0303i =\n2\u2212 \u03ba 2 \u221a 1\u2212 \u03ba \u00b7 v\u0303i\nbe the estimated Shapley value. Denote by Uj the uniform distribution over all sets of size j, so\n\u03c6i = E \u03c3\n[CA\u03c3<i(i)] = 1\nn n\u22121\u2211 j=0 E S\u223cUj |i 6\u2208S [CS(i)].\nThe main idea to improve the loss from 1 \u2212 \u03ba to \u221a\n1\u2212 \u03ba is to observe that vi can be a factor 1 \u2212 \u03ba away from the contribution ES\u223cUjl |i 6\u2208S [CS(i)] of j to sets of low sizes jl \u2264 L := (1 \u2212\n\u2032) \u00b7 n/2 or 1 \u2212 \u03ba away from its contribution ES\u223cUjh |i 6\u2208S [CS(i)] to sets of high sizes jh \u2265 H := (1 +\n\u2032) \u00b7 n/2, but not both, otherwise the curvature property would be violated as illustrated in Figure 1. The following lemma shows that ES\u223cUj |i 6\u2208S [CS(i)] is decreasing in j by submodularity.\nLemma 4. Let C be a submodular function, then for all j \u2208 {0, . . . , n\u2212 1} and all i \u2208 N ,\nE S\u223cUj |i 6\u2208S [CS(i)] \u2265 E S\u223cUj+1|i 6\u2208S [CS(i)]\nProof. By submodularity,\u2211 S:|S|=j,i6\u2208S CS(i) \u2265 \u2211 S:|S|=j,i6\u2208S\n1 n\u2212 j \u2212 1 \u2211\ni\u2032 6\u2208S\u222a{i}\nCS\u222a{i\u2032}(i).\nIn addition, observe that by counting in two ways,\u2211 S:|S|=j,i6\u2208S \u2211 i\u2032 6\u2208S\u222a{i} CS\u222a{i\u2032}(i) = (j + 1) \u2211 S:|S|=j+1,i 6\u2208S CS(i).\nBy combining the two previous observations,\nE S\u223cUj |i 6\u2208S\n[CS(i)] = 1 |{S : |S| = j, i 6\u2208 S}| \u2211\nS:|S|=j,i6\u2208S\nCS(i)\n\u2265 1(n\u22121 j ) j + 1 n\u2212 j \u2212 1 \u2211 S:|S|=j+1,i 6\u2208S CS(i)\n= 1 |{S : |S| = j + 1, i 6\u2208 S}| \u2211\nS:|S|=j+1,i 6\u2208S\nCS(i)\n= E S\u223cUj+1|i 6\u2208S [CS(i)]\nThe next lemma shows that for j slightly lower than n/2, the expected marginal contribution vi of element i to a random set cannot be much larger than ES\u223cUj |i 6\u2208S [CS(i)], and similarly for j slightly larger than n/2, it cannot be much smaller.\nLemma 5. Let C be a submodular function, then for all i \u2208 N ,( 1 + e\u2212 n 6\n1\u2212 \u03ba ) \u00b7 E S\u223cUL|i 6\u2208S [CS(i)] \u2265 vi \u2265 ( 1\u2212 e\u2212 n 6 ) \u00b7 E S\u223cUH |i 6\u2208S [CS(i)].\nProof. By Chernoff bound, L \u2264 |S| and |S| \u2264 H with probability at least 1\u2212 e\u2212 n 6 each for S drawn from the uniform distribution. Denote the uniform distribution over all sets by U . So,\nvi = n\u22121\u2211 j=0 Pr S\u223cU|i 6\u2208S [|S| = j] \u00b7 E S\u223cUj |i 6\u2208S [CS(i)]\n\u2265 H\u2211 j=0 Pr S\u223cU|i 6\u2208S [|S| = j] \u00b7 E S\u223cUj |i 6\u2208S [CS(i)]\n\u2265 Pr S\u223cU|i 6\u2208S [|S| \u2264 H] \u00b7 E S\u223cUH |i 6\u2208S [CS(i)] Lemma 4 \u2265 (1\u2212 e\u2212 n 6 ) \u00b7 E\nS\u223cUH |i 6\u2208S [CS(i)].\nSimilarly,\nvi = n\u22121\u2211 j=0 Pr S\u223cU|i 6\u2208S [|S| = j] \u00b7 E S\u223cUj |i 6\u2208S [CS(i)]\n\u2264 Pr S\u223cU|i 6\u2208S [|S| < L] \u00b7 C(i) + Pr S\u223cU|i 6\u2208S [|S| \u2265 L] \u00b7 E S\u223cUL|i 6\u2208S [CS(i)] Lemma 4 \u2264 e\u2212 n 6 \u00b7 1\n1\u2212 \u03ba \u00b7 E S\u223cUL|i 6\u2208S [CS(i)] + E S\u223cUL|i 6\u2208S [CS(i)] curvature\nWe are now ready to prove Theorem 7:\n\u03c6i = 1\nn n\u22121\u2211 j=0 E S\u223cUj |i 6\u2208S [CS(i)]\n= 1\nn H\u22121\u2211 j=0 E S\u223cUj |i 6\u2208S [CS(i)] + n\u22121\u2211 i=H E S\u223cUj |i 6\u2208S [CS(i)]  \u2264 1 + \u2032\n2 \u00b7 C(i) + 1 2 \u00b7 E S\u223cUH |i 6\u2208S [CS(i)] Lemma 4\n\u2264 1 + \u2032\n2(1\u2212 \u03ba) \u00b7 vi +\n1\n2(1\u2212 e\u2212 \u2032n 6 ) \u00b7 vi curvature and Lemma 5\n\u2264 (\n2\u2212 \u03ba 2(1\u2212 \u03ba)\n+ c1 \u2032 ) \u00b7 vi\n\u2264 (\n2\u2212 \u03ba 2(1\u2212 \u03ba)\n+ c2 \u2032 ) \u00b7 v\u0303i Lemma 11\n= ( 1\u221a\n1\u2212 \u03ba\u2212 c3 \u2032\n) \u00b7 \u03c6\u0303i definition of \u03c6\u0303i\nfor some constants c1, c2, c3 and let \u2032 = /c3 to obtain the desired result for any . Similarly,\n\u03c6i = 1\nn L\u2211 j=0 E S\u223cUj |i 6\u2208S [CS(i)] + 1 n n\u22121\u2211 j=L+1 E S\u223cUj |i 6\u2208S [CS(i)]\n\u2265 1\u2212 \u2032\n2 \u00b7 E S\u223cUL|i 6\u2208S\n[CS(i)] + 1\n2 (C(N)\u2212 C(N \\ {i}) Lemma 4\n\u2265 1\u2212 \u2032\n2 ( 1 + e \u2212 \u2032n6 1\u2212\u03ba ) \u00b7 vi + 1\u2212 \u03ba 2 \u00b7 vi Lemma 5 and curvature\n\u2265 (\n2\u2212 \u03ba 2 \u2212 c1 \u2032\n) vi\n\u2265 (\n2\u2212 \u03ba 2 \u2212 c2 \u2032\n) v\u0303i Lemma 11\n= (\u221a 1\u2212 \u03ba\u2212 c3 \u2032 ) \u03c6\u0303i definition of \u03c6\u0303i\nWe show that this approximation is optimal. We begin with a general lemma to derive information theoretic inapproximability results for the Shapley value. This lemma shows that if there exists two functions in C that cannot be distinguished from samples with high probability and that have an element with Shapley value which differs by an \u03b12 factor, then C does not have a Shapley value that is \u03b1-approximable from samples.\nLemma 6. Consider a family of cost functions C, a constant \u03b1 \u2208 (0, 1), and assume there exist C1, C2 \u2208 C such that:\n\u2022 Indistinguishable from samples. With probability 1\u2212O(e\u2212\u03b2n) over S \u223c D for some constant \u03b2 > 0,\nC1(S) = C2(S).\n\u2022 Gap in Shapley value. There exists i \u2208 N such that\n\u03c6C 1 i < \u03b1 2\u03c6C 2 i .\nThen, C does not have Shapley value that is \u03b1-approximable from samples over D.\nProof. By a union bound, given m sets S1, . . . , Sm drawn i.i.d. from D with m polynomial in n, C1(Sj) = C2(Sj) for all Sj with probability 1\u2212O(e\u2212\u03b2n).\nLet C = C1 or C = C2 with probability 1/2 each. Assume the algorithm is given m samples such that C1(Sj) = C2(Sj) and consider its (possibly randomized) choice \u03c6\u0303i. Note that \u03c6\u0303i is independent of the randomization of C since C1 and C2 are indistinguishable to the algorithm. Since \u03c6C 1\ni /\u03c6 C2 i < \u03b1 2, \u03c6\u0303i is at least a factor \u03b1 away from \u03c6Ci with probability at least 1/2 over the choices of the algorithm and C. Label the cost functions so that \u03c6\u0303i is at least a factor \u03b1 away from \u03c6C 1\ni with probability at least 1/2 over the choices of the algorithm. Thus, with \u03b4 = 1/4, there exists no algorithm such that for all C \u2032 \u2208 {C1, C2}, \u03b1 \u00b7 \u03c6C\u2032i \u2264 \u03c6\u0303C \u2032 i \u2264 1\u03b1 \u00b7 \u03c6 C\u2032 i with probability at least 3/4 over both the samples and the choices of the algorithm.\nWe obtain the inapproximability result by constructing two such functions.\nTheorem 8. For every \u03ba < 1, there exists a hypothesis class of submodular functions with curvature \u03ba that have Shapley value that is not \u221a 1\u2212 \u03ba + -approximable from samples over the uniform distribution, for every constant > 0.\nProof. We first give a technical overview.\n\u2022 We construct two functions C1 and C2 which are indistinguishable from samples but that have an element i? for which its marginal contribution differs by a factor of 1 \u2212 \u03ba for the two functions and then Lemma 6 concludes the proof.\n\u2022 The expected marginal contribution ES\u223cUj |i? 6\u2208S [CS(i?)] for both of these functions is illustrated in Figure 2 as a function of j. Informally, ES\u223cUj |i? 6\u2208S [CS(i\n?)] is equal for both functions between L and H to obtain indistinguishability from samples since samples are of size between L and H with high probability. Combining this constraint with the submodular and curvature constraints, the gap between ES\u223cUj |i? 6\u2208S [C 1 S(i ?)] and ES\u223cUj |i? 6\u2208S [C 2 S(i\n?)] is maximized for all j < L and j > H to maximize the gap in the Shapley value for i?.\nThese two functions have a simpler definition via their marginal contributions, so we start by defining them in terms of these marginal contributions and we later give their formal definition to show that they are well-defined. The marginal contributions of i? are illustrated in Figure 2.\nC1S(i) = { 1 if |S| < L 1\u2212 \u03ba otherwise\nC2S(i ?) =  1\u2212 \u03ba if |S| \u2264 H 1\u2212 \u03ba\u2212 (|S| \u2212H) \u00b7 1\u2212\u03ba\u2212(1\u2212\u03ba) 2 \u221a n if H < |S| \u2264 H + \u221a n\n(1\u2212 \u03ba)2 otherwise\nFor i 6= i?:\nC2S(i) =  L\u2212(1\u2212\u03ba) L\u22121 if |S| < L\u2212 1 or (|S| = L\u2212 1 and i ? \u2208 S) 1\u2212 \u03ba if (|S| = L\u2212 1 and i? 6\u2208 S) or L \u2264 |S| \u2264 H or (H \u2264 |S| \u2264 H + \u221a n and i? 6\u2208 S)\n1\u2212 \u03ba\u2212 1\u2212\u03ba\u2212(1\u2212\u03ba) 2\n\u221a n\notherwise\nThe formal definitions of the functions are\nC1(S) = { |S| if |S| < L L+ (|S| \u2212 L) \u00b7 (1\u2212 \u03ba) otherwise\nand\nC2(S) =  1i?\u2208S \u00b7 (1\u2212 \u03ba) + (|S| \u2212 1i?\u2208S) \u00b7 L\u2212(1\u2212\u03ba)L\u22121 if |S| < L L+ (|S| \u2212 L) \u00b7 (1\u2212 \u03ba) if L \u2264 |S| \u2264 H or (H < |S| \u2264 H + \u221a n and i? 6\u2208 S) L+ (|S| \u2212 L) \u00b7 (1\u2212 \u03ba) +1\u2212 \u03ba\u2212 (|S| \u2212H) \u00b7 1\u2212\u03ba\u2212(1\u2212\u03ba) 2 \u221a n if H < |S| \u2264 H + \u221a n and i? \u2208 S L+ (H + \u221a n\u2212 L) \u00b7 (1\u2212 \u03ba) + 1i?\u2208S \u00b7 (1\u2212 \u03ba)2\n+(|S| \u2212 1i?\u2208S \u2212 (H + \u221a n))(1\u2212 \u03ba\u2212 1\u2212\u03ba\u2212(1\u2212\u03ba) 2 \u221a n ) otherwise\nThe Shapley value of i? with respect to C1 and C2 is then:\n\u03c6C 1 i? = 1 \u00b7 1\u2212 \u2032\n2 + (1\u2212 \u03ba) \u00b7 1 +\n\u2032\n2 \u2265 2\u2212 \u03ba 2 \u2212\nand\n\u03c6C 2 i? \u2264 (1\u2212 \u03ba) \u00b7 1 + \u2032 2 + (1\u2212 \u03ba)2 \u00b7 1\u2212 \u2032 2 \u2264 (1\u2212 \u03ba)(2\u2212 \u03ba) 2 +\nfor an appropriate choice of \u2032. Next, by Chernoff bound and a union bound, L \u2264 |S| \u2264 H for polynomially many samples S from the uniform distirbution, with probability 1\u2212 e\u2212\u2126(n). Thus, C1(S) = C2(S) for all samples S with probability 1\u2212 e\u2212\u2126(n).\nIt remains to show that C1 and C2 are submodular with curvature \u03ba, i.e., for any S \u2286 T and i 6\u2208 T ,\nCS(i) \u2265 CT (i) \u2265 (1\u2212 \u03ba)CS(i),\nwhich is immediate for C1. Regarding C2, it is also immediate that C2S(i ?) \u2265 C2T (i?) \u2265 (1 \u2212 \u03ba)C2S(i?). For i 6= i?, observe that\nL\u2212 (1\u2212 \u03ba) L\u2212 1 \u2264 1 + and 1\u2212 \u03ba\u2212 1\u2212 \u03ba\u2212 (1\u2212 \u03ba) 2 \u221a n \u2265 1\u2212 \u03ba\u2212 ,\nso C2S(i) \u2265 C2T (i) \u2265 (1\u2212 \u03ba\u2212 )C2S(i)."}, {"heading": "4.2 Learnability does not imply approximability of the Shapley value", "text": "Although the Shapley value is approximable for the class of submodular functions with bounded curvature, we show that the Shapley value of coverage (and submodular) functions are not approximable from samples in general. The impossibility result is information theoretic and is not due to computational limitations. Coverage functions are an interesting class of functions because they are learnable from samples over any distribution [3], according to the PMAC learning model [5], which is a generalization of PAC learnability for real valued functions. In addition, by Theorem 1, coverage functions have Shapley value that can efficiently be approximated arbitrarily well in the value query model. Thus, this impossibility result implies that learnability and approximability in the value query model are not strong enough conditions for approximability of the Shapley value from samples.\nTheorem 9. There exists no constant \u03b1 > 0 such that coverage functions have Shapley value that is \u03b1approximable from samples over the uniform distribution.\nProof. Partition N into two parts A and B of equal size. Consider the following two functions:\nC1(S) =  0 if S = \u2205 1 if |S \u2229A| > 0, |S \u2229B| = 0 1 \u03b12 if |S \u2229A| = 0, |S \u2229B| > 0 1 + 1\n\u03b12 otherwise\nC2(S) =  0 if S = \u2205 1 if |S \u2229B| > 0, |S \u2229A| = 0 1 \u03b12 if |S \u2229B| = 0, |S \u2229A| > 0 1 + 1\n\u03b12 otherwise\nThese functions are coverage functions with U = {a, b1, . . . , b1/\u03b12} and Ti = {a} or Ti = {b1, . . . , b1/\u03b12}. By the Chernoff bound (Lemma 9) with \u03b4 = 1/2 and \u00b5 = n/2, if S is a sample from the uniform distribution, then\nPr [|S \u2229A| = 0] = Pr [|S \u2229B| = 0] < Pr [|S \u2229B| \u2264 n/4] \u2264 e\u2212n/16,\nso C1(S) 6= C2(S) with probability at most 2e\u2212n/16. It is easy to see that for any i, its Shapley value is either 2/n or 2/(\u03b12n) depending on which partition it is in. Combining this with Lemma 6 concludes the proof."}, {"heading": "5 Data Dependent Shapley Value", "text": "The general impossibility result for computing the Shapley value from samples arises from the fact that the concept was geared towards the query model, where the algorithm can ask for the cost of any set S \u2286 N . In this section, we develop an analogue that is data- or distribution-dependent. We denote it by \u03c6C,D with respect to both C and D. We define four natural distribution-dependent axioms resembling the Shapley value axioms, and then prove that our proposed value is the unique solution satisfying them. This value can be approximated arbitrarily well in the statistical model for all functions. We start by stating the four axioms.\nDefinition 8. The data-dependent axioms for cost sharing functions are: \u2022 Balance: \u2211\ni\u2208N \u03c6 D i = ES\u223cD[C(S)],\n\u2022 Symmetry: for all i and j, if PrS\u223cD [|S \u2229 {i, j}| = 1] = 0 then \u03c6Di = \u03c6Dj ,\n\u2022 Zero element: for all i, if PrS\u223cD [i \u2208 S] = 0 then \u03c6Di = 0,\n\u2022 Additivity: for all i, if D1, D2, \u03b1, and \u03b2 such that \u03b1 + \u03b2 = 1, \u03c6\u03b1D1+\u03b2D2i = \u03b1\u03c6 D1 i + \u03b2\u03c6 D2 i where\nPr [S \u223c \u03b1D1 + \u03b2D2] = \u03b1 \u00b7 Pr [S \u223c D1] + \u03b2 \u00b7 Pr [S \u223c D2].\nThe similarity to the original Shapley value axioms is readily apparent. The main distinction is that we expect these to hold with regard to D, which captures the frequency with which different coalitions S occur. Note that we no longer require that D has full support over 2N . Interpreting the axioms one by one, the balance property ensures that the expected cost is always accounted for. The symmetry axiom states that if two elements always occur together, they should have the same share, since they are indistinguishable. If an element is never observed, then it should have zero share. Finally costs should combine in a linear manner according to the distribution.\nThese axioms are specifically designed to provide some guarantees on the shares of elements to functions with complex interactions where recovery is hard from samples.\nWe define the data-dependent Shapley value:\n\u03c6Di := \u2211\nS : i\u2208S Pr [S \u223c D] \u00b7 C(S) |S| .\nInformally, for all set S, the cost C(S) is divided equally between all elements in S and is weighted with the probability that S occurs according to D. The main appeal of this cost allocation is the following theorem.\nTheorem 10. The data-dependent Shapley value is the unique value satisfying the four data-dependent axioms.\nWe first show that if there exists a value satisfying the axioms, it must be the data-dependent Shapley value. Then, we show that the data-dependent Shapley value satisfies the axioms, which concludes the proof.\nLemma 7. If there exists a value satisfying the four data-dependent Shapley axioms, then this value is the data-dependent Shapley value.\nProof. DefineDS to be the distribution such that Pr [S \u223c DS ] = 1. Observe that the unique value satisfying the balance, symmetry, and zero element axioms must satisfy\n\u03c6DSi =\n{ C(S) |S| if i \u2208 S\n0 otherwise.\nSince D = \u2211\nS Pr [S \u223c D] \u00b7 DS , the unique value satisfying the four axioms must satisfy\n\u03c6Di = \u2211 S Pr [S \u223c D] \u00b7 \u03c6DSi = \u2211 S:i\u2208S Pr [S \u223c D] \u00b7 C(S) |S|\nwhere the first equality is by additivity and the second equality by the above observation.\nLemma 8. The data-dependent Shapley value satisfies the four data-dependent Shapley axioms.\nProof. We show that each axiom is satisfied. \u2022 Balance: By definition, \u2211\ni\u2208N \u03c6 D i = \u2211 i\u2208N \u2211 S:i\u2208S Pr [S \u223c D]C(S)/|S|, then by switching the\norder of the summations,\u2211 i\u2208N \u2211 S:i\u2208S Pr [S \u223c D] C(S) |S| = \u2211 S\u2286N \u2211 i\u2208S Pr [S \u223c D] C(S) |S| = \u2211 S\u2286N Pr [S \u223c D]C(S) = E S\u223cD [C(S)].\n\u2022 Symmetry: Let Si = {S : i \u2208 S,Pr [S \u223c D] > 0}. If PrS\u223cD [|S \u2229 {i, j}| = 1] = 0, then Si = Sj and\n\u03c6Di = \u2211 S\u2208Si Pr [S \u223c D] C(S) |S| = \u2211 S\u2208Sj Pr [S \u223c D] C(S) |S| = \u03c6Dj .\n\u2022 Zero element: If PrS\u223cD [i \u2208 S] = 0, then Pr [S \u223c D] = 0 if i \u2208 S. Thus, \u03c6Di = 0.\n\u2022 Additivity: By definition of \u03c6 and \u03b1D1 + \u03b2D2,\n\u03c6\u03b1D1+\u03b2D2i = \u2211\nS : i\u2208S Pr [S \u223c \u03b1D1 + \u03b2D]\nC(S)\n|S|\n= \u03b1 \u2211 S:i\u2208S Pr [S \u223c D1] C(S) |S| + \u03b2 \u2211 S:i\u2208S Pr [S \u223c D2] C(S) |S| = \u03b1\u03c6D1i + \u03b2\u03c6 D2 i .\nThe data-dependent Shapley value can be approximated from samples with the following empirical data-dependent Shapley value:\n\u03c6\u0303Di = 1\nm \u2211 Sj : i\u2208Sj C(Sj) |Sj | .\nThese estimates are arbitrarily good with arbitrarily high probability.\nTheorem 11. The empirical data-dependent Shapley value approximates the data-dependent Shapley value arbitrarily well, i.e., |\u03c6\u0303Di \u2212 \u03c6Di | < with poly(n, 1/ , 1/\u03b4) samples and with probability at least 1 \u2212 \u03b4 for any \u03b4, > 0.\nProof. Define Xj =\n{ C(Sj) |Sj | if i \u2208 Sj\n0 otherwise and observe that (\n\u2211m j=1Xj)/m = \u03c6\u0303 D i and\nE[( \u2211m\nj=1Xj)/m] = \u03c6 D i . Clearly, Xj \u2208 [0, b] where b := maxS C(S)/|S|, so by Hoeffding\u2019s inequality, Pr [ |\u03c6\u0303Di \u2212 \u03c6Di | \u2265 ] \u2264 2e\u2212 2m 2 poly(n) with 0 < < 1."}, {"heading": "6 Discussion and Future Work", "text": "We follow a recent line of work that studies classical algorithmic problems from a statistical perspective, where the input is restricted to a collection of samples. Our results fall into two categories, we give results for approximating the Shapley value and the core and propose new cost sharing concepts that are tailored for the statistical framework. We use techniques from multiple fields that encompass statistical machine learning, combinatorial optimization, and, of course, cost sharing. The cost sharing literature being very rich, the number of directions for future work are considerable. Obvious avenues include studying other cost sharing methods in this statistical framework, considering other classes of functions to approximate known methods, and improving the sample complexity of previous algorithms. More conceptually, an exciting modeling question arises when designing \u201cdesirable\u201d axioms from data. Traditionally these axioms only depended on the cost function, whereas in this model they can depend on both the cost function and the distribution, providing an interesting interplay."}, {"heading": "A Concentration Bounds", "text": "Lemma 9 (Chernoff Bound). Let X1, . . . , Xn be independent indicator random variables with values in {0, 1}. Let X = \u2211n i=1Xi and \u00b5 = E[X]. For 0 < \u03b4 < 1,"}, {"heading": "Pr [X \u2264 (1\u2212 \u03b4)\u00b5] \u2264 e\u2212\u00b5\u03b42/2 and Pr [X \u2265 (1 + \u03b4)\u00b5] \u2264 e\u2212\u00b5\u03b42/3.", "text": "Lemma 10 (Hoeffding\u2019s inequality). LetX1, . . . , Xn be independent random variables with values in [0, b]. Let X = 1m \u2211m i=1Xi and \u00b5 = E[X]. Then for every 0 < < 1,\nPr [|X \u2212 E[X]| \u2265 ] \u2264 2e\u22122m 2/b2 ."}, {"heading": "B Estimating the Expected Marginal Contribution of an Element", "text": "Recall that Si and S\u2212i are the collections of samples containing element i and not containing it respectively and that avg(S) = ( \u2211 S\u2208S C(S))/|S| is the average value of the samples in S. Let vi = C(i) and v\u0303i = avg(Si)\u2212 avg(S\u2212i).\nLemma 1. Let C be an additive function with C(i) \u2265 1/ poly(n) for all i. Then, given poly(n, 1/\u03b4, 1/ ) samples, we can compute an estimate v\u0303i such that with probability (1\u2212 \u03b4):\n(1\u2212 )C(i) < v\u0303i < (1 + )C(i).\nThis lemma is a special case of the following stronger lemma.\nLemma 11. The expected marginal contribution of an element i to a random set from a bounded product distribution D not containing i is estimated arbitrarily well by v\u0303i, i.e., for all i \u2208 N and given poly(n, 1/\u03b4, 1/ ) samples,\n(1\u2212 )vi \u2264 v\u0303i \u2264 (1 + )vi if vi \u2265 1/ poly(n) |vi \u2212 v\u0303i| \u2264 if |vi| < 1/poly(n) (1 + )vi \u2264 v\u0303i \u2264 (1\u2212 )vi if vi \u2264 \u22121/ poly(n)\nwith probability at least 1\u2212 \u03b4 for any \u03b4 > 0.\nProof. Note that\nvi = E S\u223cD|i 6\u2208S [CS(i)] = E S\u223cD|i 6\u2208S [C(S \u222a i)]\u2212 E S\u223cD|i 6\u2208S [C(S)] = E S\u223cD|i\u2208S [C(S)]\u2212 E S\u223cD|i 6\u2208S [C(S)].\nwhere the second equality is since D is a product distribution. In addition, E[avg(Si)] = ES\u223cD|i\u2208S [C(S)] and E[avg(S\u2212i)] = ES\u223cD|i 6\u2208S [C(S)]. Since marginal probabilities of the product distributions are assumed to be bounded from below and above by 1/ poly(n) and 1 \u2212 1/ poly(n) respectively, |Si| = m/poly(n) and |S\u2212i| = m/poly(n) for all i by Chernoff bound. In addition, maxS C(S) is assumed to be bounded by poly(n). So by Hoeffding\u2019s inequality,\nPr (\u2223\u2223\u2223\u2223avg(Si)\u2212 ES\u223cD|i\u2208S[C(S)] \u2223\u2223\u2223\u2223 \u2265 |vi| /2) \u2264 2e\u2212m(|vi| )2poly(n) ,\nfor 0 < < 2/vi and\nPr (\u2223\u2223\u2223\u2223avg(S\u2212i)\u2212 ES\u223cD|i 6\u2208S[C(S)] \u2223\u2223\u2223\u2223 \u2265 |vi| /2) \u2264 2e\u2212m(|vi| )2poly(n) .\nThus,\nPr(|v\u0303i \u2212 vi| \u2265 |vi| ) \u2264 2e\u2212 m(|vi| )\n2\npoly(n)\nand, either (1 \u2212 )vi \u2264 v\u0303i \u2264 (1 + )vi if vi > 0 or (1 + )vi \u2264 v\u0303i \u2264 (1 \u2212 )vi if vi < 0, with probability at least 1 \u2212 2e\u2212 m(|vi| ) 2 poly(n) . If |vi| < 1/poly(n), we obtain |vi \u2212 v\u0303i| < with a similar analysis without any assumption on vi. Otherwise, the bounds on the estimation hold with probability at least 1\u2212 2e m 2 poly(n) .\nCorollary 2. Let C be an additive function such that C(i) \u2265 1/ poly(n). Then, C has Shapley value and a core that are (1\u2212 )-approximable from samples over bounded product distributions for any constant > 0.\nProof. For the Shapley value, it follows immediately from Lemma 11. Regarding the core, let \u03c8i = v\u0303i \u00b7 C(N)\u2211 j\u2208N v\u0303j , so roughly v\u0303i but slightly scaled to obtain the balance property, which holds since \u2211\ni\u2208N \u03c8i =\u2211 i\u2208N v\u0303i\u00b7 C(N)\u2211 j\u2208N v\u0303j = C(N). For the approximate core property, first note that C(N)\u2211 j\u2208N v\u0303j\n\u2264 C(N)(1\u2212 \u2032)\u2211j\u2208N C(j) = 1\n1\u2212 \u2032 , so,\n(1\u2212 ) \u2211 i\u2208S \u03c8i \u2264 (1\u2212 ) \u2211 i\u2208S v\u0303i \u00b7 C(N)\u2211 j\u2208N v\u0303j \u2264 (1\u2212 )(1 + \u2032) 1\u2212 \u2032 \u2211 i\u2208S C(i) \u2264 C(S)\nfor \u2032 picked accordingly small compared to .\nC VC-Dimension and Rademacher Complexity Review\nWe formally define the VC-dimension and the Rademacher complexity using definitions from [37]. We begin with the VC-dimension, which is for classes of binary functions. We first define the concepts of restriction to a set and of shattering, which are useful to define the VC-dimension.\nDefinition 9. (Restriction of H to A). Let H be a class of functions from X to {0, 1} and let A = {a1, . . . , am} \u2282 X . The restriction of H to A is the set of functions from A to {0, 1} that can be derived fromH. That is, HA = {(h(a1), . . . , h(am)) : h \u2208 H}, where we represent each function from A to {0, 1} as a vector in {0, 1}|A|.\nDefinition 10. (Shattering). A hypothesis class H shatters a finite set A \u2282 X if the restriction of H to A is the set of all functions from A to {0, 1}. That is, |HA| = 2|A|.\nDefinition 11. (VC-dimension). The VC-dimension of a hypothesis class H is the maximal size of a set S \u2282 X that can be shattered by H. If H can shatter sets of arbitrarily large size we say that H has infinite VC-dimension.\nNext, we define the Rademacher complexity, which is for more complex classes of functions than binary functions, such as real-valued functions.\nDefinition 12. (Rademacher complexity). Let \u03c3 be distributed i.i.d. with Pr[\u03c3i = 1] = Pr[\u03c3i = \u22121] = 1/2. The Rademacher complexity R(A) of a set of vectors A \u2282 Rm is R(A) := 1mE\u03c3 [supa\u2208A \u2211m i=1 \u03c3iai] ."}], "references": [{"title": "The price of stability for network design with fair cost allocation", "author": ["Elliot Anshelevich", "Anirban Dasgupta", "Jon Kleinberg", "Eva Tardos", "Tom Wexler", "Tim Roughgarden"], "venue": "SIAM Journal on Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Approximating power indices: theoretical and empirical analysis", "author": ["Yoram Bachrach", "Evangelos Markakis", "Ezra Resnick", "Ariel D Procaccia", "Jeffrey S Rosenschein", "Amin Saberi"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Sketching valuation functions", "author": ["Ashwinkumar Badanidiyuru", "Shahar Dobzinski", "Hu Fu", "Robert Kleinberg", "Noam Nisan", "Tim Roughgarden"], "venue": "In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Finding low error clusterings", "author": ["Maria-Florina Balcan", "Mark Braverman"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Learning submodular functions", "author": ["Maria-Florina Balcan", "Nicholas JA Harvey"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Agnostic clustering", "author": ["Maria Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning valuation functions", "author": ["Maria-Florina Balcan", "Florin Constantin", "Satoru Iwata", "Lei Wang"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning cooperative games", "author": ["Maria-Florina Balcan", "Ariel D. Procaccia", "Yair Zick"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Learning cooperative games", "author": ["Maria-Florina Balcan", "Ariel D Procaccia", "Yair Zick"], "venue": "arXiv preprint arXiv:1505.00039v2,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Mechanisms for fair attribution", "author": ["Eric Balkanski", "Yaron Singer"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "The power of optimization from samples", "author": ["Eric Balkanski", "Aviad Rubinstein", "Yaron Singer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "The limitations of optimization from samples", "author": ["Eric Balkanski", "Aviad Rubinstein", "Yaron Singer"], "venue": "49th Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Some applications of linear programming methods to the theory of cooperative games", "author": ["Olga N Bondareva"], "venue": "Problemy kibernetiki,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1963}, {"title": "Mechanism design for data science", "author": ["Shuchi Chawla", "Jason Hartline", "Denis Nekipelov"], "venue": "In Proceedings of the fifteenth ACM conference on Economics and computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "The sample complexity of revenue maximization", "author": ["Richard Cole", "Tim Roughgarden"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Algorithmic aspects of the core of combinatorial optimization games", "author": ["Xiaotie Deng", "Toshihide Ibaraki", "Hiroshi Nagamochi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Strategyproof cost-sharing mechanisms for set cover and facility location games", "author": ["Nikhil R Devanur", "Milena Mihail", "Vijay V Vazirani"], "venue": "Decision Support Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Sampling and representation complexity of revenue maximization", "author": ["Shaddin Dughmi", "Li Han", "Noam Nisan"], "venue": "In International Conference on Web and Internet Economics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A linear approximation method for the shapley value", "author": ["Shaheen S Fatima", "Michael Wooldridge", "Nicholas R Jennings"], "venue": "Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Sharing the cost of muliticast transmissions (preliminary version)", "author": ["Joan Feigenbaum", "Christos Papadimitriou", "Scott Shenker"], "venue": "In Proceedings of the thirty-second annual ACM symposium on Theory of computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Learning coverage functions and private release of marginals", "author": ["Vitaly Feldman", "Pravesh Kothari"], "venue": "In COLT,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Optimal bounds on approximation of submodular and xos functions by juntas", "author": ["Vitaly Feldman", "Jan Vondrak"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Solutions to general non-zero-sum games", "author": ["Donald B Gillies"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1959}, {"title": "Cooperative facility location games", "author": ["Michel X Goemans", "Martin Skutella"], "venue": "Journal of Algorithms,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "CoRR, abs/1412.6572,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Limitations of cross-monotonic costsharing schemes", "author": ["Nicole Immorlica", "Mohammad Mahdian", "Vahab S Mirrokni"], "venue": "ACM Transactions on Algorithms (TALG),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Submodular optimization with submodular cover and submodular knapsack constraints", "author": ["Rishabh K Iyer", "Jeff A Bilmes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Curvature and optimal algorithms for learning and minimizing submodular functions", "author": ["Rishabh K Iyer", "Stefanie Jegelka", "Jeff A Bilmes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Equitable cost allocations via primal-dual-type algorithms", "author": ["Kamal Jain", "Vijay V Vazirani"], "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Computing shapley value in supermodular coalitional games", "author": ["David Liben-Nowell", "Alexa Sharp", "Tom Wexler", "Kevin Woods"], "venue": "In International Computing and Combinatorics Conference,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Values of large games, IV: Evaluating the electoral college by Montecarlo techniques", "author": ["Irwin Mann"], "venue": "Rand Corporation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1960}, {"title": "On the pseudo-dimension of nearly optimal auctions", "author": ["Jamie H Morgenstern", "Tim Roughgarden"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Incremental cost sharing: Characterization by coalition strategy-proofness", "author": ["Herv\u00e9 Moulin"], "venue": "Social Choice and Welfare,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Strategyproof sharing of submodular costs: budget balance versus efficiency", "author": ["Herv\u00e9 Moulin", "Scott Shenker"], "venue": "Economic Theory,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "Group strategy proof mechanisms via primal-dual algorithms", "author": ["Martin P\u00e1l", "\u00c9va Tardos"], "venue": "In Foundations of Computer Science,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "The Shapley value: essays in honor of Lloyd S", "author": ["Alvin E Roth"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1988}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "On balanced sets and cores", "author": ["Lloyd S Shapley"], "venue": "Naval research logistics quarterly,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1967}, {"title": "A value for n-person games1", "author": ["LS Shapley"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1953}, {"title": "Optimal approximation for submodular and supermodular optimization with bounded curvature", "author": ["Maxim Sviridenko", "Jan Vondr\u00e1k", "Justin Ward"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Theory of games and economic behavior", "author": ["John Von Neumann", "Oskar Morgenstern"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1944}, {"title": "Submodularity and curvature: the optimal algorithm", "author": ["Jan Vondr\u00e1k"], "venue": "RIMS Kokyuroku Bessatsu B,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "[8] in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Two popular cost sharing concepts are the core [23], where no group of players has an incentive to deviate, and the Shapley value [39], which is the unique vector of cost shares satisfying four natural axioms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "Two popular cost sharing concepts are the core [23], where no group of players has an incentive to deviate, and the Shapley value [39], which is the unique vector of cost shares satisfying four natural axioms.", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Recent work [30] shows that one can find approximate Shapley values for a restricted subset of cost functions by looking at the costs for polynomially many specifically chosen examples.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "[8] that the main difficulty with using cost sharing methods in concrete applications is the information needed to compute them.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "While one can evaluate the objective function on any subset of features, deep networks are notorious for performing poorly on certain out of sample examples [25, 41], which may lead to misleading conclusions when using traditional cost sharing methods.", "startOffset": 157, "endOffset": 165}, {"referenceID": 40, "context": "While one can evaluate the objective function on any subset of features, deep networks are notorious for performing poorly on certain out of sample examples [25, 41], which may lead to misleading conclusions when using traditional cost sharing methods.", "startOffset": 157, "endOffset": 165}, {"referenceID": 7, "context": "[8], which studied STATISTICAL COST SHARING in the context of the core, and assume that only partial data about the cost function is observed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] relied on first learning the cost function and then computing cost shares, we show how to proceed directly, computing cost shares without explicitly learning a good estimate of the cost function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 41, "context": "The first is the notion of cost sharing in cooperative games, first introduced by Von Neumann and Morgenstern [42].", "startOffset": 110, "endOffset": 114}, {"referenceID": 38, "context": "The Shapley value [39] is studied in algorithmic mechanism design [1, 10, 20, 33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "The Shapley value [39] is studied in algorithmic mechanism design [1, 10, 20, 33].", "startOffset": 66, "endOffset": 81}, {"referenceID": 9, "context": "The Shapley value [39] is studied in algorithmic mechanism design [1, 10, 20, 33].", "startOffset": 66, "endOffset": 81}, {"referenceID": 19, "context": "The Shapley value [39] is studied in algorithmic mechanism design [1, 10, 20, 33].", "startOffset": 66, "endOffset": 81}, {"referenceID": 32, "context": "The Shapley value [39] is studied in algorithmic mechanism design [1, 10, 20, 33].", "startOffset": 66, "endOffset": 81}, {"referenceID": 35, "context": "For applications of the Shapley value, see the surveys by Roth [36] and Winter [44].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "A naive computation of the Shapley value of a cooperative game would take exponential time; recently, methods for efficiently approximating the Shapley value have been suggested [2, 19, 30, 31] for some restricted settings.", "startOffset": 178, "endOffset": 193}, {"referenceID": 18, "context": "A naive computation of the Shapley value of a cooperative game would take exponential time; recently, methods for efficiently approximating the Shapley value have been suggested [2, 19, 30, 31] for some restricted settings.", "startOffset": 178, "endOffset": 193}, {"referenceID": 29, "context": "A naive computation of the Shapley value of a cooperative game would take exponential time; recently, methods for efficiently approximating the Shapley value have been suggested [2, 19, 30, 31] for some restricted settings.", "startOffset": 178, "endOffset": 193}, {"referenceID": 30, "context": "A naive computation of the Shapley value of a cooperative game would take exponential time; recently, methods for efficiently approximating the Shapley value have been suggested [2, 19, 30, 31] for some restricted settings.", "startOffset": 178, "endOffset": 193}, {"referenceID": 22, "context": "The core, introduced by Gillies [23], is another well-studied solution concept for cooperative games.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "Bondareva [13] and Shapley [38] characterized when the core is non-empty.", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "Bondareva [13] and Shapley [38] characterized when the core is non-empty.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "The core has been studied in the context of multiple combinatorial games, such as facility location [24] and maximum flow [16].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "The core has been studied in the context of multiple combinatorial games, such as facility location [24] and maximum flow [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "In cases with no solutions in the core or when it is computationally hard to find one, the balance property has been relaxed to hold approximately [17, 26].", "startOffset": 147, "endOffset": 155}, {"referenceID": 25, "context": "In cases with no solutions in the core or when it is computationally hard to find one, the balance property has been relaxed to hold approximately [17, 26].", "startOffset": 147, "endOffset": 155}, {"referenceID": 25, "context": "In applications where players submit bids, cross-monotone cost sharing, a concept stronger than the core that satisfies the group strategy proofness property, has attracted a lot of attention [26, 29, 34, 35].", "startOffset": 192, "endOffset": 208}, {"referenceID": 28, "context": "In applications where players submit bids, cross-monotone cost sharing, a concept stronger than the core that satisfies the group strategy proofness property, has attracted a lot of attention [26, 29, 34, 35].", "startOffset": 192, "endOffset": 208}, {"referenceID": 33, "context": "In applications where players submit bids, cross-monotone cost sharing, a concept stronger than the core that satisfies the group strategy proofness property, has attracted a lot of attention [26, 29, 34, 35].", "startOffset": 192, "endOffset": 208}, {"referenceID": 34, "context": "In applications where players submit bids, cross-monotone cost sharing, a concept stronger than the core that satisfies the group strategy proofness property, has attracted a lot of attention [26, 29, 34, 35].", "startOffset": 192, "endOffset": 208}, {"referenceID": 8, "context": "[9] proved a polynomial sample complexity bound for this problem of computing cost shares that are likely to respect the core property, for all functions with a non-empty core.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], who showed how to compute an approximate core for some families of games.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "In auction design, a line of work [14, 15, 18, 32] has studied revenue maximization from samples instead of being given a Bayesian prior.", "startOffset": 34, "endOffset": 50}, {"referenceID": 14, "context": "In auction design, a line of work [14, 15, 18, 32] has studied revenue maximization from samples instead of being given a Bayesian prior.", "startOffset": 34, "endOffset": 50}, {"referenceID": 17, "context": "In auction design, a line of work [14, 15, 18, 32] has studied revenue maximization from samples instead of being given a Bayesian prior.", "startOffset": 34, "endOffset": 50}, {"referenceID": 31, "context": "In auction design, a line of work [14, 15, 18, 32] has studied revenue maximization from samples instead of being given a Bayesian prior.", "startOffset": 34, "endOffset": 50}, {"referenceID": 3, "context": "In the inductive clustering setting, the algorithm is only given a small random subset of the data set it wishes to cluster [4, 6].", "startOffset": 124, "endOffset": 130}, {"referenceID": 5, "context": "In the inductive clustering setting, the algorithm is only given a small random subset of the data set it wishes to cluster [4, 6].", "startOffset": 124, "endOffset": 130}, {"referenceID": 10, "context": "More closely related to our work is the problem of optimization from samples [11, 12] where, the goal is to approximate maxS\u2208M C(S) for some constraint M \u2286 2N and C : 2N \u2192 R from samples for some class of combinatorial functions.", "startOffset": 77, "endOffset": 85}, {"referenceID": 11, "context": "More closely related to our work is the problem of optimization from samples [11, 12] where, the goal is to approximate maxS\u2208M C(S) for some constraint M \u2286 2N and C : 2N \u2192 R from samples for some class of combinatorial functions.", "startOffset": 77, "endOffset": 85}, {"referenceID": 1, "context": "However, several sampling approaches have been suggested to approximate the Shapley value for specific classes of functions [2, 19, 30, 31].", "startOffset": 124, "endOffset": 139}, {"referenceID": 18, "context": "However, several sampling approaches have been suggested to approximate the Shapley value for specific classes of functions [2, 19, 30, 31].", "startOffset": 124, "endOffset": 139}, {"referenceID": 29, "context": "However, several sampling approaches have been suggested to approximate the Shapley value for specific classes of functions [2, 19, 30, 31].", "startOffset": 124, "endOffset": 139}, {"referenceID": 30, "context": "However, several sampling approaches have been suggested to approximate the Shapley value for specific classes of functions [2, 19, 30, 31].", "startOffset": 124, "endOffset": 139}, {"referenceID": 7, "context": "3 Statistical cost sharing With the sole exception of [8], previous work in cost-sharing critically assumes that the algorithm is given oracle access to C, i.", "startOffset": 54, "endOffset": 57}, {"referenceID": 29, "context": "[30] showed how to approximate the Shapley value with polynomially many queries C(S).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] showed how to instead first learn an approximation to C from the given samples and then compute cost shares for the learned function, but their results hold only for a limited number of games and cost functions C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "As shown in [8], this approach works if C is PAC-learnable, but there exist functions C that are not PAC-learnable and for which a cost allocation that approximately satisfies the core can still be computed.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "We begin by defining three notions of approximate core: the probably stable [8], approximately stable, and probably approximately stable cores.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Given \u03b4, > 0, a cost allocation \u03c8 such that \u2211 i\u2208N \u03c8i = C(N) is in \u2022 the probably stable core [8] if, for all D,", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "[8] showed that several families of functions D have a core that is probably stable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "Theorem 1 ([37], Theorem 6.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "Theorem 2 ([37], Theorem 9.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "Theorem 4 ([37], Theorem 26.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Thus, the distributions we consider in this section are the uniform distribution, and more generally product distributions, which are the standard distributions studied in the learning literature for combinatorial functions [5, 7, 21, 22].", "startOffset": 224, "endOffset": 238}, {"referenceID": 6, "context": "Thus, the distributions we consider in this section are the uniform distribution, and more generally product distributions, which are the standard distributions studied in the learning literature for combinatorial functions [5, 7, 21, 22].", "startOffset": 224, "endOffset": 238}, {"referenceID": 20, "context": "Thus, the distributions we consider in this section are the uniform distribution, and more generally product distributions, which are the standard distributions studied in the learning literature for combinatorial functions [5, 7, 21, 22].", "startOffset": 224, "endOffset": 238}, {"referenceID": 21, "context": "Thus, the distributions we consider in this section are the uniform distribution, and more generally product distributions, which are the standard distributions studied in the learning literature for combinatorial functions [5, 7, 21, 22].", "startOffset": 224, "endOffset": 238}, {"referenceID": 1, "context": "Other sampling methods have previously been suggested ([2, 19, 30, 31]), but the samples (S,C(S)) used in these methods are not i.", "startOffset": 55, "endOffset": 70}, {"referenceID": 18, "context": "Other sampling methods have previously been suggested ([2, 19, 30, 31]), but the samples (S,C(S)) used in these methods are not i.", "startOffset": 55, "endOffset": 70}, {"referenceID": 29, "context": "Other sampling methods have previously been suggested ([2, 19, 30, 31]), but the samples (S,C(S)) used in these methods are not i.", "startOffset": 55, "endOffset": 70}, {"referenceID": 30, "context": "Other sampling methods have previously been suggested ([2, 19, 30, 31]), but the samples (S,C(S)) used in these methods are not i.", "startOffset": 55, "endOffset": 70}, {"referenceID": 26, "context": "1 Submodular functions with bounded curvature We consider submodular functions with bounded curvature, a common assumption in the submodular maximization literature [27, 28, 40, 43].", "startOffset": 165, "endOffset": 181}, {"referenceID": 27, "context": "1 Submodular functions with bounded curvature We consider submodular functions with bounded curvature, a common assumption in the submodular maximization literature [27, 28, 40, 43].", "startOffset": 165, "endOffset": 181}, {"referenceID": 39, "context": "1 Submodular functions with bounded curvature We consider submodular functions with bounded curvature, a common assumption in the submodular maximization literature [27, 28, 40, 43].", "startOffset": 165, "endOffset": 181}, {"referenceID": 42, "context": "1 Submodular functions with bounded curvature We consider submodular functions with bounded curvature, a common assumption in the submodular maximization literature [27, 28, 40, 43].", "startOffset": 165, "endOffset": 181}, {"referenceID": 0, "context": "A monotone submodular function C has curvature \u03ba \u2208 [0, 1] if CN\\{i}(i) \u2265 (1\u2212 \u03ba)C(i) for all i \u2208 N .", "startOffset": 51, "endOffset": 57}, {"referenceID": 2, "context": "Coverage functions are an interesting class of functions because they are learnable from samples over any distribution [3], according to the PMAC learning model [5], which is a generalization of PAC learnability for real valued functions.", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Coverage functions are an interesting class of functions because they are learnable from samples over any distribution [3], according to the PMAC learning model [5], which is a generalization of PAC learnability for real valued functions.", "startOffset": 161, "endOffset": 164}], "year": 2017, "abstractText": "We study the cost sharing problem for cooperative games in situations where the cost function C is not available via oracle queries, but must instead be derived from data, represented as tuples (S,C(S)), for different subsets S of players. We formalize this approach, which we call STATISTICAL COST SHARING, and consider the computation of the core and the Shapley value, when the tuples are drawn from some distribution D. Previous work by Balcan et al. [8] in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions. We expand on their work and give an algorithm that computes such cost shares for any function with a non-empty core. We complement these results by proving an inapproximability lower bound for a weaker relaxation. We then turn our attention to the Shapley value. We first show that when cost functions come from the family of submodular functions with bounded curvature, \u03ba, the Shapley value can be approximated from samples up to a \u221a 1\u2212 \u03ba factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value. We show that these can always be approximated arbitrarily well for general functions over any distribution D. \u2217School of Engineering and Applied Sciences, Harvard University, ericbalkanski@g.harvard.edu. \u2020Google NYC, usyed@google.com. \u2021Google NYC, sergeiv@google.com. 1 ar X iv :1 70 3. 03 11 1v 1 [ cs .G T ] 9 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}