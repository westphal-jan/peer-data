{"id": "1703.05390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting", "abstract": "noisy keyword spotting ( nk kws ) constitutes a particular major specialized component industry of human - mind technology interfaces. currently maximizing for the relative detection accuracy is at a low resolution false alarm ( kg fa ) rate, encoding while minimizing decreases the footprint size, computed latency level and delay complexity are illustrating the feasible goals formulated for kws. towards achieving exceeding them, increasingly we study convolutional temporal recurrent neural networks ( crnns ). inspired by large - genome scale state - of - the - tongue art behavioral speech recognition neural systems, whereas we combine the strengths of cellular convolutional layers respectively and recurrent layers trying to exploit local structure and long - used range architectural context. we initially analyze the additive effect footprint of sensitive architecture - parameters, and propose training for strategies expected to improve performance. with only ~ 7 230k parameters, our extended crnn spectral model visually yields comparatively acceptably low latency, and achieves 97. 41 71 % accuracy at 0. 93 5 kb fa / hour threshold for assuming 5 db for signal - to - noise ratio.", "histories": [["v1", "Wed, 15 Mar 2017 21:20:44 GMT  (308kb)", "http://arxiv.org/abs/1703.05390v1", "Submitted to Interspeech 2017"], ["v2", "Wed, 24 May 2017 00:37:05 GMT  (316kb)", "http://arxiv.org/abs/1703.05390v2", "Submitted to Interspeech 2017"], ["v3", "Tue, 4 Jul 2017 22:49:18 GMT  (302kb)", "http://arxiv.org/abs/1703.05390v3", "Accepted to Interspeech 2017"]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["sercan o arik", "markus kliegl", "rewon child", "joel hestness", "rew gibiansky", "chris fougner", "ryan prenger", "adam coates"], "accepted": false, "id": "1703.05390"}, "pdf": {"name": "1703.05390.pdf", "metadata": {"source": "CRF", "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting", "authors": ["Sercan \u00d6. Ar\u0131k", "Markus Kliegl", "Rewon Child", "Joel Hestness", "Andrew Gibiansky", "Chris Fougner", "Ryan Prenger", "Adam Coates"], "emails": ["sercanarik@baidu.com,", "klieglmarkus@baidu.com"], "sections": [{"heading": "1. Introduction", "text": "Motivated by the most common way humans interact with each other, conversational human-technology interfaces are becoming increasingly popular in numerous applications. High-performance speech-to-text conversion and text-tospeech conversion constitute two important aspects of such interfaces, as most computational algorithms are developed for text inputs and outputs. Another crucial aspect of conversational interfaces is keyword spotting (KWS), to enable transitioning between different computational states based on the voice input provided by the users. KWS systems aim to detect a particular keyword from a continuous stream of audio. As their output determines different states of the device, very high detection accuracy for a very low false alarm (FA) rate is critical to enable satisfactory user experience. Typical applications exist in environments with interference from background audio, reverberation distortion, and the sounds generated by the speaker of the device in which the KWS is embedded. A KWS system should demonstrate robust performance in these wide range of situations. Furthermore, the computational complexity and model size are important concerns for KWS systems, as they are typically embedded in consumer devices with limited memory and computational resources, such as smartphones or smart-home sensors.\nThere are already millions of devices with embedded KWS systems and corresponding techniques have been investigated in the literature. Traditional approaches for KWS are based on Hidden Markov Models with sequence search algorithms [1]. More recently, with the advances in deep learning and increase in the amount of available data, state-of-\nthe-art KWS has been replaced by deep learning-based approaches due to their superior performance [2]. Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6]. A potential drawback of DNNs is that they ignore the structure and context of the input, and an audio input can have strong dependencies in time or frequency domains. With the goal of exploiting such local connectivity patterns by shared weights, Convolutional Neural Networks (CNNs) were explored for KWS [7,8]. A potential drawback of CNNs is that they cannot model the context over the entire frame without recurrent layers. Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss. Yet, a high accuracy at a low FA rate could not be obtained, given the ambitious targets of the applications of such systems. Similar to DNNs, a potential limitation of RNNs is that the modeling is done on the input features, without learning the structure between successive time and frequency steps. Recently, [11] proposed a Convolutional Recurrent Neural Network (CRNN) architecture with CTC loss. However, despite the large model size, similar to RNNs, a high accuracy at a low FA rate could not be obtained.\nIn this work, we focus on developing production-quality KWS systems using CRNNs with CE loss for a small-footprint model. Our goal is to combine the strengths of CNNs and RNNs, with additional strategies applied during training to improve the overall performance, while keeping a smallfootprint size. The rest of the paper is organized as follows. In Section 2, we describe the end-to-end architecture and training methodologies for small-footprint KWS. In Section 3, we explain the experiments and the corresponding results. In Section 4, we present our conclusions."}, {"heading": "2. Small-footprint keyword spotting", "text": ""}, {"heading": "2.1. End-to-end architecture", "text": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14]. To adapt these architectures for small-footprint KWS, the model size needs to be shrunk two to three orders of magnitude. We will analyze the impact of different parameters on performance while shrinking the size of the model.\nFig. 1 shows the CRNN architecture with the corresponding parameters. The raw time-domain inputs are converted to per-channel energy normalized (PCEN) mel spectrograms [8], for succinct representation and efficient\ntraining. (Other input representations we experimented with yielded worse performance for model architectures of comparable size.) The 2-D PCEN features are given as inputs to the convolutional layer, which employ 2-D filtering along both the time and frequency dimensions. The outputs of the convolutional layer are fed to bidirectional recurrent layers, which might include gated recurrent units (GRUs) [15] or long short-term memory (LSTM) units [16]. Outputs of the recurrent layers are given to the fully connected (FC) layer. Lastly, softmax decoding is applied over two neurons, to obtain a corresponding scalar score. We use rectified linear units as activation function in all layers."}, {"heading": "2.2. End-to-end training", "text": "In speech recognition, large-scale architectures with recurrent layers typically use variants of CTC loss to decode the most probable output label. Aside from the modeling limitations due to conditional independence assumptions of targets, CTC loss has a prohibitively high computational complexity and typically yields good performance only when the model size is sufficiently large to learn from a large dataset. As we focus on small-footprint architectures, the loss function that is optimized during the training is chosen as the CE loss for the estimated and target binary labels, indicating whether a frame corresponds to a keyword or not.\nTo train with a CE loss, unlike CTC, precise alignment of the training samples is important. We use Deep Speech 2 [14], a large-scale speech recognition model, to obtain the estimated probability distributions of keyword characters \ud835\udc50\" (1 \u2264 \ud835\udc58 \u2264 \ud835\udc3e) for each time instance. As the CTC decoding yields peaked distributions, we further smooth the output over time and obtain smoothed character occupancy scores \ud835\udc5d \ud835\udc50\", \ud835\udc61 . We then obtain the beginning and end times of the keywords using the simple heuristic algorithm shown below. An extra short padding is added while chopping the keywords to cover edge cases. The accuracy of alignments obtained were significantly beyond the time scale of human perception."}, {"heading": "3. Experiments and Results", "text": ""}, {"heading": "3.1. Data and training", "text": "We develop our KWS system for the keyword \u201cTalkType\u201d (which can be pronounced as a single word or two words). We choose a frame length of T = 1.5 seconds, which is sufficiently long to capture a reasonable pronunciation of \u201cTalkType\u201d. Using a sampling rate of 16 kHz, each frame contains 24k raw time-domain samples. Corresponding PCEN 25 ms mel spectrograms are obtained for 10 ms stride and 40 channels, yielding an input dimensionality of 40 \u00b4 151. The entire data set consists of ~16k different samples, collected from more than 5k speakers. The dataset is split into training, development and test sets with 6-1-1 ratio. Training samples are augmented by applying additive noise, with a power determined by a signal-to-noise ratio (SNR) value sampled from [-5,15] dB interval. The additive noise is sampled from a data set of representative background noise and speech audio, with a total length exceeding 300 hours. To provide robustness against alignment errors, training samples are also augmented by introducing random timing jitter.\nWe use the ADAM optimization algorithm for training [17], with a batch size of 64. The learning rate is initially chosen as 0.001, and dropped to 0.0003 in the middle of training before convergence.\nOur evaluation considers a streaming scenario. The metrics we focus on are the false rejection rate (FRR) and false alarms (FA) per hour, typically fixing the latter at a desired value such as 1 FA/hr [7]. Noise is added to the development and test sets, with a magnitude depending on the SNR value. We note that the collected samples are already noisy so the actual SNR is lower if defined precisely as the ratio of powers of the information-bearing signal and the noise. Similar to our augmentation of the training sets, negative samples and noise datasets are sampled from representative background noise and speech samples."}, {"heading": "3.2. Impact of the model architecture", "text": "Table 1 shows the performance of various CRNN architectures for the development set with 5 dB SNR. We note that all models were trained until convergence, even though it requires very different number of epochs. We observe the general trend that the larger model size typically yields better performance. Increasing the number of convolution filters or increasing the number of recurrent hidden units are the two effective approaches to improve the performance. Increasing the number of recurrent layers has a limited impact, and GRU is preferred over LSTM as a better performance can be obtained for a lower complexity.\nIt is desired to limit the model size given the resource constraints for inference latency, memory, and power consumption. Following [7], we choose the size limit as 250k (which is more than 6 times smaller than the architecture with CTC loss in [11]). For the rest of the paper, the default architecture is the set of parameters highlighted in bold, which also corresponds to a fairly optimal point given the model size vs. performance trade-off.\nWe compare the performance with a CNN architecture based on [7]. Given the discrepancy in input dimensionality and training data, we reoptimize the model hyperparameters for the best performance while upper-bounding the number of parameters to 250k for a fair comparison. For the same development set with 5 dB SNR, the best CNN architecture achieves 4.31% FRR at 1 FA/hour and 5.73% FRR at 0.5 FA/hour. Both metrics are ~51% higher compared to the FRR values of the chosen CRNN model with 229k parameters. Interestingly, the performance gap is lower for higher SNR values. We elaborate on this in Section 3.4.\nThe inference computational complexity of the chosen CRNN-based KWS model with 229k parameters is roughly ~30M floating point operations (FLOPs) when implemented on processors of modern consumer devices (without special functions to implement nonlinear operations). Even when implemented on modern smartphones without any approximations and special function units, our KWS model can achieve an inference time much faster than the time scale for reactive time for humans with auditory stimuli, which is\n~280 ms [18]. Hence, seamless real-time operation can easily be enabled for applications with conversational interfaces."}, {"heading": "3.3. Impact of the amount of training data", "text": "Given the representation capacity limit imposed by the architecture size, increasing the amount of positive samples in the training data has a limited effect on the performance. Fig. 2 shows the FRR at 0.5 FA/hour (for the test set with 5 dB SNR) vs. the number of unique \u201cTalkType\u201d samples used\nwhile training. Saturation of performance occurs faster than applications with similar type of data but with large-scale models, e.g. [14].\nBesides increasing the amount of the positive samples, we observe performance improvement by training with negative samples obtained by hard mining. We mine negative samples, by using the pre-converged model on a very large public videos dataset (that are not used in training, development, or test sets). Then, training is continued using the mined negative samples until convergence. As shown in Fig. 2, hard negative mining yields decrease in FRR for the test set."}, {"heading": "3.4. Noise robustness", "text": "For the test set with various SNR values, Fig. 3 shows the FRR vs. FA per hour. For higher SNR, lower FRR is obtained, and stable performance starts for a lower FA rate. Note that the SNR values (in dB) of the augmented training samples are sampled from a distribution with a mean of 5 dB, and deterioration in performance is observed beyond this value. Performance for lower SNR values can be improved by augmenting with lower SNR, but this comes at the expense of decreased performance for higher SNR, which can be attributed to the limited learning capacity of the model.\nWe observe the benefit of recurrent layers especially for lower SNR values. The performance gap of CRNN architectures with CNN architectures (adapted from [7] as explained in Section 3.1) reduces as the SNR increases. We hypothesize that the recurrent layers are better able to adapt to the noise signature of individual samples, since each layer processes information from the entire frame. CNNs, in contrast, require wide filters and/or great depth for this level of information propagation."}, {"heading": "3.5. Far-field robustness", "text": "Our dataset already consists of samples recorded at varying distance values, which should be representative for most applications such as smartphone KWS systems. Yet, some applications, such as smart-home KWS systems, require high performance at far-field conditions.\nFig. 4 shows performance degradation with the additional distance. Far-field test sets are constructed by augmenting the original test set with impulse responses corresponding to a variety of configurations at the given distance (considering different values for degrees of arrival etc.). Significant deterioration in performance is observed especially in conjunction with higher noise, as also explained in [19]. To provide robustness against this deterioration, we consider training with far-field-augmented training samples, using a variety of impulse responses that are different than the ones in the test set. This augmentation achieves significantly less degradation in performance for farther distances. Yet, it yields a worse performance for the original data set. This can also be attributed to the limited learning capacity of the model."}, {"heading": "4. Conclusions", "text": "We studied CRNNs for small-footprint KWS systems. We presented the trade-off between model size and performance, and demonstrated the optimal choice of parameters given the tradeoff. The capacity limitation of the model has various implications. Performance gain is limited by merely increasing the number of positive samples, yet hard negative mining improves the performance. Training sets should be carefully chosen to reflect the application environment, such as the noise level or far-field conditions. Overall, at 0.5 FA/hour (which is an acceptable value from a user perspective), our model achieves 97.71%, 98.71% and 99.3% accuracy for the test set with 5 dB, 10 dB and 20 dB SNR values, respectively. Our numerical performance results may seem better than other KWS models in the literature. However, a direct comparison is not meaningful because of the difference in the datasets and the actual keywords, i.e. the inference task. Given that human performance is excellent in the KWS task, we still believe that there is further room for improvement in terms of performance."}, {"heading": "5. Acknowledgements", "text": "Discussions with Andrew Ng, Sanjeev Satheesh, Jiaji Huang, Jue Sun, and Bing Jiang are gratefully acknowledged. We thank Hui Song for the impulse response measurements used for far-field augmentation."}, {"heading": "6. References", "text": "[1] J.R. Rohlicek, W. Russell, S. Roukos, and H. Gish, \u201cContinuous hidden Markov modeling for speaker-independent wordspotting,\u201d in IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 1990, pp. 627\u2013630. [2] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword spotting using deep neural networks,\u201d in Proceedings International Conference on Acoustics, Speech, and Signal Processing, 2014, pp. 4087-4091. [3] G. Tucker, M. Wu, M. Sun, S. Panchapagesan, G. Fu, and S. Vitaladevuni, \u201cModel compression applied to small-footprint keyword spotting,\u201d in Proceedings of Interspeech, 2016, pp. 1393-1397 [4] Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar, \u201cStructured transforms for small-footprint deep learning,\u201d in Neural Information Processing Systems, 2015, pp. 3088-3096. [5] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N. Sainath, \u201cAutomatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks,\u201d in IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. 4704\u20134708. [6] S. Panchapagesan, M. Sun, A. Khare, S. Matsoukas, A. Mandal, B. Hoffmeister, and S. Vitaladevuni, \u201cMulti-task learning and weighted cross-entropy for dnn-based keyword spotting,\u201d in Proceedings of Interspeech, 2016, pp. 760-764. [7] T. N. Sainath and C. Parada, \u201cConvolutional neural networks for small-footprint keyword spotting,\u201d in Proceedings of Interspeech, 2015, pp. 1478-1482 [8] Y. Wang, P. Getreuer, T. Hughes, R. F. Lyon, and R. A. Saurous, \u201cTrainable frontend for robust and far-field keyword spotting,\u201d arXiv preprint, arXiv:1607.05666, 2016. [9] K. Hwang, M. Lee, and W. Sung, \u201cOnline keyword spotting with a character-level recurrent neural network,\u201d arXiv preprint arXiv:1512.08903, 2015. [10] S. Fernandez, A. Graves, and J. Schmidhuber, \u201cAn application of recurrent neural networks to discriminative keyword spotting,\u201d in Artificial Neural Networks. Springer, 2007, pp. 220\u2013229. [11] C. Lengerich, and A. Hannun, \u201cAn end-to-end architecture for keyword spotting and voice activity detection,\u201d arXiv preprint arXiv:1611.09405, 2016. [12] L. Deng, and J. C. Platt, \u201cEnsemble deep learning for speech recognition,\u201d in Proceedings of Interspeech, 2014. [13] T. N. Sainath, O. Vinyals, A. Senior A, and H. Sak, \u201cConvolutional, long short-term memory, fully connected deep neural networks,\u201d in IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 4580-4584. [14] D. Amodei et al. \u201cDeep Speech 2: End-to-end speech recognition in English and Mandarin.,\u201d arXiv preprint arXiv:1512.02595, 2015. [15] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014. [16] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997. [17] D. Kingma, and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014. [18] J. Shelton, and G. P. Kumar GP, \u201cComparison between auditory and visual simple reaction times,\u201d Neuroscience and medicine, vol. 1 no. 1, pp. 30-32, 2010. [19] K. Kumatani et al. \u201cMicrophone array processing for distant speech recognition: Towards real-world deployment,\u201d in IEEE AsiaPacific Signal & Information Processing Association Annual Summit and Conference, 2012."}], "references": [{"title": "Continuous hidden Markov modeling for speaker-independent wordspotting", "author": ["J.R. Rohlicek", "W. Russell", "S. Roukos", "H. Gish"], "venue": "IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 1990, pp. 627\u2013630.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "Proceedings International Conference on Acoustics, Speech, and Signal Processing, 2014, pp. 4087-4091.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Model compression applied to small-footprint keyword spotting", "author": ["G. Tucker", "M. Wu", "M. Sun", "S. Panchapagesan", "G. Fu", "S. Vitaladevuni"], "venue": "Proceedings of Interspeech, 2016, pp. 1393-1397", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara N. Sainath", "Sanjiv Kumar"], "venue": "Neural Information Processing Systems, 2015, pp. 3088-3096.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks", "author": ["R. Prabhavalkar", "R. Alvarez", "C. Parada", "P. Nakkiran", "T.N. Sainath"], "venue": "IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. 4704\u20134708.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "Multi-task learning and weighted cross-entropy for dnn-based keyword spotting", "author": ["S. Panchapagesan", "M. Sun", "A. Khare", "S. Matsoukas", "A. Mandal", "B. Hoffmeister", "S. Vitaladevuni"], "venue": "Proceedings of Interspeech, 2016, pp. 760-764.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural networks for small-footprint keyword spotting", "author": ["T.N. Sainath", "C. Parada"], "venue": "Proceedings of Interspeech, 2015, pp. 1478-1482", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Trainable frontend for robust and far-field keyword spotting", "author": ["Y. Wang", "P. Getreuer", "T. Hughes", "R.F. Lyon", "R.A. Saurous"], "venue": "arXiv preprint, arXiv:1607.05666, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Online keyword spotting with a character-level recurrent neural network", "author": ["K. Hwang", "M. Lee", "W. Sung"], "venue": "arXiv preprint arXiv:1512.08903, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fernandez", "A. Graves", "J. Schmidhuber"], "venue": "Artificial Neural Networks. Springer, 2007, pp. 220\u2013229.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "An end-to-end architecture for keyword spotting and voice activity detection", "author": ["C. Lengerich", "A. Hannun"], "venue": "arXiv preprint arXiv:1611.09405, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensemble deep learning for speech recognition", "author": ["L. Deng", "J.C. Platt"], "venue": "Proceedings of Interspeech, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior A", "H. Sak"], "venue": "IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 4580-4584.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison between auditory and visual simple reaction times", "author": ["J. Shelton", "G.P. Kumar GP"], "venue": "Neuroscience and medicine, vol. 1 no. 1, pp. 30-32, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Microphone array processing for distant speech recognition: Towards real-world deployment", "author": ["K. Kumatani"], "venue": "IEEE Asia- Pacific Signal & Information Processing Association Annual Summit and Conference, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Traditional approaches for KWS are based on Hidden Markov Models with sequence search algorithms [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "More recently, with the advances in deep learning and increase in the amount of available data, state-ofthe-art KWS has been replaced by deep learning-based approaches due to their superior performance [2].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 110, "endOffset": 115}, {"referenceID": 3, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 110, "endOffset": 115}, {"referenceID": 4, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 151, "endOffset": 156}, {"referenceID": 5, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 151, "endOffset": 156}, {"referenceID": 6, "context": "With the goal of exploiting such local connectivity patterns by shared weights, Convolutional Neural Networks (CNNs) were explored for KWS [7,8].", "startOffset": 139, "endOffset": 144}, {"referenceID": 7, "context": "With the goal of exploiting such local connectivity patterns by shared weights, Convolutional Neural Networks (CNNs) were explored for KWS [7,8].", "startOffset": 139, "endOffset": 144}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 113, "endOffset": 119}, {"referenceID": 9, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 113, "endOffset": 119}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 2, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 3, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 4, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 5, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 10, "context": "Recently, [11] proposed a Convolutional Recurrent Neural Network (CRNN) architecture with CTC loss.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 12, "context": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 7, "context": "converted to per-channel energy normalized (PCEN) mel spectrograms [8], for succinct representation and efficient", "startOffset": 67, "endOffset": 70}, {"referenceID": 14, "context": "The outputs of the convolutional layer are fed to bidirectional recurrent layers, which might include gated recurrent units (GRUs) [15] or long short-term memory (LSTM) units [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "The outputs of the convolutional layer are fed to bidirectional recurrent layers, which might include gated recurrent units (GRUs) [15] or long short-term memory (LSTM) units [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 13, "context": "We use Deep Speech 2 [14], a large-scale speech recognition model, to obtain the estimated probability distributions of keyword characters c\" (1 \u2264 k \u2264 K) for each time instance.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Training samples are augmented by applying additive noise, with a power determined by a signal-to-noise ratio (SNR) value sampled from [-5,15] dB interval.", "startOffset": 135, "endOffset": 142}, {"referenceID": 16, "context": "We use the ADAM optimization algorithm for training [17], with a batch size of 64.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "The metrics we focus on are the false rejection rate (FRR) and false alarms (FA) per hour, typically fixing the latter at a desired value such as 1 FA/hr [7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Following [7], we choose the size limit as 250k (which is more than 6 times smaller than the architecture with CTC loss in [11]).", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "Following [7], we choose the size limit as 250k (which is more than 6 times smaller than the architecture with CTC loss in [11]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "We compare the performance with a CNN architecture based on [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 17, "context": "Even when implemented on modern smartphones without any approximations and special function units, our KWS model can achieve an inference time much faster than the time scale for reactive time for humans with auditory stimuli, which is ~280 ms [18].", "startOffset": 244, "endOffset": 248}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The performance gap of CRNN architectures with CNN architectures (adapted from [7] as explained in Section 3.", "startOffset": 79, "endOffset": 82}, {"referenceID": 18, "context": "Significant deterioration in performance is observed especially in conjunction with higher noise, as also explained in [19].", "startOffset": 119, "endOffset": 123}], "year": 2017, "abstractText": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-ofthe-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.", "creator": "Word"}}}