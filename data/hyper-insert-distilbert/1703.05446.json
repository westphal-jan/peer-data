{"id": "1703.05446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing", "abstract": "a human component parsing paradigm has recently literally attracted a lot also of ongoing research interests due to strengthening its huge application potentials. however presently existing datasets have limited as number of typical images and annotations, and lack the variety of human appearances and display the heightened coverage potential of challenging structural cases utilized in unconstrained environment. in delivering this paper, we introduce a new detailed benchmark \" look closely into person ( lip ) \" taxonomy that makes out a significant advance description in terms of scalability, diversity strength and difficulty, a leading contribution that we frankly feel valuable is crucial input for future developments in human - centric scientific analysis. this especially comprehensive 2d dataset furthermore contains out over 50, 000 elaborately annotated virtual images with 19 semantic part labels, which are repeatedly captured geographically from precisely a wider range consisting of varying viewpoints, data occlusions and background layer complexity. also given these particularly rich web annotations we today perform detailed graphic analyses of the worldwide leading human parsing support approaches, gaining insights into analyzing the overwhelming success consequences and failures required of these online methods. arguably furthermore, perhaps in contrast to the existing efforts concentrate on safely improving the feature discriminative cognitive capability, we solve human resource parsing by exploring a virtually novel self - supervised structure - only sensitive flexible learning approach, instead which imposes human skin pose structures merged into parsing required results exclusively without resorting further to extra realistic supervision ( i. e., no need encountered for specifically labeling human joints used in simulated model training ). our self - bound supervised responsive learning framework concept can confidently be injected into any advanced modeling neural networks intending to completely help incorporate our rich high - level empirical knowledge regarding functional human pin joints from a mainstream global perspective online and quickly improve accurately the parsing results. extensive evaluations on presenting our lip and displaying the public data pascal - encoded person - view part dataset demonstrate the superiority of our method.", "histories": [["v1", "Thu, 16 Mar 2017 01:14:36 GMT  (2870kb,D)", "https://arxiv.org/abs/1703.05446v1", "Accepted to appear in CVPR 2017"], ["v2", "Fri, 28 Jul 2017 01:41:39 GMT  (2869kb,D)", "http://arxiv.org/abs/1703.05446v2", "Accepted to appear in CVPR 2017"]], "COMMENTS": "Accepted to appear in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ke gong", "xiaodan liang", "dongyu zhang", "xiaohui shen", "liang lin"], "accepted": false, "id": "1703.05446"}, "pdf": {"name": "1703.05446.pdf", "metadata": {"source": "CRF", "title": "Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing", "authors": ["Ke Gong", "Xiaodan Liang", "Dongyu Zhang", "Xiaohui Shen", "Liang Lin"], "emails": ["gongk3@mail2.sysu.edu.cn,", "xiaodan1@cs.cmu.edu,", "zhangdy27@mail.sysu.edu.cn,", "linlng@mail.sysu.edu.cn,", "xshen@adobe.com"], "sections": [{"heading": "1. Introduction", "text": "Human parsing aims to segment a human image into multiple parts with fine-grained semantics and provide more detailed understanding of image contents. It can stimulate\n\u2217The first two authors contribute equally to this paper. Corresponding author is Dongyu Zhang. This work was supported by the National Natural Science Foundation of China under Grant 61401125 and 61671182.\n1The dataset is available at http://hcp.sysu.edu.cn/lip\nmany higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].\nRecently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16]. Nevertheless, as demonstrated in many other problems such as object detection [15] and semantic segmentation [37], the performance of those CNN-based approaches heavily rely on the availability of annotated images for training. In order to train a human parsing network with potentially practical value in real-word applications, it is highly desired to have a large-scale dataset composed of representative instances with varied clothing appearances, strong articulation, partial (self-)occlusions, truncation at image borders, diverse viewpoints and background clutters. Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.g., upright) [6], these datasets are limited in their coverage and scalability, as shown in Fig. 1. The largest public human parsing dataset [18] so far only contains 17,000 fashion images while others only include thousands of images.\nMoreover, to the best of our knowledge, no attempt has been made to establish a standard representative benchmark aiming to cover a wide pallet of challenges for the human parsing task. The existing datasets did not provide an evaluation server with a secret test set to avoid potential dataset over-fitting, which hinders further development on this topic. Therefore we propose a new benchmark \u201cLook into Person (LIP)\u201d and a public server for automatically reporting evaluation results. Our benchmark significantly advances the state-of-the-arts in terms of appearance variability and complexity, which includes 50,462 human images with pixel-wise annotations of 19 semantic parts.\nThe recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks. To capture rich structure information, they combine CNNs and the graphical models (e.g., Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29]. However, evaluated\n1\nar X\niv :1\n70 3.\n05 44\n6v 2\n[ cs\n.C V\n] 2\n8 Ju\nl 2 01\n7\non the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory. Without imposing human body structure priors, these general approaches based on bottom-up appearance information sometimes tend to produce unreasonable results (e.g., right arm connected with left shoulder), as shown in Fig. 2. The human body structural information has been previously well-explored in the human pose estimation [34, 7] where dense joint annotations are provided. However, since human parsing requires more extensive and detailed prediction than pose estimation, it is difficult to directly utilize joint-based pose estimation models in pixel-wise prediction to incorporate the complex structure constraints. In order to explicitly enforce the produced parsing results to be semantically consistent with the human pose / joint structures, we propose a novel structure-sensitive learning approach for human parsing. In addition to using the traditional pixel-wise part annotations as the supervision, we introduce a structure-sensitive loss to evaluate the quality of predicted parsing results from a joint structure perspective. That means a satisfactory parsing result should be able to preserve a reasonable joint structure (e.g., the spatial layouts of human parts). Note that annotating both pixel-wise labeling map and pose joints is expensive and may cause ambiguities. Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised Structure-\nsensitive Learning (SSL). Our contributions are summarized in the following three aspects. 1) We propose a new large-scale benchmark and an evaluation server to advance the human parsing research, in which 50,462 images with pixel-wise annotations on 19 semantic part labels are provided. 2) By experimenting on our benchmark, we present the detailed analyses about the existing human parsing approaches to gain some insights into the success and failures of these approaches. 3) We propose a novel self-supervised structure-sensitive learning framework for human parsing, which is capable of explicitly enforcing the consistency between the parsing results and the human joint structures. Our proposed framework significantly surpasses the previous methods on both the existing PASCAL-Person-Part dataset [6] and our new LIP dataset."}, {"heading": "1.1. Related Work", "text": "Human parsing datasets: The commonly used publicly available datasets for human parsing are summarized in Table. 1. The previous datasets were labeled with limited\nnumber of images or categories. Containing 50,462 images annotated with 20 categories, our LIP dataset is the largest and most comprehensive human parsing dataset to date. Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.\nHuman parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5]. For example, Liang et al. [18] proposed a novel Co-CNN architecture which integrates multiple levels of image contexts into a unified nerwork. Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23]. To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30]. Chen et al. [5] proposed an attention mechanism that learns to weight the multi-scale features at each pixel location. Some previous works [8, 31] explored human pose information to guide human parsing by generating \u201cpose-guided\u201d part segment proposals. To leverage human joint structure more effortlessly and efficiently, the focus in our approach is nevertheless a new self-supervised structure-sensitive learning approach, which actually can be embedded in any networks."}, {"heading": "2. Look into Person Benchmark", "text": "In this section we introduce our new \u201cLook into Person (LIP)\u201d, a new large-scale dataset focusing on semantic understanding of human bodies which has several appealing properties. First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18]. Second, LIP is annotated with elaborated pixel-wise annotations with 19 semantic human part labels and one background label. Third, the images collected from the real-world scenarios contain people appearing with challenging poses and viewpoints, heavy occlusions, various appearances and in wide range of resolutions. Furthermore, the background of images in the LIP dataset is also more complex and diverse than the one in\nprevious datasets. Some examples are showed in Fig. 1. With the LIP dataset, we propose a new benchmark suite for human parsing together with a standard evaluation server where the test set will be kept secret to avoid overfitting."}, {"heading": "2.1. Image Annotation", "text": "The images in the LIP dataset are cropped person instances from Microsoft COCO [19] training and validation sets. We defined 19 human parts or clothes labels for annotation, which are hat, hair, sunglasses, upper-clothes, dress, coat, socks, pants, gloves, scarf, skirt, jumpsuits, face, right arm, left arm, right leg, left leg, right shoe, left shoe, and in addition to a background label. We implemented an annotation tool and generated multi-scale superpixels of images based on [2] to speed up the annotation."}, {"heading": "2.2. Dataset splits", "text": "In total, there are 50,462 images in the LIP dataset including 19,081 full-body images, 13,672 upper-body images, 403 lower-body images, 3,386 head-missed images, 2,778 back-view images and 21,028 images with occlusions. We split the images into separate training, validation and test sets. Following random selection, we arrive at a unique split consisting of 30,462 training and 10,000 validation images with publicly available annotations, as well as 10,000 test images with annotations withheld for benchmarking purpose."}, {"heading": "2.3. Dataset statistics", "text": "In this section we analyse the images and categories in the LIP dataset in detail. In general, face, arms and legs\nare the most remarkable parts of a human body. However, human parsing aims to analyse every detailed regions of a person including different body parts as well as different categories of clothes. We therefore define 6 body parts and 13 clothes categories. Among these 6 body parts, we divide arms and legs into left side and right side for more precise analysis, which also increases the difficulty of the task. As for clothes classes, we have not only common clothes like upper clothes, pants and shoes, but also infrequent categories such as skirts and jumpsuits. Furthermore, small scale accessories like sunglasses, gloves and socks are also taken into account. The numbers of images for each semantic part label are presented in Fig. 3\nThe images in the LIP dataset contain diverse human appearances, viewpoints and occlusions. Additionally, more than half of the images suffer occlusions of different degrees. Occlusion is considered occurred if any of the 19 semantic parts appears in the image but is occluded or invisible. In more challenging cases, the images contain person instances in a back-view, which gives rise to more ambiguity of left and right spatial layouts. The numbers of images of different appearance (i.e. occlusion, full-body, upper-body, head-missed, back-view and lower-body) are summarized in Fig. 4."}, {"heading": "3. Empirical study of state-of-the-arts", "text": "In this section we analyse the performance of leading human parsing or semantic object segmentation approaches on our benchmark. We take advantage of our rich annotations and conduct a detailed analysis of various factors influencing the results, such as appearance, foreshortening and viewpoints. The goal of this analysis is to evaluate the robustness of the current approaches in various challenges for human parsing, and identify the existing limitations to stimulate further research advances.\nIn our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image seg-\nmentations in different ways and have completely available codes. For a fair comparison, we train each method on our LIP training set for 30 epochs and evaluate on the validation set and the test set. For DeepLabV2, we use the VGG-16 model without dense CRFs. Following [5, 30], we use the standard intersection over union (IoU) criterion and pixelwise accuracy for evaluation."}, {"heading": "3.1. Overall performance evaluation", "text": "We begin our analysis by reporting the overall human parsing performance of each approach and summarize the results in Table. 2 and Table. 3. On the LIP validation set, among the four approaches, Attention [5] achieves the best result of 54.39% mean accuracy, benefited from the attention model that softly weights the multi-scale features. For mean IoU, Attention [5] performs best with 42.92%, while both FCN-8s [22] (28.29%) and SegNet [3] (18.17%) perform significantly worse. Similar performance is observed on the LIP test set. The interesting outcome of this comparison is that the achieved performance is substantially lower than the current best results on other segmentation benchmark such as PASCAL VOC [11]. This suggests that detailed human parsing due to the small parts and diverse finegrained labels, is more challenging than object-level segmentation, which deserves more attention in the future."}, {"heading": "3.2. Performance evaluation under different challenges", "text": "We further analyse the performance of each approach with respect to the following five challenging factors: occlusion, full-body, upper-body, head-missed and back-view (see Fig. 5). We evaluate the above four approaches on the LIP validation set which contains 4,277 images with occlusions, 5,452 full-body images, 793 upper-body images, 112 head-missed images and 661 back-view images. As expected, the performance varies when affected by different factors. Back-view is clearly the most challenging case. For example, the IoU of Attention [5] drops from 42.92%\nto 33.50%. The second most influential factor is the appearance of head. The scores of all approaches are much lower on head-missed images than the average score on the whole set. The performance also suffers a lot from occlusion. And the results of full-body images are the closest to the average level. By contrast, upper-body is relatively the easiest case, where fewer semantic parts are present and the part regions are usually larger. From the results, we can draw a conclusion that head(or face) is an important cue for the existing human parsing approaches. The probability of ambiguous results will increase if the head part disappears in the images or in the back-view. Moreover, the parts or clothes on the lower-body are more difficult than the ones on the upper-body because of the existence of small labels such as shoes or socks. In this case, body joint structure can play an effective role in guiding human parsing."}, {"heading": "3.3. Per-class performance evaluation", "text": "In order to discuss and analyse each of the 20 labels in the LIP dataset in more detail, we further report the performance of per-class IoU on the LIP validation set, shown in Table. 4. We observe that the results with respect to labels with larger regions like face, upperclothes, coats and pants are much better than the ones on the small-region labels such as sunglasses, scarf and skirt. Attention [5] and DeepLabV2 [4] perform better on small labels thanks to the utilization of multi-scale features."}, {"heading": "3.4. Visualization comparison", "text": "The qualitative comparisons of four approaches on our LIP validation set are visualized in Fig. 8. We display example parsing results of the five challenging factors scenarios. For the upper-body image(a) with slight occlusion, four approaches perform well with fewer errors. For the back-view image(b), all the four methods mistakenly label the right arm as left arm. The worst results appear when it comes to the head-missed image(c). SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes. Furthermore, severe occlusion(d) also affects the performance a lot. Full-body is less challenging but the small objects in a full-body image(e) like shoes are also hard to be predicted precisely. Moreover, observed from (c) and (d), some of the results are unreasonable from the perspective of human body configuration(e.g. two shoes in one foot), because the existing approaches lack the consideration of body structures. In summary, human parsing is\nmore difficult than the general object segmentation. Particularly, human body structures should be paid more attention to strengthen the ability to predict human parts and clothes with more reasonable configurations. As a result, we consider connecting human parsing results and body joint structure to find out a better approach for human parsing."}, {"heading": "4. Self-supervised Structure-sensitive Learning", "text": ""}, {"heading": "4.1. Overview", "text": "As previously mentioned, a major limitation of the existing human parsing approaches is the lack of consideration of human body configuration, which is mainly investigated in the human pose estimation problem. The human parsing and pose estimation aim to label each image with different granularities, that is, pixel-wise semantic labeling versus joint-wise structure prediction. The pixel-wise labeling can address more detailed information while joint-wise structure provides more high-level structure. However, the results of state-of-the-art pose estimation models [34, 7] still have many errors. The predicted joints do not have high enough quality to guide human parsing compared with the joints extracted from parsing annotations. Moreover, the joints in pose estimation are not aligned with parsing annotations. For example, the arms are labeled as arms for parsing annotations only if they are not covered by any clothes, while the pose annotations are independent with clothes. To address these issues, in this work, we investigate how to leverage informative high-level structure cues to guide pixel-wise prediction. We propose a novel self-supervised structure-sensitive learning for human parsing, which introduces a self-supervised structure-sensitive loss to evaluate the quality of predicted parsing results from a joint structure perspective, as illustrated in Fig. 6.\nSpecifically, in addition to using the traditional pixelwise annotations as the supervision, we generate the approximated human joints directly from the parsing annotations which can also guide human parsing training. In order to explicitly enforce the produced parsing results semantically consistent with the human joint structures, we treat the joint structure loss as a weight of segmentation loss which becomes our structure-sensitive loss."}, {"heading": "4.2. Self-supervised Structure-sensitive Loss", "text": "Generally for the human parsing task, no other extensive information is provided besides the pixel-wise annotations. It means instead of using augmentative information,\nwe have to find a structure-sensitive supervision from the parsing annotations. As the human parsing results are semantic parts with pixel-level labels, we try to explore pose information contained in human parsing results. We define 9 joints to construct a pose structure, which are the centers of regions of head, upper body, lower body, left arm, right arm, left leg, right leg, left shoe and right shoe. The region of head are generated by merging parsing labels of hat, hair, sunglasses and face. Similarly, upper-clothes, coat and scarf are merged to be upper body, pants and skirt for lower body. The rest regions can also be obtained by corresponding labels. Some examples of generated human joints for different humans are shown in Fig. 7. Following [25], for each parsing result and corresponding ground truth, we compute the center points of regions to obtain joints represented as heatmaps for training more smoothly. Then we use Euclidean metric to evaluate the quality of the generated joint structures, which also reflect the structure consistency between the predicted parsing results and the ground truth. Finally, the pixel-wise segmentation loss is weighted by the joint structure loss, which becomes our structure-sensitive loss. Consequently the overall human parsing networks become self-supervised with the structure-sensitive loss.\nFormally, given an image I , we define a list of joints configurations CPI = {c p i |i \u2208 [1, N ]}, where c p i is the heatmap of i-th joint computed according to the parsing result map. Similarly, CGTI = {c gt i |i \u2208 [1, N ]}, which is obtained from corresponding parsing ground truth. Here N is a variate decided by the human bodies in the input images which equals to 9 for a full-body image. For the joints missed in the image, we simply replace the heatmaps with maps filled with zeros. The joint structure loss is the Euclidean (L2) loss,\ncalculated as:\nLJoint = 1\n2N N\u2211 i=1 \u2016cpi \u2212 c gt i \u2016 2 2 (1)\nThe final structure-sensitive loss, denoted as Lstructure, is the combination of the joint structure loss and the parsing segmentation loss, calculated as:\nLStructure = LJoint \u00b7 LParsing (2)\nwhere LParsing is the pixel-wise softmax loss calculated based on the parsing annotations.\nWe phrase our learning framework \u201cself-supervised\u201d as this above structure-sensitive loss can be generated from existing parsing results without any extra information. Our self-supervised learning framework thus has excellent adaptability and extensibility which can be injected into any advanced networks to help incorporate rich high-level knowledge about human joints from a global perspective."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Experimental Settings", "text": "Dataset: We evaluate the performance of our selfsupervised structure-sensitive learning method on human parsing task on two challenging datasets. One is the public PASCAL-Person-part dataset with 1,716 images for training and 1,817 for testing, which pays attention to the human part segmentation annotated by [6]. Following [5, 30], the annotations are merge to be six person part classes and one background class which are Head, Torse, Upper / Lower arms and Upper / Lower legs. The other is our large-scale LIP dataset which is highly challenging with severe pose\ncomplexity, heavily occlusions and body truncation, as introduced and analyzed in Section 3.\nNetwork architecture: We utilize the publicly available model, Attention [5], as the basic architecture due to its leading accuracy and competitive efficiency. We also train a network based on DeepLabV2 [4], which employs re-purposed VGG-16 by atrous convolution, multi-scale inputs and atrous spatial pyramid pooling.\nTraining: We use the pre-trained models and networks settings provided by DeepLabV2 [4]. The scale of the input images is fiexed as 321 \u00d7 321 for training networks based on Attention [5]. Two training steps are employed to train the networks. First, we train the basic network on our LIP dataset for 30 epochs, which takes about two days. Then we perform \u201cself-supervised\u201d strategy to fine-tune our model with structure-sensitive loss. We fine-tune the networks for roughly 20 epochs and it takes about one and a half days. We train all the models using stochastic gradient descent with a batch size of 10 images, momentum of 0.9, and weight decay of 0.0005. In the testing stage, one images takes 0.5 second on average.\nReproducibility: The proposed method is implemented by extending the Caffe framework. All networks are trained on a single NVIDIA GeForce GTX TITAN X GPU with 12GB memory. The code and models are available at https://github.com/ Engineering-Course/LIP_SSL."}, {"heading": "5.2. Results and Comparisons", "text": "We compare the proposed method with the strong baselines on the two public dataset.\nPASCAL-Person-Part dataset [6]. Table. 5 shows the performance of our models and comparisons with four state-of-the-art methods on the standard intersection over union (IoU) criterion. Our method can significantly outperform four baselines, particularly. For example, our best model achieves 59.36%, 7.58% better than DeepLabLargeFOV [4] and 2.97% better than Attention [5]. This large improvement demonstrates that our self-supervised strategy is significantly helpful for human parsing task.\nLIP dataset: We report the results and the comparisons with four state-of-the-art methods on LIP validation set and test set in Table. 2 and Table. 3. On validation set, the proposed architecture can give a huge boost in average IoU: 3.09% better than DeepLabV2 [4] and 1.81% better than Attention [5]. On test set, our method also outperforms other baselines. This superior performance achieved by our method demonstrates the effectiveness of our selfsupervised structure-sensitive learning, which incorporates the body joint structure into the pixel-wise prediction.\nIn Fig. 5, we show the results with respect to the different challenging factors on our LIP validation set. With our structure-sensitive loss, the performance of all kinds of types are improved, which demonstrates that human joint structure is helpful for human parsing task and our selfsupervised learning is reasonable and efficient.\nWe further report per-class IoU on LIP validation set to verify the detailed effectiveness of our structure-sensitive loss, presented in Table. 4. With structure-sensitive loss, we achieved the best performance on almost all the classes. As observed from the reported results, structure-sensitive loss significantly improves the performance of the labels like arms, legs, and shoes, which demonstrates its ability to refine the ambiguous of left and right. Furthermore, the labels covering small regions such as sunglasses, socks, gloves, are predicted better with higher IoU. This improvement also demonstrates the effectiveness of structure-sensitive loss especially for small labels."}, {"heading": "5.3. Qualitative Comparison", "text": "The qualitative comparisons of parsing results on the LIP validation set are visualized in Fig. 8. As can be observed\nfrom these visualized comparisons, our self-learning structure outputs more semantically meaningful and precise predictions than other four methods despite the existence of large appearance and position variations. For example, observed from the full-body image(e), the small regions (e.g. left or right shoe) can be successfully segmented out by our method with structure-sensitive loss. Taking (b) and (c) for example, our approach can also successfully handle the confusing labels such as left arm versus right arm and left leg versus right leg. These regions with similar appearances can be recognized and separated by the guidance from joint structure information. For the most difficult head-missed image(c), the left shoe, right shoe and part of the left leg are excellently corrected by our approach. In general, by effectively exploiting self-supervised structure-sensitive loss, our approach outputs more reasonable results for confusing labels on the human parsing task."}, {"heading": "5.4. Further experiments and analyses", "text": "For a better understanding of our LIP dataset, we evaluate the models trained on LIP and test on ATR [18] on the common categories, as reported in Table 6 (Left). In general, the performance on ATR are better than those on LIP because LIP dataset contains the instances with more diverse poses, appearance patterns, occlusions and resolution issues, which is more consistent with real-world situations.\nFollowing MSCOCO dataset [19], we have done an em-\npirical analysis on different object sizes, i.e., small (area < 1532), medium (1532 \u2264 area < 3212) and large (area \u2265 3212). The results of four baselines and the proposed SSL are reported in Table 6 (Middle). It can be observed that our SSL shows substantial superior performance for different sizes of objects. It further demonstrates the advantage of incorporating the structure-sensitive loss into parsing model.\nTo further research the influence of the input size, we perform an experiment over the scale of single input. The detailed analyses over different input sizes for all methods (except Attention [5] for its attention mechanism over scales) are presented in Table 6 (Right), which shows that our structure-sensitive learning is more robust for input size."}, {"heading": "6. Conclusions", "text": "In this work, we presented \u201cLook into Person (LIP)\u201d, a large-scale human parsing dataset and a carefully designed benchmark to spark progress in human parsing. LIP contains 50,462 images, which are richly labeled with 19 semantic part labels. Taking advantage of our rich annotations, we performed detailed experimental analyses to identify the success and limitations of the leading human parsing approaches. Furthermore, we design a novel learning strategy, namely self-supervised structure-sensitive learning, to explicitly enforce the produced parsing results semantically consistent with the human joint structures."}], "references": [{"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["M. Andriluka", "L. Pishchulin", "P. Gehler", "B. Schiele"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention to scale: Scale-aware semantic image segmentation", "author": ["L.-C. Chen", "Y. Yang", "J. Wang", "W. Xu", "A.L. Yuille"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun"], "venue": "Detect what you can: Detecting and representing objects using holistic models and body parts. In CVPR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Articulated pose estimation by a graphical model with image dependent pairwise relations", "author": ["X. Chen", "A. Yuille"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards unified human parsing and pose estimation", "author": ["J. Dong", "Q. Chen", "X. Shen", "J. Yang", "S. Yan"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A deformable mixture parsing model with parselets", "author": ["J. Dong", "Q. Chen", "W. Xia", "Z. Huang", "S. Yan"], "venue": "ICCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting depth", "author": ["D. Eigen", "R. Fergus"], "venue": "surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn"], "venue": "Zisserman. The pascal visual object classes challenge 2010 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["C. Gan", "M. Lin", "Y. Yang"], "venue": "de Melo, and A. G. Hauptmann. Concepts not alone: Exploring pairwise relationships for zero-shot video activity recognition. In AAAI", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments", "author": ["C. Ionescu", "D. Papava", "V. Olaru", "C. Sminchisescu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep human parsing with active template regression", "author": ["X. Liang", "S. Liu", "X. Shen", "J. Yang", "L. Liu", "J. Dong", "L. Lin", "S. Yan"], "venue": "TPAMI", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards computational baby learning: A weakly-supervised approach for object detection", "author": ["X. Liang", "S. Liu", "Y. Wei", "L. Liu", "L. Lin", "S. Yan"], "venue": "ICCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic object parsing with local-global long short-term memory", "author": ["X. Liang", "X. Shen", "D. Xiang", "J. Feng", "L. Lin", "S. Yan"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Proposal-free network for instance-level object segmentation", "author": ["X. Liang", "Y. Wei", "X. Shen", "J. Yang", "L. Lin", "S. Yan"], "venue": "arXiv preprint arXiv:1509.02636", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Human parsing with contextualized convolutional neural network", "author": ["X. Liang", "C. Xu", "X. Shen", "J. Yang", "S. Liu", "J. Tang", "L. Lin", "S. Yan"], "venue": "ICCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S.J. Belongie", "L.D. Bourdev", "R.B. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1405.0312", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Matching-CNN Meets KNN: Quasi- Parametric Human Parsing", "author": ["S. Liu", "X. Liang", "L. Liu", "X. Shen", "J. Yang", "C. Xu", "L. Lin", "X. Cao", "S. Yan"], "venue": "CVPR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations", "author": ["Z. Liu", "P. Luo", "S. Qiu", "X. Wang", "X. Tang"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Parsing semantic parts of cars using graphical models and segment appearance consistency", "author": ["W. Lu", "X. Lian", "A. Yuille"], "venue": "BMVC", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Xufeng Han", "author": ["S.L.A.C.B.T.L.B.M. Hadi Kiapour"], "venue": "Where to buy it:matching street clothing photos in online shops. In ICCV", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Flowing convnets for human pose estimation in videos", "author": ["T. Pfister", "J. Charles", "A. Zisserman"], "venue": "ICCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A High Performance CRF Model for Clothes Parsing", "author": ["E. Simo-Serra", "S. Fidler", "F. Moreno-Noguer", "R. Urtasun"], "venue": "ACCV", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic part segmentation using compositional model combining shape and appearance", "author": ["J. Wang", "A. Yuille"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Who blocks who: Simultaneous clothing segmentation for grouping images", "author": ["N. Wang", "H. Ai"], "venue": "ICCV", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint object and part segmentation using deep learned potentials", "author": ["P. Wang", "X. Shen", "Z. Lin", "S. Cohen", "B. Price", "A. Yuille"], "venue": "ICCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Zoom better to see clearer: Huamn part segmentation with auto zoom net", "author": ["F. Xia", "P. Wang", "L.-C. Chen", "A.L. Yuille"], "venue": "ECCV", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Pose-guided human parsing by an and/or graph using pose-context features", "author": ["F. Xia", "J. Zhu", "P. Wang", "A. Yuille"], "venue": "AAAI", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Paper doll parsing: Retrieving similar styles to parse clothing items", "author": ["K. Yamaguchi", "M. Kiapour", "T. Berg"], "venue": "ICCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Parsing clothing in fashion photographs", "author": ["K. Yamaguchi", "M. Kiapour", "L. Ortiz", "T. Berg"], "venue": "CVPR", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation", "author": ["W. Yang", "W. Ouyang", "H. Li", "X. Wang"], "venue": "CVPR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic topic modeling for monitoring market competition from online text and image data", "author": ["H. Zhang", "G. Kim", "E.P. Xing"], "venue": "ACM SIGKDD. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised salience learning for person re-identification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "CVPR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "ICCV", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 16, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 13, "context": "Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16].", "startOffset": 95, "endOffset": 107}, {"referenceID": 17, "context": "Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16].", "startOffset": 95, "endOffset": 107}, {"referenceID": 15, "context": "Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16].", "startOffset": 95, "endOffset": 107}, {"referenceID": 14, "context": "Nevertheless, as demonstrated in many other problems such as object detection [15] and semantic segmentation [37], the performance of those CNN-based approaches heavily rely on the availability of annotated images for training.", "startOffset": 78, "endOffset": 82}, {"referenceID": 36, "context": "Nevertheless, as demonstrated in many other problems such as object detection [15] and semantic segmentation [37], the performance of those CNN-based approaches heavily rely on the availability of annotated images for training.", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 8, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 13, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 17, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 5, "context": ", upright) [6], these datasets are limited in their coverage and scalability, as shown in Fig.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "The largest public human parsing dataset [18] so far only contains 17,000 fashion images while others only include thousands of images.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 29, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 32, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 31, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 8, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 25, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 19, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 17, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 36, "context": ", Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29].", "startOffset": 91, "endOffset": 102}, {"referenceID": 3, "context": ", Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29].", "startOffset": 91, "endOffset": 102}, {"referenceID": 28, "context": ", Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29].", "startOffset": 91, "endOffset": 102}, {"referenceID": 2, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 21, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 3, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 4, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 33, "context": "The human body structural information has been previously well-explored in the human pose estimation [34, 7] where dense joint annotations are provided.", "startOffset": 101, "endOffset": 108}, {"referenceID": 6, "context": "The human body structural information has been previously well-explored in the human pose estimation [34, 7] where dense joint annotations are provided.", "startOffset": 101, "endOffset": 108}, {"referenceID": 32, "context": "Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised StructureDataset #Training #Validation #Test Categories Fashionista [33] 456 229 56 PASCAL-Person-Part [6] 1,716 1,817 7 ATR [18] 16,000 700 1,000 18 LIP 30,462 10,000 10,000 20", "startOffset": 316, "endOffset": 320}, {"referenceID": 5, "context": "Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised StructureDataset #Training #Validation #Test Categories Fashionista [33] 456 229 56 PASCAL-Person-Part [6] 1,716 1,817 7 ATR [18] 16,000 700 1,000 18 LIP 30,462 10,000 10,000 20", "startOffset": 351, "endOffset": 354}, {"referenceID": 17, "context": "Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised StructureDataset #Training #Validation #Test Categories Fashionista [33] 456 229 56 PASCAL-Person-Part [6] 1,716 1,817 7 ATR [18] 16,000 700 1,000 18 LIP 30,462 10,000 10,000 20", "startOffset": 373, "endOffset": 377}, {"referenceID": 5, "context": "Our proposed framework significantly surpasses the previous methods on both the existing PASCAL-Person-Part dataset [6] and our new LIP dataset.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "(b): The parsing results by Attention-toscale [5] where the left-arm is wrongly labeled as right-arm.", "startOffset": 46, "endOffset": 49}, {"referenceID": 20, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 106, "endOffset": 114}, {"referenceID": 23, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 106, "endOffset": 114}, {"referenceID": 0, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 141, "endOffset": 148}, {"referenceID": 12, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 141, "endOffset": 148}, {"referenceID": 17, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 32, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 31, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 25, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 19, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 29, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 4, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 17, "context": "[18] proposed a novel Co-CNN architecture which integrates multiple levels of image contexts into a unified nerwork.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23].", "startOffset": 138, "endOffset": 150}, {"referenceID": 28, "context": "Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23].", "startOffset": 138, "endOffset": 150}, {"referenceID": 22, "context": "Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23].", "startOffset": 138, "endOffset": 150}, {"referenceID": 3, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 140, "endOffset": 147}, {"referenceID": 36, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 140, "endOffset": 147}, {"referenceID": 3, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 205, "endOffset": 215}, {"referenceID": 4, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 205, "endOffset": 215}, {"referenceID": 29, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 205, "endOffset": 215}, {"referenceID": 4, "context": "[5] proposed an attention mechanism that learns to weight the multi-scale features at each pixel location.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Some previous works [8, 31] explored human pose information to guide human parsing by generating \u201cpose-guided\u201d part segment proposals.", "startOffset": 20, "endOffset": 27}, {"referenceID": 30, "context": "Some previous works [8, 31] explored human pose information to guide human parsing by generating \u201cpose-guided\u201d part segment proposals.", "startOffset": 20, "endOffset": 27}, {"referenceID": 32, "context": "First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18].", "startOffset": 124, "endOffset": 135}, {"referenceID": 5, "context": "First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18].", "startOffset": 124, "endOffset": 135}, {"referenceID": 17, "context": "First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18].", "startOffset": 124, "endOffset": 135}, {"referenceID": 18, "context": "The images in the LIP dataset are cropped person instances from Microsoft COCO [19] training and validation sets.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "We implemented an annotation tool and generated multi-scale superpixels of images based on [2] to speed up the annotation.", "startOffset": 91, "endOffset": 94}, {"referenceID": 21, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 243, "endOffset": 246}, {"referenceID": 2, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 376, "endOffset": 379}, {"referenceID": 21, "context": "17 FCN-8s [22] 76.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "29 DeepLabV2 [4] 82.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "64 Attention [5] 83.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Method Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "37 FCN-8s [22] 76.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "69 DeepLabV2 [4] 82.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "56 Attention [5] 83.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Following [5, 30], we use the standard intersection over union (IoU) criterion and pixelwise accuracy for evaluation.", "startOffset": 10, "endOffset": 17}, {"referenceID": 29, "context": "Following [5, 30], we use the standard intersection over union (IoU) criterion and pixelwise accuracy for evaluation.", "startOffset": 10, "endOffset": 17}, {"referenceID": 4, "context": "On the LIP validation set, among the four approaches, Attention [5] achieves the best result of 54.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "For mean IoU, Attention [5] performs best with 42.", "startOffset": 24, "endOffset": 27}, {"referenceID": 21, "context": "92%, while both FCN-8s [22] (28.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "29%) and SegNet [3] (18.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "The interesting outcome of this comparison is that the achieved performance is substantially lower than the current best results on other segmentation benchmark such as PASCAL VOC [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 4, "context": "For example, the IoU of Attention [5] drops from 42.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Method hat hair gloves sunglasses u-clothes dress coat socks pants jumpsuits scarf skirt face l-arm r-arm l-leg r-leg l-shoe r-shoe Bkg Avg SegNet [3] 26.", "startOffset": 147, "endOffset": 150}, {"referenceID": 21, "context": "17 FCN-8s [22] 39.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "29 DeepLabV2 [4] 57.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "64 Attention [5] 58.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Attention [5] and DeepLabV2 [4] perform better on small labels thanks to the utilization of multi-scale features.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Attention [5] and DeepLabV2 [4] perform better on small labels thanks to the utilization of multi-scale features.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 94, "endOffset": 97}, {"referenceID": 33, "context": "However, the results of state-of-the-art pose estimation models [34, 7] still have many errors.", "startOffset": 64, "endOffset": 71}, {"referenceID": 6, "context": "However, the results of state-of-the-art pose estimation models [34, 7] still have many errors.", "startOffset": 64, "endOffset": 71}, {"referenceID": 24, "context": "Following [25], for each parsing result and corresponding ground truth, we compute the center points of regions to obtain joints represented as heatmaps for training more smoothly.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "One is the public PASCAL-Person-part dataset with 1,716 images for training and 1,817 for testing, which pays attention to the human part segmentation annotated by [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Following [5, 30], the annotations are merge to be six person part classes and one background class which are Head, Torse, Upper / Lower arms and Upper / Lower legs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 29, "context": "Following [5, 30], the annotations are merge to be six person part classes and one background class which are Head, Torse, Upper / Lower arms and Upper / Lower legs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 3, "context": "Method head torso u-arms l-arms u-legs l-legs Bkg Avg DeepLab-LargeFOV [4] 78.", "startOffset": 71, "endOffset": 74}, {"referenceID": 29, "context": "78 HAZN [30] 80.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "11 Attention [5] 81.", "startOffset": 13, "endOffset": 16}, {"referenceID": 15, "context": "39 LG-LSTM [16] 82.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Table 5: Comparison of person part segmentation performance with four state-of-the-art methods on the PASCALPerson-Part dataset [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Network architecture: We utilize the publicly available model, Attention [5], as the basic architecture due to its leading accuracy and competitive efficiency.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "We also train a network based on DeepLabV2 [4], which employs re-purposed VGG-16 by atrous convolution, multi-scale inputs and atrous spatial pyramid pooling.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "Training: We use the pre-trained models and networks settings provided by DeepLabV2 [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "The scale of the input images is fiexed as 321 \u00d7 321 for training networks based on Attention [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Method ATR LIP small medium large 153 321 513 SegNet [3] 15.", "startOffset": 53, "endOffset": 56}, {"referenceID": 21, "context": "44 FCN-8s [22] 34.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "25 DeepLabV2 [4] 48.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "28 Attention [5] 49.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "PASCAL-Person-Part dataset [6].", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "58% better than DeepLabLargeFOV [4] and 2.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "97% better than Attention [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "09% better than DeepLabV2 [4] and 1.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "81% better than Attention [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "For a better understanding of our LIP dataset, we evaluate the models trained on LIP and test on ATR [18] on the common categories, as reported in Table 6 (Left).", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "Following MSCOCO dataset [19], we have done an empirical analysis on different object sizes, i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "The detailed analyses over different input sizes for all methods (except Attention [5] for its attention mechanism over scales) are presented in Table 6 (Right), which shows that our structure-sensitive learning is more robust for input size.", "startOffset": 83, "endOffset": 86}], "year": 2017, "abstractText": "Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark1 \u201cLook into Person (LIP)\u201d that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in humancentric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-PersonPart dataset demonstrate the superiority of our method.", "creator": "LaTeX with hyperref package"}}}