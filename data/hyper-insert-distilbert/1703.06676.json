{"id": "1703.06676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation", "abstract": "translating geographic information between semantic text and image characters is previously a fundamental problem in humans artificial intelligence that connects in natural symbolic language object processing and synthetic computer vision. in the past every few years, performance in robotic image caption generation automation has seen significant improvement nationally through the adoption of microsoft recurrent neural computational networks ( mp rnn ). thus meanwhile, text - to - image generation begun to independently generate fairly plausible user images obtained using datasets of multi specific cognitive categories somewhat like flower birds and flowers. we've since even seen image generation independently from multi - category datasets more such that as putting the microsoft visual common objects database in symbolic context ( ms mscoco ) through integrating the use fork of windows generative web adversarial networks ( mn gans ). primarily synthesizing simple objects with a very complex shape, however, is still challenging. for typical example, animals and simple humans easily have mastered many physical degrees of freedom, which means that they can eventually take aim on many complex shapes. we actually propose manufacturing a new training method itself called balanced image - text - enabled image ( hp i2t2i ) blending which integrates interactive text - messaging to - image and image - graphic to - text ( pure image captioning ) synthesis to improve essentially the graphics performance quality of direct text - to - action image composite synthesis. namely we demonstrate that % possesses the capability of our algorithms method to understand the sentence and descriptions, so as possible to determine i2t2i can generate better compatible multi - categories human images using mscoco encoding than the state - of - the - board art. we then also demonstrate effectively that i2t2i vision can achieve transfer window learning by using you a pre - trained text image processing captioning mapping module to generate human morphological images on the mpii human pose", "histories": [["v1", "Mon, 20 Mar 2017 11:11:38 GMT  (3792kb,D)", "http://arxiv.org/abs/1703.06676v1", null], ["v2", "Mon, 8 May 2017 18:46:42 GMT  (3792kb,D)", "http://arxiv.org/abs/1703.06676v2", null], ["v3", "Sat, 3 Jun 2017 22:46:46 GMT  (3792kb,D)", "http://arxiv.org/abs/1703.06676v3", "International Conference on Image Processing (ICIP) 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["hao dong", "jingqing zhang", "douglas mcilwraith", "yike guo"], "accepted": false, "id": "1703.06676"}, "pdf": {"name": "1703.06676.pdf", "metadata": {"source": "META", "title": "I2T2I: LEARNING TEXT TO IMAGE SYNTHESIS WITH TEXTUAL DATA AUGMENTATION", "authors": ["Hao Dong", "Jingqing Zhang", "Douglas McIlwraith", "Yike Guo"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Deep learning, GAN, Image Synthesis"}, {"heading": "1. INTRODUCTION", "text": "As an image can be described in different ways and a caption can also be translated into different images, there is no bijection between images and captions. For example, in the sentence \u201ca group of giraffes standing next to each other\u201d, the number of giraffes, the background and many other details are uncertain. It is an open issue of text-to-image mapping that the distribution of images conditioned on a sentence is highly multi-modal. In the past few years, we\u2019ve witnessed a breakthrough in the application of recurrent neural networks (RNN) to generating textual descriptions conditioned on images [1, 2], with Xu et al. showing that the multimodality problem can be decomposed sequentially [3]. However, the lack of datasets with diversity descriptions of images limits the performance of text-to-image synthesis on multicategories dataset like MSCOCO [4]. Therefore, the problem\nof text-to-image synthesis is still far from being solved.\nRecently, both fractionally-strided convolutional and convolutional neural networks (CNN) have shown promise for image generation. PixelCNN [5] learns a conditional image generator which is able to generate realistic scenes representing different objects and portraits of people. Generative adversarial networks (GAN) [6], specifically DCGAN [7] have been applied to a variety of datasets and shown plausible results. Odena et al. [8] has shown the capability of GANs to generate high-quality images conditioned on classes. To solve the multi-modality problem for text-to-image synthesis, Reed et al. [9] proposed GAN-CLS which bridges advances of DCGAN and an RNN encoder to generate images from an latent variables and embedding of image descriptions. Besides these, recent studies learned to generate images by conditions [10, 11]. However, GAN-CLS fails to generate plausible images on more complicated and changeable realistic scenes such as those illustrating human activities.\nIn this paper, we illustrate that sentence embedding should be able to capture details from various descriptions (one image can be described by plenty of sentences). This determines the robustness of understanding image detail. The main contribution of our work is a new training method called ImageText-Image (I2T2I), which applies textual data augmentation to help text-to-image synthesis learn more descriptions of each image. I2T2I is composed of three modules: 1) the\nar X\niv :1\n70 3.\n06 67\n6v 1\n[ cs\n.C V\n] 2\n0 M\nar 2\n01 7\nimage captioning module 2) the image-text mapping module and 3) the GAN module. The image captioning module performs textual data augmentation and the other two modules are trained to realize text-to-image synthesis. Our method I2T2I achieves stronger capability on capturing details of descriptions. We evaluate our method by comparing images generated by GAN-CLS and I2T2I on the MSCOCO dataset. Furthermore, we demonstrate the flexibility and generality of I2T2I on MPII Human Pose dataset (MHP) [12] by using a pre-trained image captioning module from MSCOCO."}, {"heading": "2. PRELIMINARIES", "text": "In this section, we briefly describe the previous work that our modules are based upon."}, {"heading": "2.1. Image Captioning Module", "text": "Image captioning via deep CNN and RNN has seen significant improvements in recent years [2]. Deep CNNs can fully represent an image X by embedding it into a fixed-length vector. Then RNN especially LSTM [13] decodes the fixedlength vector to a desired output sentence S = (s1, ..., st) by iterating the recurrence relation for t defined in Equation (1) to (4):\nh0, c0 = LSTM(\u03b8,We(CNN(X))) (1) ht, ct = LSTM(ht\u22121, ct\u22121,Ws(st\u22121)) (2) pt = softmax(htWho + bo) (3) st = sample(pt, k) (4)\nwhere \u03b8 is the initial hidden and cell states of LSTM which should be all zeros, s0 is the special start token; h0 is the hidden state initialized by the image embedding vector. Equation (2) sequentially decodes h0 to ht in multiple steps, and Equation (3) gets the probabilities of words pt. In every step, given pt, the word token st is sampled from top k probabilities."}, {"heading": "2.2. Image-Text Mapping Module", "text": "Visual-semantic embedding generates vector representation that embeds images and sentences into a common space [14]. A lot of work has been done to learn the multi-modal representation of images and sentences, where images and sentences can be mapped into the same embedding space by using CNN and RNN encoders [15]. The matching cost between images and sentences can be defined as Equation (5):\nmin \u03b8\n\u2211\nx\n\u2211\nk max{0, \u03b1\u2212 cs(x, v) + cs(x, vk)}+ \u2211\nx\n\u2211\nk\nmax{0, \u03b1\u2212 cs(x, v) + cs(xk, v)} (5)\nwhere cs is cosine similarity of two embedded vectors, x is embedded output of CNN encoder, v is embedded output of RNN encoder. xk and vk are embedded outputs of mismatched images and sentences. The \u03b1 is a margin value. This cost function maximizes the cosine similarity of mismatched pair of images and sentences, and minimizes the cosine similarity of matched images and sentences.\nFigure 2 illustrates this appraoach to image-text mapping. The area marked in blue denotes image embedding and matching in a common latent space."}, {"heading": "2.3. Generative adversarial network Module", "text": "Generative adversarial networks (GAN) are composed of a generator G and a discriminator D, which are trained in a competitive manner [6]. D learns to discriminate real images from fake ones generated by G, while G learns to generate fake images from latent variables z and tries to cheat D. The cost function is can be defined as Equation (6):\nmin G max D V (D,G) = Ex\u223cPsample(x)[logD(x)] +\nEz\u223cPz(z)[log(1\u2212D(G(z)))] (6)\nBased on this, GAN-CLS [9] achieve text-to-image synthesis by concatenating text embedding features into latent variables of the generator G and the last layer of discriminator D, as Fig. 3 shows.\nPreviously introduced GAN-CLS, instead of doing binary discrimination, the discriminator D learns to discriminate three conditions, which are real images with matched text, fake images with arbitrary text, and real images with mismatched text. Figure 3 illustrates the architecture of a\nGAN-CLS for text-to-image synthesis (in blue), and includes the previously introduced RNN encoder from image-text mapping module. Details of how we perform textual data augmentation \u2013 i.e. synthesize matched and mismatched sentences through our image captioning module, are given in Section 3."}, {"heading": "3. METHOD", "text": "In this section, we introduce a significant contribution of our work \u2013 augmentation of textual data via image captioning. We demonstrate how this is performed through the use of a CNN and RNN, and how this is utilised in text-to-image synthesis."}, {"heading": "3.1. Textual data augmentation via image captioning", "text": "For each real image Xi, there is a set Si that has a limited number of human-annotated captions, which cannot cover all possible descriptions of the images, as every sentence has lots of synonymous sentences. For example, \u201ca man riding a surf board on a wave\u201d is similar with \u201da person on a surf board riding a wave\u201d, \u201ca surfer riding a large wave in the ocean\u201d and etc. Fortunately, the softmax output of RNN-based image captioning can exploit the uncertainty to generate synonymous sentences. Therefore, with k greater than 1 in Equation (4), a massive number of textual descriptions can be generated from a single image. Given an image X , the probability distribution of sentences S can be defined as Equation (7):\nP (S|X) = P (s1|X)P (s2|h1, c1, s1)... P (st|ht\u22121, ct\u22121, st\u22121) (7)\nApart from synonymous sentences, the image captioning module may output similar but mismatched sentences. For example, \u201dfour birds are flying on the blue sky\u201d and \u201dfive birds are flying in the sky\u201d.\nTraining the text encoder with massive synonymous sentences and similar sentences can help to improve the robustness of an RNN encoder. As Fig. 4 shows, with more synthetic sentences being used, an image will occupy more area in the text space and increase the coverage density.\nThen, sentences not present in training datasets can be mapped to the nearest image, e.g, \u201dfive birds are flying in the sky\u201d can be mapped with the image of \u201dfour birds are flying on the blue sky\u201d. The sentences that share semantic and syntactic properties are thus mapped to similar vector representations, e.g. \u201da man is riding a surfboard in a wave\u201d can be mapped into \u201da man riding a surf board on a wave\u201d. As our method is able to map unseen or synonymous sentences to adjacent vector representation, improving the quality of image synthesis for these sentences."}, {"heading": "3.2. Network structure", "text": "Our method includes three modules, previously introduced. These include 1) an image captioning module, 2) an imagetext mapping module and 3) a conditional generative adversarial network for text-to-image synthesis (GAN-CLS). For our image captioning module, we use a state-of-the-art Inception model [16] followed by LSTM decoder as described in [1]. To improve the diversity of generated captions, we set k = 3 in Equation (4).\nTo obtain image-text mapping (embedding), we follow the cost function defined in Equation (5), and use Inception V3 as the image encoder and LSTM as the sentence encoder. As image-text mapping requires both matched and mismatched sentences, after the image captioning module synthesizes a batch of matched sentences for a batch of images, we randomly shuffle and use them as mismatched sentences for image-text mapping module, as Fig. 5 shows.\nWe adopt GAN-CLS as image generation module, and reuse the LSTM encoder of image-to-text mapping module, then concatenate its embedded output into latent variables [9]. For the RNN encoder, we use LSTM [13] with a hidden size of 256. For GAN-CLS, in order to have a qualitative comparison, we use the same architecture describe in [9]."}, {"heading": "3.3. Training", "text": "Both matched and mismatched sentences are synthesized at the beginning of every iteration, then the GAN-CLS shown in\nFig. 3 will use these new sentences to learn image synthesis. As synthetic sentences are generated in every iteration, the RNN encoder for image and text embedding in our method is trained synchronously with the GAN-CLS module. Besides, in each iteration, we perform data augmentation for images includes random rotation, flip, and cropping.\nFor training the RNN encoder, we use learning rate of 0.0001, and the Adam optimization [17] with momentum of 0.5. For training GAN-CLS, we use initial learning rate of 0.0002 with Adam optimization with momentum of 0.5 also. Both learning rates of RNN encoder and GAN-CLS are decreased by 0.5 every 100 epochs. We use a batch size of 64 and train for 600 epochs."}, {"heading": "4. EXPERIMENTS", "text": "Two datasets were used in our experiments. The MSCOCO [4] dataset contains 82783 training and 40504 validation images, each of which is annotated with 5 sentences from different annotators. The MPII Human Pose dataset (MHP) [12] has around 25K images covering 410 human activities. No sentence description is available, which makes it suitable to evaluate transfer learning."}, {"heading": "4.1. Qualitative comparison", "text": "We evaluated our approach on the MSCOCO dataset with and without textual data augmentation applied. In both cases, the generator and discriminator use the same architectures i.e. GAN-CLS [9]. Results on the validation set can be compared in Fig. 1 and Fig. 6.\nBoth GAN-CLS and I2T2I generate a plausible background, but I2T2I performs better for certain classes such as \u201cpeople\u201d which has proved to be challenging for previous approaches [18]. Moreover, higher quality synthesis was found static object-based images such as window, toilet, bus etc."}, {"heading": "4.2. Transfer learning", "text": "In order to demonstrate transfer learning, we used the pretrained image captioning module from MSCOCO to train a text-to-image module on the MPII Human Pose dataset (MHP). As all images in this dataset contain people, we can successfully synthesize images of human activities, and the image quality is also better than GAN-CLS as Fig. 7 shows. This experiment shows that I2T2I can achieve text-to-image synthesis on unlabeled image dataset by using a pre-trained image captioning module from multi-categories dataset."}, {"heading": "5. CONCLUSION", "text": "In this work, we propose a new training method I2T2I. The qualitative comparison results show that training text-toimage synthesis with textual data augmentation can help\nto obtain higher quality images. Furthermore, we demonstrate the synthesis of images relating to humans \u2013 which has proved difficult for existing methods [18]. In the immediate future, we plan to combine stackGAN [19] to improve image resolution."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "The authors would like to thank Chao Wu and Simiao Yu for helpful comments and suggestions. Hao Dong is supported by the OPTIMISE Portal. Jingqing Zhang is supported by LexisNexis."}, {"heading": "7. REFERENCES", "text": "[1] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, \u201cShow and tell: Lessons learned from the 2015 mscoco image captioning challenge,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.\n[2] Andrej Karpathy and Li Fei-Fei, \u201cDeep visual-semantic alignments for generating image descriptions,\u201d CVPR, 2015.\n[3] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio, \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d ICML, 2015.\n[4] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla\u0301r, and C Lawrence Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d ECCV, 2014.\n[5] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu, \u201cConditional image generation with pixelcnn decoders,\u201d arXiv preprint arXiv:1606.05328, 2016.\n[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, \u201cGenerative adversarial nets,\u201d NIPS, 2014.\n[7] Alec Radford, Luke Metz, and Soumith Chintala, \u201cUnsupervised representation learning with deep convolutional generative adversarial networks,\u201d ICLR, 2016.\n[8] Augustus Odena, Christopher Olah, and Jonathon Shlens, \u201cConditional image synthesis with auxiliary classifier gans,\u201d arXiv preprint arXiv:1610.09585, 2016.\n[9] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee, \u201cGenerative adversarial text to image synthesis,\u201d ICML, 2016.\n[10] Scott Reed, A van den Oord, N Kalchbrenner, V Bapst, M Botvinick, and N de Freitas, \u201cGenerating interpretable images with controllable structure,\u201d ICLR, 2017.\n[11] Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee, \u201cLearning what and where to draw,\u201d NIPS, 2016.\n[12] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele, \u201c2d human pose estimation: New benchmark and state of the art analysis,\u201d CVPR, 2014.\n[13] Sepp Hochreiter, S Hochreiter, Ju\u0308rgen Schmidhuber, and J Schmidhuber, \u201cLong short-term memory.,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u201380, 1997.\n[14] Andrej Karpathy, Armand Joulin, and Fei Fei F Li, \u201cDeep fragment embeddings for bidirectional image sentence mapping,\u201d NIPS, 2014.\n[15] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel, \u201cUnifying visual-semantic embeddings with multimodal neural language models,\u201d TACL, 2015.\n[16] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, \u201cRethinking the inception architecture for computer vision,\u201d CVPR, 2016.\n[17] Diederik Kingma and Jimmy Ba, \u201cAdam: A Method for Stochastic Optimization,\u201d International Conference on Learning Representations, 2014.\n[18] Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune, \u201cPlug & play generative networks: Conditional iterative generation of images in latent space,\u201d NIPS, 2016.\n[19] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas, \u201cStackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks,\u201d arXiv preprint arXiv:1612.03242, 2016."}], "references": [{"title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "ICML, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "ECCV, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.05328, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "NIPS, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "ICLR, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1610.09585, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "ICML, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating interpretable images with controllable structure", "author": ["Scott Reed", "A van den Oord", "N Kalchbrenner", "V Bapst", "M Botvinick", "N de Freitas"], "venue": "ICLR, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning what and where to draw", "author": ["Scott Reed", "Zeynep Akata", "Santosh Mohan", "Samuel Tenka", "Bernt Schiele", "Honglak Lee"], "venue": "NIPS, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["Mykhaylo Andriluka", "Leonid Pishchulin", "Peter Gehler", "Bernt Schiele"], "venue": "CVPR, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "S Hochreiter", "J\u00fcrgen Schmidhuber", "J Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u201380, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li"], "venue": "NIPS, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "TACL, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "CVPR, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Anh Nguyen", "Jason Yosinski", "Yoshua Bengio", "Alexey Dosovitskiy", "Jeff Clune"], "venue": "NIPS, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "author": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaolei Huang", "Xiaogang Wang", "Dimitris Metaxas"], "venue": "arXiv preprint arXiv:1612.03242, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In the past few years, we\u2019ve witnessed a breakthrough in the application of recurrent neural networks (RNN) to generating textual descriptions conditioned on images [1, 2], with Xu et al.", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "In the past few years, we\u2019ve witnessed a breakthrough in the application of recurrent neural networks (RNN) to generating textual descriptions conditioned on images [1, 2], with Xu et al.", "startOffset": 165, "endOffset": 171}, {"referenceID": 2, "context": "showing that the multimodality problem can be decomposed sequentially [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "However, the lack of datasets with diversity descriptions of images limits the performance of text-to-image synthesis on multicategories dataset like MSCOCO [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "PixelCNN [5] learns a conditional image generator which is able to generate realistic scenes representing different objects and portraits of people.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Generative adversarial networks (GAN) [6], specifically DCGAN [7] have been applied to a variety of datasets and shown plausible results.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Generative adversarial networks (GAN) [6], specifically DCGAN [7] have been applied to a variety of datasets and shown plausible results.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "[8] has shown the capability of GANs to generate high-quality images conditioned on classes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed GAN-CLS which bridges advances of DCGAN and an RNN encoder to generate images from an latent variables and embedding of image descriptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Besides these, recent studies learned to generate images by conditions [10, 11].", "startOffset": 71, "endOffset": 79}, {"referenceID": 10, "context": "Besides these, recent studies learned to generate images by conditions [10, 11].", "startOffset": 71, "endOffset": 79}, {"referenceID": 11, "context": "Furthermore, we demonstrate the flexibility and generality of I2T2I on MPII Human Pose dataset (MHP) [12] by using a pre-trained image captioning module from MSCOCO.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Image captioning via deep CNN and RNN has seen significant improvements in recent years [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "Then RNN especially LSTM [13] decodes the fixedlength vector to a desired output sentence S = (s1, .", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "Visual-semantic embedding generates vector representation that embeds images and sentences into a common space [14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "A lot of work has been done to learn the multi-modal representation of images and sentences, where images and sentences can be mapped into the same embedding space by using CNN and RNN encoders [15].", "startOffset": 194, "endOffset": 198}, {"referenceID": 5, "context": "Generative adversarial networks (GAN) are composed of a generator G and a discriminator D, which are trained in a competitive manner [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "Based on this, GAN-CLS [9] achieve text-to-image synthesis by concatenating text embedding features into latent variables of the generator G and the last layer of discriminator D, as Fig.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "For our image captioning module, we use a state-of-the-art Inception model [16] followed by LSTM decoder as described in [1].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "For our image captioning module, we use a state-of-the-art Inception model [16] followed by LSTM decoder as described in [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "We adopt GAN-CLS as image generation module, and reuse the LSTM encoder of image-to-text mapping module, then concatenate its embedded output into latent variables [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 12, "context": "For the RNN encoder, we use LSTM [13] with a hidden size of 256.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "For GAN-CLS, in order to have a qualitative comparison, we use the same architecture describe in [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "0001, and the Adam optimization [17] with momentum of 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "The MSCOCO [4] dataset contains 82783 training and 40504 validation images, each of which is annotated with 5 sentences from different annotators.", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "The MPII Human Pose dataset (MHP) [12] has around 25K images covering 410 human activities.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "GAN-CLS [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 17, "context": "Both GAN-CLS and I2T2I generate a plausible background, but I2T2I performs better for certain classes such as \u201cpeople\u201d which has proved to be challenging for previous approaches [18].", "startOffset": 178, "endOffset": 182}, {"referenceID": 17, "context": "Furthermore, we demonstrate the synthesis of images relating to humans \u2013 which has proved difficult for existing methods [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "In the immediate future, we plan to combine stackGAN [19] to improve image resolution.", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We\u2019ve even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of textto-image synthesis. We demonstrate that I2T2I can generate better multi-categories images using MSCOCO than the stateof-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose dataset (MHP) without using sentence annotation.", "creator": "LaTeX with hyperref package"}}}