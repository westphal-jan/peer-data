{"id": "1703.07004", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "The Use of Autoencoders for Discovering Patient Phenotypes", "abstract": "mainly we use autoencoders to create low - confidence dimensional embeddings representations of underlying chronic patient phenotypes that we hypothesize are a large governing factor in determining predicting how different patients actually will visibly react to different chronic interventions. currently we can compare the performance of modern autoencoders reporting that take shorter fixed length sequences encoding of concatenated repeated timesteps as input events with just a recurrent sequence - change to - column sequence ratio autoencoder. combined we evaluate our reporting methods measured on around 35, 500 infectious patients and from the latest mimic iii viral dataset release from clinical beth michael israel deaconess college hospital.", "histories": [["v1", "Mon, 20 Mar 2017 23:30:40 GMT  (992kb,D)", "http://arxiv.org/abs/1703.07004v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harini suresh", "peter szolovits", "marzyeh ghassemi"], "accepted": false, "id": "1703.07004"}, "pdf": {"name": "1703.07004.pdf", "metadata": {"source": "CRF", "title": "The Use of Autoencoders for Discovering Patient Phenotypes", "authors": ["Harini Suresh"], "emails": ["psz@mit.edu", "mghassem@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Intensive Care Units (ICUs) are high-cost, limited-resource environments where quick and accurate decision-making is extremely valuable. However, most decision-making is often made in settings of high uncertainty and based just on the clinician\u2019s prior experience. With the rapid rise in Electronic Health Records (EHRs) available for analysis, data-driven models are well-suited to aid physicians in making decisions about when patients should be treated with or weaned off certain interventions [1].\nAchieving high-quality care depends on a robust understanding of the patient\u2019s underlying acuity throughout time [2]. Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].\nWe aim to create richer representations of patient health with the end goal of predicting actionable interventions. The efficacy of interventions can drastically vary from patient to patient, and unnecessarily administering an intervention can be harmful and expensive [7]. A model of patient health that is able to capture complex relationships in physiological signals is key to accurately predicting onset/weaning of interventions for different patients and necessary for successful personalized medicine.\nThis type of patient phenotyping is challenging because robust representations of human physiology are complicated, and contain many non-obvious dependencies between observed measurements. Moreover, modelling evolving clinical information requires using timeseries data, but this data is often varying-length, irregularly sampled or has missing values. Previously, multitask gaussian processes have been tested for modelling patient acuity but only in Traumatic Brain Injury (TBI) patients [8] or only using longitudinal billing data [2].\nOur approach uses autoencoders for physiological timeseries signal reconstruction. Autoencoders deep neural networks where the target values are the same as the input values, and the hidden layer(s) compress the inputs into a lower dimensional embedding. Since this embedding tries to reconstruct the original input, it must capture fundamental features about the input timeseries, and can be used as a measure of underlying patient acuity. Feature learning in this approach is entirely unsupervised, so unlike traditional acuity scores it is not limited by a manually-defined feature space. Furthermore,\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n70 3.\n07 00\n4v 1\n[ cs\n.L G\n] 2\n0 M\nar 2\nrecurrent autoencoders are able to model signals of varying-length and are robust to missing data due to the ability of Long-Short Term Memory (LSTM) cells to forget unimportant inputs.\nUsing autoencoders allows us to extract the low-dimensional embedding and use it in further work for predicting interventions."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Autoencoders", "text": "Previous work has attempted to use autoencoders on random 30-day patches of input vectors to learn underlying patient phenotypes [9], and has shown that they can extract relevant and useful features about patients.\nIt is also valuable to consider signals that occur throughout a patient\u2019s stay, since there are often long delays between when relevant signals are expressed [10].\nSequence autoencoders take in measured signals one timestep at a time into a layer of LSTM (LongShort Term Memory) cells and produce a fixed-length embedding. This embedding is then used as input to another layer of LSTM cells that try to predict the original input sequence.\nLSTM cells are used because of their ability to effectively model varying-length timeseries data and capture long-term dependencies. LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13]. They are well-suited to modeling clinical data because evidence of certain conditions may be spread apart over several hours or days, and important symptoms may present early on in a patient\u2019s stay.\nSequence autoencoders using LSTM cells were inspired by the success of general sequence-tosequence models applied to machine translation. They were recently used as an initialization step for recurrent neural networks for text classification [14], but have not been applied to the clinical space."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Features", "text": "Features were extracted from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC III) Database [15]. Since 2001, the MIMIC database has been built up and maintained by the Laboratory of Computational Physiology at the Massachusetts Institute of Technology, Beth Israel Deaconess Medical Center, and Philips Healthcare, with support from the National Institute of Biomedical Imaging and Bioinformatics. The database contains general patient information (ICD-9 codes, demographics, room tracking), physiological signals (vital metrics, SAPS), medications (IV meds, provider order entry data), lab tests (chemistry, imaging), fluid balance (in- take, output), and notes (discharge summary, nursing progress reports). The most recent version of MIMIC III contains data from around 38,600 adults, comprising over 58,000 hospital admissions, from 2001-2012. The MIMIC dataset is notable because it is publicly available for free use, encompasses a large and diverse set of patients, and contains numerous high resolution features for each patient.\nWe use data from from the first ICU stay of patients in the Medical Care Unit (MICU), Cardiac Care Unit (CCU), Cardiac Surgery Recovery Unit (CSRU), Surgical Intensive Care Unit (SICU), and Trauma Surgical Intensive Care Unit (TSICU). We only look at patients older than 15 and who were in the ICU for between 12 and 2000 hours. This totals 35,554 unique patients/ICU stays. We extract 30 physiological features for each patient as timeseries spanning their entire stay. Measurements are given timestamps that are rounded to the nearest hour, and if an hour has multiple measurements for a signal those are averaged.\nSince there are many missing values, we first backfill using any present values the patient has, and then fill in remaining missing values with the mean value for that variable across all patients.\nThe data is split into training/validation/testing sets with a 70/15/15 split, stratified on in-hospital mortality in order to have a spectrum of patient severity in both the train and test sets."}, {"heading": "3.2 Autoencoders", "text": "We test the ability of a simple autoencoder with a single hidden layer, an autoencoder with two hidden layers, and a sequence autoencoder to reconstruct the input (Figure 1). We also compare the performance of these models over inputs of different interval lengths, specifically 4, 16, 32 and 64 hours. We train on mini-batches of 128 samples with early stopping to determine the number of epochs.\nFor the fixed-length input autoencoders, we concatenate all 30 features for each hour throughout the given interval length. We use an embedding size equal to the total number of input values divided by 10 to achieve a compression factor of 10x. All hidden layers use a ReLU activation function, and the output layer uses a sigmoidal activation function."}, {"heading": "4 Results", "text": "We evaluate the performance of each autoencoder by taking the mean squared error (MSE) between the predicted sequence of values and the true sequence of values. The sequential autoencoder with one LSTM layer achieves a lower MSE than the single-layer fixed length autoencoder on all interval lengths, but varies in comparison to the double-layer fixed length autoencoder (Figure 2).\nWe also show that reconstructing input timeseries with autoencoders is fairly robust to stratifications in population subsets. We run the autoencoders on intervals of 32 hours with patient subsets stratified by care unit. MSEs are higher than when the autoencoders were trained on the entire patient population, but less than 0.08 in all cases, even though the training sets are much smaller (Figure 3). On these smaller subsets of patients, the sequence autoencoder appears to be able to generalize better with smaller amounts of training data and does better in all cases.\nFigure 2: Performance of autoencoders on reconstructing timeseries input of various lengths.\nFigure 3: Performance of autoencoders on patient population subsets with intervals of 32 hours.\nWe plan to compare the performance of patient embeddings from different autoencoder structures to predict onset of and weaning off of dialysis. We will compare its performance on its own and as an additional feature matrix concatenated to the raw feature values.\nIt will also be interesting to experiment with deeper sequential autoencoder structures, or to use bidirectional LSTM cells in the hidden layers to better reconstruct inputs."}], "references": [{"title": "Critical care-where have we been and where are we going? Critical Care. 2013;17(Suppl 1):S2", "author": ["Vincent JL"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep computational phenotyping.", "author": ["Che", "Zhengping"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Dynamically modeling patient\u2019s health state from electronic medical records: a time series approach", "author": ["KL Caballero Barajas", "R. Akella"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Unfolding physiological state: Mortality modelling in intensive care units", "author": ["M Ghassemi", "T Naumann", "F Doshi-Velez"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Caution when using prognostic models: a prospective comparison of 3 recent prognostic models.", "author": ["Nassar", "Antonio Paulo"], "venue": "Journal of critical care", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Assessment of six mortality prediction models in patients admitted with severe sepsis and septic shock to the intensive care unit: a prospective cohort study.", "author": ["Arabi", "Yaseen"], "venue": "Critical care", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Blood pressure targets for vasopressor therapy: a systematic review", "author": ["F D\u2019Aragon", "EP Belley-Cote", "MO Meade"], "venue": "Shock", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A multivariate timeseries modeling approach to severity of illness assessment and forecasting in ICU with sparse, heterogeneous clinical data.", "author": ["Ghassemi", "Marzyeh"], "venue": "Proceedings of the...AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data.", "author": ["Lasko", "Thomas A", "Joshua C. Denny", "Mia A. Levy"], "venue": "PloS one", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Learning to Diagnose with LSTM Recurrent Neural Networks.", "author": ["Lipton", "Zachary C"], "venue": "arXiv preprint arXiv:1511.03677", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate.", "author": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems.\" arXiv preprint", "author": ["Wen", "Tsung-Hsien"], "venue": "APA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Show and tell: A neural image caption generator.", "author": ["Vinyals", "Oriol"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Semi-supervised sequence learning.", "author": ["Dai", "Andrew M", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): A public-access intensive care unit database.", "author": ["Saeed", "Mohammed", "Mauricio Villarroel", "Andrew T. Reisner", "Gari Clif- ford", "Li-Wei Lehman", "George Moody", "Thomas Heldt", "Tin H. Kyaw", "Benjamin Moody", "Roger G. Mark"], "venue": "DOI: 10.1097/CCM.0b013e31820a92c6", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "With the rapid rise in Electronic Health Records (EHRs) available for analysis, data-driven models are well-suited to aid physicians in making decisions about when patients should be treated with or weaned off certain interventions [1].", "startOffset": 232, "endOffset": 235}, {"referenceID": 1, "context": "Achieving high-quality care depends on a robust understanding of the patient\u2019s underlying acuity throughout time [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 91, "endOffset": 97}, {"referenceID": 3, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 191, "endOffset": 197}, {"referenceID": 5, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 191, "endOffset": 197}, {"referenceID": 6, "context": "The efficacy of interventions can drastically vary from patient to patient, and unnecessarily administering an intervention can be harmful and expensive [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Previously, multitask gaussian processes have been tested for modelling patient acuity but only in Traumatic Brain Injury (TBI) patients [8] or only using longitudinal billing data [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "Previously, multitask gaussian processes have been tested for modelling patient acuity but only in Traumatic Brain Injury (TBI) patients [8] or only using longitudinal billing data [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "Previous work has attempted to use autoencoders on random 30-day patches of input vectors to learn underlying patient phenotypes [9], and has shown that they can extract relevant and useful features about patients.", "startOffset": 129, "endOffset": 132}, {"referenceID": 9, "context": "It is also valuable to consider signals that occur throughout a patient\u2019s stay, since there are often long delays between when relevant signals are expressed [10].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "They were recently used as an initialization step for recurrent neural networks for text classification [14], but have not been applied to the clinical space.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Features were extracted from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC III) Database [15].", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.", "creator": "LaTeX with hyperref package"}}}