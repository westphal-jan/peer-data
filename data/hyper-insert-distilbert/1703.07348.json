{"id": "1703.07348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "CNN-MERP: An FPGA-Based Memory-Efficient Reconfigurable Processor for Forward and Backward Propagation of Convolutional Neural Networks", "abstract": "large - scale semiconductor deep processing convolutional neural networks ( cnns ) vendors are widely used in machine learning applications. while network cnns overwhelmingly involve huge complexity, vlsi ( asic flash and fpga ) chips that deliver significant high - density integration often of small computational computer resources are regarded as a promising networking platform for funding cnn's implementation. firing at massive parallelism rate of computational units, however, speaking the wasted external memory interface bandwidth, since which is very constrained severely by the constraint pin count behavior of the vlsi chip, becomes technically the consensus system bottleneck. [ moreover, vlsi embedded solutions are usually regarded as a colossal lack incapable of robust the flexibility to openly be strongly reconfigured for the various operating parameters of cnns. this aforementioned paper easily presents cnn - infected merp to fairly address twice these issues. online cnn - merp incorporates an efficiency efficient computational memory hierarchy feature that even significantly reduces the bandwidth requirements from various multiple optimizations equally including on / off - chip data allocation, deep data flow frequency optimization and data reuse. for the controversial proposed cnn 2 - level display reconfigurability is publicly utilized online to enable fast read and generally efficient reconfiguration, encoding which is mostly based on the event control pattern logic encoding and utilizing the multiboot adaptive feature design of symmetric fpga. as in a side result, an improved external kernel memory edge bandwidth requirement capable of [UNK] 1. 94mb / gflop efficiency is unlikely achieved, which is 55 % lower compared than prior arts. reading under limited dram bandwidth, a realistic system throughput cap of n 1244gflop / 32 s, is occasionally achieved at the zebra vertex ultrascale platform, which arguably is $ 5. 48 db times higher critical than generally the current state - of - the - living art fpga implementations.", "histories": [["v1", "Wed, 22 Mar 2017 01:31:23 GMT  (1675kb)", "http://arxiv.org/abs/1703.07348v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AR", "authors": ["xushen han", "dajiang zhou", "shihao wang", "shinji kimura"], "accepted": false, "id": "1703.07348"}, "pdf": {"name": "1703.07348.pdf", "metadata": {"source": "CRF", "title": "CNN-MERP: An FPGA-Based Memory-Efficient Reconfigurable Processor for Forward and Backward Propagation of Convolutional Neural Networks", "authors": ["Xushen Han", "Dajiang Zhou", "Shihao Wang", "Shinji Kimura"], "emails": ["hanxushen@fuji.waseda.jp"], "sections": [{"heading": null, "text": "CNN-MERP: An FPGA-Based Memory-Efficient Reconfigurable Processor for Forward and Backward\nPropagation of Convolutional Neural Networks\nXushen Han, Dajiang Zhou, Shihao Wang, and Shinji Kimura Graduate School of Information, Production and Systems, Waseda University\n2-7 Hibikino, Wakamatsu-ku, Kitakyushu, Fukuoka 808-0135, Japan hanxushen@fuji.waseda.jp\nAbstract\u2014Large-scale deep convolutional neural networks (CNNs) are widely used in machine learning applications. While CNNs involve huge complexity, VLSI (ASIC and FPGA) chips that deliver high-density integration of computational resources are regarded as a promising platform for CNN\u2019s implementation. At massive parallelism of computational units, however, the external memory bandwidth, which is constrained by the pin count of the VLSI chip, becomes the system bottleneck. Moreover, VLSI solutions are usually regarded as a lack of the flexibility to be reconfigured for the various parameters of CNNs. This paper presents CNN-MERP to address these issues. CNN-MERP incorporates an efficient memory hierarchy that significantly reduces the bandwidth requirements from multiple optimizations including on/offchip data allocation, data flow optimization and data reuse. The proposed 2-level reconfigurability is utilized to enable fast and efficient reconfiguration, which is based on the control logic and the multiboot feature of FPGA. As a result, an external memory bandwidth requirement of 1.94MB/GFlop is achieved, which is 55% lower than prior arts. Under limited DRAM bandwidth, a system throughput of 1244GFlop/s is achieved at the Vertex UltraScale platform, which is 5.48 times higher than the state-of-the-art FPGA implementations.\nKeywords\u2014convolutional neural networks; FPGA; memory bandwidth; reconfigurable processor; backward propagation\nI. INTRODUCTION State-of-the-art CNNs are demonstrating high performance and flexibility in a wide range of applications including video surveillance, object recognition and mobile robot vision [1]\u2013[3]. In recent years there is a growing tendency to take advantage of large-scale and deep convolutional neural networks. For example, in AlexNet [4], a deep CNN plays a crucial role in significantly improving image identification accuracy. A CNN is generally composed of several cascades of convolutional, activation and pooling layers, with an example given in Figure 1. Currently, the depth and the number of feature maps in layers keep increasing, which lead to tremendous computing workloads, in both the forward and backward propagation phases. With two NVIDIA GTX 580 3GB GPUs, the training of AlexNet still took about six days to finish the 90 epochs, which are necessary for a satisfactory model.\nMany of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7]. These frameworks are designed for easy adjustment of the\nstructures of neural networks. From the implementation\u2019s point of view however, dedicated architectures for CNNs have a better potential for a higher throughput at the same area as well as a higher energy efficiency. Hence FPGA and ASIC implementations have come to researchers\u2019 attention. Previous FPGA/ASIC architectures already achieved a throughput of several hundreds of Gop/s. The computational components of these architectures are also extensible, i.e. higher performance can be achieved by leveraging parallelism. The current Xilinx VU13P FPGA already contains over 10 thousand DSPs, which are promising for delivering a 4TFlop/s throughput at a moderate 200MHz clock rate. However, most of these designs are still faced with two problems:\n1) While the density of VLSI chips allows a massive parallelism of computation, external memory bandwidth becomes the system bottleneck.\n2) With deep CNNs comprising many layers with different characteristics, it is particularly difficult to make a single architecture reusable by and optimum for all the layers.\nIn this paper, we present CNN-MERP, an FPGA-based CNN processor for both forward and backward propagation, addressing the above issues with the following contributions.\n1) A highly memory-efficient architecture for the highthroughput CNN processor.\n2) 2-level reconfigurability for forward and backward propagation of different layers in CNNs.\nThe rest of this paper is organized as follows. Section 2 introduces related work. We give an overview of CNNs in Section 3. Strategies of external bandwidth reduction are shown in section 4. The detailed hardware design and reconfigurability are introduced in Section 5 and Section 6, respectively. The results of implementation are given in Section 7 and the final section is for conclusion and future work.\nII. RELATED WORK FPGA-based CNN acceleration has been discussed in several previous papers. Chakradhar, et al. [8]\u2019s 16-bit processor can automatically analyze workloads and configure hardware for higher speed. To optimize the throughput with limited memory bandwidth, a roofline-model-based design was proposed by Chen Zhang, et al [9]. Their implementation can reach 61.62Gflop/s with a 1.55GB/s external memory. To\noptimize the usage of on-chip memory, in 2013, Maurice et al [18] take advantage of high-level synthesis tools to develop a configurable accelerator template, which comprises 140MACC with 150MB/s memory. In 2011, NeuFlow, an 160Gop/s processor was published by Farabet Cl\u00e9ment, et al. [10]. Another implementation is nn-X, proposed by Vinayak Gokhale, et al. [11]. This processor achieves 227Gop/s with the 7.6GB/s DMA channels.\nAnother choice for accelerating CNNs is to use ASIC implementation, e.g. NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15]. Since ASICs provide higher density, the bottleneck of the external memory is even more critical. To reduce the traffic, one idea is storing all feature maps and kernels on chip. In Dadiannao, Chen et al. [16] proposed to store the whole network on chip with eDRAM. Such a design style, however, imposes a critical constraint on the scale of CNNs that can be implemented, which makes it less practical for ultralarge-scale networks. Another idea is reusing input data. In 2016, Yu-Hsin Chen, et al [17] published a processor Eyeriss, which takes advantage of an 108KB on-chip memory to reduce normalized bandwidth requirement to 4.31MB/Gop.\nIf we apply the current design style into a training process of a large-scale CNNs, e.g. the AlexNet [4], 32-bit operands and more processing elements are necessary, since the requirement for higher precision and workloads have been demonstrated in training. While a high-performance FPGA like VU13P enables a maximum computational throughput of 4TFlop/s, the requirement of memory bandwidth correspondingly reaches 4.31MB/Gop\u00d74Top/s\u00d7(32b/16b)= 34.48GB/s, which is too heavy to attain with current mainstream DDR4 (19.2GB/s) memory.\nIII. CONVOLUTIONAL NEURAL NETWORKS Typically, modern CNNs are composed of some basic layers, i.e. convolutional layers, activation layers and pooling layers. The Training process updates kernels to build up specific functionalities via 2 phases: the forward propagation (FP), and the backward propagation (BP). The FP focuses on the prediction of input images, while the BP updates kernels with corresponding partial derivatives of a cost function."}, {"heading": "A. Forward propagation phase", "text": "Convolutional layers accept n input feature maps to produce m output feature maps. As shown in Figure 2, k\u00d7k elements in an input feature map are combined into an input window, which is passed through a filter. A filter is defined as the convolutional operations for a pair of input window and kernel. The output of a filter is a partial result of output element, which is shown in following equation.\n[ ] [ ] [ ] 1 1\n0 0 , , ,\nk k\npartial i j\ny r c x r i c j Ker i j - -\n= =\n= + + \u00d7\u00e5\u00e5 ,\nwhere [r,c] corresponds coordinates in the output feature map y. Ker means kernels. A Group of filters contributes the same output element of y, while Co-located filters are defined as the filters in different groups concerning the same window.\nActivation layers provide non-linearity using activation functions. Currently, a popular choice of activation function is the Rectified Linear Units (ReLU). In [19], Vinod Nair, et al stated that ReLU has better generalized ability to prevent training model from saturating. In the meanwhile, the ReLU is easy to implement compared to other exponent-based functions.\nPooling layers compute the average value of p\u00d7p neighboring elements in the same feature map. As shown in Figure 1, there is no interconnection among different feature maps so they can compute subsampling results independently.\nFigure 2 convolutional layer in forward propagation phase. Grey filters are composed of co-located filters. n co-located windows and n filters in one group determine one element in output feature map. m group of filters corresponds m co-located output elements"}, {"heading": "B. Backward propagation phase", "text": "The BP phase includes two steps: \u03b4 propagation and kernel updating. \u03b4 is defined as the partial derivative of the cost function J with respected to the output feature map y in FP. \u03b4's can be determined via propagations due to the propagating relation of y between layers and the chain rule of the partial derivatives. Since each element of y owns derivative, the dimensions of \u03b4\u2019s and y\u2019s keep the same. However, the calculations of \u03b4 propagation are different.\nConvolutional layers are similar as those in FP phase. Differences include 180-degrees-rotated kernels and input \u03b4 maps with zero padding of k-1. Activation layers accept outputs of corresponding activation layers in FP to calculate derivatives of ReLU. Then the derivatives and inputs are multiplied to get outputs. Pooling layers up-sample the input \u03b4 maps. One element in inputs should be multiplied by 1/(p\u00d7p), then the one result is copied to p\u00d7p neighboring outputs identically.\nThe kernel updating (KU) step determines the partial derivatives of kernels which are utilized for optimization. The algorithm of optimization is the gradient decent, which is shown in the following equation,\n[ ] [ ] [ ] [ ] ( ), , , - , - [ , ] [ , ] , r c JKer i j Ker i j Ker i j r c x r i c j Ker i j d a a d \u00ce \u00b6 = = \u00d7 + +\n\u00b6 \u00e5 where \u03b1 is a constant of learning rate, x[r+i,c+j] corresponds an input window. Hence the partial derivatives of kernels can be determined by windows of input feature maps and corresponding elements of \u03b4 maps.\nIV. ANALYSIS OF MEMORY TRAFFIC"}, {"heading": "A. Memory traffic evaluation", "text": "The parallelism of computational units (CUs) determines both the throughput and the bandwidth requirement of a processor. According to the structure of the layer, filters process data independently. As a result, The CU, which is designed for operations in a filter can be arranged in parallel for higher throughput. Figure 3 shows the arrangement of three parallel CUs with a detailed design of multiplications and additions. Considering operations in a filter, all multiplications are in parallel and pipelined with additions. This parallelism leverages high throughput\nwith the cost of heavy bandwidth requirement, which is discussed in the following section.\nWe use a typical convolutional layer, followed with activation and pooling layers as an example to evaluate the external memory traffic. These layers are the 2nd layers from the AlexNet [4] that is shown in Table 1. Two characters should be noticed. One is the convolutional layer involves the majority of computation: Three layers include 28.70G operations in total, while the convolutional layer takes up 28.67G of it. The other character is the duplicated data loading: 65.7MB off-chip data result in 117.0GB external traffic. We try to reduce the external traffic from multiple aspects including on/off-chip allocation, data flow optimization and data reuse, which are realized via the following five strategies."}, {"heading": "B. Strategy 1: on/off-chip data allocation", "text": "A perceived wisdom for reduction of memory traffic is to allocate a moderate amount of data on chip. Theoretically speaking, if there exists a powerful implementation which has ability to cover all data on chip, except for the data initialization, there is no external traffic required. Nevertheless, for our FPGA, such allocation is impractical because of space limitation. In the typical layer shown in Table 1, kernels require 614.4KB memory, while the demand of feature maps is 17.9MB+ 47.8MB\u00d74+11.0MB =220.1MB. In the meanwhile, caching all kernels and caching all feature maps contribute to the same degree of traffic reduction -- either of them reduces half of the inputs, since every multiplication only need one off-chip operand. In comparison, storing all kernels on chip is more appropriate. As a result, the external traffic for computation only comes from the feature maps.\nSuch a strategy can reduce half of external traffic of inputs, which makes the normalized traffic decrease to (114.7GB\u20442+ 2.3GB)/28.6GFlop=2085MB/GFlop. In turn, 614.4KB on-chip memory is needed for storage of kernels."}, {"heading": "C. Strategy 2: reuse input windows between filters", "text": "One window (size is k\u00d7k) of input feature maps is shared by co-located filters base on the structures of CNNs. All co-located filters acquire the same window to produce intermediate results of co-located output elements. The conventional method [11] reloads one window for different filters, which aggravates external bandwidth requirement.\nFigure 3 Parallelism within computational units (CU) and parallelism between CUs.\nFigure 4 illustrates the data reuse of an active window. m Colocated filters corresponding the current active window are processed (green part) by CUs continuously. The active window is kept with a 32\u00d7k\u00d7k-bit register so that CUs can access this window simultaneously.\nThis data-reuse strategy can reduce input memory traffic to 1/m of the conventional. The normalized traffic decreases to (114.7GB\u20442/128+2.3GB)/ 28.6GFlop=96.1MB/GFlop."}, {"heading": "D. Strategy 3: avoid off-chip traffic of intermediate results", "text": "The strategy 2 generates intermediate results of output elements, while the strategy 3 accumulates the intermediate to avoid the off-chip traffic. For the 2nd convolutional layer of the AlexNet [4], 198.2MB memory space is needed.\nFigure 4 includes an efficient data flow based on accumulation of the co-located outputs. Accumulators are designed for m outputs, which correspond all output feature maps. To accumulate outputs, the selection of input windows must jump between co-located windows, since those windows contribute to the same group of output elements. Particularly in our design, we determine the order of jumps from the first to the last. After that, calculations related to one group of co-located output elements are finished. Hence these elements can be streamed out for the next new elements.\nThe accumulators for the co-located elements take up 32\u00d7mbits registers. Only 1/n original data are streamed out after accumulation. As a result, the normalized traffic decreases to (114.7GB\u20442/128+2.3GB/48)/ 28.6GFlop=17.3MB/GFlop."}, {"heading": "E. Strategy 4: reuse input elements in a feature map", "text": "Adjacent k\u00d7k-size windows in convolutional layers have 2k overlapped elements. In our consideration, most of overlapped elements can be reused for next window if we promise flow of window are adjacent as far as possible.\nFigure 5 shows the overlapped relation between 3\u00d73 windows. we decide the window swift from the left to the right. Due to the dependency, the k lines where the current window is located can be cached on chip. When next window is required, only one extra new element rather than the whole window is streamed in. To avoid conflicts with strategy 2 and 3, we decide to cache k-lines in all input feature maps. As shown in Figure 4, when one group of co-located output elements are finished, we turn next adjacent co-located window of input feature maps.\nCache is required for all input feature maps. Hence a total of m\u00d7k\u00d7Cin\u00d74B on-chip memory are needed, where Cin means the number of columns of input feature maps. As the typical convolutional layer shown in Table 1, traffic of inputs can be reduced to 1/(5\u00d75) of the original with the memory cost of 14.16KB SRAM."}, {"heading": "F. Strategy 5: super layer integration", "text": "Strategies 2-4 reuse data in convolutional layers. Between layers, redundancy of load/save of feature maps still exists, since the output and input feature maps of adjacent layers are exactly same. Moreover, the structure of CNNs can be decomposed into several cascades of convolutional layers, followed with activation and pooling layers. As a result, we combine each cascade of the three layers as a super layer to reduce memory traffic between them.\nFigure 4 Data flow for strategies 2-4. Solid squares in input feature maps describe current co-located windows, while dash squares stand for next adjacent co-located windows. Reuse in a feature map corresponds black overlapped part. The active windows are reused by co-located filters (green part), which corresponds strategy 2. The data flow of strategy 3 and 4 are shown as red and blue arrows, respectively.\nFigure 5 Data reuse in adjacent windows. Totally k=3 lines of input feature maps are cached on chip, which is optimized for data loading. Only one next input element is needed for the new window.\nFigure 6 shows the reduction based on previous strategies. The total memory traffic is reduced by 3976x relative to a nocache design. The strategies above can promise that all input/output feature maps are loaded/saved only once. The normalized bandwidth requirement with above strategies decreases to 29.6MB/28.6GFlop=1.01MB/GFlop. This efficiency is enough to support the acceleration of the FPGA resources with a mainstream memory interface.\nV. IMPLEMENTATION OF CNN-MERP"}, {"heading": "A. Processor overview", "text": "Our implementation incorporates the functionalities of super layers, which are mapped into the FPGA. Figure 7 shows an overview of CNN-MERP. All kernels are loaded into the SRAM prior to the execution of a batch. For the convenience of kernel transfer, each computational unit is equipped with one SRAM to store all related data. We also design an input memory hierarchy corresponding the strategy 2-4 introduced in section IV."}, {"heading": "B. Computational units", "text": "The implementation of computational units is based on the parallelism evaluated in section 4. As shown in Figure 8, We arrange k\u00d7k floating-point multipliers in parallel, with k\u00d7k-1 floating-point adders followed. In order to get higher throughput, we pipeline adders and multipliers into two stages.\nC. Input memory hierarchy An imbalanced-data-width problem exists according to strategy 2 and 4. On one hand, in every clock cycle, input memory\nonly gets co-located elements for one location. On the other, CUs need k\u00d7k-location input windows. Hence we use k\u00d7k twoport SRAM banks to cache the input data. As shown in Figure 9, in each clock cycle, one of the SRAM banks gets a new element from the router, meanwhile each SRAM bank contributes one element as the input window. As windows shifted, we implement a logic circuit to restore the location of windows."}, {"heading": "D. Activation and pooling", "text": "Figure 10 shows the implementation of activation and pooling layers. ReLUs and Pooling units (PUs) are designed for calculation of each element. Because co-located output elements from the convolutional layers are generated simultaneously, we employ R ReLU and PU to calculate co-located elements in feature maps in parallel.\nThe parallelism of ReLU and PU depends on throughput of CUs. In the typical 2nd layer of AlexNet shown in Table 1, we implement 16 CUs, which takes 48\u00d7128\u204416=384 clock cycles to work out 128 parallel-out elements. Hence PUs have 384 clock cycles to finish subsampling of 128 output elements. To ensure throughput, we implement R=2 ReLUs and PUs.\nFigure 7 Architecture of CNN-MERP. This architecture works for FP of CNN. The reconfigured architectures for BP and KU contain differences, which are discussed in Section VI.\nFigure 8 Hardware implementation of computational units for k\u00d7k kernel. Here, k=2 in the typical layer of AlexNet, k=3.\nFigure 9 k\u00d7k kernel example for input memory hierarchy. k=3. The input element is cached into the memory bank with the same coordinates. Since the location of the window is shifted, while the locations of banks in window are fixed, we use a module to convert the coordinates.\nVI. PROCESSOR RECONFIGURATION FOR FP AND BP"}, {"heading": "A. Combining two levels of reconfigurability", "text": "The compatibility of different layers for the same hardware is a difficult problem, to which there are two solutions in general. One solution takes advantage of reconfigurable functionality of FPGA, but results in the cost of time on reconfiguration. The other is based on logic circuits to support the new desirable data flow on chip. This solution has an unsatisfied utilization of computational resources when the size of kernels changes. Because the size of CUs is hard to change, we must implement a large-size CU to support all smaller-size kernels, which leaves much idle multipliers and adders when size of kernel is small. Hence we combine two levels of configurability:\nWhen the sizes of kernels do not change, we implement a module to alter the data flow so that the idleness of CUs is omitted as far as possible.\nWhen the sizes of kernels or propagation styles change, we load a new bit stream configuration into the FPGA, so that the new CUs keep the same size of kernel in current super layer."}, {"heading": "B. Logic based reconfiguration", "text": "Logic modules are implemented to monitor the current active input and output feature maps, which prevent CUs from invalid inputs/outputs. The invalid is resulted from the mismatching of the real number and the supported number of input/output feature maps. A naive idea is we treat all invalid inputs as 0. This method can get the correct results of computation, however, many zeros are passed to CUs, which reduce the effective performance.\nOur solution is to control the data flow so that invalid inputs/outputs are not involved in computation. Registers are designed for current index of co-located windows and output elements. The current index must vary in the valid range, i.e. the real number of input and output feature maps. These two numbers are also from the outside and can be updated for a new layer. As a result, CUs deliver a higher effective throughput in different layers. For example, the 3rd, 4th and 5th layers in\nAlexNet contain the same size of kernel so we use the largestscale layer to support three layers without FPGA based reconfiguration (256 input feature maps and 384 output feature maps). We implement 48 CUs. The efficiency of the directed idea is 100%, 37.5%, 25%, while in our design, the efficiency keeps 100%, 100%, 88.9%."}, {"heading": "C. Multi-boot based reconfiguration of FPGA", "text": "Multi-boot based reconfiguration takes advantage of the programmability of FPGA itself. Several editions of bit streams can be accessible by the same FPGA. In [20], Xilinx FPGA can be reconfigured from flash on chip, which results in a different functionality.\nMulti-boot is realized by the BPI interface on the FPGA board. Before multi-boot, we store all bit streams, each of which corresponds to the functionality of one or more super layers in the flash and record corresponding addresses. When calculation of one super layer is finished, the MicroBlaze soft microprocessor core controls loading of a next layer. Figure 13 shows the flow of reconfiguration. The HWICAP IP core of corresponding layer is triggered by the MicroBlaze soft microprocessor when the next bit stream should be loaded.\nCurrent Xilinx Kintex 325T FPGA takes about 0.7s to process the 2nd super layer of the AlexNet with a batch size of 128. With 16-bit BPI interface at 66MHz for transmitting bit files, about 0.087s are spent for receiving data for reconfiguration of one super layer. The overhead of reconfiguration is 11%.\nFigure 11 Multiboot reconfiguration flow of FPGA. For our FPGA one version of bit file takes up 11.4MB.\nFigure 12 Illustration of data operating flow diagram. Operations in a bold black box are processing on one FPGA hardware system. Green arrows mean offchip data traffic. In the same layer, feature maps, kernels, \u03b4 maps keep the same in different phases, i.e. the input feature maps of FP are also passed into KU step. The output \u03b4 maps of \u03b4 propagation is the input of KU."}, {"heading": "D. Reconfiguration flow for FP/BP", "text": "The CNN-MERP supports both of FP and BP with the data flow shown in Figure 12. FP comprises the lth super layer which is combined with lth convolutional layer, lth activation layer and lth pooling layer. While BP combines the (l+1)th convolutional layer, lth pooling layer and lth activation layer as the (l+1)th super layer. The different order in BP is due to the inversed layers and the data dependency for the KU. Since the output \u03b4 maps are inputs of kernel updating, super layers should be separated by external traffic of \u03b4 so that \u03b4 can be stored outside. Due to the design above, \u03b4 of the 1st layer is calculated in the 2nd super layer. Hence there is no \u03b4 propagation of the 1st layer.\nVII. IMPLEMENTATION RESULTS The CNN-MERP as described is implemented on Xilinx Kintex-7 xc7k325tffg900-2 platform. The development was done with Vivado 2015. We program different bit streams for every super layer in AlexNet [4]. Each super layer is operated separately. Since the kernel size of the 3rd 4th and 5th super layers are the same, the largest-scale 3rd-super-layer design is compatible to any of them.\nThe CNN-MERP has well extensibility which makes parallelism can be easily improved on a larger platform. Table 2 shows implementation results of the 2nd layer. We use a larger Xilinx Vertex UltraScale xcvu440 platform to extend our de-\nsign. Figure 13 shows the comparison of throughput after synthesis in each layer. The throughputs are increased to over 10x. The overall throughput of UltraScale FPGA is 1244GFlop/s, which is 5.48 times larger than state-of-the-art FPGA works. Since the two-level reconfiguration are utilized, the throughput keeps high for all layers.\nTable 3 compares the performance of memory traffic reduction. Our implementation and Eyeriss both take AlexNet as an example to optimize memory traffic. The CNN-MERP outperform in each layer because of strategies proposed and the overall normalized bandwidth requirement is 1.94MB/GFop, which is 55.0% lower than Eyeriss. The first layer performs larger requirement because of the stride exists in convolutional layer. The stride means the distances of two adjacent windows. Usually, stride equals to one which is performed in 2nd -5th layer.\nBecause the stride of 1st layer takes 4, one output is obtained by 16 input elements. Hence larger requirement is needed to support the same throughput. Our design support both FP and BP. Due the structure of CNNs, more data are needed in \u03b4 propagation and KU. Hence the normalized bandwidth requirement is not as well as that of FP.\nTable 3 Comparison with Eyeriss on external bandwidth requirement. Since the training process requires 32-bit operation, Double normalized Bandwidth in this table are needed for Eyeriss-style processor.\nLayer Operations\n(Gop/s)\nEyeriss [17] (MB/Gop)\nCNN-MERP(32-bit) (MB/GFlop)\nNormalized BW of FP (16-bit) Normalized\nBW of FP\nNormalized\nBW of \u03b4P\nNormalized\nBW of KU"}, {"heading": "1 27.01 7.11 4.18 --- 8.36", "text": ""}, {"heading": "2 57.34 3.13 1.01 4.25 2.29", "text": ""}, {"heading": "3 38.27 4.26 1.45 3.37 1.45", "text": ""}, {"heading": "4 28.74 4.21 2.31 2.31 2.31", "text": ""}, {"heading": "5 19.14 4.13 1.98 2.89 2.89", "text": "Total 170.50 4.31 1.94 3.45 3.92\nTable 2 Implementation results of 2nd super layer in AlexNet Reso-\nurce\n(slices)\nKintex Kintex-7 Vertex UltraScale\nFP \u03b4P KU FP \u03b4P KU\nLUT 182367 178435 173195 1505983 1356150 1302193 FF 121498 114082 117959 854134 868097 850025\nBRAM 213 238 209 1526 1838 1498 DSP 413 408 405 2848 2848 2848\nThe comparison with other related works is shown in Table 4. Our implementation support both FP and BP, while other works only take FP into consideration. Taking the effect of stride into account, the overall normalized bandwidth requirement of FP is 45.7% lower than previous FPGA implementations. We also evaluate the maximum acceptable throughput in extensions, which is shown in Figure 14. With the support of current DDR4 memory (highest 19.2GB/s), three works are unable to reach 1Top/s. [18] meets the bottleneck at 5.37Top/s. CNN-MERP can reach 9.90Top/s.\nVIII. CONCLUSION In this paper we focus on memory hierarchy in the CNNMERP, a CNN processor. Recently for high-throughput hardware solution for CNNs, memory traffic becomes the bottleneck of acceleration. CNN-MERP is not only high-throughput but also memory-efficient. Our implementation reaches the performance of 113GFlop/s and 1.94MB/GFlop. The normalized bandwidth requirement is 45.7% lower than the state-of-the-art work. We also extend the design to a larger scale. As a result, 1244GFlop/s is achieved, which is 5.48 times larger than previous works.\nCNN-MERP can process both forward and backward propagation in CNNs. We can fully utilize hardware resources across layers with two-level reconfiguration. Our future work is to enhance the computational density in hardware to attain higher speed of acceleration for CNNs. Another problem is to utilize multiple FPGAs to accelerate CNNs.\nREFERENCES [1] F. Porikli, F. Br\u00e9mond, S. L. Dockstader, J. Ferryman, A. Hoogs, B. C.\nLovell, S. Pankanti, B. Rinner, P. Tu, and P. L. Venetianer, \u201cVideo Surveillance: Past, Present, and Now the Future,\u201d IEEE Signal Process. Mag., vol. 30, no. 3, 2013, pp. 190\u2013198.\n[2] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, \u201cFace recognition: a convolutional neural-network approach.,\u201d IEEE Trans. Neural Netw., vol. 8, no. 1, Jan. 1997, pp. 98\u2013113.\n[3] C. Farabet, C. Poulet, J. Y. Han, and Y. LeCun, \u201cCNP: An FPGAbased processor for Convolutional Networks,\u201d in 2009 International Conference on Field Programmable Logic and Applications, 2009, pp. 32\u201337.\n[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification with Deep Convolutional Neural Networks,\u201d in Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.\n[5] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \u201cCaffe,\u201d in the ACM International Conference on Multimedia - MM \u201914, 2014, pp. 675\u2013678.\n[6] R. Collobert, \u201cTorch7: A matlab-like environment for machine learning,\u201d BigLearn, NIPS Workshop, 2011.\n[7] S. Tokui, K. Oono, S. Hido, C. S. A. S. Mateo, and J. Clayton, \u201cChainer: a Next-Generation Open Source Framework for Deep Learning,\u201d learningsys.org.\n[8] S. Chakradhar, M. Sankaradas, V. Jakkula, and S. Cadambi, \u201cA dynamically configurable coprocessor for convolutional neural networks,\u201d ACM SIGARCH Comput. Archit. News, vol. 38, no. 3, Jun. 2010, p. 247.\n[9] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong, \u201cOptimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks,\u201d in 2015 ACM/SIGDA FPGA \u201915, 2015, pp. 161\u2013170.\n[10] C. Farabet, B. Martini, B. Corda, P. Akselrod, E. Culurciello, and Y. LeCun, \u201cNeuFlow: A runtime reconfigurable dataflow processor for vision,\u201d in 2011 IEEE CVPR WORKSHOPS, 2011, pp. 109\u2013116.\n[11] V. Gokhale, J. Jin, A. Dundar, B. Martini, and E. Culurciello, \u201cA 240 G-ops/s Mobile Coprocessor for Deep Neural Networks,\u201d in IEEE CVPR Workshops, 2014, pp. 682\u2013687.\n[12] P.-H. Pham, D. Jelaca, C. Farabet, B. Martini, Y. LeCun, and E. Culurciello, \u201cNeuFlow: Dataflow vision processing system-on-a-chip,\u201d in 2012 IEEE 55th International Midwest Symposium on Circuits and Systems, 2012, pp. 1044\u20131047.\n[13] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam, \u201cDianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning,\u201d ACM SIGARCH Comput. Archit. News, vol. 42, no. 1, Apr. 2014, pp. 269\u2013284.\n[14] L. Cavigelli, D. Gschwend, C. Mayer, S. Willi, B. Muheim, and L. Benini, \u201cOrigami: A Convolutional Network Accelerator,\u201d in the 25th edition on Great Lakes Symposium on VLSI - GLSVLSI \u201915, 2015, pp. 199\u2013204.\n[15] L.-S. K. J. Sim, J-S. Park, M. Kim, D. Bae, Y. Choi, \u201cA 1.42tops/w deep convolutional neural network recognition processor for intelligent ioe systems,\u201d IEEE Int. Solid-State Circuits Conf. , 2016, pp. 1\u201343.\n[16] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu, N. Sun, and O. Temam, \u201cDaDianNao: A Machine-Learning Supercomputer,\u201d in 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 2014, pp. 609\u2013622.\n[17] Y. Chen, T. Krishna, J. Emer, and V. Sze, \u201cEyeriss : An EnergyEfficient Reconfigurable Accelerator for Deep Convolutional Neural Networks Future of Deep Learning Recognit ion DCNN Accelerator is Crucial \u2022 High Throughput for Real-time,\u201d IEEE Int. Solid-State Circuits Conf. , Feb. 2016, pp. 1\u201343.\n[18] M. Peemen, A. A. A. Setio, B. Mesman, and H. Corporaal, \u201cMemorycentric accelerator design for Convolutional Neural Networks,\u201d in 2013 IEEE 31st ICCD, 2013, pp. 13\u201319.\n[19] V. Nair and G. E. Hinton, \u201cRectified Linear Units Improve Restricted Boltzmann Machines,\u201d in the 27th ICML, 2010, no. 3, pp. 807\u2013814.\n[20] K. Arshak and C. S. Ibala, \u201cMixed Design of Integrated Circuits and Systems, 2008. MIXDES 2008. 15th International Conference on,\u201d Mixed Design of Integrated Circuits and Systems, 2008. MIXDES 2008. 15th International Conference on, 2008. pp. 619\u2013623."}], "references": [{"title": "Video Surveillance: Past, Present, and Now the Future", "author": ["F. Porikli", "F. Br\u00e9mond", "S.L. Dockstader", "J. Ferryman", "A. Hoogs", "B.C. Lovell", "S. Pankanti", "B. Rinner", "P. Tu", "P.L. Venetianer"], "venue": "IEEE Signal Process. Mag., vol. 30, no. 3, 2013, pp. 190\u2013198.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Face recognition: a convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "IEEE Trans. Neural Netw., vol. 8, no. 1, Jan. 1997, pp. 98\u2013113.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "CNP: An FPGAbased processor for Convolutional Networks", "author": ["C. Farabet", "C. Poulet", "J.Y. Han", "Y. LeCun"], "venue": "2009 International Conference on Field Programmable Logic and Applications, 2009, pp. 32\u201337.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Caffe", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "the ACM International Conference on Multimedia - MM \u201914, 2014, pp. 675\u2013678.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert"], "venue": "BigLearn, NIPS Workshop, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Chainer: a Next-Generation Open Source Framework for Deep Learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "C.S.A.S. Mateo", "J. Clayton"], "venue": "learningsys.org.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 0}, {"title": "A dynamically configurable coprocessor for convolutional neural networks", "author": ["S. Chakradhar", "M. Sankaradas", "V. Jakkula", "S. Cadambi"], "venue": "ACM SIGARCH Comput. Archit. News, vol. 38, no. 3, Jun. 2010, p. 247.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "2015 ACM/SIGDA FPGA \u201915, 2015, pp. 161\u2013170.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "NeuFlow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "2011 IEEE CVPR WORKSHOPS, 2011, pp. 109\u2013116.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks", "author": ["V. Gokhale", "J. Jin", "A. Dundar", "B. Martini", "E. Culurciello"], "venue": "IEEE CVPR Workshops, 2014, pp. 682\u2013687.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "NeuFlow: Dataflow vision processing system-on-a-chip", "author": ["P.-H. Pham", "D. Jelaca", "C. Farabet", "B. Martini", "Y. LeCun", "E. Culurciello"], "venue": "2012 IEEE 55th International Midwest Symposium on Circuits and Systems, 2012, pp. 1044\u20131047.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam"], "venue": "ACM SIGARCH Comput. Archit. News, vol. 42, no. 1, Apr. 2014, pp. 269\u2013284.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Origami: A Convolutional Network Accelerator", "author": ["L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "B. Muheim", "L. Benini"], "venue": "the 25th edition on Great Lakes Symposium on VLSI - GLSVLSI \u201915, 2015, pp. 199\u2013204.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A 1.42tops/w deep convolutional neural network recognition processor for intelligent ioe systems", "author": ["L.-S.K.J. Sim", "J-S. Park", "M. Kim", "D. Bae", "Y. Choi"], "venue": "IEEE Int. Solid-State Circuits Conf. , 2016, pp. 1\u201343.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam"], "venue": "2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 2014, pp. 609\u2013622.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Eyeriss : An Energy- Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks Future of Deep Learning Recognit ion DCNN Accelerator is Crucial \u2022 High Throughput for Real-time", "author": ["Y. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "IEEE Int. Solid-State Circuits Conf. , Feb. 2016, pp. 1\u201343.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Memorycentric accelerator design for Convolutional Neural Networks", "author": ["M. Peemen", "A.A.A. Setio", "B. Mesman", "H. Corporaal"], "venue": "2013 IEEE 31st ICCD, 2013, pp. 13\u201319.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "the 27th ICML, 2010, no. 3, pp. 807\u2013814.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Mixed Design of Integrated Circuits and Systems, 2008. MIXDES 2008. 15th International Conference on", "author": ["K. Arshak", "C.S. Ibala"], "venue": "Mixed Design of Integrated Circuits and Systems, 2008. MIXDES 2008. 15th International Conference on, 2008. pp. 619\u2013623.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "State-of-the-art CNNs are demonstrating high performance and flexibility in a wide range of applications including video surveillance, object recognition and mobile robot vision [1]\u2013[3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "State-of-the-art CNNs are demonstrating high performance and flexibility in a wide range of applications including video surveillance, object recognition and mobile robot vision [1]\u2013[3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "For example, in AlexNet [4], a deep CNN plays a crucial role in significantly improving image identification accuracy.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Many of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Many of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": "Many of today\u2019s deep CNNs are implemented on GPU platforms, based on which some fast and friendly frameworks are developed such as Caffe [5], Torch [6], and Chainer [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "[8]\u2019s 16-bit processor can automatically analyze workloads and configure hardware for higher speed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "To optimize the throughput with limited memory bandwidth, a roofline-model-based design was proposed by Chen Zhang, et al [9].", "startOffset": 122, "endOffset": 125}, {"referenceID": 17, "context": "optimize the usage of on-chip memory, in 2013, Maurice et al [18] take advantage of high-level synthesis tools to develop a configurable accelerator template, which comprises 140MACC with 150MB/s memory.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "NeuFlow by Phi-Hung Pham, et al [12], Diannao by Tianshi Chen, et al [13], Origami by Lukas Cavigelli [14], and [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "[16] proposed to store the whole network on chip with eDRAM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In 2016, Yu-Hsin Chen, et al [17] published a processor Eyeriss, which takes advantage of an 108KB on-chip memory to reduce normalized bandwidth requirement to 4.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "the AlexNet [4], 32-bit operands and more processing elements are necessary, since the requirement for higher precision and workloads have been demonstrated in training.", "startOffset": 12, "endOffset": 15}, {"referenceID": 18, "context": "In [19], Vinod Nair, et al stated that ReLU has better generalized ability to prevent training model from saturating.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "These layers are the 2 layers from the AlexNet [4] that is shown in Table 1.", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "The conventional method [11] reloads one window for different filters, which aggravates external bandwidth requirement.", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "For the 2 convolutional layer of the AlexNet [4], 198.", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "In [20], Xilinx FPGA can be reconfigured from flash on chip, which results in a different functionality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "We program different bit streams for every super layer in AlexNet [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 16, "context": "Layer Operations (Gop/s) Eyeriss [17] (MB/Gop) CNN-MERP(32-bit) (MB/GFlop) Normalized BW of FP (16-bit) Normalized BW of FP Normalized BW of \u03b4P Normalized BW of KU 1 27.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Work FPGA 2015 [9] NeuFlow [10] nn-X [11] Eyeriss [17] ICCD 2013 [18] Our implementation Precision 32bit float 16bit fixed 32bit float 16bits fixed --- 32bits float FPGA / ASIC Xilinx Vertex 7 VX485T Xilinx Vertex 6 VLX240T Xilinx Zynq XC7Z045 TSMC 65nm Xilinx Vertex6 ML-605 Xilinx Kintex 7 K325T/ Vertex xc7v2000 Frequency 100MHz 200MHz 142MHz 100~250MHz 150MHz 137.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "[18] meets the bottleneck at 5.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Large-scale deep convolutional neural networks (CNNs) are widely used in machine learning applications. While CNNs involve huge complexity, VLSI (ASIC and FPGA) chips that deliver high-density integration of computational resources are regarded as a promising platform for CNN\u2019s implementation. At massive parallelism of computational units, however, the external memory bandwidth, which is constrained by the pin count of the VLSI chip, becomes the system bottleneck. Moreover, VLSI solutions are usually regarded as a lack of the flexibility to be reconfigured for the various parameters of CNNs. This paper presents CNN-MERP to address these issues. CNN-MERP incorporates an efficient memory hierarchy that significantly reduces the bandwidth requirements from multiple optimizations including on/offchip data allocation, data flow optimization and data reuse. The proposed 2-level reconfigurability is utilized to enable fast and efficient reconfiguration, which is based on the control logic and the multiboot feature of FPGA. As a result, an external memory bandwidth requirement of 1.94MB/GFlop is achieved, which is 55% lower than prior arts. Under limited DRAM bandwidth, a system throughput of 1244GFlop/s is achieved at the Vertex UltraScale platform, which is 5.48 times higher than the state-of-the-art FPGA implementations. Keywords\u2014convolutional neural networks; FPGA; memory bandwidth; reconfigurable processor; backward propagation", "creator": "Word"}}}