{"id": "1703.09684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "An Analysis of Visual Question Answering Algorithms", "abstract": "lacking in visual question error answering ( vqa ), usually an assessing algorithm must invariably answer text - based questions about images. while multiple suitable datasets for finding vqa have periodically been created since late 2014, therefore they all have flaws in continually both limiting their content and the way algorithms are evaluated exclusively on them. invariably as a result, specific evaluation algorithm scores are inflated nationally and predominantly biased determined by them answering easier comprehension questions, making it intensely difficult analytical to fundamentally compare different methods. in this paper, together we first analyze existing vqa algorithms using even a totally new dataset. hopefully it contains over 1. 57 6 million accepted questions variously organized into 12 mutually different responses categories. we also introduce simple questions variables that are meaningless for picking a given image medium to force a vqa system to reason about viewing image content. we propose making new model evaluation management schemes that basically compensate for over - represented question - types and make it also easier to formally study considering the semantic strengths and weaknesses of many algorithms. we presently analyze significantly the improving performance of both baseline learning and analytic state - implementation of - the - nature art using vqa models, namely including multi - analytic modal gradient compact phase bilinear inverse pooling ( mcb ), neural module networks, and recurrent answering service units. our experiments firstly establish measures how attention helps certain categories fit more than others, determine which models inherently work horribly better than have others, and explain how normally simple models ( e. some g. mlp ) probably can surpass more - complex models ( larger mcb ) by simply instead learning to actively answer increasingly large, especially easy question responses categories.", "histories": [["v1", "Tue, 28 Mar 2017 17:48:07 GMT  (1488kb,D)", "http://arxiv.org/abs/1703.09684v1", null], ["v2", "Wed, 13 Sep 2017 18:56:45 GMT  (1488kb,D)", "http://arxiv.org/abs/1703.09684v2", "To appear in ICCV 2017. Visitthis http URLto download the dataset"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["kushal kafle", "christopher kanan"], "accepted": false, "id": "1703.09684"}, "pdf": {"name": "1703.09684.pdf", "metadata": {"source": "CRF", "title": "An Analysis of Visual Question Answering Algorithms", "authors": ["Kushal Kafle", "Christopher Kanan", "Chester F. Carlson"], "emails": ["kanan}@rit.edu"], "sections": [{"heading": "1. Introduction", "text": "In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images [19, 3]. VQA is an exciting computer vision problem that requires a system to be capable of many tasks. Truly solving VQA would be a milestone in artificial intelligence, and would significantly advance human computer interaction. However, VQA datasets must test a wide range of abilities for progress to be adequately measured.\nVQA research began in earnest in late 2014 when the DAQUAR dataset was released [19]. Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, \u2018The\nVQA Dataset\u2019 [3], the best algorithms are now approaching 70% accuracy [5] (human performance is 83%). While these results are promising, there are critical problems with existing datasets in terms of multiple kinds of biases. Moreover, because existing datasets do not group instances into meaningful categories, it is not easy to compare the abilities of individual algorithms. For example, one method may excel at color questions compared to answering questions requiring spatial reasoning. Because color questions are far more common in the dataset, an algorithm that performs well at spatial reasoning will not be appropriately rewarded for that feat due to the evaluation metrics that are used.\nContributions: Our paper has four major contributions aimed at better analyzing and comparing VQA algorithms: 1) We create a new VQA benchmark dataset where questions are divided into 12 different categories based on the task they solve; 2) We propose two new evaluation metrics that compensate for forms of dataset bias; 3) We balance the number of yes/no object presence detection questions to assess whether a balanced distribution can help algorithms learn better; and 4) We introduce absurd questions that force\n1\nar X\niv :1\n70 3.\n09 68\n4v 1\n[ cs\n.C V\n] 2\n8 M\nar 2\n01 7\nan algorithm to determine if a question is valid for a given image. We then use the new dataset to re-train and evaluate both baseline and state-of-the-art VQA algorithms. We found that our proposed approach enables more nuanced comparisons of VQA algorithms, and helps us understand the benefits of specific techniques better. In addition, it also allowed us to answer several key questions about VQA algorithms, such as, \u2018Is the generalization capacity of the algorithms hindered by the bias in the dataset?\u2019, \u2018Does the use of spatial attention help answer specific question-types?\u2019, \u2018How successful are the VQA algorithms in answering lesscommon questions?\u2019, and \u2019Can the VQA algorithms differentiate between real and absurd questions?\u2019"}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Prior Natural Image VQA Datasets", "text": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16]. FM-IQA needs human judges and has not been widely used, so we do not discuss it further. Table 1 shows statistics for the other datasets. Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA. Detailed dataset reviews can be found in [13] and [26].\nAll of the aforementioned VQA datasets are biased. DAQUAR and COCO-QA are small and have a limited variety of question-types. Visual Genome, Visual7W, and COCO-VQA are larger, but they suffer from several biases. Bias takes the form of both the kinds of questions asked and the answers that people give for them. For COCO-VQA, a system trained using only question features achieves 50% accuracy [12]. This suggests that some questions have predictable answers. Without a more nuanced analysis, it is challenging to determine what kinds of questions are more dependent on the image. For datasets made using Mechanical Turk, annotators often ask object recognition questions, e.g., \u2018What is in the image?\u2019 or \u2018Is there an elephant in the image?\u2019. Note that in the latter example, annotators rarely ask that kind of question unless the object is in the image. On COCO-VQA, 79% of questions beginning with \u2018Is there a\u2019 will have \u2018yes\u2019 as their ground truth answer.\nIn addition to language bias, these datasets are also biased in their distribution of different types of questions, and their evaluation metrics do not compensate for these biases. Previous VQA datasets use performance metrics that treat each test instance with equal value (e.g., simple accuracy). While some do compute additional statistics for basic question-types, overall performance is not computed from these sub-scores [3, 23]. This exacerbates the issues with the bias because the question-types that are more likely to be biased are also more common. Questions beginning with\n\u2018Why\u2019 and \u2018Where\u2019 are rarely asked by annotators compared to those beginning with \u2018Is\u2019 and \u2019Are\u2019. For example, on COCO-VQA, improving accuracy on \u2018Is/Are\u2019 questions by 15% will increase overall accuracy by over 5%, but answering all \u2018Why/Where\u2019 questions correctly will increase accuracy by only 4.1% [13]. Due to the inability of the existing evaluation metrics to properly address these biases, algorithms trained on these datasets learn to exploit these biases and to answer the easy questions. As a result, they work poorly when deployed in the real-world.\nFor related reasons, major benchmarks released in the last decade do not use simple accuracy for evaluating image recognition and related computer vision tasks, but instead use metrics such as mean-per-class accuracy that compensates for unbalanced categories. For example, on Caltech101 [4], even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy and also had the largest number of test images). Mean perclass accuracy compensates for this by requiring a system to do well on each category, even when the amount of test instances in categories vary considerably.\nExisting benchmarks do not require reporting accuracies across different question-types. Even when they are reported, the question-types can be too coarse to be useful, e.g., \u2018yes/no\u2019, \u2018number\u2019 and \u2018other\u2019 in COCO-VQA. To improve the analysis of the VQA algorithms, we categorize the questions into meaningful types, calculate the sub-scores, and incorporate them in our evaluation metrics."}, {"heading": "2.2. Synthetic Datasets that Fight Bias", "text": "Previous works have studied bias in VQA and proposed countermeasures. In [31], the Yin and Yang dataset was created to study the effect of having an equal number of binary (yes/no) questions about cartoon images. They found that answering questions from a balanced dataset was harder. This work is significant, but it was limited to yes/no questions and their approach using cartoon imagery cannot be directly extended to real-world images.\nOne of the goals of this paper is to determine what kinds of questions an algorithm can answer easily. In [1], the SHAPES dataset was proposed, which has similar objectives. SHAPES is a small dataset, consisting of 64 images that are composed by arranging colored geometric shapes in different spatial orientations. Each image has the same 244 yes/no questions, resulting in 15,616 questions. Although SHAPES serves as an important adjunct evaluation, it alone cannot suffice for testing a VQA algorithm. The major limitation of SHAPES is that all of its images are of 2D shapes, which are not representative of real-world imagery. An unreleased dataset that improves on SHAPES is Compositional Language and Elementary Visual Reasoning (CLEVR) [11], which was described in a recent arXiv\nQ: What color is the suitcase? A: Absurd Q: What color is the man\u2019s hat? A: White Q: What sport is this? A: Tennis Q: What is to the left of the blue bus? A: Car Q: Is there a train in the photo? A: No Q: How many bicycles are there? A: One\nFigure 2: Images from TDIUC and their corresponding question-answer pairs.\npreprint. CLEVR is larger than SHAPES and makes use of 3D rendered geometric objects. In addition to shape and color, it also adds material property to the objects. CLEVR consists of five types of questions: attribute query, attribute comparison, integer comparison, counting, and existence.\nBoth SHAPES and CLEVR were specifically tailored for compositional language approaches [1] and downplay the importance of visual reasoning. For instance, the CLEVR question, \u2018What size is the cylinder that is left of the brown metal thing that is left of the big sphere?\u2019 requires demanding language reasoning capabilities, but only limited visual understanding is needed to parse simple geometric objects. Unlike these three synthetic datasets, our dataset contains natural images and questions. To improve algorithm analysis and comparison, our dataset has more (12) explicitly defined question-types and new evaluation metrics."}, {"heading": "3. TDIUC for Nuanced VQA Analysis", "text": "In the past two years, multiple publicly released datasets have spurred the VQA research. However, due to the biases and issues with evaluation metrics, interpreting and comparing the performance of VQA systems can be opaque. We propose a new benchmark dataset that explicitly assigns questions into 12 distinct categories. This enables measuring performance within each category and understand which kind of questions are easy or hard for today\u2019s best systems. Additionally, we use evaluation metrics that further compensate for the biases. We call the dataset the Task Driven Image Understanding Challenge (TDIUC). The overall statistics and example images of this dataset are shown in Table 1 and Fig. 2 respectively.\nTDIUC has 12 question-types that were chosen to represent both classical computer vision tasks and novel highlevel vision tasks which require varying degrees of image understanding and reasoning. The question-types are:\n1. Object Presence (e.g., \u2018Is there a cat in the image?\u2019)\n2. Subordinate Object Recognition (e.g., \u2018What kind of furniture is in the picture?\u2019) 3. Counting (e.g., \u2019How many horses are there?\u2019) 4. Color Attributes (e.g., \u2018What color is the man\u2019s tie?\u2019) 5. Other Attributes (e.g., \u2018What shape is the clock?\u2019) 6. Activity Recognition (e.g., \u2018What is the girl doing?\u2019) 7. Sport Recognition (e.g.,\u2018What are they playing?\u2019) 8. Positional Reasoning (e.g., \u2018What is to the left of the\nman on the sofa?\u2019) 9. Scene Classification (e.g., \u2018What room is this?\u2019)\n10. Sentiment Understanding (e.g.,\u2018How is she feeling?\u2019) 11. Object Utilities and Affordances (e.g.,\u2018What object\ncan be used to break glass?\u2019) 12. Absurd (i.e., Nonsensical queries about the image)\nThe number of each question-type in TDIUC is given in Table 2. The questions come from three sources. First, we imported a subset of questions from COCO-VQA and Visual Genome. Second, we created algorithms that generated questions from COCO\u2019s semantic segmentation annotations [17], and Visual Genome\u2019s objects and attributes annotations [16]. Third, we used human annotators for certain question-types. In the following sections, we briefly describe each of these methods."}, {"heading": "3.1. Importing Questions from Existing Datasets", "text": "We imported questions from COCO-VQA and Visual Genome belonging to all question-types except \u2018object utilities and affordances\u2019. We did this by using a large number of templates and regular expressions. For Visual Genome, we imported questions that had one word answers. For COCO-VQA, we imported questions with one or two word answers and in which five or more annotators agreed.\nFor color questions, a question would be imported if it contained the word \u2018color\u2019 in it and the answer was a commonly used color. Questions were classified as activity or sports recognition questions if the answer was one of nine common sports or one of fifteen common activities and the question contained common verbs describing actions or sports, e.g., playing, throwing, etc. For counting, the question had to begin with \u2018How many\u2019 and the answer had to be a small countable integer (1-16). The other categories were determined using regular expressions. For example, a question of the form \u2018Are feeling ?\u2019 was classified as sentiment understanding and \u2018What is to the right of/left of/ behind the ?\u2019 was classified as positional reasoning. Similarly, \u2018What <OBJECT CATEGORY> is in the image?\u2019 and similar templates were used to populate subordinate object recognition questions. This method was used for questions about the season and weather as well, e.g., \u2018What season is this?\u2019, \u2018Is this rainy/sunny/cloudy?\u2019, or \u2018What is the weather like?\u2019 were imported to scene classification."}, {"heading": "3.2. Generating Questions using Image Annotations", "text": "Images in the COCO dataset and Visual Genome both have individual regions with semantic knowledge attached to them. We exploit this information to generate new questions using question templates. To introduce variety, we define multiple templates for each question-type and use the annotations to populate them. For example, for counting we use 8 templates, e.g., \u2018How many <objects> are there?\u2019, \u2018How many <objects> are in the photo?\u2019, etc.\nSince the COCO and Visual Genome use different annotation formats, we discuss them separately."}, {"heading": "3.2.1 Questions Using COCO annotations", "text": "Sport recognition, counting, subordinate object recognition, object presence, scene understanding, positional reasoning, and absurd questions were created from COCO. For counting, we count the number of object instances in an image annotation. To minimize ambiguity, this was only done if objects covered an area of at least 2,000 pixels.\nFor subordinate object recognition, we create questions that require identifying an object\u2019s subordinate-level object classification based on its larger semantic category. To do this, we use COCO supercategories, which are semantic concepts encompassing several objects under a common theme, e.g., the supercategory \u2018furniture\u2019 contains chair, couch, etc. If the image contains only one type of furniture, then a question similar to \u2018What kind of furniture is in the picture?\u2019 is generated because the answer is not ambiguous. Using similar heuristics, we create questions about identifying food, electronic appliances, kitchen appliances, animals, and vehicles.\nTo create object presence questions, we find images with objects that have an area larger than 2,000 pixels and then produce a question similar to \u2018Is there a <object> in the picture?\u2019 These questions will have \u2018yes\u2019 as an answer. To create questions with \u2018no\u2019 as an answer, we ask questions about COCO objects that are not present in an image. To make this harder, we explicitly prioritize the creation of questions referring to absent objects that belong to the same\nsupercategory of objects that are present in the image. A street scene is more likely to contain trucks and cars than it is to contain couches and televisions. Therefore, it is more difficult to answer \u2018Is there a truck?\u2019 in a street scene than it is to answer \u2018Is there a couch?\u2019\nFor sport recognition questions, we detect the presence of specific sports equipment in the annotations and ask questions about the type of sport being played. Images must only contain sports equipment for one particular sport. A similar approach was used to create scene understanding questions. For example, if a toilet and a sink are present in annotations, the room is a bathroom and an appropriate scene recognition question can be created. Additionally, we use the supercategories \u2018indoor\u2019 and \u2018outdoor\u2019 to ask questions about where a photo was taken.\nFor creating positional reasoning questions, we use the relative locations of bounding boxes to create questions similar to \u2018What is to the left/right of <object>?\u2019 This can be ambiguous due to overlapping objects, so we employ the following heuristics to eliminate ambiguity: 1) The vertical separation between the two bounding boxes should be within a small threshold; 2) The objects should not overlap by more than the half the length of its counterpart; and 3) The objects should not be horizontally separated by more than a distance threshold, determined by subjectively judging optimal separation to reduce ambiguity. We tried to generate above/below questions, but the results were unreliable.\nAbsurd questions test the ability of an algorithm to judge when a question is not answerable based on the image\u2019s content. To make these, we make a list of the objects that are absent from a given image, and then we find questions from rest of TDIUC that ask about these absent objects, with the exception of yes/no and counting questions. This includes questions imported from COCO-VQA, autogenerated questions, and manually created questions. We make a list of all possible questions that would be \u2018absurd\u2019 for each image and we uniformly sample three questions per image. In effect, we will have same question repeated multiple times throughout the dataset, where it can either be a genuine question or a nonsensical question. The algorithm\nmust answer \u2019Does Not Apply\u2019 if the question is absurd."}, {"heading": "3.2.2 Questions Using Visual Genome annotations", "text": "Visual Genome\u2019s annotations contain region descriptions, relationship graphs, and object boundaries. However, the annotations can be both non-exhaustive and duplicated, which makes using them to automatically make QA pairs difficult. We only use Visual Genome to make color and positional reasoning questions. The methods we used are similar to those used with COCO, but additional precautions were needed due to quirks in their annotations. Additional details are provided in the Appendix."}, {"heading": "3.3. Manual Annotation", "text": "Creating sentiment understanding and object utility/affordance questions cannot be readily done using templates, so we used manual annotation to create these. Twelve volunteer annotators were trained to generate these questions, and they used a web-based annotation tool that we developed. They were shown random images from COCO and Visual Genome and could also upload images."}, {"heading": "3.4. Post Processing", "text": "Post processing was performed on questions from all sources. All numbers were converted to text, e.g., 2 became two. All answers were converted to lowercase, and trailing punctuation was stripped. Duplicate questions for the same image were removed. All questions had to have answers that appeared at least twice. The dataset was split into train and test splits with 70% for train and 30% for test."}, {"heading": "4. Proposed Evaluation Metric", "text": "One of the main goals of VQA research is to build computer vision systems capable of many tasks, instead of only having expertise at one specific task (e.g., object recognition). For this reason, some have argued that VQA is a kind\nof Visual Turing Test [19]. However, if simple accuracy is used for evaluating performance, then it is hard to know if a system succeeds at this goal because some questiontypes have far more questions than others. In VQA, skewed distributions of question-types are to be expected. If each test question is treated equally, then it is difficult to assess performance on rarer question-types and to compensate for bias. We propose multiple measures to compensate for bias and skewed distributions.\nTo compensate for the skewed question-type distribution, we compute accuracy for each of the 12 questiontypes separately. However, it is also important to have a final unified accuracy metric. Our overall metrics are the arithmetic and harmonic means across all per question-type accuracies, referred to as arithmetic mean-per-type (Arithmetic MPT) accuracy and harmonic mean-per-type accuracy (Harmonic MPT). Unlike the Arithmetic MPT, Harmonic MPT measures the ability of a system to have high scores across all question-types and is skewed towards lowest performing categories.\nWe also use normalized metrics that compensate for bias in the form of imbalance in the distribution of answers within each question-type, e.g., the most repeated answer \u2018two\u2019 covers over 35% of all the counting-type questions. To do this, we compute the accuracy for each unique answer separately within a question-type and then average them together for the question-type. To compute overall performance, we compute the arithmetic normalized mean per-type (N-MPT) and harmonic N-MPT scores. A large discrepancy between unnormalized and normalized scores suggests an algorithm is not generalizing to rarer answers."}, {"heading": "5. Algorithms for VQA", "text": "While there are alternative formulations (e.g., [6, 9]), the majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories. [3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20]. Almost all systems use CNN features to represent the image and either a recurrent neural network (RNN) or a bag-of-words model for the question. We briefly review some of these systems, focusing on the models we compare in experiments. For a more comprehensive review, see [13] and [26].\nTwo simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN. These simple approaches often work well and can be competitive with complex attentive models [12, 32].\nSpatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8]. These systems weigh the visual features based on their relevance to the question, instead of using global features, e.g., from the last hidden\nlayer of a CNN. For example, to answer \u2018What color is the bear?\u2019 they aim emphasize the visual features around the bear and suppress other features.\nThe MCB system [5] won the CVPR-2016 VQA Workshop Challenge. In addition to using spatial attention, it implicitly computes the outer product between the image and question features to ensure that all of their elements interact. Explicitly computing the outer product would be slow and extremely high dimensional, so it is done using an efficient approximation. It uses an long short-term memory (LSTM) networks to embed the question.\nThe neural module network (NMN) is an especially interesting compositional approach to VQA [1, 2]. The main idea is to compose a series of discrete modules (sub-networks) that can be executed collectively to answer a given question. To achieve this, they use a variety of modules, e.g., the find(x) module outputs a heat map for detecting x. To arrange the modules, the question is first parsed into a concise expression (called an S-expression), e.g., \u2018What is to the right of the car?\u2019 is parsed into (what car);(what right);(what (and car right)). Using these expressions, modules are composed into a sequence to answer the query.\nThe multi-step recurrent answering units (RAU) model for VQA is another state-of-the-art method [21]. Each inference step in RAU consists of a complete answering block that takes in an image, a question, and the output from the previous LSTM step. Each of these is part of a larger LSTM network that progressively reasons about the question."}, {"heading": "6. Experiments", "text": "We trained multiple baseline models as well as state-ofthe-art VQA methods on TDIUC. The methods we use are: \u2022 YES: Predicts \u2018yes\u2019 for all questions. \u2022 REP: Predicts the most repeated answer in a question-\ntype category using an oracle. \u2022 QUES: A linear softmax classifier given only question\nfeatures (image blind). \u2022 IMG: A linear softmax classifier given only image fea-\ntures (question blind). \u2022 Q+I: A linear classifier given both question and image\nfeatures. \u2022 MLP: A 4-layer MLP fed question and image features. \u2022 MCB: MCB [5] without spatial attention. \u2022 MCB-A: MCB [5] with spatial attention. \u2022 NMN: NMN from [1] with minor modifications. \u2022 RAU: RAU [21] with minor modifications. For image features, ResNet-152 [7] was used for all models. QUES and IMG provide information about biases in the dataset. QUES, Q+I, and MLP all use 4800-dimensional skip-thought vectors [15] to embed the question, as was done in [12]. For image features, these all use the \u2018pool5\u2019 layer of ResNet-152 normalized to unit length. MLP is a 4-\nlayer net with a softmax output layer. The 3 ReLU hidden layers have 6000, 4000, and 2000 units, respectively. During training, dropout (0.3) was used for the hidden layers.\nFor MCB, MCB-A, NMN and RAU, we used publicly available code to train them on TDIUC. The experimental setup and hyperparamters were kept unchanged from the default choices in the code, except for upgrading NMN and RAU\u2019s visual representation to both use ResNet-152.\nResults on TDIUC for these models are given in Table 3. Accuracy scores are given for each of the 12 question-types in Table 3, and scores that are normalized by using meanper-unique-answer are given in Table 5."}, {"heading": "7. Detailed Analysis of VQA Models", "text": ""}, {"heading": "7.1. Easy Question-Types for Today\u2019s Methods", "text": "By inspecting Table 3, we can see that some questiontypes are comparatively easy (> 90%) under MPT: scene recognition, sport recognition, and object presence. High accuracy is also achieved on absurd, which we discuss in greater detail in Sec. 7.4. Subordinate object recognition is moderately high (> 80%), despite having a large number of unique answers. Accuracy on counting is low across all methods, despite a large number of training data. For the remaining question-types, more analysis is needed to pinpoint whether the weaker performance is due to lower amounts of training data, bias, or limitations of the models. We next investigate how much of the good performance is due to bias in the answer distribution, which N-MPT compensates for."}, {"heading": "7.2. Effects of the Proposed Accuracy Metrics", "text": "One of our major aims was to compensate for the fact that algorithms can achieve high scores by simply learning to answer more populated and easier question-types. For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9]. On TDIUC, MLP surpasses MCB and NMN in terms of simple accuracy, but a closer inspection reveals that MLP\u2019s score is highly determined by performance on large categories, such as \u2018absurd\u2019 and \u2018object presence.\u2019 Using MPT, we find that both NMN and MCB outperform MLP. Inspecting normalized scores for each question-type (Table 5) shows an even more pronounced differences, which is also reflected in arithmetic N-MPT score presented in Table 3. This indicates that MLP is prone to overfitting. Similar observations can be made for MCB-A compared to RAU, where RAU outperforms MCBA using simple accuracy, but scores lower on all the metrics designed to compensate for the skewed answer distribution and bias.\nComparing the unnormalized and normalized metrics can help us determine the generalization capacity of the VQA algorithms for a given question-type. A large dif-\nference in these scores suggests that an algorithm is relying on the skewed answer distribution to obtain high scores. We found that for MCB-A, the accuracy on subordinate object recognition drops from 85.54% with unnormalized to 23.22% with normalized, and for scene recognition it drops from 93.06% (unnormalized) to 38.53% (normalized). Both these categories have a heavily skewed answer distribution; the top-25 answers in subordinate object recognition and the top-5 answers in scene recognition cover over 80% of all questions in their respective question-types. This shows that question-types that appear to be easy may simply be due to the algorithms learning the answer statistics. A truly easy question-type will have similar performance for both unnormalized and normalized metrics. For example, sport recognition shows only 17.39% drop compared to a 30.21% drop for counting, despite counting having same number of unique answers and far more training data. By comparing relative drop in performance between normalized and unnormalized metric, we can also compare the generalization capability of the algorithms, e.g., for subordinate object recognition, RAU has higher unnormalized score (86.11%) compared to MCB-A (85.54%). However, for normalized scores, MCB-A has significantly higher performance (23.22%) than RAU (21.67%). This shows RAU may be more dependent on the answer distribution. Similar observations can be made for MLP compared to MCB."}, {"heading": "7.3. Can Algorithms Predict Rare Answers?", "text": "In the previous section, we saw that the VQA models struggle to correctly predict rarer answers. Are the less re-\npeated questions actually harder to answer, or are the algorithms simply biased toward more frequent answers? To study this, we created a subset of TDIUC that only consisted of questions that have answers repeated less than 1000 times. We call this dataset TDIUC-Tail, which has 46,590 train and 22,065 test questions. Then, we trained MCB on: 1) the full TDIUC dataset; and 2) TDIUC-Tail. Both versions were evaluated on the validation split of TDIUC-Tail.\nWe found that MCB trained only on TDIUC-Tail outperformed MCB trained on all of TDIUC across all questiontypes (Table 6). This shows that MCB is capable of learning to correctly predict rarer answers, but it is simply biased towards predicting more common answers to maximize overall accuracy. Using normalized accuracy disincentivizes the VQA algorithms\u2019 reliance on the answer statistics, and for deploying a VQA system it may be useful to optimize directly for N-MPT."}, {"heading": "7.4. Effects of Including Absurd Questions", "text": "Absurd questions force a VQA system to look at the image to answer the question. In TDIUC, these questions are sampled from the rest of the dataset, and they have a high prior probability of being answered \u2019Does not apply.\u2019 This is corroborated by the QUES model, which achieves a high accuracy on absurd; however, for the same questions when they are genuine for an image, it only achieves 6.77% accuracy on these questions. Good absurd performance is achieved by sacrificing performance on other categories. A robust VQA system should be able to detect absurd ques-\ntions without then failing on others. By examining the accuracy on real questions that are identical to absurd questions, we can quantify an algorithm\u2019s ability to differentiate the absurd questions from the real ones. We found that simpler models had much lower accuracy on these questions, (QUES: 6.77%, Q+I: 34%), compared to more complex models (MCB: 62.44%, MCB-A: 68.83%).\nTo further study this, we we trained two VQA systems, Q+I and MCB, both with and without absurd. The results are presented in Table 3. For Q+I trained without absurd questions, accuracies for other categories increase considerably compared to Q+I trained with full TDIUC, especially for question-types that are used to sample absurd questions, e.g., activity recognition (24% when trained with absurd and 48% without). Arithmetic MPT accuracy for the Q+I model that is trained without absurd (57.03%) is also substantially greater than MPT for the model trained with absurd (51.45% for all categories except absurd). This suggests that Q+I is not properly discriminating between absurd and real questions and is biased towards misidentifying genuine questions as being absurd. In contrast, MCB, a more capable model, produces worse results for absurd, but the version trained without absurd shows much smaller differences than Q+I, which shows that MCB is more capable of identifying absurd questions."}, {"heading": "7.5. Effects of Balancing Object Presence", "text": "In Sec. 7.3, we saw that a skewed answer distribution can impact generalization. This effect is strong even for simple questions and affects even the most sophisticated algorithms. Consider MCB-A when it is trained on both COCO-VQA and Visual Genome, i.e., the winner of the CVPR-2016 VQA Workshop Challenge. When it is evaluated on object presence questions from TDIUC, which contains 50% \u2018yes\u2019 and 50% \u2018no\u2019 questions, it correctly predicts \u2018yes\u2019 answers with 86.3% accuracy, but only 11.2% for questions with \u2018no\u2019 as an answer. However, after training it on TDIUC, MCB-A is able to achieve 95.02% for \u2018yes\u2019 and 92.26% for \u2018no.\u2019 MCB-A performed poorly by learning the biases in the COCO-VQA dataset, but it is capable of performing well when the dataset is unbiased. Similar observations about balancing yes/no questions were made in [31]. Datasets could balance simple categories like object presence, but extending the same idea to all other categories is a challenging task and undermines the natural statistics of the real-world. Adopting mean-per-class and normalized accuracy metrics can help compensate for this problem."}, {"heading": "7.6. Advantages of Attentive Models", "text": "By breaking questions into types, we can assess which types benefit the most from attention. We do this by comparing the MCB model with and without attention, i.e., MCB and MCB-A. As seen in Table 3, attention helped im-\nprove results on several question categories. The most pronounced increases are for color recognition, attribute recognition, absurd, and counting. All of these question-types require the algorithm to detect specified object(s) (or lack thereof) to be answered correctly. MCB-A computes attention using local features from different spatial locations, instead of global image features. This aids in localizing individual objects. The attention mechanism learns the relative importance of these features. RAU also utilizes spatial attention and shows similar increments."}, {"heading": "7.7. Compositional and Modular Approaches", "text": "NMN, and, to a lesser extent, RAU propose compositional approaches for VQA. For COCO-VQA, NMN has performed worse than some MLP models [12] using simple accuracy. We hoped that it would achieve better performance than other models for questions that require logically analyzing an image in a step-by-step manner, e.g., positional reasoning. However, while NMN did perform better than MLP using MPT and N-MPT metric, we did not see any substantial benefits in specific question-types. This may be because NMN is limited by the quality of the \u2018S-expression\u2019 parser. Since NMN arranges the modules based on these \u2018S-expressions,\u2019 any error in parsing these expressions will propagate throughout the rest of the training. The S-expression parser used in [1] can produce wrong or misleading parses in many cases. For example, \u2018What color is the jacket of the man on the far left?\u2019 is parsed as (color jacket);(color leave);(color (and jacket leave)). This expression not only fails to parse \u2018the man\u2019, which is a crucial element needed to correctly answer the question, but also wrongly interprets \u2018left\u2019 as past tense of leave. Errors of this kind are not uncommon and are found frequently for all but the most straightforward questions.\nRAU performs inference over multiple hops, and because each hop contains a complete VQA system, it can learn to solve different tasks in each step. Since it is trained end-to-end, it does not need to rely on rigid question parses. It showed very good performance in detecting absurd questions and also performed well on other categories."}, {"heading": "8. Conclusion", "text": "In this paper, we introduced a new VQA dataset that consists of 12 explicitly defined question-types, including the absurd question-type, and performed a rigorous comparison and analysis of the VQA algorithms on this dataset. Additionally, we proposed new evaluation metrics to compensate for the biases in the dataset. Results show that the absurd questions and the new evaluation metrics enable us to get a better understanding of how the VQA algorithms perform."}, {"heading": "A. Additional Details About TDIUC", "text": "In this section, we will provide additional details about the TDIUC dataset creation and additional statistics that were omitted from the main paper due to inadequate space.\nA.1. Questions using Visual Genome Annotations\nAs mentioned in the main text, Visual Genome\u2019s annotations are both non-exhaustive and duplicated. This makes\nusing them to automatically make question-answer (QA) pairs difficult. Due to these issues, we only used them to make two types of questions: Color Attributes and Positional Reasoning. Moreover, a number of restrictions needed to be placed, which are outlined below.\nFor making Color Attribute questions, we make use of the attributes metadata in the Visual Genome annotations to populate the template \u2018What color is the <object>?\u2019 However, Visual Genome metadata can contain several color attributes for the same object as well as different names for the same object. Since the annotators type the name of the object manually rather than choosing from a predetermined set of objects, the same object can be referred by different names, e.g., \u2018xbox controller,\u2019 \u2018game controller,\u2019 \u2018joystick,\u2019 and \u2018controller\u2019 can all refer to same object in an image. The object name is sometimes also accompanied by its color, e.g., \u2018white horse\u2019 instead of \u2018horse\u2019 which makes asking the Color Attribute question \u2018What color is the white horse?\u2019 pointless. One potential solution is to use the wordnet \u2018synset\u2019 which accompanies every object annotation in the Visual Genome annotations. Synsets are used to group different variations of the common objects names under a single noun from wordnet. However, we found that the synset matching was erroneous in numerous instances, where the object category was misrepresented by the given synset. For example, A \u2018controller\u2019 is matched with synset \u2018accountant\u2019 even when the \u2018controller\u2019 is referring to a game controller. Similarly, a \u2018cd\u2019 is matched with synset of \u2018cadmium.\u2019 To avoid these problems we made a set of stringent requirements before making questions:\n1. The chosen object should only have a single attribute that belongs to a set of commonly used colors.\n2. The chosen object name or synset must be one of the 91 common objects in the MS-COCO annotations.\n3. There must be only one instance of the chosen object.\nUsing these criteria, we found that we could safely ask the question of the form \u2018What color is the <object>?\u2019.\nSimilarly, for making Positional Reasoning questions, we used the relationships metadata in the Visual Genome annotations. The relationships metadata connects two objects by a relationship phrase. Many of these relationships\ndescribe the positions of the two objects, e.g., A is \u2018on right\u2019 of B, where \u2018on right\u2019 is one of the example relationship clause from Visual Genome, with the object A as the subject and the object B as the object. This can be used to generate Positional Reasoning questions. Again, we take several measures to avoid ambiguity. First, we only use objects that appear once in the image because \u2018What is to the left of A\u2019 can be ambiguous if there are two instances of the object A. However, since visual genome annotations are non-exhaustive, there may still (rarely) be more than one instance of object A that was not annotated. To disambiguate such cases, we use the attributes metadata to further specify the object wherever possible, e.g., instead of asking \u2018What is to the right of the bus?\u2019, we ask \u2018What is to the right of the green bus?\u2019\nDue to a these stringent criteria, we could only create a small number of questions using Visual Genome annotations compared to other sources. The number of questions produced via each source is shown in Table 4.\nA.2. Answer Distribution\nFigure 3 shows the answer distribution for the different question-types. We can see that some categories, such as counting, scene recognition and sentiment understanding, have a very large share of questions represented by only a few top answers. In such cases, the performance of a VQA algorithm can be inflated unless the evaluation metric compensates for this bias. In other cases, such as positional reasoning and object utility and affordances, the answers are much more varied, with top-50 answers covering less than 60% of all answers.\nWe have completely balanced answer distribution for object presence questions, where exactly 50% of questions being answered \u2018yes\u2019 and the remaining 50% of the questions are answered \u2018no\u2019. For other categories, we have tried to design our question generation algorithms so that a single answer does not have a significant majority within a question type. For example, while scene understanding has top4 answers covering over 85% of all the questions, there are roughly as many \u2018no\u2019 questions (most common answer) as there are \u2018yes\u2019 questions (second most-common answer). Similar distributions can be seen for counting, where \u2018two\u2019 (most-common answer) is repeated almost as many times as \u2018one\u2019 (second most-common answer). By having at least the top-2 answers split almost equally, we remove the incentive for an algorithm to perform well using simple mode guessing, even when using the simple accuracy metric.\nA.3. Train and Test Split\nIn the paper, we mentioned that we split the entire collection into 70% train and 30% test/validation. To do this, we not only need to have a roughly equal distribution of question types and answers, but also need to make sure that the\nmultiple questions for same image do not end up in two different splits, i.e., the same image cannot occur in both the train and the test partitions. So, we took following measures to split the questions into train-test splits. First, we split all the images into three separate clusters.\n1. Manually uploaded images, which includes all the images manually uploaded by our volunteer annotators. 2. Images from the COCO dataset, including all the images for questions generated from COCO annotations and those imported from COCO-VQA dataset. In addition, a large number of Visual Genome questions also refer to COCO images. So, some questions that are generated and imported from Visual Genome are also included in this cluster. 3. Images exclusively in the Visual Genome dataset, which includes images for a part of the questions imported from Visual Genome and those generated using that dataset.\nWe follow simple rules to split each of these clusters of images into either belonging to the train or test splits.\n1. All the questions belonging to images coming from the \u2018train2014\u2019 split of COCO images are assigned to the train split and all the questions belonging to images from the \u2018val2014\u2019 split are assigned to test split.\n2. For manual and Visual Genome images, we randomly split 70% of images to train and rest to test."}, {"heading": "B. Additional Experimental Results", "text": "In this section, we present additional experimental results that were omitted from the main paper due to inadequate space. First, the detailed normalized scores for each of the question-types is presented in Table 3. To compute these scores, the accuracy for each unique answer is calculated separately within a question-type and averaged. Second, we present the results from the experiment in section 7.3 in table 6 (Unnormalized) and table7 (Normalized). The results are evaluated on TDIUC-Tail, which is a subset of TDIUC that only consists of questions that have answers repeated less than 1000 times (uncommon answers). Note that the TDIUC-Tail excludes the absurd and the object presence question-types, as they do not contain any questions with uncommon answers. The algorithms are identical in both Table 6 and 7 and are named as follows:\n1. MCB TDIUC-Full : MCB model trained on whole of the TDIUC dataset and evaluated on TDIUC-Tail.\n2. MCB TDIUC-Tail : MCB model trained and evaluated on TDIUC-Tail."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, 28:594\u2013611,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Are you talking to a machine? Dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "arXiv preprint arXiv:1604.01485,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Compositional memory for visual question answering", "author": ["A. Jiang", "F. Wang", "F. Porikli", "Y. Li"], "venue": "arXiv preprint arXiv:1511.05676,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "arXiv preprint arXiv:1612.06890,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual question answering: Datasets, algorithms, and future challenges", "author": ["K. Kafle", "C. Kanan"], "venue": "arXiv preprint arXiv:1610.01465,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.- W. Ha", "B.-T. Zhang"], "venue": "arXiv preprint arXiv:1606.01455,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Training recurrent answering units with joint loss minimization for VQA", "author": ["H. Noh", "B. Han"], "venue": "arXiv preprint arXiv:1606.03647,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["K. Saito", "A. Shin", "Y. Ushiku", "T. Harada"], "venue": "arXiv preprint arXiv:1606.06108,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual question answering: A survey of methods and datasets", "author": ["Q. Wu", "D. Teney", "P. Wang", "C. Shen", "A. Dick", "A. v. d. Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. van den Hengel", "A.R. Dick"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CVPR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "CoRR, abs/1512.02167,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images [19, 3].", "startOffset": 127, "endOffset": 134}, {"referenceID": 2, "context": "In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images [19, 3].", "startOffset": 127, "endOffset": 134}, {"referenceID": 18, "context": "VQA research began in earnest in late 2014 when the DAQUAR dataset was released [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "VQA Dataset\u2019 [3], the best algorithms are now approaching 70% accuracy [5] (human performance is 83%).", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "VQA Dataset\u2019 [3], the best algorithms are now approaching 70% accuracy [5] (human performance is 83%).", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 133, "endOffset": 136}, {"referenceID": 32, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "Six datasets for VQA with natural images have been released between 2014\u20132016: DAQUAR [19], COCOQA [23], FM-IQA [6], The VQA Dataset [3], Visual7W [33], and Visual Genome [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 11, "context": "Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA.", "startOffset": 17, "endOffset": 29}, {"referenceID": 31, "context": "Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA.", "startOffset": 17, "endOffset": 29}, {"referenceID": 26, "context": "Following others [12, 32, 27], we refer to the portion of The VQA Dataset containing natural images as COCO-VQA.", "startOffset": 17, "endOffset": 29}, {"referenceID": 12, "context": "Detailed dataset reviews can be found in [13] and [26].", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "Detailed dataset reviews can be found in [13] and [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "For COCO-VQA, a system trained using only question features achieves 50% accuracy [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "While some do compute additional statistics for basic question-types, overall performance is not computed from these sub-scores [3, 23].", "startOffset": 128, "endOffset": 135}, {"referenceID": 22, "context": "While some do compute additional statistics for basic question-types, overall performance is not computed from these sub-scores [3, 23].", "startOffset": 128, "endOffset": 135}, {"referenceID": 12, "context": "1% [13].", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "For example, on Caltech101 [4], even with balanced training data, simple accuracy fails to address the fact that some categories were much easier to classify than others (e.", "startOffset": 27, "endOffset": 30}, {"referenceID": 30, "context": "In [31], the Yin and Yang dataset was created to study the effect of having an equal number of binary (yes/no) questions about cartoon images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "In [1], the SHAPES dataset was proposed, which has similar objectives.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "An unreleased dataset that improves on SHAPES is Compositional Language and Elementary Visual Reasoning (CLEVR) [11], which was described in a recent arXiv", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Both SHAPES and CLEVR were specifically tailored for compositional language approaches [1] and downplay the importance of visual reasoning.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Second, we created algorithms that generated questions from COCO\u2019s semantic segmentation annotations [17], and Visual Genome\u2019s objects and attributes annotations [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "Second, we created algorithms that generated questions from COCO\u2019s semantic segmentation annotations [17], and Visual Genome\u2019s objects and attributes annotations [16].", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": "For this reason, some have argued that VQA is a kind of Visual Turing Test [19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": ", [6, 9]), the majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [6, 9]), the majority of VQA systems formulate it as a classification problem in which the system is given an image and a question, with the answers as categories.", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 22, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 4, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 24, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 7, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 13, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 9, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 17, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 21, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 23, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 26, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 28, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 29, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 31, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 8, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 19, "context": "[3, 23, 5, 25, 8, 14, 10, 18, 22, 24, 27, 29, 30, 32, 9, 20].", "startOffset": 0, "endOffset": 60}, {"referenceID": 12, "context": "For a more comprehensive review, see [13] and [26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "For a more comprehensive review, see [13] and [26].", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN.", "startOffset": 160, "endOffset": 171}, {"referenceID": 11, "context": "Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN.", "startOffset": 160, "endOffset": 171}, {"referenceID": 31, "context": "Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and image embeddings concatenated to each other [3, 12, 32], where the image features come from the last hidden layer of a CNN.", "startOffset": 160, "endOffset": 171}, {"referenceID": 11, "context": "These simple approaches often work well and can be competitive with complex attentive models [12, 32].", "startOffset": 93, "endOffset": 101}, {"referenceID": 31, "context": "These simple approaches often work well and can be competitive with complex attentive models [12, 32].", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 24, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 29, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 27, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 28, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 17, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 7, "context": "Spatial attention has been heavily investigated in VQA models [5, 25, 30, 28, 29, 18, 8].", "startOffset": 62, "endOffset": 88}, {"referenceID": 4, "context": "The MCB system [5] won the CVPR-2016 VQA Workshop Challenge.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "The neural module network (NMN) is an especially interesting compositional approach to VQA [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "The neural module network (NMN) is an especially interesting compositional approach to VQA [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 20, "context": "The multi-step recurrent answering units (RAU) model for VQA is another state-of-the-art method [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "\u2022 MCB: MCB [5] without spatial attention.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "\u2022 MCB-A: MCB [5] with spatial attention.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "\u2022 NMN: NMN from [1] with minor modifications.", "startOffset": 16, "endOffset": 19}, {"referenceID": 20, "context": "\u2022 RAU: RAU [21] with minor modifications.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "For image features, ResNet-152 [7] was used for all models.", "startOffset": 31, "endOffset": 34}, {"referenceID": 14, "context": "QUES, Q+I, and MLP all use 4800-dimensional skip-thought vectors [15] to embed the question, as was done in [12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "QUES, Q+I, and MLP all use 4800-dimensional skip-thought vectors [15] to embed the question, as was done in [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9].", "startOffset": 135, "endOffset": 146}, {"referenceID": 31, "context": "For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9].", "startOffset": 135, "endOffset": 146}, {"referenceID": 8, "context": "For existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using simple accuracy [12, 32, 9].", "startOffset": 135, "endOffset": 146}, {"referenceID": 30, "context": "Similar observations about balancing yes/no questions were made in [31].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "For COCO-VQA, NMN has performed worse than some MLP models [12] using simple accuracy.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The S-expression parser used in [1] can produce wrong or misleading parses in many cases.", "startOffset": 32, "endOffset": 35}], "year": 2017, "abstractText": "In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.", "creator": "LaTeX with hyperref package"}}}