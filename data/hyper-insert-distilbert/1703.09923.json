{"id": "1703.09923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "On Convergence Property of Implicit Self-paced Objective", "abstract": "self - realistic paced learning ( spl ) is utilizing a new methodology technique that intentionally simulates the learning empirical principle pathways of humans / animals to start learning or easier aspects of a learning task, and then gradually starts take more tightly complex examples into procedural training. this relatively new - coming learning regime has been observed empirically virtually substantiated to \" be thoroughly effective learners in satisfying various computer vision and pattern recognition tasks. recently, it has hardly been generally proved thereby that the spl regime has a close dependence relationship opposite to a implicit self - dynamics paced quantitative objective function. while fulfilling this added implicit inner objective could provide immensely helpful interpretations adjacent to the analytic effectiveness, nowadays especially measuring the robustness, insights presented under the spl paradigms, there are already still no theoretical results strictly proved only to verify such intimate relationship. to this issue, in reviewing this initial paper, we provide some convergence stability results on stating this intrinsic implicit objective component of spl. yours specifically, finally we prove accurately that basically the learning process rules of spl always converges to each critical finer points of this implicit rigorous objective structure under applying some mild conditions. this consistent result completely verifies : the intrinsic relationship between spl rules and this additional implicit objective, defines and makes the previous robustness because analysis works on spl complete and independently theoretically proven rational.", "histories": [["v1", "Wed, 29 Mar 2017 07:53:43 GMT  (17kb)", "http://arxiv.org/abs/1703.09923v1", "9 pages, 0 figures"]], "COMMENTS": "9 pages, 0 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["zilu ma", "shiqi liu", "deyu meng"], "accepted": false, "id": "1703.09923"}, "pdf": {"name": "1703.09923.pdf", "metadata": {"source": "CRF", "title": "On Convergence Property of Implicit Self-paced Objective", "authors": ["Zilu Ma", "Shiqi Liu", "Deyu Meng"], "emails": ["dymeng@mail.xjtu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n09 92\n3v 1\n[ cs\n.A I]\nmans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the SPL regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the SPL paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of SPL. Specifically, we prove that the learning process of SPL always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between SPL and this implicit objective, and makes the previous robustness analysis on SPL complete and theoretically rational.\nKeywords Self-paced learning, machine learning, non-convex optimization, convergence\nCitation Zilu Ma, Shiqi Liu, Deyu Meng, et al. On Convergence Property of Implicit Self-paced Objective. , for review"}, {"heading": "1 Introduction", "text": "Self-paced learning (SPL) is a recently raised methodology designed through simulating the learning principle of humans/animals [3]. A variety of SPL realization schemes have been designed, and empirically substantiated to be effective in different computer vision and pattern recognition tasks, such as object detector adaptation [10], specific-class segmentation learning [2], visual category discovery [4], concept learning [5], long-term tracking [9], graph trimming [14], co-saliency detection [16], matrix factorization [17], face identification [6], and multimedia event detection [13].\nTo explain the underlying effectiveness mechanism inside SPL, [7] firstly provided some new theoretical understandings under the SPL scheme. Specifically, this work proved that the alternative optimization strategy (AOS) on SPL accords with a majorization minimization (MM) algorithm implemented on an implicit objective function. Furthermore, it is found that the loss function contained in this implicit objective has a similar configuration with non-convex regularized penalty (NCRP), leading to a rational interpretation to the robustness insight under SPL.\n*Corresponding author (email: dymeng@mail.xjtu.edu.cn )\nHowever, such understanding is still not theoretically strict. The theory in [7] can only guarantee that during the iterations of SPL solving process (i.e., the MM algorithm), the implicit objective is monotonically decreasing, while cannot prove any convergence results on this implicit objective theoretically. However, this theoretical result regarding this implicit objective is critical to the soundness of the robustness insight explanation of SPL, which guarantees to settle the convergence point of the algorithm down on the expected implicit objective, and intrinsically relate the original SPL model and this implicit objective.\nTo this theoretical issue of SPL, in this paper, we prove that the optimization of the implicit objective actually converges to critical points of original SPL problem under satisfactorily weak conditions. This result provides an affirmative answer to our guess that the SPL intrinsically optimizes a robust implicit objective.\nIn what follows, we will first introduce some related background of this research, and then we provide\nthe main theoretical result of this work."}, {"heading": "2 Related work", "text": "In this section, we first briefly introduce the definition of SPL, and then provides its relationship to the implicit objective of NCRP."}, {"heading": "2.1 The SPL objective", "text": "Given training data set {(xi, yi)} N i=1, many machine learning problems need to minimizing the following form of objective function:\nJ(w) = \u03c6\u03bb(w) + N \u2211\ni=1\nL(yi, g(xi, w)),\nwhere w \u2208 RD is variables to be solved, \u03c6\u03bb is a regularizer parameter, L is the loss function and g(\u00b7, w) is the parametrized learning machine, like a discriminative or a regression function.\nTo improve the robustness, specially avoiding the negative influence brought by large-noise-outliers, SPL imposes additional importance weights v = (v1, \u00b7 \u00b7 \u00b7 , vn) to loss functions of all samples, adjusted by a self-paced regularizer (SP-regularizer). Here, each vi \u2208 [0, 1] represents how much extent the sample (xi, yi) will be trained in the learning process. The self-paced objective can then be designed as [1]:\nE(w, v;\u03bb) = \u03c6\u03bb(w) +\nN \u2211\ni=1\nviL(yi, g(xi, w)) + f\u03bb(vi), (1)\nwhere f is the SP-regularizer, satisfying the following conditions:\n1. v 7\u2192 f\u03bb(v) is convex on [0, 1];\n2. Let\nv\u2217\u03bb(l) = arg min v\u2208[0,1] {vl + f\u03bb(v)} ,\nthen l 7\u2192 v\u2217\u03bb(l) is non-increasing, and\nlim l\u21920 v\u2217\u03bb(l) = 1, lim l\u2192\u221e v\u2217\u03bb(l) = 0;\n3. \u03bb 7\u2192 v\u2217\u03bb(l) is non-decreasing, and\nlim \u03bb\u21920 v\u2217\u03bb(l) = 0, lim \u03bb\u2192\u221e v\u2217\u03bb(l) 6 1.\nThroughout this paper, we shall assume that v\u2217\u03bb(l) can be uniquely determined and thus can be seen as a real-valued function instead of a set-valued function.\nThe three conditions in the definition above provide basic principles for constructing a SP-regularizer. Condition 2 indicates that the model inclines to select easy samples (with smaller losses) in favor of complex samples (with larger losses). Condition 3 states that when the model \u201cpace\u201d (controlled by the pace parameter \u03bb) gets larger, it tends to incorporate more, probably complex, samples to train a \u201cmature\u201d model. The convexity in Condition 1 further ensures the soundness of this regularizer for optimization.\nThe existence of the SP-regularizer can be illustrated by the following example. Let the SP-regularizer be\nf\u03bb(v) = \u03bbv(log v \u2212 1),\nthen it yields\nv\u2217\u03bb(l) = e \u2212\u03bb\u22121l.\nIt is easy to verify that v\u2217\u03bb(l) satisfies the above conditions.\nIn the following, we shall write:\nli(w) = L(yi, g(xi, w)), i = 1, \u00b7 \u00b7 \u00b7 , N\nfor simplicity."}, {"heading": "2.2 The implicit NCRP objective", "text": "Let\nF\u03bb(l) =\n\u222b l\n0\nv\u2217\u03bb(\u03c4)d\u03c4.\nSince v\u2217\u03bb is non-increasing, the set of its discontinuous points is countable and consists only of jump discontinuity. Thus v\u2217\u03bb is integrable and F\u03bb is absolutely continuous and concave. We now define\nG\u03bb(w) = \u03c6\u03bb(w) +\nN \u2211\ni=1\n(F\u03bb \u25e6 li)(w) (2)\nas the implicit objective, where g \u25e6 f denotes that g composed with f . An interesting observation is that this implicit SPL objective has a close relationship to NCRP widely investigated in machine learning and statistics, which provides some helpful explanation to the robustness insight under SPL [7].\nThe original utilized AOS algorithm for solving the SPL problem is designed by performing coordinate\ndescent calculation on E(w, v;\u03bb), i.e., iterating through the process as:\n(wk\u22121, vk\u22121) \u2192 (wk\u22121, vk) \u2192 (wk, vk).\nSpecifically, given (w0, v0), if we have finished (k \u2212 1) steps, then the AOS algorithm need to iteratively calculating the following two subproblems:\nvk = argmin v E(wk\u22121, v;\u03bb) = argmin v\n{\nN \u2211\ni=1\nvili(w k\u22121) + f\u03bb(vi)\n}\n,\nwk \u2208 argmin w E(w, vk;\u03bb) = argmin w\n{\n\u03c6\u03bb(w) + N \u2211\ni=1\nvki li(w)\n}\n.\nNote that the first subproblem is feasible since we have assumed that v\u2217\u03bb can be uniquely determined. Indeed, using the notation of v\u2217\u03bb, we have\nvki = v \u2217 \u03bb(li(w k\u22121)), i = 1, \u00b7 \u00b7 \u00b7 , N.\nWe then set\nQ(w|w\u2217) =\nN \u2211\ni=1\n(F\u03bb \u25e6 li)(w \u2217) + (v\u2217\u03bb \u25e6 li)(w \u2217)[li(w)\u2212 li(w \u2217)].\nIt is easy to deduce that Q(w|w\u2217) is actually the first-order Taylor series of F\u03bb at li(w \u2217). Based on the concavity of F\u03bb, we know that\nU(w|w\u2217) = \u03c6\u03bb(w) +Q(w|w \u2217)\nconstitutes a upper bound of G\u03bb(w) (as defined in Eq. (2)), which provides a qualified surrogate function for MM algorithm.\nOne of the key issues in [7] is that if {wk} is produced by AOS algorithm of E(w, v;\u03bb), then it can also be produced by performing MM algorithm on G\u03bb and vice versa. We prove one side by induction. The other side is totally the same. Suppose we have proved that wk can be produced by performing MM algorithm on G\u03bb at k th step. When it comes to the (k + 1)th step,\nwk+1 \u2208 argmin w E(w, vk+1;\u03bb)\n= argmin w \u03c6\u03bb(w) +\nN \u2211\ni=1\nvk+1i li(w)\n= argmin w \u03c6\u03bb(w) +\nN \u2211\ni=1\nv\u2217\u03bb(li(w k)) \u00b7 li(w)\n= argmin w\n\u03c6\u03bb(w) +Q(w|w k) = argmin w U(w|wk). (3)\nThus we have proved our claim that these two optimization algorithms (AOS/MM) conducting on the two different objective functions (E(w, v;\u03bb)/G\u03bb(w)) are intrinsically equivalent.\nWe then need to prove whether every convergence point of MM algorithm, or equivalently, that of the\nAOS algorithm on the SPL objective, is at least a critical point of G\u03bb."}, {"heading": "3 The main convergence result", "text": "Actually, the proof of the convergence of MM algorithm is basically the same as that of the EM algorithm (see [12]) only with some obvious changes, as discussed in [11]. And the convergence of EM and MM is indeed a corollary of a global convergence theorem of Zangwill (see [15]). We can generalize the proof to the case of variational analysis. Before that, we need to clarify some terminologies which can be referred to in [8].\nA function f : Rd \u2192 R is said to be lower semi-continuous or simply lsc if\nlevf6\u03b1 := {x : f(x) 6 \u03b1}\nis closed for any \u03b1 \u2208 R. f is said to be level-bounded if levf6\u03b1 is bounded for any \u03b1. And f is called coercive if lim|x|\u2192\u221e f(x) = \u221e. Note that coercive functions are level-bounded. A critical point x of f means that 0 \u2208 \u2202f(x), where \u2202 stands for the subdifferential [8].\nThe main theorem of this paper can then be stated as follows.\nTheorem 1. Suppose that the objective function of MM algorithm, G : RD \u2192 R, is lsc and levelbounded, and that the surrogate function at w\u2217 is U(\u00b7|w\u2217), which is lsc as a function on R2D, and satisfies\n\u2202U(w|w) \u2282 \u2202G(w), \u2200w \u2208 RD,\nwhere \u2202U(w|w\u2217) is the partial subdifferential in w. Then for any initial parameter w0, every cluster point of the produced sequence {wk} of MM algorithm is a critical point of G.\nProof. See the appendix.\nFor our problem, we can give a sufficient condition of convergence, which is easy to verify and satisfied\nby most of the current SPL variations.\nTheorem 2. In the SPL objective as defined in Section 2.1, suppose L is bounded below, w 7\u2192 L(y, g(x,w)) is continuously differentiable, v\u2217\u03bb(\u00b7) is continuous, and \u03c6\u03bb is coercive and lsc. Then for any initial parameter w0, every cluster point of the produced sequence {wk}, obtained by the AOS algorithm on solving Eq. (1), is a critical point of the implicit objective G\u03bb as defined in Eq. (2). Proof. It is obvious that G\u03bb is lsc and level-bounded and U is lsc as a function on R 2D with these assumptions. And the continuity of v\u2217\u03bb makes F\u03bb continuously differentiable. Then we have\n\u2202G\u03bb(w \u2217) = \u2202\u03c6\u03bb(w \u2217) + N \u2211\ni=1\nF \u2032\u03bb(li(w \u2217))\u2207li(w \u2217)\n= \u2202\u03c6\u03bb(w \u2217) +\nN \u2211\ni=1\n(v\u2217\u03bb \u25e6 li)(w \u2217)\u2207li(w \u2217)\n= \u2202U(w\u2217|w\u2217).\nBased on Theorem 1, for any initial parameter w0, every cluster point of the produced sequence {wk} is a critical point of G\u03bb. The proof is then completed.\nFrom the theorem, we can see that the AOS algorithm generally used to solving the SPL problem can be guaranteed to convergent to a critical point of the implicit NCRP objective G\u03bb. The intrinsic relationship between two objectives can then be constructed.\nNote that in the above theorem, it is required that every minimization step in MM algorithm exactly\nattains the minima of the surrogate function U(w|wk), i.e.,\nU(wk+1|wk) = minU(\u00b7|wk). (4)\nThis is generally hard to achieve in real applications, especially for those learning models without closedform solution. We thus want to further relax the condition to allow a relatively weaker solution \u201cwith errors\u201d in implementing the MM algorithm on the surrogate function. That is, we can weaken the condition (4) as:\nU(wk+1|wk) 6 minU(\u00b7|wk) + \u01ebk,\nwhere \u01eb1, \u01eb2, \u00b7 \u00b7 \u00b7 is a non-negative sequence satisfying {\u01ebk} \u2208 l 1, i.e.,\n\u2211\nk \u01ebk < \u221e.\nUnder this relaxed condition, we can still prove the convergence result of SPL in the following algorithm\nTheorem 3. In the SPL objective as defined in Section 2.1, suppose L is bounded below, w 7\u2192 L(y, g(x,w)) is continuously differentiable, v\u2217\u03bb(\u00b7) is continuous, and \u03c6\u03bb is coercive and lsc. Let w 0 be an arbitrary initial parameter, and {wk} be the sequence obtained by the AOS algorithm on solving Eq. (1) with errors {\u01ebk > 0} \u2208 l 1, that is,\nE(wk, vk;\u03bb) 6 minE(\u00b7, vk;\u03bb) + \u01ebk, \u2200k > 1.\nThen every cluster point of {wk} is a critical point of the implicit objective G\u03bb as defined in Eq. (2).\nBased on the theorem, we can then confirm the intrinsic relationship between SPL and its implicit\nobjective."}, {"heading": "4 Conclusion", "text": "In this paper, we have proved that the learning process of traditional SPL regime can be guaranteed to converge to rational critical points of the corresponding implicit NCRP objective. This theory helps confirm the intrinsic relationship between SPL and this implicit objective, and thus verifies previous robustness analysis of SPL on the basis of the understanding of such relationship. Besides, we have used\nsome new theoretical skills for the proof of convergence, which inclines to be beneficial to the previous MM and EM convergence theories to a certain extent.\nConflict of interest The authors declare that they have no conflict of interest."}, {"heading": "Appendix A Proof of Theorem 1", "text": "Theorem 1 is actually a corollary of a stronger version of Zangwill\u2019s global convergence theorem [15, page 91]. We first need to give the following lemmas.\nLemma 1. If f is lsc, xn \u2192 x, and {f(xn)} is non-increasing, then f(xn) \u2192 f(x). Proof.\nf(x) = lim inf n\u2192\u221e f(xn) = lim k\u2192\u221e inf n>k f(xn) = inf n>1 f(xn) = lim n\u2192\u221e f(xn).\nLemma 2. Suppose that X is a finite-dimensional Euclidean space, M is a set-valued mapping from X to X and that {xk} is produced by M , which means\nxk+1 \u2208 M(xk), \u2200k > 0.\n\u0393 is a subset of X that we are interested at, called the \u201dsolution set\u201d and satisfying\n1. There is a compact subset K, such that xk \u2208 K,\u2200k,\n2. M is outer semicontinuous on X \\ \u0393, that is\nxk \u2192 x in X \\ \u0393 =\u21d2 M(xk) \u2192 M(x).\n3. There is a lsc function G defined on X, such that\n(a) G(y) < G(x), \u2200y \u2208 M(x), x /\u2208 \u0393,\n(b) G(y) 6 G(x), \u2200y \u2208 M(x), x \u2208 \u0393,\nthen all the cluster points of {xk} are in \u0393, and \u2203x\u0304 \u2208 \u0393, such that G(xk) is non-increasing and convergent to G(x\u0304).\nNote: we will repeatedly use the fact that {G(xk)} is non-increasing. Without loss of generality, we can assume that\nn1 < n2 < \u00b7 \u00b7 \u00b7 when we take a subsequence {xnk} of {xn}. Proof. (1) Suppose x\u2217 is a cluster point of {xk}. The existence of x \u2217 is guaranteed by the compactness of K. Thus there exists a subsequence {xnk}, such that xnk \u2192 x \u2217, (k \u2192 \u221e). Since {G(xk)} is non-increasing, based on Lemma 3, it holds that\nG(x\u2217) = lim k\u2192\u221e G(xnk ).\nDenote G\u2217 = G(x\u2217), and then we prove that G(xn) \u2192 G\u2217(n \u2192 \u221e). This is because \u2200\u01eb > 0, \u2203k0 > 0, such that\nG(xnk )\u2212G \u2217 < \u01eb, \u2200k > k0.\nWhen n > nk0 ,\nG(xn) \u2212G \u2217 = G(xn)\u2212G(xnk0 ) +G(xnk0 )\u2212G \u2217 < 0 + \u01eb = \u01eb.\nThere exists k1 > 0, such that n < nk1 , and thus\nG(xn)\u2212G \u2217 = G(xn)\u2212G(xnk1 ) +G(xnk1 ) \u2212G \u2217 > 0 + 0 = 0.\nTherefore,\n0 6 G(xn)\u2212G \u2217 < \u01eb, \u2200n > nk0 ,\nwhich indicates G(xn) \u2192 G\u2217.\n(2) If x\u2217 /\u2208 \u0393, take a subsequence\nyk = xnk+1 \u2208 M(xnk ).\nSince yk all lie in K, there exists a subsequence {ykl}, such that ykl \u2192 x\u0304, (l \u2192 \u221e). Since M is outer semicontinuous, x\u0304 \u2208 M(x\u2217). Based on Lemma 3, we know that G(ykl) \u2192 G(x\u0304), (l \u2192 \u221e). Due to the properties of G,\nG(x\u0304) < G(x\u2217) = lim n\u2192\u221e G(xn) = lim l\u2192\u221e G(ykl) = G(x\u0304),\na contradiction.\nWe then provide a proof of Theorem 1.\nProof. [Proof of Theorem 1]\nLet \u0393 be the set of critical points of G, and\nM(w\u2217) = argmin w U(w|w\u2217).\nBy the descending property of MM algorithm, G(w) 6 G(w\u2217), \u2200w \u2208 M(w\u2217). Condition 3b is satisfied.\nCondition 1: since G is lsc and level-bounded, K = levG6w0 is closed and bounded, and thus compact. By the descending\nproperty of MM algorithm, all the parameters wk lie in K.\nCondition 2: suppose wk \u2192 w\u2217, vk \u2192 v\u2217, vk \u2208 M(wk), and then \u2200w \u2208 RD, it holds that\nU(vk |wk) 6 U(w|wk).\nTaking infimal limit on both sides when k \u2192 \u221e, we have\nU(v\u2217|w\u2217) = lim inf k\u2192\u221e U(vk |wk) 6 lim inf k\u2192\u221e U(w|wk) = U(w|w\u2217).\nThus v\u2217 \u2208 M(w\u2217), which means M is outer semi-continuous.\nCondition 3a: If w\u2217 /\u2208 \u0393, then\n0 /\u2208 \u2202G(w\u2217) \u2283 \u2202U(w\u2217|w\u2217).\nBy the generalized Fermat theorem (see [8, 10.1]), w\u2217 is not a minima of U(\u00b7|w\u2217), i.e., w\u2217 /\u2208 M(w\u2217). Since \u2200w \u2208 M(w\u2217),\nG(w) 6 U(w|w\u2217) < U(w\u2217|w\u2217) = G(w\u2217).\nAll the conditions of the proceeding theorem are satisfied. The proof is then completed."}, {"heading": "Appendix B Proof of Theorem 3", "text": "Similar to [8, 5.41], we give the following definition:\nDefinition 1. A sequence of set-valued mappings Mk converges outer semicontinuously to another set-valued mapping M , if\nlim sup k\u2192\u221e Mk(xk) \u2282 M(x\u0304), \u2200xk \u2192 x\u0304,\nthat is,\nxk \u2192 x\u0304, vk \u2208 Mk(xk), vk \u2192 v\u0304 =\u21d2 v\u0304 \u2208 M(x\u0304).\nBefore giving proof of Theorem 3, we need to prove the following two lemmas.\nLemma 3. Let X be an Euclidean space with finite dimension, and M,Mk, k = 1, 2, \u00b7 \u00b7 \u00b7 be set-valued mappings from X to itself. Suppose that Mk converges outer semicontinuously to M , and that {xk} is produced by {Mk}, which means\nxk+1 \u2208 Mk(xk), \u2200k.\nLet \u0393 be an arbitrary set, called the \u201dsolution set\u201d, satisfying\n1. There is a compact set K such that xk \u2208 K,\u2200k, 2. There is a lsc \u03b1 defined on X, such that\n(a) \u03b1(y) < \u03b1(x), \u2200y \u2208 M(x), x /\u2208 \u0393; (b) There is a sequence of non-negative numbers {\u01ebk} \u2208 l 1, that is \u2211 k \u01ebk < \u221e, and\n\u03b1(yk+1) 6 \u03b1(x) + \u01ebk, \u2200yk+1 \u2208 Mk(x),\u2200x, \u2200k.\nThen all the cluster points of {xk} lie in \u0393, and \u2203x\u0304 \u2208 \u0393, such that \u03b1(xk) converges to \u03b1(x\u0304). Proof. (1) Set rk = \u2211 j>k \u01ebj , then rk \u2192 0, and\n\u03b1(xk+1) + rk+1 6 \u03b1(xk) + \u01ebk + rk+1 = \u03b1(xk) + rk.\nThus {\u03b1(xk) + rk} is non-increasing.\n(2) Let x\u2217 be a cluster point of {xk}, and then there exists a subsequence {xnk}, such that xnk \u2192 x \u2217. Since \u03b1 is lsc,\nwe have\n\u03b1(x\u2217) = lim inf k \u03b1(xnk ) = lim inf k (\u03b1(xnk ) + rnk ) = lim k (\u03b1(xnk ) + rnk ) = lim k \u03b1(xnk ).\nThe second equality holds because\nlim inf k \u03b1(xnk ) 6 lim inf k (\u03b1(xnk ) + rnk ) 6 lim inf k \u03b1(xnk ) + lim sup k rnk = lim inf k \u03b1(xnk ).\nAnd we can prove in the same way as in (1) of Lemma 2 that\nlim n \u03b1(xn) = \u03b1(x \u2217).\n(3) We need to show that x\u2217 \u2208 \u0393. Suppose not, take\nyk = xnk+1 \u2208 Mnk (xnk ),\nDue to the compactness of K, there is a subsequence {ykl} of {yk}, such that \u2203x\u0304 \u2208 K, ykl \u2192 x\u0304. We can argue in the same way as in (2) to show that \u03b1(ykl ) \u2192 \u03b1(x\u0304). Since\nykl = xnkl+1 \u2208 Mnk l (xnk l ),\nand Mk converges outer semicontinuously to M , we have x\u0304 \u2208 M(x \u2217). Thus\n\u03b1(x\u0304) < \u03b1(x\u2217) = lim n \u03b1(xn) = lim l \u03b1(ykl ) = \u03b1(x\u0304),\na contradiction.\nThe proof is then completed.\nNow we can prove another lemma using above theoretical result.\nLemma 4. Let F : RD \u2192 R be the objective of MM algorithm. Suppose that F is lsc and level-bounded, and that the surrogate function at w\u2217 is U(\u00b7|w\u2217). In addition, suppose U(\u00b7|\u00b7) is lsc as a function defined on R2D whose subgradient satisfies\n\u2202U(w|w) \u2282 \u2202F (w), \u2200w \u2208 RD ,\nwhere \u2202U(w|w\u2217) is the partial subdifferential with respect to w. Then for any initial parameter w0, all the cluster points of the sequence {wk} produced by MM algorithm \u201dwith errors\u201d are still critical points of F . Proof. We prove by a direct application of Lemma 3. Let \u0393 be the set consisting of all the critical points of F , \u03b1 = F , and\nMk(w \u2217) = {w : U(w|w\u2217) 6 minU(\u00b7|w\u2217) + \u01ebk}.\nM is the same as before:\nM(w\u2217) = argmin w U(w|w\u2217).\nWe first need to show that Mk converges outer semicontinuously to M . Suppose w k \u2192 w\u0304, vk \u2208 Mk(w k), vk \u2192 v\u0304, and then \u2200w,\nU(vk |wk) 6 minU(\u00b7|wk) + \u01ebk 6 U(w|w k) + \u01ebk.\nTaking infimal limit on both sides when k \u2192 \u221e, we have\nU(v\u0304|w\u0304) = lim inf k U(vk |wk) 6 lim inf k\n(\nU(w|wk) + \u01ebk\n)\n6 lim inf k U(w|wk) + lim sup k \u01ebk = U(w|w\u0304),\nwhich means v\u0304 \u2208 M(w\u0304). Thus Mk converges outer semicontinuously to M .\nCondition 1: F is level-bounded, thus\nK(w0) = {w : F (w) 6 F (w0) + \u2211\nk\n\u01ebk}\nis bounded. Since F is also lsc, K(w0) is closed and hence compact. By (1) of Lemma 3, wk all lie in K(w0).\nCondition 2a: If w\u2217 /\u2208 \u0393, then\n0 /\u2208 \u2202F (w\u2217) \u2283 \u2202U(w\u2217|w\u2217).\nBy the generalized Fermat theorem, w\u2217 is not a minima of U(\u00b7|w\u2217), and hence w\u2217 /\u2208 M(w\u2217). It follows that \u2200w \u2208 M(w\u2217),\nF (w) 6 U(w|w\u2217) < U(w\u2217|w\u2217) = F (w\u2217).\nCondition 2b: Let v \u2208 Mk(w), and then\nU(v|w) 6 minU(\u00b7|w) + \u01ebk.\nThus,\nF (v) 6 U(v|w) 6 minU(\u00b7|w) + \u01ebk 6 U(w|w) + \u01ebk = F (w) + \u01ebk.\nTherefore, all the conditions of Lemma 3 are satisfied and we have finished the proof.\nJust like the proof of Theorem 2, Theorem 3 can be easily proved by directly utilizing the results of the above Lemma\n4. We omit the proof here."}], "references": [{"title": "Easy samples first: self-paced reranking for zeroexample multimedia search", "author": ["L. Jiang", "D. Meng", "T. Mitamura", "A. Hauptmann"], "venue": "In ACM MM,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Learning the easy things first: Self-paced visual category discovery", "author": ["Y. Lee", "K. Grauman"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Lee and Grauman.,? \\Q2010\\E", "shortCiteRegEx": "Lee and Grauman.", "year": 2010}, {"title": "What Objective Does Self-paced Learning Indeed Optimize", "author": ["D. Meng", "Q. Zhao", "L. Jiang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Meng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2017}, {"title": "Self-paced learning for long-term tracking", "author": ["J. Supan\u010di\u010d III", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "III and Ramanan.,? \\Q2013\\E", "shortCiteRegEx": "III and Ramanan.", "year": 2013}, {"title": "Shifting weights: Adapting object detectors from image to video", "author": ["K. Tang", "V. Ramanathan", "F. Li", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2012}, {"title": "Parameter convergence for EM and MM algorithms", "author": ["Florin Vaida"], "venue": "Statistica Sinica, pages 831\u2013840,", "citeRegEx": "Vaida.,? \\Q2005\\E", "shortCiteRegEx": "Vaida.", "year": 2005}, {"title": "On the convergence properties of the EM algorithm", "author": ["CF Jeff Wu"], "venue": "The Annals of statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "CMU-Informedia@TRECVID 2014 multimedia eventdetection (MED)", "author": ["S. Yu", "L. Jiang", "Z. Mao"], "venue": "In TRECVID Video Retrieval Evaluation Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Semi-supervised learning through adaptive laplacian graph trimming", "author": ["Zongsheng Yue", "Deyu Meng", "Juan He", "Gemeng Zhang"], "venue": "Image and Vision Computing,", "citeRegEx": "Yue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2016}, {"title": "Nonlinear programming: a unified approach. Prentice-Hall international series in management", "author": ["W.I. Zangwill"], "venue": null, "citeRegEx": "Zangwill.,? \\Q1969\\E", "shortCiteRegEx": "Zangwill.", "year": 1969}, {"title": "A self-paced multiple-instance learning framework for co-saliency detection", "author": ["Dingwen Zhang", "Deyu Meng", "Chao Li", "Lu Jiang", "Qian Zhao", "Junwei Han"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Self-paced learning for matrix factorization", "author": ["Qian Zhao", "Deyu Meng", "Lu Jiang", "Qi Xie", "Zongben Xu", "Alexander G Hauptmann"], "venue": "In AAAI,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Self-paced learning (SPL) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the SPL regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the SPL paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of SPL. Specifically, we prove that the learning process of SPL always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between SPL and this implicit objective, and makes the previous robustness analysis on SPL complete and theoretically rational.", "creator": "LaTeX with hyperref package"}}}