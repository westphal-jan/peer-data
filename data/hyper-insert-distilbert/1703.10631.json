{"id": "1703.10631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention", "abstract": "deep neural perception and control causal networks algorithms are likely to be naturally a key empirical component of the self - driving reactive vehicles. these models each need to be explainable - ideally they should provide really easy - easier to - interpret creative rationales for adjusting their behavior - simulated so thoroughly that passengers, insurance agents companies, law : enforcement, developers etc., and can understand what precisely triggered a particular behavior. approximately here we explore the use of visual cognition explanations. comparing these explanations roughly take the initial form of real - time stimulus highlighted playing regions of implementing an overall image that causally significantly influence the network's output ( steering control ). our approach is two - steps stage. in the first causal stage, we literally use a purely visual reactive attention directing model to train a convolution network relating end - off to - end tasks from direct images to steering operating angle. essentially the imagery attention model naturally highlights image driving regions that potentially indirectly influence the network'external s input output. some areas of these are true influences, but some details are spurious. presently we immediately then indirectly apply a new causal filtering step mechanism to determine determined which input directed regions actually necessarily influence the output. this input produces more succinct visual explanations and more still accurately exposes the network's potential behavior. meanwhile we literally demonstrate twice the effectiveness shown of engaging our model on three datasets versus totaling averaging 16 hours of independent driving. we could first only show that brain training with attention optimization does not degrade the performance of executing the end - to - final end network. then recently we second show that conversely the network causally cues it on a variety of stimulus features that are used by humans performed while driving.", "histories": [["v1", "Thu, 30 Mar 2017 18:37:49 GMT  (2711kb,D)", "http://arxiv.org/abs/1703.10631v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jinkyu kim", "john canny"], "accepted": false, "id": "1703.10631"}, "pdf": {"name": "1703.10631.pdf", "metadata": {"source": "CRF", "title": "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention", "authors": ["Jinkyu Kim"], "emails": ["canny}@berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "Self-driving vehicle control has made dramatic progress in the last several years, and many auto vendors have pledged large-scale commercialization in a 2-3 year time frame. These controllers use a variety of approaches but recent successes [3] suggests that neural networks will be widely used in self-driving vehicles. But neural networks are notoriously cryptic - both network architecture and hidden layer activations may have no obvious relation to the function being estimated by the network. An exception to the rule is visual attention networks [26, 21, 7]. These networks provide spatial attention maps - areas of the image\nthat the network attends to - that can be displayed in a way that is easy for users to interpret. They provide their attention maps instantly on images that are input to the network, and in this case on the stream of images from automobile video. As we show from our examples later, visual attention maps lie over image areas that have intuitive influence on the vehicle\u2019s control signal.\nBut attention maps are only part of the story. Attention is a mechanism for filtering out non-salient image content. But attention networks need to find all potentially salient image areas and pass them to the main recognition network (a CNN here) for a final verdict. For instance, the attention network will attend to trees and bushes in areas of an image where road signs commonly occur. Just as a human will use peripheral vision to determine that \u201dthere is something there\u201d, and then visually fixate on the item to determine what it actually is. We therefore post-process the attention network\u2019s output, clustering it into attention \u201dblobs\u201d and then mask (set the attention weights to zero) each blob to determine the effect on the end-to-end network output. Blobs that have an causal effect on network output are retained while those that do not are removed from the visual map presented to the user.\nFigure 1 shows an overview of our model. Our approach can be divided into three steps: (1) Encoder: convolutional feature extraction, (2) Coarse-grained decoder by visual attention mechanism, and (3) Fine-grained decoder: causal visual saliency detection and refinement of attention map. Our contributions are as follows:\n\u2022 We show that visual attention heat maps are suitable \u201dexplanations\u201d for the behavior of a deep neural vehicle controller, and do not degrade control accuracy. \u2022 We show that attention maps comprise \u201dblobs\u201d that can\nbe segmented and filtered to produce simpler and more accurate maps of visual saliency. \u2022 We demonstrate the effectiveness of using our model\nwith three large real-world driving datasets that contain over 1,200,000 video frames (approx. 16 hours).\nar X\niv :1\n70 3.\n10 63\n1v 1\n[ cs\n.C V\n] 3\n0 M\nar 2\n\u2022 We illustrate typical spurious attention sources in driving video and quantify the reduction in explanation complexity from causal filtering."}, {"heading": "2. Related Works", "text": ""}, {"heading": "2.1. End-to-End Learning for Self-driving Cars", "text": "Self-driving vehicle controllers can be classified as: mediated perception approaches and end-to-end learning approaches. The mediated perception approach depends on recognizing human-designated features (i.e., lane markings and cars) in a controller with if-then-else rules. Some examples include Urmson et al. [24], Buehler et al. [4], and Levinson et al. [18].\nRecently there is growing interest in end-to-end learning vehicle control. Most of these approaches learn a controller by supervised regression to recordings from human drivers. The training data comprise video from one or more vehicle cameras, and the control outputs (steeting and possible acceleration and braking) from the driver. ALVINN (Autonomous Land Vehicle In a Neural Network) [19] was the first attempt to use neural network for directly mapping images to navigate the direction of the vehicle. More recently Bojarski et al. [3] demonstrated good performance with convolutional neural networks (CNNs) to directly map images from a front-view camera to steering controls. Xu et al. [25] proposed an end-to-end egomotion prediction approach that takes raw pixels and prior vehicle state signals as inputs and predicts several a sequence of discretized actions (i.e., straight, stop, left-turn, and right-turn). These\nmodels show good performance but their behavior is opaque and uninterpretable.\nAn intermediate approach was explored in Chen et al. [6] who defined human-interpretable intermediate features such as the curvature of lane, distances to neighboring lanes, and distances from the front-located vehicles. A CNN is trained to produce these features, and a simple controller maps them to steering angle. They also generated deconvolution maps to show image areas that affected network output. However, there were several difficulties with that work: (i) use of the intermediate layer caused significant degradation (40% or more) of control accuracy (ii) the intermediate feature descriptors provide a limited and adhoc vocabulary for explanations and (iii) the authors noted the presence of spurious input features but there was no attempt to remove them. By contrast, our work shows that state-of-the-art driving models can be made interpretable without sacrificing accuracy, that attention models provide more robust image annotation, and causal analysis further improves explanation saliency."}, {"heading": "2.2. Visual Explanation", "text": "In a landmark work, Zeiler and Fergus [28] used \u201ddeconvolution\u201d to visualize layer activations of convolutional networks. LeCun et al. [16] provides textual explanations of images as automatically-generated captions. Building on this work, Bojarski et al. [2] developed a richer notion of \u201dcontribution\u201d of a pixel to the output. However a difficulty with deconvolution-style approaches is the lack of formal\nmeasures of how the network output is affected by spatiallyextended features (rather than pixels). Attention-based approaches like ours directly extract areas of the image that did not affect network output (because they were masked out by the attention model), and causal filtering further removes spurious image areas. Hendricks et al. [11] trains a deep network to generate species specific explanation without explicitly identifying semantic features. Also, Justin Johnson et al. [14] proposes DenseCap which uses fully convolutional localization networks for dense captioning, their paper achieves both localizing objects and describing salient regions in images using natural langauge. In reinforcement learning, Zrihem et al. [27] proposes a visualization method to interpret the agents action by describing Markov Decision Process model as a directed graph on a t-SNE map."}, {"heading": "3. Method", "text": ""}, {"heading": "3.1. Preprocessing", "text": "Our model predicts continuous steering angle commands from input raw pixels in an end-to-end manner. As discussed by Bojarski et al. [3], our model predicts the inverse turning radius u\u0302t (= r\u22121t , where rt is the turning radius) at every timestep t instead of steering angle commands, which depends on the vehicle\u2019s steering geometry and also result in numerical instability when predicting near zero steering angle commands. The relationship between the inverse turning radius ut and the steering angle command \u03b8t can be approximated by Ackermann steering geometry [20] as follows:\n\u03b8t = fsteers(ut) = utdwKs(1 +Kslipvt 2) (1)\nwhere \u03b8t in degrees and vt (m/s) is a steering angle and a velocity at time t, respectively. Ks, Kslip, and dw are vehicle-specific parameters. Ks is a steering ratio between the turn of the steering and the turn of the wheels. Kslip represents the relative motion between a wheel and the surface of road. dw is the length between the front and rear wheels. Our model therefore needs two measurements for training: timestamped vehicle\u2019s speed and steering angle commands.\nTo reduce computational cost, each raw input image is down-sampled and resized to 80\u00d7160\u00d73 with nearestneighbor scaling algorithm. For images with different raw aspect ratios, we cropped the height to match the ratio before down-sampling. We also normalized pixel values to [0, 1] in HSV colorspace.\nWe utilize a single exponential smoothing method [13] to reduce the effect of human factors-related performance variation and the effect of measurement noise. Formally, given a smoothing factor 0 \u2264 \u03b1s \u2264 1, the simple exponen-\ntial smoothing method is defined as follows:( \u03b8\u0302t v\u0302t ) = \u03b1s ( \u03b8t vt ) + (1\u2212 \u03b1s) ( \u03b8\u0302t\u22121 v\u0302t\u22121 ) (2)\nwhere \u03b8\u0302t and v\u0302t are the smoothed time-series of \u03b8t and vt, respectively. Note that they are same as the original timeseries when \u03b1s = 1, while values of \u03b1s closer to zero have a greater smoothing effect and are less responsive to recent changes. The effect of applying smoothing methods is summarized in Section 4.4."}, {"heading": "3.2. Encoder: Convolutional Feature Extraction", "text": "We use a convolutional neural network to extract a set of encoded visual feature vector, which we refer to as a convolutional feature cube xt. Each feature vectors may contain high-level object descriptions that allow the attention model to selectively pay attention to certain parts of an input image by choosing a subset of feature vectors.\nAs depicted in Figure 1, we use a 5-layered convolution network that is utilized by Bojarski et al. [3] to learn a model for self-driving cars. As discussed by Lee et al. [17], we omit max-pooling layers to prevent spatial locational information loss as the strongest activation propagates through the model. We collect a three-dimensional convolutional feature cube xt from the last layer by pushing the preprocessed image through the model, and the output feature cube will be used as an input of the LSTM layers, which we will explain in Section 3.3. Using this convolutional feature cube from the last layer has advantages in generating high-level object descriptions, thus increasing interpretability and reducing computational burdens for a real-time system.\nFormally, a convolutional feature cube of size W\u00d7H\u00d7D is created at each timestep t from the last convolutional layer. We then collect xt, a set of L = W \u00d7 H vectors, each of which is a D-dimensional feature slice for different spatial parts of the given input.\nxt = {xt,1, xt,2, . . . , xt,L} (3)\nwhere xt,i \u2208 RD for i \u2208 {1, 2, . . . , L}. This allows us to focus selectively on different spatial parts of the given image by choosing a subset of these L feature vectors."}, {"heading": "3.3. Coarse-Grained Decoder: Visual Attention", "text": "The goal of soft deterministic attention mechanism \u03c0({xt,1, xt,2, . . . , xt,L}) is to search for a good context vector yt, which is defined as a combination of convolutional feature vectors xt,i, while producing better prediction accuracy. We utilize a deterministic soft attention mechanism that is trainable by standard back-propagation methods, which thus has advantages over a hard stochastic attention mechanism that requires reinforcement learning. Our\nmodel feeds \u03b1 weighted context yt to the system as discuss by several works [21, 26]:\nyt = fflatten(\u03c0({\u03b1t,i}, {xt,i})) = fflatten({\u03b1t,ixt,i})\n(4)\nwhere i = {1, 2, . . . , L}. \u03b1t,i is a scalar attention weight value associated with a certain grid of input image in such that \u2211 i \u03b1t,i = 1. These attention weights can be interpreted as the probability over L convolutional feature vectors that the location i is the important part to produce better estimation accuracy. fflatten is a flattening function. yt is thus D\u00d7L-dimensional vector that contains convolutional feature vectors weighted by attention weights. Note that, our attention mechanism \u03c0({\u03b1t,i}, {xt,i}) is different from the previous works [21, 26], which use the \u03b1 weighted average context yt = \u2211L i=1 \u03b1t,ixt,i. We observed that this change significantly improves overall prediction accuracy. The performance comparison is explained in Section 4.5.\nAs we summarize in Figure 1, we use a long shortterm memory (LSTM) network [12] that predicts the inverse turning radius u\u0302t and generates attention weights {\u03b1t,i} at each timestep t conditioned on the previous hidden state ht and a current convolutional feature cube xt. More formally, let us assume a hidden layer fattn(xt,i, ht\u22121) conditioned on the previous hidden state ht\u22121 and the current feature vectors {xt,i}. The attention weight {\u03b1t,i} for each spatial location i is then computed by multinomial logistic regression (i.e., softmax regression) function as follows:\n\u03b1t,i = exp(fattn(xt,i, ht\u22121))\u2211L j=1 exp(fattn(xt,j , ht\u22121))\n(5)\nOur network also predicts inverse turning radius u\u0302t as an output with additional hidden layer fout(yt, ht) conditioned on the current hidden state ht and \u03b1 weighted context yt.\nTo initialize memory state ct and hidden state ht of LSTM network, we follow Xu et al. [26] by averaging of the feature slices x0,i at initial time fed through two additional hidden layers: finit,c and finit,h.\nc0 = finit,c\n( 1\nL L\u2211 i=1 x0,i\n) , h0 = finit,h ( 1\nL L\u2211 i=1 x0,i ) (6)\nAs discussed by Xu et al. [26], doubly stochastic regularization can encourage the attention model to at different parts of the image. At time t, our attention model predicts a scalar \u03b2=sigm(f\u03b2(ht\u22121)) with an additional hidden layer f\u03b2 conditioned on the previous hidden state ht\u22121 such that\nyt = sigm(f\u03b2(ht\u22121))fflatten({\u03b1t,ixt,i}) (7)\nWe use the following penalized loss function L1:\nL1(ut, u\u0302t) = T\u2211 t=1 |ut \u2212 u\u0302t|+ \u03bb L\u2211 i=1\n( 1\u2212\nT\u2211 t=1 \u03b1t,i\n) (8)\nwhere T is the length of time steps, and \u03bb is a penalty coefficient that encourages the attention model to see different parts of the image at each time frame. Section 4.3 describes the effect of using regularization."}, {"heading": "3.4. Fine-Grained Decoder: Causality Test", "text": "The last step of our pipeline is a fine-grained decoder, in which we refine a map of attention and detect local visual saliencies. Though an attention map from our coarsegrained decoder provides probability of importance over a 2D image space, our model needs to determine specific regions that cause a causal effect on prediction performance. To this end, we assess a decrease in performance when a local visual saliency on an input raw image is masked out.\nWe first collect a consecutive set of attention weights {\u03b1t,i} and input raw images {It} for a user-specified T timesteps. We then create a map of attention, which we referMt as defined:Mt = fmap({\u03b1t,i}). Our 5-layer convolutional neural network uses a stack of 5 \u00d7 5 and 3 \u00d7 3 filters without any pooling layer, and therefore the input image of size 80\u00d7 160 is processed to produce the output feature cube of size 10 \u00d7 20 \u00d7 64, while preserving its aspect ratio. Thus, we use fmap({\u03b1t,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).\nTo extract a local visual saliency, we first randomly sample 2D N particles with replacement over an input raw image conditioned on the attention map Mt. Note that, we also use time-axis as the third dimension to consider temporal features of visual saliencies. We thus store spatiotemporal 3D particles P \u2190 P \u222a {Pt, t} (see Figure 2 (C)).\nWe then apply a clustering algorithm to find a local visual saliency by grouping 3D particles P into clusters {C} (see Figure 2 (D)). In our experiment, we use DBSCAN [9], a density-based clustering algorithm that has advantages to deal with a noisy dataset because they group particles together that are closely packed, while marking particles as outliers that lie alone in low-density regions. For points of each cluster c and each time frame t, we compute a convex hull H(c) to find a local region of each visual saliency detected (see Figure 2 (E, F)).\nFor points of each cluster c and each time frame t, we iteratively measure a decrease of prediction performance with an input image which we mask out a local visual saliency. We compute a convex hull H(c) to find a local, and mask out each visual saliency by assigning zero values for all pixels lying inside each convex hull. Each causal visual saliency is generated by warping into a fixed spatial resolution (=64\u00d764) as shown in Figure 2 (G, H)."}, {"heading": "4. Result and Discussion", "text": ""}, {"heading": "4.1. Datasets", "text": "As explained in Table 1, we obtain two large-scale datasets that contain over 1,200,000 frames (\u224816 hours) collected from Comma.ai [8], Udacity [23], and Hyundai Center of Excellence in Integrated Vehicle Safety Systems and Control (HCE) under a research contract. These three datasets acquired contain video clips captured by a single front-view camera mounted behind the windshield of the vehicle. Alongside the video data, a set of time-stamped sensor measurement is contained, such as vehicle\u2019s velocity, acceleration, steering angle, GPS location and gyroscope angles. Thus, these datasets are ideal for self-driving studies. Note that, for sensor logs unsynced with the timestamps of video data, we use the estimates of the interpolated measurements. Videos are mostly captured during highway driving in clear weather on daytime, and there included driving on other road types, such as residential roads (with and without lane markings), and contains the whole driver\u2019s activities such as staying in a lane and switching lanes. Note also that, we exclude frames when the vehicle stops which happens when v\u0302t <1 m/s."}, {"heading": "4.2. Training and Evaluation Details", "text": "To obtain a convolutional feature cube xt, we train the 5- layer CNNs explained in Section 3.2 by using additional 5- layer fully connected layers (i.e., # hidden variables: 1164, 100, 50, and 10, respectively), of which output predicts the measured inverse turning radius ut. Incidentally, instead of using addition fully-connected layers, we could also obtain a convolutional feature cube xt by training from scratch with the whole network. In our experiment, we obtain the 10\u00d720\u00d764-dimensional convolutional feature cube, which is then flattened to 200\u00d764 and is fed through the coarsegrained decoder. Other recent types of more recent expres-\nsive networks may give a performance boost over our CNN configuration. However, exploration of other convolutional architectures would be out of our scope.\nWe experiment with various numbers of LSTM layers (1 to 5) of the soft deterministic visual attention model but did not observe any significant improvements in model performance. Unless otherwise stated, we use a single LSTM layer in this experiment. For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.5 at hidden state connections and Xavier initialization [10]. We randomly sample a mini-batch of size 128, each of batch contains a set Consecutive frames of length T = 20. Our model took less than 24 hours to train on a single NVIDIA Titan X Pascal GPU. Our implementation is based on Tensorflow [1] and code will be publicly available upon publication.\nTwo datasets (Comma.ai [8] and HCE) we used were available with images captured by a single front-view cam-\nera. This makes it hard to use the data augmentation technique proposed by Bojarski et al. [3], which generated images with artificial shifts and rotations by using two additional off-center images (left-view and right-view) captured by the same vehicle. Data augmentation may give a performance boost, but we report performance without data augmentation."}, {"heading": "4.3. Effect of Choosing Penalty Coefficient \u03bb", "text": "Our model provides a better way to understand the rationale of the models decision by visualizing where and what the model sees to control a vehicle. Figure 3 shows a consecutive input raw images (with sampling period of 5 seconds) and their corresponding attention maps (i.e., Mt = fmap({\u03b1t,i})). We also experiment with three different penalty coefficients \u03bb \u2208 {0, 10, 20}, where the model is encouraged to pay attention to wider parts of the image (see differences between the bottom 3 rows in Figure 3 ) as we have larger \u03bb. For better visualization, an attention map is overlaid by an input raw image and color-coded; for example, red parts represent where the model pays attention. For quantitative analysis, prediction performance in terms of mean absolute error (MAE) is explained on the bottom of each figure. We observe that our model is indeed able to pay attention on road elements, such as lane markings, guardrails, and vehicles ahead, which is essential for human to drive."}, {"heading": "4.4. Effect of Varying Smoothing Factors", "text": "Recall from Section 3.1 that the single exponential smoothing method [13] is used to reduce the effect of human factors variation and the effect of measurement noise for two sensor inputs: steering angle and velocity. A robust\nmodel for autonomous vehicles would yield consistent performance, even when some measurements are noisy. Figure 4 shows the prediction performance in terms of mean absolute error (MAE) on a comma.ai testing data set. Various smoothing factors \u03b1s \u2208 {0.01, 0.05, 0.1, 0.3, 0.5, 1.0} are used to assess the effect of using smoothing methods. With setting \u03b1s=0.05, our model for the task of steering estimation performs the best. Unless otherwise stated, we will use \u03b1s as 0.05."}, {"heading": "4.5. Quantitative Analysis", "text": "In Table 2, we compare the prediction performance with alternatives in terms of MAE. We implement alternatives that include the work by Bojarski et al. [3], which used an identical base CNN and a fully-connected network (FCN) without attention. To see the contribution of LSTMs, we also test a CNN and LSTM, which is identical to ours but does not use the attention mechanism. For our model, we\ntest with three different values of penalty coefficients \u03bb \u2208 {0, 10, 20}.\nOur model shows competitive prediction performance than alternatives. Our model shows 1.18\u20134.15 in terms of MAE on testing dataset. This confirms that incorporation of attention does not degrade control accuracy. The average run-time for our model and alternatives took less than a day to train each dataset."}, {"heading": "4.6. Effect of Causal Visual Saliencies", "text": "Recall from Section 3.4, we post-process the attention networks output by clustering it into attention blobs and filtering if they have an causal effect on network output. Figure 5 (A) shows typical examples of an input raw image, an attention networkss output with spurious attention sources, and our refined attention heat map. We observe our model can produce a simpler and more accurate map of visual saliency by filtering out spurious attention blobs. In our experiment, 62% and 58% out of all attention blobs are indeed spurious attention sources on Comma.ai [8] and\nHCE datasets (see Figure 5 (B))."}, {"heading": "5. Conclusion", "text": "We described an interpretable visualization for deep selfdriving vehicle controllers. It uses a visual attention model augmented with an additional layer of causal filtering. We tested with three large-scale real driving datasets that contain over 16 hours of video frames. We showed that (i) incorporation of attention does not degrade control accuracy compared to an identical base CNN without attention (ii) raw attention highlights interpretable features in the image and (iii) causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous 7  Dataset  Model  MAE in degree [SD] Training Testing Comma.ai", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "HCE", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Visualbackprop: visualizing cnns for autonomous driving", "author": ["M. Bojarski", "A. Choromanska", "K. Choromanski", "B. Firner", "L. Jackel", "U. Muller", "K. Zieba"], "venue": "arXiv preprint arXiv:1611.05418,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The DARPA urban challenge: autonomous vehicles in city traffic, volume 56", "author": ["M. Buehler", "K. Iagnemma", "S. Singh"], "venue": "springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The laplacian pyramid as a compact image code", "author": ["P. Burt", "E. Adelson"], "venue": "IEEE Transactions on communications, 31(4):532\u2013540,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1983}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2722\u20132730,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Brain inspired cognitive model with attention for self-driving cars", "author": ["S. Chen", "S. Zhang", "J. Shang", "B. Chen", "N. Zheng"], "venue": "arXiv preprint arXiv:1702.05596,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "A densitybased algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "In Kdd,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Aistats, volume 9, pages 249\u2013256,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "European Conference on Computer Vision, pages 3\u201319. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Forecasting with exponential smoothing: the state space approach", "author": ["R. Hyndman", "A.B. Koehler", "J.K. Ord", "R.D. Snyder"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4565\u20134574,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 609\u2013616. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["J. Levinson", "J. Askeland", "J. Becker", "J. Dolson", "D. Held", "S. Kammel", "J.Z. Kolter", "D. Langer", "O. Pink", "V. Pratt"], "venue": "In Intelligent Vehicles Symposium (IV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Alvinn, an autonomous land vehicle in a neural network", "author": ["D.A. Pomerleau"], "venue": "Technical report, Carnegie Mellon University, Computer Science Department,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Vehicle dynamics and control", "author": ["R. Rajamani"], "venue": "Springer Science & Business Media,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Action recognition using visual attention", "author": ["S. Sharma", "R. Kiros", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.04119,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Autonomous driving in urban environments: Boss and the urban challenge", "author": ["C. Urmson", "J. Anhalt", "D. Bagnell", "C. Baker", "R. Bittner", "M. Clark", "J. Dolan", "D. Duggins", "T. Galatali", "C. Geyer"], "venue": "Journal of Field Robotics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "End-to-end learning of driving models from large-scale video datasets", "author": ["H. Xu", "Y. Gao", "F. Yu", "T. Darrell"], "venue": "arXiv preprint arXiv:1612.01079,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML, volume 14, pages 77\u201381,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Graying the black box: Understanding dqns", "author": ["T. Zahavy", "N.B. Zrihem", "S. Mannor"], "venue": "arXiv preprint arXiv:1602.02658,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pages 818\u2013833. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "These controllers use a variety of approaches but recent successes [3] suggests that neural networks will be widely used in self-driving vehicles.", "startOffset": 67, "endOffset": 70}, {"referenceID": 23, "context": "An exception to the rule is visual attention networks [26, 21, 7].", "startOffset": 54, "endOffset": 65}, {"referenceID": 19, "context": "An exception to the rule is visual attention networks [26, 21, 7].", "startOffset": 54, "endOffset": 65}, {"referenceID": 6, "context": "An exception to the rule is visual attention networks [26, 21, 7].", "startOffset": 54, "endOffset": 65}, {"referenceID": 21, "context": "[24], Buehler et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], and Levinson et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "ALVINN (Autonomous Land Vehicle In a Neural Network) [19] was the first attempt to use neural network for directly mapping images to navigate the direction of the vehicle.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "[3] demonstrated good performance with convolutional neural networks (CNNs) to directly map images from a front-view camera to steering controls.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[25] proposed an end-to-end egomotion prediction approach that takes raw pixels and prior vehicle state signals as inputs and predicts several a sequence of discretized actions (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] who defined human-interpretable intermediate features such as the curvature of lane, distances to neighboring lanes, and distances from the front-located vehicles.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "In a landmark work, Zeiler and Fergus [28] used \u201ddeconvolution\u201d to visualize layer activations of convolutional networks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "[16] provides textual explanations of images as automatically-generated captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] developed a richer notion of \u201dcontribution\u201d of a pixel to the output.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] trains a deep network to generate species specific explanation without explicitly identifying semantic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] proposes DenseCap which uses fully convolutional localization networks for dense captioning, their paper achieves both localizing objects and describing salient regions in images using natural langauge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] proposes a visualization method to interpret the agents action by describing Markov Decision Process model as a directed graph on a t-SNE map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3], our model predicts the inverse turning radius \u00fbt (= r\u22121 t , where rt is the turning radius) at every timestep t instead of steering angle commands, which depends on the vehicle\u2019s steering geometry and also result in numerical instability when predicting near zero steering angle commands.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "The relationship between the inverse turning radius ut and the steering angle command \u03b8t can be approximated by Ackermann steering geometry [20] as follows:", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "We also normalized pixel values to [0, 1] in HSV colorspace.", "startOffset": 35, "endOffset": 41}, {"referenceID": 11, "context": "We utilize a single exponential smoothing method [13] to reduce the effect of human factors-related performance variation and the effect of measurement noise.", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "[3] to learn a model for self-driving cars.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[17], we omit max-pooling layers to prevent spatial locational information loss as the strongest activation propagates through the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "model feeds \u03b1 weighted context yt to the system as discuss by several works [21, 26]:", "startOffset": 76, "endOffset": 84}, {"referenceID": 23, "context": "model feeds \u03b1 weighted context yt to the system as discuss by several works [21, 26]:", "startOffset": 76, "endOffset": 84}, {"referenceID": 19, "context": "Note that, our attention mechanism \u03c0({\u03b1t,i}, {xt,i}) is different from the previous works [21, 26], which use the \u03b1 weighted average context yt = \u2211L i=1 \u03b1t,ixt,i.", "startOffset": 90, "endOffset": 98}, {"referenceID": 23, "context": "Note that, our attention mechanism \u03c0({\u03b1t,i}, {xt,i}) is different from the previous works [21, 26], which use the \u03b1 weighted average context yt = \u2211L i=1 \u03b1t,ixt,i.", "startOffset": 90, "endOffset": 98}, {"referenceID": 10, "context": "As we summarize in Figure 1, we use a long shortterm memory (LSTM) network [12] that predicts the inverse turning radius \u00fbt and generates attention weights {\u03b1t,i} at each timestep t conditioned on the previous hidden state ht and a current convolutional feature cube xt.", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "[26] by averaging of the feature slices x0,i at initial time fed through two additional hidden layers: finit,c and finit,h.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26], doubly stochastic regularization can encourage the attention model to at different parts of the image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Thus, we use fmap({\u03b1t,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).", "startOffset": 104, "endOffset": 107}, {"referenceID": 23, "context": "Thus, we use fmap({\u03b1t,i}) as up-sampling function by the factor of eight followed by Gaussian filtering [5] as discussed in [26] (see Figure 2 (A,B)).", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "In our experiment, we use DBSCAN [9], a density-based clustering algorithm that has advantages to deal with a noisy dataset because they group particles together that are closely packed, while marking particles as outliers that lie alone in low-density regions.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "(C) We randomly sample 3D N = 500 particles over the attention map, and (D) we apply a density-based clustering algorithm (DBSCAN [9]) to find a local visual saliency by grouping particles into clusters.", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "For training, we use Adam optimization algorithm [15] and also use dropout [22] of 0.", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "5 at hidden state connections and Xavier initialization [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Our implementation is based on Tensorflow [1] and code will be publicly available upon publication.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "[3], which generated images with artificial shifts and rotations by using two additional off-center images (left-view and right-view) captured by the same vehicle.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "1 that the single exponential smoothing method [13] is used to reduce the effect of human factors variation and the effect of measurement noise for two sensor inputs: steering angle and velocity.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "[3], which used an identical base CNN and a fully-connected network (FCN) without attention.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable they should provide easy-tointerpret rationales for their behavior so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network\u2019s output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network endto-end from images to steering angle. The attention model highlights image regions that potentially influence the network\u2019s output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network\u2019s behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.", "creator": "LaTeX with hyperref package"}}}