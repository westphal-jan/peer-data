{"id": "1704.00052", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion", "abstract": "we currently present at a nearly novel visual cross - lingual transfer communication method for interactive paradigm completion, the trivial task of mapping similarly a lemma performed to its inflected forms, also using a structural neural transfer encoder - decoder function model, describes the state of learning the art implemented for the monolingual cognitive task. how we use labeled data from examining a high - powered resource language group to increase performance sensitivity on a low - resource language. in experiments on 21 coupled language pairs from their four closely different language families, we obtain graphs up to maximum 58 % higher recall accuracy recall than students without transfer and more show examples that even zero - shot and one - take shot learning are generally possible. we further find that the degree modulation of actual language relatedness strongly profoundly influences the ability to transfer morphological syntax knowledge.", "histories": [["v1", "Fri, 31 Mar 2017 20:39:38 GMT  (971kb,D)", "http://arxiv.org/abs/1704.00052v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katharina kann", "ryan cotterell", "hinrich sch\u00fctze"], "accepted": true, "id": "1704.00052"}, "pdf": {"name": "1704.00052.pdf", "metadata": {"source": "CRF", "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion", "authors": ["Katharina Kann", "Ryan Cotterell"], "emails": ["kann@cis.lmu.de", "ryan.cotterell@jhu.edu", "inquiries@cislmu.org"], "sections": [{"heading": null, "text": "Pr ep\nrin"}, {"heading": "1 Introduction", "text": "Low-resource natural language processing remains an open problem for many tasks of interest. Furthermore, for most languages in the world, high-cost linguistic annotation and resource creation are unlikely to be undertaken in the near future. In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) \u2013 although morphology is easy to annotate compared to syntax and semantics. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data.\nIn morphologically rich languages, individual lexical entries may be realized as distinct inflec-\ntions of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In many languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C\u0327etinog\u0306lu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7\u2192 brought (with Past being the target tag).\nRNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Schu\u0308tze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows the model to apply knowledge distilled from the high-resource training data to the low-resource language as needed. ar X iv :1\n70 4.\n00 05\n2v 1\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 7\nPr ep\nrin t\nWe conduct experiments on 21 language pairs from four language families, emulating a lowresource setting. Our results demonstrate successful transfer of morphological knowledge. We show improvements in accuracy and edit distance of up to 58% (accuracy) and 4.62 (edit distance) over the same model with only in-domain language data on the paradigm completion task. We further obtain up to 44% (resp. 14%) improvement in accuracy for the one-shot (resp. zero-shot) setting, i.e., one (resp. zero) in-domain language sample per target tag. We also show that the effectiveness of morphological transfer depends on language relatedness, measured by lexical similarity."}, {"heading": "2 Inflectional Morphology and Paradigm Completion", "text": "Many languages exhibit inflectional morphology, i.e., the form of an individual lexical entry mutates to show properties such as person, number or case. The citation form of a lexical entry is referred to as the lemma and the collection of its possible inflections as its paradigm. Tab. 1 shows an example of a partial paradigm; we display several forms for the Spanish verbal lemma son\u0303ar. We may index the entries of a paradigm by a morphological tag, e.g., the 2SgPresInd form suen\u0303as in Tab. 1. In generation, the speaker must select an entry of the paradigm given the form\u2019s context. In general, the presence of rich inflectional morphology is problematic for NLP systems as it greatly increases the token-type ratio and, thus, word form sparsity.\nAn important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016). Its goal is to map a lemma to all individual inflections, e.g., (son\u0303ar, 1SgPresInd) 7\u2192 suen\u0303o. There are good solutions for paradigm completion when a large amount of annotated training data is available (Cotterell et al., 2016a).1 In this work, we address the lowresource setting, an up to now unsolved challenge."}, {"heading": "2.1 Transferring Inflectional Morphology", "text": "In comparison to other NLP annotations, e.g., partof-speech (POS) and named entities, morphological inflection does not lend itself easily to transfer.\n1The SIGMORPHON 2016 shared task (Cotterell et al., 2016a) on morphological reinflection, a harder generalization of paradigm completion, found that \u2265 98% accuracy can be achieved in many languages with neural sequence-to-sequence models, improving the state of the art by 10%.\nWe can define a universal set of POS tags (Petrov et al., 2012) or of entity types (e.g., coarse-grained types like person and location or fine-grained types (Yaghoobzadeh and Schu\u0308tze, 2015)), but inflection is much more language-specific. It is infeasible to transfer morphological knowledge from Chinese to Portuguese as Chinese does not use inflected word forms. Transferring named entity recognition, however, among Chinese and European languages works well (Wang and Manning, 2014a). But even transferring inflectional paradigms from morphologically rich Arabic to Portuguese seems difficult as the inflections often mark dissimilar subcategories. In contrast, transferring morphological knowledge from Spanish to Portuguese, two languages with similar conjugations and 89% lexical similarity, appears promising. Thus, we conjecture that transfer of inflectional morphology is only viable among related languages."}, {"heading": "2.2 Formalization of the Task", "text": "We now offer a formal treatment of the crosslingual paradigm completion task and develop our notation. Let \u03a3` be a discrete alphabet for language ` and let T` be a set of morphological tags for `. Given a lemma w` in `, the morphological paradigm (inflectional table) \u03c0 can be formalized as a set of pairs\n\u03c0(w`) = {( fk[w`], tk )} k\u2208T (w`)\n(1)\nwhere fk[w`] \u2208 \u03a3+` is an inflected form, tk \u2208 T` is its morphological tag and T (w`) is the set of slots in the paradigm; e.g., a Spanish paradigm is:\n\u03c0(SON\u0303AR)= {( suen\u0303o, 1SgPresInd ) , . . . , ( son\u0303aran, 3PlPastSbj )}\nParadigm completion consists of predicting the entire paradigm \u03c0(w`) given the lemma w`.\nIn cross-lingual paradigm completion, we consider a high-resource source language `s (lots of training data available) and a low-resource target language `t (little training data available). We denote the source training examples as Ds (with |Ds| = ns) and the target training examples as Dt (with |Dt| = nt). The goal of cross-lingual paradigm completion is to populate paradigms in the low-resource target language with the help of data from the high-resource source language, using only few in-domain examples.\nPr ep\nrin\nt"}, {"heading": "3 Cross-Lingual Transfer as Multi-Task Learning", "text": "We describe our probability model for morphological transfer using terminology from multi-task learning (Caruana, 1997; Collobert et al., 2011). We consider two tasks, training a paradigm completor (i) for a high-resource language and (ii) for a low-resource language. We want to train jointly so we reap the benefits of having related languages. Thus, we define the log-likelihood as\nL(\u03b8)= \u2211\n(k,w`t )\u2208Dt\nlog p\u03b8 (fk[w`t ] | w`t , tk) (2)\n+ \u2211\n(k,w`s )\u2208Ds\nlog p\u03b8(fk[w`s ] | w`s , tk)\nwhere we tie parameters \u03b8 for the two languages together to allow the transfer of morphological knowledge between languages. Each probability distribution p\u03b8 defines a distribution over all possible realizations of an inflected form, i.e., a distribution over \u03a3\u2217. For example, consider the related Romance languages Spanish and French; focusing on one term from each of the summands in Eq. (2) (the past participle of the translation of to visit in each language), we arrive at\nLvisit(\u03b8) = log p\u03b8(visitado | visitar, PastPart) + log p\u03b8(visite\u0301 | visiter, PastPart) (3)\nOur cross-lingual setting forces both transductions to share part of the parameter vector \u03b8, to represent morphological regularities between the two languages in a common embedding space and, thus, to enable morphological transfer. This is no different from monolingual multi-task settings, e.g., jointly training a parser and tagger for transfer of syntax.\nBased on recent advances in neural transducers, we parameterize each distribution as an encoderdecoder RNN, as in (Kann and Schu\u0308tze, 2016b). In their setup, the RNN encodes the input and predicts the forms in a single language. In contrast, we force the network to predict two languages."}, {"heading": "3.1 Encoder-Decoder RNN", "text": "We parameterize the distribution p\u03b8 as an encoderdecoder gated RNN with attention (Bahdanau et al., 2015), the state-of-the-art solution for the monolingual case (Kann and Schu\u0308tze, 2016b). A bidirectional gated RNN encodes the input sequence (Cho et al., 2014) \u2013 the concatenation of (i) the language\ntag, (ii) the morphological tag of the form to be generated and (iii) the characters of the input word \u2013 represented by embeddings. The input to the decoder consists of concatenations of \u2212\u2192 hi and \u2190\u2212 hi , the forward and backward hidden states of the encoder. The decoder, a unidirectional RNN, uses attention: it computes a weight for each hi. Each weight reflects the importance given to that input position. Using the attention weights \u03b1ij , the probability of the output sequence given the input sequence is:\np(y | x1, . . . , x|X|) = |Y |\u220f\nt=1\ng(yt\u22121, st, ct) (4)\nwhere y = (y1, . . . , y|Y |) is the output sequence (a sequence of |Y | characters), x = (x1, . . . x|X|) is the input sequence (a sequence of |X| characters), g is a non-linear function, st is the hidden state of the decoder and ct is the sum of the encoder states hi, weighted by attention weights \u03b1i(st\u22121) which depend on the decoder state:\nct =\n|X|\u2211\ni=1\n\u03b1i(st\u22121)hi (5)\nFig. 1 shows the encoder-decoder. See Bahdanau et al. (2015) for further details."}, {"heading": "3.2 Input Format", "text": "Each source form is represented as a sequence of characters; each character is represented as an embedding. In the same way, each source tag is represented as a sequence of subtags, and each subtag is represented as an embedding. More formally,\nPr ep\nrin t\nwe define the alphabet \u03a3 = \u222a`\u2208L\u03a3` as the set of characters in the languages in L, with L being the set of languages in the given experiment. Next, we define S as the set of subtags that occur as part of the set of morphological tags T = \u222a`\u2208LT`; e.g., if 1SgPresInd \u2208 T , then 1, Sg, Pres, Ind \u2208 S . Note that the set of subtags S is defined as attributes from the UNIMORPH schema (Sylak-Glassman, 2016) and, thus, is universal across languages; the schema is derived from research in linguistic typology.2 The format of the input to our system is S+\u03a3+. The output format is \u03a3+. Both input and output are padded with distinguished BOW and EOW symbols.\nWhat we have described is the representation of Kann and Schu\u0308tze (2016b). In addition, we preprend a symbol \u03bb \u2208 L to the input string (e.g., \u03bb = Es, also represented by an embedding), so the RNN can handle multiple languages simultaneously and generalize over them."}, {"heading": "4 Languages and Language Families", "text": "To verify the applicability of our method to a wide range of languages, we perform experiments on example languages from several different families.\nRomance languages, a subfamily of IndoEuropean, are widely spoken, e.g., in Europe and Latin America. Derived from the common ancestor Vulgar Latin (Harris and Vincent, 2003), they share large parts of their lexicon and inflectional morphology; we expect knowledge among them to be easily transferable.\nWe experiment on Catalan, French, Italian, Portuguese and Spanish. Tab. 2 shows that Spanish \u2013 which takes the role of the low-resource language in our experiments \u2013 is closely related with the other four, with Portuguese being most similar. We hypothesize that the transferability of morphological knowledge between source and target corresponds to the degree of lexical similarity; thus, we expect Portuguese and Catalan to be more beneficial for Spanish than Italian and French.\nThe Indo-European Slavic language family has its origin in eastern-central Europe (Corbett and Comrie, 2003). We experiment on Bulgarian, Macedonian, Russian and Ukrainian (Cyrillic script) and on Czech, Polish and Slovene (Latin script). Macedonian and Ukranian are low-resource\n2Note that while the subtag set is universal, which subtags a language actually uses is language-specific; e.g., Spanish does not mark animacy as Russian does. We contrast this with the universal POS set (Petrov et al., 2012), where it is reasonable to expect that we see all 17 tags in every language.\nlanguages, so we assign them the low-resource role. For Romance and for Uralic, we experiment with groups containing three or four source languages. To arrive at a comparable experimental setup for Slavic, we run two experiments, each with three source and one target language: (i) from Russian, Bulgarian and Czech to Macedonian; and (ii) from Russian, Polish and Slovene to Ukrainian.\nWe hope that the paradigm completor learns similar embeddings for, say, the characters \u201ce\u201d in Polish and \u201c \u201d in Ukrainian. Thus, the use of two scripts in Slavic allows us to explore transfer across different alphabets.\nWe further consider a non-Indo-European language family, the Uralic languages. We experiment on the three most commonly spoken languages \u2013 Finnish, Estonian and Hungarian (Abondolo, 2015) \u2013 as well as Northern Sami, a language used in Northern Scandinavia. While Finnish and Estonian are closely related (both are members of the Finnic subfamily), Hungarian is a more distant cousin. Estonian and Northern Sami are lowresource languages, so we assign them the lowresource role, resulting in two groups of experiments: (i) Finnish, Hungarian and Estonian to Northern Sami; (ii) Finnish, Hungarian and Northern Sami to Estonian.\nArabic (baseline) is a Semitic language (part of the Afro-Asiatic family (Hetzron, 2013)) that is spoken in North Africa, the Arabian Peninsula and other parts of the Middle East. It is unrelated to all other languages used in this work. Both in terms of form (new words are mainly built using a templatic system) and categories (it has tags such as construct state), Arabic is very different. Thus, we do not expect it to support morphological knowledge transfer and we use it as a baseline for all target languages."}, {"heading": "5 Experiments", "text": "We run three experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method. We first discuss details common to all experiments.\nWe keep hyperparameters during all experiments (and for all languages) fixed to the following values. Encoder and decoder RNNs each have 100\nPr ep\nrin\nt50\u00b720 50\u00b721 50\u00b722 50\u00b723 50\u00b724 50\u00b725 50\u00b726 50\u00b727Number of Samples0.00.20.4\n0.6\n0.8 1.0 A cc ur ac y\nLanguages Pt Ca It Fr Ar Es\nFigure 2: Learning curves showing the accuracy on Spanish test when training on language \u03bb \u2208 {PT, CA, IT, FR, AR, ES}. Except for \u03bb=ES, each model is trained on 12,000 samples from \u03bb and \u201cNumber of Samples\u201d (x-axis) of Spanish.\nhidden units and the size of all subtag, character and language embeddings is 300. For training we use ADADELTA (Zeiler, 2012) with minibatch size 20. All models are trained for 300 epochs. Following Le et al. (2015), we initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder to the identity matrix. Biases are initialized to zero.\nEvaluation metrics: (i) 1-best accuracy: the percentage of predictions that match the true answer exactly; (ii) average edit distance between prediction and true answer. The two metrics differ in that accuracy gives no partial credit and incorrect answers may be drastically different from the annotated form without incurring additional penalty. In contrast, edit distance gives partial credit for forms that are closer to the true answer."}, {"heading": "5.1 Exp. 1: Transfer Learning for Paradigm Completion", "text": "In this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. (i) Is transfer learning possible for morphology? (ii) How much annotated data do we need in the low-resource target language? (iii) How closely related must the two languages be to achieve good results?\nData. Based on complete inflection tables from unimorph.org (Kirov et al., 2016), we create datasets as follows. Each training set consists of 12,000 samples in the high-resource source language and nt\u2208{50, 200} samples in the lowresource target language. We create target lan-\nguage dev and test sets of sizes 1600 and 10,000, respectively.3 For Romance and Arabic, we create learning curves for nt\u2208{100, 400, 800, 1600, 3200, 6400, 12000}. Lemmata and inflections are randomly selected from all available paradigms.\nResults and Discussion. Tab. 3 shows the effectiveness of transfer learning. There are two baselines. (i) \u201c0\u201d: no transfer, i.e., we consider only in-domain data; (ii) \u201cAR\u201d: Arabic, which is unrelated to all target languages.\nWith the exception of the 200 sample case of ET\u2192SME, cross-lingual transfer is always better than the two baselines; the maximum improvement is 0.58 (0.58 vs. 0.00) in accuracy for the 50 sample case of CA\u2192ES. More closely related source languages improve performance more than distant ones. French, the Romance language least similar to Spanish, performs worst for \u2192ES. For the target language Macedonian, Bulgarian provides most benefit. This can again be explained by similarity: Bulgarian is closer to Macedonian than the other languages in this group. The best result for Ukrainian is RU\u2192UK. Unlike Polish and Slowenian, Russian is the only language in this group that uses the same script as Ukrainian, showing the importance of the alphabet for transfer. Still, the results also demonstrate that transfer works across alphabets (although not as well); this suggests that similar embeddings for similar characters have been learned. Finnish is the language that is closest to Estonian and it again performs best as a source language for Estonian. For Northern Sami, transfer works least well, probably because the distance between sources and target is largest in this case. The distance of the Sami languages from the Finnic (Estonian, Finnish) and Ugric (Hungarian) languages is much larger than the distances within Romance and within Slavic. However, even for Northern Sami, adding an additional language is still always beneficial compared to the monolingual baseline.\nLearning curves for Romance and Arabic further support our finding that language similarity is important. In Fig. 2, knowledge is transferred to Spanish, and a baseline \u2013 a model trained only on Spanish data \u2013 shows the accuracy obtained without any transfer learning. Here, Catalan and Italian help the most, followed by Portuguese, French and, finally, Arabic. This corresponds to the order of\n3For Estonian, we use 7094 (not 12,000) train and 5000 (not 10,000) test samples as more data is unavailable.\nPr ep\nrin\nlexical similarity with Spanish, except for the performance of Portuguese (cf. Tab. 2). A possible explanation is the potentially confusing overlap of lemmata between the two languages \u2013 cf. discussion in the next subsection. That the transfer learning setup improves performance for the unrelated language Arabic as source is at first surprising. But adding new samples to a small training set helps prevent overfitting (e.g., rote memorization) even if the source is a morphologically unrelated language; effectively acting as a regularizer.4 This will also be discussed below.\nError Analysis for Romance. Even for only 50 Spanish instances, many inflections are correctly produced in transfer. For, e.g., (criar, 3PlFutSbj) 7\u2192 criaren, model outputs are: fr: criaren, ca: criaren, es: crntaron, it: criaren, ar: ecriren, pt: criaren (all correct except for the two baselines). Many errors involve accents, e.g., (contrastar, 2PlFutInd) 7\u2192 contrastare\u0301is; model outputs are: fr: contrastareis, ca: contrastareis, es: conterar\u0131\u0301an, it: contrastareis, ar: contastar\u0131\u0301as, pt: contrastareis. Some inflections all systems get wrong, mainly because of erroneously applying the inflectional rules of the source to the target. Finally, the output of the model trained on Portuguese contains a class of errors that are unlike those of other systems. Example: (contraatacar, 1SgCond) 7\u2192 contraatacar\u0131\u0301a with those solutions: fr: contratacar\u0131\u0301am, ca: contraatacar\u0131\u0301a, es: concarnar, it: contratace\u0301, ar: cuntatar\u0131\u0301a and pt: contra-atacar\u0131\u0301a. The Portuguese model inserts \u201c-\u201d because Portuguese train data contains contraatacar and \u201c-\u201d appears in its inflected form.5\n4Following (Kann and Schu\u0308tze, 2016b) we did not use standard regularizers. To verify that the effect of Arabic is a regularization effect, we ran a small monolingual experiment on ES (200 setting) with dropout 0.5 (Srivastava et al., 2014). The resulting accuracy is 0.57, very similar to the comparable Arabic number of 0.54 in the table.\n5To investigate this in more detail we retrain the Portuguese model with 50 Spanish samples, but exclude all lemmata that appear in Spanish train/dev/test, resulting in only 3695\nAn example for the generally improved performance across languages for 200 Spanish training samples is (contrastar, 2PlIndFut) 7\u2192 contrastare\u0301is: all models now produce the correct form."}, {"heading": "5.2 Exp. 2: Zero-Shot/One-Shot Transfer", "text": "In \u00a75.1, we investigated the relationship between indomain (target) training set size and performance. Here, we look at the extreme case of training set sizes 1 (one-shot) and 0 (zero-shot) for a tag. We train our model on a single sample for half of the tags appearing in the low-resource language, i.e., if T` is the set of morphological tags for the target language, train set size is |T`|/2. As before, we add 12,000 source samples.\nWe report one-shot accuracy (resp. zero-shot accuracy), i.e., the accuracy for samples with a tag that has been seen once (resp. never) during training. Note that the model has seen the individual subtags each tag is composed of.6\nData. Our experimental setup is similar to \u00a75.1: we use the same dev, test and high-resource train sets as before. However, the low-resource data is created in the way specified above. To remove a potentially confounding variable, we impose the condition that no two training samples belong to\ntraining samples. Accuracy on test increases by 0.09 despite the reduced size of the training set.\n6It is very unlikely that due to random selection a subtag will not be in train; this case did not occur in our experiments.\nPr ep\nrin t\nthe same lemma. Results and Discussion. Tab. 4 shows that the Spanish and Arabic systems do not learn anything useful for either half of the tags. This is not surprising as there is not enough Spanish data for the system to generalize well and Arabic does not contribute exploitable information. The systems trained on French and Italian, in contrast, get a nonzero accuracy for the zero-shot case as well as 0.13 and 0.23, respectively, in the one-shot case. This shows that a single training example is sometimes sufficient for successful generation although generalization to tags never observed is rarely possible. Catalan and Portuguese show the best performance in both settings; this is intuitive since they are the languages closest to the target (cf. Tab. 2). In fact, adding Portuguese to the training data yields an absolute increase in accuracy of 0.44 (0.44 vs. 0.00) for one-shot and 0.14 (0.14 vs. 0.00) for zero-shot with corresponding improvements in edit distance.\nOverall, this experiment shows that with transfer learning from a closely related language the performance of zero-shot morphological generation improves over the monolingual approach, and, in the one-shot setting, it is possible to generate the right form nearly half the time."}, {"heading": "5.3 Exp. 3: True Transfer vs. Other Effects", "text": "We would like to separate the effects of regularization that we saw for Arabic from true transfer.\nTo this end, we generate a random cipher (i.e., a function \u03b3 : \u03a3 \u222a S 7\u2192 \u03a3 \u222a S) and apply it to all word forms and morphological tags of the high-resource train set; target language data are not changed. Ciphering makes it harder to learn true \u201clinguistic\u201d transfer of morphology. Consider the simplest case of transfer: an identical mapping in two languages, e.g., (visitar, 1SgPresInd) 7\u2192 visito in both Portuguese and Spanish. If we transform Portuguese using the cipher \u03b3(iostv...) = kltqa..., then visito becomes aktkql in Portuguese and its tag becomes similarly unrecognizable as being identical to the Spanish tag 1SgPresInd. Our intuition is that ciphering will disrupt transfer of morphology.7 On the other hand, the regularization effect we observed with Arabic should still be effective.\nData. We use the Portuguese-Spanish and 7Note that ciphered input is much harder than transfer between two alphabets (Latin/Cyrillic) because it creates ambiguous input. In the example, Spanish \u201ci\u201d is totally different from Portuguese \u201ci\u201d (which is really \u201ck\u201d), but the model must use the same representation.\nArabic-Spanish data from Experiment 1. We generate a random cipher and apply it to morphological tags and word forms for Portuguese and Arabic. The language tags are kept unchanged. Spanish is also not changed. For comparability with Tab. 3, we use the same dev and test sets as before.\nResults and Discussion. Tab. 5 shows that performance of PT\u2192ES drops a lot: from 0.48 to 0.09 for 50 samples and from 0.62 to 0.54 for 200 samples. This is because there are no overt similarities between the two languages left after applying the cipher, e.g., the two previously identical forms visito are now different.\nThe impact of ciphering on AR\u2192ES varies: slightly improved in one case (0.54 vs. 0.56), slightly worse in three cases. We also apply the cipher to the tags and Arabic and Spanish share subtags, e.g., Sg. Just the knowledge that something is a subtag is helpful because subtags must not be generated as part of the output. We can explain the tendency of ciphering to decrease performance on AR\u2192ES by the \u201cmasking\u201d of common subtags.\nFor 200 samples and ciphering, there is no clear difference in performance between Portuguese and Arabic. However, for 50 samples and ciphering, Portuguese (0.09) seems to perform better than Arabic (0.02) in accuracy. Portuguese uses suffixation for inflection whereas Arabic is templatic and inflectional changes are not limited to the end of the word. This difference is not affected by ciphering. Perhaps even ciphered Portugese lets the model learn better that the beginnings of words just need to be copied. For 200 samples, the Spanish dataset may be large enough, so that ciphered Portuguese no longer helps in this regard.\nComparing no transfer with transfer from a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0\u2192ES) vs. 0.54 (PT\u2192ES) and 0.56 (AR\u2192ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place."}, {"heading": "6 Related Work", "text": "Cross-lingual transfer learning has been used for many tasks: automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; S\u00f8gaard, 2011; Naseem et al., 2012), entity recog-\nPr ep\nrin t\nnition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pado\u0301 and Lapata, 2005). The drawback is that machine translation errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010).\nIn the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages.\nWork on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2015; Kann and Schu\u0308tze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art.\nEncoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves\nand Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k \u2208 {1, 2} and m = k. Other work has addressed cases with k > 2 or m > 2; this would be an interesting avenue of future research for paradigm completion. (iii) Whereas training RNNs in MT is hard, we only experienced one difficult issue in our experiments (due to the low-resource setting): regularization. (iv) Some work is word- or subword-based, our work is character-based. The same way that similar word embeddings are learned for the inputs cow and vache (French for \u201ccow\u201d) in MT, we expect similar embeddings to be learned for similar Cyrillic/Latin characters. (v) Similar to work in MT, we show that zero-shot (and, by extension, one-shot) learning is possible.\n(Ha et al., 2016) (which was developed in parallel to our transfer model although we did not prepublish our paper on arxiv) is most similar to our work. Whereas Ha et al. (2016) address MT, we focus on the task of paradigm completion in low-resource settings and establish the state of the art for this problem."}, {"heading": "7 Conclusion", "text": "We presented a cross-lingual transfer learning method for paradigm completion, based on an RNN encoder-decoder model. Our experiments showed that information from a high-resource language can be leveraged for paradigm completion in a related low-resource language. Our analysis showed that the degree to which the source language data helps for a certain target language depends on their relatedness. Our method led to significant improvements in settings with limited training data \u2013 up to 58% absolute improvement in accuracy \u2013 and, thus, enables the use of state-of-the-art models for paradigm completion in low-resource languages.\nPr ep\nrin t"}], "references": [{"title": "The Uralic Languages", "author": ["Daniel Abondolo."], "venue": "Routledge.", "citeRegEx": "Abondolo.,? 2015", "shortCiteRegEx": "Abondolo.", "year": 2015}, {"title": "Improving sequence to sequence learning for morphological inflection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinflection", "author": ["Roee Aharoni", "Yoav Goldberg", "Yonatan Belinkov."], "venue": "SIGMORPHON.", "citeRegEx": "Aharoni et al\\.,? 2016", "shortCiteRegEx": "Aharoni et al\\.", "year": 2016}, {"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden."], "venue": "EACL.", "citeRegEx": "Ahlberg et al\\.,? 2014", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Cross-lingual morphological tagging for low-resource languages", "author": ["Jan Buys", "Jan A Botha."], "venue": "ACL.", "citeRegEx": "Buys and Botha.,? 2016", "shortCiteRegEx": "Buys and Botha.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine Learning .", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "An empirical comparison of simple domain adaptation methods for neural machine translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "arXiv preprint 1701.03214 .", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "Unsupervised structure prediction with non-parallel multilingual guidance", "author": ["Shay B Cohen", "Dipanjan Das", "Noah A Smith."], "venue": "EMNLP.", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "JMLR 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The Slavonic Languages", "author": ["Greville Corbett", "Bernard Comrie."], "venue": "Routledge.", "citeRegEx": "Corbett and Comrie.,? 2003", "shortCiteRegEx": "Corbett and Comrie.", "year": 2003}, {"title": "The SIGMORPHON 2016 shared task\u2014 morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "SIGMORPHON.", "citeRegEx": "Cotterell et al\\.,? 2016a", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Modeling word forms using latent underlying morphs and phonology", "author": ["Ryan Cotterell", "Nanyun Peng", "Jason Eisner."], "venue": "TACL 3:433\u2013447.", "citeRegEx": "Cotterell et al\\.,? 2015", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "Morphological smoothing and extrapolation of word embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze", "Jason Eisner."], "venue": "ACL.", "citeRegEx": "Cotterell et al\\.,? 2016b", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Analysis of morph-based speech recognition and the modeling of out-of", "author": ["Mathias Creutz", "Teemu Hirsim\u00e4ki", "Mikko Kurimo", "Antti Puurula", "Janne Pylkk\u00f6nen", "Vesa Siivola", "Matti Varjokallio", "Ebru Arisoy", "Murat Sara\u00e7lar", "Andreas Stolcke"], "venue": null, "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "ACL-IJCNLP.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "NAACL.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Generalizing word lattice translation", "author": ["Christopher Dyer", "Smaranda Muresan", "Philip Resnik."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2008", "shortCiteRegEx": "Dyer et al\\.", "year": 2008}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "arXiv preprint arXiv:1512.06110 .", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "NAACL.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1601.01073.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "The use of machine translation tools for cross-lingual text mining", "author": ["Blaz Fortuna", "John Shawe-Taylor."], "venue": "ICML Workshop on Learning with Multiple Views.", "citeRegEx": "Fortuna and Shawe.Taylor.,? 2005", "shortCiteRegEx": "Fortuna and Shawe.Taylor.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "author": ["Thanh-Le Ha", "Jan Niehues", "Alexander Waibel."], "venue": "arXiv preprint arXiv:1611.04798 .", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "The Romance languages", "author": ["Martin Harris", "Nigel Vincent."], "venue": "Routledge.", "citeRegEx": "Harris and Vincent.,? 2003", "shortCiteRegEx": "Harris and Vincent.", "year": 2003}, {"title": "The Semitic Languages", "author": ["Robert Hetzron."], "venue": "Routledge.", "citeRegEx": "Hetzron.,? 2013", "shortCiteRegEx": "Hetzron.", "year": 2013}, {"title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers", "author": ["Jui-Ting Huang", "Jinyu Li", "Dong Yu", "Li Deng", "n Gong"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Neural morphological analysis: Encodingdecoding canonical segments", "author": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "EMNLP.", "citeRegEx": "Kann et al\\.,? 2016", "shortCiteRegEx": "Kann et al\\.", "year": 2016}, {"title": "Singlemodel encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Kann and Sch\u00fctze.,? 2016a", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Kann and Sch\u00fctze.,? 2016b", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Very-large scale parsing and normalization of wiktionary morphological paradigms", "author": ["Christo Kirov", "John Sylak-Glassman", "Roger Que", "David Yarowsky."], "venue": "LREC.", "citeRegEx": "Kirov et al\\.,? 2016", "shortCiteRegEx": "Kirov et al\\.", "year": 2016}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton."], "venue": "CoRR abs/1504.00941.", "citeRegEx": "Le et al\\.,? 2015", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Ethnologue: Languages of the World. SIL International, Dallas, Texas, 16 edition", "author": ["M Paul Lewis", "editor"], "venue": null, "citeRegEx": "Lewis and editor.,? \\Q2009\\E", "shortCiteRegEx": "Lewis and editor.", "year": 2009}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Morphological segmentation for keyword spotting", "author": ["Karthik Narasimhan", "Damianos Karakos", "Richard Schwartz", "Stavros Tsakalidis", "Regina Barzilay."], "venue": "EMNLP.", "citeRegEx": "Narasimhan et al\\.,? 2014", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2014}, {"title": "Selective sharing for multilingual dependency parsing", "author": ["Tahira Naseem", "Regina Barzilay", "Amir Globerson."], "venue": "ACL.", "citeRegEx": "Naseem et al\\.,? 2012", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak."], "venue": "NAACL.", "citeRegEx": "Nicolai et al\\.,? 2015", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Cross-language text classification", "author": ["J Scott Olsson", "Douglas W Oard", "Jan Haji\u010d."], "venue": "ACM SIGIR conference on Research and development in information retrieval.", "citeRegEx": "Olsson et al\\.,? 2005", "shortCiteRegEx": "Olsson et al\\.", "year": 2005}, {"title": "Crosslinguistic projection of role-semantic information", "author": ["Sebastian Pad\u00f3", "Mirella Lapata."], "venue": "HLT/EMNLP.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2005", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2005}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "LREC.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Semimarkov conditional random fields for information extraction", "author": ["Sunita Sarawagi", "William W Cohen."], "venue": "NIPS.", "citeRegEx": "Sarawagi and Cohen.,? 2004", "shortCiteRegEx": "Sarawagi and Cohen.", "year": 2004}, {"title": "A graphbased lattice dependency parser for joint morphological segmentation and syntactic analysis", "author": ["Wolfgang Seeker", "\u00d6zlem \u00c7etino\u011flu."], "venue": "TACL 3:359\u2013373.", "citeRegEx": "Seeker and \u00c7etino\u011flu.,? 2015", "shortCiteRegEx": "Seeker and \u00c7etino\u011flu.", "year": 2015}, {"title": "Cross language text classification by model translation and semi-supervised learning", "author": ["Lei Shi", "Rada Mihalcea", "Mingjun Tian."], "venue": "EMNLP.", "citeRegEx": "Shi et al\\.,? 2010", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Crosslingual propagation for morphological analysis", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "AAAI.", "citeRegEx": "Snyder and Barzilay.,? 2008a", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ACL-HLT .", "citeRegEx": "Snyder and Barzilay.,? 2008b", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Data point selection for crosslanguage adaptation of dependency parsers", "author": ["Anders S\u00f8gaard."], "venue": "ACLHLT .", "citeRegEx": "S\u00f8gaard.,? 2011", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The composition and use of the universal morphological feature schema (unimorph schema)", "author": ["John Sylak-Glassman."], "venue": "Technical report, Department of Computer Science, Johns Hopkins University.", "citeRegEx": "Sylak.Glassman.,? 2016", "shortCiteRegEx": "Sylak.Glassman.", "year": 2016}, {"title": "A language-independent feature schema for inflectional morphology", "author": ["John Sylak-Glassman", "Christo Kirov", "David Yarowsky", "Roger Que."], "venue": "ACLIJCNLP.", "citeRegEx": "Sylak.Glassman et al\\.,? 2015", "shortCiteRegEx": "Sylak.Glassman et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Cross-lingual projected expectation regularization for weakly supervised learning", "author": ["Mengqiu Wang", "Christopher D Manning."], "venue": "TACL 2:55\u201366.", "citeRegEx": "Wang and Manning.,? 2014a", "shortCiteRegEx": "Wang and Manning.", "year": 2014}, {"title": "Cross-lingual pseudo-projected expectation regularization for weakly supervised learning", "author": ["Mengqiu Wang", "Christopher D Manning."], "venue": "TACL 2:55\u2013", "citeRegEx": "Wang and Manning.,? 2014b", "shortCiteRegEx": "Wang and Manning.", "year": 2014}, {"title": "Corpus-level fine-grained entity typing using contextual information", "author": ["Yadollah Yaghoobzadeh", "Hinrich Sch\u00fctze."], "venue": "EMNLP.", "citeRegEx": "Yaghoobzadeh and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Yaghoobzadeh and Sch\u00fctze.", "year": 2015}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["David Yarowsky", "Grace Ngai", "Richard Wicentowski."], "venue": "HLT .", "citeRegEx": "Yarowsky et al\\.,? 2001", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "CoRR abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "arXiv preprint arXiv:1601.00710 .", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 50, "context": "In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) \u2013 although morphology is easy to annotate compared to syntax and semantics.", "startOffset": 135, "endOffset": 164}, {"referenceID": 17, "context": ", machine translation (Dyer et al., 2008), speech recognition (Creutz et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 14, "context": ", 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and \u00c7etino\u011flu, 2015), keyword spotting (Narasimhan et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 42, "context": ", 2007), parsing (Seeker and \u00c7etino\u011flu, 2015), keyword spotting (Narasimhan et al.", "startOffset": 17, "endOffset": 45}, {"referenceID": 35, "context": ", 2007), parsing (Seeker and \u00c7etino\u011flu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al.", "startOffset": 64, "endOffset": 89}, {"referenceID": 13, "context": ", 2014) and word embeddings (Cotterell et al., 2016b), inter alia.", "startOffset": 28, "endOffset": 53}, {"referenceID": 48, "context": "RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al.", "startOffset": 32, "endOffset": 79}, {"referenceID": 3, "context": "RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al.", "startOffset": 32, "endOffset": 79}, {"referenceID": 19, "context": ", 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016a; Cotterell et al., 2016a).", "startOffset": 57, "endOffset": 129}, {"referenceID": 29, "context": ", 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016a; Cotterell et al., 2016a).", "startOffset": 57, "endOffset": 129}, {"referenceID": 11, "context": ", 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016a; Cotterell et al., 2016a).", "startOffset": 57, "endOffset": 129}, {"referenceID": 16, "context": "An important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016).", "startOffset": 68, "endOffset": 184}, {"referenceID": 2, "context": "An important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016).", "startOffset": 68, "endOffset": 184}, {"referenceID": 37, "context": "An important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016).", "startOffset": 68, "endOffset": 184}, {"referenceID": 12, "context": "An important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016).", "startOffset": 68, "endOffset": 184}, {"referenceID": 19, "context": "An important task in inflectional morphology is paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Cotterell et al., 2015; Faruqui et al., 2016).", "startOffset": 68, "endOffset": 184}, {"referenceID": 11, "context": "There are good solutions for paradigm completion when a large amount of annotated training data is available (Cotterell et al., 2016a).", "startOffset": 109, "endOffset": 134}, {"referenceID": 11, "context": "The SIGMORPHON 2016 shared task (Cotterell et al., 2016a) on morphological reinflection, a harder generalization of paradigm completion, found that \u2265 98% accuracy can be achieved in many languages with neural sequence-to-sequence models, improving the state of the art by 10%.", "startOffset": 32, "endOffset": 57}, {"referenceID": 40, "context": "We can define a universal set of POS tags (Petrov et al., 2012) or of entity types (e.", "startOffset": 42, "endOffset": 63}, {"referenceID": 54, "context": ", coarse-grained types like person and location or fine-grained types (Yaghoobzadeh and Sch\u00fctze, 2015)), but inflection is much more language-specific.", "startOffset": 70, "endOffset": 102}, {"referenceID": 52, "context": "Transferring named entity recognition, however, among Chinese and European languages works well (Wang and Manning, 2014a).", "startOffset": 96, "endOffset": 121}, {"referenceID": 5, "context": "We describe our probability model for morphological transfer using terminology from multi-task learning (Caruana, 1997; Collobert et al., 2011).", "startOffset": 104, "endOffset": 143}, {"referenceID": 9, "context": "We describe our probability model for morphological transfer using terminology from multi-task learning (Caruana, 1997; Collobert et al., 2011).", "startOffset": 104, "endOffset": 143}, {"referenceID": 30, "context": "Based on recent advances in neural transducers, we parameterize each distribution as an encoderdecoder RNN, as in (Kann and Sch\u00fctze, 2016b).", "startOffset": 114, "endOffset": 139}, {"referenceID": 3, "context": "We parameterize the distribution p\u03b8 as an encoderdecoder gated RNN with attention (Bahdanau et al., 2015), the state-of-the-art solution for the monolingual case (Kann and Sch\u00fctze, 2016b).", "startOffset": 82, "endOffset": 105}, {"referenceID": 30, "context": ", 2015), the state-of-the-art solution for the monolingual case (Kann and Sch\u00fctze, 2016b).", "startOffset": 64, "endOffset": 89}, {"referenceID": 6, "context": "A bidirectional gated RNN encodes the input sequence (Cho et al., 2014) \u2013 the concatenation of (i) the language ! h1 ! h2 ! h3 ! hN", "startOffset": 53, "endOffset": 71}, {"referenceID": 3, "context": "See Bahdanau et al. (2015) for further details.", "startOffset": 4, "endOffset": 27}, {"referenceID": 49, "context": "Note that the set of subtags S is defined as attributes from the UNIMORPH schema (Sylak-Glassman, 2016) and, thus, is universal across languages; the schema is derived from research in linguistic typology.", "startOffset": 81, "endOffset": 103}, {"referenceID": 29, "context": "What we have described is the representation of Kann and Sch\u00fctze (2016b). In addition, we preprend a symbol \u03bb \u2208 L to the input string (e.", "startOffset": 48, "endOffset": 73}, {"referenceID": 25, "context": "tor Vulgar Latin (Harris and Vincent, 2003), they share large parts of their lexicon and inflectional morphology; we expect knowledge among them to be easily transferable.", "startOffset": 17, "endOffset": 43}, {"referenceID": 10, "context": "The Indo-European Slavic language family has its origin in eastern-central Europe (Corbett and Comrie, 2003).", "startOffset": 82, "endOffset": 108}, {"referenceID": 40, "context": "We contrast this with the universal POS set (Petrov et al., 2012), where it is reasonable to expect that we see all 17 tags in every language.", "startOffset": 44, "endOffset": 65}, {"referenceID": 0, "context": "We experiment on the three most commonly spoken languages \u2013 Finnish, Estonian and Hungarian (Abondolo, 2015) \u2013 as well as Northern Sami, a language used in Northern Scandinavia.", "startOffset": 92, "endOffset": 108}, {"referenceID": 26, "context": "Arabic (baseline) is a Semitic language (part of the Afro-Asiatic family (Hetzron, 2013)) that is spoken in North Africa, the Arabian Peninsula and other parts of the Middle East.", "startOffset": 73, "endOffset": 88}, {"referenceID": 56, "context": "For training we use ADADELTA (Zeiler, 2012) with minibatch size 20.", "startOffset": 29, "endOffset": 43}, {"referenceID": 32, "context": "Following Le et al. (2015), we initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder to the identity matrix.", "startOffset": 10, "endOffset": 27}, {"referenceID": 31, "context": "org (Kirov et al., 2016), we create datasets as follows.", "startOffset": 4, "endOffset": 24}, {"referenceID": 30, "context": "Following (Kann and Sch\u00fctze, 2016b) we did not use standard regularizers.", "startOffset": 10, "endOffset": 35}, {"referenceID": 47, "context": "5 (Srivastava et al., 2014).", "startOffset": 2, "endOffset": 27}, {"referenceID": 27, "context": "Cross-lingual transfer learning has been used for many tasks: automatic speech recognition (Huang et al., 2013), parsing (Cohen et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 8, "context": ", 2013), parsing (Cohen et al., 2011; S\u00f8gaard, 2011; Naseem et al., 2012), entity recog-", "startOffset": 17, "endOffset": 73}, {"referenceID": 46, "context": ", 2013), parsing (Cohen et al., 2011; S\u00f8gaard, 2011; Naseem et al., 2012), entity recog-", "startOffset": 17, "endOffset": 73}, {"referenceID": 36, "context": ", 2013), parsing (Cohen et al., 2011; S\u00f8gaard, 2011; Naseem et al., 2012), entity recog-", "startOffset": 17, "endOffset": 73}, {"referenceID": 53, "context": "nition (Wang and Manning, 2014b) and machine translation (Johnson et al.", "startOffset": 7, "endOffset": 32}, {"referenceID": 24, "context": "nition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016).", "startOffset": 57, "endOffset": 96}, {"referenceID": 21, "context": "One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005).", "startOffset": 87, "endOffset": 140}, {"referenceID": 38, "context": "One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005).", "startOffset": 87, "endOffset": 140}, {"referenceID": 55, "context": "Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad\u00f3 and Lapata, 2005).", "startOffset": 104, "endOffset": 150}, {"referenceID": 39, "context": "Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad\u00f3 and Lapata, 2005).", "startOffset": 104, "endOffset": 150}, {"referenceID": 43, "context": ", to port a model trained on the source language to the target language (Shi et al., 2010).", "startOffset": 72, "endOffset": 90}, {"referenceID": 4, "context": "In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text.", "startOffset": 28, "endOffset": 50}, {"referenceID": 11, "context": "Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a).", "startOffset": 122, "endOffset": 147}, {"referenceID": 16, "context": "Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.", "startOffset": 133, "endOffset": 181}, {"referenceID": 37, "context": "Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.", "startOffset": 133, "endOffset": 181}, {"referenceID": 41, "context": ", a semi-Markov conditional random field (Sarawagi and Cohen, 2004).", "startOffset": 41, "endOffset": 67}, {"referenceID": 1, "context": "Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2015; Kann and Sch\u00fctze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art.", "startOffset": 20, "endOffset": 89}, {"referenceID": 18, "context": "Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2015; Kann and Sch\u00fctze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art.", "startOffset": 20, "endOffset": 89}, {"referenceID": 30, "context": "Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2015; Kann and Sch\u00fctze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art.", "startOffset": 20, "endOffset": 89}, {"referenceID": 23, "context": "They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al.", "startOffset": 60, "endOffset": 111}, {"referenceID": 22, "context": "They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al.", "startOffset": 60, "endOffset": 111}, {"referenceID": 51, "context": ", 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 28, "context": ", 2015) and segmentation (Kann et al., 2016).", "startOffset": 25, "endOffset": 44}, {"referenceID": 15, "context": "More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia.", "startOffset": 159, "endOffset": 298}, {"referenceID": 57, "context": "More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia.", "startOffset": 159, "endOffset": 298}, {"referenceID": 7, "context": "More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia.", "startOffset": 159, "endOffset": 298}, {"referenceID": 34, "context": "More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia.", "startOffset": 159, "endOffset": 298}, {"referenceID": 20, "context": "More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia.", "startOffset": 159, "endOffset": 298}, {"referenceID": 24, "context": "More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation (MT): (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia.", "startOffset": 159, "endOffset": 298}, {"referenceID": 5, "context": "Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 5, "context": "Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 3, "context": "(2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization.", "startOffset": 47, "endOffset": 70}, {"referenceID": 24, "context": "(Ha et al., 2016) (which was developed in parallel to our transfer model although we did not prepublish our paper on arxiv) is most similar to our work.", "startOffset": 0, "endOffset": 17}, {"referenceID": 24, "context": "(Ha et al., 2016) (which was developed in parallel to our transfer model although we did not prepublish our paper on arxiv) is most similar to our work. Whereas Ha et al. (2016) address MT, we focus on the task of paradigm completion in low-resource settings and establish the state of the art for this problem.", "startOffset": 1, "endOffset": 178}], "year": 2017, "abstractText": "We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a lowresource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.", "creator": "LaTeX with hyperref package"}}}