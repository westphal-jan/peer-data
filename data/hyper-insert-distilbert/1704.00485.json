{"id": "1704.00485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity Classifiers?", "abstract": "similarly many datasets have multiple tables connected independently by key - shifting foreign variable key design dependencies. data scientists ideally usually join often all tables to bring in mutually extra features selected from through the so - called implicit dimension tables. unlike the simpler statistical linear relational learning setting, such joins don't mainly cause record duplications, which nonetheless means regular iid models are nevertheless typically used. recent extensive work demonstrated the higher possibility of using foreign hidden key dependent features as representatives for the unfamiliar dimension tables'data features characteristics and dramatically eliminating either the interaction latter a priori, then potentially saving runtime input and effort of emerging data scientists. where however, the research prior, work was restricted completely to linear models and it established effectively a broader dichotomy of when dimension valued tables are probably safe to partially discard due to extra overfitting caused by declining the use of foreign foreign key features. now in reducing this singular work, ultimately we carefully revisit solving that inherent question for two popular high inference capacity models : nonlinear decision procedure tree stability and svm with strict rbf prediction kernel. our extensive empirical and operational simulation - instrument based analyses still show that these two basic classifiers overall are surprisingly and counter - intuitively more robust to explicitly discarding dimension tables and face much noticeably less extra actual overfitting than mainstream linear key models. importantly we provide intuitive logical explanations for their behavior constituents and so identify new open theory questions for further science ml theoretical research. we also generally identify and clearly resolve two existing key practical bottlenecks in using foreign key features.", "histories": [["v1", "Mon, 3 Apr 2017 09:16:58 GMT  (4494kb,D)", "https://arxiv.org/abs/1704.00485v1", "10 pages"], ["v2", "Sun, 9 Apr 2017 04:02:56 GMT  (4494kb,D)", "http://arxiv.org/abs/1704.00485v2", "10 pages"], ["v3", "Sun, 4 Jun 2017 19:02:20 GMT  (7283kb,D)", "http://arxiv.org/abs/1704.00485v3", "14 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.DB cs.LG", "authors": ["vraj shah", "arun kumar", "xiaojin zhu"], "accepted": false, "id": "1704.00485"}, "pdf": {"name": "1704.00485.pdf", "metadata": {"source": "CRF", "title": "Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity Classifiers?", "authors": ["Vraj Shah", "Arun Kumar", "Xiaojin Zhu"], "emails": ["arunkk}@eng.ucsd.edu,", "jerryzhu@cs.wisc.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "The data management community has long studied how to integrate ML with data systems (e.g., [16, 11, 46]), how to scale ML (e.g., [5, 28]), and how to use database ideas to improve ML tasks (e.g., [22, 23]). However, little work has tackled the pains of sourcing data for ML tasks in the first place, especially, how fundamental data properties affect end-to-end data workflows for ML tasks [4]. In particular, real-world relational databases often have many tables\nconnected by database dependencies such as key-foreign key dependencies (KFKDs) [33]. Thus, given an ML task, data scientists almost always join multiple tables because they like to obtain more features for ML models [25]. But from conversations with data scientists at many enterprise and Web companies, we learned that even this simple process of procuring features by joining tables could be painful in practice because different tables could be \u201cowned\u201d by different teams with different access restrictions. This slows down the ML analytics lifecycle. Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].\nIn this context, Kumar et al. [26] showed that one can often omit entire tables by exploiting KFKDs in the database schema. That is, one can ignore a table without even looking at its contents (i.e., \u201cavoid the join\u201d), but crucially, do so without significantly affecting ML accuracy (i.e., \u201csafely\u201d). The basis for this dramatic capability is that a KFK join creates a functional dependency (FD) between a foreign key feature and the foreign features brought in by the join.1\nExample (based on [26]). Consider a common classification task: predicting customer churn. The data scientist starts with the main table for training (simplified for exposition): Customers (CustomerID, Churn, Gender, Age, Employer). Churn is the target, while Gender, Age, and Employer are features. So far, this is a standard classification task. She then notices the table Employers (Employer,\nState, Revenue) in her database with extra features about customers\u2019 employers. Customers.Employer is thus a foreign key feature connecting these tables. She joins the tables to bring in the foreign features (about employers) because she has a hunch that customers employed by rich companies in coastal states might be less likely to churn. She then tries different classifiers, e.g., logistic regression or decision trees.\nThe analysis in [26] revealed a dichotomy in how safe it is to avoid a join from an accuracy standpoint: in terms of the bias-variance trade-off, avoiding a join is unlikely to increase bias but it might significantly increase variance, since foreign key features often have larger domains than foreign features. In simple terms, avoiding joins might cause extra overfitting. But this extra overfitting subsides with more training examples. In [26], the tuple ratio quantifies this behavior; in our example, it is the ratio of the number of labeled customers to the number of employers. When the tuple ratio is above a\n1While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26].\nar X\niv :1\n70 4.\n00 48\n5v 3\n[ cs\n.D B\n] 4\nJ un\n2 01\n7\ncertain VC dimension-based threshold, we can safely avoid the join. For simpler classifiers with linear VC dimensions (e.g., logistic regression and Naive Bayes), this threshold was about 20. Since there were public real-world datasets that satisfied this threshold, this idea of avoiding joins safely could be empirically validated.\nWhile initially controversial, the idea of avoiding joins safely has been adopted by many data scientists, including at Facebook, LogicBlox, and MakeMyTrip [1]. Based on the value of the easy-to-compute tuple ratio, which only needs the foreign table\u2019s cardinality rather than the table itself, the data scientist can decide if they want to avoid the join or procure the extra table. However, the results in [26] had a major caveat\u2013they applied only to linear classifiers. In fact, their VC dimension-based analysis suggested that the tuple ratio thresholds might be too high for high-capacity nonlinear classifiers, potentially rendering this idea of avoiding joins safely inapplicable to such classifiers in practice.\nIn this paper, we perform a comprehensive empirical and simulation study and analysis to verify (or refute) the applicability of the idea of avoiding joins safely to three popular high-capacity classifiers: decision trees, SVMs, and ANNs.\nThe natural expectation is that these complex models, some with infinite VC dimensions, will likely face larger extra overfitting by avoiding joins compared to linear classifiers. Surprisingly, our results show that their behavior is the exact opposite! We start by rerunning the experiments on the real-world datasets with KFK joins from [26] for these models.2 Irrespective of whether we use linear classifiers or the higher-capacity classifiers, the same set of joins turn out to be safe to avoid. Furthermore, on the datasets that had joins that were not safe to avoid, the decrease in accuracy caused by avoiding said joins (unsafely) was lower for the higher-capacity classifiers compared to the linear classifiers. In other words, our work refutes an intuition from the VC dimension-based analysis of [26] and shows that these popular high-capacity classifiers are counter-intuitively comparably or more robust to avoiding joins than linear classifiers, not less.\nTo understand the above surprising behavior in depth, we conduct an in-depth Monte Carlo-style simulation study to \u201cstress test\u201d how safe it is to avoid the join. We use decision trees for the simulation study, since they were the most robust to avoiding joins. We generate data for a twotable KFK join and embed various \u201ctrue\u201d distributions for the target. This includes a known \u201cworst-case\u201d scenario for avoiding joins with linear classifiers (i.e., the holdout test errors blow up) [26]. We vary different properties of the data and the true distribution: numbers of features in each base table, numbers of training examples, foreign key domain size, noise in the data, and foreign key skew. In very few of these cases does avoiding the join cause the error to rise beyond 1%! Indeed, the only scenario where avoiding the join caused significantly higher overfitting was when the tuple ratio was less than 3; this scenario arose in only 1 of the 7 real datasets. These results are in stark constrast to the results for linear classifiers.\nOur counter-intuitive results raise new research questions at the intersection of data management and ML theory. In particular, there is a need to formalize the effects of KFKDs/FDs on the behavior of decision trees, SVMs, and ANNs.\n2But for simplicity and ease of comparison of all the models, we binarize all classification tasks.\nAs a step in this direction, we analyze and intuitively explain the behavior of decision trees and SVMs. Other open questions include the implications of more general database dependencies such as embedded multi-valued dependencies on the behavior of such models and the implications of all database dependencies for other ML tasks such as regression and clustering. We believe that solving these fundamental questions could lead new analytics systems functionalities that make it easier to use ML for data analytics.\nFinally, we observed that foreign key features cause two new practical bottlenecks for data scientists, especially with decision trees. First, the sheer size of their domains makes it hard to interpret and visualize the trees. Second, some foreign key values may not have any training examples even if they are known to be in the domain. We identify and adapt standard heuristics from the literature to resolve these bottlenecks and verify their effectiveness empirically.\nOverall, the contributions of this paper are as follows:\n\u2022 To the best of our knowledge, this is the first paper to analyze the effects of avoiding KFK joins on three popular high-capacity classifiers: decision trees, SVMs, and ANNs. We present a comprehensive empirical study that refutes an intuition from prior work and shows that these classifiers are counter-intuitively more robust to avoiding joins than linear classifiers. \u2022 We conduct an in-depth simulation study with a decision tree to assess the effects of various data properties on how safe it is to avoid a KFK join. \u2022 We present an intuitive analysis to explain the behavior of decision trees and SVMs when joins are avoided. We identify open questions for research at the intersection of data management and ML theory. \u2022 We identify two practical bottlenecks with foreign key features and verify the effectiveness of standard heuristics in resolving them.\nOutline. Section 2 presents our notation, assumptions, and scope. Section 3 presents results on the real data, while Section 4 presents our simulation study. Section 5 presents our intuitive analysis of the results and identifies open research questions. Section 6 verifies the techniques to make foreign key features more practical. We discuss related prior work in Section 7 and conclude in Section 8."}, {"heading": "2. PRELIMINARIES", "text": ""}, {"heading": "2.1 Notation", "text": "The setting we focus on is the following: the dataset has a set of tables in the star schema with KFK dependencies (KFKDs). Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25]. The fact table, which has the target variable, is denoted S. It has the schema S(SID, Y,XS , FK1, . . . , FKq). A dimension table is denoted Ri (i = 1 to q) and it has the schema Ri(RIDi,XRi). Y is the target variable (class label), XS and XRi are vectors (sequences) of features, RIDi is the primary key of Ri, while FKi is a foreign key feature that refers to Ri. We call XS home features and XRi foreign features. For ease of exposition, we also treat X as a set of features since the order among features is immaterial in our setting. Let T\ndenote the output of the projected equi-join (key-foreign key, or KFK for short) query that constructs the full training dataset by concatenating the features from all base tables: T \u2190 \u03c0(R ./RID=FK S). In general, its schema is T(SID, Y,XS , FK1, . . . , FKq,XR1 , . . . ,XRq ). In contrast to our setting, traditional ML formulations do not distinguish between home features, foreign keys, and foreign features. The number of tuples in S (resp. R) is denoted nS (resp. nR); the number of features in XS (resp. XR) is denoted dS (resp. dR). Without loss of generality, we assume that the join is not selective, which means nS is also the number of tuples in T. DFK denotes the domain of FK and by definition, |DFK | = nR. We call nSnR the tuple ratio. Note that our setting is different from the statistical relational learning (SRL) setting, which deals with joins that violate the IID assumption and duplicate labeled examples from S [13]. KFK joins do not cause such duplication and thus, regular IID models are typically used in this setting."}, {"heading": "2.2 Assumptions and Scope", "text": "For the sake of tractability, in this paper, we adopt some assumptions from [26]. In particular, we assume the features are categorical. Numeric features can be discretized using standard techniques such as binning [29]. We also focus on binary classification but our ideas can be easily applied to multi-class targets as well. We assume that the foreign key features (FKi) are not (primary) keys in the fact table, e.g., Employer does not uniquely identify a customer.3 Finally, we also do not study the \u201ccold start\u201d issue because it is orthogonal to the focus of this paper [36]. Thus, all features have known finite domains, possibly including a special \u201cOthers\u201d placeholder to temporarily handle hitherto unseen values. In our example, this means that both Employer and Gender have known finite domains. In general, FKi can take values only from the given set of Ri.RIDi values (new FKi values are mapped to \u201cOthers\u201d). Since ML models are rebuilt periodically in practice, new information can then be added to expand feature domains. We emphasize that our goal is not to create new ML or feature selection algorithms, nor is to compare which algorithms yield the best accuracy or runtime. Our goal is to expose and analyze how KFKDs/FDs enable us to dramatically discard foreign features a priori when learning some popular high-capacity classifiers."}, {"heading": "3. EMPIRICAL STUDY WITH REAL DATA", "text": "We now present our detailed empirical study using realworld datasets on 10 classifiers, including 7 high-capacity classifiers (CART decision tree with gini, information gain, and gain ratio; SVM with RBF and quadratic kernels; multilayer perceptron ANN; the \u201cbraindead\u201d 1-nearest neighbor), and 3 linear classifiers (Naive Bayes with backward selection, logistic regression with L1 regularization, and linear SVM). We also conducted experiments with a few other feature selection techniques for the linear classifiers: Naive Bayes with forward selection and filter methods, as well as logistic regression L2 regularization Since these additional linear classifiers did not provide any new insights, we omit them due to space constraints."}, {"heading": "3.1 Datasets", "text": "3Primary keys in the fact table are not generalizable features, unlike foreign keys.\nWe take the seven real datasets from [26]; these are originally from Kaggle, GroupLens, openflights.org, mtg.upf. edu/node/1671, and last.fm. Two datasets have binary targets (Flights and Expedia); the others have multi-class ordinal targets. For the sake of simplicity, we binarize all targets for this paper by grouping ordinal targets into lower and upper halves (this change does not affect our overall conclusions). The dataset statistics are provided in Table 1. We briefly describe the task for each dataset and explain what the foreign features are. More details about their schemas, including the list of all features are already in the public domain (listed in [26]). All of our datasets, scripts, and code are available for download on our project webpage4 to make reproducibility easier.\nWalmart : predict if department-wise sales will be high using past sales (fact table) joined with stores and weather/economic indicators (two dimension tables).\nFlights: predict if a route is codeshared by using other routes (fact table) joined with airlines, source, and destination airports (three dimension tables).\nYelp: predict if a business will be rated highly using past ratings (fact table) joined with users and businesses (two dimension tables).\nMovieLens: predict if a movie will be rated highly using past ratings (fact table) joined with users and movies (two dimension tables).\nExpedia: predict if a hotel will be ranked highly using past search listings (fact table) joined with hotels and search events (two dimension tables but one foreign key, viz., the search ID, has an \u201copen\u201d domain, i.e., past values will not be seen in the future, which makes it unusable as a feature).\n4http://cseweb.ucsd.edu/~arunkk/hamlet\nLastFM : predict if a song will be played often using past play level information (fact table) joined with users and artists (two dimension tables).\nBooks: predict if a book will be rated highly using past ratings (fact table) joined with readers and books (two dimension tables)."}, {"heading": "3.2 Methodology", "text": "Each dataset is pre-split 50%:25%:25% for training, validation (during feature selection and hyper-parameter tuning), and holdout testing. We retain the splits as is. We compare two approaches: JoinAll, which joins all base tables to provide all features to the classifier (the current widespread practice), and NoJoin, which avoids all foreign features a priori (the approach we study). We compare these two approaches for all the 10 classifiers mentioned earlier. For additional insights, we also include a third approach for the decision trees: NoFK, which is essentially the same as JoinAll but with all foreign key features dropped a priori. We used the popular R packages \u201crpart\u201d for the decision trees5 and \u201ce1071\u201d for the SVMs. For the ANNs, we used the popular Python library Keras on top of TensorFlow.\n5Except for the gain ratio case for which we used the \u201cCORElearn\u201d package in R.\nFor Naive Bayes, we used the code from [26], while for logistic regression with L1 regularization, we used the popular R package \u201cglmnet.\u201d We use the validation set for hyperparameter tuning using a standard grid search for each classifier with the grids described in detail below. Note that for Naive Bayes, there is no hyper-parameter tuning.\nDecision Trees: There are two hyper-parameters to tune: minsplit and cp. minsplit is the number of observations that must exist in a node for a split to be attempted. Any split that does not improve the fit by a factor of cp is pruned off. The grid is set as follows: minsplit \u2208 {1, 10, 100, 103} and cp \u2208 {10\u22124, 10\u22123, 0.01, 0.1, 0}\nRBF-SVM : There are two hyper-parameters to tune: C and \u03b3. C controls the cost of misclassification. \u03b3 > 0 controls the bandwidth in the Gaussian kernel (given two data points xi and xj): k(xi, xj) = exp(\u2212\u03b3 \u00b7 \u2016xi \u2212 xj\u20162). The grid is set as follows: C \u2208 {10\u22121, 1, 10, 100, 103} and \u03b3 \u2208 {10\u22124, 10\u22123, 0.01, 0.1, 1, 10}.6\nQuadratic-SVM : We tune the same hyper-parameters C\n6On Movies and Expedia alone, we perform an extra fine tuning step with \u03b3 \u2208 {2\u22127, 2\u22126, 2\u22125, 2\u22124, 2\u22123, 2\u22122, 2\u22121, 1, 2, 22, 23} to improve accuracy.\nDataset Expedia Movies Yelp Walmart LastFM Books\nNoR1 0.7945 0.8537 0.8321 0.9327 0.8242 0.6417\nNoR2 X 0.8539 0.8205 0.9327 0.8230 0.6429\nJoinAll 0.7946 0.8537 0.8320 0.9330 0.8242 0.6417\nNoJoins 0.7945 0.8539 0.8204 0.9323 0.8230 0.6428\nFlights : NoR1 : 0.8466 NoR2 : 0.8490 NoR3 : 0.8483\nNoR1, R2 : 0.8488 NoR1, R3 : 0.8481 NoR2, R3 : 0.8519\nTable 4: Robustness study for discarding dimension tables on the real-world datasets with a Gini decision tree.\nand \u03b3 for the polynomial kernel of degree 2: k(xi, xj) = (\u2212\u03b3 xTi \u00b7 xj)degree. We use the same grid as RBF-SVM.\nLinear-SVM : We tune the C hyper-parameter for the linear kernel: k(xi, xj) = x T i \u00b7 xj , C \u2208 {10\u22121, 1, 10, 100, 103}.\nANN : The multi-layer perceptron architecture comprises of 2 hidden units with 256 and 64 neurons respectively. Rectified linear unit (ReLU) is used as the activation function. In order to allow penalties on layer parameters, we do L2 regularization, with the regularization parameter tuned using the following grid axis: {10\u22124, 10\u22123, 10\u22122}. We choose the popular Adam stochastic gradient optimization algorithm [20] with the learning rate tuned using the following grid axis: {10\u22123, 10\u22122, 10\u22121}. The other hyper-parameters of the Adam algorithm used the default values.\nLogistic Regression: The glmnet package performs automatic hyper-parameter tuning for the L1 regularizer, as well as the optimization algorithm. However, it has three parameters to specify a desired convergence threshold and a limit on the execution time: nlambda, which we set to 100, maxit, which we set to 10000, and thresh, which we set to 0.001.\nTables 2 and 3 present the holdout test accuracy results for all the models on all the datasets."}, {"heading": "3.3 Results", "text": "Accuracy Our first and most important observation is that for almost all the datasets (Yelp being the exception) and for all three split criteria, the accuracy of the decision tree is comparable (a gap of within 1%) between JoinAll and NoJoin.7 The trend is virtually the same for the RBF-SVM and ANN as well. We also observe that the trend is virtually the same for the linear models as well! Thus, regardless of whether our classifier is linear or higher capacity, the relative behavior of NoJoin vis-a-vis JoinAll is virtually the same. These results represent our key counter-intuitive finding: joins are no less safe to avoid with the high-capacity classifiers than with the linear classifiers. The absolute accuracy of the high-capacity classifiers is often significantly higher than the linear classifiers, which is as expected but is also orthogonal and irrelevant to this paper\u2019s focus. Interestingly, on the Yelp dataset, in which both joins are known to be not safe to avoid with the linear classifiers [26], NoJoin correctly sees a large reduction in accuracy from JoinAll\u2013about 0.03. However, the drop in accuracy is smaller for the high-capacity classifiers,\n7Except for gain ratio on LastFM, where NoJoin is actually significantly more accurate than JoinAll !\ne.g., the RBF-SVM, Gini decision tree, and ANN all see a drop of only about 0.01! This suggests that these highcapacity classifiers are sometimes counter-intuitively more robust than linear classifiers to avoiding joins.\nAs for NoFK, it often has much lower accuracy than both JoinAll and NoJoin on all three forms of decision trees. For example, on Flights, the drop is about 0.05. This reaffirms the importance of foreign key features; as such, it is known that dropping foreign key features could cause the bias to shoot up with linear classifiers [26]. A similar scenario arises for the high-capacity classifiers too. Interestingly, in some cases (e.g., Gini on Flights and gain ratio on Books), NoJoin has marginally higher accuracy than JoinAll.\nTo understand the above results more deeply, we conduct a \u201crobustness\u201d experiment by discarding dimension tables one at a time instead of all together. Table 4 presents this experiment\u2019s results for the Gini decision tree. Discarding each dimension table one at a time (and also two at a time in the case of Flights) did not differ much from NoJoin, except for Yelp. On Yelp, the accuracy drops only when R2 (users table) is dropped. From Table 1, we find that the tuple ratio for R2 in Yelp is extremely low: 2.5. That is, there are not enough training examples per unique foreign key value for R2 in Yelp.\n8 Almost every other dimension table can safely be discarded. A similar situation arises for the ANN on Yelp and for the RBF-SVM on Yelp, LastFM, and Books.\nOverall, out of 14 dimension tables across the 7 datasets that can potentially be discarded, we were able to safely discard 13 for the decision tree and ANN, with the tuple ratio threshold being only about 3x. For the RBF-SVM, we were able to discard 11 dimension tables, with the tuple ratio threshold being about 6x. These results are surprising given the more conservative behavior predicted even for the linear classifiers in [26]. For both Naive Bayes and logistic regression, only 7 of the dimension tables were deemed \u201csafe to avoid\u201d with the tuple ratio threshold being about 20x. But of course, even tables that were predicted not safe to avoid could have been avoided without lowering accuracy significantly. Overall, we see that the decision trees and ANN need six times fewer training examples and the RBF-SVM needs three times fewer training examples than linear classifiers to avoid extra overfitting when avoiding the joins. These results are counter-intuitive because conventional wisdom holds that such complex models need more (not fewer) training examples to avoid extra overfitting.\nFor an interesting comparison that we will use later on in Section 5, we also present the results for 1-NN (from package \u201cRWeka\u201d in R; it has no hyper-parameters). Surprisingly, as Table 2 shows, the accuracy of even this braindead classifiers is sometimes comparable to decision trees and RBF-SVMs! More importantly, on most of the datasets, 1-NN with NoJoin has a higher accuracy than with JoinAll. We discuss this behavior further in Section 5.\nRuntimes A key benefit of avoiding KFK joins safely is that ML runtimes (including feature selection) could be significantly lowered for the linear classifiers [26]. We now verify if this is\n8Interestingly, the tuple ratio is similarly low (2.6) for R2 in Books but the error of NoJoin is not much higher. So, the tuple ratio seems to be a conservative indicator: it can tell if an error is likely to rise but the error may not actually rise in some cases.\nthe case for the high-capacity classifiers as well. We compare the runtimes of the end-to-end execution of the ML training (including the grid search) and testing for all models on all datasets. Due to space constraints, we only report Gini metric for the decision tree and the RBF kernel for the SVM; these were also the most robust to avoiding joins. All experiments (except for ANN) were run on CloudLab, which offers free access to physical compute nodes for research [35]. We use a custom OpenStack profile running Ubuntu 14.10 with 40 Intel Xeon cores and 160GB of RAM. The ANN experiments were run on a commodity laptop with Nvidia GeForce GTX 1050 GPU, 16GB RAM and running Windows 10. The version of R used is 3.2.2 and the version of TensorFlow used is 1.1.0. Figure 1 presents the results.\nFor the high-capacity classifiers, we saw an average speedup of about 2x for NoJoin over JoinAll. The highest speedup was on the Movies: 3.6x for the decision tree and 6.2x for the RBF-SVM. As for the ANN, LastFM reported the largest speedup of 2.5x. The speedup for the linear classifiers were more significant. For example, on Movies, we saw a speedup of 707x for Naive Bayes, while on LastFM, we saw a speedup of 20x for logistic regression. Thus, these results corroborate the orders of magnitude speedup reported in [26] for Naive Bayes with backward selection."}, {"heading": "4. IN-DEPTH SIMULATION STUDY", "text": "We now dive deeper into the behavior of the decision tree classifier using a simulation study in which we vary the properties of the underlying \u201ctrue\u201d data distribution. We focus on a two-table KFK join for simplicity. We sample datasets of different dimensions. We use the decision tree for this study because it exhibited the maximum robustness to avoiding KFK joins on the real-world datasets. Our simulation study is designed to comprehensively \u201cstress test\u201d this robustness. Note that our simulation methodology is\nnot tied to decision trees; it is generic enough to be applicable to classifier because we only use standard generic notions of error and net variance as defined in [26].\nSetup and Data Synthesis. There is one dimension table R (q = 1), and all of XS , XR, and Y are boolean (domain size 2). We control the \u201ctrue\u201d distribution P (Y,X) and sample labeled examples in an IID manner from it. We study two different scenarios for what features are used to (probabilistically) determine Y : OneXr and XSXR. These scenarios represent opposite extremes for how likely the (test) error is likely to shoot up when XR is discarded and FK is used as a representative [26]. In OneXr, a lone feature Xr \u2208 XR determines Y ; the rest of XR and XS are random noise (but note that FK will not be noise because it functionally determines Xr). In XSXR, all features in XS and XR determine Y . Intuitively, OneXr is the worst-case scenario for discarding XR because Xr is typically far more succinct than FK, which we expect to translate to less possibility of overfitting with NoJoin. Note that if we use FK directly in P , XR can be more easily discarded because FK conveys more information anyway; so, we skip this scenario.\nThe following data parameters are varied one at a time: number of training examples (nS), size of foreign key domain (|DFK | = nR), number of features in XR (dR), and number of features in XS (dS). We also sample\nnS 4\nexamples each for the validation set (for hyper-parameter tuning) and the holdout test set (final indicator of error). We generate 100 different training datasets and measure the average test error and average net variance (as defined in [9]) based on the different models obtained from these 100 runs."}, {"heading": "4.1 Scenario OneXr", "text": "The \u201ctrue\u201d distribution is set as follows: P (Y = 0|Xr = 0) = P (Y = 1|Xr = 1) = p, where p is called the proba-\nbility skew parameter that controls the Bayes error (noise). The exact procedure for sampling examples is as follows: (1) Construct tuples of R by sampling XR values randomly (each feature value is an independent coin toss). (2) Construct the tuples of S by sampling XS values randomly (independent coin tosses). (3) Assign FK values to S tuples uniformly randomly from DFK . (4) Assign Y values to S tuples by looking up into their respective Xr value (implicit join on FK = RID) and sampling from the above conditional distribution.\nWe compare the same three approaches: JoinAll, which uses X \u2261 [XS , FK,XR], NoJoin, which uses X \u2261 [XS , FK] (i.e., discard XR), and NoFK, which uses X \u2261 [XS ,XR] (i.e., discard FK). We include NoFK for a lower bound on errors, since we know FK does not determine determine Y (although indirectly it does).9 Figure 2 presents the results for the (holdout) test errors for varying each relevant data and distribution parameter, one at a time.\nInterestingly, regardless of the parameter being varied, in almost all cases, NoJoin and JoinAll have virtually identical errors (close to the Bayes error)! From inspecting the actual decision trees learned in these two settings, we found that in almost all cases, FK was used heavily for partitioning and seldom was a feature from XR, including Xr, used. This suggests that FK can indeed act as a good representative of XR even in this extreme sccenario. In contrast to these results, [26] reported that for linear models, the errors of NoJoin shot up compared to JoinAll (a gap of nearly 0.05) as the tuple ratio starts falling below 20. In stark contrast, as Figure 2(B) shows, even for a tuple ratio of just 3, NoJoin and JoinAll have similar errors with the decision tree. This corroborates the results seen for the decision tree on the real datasets (Table 2). When nS becomes very low or when |DFK | becomes very high, the absolute errors of JoinAll and NoJoin increase compared to NoFK. This suggests that when the tuple ratio is very low, NoFK is perhaps worth trying too. This is similar to the behavior seen\n9In general though, NoFK could have much higher errors if FK is part of the true distribution; indeed, NoFK had much higher errors on many real datasets (Table 2).\non Yelp. Overall, NoJoin exhibits similar behavior as the current practice of JoinAll.\nFinally, we also ran this scenario for the RBF-SVM (and 1-NN) and found the trends to be similar, except for the magnitude of the tuple ratio at which NoJoin deviates from JoinAll. Figure 3 presents the results for the experiment in which we increase |DFK | = nR, while fixing everything else, similar to Figure 2(B) for the decision tree. We see that for the RBF-SVM, the error deviation starts when the tuple ratio (nS/nR) falls below roughly 6x. This corroborates its behavior on the real datasets (Table 3). The 1-NN, as expected, is far less stable and the deviation starts even at a tuple ratio of 100x, i.e., nR = 10). As Figure 4 confirms, the deviation in accuracy for the RBF-SVM arises due to the net variance, which helps quantify the extra overfitting. This is akin to the extra overfitting reported in [26] using the plots of the net variance. Intriguingly, the 1-NN sees its net variance exhibit non-monotonic behavior; this is likely an artifact of its unstable behavior, since fewer and fewer training examples will match on FK as nR keeps rising.\nForeign Key Skew. The regular OneXr scenario samples FK uniformly randomly from DFK (step 3 in the procedure). We now ask if a skew in the distribution of FK values could widen the gap between JoinAll and NoJoin. To study this scenario, we modify the data generation procedure slightly: in step 3, we sample FK values with a Zipfian skew or a needle-and-thread skew. The Zipfian skew simply uses a Zipfian distribution for P (FK) controlled by the Zipfian skew parameter. The needle-and-thread skew allocates a large probability mass (parameter p) to a single FK value (the \u201cneedle\u201d) and uniformly distributes the rest of the probability mass to all other FK values (the \u201cthread\u201d). For the linear model case, [26] reported that as the skew parameters increased, the gap widened. Figure 5 presents the results for the decision tree.\nSurprisingly, the gap between NoJoin and JoinAll does not widen significantly no matter how much skew introduced in either the Zipfian or the needle-and-thread case! This result further affirms the remarkable robustness of the decision tree when discarding foreign features. As expected, NoFK is better when nS is low, while overall, NoJoin is quite similar to JoinAll."}, {"heading": "4.2 Scenario XSXR", "text": "Unlike the OneXr scenario, we now create a true distribution that maps X \u2261 [XS ,XR] to Y without any Bayes error (noise). The exact procedure for sampling examples is as follows: (1) Construct a true probability table (TPT) with entries for all possible values of [XS ,XR] and assign a random probability to each entry such that the total probability is 1. (2) For each entry in the TPT, pick a Y value randomly and append the TPT entry; this ensures H(Y |X) = 0. (3) Marginalize the TPT to obtain P (XR) and from it, sample nR = DFK tuples for R along with an associated sequential RID value. (4) In the original TPT, push the probability of each entry to 0 if its XR values did not get picked for R in step 3. (5) Renormalize the TPT so that the total probability is 1 and sample nS examples (Y values do not change) and construct S. (6) For each tuple in S, pick its FK value\nuniformly randomly from the subset of RID values that map to its XR value in R (an implicit join).\nWe compare three settings: JoinAll, NoJoin, and NoFK, with NoFK meant to be a lower bound on the errors possible (because it uses the knowledge that FK is not directly a part of the true distribution). Once again, our hypothesis is that JoinAll and NoJoin will exhibit similar errors in most cases, while NoFK will perform better when the tuple ratio is low. Figure 6 presents the results.\nOnce again, we see that NoJoin and JoinAll exhibit similar errors in almost all cases, with the largest gap being 0.017 in Figure 6(C)). Interestingly, even when the tuple ratio is close to 1, the gap between NoJoin and JoinAll does not widen much. Figure 6(B)) shows that as |DFK | increases, NoFK remains at low overall errors, unlike both JoinAll and NoJoin. But as we increase dR or dS , the gap between JoinAll/NoJoin and NoFK narrows because even NoFK does not have enough training examples. Of course, all gaps virtually disappear as the number of training examples increases, as shown by Figure 6(A). Overall, NoJoin exhibits similar behavior as the current practice of JoinAll even in this scenario."}, {"heading": "4.3 Scenario RepOneXr", "text": "We now present results for a new simulation scenario in which the true distribution is precisely captured using a lone feature Xr \u2208 XR. We sample examples similarly as per\nthe procedure mentioned earlier for OneXr, except that the tuples of R will now be constructed by replicating the value of Xr sampled for a tuple to create all the other features in XR. That is, XR of an example is just the same value repeated dR times. Note that the FD FK \u2192 XR implies there are at least as many unique FK values as XR values. Thus, by increasing the number of FK values relative to XR values, we hope to increase the chance of the model getting \u201cconfused\u201d with NoJoin. Our goal is to see if this widens the gap between JoinAll and NoJoin.\nFigure 7 presents the results for the two experiments on decision trees where (A) has a high tuple ratio of 25x and (B) has a low tuple ratio of 5x. We see that once again, JoinAll and NoJoin exhibit similar errors in both the cases. We also run this simulation scenario for both the RBF-SVM and 1- NN as well; the results are shown in Figure 8) and Figure 9 respectively. We see that the trends are similar to the decision tree. For the RBF-SVM, the error of NoJoin deviates at a tuple ratio of about 5x. As for 1-NN, as expected, it is much less stable and the deviation happens even at a higher tuple ratio of 25x. At low tuple ratios, as expected, the absolute errors of JoinAll and NoJoin increase compared to NoFK for all three models."}, {"heading": "5. ANALYSIS AND OPEN QUESTIONS", "text": ""}, {"heading": "5.1 Explaining the Results", "text": "We now intuitively explain the surprising behavior of decision trees and RBF-SVM with NoJoin vis-a-vis JoinAll. We first ask: Does NoJoin compromise the \u201cgeneralization error\u201d? The generalization error is the difference of the test and train errors. Tables 2 and 3 already provided the test accuracy. Tables 5 and 6 now provide the train accuracy. Clearly, JoinAll vs NoJoin are almost indistinguishable for the decision tree! The only exception is Yelp, which we already noted. Note that the absolute generalization error is often high, which is expected for decision trees [17]. For example, the train accuracy is nearly 100% on Flights, while the test accuracy on it is only 85%. But the absolute generalization error is orthogonal to our focus; we only note that NoJoin does not increase this difference significantly."}, {"heading": "In other words, discarding foreign features did not significantly affect the generalization error of the decision tree!", "text": "The generalization errors of the RBF-SVM also exhibit a similar trend.\nReturning to 1-NN, Table 2 showed that it has similar accuracy as RBF-SVM on some datasets. We now explain why that comparison is useful: the RBF-SVM essentially behaves similar to the 1-NN in some cases when FK is used (both JoinAll and NoJoin)! But this does not necessarily hurt its test accuracy. Note that FK is represented using the standard one-hot encoding for RBF-SVM and 1-NN. So, FK can contribute to a maximum distance of 2 in a (squared) Euclidean distance between two examples xi and xj . But since XR is functionally dependent on FK, if xi.FK = xj .FK, then xi.XR = xj .XR. So, if xi.FK = xj .FK, the only contributor to the distance is XS . But in many of the datasets, since XS is empty (dS = 0), FK becomes the sole determiner of the distances for NoJoin. This is akin to sheer memorization of a feature\u2019s large domain. Since we operate on features with finite domains, test examples will also have FK from that domain. Thus, memorizing FK does not hurt generalization. While this seems similar to how deep neural networks excel at sheer memorization but still offer good test accuracy [45], the models in our setting are not necessarily memorizing all features \u2013 only the foreign keys. A similar explanation holds for the decision tree. If XS is not empty, then it will likely play a major role in the distance computations and our setting becomes more similar to the traditional single-table learning setting (no FDs).\nWe now explain why NoJoin might deviate from JoinAll when the tuple ratio is very low for the RBF-SVM. Even if xi.FK 6= xj .FK, it is possible that xi.XR = xj .XR. Suppose the \u201ctrue\u201d distribution is captured by XR, e.g., as in OneXr. If the tuple ratio is very low, there are many FK values but the number of XR values might still be small. In this case, given xi, RBF-SVM (and 1-NN) is more likely to pick an xj that minimizes the distances on XR, thus potentially yielding lower errors. But since NoJoin does not have access to XR, it can only use XS and FK. So, if XS is mostly noise, the possibility of the model getting \u201cconfused\u201d increases. To see why, if there are very few other examples that share xi.FK, then matching on XS might become more important. Thus, a non-match on FK becomes more likely, which means a non-match on the implicit XR becomes more likely, which in turns makes higher errors more likely. But if there are more examples that share xi.FK, then a match on FK is more likely. Thus, as the tuple ratio increases, the gap between NoJoin and JoinAll disappears, as Figure 3 showed. Internally, the RBF-SVM seems more robust to such chance mismatches by learning a higher-level relationship between all features compared to the stark 1-NN. Thus, the RBF-SVM is more robust to discarding foreign features at lower tuples ratios than 1-NN.\nFinally, focusing on the decision tree, its internal feature selection and partitioning seems to make it quite robust to noise from any other features. Suppose again the \u201ctrue\u201d distribution is similar to OneXr. Since FK already encodes all information that XR provides [26], the tree almost always uses FK in its partitioning, often multiple times. This is not necessarily \u201cbad\u201d for test accuracy because test examples share the FK domain. But when the tuple ratio becomes extremely low, the chance of XS \u201cconfusing\u201d the tree over the information FK provides goes up, potentially leading to\nhigher errors with NoJoin. JoinAll escapes such a confusion thanks to XR. If XS is empty, then FK will almost surely be used for partitioning. But with very few training examples per FK value, the chance of sending it to a wrong partition goes up, leading to higher errors. It turns out that even with just 3 or 4 training examples per FK value, such issues get mitigated. Thus, the decision tree seems even more robust to discarding foreign features."}, {"heading": "5.2 Open Research Questions", "text": "While our intuitive explanations capture the fine-grained behavior of the decision tree and RBF-SVM with NoJoin vis-a-vis JoinAll, there are many open questions for more research. Is it possible to quantify the probability of wrong partitioning with a decision tree as a function of the data properties? Is it possible to quantify the probability of mismatched examples being picked by the RBF-SVM? Why does the theory of VC-dimensions predict the opposite of the observed behavior with these models? How do we quantify their generalizability if memorization is allowable and what forms of memorization are allowed? Answering these questions would provide deeper insights into the effects of KFKDs/FDs on the generalizability and accuracy of such classifiers. It could also yield more formal mechanisms to characterize when discarding foreign features is feasible beyond just looking at tuple ratios.\nFrom a data management perspective, there are database dependencies more general than FDs: embedded multi-valued dependencies (EMVDs) and join dependencies (JDs) [39]. How does the presence of such database dependencies among features affect the behavior of ML models? There are also conditional FDs, which satisfy FD-like constraints among subsets of the dataset [39]. How do such properties of the data distribution affect ML behavior? Finally, the axioms of FDs imply that foreign features can be divided into arbitrary subsets before being avoided, which opens up a new tradeoff space between fully avoiding a foreign table and fully using it. How do we quantify this trade-off and exploit it? Answering these questions could open up new connections between data management and ML theory and potentially lead to new functionalities for ML systems."}, {"heading": "6. MAKING FK FEATURES PRACTICAL", "text": "We now discuss two key practical issues caused by the large domains of foreign key features and verify how standard approaches can be adapted to resolve them. In contrast to prior work on handling regular large-domain features [7], foreign key features are distinct in that they have coarsergrained side information available in the form of foreign features. This suggests an alternative way to exploiting such features, if possible, rather than always joining them in."}, {"heading": "6.1 Foreign Key Domain Compression", "text": "While foreign keys often act as good representatives of foreign features for accuracy, they pose a practical bottleneck for interpretability due to their domain sizes. For example, it is hard to visualize a decision tree that uses a foreign key feature with 1000s of values. In order to make foreign key features more practical, we consider a simple approach that is standard in the ML literature: lossy domain compression to a (much) smaller user-defined domain size. Essentially, given a foreign key feature FK with domain DFK recoded as [m] (where m = |DFK |) and a user-specified positive integer \u201cbudget\u201d l m, we want a mapping f : [m]\u2192 [l].\nA standard unsupervised method to construct f is the Random hashing trick [41], i.e., randomly hash from [m] to [l]. We also try a simple supervised method we call the Sort-based method to preserve more of the information contained in FK about Y . Sort-based is a greedy approach: sort DFK based on H(Y |FK = z), z \u2208 DFK , compute the differences among adjacent pairs of values, and pick the boundaries corresponding to the top l \u2212 1 differences (ties broken randomly). This gives us an l-partition of DFK . The intuition is that by grouping FK values that have comparable conditional entropy, H(Y |f(FK)) is unlikely to be much higher than H(Y |FK). Note that the lower H(Y |FK) is, the more informative FK is to predict Y . We leave more sophisticated approaches to future work.\nWe now empirically compare the above two heuristics using two of the real datasets for the Gini decision tree with NoJoin. Our methodology is as follows. We retain the 50:25:25 train-validate-test split from before. We use the training split to construct f and then compress FK for the whole dataset. We then use the validation set as before to tune the hyper-parameters and measure the holdout test error. For random hashing, we report the average across five runs. Figure 10 presents the results.\nOn Yelp, both Random and Sort-based have comparable accuracy although Sort-based is marginally higher, especially as the budget l increases. But on Flights, we see a larger gap for some values of l although the gap narrows as the l increases. The test accuracy with the whole DFK (l = m) for NoJoin on Flights was 0.8516 (see Table 2). So, it is surprising the test accuracy is about 0.83 with such high compression. Even more surprisingly, the test accuracy with the whole DFK (l = m) for NoJoin on Yelp was 0.8204 and for NoFK was 0.8644. So, with domain compression, we see significantly higher accuracy for NoJoin, even higher than NoFK. Overall, these results suggest that FK domain compression is a promising way to resolve the large-domain issue rather than simply dropping FK."}, {"heading": "6.2 Foreign Key Smoothing", "text": "Another issue caused by large |DFK | is that some FK values might not arise in the train set but arise in the test\nset or during deployment. This is not a cold start issue \u2013 the FK values are all still from the fully known DFK . This issue arises because there are not enough labeled examples to cover all of DFK during training. Typically, this issue is handled using some form of smoothing, e.g., Laplacian smoothing for Naive Bayes by adding a pseudocount of 1 to all frequency counts [29]. While similar smoothing techniques have been studied for probability estimation using decision trees [32], to the best of our knowledge, this issue has not been handled in general for classification using decision trees. In fact, popular decision tree implementations in R simply crash if a value of FK not seen during training arises during testing! Note that SVMs (or any other classifier operating on numeric feature spaces) do not face this issue due to the one-hot encoding of FK.\nWe consider a simple approach to mitigate this issue: smooth by reassigning an FK value not seen during training to an FK value that was seen. There are various ways to reassign; for simplicity sake, we only study two lightweight unsupervised methods. We leave more sophisticated approaches to future work. We consider both random reassignment and alternative approach that uses the foreign features (XR) to decide the reassignment. Note that the latter is only feasible in cases where the dimension tables are available and not discarded. Since R provides auxiliary descriptive information about FK, we can utilize it for smoothing even if not for learning directly over them. Our algorithm is simple: given a test example with FK not seen during training, obtain an FK seen during training whose corresponding XR feature vector has the minimum l0 distance with the test example\u2019s XR (ties broken randomly). The l0 distance is simply the count of the number of pairwise mismatches of the respective features in the two XR feature vectors.\nThe intuition for XR-based smoothing is that if XR is part of the \u201ctrue\u201d distribution, it may yield higher accuracy than random reassignment. But if XR is just noise, this becomes essentially random reassignment. To validate our claim, we use the OneXr simulation scenario. Recall that a feature Xr \u2208 XR determines the target (with some Bayes noise as before). We introduce a parameter \u03b3 that is the ratio of the number of FK values not seen during training to |DFK |. If \u03b3 = 0, no smoothing is needed; as \u03b3 increases, more smoothing is needed. Figure 11 presents the results.\nThe plots confirm our intuition: the XR-based smoothing yields much lower test errors for both NoJoin and JoinAll\u2013 in fact, errors comparable to NoFK and the Bayes error\u2013for lower values of \u03b3 (< 0.5). But as \u03b3 gets closer to 1, the errors of XR-based smoothing also increase but not as much as random hashing. Overall, these results suggest that even if foreign features are available, rather for using them directly for learning the model, we could use them as side information for smoothing FK features. Overall, these results suggest that it is possible to get \u201cthe best of both worlds\u201d: the\nruntime and usability gains of NoJoin (as against JoinAll, which unnecessarily also learns over the foreign features) along with exploiting the extra information provided by foreign features (if they are available) for smoothing foreign key features."}, {"heading": "7. RELATED WORK", "text": "Database Dependencies and ML. The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy. It was also studied in [43] but their focus was on devising a new ML algorithm. In contrast, our work focuses on the more fundamental question of whether KFK joins can be avoided safely for existing popular ML algorithms. We first demonstrated the feasibility of avoiding joins safely in [26] for linear models. In this work, we revisit that idea for higher capacity models and find that they are counter-intuitively more robust than linear models to avoiding joins, not less as the VC dimension-based analysis in [26] suggested. We also empirically verify mechanisms to make foreign key features more practical. Embedded multi-valued dependencies (EMVDs) are database dependencies that are more general than functional dependencies [2]. The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42]. However, their use of EMVDs still requires computations over all features in the data instance. In contrast, avoiding joins safely omits entire sets of features for complex ML models without performing any computations on the foreign features. There is a large body of work on statistical relational learning (SRL) to handle joins that cause duplicates in the fact table [13]. But as mentioned before, our work focuses on the regular IID setting for which SRL might be an overkill.\nFeature Selection. The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15]. In contrast, our goal is not to design new feature selection methods nor is it compare existing methods. Rather, we want to understand if KFKDs/FDs in the schema can enable us to avoid entire sets of features a priori for some popular complex classifiers. This is a way of \u201cshort-circuiting\u201d the feature selection process using database schema information to reduce the burden of data sourcing. The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21]. The conventional wisdom is that even a feature that is redundant might be highly relevant and hence, unavoidable in the mix [14]. Our work establishes, perhaps surprisingly, that this is not the case for foreign features; even if a foreign feature is highly relevant, it can be safely discarded in most practical cases for decision trees, RBF-SVMs, and ANNs. There is prior work on exploiting FDs in feature selection. [40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search. The idea of avoiding joins safely is orthogonal to these algorithms because they all still require computations over all\nfeatures, while avoiding a join safely omits dependent features without even looking at them and obviously, without performing any computations on them! To the best of our knowledge, no feature selection method exhibits such a dramatic capability. Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17]. Our problem is orthogonal because we study on how KFKDs/FDs enable us to ignore foreign features a priori safely. Even with the gain ratio score that is known to mitigate the bias towards largedomain features, our main findings stand. Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29]. Our lossy compression techniques to reduce the domains of foreign key features for decision trees are inspired by such methods.\nData Integration. Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8]. These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10]. Avoiding joins safely can be seen as one schema-based mechanism to reduce the integration burden by predicting a priori if a source table is unlikely to improve ML accuracy. It is a major open challenge to devise similar mechanisms can be devised for other types of data sources, say, using other forms of schema constraints, ontology information, and sampling. There is also a growing interest in making data discovery and other forms of metadata management easier [?, ?]. Our work can be seen as a mechanism to verify the potential utility of some of the discovered data sources using their metadata. We hope our work spurs more research in this direction of exploiting ideas from data integration and data discovery to reduce the data sourcing burden for ML tasks."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "We think it is high time for the data management community to look beyond just building faster or scalable ML systems and help reduce the pains of data sourcing and feature engineering for ML. Understanding how fundamental properties of data sources, especially schema information, affect ML behavior is one promising step in this direction. While the idea of avoiding joins safely has been adopted in practice for linear classifiers, in this comprehensive experimental study, we show that it works even better for popular highcapacity classifiers, which goes against the intuition that high-capacity classifiers are typically more prone to overfitting. We hope that our results and analysis spur more discussions and new research on simplifying data sourcing for ML-based analytics.\nAs for future work, we plan to formally analyze the effects of KFKDs/FDs on high-capacity classifiers from a learning theoretic perspective. Other interesting avenues for future work include understanding the effects of more general database dependencies on classifiers, the effects of all database dependencies on regression and clustering models, and designing an automated \u201cadvisor\u201d for data sourcing for ML tasks, especially when there are heterogeneous data types and sources."}, {"heading": "9. REFERENCES", "text": "[1] Personal communications: Facebook friend\nrecommendation system; LogicBlox retail analytics; MakeMyTrip customer analytics.\n[2] S. Abiteboul, R. Hull, and V. Vianu, editors. Foundations of Databases: The Logical Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1st edition, 1995.\n[3] H. Almuallim and T. G. Dietterich. Efficient Algorithms for Identifying Relevant Features. Technical report, 1992.\n[4] M. Anderson, D. Antenucci, V. Bittorf, M. Burgess, M. J. Cafarella, A. Kumar, F. Niu, Y. Park, C. Re\u0301, and C. Zhang. Brainwash: A Data System for Feature Engineering. In CIDR, 2013.\n[5] M. Boehm, M. W. Dusenberry, D. Eriksson, A. V. Evfimievski, F. M. Manshadi, N. Pansare, B. Reinwald, F. R. Reiss, P. Sen, A. C. Surve, and S. Tatikonda. SystemML: Declarative Machine Learning on Spark. In VLDB, 2016.\n[6] M. Dash, H. Liu, and H. Motoda. Consistency based feature selection. In Proceedings of the 4th Pacific-Asia Conference on Knowledge Discovery and Data Mining, Current Issues and New Applications, PAKDK, pages 98\u2013109, London, UK, UK, 2000. Springer-Verlag.\n[7] H. Deng, G. Runger, and E. Tuv. Bias of importance measures for multi-valued attributes and solutions. In Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II, ICANN\u201911, pages 293\u2013300, Berlin, Heidelberg, 2011. Springer-Verlag.\n[8] A. Doan, A. Halevy, and Z. Ives. Principles of Data Integration. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition, 2012.\n[9] P. Domingos. A Unified Bias-Variance Decomposition and its Applications. In Proceedings of 17th International Conference on Machine Learning, 2000.\n[10] X. L. Dong and D. Srivastava. Big data integration. Proceedings of the VLDB Endowment, 6(11):1188\u20131189, Aug. 2013.\n[11] X. Feng, A. Kumar, B. Recht, and C. Re\u0301. Towards a Unified Architecture for in-RDBMS Analytics. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201912, 2012.\n[12] Y. Fu, L. Cao, G. Guo, and T. S. Huang. Multiple feature fusion by subspace learning. In Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, CIVR \u201908, pages 127\u2013134, New York, NY, USA, 2008. ACM.\n[13] L. Getoor and B. Taskar. Introduction to Statistical Relational Learning). The MIT Press, 2007.\n[14] I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh. Feature Extraction: Foundations and Applications. New York: Springer-Verlag, 2001.\n[15] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data mining, Inference, and Prediction. Springer-Verlag, 2001.\n[16] J. M. Hellerstein, C. Re\u0301, F. Schoppmann, D. Z. Wang, E. Fratkin, A. Gorajek, K. S. Ng, C. Welton, X. Feng, K. Li, and A. Kumar. The MADlib Analytics Library or MAD Skills, the SQL. In VLDB, 2012.\n[17] T. Hothorn, K. Hornik, and A. Zeileis. Unbiased recursive partitioning: A conditional inference framework. JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS, 15(3):651\u2013674, 2006.\n[18] Y. Jing, D. Liu, D. Kislyuk, A. Zhai, J. Xu, J. Donahue, and S. Tavel. Visual search at pinterest. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1889\u20131898, New York, NY, USA, 2015. ACM.\n[19] B. Khaleghi, A. Khamis, F. O. Karray, and S. N. Razavi. Multisensor data fusion: A review of the state-of-the-art. Information Fusion, 14(1):28\u201344, Jan. 2013.\n[20] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In 3rd International Conference for Learning Representations (ICLR), 2015.\n[21] D. Koller and M. Sahami. Toward Optimal Feature Selection. In ICML, 1995.\n[22] P. Konda, A. Kumar, C. Re\u0301, and V. Sashikanth. Feature Selection in Enterprise Analytics: A Demonstration using an R-based Data Analytics System. In VLDB, 2013.\n[23] T. Kraska, A. Talwalkar, J. C. Duchi, R. Griffith, M. J. Franklin, and M. I. Jordan. MLbase: A Distributed Machine-learning System. In CIDR, 2013.\n[24] A. Kumar, M. Jalal, B. Yan, J. Naughton, and J. M. Patel. Demonstration of Santoku: Optimizing Machine Learning over Normalized Data. In VLDB, 2015.\n[25] A. Kumar, J. Naughton, and J. M. Patel. Learning Generalized Linear Models Over Normalized Data. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201915, 2015.\n[26] A. Kumar, J. Naughton, J. M. Patel, and X. Zhu. To Join or Not to Join? Thinking Twice about Joins before Feature Selection. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, 2016.\n[27] Y. Li and A. Ngom. Data Integration in Machine Learning. In IEEE International Conference on Bioinformatics and Biomedicine (BTBM), 2015.\n[28] Y. Low, J. E. Gonzalez, A. Kyrola, D. Bickson, C. E. Guestrin, and J. Hellerstein. GraphLab: A New Framework For Parallel Machine Learning. In UAI, 2010.\n[29] T. M. Mitchell. Machine Learning. McGraw Hill, 1997.\n[30] J. Pearl and T. Verma. The Logic of Representing Dependencies by Directed Graphs. In AAAI, 1987.\n[31] N. Polyzotis, S. Roy, S. E. Whang, and M. Zinkevich. Data management challenges in production machine learning. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1723\u20131726, New York, NY, USA, 2017. ACM.\n[32] F. Provost and P. Domingos. Tree Induction for Probability-Based Ranking. Machine Learning, 52(3):199\u2013215, 2003.\n[33] R. Ramakrishnan and J. Gehrke. Database Management Systems. McGraw-Hill, Inc., 2003.\n[34] S. Rendle. Scaling Factorization Machines to Relational Data. In Proceedings of the VLDB Endowment, 2013.\n[35] R. Ricci, E. Eide, and C. Team. Introducing CloudLab: Scientific Infrastructure for Advancing Cloud Architectures and Applications. ; login:: the magazine of USENIX & SAGE, 39(6):36\u201338, 2014.\n[36] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock. Methods and Metrics for Cold-start Recommendations. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002.\n[37] M. Schleich, D. Olteanu, and R. Ciucanu. Learning Linear Regression Models over Factorized Joins. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916, 2016.\n[38] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M. Young, J.-F. Crespo, and D. Dennison. Machine Learning: The High Interest Credit Card of Technical Debt. In SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop), 2014.\n[39] A. Silberschatz, H. Korth, and S. Sudarshan. Database Systems Concepts. McGraw-Hill, Inc., New York, NY, USA, 5 edition, 2006.\n[40] O. Uncu and I. Turksen. A Novel Feature Selection Approach: Combining Feature Wrappers and Filters.\nInformation Sciences, 177(2), 2007.\n[41] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. Attenberg. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, New York, NY, USA, 2009. ACM.\n[42] S. K. M. Wong, , C. J. Butz, and Y. Xiang. A Method for Implementing a Probabilistic Model as a Relational Database. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, 1995.\n[43] X. Yin, J. Han, J. Yang, and P. S. Yu. Crossmine: Efficient classification across multiple database relations. In Proceedings of the 2004 European Conference on Constraint-Based Mining and Inductive Databases, pages 172\u2013195, Berlin, Heidelberg, 2005. Springer-Verlag.\n[44] L. Yu and H. Liu. Efficient Feature Selection via Analysis of Relevance and Redundancy. Journal of Machine Learning Research, 5, Dec. 2004.\n[45] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding Deep Learning Requires Rethinking Generalization. In International Conference on Learning Representations (ICLR), 2017.\n[46] Y. Zhang et al. I/O-Efficient Statistical Computing with RIOT. In ICDE, 2010."}], "references": [{"title": "editors", "author": ["S. Abiteboul", "R. Hull", "V. Vianu"], "venue": "Foundations of Databases: The Logical Level. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1st edition", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Efficient Algorithms for Identifying Relevant Features", "author": ["H. Almuallim", "T.G. Dietterich"], "venue": "Technical report", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Brainwash: A Data System for Feature Engineering", "author": ["M. Anderson", "D. Antenucci", "V. Bittorf", "M. Burgess", "M.J. Cafarella", "A. Kumar", "F. Niu", "Y. Park", "C. R\u00e9", "C. Zhang"], "venue": "CIDR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "SystemML: Declarative Machine Learning on Spark", "author": ["M. Boehm", "M.W. Dusenberry", "D. Eriksson", "A.V. Evfimievski", "F.M. Manshadi", "N. Pansare", "B. Reinwald", "F.R. Reiss", "P. Sen", "A.C. Surve", "S. Tatikonda"], "venue": "VLDB", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Consistency based feature selection", "author": ["M. Dash", "H. Liu", "H. Motoda"], "venue": "Proceedings of the 4th Pacific-Asia Conference on Knowledge Discovery and Data Mining, Current Issues and New Applications, PAKDK, pages 98\u2013109, London, UK, UK", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Bias of importance measures for multi-valued attributes and solutions", "author": ["H. Deng", "G. Runger", "E. Tuv"], "venue": "Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II, ICANN\u201911, pages 293\u2013300, Berlin, Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Principles of Data Integration", "author": ["A. Doan", "A. Halevy", "Z. Ives"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A Unified Bias-Variance Decomposition and its Applications", "author": ["P. Domingos"], "venue": "Proceedings of 17th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Big data integration", "author": ["X.L. Dong", "D. Srivastava"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Towards a Unified Architecture for in-RDBMS Analytics", "author": ["X. Feng", "A. Kumar", "B. Recht", "C. R\u00e9"], "venue": "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201912", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple feature fusion by subspace learning", "author": ["Y. Fu", "L. Cao", "G. Guo", "T.S. Huang"], "venue": "Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, CIVR \u201908, pages 127\u2013134, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Introduction to Statistical Relational Learning)", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature Extraction: Foundations and Applications", "author": ["I. Guyon", "S. Gunn", "M. Nikravesh", "L.A. Zadeh"], "venue": "New York: Springer-Verlag", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "The Elements of Statistical Learning: Data mining", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Inference, and Prediction. Springer-Verlag", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The MADlib Analytics Library or MAD Skills", "author": ["J.M. Hellerstein", "C. R\u00e9", "F. Schoppmann", "D.Z. Wang", "E. Fratkin", "A. Gorajek", "K.S. Ng", "C. Welton", "X. Feng", "K. Li", "A. Kumar"], "venue": "the SQL. In VLDB", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unbiased recursive partitioning: A conditional inference framework", "author": ["T. Hothorn", "K. Hornik", "A. Zeileis"], "venue": "JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS, 15(3):651\u2013674", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Visual search at pinterest", "author": ["Y. Jing", "D. Liu", "D. Kislyuk", "A. Zhai", "J. Xu", "J. Donahue", "S. Tavel"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1889\u20131898, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Multisensor data fusion: A review of the state-of-the-art", "author": ["B. Khaleghi", "A. Khamis", "F.O. Karray", "S.N. Razavi"], "venue": "Information Fusion,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "3rd International Conference for Learning Representations (ICLR)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Toward Optimal Feature Selection", "author": ["D. Koller", "M. Sahami"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Feature Selection in Enterprise Analytics: A Demonstration using an R-based Data Analytics System", "author": ["P. Konda", "A. Kumar", "C. R\u00e9", "V. Sashikanth"], "venue": "VLDB", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "MLbase: A Distributed Machine-learning System", "author": ["T. Kraska", "A. Talwalkar", "J.C. Duchi", "R. Griffith", "M.J. Franklin", "M.I. Jordan"], "venue": "CIDR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Demonstration of Santoku: Optimizing Machine Learning over Normalized Data", "author": ["A. Kumar", "M. Jalal", "B. Yan", "J. Naughton", "J.M. Patel"], "venue": "VLDB", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Generalized Linear Models Over Normalized Data", "author": ["A. Kumar", "J. Naughton", "J.M. Patel"], "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201915", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "To Join or Not to Join? Thinking Twice about Joins before Feature Selection", "author": ["A. Kumar", "J. Naughton", "J.M. Patel", "X. Zhu"], "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Data Integration in Machine Learning", "author": ["Y. Li", "A. Ngom"], "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BTBM)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "GraphLab: A New Framework For Parallel Machine Learning", "author": ["Y. Low", "J.E. Gonzalez", "A. Kyrola", "D. Bickson", "C.E. Guestrin", "J. Hellerstein"], "venue": "UAI", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning", "author": ["T.M. Mitchell"], "venue": "McGraw Hill", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "The Logic of Representing Dependencies by Directed Graphs", "author": ["J. Pearl", "T. Verma"], "venue": "AAAI", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1987}, {"title": "Data management challenges in production machine learning", "author": ["N. Polyzotis", "S. Roy", "S.E. Whang", "M. Zinkevich"], "venue": "Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD \u201917, pages 1723\u20131726, New York, NY, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Tree Induction for Probability-Based Ranking", "author": ["F. Provost", "P. Domingos"], "venue": "Machine Learning, 52(3):199\u2013215", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Database Management Systems", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": "McGraw-Hill, Inc.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Scaling Factorization Machines to Relational Data", "author": ["S. Rendle"], "venue": "Proceedings of the VLDB Endowment", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Introducing CloudLab: Scientific Infrastructure for Advancing Cloud Architectures and Applications", "author": ["R. Ricci", "E. Eide", "C. Team"], "venue": "; login:: the magazine of USENIX & SAGE, 39(6):36\u201338", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods and Metrics for Cold-start Recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning Linear Regression Models over Factorized Joins", "author": ["M. Schleich", "D. Olteanu", "R. Ciucanu"], "venue": "Proceedings of the 2016 International Conference on Management of Data, SIGMOD \u201916", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning: The High Interest Credit Card of Technical Debt", "author": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young", "J.-F. Crespo", "D. Dennison"], "venue": "SE4ML: Software Engineering for Machine Learning ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Database Systems Concepts", "author": ["A. Silberschatz", "H. Korth", "S. Sudarshan"], "venue": "McGraw-Hill, Inc., New York, NY, USA, 5 edition", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "A Novel Feature Selection Approach: Combining Feature Wrappers and Filters", "author": ["O. Uncu", "I. Turksen"], "venue": " Information Sciences, 177(2)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, New York, NY, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A Method for Implementing a Probabilistic Model as a Relational Database", "author": ["S.K.M. Wong", "C.J. Butz", "Y. Xiang"], "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1995}, {"title": "Crossmine: Efficient classification across multiple database relations", "author": ["X. Yin", "J. Han", "J. Yang", "P.S. Yu"], "venue": "Proceedings of the 2004 European Conference on Constraint-Based Mining and Inductive Databases, pages 172\u2013195, Berlin, Heidelberg", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient Feature Selection via Analysis of Relevance and Redundancy", "author": ["L. Yu", "H. Liu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Understanding Deep Learning Requires Rethinking Generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2017}, {"title": "I/O-Efficient Statistical Computing with RIOT", "author": ["Y. Zhang"], "venue": "In ICDE,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": ", [16, 11, 46]), how to scale ML (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 9, "context": ", [16, 11, 46]), how to scale ML (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 44, "context": ", [16, 11, 46]), how to scale ML (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 3, "context": ", [5, 28]), and how to use database ideas to improve ML tasks (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": ", [5, 28]), and how to use database ideas to improve ML tasks (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 20, "context": ", [22, 23]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 21, "context": ", [22, 23]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 2, "context": "However, little work has tackled the pains of sourcing data for ML tasks in the first place, especially, how fundamental data properties affect end-to-end data workflows for ML tasks [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 31, "context": "In particular, real-world relational databases often have many tables connected by database dependencies such as key-foreign key dependencies (KFKDs) [33].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "Thus, given an ML task, data scientists almost always join multiple tables because they like to obtain more features for ML models [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 36, "context": "Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].", "startOffset": 191, "endOffset": 199}, {"referenceID": 29, "context": "Furthermore, recent reports of Google\u2019s production ML systems show that features that yield marginal benefits incur high \u201ctechnical debt\u201d that decreases code mangeability and increases costs [38, 31].", "startOffset": 191, "endOffset": 199}, {"referenceID": 24, "context": "[26] showed that one can often omit entire tables by exploiting KFKDs in the database schema.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Example (based on [26]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "The analysis in [26] revealed a dichotomy in how safe it is to avoid a join from an accuracy standpoint: in terms of the bias-variance trade-off, avoiding a join is unlikely to increase bias but it might significantly increase variance, since foreign key features often have larger domains than foreign features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "In [26], the tuple ratio quantifies this behavior; in our example, it is the ratio of the number of labeled customers to the number of employers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26].", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "While KFKDs are not the same as FDs [39], assuming features have \u201cclosed\u201d domains, they behave essentially as FDs in the output of the join [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "However, the results in [26] had a major caveat\u2013they applied only to linear classifiers.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "Surprisingly, our results show that their behavior is the exact opposite! We start by rerunning the experiments on the real-world datasets with KFK joins from [26] for these models.", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "In other words, our work refutes an intuition from the VC dimension-based analysis of [26] and shows that these popular high-capacity classifiers are counter-intuitively comparably or more robust to avoiding joins than linear classifiers, not less.", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": ", the holdout test errors blow up) [26].", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].", "startOffset": 120, "endOffset": 132}, {"referenceID": 24, "context": "Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].", "startOffset": 120, "endOffset": 132}, {"referenceID": 23, "context": "Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "Note that our setting is different from the statistical relational learning (SRL) setting, which deals with joins that violate the IID assumption and duplicate labeled examples from S [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 24, "context": "For the sake of tractability, in this paper, we adopt some assumptions from [26].", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "Numeric features can be discretized using standard techniques such as binning [29].", "startOffset": 78, "endOffset": 82}, {"referenceID": 34, "context": "Finally, we also do not study the \u201ccold start\u201d issue because it is orthogonal to the focus of this paper [36].", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "We take the seven real datasets from [26]; these are originally from Kaggle, GroupLens, openflights.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "More details about their schemas, including the list of all features are already in the public domain (listed in [26]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 24, "context": "For Naive Bayes, we used the code from [26], while for logistic regression with L1 regularization, we used the popular R package \u201cglmnet.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "We choose the popular Adam stochastic gradient optimization algorithm [20] with the learning rate tuned using the following grid axis: {10\u22123, 10\u22122, 10\u22121}.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Interestingly, on the Yelp dataset, in which both joins are known to be not safe to avoid with the linear classifiers [26], NoJoin correctly sees a large reduction in accuracy from JoinAll\u2013about 0.", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "This reaffirms the importance of foreign key features; as such, it is known that dropping foreign key features could cause the bias to shoot up with linear classifiers [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "These results are surprising given the more conservative behavior predicted even for the linear classifiers in [26].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "A key benefit of avoiding KFK joins safely is that ML runtimes (including feature selection) could be significantly lowered for the linear classifiers [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "All experiments (except for ANN) were run on CloudLab, which offers free access to physical compute nodes for research [35].", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "Thus, these results corroborate the orders of magnitude speedup reported in [26] for Naive Bayes with backward selection.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Note that our simulation methodology is not tied to decision trees; it is generic enough to be applicable to classifier because we only use standard generic notions of error and net variance as defined in [26].", "startOffset": 205, "endOffset": 209}, {"referenceID": 24, "context": "These scenarios represent opposite extremes for how likely the (test) error is likely to shoot up when XR is discarded and FK is used as a representative [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "We generate 100 different training datasets and measure the average test error and average net variance (as defined in [9]) based on the different models obtained from these 100 runs.", "startOffset": 119, "endOffset": 122}, {"referenceID": 24, "context": "In contrast to these results, [26] reported that for linear models, the errors of NoJoin shot up compared to JoinAll (a gap of nearly 0.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "This is akin to the extra overfitting reported in [26] using the plots of the net variance.", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "For the linear model case, [26] reported that as the skew parameters increased, the gap widened.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Note that the absolute generalization error is often high, which is expected for decision trees [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 43, "context": "While this seems similar to how deep neural networks excel at sheer memorization but still offer good test accuracy [45], the models in our setting are not necessarily memorizing all features \u2013 only the foreign keys.", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "Since FK already encodes all information that XR provides [26], the tree almost always uses FK in its partitioning, often multiple times.", "startOffset": 58, "endOffset": 62}, {"referenceID": 37, "context": "From a data management perspective, there are database dependencies more general than FDs: embedded multi-valued dependencies (EMVDs) and join dependencies (JDs) [39].", "startOffset": 162, "endOffset": 166}, {"referenceID": 37, "context": "How does the presence of such database dependencies among features affect the behavior of ML models? There are also conditional FDs, which satisfy FD-like constraints among subsets of the dataset [39].", "startOffset": 196, "endOffset": 200}, {"referenceID": 5, "context": "In contrast to prior work on handling regular large-domain features [7], foreign key features are distinct in that they have coarsergrained side information available in the form of foreign features.", "startOffset": 68, "endOffset": 71}, {"referenceID": 39, "context": "A standard unsupervised method to construct f is the Random hashing trick [41], i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": ", Laplacian smoothing for Naive Bayes by adding a pseudocount of 1 to all frequency counts [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "While similar smoothing techniques have been studied for probability estimation using decision trees [32], to the best of our knowledge, this issue has not been handled in general for classification using decision trees.", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 35, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 32, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 22, "context": "The scenario of learning over joins of multiple tables without materializing the output of the join was studied in [25, 37, 34, 24], but their goal was primarily to reduce runtimes of some ML techniques without affecting accuracy.", "startOffset": 115, "endOffset": 131}, {"referenceID": 41, "context": "It was also studied in [43] but their focus was on devising a new ML algorithm.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "We first demonstrated the feasibility of avoiding joins safely in [26] for linear models.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "In this work, we revisit that idea for higher capacity models and find that they are counter-intuitively more robust than linear models to avoiding joins, not less as the VC dimension-based analysis in [26] suggested.", "startOffset": 202, "endOffset": 206}, {"referenceID": 0, "context": "Embedded multi-valued dependencies (EMVDs) are database dependencies that are more general than functional dependencies [2].", "startOffset": 120, "endOffset": 123}, {"referenceID": 28, "context": "The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 40, "context": "The implication of EMVDs for probabilistic conditional independence in Bayesian networks was originally described by [30] and further explored by [42].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "There is a large body of work on statistical relational learning (SRL) to handle joins that cause duplicates in the fact table [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 13, "context": "The data mining and ML communities have long worked on feature selection methods to improve ML accuracy [14, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 12, "context": "The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].", "startOffset": 71, "endOffset": 83}, {"referenceID": 42, "context": "The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].", "startOffset": 71, "endOffset": 83}, {"referenceID": 19, "context": "The trade-off between feature redundancy and relevancy is well-studied [14, 44, 21].", "startOffset": 71, "endOffset": 83}, {"referenceID": 12, "context": "The conventional wisdom is that even a feature that is redundant might be highly relevant and hence, unavoidable in the mix [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "[40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "[40] infers approximate FDs using the dataset instance and exploits them during feature selection, FOCUS [3] is an approach to bias the input and reduce the number of features by performing some computations over those features, while [6] proposes a measure called consistency to aid in feature subset search.", "startOffset": 235, "endOffset": 238}, {"referenceID": 5, "context": "Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17].", "startOffset": 120, "endOffset": 123}, {"referenceID": 15, "context": "Scores such as Gini and information gain are known to be biased towards large-domain features in decision tree learning [7] and different approaches have explored alternatives to solve that issue [17].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29].", "startOffset": 93, "endOffset": 101}, {"referenceID": 27, "context": "Unsupervised dimensionality reduction methods such as random hashing or PCA are also popular [15, 29].", "startOffset": 93, "endOffset": 101}, {"referenceID": 25, "context": "Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8].", "startOffset": 172, "endOffset": 179}, {"referenceID": 6, "context": "Integrating data and features from different sources for ML and data mining algorithms often requires applying and adapting techniques from the data integration literature [27, 8].", "startOffset": 172, "endOffset": 179}, {"referenceID": 16, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 8, "context": "These include integrating features from different data types in recommendation systems [18], sensor fusion [19], dimensionality reduction during feature fusion [12], and techniques to control data quality during data fusion [10].", "startOffset": 224, "endOffset": 228}], "year": 2017, "abstractText": "Machine learning (ML) over relational data is a booming area of the database industry and academia. While several projects aim to build scalable and fast ML systems, little work has addressed the pains of sourcing data and features for ML tasks. Real-world relational databases typically have many tables (often, dozens) and data scientists often struggle to even obtain and join all possible tables that provide features for ML. In this context, Kumar et al. showed recently that key-foreign key dependencies (KFKDs) between tables often lets us avoid such joins without significantly affecting prediction accuracy\u2013an idea they called \u201cavoiding joins safely.\u201d While initially controversial, this idea has since been used by multiple companies to reduce the burden of data sourcing for ML. But their work applied only to linear classifiers. In this work, we verify if their results hold for three popular high-capacity classifiers: decision trees, non-linear SVMs, and ANNs. We conduct an extensive experimental study using both real-world datasets and simulations to analyze the effects of avoiding KFK joins on such models. Our results show that these high-capacity classifiers are surprisingly and counter-intuitively more robust to avoiding KFK joins compared to linear classifiers, refuting an intuition from the prior work\u2019s analysis. We explain this behavior intuitively and identify open questions at the intersection of data management and ML theoretical research. All of our code and datasets are available for download from http://cseweb.ucsd.edu/~arunkk/hamlet.", "creator": "LaTeX with hyperref package"}}}