{"id": "1704.00648", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "abstract": "secondly in this work we truly present a new empirical approach \u2026 to learn compressible key representations in neural deep architectures with an engaging end - to - end training strategy. both our evolving method practice is based on a flexible soft task relaxation technique of initial quantization and entropy, which we anneal to recapture their normally discrete counterparts throughout training. visually we effectively showcase this method techniques for yielding two significantly challenging experimental applications : image video compression and raw neural network content compression. while these tasks have typically been approached with different delivery methods, our soft - to - sleep hard quantization approach gives meaningful state - based of - only the - art results for both.", "histories": [["v1", "Mon, 3 Apr 2017 15:39:56 GMT  (5688kb,D)", "http://arxiv.org/abs/1704.00648v1", "Supplementary visual examples available at:this http URL"], ["v2", "Thu, 8 Jun 2017 09:18:22 GMT  (1366kb,D)", "http://arxiv.org/abs/1704.00648v2", null]], "COMMENTS": "Supplementary visual examples available at:this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["eirikur agustsson", "fabian mentzer", "michael tschannen", "lukas cavigelli", "radu timofte", "luca benini", "luc van gool"], "accepted": true, "id": "1704.00648"}, "pdf": {"name": "1704.00648.pdf", "metadata": {"source": "META", "title": "Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and Neural Networks", "authors": ["Eirikur Agustsson", "Fabian Mentzer", "Michael Tschannen", "Lukas Cavigelli", "Radu Timofte", "Luca Benini", "Luc Van Gool"], "emails": ["<aeirikur@vision.ee.ethz.ch>."], "sections": [{"heading": "1. Introduction", "text": "In recent years, deep neural networks (DNNs) have led to many breakthrough results in machine learning and computer vision (Krizhevsky et al., 2012; Silver et al., 2016; Esteva et al., 2017), and are now widely deployed in industry. Modern DNN models often have millions or tens of millions of parameters, leading to highly redundant structures, both in the intermediate feature representations they generate and in the model itself. Although overparametrization of DNN models can have a favorable effect on training, for inference it is often desirable to compress DNN models in practice, e.g., when deploying them on mobile or embedded devices with limited memory. The ability to learn compressible feature representations, on the other hand, has a large potential for the development of (data-adaptive) compression algorithms for various data types such as images, audio, video, and text, for all of which various DNN architectures are now available.\nDNN model compression and lossy image compression using DNNs have both independently attracted a lot of attention lately. In order to compress a set of continuous model parameters or features, we need to approximate each pa-\n1ETH Zurich, Switzerland 2KU Leuven, Belgium. Correspondence to: Eirikur Agustsson <aeirikur@vision.ee.ethz.ch>.\nrameter or feature by one representative from a set of quantization levels (or vectors, in the multi-dimensional case), each associated with a symbol, and then store the assignments (symbols) of the parameters or features, as well as the quantization levels. Representing each parameter of a DNN model or each feature in a feature representation by the corresponding quantization level will come at the cost of a distortion D, i.e., a loss in performance (e.g., in classification accuracy for a classification DNN with quantized model parameters, or in reconstruction error in the context of autoencoders with quantized intermediate feature representations). The rate R, i.e., the entropy of the symbol stream, determines the cost of encoding the model or features in a bitstream.\nTo learn a compressible DNN model or feature representation we need to minimize D + \u03b2R, where \u03b2 > 0 controls the rate-distortion trade-off. Including the entropy into the learning cost function can be seen as adding a regularizer that promotes a compressible representation of the network or feature representation. However, two major challenges arise when minimizing D + \u03b2R for DNNs: i) coping with the non-differentiablity (due to quantization operations) of the cost function D + \u03b2R, and ii) obtaining an accurate and differentiable estimate of the entropy (i.e., R). To tackle i), various methods have been proposed. Among the most popular ones are stochastic approximations (Williams, 1992; Krizhevsky & Hinton, 2011; Courbariaux et al., 2015; Toderici et al., 2015; Balle\u0301 et al., 2016b) and rounding with a smooth derivative approximation (Hubara et al., 2016; Theis et al., 2017). To address ii) a common approach is to assume the symbol stream to be i.i.d. and to model the marginal symbol distribution with a parametric model, such as a Gaussian mixture model (Theis et al., 2017; Ullrich et al., 2017), a piecewise linear model (Balle\u0301 et al., 2016b), or a Bernoulli distribution (Toderici et al., 2016) (in the case of binary symbols).\nIn this paper, we propose a unified end-to-end learning framework for compression, jointly optimizing the model parameters, the quantization levels, and the entropy of the resulting symbol stream to compress either a subset of feature representations in the network or the model itself. We\nar X\niv :1\n70 4.\n00 64\n8v 1\n[ cs\n.L G\n] 3\nA pr\n2 01\n7\naddress both challenges i) and ii) above with methods that are novel in the context DNN model and feature compression. Our main contributions are:\n\u2022 We provide the first unified view end-to-end learned compression of feature representations and DNN models. To the best of our knowledge, these two problems have been studied largely independently in the literature so far, despite many similarities.\n\u2022 Our method is simple and intuitively appealing, relying on soft assignments of a given scalar or vector to be quantized to each quantization level. A parameter controls the \u201chardness\u201d of the assignment and allows to gradually transition from soft to hard assignments during training. In contrast to roundingbased or stochastic quantization schemes, our coding scheme is directly differentiable, thus trainable endto-end. Furthermore, our method does not force the network to adapt to specific (given) quantization outputs (e.g., integers) but learns the quantization levels jointly with the weights, enabling application of our method to a wider set of problems.\n\u2022 Unlike essentially all previous works, we make no assumption on the marginal distribution of the features or model parameters to be quantized by relying on a histogram of the assignment probabilities rather than the parametric models commonly used in literature.\n\u2022 We apply our method to DNN model compression for a 32-layer ResNet model (He et al., 2016) and fullresolution image compression using a variant of the compressive autoencoder proposed recently in (Theis et al., 2017) (see Figure 1 for an overview). In both cases, we obtain state-of-the-art performance, while making fewer model assumptions and significantly simplifying the training procedure compared to the original works (Theis et al., 2017; Choi et al., 2016).\nThe remainder of the paper is organized as follows. Section 2 reviews related work, before our soft-to-hard vector quantization method is introduced in Section 3. Then we apply it to a compressive autoencoder for image compression and to ResNet for DNN compression in Section 4 and 5, respectively. Section 6 concludes the paper."}, {"heading": "2. Related Work", "text": "There has been a surge of interest in DNN models for full-resolution image compression, most notably (Toderici et al., 2015; 2016; Balle\u0301 et al., 2016a;b; Theis et al., 2017), all of which outperform JPEG and some even JPEG2000. The pioneering work of Toderici et al. (2015; 2016) showed that progressive image compression can be learned with convolutional recurrent neural networks (RNNs), employing a stochastic quantization method during training. Balle\u0301\net al. (2016a); Theis et al. (2017) both rely on convolutional autoencoder architectures. Balle\u0301 et al. (2016a) model the quantization error by uniform additive noise and incorporate generalized divisive normalization (GDN) into their architecture to reduce higher-order statistical dependencies among the features to be compressed. Their optimization framework has a close relation to variational autoencoders (Kingma & Welling, 2013). In contrast, Theis et al. (2017) quantize the representation in the bottleneck of the autoencoder to integer values, employing the identity mapping for gradient backpropagation.\nIn the context of DNN model compression, the line of works (Han et al., 2015b;a; Choi et al., 2016) adopts a multi-step procedure in which the weights of a pretrained DNN are first pruned and the remaining parameters are quantized using a k-means like algorithm, the DNN is then retrained, and finally the quantized DNN model is encoded using arithmetic coding. Pruning, although computationally essentially free, can cause considerable accuracy loss. To overcome this issue, (Guo et al., 2016) propose a dynamic network surgery technique that allows to reactivate pruned weights. A notable different approach is taken by (Ullrich et al., 2017), where the DNN compression task is tackled using the minimum description length principle, which has a solid information-theoretic foundation.\nIt is worth noting that many recent works target quantization of the DNN model parameters and possibly the feature representation to speed up DNN evaluation on hardware with low-precision arithmetic, see, e.g., (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017). However, most of these works do not specifically train the DNN such that the quantized parameters are compressible in an information-theoretic sense.\nGradually moving from an easy (convex or differentiable) problem to the actual harder problem during optimization,\nas done in our soft-to-hard quantization framework, has been studied in various contexts and falls under the umbrella of continuation methods (see (Allgower & Georg, 2012) for an overview). Formally related but motivated from a probabilistic perspective are deterministic annealing methods for maximum entropy clustering/vector quantization, see, e.g., (Rose et al., 1992; Yair et al., 1992). In the context of neural networks, Magnussen et al. (1994) optimize the parameters of a discrete-time cellular neural network by gradually moving from a sigmoid to a sign function. Arguably most related to our approach is (Wohlhart et al., 2013), which also employs continuation for nearest neighbor assignments, but in the context of learning a supervised prototype classifier. To the best of our knowledge, continuation methods have not been employed before in an end-to-end learning framework for neural network-based image compression or DNN compression."}, {"heading": "3. Proposed Soft-to-Hard Vector Quantization", "text": ""}, {"heading": "3.1. Problem Formulation", "text": "Preliminaries and Notations. We consider the standard model for DNNs, where we have an architecture F : Rd1 7\u2192 RdK+1 composed of K layers:\nF = FK \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 F1,\nwhere layer Fi maps Rdi \u2192 Rdi+1 , and has parameters wi \u2208 Rmi . We refer to W = [w1, \u00b7 \u00b7 \u00b7 ,wK ] as the parameters of the network and we denote the intermediate layer outputs of the network as\nx(0) := x and x(i) := Fi(x(i\u22121)),\nsuch that F (x) = x(K) and x(i) is the feature vector produced by layer Fi.\nThe parameters of the network are learned w.r.t. training data X = {x1, \u00b7 \u00b7 \u00b7 ,xN} \u2282 Rd1 and labels Y = {y1, \u00b7 \u00b7 \u00b7 ,yN} \u2282 RdK+1 , by minimizing a real-valued loss L(X ,Y;F ). Typically, the loss can be decomposed as a sum over the training data plus a regularization term,\nL(X ,Y;F ) = 1 N N\u2211 i=1 `(F (xi),yi) + \u03bbR(W), (1)\nwhere `(F (x),y, ) is the sample loss, \u03bb > 0 sets the regularization strength, and R(W) is a regularizer (e.g., R(W) = \u2211 i \u2016wi\u20162 for l2 regularization). In this case, the parameters of the network can be learned using stochastic gradient descent over mini-batches. Assuming that the data X ,Y on which the network is trained is drawn from some distribution PX,Y, the loss (1) can be thought of as an estimator of the expected loss E[`(F (X),Y) + \u03bbR(W)]. In the context of image classification, Rd1 would correspond\nto the input image space and RdK+1 to the classification probabilities, and ` would be the categorical cross entropy.\nWe say that the deep architecture is an autoencoder when the network maps back into the input space, with the goal of reproducing the input. In this case, d1 = dK+1 and F (x) is trained to approximate x, for example with a mean squared error loss `(F (x),y) = \u2016F (x) \u2212 y\u20162. Autoencoders typically condense the dimensionality of the input into some smaller dimensionality inside the network, i.e., the layer with the smallest output dimension, x(b) \u2208 Rdb , has db d1, which we refer to as the \u201cbottleneck\u201d. Compressible representations. We say that a weight parameter wi or a feature x(i) has a compressible representation if it can be serialized to a binary stream using few bits. For DNN compression, we want the entire network parameters W to be compressible. For image compression via an autoencoder, we just need the features in the bottleneck, x(b), to be compressible.\nSuppose we want to compress a feature representation z \u2208 Rd in our network (e.g., x(b) of an autoencoder) given an input x. Assuming that the data X ,Y is drawn from some distribution PX,Y, z will be a sample from a continuous random variable Z.\nTo store z with a finite number of bits, we need to map it to a discrete space. Specifically, we map z to a sequence of m symbols using a (symbol) encoder E : Rd 7\u2192 [L]m, where each symbol is an index ranging from 1 to L, i.e., [L] := {1, . . . , L}. The reconstruction of z is then produced by a (symbol) decoder D : [L]m 7\u2192 Rd, which maps the symbols back to z\u0302 = D(E(z)) \u2208 Rd. Since z is a sample from Z, the symbol stream E(z) is drawn from the the discrete probability distribution of PE(Z). Thus, given the encoder E, according to Shannon\u2019s source coding theorem (Cover & Thomas, 2012), the correct metric for compressibility is the entropy of E(Z):\nH(E(Z)) = \u2212 \u2211\ne\u2208[L]m P (E(Z) = e) log(P (E(Z) = e)).\n(2) Our generic goal is hence to optimize the rate distortion trade-off between the expected loss and the entropy of E(Z):\nmin E,D,W\nEX,Y[`(F\u0302 (X),Y) + \u03bbR(W)] + \u03b2H(E(Z)), (3)\nwhere F\u0302 is the architecture where z has been replaced with z\u0302, and \u03b2 > 0 controls the trade-off between compressibility of z and the distortion it imposes on F\u0302 .\nHowever, we cannot optimize (3) directly. First, we do not know the distribution of X and Y. Second, the distribution of Z depends in a complex manner on the network parameters W and the distribution of X. Third, the encoder E is\na discrete mapping that is not differentiable. For our first approximation we consider the sample entropy instead of H(E(Z)). That is, given the data X and some fixed network parameters W, we can estimate the the probabilities P (E(Z) = e) for e \u2208 [L]m via a histogram. For this estimate to be accurate, we however would need |X | Lm. If z is the bottleneck of an autoencoder, this would correspond to trying to learn a single histogram for the entire discretized data space. We relax this by assuming the entries of E(Z) are i.i.d. such that we can instead compute the histogram over the L distinct values. More precisely, we assume that for e = (e1, \u00b7 \u00b7 \u00b7 , em) \u2208 [L]m we can approximate\nP (E(Z) = e) \u2248 m\u220f l=1 pel ,\nwhere pj is the histogram estimate\npj := |{el(zi)|l \u2208 [m], i \u2208 [N ], el(zi) = j}|\nmN , (4)\nwhere we denote the entries ofE(z) = (e1(z), \u00b7 \u00b7 \u00b7 , em(z)) and zi is the output feature z for training data point xi \u2208 X . We then obtain an estimate of the entropy of Z by substituting the approximation (3.1) into (2),\nH(E(Z)) \u2248 \u2212 \u2211\ne\u2208[L]m ( m\u220f l=1 pel ) log ( m\u220f l=1 pel ) (5)\n= \u2212m L\u2211 j=1 pj log pj = mH(p), (6)\nwhere step from (5) to (6) is due to (Cover & Thomas, 2012), Thm. 2.6.6, and H(p) := \u2212\u2211Lj=1 pj log pj is the sample entropy for the (i.i.d., by assumption) components of E(Z) 1.\nWe now can simplify the ideal objective of (3), by replacing the expected loss with the sample mean over ` and the entropy using the sample entropy H(p), obtaining\n1\nN N\u2211 i=1 `(F (xi),yi) + \u03bbR(W) + \u03b2mH(p). (7)\nWe note that so far we have assumed that z is a feature output in F , i.e., z = x(k) for some k \u2208 [K]. However, the above treatment would stay the same if z is the concatenation of multiple feature outputs. One can also obtain a separate sample entropy term for separate feature outputs and add them to the objective in (7).\nIn case z is composed of one or more parameter vectors, such as in DNN compression where z = W, z and z\u0302\n1In fact, from (Cover & Thomas, 2012), Thm. 2.6.6, it follows that if the histogram estimates pj are exact, (6) is an upper bound for the true H(E(Z)) (i.e., without i.i.d. assumption).\ncease to be random variables, since W is a parameter of the model. That is, opposed the case where we have a sourceX that produces another source Z\u0302 which we want to be compressible, we want the discretization of a single parameter vector W to be compressible. This is analogous to compressing a single document, instead of learning a model that can compress a stream of documents. In this case, (3) is not the appropriate objective, but our simplified objective in (7) remains appropriate. This is because a standard technique in compression is to build a statistical model of the (finite) data, which has a small sample entropy. The only difference is that now the histogram probabilities in (4) are taken over W instead of the dataset X , i.e., N = 1 and zi = W in (4), and they count towards storage as well as the encoder E and decoder D.\nChallenges. Equation (7) gives us a unified objective that can well describe the trade-off between compressible representations in a deep architecture and the original training objective of the architecture.\nHowever, the problem of finding a good encoderE, a corresponding decoder D, and parameters W that minimize the objective remains. First, we need to impose a form for the encoder and decoder, and second we need an approach that can optimize (7) w.r.t. the parameters W. Independently of the choice of E, (7) is challenging since E is a mapping to a finite set and, therefore, not differentiable. This implies that neither H(p) is differentiable nor F\u0302 is differentiable w.r.t. the parameters of z and layers that feed into z. For example, if F\u0302 is an autoencoder and z = x(b), the output of the network will not be differentiable w.r.t. w1, \u00b7 \u00b7 \u00b7 ,wb and x(0), \u00b7 \u00b7 \u00b7 ,x(b\u22121). These challenges motivate the design decisions of our softto-hard annealing approach, described in the next section."}, {"heading": "3.2. Our Method", "text": "Encoder and decoder form. For the encoder E : Rd 7\u2192 [L]m we assume that we have L centers vectors C = {c1, \u00b7 \u00b7 \u00b7 , cL} \u2282 Rd/m. The encoding of z \u2208 Rd is then performed by reshaping it into a matrix Z = [z\u0304(1), \u00b7 \u00b7 \u00b7 , z\u0304(m)] of size R(d/m)\u00d7m, and assigning each column z\u0304(l) to the index of its nearest neighbor in C. That is, we assume the feature z \u2208 Rd can be modeled as a sequence of m points in Rd/m, which we partition into the Voronoi tessellation over the centers C. The decoder D : [L]m 7\u2192 Rd then simply constructs Z\u0302 \u2208 R(d/m)\u00d7m from a symbol sequence (e1, \u00b7 \u00b7 \u00b7 , em) by picking the corresponding centers:\nZ\u0302 = [ce1 , \u00b7 \u00b7 \u00b7 , cem ],\nfrom which z\u0302 is formed by reshaping Z\u0302 back into Rd. We will interchangeably write z\u0302 = D(E(z)) and Z\u0302 = D(E(Z)).\nThe idea is then to relaxE andD into continuous mappings via soft assignments instead of the hard nearest neighbor assignment of E.\nSoft assignments. We define the soft assignment of z\u0304 \u2208 Rd/m to C as\n\u03c6(z\u0304) := softmax(\u2212\u03c3 \u2016z\u0304\u2212 c1\u2016 2\n... \u2016z\u0304\u2212 cL\u20162 ) \u2208 RL, (8) where softmax(y1, \u00b7 \u00b7 \u00b7 , yL)j := e yj\ney1+\u00b7\u00b7\u00b7+eyL is the standard softmax operator, such that \u03c6(z\u0304) has positive entries and \u2016\u03c6(z\u0304)\u20161 = 1. We denote the j-th entry of \u03c6(z\u0304) with \u03c6j(z\u0304) and note that\nlim \u03c3\u2192\u221e \u03c6j(z\u0304) = { 1 if j = arg minj\u2032\u2208[L]\u2016z\u0304\u2212 cj\u2032\u2016 0 otherwise\nsuch that \u03c6\u0302(z\u0304) := lim\u03c3\u2192\u221e \u03c6(z\u0304) converges to a one-hot encoding of the nearest center to z\u0304 in C. We therefore refer to \u03c6\u0302(z\u0304) as the hard assignment of z\u0304 to C and the parameter \u03c3 > 0 as the hardness of the soft assignment \u03c6(z\u0304).\nUsing soft assignment, we define the soft quantization of z\u0304 as\nQ\u0303(z\u0304) := L\u2211 j=1 cj\u03c6i(z\u0304) = C\u03c6(z\u0304),\nwhere we write the centers as a matrix C = [c1, \u00b7 \u00b7 \u00b7 , cL] \u2208 Rd/m\u00d7L. The corresponding hard assignment is taken with Q\u0302(z\u0304) := lim\u03c3\u2192\u221e Q\u0303(z\u0304) = ce(z\u0304), where e(z\u0304) is the center in C nearest to z\u0304. Therefore, we can now write:\nZ\u0302 = D(E(Z)) = [Q\u0302(z\u0304(1)), \u00b7 \u00b7 \u00b7 , Q\u0302(z\u0304(m))] = C[\u03c6\u0302(z\u0304(1)), \u00b7 \u00b7 \u00b7 , \u03c6\u0302(z\u0304(m))].\nNow, instead of computing Z\u0302 via hard nearest neighbor assignments, we can approximate it with a smooth relaxation\nZ\u0303 := C[\u03c6(z\u0304(1)), \u00b7 \u00b7 \u00b7 , \u03c6(z\u0304(m))]\nby using the soft assignments instead of the hard assignments. Denoting the corresponding vector form with z\u0303, this gives us a differentiable approximation F\u0303 of the quantized architecture F\u0302 , by replacing z\u0302 in the network with z\u0303.\nEntropy estimation. Using the soft assignments, we can similarly define a soft histogram, by summing up the partial assignments to each center instead of counting as in (4):\nqj := 1\nmN N\u2211 i=1 m\u2211 l=1 \u03c6j(z\u0304 (l) i ).\nThis gives us a valid probability mass q = (q1, \u00b7 \u00b7 \u00b7 , qL), which is differentiable but converges to p = (p1, \u00b7 \u00b7 \u00b7 , pL) as \u03c3 \u2192\u221e.\nWe can now define the \u201csoft entropy\u201d as the cross entropy between p and q:\nH\u0303(\u03c6) := H(p, q) = \u2212 L\u2211 j=1 pj log qj .\nUsing the Kullback\u2013Leibler divergence DKL(p||q) =\u2211 j pj log(pj/qj), we can write\nH(p, q) = H(p) +DKL(p||q).\nSince DKL(p||q) \u2265 0, this establishes H\u0303(\u03c6) as an upper bound for H(p), where equality is obtained when p = q.\nWe have therefore obtained a differentiable \u201csoft entropy\u201d loss (w.r.t.q), which is an upper bound on the sample entropy H(p). Hence, we can indirectly minimize H(p) by minimizing H\u0303(\u03c6). However, we note that while qj is additive over the training data and the symbol sequence, log(qj) is not. This prevents the use of mini-batch gradient descent on H\u0303(\u03c6), which can be an issue for large scale learning problems. In this case, we can instead re-define the soft entropy H\u0303(\u03c6) as H(q, p). As before, H\u0303(\u03c6) \u2192 H(p) as \u03c3 \u2192 \u221e, but H\u0303(\u03c6) ceases to be an upper bound for H(p). The benefit is that now H\u0303(\u03c6) can be decomposed as\nH\u0303(\u03c6) := H(q, p) = \u2212 L\u2211 j=1 qj log pj\n= \u2212 L\u2211 j=1 1 mN N\u2211 i=1 m\u2211 l=1 \u03c6j(z\u0304 (l) i ) log pj = \u2212 N\u2211 i=1  1 mN m\u2211 l=1 L\u2211 j=1 \u03c6j(z\u0304 (l) i ) log pj\n , such that we get an additive loss over the samples xi \u2208 X and the components l \u2208 [m]. Soft-to-hard deterministic annealing. Our soft assignment scheme gives us a differentiable approximation F\u0303 of the discretized network F\u0302 , and H\u0303(\u03c6) of the sample entropy H(p). However, our objective is to learn network parameters W that minimize (7) when using the encoder and decoder with hard assignments, such that we obtain a compressible symbol stream E(z) which we can compress using, e.g., arithmetic coding (Witten et al., 1987).\nIf we simply pick some value for the hardness parameter \u03c3 and optimize the objective (7) by substituting the soft approximation, the network could learn to invert the soft assignment during the optimization. To prevent this, we anneal \u03c3 from some initial value \u03c30 to infinity during training, such that the soft approximation gradually becomes a better approximation of the final hard quantization we will use. For a simple initialization of \u03c30 and the centers C, we\ncan sample the centers from the set Z := {z\u0304(l)i |i \u2208 [N ], l \u2208 [m]} and then cluster Z by minimizing the cluster energy\u2211\nz\u0304\u2208Z \u2016z\u0304\u2212 Q\u0303(z\u0304)\u20162 using SGD. When we anneal \u03c3, a practical issue could occur where we get very large gradients, since Q\u0303 converges to a discontinuous function. Therefore, we do not actually anneal \u03c3 to\u221e but switch to actual hard assignments when \u03c3 \u03c30. 2"}, {"heading": "4. Image Compression", "text": "We will now show how we can use our framework to realize an image compression system. In terms of network architecture, we rely on a variant of the compressive autoencoder proposed recently in (Theis et al., 2017), using convolutional neural networks for the image encoder and image decoder3. The first two convolutional layers in the image encoder each downsample the input image by a factor 2 and collectively increase the number of channels from 3 to 128. This is followed by three residual blocks, each with 128 filters. Another convolutional layer then downsamples again by a factor 2 and decreases the number of channels to c, where c is a hyperparameter4. For a w \u00d7 hdimensional input image, the output of the image encoder is the w/8\u00d7 h/8\u00d7 c-dimensional \u201cbottleneck tensor\u201d. The image decoder then mirrors the image encoder, using upsampling instead of downsampling, and deconvolutions instead of convolutions, mapping the bottleneck tensor into a w\u00d7 h-dimensional output image. In contrast to the \u201csubpixel\u201d layers (Shi et al., 2016a;b) used in (Theis et al., 2017), we use standard deconvolutions for simplicity.\nWe note that while we use the architecture of (Theis et al., 2017), we train it using our soft-to-hard entropy minimization method, which differs significantly from the approach in (Theis et al., 2017).\nBecause we do not expect the features from different channels to be identically distributed, we model each channel\u2019s distribution with a different histogram and entropy loss, adding each entropy term to the total loss using the same \u03b2 parameter. Each channel in the bottleneck tensor is a w/8 \u00d7 h/8-dimensional matrix, which corresponds to a vector z \u2208 Rd, d = w/8 \u00b7 h/8 in our framework in Section 3. Our goal is to learn a compressible representation for z.\nTo encode a channel into symbols, we separate the channel matrix into a sequence of pw \u00d7 ph-dimensional patches.\n2One could also employ simple gradient clipping. 3We note that the image encoder (decoder) refers to the left (right) part of the autoencoder, which encodes (decodes) the data to (from) the bottleneck. This is not to be confused with the symbol encoder (decoder) in Section 3, which maps the real valued bottleneck to (from) symbols.\n4(Theis et al., 2017) use 64 and 96 channels.\nThese patches (vectorized) form the columns of Z \u2208 Rd/m\u00d7m, where m = d/(pwph), such that Z contains m (pwph)-dimensional points. Having ph or pw greater than one allows symbols to capture local correlations in the bottleneck, which is desirable since we model the symbols as i.i.d. random variables for entropy coding. At test time, the symbol encoder E then determines the symbols in the channel by performing a nearest neighbor assignment over a set of L centers C \u2282 Rpwph , resulting in Z\u0302, as described above. During training we obtain the soft quantized Z\u0303, also w.r.t. the centers C. From the Z\u0303 we get the channel back by rearranging the patches (columns of Z\u0303) into the channel tensor. The symbol decoder thus obtains a (soft) quantized version of the w/8\u00d7 h/8\u00d7 c-dimensional bottleneck. All models were trained using Adam (Kingma & Ba, 2014). Our training set is composed similarly to that described in (Balle\u0301 et al., 2016a). We used a subset of 90,000 images from ImageNet (Deng et al., 2009), which we downsampled by a factor 0.7 and trained on crops of 128 \u00d7 128 pixels, with a batch size of 15.\nThe training of our autoencoder network takes place in two stages, where we move from an identity function in the bottleneck to hard quantization. In the first stage, we train the autoencoder without the quantization layer by directly feeding the output of the image encoder to the image decoder. Similar to (Theis et al., 2017) we gradually unfreeze the channels in the bottleneck during training.5 This yields an efficient weight initialization and enables us to then initialize \u03c30 and C as described above. In the second stage, we minimize (7), jointly learning network weights and quantization levels.\nWe anneal \u03c3 by letting the gap between soft and hard quantization error go to zero as the number of iterations t goes to infinity. Let eS = \u2016F\u0303 (x) \u2212 x\u20162 be the soft error, eH = \u2016F\u0302 (x) \u2212 x\u20162 be the hard error. With gap(t) = eH \u2212 eS we can denote the error between the actual the desired gap with eG(t) = gap(t)\u2212T/(T + t) gap(0), such that the gap is halved after T iterations. We update sigma according to \u03c3(t+1) = \u03c3(t)+KG eG(t). Fig. 2 shows the evolution of the gap, soft and hard loss as sigma grows during training.\nTo estimate the probability distribution of the symbols, we maintain a histogram over 5,000 images, which we update every 10 iterations with the images from the current batch.\nWe trained different combinations of \u03b2 and c to explore different rate-distortion tradeoffs (measuring distortion in MSE). As \u03b2 controls to which extent the network minimizes entropy, \u03b2 directly controls bpp (see top left plot in Fig. 2). We evaluated all pairs (c, \u03b2) with c \u2208 {8, 16, 32, 48} and \u03b2 \u2208 {1e\u22124, . . . , 9e\u22124}, and selected\n5We observed that this gives a slight improvement over learning all channels jointly from the start.\n5 representative pairs (models) with average bpps roughly corresponding to uniformly spread points in the interval [0.1, 0.8] bpp. This defines a \u201cquality index\u201d for our model family, analogous to the JPEG quality factor.\nWe experimented with the other training parameters on a setup with c = 32, which we chose as follows. In the first stage we train for 250k iterations using a learning rate of 1e\u22124. In the second stage, we use an annealing schedule with T = 50k,KG = 100, over 600k iterations using a learning rate of 1e\u22125. In both stages, we use a weak l2 regularizer over all learnable parameters, with \u03bb = 1e\u221212."}, {"heading": "4.1. Experimental setup", "text": "To evaluate the image compression performance of our Soft-to-Hard Autoencoder (SHA) method we use four datasets (Kodak, B100, Urban100, ImageNet100) and three standard quality measures (PSNR, SSIM, MS-SSIM).\nKodak 6 is the most frequently employed dataset for analizing image compression performance in recent years. It contains 24 photographic quality color 768 \u00d7 512 images covering a variety of subjects, locations and lighting conditions. B100 (Timofte et al., 2015) is a set of 100 content diverse color 481\u00d7321 test images from the Berkeley Segmentation Dataset (Martin et al., 2001). Urban100 (Huang et al., 2015) has 100 color images selected from Flickr with labels such as urban, city, architecture, and structure. The images are a couple of times larger than those from B100 or Kodak. Both B100 and Urban100 are commonly used to evaluate image super-resolution methods. ImageNet100 contains 100 images randomly selected by us from ImageNet (Russakovsky et al., 2015), also downsampled and\n6http://r0k.us/graphics/kodak/\ncropped, see above.\nPSNR (Peak Signal-to-Noise Ratio) is a standard measure in direct monotonous relation with the mean square error (MSE) computed between two signals. SSIM and MSSSIM are the structural similarity index (Wang et al., 2004) and its multi-scale SSIM computed variant (Wang et al., 2003) proposed to measure the similarity of two images. They correlate better with human perception than PSNR."}, {"heading": "4.2. Results", "text": "To validate our SHA, we compare its performance with the standard JPEG and JPEG2000. For this we compute quantitative similarity scores between each compressed image and the corresponding uncompressed image and average them over whole datasets of images. In Figure 3 we report plots for the average PSNR, SSIM and MS-SSIM scores obtained for each of the compression methods as a function of the bits per pixel (bpp) achieved compression rate (individual PSNR, SSIM, and MS-SSIM curves for each image of the Kodak data set are given in supplementary material). In Figure 4 we show individual compression results for an exemplary image from each testing set (see online supplementary material for more visual examples7).\nFor JPEG and JPEG2000 we used libjpeg8 and the Kakadu implementation9, respectively, subtracting in both cases the size of the header from the file size to compute the compression rate. We parameterized JPEG2000 by the bpp setting, and JPEG with its qualitiy factor.\nAs shown in Fig. 3 for all the datasets, in PSNR terms, our SHA is comparable with JPEG2000 for very high compression rates (< 0.3 bpp) and better than JPEG for the whole reported compression rate interval (< 0.7 bpp). However, when we look at the SSIM results we see that our SHA is almost always better or at least comparable with JPEG2000, and significantly better than JPEG. The MS-SSIM results follow the same trend as for SSIM. SHA-compressed images have fewer artifacts than the JPEG2000 ones (see Figs. 4) which is captured by the structural similarity measures (SSIM, MS-SSIM).\nSHA performs best on ImageNet100 and is most challenged on Kodak when compared with JPEG2000. On ImageNet our SHA significantly outperforms JPEG2000 in terms of both SSIM and MS-SSIM. This is of no surprise as our SHA learned from ImageNet images. The relative poorer performance of SHA on Kodak suggests a mismatch between the image content distributions from Kodak and ImageNet, but also between Kodak and both Urban100 and\n7http://www.vision.ee.ethz.ch/\u02dcaeirikur/ compression/visuals.pdf\n8http://libjpeg.sourceforge.net/ 9http://kakadusoftware.com/\nB100 datasets. The content variety and the number of images are also larger in Urban100, B100, and ImageNet100 than in the Kodak dataset.\nWe conclude that performance on the Kodak dataset may not be a strong indicator for compression performance on images in the wild. Furthermore we observe that even though our SHA model is affected by distribution mismatch between its training material and the test dataset, we still obtain good performance on datasets that the SHA was not trained on."}, {"heading": "4.3. Detailed Discussion of with Related Methods", "text": "We note that recent works of (Theis et al., 2017; Balle\u0301 et al., 2016b) also showed competitive performance with JPEG2000.\nWhile we use the architecture of (Theis et al., 2017), there are stark differences between the works: For quantization, we perform vector quantization while they perform rounding to integers. For differentiability, we use our soft relaxation of the quantization, while Theis et al. re-define the gradients in the backward pass as the gradients of an identity mapping. For entropy minimization, they use a Gaussian scale mixture model which encourages sparsity of coefficients, whereas we directly minimize a soft relaxation of the sample entropy. Thus our histograms learned end-to-end during training can be directly incorporated for arithmetic coding, whereas Theis et al. separately estimate Laplacian smoothed histograms for coding, after training. As training material, we restrict ourselves to a subset of the ImageNet database (Deng et al., 2009), while Theis et al collect a set of high quality photos from Flickr. For evaluating, Theis et al. use an ensemble of models for encoding, encoding the best model with an index in the header, whereas we just use a single model for each operating point.\nThe works of (Balle\u0301 et al., 2016b) build a deep model using multiple generalized divisive normalization (GDN) layers and their inverses (IGDN), which are specialized layers designed to capture local joint statistics of natural images. The quantization of Balle et al. is also based on rounding at test time, while during training they perturb the bottleneck representation with additive noise to model the quantization error. For entropy minimization, Balle et al. model marginals using linear splines which are re-adapted regularly during training. Instead of standard arithmetic coding, they use a more powerful context-adaptive binary arithmetic coding (CABAC) (Marpe et al., 2003) for the lossless coding of the symbols. We believe that our framework would also benefit from better domain-specific layers such as DGN/IDGN and stronger lossless coding techniques such as CABAC.\nSince neither raw bpp/PSNR/SSIM numbers nor codes are available and training procedures and architectures differ significantly, a direct comparison with (Theis et al., 2017; Balle\u0301 et al., 2016b) is hard.\nConcurrent to our work, the method of (Johnston et al., 2017) builds on the architecture proposed in (Toderici et al., 2016), and shows that impressive performance on the perceptual MS-SSIM metric (Wang et al., 2003) can be obtained by incorporating it into the optimization (and using sophisticated coding techniques). In contrast, our work adheres to the setting of (Theis et al., 2017; Balle\u0301 et al., 2016b; Toderici et al., 2016; 2015), simply optimizing for mean squared error, while evaluating using PSNR, SSIM and MS-SSIM. We believe the works are complementary and that our method could also improve on MS-SSIM using techniques from (Johnston et al., 2017).\nSHA (ours) JPEG2000 JPEG\nhttp://www.vision.ee.ethz.ch/\u02dcaeirikur/compression/visuals.pdf"}, {"heading": "5. DNN Compression", "text": "For DNN compression, we investigate the ResNet (He et al., 2016) architecture for image classification. We adopt the same setting as (Choi et al., 2016) and consider a 32-layer architecture trained for the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). As in (Choi et al., 2016), our goal is to learn a compressible representation for all 464,154 trainable parameters of the model.\nWe concatenate the parameters into a vector W \u2208 R464,154 and set z = W. Our goal is then to minimize the objective in (7). For a straightforward comparison with (Choi et al., 2016), we constrain ourselves to scalar quantization. That is, we set m = d such that E(z) is a symbol vector of length d with indexes taking value in [L].\nWe started from the pre-trained original model, which obtains a 92.6% accuracy on the test set. We implemented the entropy minimization by using L = 75 centers and chose \u03b2 = 0.1 such that the converged entropy would give a compression factor \u2248 20, i.e., such that the converged entropy is \u2248 32/20 = 1.6 bits per weight. The training was performed with the same learning parameters as the original model was trained on (SGD with momentum 0.9), using the final learning rate of the pre-trained model. The annealing schedule used was a simple exponential one, \u03c3(t+1) = 1.001 \u00b7\u03c3(t) with \u03c3(0) = 0.4, where \u03c3(t) denotes \u03c3 at iteration t. After 4 epochs of training, when \u03c3(t) has increased by a factor \u2248 20, we switched to hard assignments and continued fine-tuning at a 10\u00d7 lower learning rate.\nWe used both Huffman and arithmetic coding to compress the quantized symbols. Adhering to the benchmark of (Choi et al., 2016; Han et al., 2015b;a), we obtain the compression factor by dividing the bit cost of storing the uncompressed weights as floats (464, 154\u00d7 32) with the total encoding cost of compressed weights (i.e., L \u00d7 32 bits for the centers plus the size of the compressed index stream).\nOur compressible model achieves a comparable test accuracy of 92.1% while compressing the DNN by a factor\n19.15 with Huffman and 20.15 using arithmetic coding. Table 1 compares our results with state-of-the-art approaches reported by (Choi et al., 2016). We note that while the top methods also achieve accuracies above 92% and compression factors above 20\u00d7, they employ a considerable amount of hand-designed steps, such as pruning, retraining, various types of weight clustering, special encoding of the sparse weight matrices into an index-difference based format and then finally use Huffman coding. In contrast, we directly minimize the entropy of the weights in the training, obtaining a highly compressible representation using standard compression techniques such as Huffman or arithmetic coding.\nIn Figure 5 we show how the sample entropy H(p) decays during training, due to the entropy loss term in (7), and corresponding index histograms at three time instants. Initially the weights are well spread, giving a high entropy of H(p) = 4.07. As the entropy and classification accuracy are jointly optimized, the network learns to condense most of the weights to a couple of centers. This happens automatically in the learning process, as opposed to the methods of (Han et al., 2015b;a; Choi et al., 2016), which manually impose 0 as the most frequent weight by pruning the network.\nWe note that the recent works by (Ullrich et al., 2017), where the compression task is approached using the minimum description length principle, also manages to tackle the problem in a single training procedure. In contrast to our framework, they take a Bayesian perspective and rely on a parametric assumption on the symbol distribution."}, {"heading": "6. Conclusions", "text": "In this paper we proposed a unified framework for end-toend learning of compressed representations for deep architectures.\nBy training with a soft-to-hard annealing scheme, gradually transferring from a soft relaxation of the sample entropy and network discretization process to the actual nondifferentiable quantization process, we manage to optimize the rate distortion trade-off between the original network loss and the entropy.\nOur framework can elegantly capture diverse compression tasks, obtaining state-of-the-art results both for image compression as well as DNN compression. The simplicity of our approach opens up various directions for future work, since our framework can be easily adapted for other architectures where a compressible representation is desired."}], "references": [{"title": "Numerical continuation methods: an introduction, volume 13", "author": ["Allgower", "Eugene L", "Georg", "Kurt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Allgower et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Allgower et al\\.", "year": 2012}, {"title": "End-to-end optimization of nonlinear transform codes for perceptual quality", "author": ["Ball\u00e9", "Johannes", "Laparra", "Valero", "Simoncelli", "Eero P"], "venue": "arXiv preprint arXiv:1607.05006,", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2016}, {"title": "End-to-end optimized image compression", "author": ["Ball\u00e9", "Johannes", "Laparra", "Valero", "Simoncelli", "Eero P"], "venue": "arXiv preprint arXiv:1611.01704,", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2016}, {"title": "Towards the limit of network quantization", "author": ["Choi", "Yoojin", "El-Khamy", "Mostafa", "Lee", "Jungwon"], "venue": "arXiv preprint arXiv:1612.01543,", "citeRegEx": "Choi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "JeanPierre. Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Elements of information theory", "author": ["Cover", "Thomas M", "Thomas", "Joy A"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cover et al\\.", "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Dermatologist-level classification of skin cancer", "author": ["Esteva", "Andre", "Kuprel", "Brett", "Novoa", "Roberto A", "Ko", "Justin", "Swetter", "Susan M", "Blau", "Helen M", "Thrun", "Sebastian"], "venue": null, "citeRegEx": "Esteva et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Esteva et al\\.", "year": 2017}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Guo", "Yiwen", "Yao", "Anbang", "Chen", "Yurong"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Single image super-resolution from transformed selfexemplars", "author": ["Huang", "Jia-Bin", "Singh", "Abhishek", "Ahuja", "Narendra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Hubara", "Itay", "Courbariaux", "Matthieu", "Soudry", "Daniel", "ElYaniv", "Ran", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In ESANN,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "nuation-based learning algorithm for discrete-time cellular neural networks", "author": ["Magnussen", "Holger", "Papoutsis", "Georgios", "Nossek", "Josef A. Conti"], "venue": "In Cellular Neural Networks and their Applications,", "citeRegEx": "Magnussen et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Magnussen et al\\.", "year": 1994}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In Proc. Int\u2019l Conf. Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Vector quantization by deterministic annealing", "author": ["Rose", "Kenneth", "Gurewitz", "Eitan", "Fox", "Geoffrey C"], "venue": "IEEE Transactions on Information theory,", "citeRegEx": "Rose et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rose et al\\.", "year": 1992}, {"title": "Is the deconvolution layer the same as a convolutional layer", "author": ["Shi", "Wenzhe", "Caballero", "Jose", "Theis", "Lucas", "Huszar", "Ferenc", "Aitken", "Andrew", "Ledig", "Christian", "Wang", "Zehan"], "venue": "arXiv preprint arXiv:1609.07009,", "citeRegEx": "Shi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Lossy image compression with compressive autoencoders", "author": ["Theis", "Lucas", "Shi", "Wenzhe", "Cunningham", "Andrew", "Huszar", "Ferenc"], "venue": "In ICLR", "citeRegEx": "Theis et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2017}, {"title": "A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, pp. 111\u2013126", "author": ["Timofte", "Radu", "De Smet", "Vincent", "Van Gool", "Luc"], "venue": null, "citeRegEx": "Timofte et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Timofte et al\\.", "year": 2015}, {"title": "Variable rate image compression with recurrent neural networks", "author": ["Toderici", "George", "O\u2019Malley", "Sean M", "Hwang", "Sung Jin", "Vincent", "Damien", "Minnen", "David", "Baluja", "Shumeet", "Covell", "Michele", "Sukthankar", "Rahul"], "venue": "arXiv preprint arXiv:1511.06085,", "citeRegEx": "Toderici et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2015}, {"title": "Full resolution image compression with recurrent neural networks", "author": ["Toderici", "George", "Vincent", "Damien", "Johnston", "Nick", "Hwang", "Sung Jin", "Minnen", "David", "Shor", "Joel", "Covell", "Michele"], "venue": "arXiv preprint arXiv:1608.05148,", "citeRegEx": "Toderici et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2016}, {"title": "Soft weight-sharing for neural network compression", "author": ["Ullrich", "Karen", "Meeds", "Edward", "Welling", "Max"], "venue": "arXiv preprint arXiv:1702.04008,", "citeRegEx": "Ullrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2017}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Z. Wang", "E.P. Simoncelli", "A.C. Bovik"], "venue": "In Asilomar Conference on Signals, Systems Computers,", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wen", "Wei", "Wu", "Chunpeng", "Wang", "Yandan", "Chen", "Yiran", "Li", "Hai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Arithmetic coding for data compression", "author": ["Witten", "Ian H", "Neal", "Radford M", "Cleary", "John G"], "venue": "Commun. ACM,", "citeRegEx": "Witten et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1987}, {"title": "Optimizing 1nearest prototype classifiers", "author": ["Wohlhart", "Paul", "Kostinger", "Martin", "Donoser", "Michael", "Roth", "Peter M", "Bischof", "Horst"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wohlhart et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wohlhart et al\\.", "year": 2013}, {"title": "Competitive learning and soft competition for vector quantizer design", "author": ["Yair", "Eyal", "Zeger", "Kenneth", "Gersho", "Allen"], "venue": "IEEE transactions on Signal Processing,", "citeRegEx": "Yair et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Yair et al\\.", "year": 1992}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["Zhou", "Aojun", "Yao", "Anbang", "Guo", "Yiwen", "Xu", "Lin", "Chen", "Yurong"], "venue": "arXiv preprint arXiv:1702.03044,", "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 18, "context": "In recent years, deep neural networks (DNNs) have led to many breakthrough results in machine learning and computer vision (Krizhevsky et al., 2012; Silver et al., 2016; Esteva et al., 2017), and are now widely deployed in industry.", "startOffset": 123, "endOffset": 190}, {"referenceID": 7, "context": "In recent years, deep neural networks (DNNs) have led to many breakthrough results in machine learning and computer vision (Krizhevsky et al., 2012; Silver et al., 2016; Esteva et al., 2017), and are now widely deployed in industry.", "startOffset": 123, "endOffset": 190}, {"referenceID": 4, "context": "Among the most popular ones are stochastic approximations (Williams, 1992; Krizhevsky & Hinton, 2011; Courbariaux et al., 2015; Toderici et al., 2015; Ball\u00e9 et al., 2016b) and rounding with a smooth derivative approximation (Hubara et al.", "startOffset": 58, "endOffset": 171}, {"referenceID": 26, "context": "Among the most popular ones are stochastic approximations (Williams, 1992; Krizhevsky & Hinton, 2011; Courbariaux et al., 2015; Toderici et al., 2015; Ball\u00e9 et al., 2016b) and rounding with a smooth derivative approximation (Hubara et al.", "startOffset": 58, "endOffset": 171}, {"referenceID": 13, "context": ", 2016b) and rounding with a smooth derivative approximation (Hubara et al., 2016; Theis et al., 2017).", "startOffset": 61, "endOffset": 102}, {"referenceID": 24, "context": ", 2016b) and rounding with a smooth derivative approximation (Hubara et al., 2016; Theis et al., 2017).", "startOffset": 61, "endOffset": 102}, {"referenceID": 24, "context": "and to model the marginal symbol distribution with a parametric model, such as a Gaussian mixture model (Theis et al., 2017; Ullrich et al., 2017), a piecewise linear model (Ball\u00e9 et al.", "startOffset": 104, "endOffset": 146}, {"referenceID": 28, "context": "and to model the marginal symbol distribution with a parametric model, such as a Gaussian mixture model (Theis et al., 2017; Ullrich et al., 2017), a piecewise linear model (Ball\u00e9 et al.", "startOffset": 104, "endOffset": 146}, {"referenceID": 27, "context": ", 2016b), or a Bernoulli distribution (Toderici et al., 2016) (in the case of binary symbols).", "startOffset": 38, "endOffset": 61}, {"referenceID": 11, "context": "\u2022 We apply our method to DNN model compression for a 32-layer ResNet model (He et al., 2016) and fullresolution image compression using a variant of the compressive autoencoder proposed recently in (Theis et al.", "startOffset": 75, "endOffset": 92}, {"referenceID": 24, "context": ", 2016) and fullresolution image compression using a variant of the compressive autoencoder proposed recently in (Theis et al., 2017) (see Figure 1 for an overview).", "startOffset": 113, "endOffset": 133}, {"referenceID": 24, "context": "In both cases, we obtain state-of-the-art performance, while making fewer model assumptions and significantly simplifying the training procedure compared to the original works (Theis et al., 2017; Choi et al., 2016).", "startOffset": 176, "endOffset": 215}, {"referenceID": 3, "context": "In both cases, we obtain state-of-the-art performance, while making fewer model assumptions and significantly simplifying the training procedure compared to the original works (Theis et al., 2017; Choi et al., 2016).", "startOffset": 176, "endOffset": 215}, {"referenceID": 26, "context": "There has been a surge of interest in DNN models for full-resolution image compression, most notably (Toderici et al., 2015; 2016; Ball\u00e9 et al., 2016a;b; Theis et al., 2017), all of which outperform JPEG and some even JPEG2000.", "startOffset": 101, "endOffset": 173}, {"referenceID": 24, "context": "There has been a surge of interest in DNN models for full-resolution image compression, most notably (Toderici et al., 2015; 2016; Ball\u00e9 et al., 2016a;b; Theis et al., 2017), all of which outperform JPEG and some even JPEG2000.", "startOffset": 101, "endOffset": 173}, {"referenceID": 3, "context": "In the context of DNN model compression, the line of works (Han et al., 2015b;a; Choi et al., 2016) adopts a multi-step procedure in which the weights of a pretrained DNN are first pruned and the remaining parameters are quantized using a k-means like algorithm, the DNN is then retrained, and finally the quantized DNN model is encoded using arithmetic coding.", "startOffset": 59, "endOffset": 99}, {"referenceID": 8, "context": "To overcome this issue, (Guo et al., 2016) propose a dynamic network surgery technique that allows to reactivate pruned weights.", "startOffset": 24, "endOffset": 42}, {"referenceID": 28, "context": "A notable different approach is taken by (Ullrich et al., 2017), where the DNN compression task is tackled using the minimum description length principle, which has a solid information-theoretic foundation.", "startOffset": 41, "endOffset": 63}, {"referenceID": 13, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 21, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 31, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 36, "context": ", (Hubara et al., 2016; Rastegari et al., 2016; Wen et al., 2016; Zhou et al., 2017).", "startOffset": 2, "endOffset": 84}, {"referenceID": 16, "context": "(2016a); Theis et al. (2017) both rely on convolutional autoencoder architectures.", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "Ball\u00e9 et al. (2016a) model the quantization error by uniform additive noise and incorporate generalized divisive normalization (GDN) into their architecture to reduce higher-order statistical dependencies among the features to be compressed.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Ball\u00e9 et al. (2016a) model the quantization error by uniform additive noise and incorporate generalized divisive normalization (GDN) into their architecture to reduce higher-order statistical dependencies among the features to be compressed. Their optimization framework has a close relation to variational autoencoders (Kingma & Welling, 2013). In contrast, Theis et al. (2017) quantize the representation in the bottleneck of the autoencoder to integer values, employing the identity mapping for gradient backpropagation.", "startOffset": 0, "endOffset": 379}, {"referenceID": 22, "context": ", (Rose et al., 1992; Yair et al., 1992).", "startOffset": 2, "endOffset": 40}, {"referenceID": 35, "context": ", (Rose et al., 1992; Yair et al., 1992).", "startOffset": 2, "endOffset": 40}, {"referenceID": 34, "context": "Arguably most related to our approach is (Wohlhart et al., 2013), which also employs continuation for nearest neighbor assignments, but in the context of learning a supervised prototype classifier.", "startOffset": 41, "endOffset": 64}, {"referenceID": 19, "context": "In the context of neural networks, Magnussen et al. (1994) optimize the parameters of a discrete-time cellular neural network by gradually moving from a sigmoid to a sign function.", "startOffset": 35, "endOffset": 59}, {"referenceID": 33, "context": ", arithmetic coding (Witten et al., 1987).", "startOffset": 20, "endOffset": 41}, {"referenceID": 24, "context": "In terms of network architecture, we rely on a variant of the compressive autoencoder proposed recently in (Theis et al., 2017), using convolutional neural networks for the image encoder and image decoder3.", "startOffset": 107, "endOffset": 127}, {"referenceID": 24, "context": ", 2016a;b) used in (Theis et al., 2017), we use standard deconvolutions for simplicity.", "startOffset": 19, "endOffset": 39}, {"referenceID": 24, "context": "We note that while we use the architecture of (Theis et al., 2017), we train it using our soft-to-hard entropy minimization method, which differs significantly from the approach in (Theis et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 24, "context": ", 2017), we train it using our soft-to-hard entropy minimization method, which differs significantly from the approach in (Theis et al., 2017).", "startOffset": 122, "endOffset": 142}, {"referenceID": 24, "context": "(Theis et al., 2017) use 64 and 96 channels.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "We used a subset of 90,000 images from ImageNet (Deng et al., 2009), which we downsampled by a factor 0.", "startOffset": 48, "endOffset": 67}, {"referenceID": 24, "context": "Similar to (Theis et al., 2017) we gradually unfreeze the channels in the bottleneck during training.", "startOffset": 11, "endOffset": 31}, {"referenceID": 25, "context": "B100 (Timofte et al., 2015) is a set of 100 content diverse color 481\u00d7321 test images from the Berkeley Segmentation Dataset (Martin et al.", "startOffset": 5, "endOffset": 27}, {"referenceID": 20, "context": ", 2015) is a set of 100 content diverse color 481\u00d7321 test images from the Berkeley Segmentation Dataset (Martin et al., 2001).", "startOffset": 105, "endOffset": 126}, {"referenceID": 12, "context": "Urban100 (Huang et al., 2015) has 100 color images selected from Flickr with labels such as urban, city, architecture, and structure.", "startOffset": 9, "endOffset": 29}, {"referenceID": 30, "context": "SSIM and MSSSIM are the structural similarity index (Wang et al., 2004) and its multi-scale SSIM computed variant (Wang et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 29, "context": ", 2004) and its multi-scale SSIM computed variant (Wang et al., 2003) proposed to measure the similarity of two images.", "startOffset": 50, "endOffset": 69}, {"referenceID": 24, "context": "Detailed Discussion of with Related Methods We note that recent works of (Theis et al., 2017; Ball\u00e9 et al., 2016b) also showed competitive performance with JPEG2000.", "startOffset": 73, "endOffset": 114}, {"referenceID": 24, "context": "While we use the architecture of (Theis et al., 2017), there are stark differences between the works: For quantization, we perform vector quantization while they perform rounding to integers.", "startOffset": 33, "endOffset": 53}, {"referenceID": 6, "context": "As training material, we restrict ourselves to a subset of the ImageNet database (Deng et al., 2009), while Theis et al collect a set of high quality photos from Flickr.", "startOffset": 81, "endOffset": 100}, {"referenceID": 24, "context": "Since neither raw bpp/PSNR/SSIM numbers nor codes are available and training procedures and architectures differ significantly, a direct comparison with (Theis et al., 2017; Ball\u00e9 et al., 2016b) is hard.", "startOffset": 153, "endOffset": 194}, {"referenceID": 27, "context": ", 2017) builds on the architecture proposed in (Toderici et al., 2016), and shows that impressive performance on the perceptual MS-SSIM metric (Wang et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 29, "context": ", 2016), and shows that impressive performance on the perceptual MS-SSIM metric (Wang et al., 2003) can be obtained by incorporating it into the optimization (and using sophisticated coding techniques).", "startOffset": 80, "endOffset": 99}, {"referenceID": 24, "context": "In contrast, our work adheres to the setting of (Theis et al., 2017; Ball\u00e9 et al., 2016b; Toderici et al., 2016; 2015), simply optimizing for mean squared error, while evaluating using PSNR, SSIM and MS-SSIM.", "startOffset": 48, "endOffset": 118}, {"referenceID": 27, "context": "In contrast, our work adheres to the setting of (Theis et al., 2017; Ball\u00e9 et al., 2016b; Toderici et al., 2016; 2015), simply optimizing for mean squared error, while evaluating using PSNR, SSIM and MS-SSIM.", "startOffset": 48, "endOffset": 118}, {"referenceID": 3, "context": "The pruning based results are from (Choi et al., 2016).", "startOffset": 35, "endOffset": 54}, {"referenceID": 11, "context": "For DNN compression, we investigate the ResNet (He et al., 2016) architecture for image classification.", "startOffset": 47, "endOffset": 64}, {"referenceID": 3, "context": "We adopt the same setting as (Choi et al., 2016) and consider a 32-layer architecture trained for the CIFAR-10 dataset (Krizhevsky & Hinton, 2009).", "startOffset": 29, "endOffset": 48}, {"referenceID": 3, "context": "As in (Choi et al., 2016), our goal is to learn a compressible representation for all 464,154 trainable parameters of the model.", "startOffset": 6, "endOffset": 25}, {"referenceID": 3, "context": "For a straightforward comparison with (Choi et al., 2016), we constrain ourselves to scalar quantization.", "startOffset": 38, "endOffset": 57}, {"referenceID": 3, "context": "Table 1 compares our results with state-of-the-art approaches reported by (Choi et al., 2016).", "startOffset": 74, "endOffset": 93}, {"referenceID": 3, "context": "This happens automatically in the learning process, as opposed to the methods of (Han et al., 2015b;a; Choi et al., 2016), which manually impose 0 as the most frequent weight by pruning the network.", "startOffset": 81, "endOffset": 121}, {"referenceID": 28, "context": "We note that the recent works by (Ullrich et al., 2017), where the compression task is approached using the minimum description length principle, also manages to tackle the problem in a single training procedure.", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.", "creator": "LaTeX with hyperref package"}}}