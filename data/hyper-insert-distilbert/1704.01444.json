{"id": "1704.01444", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Learning to Generate Reviews and Discovering Sentiment", "abstract": "we carefully explore intriguing the properties properties of byte - level implicit recurrent speech language model models. consequently when given previously sufficient amounts of capacity, namely training minimum data, bandwidth and immediate compute time, the representations learned by acquiring these new models include disentangled features accurately corresponding to high - density level classification concepts. specifically, we find a single unit diagram which performs sentiment analysis. these representations, if learned frequently in just an unsupervised manner, achieve state of the art on the binary subset dimensions of the stanford sentiment treebank. they are essentially also very data efficient. when using slightly only a handful of historically labeled examples, frequently our approach matches upon the baseline performance of strong squared baselines fully trained explicitly on representing full datasets. we may also demonstrate ways the reverse sentiment unit tensor has somehow a direct influence on the computational generative process level of underlying the model. simply fixing its nominal value to better be combined positive or negative values generates separate samples with the corresponding positive or weak negative sentiment.", "histories": [["v1", "Wed, 5 Apr 2017 14:20:28 GMT  (445kb,D)", "http://arxiv.org/abs/1704.01444v1", null], ["v2", "Thu, 6 Apr 2017 09:48:20 GMT  (446kb,D)", "http://arxiv.org/abs/1704.01444v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["alec radford", "rafal jozefowicz", "ilya sutskever"], "accepted": false, "id": "1704.01444"}, "pdf": {"name": "1704.01444.pdf", "metadata": {"source": "META", "title": "Learning to Generate Reviews and Discovering Sentiment", "authors": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever"], "emails": ["<alec@openai.com>."], "sections": [{"heading": "1. Introduction and Motivating Work", "text": "Representation learning (Bengio et al., 2013) plays a critical role in many modern machine learning systems. Representations map raw data to more useful forms and the choice of representation is an important component of any application. Broadly speaking, there are two areas of research emphasizing different details of how to learn useful representations.\nThe supervised training of high-capacity models on large labeled datasets is critical to the recent success of deep learning techniques for a wide range of applications such as image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), and machine translation (Wu et al., 2016). Analysis of the task specific representations learned by these models reveals many fascinating properties. Image classifiers learn a broadly useful hierarchy of feature detectors re-representing raw pixels as edges, textures, and objects (Zeiler & Fergus, 2014). In the field of computer vision, it is now commonplace to reuse these\n1OpenAI, San Francisco, California, USA. Correspondence to: Alec Radford <alec@openai.com>.\nrepresentations on a broad suite of related tasks - one of the most successful examples of transfer learning to date.\nThere is also a long history of unsupervised representation learning. Much of the early research into modern deep learning was developed and validated via this approach (Hinton & Salakhutdinov, 2006) (Huang et al., 2007) (Vincent et al., 2008). Unsupervised learning is promising due to its ability to scale beyond only the subsets or domains of data that can be cleaned and labeled given resource, privacy, or other constraints. This advantage is also its difficulty. While supervised approaches have clear objectives that can be directly optimized, unsupervised approaches rely on proxy tasks such as reconstruction, density estimation, or generation, which do not directly encourage useful representations for specific tasks. As a result, much work has gone into designing objectives, priors, and architectures meant to encourage the learning of useful representations. We refer readers to Bengio et al. (2013) for a detailed review.\nDespite these difficulties, there are notable applications of unsupervised learning. Pre-trained word vectors are a vital part of many modern NLP systems. These representations, learned by modeling word co-occurrences, increase the data efficiency and generalization capability of NLP systems (Pennington et al., 2014). Topic modelling can also discover factors within a corpus of text which align to human interpretable concepts such as art or education (Blei et al., 2003).\nHow to learn representations of phrases, sentences, and documents is an open area of research. Inspired by the success of word vectors, Kiros et al. (2015) propose skipthought vectors, a method of training a sentence encoder by predicting the preceding and following sentence. The representation learned by this objective performs competitively on a broad suite of evaluated tasks. More advanced training techniques such as layer normalization (Ba et al., 2016) further improve results. However, skip-thought vectors are still outperformed by supervised models which directly optimize the desired performance metric on a specific dataset. This is the case for both text classification tasks, which measure whether a specific concept is well encoded in a representation, and more general semantic similarity tasks. This is even the case for datasets that are relaar X\niv :1\n70 4.\n01 44\n4v 1\n[ cs\n.L G\n] 5\nA pr\n2 01\n7\ntively small by modern standards, often consisting of only a few thousand labeled examples.\nIn contrast to learning a generic representation on one large dataset and then evaluating on other tasks/datasets, Dai & Le (2015) proposed using similar unsupervised objectives such as sequence autoencoding and language modeling to first pretrain a model on a dataset and then finetune it for a given task. This approach outperformed training the same model from random initialization and achieved state of the art performance on several text classification datasets. Combining language modelling with topic modelling and fitting a small supervised feature extractor on top has also achieved strong results on in-domain document level sentiment analysis (Dieng et al., 2016).\nConsidering this, we hypothesize two effects may be combining to result in the weaker performance of purely unsupervised approaches. Skip-thought vectors were trained on a corpus of books. But some of the classification tasks they are evaluated on, such as sentiment analysis of reviews of consumer goods, do not have much overlap with the text of novels. We propose this distributional issue, combined with the limited capacity of current models, results in representational underfitting. Current generic distributed sentence representations may be very lossy - good at capturing the gist, but poor with the precise semantic or syntactic details which are critical for applications.\nCurrent experimental and evaluation protocols may be underestimating the quality of unsupervised representation learning for sentences and documents due to certain seemingly insignificant design decisions. Hill et al. (2016) also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations - including the above mentioned skip-thoughts.\nIn this work, we test whether this is the case. We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept. As an approach, we consider the popular research benchmark of byte (character) level language modelling due to its simplicity and generality. We are also interested in evaluating this approach as it is not immediately clear whether such a low-level training objective supports the learning of high-level representations. We train on a very large corpus picked to have a similar distribution as our task of interest. We also benchmark on a wider range of tasks to quantify the sensitivity of the learned representation to various degrees of out-of-domain data and tasks."}, {"heading": "2. Dataset", "text": "Much previous work on language modeling has evaluated on relatively small but competitive datasets such as Penn\nTreebank (Marcus et al., 1993) and Hutter Prize Wikipedia (Hutter, 2006). As discussed in Jozefowicz et al. (2016) performance on these datasets is primarily dominated by regularization. Since we are interested in high-quality sentiment representations, we chose the Amazon product review dataset introduced in McAuley et al. (2015) as a training corpus. In de-duplicated form, this dataset contains over 82 million product reviews from May 1996 to July 2014 amounting to over 38 billion training bytes. Due to the size of the dataset, we first split it into 1000 shards containing equal numbers of reviews and set aside 1 shard for validation and 1 shard for test."}, {"heading": "3. Model and Training Details", "text": "Many potential recurrent architectures and hyperparameter settings were considered in preliminary experiments on the dataset. Given the size of the dataset, searching the wide space of possible configurations is quite costly. To help alleviate this, we evaluated the generative performance of smaller candidate models after a single pass through the dataset. The model chosen for the large scale experiment is a single layer multiplicative LSTM (Krause et al., 2016) with 4096 units. We observed multiplicative LSTMs to converge faster than normal LSTMs for the hyperparameter settings that were explored both in terms of data and wall-clock time. The model was trained for a single epoch on mini-batches of 128 subsequences of length 256 for a total of 1 million weight updates. States were initialized to zero at the beginning of each shard and persisted across updates to simulate full-backpropagation and allow for the\nforward propagation of information outside of a given subsequence. Adam (Kingma & Ba, 2014) was used to accelerate learning with an initial 5e-4 learning rate that was decayed linearly to zero over the course of training. Weight normalization (Salimans & Kingma, 2016) was applied to the LSTM parameters. Data-parallelism was used across 4 Pascal Titan X gpus to speed up training and increase effective memory size. Training took approximately one month. The model is compact, containing approximately as many parameters as there are reviews in the training dataset. It also has a high ratio of compute to total parameters compared to other large scale language models due to operating at a byte level. The selected model reaches 1.12 bits per byte."}, {"heading": "4. Experimental Setup and Results", "text": "Our model processes text as a sequence of UTF-8 encoded bytes (Yergeau, 2003). For each byte, the model updates its hidden state and predicts a probability distribution over the next possible byte. The hidden state of the model serves as an online summary of the sequence which encodes all information the model has learned to preserve that is relevant to predicting the future bytes of the sequence. We are\ninterested in understanding the properties of the learned encoding. The process of extracting a feature representation is outlined as follows:\n\u2022 Since newlines are used as review delimiters in the training dataset, all newline characters are replaced with spaces to avoid the model resetting state.\n\u2022 Any leading whitespace is removed and replaced with a newline+space to simulate a start token. Any trailing whitespace is removed and replaced with a space to simulate an end token. The text is encoded as a UTF8 byte sequence.\n\u2022 Model states are initialized to zeros. The model processes the sequence and the final cell states of the mLSTM are used as a feature representation. Tanh is applied to bound values between -1 and 1.\nWe follow the methodology established in Kiros et al. (2015) by training a logistic regression classifier on top of our model\u2019s representation on datasets for tasks including semantic relatedness, text classification, and paraphrase detection. For the details on these comparison experiments, we refer the reader to their work. One exception is that we use an L1 penalty for text classification results instead of L2 as we found this performed better in the very low data regime."}, {"heading": "4.1. Review Sentiment Analysis", "text": "Table 1 shows the results of our model on 4 standard text classification datasets. The performance of our model is noticeably lopsided. On the MR (Pang & Lee, 2005) and CR (Hu & Liu, 2004) sentiment analysis datasets we improve the state of the art by a significant margin. The MR and CR datasets are sentences extracted from Rotten Tomatoes, a movie review website, and Amazon product reviews (which almost certainly overlaps with our training corpus). This suggests that our model has learned a rich representation of text from a similar domain. On the other two datasets, SUBJ\u2019s subjectivity/objectivity detection (Pang & Lee, 2004) and MPQA\u2019s opinion polarity (Wiebe et al.,\n2005) our model has no noticeable advantage over other unsupervised representation learning approaches and is still outperformed by a supervised approach.\nTo better quantify the learned representation, we also test on a wider set of sentiment analysis datasets with different properties. The Stanford Sentiment Treebank (SST) (Socher et al., 2013) was created specifically to evaluate more complex compositional models of language. It is derived from the same base dataset as MR but was relabeled via Amazon Mechanical and includes dense labeling of the phrases of parse trees computed for all sentences. For the binary subtask, this amounts to 76961 total labels compared to the 6920 sentence level labels. As a demonstration of the capability of unsupervised representation learning to simplify data collection and remove preprocessing steps, our reported results ignore these dense labels and computed parse trees, using only the raw text and sentence level labels.\nThe representation learned by our model achieves 91.8% significantly outperforming the state of the art of 90.2% by a 30 model ensemble (Looks et al., 2017). As visualized in Figure 2, our model is very data efficient. It matches the performance of baselines using as few as a dozen labeled examples and outperforms all previous results with only a few hundred labeled examples. This is under 10% of the total sentences in the dataset. Confusingly, despite a 16% relative error reduction on the binary subtask, it does not reach the state of the art of 53.6% on the fine-grained subtask, achieving 52.9%."}, {"heading": "4.2. Sentiment Unit", "text": "We conducted further analysis to understand what representations our model learned and how they achieve the ob-\nserved data efficiency. The benefit of an L1 penalty in the low data regime (see Figure 2) is a clue. L1 regularization is known to reduce sample complexity when there are many irrelevant features (Ng, 2004). This is likely to be the case for our model since it is trained as a language model and not as a supervised feature extractor. By inspecting the relative contributions of features on various datasets, we discovered a single unit within the mLSTM that directly corresponds to sentiment. In Figure 3 we show the histogram of the final activations of this unit after processing IMDB reviews (Maas et al., 2011) which shows a bimodal distribution with a clear separation between positive and negative reviews. In Figure 4 we visualize the activations of this unit on 6 randomly selected reviews from a set of 100 high contrast reviews which shows it acts as an online estimate of the local sentiment of the review. Fitting a threshold to this single unit achieves a test accuracy of 92.30% which outperforms a strong supervised results on the dataset, the 91.87% of NB-SVM trigram (Mesnil et al.,\n2014), but is still below the semi-supervised state of the art of 94.09% (Miyato et al., 2016). Using the full 4096 unit representation achieves 92.88%. This is an improvement of only 0.58% over the sentiment unit suggesting that almost all information the model retains that is relevant to sentiment analysis is represented in the very compact form of a single scalar. Table 2 has a full list of results on the IMDB dataset."}, {"heading": "4.3. Capacity Ceiling", "text": "Encouraged by these results, we were curious how well the model\u2019s representation scales to larger datasets. We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in Zhang et al. (2015). This dataset contains 598,000 examples which is an order of magnitude larger than any other datasets we tested on. When visualizing performance as a function of number of training examples in Figure 5, we observe a \u201dcapacity ceiling\u201d where the test accuracy of our model only improves by a little over 1% across a four order of magnitude increase in training data. Using the full dataset, we achieve 95.22%\ntest accuracy. This better than a BoW TFIDF baseline at 93.66% but slightly worse than the 95.64% of a linear classifier on top of the 500,000 most frequent n-grams up to length 5.\nThe observed capacity ceiling is an interesting phenomena and stumbling point for scaling our unsupervised representations. We think a variety of factors are contributing to cause this. Since our model is trained only on Amazon reviews, it is does not appear to be sensitive to concepts specific to other domains. For instance, Yelp reviews are of businesses, where details like hospitality, location, and atmosphere are important. But these ideas are not present in reviews of products. Additionally, there is a notable drop in the relative performance of our approach transitioning from sentence to document datasets. This is likely due to our model working on the byte level which leads to it focusing on the content of the last few sentences instead of the whole document. Finally, as the amount of labeled data increases, the performance of the simple linear model we train on top of our static representation will eventually saturate. Complex models explicitly trained for a task can continue to improve and eventually outperform our approach with enough labeled data.\nWith this context, the observed results make a lot of sense. On a small sentence level dataset of a known domain (the movie reviews of Stanford Sentiment Treebank) our model sets a new state of the art. But on a large, document level dataset of a different domain (the Yelp reviews) it is only competitive with standard baselines."}, {"heading": "4.4. Other Tasks", "text": "Besides classification, we also evaluate on two other standard tasks: semantic relatedness and paraphrase detection.\nWhile our model performs competitively on Microsoft Research Paraphrase Corpus (Dolan et al., 2004) in Table 3, it performs poorly on the SICK semantic relatedness task (Marelli et al., 2014) in Table 4. It is likely that the form and content of the semantic relatedness task, which is built on top of descriptions of images and videos and contains sentences such as \u201dA sea turtle is hunting for fish\u201d is effectively out-of-domain for our model which has only been trained on the text of product reviews."}, {"heading": "4.5. Generative Analysis", "text": "Although the focus of our analysis has been on the properties of our model\u2019s representation, it is trained as a generative model and we are also interested in its generative capabilities. We confirmed that the sentiment unit is not just used for internal computation, but strongly influences the generative process itself. In Table 5 we show that by simply setting the sentiment unit to be positive or negative, the model generates corresponding positive or negative reviews. It is interesting to see that such a simple manipulation of the model\u2019s representation has a strong effect on its behavior. The samples are also high quality for a byte level language model and often include valid sentences."}, {"heading": "5. Discussion and Future Work", "text": "It is an open question why our model recovers the concept of sentiment in such a precise, disentangled, interpretable, and manipulable way. It is possible that sentiment as a conditioning feature has strong predictive capability for language modelling. This is likely since sentiment is such an important component of a review. Previous work analysing LSTM language models showed the existence of interpretable units that indicate position within a line or presence inside a quotation (Karpathy et al., 2015). In many ways, the sentiment unit in this model is just a scaled up example of the same phenomena. The update equation of an LSTM could play a role. The element-wise operation of its gates may encourage axis-aligned representations. Models such as word2vec have also been observed to have small subsets of dimensions strongly associated with specific tasks (Li et al., 2016).\nOur work highlights the sensitivity of learned representations to the data distribution they are trained on. The results make clear that it is unrealistic to expect a model trained on a corpus of books, where the two most common genres are Romance and Fantasy, to learn an encoding which preserves the exact sentiment of a review. Likewise, it is unrealistic to expect a model trained on Amazon product reviews to represent the precise semantic content of a textual description of an image or a video.\nThere are several promising directions for future work highlighted by our results. The observed performance plateau, even on relatively similar domains, suggests improving the representation model both in terms of architecture and size. Since our model operates at the byte-level, hierarchical/multi-timescale extensions could improve the quality of representations for longer documents. The sensitivity of learned representations to their training domain could be addressed by training on a wider mix of datasets with better coverage of target tasks. Finally, our work encourages further research into language modelling as it demonstrates that the standard language modelling objective with no modifications is sufficient to learn high-quality representations."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Topicrnn: A recurrent neural network with long-range semantic dependency", "author": ["Dieng", "Adji B", "Wang", "Chong", "Gao", "Jianfeng", "Paisley", "John"], "venue": "arXiv preprint arXiv:1611.01702,", "citeRegEx": "Dieng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieng et al\\.", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Hill", "Felix", "Cho", "Kyunghyun", "Korhonen", "Anna"], "venue": "arXiv preprint arXiv:1602.03483,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Minqing", "Liu", "Bing"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["Huang", "Fu Jie", "Boureau", "Y-Lan", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "URL http://prize", "author": ["Hutter", "Marcus. The human knowledge compression contest."], "venue": "hutter1. net, 2006.", "citeRegEx": "Hutter and contest.,? 2006", "shortCiteRegEx": "Hutter and contest.", "year": 2006}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Multiplicative lstm for sequence modelling", "author": ["Krause", "Ben", "Lu", "Liang", "Murray", "Iain", "Renals", "Steve"], "venue": "arXiv preprint arXiv:1609.07959,", "citeRegEx": "Krause et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": "CoRR, abs/1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Understanding neural networks through representation erasure", "author": ["Li", "Jiwei", "Monroe", "Will", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1612.08220,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep learning with dynamic computation graphs", "author": ["Looks", "Moshe", "Herreshoff", "Marcello", "Hutchins", "DeLesley", "Norvig", "Peter"], "venue": "arXiv preprint arXiv:1702.02181,", "citeRegEx": "Looks et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Looks et al\\.", "year": 2017}, {"title": "Reexamining machine translation metrics for paraphrase identification", "author": ["Madnani", "Nitin", "Tetreault", "Joel", "Chodorow", "Martin"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entail", "author": ["Marelli", "Marco", "Bentivogli", "Luisa", "Baroni", "Bernardi", "Raffaella", "Menini", "Stefano", "Zamparelli", "Roberto"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Inferring networks of substitutable and complementary products", "author": ["McAuley", "Julian", "Pandey", "Rahul", "Leskovec", "Jure"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "McAuley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Mesnil", "Gr\u00e9goire", "Mikolov", "Tomas", "Ranzato", "Marc\u2019Aurelio", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Adversarial training methods for semi-supervised text classification", "author": ["Miyato", "Takeru", "Dai", "Andrew M", "Goodfellow", "Ian"], "venue": "arXiv preprint arXiv:1605.07725,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Munkhdalai", "Tsendsuren", "Yu", "Hong"], "venue": "arXiv preprint arXiv:1607.04315,", "citeRegEx": "Munkhdalai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Wiebe", "Janyce", "Wilson", "Theresa", "Cardie", "Claire"], "venue": "Language resources and evaluation,", "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "arXiv preprint arXiv:1511.08198,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Utf-8, a transformation format of iso 10646", "author": ["Yergeau", "Francois"], "venue": null, "citeRegEx": "Yergeau and Francois.,? \\Q2003\\E", "shortCiteRegEx": "Yergeau and Francois.", "year": 2003}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Characterlevel convolutional networks for text classification", "author": ["Zhang", "Xiang", "Zhao", "Junbo", "LeCun", "Yann"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Selfadaptive hierarchical sentence model", "author": ["Zhao", "Han", "Lu", "Zhengdong", "Poupart", "Pascal"], "venue": "arXiv preprint arXiv:1504.05070,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Representation learning (Bengio et al., 2013) plays a critical role in many modern machine learning systems.", "startOffset": 24, "endOffset": 45}, {"referenceID": 15, "context": "The supervised training of high-capacity models on large labeled datasets is critical to the recent success of deep learning techniques for a wide range of applications such as image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 198, "endOffset": 223}, {"referenceID": 7, "context": "Much of the early research into modern deep learning was developed and validated via this approach (Hinton & Salakhutdinov, 2006) (Huang et al., 2007) (Vincent et al.", "startOffset": 130, "endOffset": 150}, {"referenceID": 32, "context": ", 2007) (Vincent et al., 2008).", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "We refer readers to Bengio et al. (2013) for a detailed review.", "startOffset": 20, "endOffset": 41}, {"referenceID": 28, "context": "These representations, learned by modeling word co-occurrences, increase the data efficiency and generalization capability of NLP systems (Pennington et al., 2014).", "startOffset": 138, "endOffset": 163}, {"referenceID": 1, "context": "Topic modelling can also discover factors within a corpus of text which align to human interpretable concepts such as art or education (Blei et al., 2003).", "startOffset": 135, "endOffset": 154}, {"referenceID": 13, "context": "Inspired by the success of word vectors, Kiros et al. (2015) propose skipthought vectors, a method of training a sentence encoder by predicting the preceding and following sentence.", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "Combining language modelling with topic modelling and fitting a small supervised feature extractor on top has also achieved strong results on in-domain document level sentiment analysis (Dieng et al., 2016).", "startOffset": 186, "endOffset": 206}, {"referenceID": 4, "context": "Hill et al. (2016) also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations - including the above mentioned skip-thoughts.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "Much previous work on language modeling has evaluated on relatively small but competitive datasets such as Penn Treebank (Marcus et al., 1993) and Hutter Prize Wikipedia (Hutter, 2006).", "startOffset": 121, "endOffset": 142}, {"referenceID": 9, "context": "As discussed in Jozefowicz et al. (2016) performance on these datasets is primarily dominated by regularization.", "startOffset": 16, "endOffset": 41}, {"referenceID": 9, "context": "As discussed in Jozefowicz et al. (2016) performance on these datasets is primarily dominated by regularization. Since we are interested in high-quality sentiment representations, we chose the Amazon product review dataset introduced in McAuley et al. (2015) as a training corpus.", "startOffset": 16, "endOffset": 259}, {"referenceID": 14, "context": "The model chosen for the large scale experiment is a single layer multiplicative LSTM (Krause et al., 2016) with 4096 units.", "startOffset": 86, "endOffset": 107}, {"referenceID": 30, "context": "RNTN (Socher et al., 2013)", "startOffset": 5, "endOffset": 26}, {"referenceID": 16, "context": "CNN (Kim, 2014) DMN (Kumar et al., 2015) LSTM (Wieting", "startOffset": 20, "endOffset": 40}, {"referenceID": 13, "context": "We follow the methodology established in Kiros et al. (2015) by training a logistic regression classifier on top of our model\u2019s representation on datasets for tasks including semantic relatedness, text classification, and paraphrase detection.", "startOffset": 41, "endOffset": 61}, {"referenceID": 30, "context": "The Stanford Sentiment Treebank (SST) (Socher et al., 2013) was created specifically to evaluate more complex compositional models of language.", "startOffset": 38, "endOffset": 59}, {"referenceID": 18, "context": "2% by a 30 model ensemble (Looks et al., 2017).", "startOffset": 26, "endOffset": 46}, {"referenceID": 24, "context": "09% (Miyato et al., 2016).", "startOffset": 4, "endOffset": 25}, {"referenceID": 37, "context": "We try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in Zhang et al. (2015). This dataset contains 598,000 examples which is an order of magnitude larger than any other datasets we tested on.", "startOffset": 97, "endOffset": 117}, {"referenceID": 21, "context": ", 2004) in Table 3, it performs poorly on the SICK semantic relatedness task (Marelli et al., 2014) in Table 4.", "startOffset": 77, "endOffset": 99}, {"referenceID": 10, "context": "Previous work analysing LSTM language models showed the existence of interpretable units that indicate position within a line or presence inside a quotation (Karpathy et al., 2015).", "startOffset": 157, "endOffset": 180}, {"referenceID": 17, "context": "Models such as word2vec have also been observed to have small subsets of dimensions strongly associated with specific tasks (Li et al., 2016).", "startOffset": 124, "endOffset": 141}], "year": 2017, "abstractText": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.", "creator": "LaTeX with hyperref package"}}}