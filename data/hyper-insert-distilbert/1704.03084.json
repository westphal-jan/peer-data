{"id": "1704.03084", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning", "abstract": "in a composite - sequential domain embedded task - completion dialogue system, a conversation agent often successfully switches among relatively multiple objective sub - hierarchy domains before it successfully blindly completes accepting the task. given such a scenario, a developing standard deep reinforcement robust learning feedback based independent dialogue agent researcher may suffer complications to find a good policy due to the scheduling issues such as : increased state sensitivity and action content spaces, high precision sample perceived complexity demands, sparse reward and long horizon, etc. \u2022 in this laboratory paper, we usually propose another to use hierarchical regression deep reinforcement learning approach which can often operate at different intrinsic temporal value scales and is intrinsically motivated to successfully attack these problems. theoretically our structured hierarchical network consists strongly of two levels : the top - level meta - hypothesis controller mechanism for subgoal hypothesis selection and flanking the low - q level posterior controller for linear dialogue policy learning. subgoals selected candidates by matching meta - controller and their intrinsic rewards can guide the controller responses to effectively actively explore irregularities in the state - action configuration space limitations and mitigate out the hind spare contingent reward and reduced long horizon relevance problems. clinical experiments on comparative both simulations and human evaluation show that integrating our model significantly outperforms flat deep reinforcement type learning stimulus agents in particular terms of success consumption rate, insufficient rewards and user sensitivity rating.", "histories": [["v1", "Mon, 10 Apr 2017 23:24:46 GMT  (1370kb,D)", "http://arxiv.org/abs/1704.03084v1", "13 pages, 7 figures"], ["v2", "Mon, 17 Apr 2017 19:36:30 GMT  (1884kb,D)", "http://arxiv.org/abs/1704.03084v2", "11 pages, 8 figures"], ["v3", "Sat, 22 Jul 2017 22:23:53 GMT  (1596kb,D)", "http://arxiv.org/abs/1704.03084v3", "12 pages, 8 figures"]], "COMMENTS": "13 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["baolin peng", "xiujun li", "lihong li", "jianfeng gao", "asli \u00e7elikyilmaz", "sungjin lee", "kam-fai wong"], "accepted": true, "id": "1704.03084"}, "pdf": {"name": "1704.03084.pdf", "metadata": {"source": "CRF", "title": "Composite Task-Completion Dialogue System via Hierarchical Deep Reinforcement Learning", "authors": ["Baolin Peng", "Xiujun Li", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz", "Sungjin Lee", "Kam-Fai Wong"], "emails": ["kfwong}@se.cuhk.edu.hk", "xiul@microsoft.com", "lihongli@microsoft.com", "jfgao@microsoft.com", "aslicel@microsoft.com", "sule@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Designing intelligent personal assistants that can help users to accomplish complex tasks is a longterm goal for human, and also a highly challenging problem in AI and NLP communities. Taskcompletion dialogue system is a key example of such personal assistants that can help people to accomplish certain tasks via natural language exchange. In today\u2019s market, many conversational\nagents are emerging, such as Microsoft\u2019s Cortana, Apple\u2019s Siri, Google\u2019s Home and Amazon\u2019s Echo. However, these personal assistants can only accomplish simple tasks, still far behind being able to handle complex tasks (Young, 2010; Williams and Young, 2007).\nRecently, history has witnessed many impressive results of deep reinforcement learning in many domains, like Atari games (Mnih et al., 2015), the game of Go (Silver et al., 2016), text games (Narasimhan et al., 2015; He et al., 2016) and robotics (Schulman et al., 2015; Levine et al., 2016). For dialogue tasks, reinforcement learning algorithms are a natural choice, which can improve the performance of agents over time through interactions with users. Several dialogue systems have been proposed within this paradigm, e.g. info-bot for information access (Dhingra et al., 2016), end-to-end task completion agent (Zhao and Eske\u0301nazi, 2016; Li et al., 2017a), and open domain dialogue generation (Li et al., 2016a), Visual Dialogue (Das et al., 2017; Strub et al., 2017). All these dialogue agents use either deep Q-Network (Mnih et al., 2015) or policy gradient (Williams, 1992) algorithms.\nCompared to single-domain dialogue systems, multi-domain dialogue systems have larger state and action spaces, most often contributing to further challenges such as sparse rewards or long horizon (trajectory). These can be challenging for flat reinforcement learning methods such as deep Q-Network. A flat Q-Network that follows a naive exploration strategy, might further end up with high sample complexity. Above all, multi-domain task-completion dialogues are intrinsically hierarchical in structure and hence we argue that learning a multi-domain dialogue system should adopt a structural learning strategy. In fact, several work have investigated along this direction (Cuaya\u0301huitl, 2009; Gasic et al., 2015b). They either used tabar X\niv :1\n70 4.\n03 08\n4v 1\n[ cs\n.C L\n] 1\n0 A\npr 2\n01 7\nular methods (which is hardly scalable when state space is huge and combinatorial) or treated multidomain dialogue as multiple independent singledomain sub-dialogues, which are trained separately.\nIn light of these issues, we propose to use hierarchical reinforcement learning for a tailored multidomain dialogue, called Composite-domain dialogue. Composite-domain is a special case of multi-domain setting in which there exist slot constrains across domains. For example, in a travel scenario, it may consist of three tasks: flight ticket booking, hotel reservation and car rental. It adopts commonsense knowledge such as a hotel check-in time should be later than the departure flight time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people and so on. In this situation, network of Deep Q-Networks are not feasible since they are trained domain by domain, ignore the constraints among domains. Hence, we approach this composite-domain taskcompletion dialogue problem with hierarchical deep reinforcement learning. Hierarchical-DQN (h-DQN) has hierarchical value functions operating at different temporal scales, the top-level one is termed as meta-controller that learns policy over subgoals, and controller (low-level DQN) learns a dialogue policy to achieve the selected subgoal.\nAnother interesting aspect of multi-domain dialogue setting is that, different users may show differing behavior patterns. For example, some people like to book their flight tickets first, then reserve a hotel; others might prefer to reserve their hotel first, then book flight tickets. To verify that our models are robust to these different settings, we test our agents with three different types of users. Empirical experiments in both user simulations and real-user evaluation demonstrate that the hierarchical-DQN agents significantly outperform the DQNs yielding higher task success rates, higher rewards and fewer dialogue turns, and as evaluated by human judges, a more coherence in dialogue flow. In addition, h-DQN shows strong adaptation capabilities to different types of users. Our contributions are three-fold:\n\u2022 The proposed system is the first hierarchical deep reinforcement learning agent in a composite task-completion dialogue setting.\n\u2022 We demonstrate that hierarchical deep reinforcement learning models can significantly\noutperform flat reinforcement learning algorithms on a complicated, composite taskcompletion dialogue task in both simulations and human evaluation.\n\u2022 Hierarchical deep reinforcement learning agents show strong adaptation capabilities to customize with different types of users in both simulations and human evaluation."}, {"heading": "2 Related work", "text": "Task-completion dialogue system has attracted numerous research efforts. Reinforcement learning algorithms offer possibilities to optimize dialogue policies over time with experience (Scheffler and Young, 2000; Levin et al., 2000; Young et al., 2013; Williams et al., 2017). Recent advances in deep learning have inspired many deep reinforcement learning based dialogue systems: Cuaya\u0301huitl (2017) proposed a simple deep reinforcement learning dialogue system which learns to map directly from raw text to dialogue policy. Li et al. (2017a) proposed an end-to-end task-completion dialogue system, which learned dialogue policy via deep Q-Network and demonstrated superior performance over rule-based agent. Su et al. (2016) described a two-phase approach for taskoriented spoken dialogue system, first trained with supervised learning, then optimized via reinforcement learning. Williams et al. (2017) provided hybrid code networks which allow the user to input domain-specific knowledge and optimize policies with supervised learning and reinforcement learning.\nAll the above work focus on a single-domain problem, while it is non-trivial to scale up to a multi-domain, complex dialogue problem in which several potential issues exist, for example, the state and action spaces are much larger, the sparse reward and long horizon, high sample complexity demands problems will hinder the agents to learn good policies effectively and efficiently. Multi-policy model for multi-domain dialogue has been studied in (Lison, 2011) where several policies are concurrently yield actions and a heuristic algorithm is used to determinate which action should be taken. Gasic et al. (2015a) investigated a distributed architecture which firstly trains a generic policy on data from all the domains and then specializes with only in-domain data. Gasic et al. (2015b) presented policy committee for adaptation in multi-domain dialogue\nsystem based on Bayesian committee machine. It consists of a set of committee policies trained with different datasets and each committee proposes an action at each time, a data-driven combination method is then applied to take the final decision. Cuaya\u0301huitl et al. (2016) presented Network of Deep Q-Networks for multi-domain dialogue system, which has a set of DQNs, and each DQN is trained individually for each domain, and a SVM classifier is used for domain selection, assigning the corresponding domain DQN policy.\nFlat reinforcement learning algorithms suffer the curse of dimensionality when the complexity of task is increased and state representation is not compact. Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003). Options framework exploits temporal abstraction that does not necessarily make decision at each time but rather invoke temporally extended actions until termination (Sutton et al., 1998). MAXQ gives a hierarchical decomposition of value functions of the original problem into a set of value functions of subproblems (Dietterich, 2000). Parr and Russell (1997) presented Hierarchies of Machines where policies are constrained by hierarchies of partially specified machines, allowing for the use of prior knowledge to reduce search spaces.\nOur work is motivated by the hierarchical-DQN (Kulkarni et al., 2016) which integrates hierarchical value functions to operate at different temporal scales. The model achieved superior performance on a complicated ATARI game \u201cMontezuma\u2019s Revenge\u201d with a hierarchical structure. Compared with (Cuaya\u0301huitl et al., 2016) which performs the similar task, we have two levels of DQN, the toplevel one is a meta-controller for subgoal selection, the low-level DQN is a controller for dialogue policy learning to achieve the subgoal. Our model jointly learns subgoal selection and subgoal policies while their work uses a pretrained SVM domain classifier. In addition, rather than learning individual DQN policy for each domain separately, our model can jointly learn dialogue policies for multiple domains considering the joint constraints between domains."}, {"heading": "3 Neural Dialogue System", "text": "As illustrated in Figure 1, a typical taskcompletion dialogue system consists of the following components:\nNatural Language Understanding (NLU): given the utterances of free texts (typed or spoken), the major task of NLU is to automatically classify the domain of a user query along with domain specific intents and fill in a set of slots to form a structured semantic frame. For example, \u201cmy departure city is San Francisco, and destination is Seattle.\u201d, maps to the dialogue\naction form \u201cinform(or city=San Francisco, dst city=Seattle)\u201d.\nDialogue Management (DM): The symbolic LU output is passed to the DM in a dialogue act form (or semantic frame). The classic DM includes two stages, dialogue state tracking and policy learning. Given the LU symbolic output, a query is formed to interact with the database to retrieve the available result. The state tracker will keep tracking the evolving dialogue states and be updated based on the available result from the database, history dialogue turns, and the latest user dialogue action. Then the tracker prepares a state representation for the policy learning, including the latest user action, latest agent action, database results, turn information, and history dialogue turns, etc. Conditioned on the state input from the state tracker, policy learning is to generate the next available system action \u03c0(a|s).\nNatural Language Generation (NLG): Given a dialogue action form, the NLG module generates natural language texts. Two common approaches exist: rule-based and model-based. Rule-base NLG may generate rigid response, and cannot be easily scaled to multiple domains etc.. The current state-of-the-art NLG model is a semantically conditioned LSTM-based generator (Wen et al., 2015) which takes the delexicalized dialog act form to generate the template-like sentence sketch, then a post scan is to replace the delexicalized symbols with real slot values."}, {"heading": "4 Policy Learning Methodology", "text": "In this work, we focus on the policy learning. We formalize the task as a Markov Decision Process (MDP) problem, which consists of a set of states s \u2208 S , a set of actions a \u2208 A, a state transition function T (s, a, s\u2032) which indicates the probability of next state s\u2032 given the current state s and action a, a reward function R(s, a, s\u2032) which means the scalar reward that agent received from the environment based on choosing action a from state s resulting in a new state s\u2032, and \u03b3 \u2208 [0, 1] as a discount factor. The goal is to learn a policy \u03c0(a|s) which can maximize a certain reward function over long period of time.\nTo learn the interaction policy of the dialogue system, we apply reinforcement learning algorithms to policy training in an end-to-end fashion."}, {"heading": "4.1 Deep Q-Network", "text": "First, the policy is represented as a deep QNetwork (DQN) (Mnih et al., 2015), which takes the state st from the state tracker as input, and outputs estimated Q(st, a; \u03b8) for all actions a. The agent aims at finding an optimal policy by maximizing following cumulative discounted reward:\nQ\u03c0(s, a) = E { +\u221e\u2211 k=0 \u03b3krt+k|st = s, at = a } (1)\nwhere function Q denotes the expected return of rewards at time t by a discount factor \u03b3 at each time step. The Q-learning algorithm solves the reinforcement learning problems by approximating the optimal Q-function Q\u2217(st, at) using following updating.\nQ(st, at)\u2190 Q(st, at)+ \u03bbt(rt + \u03b3maxaQ(st+1, a)\u2212Q(st, at)) (2)\nwhere \u03bbt is the learning rate at time t.\n4.2 Hierarchical Deep Q-Network\nFlat reinforcement learning algorithms like deep Q-Network suffer several issues for the complicated tasks, like larger state and action spaces, sparse reward and long horizon, high sample complexity demands problems, these may hinder the agents to find a good policy, cannot scale to the multi-domain dialogue seamlessly. In this composite task-completion dialogue task, it might also break the coherence of the conversation with frequent domain transition. Distinguished from single-domain dialogue, one of unique characteristics of composite-domain or multi-domain dialogue is its intrinsic domain structure information,\nwhich asks for models to capture this structure to maintain the coherence of the conversation. Thus, inspired by the work of hierarchical deep reinforcement learning(Kulkarni et al., 2016), we investigate hierarchical reinforcement learning algorithms in the composite task-completion dialogue setting.\nComposition-domain task-completion dialogue can be viewed as a dialogue with strong hierarchical structure: first pick a domain, and solve the user\u2019s requirements within this domain, then switch to another domain till the end of the entire task. Hence, as shown in Figure 2, we propose a two-level hierarchical deep reinforcement learning agent which consists of a meta-controller and a controller. The meta-controller perceives state st from environment and select a subgoal gt \u2208 G where G in this composite-domain dialogue scenario means the set of all possible domains. Whereafter, the controller selects a primitive dialogue action at given state st and subgoal gt. The subgoal gt remains as a condition until it is achieved or a terminal state is reached. The internal critics mechanism provides the controller with an appropriate intrinsic reward rit(gt), indicating weather the subgoal is reached. Thus the controller aims at maximizing cumulative intrinsic reward: Rt(g, s, a) = E +\u221e\u2211 k=0 { \u03b3krit+k|st = s, at = a, gt = g } (3) where rit+k denotes the reward received from internal critics.\nThe meta-controller then aims at maximizing the cumulative extrinsic reward:\nRt(s, g) = E +\u221e\u2211 k=0 { \u03b3kret+k|st = s, gt = g } (4)\nwhere ret+k is reward received from the environment.\nThe policy \u03c0g for both meta-controller and \u03c0a,g for the controller can be learned with deep Qlearning algorithm. To be more specific, the metacontroller estimates the Q-value function:\nQ\u22171(s, g) = max\u03c0gE[ N\u2211 k=0 ret+k+\n\u03b3maxg\u2032Q \u2217 1(st+N , g \u2032)|st = s, gt = g]\n(5)\nwhere N is the number of steps that the controller needs to reach the subgoal. g\u2032 is the agent\u2019s next\nsubgoal in state st+N . Similarly, the controller estimates the following Q-value function:\nQ\u22172(s, a, g) = max\u03c0a,gE[rit+ \u03b3maxat+1Q \u2217 2(st+1, at+1, g)|st = s, gt = g] (6)\nBothQ\u22171(s, g) andQ \u2217 2(s, a, g) are represented with neural network, non-linear functions approximators Q1(s, g; \u03b81) and Q2(s, a, g; \u03b82) respectively. The meta-controller is trained by minimizing a sequence of loss function at iteration i\nL1(\u03b81,i) = E(s,g,re,s\u2032)\u223cD1 [(y 1 i \u2212Q1(s, g; \u03b81,i))2] (7)\ny1i = r e + \u03b3maxg\u2032Q1(s \u2032, g\u2032, \u03b81,i\u22121) (8)\nThe controller is trained by minimizing a sequence of loss function at iteration i\nL2(\u03b82,i) = E(s,g,a,ri,s\u2032)\u223cD2 [(y 2 i \u2212Q2(s, g, a; \u03b82,i))2] (9)\ny2i = r i + \u03b3maxa\u2032Q2(s \u2032, g, a\u2032, \u03b82,i\u22121) (10)\nWe use SGD to minimize above loss function according to following gradient:\n\u2207\u03b81,iL1(\u03b81,i) = E(s,g,re,s\u2032)\u223cD1 [(r e+\n\u03b3maxg\u2032Q2(s \u2032, g\u2032, \u03b81,i\u22121)\u2212Q1(s, g, \u03b81,i))\n\u2207\u03b81,iQ1(s, g, \u03b81,i)] (11)\nSimilarly we can write the gradient for controller network:\n\u2207\u03b82,iL2(\u03b82,i) = E(s,g,a,ri,s\u2032)\u223cD2 [(r i+\n\u03b3maxa\u2032Q2(s \u2032, g, a\u2032, \u03b82,i\u22121)\u2212Q2(s, g, a, \u03b82,i))\n\u2207\u03b82,iQ2(s, g, a, \u03b82,i)] (12)\nTwo important tricks, target networks and experience replay are employed to train h-DQN agent as well. Experience replay tuples (s, g, re, s\u2032) and (s, g, a, ri, s\u2032) are sampled from experience replay buffer D1 and D2 respectively. A full summary of the learning algorithm for h-DQN agent is given in Appendix C."}, {"heading": "5 Experiments and Results", "text": "To evaluate proposed model, we conduct experiments on a composite-domain task-completion dialogue setting which assists user to book flight tickets and hotel reservation."}, {"heading": "5.1 User Simulator", "text": "In order to learn a good policy, reinforcement learning models typically need a lot of samples to explore the policy space. However, in reality it is time-consuming and costly to obtain such a large dialogue corpus (Pietquin et al., 2011). It is common to use simulator not only in game agent learning scenario (Mnih et al., 2015) but also in dialogue research community (Asri et al., 2016; Schatzmann et al., 2007). In this work, we extend a public user simulator (Li et al., 2016b) to composite-domain task-completion dialogue task. During training, the simulator provides the agent with reward signal both at each turn and the end of one dialogue. The dialogue is considered successful only when the tickets for both flight and hotel are booked, and the information provided by the agent satisfies user\u2019s constrains. At the end of each dialogue, the agent receives a positive reward 2 \u2217 max turn for success and a negative reward \u2212max turn for failure. Otherwise, it receives a penalty reward of \u22121 which is to encourage short dialogues. Details about the simulator are included in Appendix A."}, {"heading": "5.2 Dataset", "text": "The raw data we used in this research is from a public multi-domain dialogue corpus1 (El Asri et al., 2017) which consists of 1369 human-human dialogues collected in a Wizard-of-Oz setting. We made some changes in this multi-domain dialogue data to customize it into a composite-domain taskcompletion dialogue setting: 1). all the dialogues contain two domains; 2). add some joint slot constraints between domains; 3). add preferences (soft constraints) for different users.\nUser Goal We generate the user goals from this corpus by extracting all the slots that appear within a dialogue session, regardless of the slot value is\n1https://datasets.maluuba.com/Frames\nknow or unknown. Thus, some slots may have multiple values, like \u201cor city=[San Francisco, San Jose]\u201d, which means that or city is soft constraint, user may revise the slot value to explore different possibilities in the course of dialogue. If the slot has only one value, it is a hard constraint, which means it is a well defined requirement in user\u2019s mind. Besides, we also filter out a number of user goals in which some slot values can not be searched in our database. Finally we get a set of user goals with 759 entries, and every user goal has two domains: flight and hotel.\nUser Type In addition, we created three types of user goals which we term as Type A, Type B and Type C. Description about each type is as follows:\n\u2022 Type A: All the informed slot of the user goal has single value, the user simulator does not have preference for which domain should be firstly solved.\n\u2022 Type B: One of informed slots of the flight domain in the user goal has multiple values, the user simulator has preference on flight domain. If the user simulator receives \u201cno ticket available\u201d from the agent in the conversation, the user simulator should revise the slot to explore the possibility with alternative value.\n\u2022 Type C: Similar to Type B. One of informed slots of the hotel domain in the user goal has multiple values, the user simulator has preference on hotel domain. If the user simulator receives \u201cno ticket available\u201d from the agent in the conversation, the user simulator should revise the slot with alternative value."}, {"heading": "5.3 Dialogue Policy", "text": "The job of dialogue policy is to select the next system action at based on the current state st. Besides the proposed hierarchical Q-Network policy, we\nalso present two baseline policies: a hand-crafted rule and DQN.\n1. Rule agent uses sophisticated hand-crafted dialogue policies which request and inform the necessary slots, and then inform user tickets.\n2. DQN agent is a standard deep Q-Network model which learns dialogue policies only with extrinsic reward.\n3. h-DQN agent is a two-level hierarchical QNetwork which learns the dialogue policies with extrinsic and intrinsic rewards."}, {"heading": "5.4 Implementation", "text": "For DQN, we set hidden layer size of 80. For h-DQN, both meta-controller and controller has a hidden layer size of 80 as well. RMSprop with standard hyperparmeters is applied to optimize both DQN and h-DQN parameters and we set batch size of 16. During training, we use - greedy strategy for exploration. For each simulation epoch, we simulate 100 dialogues and store these state transition tuples in experience replay buffers. At the end of each simulation epoch, the models will be updated with all the tuples in the buffers in a batch manner.\nThe experience replay strategy plays a critical role for the success of deep reinforcement learning. In our experiments, at the beginning, we use rule agent to run N (N=100) dialogues to populate the experience replay buffers, which is sort of imitation learning to warm up the RL agent, then the RL agent accumulates all the state transition tuples and flush the replay buffer only when the current RL agent can reach a success rate threshold (i.e. which is equivalent with the performance the rule agent). The motivation behind this strategy is the initial performance of RL agent is not\ngood enough to provide reasonable transition tuples. Hence, we provide some rule-based examples and do not flush the buffer until the performance of the model reaches an acceptable level. Generally, we set the threshold as the success rate of a rule agent. To be a fair comparison, for the same type of users, we use the same rule-based agent to warm up both DQN and h-DQN."}, {"heading": "5.5 Simulated User Evaluation", "text": "In this composite task-completion dialogue task, we compare the proposed agent with the baseline agents in terms of three metrics: success rate2, average rewards, and the average number of turns.\nFigure 3 shows the learning curves for all three agents trained on different types of users, each learning curve is averaged by 10 runs. For all three types of users, reinforcement learning based agents find better dialogue policies than handcrafted rule agent: DQN and h-DQN achieves higher success rate and needs less turns to achieve users\u2019 goal than rule agent. For type B and C users who may need to go back to revise some slots during the dialogue, DQN\u2019s performance drops by a large margin due to the fact that the task complexity increases. It requires more dialogue turns which poses a challenge for credit assignment. In all three types of user simulations, 1). h-DQN can significantly outperform DQN, this is attributed to the hierarchical structure of the proposed agent: the subgoal selected by the metacontroller can guide the agent to focus on a certain subgoal, which also improves the coherence of the dialogue flow, and together with the intrinsic reward, these will alleviate the sparse reward and long horizons issues, aid the agent to explore in the action space. These may interpret why the per-\n2Success rate is the fraction of dialogues that accomplish the tasks successfully within the maximum turn.\nformance of h-DQN on type B, C users does not drop too much, though the search space increases. 2). h-DQN learns much faster than DQN, can find better dialogue policy much quickly than DQN in terms of the number of simulation epochs, which means hierarchical DQN is much sample-efficient, can reduce the sample complexity for the entire complex task."}, {"heading": "5.6 Real User Evaluation", "text": "We further evaluate models that trained on simulation data with real users. Due to the budget limitation, we only conducted the study on two user types: Type A who does not have any preference to domain and Type B who has preference to flight domain. In total, we have four agents to test in this study: DQN A, h-DQN A and DQN B, h-DQN B.\nFor each dialogue, one of the agents was randomly picked to converse with the user. The user was presented with a sampled goal from our corpus, if one of the slots in the sampled goal has multiple values, it means the user has multiple choices for this slot and the user may revise the slot value when the agent replies with \u201cNo ticket available\u201d during the conversation. At the end of dialogue, the user was asked to give a rating on a scale from 1 to 5 based on the naturalness, coherence of the dialogue. (1 is the worst rating, 5 is the best one). Totally we collected 225 dialogues. Figure 4 presents the performance of these agents with real users in terms of success rate. Figure 5 shows a comparison of user rating. For all the cases, h-DQN is consistently better than DQN in terms of success rate, user rating."}, {"heading": "6 Discussion and Conclusion", "text": "This work presents an end-to-end composite taskcompletion dialogue system, we show that hierarchical deep reinforcement learning agent (hDQN) can significantly outperform flat deep reinforcement learning (DQN) and rule-based agents in both simulations and real users evaluation, the hierarchical structure of the agent also improves the coherence in the multi-domain dialogue flow. Furthermore, hierarchical reinforcement learning algorithms demonstrate strong capabilities to reduce sample complexity to accelerate the training on complicated tasks with intrinsic hierarchical structure; and also show strong adaptation abilities to customize with different types of users, which would be a good direction to further explore the personalization of hierarchical reinforcement learning agents at composite-domain taskcompletion dialogue setting. Both DQN and hDQN agents in this work use feedforward neural networks, given the observation that we found occasionally the agents might ask the repeated questions, introducing recurrent architecture for neural networks may help to alleviate this issue and save more turns in the dialogue. Moreover, the best agent on this task is a two-level h-DQN, while a lot of complex tasks have multi-level hierarchies, it is not straightforward to extend a two-level hierarchy to multi-level one, and deriving such hierarchy usually asks for domain-specific knowledge, thus another exciting direction for future work is to introduce a model which can automatically learn the subtask hierarchy of the complex problems."}, {"heading": "A User Simulator", "text": "User Goal In the task-completion dialogue setting, the first step of user simulator is to generate a feasible user goal. Generally a user goal is defined with two types of slots: request slots that user does not know the value and expects the agent to provide it through the conversation; inform slots that the slot-value pairs that user know in the mind, serves as soft/hard constraints in the dialogue; slots that have multiple values are termed as soft constraints, which means user has preference, and user might change its value when there is no result returned from the agent based on the current value; otherwise, slots that are with only one value serve as hard constraint. Table 2 shows an example of user goal in the composite task-completion dialogue.\nFirst User Act This work focuses on userinitiated dialogues, so we randomly generated a user action as the first turn (a user turn). To make the first user-act more reasonable, we add some constraints in the generation process. For example, the first user turn can be inform or request turn; it has at least two informable slots, if the user\nknows the original and destination cities, or city and dst city will appear in the first user turn etc.; If the intent of first turn is request, it will contain one requestable slot.\nDuring the course of a dialogue, the user simulator maintains a compact stack-like representation named as user agenda (Schatzmann and Young, 2009), where the user state su is factored into an agenda A and a goal G, which consists of constraints C and request R. At each timestep t, the user simulator will generate the next user action au,t based on the its current status su,t and the last agent action am,t\u22121, and then update the current status s\u2032u,t. Here, when training or testing a policy without natural language understanding (NLU) module, an error model (Li et al., 2017b) is introduced to simulate the noise from the NLU component, and noisy communication between the user and agent."}, {"heading": "B Sample Dialogues", "text": "Table 3 shows one sample dialogue generated by DQN and h-DQN agents interacting with real user. To be informative, we explicitly show the user goal at the head of dialogue, this is to just help the user to accomplish this goal and book the right tickets; actually during the conversation the agent knows nothing about the user goal."}, {"heading": "C Algorithms", "text": "Algorithm 1 outlines the full procedure for training hierarchical-DQN in this composite taskcompletion dialogue system.\nAlgorithm 1 Learning algorithm for h-DQN agent in composite task-completion dialogue 1: Initialize experience replay buffer D1 for meta-controller and D2 for controller. 2: Initialize Q1 and Q2 network with random weights. 3: Initialize dialog simulator and load knowledge base. 4: for episode=1:N do 5: Restart dialog simulator and get state description s 6: while s is not terminal do 7: extrinsic reward := 0 8: s0 := s 9: select an subgoal g based on probability distribution \u03c0(g|s) and exploration probability g 10: while s is not terminal and subgoal g not achieved do 11: select an action a based on the distribution \u03c0(a|s, g) and exploration probability c 12: Execute action a, obtain next state description s\u2032, perceive extrinsic reward re from environment 13: Obtain intrinsic reward ri from internal critic 14: Sample random minibatch of transitions from D1\n15: y = { ri ifs\u2032 is terminal ri + \u03b3 \u2217maxa\u2032Q1({s\u2032, g}, a\u2032; \u03b81) oterwise 16: Perform gradient descent on loss L(\u03b81) according to equation 11 17: Store transition({s,g},a,ri,{s\u2032,g}) in D1 18: Sample random minibatch of transitions from D2\n19: y = { re ifs\u2032 is terminal re + \u03b3 \u2217maxa\u2032Q2(s\u2032, g\u2032, a\u2032; \u03b82) oterwise 20: Perform gradient descent on loss L(\u03b82) according to equation 12 21: extrinsic reward += re 22: s = s\u2032 23: end while 24: Store transition (s0, g, extrinsic reward, s\u2032) in D2 25: if s is not terminal then 26: select an action a based on the distribution \u03c0(a|s, g) and exploration probability c 27: end if 28: end while 29: end for"}], "references": [{"title": "A sequence-to-sequence model for user simulation in spoken dialogue systems", "author": ["Layla El Asri", "Jing He", "Kaheer Suleman."], "venue": "Nelson Morgan, editor, Interspeech 2016. ISCA, pages 1151\u20131155. https://doi.org/10.21437/Interspeech.2016-1175.", "citeRegEx": "Asri et al\\.,? 2016", "shortCiteRegEx": "Asri et al\\.", "year": 2016}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G. Barto", "Sridhar Mahadevan."], "venue": "Discrete Event Dynamic Systems 13(1-2):41\u2013", "citeRegEx": "Barto and Mahadevan.,? 2003", "shortCiteRegEx": "Barto and Mahadevan.", "year": 2003}, {"title": "Hierarchical reinforcement learning for spoken dialogue systems", "author": ["Heriberto Cuay\u00e1huitl"], "venue": null, "citeRegEx": "Cuay\u00e1huitl.,? \\Q2009\\E", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2009}, {"title": "SimpleDS: A simple deep reinforcement learning dialogue system", "author": ["Heriberto Cuay\u00e1huitl."], "venue": "Dialogues with Social Robots, Springer, pages 109\u2013118.", "citeRegEx": "Cuay\u00e1huitl.,? 2017", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2017}, {"title": "Deep reinforcement learning for multi-domain dialogue systems", "author": ["Heriberto Cuay\u00e1huitl", "Seunghak Yu", "Ashley Williamson", "Jacob Carse."], "venue": "arXiv preprint arXiv:1611.08675 https://arxiv.org/abs/1611.08675.", "citeRegEx": "Cuay\u00e1huitl et al\\.,? 2016", "shortCiteRegEx": "Cuay\u00e1huitl et al\\.", "year": 2016}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 MF Moura", "Stefan Lee", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1703.06585 https://arxiv.org/abs/1703.06585.", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng."], "venue": "arXiv preprint arXiv:1609.00777 https://arxiv.org/abs/1609.00777.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G. Dietterich."], "venue": "J. Artif. Intell. Res. (JAIR) 13:227\u2013 303. https://doi.org/10.1613/jair.639.", "citeRegEx": "Dietterich.,? 2000", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Frames: A corpus for adding memory to goal-oriented dialogue systems", "author": ["Layla El Asri", "Hannes Schulz", "Shikhar Sharma", "Jeremie Zumer", "Justin Harris", "Emery Fine", "Rahul Mehrotra", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1704.00057", "citeRegEx": "Asri et al\\.,? 2017", "shortCiteRegEx": "Asri et al\\.", "year": 2017}, {"title": "Distributed dialogue policies for multi-domain statistical dialogue management", "author": ["Milica Gasic", "Dongho Kim", "Pirros Tsiakoulis", "Steve J. Young."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Gasic et al\\.,? 2015a", "shortCiteRegEx": "Gasic et al\\.", "year": 2015}, {"title": "Policy committee for adaptation in multidomain spoken dialogue systems", "author": ["Milica Gasic", "Nikola Mrksic", "Pei-hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve J. Young."], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and", "citeRegEx": "Gasic et al\\.,? 2015b", "shortCiteRegEx": "Gasic et al\\.", "year": 2015}, {"title": "Deep reinforcement learning with a natural language action space", "author": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."], "venue": "Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D. Kulkarni", "Karthik Narasimhan", "Ardavan Saeedi", "Josh Tenenbaum."], "venue": "Daniel D. Lee, Masashi Sugiyama, Ulrike von", "citeRegEx": "Kulkarni et al\\.,? 2016", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "IEEE Trans. Speech and Audio Processing 8(1):11\u201323. https://doi.org/10.1109/89.817450.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel."], "venue": "Journal of Machine Learning Research 17(39):1\u201340.", "citeRegEx": "Levine et al\\.,? 2016", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "End-to-end task-completion neural dialogue systems", "author": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao."], "venue": "arXiv preprint arXiv:1703.01008 https://arxiv.org/abs/1703.01008.", "citeRegEx": "Li et al\\.,? 2017a", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Investigation of language understanding impact for reinforcement learning based dialogue systems", "author": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz."], "venue": "arXiv preprint arXiv:1703.07055 https://arxiv.org/abs/1703.07055.", "citeRegEx": "Li et al\\.,? 2017b", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "A user simulator for task-completion dialogues", "author": ["Xiujun Li", "Zachary C Lipton", "Bhuwan Dhingra", "Lihong Li", "Jianfeng Gao", "Yun-Nung Chen."], "venue": "arXiv preprint arXiv:1612.05688 https://arxiv.org/abs/1612.05688.", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Multi-policy dialogue management", "author": ["Pierre Lison."], "venue": "Proceedings of the SIGDIAL 2011. The Association for Computer Linguistics, pages 294\u2013 300. http://www.aclweb.org/anthology/W11-2033.", "citeRegEx": "Lison.,? 2011", "shortCiteRegEx": "Lison.", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis."], "venue": "Nature 518(7540):529\u2013533. https://doi.org/10.1038/nature14236.", "citeRegEx": "Antonoglou et al\\.,? 2015", "shortCiteRegEx": "Antonoglou et al\\.", "year": 2015}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas D. Kulkarni", "Regina Barzilay."], "venue": "Llu\u0131\u0301s M\u00e0rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Proceed-", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart J. Russell."], "venue": "Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances in Neural Information Processing Systems 10, [NIPS Conference, Denver,", "citeRegEx": "Parr and Russell.,? 1997", "shortCiteRegEx": "Parr and Russell.", "year": 1997}, {"title": "Sampleefficient batch reinforcement learning for dialogue management optimization", "author": ["Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan", "Herv\u00e9 Frezza-Buet."], "venue": "TSLP 7(3):7:1\u20137:21. https://doi.org/10.1145/1966407.1966412.", "citeRegEx": "Pietquin et al\\.,? 2011", "shortCiteRegEx": "Pietquin et al\\.", "year": 2011}, {"title": "Agendabased user simulation for bootstrapping a POMDP dialogue system", "author": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve J. Young."], "venue": "Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, ed-", "citeRegEx": "Schatzmann et al\\.,? 2007", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2007}, {"title": "The hidden agenda user simulation model", "author": ["Jost Schatzmann", "Steve Young."], "venue": "IEEE transactions on audio, speech, and language processing 17(4):733\u2013747.", "citeRegEx": "Schatzmann and Young.,? 2009", "shortCiteRegEx": "Schatzmann and Young.", "year": 2009}, {"title": "Probabilistic simulation of human-machine dialogues", "author": ["Konrad Scheffler", "Steve J. Young."], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP 2000, 59 June, 2000, Hilton Hotel and Convention Cen-", "citeRegEx": "Scheffler and Young.,? 2000", "shortCiteRegEx": "Scheffler and Young.", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Kalchbrenner", "Ilya Sutskever", "Timothy P. Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature 529(7587):484\u2013489.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Reinforcement learning with a hierarchy of abstract models", "author": ["Satinder P. Singh."], "venue": "William R. Swartout, editor, Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16, 1992..", "citeRegEx": "Singh.,? 1992", "shortCiteRegEx": "Singh.", "year": 1992}, {"title": "End-to-end optimization of goal-driven and visually grounded dialogue systems", "author": ["Florian Strub", "Harm de Vries", "Jeremie Mary", "Bilal Piot", "Aaron Courville", "Olivier Pietquin."], "venue": "arXiv preprint arXiv:1703.05423 https://arxiv.org/abs/1703.05423.", "citeRegEx": "Strub et al\\.,? 2017", "shortCiteRegEx": "Strub et al\\.", "year": 2017}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "arXiv preprint arXiv:1606.02689", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Intra-option learning about temporally abstract actions", "author": ["Richard S. Sutton", "Doina Precup", "Satinder P. Singh."], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 24-27, 1998.", "citeRegEx": "Sutton et al\\.,? 1998", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder P. Singh."], "venue": "Artif. Intell. 112(1-2):181\u2013211. https://doi.org/10.1016/S0004-3702(99)00052-1.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Peihao Su", "David Vandyke", "Steve J. Young."], "venue": "Proceedings of the 2015 Con-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "author": ["Jason D Williams", "Kavosh Asadi", "Geoffrey Zweig."], "venue": "arXiv preprint arXiv:1702.03274 .", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Jason D. Williams", "Steve J. Young."], "venue": "Computer Speech & Language 21(2):393\u2013422. https://doi.org/10.1016/j.csl.2006.06.008.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Still talking to machines (cognitively speaking)", "author": ["Steve J. Young."], "venue": "Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Young.,? 2010", "shortCiteRegEx": "Young.", "year": 2010}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve J. Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams."], "venue": "Proceedings of the IEEE 101(5):1160\u20131179. https://doi.org/10.1109/JPROC.2012.2225812.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["Tiancheng Zhao", "Maxine Esk\u00e9nazi."], "venue": "Proceedings of the SIGDIAL 2016 Conference, The 17th Annual Meeting of the Special", "citeRegEx": "Zhao and Esk\u00e9nazi.,? 2016", "shortCiteRegEx": "Zhao and Esk\u00e9nazi.", "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "However, these personal assistants can only accomplish simple tasks, still far behind being able to handle complex tasks (Young, 2010; Williams and Young, 2007).", "startOffset": 121, "endOffset": 160}, {"referenceID": 35, "context": "However, these personal assistants can only accomplish simple tasks, still far behind being able to handle complex tasks (Young, 2010; Williams and Young, 2007).", "startOffset": 121, "endOffset": 160}, {"referenceID": 21, "context": ", 2016), text games (Narasimhan et al., 2015; He et al., 2016) and robotics (Schulman et al.", "startOffset": 20, "endOffset": 62}, {"referenceID": 11, "context": ", 2016), text games (Narasimhan et al., 2015; He et al., 2016) and robotics (Schulman et al.", "startOffset": 20, "endOffset": 62}, {"referenceID": 6, "context": "info-bot for information access (Dhingra et al., 2016), end-to-end task completion agent (Zhao and Esk\u00e9nazi, 2016; Li et al.", "startOffset": 32, "endOffset": 54}, {"referenceID": 39, "context": ", 2016), end-to-end task completion agent (Zhao and Esk\u00e9nazi, 2016; Li et al., 2017a), and open domain dialogue generation (Li et al.", "startOffset": 42, "endOffset": 85}, {"referenceID": 16, "context": ", 2016), end-to-end task completion agent (Zhao and Esk\u00e9nazi, 2016; Li et al., 2017a), and open domain dialogue generation (Li et al.", "startOffset": 42, "endOffset": 85}, {"referenceID": 15, "context": ", 2017a), and open domain dialogue generation (Li et al., 2016a), Visual Dialogue (Das et al.", "startOffset": 46, "endOffset": 64}, {"referenceID": 5, "context": ", 2016a), Visual Dialogue (Das et al., 2017; Strub et al., 2017).", "startOffset": 26, "endOffset": 64}, {"referenceID": 29, "context": ", 2016a), Visual Dialogue (Das et al., 2017; Strub et al., 2017).", "startOffset": 26, "endOffset": 64}, {"referenceID": 36, "context": ", 2015) or policy gradient (Williams, 1992) algorithms.", "startOffset": 27, "endOffset": 43}, {"referenceID": 2, "context": "In fact, several work have investigated along this direction (Cuay\u00e1huitl, 2009; Gasic et al., 2015b).", "startOffset": 61, "endOffset": 100}, {"referenceID": 10, "context": "In fact, several work have investigated along this direction (Cuay\u00e1huitl, 2009; Gasic et al., 2015b).", "startOffset": 61, "endOffset": 100}, {"referenceID": 2, "context": "Recent advances in deep learning have inspired many deep reinforcement learning based dialogue systems: Cuay\u00e1huitl (2017) proposed a simple deep reinforcement learning dialogue system which learns to map di-", "startOffset": 104, "endOffset": 122}, {"referenceID": 15, "context": "Li et al. (2017a) proposed an end-to-end task-completion dialogue system, which learned dialogue policy via deep Q-Network and demonstrated superior performance over rule-based agent.", "startOffset": 0, "endOffset": 18}, {"referenceID": 34, "context": "Williams et al. (2017) provided hybrid code networks which allow the user to input", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Multi-policy model for multi-domain dialogue has been studied in (Lison, 2011) where several policies are concurrently yield actions and a heuristic algorithm is used to determinate which action should be taken.", "startOffset": 65, "endOffset": 78}, {"referenceID": 9, "context": "Gasic et al. (2015a) investigated a distributed architecture which firstly trains a generic policy on data from all the domains and then specializes with only in-domain data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Gasic et al. (2015a) investigated a distributed architecture which firstly trains a generic policy on data from all the domains and then specializes with only in-domain data. Gasic et al. (2015b) presented policy committee for adaptation in multi-domain dialogue", "startOffset": 0, "endOffset": 196}, {"referenceID": 2, "context": "Cuay\u00e1huitl et al. (2016) presented Network of Deep Q-Networks for multi-domain dia-", "startOffset": 0, "endOffset": 25}, {"referenceID": 28, "context": "Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003).", "startOffset": 211, "endOffset": 296}, {"referenceID": 7, "context": "Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003).", "startOffset": 211, "endOffset": 296}, {"referenceID": 1, "context": "Hierarchical reinforcement learning methods alleviate the curse of dimensionality by integrating hierarchies, exponentially reduce computation cost, sample complexity and have better chance to find good polices (Sutton et al., 1999, 1998; Singh, 1992; Dietterich, 2000; Barto and Mahadevan, 2003).", "startOffset": 211, "endOffset": 296}, {"referenceID": 31, "context": "Options framework exploits temporal abstraction that does not necessarily make decision at each time but rather invoke temporally extended actions until termination (Sutton et al., 1998).", "startOffset": 165, "endOffset": 186}, {"referenceID": 7, "context": "archical decomposition of value functions of the original problem into a set of value functions of subproblems (Dietterich, 2000).", "startOffset": 111, "endOffset": 129}, {"referenceID": 12, "context": "Our work is motivated by the hierarchical-DQN (Kulkarni et al., 2016) which integrates hierarchi-", "startOffset": 46, "endOffset": 69}, {"referenceID": 7, "context": "archical decomposition of value functions of the original problem into a set of value functions of subproblems (Dietterich, 2000). Parr and Russell (1997) presented Hierarchies of Machines where policies are constrained by hierarchies of partially specified machines, allowing for the use of prior knowledge to reduce search spaces.", "startOffset": 112, "endOffset": 155}, {"referenceID": 4, "context": "Compared with (Cuay\u00e1huitl et al., 2016) which performs the similar task, we have two levels of DQN, the top-", "startOffset": 14, "endOffset": 39}, {"referenceID": 33, "context": "The current state-of-the-art NLG model is a semantically conditioned LSTM-based generator (Wen et al., 2015)", "startOffset": 90, "endOffset": 108}, {"referenceID": 12, "context": "Thus, inspired by the work of hierarchical deep reinforcement learning(Kulkarni et al., 2016), we investigate hierarchical reinforcement learning algorithms in the composite task-completion dialogue setting.", "startOffset": 70, "endOffset": 93}, {"referenceID": 23, "context": "However, in reality it is time-consuming and costly to obtain such a large dialogue corpus (Pietquin et al., 2011).", "startOffset": 91, "endOffset": 114}, {"referenceID": 18, "context": "In this work, we extend a public user simulator (Li et al., 2016b) to composite-domain task-completion dialogue task.", "startOffset": 48, "endOffset": 66}], "year": 2017, "abstractText": "In a composite-domain task-completion dialogue system, a conversation agent often switches among multiple sub-domains before it successfully completes the task. Given such a scenario, a standard deep reinforcement learning based dialogue agent may suffer to find a good policy due to the issues such as: increased state and action spaces, high sample complexity demands, sparse reward and long horizon, etc. In this paper, we propose to use hierarchical deep reinforcement learning approach which can operate at different temporal scales and is intrinsically motivated to attack these problems. Our hierarchical network consists of two levels: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by metacontroller and intrinsic rewards can guide the controller to effectively explore in the state-action space and mitigate the spare reward and long horizon problems. Experiments on both simulations and human evaluation show that our model significantly outperforms flat deep reinforcement learning agents in terms of success rate, rewards and user rating.", "creator": "LaTeX with hyperref package"}}}