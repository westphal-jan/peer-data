{"id": "1704.04341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Environment-Independent Task Specifications via GLTL", "abstract": "we generally propose a truly new task - specification language specific for markov decision processes that is designed to be an effective improvement criterion over reward functions by supposedly being statistical environment independent. tentatively the language is called a variant development of generic linear approximate temporal logic ( ltl ) that therefore is broadly extended to quantum probabilistic specifications in more a general way that permits approximations to be learned in virtually finite time. furthermore we provide several small environments classes that demonstrate explicitly the advantages of our unique geometric primitive ltl ( gltl ) python language simulation and illustrate : how it demonstrates can be later used to specify standard fuzzy reinforcement - specification learning tasks straightforwardly.", "histories": [["v1", "Fri, 14 Apr 2017 03:41:59 GMT  (108kb,D)", "http://arxiv.org/abs/1704.04341v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael l littman", "ufuk topcu", "jie fu", "charles isbell", "min wen", "james macglashan"], "accepted": false, "id": "1704.04341"}, "pdf": {"name": "1704.04341.pdf", "metadata": {"source": "CRF", "title": "Environment-Independent Task Specifications via GLTL", "authors": ["Michael L. Littman", "Ufuk Topcu", "Jie Fu", "Charles Isbell", "Min Wen", "James MacGlashan"], "emails": [], "sections": [{"heading": null, "text": "We propose a new task-specification language for Markov decision processes that is designed to be an improvement over reward functions by being environment independent. The language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications in a way that permits approximations to be learned in finite time. We provide several small environments that demonstrate the advantages of our geometric LTL (GLTL) language and illustrate how it can be used to specify standard reinforcementlearning tasks straightforwardly."}, {"heading": "1 Introduction", "text": "The thesis of this work is that (1) rewards are an excellent way of controlling the behavior of agents, but (2) rewards are difficult to use for specifying behaviors in an environment-independent way, therefore (3) we need intermediate representations between behavior specifications and reward functions.\nThe intermediate representation we propose is a novel variant of linear temporal logic that is modified to be probabilistic so as to better support reinforcement-learning tasks. Linear temporal logic has been used in the past to specify reward functions that depend on temporal sequences (Bacchus et al., 1996); here, we expand the role to provide a robust and consistent semantics that allows desired behaviors to be specified in an environment-independent way. Briefly, our approach involves the specification of tasks via temporal operators that have a constant probability of expiring on each step. As such, it bears a close relationship to the notion of discounting in standard Markov decision process (MDP) reward functions (Puterman, 1994).\nAt a philosophical level, we are viewing behavior specification as a kind of programming problem. That is, if we think of a Markov decision process (MDP) as an input, a\nreward function as a program, and a policy as an output, then reinforcement learning can be viewed as a process of program interpretation. We would like the same program to work across all possible inputs."}, {"heading": "1.1 Specifying behavior via reward functions", "text": "An MDP consists of a finite state space, action space, transition function, and reward function. Given an environment, an agent should behave in a way that maximizes cumulative discounted expected reward. The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009). A reinforcement-learning (RL) agent needs to learn to maximize cumulative discounted expected reward starting with an incomplete model of the MDP itself.\nFor \u201cprogramming\u201d reinforcement-learning agents, the state of the art is to define a reward function and then for the learning agent to interact with the environment to discover ways to maximize its reward. Reward-based specifications have proven to be extremely valuable for optimal planning in complex, uncertain environments (Russell & Norvig, 1994). However, we can show that reward functions, as they are currently structured, are very difficult to work with as a way of reliably specifying tasks. The best use case for reward functions is when the utilities of all actions and outcomes can be expressed in a consistent unit, for example, time or money or energy. In reality, however, putting a meaningful dollar figure on scuffing a wall or dropping a clean fork is challenging. When informally adding negative rewards to undesirable outcomes, it is difficult to ensure a consistent semantics over which planning and reasoning can be carried out. Further, reward values often need to be changed if the environment itself changes\u2014 they are not environment independent. Therefore, to get a system to exhibit a desired behavior, it can be necessary to try different reward structures and carry out learning multiple times in the target environment, greatly undermining the purpose of autonomous learning in the first place. ar X iv :1\n70 4.\n04 34\n1v 1\n[ cs\n.A I]\n1 4\nA pr\n2 01\nConsider the simple example MDP in Figure 1. The agent is choosing between a1 and a2 in the initial state s0. Choosing a1 causes the agent to pass through bad state b1 for one step, then to continue on to the goal g. Action a2, however, results in a probabilistic transition to s1 (with probability 1\u2212p) or bad state b2 (with slip probability p). From s1, the agent can continue on to the goal. If it reaches b2, it gets stuck there forever.\nLet\u2019s say our desired behavior is \u201cmaximize the probability of reaching g without hitting a bad state\u201d. (A bad state could be something like colliding with a wall or bumping up against a table.) The probability of success of a1 is zero and a2 is 1\u2212 p. Thus, for any 0 \u2264 p < 1, it is better to take action a2.\nWhat reward function encourages this behavior? For concreteness, let\u2019s assume a discount of \u03b3 = 0.8 and a reward of +1 for reaching the goal. We can assign bad states a value of \u2212r. In the case where p = 0.1, setting r > 0.16 encourages the desired behavior.\nConsider, though, what happens if the slip probability is p = 0.3. Now, there is no value of r for which a2 is preferred to a11. That is, it has become impossible to find a reward function that creates the correct incentives for the desired behavior to be optimal.\nThis example is perhaps a bit contrived, but we have observed the same phenomenon in large and natural state spaces as well. The reason for this result is that reward functions force us to express utility in terms of the discounted expected visit frequency of states. In this case, we are stuck trying to make a tradeoff between the certainty of encountering a bad state once and the possibility of encountering a bad state repeatedly. Since we are trying to maximize the probability of zero encounters with a bad state, the expected number of encounters is only useful for distinguishing zero from more than zero\u2014the objective cannot be translated into a reward function when bad states\n1Actually, r < \u221224/50 works for this example, but that is tantamount to rewarding the agent for bumping into things\u2014 something bound to result in other problems.\nare unavoidable."}, {"heading": "1.2 Specifying behavior via LTL", "text": "An alternative to specifying tasks via reward functions is to use a formal specification like linear temporal logic or LTL (Manna & Pnueli, 1992; Baier & Katoen, 2008).\nLinear temporal logic formulas are built up from a set of atomic propositions; the logic connectives: negation (\u00ac), disjunction (\u2228), conjunction (\u2227) and material implication (\u2192); and the temporal modal operators: next (\u00a9), always ( ), eventually (\u2666) and until (U). A wide class of properties including safety ( \u00acb), goal guarantee (\u2666g), progress ( \u2666g), response ( (b \u2192 \u2666g)), and stability (\u2666 g), where b and g are atomic propositions, can be expressed as LTL formulas. More complicated specifications can be obtained from the composition of such simple formulas. For example, the specification of \u201crepeatedly visit certain locations of interest in a given order while avoiding certain other unsafe or undesirable locations\u201d can be obtained through proper composition of simpler safety and progress formulas (Manna & Pnueli, 1992; Baier & Katoen, 2008).\nReturning to the example in Figure 1, the task is to avoid b states until g is reached: \u00acbUg. Given an LTL specification and an environment, an agent, for example, should adopt a behavior that maximizes the probability that the specification is satisfied. One advantage of this approach is its ability to specify tasks that cannot be expressed using simple reward functions (like the example MDP in Section 1.1). Indeed, in the context of reinforcement-learning problems, we have found it very natural to express standard MDP task specifications using LTL.\nStandard MDP tasks can be expressed well using these temporal operators. For example:\n\u2022 Goal-based tasks like mountain car (Moore, 1991): If p represents the attribute of being at the goal (the top of the hill, say), \u2666p corresponds to eventually reaching the goal.\n\u2022 Avoidance-type tasks like cart pole (Barto et al., 1983): If q represents the attribute of being in the failure state (dropping the pole, say), \u00acq corresponds to always avoiding the failure state.\n\u2022 Sequence tasks like taxi (Dietterich, 2000): If p represents some task being completed (getting the passenger, say) and q represents another task being completed (delivering the passenger, say), \u2666(p\u2227\u2666q) corresponds to eventually completing the first task, then, from there, eventually completing the second task.\n\u2022 Stabilizing tasks like pendulum swing up (Atkeson, 1994): If p represents the property that needs to be stabilized (the pendulum being above the vertical, say),\n\u2666 p corresponds to eventually achieving and continually maintaining the desired property.\n\u2022 Approach-avoid tasks like the 4\u00d73 grid (Russell & Norvig, 1994): If p represents the attribute of being at the goal (the upper right corner the grid, say), and q represents the attribute of being at a bad state (the state below it, say), \u00acqUp corresponds to avoiding the bad state en route to the goal.\nOn the other hand, there are barriers to straightforwardly adopting temporal logic-based languages in a reinforcement-learning setup. The most significant is that we can show that it is simply impossible to learn to satisfy classical LTL specifications in some cases. A key property for being able to learn near-optimal policies efficiently in the context of reward-based MDPs is what is known as the Simulation Lemma (Kearns & Singh, 1998). Informally, it says that, for any MDP and any > 0, there exists an \u2032 > 0 such that finding optimal policies in an \u2032-close model of the real environment results in behavior that is -close to optimal in the real environment.\nUnfortunately, tasks specified via LTL do not have this property. In particular, there is an MDP and an > 0 such that no \u2032-close approximation for \u2032 > 0 is sufficient to produce a policy with -close satisfaction probability.\nConsider the MDP in Figure 2. If we want to find a behavior that nearly maximizes the probability of satisfying the specification g (stay in the good state forever), we need accurate estimates of p1 and p2. If p1 = p2 = 1 or p1 < 1 and p2 < 1, either policy is equally good. If p1 = 1 and p2 < 1, only action a1 is near optimal. If p2 = 1 and p1 < 1, only action a2 is near optimal. As there is no finite bound on the number of learning trials needed to distinguish p1 = 1 from p1 < 1, a near optimal behavior cannot be found in worst-case finite time. LTL expressions are simply too unforgiving to be used with any confidence in a learning setting.\nIn this work, we develop a hybrid approach for specifying behavior in reinforcement learning that combines the\nstrengths of both reward functions and temporal logic specifications."}, {"heading": "2 Learning To Satisfy LTL", "text": "While provable guarantees of efficiency and optimality have been at the core of the literature on learning (Fiechter, 1994; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Li et al., 2011), correctness with respect to complicated, high-level task specifications\u2014during the learning itself or in the behavior resulting from the learning phase\u2014 has attracted limited attention (Abbeel & Ng, 2005)."}, {"heading": "2.1 Geometric linear temporal logic", "text": "We present a variant of LTL we call geometric linear temporal logic (GLTL) that builds on the logical and temporal operators in LTL while ensuring learnability. The idea of GLTL is roughly to restrict the period of validity of the temporal operators to bounded windows\u2014similar to the bounded semantics of LTL (Manna & Pnueli, 1992). To this end, GLTL introduces operators of the form of \u2666\u00b5b with the atomic proposition b, which is interpreted as \u201cb eventually holds within k time steps where k is a random variable following a geometric distribution with parameter \u00b5.\u201d Similar semantics stochastically restricting the window of validity for other temporal operators are also introduced.\nThis kind of geometric decay fits very nicely with MDPs for a few reasons. It can be viewed as a generalization of reward discounting, which is already present in many MDP models. It also avoids unnecessarily expanding the specification state space by only requiring extra states to represent events and not simply the passage of time.\nUsing G1(\u00b5) to represent the geometric distribution with parameter \u00b5, the temporal operators are:\n\u2022 \u2666\u00b5p: p is achieved in the next k steps, k \u223c G1(\u00b5).\n\u2022 \u00b5q: q holds for the next k steps, k \u223c G1(\u00b5).\n\u2022 q U\u00b5q: q must hold at least until p becomes true, which itself must be achieved in the next k steps, k \u223c G1(\u00b5).\nReturning to our earlier example from Figure 2, evaluating the probability of satisfaction for g requires infinite precision in the learned transition probabilities in the environment. Consider instead evaluating \u00b5g in this environment. An encoding of the specification for this example is shown in Figure 3 (Third). We call it a specification MDP, as it specifies the task using states (derived from the formula), actions (representing conditions), and probabilities (capturing the stochasticity of operator expiration). This example says that, from the initial state q0, encountering any state where g is not true results in immediately failing the specification. In contrast, encountering any state\nwhere g is true results in either continued evaluation (with probability \u00b5) or success (with probability 1\u2212 \u00b5). Success represents the idea that the temporal window in which g must hold true has expired without g being violated.\nComposing these two MDPs leads to the composite MDP in Figure 3 (Fourth). The true satisfaction probability for action ai is 1\u2212\u00b5(1\u2212\u00b5+pi) . Thus, if \u00b5 = .9, the dependence of this value on is .1.1+ , which is well behaved for all values of . The sensitivity of the computed satisfaction probability has a maximum 1/(1\u2212 \u00b5)2 dependence on the accuracy of the estimate of . Thus, GLTL is considerably more friendly to learning than is LTL.\nReturning to the MDP example in Figure 1, we find that GLTL is also more expressive than rewards. The GLTL formula \u00acq U\u00b5p can be translated to a specification MDP. Essentially, the idea is that encountering a bad state (q) even once or running out of time results in specification failure. Maximizing the satisfaction of this GLTL formula results in taking action a1 regardless of the value of p. That is, it is an environment-independent specification of the task.\nThe reason the GLTL formulation is able to succeed where standard rewards fail is that the GLTL formula results in an augmentation of the state space so that the reward function can depend on whether a bad state has yet been encountered. On the first encounter, a penalty can be issued. After the first encounter, no additional penalty is added. By composing the environment MDP with this bit of internal memory, the task can be expressed provably correctly and in an environment-independent way."}, {"heading": "3 Related Work", "text": "Discounting has been used in previous temporal models. In quantitative temporal logic, it gives more weight to the satisfaction of a logic property in the near future than the far future. De Alfaro et al. (2003, 2004) augment computation tree logic (CTL) with discounting and develop fixpointbased algorithms for checking such properties for probabilistic systems and games. Almagor et al. (2014) explicitly refine the \u201ceventually\u201d operator of LTL to a discounting operator such that the longer it takes to fulfill the task the smaller the value of satisfaction. Further, they show that discounted LTL is more expressive than discounted CTL. They use both discounted until and undiscounted until for expressing traditional eventually as well as its discounted version. However, algorithms for model checking and synthesis discounted LTL for probabilistic systems and games are yet to be developed.\nLTL has been used extensively in robotics domains. Work on the trustworthiness of autonomous robots, automated verification and synthesis with provable correctness with respect to temporal logic-based specifications in motion, task, and mission planning have attracted considerable at-\ntention recently. The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011). While temporal logic had initially focused on reasoning about temporal and logical relations, its dialects with probabilistic modalities have been used increasingly for robotics applications (Baier & Katoen, 2008; De Alfaro, 1998; Kwiatkowska et al., 2002)."}, {"heading": "4 Generating Specification MDPs", "text": "Similar to LTL, GLTL formulas are built from a set of atomic propositions AP , Boolean operators \u2227 (conjunction), \u00ac (negation) and temporal operator U\u00b5 (\u00b5-until). Useful operators such as\u2228 (disjunction),\u2666\u00b5 (\u00b5-eventually) and \u00b5 (\u00b5-always) can be derived from these basic operators.\nGLTL formulas can be converted to the corresponding specification MDPs recursively, with the operator precedence listed in descending order in Table 1. Operators of the same precedence are read from right to left. For example, \u00b51\u2666\u00b52\u03d5 = ( \u00b51(\u2666\u00b52\u03d5)), \u03d51U\u00b51\u03d52U\u00b52\u03d53 = (\u03d51U\u00b51(\u03d52U\u00b52\u03d53)).\nAssume \u03d5,\u03d51, \u03d52 are GLTL formulas in the following discussion.\n\u2022 b, where b \u2208 AP is an atomic proposition: A specification MDP Mb = ({sini, acc, rej}, {a}, T,R) for b can be constructed such that, if p holds at sini, the transition (sini, a, acc) is taken with probability 1; otherwise, the transition (sini, a, rej) is taken with probability 1.\n\u2022 \u00ac\u03d5: A specification MDP M\u00ac\u03d5 can be constructed from a specification MDP M\u03d5 by swapping the terminal states acc and rej.\n\u2022 \u03d51 \u2227 \u03d52: A specification MDP M\u03d51\u2227\u03d52 = (S,A, T,R) can be constructed from specification MDPs M\u03d51 = (S1, A1, T1, R1)\nand M\u03d52 = (S2, A2, T2, R2) such that (1) S = (S1\\{rej1}) \u00d7 (S2\\{rej2}) \u22c3 {rej}, and the accepting state is acc = (acc1, acc2); (2) A = A1 \u00d7 A2; (3) for all transitions (s1, a1, s\u20321) of M\u03d51 and (s2, a2, s \u2032 2) of M\u03d52 , if either s \u2032 1 = rej1 or s\u20322 = rej2, let T ((s1, s2), (a1, a2), rej) = T1(s1, a1, s \u2032 1)T2(s2, a2, s \u2032 2); otherwise, T ((s1, s2), (a1, a2), (s\u20321, s \u2032 2)) = T1(s1, a1, s \u2032 1)T2(s2, a2, s \u2032 2).\n\u2022 \u03d51 \u2228 \u03d52 = \u00ac(\u00ac\u03d51 \u2227 \u00ac\u03d52).\n\u2022 \u03d51U\u00b5\u03d52: The operator \u00b5-until has two operands, \u03d51 and \u03d52, which generate specification MDPs M\u03d51 = (S1, A1, T1, R1) and M\u03d52 = (S2, A2, T2, R2). The new specification MDP M\u03d51U\u00b5\u03d52 = (S,A, T,R) is constructed from M\u03d51 and M\u03d52 : S = (S1\\{acc1, rej1}) \u00d7 (S2\\{acc2, rej2}) \u22c3 {acc, rej},\nwhere acc and rej are the accepting and rejecting state, respectively, and sini = (sini1 , s ini 2 ) \u2208 S is the initial state; A = A1 \u00d7 A2; for all s = (s1, s2) \u2208 S\\{acc, rej}, a = (a1, a2) \u2208 A, s\u20321 \u2208 S1 and s\u20322 \u2208 S2, if T1(s1, a1, s\u20321) > 0 and T2(s2, a2, s \u2032 2) > 0, a transition (s, a, s\u2032) is added to M\u03d51U\u00b5\u03d52 with probability T (s, a, s\u2032) as specified in Table 2.\nHere are some intuitions behind the construction of T . The formula \u03d51U\u00b5\u03d52 means that, within some stochastically decided time period k, we would like to successfully implement task \u03d52 in at most k steps without ever failing in task \u03d51. If we observe a success in M\u03d52 (that is, the specification reaches acc2) before \u03d51 fails (that is the sepcification reaches rej1), M\u03d51U\u00b5\u03d52 goes to state acc for sure; if we observe a failure in M\u03d51 (that, the specifcation reaches rej1) before succeeding in M\u03d52 (that is, the specification reaches acc2), M\u03d51U\u00b5\u03d52 goes to state rej for sure. In all other cases, M\u03d51U\u00b5\u03d52 primarily keeps track of the transitions inM\u03d51 andM\u03d52 , with a tiny probability of failing immediately, which corresponds to the operator expiring.\n\u2022 \u2666\u00b5\u03d52: As in the semantics of LTL, \u00b5-eventually \u2666\u00b5\u03d52 = True U\u00b5\u03d52. Hence, given a specification\nTable 2: Transition (s, a, s\u2032) in M\u03d51U\u00b5\u03d52 constructed from a transition (s1, a1, s\u20321) in M\u03d51 and a transition (s2, a2, s \u2032 2)\nin M\u03d52 . Here, p(s \u2032|s\u20321, s\u20322) = T (s,a,s\u2032) T1(s1,a1,s\u20321)T2(s2,a2,s \u2032 2)\n. That is, to get the transition probability, multiple the p column by the corresponding T1 and T2 transition probabilities.\ns\u20321 s \u2032 2 s \u2032 p(s\u2032|s\u20321, s\u20322) acc1 acc2 acc 1 acc1 rej2 (s\u20321, s \u2032 2) 1\u2212 \u00b5\nrej \u00b5\nacc1 S2\\{acc2, rej2} (sini1 , s \u2032 2) 1\u2212 \u00b5 rej \u00b5 rej1 acc2 acc 1 rej1 S2\\{acc2} rej 1\nS1\\{acc1, rej1} acc2 acc 1 S1\\{acc1, rej1} rej2 (s\u20321, s ini 2 ) 1\u2212 \u00b5\nrej \u00b5\nS1\\{acc1, rej1} S2\\{acc2, rej2} (s\u20321, s \u2032 2) 1\u2212 \u00b5\nrej \u00b5\nMDP M\u03d52 = (S2, A2, T2, R2) for \u03d52, we can construct a specification MDP M\u2666\u00b5\u03d52 = (S,A, T,R) for \u2666\u00b5\u03d52: S = S2, sini = sini2 , acc = acc2, rej = rej2; A = A2; transitions of M\u2666\u00b5\u03d52 are modified from those of M\u03d52 as in Table 3. Informally, \u2666\u00b5\u03d52 is satisfied if we succeed in task \u03d52 within the stochastic observation time period.\n\u2022 \u00b5\u03d52: \u00b5-always \u03d52 is equivalent to \u00ac\u2666\u00b5\u00ac\u03d52 = \u00ac(\u2666\u00b5(\u00ac\u03d52)). In other words, \u00b5\u03d52 is satisfied if we did not witness a failure of \u03d52 within the stochastic observation time period. The transitions of a specification MDP M \u00b5\u03d52 can be constructed from Table 3, or directly from Table 4.\nUsing the transitions as described, a given GLTL formula can be converted into a specification MDP. To satisfy the specification in a given environment, a joint MDP is created as follows:\n1. Take the cross product of the MDP representing the environment and the specification MDP.\nThe resulting policy is one that maximizes the probability of satisfying the given formula where the random events are both the transitions in the environment and the stochastic transitions in the specification MDP. Such policies tend to prefer satisfying formulas quickly, as that increases the chance of successful completion before operators expire."}, {"heading": "5 Example Domain", "text": "Consider the following formula:\n(\u00acblueU\u00b5red) \u2227 (\u2666\u00b5(red \u2227 \u2666\u00b5green)).\nIt specifies a task of reaching a red state without encountering a blue state and, once a red state is reached, going to a green state.\nFigure 5 illustrates a grid world environment in which this task can be carried out. It consists of different colored grid cells. The agent can move to one of the four adject cells to its current position with a north, south, east, or west action. However, selecting an action for one direction has a 0.02 probability of moving in the one of the three other directions. This stochastic movement causes the agent to keep\nits distance from dangerous grid cells that could result in task failure, whenever possible. The solid line in the figure traces the path of the optimal policy of following this specification in the grid. As can be seen, the agent moves to red and then green. Note that this behavior can be very difficult to encode in a standard reward function as both green and red need to be given positive reward and therefore either would be a sensible place for the agent to stop.\nFigure 5 illustrates a grid world environment in which the blue cells create a partial barrier between the red and green cells. As a result of the \u201cuntil\u201d in the specification, the agent goes around the blue wall to get to the red cell. However, since the prohibition against blue cells is lifted once the red cell is reached, it goes directly through the barrier to reach green.\nThese 25-state environments become 98-state MDPs when combined with the specification MDP."}, {"heading": "6 Conclusion", "text": "In contrast to standard MDP reward functions, we have provided an environment-independent specification for tasks. We have shown that this specification language can capture standard tasks used in the MDP community and that it can be automatically incorporated into an environment MDP to create a fixed MDP to solve. Maximizing reward in this resulting MDP maximizes the probability of satisfying the task specification.\nFuture work includes inverse reinforcement learning of task specifications and techniques for accelerating planning."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2005}, {"title": "Discounting in ltl", "author": ["Almagor", "Shaull", "Boker", "Udi", "Kupferman", "Orna"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Almagor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Almagor et al\\.", "year": 2014}, {"title": "Using local trajectory optimizers to speed up global optimization in dynamic programming", "author": ["Atkeson", "Christopher G"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Atkeson and G.,? \\Q1994\\E", "shortCiteRegEx": "Atkeson and G.", "year": 1994}, {"title": "Rewarding behaviors", "author": ["Bacchus", "Fahiem", "Boutilier", "Craig", "Grove", "Adam"], "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence,", "citeRegEx": "Bacchus et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 1996}, {"title": "Principles of Model Checking", "author": ["Baier", "Christel", "Katoen", "Joost-Pieter"], "venue": null, "citeRegEx": "Baier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baier et al\\.", "year": 2008}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["Barto", "Andrew G", "Sutton", "Richard S", "Anderson", "Charles W"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["Boutilier", "Craig", "Dean", "Thomas", "Hanks", "Steve"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Formal verification of probabilistic systems", "author": ["De Alfaro", "Luca"], "venue": "PhD thesis,", "citeRegEx": "Alfaro and Luca.,? \\Q1998\\E", "shortCiteRegEx": "Alfaro and Luca.", "year": 1998}, {"title": "Discounting the future in systems theory", "author": ["De Alfaro", "Luca", "Henzinger", "Thomas A", "Majumdar", "Rupak"], "venue": "In Automata, Languages and Programming,", "citeRegEx": "Alfaro et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Alfaro et al\\.", "year": 2003}, {"title": "Model checking discounted temporal properties", "author": ["De Alfaro", "Luca", "Faella", "Marco", "Henzinger", "Thomas A", "Majumdar", "Rupak", "Stoelinga", "Mari\u00eblle"], "venue": null, "citeRegEx": "Alfaro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Alfaro et al\\.", "year": 2004}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Ltl control in uncertain environments with probabilistic satisfaction", "author": ["Ding", "Xu Chu", "Smith", "Stephen L", "Belta", "Calin", "Rus", "Daniela"], "venue": "guarantees. CoRR,", "citeRegEx": "Ding et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2011}, {"title": "Efficient reinforcement learning", "author": ["Fiechter", "Claude-Nicolas"], "venue": "Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory,", "citeRegEx": "Fiechter and Claude.Nicolas.,? \\Q1994\\E", "shortCiteRegEx": "Fiechter and Claude.Nicolas.", "year": 1994}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "In Proceedings of the 15th International Conference on Machine Learning, pp", "citeRegEx": "Kearns et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1998}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael J", "Singh", "Satinder P"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Temporal-logic-based reactive mission and motion planning", "author": ["H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas"], "venue": "IEEE Tans. on Robotics,", "citeRegEx": "Kress.Gazit et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kress.Gazit et al\\.", "year": 2009}, {"title": "Correct, reactive robot control from abstraction and temporal logic specifications", "author": ["H. Kress-Gazit", "T. Wongpiromsarn", "U. Topcu"], "venue": "IEEE RAM,", "citeRegEx": "Kress.Gazit et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kress.Gazit et al\\.", "year": 2011}, {"title": "Prism: Probabilistic symbolic model checker", "author": ["Kwiatkowska", "Marta", "Norman", "Gethin", "Parker", "David"], "venue": "In Computer Performance Evaluation: Modelling Techniques and Tools,", "citeRegEx": "Kwiatkowska et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kwiatkowska et al\\.", "year": 2002}, {"title": "Control of Markov decision processes from PCTL specifications", "author": ["M. Lahijanian", "S.B. Andersson", "C. Belta"], "venue": "In Proc. of the American Control Conference,", "citeRegEx": "Lahijanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lahijanian et al\\.", "year": 2011}, {"title": "Knows what it knows: A framework for self-aware learning", "author": ["Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J", "Strehl", "Alexander L"], "venue": "Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Synthesis of reactive switching protocols from temporal logic specifications", "author": ["Liu", "Jun", "Ozay", "Necmiye", "Topcu", "Ufuk", "Murray", "Richard M"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "The Temporal Logic of Reactive & Concurrent Sys", "author": ["Manna", "Zohar", "Pnueli", "Amir"], "venue": null, "citeRegEx": "Manna et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Manna et al\\.", "year": 1992}, {"title": "Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces", "author": ["Moore", "Andrew W"], "venue": "In Proc. Eighth International Machine Learning Workshop,", "citeRegEx": "Moore and W.,? \\Q1991\\E", "shortCiteRegEx": "Moore and W.", "year": 1991}, {"title": "Markov Decision Processes\u2014 Discrete Stochastic Dynamic Programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q1994\\E", "shortCiteRegEx": "Puterman and L.", "year": 1994}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Russell", "Stuart J", "Norvig", "Peter"], "venue": null, "citeRegEx": "Russell et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Russell et al\\.", "year": 1994}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["Strehl", "Alexander L", "Li", "Lihong", "Littman", "Michael L"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "Christopher J.C. H"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}, {"title": "Robust control of uncertain markov decision processes with temporal logic specifications", "author": ["Wolff", "Eric M", "Topcu", "Ufuk", "Murray", "Richard M"], "venue": "In Proc. of the IEEE Conference on Decision and Control,", "citeRegEx": "Wolff et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wolff et al\\.", "year": 2012}, {"title": "Receding horizon temporal logic planning", "author": ["T. Wongpiromsarn", "U. Topcu", "R.M. Murray"], "venue": "IEEE T. on Automatic Control,", "citeRegEx": "Wongpiromsarn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wongpiromsarn et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Linear temporal logic has been used in the past to specify reward functions that depend on temporal sequences (Bacchus et al., 1996); here, we expand the role to provide a robust and consistent semantics that allows desired behaviors to be specified in an environment-independent way.", "startOffset": 110, "endOffset": 132}, {"referenceID": 6, "context": "The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).", "startOffset": 126, "endOffset": 186}, {"referenceID": 25, "context": "The problems of learning and planning in such environments have been vigorously studied in the AI community for over 25 years (Watkins, 1989; Boutilier et al., 1999; Strehl et al., 2009).", "startOffset": 126, "endOffset": 186}, {"referenceID": 19, "context": "While provable guarantees of efficiency and optimality have been at the core of the literature on learning (Fiechter, 1994; Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Li et al., 2011), correctness with respect to complicated, high-level task specifications\u2014during the learning itself or in the behavior resulting from the learning phase\u2014 has attracted limited attention (Abbeel & Ng, 2005).", "startOffset": 107, "endOffset": 191}, {"referenceID": 1, "context": "Almagor et al. (2014) explicitly refine the \u201ceventually\u201d operator of LTL to a discounting operator such that the longer it takes to fulfill the task the smaller the value of satisfaction.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 15, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 20, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 27, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 11, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 18, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 16, "context": "The results include open-loop and reactive control of deterministic, stochastic or non-deterministic finite-state models as well as continuous state models through appropriate finite-state abstractions (Wongpiromsarn et al., 2012; Kress-Gazit et al., 2009; Liu et al., 2013; Wolff et al., 2012; Ding et al., 2011; Lahijanian et al., 2011; Kress-Gazit et al., 2011).", "startOffset": 202, "endOffset": 364}, {"referenceID": 17, "context": "While temporal logic had initially focused on reasoning about temporal and logical relations, its dialects with probabilistic modalities have been used increasingly for robotics applications (Baier & Katoen, 2008; De Alfaro, 1998; Kwiatkowska et al., 2002).", "startOffset": 191, "endOffset": 256}], "year": 2017, "abstractText": "We propose a new task-specification language for Markov decision processes that is designed to be an improvement over reward functions by being environment independent. The language is a variant of Linear Temporal Logic (LTL) that is extended to probabilistic specifications in a way that permits approximations to be learned in finite time. We provide several small environments that demonstrate the advantages of our geometric LTL (GLTL) language and illustrate how it can be used to specify standard reinforcementlearning tasks straightforwardly.", "creator": "LaTeX with hyperref package"}}}