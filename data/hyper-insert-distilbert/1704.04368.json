{"id": "1704.04368", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "abstract": "such neural sequence - formulation to - object sequence models have provided ourselves a viable new dynamic approach for abstractive text summarization ( meaning they additionally are not restricted partly to simply directly selecting data and rearranging passages differently from identifying the original target text ). however, even these models combined have two shortcomings : wherein they are liable to prematurely reproduce factual details inaccurately, mechanically and physically they continually tend to repeat themselves. indeed in this work presently we jointly propose a novel architecture tool that augments the conventional standard sequence - description to - sequence attentional reduction model in the two orthogonal development ways. first, we use of a flexible hybrid computational pointer - generator network that can copy words displayed from among the source text simultaneously via spatial pointing, which itself aids accurate interactive reproduction scanning of information, while retaining the ability to properly produce novel words through the generator. second, moreover we inherently use coverage techniques to keep track percent of times what this has been summarized, which discourages repeated repetition. as we fundamentally apply our model to define the cnn / daily financial mail instant summarization task, outperforming the currently current collaborative abstractive state - of - the - art by at least approaching 2 sql rouge melting points.", "histories": [["v1", "Fri, 14 Apr 2017 07:55:19 GMT  (124kb,D)", "http://arxiv.org/abs/1704.04368v1", "Accepted to ACL 2017"], ["v2", "Tue, 25 Apr 2017 05:47:50 GMT  (125kb,D)", "http://arxiv.org/abs/1704.04368v2", "Add METEOR evaluation results, add some citations, fix some equations (what are now equations 1, 8 and 11 were missing a bias term), fix url to pyrouge package, add acknowledgments"]], "COMMENTS": "Accepted to ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["abigail see", "peter j liu", "christopher d manning"], "accepted": true, "id": "1704.04368"}, "pdf": {"name": "1704.04368.pdf", "metadata": {"source": "CRF", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning"], "emails": ["abisee@stanford.edu", "peterjliu@google.com", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original. There are two broad approaches to summarization: extractive and abstractive. Extractive methods assemble summaries exclusively from passages (usually whole sentences) taken directly from the source text, while abstractive methods may generate novel words and phrases not featured in the source text \u2013 as a human-written abstract usually does. The extractive approach is easier, because copying large\nchunks of text from the source document ensures baseline levels of grammaticality and accuracy. On the other hand, sophisticated abilities that are crucial to high-quality summarization, such as paraphrasing, generalization, or the incorporation of real-world knowledge, are possible only in an abstractive framework (see Figure 5).\nDue to the difficulty of abstractive summarization, the great majority of past work has been extractive (Saggion and Poibeau, 2013; Mehta, 2016). However, the recent success of sequence-\nar X\niv :1\n70 4.\n04 36\n8v 1\n[ cs\n.C L\n] 1\n4 A\npr 2\n01 7\nto-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016). Though these systems are promising, they exhibit undesirable behavior such as inaccurately reproducing factual details, an inability to deal with out-of-vocabulary (OOV) words, and repeating themselves (see Figure 1).\nIn this paper we present a novel architecture that addresses all these issues in the context of multi-sentence summaries. While most recent abstractive work has focused on headline generation tasks (reducing one or two sentences to a single headline), we believe that longer-text summarization is both more challenging (requiring higher levels of abstraction while avoiding repetition) and ultimately more useful. Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.\nOur hybrid pointer-generator network facilitates copying words from the source text via pointing (Vinyals et al., 2015), which improves accuracy and handling of OOV words, while retaining the ability to generate new words. The network, which can be viewed as a balance between extractive and abstractive approaches, is similar to Gu et al.\u2019s (2016) CopyNet that was applied to short-\ntext Chinese summarization. We propose a novel variant of the coverage vector (Tu et al., 2016) from Neural Machine Translation, which we use to track and control coverage of the source document. We show that coverage is remarkably effective for eliminating repetition, and present the first application of this technique to summarization."}, {"heading": "2 Our Models", "text": "In this section we describe (1) our baseline sequence-to-sequence model, (2) our pointergenerator model, and (3) our coverage mechanism that can be added to either of the first two models.1"}, {"heading": "2.1 Sequence-to-sequence attentional model", "text": "Our baseline model is similar to that of Nallapati et al. (2016), and is depicted in Figure 2. The tokens of the article wi are fed one-by-one into the encoder (a single-layer bidirectional LSTM), producing a sequence of encoder hidden states hi. On each step t, the decoder (a single-layer unidirectional LSTM) receives the word embedding of the previous word (while training, this is the previous word of the reference summary; at test time it is the previous word emitted by the decoder), and has decoder state st. The attention distribution at is calculated as in Bahdanau et al. (2015):\neti = v T tanh(Whhi +Wsst) (1) at = softmax(et) (2)\nwhere v, Wh and Ws are learnable parameters. The attention distribution can be viewed as a prob-\n1The TensorFlow code for the models will be released with camera-ready version of this paper.\nability distribution over the source words, that tells the decoder \u2018where to look\u2019 to produce the next word. Next, the attention distribution is used to produce a weighted sum of the encoder hidden states, known as the context vector h\u2217t :\nh\u2217t = \u2211\ni atihi (3)\nThe context vector can be seen as a fixed-size representation of \u2018what has been read from the source\u2019 for this step. It is used alongside the decoder step st to produce a probability distribution Pvocab that also serves as our final probability distribution P (w) from which we make predictions. During training, the loss for timestep t is the negative log likelihood of the target word w\u2217t for that timestep:\nlosst = \u2212 logP (w\u2217t ) (4)\nand the overall loss for the whole sequence is:\nloss = 1\nT \u2211T t=0 losst (5)"}, {"heading": "2.2 Pointer-generator network", "text": "Our pointer-generator network is a hybrid between our baseline and a pointer network (Vinyals et al., 2015), as it allows both copying words via pointing, and generating words from a fixed vocabulary.\nIn the pointer-generator model (depicted in Figure 3) the attention distribution at and context vector h\u2217t are calculated as in section 2.1. In addition, the generation probability pgen \u2208 [0, 1] for timestep t is calculated from the context vector h\u2217t , the decoder state st and the decoder input xt:\npgen = \u03c3(W \u2032 h\u2217h \u2217 t +W \u2032 sst +W \u2032 xxt) (6)\nwhere W \u2032h\u2217 , W \u2032 s and W \u2032 x are learnable parameters. Next, pgen is used as a soft switch to choose between generating a word from the vocabulary by sampling from Pvocab, or copying a word from the input sequence by sampling from the attention distribution at. For each document let the extended vocabulary denote the union of the vocabulary, and all words appearing in the source document. We obtain the following probability distribution over the extended vocabulary:\nP (w) = pgenPvocab(w)+(1\u2212pgen) \u2211\ni:wi=w\nati (7)\nNote that if w is an out-of-vocabulary (OOV) word, then Pvocab(w) is zero; similarly if w does not appear in the source document, then\u2211\ni:wi=w ati is zero. The ability to produce OOV words is one of the primary advantages of pointergenerator models; by contrast models such as our baseline are restricted to their pre-set vocabulary.\nThe loss function is as described in equations (4) and (5), but with respect to our modified probability distribution P (w) given in equation (7)."}, {"heading": "2.3 Coverage mechanism", "text": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1). We adapt the coverage model of Tu et al. (2016) to solve the problem. In our coverage model, we maintain a coverage vector ct, which is the sum of attention distributions over all previous decoder timesteps:\nct = \u2211t\u22121\nt\u2032=0 at \u2032\n(8)\nIntuitively, ct is a (unnormalized) distribution over the source document words that represents the degree of \u2018coverage\u2019 that those words have received from the attention mechanism so far. Note that c0 is a zero vector, because on the first timestep, none of the source document has been covered.\nThe coverage vector is used as extra input to the attention mechanism, changing equation (1) to:\neti = v T tanh(Whhi +Wsst +Wcc t). (9)\nThis ensures that the attention mechanism\u2019s current decision (choosing where to attend next) is informed by a reminder of its previous decisions (summarized in ct). This should make it easier for the attention mechanism to avoid repeatedly attending to the same locations, and thus avoid generating repetitive text.\nWe find it necessary (see section 5) to additionally define a coverage loss to penalize repeatedly attending to the same locations:\ncovlosst = \u2211\ni min(ati, c t i) (10)\nNote that the coverage loss is bounded; in particular covlosst \u2264 \u2211 i a t i = 1. Equation (10) differs from the coverage loss used in Machine Translation. In MT, we assume that there should be a roughly one-to-one translation ratio; accordingly the final coverage vector is penalized if it is more or less than 1. Our loss function is more flexible: because summarization should not require uniform coverage, we only penalize the overlap between each attention distribution and the coverage so far \u2013 preventing repeated attention. Finally,\nthe coverage loss, reweighted by some hyperparameter \u03bb, is added to the primary loss function to yield a new composite loss function:\nlosst = \u2212 logP (w\u2217t ) + \u03bb \u2211 i min(ati, c t i) (11)"}, {"heading": "3 Related Work", "text": "Neural abstractive summarization. Rush et al. (2015) were the first to apply modern neural networks to abstractive text summarization, achieving state-of-the-art performance on DUC-2004 and Gigaword, two sentence-level summarization datasets. Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al., 2016), and hierarchical networks (Nallapati et al., 2016), further improving performance on those datasets.\nHowever, large-scale datasets for summarization of longer text are rare. Nallapati et al. (2016) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines. The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hierarchical RNNs to select sentences, and found that it significantly outperformed their abstractive result with respect to the ROUGE metric. To our knowledge, these are the only two published results on the dataset.\nPointer-generator networks. The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from the input sequence. The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).\nOur approach is closest to Gu et al. (2016), with two differences: (i) We calculate an explicit switch probability pgen, whereas Gu et al. induce competition through a shared softmax function. (ii) We recycle the attention distribution to serve as the copy distribution, but Gu et al. use two separate distributions. Our reasoning is that (i) calculating an explicit pgen usefully enables us to raise or lower the probability of all generated words or\nall copy words at once, rather than individually, and (ii) the two distributions serve such similar purposes that we find our simpler approach suffices. Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying.\nCoverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step. We find that a simpler approach \u2013 summing the attention distributions to obtain the coverage vector \u2013 suffices. In this respect our approach is similar to Xu et al. (2015), who apply a similar method to image captioning.\nTemporal attention is a related technique that has been applied to NMT (Sankaran et al., 2016) and summarization (Nallapati et al., 2016). In this approach, each attention distribution is divided by the sum of the previous, which effectively dampens repeated attention. We tried this method but found it too destructive, distorting the signal from the attention mechanism and reducing performance. We hypothesize that an \u2018early intervention\u2019 method such as coverage is preferable to a \u2018post hoc\u2019 method such as temporal attention \u2013 it is better to inform the attention mechanism to help it make better decisions, than to override its decisions altogether. This theory is supported by the large boost that coverage gives our ROUGE scores (see Table 1), compared to the smaller boost given by temporal attention for the same task (Nallapati et al., 2016)."}, {"heading": "4 Dataset", "text": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sen-\ntences or 56 tokens on average). We used scripts supplied by Nallapati et al. (2016) to obtain the same version of the the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset\u2019s published results (Nallapati et al., 2016, 2017) use the anonymized version of the data, which has been pre-processed to replace each named entity, e.g., The United Nations, with its own unique identifier for the example pair, e.g., @entity5. By contrast, we operate directly on the original text (or non-anonymized version of the data),2 which we believe is the favorable problem to solve because it requires no pre-processing."}, {"heading": "5 Experiments", "text": "For all experiments, our model has 256- dimensional hidden states and 128-dimensional word embeddings. For the pointer-generator models, we use a vocabulary of 50k words for both source and target \u2013 note that due to the pointer network\u2019s ability to handle OOV words, we may use a smaller vocabulary size than Nallapati et al.\u2019s (2016) 150k source and 60k target vocabularies. For the baseline model, we also try a larger vocabulary size of 150k.\nNote that the addition of the pointer and the coverage mechanism introduces very few extra parameters to the network: for the models with vocabulary size 50k, the baseline model has 21,499,600 parameters, the pointer-generator adds 1153 extra parameters, and coverage adds 512 extra parameters.\nUnlike Nallapati et al. (2016), we do not pretrain the word embeddings \u2013 they are learned from scratch during training. We train using Adagrad (Duchi et al., 2011) with learning rate 0.15 and an initial accumulator value of 0.1. We use gradient clipping with a maximum gradient norm of 2, but do not use any form of regularization. We use loss on the validation set to implement early stopping.\nDuring training and at test time we truncate the article to 400 tokens and limit the length of the summary to 100 tokens for training and 120 tokens at test time.3 This is done to expedite training and testing, but we also found that truncating the article can raise the performance of the model\n2We will provide the non-anonymized version of the dataset with the camera-ready version of this paper.\n3The upper limit of 120 is mostly invisible: the beam search algorithm is self-stopping and almost never reaches the 120th step.\n(see section 7.1 for more details). For training, we found it efficient to start with highly-truncated sequences, then raise the maximum length once converged. We train on a single Tesla K40m GPU with a batch size of 16. At test time our summaries are produced using beam search with beam size 4. We obtain our ROUGE scores using the same pyrouge package4 as Nallapati et al. (2016).\nWe trained both our baseline models for about 600,000 iterations (33 epochs) \u2013 this is similar to the 35 epochs required by Nallapati et al.\u2019s (2016) best model. Training took 4 days and 14 hours for the 50k vocabulary model, and 8 days 21 hours for the 150k vocabulary model. We found the pointer-generator model quicker to train, requiring less than 230,000 training iterations (12.8 epochs); a total of 3 days and 4 hours. In particular, the pointer-generator model makes much quicker progress in the early phases of training. To obtain our final coverage model, we added the coverage mechanism with coverage loss weighted to \u03bb = 1 (as described in equation 11), and trained for a further 3000 iterations (about 2 hours). In this time the coverage loss converged to about 0.2, down from an initial value of about 0.5. We also tried a more aggressive value of \u03bb = 2; this reduced coverage loss but increased the primary loss function, thus we did not use it.\nWe tried training the coverage model without the loss function, hoping that the attention mechanism may learn by itself not to attend repeatedly to the same locations, but we found this to be ineffective, with no discernible reduction in repetition. We also tried training with coverage from the first iteration rather than as a separate training phase,\n4https://pypi.python.org/pypi/pyrouge/0.1.0\nbut found that in the early phase of training, the coverage objective interfered with the main objective, reducing overall performance."}, {"heading": "6 Results", "text": "Our results are given in Table 1. We evaluate our models with the standard ROUGE metric, reporting the F1 scores for ROUGE-1, ROUGE-2 and ROUGE-L (which respectively measure the wordoverlap, bigram-overlap, and longest common sequence between the reference summary and the summary to be evaluated). We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models.\nGiven that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate \u2018anonymized\u2019 summaries (and do not re-substitute the original named entity text before running the ROUGE evaluation), our ROUGE scores are not strictly comparable. There is evidence to suggest that the original-text dataset may result in higher ROUGE scores in general than the anonymized dataset \u2013 the lead-3 baseline is higher on the former than the latter. One possible explanation is that multi-word named entities lead to a higher rate of n-gram overlap. As Nallapati et al. (2016) have not released their output on the test set, we have no means of comparison with their work beyond the ROUGE scores. Nevertheless, given that the disparity in the lead-3 scores is (+1.1 ROUGE1, +2 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed Nallapati et al. (2016) by (+4.07 ROUGE-1, +3.98 ROUGE-2, +3.73 ROUGE-L) points, we may es-\ntimate that we outperform the best abstractive system by at least 2 ROUGE points all-round. So that other researchers can inspect our work qualitatively and perform further analysis, we will release the output of all our models on the test set with the camera-ready version of this paper.\nWe find that both our baseline models perform poorly with respect to ROUGE, and in fact the larger vocabulary size (150k) does not seem to help. Even the better-performing baseline (with 50k vocabulary) produces summaries with several common problems. Factual details are frequently reproduced incorrectly, often replacing an uncommon (but in-vocabulary) name, date, number or place with a more-common alternative. For example in Figure 1, the baseline model changes new zealand to dutch, and auckland-based to irish \u2013 perhaps reflecting the European bias of the training data. Even more concerningly, the summaries sometimes contain completely false information that is wholly absent from the source document \u2013 in Figure 1 the model inexplicably produces the phrase respective prospects, though neither word appears in the article. In addition, output is sometimes nonsensical, often repeats itself, and can\u2019t reproduce out-of-vocabulary words (such as saili in Figure 1). Further examples of all these problems are provided in the supplementary material.\nOur pointer-generator model achieves much better ROUGE scores than the baseline, despite many fewer training epochs. The difference in the summaries is also marked: out-of-vocabulary words are handled easily, factual details are almost always copied correctly, and there are no fabrications (see Figure 1). However, repetition is still very common.\nOur pointer-generator model with coverage improves the ROUGE scores further, convincingly surpassing the current best abstractive model (Nallapati et al., 2016) by several ROUGE points. Despite the brevity of the coverage training phase (about 1% of the total training time), the repetition problem is almost completely eliminated, which can be seen both qualitatively (Figure 1) and quantitatively (Figure 4). However, our best model does not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model (Nallapati et al., 2017). We discuss this issue in section 7.1."}, {"heading": "7 Discussion", "text": ""}, {"heading": "7.1 Comparison with extractive systems", "text": "It is clear from Table 1 that extractive systems tend to achieve higher ROUGE scores than abstractive, and that the extractive lead-3 baseline is extremely strong (even the best extractive system beats it by only a small margin). We offer two possible explanations for these observations.\nFirstly, news articles tend to be structured with the most important information at the start; this partially explains the strength of the lead-3 baseline. Indeed, we found that using only the first 400 tokens (about 20 sentences) of the article yielded significantly higher ROUGE scores than using the first 800 tokens.\nSecondly, the nature of the task and the ROUGE metric make extractive approaches and the lead3 baseline difficult to beat. The choice of content for the reference summaries is quite subjective \u2013 sometimes the sentences form a self-contained summary; other times they simply showcase a few\ninteresting details from the article. Given that the articles contain 39 sentences on average, there are many equally valid ways to choose 3 or 4 \u2018highlights\u2019 in this style. Abstraction introduces even more options (choice of phrasing), further decreasing the likelihood of matching the reference summary. For example, smugglers profit from desperate migrants is a valid alternative abstractive summary for the first example in Figure 5, but it scores 0 ROUGE with respect to the reference summary. This inflexibility of ROUGE is exacerbated by only having one reference summary, which has been shown to lower ROUGE\u2019s reliability compared to multiple reference summaries (Lin, 2004).\nDue to the subjectivity of the task and thus the diversity of valid summaries, it seems that the ROUGE metric rewards \u2018safe\u2019 strategies such as selecting the first-appearing content, or preserving original phrasing. These strategies pay off because while the reference summaries do sometimes deviate from these techniques, those deviations are unpredictable enough that the \u2018safe option\u2019 obtains higher ROUGE scores on average. This may explain why extractive systems tend to obtain higher ROUGE numbers than abstractive, and even extractive systems do not significantly exceed the lead-3 baseline."}, {"heading": "7.2 How abstractive is our model?", "text": "We have shown that our pointer mechanism makes our abstractive system more reliable, copying factual details correctly more often. But does the ease of copying make our system any less abstractive?\nFigure 6 shows that our final model\u2019s sum-\nmaries contain a much lower rate of novel n-grams (i.e. those that don\u2019t appear in the article) than the reference summaries, indicating a lower degree of abstraction. Note that the baseline model produces novel n-grams more frequently \u2013 however, this statistic includes all the incorrectly copied words, UNK tokens and fabrications alongside the good instances of abstraction.\nAlthough our final model copies whole article sentences 35% of the time, we observe that the other 65% encompasses a range of abstractive techniques. Article sentences are truncated to form grammatically-correct shorter versions, and new sentences are composed by stitching together fragments. Unnecessary interjections, clauses and parenthesized phrases are often omitted from copied passages, or paraphrased. All these abilities are demonstrated in Figure 1, and the supplementary material contains many more examples.\nFigure 7 shows two examples of more impressive abstraction \u2013 both with similar structure. The dataset contains many sports stories whose summaries follow the predictable X beat Y \u3008score\u3009 on \u3008day\u3009 template, which may explain why our model is most confidently abstractive on these examples.\nThe value of the generation probability pgen also gives a measure of the abstractiveness of our model. During training, pgen starts with a value of about 0.30 then increases, converging to about 0.53 by the end of training. This indicates that the model first learns to mostly copy, then learns to generate about half the time. However at test time, pgen is heavily skewed towards copying, with a mean value of 0.17. The disparity is likely due to the fact that during training, the model receives word-by-word supervision in the form of the reference summary, but at test time it does not. Nonetheless, the generator module is useful even\nwhen the model is copying. We find that pgen is highest at times of \u2018uncertainty\u2019 such as the beginning of sentences, the join between stitchedtogether fragments, and when producing periods that truncate a copied sentence. Our mixture model allows the network to copy while simultaneously consulting the language model \u2013 enabling operations like stitching and truncation to be performed with grammaticality. In any case, encouraging the pointer-generator model to be more abstractive, while retaining the accuracy advantages of the pointer module, is an exciting direction for future work."}, {"heading": "8 Conclusion", "text": "In this work we presented a hybrid pointergenerator architecture with coverage, and showed that it reduces inaccuracies and repetition. We applied our model to a new and challenging longtext dataset, and significantly outperformed the abstractive state-of-the-art result. Our model exhibits many abstractive abilities, but attaining higher levels of abstraction remains an open research question."}, {"heading": "Supplementary Material", "text": "This appendix provides examples from the test set, with side-by-side comparisons of the reference summaries and the summaries produced by our models. In each example:\n\u2022 italics denote out-of-vocabulary words\n\u2022 red denotes factual errors in the summaries\n\u2022 green shading intensity represents the value of the generation probability pgen\n\u2022 yellow shading intensity represents final value of the coverage vector at the end of final model\u2019s summarization process."}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}, {"heading": "Pointer-Generator, With Coverage:", "text": ""}, {"heading": "Pointer-Generator, No Coverage:", "text": ""}, {"heading": "Baseline:", "text": ""}, {"heading": "Reference Summary:", "text": ""}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M Rush."], "venue": "North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "North American Chapter of the Association for Computational Linguistics on Human Language Technology.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Looking for a few good metrics: Automatic summarization evaluation-how many samples are enough? In NACSIS/NII Test Collection for Information Retrieval (NTCIR) Workshop", "author": ["Chin-Yew Lin"], "venue": null, "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "From extractive to abstractive summarization: A journey", "author": ["Parth Mehta."], "venue": "ACL 2016 Student Research Workshop.", "citeRegEx": "Mehta.,? 2016", "shortCiteRegEx": "Mehta.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "NIPS 2016 Workshop on Multi-class and Multi-label Learning in Extremely Large Label Spaces.", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Coverage embedding models for neural machine translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents", "author": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."], "venue": "Association for the Advancement of Artificial Intelligence.", "citeRegEx": "Nallapati et al\\.,? 2017", "shortCiteRegEx": "Nallapati et al\\.", "year": 2017}, {"title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "\u00c7aglar Gul\u00e7ehre", "Bing Xiang."], "venue": "Computational Natural Language Learning.", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Automatic text summarization: Past, present and future", "author": ["Horacio Saggion", "Thierry Poibeau."], "venue": "Multi-source, Multilingual Information Extraction and Summarization, Springer, pages 3\u201321.", "citeRegEx": "Saggion and Poibeau.,? 2013", "shortCiteRegEx": "Saggion and Poibeau.", "year": 2013}, {"title": "Temporal attention model for neural machine translation", "author": ["Baskaran Sankaran", "Haitao Mi", "Yaser Al-Onaizan", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1608.02927 .", "citeRegEx": "Sankaran et al\\.,? 2016", "shortCiteRegEx": "Sankaran et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "RNN-based encoder-decoder approach with word frequency estimation", "author": ["Jun Suzuki", "Masaaki Nagata."], "venue": "arXiv preprint arXiv:1701.00138 .", "citeRegEx": "Suzuki and Nagata.,? 2016", "shortCiteRegEx": "Suzuki and Nagata.", "year": 2016}, {"title": "Neural headline generation on abstract meaning representation", "author": ["Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Takase et al\\.,? 2016", "shortCiteRegEx": "Takase et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Neural Information Processing Systems.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "International Conference on Machine", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Efficient summarization with read-again and copy mechanism", "author": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv preprint arXiv:1611.03382 .", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Due to the difficulty of abstractive summarization, the great majority of past work has been extractive (Saggion and Poibeau, 2013; Mehta, 2016).", "startOffset": 104, "endOffset": 144}, {"referenceID": 8, "context": "Due to the difficulty of abstractive summarization, the great majority of past work has been extractive (Saggion and Poibeau, 2013; Mehta, 2016).", "startOffset": 104, "endOffset": 144}, {"referenceID": 16, "context": "to-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 1, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 12, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 13, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 22, "context": ", 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).", "startOffset": 129, "endOffset": 212}, {"referenceID": 5, "context": "Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.", "startOffset": 80, "endOffset": 126}, {"referenceID": 12, "context": "Therefore we apply our model to the recently-introduced CNN/ Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains news articles (39 sentences on average) paired with multi-sentence summaries, and show that we outperform the stateof-the-art abstractive system by at least 2 ROUGE points.", "startOffset": 80, "endOffset": 126}, {"referenceID": 20, "context": "Our hybrid pointer-generator network facilitates copying words from the source text via pointing (Vinyals et al., 2015), which improves accuracy and handling of OOV words, while retaining the ability to generate new words.", "startOffset": 97, "endOffset": 119}, {"referenceID": 19, "context": "We propose a novel variant of the coverage vector (Tu et al., 2016) from Neural Machine Translation, which we use to track and control coverage of the source document.", "startOffset": 50, "endOffset": 67}, {"referenceID": 3, "context": "The network, which can be viewed as a balance between extractive and abstractive approaches, is similar to Gu et al.\u2019s (2016) CopyNet that was applied to shorttext Chinese summarization.", "startOffset": 107, "endOffset": 126}, {"referenceID": 10, "context": "Our baseline model is similar to that of Nallapati et al. (2016), and is depicted in Figure 2.", "startOffset": 41, "endOffset": 65}, {"referenceID": 0, "context": "The attention distribution at is calculated as in Bahdanau et al. (2015):", "startOffset": 50, "endOffset": 73}, {"referenceID": 20, "context": "Our pointer-generator network is a hybrid between our baseline and a pointer network (Vinyals et al., 2015), as it allows both copying words via pointing, and generating words from a fixed vocabulary.", "startOffset": 85, "endOffset": 107}, {"referenceID": 19, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 10, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 15, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 17, "context": "Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).", "startOffset": 62, "endOffset": 144}, {"referenceID": 10, "context": ", 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1). We adapt the coverage model of Tu et al. (2016) to solve the problem.", "startOffset": 8, "endOffset": 203}, {"referenceID": 1, "context": "Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 18, "context": ", 2016), Abstract Meaning Representations (Takase et al., 2016), and hierarchical networks (Nallapati et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 12, "context": ", 2016), and hierarchical networks (Nallapati et al., 2016), further improving performance on those datasets.", "startOffset": 35, "endOffset": 59}, {"referenceID": 5, "context": "(2016) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines.", "startOffset": 55, "endOffset": 77}, {"referenceID": 11, "context": "The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hierarchical RNNs to select sentences, and found that it significantly outperformed their abstractive result with respect to the ROUGE metric.", "startOffset": 61, "endOffset": 85}, {"referenceID": 20, "context": "The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 4, "context": "The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 9, "context": ", 2016), language modeling (Merity et al., 2016), and summarization (Gulcehre et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 4, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 22, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 3, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 12, "context": ", 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016).", "startOffset": 27, "endOffset": 110}, {"referenceID": 5, "context": "Rush et al. (2015) were the first to apply modern neural networks to abstractive text summarization, achieving state-of-the-art performance on DUC-2004 and Gigaword, two sentence-level summarization datasets.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al., 2016), and hierarchical networks (Nallapati et al., 2016), further improving performance on those datasets. However, large-scale datasets for summarization of longer text are rare. Nallapati et al. (2016) adapted the DeepMind question-answering dataset (Hermann et al.", "startOffset": 106, "endOffset": 382}, {"referenceID": 0, "context": ", 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from the input sequence.", "startOffset": 84, "endOffset": 107}, {"referenceID": 0, "context": ", 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from the input sequence. The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gulcehre et al., 2016; Zeng et al., 2016; Gu et al., 2016; Nallapati et al., 2016). Our approach is closest to Gu et al. (2016), with two differences: (i) We calculate an explicit switch probability pgen, whereas Gu et al.", "startOffset": 84, "endOffset": 468}, {"referenceID": 6, "context": "Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 15, "context": "Temporal attention is a related technique that has been applied to NMT (Sankaran et al., 2016) and summarization (Nallapati et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 12, "context": ", 2016) and summarization (Nallapati et al., 2016).", "startOffset": 26, "endOffset": 50}, {"referenceID": 12, "context": "This theory is supported by the large boost that coverage gives our ROUGE scores (see Table 1), compared to the smaller boost given by temporal attention for the same task (Nallapati et al., 2016).", "startOffset": 172, "endOffset": 196}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution.", "startOffset": 51, "endOffset": 102}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al.", "startOffset": 51, "endOffset": 841}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step.", "startOffset": 51, "endOffset": 862}, {"referenceID": 4, "context": "Our approach is drastically different from that of Gulcehre et al. (2016) and Nallapati et al. (2016). Those works train their pointer components to activate only for out-of-vocabulary words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We strongly believe our approach is better for abstractive summarization \u2013 in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn et al., 2003), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step. We find that a simpler approach \u2013 summing the attention distributions to obtain the coverage vector \u2013 suffices. In this respect our approach is similar to Xu et al. (2015), who apply a similar method to image captioning.", "startOffset": 51, "endOffset": 1095}, {"referenceID": 5, "context": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.", "startOffset": 34, "endOffset": 80}, {"referenceID": 12, "context": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.", "startOffset": 34, "endOffset": 80}, {"referenceID": 5, "context": "We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average). We used scripts supplied by Nallapati et al. (2016) to obtain the same version of the the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.", "startOffset": 35, "endOffset": 273}, {"referenceID": 2, "context": "We train using Adagrad (Duchi et al., 2011) with learning rate 0.", "startOffset": 23, "endOffset": 43}, {"referenceID": 10, "context": "For the pointer-generator models, we use a vocabulary of 50k words for both source and target \u2013 note that due to the pointer network\u2019s ability to handle OOV words, we may use a smaller vocabulary size than Nallapati et al.\u2019s (2016) 150k source and 60k target vocabularies.", "startOffset": 206, "endOffset": 232}, {"referenceID": 10, "context": "For the pointer-generator models, we use a vocabulary of 50k words for both source and target \u2013 note that due to the pointer network\u2019s ability to handle OOV words, we may use a smaller vocabulary size than Nallapati et al.\u2019s (2016) 150k source and 60k target vocabularies. For the baseline model, we also try a larger vocabulary size of 150k. Note that the addition of the pointer and the coverage mechanism introduces very few extra parameters to the network: for the models with vocabulary size 50k, the baseline model has 21,499,600 parameters, the pointer-generator adds 1153 extra parameters, and coverage adds 512 extra parameters. Unlike Nallapati et al. (2016), we do not pretrain the word embeddings \u2013 they are learned from scratch during training.", "startOffset": 206, "endOffset": 669}, {"referenceID": 12, "context": "ROUGE-1 ROUGE-2 ROUGE-L abstractive model (Nallapati et al., 2016)* 35.", "startOffset": 42, "endOffset": 66}, {"referenceID": 11, "context": "57 lead-3 baseline (Nallapati et al., 2017)* 39.", "startOffset": 19, "endOffset": 43}, {"referenceID": 11, "context": "5 extractive model (Nallapati et al., 2017)* 39.", "startOffset": 19, "endOffset": 43}, {"referenceID": 11, "context": "We obtain our ROUGE scores using the same pyrouge package4 as Nallapati et al. (2016). We trained both our baseline models for about 600,000 iterations (33 epochs) \u2013 this is similar to the 35 epochs required by Nallapati et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 11, "context": "We obtain our ROUGE scores using the same pyrouge package4 as Nallapati et al. (2016). We trained both our baseline models for about 600,000 iterations (33 epochs) \u2013 this is similar to the 35 epochs required by Nallapati et al.\u2019s (2016) best model.", "startOffset": 62, "endOffset": 237}, {"referenceID": 12, "context": "We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al.", "startOffset": 147, "endOffset": 171}, {"referenceID": 11, "context": ", 2016) and extractive (Nallapati et al., 2017) models.", "startOffset": 23, "endOffset": 47}, {"referenceID": 11, "context": "We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models. Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate \u2018anonymized\u2019 summaries (and do not re-substitute the original named entity text before running the ROUGE evaluation), our ROUGE scores are not strictly comparable. There is evidence to suggest that the original-text dataset may result in higher ROUGE scores in general than the anonymized dataset \u2013 the lead-3 baseline is higher on the former than the latter. One possible explanation is that multi-word named entities lead to a higher rate of n-gram overlap. As Nallapati et al. (2016) have not released their output on the test set, we have no means of comparison with their work beyond the ROUGE scores.", "startOffset": 148, "endOffset": 794}, {"referenceID": 11, "context": "We also report the lead3 baseline (which uses the first three sentences of the article as a summary), and compare to the best previous abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models. Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate \u2018anonymized\u2019 summaries (and do not re-substitute the original named entity text before running the ROUGE evaluation), our ROUGE scores are not strictly comparable. There is evidence to suggest that the original-text dataset may result in higher ROUGE scores in general than the anonymized dataset \u2013 the lead-3 baseline is higher on the former than the latter. One possible explanation is that multi-word named entities lead to a higher rate of n-gram overlap. As Nallapati et al. (2016) have not released their output on the test set, we have no means of comparison with their work beyond the ROUGE scores. Nevertheless, given that the disparity in the lead-3 scores is (+1.1 ROUGE1, +2 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed Nallapati et al. (2016) by (+4.", "startOffset": 148, "endOffset": 1095}, {"referenceID": 12, "context": "Our pointer-generator model with coverage improves the ROUGE scores further, convincingly surpassing the current best abstractive model (Nallapati et al., 2016) by several ROUGE points.", "startOffset": 136, "endOffset": 160}, {"referenceID": 11, "context": "However, our best model does not quite surpass the ROUGE scores of the lead-3 baseline, nor the current best extractive model (Nallapati et al., 2017).", "startOffset": 126, "endOffset": 150}, {"referenceID": 7, "context": "This inflexibility of ROUGE is exacerbated by only having one reference summary, which has been shown to lower ROUGE\u2019s reliability compared to multiple reference summaries (Lin, 2004).", "startOffset": 172, "endOffset": 183}], "year": 2017, "abstractText": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.", "creator": "LaTeX with hyperref package"}}}