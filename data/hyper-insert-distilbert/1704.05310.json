{"id": "1704.05310", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Unsupervised Learning by Predicting Noise", "abstract": "^ convolutional neural networks provide visual neural features technologies that consistently perform remarkably well in many computer vision applications. however, numerical training these networks requires significant amounts devoid of continual supervision. \u2033 this paper similarly introduces completely a generic framework to train deep nonlinear networks, but end - to - end, operating with no supervision. we propose successfully to voluntarily fix together a matching set of target partial representations, called fuzzy noise as targets ( / nat ), and conversely to merely constrain the deep features to align to find them. although this limited domain agnostic approach avoids the standard unsupervised spatial learning issues concerned of designing trivial solutions and collapsing mechanisms of weak features. thanks solely to a stochastic batch mapping reassignment scheme strategy and therefore a consistent separable symmetric square wave loss function, therefore it necessarily scales equally to millions ml of sampled images. currently the proposed approach produces abstract representations implemented that perform on par with state - of - the -'art ) unsupervised methods on yahoo imagenet and pascal voc.", "histories": [["v1", "Tue, 18 Apr 2017 12:51:47 GMT  (2432kb,D)", "http://arxiv.org/abs/1704.05310v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["piotr bojanowski", "armand joulin"], "accepted": true, "id": "1704.05310"}, "pdf": {"name": "1704.05310.pdf", "metadata": {"source": "META", "title": "Unsupervised Learning by Predicting Noise", "authors": ["Piotr Bojanowski", "Armand Joulin"], "emails": ["<bojanowski@fb.com>."], "sections": [{"heading": "1. Introduction", "text": "In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al., 2015). Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al., 2016). However, human annotations may introduce unforeseen bias that could limit the potential of learned features to capture subtle information hidden in a vast collection of images.\nSeveral strategies exist to learn deep convolutional features with no annotation (Donahue et al., 2016). They either try to capture a signal from the source as a form of selfsupervision (Doersch et al., 2015; Wang & Gupta, 2015) or learn the underlying distribution of images (Vincent et al.,\n1Facebook AI Research. Correspondence to: Piotr Bojanowski <bojanowski@fb.com>.\n2010; Goodfellow et al., 2014). While some of these approaches obtain promising performance in transfer learning (Donahue et al., 2016; Wang & Gupta, 2015), they do not explicitly aim to learn discriminative features. Some attempts were made with retrieval based approaches (Dosovitskiy et al., 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets. Unfortunately, as in the supervised case, a lot of data is required to learn good representations.\nIn this work, we propose a novel discriminative framework designed to learn deep architectures on massive amounts of data. Our approach is general, but we focus on convnets since they require millions of images to produce good features. Similar to self-organizing maps (Kohonen, 1982; Martinetz & Schulten, 1991), we map deep features to a set of predefined representations in a low dimensional space. As opposed to these approaches, we aim to learn the features in a end-to-end fashion, which traditionally suffers from a feature collapsing problem. Our approach deals with this issue by fixing the target representations and aligning them to our features. These representations are sampled from a uninformative distribution and we use this Noise As Targets (NAT). Our approach also shares some similarities with standard clustering approches like k-means (Lloyd, 1982) or discriminative clustering (Bach & Harchaoui, 2007).\nIn addition, we propose an online algorithm able to scale to massive image databases like ImageNet (Deng et al., 2009). Importantly, our approach is barely less efficient to train than standard supervised approaches and can reuse any optimization procedure designed for them. This is achieved by using a quadratic loss as in (Tygert et al., 2017) and a fast approximation of the Hungarian algorithm. We show the potential of our approach by training end-to-end on ImageNet a standard architecture, namely AlexNet (Krizhevsky et al., 2012) with no supervision.\nWe test the quality of our features on several image classification problems, following the setting of Donahue et al. (2016). We are on par with state-of-the-art unsupervised and self-supervised learning approaches while being much simpler to train and to scale.\nThe paper is organized as follows: after a brief review of the related work in Section 2, we present our approach in\nar X\niv :1\n70 4.\n05 31\n0v 1\n[ st\nat .M\nL ]\n1 8\nA pr\n2 01\n7\nSection 3. We then validate our solution with several experiments and comparisons with standard unsupervised and self-supervised approaches in Section 4."}, {"heading": "2. Related work", "text": "Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion. In our work, we train the convnet end-to-end with a loss that shares similarities with k-means. Closer to our work, Dosovitskiy et al. (2014) proposes to train convnets by solving a retrieval problem. They assign a class per image and its transformation. In contrast to our work, this approach can hardly scale to more than a few hundred of thousands of images, and requires a custom-tailored architecture while we use a standard AlexNet.\nAnother traditional approach for learning visual representations in an unsupervised manner is to define a parametrized mapping between a predefined random variable and a set of images. Traditional examples of this approach are variational autoencoders (Kingma & Welling, 2013), generative adversarial networks (Goodfellow et al., 2014), and to a lesser extent, noisy autoencoders (Vincent et al., 2010). In our work, we are doing the opposite; that is, we map images to a predefined random variable. This allows us to re-use standard convolutional networks and greatly simplifies the training.\nGenerative adversarial networks. Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. While these models cannot learn an inverse mapping, Donahue et al. (2016) recently proposed to add an encoder to extract visual features from GANs. Like ours, their encoder can be any standard convolutional network. However, their loss aims at differentiating real and generated images, while we are aiming directly at differentiating between images. This makes our approach much simpler and faster to train, since we do not need to learn the generator nor the discriminator.\nSelf-supervision. Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as\nword2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features. Noroozi & Favaro (2016) have further extended this work. Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015). In particular, Wang & Gupta (2015) show that such features provide promising performance on ImageNet. In contrast to our work, these approaches are domain dependent since they require explicit derivation of weak supervision directly from the input.\nAutoencoders. Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011). The idea is to encode and decode an image, while minimizing the loss between the decoded and original images. Once trained, the encoder produces image features and the decoder can be used to generate images from codes. The decoder is often a fully connected network (Ranzato et al., 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al., 2016).\nSelf-organizing map. This family of unsupervised methods aims at learning a low dimensional representation of the data that preserves certain topological properties (Kohonen, 1982; Vesanto & Alhoniemi, 2000). In particular, Neural Gas (Martinetz & Schulten, 1991) aligns feature vectors to the input data. Each input datum is then assigned to one of these vectors in a winner-takes-all manner. These feature vectors are in spirit similar to our target representations and we use a similar assignment strategy. In contrast to our work, the target vectors are not fixed and aligned to the input vectors. Since we primarly aim at learning the input features, we do the opposite.\nDiscriminative clustering. Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012). In particular, Bach & Harchaoui (2007) shows that the ridge regression loss could be use to learn discriminative clusters. It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014). In this work, we show that a similar framework can be designed for neural networks. As opposed to Xu et al. (2004), we address the empty assignment problems by restricting the set of possible reassignments to permutations rather than using global linear constrains the assignments. Our assignments can be updated online, allowing our approach to scale to very large datasets."}, {"heading": "3. Method", "text": "In this section, we present our model and discuss its relations with several clustering approaches including kmeans. Figure 1 shows an overview of our approach. We also show that it can be trained on massive datasets using an online procedure. Finally, we provide all the implementation details."}, {"heading": "3.1. Unsupervised learning", "text": "We are interested in learning visual features with no supervision. These features are produced by applying a parametrized mapping f\u03b8 to the images. In the presence of supervision, the parameters \u03b8 are learned by minimizing a loss function between the features produced by this mapping and some given targets, e.g., labels. In absence of supervision, there is no clear target representations and we thus need to learn them as well. More precisely, given a set of n images xi, we jointly learn the parameters \u03b8 of the mapping f\u03b8, and some target vectors yi:\nmin \u03b8\n1\nn n\u2211 i=1 min yi\u2208Rd `(f\u03b8(xi), yi), (1)\nwhere d is the dimension of target vectors. In the rest of the paper, we use matrix notations, i.e., we denote by Y the matrix whose rows are the target representations yi, and by X the matrix whose rows are the images xi. With a slight abuse of notation, we denote by f\u03b8(X) the n\u00d7 d matrix of features whose rows are obtained by applying the function f\u03b8 to each image independently.\nChoosing the loss function. In the supervised setting, a popular choice for the loss ` is the softmax function. However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001). While there are workarounds to scale these losses to large output spaces, Tygert et al. (2017) has recently shown that using a squared `2 distance works well in many supervised settings, as long as the final activations are unit normalized. This loss only requires access to a single target per sample, making its computation independent of the number of targets. This leads to the following problem:\nmin \u03b8 min Y \u2208Rn\u00d7d\n1\n2n \u2016f\u03b8(X)\u2212 Y \u20162F , (2)\nwhere we still denote by f\u03b8(X) the unit normalized features.\nUsing fixed target representations. Directly solving the problem defined in Eq. (2) would lead to a representation collapsing problem: all the images would be assigned to the same representation (Xu et al., 2004). We avoid this issue by fixing a set of k predefined target representations and matching them to the visual features. More precisely, the matrix Y is defined as the product of a matrix C containing these k representations and an assignment matrix P in {0, 1}n\u00d7k, i.e.,\nY = PC. (3)\nNote that we can assume that k is greater than n with no loss of generality (by duplicating representations otherwise). Each image is assigned to a different target and each target can only be assigned once. This leads to a set P of constraints for the assignment matrices:\nP = {P \u2208 {0, 1}n\u00d7k | P1k \u2264 1n, P>1n = 1k}. (4)\nThis formulation forces the visual features to be diversified, avoiding the collapsing issue at the cost of fixing the target representations. Predefining these targets is an issue if their number k is small, which is why we are interested in the case where k is at least as large as the number n of images.\nChoosing the target representations. Until now, we have not discussed the set of target representations stored in C. A simple choice for the targets would be to take k elements of the canonical basis of Rd. If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n. On the other hand, if d is smaller than n, this formulation is equivalent to the discriminative clustering approach of Bach & Harchaoui (2007). Choosing such targets makes very strong assumptions on the nature of the underlying problem. Indeed, it assumes that each image belongs to a unique class and that all classes are orthogonal. While this assumption might be true for some classification datasets, it\ndoes not generalize to huge image collections nor capture subtle similarities between images belonging to different classes.\nSince our features are unit normalized, another natural choice is to uniformly sample target vectors on the `2 unit sphere. Note that the dimension d will then directly influence the level of correlation between representations, i.e., the correlation is inversely proportional to the square root of d. Using this Noise As Targets (NAT), Eq. (2) is now equivalent to:\nmax \u03b8 max P\u2208P\nTr ( PCf\u03b8(X) >) . (5) This problem can be interpreted as mapping deep features to a uniform distribution over a manifold, namely the ddimension `2 sphere. Using k predefined representations is a discrete approximation of this manifold that justifies the restriction of the mapping matrices to the set P of 1-to-1 assignment matrices. In some sense, we are optimizing a crude approximation of the earth mover\u2019s distance between the distribution of deep features and a given target distribution (Rubner et al., 1998).\nRelation to clustering approaches. Using the same notations as in Eq. (5), several clustering approaches share similarities with our method. In the linear case, spherical k-means minimizes the same loss function w.r.t. P and C, i.e.,\nmax C max P\u2208Q\ntr ( PCXT ) .\nA key difference is the set Q of assignment matrices:\nQ = {P \u2208 {0, 1}n\u00d7k | P1k = 1n}.\nThis set only guarantees that each data point is assigned to a single target representation. Once we jointly learn the features and the assignment, this set does not prevent the collapsing of the data points to a single target representation.\nAnother similar clustering approach is Diffrac (Bach & Harchaoui, 2007). Their loss is equivalent to ours in the case of unit normalized features. Their setR of assignment matrices, however, is different:\nR = {P \u2208 {0, 1}n\u00d7k | P>1n \u2265 c1k},\nwhere c > 0 is some fixed parameter. While restricting the assignment matrices to this set prevents the collapsing issue, it introduces global constraints that are not suited for online optimization. This makes their approach hard to scale to large datasets."}, {"heading": "3.2. Optimization", "text": "In this section, we describe how to efficiently optimize the cost function described in Eq. (5). In particular, we explore\nAlgorithm 1 Stochastic optimization of Eq. (5). Require: T batches of images, \u03bb0 > 0\nfor t = {1, . . . , T} do Obtain batch b and representations r Compute f\u03b8(Xb) Compute P \u2217 by minimizing Eq. (2) w.r.t. P Compute\u2207\u03b8L(\u03b8) from Eq. (2) with P \u2217 Update \u03b8 \u2190 \u03b8 \u2212 \u03bbt\u2207\u03b8L(\u03b8) end for\napproximated updates of the assignment matrix that are compatible with online optimization schemes, like stochastic gradient descent (SGD).\nUpdating the assignment matrix P . Directly solving for the optimal assignment requires to evaluate the distances between all the n features and the k representations. In order to efficiently solve this problem, we first reduce the number k of representations to n. This limits the set P to the set of permutation matrices, i.e.,\nP = {P \u2208 {0, 1}n\u00d7n | P1n = 1n, P>1n = 1n}. (6)\nRestricting the problem defined in Eq. (5) to this set, the linear assignment problem in P can be solved exactly with the Hungarian algorithm (Kuhn, 1955), but at the prohibitive cost of O(n3).\nInstead, we perform stochastic updates of the matrix. Given a batch of samples, we optimize the assignment matrix P on its restriction to this batch. Given a subset B of b distinct images, we only update the b \u00d7 b square sub matrix PB obtained by restricting P to these b images and their corresponding targets. In other words, each image can only be re-assigned to a target that was previously assigned to another image in the batch. This procedure has a complexity of O(b3) per batch, leading to an overall complexity of O(nb2), which is linear in the number of data points. We perform this update before updating the parameters \u03b8 of our features, in an on-line manner. Note that this simple procedure would not have been possible if k > n; we would have had to also consider the k \u2212 n unassigned representations.\nStochastic gradient descent. Apart from the update of the assignment matrix P , we use the same optimization scheme as standard supervised approaches, i.e., SGD with batch normalization (Ioffe & Szegedy, 2015). As noted by Tygert et al. (2017), batch normalization plays a crucial role when optimizing the l2 loss, as it avoids exploding gradients. For each batch b of images, we first perform a forward pass to compute the distance between the images and the corresponding subset of target representations r. The Hungarian algorithm is then used on these distances to obtain the optimal reassignments within the batch. Once\nthe assignments are updated, we use the chain rule in order to compute the gradients of all our parameters. Our optimization algorithm is summarized in Algorithm 1."}, {"heading": "3.3. Implementation details", "text": "Our experiments solely focus on learning visual features with convnets. All the details required to train these architectures with our approach are described below. Most of them are standard tricks used in the usual supervised setting.\nDeep features. To ensure a fair empirical comparison with previous work, we follow Wang & Gupta (2015) and use an AlexNet architecture. We train it end to end using our unsupervised loss function. We subsequently test the quality of the learned visual feature by re-training a classifier on top. During transfer learning, we consider the output of the last convolutional layer as our features as in Razavian et al. (2014). We use the same multi-layer perceptron (MLP) as in Krizhevsky et al. (2012) for the classifier.\nPre-processing. We observe in practice that preprocessing the images greatly helps the quality of our learned features. As in Ranzato et al. (2007), we use image gradients instead of the images to avoid trivial solutions like clustering according to colors. Using this preprocessing is not surprising since most hand-made features like SIFT or HoG are based on image gradients (Lowe, 1999; Dalal & Triggs, 2005). In addition to this preprocessing, we also perform all the standard image transformations that are commonly applied in the supervised setting (Krizhevsky et al., 2012), such as random cropping and flipping of images.\nOptimization details. We project the output of the network on the `2 sphere as in Tygert et al. (2017). The network is trained with SGD with a batch size of 256. During the first t0 batches, we use a constant step size. After t0 batches, we use a linear decay of the step size, i.e., lt =\nl0 1+\u03b3[t\u2212t0]+ . Unless mentioned otherwise, we permute\nthe assignments within batches every 3 epochs. For the transfer learning experiments, we follow the guideline described in Donahue et al. (2016)."}, {"heading": "4. Experiments", "text": "We perform several experiments to validate different design choices in NAT. We then evaluate the quality of our features by comparing them to state-of-the-art unsupervised approaches on several auxiliary supervised tasks, namely object classification on ImageNet and object classification and detection of PASCAL VOC 2007 (Everingham et al., 2010).\nTransfering the features. In order to measure the quality of our features, we measure their performance on transfer learning. We freeze the parameters of all the convolutional layers and overwrite the parameters of the MLP classifier with random Gaussian weights. We precisely follow the training and testing procedure that is specific to each of the datasets following Donahue et al. (2016).\nDatasets and baselines. We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009). This dataset is composed of 1, 281, 167 images that belong to 1, 000 object categories. For the transfer learning experiments, we also consider PASCAL VOC 2007. In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.e., autoencoder, GAN and BiGAN as reported in Donahue et al. (2016). We also compare to selfsupervised approaches, i.e., Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al. (2016). Finally we compare to state-ofthe-art hand-made features, i.e., SIFT with Fisher Vectors (SIFT+FV) (Sa\u0301nchez et al., 2013). They reduce the Fisher Vectors to a 4, 096 dimensional vector with PCA, and apply an 8, 192 unit 3-layer MLP on top."}, {"heading": "4.1. Detailed analysis", "text": "In this section, we validate some of our design choices, like the loss function, representations and the influences of some parameters on the quality of our features. All the experiments are run on ImageNet.\nSoftmax versus square loss. Table 1 compares the performance of an AlexNet trained with a softmax and a square loss. We report the accuracy on the validation set. The square loss requires the features to be unit normalized to avoid exploding gradients. As previously observed by Tygert et al. (2017), the performances are similar, hence validating our choice of loss function.\nEffect of image preprocessing. In supervised classification, image pre-processing is not frequently used, and transformations that remove information are usually avoided. In the unsupervised case, however, we observe that is it is preferable to work with simpler inputs as\nit avoids learning trivial features. In particular, we observe that using grayscale image gradients greatly helps our method, as mentioned in Sec. 3. In order to verify that this preprocessing does not destroy crucial information, we propose to evaluate its effect on supervised classification. We also compare with high-pass filtering. Table 2 shows the impact of this preprocessing methods on the accuracy of an AlexNet on the validation set of ImageNet. None of these pre-processings degrade the perform significantly, meaning that the information related to gradients are sufficient for object classification. This experiment confirms that such pre-processing does not lead to a significant drop in the upper bound performance for our model.\nContinuous versus discrete representations. We compare our choice for the target vectors to those commonly used for clustering, i.e., elements of the canonical basis of a k dimensional space. Such discrete representation make a strong assumption on the underlying structure of the problem, that it can be linearly separated in k different classes. This assumption holds for ImageNet giving a fair advantage to this discrete representation. We test this representation with k in {103, 104, 105}, which is a range well-suited for the 1, 000 classes of ImageNet. The matrix C contains n/k replications of k elements of the canonical basis. This assumes that the clusters are balanced, which is verified on ImageNet.\nWe compare these cluster-like representations to our continuous target vectors on the transfer task on ImageNet. Using discrete targets achieves an accuracy of 19%, which is significantly worse that our best performance, i.e., 33.5%. A possible explanation is that binary vectors induce sharp discontinuous distances between representations. Such distances are hard to optimize over and may result in early convergence to poorer local minima.\nEvolution of the features. In this experiment, we are interested in understanding how the quality of our features evolves with the optimization of our cost function. During the unsupervised training, we freeze the network every 20 epochs and learn a MLP classifier on top. We report the accuracy on the validation set of ImageNet. Figure 2 shows the evolution of the performance on this transfer task as we optimize for our unsupervised approach. The training performance improves monotonically with the epochs\nof the unsupervised training. This suggests that optimizing our objective function correlates with learning transferable features, i.e., our features do not destroy useful class-level information. On the other hand, the test accuracy seems to saturate after a hundred epochs. This suggests that the MLP is overfitting rapidly on pre-trained features.\nEffect of permutations. Assigning images to their target representations is a crucial feature of our approach. In this experiment, we are interested in understanding how frequently we should update this assignment. Indeed, updating the assignment, even partially, is relatively costly and may not be required to achieve good performance. Figure 2 shows the transfer accuracies on ImageNet as a function of the frequency of these updates. The model is quite robust to choice of frequency, with a test accuracy always above 30%. Interestingly, the accuracy actually degrades slightly with high frequency. A possible explanation is that the network overfits rapidly to its own output, leading to relatively worse features. In practice, we observe that updating the assignment matrix every 3 epochs offers a good trade-off between performance and accuracy.\nVisualizing the filters. Figure 4 shows a comparison between the first convolutional layer of an AlexNet trained with and without supervision. Both take grayscale gradient images as input. The visualization are obtained by composing the Sobel filtering with the filters of the first layer of the AlexNet. Unsupervised filters are slightly less sharp than their supervised counterpart, but still maintain edge and orientation information.\nNearest neighbor queries. Our loss optimizes a distance between features and fixed vectors. This means that looking at the distance between features should provide some information about the type of structure that our model cap-\ntures. Given a query image x, we compute its feature f\u03b8(x) and search for its nearest neighbors according to the `2 distance. Figure 3 shows images and their nearest neighbors.\nThe features capture relatively complex structures in images. Objects with distinctive structures, like trunks or fruits, are well captured by our approach. However, this information is not always related to true labels. For example, the image of bird over the sea is matched to images capturing information about the sea or the sky rather than\nthe bird."}, {"heading": "4.2. Comparison with the state of the art", "text": "We report results on the transfer task both on ImageNet and PASCAL VOC 2007. In both cases, the model is trained on ImageNet.\nImageNet classification. In this experiment, we evaluate the quality of our features for the object classification task of ImageNet. Note that in this setup, we build the unsupervised features on images that correspond to predefined image categories. Even though we do not have access to category labels, the data itself is biased towards these classes. In order to evaluate the features, we freeze the layers up to the last convolutional layer and train the classifier with supervision. This experimental setting follows Noroozi & Favaro (2016).\nWe compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.e., Donahue et al. (2016). Note that self-supervised approaches use losses specifically designed for visual features. Like BiGANs (Donahue et al., 2016), NAT does not make any assumption about the domain but of the structure of its features. Table 3 compares NAT with these approaches.\nAmong unsupervised approaches, NAT compares favorably to BiGAN (Donahue et al., 2016). Interestingly, the performance of NAT are slightly better than self-supervised\nmethods, even though we do not explicitly use domainspecific clues in images or videos to guide the learning. While all the models provide performance in the 30\u2212 36% range, it is not clear if they all learn the same features. Finally, all the unsupervised deep features are outperformed by hand-made features, in particular Fisher Vectors with SIFT descriptors. This baseline uses a slightly bigger MLP for the classifier and its performance can be improved by 2.2% by bagging 8 of these models. This difference of 20% in accuracy shows that unsupervised deep features are still quite far from the state-of-the-arts among all unsupervised features.\nTransferring to PASCAL VOC 2007. We carry out a second transfer experiment on the PASCAL VOC dataset, on the classification and detection tasks. The model is trained on ImageNet. Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kra\u0308henbu\u0308hl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al. (2016).\nAs with the ImageNet classification task, our performance is on par with self-supervised approaches, for both detection and classification. Among purely unsupervised approaches, we outperform standard approaches like autoencoders or GANs by a large margin. Our model also\nperforms slightly better than the best performing BiGAN model (Donahue et al., 2016). These experiments confirm our findings from the ImageNet experiments. Despite its simplicity, NAT learns feature that are as good as those obtained with more sophisticated and data-specific models."}, {"heading": "5. Conclusion", "text": "This paper presents a simple unsupervised framework to learn discriminative features. By aligning the output of a neural network to low-dimensional noise, we obtain features on par with state-of-the-art unsupervised learning approaches. Our approach explicitly aims at learning discriminative features, while most unsupervised approaches target surrogate problems, like image denoising or image generation. As opposed to self-supervised approaches, we make very few assumptions about the input space. This makes our appproach very simple and fast to train. Interestingly, it also shares some similarities with traditional clustering approaches as well as retrieval methods. While we show the potential of our approach on visual data, it will be interesting to try other domains. Finally, this work only considers simple noise distributions and alignment methods. A possible direction of research is to explore target distributions and alignments that are more informative. This also would strengthen the relation between NAT and methods based on distribution matching like the earth mover distance.\nAcknowledgement. We greatly thank Herve\u0301 Je\u0301gou for his help throughout the development of this project. We also thank Allan Jabri, Edouard Grave, Iasonas Kokkinos,\nLe\u0301on Bottou, Matthijs Douze and the rest of FAIR for their support and helpful discussion. Finally, we thank Richard Zhang, Jeff Donahue and Florent Perronnin for their help."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In ICCV,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Diffrac: a discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "In NIPS,", "citeRegEx": "Bach and Harchaoui,? \\Q2007\\E", "shortCiteRegEx": "Bach and Harchaoui", "year": 2007}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Finding actors and actions in movies", "author": ["P. Bojanowski", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "In ICCV,", "citeRegEx": "Bojanowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2013}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "In ECCV,", "citeRegEx": "Bojanowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2014}, {"title": "Learning feature representations with k-means", "author": ["A. Coates", "A. Ng"], "venue": "In Neural Networks: Tricks of the Trade. Springer,", "citeRegEx": "Coates and Ng,? \\Q2012\\E", "shortCiteRegEx": "Coates and Ng", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR,", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A. Efros"], "venue": "In CVPR,", "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "In NIPS,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "The PASCAL visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "In CVPR,", "citeRegEx": "Girshick,? \\Q2015\\E", "shortCiteRegEx": "Girshick", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Classes for fast maximum entropy training", "author": ["J. Goodman"], "venue": "In ICASSP,", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "In ICCV,", "citeRegEx": "Jayaraman and Grauman,? \\Q2015\\E", "shortCiteRegEx": "Jayaraman and Grauman", "year": 2015}, {"title": "A convex relaxation for weakly supervised classifiers", "author": ["A. Joulin", "F. Bach"], "venue": "In ICML,", "citeRegEx": "Joulin and Bach,? \\Q2012\\E", "shortCiteRegEx": "Joulin and Bach", "year": 2012}, {"title": "Discriminative clustering for image co-segmentation", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "Joulin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2010}, {"title": "Learning visual features from large weakly supervised data", "author": ["A. Joulin", "L. van der Maaten", "A. Jabri", "N. Vasilache"], "venue": null, "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological cybernetics,", "citeRegEx": "Kohonen,? \\Q1982\\E", "shortCiteRegEx": "Kohonen", "year": 1982}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Doersch", "Carl", "Donahue", "Jeff", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1511.06856,", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2015}, {"title": "Discriminative clustering by regularized information maximization", "author": ["A. Krause", "P. Perona", "R.G. Gomes"], "venue": "In NIPS,", "citeRegEx": "Krause et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval research logistics quarterly,", "citeRegEx": "Kuhn,? \\Q1955\\E", "shortCiteRegEx": "Kuhn", "year": 1955}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning deep parsimonious representations", "author": ["R. Liao", "A. Schwing", "R. Zemel", "R. Urtasun"], "venue": "In NIPS,", "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "Transactions on information theory,", "citeRegEx": "Lloyd,? \\Q1982\\E", "shortCiteRegEx": "Lloyd", "year": 1982}, {"title": "Object recognition from local scale-invariant features", "author": ["D. Lowe"], "venue": "In ICCV,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "In NIPS,", "citeRegEx": "Mairal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2014}, {"title": "neural-gas\u201d network learns topologies", "author": ["T. Martinetz", "Schulten", "K. A"], "venue": null, "citeRegEx": "Martinetz et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Martinetz et al\\.", "year": 1991}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "In ICANN,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "arXiv preprint arXiv:1603.09246,", "citeRegEx": "Noroozi and Favaro,? \\Q2016\\E", "shortCiteRegEx": "Noroozi and Favaro", "year": 2016}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": null, "citeRegEx": "Pathak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2016}, {"title": "Learning to segment object candidates", "author": ["P.O. Pinheiro", "R. Collobert", "P. Dollar"], "venue": "In NIPS,", "citeRegEx": "Pinheiro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pinheiro et al\\.", "year": 2015}, {"title": "Linking people in videos with their names using coreference resolution", "author": ["V. Ramanathan", "A. Joulin", "P. Liang", "L. Fei-Fei"], "venue": "In ECCV,", "citeRegEx": "Ramanathan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2014}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y.L. Boureau", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Razavian", "A. Sharif", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "In arXiv 1403.6382,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "A metric for distributions with applications to image databases", "author": ["Y. Rubner", "C. Tomasi", "L.J. Guibas"], "venue": "In ICCV,", "citeRegEx": "Rubner et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Rubner et al\\.", "year": 1998}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": null, "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2013}, {"title": "Colocalization in real-world images", "author": ["K. Tang", "A. Joulin", "Li", "L.-J", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Scale-invariant learning and convolutional networks", "author": ["M. Tygert", "S. Chintala", "A. Szlam", "Y. Tian", "W. Zaremba"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Tygert et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tygert et al\\.", "year": 2017}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Clustering of the selforganizing", "author": ["J. Vesanto", "E. Alhoniemi"], "venue": "map. Transactions on neural networks,", "citeRegEx": "Vesanto and Alhoniemi,? \\Q2000\\E", "shortCiteRegEx": "Vesanto and Alhoniemi", "year": 2000}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "JMLR, 11(Dec):3371\u20133408,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "In ICCV,", "citeRegEx": "Wang and Gupta,? \\Q2015\\E", "shortCiteRegEx": "Wang and Gupta", "year": 2015}, {"title": "Unsupervised deep embedding for clustering analysis", "author": ["J. Xie", "R. Girshick", "A. Farhadi"], "venue": "In ICML,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2004}, {"title": "Joint unsupervised learning of deep representations and image clusters", "author": ["J. Yang", "D. Parikh", "D. Batra"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A. Efros"], "venue": "In ECCV,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Stacked What-Where Auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. LeCun"], "venue": "In Workshop at ICLR,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al.", "startOffset": 60, "endOffset": 97}, {"referenceID": 29, "context": "In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al.", "startOffset": 60, "endOffset": 97}, {"referenceID": 27, "context": ", 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.", "startOffset": 50, "endOffset": 92}, {"referenceID": 17, "context": ", 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.", "startOffset": 50, "endOffset": 92}, {"referenceID": 14, "context": ", 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.", "startOffset": 84, "endOffset": 100}, {"referenceID": 40, "context": ", 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al., 2015).", "startOffset": 123, "endOffset": 146}, {"referenceID": 43, "context": "Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al.", "startOffset": 145, "endOffset": 188}, {"referenceID": 38, "context": "Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al.", "startOffset": 145, "endOffset": 188}, {"referenceID": 22, "context": ", 2014) or weaklysupervised data (Joulin et al., 2016).", "startOffset": 33, "endOffset": 54}, {"referenceID": 10, "context": "Several strategies exist to learn deep convolutional features with no annotation (Donahue et al., 2016).", "startOffset": 81, "endOffset": 103}, {"referenceID": 9, "context": "They either try to capture a signal from the source as a form of selfsupervision (Doersch et al., 2015; Wang & Gupta, 2015) or learn the underlying distribution of images (Vincent et al.", "startOffset": 81, "endOffset": 123}, {"referenceID": 10, "context": "While some of these approaches obtain promising performance in transfer learning (Donahue et al., 2016; Wang & Gupta, 2015), they do not explicitly aim to learn discriminative features.", "startOffset": 81, "endOffset": 123}, {"referenceID": 11, "context": "Some attempts were made with retrieval based approaches (Dosovitskiy et al., 2014) and clustering (Yang et al.", "startOffset": 56, "endOffset": 82}, {"referenceID": 54, "context": ", 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets.", "startOffset": 23, "endOffset": 61}, {"referenceID": 30, "context": ", 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets.", "startOffset": 23, "endOffset": 61}, {"referenceID": 24, "context": "Similar to self-organizing maps (Kohonen, 1982; Martinetz & Schulten, 1991), we map deep features to a set of predefined representations in a low dimensional space.", "startOffset": 32, "endOffset": 75}, {"referenceID": 31, "context": "Our approach also shares some similarities with standard clustering approches like k-means (Lloyd, 1982) or discriminative clustering (Bach & Harchaoui, 2007).", "startOffset": 91, "endOffset": 104}, {"referenceID": 7, "context": "In addition, we propose an online algorithm able to scale to massive image databases like ImageNet (Deng et al., 2009).", "startOffset": 99, "endOffset": 118}, {"referenceID": 47, "context": "This is achieved by using a quadratic loss as in (Tygert et al., 2017) and a fast approximation of the Hungarian algorithm.", "startOffset": 49, "endOffset": 70}, {"referenceID": 27, "context": "We show the potential of our approach by training end-to-end on ImageNet a standard architecture, namely AlexNet (Krizhevsky et al., 2012) with no supervision.", "startOffset": 113, "endOffset": 138}, {"referenceID": 10, "context": "We test the quality of our features on several image classification problems, following the setting of Donahue et al. (2016). We are on par with state-of-the-art unsupervised and self-supervised learning approaches while being much simpler to train and to scale.", "startOffset": 103, "endOffset": 125}, {"referenceID": 33, "context": "Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014).", "startOffset": 99, "endOffset": 165}, {"referenceID": 11, "context": "Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014).", "startOffset": 99, "endOffset": 165}, {"referenceID": 52, "context": "Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.", "startOffset": 44, "endOffset": 100}, {"referenceID": 54, "context": "Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.", "startOffset": 44, "endOffset": 100}, {"referenceID": 30, "context": "Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.", "startOffset": 44, "endOffset": 100}, {"referenceID": 11, "context": ", 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion.", "startOffset": 8, "endOffset": 242}, {"referenceID": 11, "context": ", 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion. In our work, we train the convnet end-to-end with a loss that shares similarities with k-means. Closer to our work, Dosovitskiy et al. (2014) proposes to train convnets by solving a retrieval problem.", "startOffset": 8, "endOffset": 480}, {"referenceID": 15, "context": "Traditional examples of this approach are variational autoencoders (Kingma & Welling, 2013), generative adversarial networks (Goodfellow et al., 2014), and to a lesser extent, noisy autoencoders (Vincent et al.", "startOffset": 125, "endOffset": 150}, {"referenceID": 50, "context": ", 2014), and to a lesser extent, noisy autoencoders (Vincent et al., 2010).", "startOffset": 52, "endOffset": 74}, {"referenceID": 15, "context": "Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.", "startOffset": 63, "endOffset": 131}, {"referenceID": 8, "context": "Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.", "startOffset": 63, "endOffset": 131}, {"referenceID": 10, "context": "Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.", "startOffset": 63, "endOffset": 131}, {"referenceID": 8, "context": ", 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. While these models cannot learn an inverse mapping, Donahue et al. (2016) recently proposed to add an encoder to extract visual features from GANs.", "startOffset": 8, "endOffset": 254}, {"referenceID": 9, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016).", "startOffset": 107, "endOffset": 174}, {"referenceID": 39, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016).", "startOffset": 107, "endOffset": 174}, {"referenceID": 36, "context": "In the same vein as word2vec (Mikolov et al., 2013), Doersch et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 0, "context": "Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015).", "startOffset": 126, "endOffset": 195}, {"referenceID": 8, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as word2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features.", "startOffset": 108, "endOffset": 251}, {"referenceID": 8, "context": "Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as word2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features. Noroozi & Favaro (2016) have further extended this work.", "startOffset": 108, "endOffset": 346}, {"referenceID": 0, "context": "Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015). In particular, Wang & Gupta (2015) show that such features provide promising performance on ImageNet.", "startOffset": 127, "endOffset": 232}, {"referenceID": 2, "context": "Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).", "startOffset": 60, "endOffset": 123}, {"referenceID": 42, "context": "Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).", "startOffset": 60, "endOffset": 123}, {"referenceID": 35, "context": "Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).", "startOffset": 60, "endOffset": 123}, {"referenceID": 42, "context": "The decoder is often a fully connected network (Ranzato et al., 2007) or a deconvolutional network (Masci et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 35, "context": ", 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 56, "context": ", 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 24, "context": "This family of unsupervised methods aims at learning a low dimensional representation of the data that preserves certain topological properties (Kohonen, 1982; Vesanto & Alhoniemi, 2000).", "startOffset": 144, "endOffset": 186}, {"referenceID": 53, "context": "Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012).", "startOffset": 76, "endOffset": 159}, {"referenceID": 26, "context": "Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012).", "startOffset": 76, "endOffset": 159}, {"referenceID": 21, "context": "It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al.", "startOffset": 96, "endOffset": 136}, {"referenceID": 46, "context": "It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al.", "startOffset": 96, "endOffset": 136}, {"referenceID": 3, "context": ", 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014).", "startOffset": 32, "endOffset": 88}, {"referenceID": 41, "context": ", 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014).", "startOffset": 32, "endOffset": 88}, {"referenceID": 22, "context": ", 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012). In particular, Bach & Harchaoui (2007) shows that the ridge regression loss could be use to learn discriminative clusters.", "startOffset": 32, "endOffset": 114}, {"referenceID": 3, "context": ", 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014). In this work, we show that a similar framework can be designed for neural networks. As opposed to Xu et al. (2004), we address the empty assignment problems by restricting the set of possible reassignments to permutations rather than using global linear constrains the assignments.", "startOffset": 33, "endOffset": 205}, {"referenceID": 16, "context": "However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001).", "startOffset": 111, "endOffset": 126}, {"referenceID": 16, "context": "However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001). While there are workarounds to scale these losses to large output spaces, Tygert et al. (2017) has recently shown that using a squared `2 distance works well in many supervised settings, as long as the final activations are unit normalized.", "startOffset": 112, "endOffset": 223}, {"referenceID": 53, "context": "(2) would lead to a representation collapsing problem: all the images would be assigned to the same representation (Xu et al., 2004).", "startOffset": 115, "endOffset": 132}, {"referenceID": 11, "context": "If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n.", "startOffset": 77, "endOffset": 103}, {"referenceID": 11, "context": "If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n. On the other hand, if d is smaller than n, this formulation is equivalent to the discriminative clustering approach of Bach & Harchaoui (2007). Choosing such targets makes very strong assumptions on the nature of the underlying problem.", "startOffset": 77, "endOffset": 279}, {"referenceID": 44, "context": "In some sense, we are optimizing a crude approximation of the earth mover\u2019s distance between the distribution of deep features and a given target distribution (Rubner et al., 1998).", "startOffset": 159, "endOffset": 180}, {"referenceID": 28, "context": "(5) to this set, the linear assignment problem in P can be solved exactly with the Hungarian algorithm (Kuhn, 1955), but at the prohibitive cost of O(n).", "startOffset": 103, "endOffset": 115}, {"referenceID": 47, "context": "As noted by Tygert et al. (2017), batch normalization plays a crucial role when optimizing the l2 loss, as it avoids exploding gradients.", "startOffset": 12, "endOffset": 33}, {"referenceID": 47, "context": "The features are unit normalized for the square loss (Tygert et al., 2017).", "startOffset": 53, "endOffset": 74}, {"referenceID": 42, "context": "During transfer learning, we consider the output of the last convolutional layer as our features as in Razavian et al. (2014). We use the same multi-layer perceptron (MLP) as in Krizhevsky et al.", "startOffset": 103, "endOffset": 126}, {"referenceID": 27, "context": "We use the same multi-layer perceptron (MLP) as in Krizhevsky et al. (2012) for the classifier.", "startOffset": 51, "endOffset": 76}, {"referenceID": 32, "context": "Using this preprocessing is not surprising since most hand-made features like SIFT or HoG are based on image gradients (Lowe, 1999; Dalal & Triggs, 2005).", "startOffset": 119, "endOffset": 153}, {"referenceID": 27, "context": "In addition to this preprocessing, we also perform all the standard image transformations that are commonly applied in the supervised setting (Krizhevsky et al., 2012), such as random cropping and flipping of images.", "startOffset": 142, "endOffset": 167}, {"referenceID": 40, "context": "As in Ranzato et al. (2007), we use image gradients instead of the images to avoid trivial solutions like clustering according to colors.", "startOffset": 6, "endOffset": 28}, {"referenceID": 46, "context": "We project the output of the network on the `2 sphere as in Tygert et al. (2017). The network is trained with SGD with a batch size of 256.", "startOffset": 60, "endOffset": 81}, {"referenceID": 10, "context": "For the transfer learning experiments, we follow the guideline described in Donahue et al. (2016). 4.", "startOffset": 76, "endOffset": 98}, {"referenceID": 12, "context": "We then evaluate the quality of our features by comparing them to state-of-the-art unsupervised approaches on several auxiliary supervised tasks, namely object classification on ImageNet and object classification and detection of PASCAL VOC 2007 (Everingham et al., 2010).", "startOffset": 246, "endOffset": 271}, {"referenceID": 10, "context": "We precisely follow the training and testing procedure that is specific to each of the datasets following Donahue et al. (2016).", "startOffset": 106, "endOffset": 128}, {"referenceID": 7, "context": "We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009).", "startOffset": 71, "endOffset": 90}, {"referenceID": 27, "context": "In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.", "startOffset": 43, "endOffset": 68}, {"referenceID": 45, "context": ", SIFT with Fisher Vectors (SIFT+FV) (S\u00e1nchez et al., 2013).", "startOffset": 37, "endOffset": 59}, {"referenceID": 6, "context": "We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009). This dataset is composed of 1, 281, 167 images that belong to 1, 000 object categories. For the transfer learning experiments, we also consider PASCAL VOC 2007. In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.e., autoencoder, GAN and BiGAN as reported in Donahue et al. (2016). We also compare to selfsupervised approaches, i.", "startOffset": 72, "endOffset": 451}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al.", "startOffset": 2, "endOffset": 47}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al.", "startOffset": 2, "endOffset": 69}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al.", "startOffset": 2, "endOffset": 90}, {"referenceID": 0, "context": ", Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al. (2016). Finally we compare to state-ofthe-art hand-made features, i.", "startOffset": 2, "endOffset": 114}, {"referenceID": 47, "context": "As previously observed by Tygert et al. (2017), the performances are similar, hence validating our choice of loss function.", "startOffset": 26, "endOffset": 47}, {"referenceID": 9, "context": "We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.", "startOffset": 61, "endOffset": 123}, {"referenceID": 55, "context": "We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.", "startOffset": 61, "endOffset": 123}, {"referenceID": 10, "context": "Like BiGANs (Donahue et al., 2016), NAT does not make any assumption about the domain but of the structure of its features.", "startOffset": 12, "endOffset": 34}, {"referenceID": 9, "context": "We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.e., Donahue et al. (2016). Note that self-supervised approaches use losses specifically designed for visual features.", "startOffset": 82, "endOffset": 182}, {"referenceID": 10, "context": "Among unsupervised approaches, NAT compares favorably to BiGAN (Donahue et al., 2016).", "startOffset": 63, "endOffset": 85}, {"referenceID": 45, "context": "SIFT+FV (S\u00e1nchez et al., 2013) 55.", "startOffset": 8, "endOffset": 30}, {"referenceID": 9, "context": "8 Doersch et al. (2015) 30.", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": "8 Doersch et al. (2015) 30.4 Zhang et al. (2016) 35.", "startOffset": 2, "endOffset": 49}, {"referenceID": 9, "context": "8 Doersch et al. (2015) 30.4 Zhang et al. (2016) 35.2 Noroozi & Favaro (2016) 38.", "startOffset": 2, "endOffset": 78}, {"referenceID": 10, "context": "BiGAN (Donahue et al., 2016) 32.", "startOffset": 6, "endOffset": 28}, {"referenceID": 10, "context": ", BiGAN (Donahue et al., 2016).", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": ", BiGAN (Donahue et al., 2016). Noroozi & Favaro (2016) uses a significantly larger amount of features than the original AlexNet.", "startOffset": 9, "endOffset": 56}, {"referenceID": 25, "context": "We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015).", "startOffset": 94, "endOffset": 119}, {"referenceID": 10, "context": "Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach.", "startOffset": 98, "endOffset": 120}, {"referenceID": 10, "context": "Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al.", "startOffset": 98, "endOffset": 608}, {"referenceID": 10, "context": "Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al. (2016).", "startOffset": 98, "endOffset": 639}, {"referenceID": 10, "context": "4 BiGAN (Donahue et al., 2016) 52.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "The GAN and autoencoder baselines are from Donahue et al. (2016). We report mean average prevision as customary on PASCAL VOC.", "startOffset": 43, "endOffset": 65}, {"referenceID": 10, "context": "performs slightly better than the best performing BiGAN model (Donahue et al., 2016).", "startOffset": 62, "endOffset": 84}], "year": 2017, "abstractText": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.", "creator": "LaTeX with hyperref package"}}}