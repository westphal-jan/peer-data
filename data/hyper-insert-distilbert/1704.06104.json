{"id": "1704.06104", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Neural End-to-End Learning for Computational Argumentation Mining", "abstract": "we investigate neural techniques for testing end - peer to - the end computational argumentation mining. ideally we directly frame the problem as providing a token - consistency based posterior dependency parsing as well as in a token - alignment based sequence tagging model, basically including offering a multi - temporal task learning bias setup. citing contrary approach to models that potentially operate strictly on the argument component processor level, we actually find that framing approaching the problem such as dependency parsing leads to disastrous subpar performance results. in good contrast, less complexity complex ( increasingly local ) graph tagging models based externally on newer bilstms perform robustly across classification filtering scenarios, being explicitly able yet to catch long - range memory dependencies still inherent closely to the semantic argumentation correlation mining problem. in moreover, we find that implementing jointly biased learning'natural'validation subtasks, in a multi - task strategy learning setup, improves performance.", "histories": [["v1", "Thu, 20 Apr 2017 12:20:43 GMT  (62kb)", "https://arxiv.org/abs/1704.06104v1", "To be published at ACL 2017"], ["v2", "Sat, 22 Apr 2017 12:20:45 GMT  (59kb)", "http://arxiv.org/abs/1704.06104v2", "To be published at ACL 2017"]], "COMMENTS": "To be published at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["steffen eger", "johannes daxenberger", "iryna gurevych"], "accepted": true, "id": "1704.06104"}, "pdf": {"name": "1704.06104.pdf", "metadata": {"source": "CRF", "title": "Neural End-to-End Learning for Computational Argumentation Mining", "authors": ["Steffen Eger", "Johannes Daxenberger", "Iryna Gurevych"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n06 10\n4v 2\n[ cs\n.C L\n] 2\n2 A\npr 2\n01 7\nWe investigate neural techniques for endto-end computational argumentation mining (AM). We frame AM both as a tokenbased dependency parsing and as a tokenbased sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch longrange dependencies inherent to the AM problem. Moreover, we find that jointly learning \u2018natural\u2019 subtasks, in a multi-task learning setup, improves performance."}, {"heading": "1 Introduction", "text": "Computational argumentation mining (AM) deals with finding argumentation structures in text. This involves several subtasks, such as: (a) separating argumentative units from non-argumentative units, also called \u2018component segmentation\u2019; (b) classifying argument components into classes such as \u201cPremise\u201d or \u201cClaim\u201d; (c) finding relations between argument components; (d) classifying relations into classes such as \u201cSupport\u201d or \u201cAttack\u201d (Persing and Ng, 2016; Stab and Gurevych, 2017).\nThus, AM would have to detect claims and premises (reasons) in texts such as the following, where premise P supports claim C:\nSince it killed many marine livesP ,\n\u273f\u273f\u273f\u273f\u273f\u273f\u273f tourism \u273f\u273f\u273f has \u273f\u273f\u273f\u273f\u273f\u273f\u273f\u273f\u273f\u273f threatened \u273f\u273f\u273f\u273f\u273f\u273f natureC .\nArgument structures in real texts are typically much more complex, cf. Figure 1.\nWhile different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text. Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features.\nHand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017). Accordingly, datasets typically differ with respect to their annotation of (often rather complex) argument structure. Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task. The same critique applies to the designing of ILP constraints. Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to error propagation rather than exploiting interrelationships between variables. In contrast to this, we investigate neural techniques for end-to-end learning in computational AM, which do not require the hand-crafting of features or constraints. The models we survey also all capture some notion of \u201cjoint\u201d\u2014rather than \u201cpipeline\u201d\u2014learning. We investigate several approaches.\nFirst, we frame the end-to-end AM problem as a dependency parsing problem. Dependency parsing may be considered a natural choice for AM, because argument structures often form trees,\nor closely resemble them (see \u00a73). Hence, it is not surprising that \u2018discourse parsing\u2019 (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given.\nSecond, we frame the problem as a sequence tagging problem. This is a natural choice especially for component identification (segmentation and classification), which is a typical entity recognition problem for which BIO tagging is a standard approach, pursued in AM, e.g., by Habernal and Gurevych (2016). The challenge in the end-to-end setting is to also include relations into the tagging scheme, which we realize by coding the distances between linked components into the tag label. Since related entities in AM are oftentimes several dozens of tokens apart from each other, neural sequence tagging models are in principle ideal candidates for such a framing because they can take into account long-range dependencies\u2014something that is inherently difficult to capture with traditional feature-based tagging models such as conditional random fields (CRFs).\nThird, we frame AM as a multi-task (tagging) problem (Caruana, 1997; Collobert and Weston, 2008). We experiment with subtasks of AM\u2014e.g., component identification\u2014as auxiliary tasks and investigate whether this improves performance on the AM problem. Adding such subtasks can be seen as analogous to de-coupling, e.g., component identification from the full AM problem.\nFourth, we evaluate the model of Miwa and Bansal (2016) that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations. As such, this model makes fewer assumptions than our dependency parsing and tagging approaches.\nThe contributions of this paper are as follows. (1) We present the first neural end-to-end solutions to computational AM. (2) We show that several of them perform better than the state-of-theart joint ILP model. (3) We show that a framing of AM as a token-based dependency parsing problem is ineffective\u2014in contrast to what has been\nproposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments. Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.1"}, {"heading": "2 Related Work", "text": "AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016).\nMost works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types."}, {"heading": "3 Data", "text": "We use the dataset of persuasive essays (PE) from Stab and Gurevych (2017), which contains student essays written in response to controversial topics such as \u201ccompetition or cooperation\u2014which is better?\u201d\nAs Table 1 details, the corpus consists of 402 essays, 80 of which are reserved for testing. The an-\n1Scripts that document how we ran our experiments are available from https://github.com/UKPLab/acl2017-neural_end2end_AM.\nnotation distinguishes between major claims (the central position of an author with respect to the essay\u2019s topic), claims (controversial statements that are either for or against the major claims), and premises, which give reasons for claims or other premises and either support or attack them. Overall, there are 751 major claims, 1506 claims, and 3832 premises. There are 5338 relations, most of which are supporting relations (>90%).\nThe corpus has a special structure, illustrated in Figure 1. First, major claims relate to no other components. Second, claims always relate to all other major claims.2 Third, each premise relates to exactly one claim or premise. Thus, the argument structure in each essay is\u2014almost\u2014a tree. Since there may be several major claims, each claim potentially connects to multiple targets, violating the tree structure. This poses no problem, however, since we can \u201closs-lessly\u201d re-link the claims to one of the major claims (e.g., the last major claim in a document) and create a special root node to which the major claims link. From this tree, the actual graph can be uniquely reconstructed.\nThere is another peculiarity of this data. Each essay is divided into paragraphs, of which there are 2235 in total. The argumentation structure is completely contained within a paragraph, except, possibly, for the relation from claims to major claims. Paragraphs have an average length of 66 tokens and are therefore much shorter than essays, which have an average length of 368 tokens. Thus, prediction on the paragraph level is easier than\n2All MCs are considered as equivalent in meaning.\nprediction on the essay level, because there are fewer components in a paragraph and hence fewer possibilities of source and target components in argument relations. The same is true for component classification: a paragraph can never contain premises only, for example, since premises link to other components."}, {"heading": "4 Models", "text": "This section describes our neural network framings for end-to-end AM.\nSequence Tagging is the problem of assigning each element in a stream of input tokens a label. In a neural context, the natural choice for tagging problems are recurrent neural nets (RNNs) in which a hidden vector representation ht at time point t depends on the previous hidden vector representation ht\u22121 and the input xt. In this way, an infinite window (\u201clong-range dependencies\u201d) around the current input token xt can be taken into account when making an output prediction yt. We choose particular RNNs, namely, LSTMs (Hochreiter and Schmidhuber, 1997), which are popular for being able to address vanishing/exploding gradients problems. In addition to considering a left-to-right flow of information, bidirectional LSTMs (BL) also capture information to the right of the current input token.\nThe most recent generation of neural tagging models add label dependencies to BLs, so that successive output decisions are not made independently. This class of models is called BiLSTM-\nCRF (BLC) (Huang et al., 2015). The model of Ma and Hovy (2016) adds convolutional neural nets (CNNs) on the character-level to BiLSTMCRFs, leading to BiLSTM-CRF-CNN (BLCC) models. The character-level CNN may address problems of out-of-vocabulary words, that is, words not seen during training.\nAM as Sequence Tagging: We frame AM as the following sequence tagging problem. Each input token has an associated label from Y , where\nY = {(b, t, d, s) | b \u2208 {B, I,O}, t \u2208 {P,C,MC,\u22a5},\nd \u2208 {. . . ,\u22122,\u22121, 1, 2, . . . ,\u22a5}, s \u2208 {Supp,Att, For,Ag,\u22a5}}.\n(1)\nIn other words, Y consists of all four-tuples (b, t, d, s) where b is a BIO encoding indicating whether the current token is non-argumentative (O) or begins (B) or continues (I) a component; t indicates the type of the component (claim C, premise P, or major claim MC for our data). Moreover, d encodes the distance\u2014measured in number of components\u2014between the current component and the component it relates to. We encode the same d value for each token in a given component. Finally, s is the relation type (\u201cstance\u201d) between two components and its value may be Support (Supp), Attack (Att), or For or Against (Ag). We also have a special symbol \u22a5 that indicates when a particular slot is not filled: e.g., a nonargumentative unit (b = O) has neither component type, nor relation, nor relation type. We refer to this framing as STagT (for \u201cSimple Tagging\u201d), where T refers to the tagger used. For the example from \u00a71, our coding would hence be:\nSince it killed many (O,\u22a5,\u22a5,\u22a5) (B,P,1,Supp) (I,P,1,Supp) (I,P,1,Supp)\nmarine lives , tourism (I,P,1,Supp) (I,P,1,Supp) (O,\u22a5,\u22a5,\u22a5) (B,C,\u22a5,For)\nhas threatened nature . (I,C,\u22a5,For) (I,C,\u22a5,For) (I,C,\u22a5,For) (O,\u22a5, \u22a5, \u22a5)\nWhile the size of the label set Y is potentially infinite, we would expect it to be finite even in a potentially infinitely large data set, because humans also have only finite memory and are therefore expected to keep related components close in textual space. Indeed, as Figure 2 shows, in our PE essay data set about 30% of all relations between components have distance \u22121, that is, they follow the claim or premise that they attach to. Overall, around 2/3 of all relation distances d lie\nin {\u22122,\u22121, 1}. However, the figure also illustrates that there are indeed long-range dependencies: distance values between \u221211 and +10 are observed in the data.\nMulti-Task Learning Recently, there has been a lot of interest in so-called multi-task learning (MTL) scenarios, where several tasks are learned jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; He\u0301ctor and Plank, 2017). It has been argued that such learning scenarios are closer to human learning because humans often transfer knowledge between several domains/tasks. In a neural context, MTL is typically implemented via weight sharing: several tasks are trained in the same network architecture, thereby sharing a substantial portion of network\u2019s parameters. This forces the network to learn generalized representations.\nIn the MTL framework of S\u00f8gaard and Goldberg (2016) the underlying model is a BiLSTM with several hidden layers. Then, given different tasks, each task k \u2018feeds\u2019 from one of the hidden layers in the network. In particular, the hidden states encoded in a specific layer are fed into a multiclass classifier fk. The same work has demonstrated that this MTL protocol may be successful when there is a hierarchy between tasks and \u2018lower\u2019 tasks feed from lower layers.\nAM as MTL: We use the same framework STagT for modeling AM as MTL. However, we in addition train auxiliary tasks in the network\u2014 each with a distinct label set Y \u2032.\nDependency Parsing methods can be classified into graph-based and transition-based approaches (Kiperwasser and Goldberg, 2016). Transitionbased parsers encode the parsing problem as a sequence of configurations which may be modified by application of actions such as shift, reduce,\netc. The system starts with an initial configuration in which sentence elements are on a buffer and a stack, and a classifier successively decides which action to take next, leading to different configurations. The system terminates after a finite number of actions, and the parse tree is read off the terminal configuration. Graph-based parsers solve a structured prediction problem in which the goal is learning a scoring function over dependency trees such that correct trees are scored above all others.\nTraditional dependency parsers used handcrafted feature functions that look at \u201ccore\u201d elements such as \u201cword on top of the stack\u201d, \u201cPOS of word on top of the stack\u201d, and conjunctions of core features such as \u201cword is X and POS is Y\u201d (see McDonald et al. (2005)). Most neural parsers have not entirely abandoned feature engineering. Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)\u2019s neural parser associates each stack with a \u201cstack LSTM\u201d that encodes their contents. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary. Moreover, their parser has thus access to any part of the input, its history and stack contents.\nAM as Dependency Parsing: To frame a problem as a dependency parsing problem, each instance of the problem must be encoded as a directed tree, where tokens have heads, which in turn are labeled. For end-to-end AM, we propose the framing illustrated in Figure 3. We highlight two design decisions, the remaining are analogous and/or can be read off the figure.\n\u2022 The head of each non-argumentative text token is the document terminating token END,\nwhich is a punctuation mark in all our cases. The label of this link is O, the symbol for non-argumentative units.\n\u2022 The head of each token in a premise is the first token of the claim or premise that it\nlinks to. The label of each of these links is (b,P,Supp) or (b,P,Att) depending on whether a premise \u201csupports\u201d or \u201cattacks\u201d a claim or premise; b \u2208 {B, I}.\nLSTM-ER Miwa and Bansal (2016) present a neural end-to-end system for identifying both entities as well as relations between them. Their entity detection system is a BLC-type tagger and their relation detection system is a neural net that predicts a relation for each pair of detected entities. This relation module is a TreeLSTM model that makes use of dependency tree information. In addition to de-coupling entity and relation detection but jointly modeling them,3 pretraining on entities and scheduled sampling (Bengio et al., 2015) is applied to prevent low performance at early training stages of entity detection and relation classification. To adapt LSTM-ER for the argument structure encoded in the PE dataset, we model three types of entities (premise, claim, major claim) and four types of relations (for, against, support, attack).\nWe use the feature-based ILP model from Stab and Gurevych (2017) as a comparison system. This system solves the subtasks of AM\u2014component segmentation, component classification, relation detection and classification\u2014 independently. Afterwards, it defines an ILP model with various constraints to enforce valid argumentation structure. As features it uses structural, lexical, syntactic and context features, cf. Stab and Gurevych (2017) and Persing and Ng (2016).\nSummarizing, we distinguish our framings in terms of modularity and in terms of their constraints. Modularity: Our dependency parsing framing and LSTM-ER are more modular than STagT because they de-couple relation information from entity information. However, (part of)\n3By \u2018de-coupling\u2019, we mean that both tasks are treated separately rather than merging entity and relation information in the same tag label (output space). Still, a joint model like that of Miwa and Bansal (2016) de-couples the two tasks in such a way that many model parameters are shared across the tasks, similarly as in MTL.\nthis modularity can be regained by using STagT in an MTL setting. Moreover, since entity and relation information are considerably different, such a de-coupling may be advantageous. Constraints: LSTM-ER can, in principle, model any kind of\u2014 even many-to-many\u2014relationships between detected entities. Thus, it is not guaranteed to produce trees, as we observe in AM datasets. STagT also does not need to produce trees, but it more severely restricts search space than does LSTMER: each token/component can only relate to one (and not several) other tokens/components. The same constraint is enforced by the dependency parsing framing. All of the tagging modelings, including LSTM-ER, are local models whereas our parsing framing is a global model: it globally enforces a tree structure on the token-level.\nFurther remarks: (1) part of the TreeLSTM modeling inherent to LSTM-ER is ineffective for our data because this modeling exploits dependency tree structures on the sentence level, while relationships between components are almost never on the sentence level. In our data, roughly 92% of all relationships are between components that appear in different sentences. Secondly, (2) that a model enforces a constraint does not necessarily mean that it is more suitable for a respective task. It has frequently been observed that models tend to produce output consistent with constraints in their training data in such situations (Zhang et al., 2017; He\u0301ctor and Plank, 2017); thus, they have learned the constraints."}, {"heading": "5 Experiments", "text": "This section presents and discusses the empirical results for the AM framings outlined in \u00a74. We relegate issues of pre-trained word embeddings, hyperparameter optimization and further practical issues to the supplementary material. Links to software used as well as some additional error analysis can also be found there.\nEvaluation Metric We adopt the evaluation metric suggested in Persing and Ng (2016). This computes true positives TP, false positives FP, and false negatives FN, and from these calculates component and relation F1 scores as F1 = 2TP\n2TP+FP+FN .\nFor space reasons, we refer to Persing and Ng (2016) for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that\n\u2018matches\u2019. Persing and Ng (2016) define a notion of what we may term \u2018level \u03b1 matching\u2019: for example, at the 100% level (exact match) predicted and gold components must have exactly the same spans, whereas at the 50% level they must only share at least 50% of their tokens (approximate match). We refer to these scores as C-F1 (100%) and C-F1 (50%), respectively. For relations, an analogous F1 score is determined, which we denote by R-F1 (100%) and R-F1 (50%). We note that R-F1 scores depend on C-F1 scores because correct relations must have correct arguments. We also define a \u2018global\u2019 F1 score, which is the F1score of C-F1 and R-F1.\nMost of our results are shown in Table 2.\n(a) Dependency Parsing We show results for the two feature-based parsers MST (McDonald et al., 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al. (2015) (LSTM-Parser) and Kiperwasser and Goldberg (2016) (Kiperwasser). We train and test all parsers on the paragraph level, because training them on essay level was typically too memory-exhaustive.\nMST mostly labels only non-argumentative units correctly, except for recognizing individual major claims, but never finds their exact spans (e.g., \u201ctourism can create negative impacts on\u201d while the gold major claim is \u201cinternational tourism can create negative impacts on the destination countries\u201d). Mate is slightly better and in particular recognizes several major claims correctly. Kiperwasser performs decently on the approximate match level, but not on exact level. Upon inspection, we find that the parser often predicts \u2018too large\u2019 component spans, e.g., by including following punctuation. The best parser by far is the LSTM-Parser. It is over 100% better than Kiperwasser on exact spans and still several percentage points on approximate spans.\nHow does performance change when we switch to the essay level? For the LSTM-Parser, the best performance on essay level is 32.84%/47.44% CF1 (100%/50% level), and 9.11%/14.45% on RF1, but performance result varied drastically between different parametrizations. Thus, the performance drop between paragraph and essay level is in any case immense.\nSince the employed features of modern featurebased parsers are rather general\u2014such as distance between words or word identities\u2014we had expected them to perform much better. The mini-\nmal feature set employed by Kiperwasser is apparently not sufficient for accurate AM but still a lot more powerful than the hand-crafted feature approaches. We hypothesize that the LSTM-Parser\u2019s good performance, relative to the other parsers, is due to its encoding of the whole stack history\u2014 rather than just the top elements on the stack as in Kiperwasser\u2014 which makes it aware of much larger \u2018contexts\u2019. While the drop in performance from paragraph to essay level is expected, the LSTM-Parser\u2019s deterioration is much more severe than the other models\u2019 surveyed below. We believe that this is due to a mixture of the following: (1) \u2018capacity\u2019, i.e., model complexity, of the parsers\u2014 that is, risk of overfitting; and (2) few, but very long sequences on essay level\u2014that is, little training data (trees), paired with a huge search space on each train/test instance, namely, the number of possible trees on n tokens. See also our discussions below, particularly, our stability analysis.\n(b) Sequence Tagging For these experiments, we use the BLCC tagger from Ma and Hovy (2016) and refer to the resulting system as STagBLCC. Again, we observe that paragraph level is considerably easier than essay level; e.g., for relations, there is \u223c5% points increase from essay to paragraph level. Overall, STagBLCC is \u223c13% better than the best parser for C-F1 and \u223c11% better for R-F1 on the paragraph level. Our explanation is that taggers are simpler local models, and thus need less training data and are less prone to overfitting. Moreover, they can much better deal with the long sequences because they are largely invariant to length: e.g., it does in principle not matter, from a parameter estimation perspective, whether we train our taggers on two sequences of lengths n and m, respectively, or on\none long sequence of length n+m.\n(c) MTL As indicated, we use the MTL tagging framework from S\u00f8gaard and Goldberg (2016) for multi-task experiments. The underlying tagging framework is weaker than that of BLCC: there is no CNN which can take subword information into account and there are no dependencies between output labels: each tagging prediction is made independently of the other predictions. We refer to this system as STagBL.\nAccordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: For exact match, C-F1 values are about \u223c10% points below those of STagBLCC, while approximate match performances are much closer. Hence, the independence assumptions of the BL tagger apparently lead to more \u2018local\u2019 errors such as exact argument span identification (cf. error analysis). An analogous trend holds for argument relations.\nAdditional Tasks: We find that when we train STagBL with only its main task\u2014with label set Y as in Eq. (1)\u2014the overall result is worst. In contrast, when we include the \u2018natural subtasks\u2019 \u201cC\u201d (label set YC consists of the projection on the coordinates (b, t) in Y) and/or \u201cR\u201d (label set YR consists of the projection on the coordinates (d, s)), performance increases typically by a few percentage points. This indicates that complex sequence tagging may benefit when we train a \u201csublabeler\u201d in the same neural architecture, a finding that may be particularly relevant for morphological POS tagging (Mu\u0308ller et al., 2013). Unlike S\u00f8gaard and Goldberg (2016), we do not find that the optimal architecture is the one in which \u201clower\u201d tasks (such as C or R) feed from lower layers. In fact, in one of the best parametrizations\nthe C task and the full task feed from the same layer in the deep BiLSTM. Moreover, we find that the C task is consistently more helpful as an auxiliary task than the R task.\nOn essay level, (d) LSTM-ER performs very well on component identification (+5% C-F1 compared to STagBLCC), but rather poor on relation identification (-18% R-F1). Hence, its overall F1 on essay level is considerably below that of STagBLCC. In contrast, LSTM-ER trained and tested on paragraph level substantially outperforms all other systems discussed, both for component as well as for relation identification.\nWe think that its generally excellent performance on components is due to LSTM-ER\u2019s de-coupling of component and relation tasks. Our findings indicate that a similar result can be achieved for STagT via MTL when components and relations are included as auxiliary tasks, cf. Table 3. For example, the improvement of LSTM-ER over STagBLCC, for C-F1, roughly matches the increase for STagBL when including components and relations separately (Y-3:YC-3:YR-3) over not including them as auxiliary tasks (Y-3). Lastly, the better performance of LSTM-ER over STagBLCC for relations on paragraph level appears to be a consequence of its better performance on components. E.g., when both arguments are correctly predicted, STagBLCC has even higher chance of getting their relation correct than LSTM-ER (95.34% vs. 94.17%).\nWhy does LSTM-ER degrade so much on essay level for R-F1? As said, text sequences are much longer on essay level than on paragraph level\u2014 hence, there are on average many more entities on essay level. Thus, there are also many more possible relations between all entities discovered in a text\u2014namely, there are O(2m 2\n) possible relations between m discovered components. Due to its\ngenerality, LSTM-ER considers all these relations as plausible, while STagT does not (for any of choice of T ): e.g., our coding explicitly constrains each premise to link to exactly one other component, rather than to 0, . . . ,m possible components, as LSTM-ER allows. In addition, our explicit coding of distance values d biases the learner T to reflect the distribution of distance values found in real essays\u2014namely, that related components are typically close in terms of the number of components between them. In contrast, LSTM-ER only mildly prefers short-range dependencies over long-range dependencies, cf. Figure 4.\nThe (e) ILP has access to both paragraph and essay level information and thus has always more information than all neural systems compared to. Thus, it also knows in which paragraph in an essay it is. This is useful particularly for major claims, which always occur in first or last paragraphs in our data. Still, its performance is equal to or lower than that of LSTM-ER and STagBLCC when both are evaluated on paragraph level."}, {"heading": "Stability Analysis", "text": "Table 4 shows averages and standard deviations of two selected models, namely, the STagBLCC tagging framework as well as the LSTM-Parser over several different runs (different random initializations as well as different hyperparameters as discussed in the supplementary material). These results detail that the taggers have lower standard deviations than the parsers. The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%. As discussed above, we attribute this to the parsers\u2019 increased model capacity relative to the taggers, which makes them more prone to overfitting. Data scarcity is another very likely source of error in this context, as the parsers only observe 322 (though very rich) trees\nin the training data, while the taggers are always roughly trained on 120K tokens. On paragraph level, they do observe more trees, namely, 1786."}, {"heading": "Error analysis", "text": "A systematic source of errors for all systems is detecting exact argument spans (segmentation). For instance, the ILP system predicts the following premise: \u201cAs a practical epitome , students should be prepared to present in society after their graduation\u201d, while the gold premise omits the preceding discourse marker, and hence reads: \u201cstudents should be prepared to present in society after their graduation\u201d. On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries (Persing and Ng, 2016; Yang and Cardie, 2013). On the other hand, our results in Table 2 indicate that the neural taggers BLCC and BLC (in the LSTMER model) are much better at such exact identification than either the ILP model or the neural parsers. While the parsers\u2019 problems are most likely due to model complexity, we hypothesize that the ILP model\u2019s increased error rates stem from a weaker underlying tagging model (featurebased CRF vs. BiLSTM) and/or its features.4 In fact, as Table 5 shows, the macro-F1 scores5 on only the component segmentation tasks (BIO labeling) are substantially higher for both LSTMER and STagBLCC than for the ILP model. Noteworthy, the two neural systems even outperform the human upper bound (HUB) in this context, reported as 88.6% in Stab and Gurevych (2017)."}, {"heading": "6 Conclusion", "text": "We present the first study on neural end-to-end AM. We experimented with different framings,\n4The BIO tagging task is independent and thus not affected by the ILP constraints in the model of Stab and Gurevych (2017). The same holds true for the model of Persing and Ng (2016).\n5Denoted FscoreM in Sokolova and Lapalme (2009).\nsuch as encoding AM as a dependency parsing problem, as a sequence tagging problem with particular label set, as a multi-task sequence tagging problem, and as a problem with both sequential and tree structure information. We show that (1) neural computational AM is as good or (substantially) better than a competing feature-based ILP formulation, while eliminating the need for manual feature engineering and costly ILP constraint designing. (2) BiLSTM taggers perform very well for component identification, as demonstrated for our STagT frameworks, for T = BLCC and T = BL, as well as for LSTM-ER (BLC tagger). (3) (Naively) coupling component and relation identification is not optimal, but both tasks should be treated separately, but modeled jointly. (4) Relation identification is more difficult: when there are few entities in a text (\u201cshort documents\u201d), a more general framework such as that provided in LSTM-ER performs reasonably well. When there are many entities (\u201clong documents\u201d), a more restrained modeling is preferable. These are also our policy recommendations. Our work yields new state-of-the-art results in end-to-end AMon the PE dataset from Stab and Gurevych (2017).\nAnother possible framing, not considered here, is to frame AM as an encoder-decoder problem (Bahdanau et al., 2015; Vinyals et al., 2015). This is an even more general modeling than LSTM-ER. Its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in Potash et al. (2016)."}, {"heading": "Acknowledgments", "text": "We thank Lucie Flekova, Judith Eckle-Kohler, Nils Reimers, and Christian Stab for valuable feedback and discussions. We also thank the anonymous reviewers for their suggestions. The second author was supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 01UG1416B (CEDIFOR)."}, {"heading": "Supplementary Material", "text": "Pre-trained word embeddings: The sequence tagging systems, including the multi-task learners, as well as the neural dependency parsers can be initialized with pre-trained word embeddings. For our experiments, we chose Glove embeddings (Pennington et al., 2014) of different sizes (50, 100, and 200), the syntactic embeddings of Komninos and Manandhar (2016), and the \u201cstructured skip n-gram\u201d model of Ling et al. (2015).\nHyperparameter optimization: Hyperparameter optimization is an art in itself and often makes the difference between state-of-the-art results or subpar performance (Wang et al., 2015). Finding good parametrizations for neural networks\u2014 such as size of the hidden units or number of hidden layers\u2014is often a very challenging problem. For the dependency parsers as well as for the sequence taggers T in the STagT framing, we performed random hyperparameter optimization (Bergstra and Bengio, 2012), running systems 20 times with hyperparameters randomly chosen within pre-defined ranges, and then averaged this ensemble of 20 systems. These ranges were:6\n\u2022 BiLSTM tagger in MTL setup: hidden layers of size 150 and 50 dimensional embedding\nlayers (always using 50-dimensional Glove embeddings); the system was trained for 15 iterations and the best model on development set was chosen. All other hyperparameters at their defaults.\n\u2022 BiLSTM-CNN-CRF tagger: one hidden layer of size in {125, 150, 200, 250}, randomly drawn; training was stopped when per-\nformance on development set did not improve for 5 iterations. All other hyperparameters at their defaults. Embeddings randomly chosen from the above-named pre-\n6In all cases for the neural networks, we chose a development set of roughly 10% of the training set.\ntrained word embeddings, with a preference for 50 dimensional Glove embeddings.\nFor LSTM-ER, we ran the system with 50- dimensional Glove embeddings, which yielded better results than other embeddings we tried, and no further tuning. This is because, as outlined, the system already performs regularization techniques such as entity pre-training and scheduled sampling, which we did not implement for any of the other models. In addition, the system took considerably longer for training, which made it less suitable for ensembling.\nFor the neural parsers, our chosen hyperparameters can be read off from the accompanying scripts on our github. We trained the non-neural parsers with default hyperparameters.\nPractical issues As outlined in the data section, our data has a particular structure, but the models we investigate are not guaranteed to yield outputs that agree with these conditions (unlike, e.g., ILP models where such constraints can be enforced). For example, the taggers T in the STagT framing do not need to produce a tree structure, nor do they need to produce legitimate B, I, O labeling\u2014 e.g., in BIO labeling, an \u201cI\u201d may never follow an \u201cO\u201d. Likewise, while the parsers are guaranteed to output trees, the labeling they produce need not be consistent with our data. For example, an argumentative token may be predicted to link to a non-argumentative unit. Throughout, we observe very few such violations\u2014that is, the systems tend to produce output consistent with the structures on which they were trained. Still, for such violations, we implemented simple and innocuous postprocessing rules.\nFor the STagT systems, we corrected the fol-\nlowing:\n(1) Invalid BIO structure, i.e., \u201cI\u201d follows \u201cO\u201d.\n(2) A predicted component is not homogeneous:\nfor example, one token is predicted to link to the following argument component, while another token within the same component is predicted to link to the preceding argument component.\n(3) A link goes \u2018beyond\u2019 the actual text, e.g.,\nwhen a premise is predicted to link to another component at \u2018too large\u2019 distance |d|.\nIn case (1), we corrected \u201cI\u201d to \u201dB\u201d. In case (2), we chose the majority labeling within the predicted component. In case (3), we link the component to the maximum permissible component; e.g., when a premise links to a claim at distance 3, but the last component in the document has distance 2, we link the premise to this claim. We applied (1), (2), and (3) in order. For STagBLCC this correction scheme led to 61 out of 29537 tokens changing their labeling in the test data (0.20%) on essay level and 69 on the paragraph level. For STagBL there were on average many more corrections. For example, 1373 (4.64%) tokens changed their labeling in the Y-3:YC-3 setting described in Table 2. This is understandable because a standard BiLSTM tagger makes output predictions independently; thus, more BIO, etc., violations can be expected.\nFor the parsers, we additionally corrected when (4) they linked to a non-argumentative unit at index in. In this case, we would re-direct the faulty link to the \u201cclosest\u201d component in the vicinity of in (measured in absolute distance). Again, we applied (1) to (4) in order. For the LSTM-Parser, this led to 1224 corrections on token level (4.14%). While this may seem as leading to considerable improvements, this was actually not the case; most of our \u2018corrections\u2019 did not improve the measures reported\u2014e.g., token level accuracy decreased, from 57.17% to 55.68%. This indicates that a better strategy might have been to re-name the non-\nargumentative unit to an argumentative unit.\nFor LSTM-ER, when a source component is predicted to relate to several targets (something which is always incorrect for our data), we connect the source to its closest target (and no other targets), measured in absolute distance. This is in agreement with the distributional properties of d sketched in Figure 2, which prefers shorter distances over longer ones."}, {"heading": "Links to code used", "text": "We used the following code for our experiments: BLCC (https://github.com/XuezheMax/LasagneNLP); MTL BL (https://bitbucket.org/soegaard/mtlcnn/src); LSTM-ER (https://github.com/tticoin/LSTM-ER); LSTM-Parser (https://github.com/clab/lstmparser); Kiperwasser parser (https://github.com/elikip/bist-parser); Mate parser (https://code.google.com/archive/p/matetools/wikis/ParserAndModels.wiki); MST parser (http://www.seas.upenn.edu/ strctlrn/MSTParser/MSTParser.html). The results for the ILP model were provided to us by the first author of Stab and Gurevych (2017)."}, {"heading": "Error Analysis", "text": "We conduct some more error analysis, focussing on the three best models ILP, LSTM-ER and STagBLCC.\nWhich component types are particularly difficult to detect? Table 6 investigates F1-scores for component segmentation+classification. In this case, there are seven classes: {B, I} \u00d7 {C,MC,P} \u222a {O}. We observe that the O class is particularly easy, as well as I-P. These two are the most frequent labels in the data and are thus most robustly estimated. While all systems are more troubled predicting the beginning of a claim than its continuation (this is often due to difficulty of predicting the inclusion or omission of discourse markers as illustrated above), major claims follow a reverse trend. Further analysis reveals that claims are often mistaken for premises and vice versa, and major claims for claims or\u2014to a lesser degree\u2014for premises. The mismatch between claims and premises is sometimes due to misleading introductory phrases such as \u201cConsequently ,\u201d which often imply conclusions (and hence claims), but sometimes also give reasons\u2014i.e., premises\u2014\nfor other claims or premises.\nWe also note that the ILP model is substantially worse than the two LSTMs in all cases except for I-P on the component segmentation+classification task.\nA major source of errors for relations is that either of their arguments (the two components) do not match exactly or approximately. When they do match, errors are mostly a mismatch between actual Attack/Against vs. predicted Support/For relations. Support/For relations are the vast majority in the PE data (94% and 82%, respectively). In rare cases, the two arguments have been correctly identified but their types are wrong (e.g. premise and claim while the gold components are claim and major claim, respectively)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio."], "venue": "J. Mach. Learn. Res. 13:281\u2013305.", "citeRegEx": "Bergstra and Bengio.,? 2012", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Identifying justifications in written dialogs", "author": ["Or Biran", "Owen Rambow."], "venue": "Fifth IEEE International Conference on Semantic Computing (ICSC). pages 162\u2013168.", "citeRegEx": "Biran and Rambow.,? 2011", "shortCiteRegEx": "Biran and Rambow.", "year": 2011}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning. ACM, New", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Argument extraction for supporting public policy formulation", "author": ["Eirini Florou", "Stasinos Konstantopoulos", "Antonis Koukourikos", "Pythagoras Karampiperis."], "venue": "Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social", "citeRegEx": "Florou et al\\.,? 2013", "shortCiteRegEx": "Florou et al\\.", "year": 2013}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "author": ["Xuezhe Ma", "Eduard Hovy."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational", "citeRegEx": "Ma and Hovy.,? 2016", "shortCiteRegEx": "Ma and Hovy.", "year": 2016}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures", "author": ["Makoto Miwa", "Mohit Bansal."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Compu-", "citeRegEx": "Miwa and Bansal.,? 2016", "shortCiteRegEx": "Miwa and Bansal.", "year": 2016}, {"title": "Automatic detection of arguments in legal texts", "author": ["Marie-Francine Moens", "Erik Boiy", "Raquel Mochales Palau", "Chris Reed."], "venue": "Proceedings of the 11th International Conference on Artificial Intelligence and Law. ACM, New", "citeRegEx": "Moens et al\\.,? 2007", "shortCiteRegEx": "Moens et al\\.", "year": 2007}, {"title": "Constrained decoding for text-level discourse parsing", "author": ["Philippe Muller", "Stergos D. Afantenos", "Pascal Denis", "Nicholas Asher."], "venue": "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference:", "citeRegEx": "Muller et al\\.,? 2012", "shortCiteRegEx": "Muller et al\\.", "year": 2012}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Argumentation mining: The detection, classification and structure of arguments in text", "author": ["Raquel Mochales Palau", "Marie-Francine Moens."], "venue": "Proceedings of the 12th International Conference on Artificial Intelligence and Law. ACM,", "citeRegEx": "Palau and Moens.,? 2009", "shortCiteRegEx": "Palau and Moens.", "year": 2009}, {"title": "Joint prediction in mst-style discourse parsing for argumentation mining", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Peldszus and Stede.,? 2015", "shortCiteRegEx": "Peldszus and Stede.", "year": 2015}, {"title": "An annotated corpus of argumentative microtexts", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation. Lisabon, pages 801\u2013815.", "citeRegEx": "Peldszus and Stede.,? 2016", "shortCiteRegEx": "Peldszus and Stede.", "year": 2016}, {"title": "Multi-task multi-domain representation learning for sequence tagging", "author": ["Nanyun Peng", "Mark Dredze."], "venue": "CoRR abs/1608.02689. http://arxiv.org/abs/1608.02689.", "citeRegEx": "Peng and Dredze.,? 2016", "shortCiteRegEx": "Peng and Dredze.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Modeling argument strength in student essays", "author": ["Isaac Persing", "Vincent Ng."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "Persing and Ng.,? 2015", "shortCiteRegEx": "Persing and Ng.", "year": 2015}, {"title": "End-to-end argumentation mining in student essays", "author": ["Isaac Persing", "Vincent Ng."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Persing and Ng.,? 2016", "shortCiteRegEx": "Persing and Ng.", "year": 2016}, {"title": "Here\u2019s my point: Argumentation Mining with Pointer Networks", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "Arxiv preprint https://arxiv.org/abs/1612.08994 .", "citeRegEx": "Potash et al\\.,? 2016", "shortCiteRegEx": "Potash et al\\.", "year": 2016}, {"title": "Language resources for studying argument", "author": ["Chris Reed", "Raquel Mochales-Palau", "Glenn Rowe", "Marie-Francine Moens."], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation. Marrakech, Morocco, LREC \u201908,", "citeRegEx": "Reed et al\\.,? 2008", "shortCiteRegEx": "Reed et al\\.", "year": 2008}, {"title": "Show me your evidence - an automatic method for context dependent evidence detection", "author": ["Ruty Rinott", "Lena Dankin", "Carlos Alzate Perez", "Mitesh M. Khapra", "Ehud Aharoni", "Noam Slonim."], "venue": "Proceedings of the 2015 Conference on Em-", "citeRegEx": "Rinott et al\\.,? 2015", "shortCiteRegEx": "Rinott et al\\.", "year": 2015}, {"title": "Applying kernel methods to argumentationmining", "author": ["N. Rooney", "H. Wang", "F. Browne."], "venue": "TwentyFifth International FLAIRS Conference.", "citeRegEx": "Rooney et al\\.,? 2012", "shortCiteRegEx": "Rooney et al\\.", "year": 2012}, {"title": "Progressive neural networks", "author": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell."], "venue": "arXiv preprint arXiv:1606.04671 .", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["Marina Sokolova", "Guy Lapalme."], "venue": "Information Processing & Management 45(4):427\u2013437. https://doi.org/10.1016/j.ipm.2009.03.002.", "citeRegEx": "Sokolova and Lapalme.,? 2009", "shortCiteRegEx": "Sokolova and Lapalme.", "year": 2009}, {"title": "Evaluating argumentative and narrative essays using graphs", "author": ["Swapna Somasundaran", "Brian Riordan", "Binod Gyawali", "Su-Youn Yoon."], "venue": "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the", "citeRegEx": "Somasundaran et al\\.,? 2016", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2016}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "Computational Linguistics (in press), preprint: http://arxiv.org/abs/1604.07370).", "citeRegEx": "Stab and Gurevych.,? 2017", "shortCiteRegEx": "Stab and Gurevych.", "year": 2017}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Efficient hyper-parameter optimization for NLP applications", "author": ["Lidan Wang", "Minwei Feng", "Bowen Zhou", "Bing Xiang", "Sridhar Mahadevan."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational", "citeRegEx": "Yang and Cardie.,? 2013", "shortCiteRegEx": "Yang and Cardie.", "year": 2013}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen."], "venue": "CoRR abs/1603.06270.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Using context to predict the purpose of argumentative writing revisions", "author": ["Fan Zhang", "Diane J. Litman."], "venue": "The Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages", "citeRegEx": "Zhang and Litman.,? 2016", "shortCiteRegEx": "Zhang and Litman.", "year": 2016}, {"title": "Dependency parsing as head selection", "author": ["Xingxing Zhang", "Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of EACL 2017 (long papers). Association for Computational Linguistics.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "2016), and the \u201cstruc", "author": ["Komninos", "Manandhar"], "venue": null, "citeRegEx": "Komninos and Manandhar,? \\Q2016\\E", "shortCiteRegEx": "Komninos and Manandhar", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "tween argument components; (d) classifying relations into classes such as \u201cSupport\u201d or \u201cAttack\u201d (Persing and Ng, 2016; Stab and Gurevych, 2017).", "startOffset": 96, "endOffset": 143}, {"referenceID": 31, "context": "tween argument components; (d) classifying relations into classes such as \u201cSupport\u201d or \u201cAttack\u201d (Persing and Ng, 2016; Stab and Gurevych, 2017).", "startOffset": 96, "endOffset": 143}, {"referenceID": 21, "context": "Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017).", "startOffset": 63, "endOffset": 85}, {"referenceID": 21, "context": "Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc.", "startOffset": 63, "endOffset": 114}, {"referenceID": 24, "context": "Hand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).", "startOffset": 179, "endOffset": 276}, {"referenceID": 3, "context": "Hand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).", "startOffset": 179, "endOffset": 276}, {"referenceID": 31, "context": "Hand-crafted features pose a problem because AM is to some degree an \u201carbitrary\u201d problem in that the notion of \u201cargument\u201d critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).", "startOffset": 179, "endOffset": 276}, {"referenceID": 14, "context": "Hence, it is not surprising that \u2018discourse parsing\u2019 (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015).", "startOffset": 53, "endOffset": 74}, {"referenceID": 17, "context": ", 2012) has been suggested for AM (Peldszus and Stede, 2015).", "startOffset": 34, "endOffset": 60}, {"referenceID": 6, "context": "Third, we frame AM as a multi-task (tagging) problem (Caruana, 1997; Collobert and Weston, 2008).", "startOffset": 53, "endOffset": 96}, {"referenceID": 12, "context": "Fourth, we evaluate the model of Miwa and Bansal (2016) that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations.", "startOffset": 33, "endOffset": 56}, {"referenceID": 16, "context": "AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), doc-", "startOffset": 45, "endOffset": 88}, {"referenceID": 13, "context": "AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), doc-", "startOffset": 45, "endOffset": 88}, {"referenceID": 36, "context": "Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al.", "startOffset": 100, "endOffset": 124}, {"referenceID": 21, "context": "Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016).", "startOffset": 143, "endOffset": 192}, {"referenceID": 30, "context": "Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016).", "startOffset": 143, "endOffset": 192}, {"referenceID": 8, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 13, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 26, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 25, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).", "startOffset": 80, "endOffset": 203}, {"referenceID": 8, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity.", "startOffset": 81, "endOffset": 322}, {"referenceID": 8, "context": "Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types.", "startOffset": 81, "endOffset": 473}, {"referenceID": 10, "context": "The model of Ma and Hovy (2016) adds convolutional neural nets (CNNs) on the character-level to BiLSTMCRFs, leading to BiLSTM-CRF-CNN (BLCC) models.", "startOffset": 13, "endOffset": 32}, {"referenceID": 28, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 19, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 35, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 27, "context": "jointly (S\u00f8gaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; H\u00e9ctor and Plank, 2017).", "startOffset": 8, "endOffset": 121}, {"referenceID": 28, "context": "In the MTL framework of S\u00f8gaard and Goldberg (2016) the underlying model is a BiLSTM with several hidden layers.", "startOffset": 24, "endOffset": 52}, {"referenceID": 5, "context": "Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations.", "startOffset": 111, "endOffset": 135}, {"referenceID": 9, "context": "of word on top of the stack\u201d, and conjunctions of core features such as \u201cword is X and POS is Y\u201d (see McDonald et al. (2005)).", "startOffset": 102, "endOffset": 125}, {"referenceID": 5, "context": "Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer.", "startOffset": 112, "endOffset": 201}, {"referenceID": 5, "context": "Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)\u2019s neural parser associates each stack with a \u201cstack LSTM\u201d that encodes their contents.", "startOffset": 112, "endOffset": 386}, {"referenceID": 12, "context": "LSTM-ER Miwa and Bansal (2016) present a", "startOffset": 8, "endOffset": 31}, {"referenceID": 1, "context": "In addition to de-coupling entity and relation detection but jointly modeling them, pretraining on entities and scheduled sampling (Bengio et al., 2015) is applied to prevent low performance at early training stages of entity detection and relation classification.", "startOffset": 131, "endOffset": 152}, {"referenceID": 31, "context": "We use the feature-based ILP model from Stab and Gurevych (2017) as a comparison system.", "startOffset": 40, "endOffset": 65}, {"referenceID": 21, "context": "Stab and Gurevych (2017) and Persing and Ng (2016).", "startOffset": 29, "endOffset": 51}, {"referenceID": 12, "context": "Still, a joint model like that of Miwa and Bansal (2016) de-couples the two tasks in such a way that many model parameters are shared across the tasks, similarly as in MTL.", "startOffset": 34, "endOffset": 57}, {"referenceID": 37, "context": "It has frequently been observed that models tend to produce output consistent with constraints in their training data in such situations (Zhang et al., 2017; H\u00e9ctor and Plank, 2017); thus, they have learned the constraints.", "startOffset": 137, "endOffset": 181}, {"referenceID": 21, "context": "Evaluation Metric We adopt the evaluation metric suggested in Persing and Ng (2016). This computes true positives TP, false positives FP, and", "startOffset": 62, "endOffset": 84}, {"referenceID": 21, "context": "For space reasons, we refer to Persing and Ng (2016) for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that \u2018matches\u2019.", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "For space reasons, we refer to Persing and Ng (2016) for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that \u2018matches\u2019. Persing and Ng (2016) define a notion of what we may term \u2018level \u03b1 matching\u2019: for example, at the 100% level (exact match) predicted and gold components must have exactly the same spans, whereas at the 50% level they must only share at least 50% of their tokens (approximate match).", "startOffset": 31, "endOffset": 278}, {"referenceID": 11, "context": "(a) Dependency Parsing We show results for the two feature-based parsers MST (McDonald et al., 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 4, "context": ", 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": ", 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al. (2015) (LSTM-Parser) and Kiperwasser and Goldberg (2016) (Kiperwasser).", "startOffset": 15, "endOffset": 91}, {"referenceID": 4, "context": ", 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al. (2015) (LSTM-Parser) and Kiperwasser and Goldberg (2016) (Kiperwasser).", "startOffset": 15, "endOffset": 141}, {"referenceID": 10, "context": "we use the BLCC tagger from Ma and Hovy (2016) and refer to the resulting system as", "startOffset": 28, "endOffset": 47}, {"referenceID": 28, "context": "(c) MTL As indicated, we use the MTL tagging framework from S\u00f8gaard and Goldberg (2016) for multi-task experiments.", "startOffset": 60, "endOffset": 88}, {"referenceID": 15, "context": "This indicates that complex sequence tagging may benefit when we train a \u201csublabeler\u201d in the same neural architecture, a finding that may be particularly relevant for morphological POS tagging (M\u00fcller et al., 2013).", "startOffset": 193, "endOffset": 214}, {"referenceID": 15, "context": "This indicates that complex sequence tagging may benefit when we train a \u201csublabeler\u201d in the same neural architecture, a finding that may be particularly relevant for morphological POS tagging (M\u00fcller et al., 2013). Unlike S\u00f8gaard and Goldberg (2016), we do not find that the optimal architecture is the one in which \u201clower\u201d tasks (such as C or R) feed from lower layers.", "startOffset": 194, "endOffset": 251}, {"referenceID": 22, "context": "On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries (Persing and Ng, 2016; Yang and Cardie, 2013).", "startOffset": 112, "endOffset": 157}, {"referenceID": 34, "context": "On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries (Persing and Ng, 2016; Yang and Cardie, 2013).", "startOffset": 112, "endOffset": 157}, {"referenceID": 31, "context": "6% in Stab and Gurevych (2017).", "startOffset": 6, "endOffset": 31}, {"referenceID": 28, "context": "The BIO tagging task is independent and thus not affected by the ILP constraints in the model of Stab and Gurevych (2017). The same holds true for the model of Persing and Ng (2016).", "startOffset": 97, "endOffset": 122}, {"referenceID": 21, "context": "The same holds true for the model of Persing and Ng (2016). Denoted FscoreM in Sokolova and Lapalme (2009).", "startOffset": 37, "endOffset": 59}, {"referenceID": 21, "context": "The same holds true for the model of Persing and Ng (2016). Denoted FscoreM in Sokolova and Lapalme (2009). STagBLCC LSTM-ER ILP HUB", "startOffset": 37, "endOffset": 107}, {"referenceID": 31, "context": "Our work yields new state-of-the-art results in end-to-end AMon the PE dataset from Stab and Gurevych (2017).", "startOffset": 84, "endOffset": 109}, {"referenceID": 0, "context": "Another possible framing, not considered here, is to frame AM as an encoder-decoder problem (Bahdanau et al., 2015; Vinyals et al., 2015).", "startOffset": 92, "endOffset": 137}, {"referenceID": 32, "context": "Another possible framing, not considered here, is to frame AM as an encoder-decoder problem (Bahdanau et al., 2015; Vinyals et al., 2015).", "startOffset": 92, "endOffset": 137}, {"referenceID": 23, "context": "ponent classification and relation identification has been investigated in Potash et al. (2016).", "startOffset": 75, "endOffset": 96}], "year": 2017, "abstractText": "We investigate neural techniques for endto-end computational argumentation mining (AM). We frame AM both as a tokenbased dependency parsing and as a tokenbased sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch longrange dependencies inherent to the AM problem. Moreover, we find that jointly learning \u2018natural\u2019 subtasks, in a multi-task learning setup, improves performance.", "creator": "gnuplot 5.0 patchlevel 3"}}}