{"id": "1704.06440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Equivalence Between Policy Gradients and Soft Q-Learning", "abstract": "two of the leading approaches for model - free reinforcement conventional learning software are modern policy inflation gradient methods and $ q $ - proof learning methods. $ qb q $ - learning filtering methods hence can traditionally be effective consciously and sample - efficient internally when they work, however, or it is ultimately not that well - equipped understood beforehand why they work, since most empirically, hence the $ c q $ - values they might estimate are very inaccurate. a partial reconstruction explanation may be that $ q $ - theory learning methods though are mostly secretly incorrectly implementing policy gradient updates : we show that nowadays there is perhaps a precise precise dynamic equivalence between $ q $ - learning formulas and policy gradient evolution methods embodied in the setting behavior of entropy - regularized global reinforcement classical learning, but that \" soft \" ( true entropy - regularized ) $ q $ - learning is usually exactly equivalent to a human policy pressure gradient method. we next also point out a connection between $ 32 q $ - learning methods vs and evolutionary natural policy preferences gradient reconstruction methods. additionally experimentally, we explore the entropy - regularized versions curve of $ q $ - learning optimization and policy gradients, where and we find them trying to exactly perform as adequately well fit as ( or slightly better than ) the exact standard variants on the conditional atari benchmark. secondly we also show that clearly the dependency equivalence problem holds correct in practical settings by efficiently constructing indirectly a $ d q $ - learning experimental method that quite closely properly matches purely the simulated learning dynamics problems of a3c without using a relational target learning network hierarchy or $ \\ def epsilon $ - greedy exploration schedule.", "histories": [["v1", "Fri, 21 Apr 2017 08:33:59 GMT  (878kb,D)", "http://arxiv.org/abs/1704.06440v1", null], ["v2", "Sun, 27 Aug 2017 23:43:20 GMT  (878kb,D)", "http://arxiv.org/abs/1704.06440v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john schulman", "pieter abbeel", "xi chen"], "accepted": false, "id": "1704.06440"}, "pdf": {"name": "1704.06440.pdf", "metadata": {"source": "CRF", "title": "Equivalence Between Policy Gradients and Soft Q-Learning", "authors": ["John Schulman", "Xi Chen", "Pieter Abbeel"], "emails": ["pieter}@openai.com"], "sections": [{"heading": null, "text": "Experimentally, we explore the entropy-regularized versions of Q-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a Q-learning method that closely matches the learning dynamics of A3C without using a target network or -greedy exploration schedule."}, {"heading": "1 Introduction", "text": "Policy gradient methods (PG) and Q-learning (QL) methods perform updates that are qualitatively similar. In both cases, if the return following an action at is high, then that action is reinforced: in policy gradient methods, the probability \u03c0(at | st) is increased; whereas in Q-learning methods, the Q-value Q(st, at) is increased. The connection becomes closer when we add entropy regularization to these algorithms. With an entropy cost added to the returns, the optimal policy has the form \u03c0(a | s) \u221d exp(Q(s, a)); hence policy gradient methods solve for the optimal Q-function, up to an additive constant (Ziebart [2010]). O\u2019Donoghue et al. [2016] also discuss the connection between the fixed points and updates of PG and QL methods, though the discussion of fixed points is restricted to the tabular setting, and the discussion comparing updates is informal and shows an approximate equivalence. Going beyond past work, this paper shows that under appropriate conditions, the gradient of the loss function used in n-step Q-learning is equal to the gradient of the loss used in an n-step policy gradient method, including a squared-error term on the value function. Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning.\nSection 2 uses the bandit setting to provide the reader with a simplified version of our main calculation. (The main calculation applies to the MDP setting.) Section 3 discusses the entropy-regularized formulation of RL, which is not original to this work, but is included for the reader\u2019s convenience. Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016]. Section 5 draws a connection between QL methods that use batch updates or replay-buffers, and natural policy gradient methods.\nSome previous work on entropy regularized reinforcement learning (e.g., O\u2019Donoghue et al. [2016], Nachum et al. [2017]) uses entropy bonuses, whereas we use a penalty on Kullback-Leibler (KL) divergence, which is a bit more general. However, in the text, we often refer to \u201centropy\u201d terms; this refers to \u201crelative entropy\u201d, i.e., the KL divergence."}, {"heading": "2 Bandit Setting", "text": "Let\u2019s consider a bandit problem with a discrete or continuous action space: at each timestep the agent chooses an action a, and the reward r is sampled according to P (r |a), where P is unknown to the agent. Let\nar X\niv :1\n70 4.\n06 44\n0v 1\n[ cs\n.L G\n] 2\n1 A\npr 2\n01 7\nr\u0304(a) = E [r | a], and let \u03c0 denote a policy, where \u03c0(a) is the probability of action a. Then, the expected pertimestep reward of the policy \u03c0 is Ea\u223c\u03c0 [r] = \u2211 a \u03c0(a)r\u0304(a) or \u222b da \u03c0(a)r\u0304(a). Let\u2019s suppose we are maximizing \u03b7(\u03c0), an entropy-regularized version of this objective:\n\u03b7(\u03c0) = Ea\u223c\u03c0,r [r]\u2212 \u03c4DKL [\u03c0 \u2016 \u03c0] (1)\nwhere \u03c0 is some \u201creference\u201d policy, \u03c4 is a \u201ctemperature\u201d parameter, and DKL is the Kullback-Leibler divergence. Note that the temperature \u03c4 can be eliminated by rescaling the rewards. However, we will leave it so that our calculations are checkable through dimensional analysis, and to make the temperaturedependence more explicit.\nFirst, let us calculate the policy \u03c0 that maximizes \u03b7. We claim that \u03b7(\u03c0) is maximized by \u03c0Br\u0304 , defined as\n\u03c0Br\u0304 (a) = \u03c0(a) exp(r\u0304(a)/\u03c4)/Ea\u2032\u223c\u03c0 [exp(r\u0304(a\u2032)/\u03c4)]\ufe38 \ufe37\ufe37 \ufe38 normalizing constant . (2)\nTo derive this, consider the KL divergence between \u03c0 and \u03c0Br\u0304 :\nDKL [ \u03c0 \u2225\u2225 \u03c0Br\u0304 ] = Ea\u223c\u03c0 [log \u03c0(a)\u2212 log \u03c0Br\u0304 (a)] (3)\n= Ea\u223c\u03c0 [log \u03c0(a)\u2212 log \u03c0(a)\u2212 r\u0304(a)/\u03c4 + logEa\u223c\u03c0 [exp(r\u0304(a)/\u03c4)]] (4) = DKL [\u03c0 \u2016 \u03c0]\u2212 Ea\u223c\u03c0 [r\u0304(a)/\u03c4 ] + logEa\u223c\u03c0 [exp(r\u0304(a)/\u03c4)] (5)\nRearranging and multiplying by \u03c4 , Ea\u223c\u03c0 [r\u0304(a)]\u2212 \u03c4DKL [\u03c0 \u2016 \u03c0] = \u03c4 logEa\u223c\u03c0 [exp(r\u0304(a)/\u03c4)]\u2212 \u03c4DKL [ \u03c0 \u2225\u2225 \u03c0Br\u0304 ] (6)\nClearly the left-hand side is maximized (with respect to \u03c0) when the KL term on the right-hand side is minimized (as the other term does not depend on \u03c0), and DKL [ \u03c0 \u2225\u2225 \u03c0Br\u0304 ] is minimized at \u03c0 = \u03c0Br\u0304 .\nThe preceding calculation gives us the optimal policy when r\u0304 is known, but in the entropy-regularized bandit problem, it is initially unknown, and the agent learns about it by sampling. There are two approaches for solving the entropy-regularized bandit problem:\n1. A direct, policy-based approach, where we incrementally update the agent\u2019s policy \u03c0 based on stochastic gradient ascent on \u03b7.\n2. An indirect, value-based approach, where we learn an action-value function q\u03b8 that estimates and approximates r\u0304, and we define \u03c0 based on our current estimate of q\u03b8.\nFor the policy-based approach, we can obtain unbiased estimates the gradient of \u03b7. For a parameterized policy \u03c0\u03b8, the gradient is given by\n\u2207\u03b8\u03b7(\u03c0\u03b8) = Ea\u223c\u03c0\u03b8,r [\u2207\u03b8 log \u03c0\u03b8(a)r \u2212 \u03c4\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0]] . (7)\nWe can obtain an unbiased gradient estimate using a single sample (a, r). In the indirect, value-based approach approach, it is natural to use a squared-error loss:\nL\u03c0(\u03b8) := 1 2Ea\u223c\u03c0,r [ (q\u03b8(a)\u2212 r)2 ] (8)\nTaking the gradient of this loss, with respect to the parameters of q\u03b8, we get\n\u2207\u03b8L\u03c0(\u03b8) = Ea\u223c\u03c0,r [\u2207\u03b8q\u03b8(a)(q\u03b8(a)\u2212 r)] (9)\nSoon, we will calculate the relationship between this loss gradient and the policy gradient from Equation (7). In the indirect, value-based approach, a natural choice for policy \u03c0 is the one that would be optimal if q\u03b8 = r\u0304. Let\u2019s denote this policy, called the Boltzmann policy, by \u03c0 B q\u03b8 , where\n\u03c0Bq\u03b8 (a) = \u03c0(a) exp(q\u03b8(a)/\u03c4)/Ea\u2032\u223c\u03c0 [exp(q\u03b8(a \u2032)/\u03c4)] . (10)\nIt will be convenient to introduce a bit of notation for the normalizing factor; namely, we define the scalar\nv\u03b8 = \u03c4 logEa\u223c\u03c0 [exp(q\u03b8(a))/\u03c4 ] (11)\nThen the Boltzmann policy can be written as\n\u03c0Bq\u03b8 (a) = \u03c0(a) exp((q\u03b8(a)\u2212 v\u03b8)/\u03c4). (12)\nNote that the term \u03c4 logEa\u223c\u03c0 [exp(r\u0304(a)/\u03c4)], appeared earlier in Equation (6)). Repeating the calculation from Equation (2) through Equation (6), but with q\u03b8 instead of r\u0304,\nv\u03b8 = Ea\u223c\u03c0Bq\u03b8 [q\u03b8(a)]\u2212 \u03c4DKL [ \u03c0Bq\u03b8 \u2225\u2225 \u03c0] . (13) Hence, v\u03b8 is an estimate of \u03b7(\u03c0 B q\u03b8\n), plugging in q\u03b8 for r\u0304. Now we shall show the connection between the gradient of the squared-error loss (Equation (9)) and the policy gradient (Equation (7)). Rearranging Equation (12), we can write q\u03b8 in terms of v\u03b8 and the Boltzmann policy \u03c0Bq\u03b8 :\nq\u03b8(a) = v\u03b8 + \u03c4 log\n( \u03c0Bq\u03b8 (a)\n\u03c0(a)\n) (14)\nLet\u2019s substitute this expression for q\u03b8 into the squared-error loss gradient (Equation (9)).\n\u2207\u03b8L\u03c0(q\u03b8) = Ea\u223c\u03c0,r [\u2207\u03b8q\u03b8(a)(q\u03b8(a)\u2212 r)] (15) = Ea\u223c\u03c0,r [ \u2207\u03b8 ( v\u03b8 + \u03c4 log ( \u03c0Bq\u03b8 (a)\n\u03c0(a)\n))( v\u03b8 + \u03c4 log ( \u03c0Bq\u03b8 (a)\n\u03c0(a)\n) \u2212 r )]\n(16) = Ea\u223c\u03c0,r [ \u03c4\u2207\u03b8 log \u03c0Bq\u03b8 (a) ( v\u03b8 + \u03c4 log ( \u03c0Bq\u03b8 (a) \u03c0(a) ) \u2212 r ) +\u2207\u03b8v\u03b8 ( v\u03b8 + \u03c4 log ( \u03c0Bq\u03b8 (a) \u03c0(a) ) \u2212 r )] (17)\nNote that we have not yet decided on a sampling distribution \u03c0. Henceforth, we\u2019ll assume actions were sampled by \u03c0 = \u03c0Bq\u03b8 . Also, note the derivative of the KL-divergence:\n\u2207\u03b8DKL [ \u03c0Bq\u03b8 \u2225\u2225 \u03c0] = \u2207\u03b8 \u222b da \u03c0Bq\u03b8 (a) log(\u03c0Bq\u03b8 (a)\u03c0(a) ) (18) = \u222b da \u2207\u03b8\u03c0Bq\u03b8 (a) ( log ( \u03c0Bq\u03b8 (a) \u03c0(a) ) + \u03c0Bq\u03b8 (a)\n1 \u03c0Bq\u03b8 (a)\n) (19)\n= \u222b da \u03c0Bq\u03b8 (a)\u2207\u03b8 log \u03c0Bq\u03b8 (a) log ( \u03c0Bq\u03b8 (a) \u03c0(a) ) (20)\n= Ea\u223c\u03c0Bq\u03b8 [ \u2207\u03b8 log \u03c0Bq\u03b8 (a) log ( \u03c0Bq\u03b8 (a) \u03c0(a) )] (21)\nContinuing from Equation (17) but setting \u03c0 = \u03c0Bq\u03b8 ,\n\u2207\u03b8L\u03c0Bq\u03b8 (q\u03b8) = Ea\u223c\u03c0Bq\u03b8 ,r [ \u03c4\u2207\u03b8 log \u03c0Bq\u03b8 (a)(v\u03b8 \u2212 r) + \u03c42\u2207\u03b8DKL [ \u03c0Bq\u03b8 \u2225\u2225 \u03c0]] +\u2207\u03b8Ea\u223c\u03c0Bq\u03b8 ,r [ v\u03b8 ( v\u03b8 + \u03c4DKL [ \u03c0Bq\u03b8\n\u2225\u2225 \u03c0]\u2212 r)] (22) = \u2212\u03c4 \u2207\u03b8Ea\u223c\u03c0Bq\u03b8 ,r [ r \u2212 \u03c4DKL [ \u03c0Bq\u03b8\n\u2225\u2225 \u03c0]]\ufe38 \ufe37\ufe37 \ufe38 policy gradient +\u2207\u03b8Ea\u223c\u03c0Bq\u03b8 ,r [ 1 2 (v\u03b8 \u2212 (r \u2212 \u03c4DKL [ \u03c0Bq\u03b8 \u2225\u2225 \u03c0]))2]\ufe38 \ufe37\ufe37 \ufe38 value error gradient (23)\nHence, the gradient of the squared error for our action-value function can be broken into two parts: the first part is the policy gradient of the Boltzmann policy corresponding to q\u03b8, the second part arises from a squared error objective, where we are fitting v\u03b8 to the entropy-augmented expected reward r\u0304(a)\u2212 \u03c4DKL [ \u03c0Bq\u03b8 \u2225\u2225 \u03c0]. Soon we will derive an equivalent interpretation of Q-function regression in the MDP setting, where we are approximating the state-value function Q\u03c0,\u03b3 . However, we first need to introduce an entropy-regularized version of the reinforcement learning problem."}, {"heading": "3 Entropy-Regularized Reinforcement Learning", "text": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al. [2017]). Specifically, let us define the entropy-augmented return to be \u2211\u221e t=0 \u03b3\nt(rt \u2212 \u03c4 KLt) where rt is the reward, \u03b3 \u2208 [0, 1] is the discount factor, \u03c4 is a scalar temperature coefficient, and KLt is the Kullback-Leibler divergence between the current policy \u03c0 and a reference policy \u03c0 at timestep t: KLt = DKL [\u03c0(\u00b7 | st) \u2016 \u03c0(\u00b7 | st)]. We will sometimes use the notation KL(s) = DKL [\u03c0 \u2016 \u03c0] (s) = DKL [\u03c0(\u00b7 | s) \u2016 \u03c0(\u00b7 | s)]. To emulate the effect of a standard entropy bonus (up to a constant), one can define \u03c0 to be the uniform distribution. The subsequent sections will generalize some of the concepts from reinforcement learning to the setting where we are maximizing the entropy-augmented discounted return."}, {"heading": "3.1 Value Functions", "text": "We are obliged to alter our definitions of value functions to include the new KL penalty terms. We shall define the state-value function as the expected return:\nV\u03c0(s) = E [ \u221e\u2211 t=0 \u03b3t(rt \u2212 \u03c4 KLt) \u2223\u2223\u2223\u2223\u2223 s0 = s ] (24)\nand we shall define the Q-function as\nQ\u03c0(s, a) = E [ r0 +\n\u221e\u2211 t=1 \u03b3t(rt \u2212 \u03c4 KLt) \u2223\u2223\u2223\u2223\u2223 s0 = s, a0 = a ] (25)\nNote that this Q-function does not include the first KL penalty term, which does not depend on the action a0. This definition makes some later expressions simpler, and it leads to the following relationship between Q\u03c0 and V\u03c0:\nV\u03c0(s) = Ea\u223c\u03c0 [Q\u03c0(s, a)]\u2212 \u03c4 KL(s), (26)\nwhich follows from matching terms in the sums in Equations (24) and (25)."}, {"heading": "3.2 Boltzmann Policy", "text": "In standard reinforcement learning, the \u201cgreedy policy\u201d for Q is defined as [GQ](s) = arg maxaQ(s, a). With entropy regularization, we need to alter our notion of a greedy policy, as the optimal policy is stochastic. Since Q\u03c0 omits the first entropy term, it is natural to define the following stochastic policy, which is called the Boltzmann policy, and is analogous to the greedy policy:\n\u03c0BQ(\u00b7 | s) = arg max \u03c0 {Ea\u223c\u03c0 [Q(s, a)]\u2212 \u03c4DKL [\u03c0 \u2016 \u03c0] (s)} (27)\n= \u03c0(a | s) exp(Q(s, a)/\u03c4)/Ea\u2032\u223c\u03c0 [exp(Q(s, a\u2032)/\u03c4)]\ufe38 \ufe37\ufe37 \ufe38 normalizing constant . (28)\nwhere the second equation is analogous to Equation (2) from the bandit setting. Also analogously to the bandit setting, it is natural to define VQ (a function of Q) as\nVQ(s) = \u03c4 logEa\u2032\u223c\u03c0 [exp(Q(s, a\u2032)/\u03c4)] (29)\nso that\n\u03c0BQ(a | s) = \u03c0(a | s) exp((Q(s, a)\u2212 VQ(s))/\u03c4) (30)\nUnder this definition, it also holds that VQ(s) = Ea\u223c\u03c0Bq\u03b8 (s) [Q(s, a)]\u2212 \u03c4DKL [ \u03c0BQ \u2225\u2225 \u03c0] (s) (31)\nin analogy with Equation (13). Hence, VQ(s) can be interpreted as an estimate of the expected entropyaugmented return, under the Boltzmann policy \u03c0BQ.\nAnother way to interpret the Boltzmann policy is as the exponentiated advantage function. Defining the\nadvantage function as AQ(s, a) = Q(s, a)\u2212 VQ(s), Equation (30) implies that \u03c0 B Q(a | s) \u03c0(a | s) = exp(AQ(s, a)/\u03c4)."}, {"heading": "3.3 Fixed-Policy Backup Operators", "text": "The T\u03c0 operators (for Q and V ) in standard reinforcement learning correspond to computing the expected return with a one-step lookahead: they take the expectation over one step of dynamics, and then fall back on the value function at the next timestep. We can easily generalize these operators to the entropy-regularized setting. We define\n[T\u03c0V ](s) = Ea\u223c\u03c0,(r,s\u2032)\u223cP (r,s\u2032 | s,a) [r \u2212 \u03c4 KL(s) + \u03b3V (s\u2032)] (32) [T\u03c0Q](s, a) = E(r,s\u2032)\u223cP (r,s\u2032 | s,a) [r + \u03b3(Ea\u2032\u223c\u03c0 [Q(s\u2032, a\u2032)]\u2212 \u03c4 KL(s\u2032))] . (33)\nRepeatedly applying the T\u03c0 operator (T n\u03c0 V = T\u03c0(T\u03c0(. . . T\u03c0\ufe38 \ufe37\ufe37 \ufe38 n times (V )))) corresponds to computing the expected\nreturn with a multi-step lookahead. That is, repeatedly expanding the definition of T\u03c0, we obtain\n[T n\u03c0 V ](s) = E [ n\u22121\u2211 t=0 \u03b3t(rt \u2212 \u03c4 KLt) + \u03b3nV (sn) \u2223\u2223\u2223\u2223\u2223 s0 = s ] (34)\n[T n\u03c0 Q](s, a)\u2212 \u03c4 KL(s) = E [ n\u22121\u2211 t=0 \u03b3t(rt \u2212 \u03c4 KLt) + \u03b3n(Q(sn, an)\u2212 \u03c4 KLn) \u2223\u2223\u2223\u2223\u2223 s0 = s, a0 = a ] . (35)\nAs a sanity check, note that in both equations, the left-hand side and right-hand side correspond to estimates of the total discounted return \u2211\u221e t=0 \u03b3\nt(rt \u2212 \u03c4 KLt). The right-hand side of these backup formulas can be rewritten using \u201cBellman error\u201d terms \u03b4t. To rewrite\nthe state-value (V ) backup, define\n\u03b4t = (rt \u2212 \u03c4 KLt) + \u03b3V (st+1)\u2212 V (st) (36)\nThen we have\n[T n\u03c0 V ](s) = E [ n\u22121\u2211 t=0 \u03b3t\u03b4t + \u03b3 nV (sn) \u2223\u2223\u2223\u2223\u2223 s0 = s ] . (37)"}, {"heading": "3.4 Boltzmann Backups", "text": "We can define another set of backup operators corresponding to the Boltzmann policy, \u03c0(a|s) \u221d \u03c0(a|s) exp(Q(s, a)/\u03c4). We define the following Boltzmann backup operator:\n[T Q](s, a) = E(r,s\u2032)\u223cP (r,s\u2032 | s,a) r + \u03b3 Ea\u2032\u223cGQ [Q(s, a)]\u2212 \u03c4DKL [GQ \u2016 \u03c0] (s\u2032)\ufe38 \ufe37\ufe37 \ufe38 (\u2217)  (38) = E(r,s\u2032)\u223cP (r,s\u2032 | s,a)\nr + \u03b3 \u03c4 logEa\u2032\u223c\u03c0 [exp(Q(s\u2032, a\u2032)/\u03c4)]\ufe38 \ufe37\ufe37 \ufe38 (\u2217\u2217)  (39) where the simplification from (\u2217) to (\u2217\u2217) follows from the same calculation that we performed in the bandit setting (Equations (11) and (13)).\nThe n-step operator T n\u03c0 for Q-functions also simplifies in the case that we are executing the Boltzmann policy. Starting with the equation for T n\u03c0 Q (Equation (35)) and setting \u03c0 = \u03c0BQ, and then using Equation (31)\nto rewrite the expected Q-function terms in terms of VQ, we obtain\n[(T\u03c0BQ) nQ](s, a)\u2212 \u03c4 KL(s) = E [ n\u22121\u2211 t=0 \u03b3t(rt \u2212 \u03c4 KLt) + \u03b3n(Q(sn, an)\u2212 \u03c4 KLn) \u2223\u2223\u2223\u2223\u2223 s0 = s, a0 = a ] (40)\n= E [ n\u22121\u2211 t=0 \u03b3t(rt \u2212 \u03c4 KLt) + \u03b3nVQ \u2223\u2223\u2223\u2223\u2223 s0 = s, a0 = a ] . (41)\nFrom now on, let\u2019s denote this n-step backup operator by T\u03c0B,n. (Note T\u03c0BQ,n 6= T nQ, even though T\u03c0BQ,1Q = T Q, because T\u03c0BQ depends on Q.) One can similarly define the TD(\u03bb) version of this backup operator\n[T\u03c0BQ,\u03bbQ] = (1\u2212 \u03bb)(1 + \u03bbT\u03c0BQ + (\u03bbT\u03c0BQ) 2 + . . . )T\u03c0BQQ. (42)\nOne can straightforwardly verify by comparing terms that it satisfies [T\u03c0BQ,\u03bbQ](s, a) = Q(s, a) + E [ \u221e\u2211 t=0 (\u03b3\u03bb)t\u03b4t \u2223\u2223\u2223\u2223\u2223 s0 = s, a0 = a ] ,\nwhere \u03b4t = (rt \u2212 \u03c4 KLt) + \u03b3VQ(st+1)\u2212 VQ(st). (43)\n3.5 Soft Q-Learning\nThe Boltzmann backup operators defined in the preceding section can be used to define practical variants of Q-learning that can be used with nonlinear function. These methods, which optimize the entropy-augmented, will be called soft Q-learning. Following Mnih et al. [2015], modern implementations of Q-learning, and nstep Q-learning (see Mnih et al. [2016]) update the Q-function incrementally to compute the backup against a fixed target Q-function, which we\u2019ll call Q. In the interval between each target network update, the algorithm is approximately performing the backup operation Q\u2190 T Q (1-step) or Q\u2190 T\u03c0BQ,nQ (n-step). To perform this approximate minimization, the algorithms minimize the least squares loss\nL(Q) = Et,st,at [ 1 2 (Q(st, at)\u2212 yt)2 ] , where (44)\nyt = rt + \u03b3VQ(st+1) 1-step Q-learning (45)\nyt = \u03c4 KLt + n\u22121\u2211 d=0 \u03b3d(rt+d \u2212 \u03c4 KLt+d) + \u03b3t+nVQ(st+n) n-step Q-learning (46)\n= \u03c4 KLt +VQ(st) + n\u22121\u2211 d=0 \u03b3d\u03b4t+d\nwhere \u03b4t = (rt \u2212 \u03c4 KLt) + \u03b3VQ(st+1)\u2212 VQ(st) (47)\nIn one-step Q-learning (Equation (45)), yt is an unbiased estimator of [T Q](st, at), regardless of what behavior policy was used to collect the data. In n-step Q-learning (Equation (46)), for n > 1, yt is only an unbiased estimator of [T\u03c0BQ,nQ](st, at) if actions at, at+1, . . . , at+d\u22121 are sampled using \u03c0 B Q."}, {"heading": "3.6 Policy Gradients", "text": "Entropy regularization is often used in policy gradient algorithms, with gradient estimators of the form\nEt,st,at \u2207\u03b8 log \u03c0\u03b8(at | st)\u2211 t\u2032\u2265t rt\u2032 \u2212 \u03c4\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st)  (48) (Williams [1992], Mnih et al. [2016]).\nHowever, these are not proper estimators of the entropy-augmented return \u2211 t(rt \u2212 \u03c4 KLt), since they don\u2019t account for how actions affect entropy at future timesteps. Intuitively, one can think of the KL terms as a cost for \u201cmental effort\u201d. Equation (48) only accounts for the instantaneous effect of actions on mental effort, not delayed effects.\nTo compute proper gradient estimators, we need to include the entropy terms in the return. We will define the discounted policy gradient in the following two equivalent ways\u2014first, in terms of the empirical return; second, in terms of the value functions V\u03c0 and Q\u03c0:\ng\u03b3(\u03c0\u03b8) = E [ \u221e\u2211 t=0 ( \u2207\u03b8 log \u03c0\u03b8(at | st) \u221e\u2211 d=0 \u03b3d(rt+d \u2212 \u03c4 KLt+d)\u2212 \u03c4\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st) )]\n(49)\n= E [ \u221e\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8(at | st)(Q\u03c0(st, at)\u2212 \u03c4 KLt\u2212V\u03c0(st))\u2212 \u03c4\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st) ]\n(50)\nIn the special case of a finite-horizon problem\u2014i.e., rt = KLt = 0 for all t \u2265 T\u2014the undiscounted (\u03b3 = 1) return is finite, and it is meaningful to compute its gradient. In this case, g1(\u03c0\u03b8) equals the undiscounted policy gradient:\ng1(\u03c0) = \u2207\u03b8E [ T\u22121\u2211 t=0 (rt \u2212 \u03c4 KLt) ]\n(51)\nThis result is obtained directly by considering the stochastic computation graph for the loss (Schulman et al. [2015a]), shown in the figure on the right. The edges from \u03b8 to the KL loss terms lead to the \u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st) terms in the gradient; the edges to the stochastic actions at lead to the \u2207\u03b8 log \u03c0\u03b8(at | st) \u2211T\u22121 t=d (rt+d \u2212 \u03c4 KLt+d) terms in the gradient.\ns0 s1 . . . sT\u22121\na0 a1 . . . aT\u22121\nr0 r1 . . . rT\u22121\nk0 k1 . . . kT\u22121\n\u03b8\nSince g1(\u03c0\u03b8) computes the gradient of the entropy-regularized return, one interpretation of g\u03b3(\u03c0\u03b8) is that it is an approximation of the undiscounted policy gradient g1(\u03c0\u03b8), but that it allows for lower-variance gradient estimators by ignoring some long-term dependencies. A different interpretation of g\u03b3(\u03c0) is that it gives a gradient flow such that \u03c0\u2217 = \u03c0BQ\u2217 is the (possibly unique) fixed point.\nAs in the standard MDP setting, one can define approximations to g\u03b3 that use a value function to truncate the returns for variance reduction. These approximations can take the form of n-step methods (Mnih et al. [2016]) or TD(\u03bb)-like methods (Schulman et al. [2015b]), though we will focus on n-step returns here. Based on the definition of g\u03b3 above, the natural choice of variance-reduced estimator is\nEt,st,at [ \u2207\u03b8 log \u03c0\u03b8(at | st) n\u22121\u2211 d=0 \u03b3d\u03b4t+d ] (52)\nwhere \u03b4t was defined in Equation (36). The state-value function V we use in the above formulas should approximate the entropy augmented return \u2211\u221e t=0 \u03b3\nt(rt \u2212 \u03c4 KLt). We can fit V iteratively by approximating the n-step backup V \u2190 T n\u03c0 V , by minimizing a squared-error loss\nL(V ) = Et,st [ 1 2 (V (st)\u2212 yt)2 ] , (53)\nwhere yt = n\u22121\u2211 d=0 \u03b3drt+d + \u03b3 dV (st+d) = V (st) + n\u22121\u2211 d=0 \u03b3d\u03b4t+d. (54)\n4 Soft Q-learning Gradient Equals Policy Gradient\nThis section shows that the gradient of the squared-error loss from soft Q-learning (Section 3.5) equals the policy gradient (in the family of policy gradients described in Section 3.6) plus the gradient of a squared-error term for fitting the value function. We will not make any assumption about the parameterization of the Q-function, but we define V\u03b8 and \u03c0\u03b8 as the following functions of the parameterized Q-function Q\u03b8:\nV\u03b8(s) := \u03c4 logEa [exp(Q\u03b8(s, a)/\u03c4)] (55) \u03c0\u03b8(a | s) := \u03c0(a | s) exp((Q\u03b8(s, a)\u2212 V\u03b8(s))/\u03c4) (56)\nHere, \u03c0\u03b8 is the Boltzmann policy for Q\u03b8, and V\u03b8 is the normalizing factor we described above. From these definitions, it follows that the Q-function can be written as\nQ\u03b8(s, a) = V\u03b8(s) + \u03c4 log \u03c0\u03b8(a | s) \u03c0(a | s) (57)\nWe will substitute this expression into the squared-error loss function. First, for convenience, let us define \u2206t = \u2211n\u22121 d=0 \u03b3\nd\u03b4t+d. Now, let\u2019s consider the gradient of the n-step soft Q-learning objective:\n\u2207\u03b8Et,st,at\u223c\u03c0\u03b8 [ 1 2\u2016Q\u03b8(st, at)\u2212 yt\u2016 2 ]\n(58)\nswap gradient and expectation, treating state distribution as fixed:\n= Et,st,at\u223c\u03c0\u03b8 [\u2207\u03b8Q\u03b8(st, at)(Q\u03b8(st, at)\u2212 yt)] (59) replace Q\u03b8 using Equation (57), and replace Q-value backup yt by Equation (46):\n= Et,st,at\u223c\u03c0\u03b8 [ \u2207\u03b8Q\u03b8(st, at)(\u03c4 log \u03c0(at | st)\u03c0(at | st) + V\u03b8(st)\u2212 (V\u03b8(st)\u2212 \u03c4DKL [\u03c0\u03b8 \u2016 \u03c0] (st) + \u2206t)) ] (60)\ncancel out V\u03b8(st): = Et,st,at\u223c\u03c0\u03b8 [ \u2207\u03b8Q\u03b8(st, at)(\u03c4 log \u03c0(at | st)\u03c0(at | st) \u2212 \u03c4DKL [\u03c0\u03b8 \u2016 \u03c0] (st)\u2212\u2206t) ] (61)\nreplace the other Q\u03b8 by Equation (57):\n= Et,st,at\u223c\u03c0\u03b8 [ (\u03c4\u2207\u03b8 log \u03c0\u03b8(at | st) +\u2207\u03b8V\u03b8(st)) \u00b7 (\u03c4 log \u03c0(at | st)\u03c0(at | st) \u2212 \u03c4DKL [\u03c0\u03b8 \u2016 \u03c0] (st)\u2212\u2206t) ] (62)\nexpand out terms:\n= Et,st,at\u223c\u03c0\u03b8 [ \u03c42\u2207\u03b8 log \u03c0\u03b8(at | st) log \u03c0\u03b8(at | st)\u03c0(at | st) + \u03c4\n2\u2207\u03b8 log \u03c0\u03b8(at | st)DKL [\u03c0\u03b8 \u2016 \u03c0] (st)\ufe38 \ufe37\ufe37 \ufe38 (\u2217)\n\u2212 \u03c4\u2207\u03b8 log \u03c0\u03b8(at | st)\u2206t + \u03c4\u2207\u03b8V\u03b8(st) log \u03c0\u03b8(at | st)\u03c0(at | st) + \u03c4\u2207\u03b8V\u03b8(st)DKL [\u03c0\u03b8 \u2016 \u03c0] (st)\ufe38 \ufe37\ufe37 \ufe38 (\u2217\u2217)\n\u2212\u2207\u03b8V\u03b8(st)\u2206t ]\n(63)\n(\u2217) vanishes because Ea\u223c\u03c0\u03b8(\u00b7 | st) [\u2207\u03b8 log \u03c0\u03b8(at | st) \u00b7 const] = 0 (\u2217\u2217) vanishes because Ea\u223c\u03c0\u03b8(\u00b7 | st) [ \u03c0\u03b8(at | st) \u03c0(at | st) ] = DKL [\u03c0\u03b8 \u2016 \u03c0] (st)\n= Et,st,at\u223c\u03c0\u03b8 [ \u2212 \u03c42\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st) + 0\u2212 \u03c4\u2207\u03b8 log \u03c0\u03b8(at | st)\u2206t + 0\u2212\u2207\u03b8V\u03b8(st)\u2206t ] (64)\nrearrange terms: = Et,st,at\u223c\u03c0\u03b8 [ \u2212\u03c4\u2207\u03b8 log \u03c0\u03b8(at | st)\u2206t + \u03c42\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0]](st)\ufe38 \ufe37\ufe37 \ufe38\npolicy grad\n+\u2207\u03b8 12 \u2225\u2225\u2225V\u03b8(st)\u2212 V\u0302t\u2225\u2225\u22252\ufe38 \ufe37\ufe37 \ufe38\nvalue function grad\n] (65)\nNote that the equivalent policy gradient method multiplies the policy gradient by a factor of \u03c4 , relative to the value function error. Effectively, the value function error has a coefficient of \u03c4\u22121, which is larger than what is typically used in practice (Mnih et al. [2016]). We will analyze this choice of coefficient in the experiments.\n5 Soft Q-learning and Natural Policy Gradients\nThe previous section gave a first-order view on the equivalence between policy gradients and soft Q-learning; this section gives a second-order, coordinate-free view. As previous work has pointed out, the natural gradient is the solution to a regression problem; here we will explore the relation between that problem and the nonlinear regression in soft Q-learning.\nThe natural gradient is defined as F\u22121g, where F is the average Fisher information matrix, F = Es,a\u223c\u03c0 [ (\u2207\u03b8 log \u03c0\u03b8(a | s))T (\u2207\u03b8 log \u03c0\u03b8(a | s)) ] , and g is the policy gradient estimate g \u221d E [\u2207\u03b8 log \u03c0\u03b8(a | s)\u2206], where \u2206 is an estimate of the advantage function. As pointed out by Kakade [2002], the natural gradient step can be computed as the solution to a least squares problem. Given timesteps t = 1, 2, . . . , T , define \u03c8t = \u2207\u03b8 log \u03c0\u03b8(at | st). Define \u03a8 as the matrix whose tth row is \u03c8t, let \u2206 denote the vector whose tth element is the advantage estimate \u2206t, and let denote a scalar stepsize parameter. Consider the least squares problem\nmin w\n1 2\u2016\u03a8w \u2212 \u2206\u2016 2 (66)\nThe least-squares solution is w = (\u03a8T\u03a8)\u22121\u03a8T\u2206. Note that E [ \u03a8T\u03a8 ] is the Fisher information matrix F ,\nand E [ \u03a8T\u2206 ] is the policy gradient g, so w is the estimated natural gradient.\nNow let us interpret the least-squares problem in Equation (66). \u03a8w is the vector whose tth row is \u2207\u03b8 log \u03c0\u03b8(a | s) \u00b7 w. According to the definition of the gradient, if we perform a parameter update with \u03b8 \u2212 \u03b8old = w, the change in log \u03c0\u03b8(a | s) is as follows, to first order in :\nlog \u03c0\u03b8(a | s)\u2212 log \u03c0\u03b8old(a | s) \u2248 \u2207\u03b8 log \u03c0\u03b8(a | s) \u00b7 w = \u03c8 \u00b7w (67)\nThus, we can interpret the least squares problem (Equation (66)) as solving\nmin \u03b8 T\u2211 t=1 1 2 (log \u03c0\u03b8(at | st)\u2212 log \u03c0\u03b8old(at | st)\u2212 \u2206t)2 (68)\nThat is, we are adjusting each log-probility log \u03c0\u03b8old(at | st) by the advantage function \u2206t, scaled by . In entropy-regularized reinforcement learning, we have an additional term for the gradient of the KLdivergence:\ng \u221d E [\u2207\u03b8 log \u03c0\u03b8(at | st)\u2206t \u2212 \u03c4\u2207\u03b8 KL[\u03c0\u03b8, \u03c0](st)] (69) = E [ \u2207\u03b8 log \u03c0\u03b8(at | st) ( \u2206t \u2212 \u03c4 [ log ( \u03c0\u03b8(at | st) \u03c0(at | st) ) \u2212KL[\u03c0\u03b8, \u03c0](st) ])] (70)\nwhere the second line used the formula for the KL-divergence (Equation (21)) and the identity that Eat\u223c\u03c0\u03b8 [\u2207\u03b8 log \u03c0\u03b8(at | st) \u00b7 const] = 0 (where the KL term is the constant.) In this case, the corresponding least squares problem (to compute F\u22121g) is\nmin \u03b8 T\u2211 t=1 1 2 ( log \u03c0\u03b8(at | st)\u2212 log \u03c0\u03b8old(at | st)\u2212 ( \u2206t \u2212 \u03c4 [ log ( \u03c0\u03b8(at | st) \u03c0(at | st) ) \u2212KL[\u03c0\u03b8old , \u03c0](st) ]))2 . (71)\nNow let\u2019s consider Q-learning. Let\u2019s assume that the value function is unchanged by optimization, so V\u03b8 = V\u03b8old . (Otherwise, the equivalence will not hold, since the value function will try to explain the measured advantage \u2206, shrinking the advantage update.)\n1 2 (Q\u03b8(st, at)\u2212 yt) 2 = 12 (( V\u03b8(st, at) + \u03c4 log ( \u03c0\u03b8(at | st) \u03c0(at | st) )) \u2212 (V\u03b8old(st) + \u03c4 KL[\u03c0\u03b8old , \u03c0](st) + \u2206t) )2 (72)\n= 12\n( \u03c4 log ( \u03c0\u03b8(at | st) \u03c0(at | st) ) \u2212 (\u2206t + \u03c4 KL[\u03c0\u03b8old , \u03c0](st)) )2 (73)\nEvidently, we are regressing log \u03c0\u03b8(at | st) towards log \u03c0\u03b8old(at | st) + \u2206t/\u03c4 + KL[\u03c0\u03b8old , \u03c0](st). This loss is not equivalent to the natural policy gradient loss that we obtained above.\nWe can recover the natural policy gradient by instead solving a damped version of the Q-function regression problem. Define Q\u0302 t = (1\u2212 )Q\u03b8old(st, at) + Q\u0302t, i.e., we are interpolating between the old value and the backed-up value.\nQ\u0302 t = (1\u2212 )Q\u03b8old(st, at) + Q\u0302t = Q\u03b8old(st, at) + (Q\u0302t \u2212Q\u03b8old(st, at)) (74) Q\u0302t \u2212Q\u03b8old(st, at) = (V\u03b8(st) + \u03c4 KL[\u03c0\u03b8old , \u03c0](st) + \u2206t)\u2212 ( V\u03b8old(st) + \u03c4 log ( \u03c0\u03b8old (at | st) \u03c0(at | st) )) (75)\n= \u2206t + \u03c4 [ KL[\u03c0\u03b8old , \u03c0](st)\u2212 log ( \u03c0\u03b8old (at | st) \u03c0(at | st) )] (76)\nQ\u03b8(st, at)\u2212 Q\u0302 t = Q\u03b8(st, at)\u2212 ( Q\u03b8old(st, at) + ( Q\u0302t \u2212Q\u03b8old(st, at) )) (77)\n= V\u03b8(st) + log ( \u03c0\u03b8(at | st) \u03c0(at | st) ) \u2212 { V\u03b8old(st) + log ( \u03c0\u03b8old (at | st) \u03c0(at | st) ) + ( \u2206 + \u03c4 [ KL[\u03c0\u03b8old , \u03c0](st)\u2212 log ( \u03c0\u03b8old (at | st) \u03c0(at | st) )])} = log \u03c0\u03b8(at | st)\u2212 log \u03c0\u03b8old(at | st)\u2212 ( \u2206t \u2212 \u03c4 [ log ( \u03c0\u03b8old (at | st) \u03c0(at | st) ) \u2212KL[\u03c0\u03b8old , \u03c0](st) ]) (78)\nwhich exactly matches the expression in the least squares problem in Equation (71), corresponding to entropyregularized natural policy gradient. Hence, the \u201cdamped\u201d Q-learning update corresponds to a natural gradient step."}, {"heading": "6 Experiments", "text": "To complement our theoretical analyses, we designed experiments to study the following questions:\n1. Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al. [2016]), how do the entropy-regularized RL versions of policy gradients and Q-learning described in Section 3 perform on challenging RL benchmark problems? How does the \u201cproper\u201d entropy-regularized policy gradient method (with entropy in the returns) compare to the naive one (with one-step entropy bonus)? (Section 6.1)\n2. How do the entropy-regularized versions of Q-learning (with logsumexp) compare to the standard DQN of Mnih et al. [2015]? (Section 6.2)\n3. The equivalence between PG and soft Q-learning is established in expectation, however, the actual gradient estimators are slightly different due to sampling. Furthermore, soft Q-learning is equivalent to PG with a particular penalty coefficient on the value function error. Does the equivalence hold under practical conditions? (Section 6.3)"}, {"heading": "6.1 A2C on Atari: Naive vs Proper Entropy Bonuses", "text": "Here we investigated whether there is an empirical effect of including entropy terms when computing returns, as described in Section 3. In this section, we compare the naive and proper policy gradient estimators:\nnaive / 1-step: \u2207 log \u03c0\u03b8(at | st) ( n\u22121\u2211 d=0 \u03b3drt+d \u2212 V (st) ) + \u03c4\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st) (79)\nproper: \u2207 log \u03c0\u03b8(at | st) ( n\u22121\u2211 d=0 \u03b3d(rt+d \u2212 \u03c4DKL [\u03c0\u03b8 \u2016 \u03c0] (st+d))\u2212 V (st) ) + \u03c4\u2207\u03b8DKL [\u03c0\u03b8 \u2016 \u03c0] (st) (80)\nIn the experiments on Atari, we take \u03c0 to be the uniform distribution, which gives a standard entropy bonus up to a constant.\nWe start with a well-tuned (synchronous, deterministic) version of A3C (Mnih et al. [2016]), henceforth called A2C (advantage actor critic), to optimize the entropy-regularized return. We use the parameter \u03c4 = 0.01 and train for 320 million frames. We did not tune any hyperparameters for the \u201cproper\u201d algorithm\u2014 we used the same hyperparameters that had been tuned for the \u201cnaive\u201d algorithm.\nAs shown in Figure 1, the \u201cproper\u201d version yields performance that is the same or possibly greater than the \u201cnaive\u201d version. Hence, besides being attractive theoretically, the entropy-regularized formulation could lead to practical performance gains."}, {"heading": "6.2 DQN on Atari: Standard vs Soft", "text": "Here we investigated whether soft Q-learning (which optimizes the entropy-augmented return) performs differently from standard \u201chard\u201d Q-learning on Atari. We made a one-line change to a DQN implementation:\nyt = rt + \u03b3max a\u2032\nQ(st+1, a \u2032) Standard (81)\nyt = rt + \u03b3 log \u2211 a\u2032 exp(Q(st+1, a \u2032)/\u03c4)\u2212 log|A| \u201cSoft\u201d: KL penalty (82)\nyt = rt + \u03b3 log \u2211 a\u2032 exp(Q(st+1, a \u2032)/\u03c4) \u201cSoft\u201d: Entropy bonus (83)\nThe difference between the entropy bonus and KL penalty (against uniform) is simply a constant, however, this constant made a big difference in the experiments, since a positive constant added to the reward encourages longer episodes. Note that we use the same epsilon-greedy exploration in all conditions; the only difference is the backup equation used for computing yt and defining the loss function.\nThe results of two runs on each game are shown in Figure 2. The entropy-bonus version with \u03c4 = 0.1 seems to perform a bit better than standard DQN, however, the KL-bonus version performs worse, so the benefit may be due to the effect of adding a small constant to the reward. We have also shown the results for 5-step Q-learning, where the algorithm is otherwise the same. The performance is better on Pong and Q-bert but worse on other games\u2014this is the same pattern of performance found with n-step policy gradients. (E.g., see the A2C results in the preceding section.)\n6.3 Entropy Regularized PG vs Online Q-Learning on Atari\nNext we investigate if the equivalence between soft Q-learning and PG is relevant in practice\u2014we showed above that the gradients are the same in expectation, but their variance might be different, causing different\nlearning dynamics. For these experiments, we modified the gradient update rule used in A2C while making no changes to any algorithmic component, i.e. parallel rollouts, updating parameters every 5 steps, etc. The Q-function was represented as: Q\u03b8(s, a) = V\u03b8(s) + \u03c4 log \u03c0\u03b8(a | s), which can be seen as a form of dueling architecture with \u03c4 log \u03c0\u03b8(a | s) being the \u201cadvantage stream\u201d (Wang et al. [2015]). V\u03b8, \u03c0\u03b8 are parametrized as the same neural network as A2C, where convolutional layers and the first fully connected layer are shared. \u03c0\u03b8(a | s) is used as behavior policy.\nA2C can be seen as optimizing a combination of a policy surrogate loss and a value function loss, weighted by hyperparameter c:\nLpolicy = \u2212 log \u03c0\u03b8(at | st)\u2206t + \u03c4DKL [\u03c0\u03b8 \u2016 \u03c0]](st) (84)\nLvalue = 1 2 \u2225\u2225\u2225V\u03b8(st)\u2212 V\u0302t\u2225\u2225\u22252 (85) La3c = Lpolicy + cLvalue (86)\nIn normal A2C, we have found c = 0.5 to be a robust setting that works across multiple environments. On the other hand, our theory suggests that if we use this Q-function parametrization, soft Q-learning has the same expected gradient as entropy-regularized A2C with a specific weighting c = 1\u03c4 . Hence, for the usual entropy bonus coefficient setting \u03c4 = 0.01, soft Q-learning is implicitly weighting value function loss a lot more than usual A2C setup (c = 100 versus c = 0.5). We have found that such emphasis on value function (c = 100) results in unstable learning for both soft Q-learning and entropy-regularized A2C. Therefore, to make Q-learning exactly match known good hyperparameters used in A2C, we scale gradients that go into advantage stream by 1\u03b3 and scale gradients that go into value function stream by c = 0.5.\nWith the same default A2C hyperparameters, learning curves of PG and QL are almost identical in most games (Figure 3), which indicates that the learning dynamics of both update rules are essentially the same even when the gradients are approximated with a small number of samples. Notably, the Q-value Regression method here demonstrates stable learning without the use of target network or schedule."}, {"heading": "7 Related Work", "text": "Three recent papers have drawn the connection between policy-based methods and value-based methods, which becomes close with entropy regularization.\n\u2022 O\u2019Donoghue et al. [2016] begin with a similar motivation as the current paper: that a possible explanation for Q-learning and SARSA is that their updates are similar to policy gradient updates. They decompose the Q-function into a policy part and a value part, inspired by dueling Q-networks (Wang et al. [2015]):\nQ(s, a) = V (s) + \u03c4(log \u03c0(a | s) + \u03c4S[\u03c0(\u00b7 | s)]) (87)\nThis form is chosen so that the term multiplying \u03c4 has expectation zero under \u03c0, which is a property that the true advantage function satisfies: E\u03c0 [A\u03c0] = 0. Note that our work omits that S term, because it is most natural to define the Q-function to not include the first entropy term. The authors show that taking the gradient of the Bellman error of the above Q-function leads to a result similar to the policy gradient. They then propose an algorithm called PGQ that mixes together the updates from different prior algorithms.\n\u2022 Nachum et al. [2017] also discuss the entropy-regularized reinforcement learning setting, and develop an off-policy method that applies in this setting. Their argument (modified to use our notation and KL penalty instead of entropy bonus) is as follows. The advantage function A\u03c0(s, a) = Q\u03c0(s, a)\u2212V\u03c0(s) lets us define a multi-step consistency equation, which holds even if the actions were sampled from a different (suboptimal) policy. In the setting of deterministic dynamics, Q\u03c0(st, at) = rt + \u03b3V\u03c0(st+1), hence\nn\u22121\u2211 t=0 \u03b3tA\u03c0(st, at) = n\u22121\u2211 t=0 \u03b3t(rt + \u03b3V\u03c0(st+1)\u2212 V\u03c0(st)) = n\u22121\u2211 t=0 \u03b3trt + \u03b3 nV\u03c0(sn)\u2212 V\u03c0(s0) (88)\nIf \u03c0 is the optimal policy (for the discounted, entropy-augmented return), then it is the Boltzmann policy for Q\u03c0, thus\n\u03c4(log \u03c0(a | s)\u2212 log \u03c0(a | s)) = AQ\u03c0 (s, a) (89)\nThis expression for the advantage can be substituted into Equation (88), giving the consistency equation\nn\u22121\u2211 t=0 \u03b3t\u03c4(log \u03c0(st, at)\u2212 log \u03c0(st, at)) = n\u22121\u2211 t=0 \u03b3trt + \u03b3 nV\u03c0(sn)\u2212 V\u03c0(s0), (90)\nwhich holds when \u03c0 is optimal. The authors define a squared error objective formed from by taking LHS - RHS in Equation (90), and jointly minimize it with respect to the parameters of \u03c0 and V . The resulting algorithm is a kind of Bellman residual minimization\u2014it optimizes with respect to the future target values, rather than treating them as fixed Scherrer [2010].\n\u2022 Haarnoja et al. [2017] work in the same setting of soft Q-learning as the current paper, and they are concerned with tasks with high-dimensional action spaces, where we would like to learn stochastic policies that are multi-modal, and we would like to use Q-functions for which there is no closed-form way of sampling from the Boltzmann distribution \u03c0(a | s) \u221d \u03c0(a | s) exp(Q(s, a)/\u03c4). Hence, they use a method called Stein Variational Gradient Descent to derive a procedure that jointly updates the Qfunction and a policy \u03c0, which approximately samples from the Boltzmann distribution\u2014this resembles variational inference, where one makes use of an approximate posterior distribution."}, {"heading": "8 Conclusion", "text": "We study the connection between two of the leading families of RL algorithms used with deep neural networks. In a framework of entropy-regularized RL we show that soft Q-learning is equivalent to a policy gradient method (with value function fitting) in terms of expected gradients (first-order view). In addition, we also analyze how a damped Q-learning method can be interpreted as implementing natural policy gradient (second-order view). Empirically, we show that the entropy regularized formulation considered in our theoretical analysis works in practice on the Atari RL benchmark, and that the equivalence holds in a practically relevant regime."}], "references": [{"title": "Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1512.08562,", "citeRegEx": "Fox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2015}, {"title": "Reinforcement learning with deep energy-based policies", "author": ["Tuomas Haarnoja", "Haoran Tang", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1702.08165,", "citeRegEx": "Haarnoja et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Haarnoja et al\\.", "year": 2017}, {"title": "A natural policy gradient", "author": ["Sham Kakade"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Kakade.,? \\Q2002\\E", "shortCiteRegEx": "Kakade.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Bridging the gap between value and policy based reinforcement learning", "author": ["Ofir Nachum", "Mohammad Norouzi", "Kelvin Xu", "Dale Schuurmans"], "venue": "arXiv preprint arXiv:1702.08892,", "citeRegEx": "Nachum et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Nachum et al\\.", "year": 2017}, {"title": "Pgq: Combining policy gradient and q-learning", "author": ["Brendan O\u2019Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "venue": "arXiv preprint arXiv:1611.01626,", "citeRegEx": "O.Donoghue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2016}, {"title": "Should one compute the temporal difference fix point or minimize the bellman residual? the unified oblique projection view", "author": ["Bruno Scherrer"], "venue": "arXiv preprint arXiv:1011.4362,", "citeRegEx": "Scherrer.,? \\Q2010\\E", "shortCiteRegEx": "Scherrer.", "year": 2010}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["Brian D Ziebart"], "venue": null, "citeRegEx": "Ziebart.,? \\Q2010\\E", "shortCiteRegEx": "Ziebart.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016].", "startOffset": 197, "endOffset": 216}, {"referenceID": 8, "context": "With an entropy cost added to the returns, the optimal policy has the form \u03c0(a | s) \u221d exp(Q(s, a)); hence policy gradient methods solve for the optimal Q-function, up to an additive constant (Ziebart [2010]).", "startOffset": 192, "endOffset": 207}, {"referenceID": 3, "context": "O\u2019Donoghue et al. [2016] also discuss the connection between the fixed points and updates of PG and QL methods, though the discussion of fixed points is restricted to the tabular setting, and the discussion comparing updates is informal and shows an approximate equivalence.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning.", "startOffset": 128, "endOffset": 147}, {"referenceID": 3, "context": "Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning. Section 2 uses the bandit setting to provide the reader with a simplified version of our main calculation. (The main calculation applies to the MDP setting.) Section 3 discusses the entropy-regularized formulation of RL, which is not original to this work, but is included for the reader\u2019s convenience. Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016]. Section 5 draws a connection between QL methods that use batch updates or replay-buffers, and natural policy gradient methods. Some previous work on entropy regularized reinforcement learning (e.g., O\u2019Donoghue et al. [2016], Nachum et al.", "startOffset": 128, "endOffset": 976}, {"referenceID": 3, "context": "Altogether, the update matches what is typically done in \u201cactor-critic\u201d policy gradient methods such as A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and n-step Q-learning. Section 2 uses the bandit setting to provide the reader with a simplified version of our main calculation. (The main calculation applies to the MDP setting.) Section 3 discusses the entropy-regularized formulation of RL, which is not original to this work, but is included for the reader\u2019s convenience. Section 4 shows that the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016]. Section 5 draws a connection between QL methods that use batch updates or replay-buffers, and natural policy gradient methods. Some previous work on entropy regularized reinforcement learning (e.g., O\u2019Donoghue et al. [2016], Nachum et al. [2017]) uses entropy bonuses, whereas we use a penalty on Kullback-Leibler (KL) divergence, which is a bit more general.", "startOffset": 128, "endOffset": 998}, {"referenceID": 9, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al.", "startOffset": 118, "endOffset": 133}, {"referenceID": 0, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al.", "startOffset": 134, "endOffset": 152}, {"referenceID": 0, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al.", "startOffset": 134, "endOffset": 176}, {"referenceID": 0, "context": "We shall consider an entropy-regularized version of the reinforcement learning problem, following various prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al. [2017]).", "startOffset": 134, "endOffset": 198}, {"referenceID": 3, "context": "Following Mnih et al. [2015], modern implementations of Q-learning, and nstep Q-learning (see Mnih et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 3, "context": "Following Mnih et al. [2015], modern implementations of Q-learning, and nstep Q-learning (see Mnih et al. [2016]) update the Q-function incrementally to compute the backup against a fixed target Q-function, which we\u2019ll call Q.", "startOffset": 10, "endOffset": 113}, {"referenceID": 9, "context": "(Williams [1992], Mnih et al.", "startOffset": 1, "endOffset": 17}, {"referenceID": 3, "context": "(Williams [1992], Mnih et al. [2016]).", "startOffset": 18, "endOffset": 37}, {"referenceID": 8, "context": "This result is obtained directly by considering the stochastic computation graph for the loss (Schulman et al. [2015a]), shown in the figure on the right.", "startOffset": 95, "endOffset": 119}, {"referenceID": 3, "context": "These approximations can take the form of n-step methods (Mnih et al. [2016]) or TD(\u03bb)-like methods (Schulman et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 3, "context": "These approximations can take the form of n-step methods (Mnih et al. [2016]) or TD(\u03bb)-like methods (Schulman et al. [2015b]), though we will focus on n-step returns here.", "startOffset": 58, "endOffset": 125}, {"referenceID": 3, "context": "Effectively, the value function error has a coefficient of \u03c4\u22121, which is larger than what is typically used in practice (Mnih et al. [2016]).", "startOffset": 121, "endOffset": 140}, {"referenceID": 2, "context": "As pointed out by Kakade [2002], the natural gradient step can be computed as the solution to a least squares problem.", "startOffset": 18, "endOffset": 32}, {"referenceID": 9, "context": "Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al.", "startOffset": 84, "endOffset": 100}, {"referenceID": 3, "context": "Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al. [2016]), how do the entropy-regularized RL versions of policy gradients and Q-learning described in Section 3 perform on challenging RL benchmark problems? How does the \u201cproper\u201d entropy-regularized policy gradient method (with entropy in the returns) compare to the naive one (with one-step entropy bonus)? (Section 6.", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992], Mnih et al. [2016]), how do the entropy-regularized RL versions of policy gradients and Q-learning described in Section 3 perform on challenging RL benchmark problems? How does the \u201cproper\u201d entropy-regularized policy gradient method (with entropy in the returns) compare to the naive one (with one-step entropy bonus)? (Section 6.1) 2. How do the entropy-regularized versions of Q-learning (with logsumexp) compare to the standard DQN of Mnih et al. [2015]? (Section 6.", "startOffset": 101, "endOffset": 558}, {"referenceID": 3, "context": "We start with a well-tuned (synchronous, deterministic) version of A3C (Mnih et al. [2016]), henceforth called A2C (advantage actor critic), to optimize the entropy-regularized return.", "startOffset": 72, "endOffset": 91}, {"referenceID": 10, "context": "The Q-function was represented as: Q\u03b8(s, a) = V\u03b8(s) + \u03c4 log \u03c0\u03b8(a | s), which can be seen as a form of dueling architecture with \u03c4 log \u03c0\u03b8(a | s) being the \u201cadvantage stream\u201d (Wang et al. [2015]).", "startOffset": 174, "endOffset": 193}, {"referenceID": 5, "context": "\u2022 O\u2019Donoghue et al. [2016] begin with a similar motivation as the current paper: that a possible explanation for Q-learning and SARSA is that their updates are similar to policy gradient updates.", "startOffset": 2, "endOffset": 27}, {"referenceID": 5, "context": "\u2022 O\u2019Donoghue et al. [2016] begin with a similar motivation as the current paper: that a possible explanation for Q-learning and SARSA is that their updates are similar to policy gradient updates. They decompose the Q-function into a policy part and a value part, inspired by dueling Q-networks (Wang et al. [2015]): Q(s, a) = V (s) + \u03c4(log \u03c0(a | s) + \u03c4S[\u03c0(\u00b7 | s)]) (87) This form is chosen so that the term multiplying \u03c4 has expectation zero under \u03c0, which is a property that the true advantage function satisfies: E\u03c0 [A\u03c0] = 0.", "startOffset": 2, "endOffset": 314}, {"referenceID": 5, "context": "\u2022 Nachum et al. [2017] also discuss the entropy-regularized reinforcement learning setting, and develop an off-policy method that applies in this setting.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": "The resulting algorithm is a kind of Bellman residual minimization\u2014it optimizes with respect to the future target values, rather than treating them as fixed Scherrer [2010]. \u2022 Haarnoja et al.", "startOffset": 157, "endOffset": 173}, {"referenceID": 1, "context": "\u2022 Haarnoja et al. [2017] work in the same setting of soft Q-learning as the current paper, and they are concerned with tasks with high-dimensional action spaces, where we would like to learn stochastic policies that are multi-modal, and we would like to use Q-functions for which there is no closed-form way of sampling from the Boltzmann distribution \u03c0(a | s) \u221d \u03c0(a | s) exp(Q(s, a)/\u03c4).", "startOffset": 2, "endOffset": 25}], "year": 2017, "abstractText": "Two of the leading approaches for model-free reinforcement learning are policy gradient methods and Q-learning methods. Q-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the Q-values they estimate are very inaccurate. A partial explanation may be that Q-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between Q-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that \u201csoft\u201d (entropy-regularized) Q-learning is exactly equivalent to a policy gradient method. We also point out a connection between Q-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of Q-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a Q-learning method that closely matches the learning dynamics of A3C without using a target network or -greedy exploration schedule.", "creator": "LaTeX with hyperref package"}}}