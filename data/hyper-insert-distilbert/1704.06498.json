{"id": "1704.06498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces", "abstract": "graph models nevertheless are explicitly relevant in such many fields, such especially as centralized distributed computing, enterprise intelligent computational tutoring business systems database or social network analysis. in many cases, such models need to rapidly take changes in the resulting graph structure graph into realistic account, i. e. a varying number density of nodes or edges. computers predicting such changes significantly within graphs can conversely be usually expected to explicitly yield reasonably important insight with respect externally to the underlying dynamics, just e. g. with respect to optimal user behaviour. however, predictive tuning techniques in predicting the prehistoric past have almost overwhelmingly exclusively focused on establishing single unit edges roles or nodes. in analyzing this contribution, moreover we deliberately attempt too to simply predict the future state of a composite graph as a whole. often we propose here to phrase time series process prediction as a regression complex problem response and apply dissimilarity - or empirical kernel - based regression techniques, such as 1 - nearest mutual neighbor, kernel component regression and gaussian process regression, terms which hence can truly be mainly applied to graphs prediction via graph kernels. moreover the true output characteristic of writing the regression equations is a fix point embedded in a pseudo - random euclidean data space, which can actually be analyzed using subsequent matrix dissimilarity - or kernel - based processing analytic methods. both we discuss suitable strategies to speed speeds up gaussian reconstruction processes regression vary from cubic to linear information time model and can evaluate hence our approach on two well - sufficiently established theoretical models of graph kernel evolution as well a as two real theoretical data sets dating from the domain of intelligent tutoring systems. but we find that a simple neural regression control methods, such as kernel parameter regression, similarly are necessarily sufficient to capture specifically the dynamics in the detailed theoretical models, but that gaussian process regression significantly actively improves the prediction error risk for constructing real - world search data.", "histories": [["v1", "Fri, 21 Apr 2017 12:08:30 GMT  (130kb,D)", "http://arxiv.org/abs/1704.06498v1", "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')"], ["v2", "Tue, 6 Jun 2017 10:21:36 GMT  (136kb,D)", "http://arxiv.org/abs/1704.06498v2", "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')"], ["v3", "Fri, 11 Aug 2017 11:47:42 GMT  (138kb,D)", "http://arxiv.org/abs/1704.06498v3", "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')"]], "COMMENTS": "preprint of a submission to 'Neural Processing Letters' (Special issue 'Off the mainstream')", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["benjamin paa{\\ss}en", "christina g\\\"opfert", "barbara hammer"], "accepted": false, "id": "1704.06498"}, "pdf": {"name": "1704.06498.pdf", "metadata": {"source": "CRF", "title": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces\u2217\u2020", "authors": ["Benjamin Paa\u00dfen", "Christina G\u00f6pfert", "Barbara Hammer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "To model connections between entities, graphs are oftentimes the method of choice, e.g. to model traffic connections between cities [42], data lines between \u2217Funding by the DFG under grant number HA 2719/6-2 and the CITEC center of excellence (EXC 277) is gratefully acknowledged. \u2020This contribution is an extension of the work presented at ESANN 2016 under the title \u201cGaussian process prediction for time series of structured data\u201d [40].\n1\nar X\niv :1\n70 4.\n06 49\n8v 1\n[ cs\n.A I]\n2 1\ncomputing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41]. In all these examples, nodes as well as connections change significantly with time. For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31]. In all these cases it would be beneficial to predict the next state of the graph in question, e.g. to re-route traffic accordingly, to provide additional communication bandwidth if required or to guide a student through her learning process using hints.\nTraditionally, predicting the future development based on knowledge of the past is the topic of time series prediction, which has wide-ranging applications in physics, sociology, medicine, engineering, finance and other fields [49, 52]. However, classic models in time series prediction, such as ARIMA, NARX, Kalman filters, recurrent networks or reservoir models focus on vectorial data representations, and they are not equipped to handle time series of graphs [52]. Accordingly, past work on predicting changes in graphs has focused on simpler sub-problems which can be modelled using such classic problems, e.g. predicting the overall load in an energy network [2] or predicting the appearance of single edges in a social network [34].\nIn this contribution, we develop an approach to address the time series prediction problem for graphs, which we frame as a regression problem with structured data as input and as output. Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54]. This representation embeds the discrete set of graphs in a smooth kernel space. Second, within this space, we can apply similarity- and kernel-based regression methods, such as nearest neighbor regression, kernel regression [39] or Gaussian processes [44] to predict the next position in the kernel space given the current position. While this point in the kernel space can not be directly interpreted as a graph, we will show that this data point can still be analyzed with subsequent kernel- or dissimilarity based methods.\nIf the underlying dynamics in the kernel space can not be captured by a simple regression scheme, such as 1-nearest neighbor or kernel regression, more complex models, such as Gaussian processes (GPs), are required. However, GPs suffer from a relatively high computational complexity due to a kernel matrix inversion. Fortunately, Deisenroth and Ng have suggested a simple strategy to permit predictions in linear time, namely distributing the prediction to multiple Gaussian processes, each of which handles only a constant-sized subset of the data [15]. Further, pre- and postprocessing of kernel data - e.g. for eigenvalue correction - usually requires quadratic or cubic time. This added complexity\ncan be avoided using the well-known Nystr\u00f6m approximation as investigated by [22].\nThe key contributions of our work are: First, we provide an integrative overview of seemingly disparate threads of research which are related to timevarying graphs. Second, we provide a scheme for time series prediction in dissimilarity and kernel spaces without any need for an explicit vectorial embedding, using one of three non-parametric regression methods. Third, we discuss how the predictive result, which is a point in an implicit kernel feature space, can be analyzed using subsequent kernel- or dissimilarity-based methods. Fourth, we provide an efficient realisation of our prediction pipeline for Gaussian processes in linear time. Finally, we evaluate our proposed approaches on two theoretical and two practical data sets.\nWe start our investigation by covering related work and introducing notation to describe dynamics on graph data. Based on this work, we develop our proposed approach of time series prediction in kernel and dissimilarity spaces. We also discuss speedup techniques to provide predictions in linear time. Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38]. We find that for the theoretical models, even simple regression schemes may be sufficient to capture the underlying dynamics, but for the Java data sets, applying Gaussian processes significantly reduces the prediction error."}, {"heading": "2 Background and Related Work", "text": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37]. Due to the breadth of the field, we focus here on relatively general concepts which can be applied to a wide variety of domains. We begin with two formalisms to model dynamics in graphs, namely time-varying graphs [11], and sequential dynamical systems [6]. We then turn towards our research question: predicting the dynamics in graphs. This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23]. Finally, as preparation for our own approach, we discuss graph kernels and dissimilarities as well as prior work on kernel-based approaches for time series prediction."}, {"heading": "2.1 Models of Graph Dynamics", "text": "Time-Varying Graphs: Time-varying graphs have been introduced by Casteigts and colleagues in an effort to integrate different notations found in the fields of delay-tolerant networks, opportunistic-mobility networks or social networks [11]. The authors note that in all these domains, graph topology changes over time and the influence of such changes is too severe as to model them only in terms\nof system anomalies. Rather, dynamics have to be regarded as an \u201cintegral part of the nature of the system\u201d [11]. We revisit a slightly varied version of the notation developed in their work here to provide an overview of the types of problems which are related to dynamics in graphs.\nDefinition 1 (Time-Varying Graph [11]). A time-varying graph is defined as a five-tuple G = (V,E, T , \u03c8, \u03c1) where\n\u2022 V is an arbitrary set called nodes;\n\u2022 E \u2286 V \u00d7 V is a set of node tupels called edges,\n\u2022 T is an interval [t0, T ] \u2282 R called lifetime of the graph,\n\u2022 \u03c8 : V \u00d7 T \u2192 {0, 1} is called node presence function, and node v is called present at time t if and only if \u03c8(v, t) = 1, and\n\u2022 \u03c1 : E \u00d7 T \u2192 {0, 1} is called edge presence function, and edge e is called present at time t if and only if \u03c1(e, t) = 1.\nIn figure 1, we show an example of a time-varying graph modelling the connectivity in simple public transportation graph over the course of a day. In this example, nodes model stations and edges model train connections between stations. In the night (left), all nodes may be present but no edges, because no lines are active yet. During the early morning (middle), some lines become active while others remain inactive. Finally, in mid-day (right), all lines are scheduled to be active, but due to a disturbance - e.g. construction work - a station is closed and all adjacent connections become unavailable.\nUsing the notion of a presence function, many interesting concepts from static graph theory can be generalized to a dynamic version. For example, the neighborhood of a node u at time t can be defined as the set of all nodes v for which an edge (u, v) \u2208 E exists such that \u03c1((u, v), t) = 1. Similarly, a path at time t between two nodes u and v can be defined as a sequence of edges (u1, v1), . . . , (uK , vK), such that u = u1, v = vK and for all k (uk, vk) \u2208 E, uk+1 = vk, \u03c8(uk, t) = 1, and \u03c1((uk, vk), t) = 1. Two nodes u and v can be called connected at time t if a path between them exists at time t. Further, we can\ndefine the temporal subgraph Gt of graph G at time t as the graph of all nodes and edges of G which are present at time t.\nNote that the entire dynamic of a graph is captured by the node and edge presence function. As both these functions are discrete, changes take the form of discrete events, where an event is a time where the value of the presence function changes for at least one node or edge. Assuming that there are only finitely many such events, we can write all events in the lifetime of a graph as an ascending sequence t1, . . . , tK . Accordingly, all changes in the graph are fully described by the sequence of temporal subgraphs Gt1 , . . . ,GtK [11, 50]. As such, even time-varying graphs defined on continuous time can be mapped to a discrete-time representation by considering this event-based representation. Therefore, we will subsequently assume that time is discrete.\nSequential Dynamical Systems: Sequential dynamical systems (SDS) have been introduced by Barret, Reidys and Mortvart to generalize cellular automata to arbitrary graphical structures [6, 8]. In essence, they assign a binary state to each node v in a static graph G = (V,E). This state is updated according to a function fv which maps the current states of the node itself and all of its neighbors to the next state of the node v. This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8]. Interestingly, SDSs can be related to time-varying graphs by interpreting the binary state of a node v at time t as the value of its presence function \u03c8(v, t). As such, sequential dynamical systems provide a compact model for the node presence function of a time-varying graph. Furthermore, if the graph dynamics is gouverned by a known SDS, its future can be predicted by simply simulating the dynamic system. Unfortunately, to our knowledge, no learning schemes exists to date to infer an underlying SDS from data. Therefore, other predictive methods are required."}, {"heading": "2.2 Predicting Changes in Graphs", "text": "In accordance with classical time series prediction, one can describe time series prediction in graphs as predicting the next temporal subgraph Gt+1 given a sequence of temporal subgraphs G0, . . . ,Gt in a time-varying graph. To our knowledge, there exists no approach which addresses this problem in this general form. However, more specific subproblems have been addressed in the literature.\nLink Prediction: In the realm of social network analysis, [34] have formulated the link prediction problem, which can be stated as: Given a sequence of temporal subgraphs G0, . . . ,Gt for a time-varying graph G, which edges will be added to the graph in the next time step, i.e. for which edges do we find \u03c1(e, t) = 0 but \u03c1(e, t + 1) = 1 [55]? For example, given all past collaborations in a scientific community, can we predict new collaborations in the future? The simplest approaches to address this challenge derive a similarity index between nodes, such as the number of common neighbors, rank all non-existing edges\naccording to the similarity index of their nodes and predict all edges which are above a certain threshold [34, 35]. However, more complex models exist as well, such as supervised models [35], and probabilistic as well as stochastic models [36]. Note that the formulation of the link prediction problem presented here implicitly makes the Markov assumption (i.e. the next state of the graph only depends on the present state and not on any past state). We will make the very same assumption in our work later on.\nGrowth models: In a seminal paper, Barab\u00e1si and Albert described a simple model to incrementally grow an undirected graph node by node from a small, fully connected seed graph [4]. Since then, many other models of graph growth have emerged, most notably stochastic block models and latent space models [12, 23]. Stochastic block models assign each node to a block and model the probability of an edge between two nodes only dependent on their respective blocks [30]. Latent space models embed all nodes in an underlying, latent space and model the probability of an edge depending on the distance in this space [27]. Both classes of models can be used for link prediction as well as graph generation. Further, they can be trained with pre-observed data in order to provide more accurate models of the data. However, none of the discussed models addresses the question of time series prediction in general because the deletion of nodes or edges or label changes are not covered by the models."}, {"heading": "2.3 Graph Dissimilarities and Kernels", "text": "Instead of predicting the next graph in a sequence directly, one can consider indirect approaches, which base a prediction on pairwise dissimilarities d(G,G\u2032) or kernels k(G,G\u2032). As a simple example, consider a 1-nearest-neighbor approach: We first aggregate a database of training data consisting of graphs Gt and their respective successors Gt+1. If we are confronted with a new graph G\u2032, we simply predict the graph Gt+1 such that d(Gt,G\u2032) is minimized or k(Gt,G\u2032) is maximized. But how can such a dissimilarity or kernel on graphs be formalized? In the literature, we observe two main streams of research:\nGraph Edit Distance: The graph edit distance between two graphs G and G\u2032 is traditionally defined as the minimum number of edit operations required to transform G into G\u2032. Permitted edit operations include node insertion, node deletion, edge insertion, edge deletion, and the substitution of labels in nodes or edges [48]. This problem is a generalization of the classic string or tree edit distance, which is defined as the minimum number of operations required to transform a string into another or a tree into another respectively [33, 57]. Unfortunately, while the string edit distance and the tree edit distance can be efficiently computed in O(n2) and O(n4) respectively, computing the exact graph edit distance is NP-hard [56]. However, many approximation schemes exist, e.g. relying on self-organizing maps, Gaussian mixture models, graph kernels or binary linear programming [19]. A particularly simple approximation\nscheme is to order the nodes of a graph in a sequence and then apply a standard string edit distance measure on these sequences (refer e.g. to [41, 46, 47]). For our experiments on real-world java data we will rely on a kernel over such an approximated graph distance as suggested in [41].\nNote that for labelled graphs one may desire to assign non-uniform costs for substituting labels. Indeed, some labels may be semantically more related than others, implying a lower edit cost. Learning such edit costs from data is the topic of structure metric learning, which has mainly been investigated for string edit distances [9]. However, if string edit distance is applied as a substitute for graph edits, the results apply to graphs as well [41].\nGraph Kernels: Complementary to the view of graph edit distances, graph kernels represent the similarity between two graphs instead of their dissimilarity. The only formal requirement on a graph kernel is that it implicitly or explicitly maps a graph G to a vectorial feature representation \u03c6(G) and computes the pairwise kernel values of two graphs G and G\u2032 as the dot-product of their feature vectors k(G,G\u2032) = \u03c6(G)T \u00b7 \u03c6(G\u2032) [14]. If each graph has a unique representation (that is, \u03c6 is injective) computing such a kernel is at least as hard as the graph isomorphy problem [24]. Thus, efficient graph kernels rely on a non-injective feature embedding, which is still expressive enough to capture important differences between graphs [14]. A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16]. In our experiments, we will apply the shortest-path kernel suggested by Borgwardt and colleagues, which compares the lengths of shortest paths in both graphs to construct an overall graph kernel [10]."}, {"heading": "2.4 Kernel-Based approaches for Vectorial Time Series Prediction", "text": "The idea to apply kernel-based methods for time series prediction as such is not new. One popular example is the use of support vector regression with wide-ranging applications in finance, business, environmental research and engineering [49]. Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45]. To our knowledge, however, all applications of time series prediction to date have focused on vectorial data. Our work, on the other hand, is concerned with structured data, such as sequences, trees and graphs."}, {"heading": "3 Time Series Prediction for Graphs", "text": ""}, {"heading": "3.1 Time Series Prediction as a Regression Problem", "text": "Relying on the notation of time-varying graphs as introduced above, we can describe a time series of graphs as a sequence of temporal subgraphs G0, . . . ,Gt.\nTime series prediction is the problem of predicting the next graph in the series, Gt+1. We first transform this problem into a regression problem as suggested by Sapankecych and colleagues, that is, we try to learn a function f which maps the past K states of a time series to a successor state [49]. For simplicity, we will assume that K = 1, which is equivalent to the Markov assumption. Note that this assumption can easily be dropped by considering dissimilarities or similarities on sequences of graphs instead of simple graphs. Using this simplification, we can express our training data as tuples {(xi, yi)} for i \u2208 {1, . . . , N} where yi is the successor of xi in some time series. We denote the unseen test data point, for which a prediction is desired, by x.\nWe proceed by introducing three non-parametric regression techniques, namely 1-nearest neighbor, kernel regression and Gaussian process regression. We then revisit some basic research on dissimilarities, similarities and kernels and leverage these insights to derive time series prediction purely in latent spaces without referring to any explicit vectorial form. Finally, we discuss how to speed up Gaussian process prediction from cubic to linear time."}, {"heading": "3.2 Non-Parametric Regression techniques", "text": "In this section, we introduce three non-parametric regression techniques in their standard form, assuming vectorial input and output data. Further below we explain how these methods can be applied to structured input and output data. We assume that a training data set of tuples {(~xi, ~yi)} is available, where ~yi is the desired output for input ~xi.\n1-Nearest Neighbor (1-NN): We define the predictive function for onenearest neighbor regression (1-NN) as follows:\nf(~x) := ~yi+ where i+ = argmin i\u2208{1,...,N} d(~x, ~xi) (1)\nNote that the predictive function of 1-NN is not smooth, because the argmin result is ill-defined if there exist two i, i\u2032 with i 6= i\u2032 and ~yi 6= ~yi\u2032 such that d(~x, ~xi) = d(~x, ~xi\u2032). At these points, the predictive function is discontinuous and jumps between ~yi and ~yi\u2032 . A simple way to address this issue is kernel regression.\nKernel Regression (KR): Kernel regression was first proposed by Nadaraya and Watson and can be seen as a generalization of 1-nearest neighbor to a smooth predictive function f using a kernel k instead of a dissimilarity [39]. The predictive function is given as:\nf(~x) := \u2211N i=1 k(~x, ~xi) \u00b7 ~yi\u2211N i=1 k(~x, ~xi)\n(2)\nNote that kernel regression requires that at least one i exists such that k(~x, ~xi) > 0, i.e. if the test data point is not similar to any training data point, the prediction degenerates. Further, in general an exact reproduction of the training data\nis not possible, i.e. f(~xi) 6= ~yi. To achieve both, training data reproduction as well as a smooth predictive function, we turn to Gaussian process regression.\nGaussian Process Regression (GPR): In Gaussian process regression (GPR) we assume that the output points (training as well as test) are a realization of a multivariate random variable with a Gaussian distribution [44]. The model extends KR in several ways. First, we can encode prior knowledge regarding the output points via the mean of our prior distribution, denoted as ~\u03b8i and ~\u03b8 for ~yi and ~y respectively. Second, we can cover Gaussian noise on our training output points within out model. For this noise, we assume mean 0 and standard deviation \u03c3\u0303.\nLet now k be a kernel on X . We define ~k := ( k(~x, ~x1), . . . , k(~x, ~xN ) ) and (3)\nK := ( k(~xi, ~xi\u2032) ) i,i\u2032=1...N\n(4)\nThen under the GP model, the conditional density function of the output points given the input points is\np(~y1, . . . , ~yN , ~y|~x1, . . . , ~xN , ~x) = N ( ~\u03b81, . . . , ~\u03b8N , ~\u03b8,\n( K + \u03c3\u03032IN ~kT\n~k k(~x, ~x)\n)) (5)\nwhere IN is the N -dimensional identity matrix. Note that our distribution assumption is with respect to the whole training data set, not just with respect to single points.\nGiven this model, the posterior distribution just for the output ~y of the test input point ~x can be obtained by marginalization:\nTheorem 1 (Gaussian Process Posterior Distribution). Let Y be the matrix (~y1, . . . , ~yN ) T and \u0398 := (~\u03b81, . . . , ~\u03b8N ) T . Then the posterior density function for Gaussian process regression is given as:\np(~y|~x, ~x1, . . . , ~xN , ~y1, . . . , ~yN ) = N ( ~\u00b5, \u03c32 ) where (6)\n~\u00b5 = ~\u03b8T + ~k \u00b7 (K + \u03c3\u03032 \u00b7 IN )\u22121 \u00b7 (Y \u2212\u0398) (7) \u03c32 = k(~x, ~x)\u2212 ~k \u00b7 (K + \u03c3\u03032 \u00b7 IN )\u22121 \u00b7 ~kT (8)\nWe call ~\u00b5 the predictive mean and \u03c32 the predictive variance.\nProof. Refer e.g. to [44, p. 27].\nNote that the posterior distribution is, again, a Gaussian distribution. For such a distribution, the mean corresponds to the point of maximum probability density, such that we can define our predictive function as f(~x) := ~\u00b5T where ~\u00b5 is the predictive mean of the posterior distribution for point ~x. Further note that the predictive mean becomes the prior mean if ~k is the zero vector, i.e. if the test data point is not similar to any training data point.\nThe main drawback of GPs is their high computational complexity: For training, the inversion of the matrix (K + \u03c3\u03032 \u00b7 IN )\u22121 requires cubic time. We will address this problem below."}, {"heading": "3.3 Dissimilarities, Similarities and Kernels", "text": "Each of the methods described above relies either on a dissimilarity (in the case of 1-NN), or on a kernel (in the case of KR and GPR). Note that we have not strictly defined these concepts until now. Indeed, dissimilarities as well as similarities are inherently ill-defined concepts. A dissimilarity for the set X is any function of the form d : X \u00d7 X \u2192 R, such that d decreases if the two input arguments are in some sense more related to each other [43]. Conversely, a similarity for the set X is any function s : X \u00d7 X \u2192 R which increases if the two input arguments are in some sense more related to each other [43]. While in many cases, more rigorous criteria apply as well (such as non-negativity, symmetry, or that any element is most similar to itself) any or all of them may be violated in practice [51]. On the other hand, kernels are strictly defined as inner products. Thus, an important question for practical applications is: If we have only a dissimilarity or similarity available, how do we obtain a corresponding kernel? First, we observe that the conversion of a dissimilarity d to a similarity s is simple: it is sufficient to apply any monotonously decreasing function. In the context of this work, we will apply the popular radial basis function transformation\nsd(x, x \u2032) := exp ( \u22121\n2\nd(x, x\u2032)2\n\u03c82\n) (9)\nwhere \u03c8 is a positive real number called bandwidth. Further, for any finite set of data points x1, . . . , xN , we can transform a similarity into a kernel. Let S be a matrix of pairwise similarities for these data points with the entries Sii\u2032 = s(xi, xi\u2032). Such a similarity matrix S is a kernel matrix if and only if it is positive semi-definite [43]. Positive semidefiniteness can be enforced by applying an eigenvalue decomposition S = U\u039bV and removing the negative eigenvalues in the matrix \u039b, e.g. by setting them to zero or by taking their absolute value. The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51]. Note that the eigenvalue decomposition of a similarity matrix has cubic time-complexity in the number of data points. However, linear-time approximations have recently been discovered based on the Nystr\u00f6m method [22]. Therefore, in our further work, we can assume that a kernel can be obtained as needed."}, {"heading": "3.4 Prediction in Kernel and Dissimilarity Spaces", "text": "In introducing the three regression techniques above we have assumed that each data point is a vector, such that algebraic operations like scalar multiplication (for kernel regression), as well as matrix-vector multiplication and vector addition (for Gaussian process regression) are permitted. In the case of graphs, such operations are not well-defined. However, we can rely on prior work regarding\nvectorial embeddings of dissimilarities and similarities to define our regression in a latent space. In particular:\nTheorem 2 (Pseudo-Euclidean Embeddings [43]). For dissimilarities: Let x1, . . . , xN be some points from a set X , and let d be a symmetric and reflexive dissimilarity on X . Then, there exists a pseudo-Euclidean space Y and a function \u03c6 : X \u2192 Y such that for all i, i\u2032 \u2208 {1, . . . , N} it holds:\nd(xi, xi\u2032) = \u221a( \u03c6(xi)\u2212 \u03c6(xi\u2032) )T \u00b7 ( \u03c6(xi)\u2212 \u03c6(xi\u2032) ) (10)\nFor kernels: Let k be a kernel on X . Then there exists an Euclidean space Y and a function \u03c6 : X \u2192 Y such that for all i, i\u2032 \u2208 {1, . . . , N} it holds:\nk(xi, xi\u2032) = \u03c6(xi) T \u00b7 \u03c6(xi\u2032) (11)\nProof. Refer to [43].\nThe key idea of our approach to time series prediction for structured data (i.e. graphs) is to apply time series prediction in the implicit (pseudo-)Euclidean space Y, without any need for an explicit embedding in Y. This is obvious for 1- NN: Our prediction is just the successor to the closest training data point. This is a graph itself, providing us with a function mapping directly from graphs as inputs to graphs as outputs. The situation is less clear for KR or GPR. Here, our approach is to express the predictive output as a linear combination of known data points, which permits further processing as we will show later. In particular, we can proof that the resulting linear combinations are convex or at least affine. Linear, convex and affine combinations are defined as follows:\nDefinition 2 (Linear, Affine, and Convex Combinations). Let ~x1, . . . , ~xN be vectors from some vector space X and let \u03b11, . . . , \u03b1N be real numbers. The sum\u2211N i=1 \u03b1i \u00b7~xi is called a linear combination and the numbers \u03b1i linear coefficients.\nIf it holds \u2211N i=1 \u03b1i = 1 the linear combination is called affine. If it additionally holds that for all i \u03b1i \u2265 0 the affine combination is called convex.\nOur proof that the prediction provided by GPR is an affine combination requires two additional assumptions. First, we require a certain, natural prior, namely: In the abscence of any further knowledge, our best prediction for the point x is the identity, i.e. staying where we are. In this case, we obtain ~\u03b8 = ~x and ~\u03b8i = ~xi and the predictive mean\n~\u00b5 = ~xT + ~k \u00b7 (K + \u03c3\u03032 \u00b7 IN )\u22121 \u00b7 (Y \u2212X) (12)\nwhere X = (~x1, . . . , ~xN ) T . The predictive variance is still the same as in equation 8. We will assume this prior in the remainder of this paper. Further, we observe that our training inputs and outputs for time series prediction have a special form: Training data is presented in the form of sequences xj1, . . . , x j Tj for j \u2208 {1, . . . ,M} where xjt+1 is the successor of xjt . This implies\nthat each point that is a predecessor of another point is also a successor of another point (except for the first element in each sequence) and vice versa (except for the last element in each sequence).\nTheorem 3 (Predictive results as combinations). If training data is provided in terms of sequences xj1, . . . , x j Tj\nfor j \u2208 {1, . . . ,M} where xjt+1 is the successor of xjt , then it holds:\n1. The predictive result for 1-NN is a single training data point (i.e. a convex combination of only a single data point).\n2. The predictive result for KR is an affine combination of training data points.\n3. If the prior \u03b8 = x and \u03b8jt = x j t is assumed, the predictive result for GPR\nis an affine combination of training data points and the test data point, where the test data point has the coefficient 1.\nProof. 1. This follows directly from the form of the predictive function in equation 1\n2. The coefficients are given as\nk(x, xjt )\u2211M j\u2032=1 \u2211Tj\u2032\u22121 t\u2032=1 k(x, x j\u2032 t\u2032 ) (13)\nfor any point xjt+1 with j \u2208 {1, . . . ,M} and t \u2208 {1, . . . , Tj \u2212 1}. This combination is affine due to the normalization by \u2211M j\u2032=1 \u2211Tj\u2032\u22121 t\u2032=1 k(x, x j\u2032\nt\u2032 ). If the kernel is non-negative, the combination is convex.\n3. We define ~\u03b3 = (\u03b311 , . . . , \u03b31T1\u22121, . . . , \u03b3 M 1 , . . . , \u03b3 M TM\u22121) := ~k \u00b7 (K + \u03c3\u03032 \u00b7 IN )\u22121. The predictive mean is given as ~\u00b5 = ~x+ ~\u03b3 \u00b7 (Y \u2212X) which yields\n~\u00b5 = ~x+ M\u2211 j=1 Tj\u22121\u2211 t=1 \u03b3jt \u00b7 ( ~xjt+1 \u2212 ~xjt ) (14)\n= ~x+ M\u2211 j=1 \u2212\u03b3j1 \u00b7 ~xj1 + ( Tj\u22121\u2211 t=2 (\u03b3jt\u22121 \u2212 \u03b3jt ) \u00b7 ~xjt ) + \u03b3jTj\u22121 \u00b7 ~x j Tj\n(15)\nThus, the coefficients within each sequence are \u2212\u03b3j1, \u03b3j1 \u2212 \u03b3j2, . . ., \u03b3jTj\u22122 \u2212 \u03b3jTj\u22122, \u03b3 j Tj\u22121. These coefficients add up to zero. Finally, we have a coefficient of 1 for ~x, which is our prior for the prediction. Therefore, the overall sum of all coefficients is 1 and the combination is affine.\nIn effect, we can reformulate the predictive function as f : X \u2192 RN+1 mapping a test data point to the coefficients of an affine combination, which\nrepresent our actual predictive result in the (pseudo-)Euclidean space Y. Note that these coefficients do not provide us with a primal space representation of our data, i.e. we do not know what the graph which corresponds to a particular affine combination looks like. Indeed, as graph kernels are generally not injective [24], there might be multiple graphs that correspond to the same affine combination. Finding such a graph is called the kernel pre-image problem and is hard to solve even for vectorial data [32]. This poses a challenge with respect to further processing: How do we interpret a data point for which we have no explicit representation [29]?\nFortunately, we can still address many classical questions of data analysis for such a representation relying on our already existing implicit embedding in Y. We can simply extend this embedding for the predicted point via the affine combination as follows:\nTheorem 4 (Distances and Kernels for Affine Combinations). For dissimilarities: Let x1, . . . , xN , \u03c6 and d be as in theorem 2. Let D2 be the matrix of squared pairwise dissimilarities between the points x1, . . . , xN , and let ~\u03b1 be a 1\u00d7N vector of affine coefficients. Then it holds for any i \u2208 {1, . . . , N}: ( \u03c6(xi)\u2212\nN\u2211 i\u2032=1 \u03b1i\u2032\u03c6(xi\u2032) )T \u00b7 ( \u03c6(xi)\u2212 N\u2211 i\u2032=1 \u03b1i\u2032\u03c6(xi\u2032) ) = N\u2211 i\u2032=1 \u03b1i\u2032d(xi, xi\u2032) 2\u22121 2 ~\u03b1\u00b7D2\u00b7~\u03b1T\n(16) For kernels: Let x1, . . . , xN , \u03c6 and k be as in theorem 2 and let K be the matrix of pairwise kernel values between the points x1, . . . , xN . Let ~\u03b1 be a 1\u00d7N vector of linear coefficients. Then it holds for any i \u2208 {1, . . . , N}:\n\u03c6(xi) T \u00b7 (\nN\u2211 i\u2032=1 \u03b1i\u2032\u03c6(xi\u2032)\n) =\nN\u2211 i\u2032=1 \u03b1i\u2032k(xi, xi\u2032) (17)\nProof. Refer to [25] for a proof of 16. 17 follows by simple linear algebra.\nUsing this extended embedding, we can simply apply any dissimilarity- or kernel-based method on the predicted point as well. For dissimilarities, this includes relational learning vector quantization for classification [26] or relational neural gas for clustering [25]. For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17]. Therefore, we have achieved a full methodological pipeline for preprocessing, prediction and post-processing, which can be summarized as follows:\n1. If we intend to use a dissimilarity measure on graphs, we start off by computing the matrix of pairwise dissimilarities D on our training data. If required we symmetrize this matrix and set the diagonal to zero to ensure symmetry and reflexivity. Implicitly, this step embeds our training data in a pseudo-Euclidean space Y where D are pairwise Euclidean distances.\nWe transform this matrix into a similarity matrix S using, for example, the radial basis function transformation.\n2. If we intend to use a similarity measure on graphs, we start off by computing the matrix of pairwise similarities S on our training data. Otherwise we use the transformed matrix from the previous step.\n3. We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43]. Implicitly, this step embeds our data in an Euclidean space Y where our pairwise kernel values represent inner products of our training data.\n4. For any test data point x we compute the vector of kernel values ~k to all training data points.\n5. We apply 1-NN, KR or GPR as needed to infer a prediction for our test data point x in form of an affine coefficient vector ~\u03b1.\n6. We extend our dissimilarity matrix and/or kernel matrix for the predicted point via equations 16 and/or 17 respectively.\n7. We apply any further dissimilarity- or kernel-based method on the predicted point as desired.\nThe only challenge left to address is to speed up predictions in GPR to reduce the cubic time complexity to linear time complexity."}, {"heading": "3.5 Linear Time Predictions", "text": "GP regression involves the inversion of the matrix (K + \u03c3\u03032 \u00b7 IN ), resulting in O(N3) complexity. A variety of efficient approximation schemes exist [44]. Recently, the robust Bayesian Committee Machine (rBCM) has been introduced as a particularly fast and accurate approximation [15]. The rBCM approach is to distribute the examples into C disjoint sets, based e.g. on clustering in the input data space. For each of these sets, a separate GP regression is used, yielding the predictive distributions N (~\u00b5c, \u03c32c ) for c \u2208 {1, . . . , C}. These distributions are combined to the final predictive distribution N (~\u00b5rBCM, \u03c32rBCM) with\n\u03c3\u22122rBCM = C\u2211 c=1 \u03b2c \u03c32c + ( 1\u2212 C\u2211 c=1 \u03b2c ) \u00b7 1 \u03c32prior\n(18)\n\u00b5rBCM = \u03c3 2 rBCM \u00b7 ( C\u2211 c=1 \u03b2c \u03c32c \u00b7 ~\u00b5c + ( 1\u2212 C\u2211 c=1 \u03b2c ) \u00b7 1 \u03c32prior \u00b7 ~\u03b8 )\n(19)\nHere, \u03c32prior is the variance of the prior for the prediction, which is a new metaparameter introduced in the model. The weights \u03b2c can be seen as a measure for the predictive power of the single GP experts. As suggested by the authors, we use the differential entropy, given as \u03b2c = 12 \u00b7 ( log(\u03c32prior) \u2212 log(\u03c32c ) ) [15]. Note that ~\u03b8 = ~x in our case. This approach results in linear-time complexity\nif the size of any single cluster is considered to be constant (i.e. the number of clusters is proportional to N), such that only a matrix of constant size has to be inverted.\nTwo challenges remain to apply rBCM productively within our proposed pipeline. First, it remains to show that the predictive mean of rBCM still has the form of an affine combination. Second, we require a dissimilarity- or kernel-based clustering scheme which runs in linear time, as to ensure overall linear-time complexity. Within this work, we address the latter issue by applying relational neural gas only on a constant-sized subset of the data and extending the clustering to the full data set later on, resulting in overall linear time [25].\nRegarding the first issue we show:\nTheorem 5 (rBCM prediction as affine combination). If training data is provided in terms of sequences xj1, . . . , x j Tj\nfor j \u2208 {1, . . . ,M} where xjt+1 is the successor of xjt , the predictive result of an rBCM is an affine combination of training data points and the test data point.\nProof. We define \u03b1c := \u03b2c\u03c32c and \u03b1prior := ( 1 \u2212\u2211Cc=1 \u03b2c) \u00b7 1\u03c32prior . For every cluster, the respective predictive mean has the shape ~\u00b5c = ~x + \u2211N i=1 \u03b3 c i \u00b7 ~xi\nwhere \u2211N i=1 \u03b3 c i = 0 (as demonstrated in the proof to theorem 3). So the overall coefficient assigned to ~x is\n\u03c32rBCM \u00b7 ( C\u2211 c=1 \u03b1c + \u03b1prior ) = 1 (20)\nand all other coefficients add up to\nC\u2211 c=1 \u03c32rBCM \u00b7 \u03b1c \u00b7 ( N\u2211 i=1 \u03b3ci ) = C\u2211 c=1 \u03c32rBCM \u00b7 \u03b1c \u00b7 0 = 0 (21)\nTherefore, we obtain an affine combination."}, {"heading": "4 Experiments", "text": "In our experimental evaluation, we apply the pipeline introduced in the previous section to four data sets, two of which are theoretical models and two of which are real-world data sets of Java programs. In all cases, we evaluate the root mean square error (RMSE) of the prediction for each method in a leave-oneout-crossvalidation over the sequences in our data set. We denote the current test trajectory as x\u20321, . . . , x\u2032T , the training trajectories as {xj1, . . . , xjTj}j=1,...,M , the predicted affine coefficients for point x\u2032t\u2032 as ~\u03b1t\u2032 = (\u03b1 1 t\u2032,1, . . . , \u03b1 M t\u2032,TM\n, \u03b1\u2032t\u2032) and the matrix of squared pairwise dissimilarities (including the test data points)\nas D2. Accordingly, the RMSE for each fold has the following form (refer to equation 16).\nE = \u221a\u221a\u221a\u221a 1 T \u2212 1 T\u22121\u2211 t\u2032=1 M\u2211 j=1 Tj\u2211 t=1 \u03b1jt\u2032,td(x j t , x \u2032 t\u2032+1) 2 + \u03b1\u2032t\u2032d(x \u2032 t\u2032 , x \u2032 t\u2032+1) 2 \u2212 1 2 ~\u03b1Tt\u2032D 2~\u03b1t\u2032\n(22) We evaluate our four regression models, namely 1-NN, KR, GPR and rBCM, as well as a simple baseline, namely the identity function, i.e. we predict the current point as next point. We optimized the hyper parameters for all methods (i.e. the RBF bandwidth \u03c8 and the noise standard deviation \u03c3\u0303 for GPR and rBCM) using a random search with 10 random trials. In each trial, we evaluated the RMSE in a nested leave-one-out-crossvalidation over the training sequences and chose the parameters which lead to the lowest RMSE. Let d\u0304 be the average dissimilarity over the data set. We drew \u03c8 from a uniform distribution in the range [0.05 \u00b7 d\u0304, d\u0304] for the theoretical data set and fixed it to 0.3 \u00b7 d\u0304 for the java data sets to avoid the need for a new eigenvalue correction in each random trial. We drew \u03c3\u0303 from an exponential distribution in the range [10\u22123 \u00b7 d\u0304, d\u0304] for the theoretical and [10\u22122 \u00b7 d\u0304, d\u0304] for the Java data sets. We fixed the prior standard deviation \u03c3prior = d\u0304 for all data sets. We set the number of clusters for rBCM to \u230a N 100 \u230b for all data sets.\nOur experimental hypotheses are that all prediction methods should yield lower RMSE compared to the baseline (H1), that rBCM should outperform 1- NN and KR (H2) and that rBCM should be not significantly worse compared to GPR (H3). To evaluate significance we use a Wilcoxon signed rank sum test."}, {"heading": "4.1 Theoretical Data Sets", "text": "We investigate the following theoretical data sets:\nBarab\u00e1si-Albert model: This is a simple stochastic model of graph growth in undirected graphs [4]. Starting with a fully connected initial graph of m0 nodes, nodes are added one by one. In each iteration, the newly added node is connected to k of the existing nodes which are randomly selected with the probability P (u) = degt(u)/( \u2211 v degt(v)) where degt is the node degree at time\nt, i.e. degt(v) = \u2211 u \u03c1((u, v), t). It has been shown that the edge distribution resulting from this growth model is scale-free, more specifically the probability of a certain degree k is P (k) = m \u00b7k\u22123, where m is the number of nodes [4]. Our data set consists of 20 graphs with m = 27 nodes each, grown from an initial graph of size m0 = 3 and k = 2 new edges per node. This resulted in 500 graphs overall.\nConway\u2019s Game of Life: John Conway\u2019s Game of Life [20] is a simple, 2-dimensional cellular automaton model. Nodes are ordered in a regular, 2- dimensional grid and connected to their eight neighbors in the grid. Let N (v) denote this eight- neighborhood in the grid. Then we can describe Conway\u2019s\nGame of Life with the following equations for the node presence function \u03c8 and the edge presence function \u03c1 respectively:\n\u03c8(v, t) = { 1 if 5 \u2264 \u03c8(v, t\u2212 1) + 2 \u00b7\u2211u\u2208N (v) \u03c8(u, t\u2212 1) \u2264 7 0 otherwise\n(23)\n\u03c1((u, v), t) = { 1 if \u03c8(u, t) = 1 \u2227 \u03c8(v, t) = 1 0 otherwise\n(24)\nNote that Conway\u2019s Game of Life is turing-complete and its evolution is, in general, unpredictable without computing every single step according to the rules [1]. We created 30 trajectories by initializing a 20 \u00d7 20 grid with one of six standard patterns at a random position, namely blinker, beacon, toad, block, glider, and block and glider (see figure 2). The first four patterns are simple oscillators with a period of two, the glider is an infinitely moving structure with a period of two (up to rotation) and the block and glider is a chaotic structure which converges to a block of four and a glider after 105 steps 1 We let the system run for T = 10 time steps resulting in 300 graphs overall. In every step, we further activated 5% of the cells at random, simulating observational noise.\nAs data representation for the theoretical data set we use an explicit feature embedding inspired by the shortest-path-kernel of Borgwardt and colleagues [10]. Using the Floyd-Warshall algorithm [18] we compute all shortest paths in the graph and then compute a histogram over the lengths of these shortest paths as a feature map (see figure 3 for an example). As dissimilarity we use the simple Euclidean distance on these features, which we normalize by the average distance over the data set. We transformed the distance to a kernel via the radial basis function transformation. Eigenvalue correction was not required.\n1Also see the Life Wiki http://conwaylife.com/wiki/ for more information on the patterns.\npublic static boolean palindromic(String sentence) {\nfor (String word : sentence.split(\" \")) { if (!word.equals ((new StringBuilder(word\n)).reverse ().toString ())) { return false;\n} } return true;\n}\npublic static int[] insertionSort(int[] A) { for (int i = 1; i < A.length; i++) {\nint a = A[i]; int j; for (j = i - 1; j >= 0 && A[j] > a; j--)\n{ A[j + 1] = A[j];\n} A[j + 1] = a;\n} return A;\n}\nFigure 4: Example java code from the MiniPalindrome data set (left) and from the Sorting data set (right).\nThe RMSE and runtimes for all three data sets are shown in table 1. As expected, KR, GPR and rBCM outperform the identity-baseline (p < 10\u22123 for both data sets), supporting H1. 1-NN outperforms the baseline only in the Barab\u00e1si-Albert data set (p < 10\u22123). Also, our results lend support to H2 as rBCM outperforms 1-NN in both data sets (p < 0.05 for Barab\u00e1si-Albert, and p < 0.01 for Conway\u2019s Game of Life). However, rBCM is significantly better than KR only for the Barab\u00e1si-Albert data set (p < 0.001), indicating that for simple data sets such as our theoretical ones, KR might already provide a sufficient predictive quality. Finally, we do not observe a significant difference between rBCM and GPR, as expected in H3. Interestingly, for these data sets, rBCM is slower compared to GP, which is probably due to constant overhead for maintaining multiple models."}, {"heading": "4.2 Java Programs", "text": "Our two real-world java datasets consist of programs for two different problems from beginner\u2019s programming courses. The motivation for time series prediction on such data is to help students in achieving a correct solution in an intelligent tutoring system (ITS). In such an ITS, students incrementally work on their program and at some point, they might not know how to proceed. Then, we would like to predict the most likely next state of their program, given the trajectories of other students who have already correctly solved the problem. This scheme is inspired by prior work in intelligent tutoring systems, such as the hint factory by Barnes and colleagues [5].\nAs our data sets consist only of final, working versions of programs, we have to simulate the incremental growth. To that end we first represented the programs as graphs via their abstract syntax tree, and then recursively removed the last semantically important node (where we regarded nodes as important which introduce a visibility scope in the Java program, such as class declarations, method declarations, and loops), until the program was empty. Then, we reversed the order of the resulting sequence, as to achieve a growing program. In detail, we have the following two data sets:\nMiniPalindrome: This dataset consists of 48 Java programs, each realizing one of eight different strategies to recognize palindromic input (see figure 4) [37] 2. The abstract syntax trees of these programs contain 135 nodes on average. The programs come in eight different variations described in [37]. Our simulation resulted in 834 data points.\nSorting: This is a benchmark dataset of 64 Java sorting programs taken from the web, implementing one of two sorting algorithms, namely BubbleSort or InsertionSort (see figure 4) [38] 3. The abstract syntax trees contain 94 nodes on average. Our simulation resulted in 800 data points.\nTo achieve a dissimilarity representation we first ordered the nodes of the abstract syntax trees in order of their appearance in the original program code. Then, we computed a sequence alignment distance on the resulting node sequenes, similar to the method described by Robles-Kelly and Hancock [47]. In particular, we used an affine sequence alignment with learned node dissimilarity as suggested in [41]. We transformed the dissimilarity so a similarity via the radial basis function transformation and obtained a kernel via clip Eigenvalue correction [22].\nWe show the RMSEs and runtimes for both data sets in table 2. Contrary to the theoretical data sets before, we observe strong differences both between the predictive models and the baseline, as well as between rBCM and 1-NN as well as KR (p < 0.01 in all cases), which supports both H1 and H2. Interestingly, rBCM appears to achieve better results compared to GPR, which might be the case due to additional smoothing provided by the averaging operation over all cluster-wise GPR results. This result supports H3. Also, we observe that rBCM is about 10 times faster compared to GPR.\n2The data set is available online under http://doi.org/10.4119/unibi/2900666 3The data set is available online under http://doi.org/10.4119/unibi/2900684"}, {"heading": "5 Discussion and Conclusion", "text": "Our results indicate that it is possible to achieve time series prediction in kernel and dissimilarity spaces, in particular for graphs. In all our experiments, even simple predictive models (1-nearest neighbor and kernel regression) did outperform the baseline of staying where you are. For real-world data we could further improve the predictive error significantly by applying a more complex predictive model, namely the robust Bayesian Committee Machine (rBCM). This indicates a trade-off in terms of model choice: Simpler models are faster and in the case of 1-nearest neighbor the result is easier to inerpret. However, in the case of real-world data, it is likely that a more complex predictive model is required to acurrately describe the underlying dynamics in the kernel space. Fortunately, the runtime overhead is only a constant factor as rBCM can be applied in linear time.\nOur key idea is to apply time series prediction in a (pseudo-)Euclidean space and representing the graph output as a point in this space. We have shown that this point can be analyzed using subsequent dissimilarity- or kernel-based methods as the dissimilarities and kernel values with respect to the predicted point can still be calculated.\nHowever, two problems remain open: First, usual hyperparameter optimization techniques depend on a vectorial data representation [15] and one has to adapt them for a relational case. Second, an affine combination might not be a sufficient data representation of the predicted point for some applications, for example for feedback provision in intelligent tutoring systems. For such cases, an inverse problem has to be solved: Finding the original point that maps to the affine combination in the pseudo-Euclidean space. Both problems pose interesting challenges for further research in the field."}], "references": [{"title": "Collision-Based Computing", "author": ["A. Adamatzky"], "venue": "Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A review on applications of ANN and SVM for building electrical energy consumption forecasting", "author": ["A. Ahmad", "M. Hassan", "M. Abdullah", "H. Rahman", "F. Hussin", "H. Abdullah", "R. Saidur"], "venue": "Renewable and Sustainable Energy Reviews, 33:102\u2013109", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An efficient topological distance-based tree kernel", "author": ["F. Aiolli", "G.D.S. Martino", "A. Sperduti"], "venue": "IEEE Trans. Neural Netw. Learning Syst., 26(5):1115\u20131120", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of scaling in random networks", "author": ["A.-L. Barab\u00e1si", "R. Albert"], "venue": "Science, 286(5439):509\u2013512", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Toward automatic hint generation for logic proof tutoring using historical student data", "author": ["T. Barnes", "J. Stamper"], "venue": "B. P. Woolf, E. A\u00efmeur, R. Nkambou, and S. Lajoie, editors, Intelligent Tutoring Systems, volume  Preprint as provided by the authors.  22 5091 of Lecture Notes in Computer Science, pages 373\u2013382. Springer Berlin Heidelberg", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Elements of a theory of simulation II: sequential dynamical systems", "author": ["C.L. Barrett", "H. Mortveit", "C.M. Reidys"], "venue": "Applied Mathematics and Computation, 107(2-3):121\u2013136", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "ETS IV: Sequential dynamical systems: fixed points", "author": ["C.L. Barrett", "H.S. Mortveit", "C.M. Reidys"], "venue": "invertibility and equivalence. Applied Mathematics and Computation, 134(1):153\u2013171", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Elements of a theory of computer simulation I: Sequential ca over random graphs", "author": ["C.L. Barrett", "C.M. Reidys"], "venue": "Applied Mathematics and Computation, 98(2-3):241\u2013259", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "A Survey on Metric Learning for Feature Vectors and Structured Data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "ArXiv e-prints, ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1306}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.P. Kriegel"], "venue": "In Fifth IEEE International Conference on Data Mining (ICDM\u201905),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Timevarying graphs and dynamic networks", "author": ["A. Casteigts", "P. Flocchini", "W. Quattrociocchi", "N. Santoro"], "venue": "International Journal of Parallel, Emergent and Distributed Systems, 27(5):387\u2013408", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative models for complex network structure", "author": ["A. Clauset"], "venue": "NetSci 2013 - Complex Networks meets Machine Learning", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Mining structured data", "author": ["G. Da San Martino", "A. Sperduti"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Distributed gaussian processes", "author": ["M.P. Deisenroth", "J.W. Ng"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 1481\u20131490", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["A. Feragen", "N. Kasenburg", "J. Petersen"], "venue": "de Bruijne, and K. Borgwardt. Scalable kernels for graphs with continuous attributes. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 216\u2013224. Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, 41(1):176\u2013190", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey of graph edit distance", "author": ["X. Gao", "B. Xiao", "D. Tao", "X. Li"], "venue": "Pattern Analysis and Applications, 13(1):113\u2013129", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Mathematical games \u2013 the fantastic combinations of John Conway\u2019s new solitaire game \u201clife", "author": ["M. Gardner"], "venue": "Scientific American, 223:120\u2013123", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1970}, {"title": "Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting", "author": ["A. Girard", "C.E. Rasmussen", "J.Q. Candela", "R. Murray-Smith"], "venue": "Advances in neural information processing systems, pages 545\u2013552", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Metric and non-metric proximity transformations at linear costs", "author": ["A. Gisbrecht", "F.-M. Schleif"], "venue": "Neurocomputing, 167:643\u2013657", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of statistical network models", "author": ["A. Goldenberg", "A.X. Zheng", "S.E. Fienberg", "E.M. Airoldi"], "venue": "Foundations and Trends in Machine Learning, 2(2):129\u2013233", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P. Flach", "S. Wrobel"], "venue": "CONFERENCE ON LEARNING THEORY, pages 129\u2013143", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Topographic mapping of large dissimilarity data sets", "author": ["B. Hammer", "A. Hasenfuss"], "venue": "Neural Computation, 22(9):2229\u20132284", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning vector quantization for (dis-)similarities", "author": ["B. Hammer", "D. Hofmann", "F.-M. Schleif", "X. Zhu"], "venue": "NeuroComputing, 131:43\u201351", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent space approaches to social network analysis", "author": ["P.D. Hoff", "A.E. Raftery", "M.S. Handcock"], "venue": "Journal of the American Statistical Association, 97(460):1090\u20131098", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel Robust Soft Learning Vector Quantization", "author": ["D. Hofmann", "B. Hammer"], "venue": "pages 14\u201323. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning interpretable kernelized prototype-based models", "author": ["D. Hofmann", "F.-M. Schleif", "B. Paa\u00dfen", "B. Hammer"], "venue": "Neurocomputing, 141:84\u201396", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic block models: First steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social Networks, 5:109\u2013137", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1983}, {"title": "New potentials for data-driven intelligent tutoring system development and optimization", "author": ["K.R. Koedinger", "E. Brunskill", "R.S. Baker", "E.A. McLaughlin", "J. Stamper"], "venue": "AI Magazine, 34(3):27\u201341", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "The pre-image problem in kernel methods", "author": ["J.-Y. Kwok", "I. Tsang"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Binary codes capable of correcting deletions", "author": ["V.I. Levenshtein"], "venue": "insertions, and reversals. Soviet Physics Doklady, 10(8):707\u2013710", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1965}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American Society for Information Science and Technology, 58(7):1019\u20131031", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "New perspectives and methods in link prediction", "author": ["R.N. Lichtenwalter", "J.T. Lussier", "N.V. Chawla"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201910, pages 243\u2013252, New York, NY, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Link prediction in complex networks: A survey", "author": ["L. L\u00fc", "T. Zhou"], "venue": "Physica A: Statistical Mechanics and its Applications, 390(6):1150 \u2013 1170", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Domainindependent proximity measures in intelligent tutoring systems", "author": ["B. Mokbel", "S. Gross", "B. Paa\u00dfen", "N. Pinkwart", "B. Hammer"], "venue": "Proceedings of the 6th International Conference on Educational Data Mining (EDM), pages 334\u2013335", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning for sequences in relational lvq", "author": ["B. Mokbel", "B. Paa\u00dfen", "F.-M. Schleif", "B. Hammer"], "venue": "Neurocomputing, 169:306\u2013322", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability & Its Applications, 9(1):141\u2013142", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1964}, {"title": "Gaussian process prediction for time series of structured data", "author": ["B. Paa\u00dfen", "C. G\u00f6pfert", "B. Hammer"], "venue": "M. Verleysen, editor, 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN), pages 41\u201346. i6doc.com", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive structure metrics for automated feedback provision in intelligent tutoring systems", "author": ["B. Paa\u00dfen", "B. Mokbel", "B. Hammer"], "venue": "Neurocomputing, pages 3\u201313", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic modeling", "author": ["M. Papageorgiou"], "venue": "assignment, and route guidance in traffic networks. Transportation Research Part B: Methodological, 24(6):471\u2013495", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1990}, {"title": "The dissimilarity representation for pattern recognition: foundations and applications", "author": ["E. P\u0119kalska"], "venue": "PhD thesis, Delft University of Technology", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Gaussian processes for time-series modelling", "author": ["S. Roberts", "M. Osborne", "M. Ebden", "S. Reece", "N. Gibson", "S. Aigrain"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 371", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1984}, {"title": "Edit distance from graph spectra", "author": ["A. Robles-Kelly", "E.R. Hancock"], "venue": "In Proceedings Ninth IEEE International Conference on Computer Vision,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2003}, {"title": "Graph edit distance from spectral seriation", "author": ["A. Robles-Kelly", "E.R. Hancock"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(3):365\u2013378", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "A distance measure between attributed relational graphs for pattern recognition", "author": ["A. Sanfeliu", "K.S. Fu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1983}, {"title": "Time series prediction using support vector machines: A survey", "author": ["N.I. Sapankevych", "R. Sankar"], "venue": "IEEE Computational Intelligence Magazine, 4(2):24\u201338", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Description and simulation of dynamic mobility networks", "author": ["A. Scherrer", "P. Borgnat", "E. Fleury", "J.-L. Guillaume", "C. Robardet"], "venue": "Computer Networks, 52(15):2842 \u2013 2858", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning in indefinite proximity spaces - recent trends", "author": ["F.-M. Schleif", "P. Tino", "Y. Liang"], "venue": "M. Verleysen, editor, 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN), pages 113\u2013122. i6doc.com", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Time series analysis and its applications", "author": ["R.H. Shumway", "D.S. Stoffer"], "venue": "Springer Science & Business Media", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian process dynamical models", "author": ["J. Wang", "A. Hertzmann", "D.M. Blei"], "venue": "Advances in neural information processing systems, pages 1441\u2013 1448", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep graph kernels", "author": ["P. Yanardag", "S. Vishwanathan"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1365\u20131374, New York, NY, USA", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating link prediction methods", "author": ["Y. Yang", "R.N. Lichtenwalter", "N.V. Chawla"], "venue": "Knowledge and Information Systems, 45(3):751\u2013782", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing stars: On approximating graph edit distance", "author": ["Z. Zeng", "A.K.H. Tung", "J. Wang", "J. Feng", "L. Zhou"], "venue": "Proc. VLDB Endow., 2(1):25\u201336", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Simple fast algorithms for the editing distance between trees and related problems", "author": ["K. Zhang", "D. Shasha"], "venue": "SIAM Journal on Computing, 18(6):1245\u20131262", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 40, "context": "to model traffic connections between cities [42], data lines between \u2217Funding by the DFG under grant number HA 2719/6-2 and the CITEC center of excellence (EXC 277) is gratefully acknowledged.", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "\u2020This contribution is an extension of the work presented at ESANN 2016 under the title \u201cGaussian process prediction for time series of structured data\u201d [40].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 70, "endOffset": 74}, {"referenceID": 35, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 170, "endOffset": 178}, {"referenceID": 39, "context": "computing nodes [11], communication between people in social networks [34], or the structure of a student\u2019s solution to a learning task in an intelligent tutoring system [37, 41].", "startOffset": 170, "endOffset": 178}, {"referenceID": 40, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 371, "endOffset": 375}, {"referenceID": 32, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 529, "endOffset": 533}, {"referenceID": 35, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 649, "endOffset": 657}, {"referenceID": 29, "context": "For example, in traffic graphs, the traffic load changes significantly over the course of a day, making optimal routing a timedependent problem [42]; in distributed computing, the distribution of computing load and communication between machines crucially depends on the availability and speed of connections and the current load of the machines, which changes with time [11]; in social networks or communication networks new users may enter the network, old users may leave and the interactions between users may change rapidly [34]; and in intelligent tutoring systems, students change their solution over time to get closer to a correct solution [37, 31].", "startOffset": 649, "endOffset": 657}, {"referenceID": 47, "context": "Traditionally, predicting the future development based on knowledge of the past is the topic of time series prediction, which has wide-ranging applications in physics, sociology, medicine, engineering, finance and other fields [49, 52].", "startOffset": 227, "endOffset": 235}, {"referenceID": 50, "context": "Traditionally, predicting the future development based on knowledge of the past is the topic of time series prediction, which has wide-ranging applications in physics, sociology, medicine, engineering, finance and other fields [49, 52].", "startOffset": 227, "endOffset": 235}, {"referenceID": 50, "context": "However, classic models in time series prediction, such as ARIMA, NARX, Kalman filters, recurrent networks or reservoir models focus on vectorial data representations, and they are not equipped to handle time series of graphs [52].", "startOffset": 226, "endOffset": 230}, {"referenceID": 1, "context": "predicting the overall load in an energy network [2] or predicting the appearance of single edges in a social network [34].", "startOffset": 49, "endOffset": 52}, {"referenceID": 32, "context": "predicting the overall load in an energy network [2] or predicting the appearance of single edges in a social network [34].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 9, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 13, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 15, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 39, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 52, "context": "Our approach has two key steps: First, we represent graphs via pairwise kernel values, which are well-researched in the scientific literature [3, 10, 14, 16, 41, 54].", "startOffset": 142, "endOffset": 165}, {"referenceID": 37, "context": "Second, within this space, we can apply similarity- and kernel-based regression methods, such as nearest neighbor regression, kernel regression [39] or Gaussian processes [44] to predict the next position in the kernel space given the current position.", "startOffset": 144, "endOffset": 148}, {"referenceID": 42, "context": "Second, within this space, we can apply similarity- and kernel-based regression methods, such as nearest neighbor regression, kernel regression [39] or Gaussian processes [44] to predict the next position in the kernel space given the current position.", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "Fortunately, Deisenroth and Ng have suggested a simple strategy to permit predictions in linear time, namely distributing the prediction to multiple Gaussian processes, each of which handles only a constant-sized subset of the data [15].", "startOffset": 232, "endOffset": 236}, {"referenceID": 20, "context": "can be avoided using the well-known Nystr\u00f6m approximation as investigated by [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 157, "endOffset": 160}, {"referenceID": 18, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 187, "endOffset": 191}, {"referenceID": 35, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 308, "endOffset": 316}, {"referenceID": 36, "context": "Finally, we evaluate our approach empirically on four data sets: two well-established theoretical models of graph dynamics, namely the Barabasi-Albert model [4] and Conway\u2019s Game of Life [20], as well as two real-world data sets of Java programs, where we try to predict the next step in program development [37, 38].", "startOffset": 308, "endOffset": 316}, {"referenceID": 40, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 170, "endOffset": 178}, {"referenceID": 35, "context": "Dynamically changing graphs are relevant in many different fields, such as traffic [42], distributed computing [11], social networks [34] or intelligent tutoring systems [31, 37].", "startOffset": 170, "endOffset": 178}, {"referenceID": 10, "context": "We begin with two formalisms to model dynamics in graphs, namely time-varying graphs [11], and sequential dynamical systems [6].", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "We begin with two formalisms to model dynamics in graphs, namely time-varying graphs [11], and sequential dynamical systems [6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 32, "context": "This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 53, "context": "This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 21, "context": "This has mainly been addressed in the domain of social networks under the umbrella of link prediction [34, 55], as well as in models of graph growth [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "Time-Varying Graphs: Time-varying graphs have been introduced by Casteigts and colleagues in an effort to integrate different notations found in the fields of delay-tolerant networks, opportunistic-mobility networks or social networks [11].", "startOffset": 235, "endOffset": 239}, {"referenceID": 10, "context": "Rather, dynamics have to be regarded as an \u201cintegral part of the nature of the system\u201d [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Definition 1 (Time-Varying Graph [11]).", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": ",GtK [11, 50].", "startOffset": 5, "endOffset": 13}, {"referenceID": 48, "context": ",GtK [11, 50].", "startOffset": 5, "endOffset": 13}, {"referenceID": 5, "context": "Sequential Dynamical Systems: Sequential dynamical systems (SDS) have been introduced by Barret, Reidys and Mortvart to generalize cellular automata to arbitrary graphical structures [6, 8].", "startOffset": 183, "endOffset": 189}, {"referenceID": 7, "context": "Sequential Dynamical Systems: Sequential dynamical systems (SDS) have been introduced by Barret, Reidys and Mortvart to generalize cellular automata to arbitrary graphical structures [6, 8].", "startOffset": 183, "endOffset": 189}, {"referenceID": 5, "context": "This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8].", "startOffset": 94, "endOffset": 103}, {"referenceID": 6, "context": "This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8].", "startOffset": 94, "endOffset": 103}, {"referenceID": 7, "context": "This induces a discrete dynamical system on graphs (where edges and neighborhoods stay fixed) [6, 7, 8].", "startOffset": 94, "endOffset": 103}, {"referenceID": 32, "context": "Link Prediction: In the realm of social network analysis, [34] have formulated the link prediction problem, which can be stated as: Given a sequence of temporal subgraphs G0, .", "startOffset": 58, "endOffset": 62}, {"referenceID": 53, "context": "for which edges do we find \u03c1(e, t) = 0 but \u03c1(e, t + 1) = 1 [55]? For example, given all past collaborations in a scientific community, can we predict new collaborations in the future? The simplest approaches to address this challenge derive a similarity index between nodes, such as the number of common neighbors, rank all non-existing edges", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "according to the similarity index of their nodes and predict all edges which are above a certain threshold [34, 35].", "startOffset": 107, "endOffset": 115}, {"referenceID": 33, "context": "according to the similarity index of their nodes and predict all edges which are above a certain threshold [34, 35].", "startOffset": 107, "endOffset": 115}, {"referenceID": 33, "context": "However, more complex models exist as well, such as supervised models [35], and probabilistic as well as stochastic models [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "However, more complex models exist as well, such as supervised models [35], and probabilistic as well as stochastic models [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Growth models: In a seminal paper, Barab\u00e1si and Albert described a simple model to incrementally grow an undirected graph node by node from a small, fully connected seed graph [4].", "startOffset": 176, "endOffset": 179}, {"referenceID": 11, "context": "Since then, many other models of graph growth have emerged, most notably stochastic block models and latent space models [12, 23].", "startOffset": 121, "endOffset": 129}, {"referenceID": 21, "context": "Since then, many other models of graph growth have emerged, most notably stochastic block models and latent space models [12, 23].", "startOffset": 121, "endOffset": 129}, {"referenceID": 28, "context": "Stochastic block models assign each node to a block and model the probability of an edge between two nodes only dependent on their respective blocks [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Latent space models embed all nodes in an underlying, latent space and model the probability of an edge depending on the distance in this space [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 46, "context": "Permitted edit operations include node insertion, node deletion, edge insertion, edge deletion, and the substitution of labels in nodes or edges [48].", "startOffset": 145, "endOffset": 149}, {"referenceID": 31, "context": "This problem is a generalization of the classic string or tree edit distance, which is defined as the minimum number of operations required to transform a string into another or a tree into another respectively [33, 57].", "startOffset": 211, "endOffset": 219}, {"referenceID": 55, "context": "This problem is a generalization of the classic string or tree edit distance, which is defined as the minimum number of operations required to transform a string into another or a tree into another respectively [33, 57].", "startOffset": 211, "endOffset": 219}, {"referenceID": 54, "context": "Unfortunately, while the string edit distance and the tree edit distance can be efficiently computed in O(n2) and O(n4) respectively, computing the exact graph edit distance is NP-hard [56].", "startOffset": 185, "endOffset": 189}, {"referenceID": 17, "context": "relying on self-organizing maps, Gaussian mixture models, graph kernels or binary linear programming [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 39, "context": "to [41, 46, 47]).", "startOffset": 3, "endOffset": 15}, {"referenceID": 44, "context": "to [41, 46, 47]).", "startOffset": 3, "endOffset": 15}, {"referenceID": 45, "context": "to [41, 46, 47]).", "startOffset": 3, "endOffset": 15}, {"referenceID": 39, "context": "For our experiments on real-world java data we will rely on a kernel over such an approximated graph distance as suggested in [41].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Learning such edit costs from data is the topic of structure metric learning, which has mainly been investigated for string edit distances [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 39, "context": "However, if string edit distance is applied as a substitute for graph edits, the results apply to graphs as well [41].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The only formal requirement on a graph kernel is that it implicitly or explicitly maps a graph G to a vectorial feature representation \u03c6(G) and computes the pairwise kernel values of two graphs G and G\u2032 as the dot-product of their feature vectors k(G,G\u2032) = \u03c6(G) \u00b7 \u03c6(G\u2032) [14].", "startOffset": 270, "endOffset": 274}, {"referenceID": 22, "context": "If each graph has a unique representation (that is, \u03c6 is injective) computing such a kernel is at least as hard as the graph isomorphy problem [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Thus, efficient graph kernels rely on a non-injective feature embedding, which is still expressive enough to capture important differences between graphs [14].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16].", "startOffset": 169, "endOffset": 181}, {"referenceID": 9, "context": "A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16].", "startOffset": 169, "endOffset": 181}, {"referenceID": 15, "context": "A particularly popular class of graph kernels are walk kernels which decompose the kernel between two graphs into kernels between paths which can be taken in the graphs [14, 10, 16].", "startOffset": 169, "endOffset": 181}, {"referenceID": 9, "context": "In our experiments, we will apply the shortest-path kernel suggested by Borgwardt and colleagues, which compares the lengths of shortest paths in both graphs to construct an overall graph kernel [10].", "startOffset": 195, "endOffset": 199}, {"referenceID": 47, "context": "One popular example is the use of support vector regression with wide-ranging applications in finance, business, environmental research and engineering [49].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45].", "startOffset": 69, "endOffset": 73}, {"referenceID": 51, "context": "Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45].", "startOffset": 87, "endOffset": 91}, {"referenceID": 43, "context": "Another example are gaussian processes to predict chemical processes [21], motion data [53] and physics data [45].", "startOffset": 109, "endOffset": 113}, {"referenceID": 47, "context": "We first transform this problem into a regression problem as suggested by Sapankecych and colleagues, that is, we try to learn a function f which maps the past K states of a time series to a successor state [49].", "startOffset": 207, "endOffset": 211}, {"referenceID": 37, "context": "Kernel Regression (KR): Kernel regression was first proposed by Nadaraya and Watson and can be seen as a generalization of 1-nearest neighbor to a smooth predictive function f using a kernel k instead of a dissimilarity [39].", "startOffset": 220, "endOffset": 224}, {"referenceID": 42, "context": "Gaussian Process Regression (GPR): In Gaussian process regression (GPR) we assume that the output points (training as well as test) are a realization of a multivariate random variable with a Gaussian distribution [44].", "startOffset": 213, "endOffset": 217}, {"referenceID": 41, "context": "A dissimilarity for the set X is any function of the form d : X \u00d7 X \u2192 R, such that d decreases if the two input arguments are in some sense more related to each other [43].", "startOffset": 167, "endOffset": 171}, {"referenceID": 41, "context": "Conversely, a similarity for the set X is any function s : X \u00d7 X \u2192 R which increases if the two input arguments are in some sense more related to each other [43].", "startOffset": 157, "endOffset": 161}, {"referenceID": 49, "context": "While in many cases, more rigorous criteria apply as well (such as non-negativity, symmetry, or that any element is most similar to itself) any or all of them may be violated in practice [51].", "startOffset": 187, "endOffset": 191}, {"referenceID": 41, "context": "Such a similarity matrix S is a kernel matrix if and only if it is positive semi-definite [43].", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51].", "startOffset": 79, "endOffset": 91}, {"referenceID": 41, "context": "The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51].", "startOffset": 79, "endOffset": 91}, {"referenceID": 49, "context": "The resulting matrix \u039b\u0303 can then be applied to obtain a kernel matrix K = U\u039b\u0303V [22, 43, 51].", "startOffset": 79, "endOffset": 91}, {"referenceID": 20, "context": "However, linear-time approximations have recently been discovered based on the Nystr\u00f6m method [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 41, "context": "Theorem 2 (Pseudo-Euclidean Embeddings [43]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "Refer to [43].", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "Indeed, as graph kernels are generally not injective [24], there might be multiple graphs that correspond to the same affine combination.", "startOffset": 53, "endOffset": 57}, {"referenceID": 30, "context": "Finding such a graph is called the kernel pre-image problem and is hard to solve even for vectorial data [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "This poses a challenge with respect to further processing: How do we interpret a data point for which we have no explicit representation [29]? Fortunately, we can still address many classical questions of data analysis for such a representation relying on our already existing implicit embedding in Y.", "startOffset": 137, "endOffset": 141}, {"referenceID": 23, "context": "Refer to [25] for a proof of 16.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "For dissimilarities, this includes relational learning vector quantization for classification [26] or relational neural gas for clustering [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "For dissimilarities, this includes relational learning vector quantization for classification [26] or relational neural gas for clustering [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 16, "context": "For kernels, this includes the non-parametric regression techniques discussed here, but also techniques like kernel vector quantization [28] or support vector machines for classification [13] and kernel variants of k-means, SOM and neural gas for clustering [17].", "startOffset": 258, "endOffset": 262}, {"referenceID": 20, "context": "We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43].", "startOffset": 61, "endOffset": 73}, {"referenceID": 27, "context": "We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43].", "startOffset": 61, "endOffset": 73}, {"referenceID": 41, "context": "We transform S to a kernel matrixK via Eigenvalue correction [22, 29, 43].", "startOffset": 61, "endOffset": 73}, {"referenceID": 42, "context": "A variety of efficient approximation schemes exist [44].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Recently, the robust Bayesian Committee Machine (rBCM) has been introduced as a particularly fast and accurate approximation [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "As suggested by the authors, we use the differential entropy, given as \u03b2c = 12 \u00b7 ( log(\u03c3 prior) \u2212 log(\u03c3 c ) ) [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "Within this work, we address the latter issue by applying relational neural gas only on a constant-sized subset of the data and extending the clustering to the full data set later on, resulting in overall linear time [25].", "startOffset": 217, "endOffset": 221}, {"referenceID": 3, "context": "Barab\u00e1si-Albert model: This is a simple stochastic model of graph growth in undirected graphs [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "It has been shown that the edge distribution resulting from this growth model is scale-free, more specifically the probability of a certain degree k is P (k) = m \u00b7k\u22123, where m is the number of nodes [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 18, "context": "Conway\u2019s Game of Life: John Conway\u2019s Game of Life [20] is a simple, 2-dimensional cellular automaton model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "Note that Conway\u2019s Game of Life is turing-complete and its evolution is, in general, unpredictable without computing every single step according to the rules [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "As data representation for the theoretical data set we use an explicit feature embedding inspired by the shortest-path-kernel of Borgwardt and colleagues [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "This scheme is inspired by prior work in intelligent tutoring systems, such as the hint factory by Barnes and colleagues [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 35, "context": "MiniPalindrome: This dataset consists of 48 Java programs, each realizing one of eight different strategies to recognize palindromic input (see figure 4) [37] 2.", "startOffset": 154, "endOffset": 158}, {"referenceID": 35, "context": "The programs come in eight different variations described in [37].", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "Sorting: This is a benchmark dataset of 64 Java sorting programs taken from the web, implementing one of two sorting algorithms, namely BubbleSort or InsertionSort (see figure 4) [38] 3.", "startOffset": 179, "endOffset": 183}, {"referenceID": 45, "context": "Then, we computed a sequence alignment distance on the resulting node sequenes, similar to the method described by Robles-Kelly and Hancock [47].", "startOffset": 140, "endOffset": 144}, {"referenceID": 39, "context": "In particular, we used an affine sequence alignment with learned node dissimilarity as suggested in [41].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "We transformed the dissimilarity so a similarity via the radial basis function transformation and obtained a kernel via clip Eigenvalue correction [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "However, two problems remain open: First, usual hyperparameter optimization techniques depend on a vectorial data representation [15] and one has to adapt them for a relational case.", "startOffset": 129, "endOffset": 133}], "year": 2017, "abstractText": "Graph models are relevant in many fields, such as distributed computing, intelligent tutoring systems or social network analysis. In many cases, such models need to take changes in the graph structure into account, i.e. a varying number of nodes or edges. Predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics, e.g. with respect to user behaviour. However, predictive techniques in the past have almost exclusively focused on single edges or nodes. In this contribution, we attempt to predict the future state of a graph as a whole. We propose to phrase time series prediction as a regression problem and apply dissimilarityor kernel-based regression techniques, such as 1-nearest neighbor, kernel regression and Gaussian process regression, which can be applied to graphs via graph kernels. The output of the regression is a point embedded in a pseudo-Euclidean space, which can be analyzed using subsequent dissimilarityor kernel-based processing methods. We discuss strategies to speed up Gaussian Processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems. We find that simple regression methods, such as kernel regression, are sufficient to capture the dynamics in the theoretical models, but that Gaussian process regression significantly improves the prediction error for real-world data.", "creator": "LaTeX with hyperref package"}}}