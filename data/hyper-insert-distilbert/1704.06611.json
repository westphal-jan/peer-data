{"id": "1704.06611", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "as empirically, various neural operational networks solutions that attempt to internally learn programs built from data have exhibited slightly poor response generalizability. moreover, it has traditionally been already difficult longer to reason about the behavior of examining these models itself beyond depicting a somewhat certain appropriate level of pure input knowledge complexity. in order to address these issues, we thus propose augmenting neural architectures accordingly with a key computation abstraction : recursion. as an application, we correctly implement recursion in the neural programmer - paradigm interpreter test framework formulated on four tasks : grade - dependent school addition, fast bubble print sort, topological sort, and digital quicksort. nevertheless we demonstrate superior effort generalizability and interpretability as with surprisingly small amounts not of adequate training data. so recursion divides altogether the resulting problem into smaller organizational pieces themselves and presumably drastically reduces of the domain of each neural network component, hence making it tractable wrong to blindly prove turing guarantees about the overall system's mental behavior. our experience continually suggests that in great order for neural logic architectures fail to robustly learn program semantics, defeating it alone is necessary today to incorporate a concept structure like recursion.", "histories": [["v1", "Fri, 21 Apr 2017 16:02:26 GMT  (284kb,D)", "http://arxiv.org/abs/1704.06611v1", "Published in ICLR 2017"]], "COMMENTS": "Published in ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.PL", "authors": ["jonathon cai", "richard shin", "dawn song"], "accepted": true, "id": "1704.06611"}, "pdf": {"name": "1704.06611.pdf", "metadata": {"source": "CRF", "title": "MAKING NEURAL PROGRAMMING ARCHITECTURES GENERALIZE VIA RECURSION", "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "emails": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"], "sections": null, "references": [{"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Marcin Andrychowicz", "Karol Kurach"], "venue": "CoRR, abs/1602.03218,", "citeRegEx": "Andrychowicz and Kurach.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz and Kurach.", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["Lukasz Kaiser", "Ilya Sutskever"], "venue": "CoRR, abs/1511.08228,", "citeRegEx": "Kaiser and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2015}, {"title": "Neural random access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "ERCIM News,", "citeRegEx": "Kurach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Pointer networks. In Advances in Neural Information Processing Systems", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Zaremba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al.", "startOffset": 181, "endOffset": 203}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al.", "startOffset": 181, "endOffset": 230}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al.", "startOffset": 181, "endOffset": 256}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al. (2016), Zaremba et al.", "startOffset": 181, "endOffset": 278}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al. (2016), Zaremba et al. (2016)).", "startOffset": 181, "endOffset": 301}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al.", "startOffset": 40, "endOffset": 112}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al. (2016)).", "startOffset": 40, "endOffset": 157}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al. (2016)). In this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change. Although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in Zaremba et al. (2016). That is to say, the network does not learn the true program semantics.", "startOffset": 40, "endOffset": 487}, {"referenceID": 3, "context": ", 2014), Neural GPU (Kaiser & Sutskever, 2015), Neural Programmer (Neelakantan et al., 2015), Pointer Network (Vinyals et al.", "startOffset": 66, "endOffset": 92}, {"referenceID": 4, "context": ", 2015), Pointer Network (Vinyals et al., 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), and Neural Random Access Machine (Kurach et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 2, "context": ", 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), and Neural Random Access Machine (Kurach et al., 2016).", "startOffset": 103, "endOffset": 124}], "year": 2017, "abstractText": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "creator": "LaTeX with hyperref package"}}}