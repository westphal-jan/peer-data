{"id": "1704.06692", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Improving Semantic Composition with Offset Inference", "abstract": "functional count - variance based complex distributional semantic models suffer from sparsity due to unobserved descriptive but likely plausible co - occurrences arising in any reference text collection. because this problem is amplified for models scenarios like the anchored packed suffix trees ( apts ), simulations that take the relevant grammatical type of semantic a co - occurrence into sole account. accordingly we therefore intentionally introduce also a novel form of simplified distributional inference that exploits incorrectly the rich feature type structure in apts and possibly infers by missing data accessed by the same mechanism syntax that is used largely for semantic response composition.", "histories": [["v1", "Fri, 21 Apr 2017 19:47:30 GMT  (2001kb,D)", "http://arxiv.org/abs/1704.06692v1", "to appear at ACL 2017 (short papers)"]], "COMMENTS": "to appear at ACL 2017 (short papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thomas kober", "julie weeds", "jeremy reffin", "david j weir"], "accepted": true, "id": "1704.06692"}, "pdf": {"name": "1704.06692.pdf", "metadata": {"source": "CRF", "title": "Improving Semantic Composition with Offset Inference", "authors": ["Thomas Kober", "Julie Weeds", "Jeremy Reffin", "David Weir"], "emails": ["d.j.weir}@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Anchored Packed Trees (APTs) is a recently proposed approach to distributional semantics that takes distributional composition to be a process of lexeme contextualisation (Weir et al., 2016). A lexeme\u2019s meaning, characterised as knowledge concerning co-occurrences involving that lexeme, is represented with a higher-order dependencytyped structure (the APT) where paths associated with higher-order dependencies connect vertices associated with weighted lexeme multisets. The central innovation in the compositional theory is that the APT\u2019s type structure enables the precise alignment of the semantic representation of each of the lexemes being composed. Like other countbased distributional spaces, however, it is prone to considerable data sparsity, caused by not observing all plausible co-occurrences in the given data. Recently, Kober et al. (2016) introduced a simple unsupervised algorithm to infer missing cooccurrence information by leveraging the distributional neighbourhood and ease the sparsity effect in count-based models.\nIn this paper, we generalise distributional inference (DI) in APTs and show how precisely\nthe same mechanism that was introduced to support distributional composition, namely \u201coffsetting\u201d APT representations, gives rise to a novel form of distributional inference, allowing us to infer co-occurrences from neighbours of these representations. For example, by transforming a representation of white to a representation of \u201cthings that can be white\u201d, inference of unobserved, but plausible, co-occurrences can be based on finding near neighbours (which will be nouns) of the \u201cthings that can be white\u201d structure. This furthermore exposes an interesting connection between distributional inference and distributional composition. Our method is unsupervised and maintains the intrinsic interpretability of APTs1."}, {"heading": "2 Offset Representations", "text": "The basis of how composition is modelled in the APT framework is the way that the co-occurrences are structured. In characterising the distributional semantics of some lexeme w, rather than just recording a co-occurrence between w and w\u2032 within some context window, we follow Pado\u0301 and Lapata (2007) and record the dependency path from w to w\u2032. This syntagmatic structure makes it possible to appropriately offset the semantic representations of each of the lexemes being composed in some phrase. For example many nouns will have distributional features starting with the type amod, which cannot be observed for adjectives or verbs. Thus, when composing the adjective white with the noun clothes, the feature spaces of the two lexemes need to be aligned first. This can be achieved by offsetting one of the constituents, which we will explain in more detail in this section.\nWe will make use of the following nota-\n1We release our code and data at https://github. com/tttthomasssss/acl2017\nar X\niv :1\n70 4.\n06 69\n2v 1\n[ cs\n.C L\n] 2\n1 A\npr 2\n01 7\ntion throughout this work. A typed distributional feature consists of a path and a lexeme such as in amod:white. Inverse paths are denoted by a horizontal bar above the dependency relation such as in dobj:prefer and higherorder paths are separated by a dot such as in amod.compound:dress.\nOffset representations are the central component in the composition process in the APT framework. Figure 1 shows the APT representations for the adjective white (left) and the APT for the noun clothes (right), as might have been observed in a text collection. Each node holds a multiset of lexemes and the anchor of an APT reflects the current perspective of a lexeme at the given node. An offset representation can be created by shifting the anchor along a given path. For example the lexeme white is at the same node as other adjectives such as black and clean, whereas nouns such as shoes or noise are typically reached via the amod edge.\nOffsetting in APTs only involves a change in the anchor, the underlying structure remains unchanged. By offsetting the lexeme white by amod the anchor is shifted along the amod edge, which results in creating a noun view for the adjective white. We denote the offset view of a lexeme for a given path by superscripting the offset path, for example the amod offset of the adjective white is denoted as whiteamod. The offsetting procedure changes the starting points of the paths as visible in Figure 1 between the anchors for white and whiteamod, since paths always begin at the anchor. The red dashed line in Figure 1 reflects that anchor shift. The lexeme whiteamod represents a prototypical \u201cwhite thing\u201d, that is, a noun that has been modified by the adjective white. We note that all edges in the APT space are bi-directional as exemplified in the coloured amod and amod edges in the APT for white, however for brevity we only show uni-directional edges in Figure 1.\nBy considering the APT representations for the lexemes white and clothes in Figure 1, it becomes apparent that lexemes with different parts of speech are located in different areas of the semantic space. If we want to compose the adjective-noun phrase white clothes, we need to offset one of the two constituents to align the feature spaces in order to leverage their distributional commonalities. This can be achieved by either creating a noun offset view of white, by shift-\ning the anchor along the amod edge, or by creating an adjective offset representation of clothes by shifting its anchor along amod. In this work we follow Weir et al. (2016) and always offset the dependent in a given relation. Table 1 shows a subset of the features of Figure 1 as would be represented in a vectorised APT. Vectorising the whole APT lexicon results in a very highdimensional and sparse typed distributional space. The features for whiteamod (middle column) highlight the change in feature space caused by offsetting the adjective white. The features of the offset view whiteamod, are now aligned with the noun clothes such that the two can be composed. Composition can be performed by either selecting the union or intersection of the aligned features."}, {"heading": "2.1 Qualitative Analysis of Offset Representations", "text": "Any offset view of a lexeme is behaviourally identical to a \u201cnormal\u201d lexeme. It has an associated part of speech, a distributional representation which locates it in semantic space, and we can find neighbours for it in the same way that we find neighbours for any other lexeme. In this way, a single APT data structure is able to provide many different views of any given lexeme. These views reflect the different ways in which the lexeme is used. For example lawnsubj is the nsubj offset representation of the noun law. This lexeme is a verb and represents an action carried out by the law. This contrasts with lawdobj, which is the dobj offset representation of the noun law. It is also a verb, however represents actions done to the law. Table 2 lists the 10 nearest neighbours for a number of lexemes, offset by amod, dobj and nsubj respectively.\nFor example, the neighbourhood of the lexeme ancient in Table 2 shows that the offset view for ancientamod is a prototypical representation of an \u201cancient thing\u201d, with neighbours easily associated with the property ancient. Furthermore, Table 2\nillustrates that nearest neighbours of offset views are often other offset representations. This means that for example actions carried out by a mother tend to be similar to actions carried out by a father or a parent."}, {"heading": "2.2 Offset Inference", "text": "Our approach generalises the unsupervised algorithm proposed by Kober et al. (2016), henceforth \u201cstandard DI\u201d, as a method for inferring missing knowledge into an APT representation. Rather than simply inferring potentially plausible, but unobserved co-occurrences from near distributional neighbours, inferences can be made involving offset APTs. For example, the adjective white can be offset so that it represents a noun \u2014 a prototypical \u201cwhite thing\u201d. This allows inferring plausible co-occurrences from other \u201cthings that can be white\u201d, such as shoes or shirts. Our algorithm therefore reflects the contextualised use of a word. This has the advantage of being able to make flexible and fine grained distinctions in the inference process. For example if the noun law is used as a subject, our algorithm allows inferring plausible co-occurrences from \u201cother actions carried out by the law\u201d. This contrasts the use of law as an object, where offset inference is able to find cooccurrences on the basis of \u201cother actions done to the law\u201d. This is a crucial advantage over the method of Kober et al. (2016) which only supports inference on uncontextualised lexemes.\nA sketch of how offset inference for a lexeme w works is shown in Algorithm 1. Our algorithm requires a distributional model M , an APT representation for the lexeme w for which to perform offset inference, a dependency path p, describing the offset for w, and the number of neighbours k. The offset representation of w\u2032 is then enriched\nwith the information from its distributional neighbours by some merge function. We note that if the offset path p is the empty path, we would recover the algorithm presented by Kober et al. (2016). Our algorithm is unsupervised, and agnostic to the input distributional model and the neighbour retrieval function.\nAlgorithm 1 Offset Inference 1: procedure OFFSET INFERENCE(M , w, p, k) 2: w\u2032 \u2190 offset(w, p) 3: for all n in neighbours(M,w\u2032, k) do 4: w\u2032\u2032 \u2190 merge(w\u2032\u2032, n) 5: end for 6: return w\u2032\u2032 7: end procedure\nConnection to Distributional Composition\nAn interesting observation is the similarity between distributional inference and distributional composition, as both operations are realised by the same mechanism \u2014 an offset followed by inferring plausible co-occurrence counts for a single lexeme in the case of distributional inference, or for a phrase in the case of composition. The merging of co-occurrence dimensions for distributional inference can also be any of the operations commonly used for distributional composition such as pointwise minimum, maximum, addition or multiplication.\nThis relation creates an interesting dynamic between distributional inference and composition when used in a complementary manner as in this work. The former can be used as a process of cooccurrence embellishment which is adding missing information, however with the risk of introducing some noise. The latter on the other hand can be\nused as a process of co-occurrence filtering, that is leveraging the enriched representations, while also sieving out the previously introduced noise."}, {"heading": "3 Experiments", "text": "For our experiments we re-implemented the standard DI method of Kober et al. (2016) for a direct comparison. We built an order 2 APT space on the basis of the concatenation of ukWaC, Wackypedia and the BNC (Baroni et al., 2009), pre-parsed with the Malt parser (Nivre et al., 2006). We PPMI transformed the raw co-occurrence counts prior to composition, using a negative SPPMI shift of log 5 (Levy and Goldberg, 2014b). We also experimented with composing normalised counts and applying the PPMI transformation after composition as done by Weeds et al. (2017), however found composing PPMI scores to work better for this task.\nWe evaluate our offset inference algorithm on two popular short phrase composition benchmarks by Mitchell and Lapata (2008) and Mitchell and Lapata (2010), henceforth ML08 and ML10 respectively. The ML08 dataset consists of 120 distinct verb-object (VO) pairs and the ML10 dataset contains 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object pairs. The goal is to compare a model\u2019s similarity estimates to human provided judgements. For both tasks, each phrase pair has been rated by multiple human annotators on a scale between 1 and 7, where 7 indicates maximum similarity. Comparison with human judgements is achieved by calculating Spearman\u2019s \u03c1 between the model\u2019s similarity estimates and the scores of each human annotator individually. We performed composition by intersection and tuned the number of neighbours by a grid search over {0, 10, 30, 50, 100, 500, 1000} on the ML10 development set, selecting 10 neighbours for NNs, 100 for ANs and 50 for VOs for both DI algorithms. We\ncalculate statistical significance using the method of Steiger (1980).\nEffect of the number of neighbours Figure 2 shows the effect of the number of neighbours for AN, NN and VO phrases, using offset inference, on the ML10 development set. Interestingly, NN compounds exhibit an early saturation effect, while VOs and ANs require more neighbours for optimal performance. One explanation for the observed behaviour is that up to some threshold, the neighbours being added contribute actually missing co-occurrence events, whereas past that threshold distributional inference degrades to just generic smoothing that is simply compensating for sparsity, but overwhelming the representations with non-plausible co-occurrence information. A similar effect has also been observed by Erk and Pado (2010) in an exemplarbased model.\nResults Table 3 shows that both forms of distributional inference significantly outperform a baseline without DI. On average, offset inference outperforms the method of Kober et al. (2016) by a statistically\nsignificant margin on both datasets.\nTable 4 shows that offset inference substantially outperforms comparable sparse models by Dinu et al. (2013) on ML08, achieving a new state-ofthe-art, and matches the performance of the stateof-the-art neural network model of Hashimoto et al. (2014) on ML10, while being fully interpretable."}, {"heading": "4 Related Work", "text": "Distributional inference has its roots in the work of Dagan et al. (1993, 1994), who aim to find probability estimates for unseen words in bigrams, and Schu\u0308tze (1992, 1998) who leverages the distributional neighbourhood through clustering of contexts for word-sense discrimination. Recently Kober et al. (2016) revitalised the idea for compositional distributional semantic models.\nComposition with distributional semantic models has become a popular research area in recent years. Simple, yet competitive methods, are based on pointwise vector addition or multiplication (Mitchell and Lapata, 2008, 2010). However, these approaches neglect the structure of the text defining composition as a commutative operation.\nA number of approaches proposed in the literature attempt to overcome this shortcoming by introducing weighted additive variants (Guevara, 2010, 2011; Zanzotto et al., 2010). Another popular strand of work models semantic composition on the basis of ideas arising in formal semantics. Composition in such models is usually implemented as operations on higher-order tensors (Baroni and Zamparelli, 2010; Baroni et al., 2014; Co-\necke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell\u2019Arciprete, 2012; Annesi et al., 2014) as composition functions.\nThe above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pado\u0301 (2008); Gamallo and Pereira-Farin\u0303a (2017); Levy and Goldberg (2014a); Pado\u0301 and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014).\nThe perhaps most popular approach in the literature to evaluating compositional distributional semantic models is to compare human word and phrase similarity judgements with similarity estimates of composed meaning representations, under the assumption that better distributional representations will perform better at these tasks (Blacoe and Lapata, 2012; Dinu et al., 2013; Erk and Pado\u0301, 2008; Hashimoto et al., 2014; Hermann and Blunsom, 2013; Kiela et al., 2014; Turney, 2012)."}, {"heading": "5 Conclusion", "text": "In this paper we have introduced a novel form of distributional inference that generalises the method introduced by Kober et al. (2016). We have shown its effectiveness for semantic composition on two benchmark phrase similarity tasks where we achieved state-of-the-art performance while retaining the interpretability of our model. We have furthermore highlighted an interesting connection between distributional inference and distributional composition.\nIn future work we aim to apply our novel method to improve modelling selectional preferences, lexical inference, and scale up to longer phrases and full sentences."}], "references": [{"title": "Semantic compositionality in tree kernels", "author": ["Paolo Annesi", "Danilo Croce", "Roberto Basili."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, New", "citeRegEx": "Annesi et al\\.,? 2014", "shortCiteRegEx": "Annesi et al\\.", "year": 2014}, {"title": "Frege in space: A program for compositional distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Linguistic Issues in Language Technology 9(6):5\u2013110.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "The wacky wide web: a collection of very large linguistically processed web-crawled corpora", "author": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta."], "venue": "Language Resources and Evaluation 43(3):209\u2013226.", "citeRegEx": "Baroni et al\\.,? 2009", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics 36(4):673\u2013721.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the As-", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Mathematical foundations for a compositional distributed model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "Linguistic Analysis 36(1-4):345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2011", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "Structured lexical similarity via convolution kernels on dependency trees", "author": ["Danilo Croce", "Alessandro Moschitti", "Roberto Basili."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Croce et al\\.,? 2011", "shortCiteRegEx": "Croce et al\\.", "year": 2011}, {"title": "Contextual word similarity and estimation from sparse data", "author": ["Ido Dagan", "Shaul Marcus", "Shaul Markovitch."], "venue": "Proceedings of the 31st Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Dagan et al\\.,? 1993", "shortCiteRegEx": "Dagan et al\\.", "year": 1993}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Ido Dagan", "Fernando Pereira", "Lillian Lee."], "venue": "Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Dagan et al\\.,? 1994", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "General estimation and evaluation of compositional distributional semantic models", "author": ["Georgiana Dinu", "Nghia The Pham", "Marco Baroni."], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality. Association for", "citeRegEx": "Dinu et al\\.,? 2013", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "A structured vector space model for word meaning in context", "author": ["Katrin Erk", "Sebastian Pad\u00f3."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "Exemplarbased models for word meaning in context", "author": ["Katrin Erk", "Sebastian Pado."], "venue": "Proceedings of the ACL 2010 Conference Short Papers. Association for Computational Linguistics, Uppsala, Sweden, pages 92\u201397.", "citeRegEx": "Erk and Pado.,? 2010", "shortCiteRegEx": "Erk and Pado.", "year": 2010}, {"title": "Compositional semantics using feature-based models from wordnet", "author": ["Pablo Gamallo", "Mart\u0131\u0301n Pereira-Fari\u00f1a"], "venue": "In Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications", "citeRegEx": "Gamallo and Pereira.Fari\u00f1a.,? \\Q2017\\E", "shortCiteRegEx": "Gamallo and Pereira.Fari\u00f1a.", "year": 2017}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni."], "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Concrete sentence spaces for compositional distributional models of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh", "Stephen Clark", "Bob Coecke", "Stephen Pulman."], "venue": "Proceedings of the 9th International Conference on Computational Seman-", "citeRegEx": "Grefenstette et al\\.,? 2011", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["Emiliano Guevara."], "venue": "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics. Association for Computa-", "citeRegEx": "Guevara.,? 2010", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Computing semantic compositionality in distributional semantics", "author": ["Emiliano Guevara."], "venue": "Proceedings of the Ninth International Conference on Computational Semantics. Association for Computational Linguistics, Stroudsburg, PA, USA, IWCS", "citeRegEx": "Guevara.,? 2011", "shortCiteRegEx": "Guevara.", "year": 2011}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 2014 Conference on", "citeRegEx": "Hashimoto et al\\.,? 2014", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["Felix Hill", "KyungHyun Cho", "Anna Korhonen", "Yoshua Bengio."], "venue": "Transactions of the Association for Computational Linguistics 4:17\u201330.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL).", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2014", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2014}, {"title": "Improving multi-modal representations using image dispersion: Why less is sometimes more", "author": ["Douwe Kiela", "Felix Hill", "Anna Korhonen", "Stephen Clark."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kiela et al\\.,? 2014", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Improving sparse word", "author": ["David Weir"], "venue": null, "citeRegEx": "Weir.,? \\Q2016\\E", "shortCiteRegEx": "Weir.", "year": 2016}, {"title": "Discriminative neural sentence", "author": ["Zhi Jin"], "venue": null, "citeRegEx": "Jin.,? \\Q2015\\E", "shortCiteRegEx": "Jin.", "year": 2015}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze."], "venue": "Computational Linguistics 24(1):97\u2013123.", "citeRegEx": "Sch\u00fctze.,? 1998", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc Le", "Christopher Manning", "Andrew Ng."], "venue": "Transactions of the Association for Computational Linguistics 2:207\u2013218.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Tests for comparing elements of a correlation matrix", "author": ["James H Steiger."], "venue": "Psychological Bulletin 87(2):245.", "citeRegEx": "Steiger.,? 1980", "shortCiteRegEx": "Steiger.", "year": 1980}, {"title": "Contextualizing semantic representations using syntactically enriched vector models", "author": ["Stefan Thater", "Hagen F\u00fcrstenau", "Manfred Pinkal."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for", "citeRegEx": "Thater et al\\.,? 2010", "shortCiteRegEx": "Thater et al\\.", "year": 2010}, {"title": "Word meaning in context: A simple and effective vector model", "author": ["Stefan Thater", "Hagen F\u00fcrstenau", "Manfred Pinkal."], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Pro-", "citeRegEx": "Thater et al\\.,? 2011", "shortCiteRegEx": "Thater et al\\.", "year": 2011}, {"title": "Learning semantically and additively compositional distributional representations", "author": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Tian et al\\.,? 2016", "shortCiteRegEx": "Tian et al\\.", "year": 2016}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["Peter D. Turney."], "venue": "Journal of Artificial Intelligence Research 44(1):533\u2013585.", "citeRegEx": "Turney.,? 2012", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "A tensor-based factorization model of semantic compositionality", "author": ["Tim Van de Cruys", "Thierry Poibeau", "Anna Korhonen."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Cruys et al\\.,? 2013", "shortCiteRegEx": "Cruys et al\\.", "year": 2013}, {"title": "When a red herring in not a red", "author": ["Julie Weeds", "Thomas Kober", "Jeremy Reffin", "David Weir"], "venue": null, "citeRegEx": "Weeds et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2017}, {"title": "Distributional composition using higher-order dependency vectors", "author": ["Julie Weeds", "David Weir", "Jeremy Reffin."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC). Association for Compu-", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Aligning packed dependency trees: a theory of composition for distributional semantics", "author": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober."], "venue": "Computational Linguistics, special issue on Formal Distributional Semantics 42(4):727\u2013761.", "citeRegEx": "Weir et al\\.,? 2016", "shortCiteRegEx": "Weir et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Transactions of the Association for Computational Linguistics 3:345\u2013358.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Mo Yu", "Mark Dredze."], "venue": "Transactions of the Association for Computational Linguistics 3:227\u2013 242. http://aclweb.org/anthology/Q/Q15/Q151017.pdf.", "citeRegEx": "Yu and Dredze.,? 2015", "shortCiteRegEx": "Yu and Dredze.", "year": 2015}, {"title": "Distributed tree kernels", "author": ["Fabio Massimo Zanzotto", "Lorenzo Dell\u2019Arciprete"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "citeRegEx": "Zanzotto and Dell.Arciprete.,? \\Q2012\\E", "shortCiteRegEx": "Zanzotto and Dell.Arciprete.", "year": 2012}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar."], "venue": "Proceedings of Coling. pages 1263\u20131271. http://www.aclweb.org/anthology/C10-", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 38, "context": "Anchored Packed Trees (APTs) is a recently proposed approach to distributional semantics that takes distributional composition to be a process of lexeme contextualisation (Weir et al., 2016).", "startOffset": 171, "endOffset": 190}, {"referenceID": 25, "context": "In this work we follow Weir et al. (2016) and always offset the dependent in a given relation.", "startOffset": 23, "endOffset": 42}, {"referenceID": 2, "context": "We built an order 2 APT space on the basis of the concatenation of ukWaC, Wackypedia and the BNC (Baroni et al., 2009), pre-parsed with the Malt parser (Nivre et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 36, "context": "We also experimented with composing normalised counts and applying the PPMI transformation after composition as done by Weeds et al. (2017), however", "startOffset": 120, "endOffset": 140}, {"referenceID": 30, "context": "We calculate statistical significance using the method of Steiger (1980).", "startOffset": 58, "endOffset": 73}, {"referenceID": 13, "context": "A similar effect has also been observed by Erk and Pado (2010) in an exemplarbased model.", "startOffset": 43, "endOffset": 63}, {"referenceID": 11, "context": "Table 4 shows that offset inference substantially outperforms comparable sparse models by Dinu et al. (2013) on ML08, achieving a new state-of-", "startOffset": 90, "endOffset": 109}, {"referenceID": 20, "context": "the-art, and matches the performance of the stateof-the-art neural network model of Hashimoto et al. (2014) on ML10, while being fully interpretable.", "startOffset": 84, "endOffset": 108}, {"referenceID": 5, "context": "31 Blacoe and Lapata (2012) 0.", "startOffset": 3, "endOffset": 28}, {"referenceID": 5, "context": "31 Blacoe and Lapata (2012) 0.44 Hashimoto et al. (2014) 0.", "startOffset": 3, "endOffset": 57}, {"referenceID": 5, "context": "31 Blacoe and Lapata (2012) 0.44 Hashimoto et al. (2014) 0.48 Weir et al. (2016) 0.", "startOffset": 3, "endOffset": 81}, {"referenceID": 5, "context": "31 Blacoe and Lapata (2012) 0.44 Hashimoto et al. (2014) 0.48 Weir et al. (2016) 0.43 0.26 Dinu et al. (2013) 0.", "startOffset": 3, "endOffset": 110}, {"referenceID": 5, "context": "31 Blacoe and Lapata (2012) 0.44 Hashimoto et al. (2014) 0.48 Weir et al. (2016) 0.43 0.26 Dinu et al. (2013) 0.23\u2212 0.26 Erk and Pad\u00f3 (2008) 0.", "startOffset": 3, "endOffset": 141}, {"referenceID": 42, "context": "introducing weighted additive variants (Guevara, 2010, 2011; Zanzotto et al., 2010).", "startOffset": 39, "endOffset": 83}, {"referenceID": 8, "context": ", 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell\u2019Arciprete, 2012; Annesi et al., 2014) as composition functions.", "startOffset": 60, "endOffset": 136}, {"referenceID": 41, "context": ", 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell\u2019Arciprete, 2012; Annesi et al., 2014) as composition functions.", "startOffset": 60, "endOffset": 136}, {"referenceID": 0, "context": ", 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell\u2019Arciprete, 2012; Annesi et al., 2014) as composition functions.", "startOffset": 60, "endOffset": 136}, {"referenceID": 3, "context": "The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in", "startOffset": 127, "endOffset": 151}, {"referenceID": 12, "context": "the models of Erk and Pad\u00f3 (2008); Gamallo and Pereira-Fari\u00f1a (2017); Levy and Goldberg (2014a); Pad\u00f3 and Lapata (2007); Thater et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 12, "context": "the models of Erk and Pad\u00f3 (2008); Gamallo and Pereira-Fari\u00f1a (2017); Levy and Goldberg (2014a); Pad\u00f3 and Lapata (2007); Thater et al.", "startOffset": 14, "endOffset": 69}, {"referenceID": 12, "context": "the models of Erk and Pad\u00f3 (2008); Gamallo and Pereira-Fari\u00f1a (2017); Levy and Goldberg (2014a); Pad\u00f3 and Lapata (2007); Thater et al.", "startOffset": 14, "endOffset": 96}, {"referenceID": 12, "context": "the models of Erk and Pad\u00f3 (2008); Gamallo and Pereira-Fari\u00f1a (2017); Levy and Goldberg (2014a); Pad\u00f3 and Lapata (2007); Thater et al.", "startOffset": 14, "endOffset": 120}, {"referenceID": 12, "context": "the models of Erk and Pad\u00f3 (2008); Gamallo and Pereira-Fari\u00f1a (2017); Levy and Goldberg (2014a); Pad\u00f3 and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in the lit-", "startOffset": 14, "endOffset": 169}], "year": 2017, "abstractText": "Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition.", "creator": "LaTeX with hyperref package"}}}