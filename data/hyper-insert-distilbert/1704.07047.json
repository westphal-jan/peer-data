{"id": "1704.07047", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Fast and Accurate Neural Word Segmentation for Chinese", "abstract": "neural models with minimal performance feature engineering value have historically achieved competitive performance success against traditional methods working for the task efficiency of chinese word segmentation. however, both neurological training models and working procedures upstream of the newly current neural models are universally computationally demonstrated inefficient. besides this paper furthermore presents a nearly greedy analytical neural modified word segmenter with theoretically balanced sensory word analysis and character embedding logic inputs to alleviate problems the existing drawbacks. nonetheless our segmenter is truly end - to - short end, capable of performing segmentation much faster and relatively even more universally accurate so than state - built of - \u2013 the - art complete neural series models on chinese finite benchmark datasets.", "histories": [["v1", "Mon, 24 Apr 2017 05:50:29 GMT  (3422kb,D)", "http://arxiv.org/abs/1704.07047v1", "To appear in ACL2017"]], "COMMENTS": "To appear in ACL2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["deng cai", "hai zhao", "zhisong zhang", "yuan xin", "yongjian wu", "feiyue huang"], "accepted": true, "id": "1704.07047"}, "pdf": {"name": "1704.07047.pdf", "metadata": {"source": "CRF", "title": "Fast and Accurate Neural Word Segmentation for Chinese", "authors": ["Deng Cai", "Hai Zhao", "Zhisong Zhang", "Yuan Xin", "Yongjian Wu", "Feiyue Huang"], "emails": ["thisisjcykcd@gmail.com,", "{zhaohai@cs,", "zzs2011@}sjtu.edu.cn", "macxin@tencent.com", "littlekenwu@tencent.com", "garyhuang@tencent.com"], "sections": [{"heading": "1 Introduction", "text": "Word segmentation is a fundamental task for processing most east Asian languages, typically Chinese. Almost all practical Chinese processing applications essentially rely on Chinese word segmentation (CWS), e.g., (Zhao et al., 2017).\nSince (Xue, 2003), most methods formalize this task as a sequence labeling problem. In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) (Low et al., 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004). However, these models rely heavily on hand-crafted features.\n\u2217Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), Key Project of National Society Science Foundation of China (No. 15-ZDA041), and the joint research project with Youtu Lab of Tencent.\nTo minimize the efforts in feature engineering, neural word segmentation has been actively studied recently. Zheng et al. (2013) first adapted the sliding-window based sequence labeling (Collobert et al., 2011) with character embeddings as input. A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity. Pei et al. (2014) introduced tag embeddings. Chen et al. (2015a) proposed to model ngram features via a gated recursive neural network (GRNN). Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction.\nBesides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework. Liu et al. (2016) used a zero-order semiCRF based model. However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops rapidly when solely depending on neural models. Most closely related to this work, Cai and Zhao (2016) proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word representation generation and an LSTM scoring model for segmentation result evaluation.\nDespite the active progress of most existing works in terms of accuracy, their computational needs have been significantly increased to the extent that training a neural segmenter usually takes days even using cutting-edge hardwares. Meanwhile, different applications often require diverse segmenters and offer large-scale incoming data. The efficiency of a word segmenter either for training and decoding is crucial in practice. In this paper, we propose a simple yet accurate neu-\nar X\niv :1\n70 4.\n07 04\n7v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nral word segmenter who searches greedily during both training and working to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets."}, {"heading": "2 Related Work", "text": "Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016).\nWord based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013).\nUnlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding. In this work, we will make a series of significant improvement over the basic framework and especially adopt greedy search instead.\nAnother notable exception of embedding based methods is (Ma and Hinrichs, 2015), which used character-specified tags matching for fast decoding and resulted in a character-based greedy segmenter."}, {"heading": "3 Models", "text": "To segment a character sequence, we employ neural networks to score the likelihood of a candidate segmented sequence being a true sentence, and the\none with the highest score will be picked as output."}, {"heading": "3.1 Neural Scorer", "text": "Our neural architecture to score a segmented sequence (word sequence) can be described in the following three steps (illustrated in Figure 1).\nEncoding To make use of neural networks, symbolic data needs to be transformed into distributed representations. The most straightforward solution is to use a lookup table for word vectors (Bengio et al., 2003). However, in the context of neural word segmentation, it will generalize poorly due to the severe word sparsity in Chinese. An alternative is employing neural networks to compose word representations from character embedding inputs. However, it is empirically hard to learn a satisfactory composition function. In fact, quite a lot of Chinese words, like \u201c\u6c99(sand)\u53d1(issue)\u201d (sofa) , are not semantically character-level compositional at all.\nFor the dilemma that composing word representations from character may be insufficient while the direct use of word embedding may lose generalization ability, we propose a hybrid mechanism to alleviate the problem. Concretely, we keep a short list H of the most frequent words w = c1..cl to balance character composition. If w inH, the immediate word embedding w \u2208 Rdw is attached via average pooling1, otherwise, the character composition is used alone.\nWORD(c1..cl) =\n{ COMP(c1..cl)+w[w]\n2 if c1..cl \u2208 H COMP(c1..cl) otherwise\nOur character composition function COMP(\u00b7) for 1We tried other two integration functions, concatenation and adaptive gating mechanism, but it finally shows that the simplest averaging works best.\nl-length word is\nCOMP(c1..cl) = tanh(Wcl [r1 c1; . . . ; rl cl]+bcl )\nwhere denotes the element-wise multiplication. ri \u2208 Rdc is the gate that controls the information flow from character embedding ci \u2208 Rdc to word. Intuitively, the gating mechanism is used to determine which part of the character vectors should be retrieved when composing a certain word. This is indeed important due to the ambiguity of individual Chinese characters.\n[r1; . . . ; rl] = sigmoid(Wrl [c1; . . . ; cl] + b r l )\nIn contrast, the model in (Cai and Zhao, 2016) further combined COMP(\u00b7) and character embeddings ci via an update gate z (As in Figure 2), which has been shown helpless to the performance but requires huge computational cost according to our empirical study.\nLinking To capture word interactions within a word sequence, the resulted word vectors are then linked sequentially via an LSTM (Sundermeyer et al., 2012). At each time step i, a prediction about next word is made according to the current hidden state hi \u2208 RH of LSTM. The procedure can be described as the following equation.\npi+1 = tanh(W phi + b p)\nThe predictions p \u2208 Rdw will then be used to evaluate how reasonable the transition is between next word and the preceding word sequence.\nScoring The segmented sequence is evaluated from two perspectives, (i) the legality of individual words, (ii) the smoothness or coherence of the word sequence. The former is judged by a trainable parameter vector u \u2208 Rdw , which is supposed to work like a hyperplane separating legal and illegal words. For the latter, the prediction p made for each position can be used to score the fitness of the\nactual word. Both scoring operations are implemented via dot product in our settings. Summing up all scores, the segmented sequence (sentence) is scored as follow.\ns([w1, w2, . . . , wn]) = n\u2211\ni=1\n(u+ pi) TWORD(wi)"}, {"heading": "3.2 Search", "text": "The number of possible segmented sentences grows exponentially with the length of the input character sequence. Most existing methods made Markov assumptions to keep the exact search tractable.2 However, such assumptions cannot be made in our model as the LSTM component takes advantage of the full segmentation history. We then adopt a beam search scheme, which works iteratively on every prefix of the input character sequence, approximating the k highest-scored word sequences of each prefix (i.e., k is the beam size). The time complexity of our beam search is O(wkn), where w is the maximum word length and n is the input sequence length."}, {"heading": "3.3 Training Criteria", "text": "Our segmenter is trained using max-margin methods (Taskar et al., 2005) where the structured margin loss is defined as \u00b5 times the number of incorrectly segmented characters (Cai and Zhao, 2016). However, according to (Huang et al., 2012), standard parameter update cannot guarantee convergence in the case of inexact search. We thus additionally examine two strategies as follows.\nEarly update This strategy proposed in (Collins and Roark, 2004) can be simplified into \u201cupdate once the golden answer is unreachable\u201d. In our case, when the considering character prefix can be correctly segmented but the correct one falls off the beam, an update operation will be conducted and the rest part will be ignored.\nLaSO update One drawback of early update is that the search may never reach the end of a training instance, which means the rest part of the instance is \u201cwasted\u201d. Differently, LaSO method of (Daume\u0301 III and Marcu, 2005) continues on the same instance with correct hypothesis after each update. In our case, the beam will be emptied and the corresponding prefix of the correct word sequence will be inserted into the beam.\n2By assuming that tag interactions or word interactions only exist in adjacent positions."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets and Settings", "text": "We conduct experiments on two popular benchmark datasets, namely PKU and MSR, from the second international Chinese word segmentation bakeoff (Emerson, 2005) (Bakeoff-2005). Data statistics are in Table 1.\nThroughout this paper, we use the same model setting as shown in Table 2. These numbers are tuned on development sets.3 We follow (Dyer et al., 2015) to train model parameters. The learning rate at epoch t is set as \u03b7t = 0.2/(1 + \u03b3t), where \u03b3 = 0.1 for PKU dataset and \u03b3 = 0.2 for MSR dataset. The character embeddings are either randomly initialized or pre-trained by word2vec (Mikolov et al., 2013) toolkit on Chinese Wikipedia corpus (which will be indicated by +pre-train in tables.), while the word embeddings are always randomly initialized. The beam size is kept the same during training and working. By default, early update strategy is adopted and the word table H is top half of in-vocabulary (IV) words by frequency."}, {"heading": "4.2 Model Analysis", "text": "Beam search collapses into greedy search Figure 3 demonstrates the effect of beam size. To our surprise, beam size change has little influence on the performance. Namely, simple stepwise greedy search nearly achieves the best performance, which suggests that word segmentation can be greedily solvable at word-level. It may be due to that right now the model is optimal\n3Following conventions, the last 10% sentences of training corpus are used as development set.\nenough to make correct decisions at the first position. In fact, similar phenomenon was observed at character-level (Ma and Hinrichs, 2015). The rest experiments will thus only report the results of our greedy segmenter.\nComparing different update methods Table 3 compares the concerned three training strategies. We find that early update leads to faster convergence and a bit better performance compared to both standard and LaSO update.\nCharacter composition versus word embedding Following Section 3.1, direct use of word embedding may bring efficiency and effectiveness for identifying IV words, but weaken the ability to recognize out-of-vocabulary (OOV) words. We accordingly conduct experiments on different sizes of word table H. Concretely, sorting all IV words by frequency, the first {0, 25%, 50%, 75%, 100%} fraction of them respectively forms the word table. The corresponding results on PKU in Figure 4 demonstrate that by the use of direct word embedding, F1 score increases first but then drops rapidly. In contrast, OOV recall, which partially reflects the model generalization ability, decreases consistently. In addition, we also found the number of training epochs to convergence decreases continually. Overall, the results indicate that word-aware segmenter learns faster and fits better on training set, but generalizes relatively poorly."}, {"heading": "4.3 Main Results", "text": "Table 4 compares our final results (greedy search is adopted by setting k=1) to prior neural models. Pre-training character embeddings on largescale unlabeled corpus (not limited to the training corpus) has been shown helpful for extra performance improvement. The results with or without pre-trained character embeddings are listed separately for following the strict closed test setting of SIGHAN Bakeoff in which no linguistic resource other than training corpus is allowed. We also show the state-of-the-art results in (Zhao and Kit, 2008b) of traditional methods. The comparison shows our neural word segmenter outperforms all state-of-the-art neural systems with much less computational cost.\nFinally, we present the results on all four Bakeoff-2005 datasets compared to (Zhao and Kit, 2008c) in Table 5. Note (Zhao and Kit, 2008c) used AV features, which are derived from global\n4To distinguish the performance improvement from model optimization, we especially list the results of standalone neural models in (Zhang et al., 2016) and (Liu et al., 2016). All the running time results are from our runs of released implementations on a single CPU (Intel i7-5960X) with two threads only, except for those of (Zhang et al., 2016) which are from personal communication. The results of (Xu and Sun, 2016) are not listed due to their use of external Chinese idiom dictionary.\nstatistics over entire training corpus in a similar way of unsupervised segmentation (Zhao and Kit, 2008a), for performance enhancement.5 The comparison to their results without AV features show that our neural models for the first time present comparable performance against state-of-the-art traditional ones under strict closed test setting.6"}, {"heading": "5 Conclusion", "text": "In this paper, we presented a fast and accurate word segmenter using neural networks. Our experiments show a significant improvement over existing state-of-the-art neural models by adopting the following key model refinements.\n(1) A novel character-word balanced mechanism for word representation generation. (2) A more efficient model for character composition by dropping unnecessary designs. (3) Early update strategy during max-margin training. (4) With the above modifications, we discover that beam size has little influence on the performance. Actually, greedy search achieves very high accuracy.\nThrough these improvement from both neural models and linguistic motivation, our model becomes simpler, faster and more accurate.7\n5In fact, this kind of features may also be incorporated to our model. We leave it as future work.\n6To our knowledge, none of previous neural models has ever performed a complete evaluation over all four segmentation corpora of Bakeoff-2005, in which only two, PKU and MSR, are used since (Pei et al., 2014).\n7Our code based on Dynet (Neubig et al., 2017) is released at https://github.com/jcyk/greedyCWS."}], "references": [{"title": "A hybrid markov/semi-markov conditional random field for sequence segmentation", "author": ["Galen Andrew."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Sydney, Australia, pages 465\u2013472.", "citeRegEx": "Andrew.,? 2006", "shortCiteRegEx": "Andrew.", "year": 2006}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Neural word segmentation learning for Chinese", "author": ["Deng Cai", "Hai Zhao."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany, pages 409\u2013420.", "citeRegEx": "Cai and Zhao.,? 2016", "shortCiteRegEx": "Cai and Zhao.", "year": 2016}, {"title": "Gated recursive neural network for Chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory neural networks for Chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon,", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume. Barcelona, Spain, pages 111\u2013118.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 22nd international conference on Machine learning. Bonn, Germany, pages 169\u2013176.", "citeRegEx": "III and Marcu.,? 2005", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "The second international Chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. Jeju Island, Korea, pages 123\u2013133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Which is essential for Chinese word segmentation: Character versus word", "author": ["Chang-Ning Huang", "Hai Zhao."], "venue": "The 20th Pacific Asia Conference on Language, Information and Computation. Wuhan, China, pages 1\u201312.", "citeRegEx": "Huang and Zhao.,? 2006", "shortCiteRegEx": "Huang and Zhao.", "year": 2006}, {"title": "Chinese word segmentation: A decade review", "author": ["Changning Huang", "Hai Zhao."], "venue": "Journal of Chinese Information Processing 21(3):8\u201320.", "citeRegEx": "Huang and Zhao.,? 2007", "shortCiteRegEx": "Huang and Zhao.", "year": 2007}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira."], "venue": "Proceedings of the Eighteenth Interntional Conference on Machine Learning. San", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Exploring segment representations for neural segmentation models", "author": ["Yijia Liu", "Wanxiang Che", "Jiang Guo", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. New York, USA, pages 2880\u2013", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A maximum entropy approach to Chinese word segmentation", "author": ["Jin Kiat Low", "Hwee Tou Ng", "Wenyuan Guo."], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing. Jeju Island, Korea, pages 448\u2013455.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "Accurate linear-time Chinese word segmentation via embedding matching", "author": ["Jianqiang Ma", "Erhard Hinrichs."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Ma and Hinrichs.,? 2015", "shortCiteRegEx": "Ma and Hinrichs.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Maxmargin tensor neural network for Chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland,", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum."], "venue": "Proceedings of the 20th international conference on Computational Linguistics. Geneva, Switzerland, pages", "citeRegEx": "Peng et al\\.,? 2004", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "Deep learning for character-based information extraction", "author": ["Yanjun Qi", "Sujatha G Das", "Ronan Collobert", "Jason Weston."], "venue": "Advances in Information Retrieval, pages 668\u2013674.", "citeRegEx": "Qi et al\\.,? 2014", "shortCiteRegEx": "Qi et al\\.", "year": 2014}, {"title": "Word-based and character-based word segmentation models: Comparison and combination", "author": ["Weiwei Sun."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. Beijing, China, pages 1211\u20131219.", "citeRegEx": "Sun.,? 2010", "shortCiteRegEx": "Sun.", "year": 2010}, {"title": "Fast online training with frequency-adaptive learning rates for Chinese word segmentation and new word detection", "author": ["Xu Sun", "Houfeng Wang", "Wenjie Li."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Jeju", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "13th Annual Conference of the International Speech Communication Association. Portland, Oregon, USA, pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22nd international conference on Machine learning. Bonn, Germany, pages 896\u2013903.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "A conditional random field word segmenter for SIGHAN bakeoff 2005", "author": ["Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning."], "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. Jeju Is-", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Two knives cut better than one: Chinese word segmentation with dual decomposition", "author": ["Mengqiu Wang", "Rob Voigt", "Christopher D. Manning."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Dependency-based gated recursive neural network for Chinese word segmentation", "author": ["Jingjing Xu", "Xu Sun."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Berlin, Germany,", "citeRegEx": "Xu and Sun.,? 2016", "shortCiteRegEx": "Xu and Sun.", "year": 2016}, {"title": "Chinese word segmentation as character tagging", "author": ["Nianwen Xue."], "venue": "Computational Linguistics and Chinese Language Processing 8(1):29\u201348.", "citeRegEx": "Xue.,? 2003", "shortCiteRegEx": "Xue.", "year": 2003}, {"title": "Graph-based semi-supervised model for joint Chinese word segmentation and partof-speech tagging", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational", "citeRegEx": "Zeng et al\\.,? 2013", "shortCiteRegEx": "Zeng et al\\.", "year": 2013}, {"title": "Exploring representations from unlabeled data with co-training for Chinese word segmentation", "author": ["Longkai Zhang", "Houfeng Wang", "Xu Sun", "Mairgup Mansur."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, pages 421\u2013431.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Prague, Czech Republic, pages 840\u2013847.", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational linguistics 37(1):105\u2013151.", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "A hybrid model for Chinese spelling check", "author": ["Hai Zhao", "Deng Cai", "Yang Xin", "Wang Yuzhu", "Zhongye Jia."], "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing 16(3).", "citeRegEx": "Zhao et al\\.,? 2017", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}, {"title": "Effective tag set selection in Chinese word segmentation via conditional random field modeling", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "The 20th Pacific Asia Conference on Language, Information and Computation. Wuhan,", "citeRegEx": "Zhao et al\\.,? 2006", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "A unified character-based tagging framework for Chinese word segmentation", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "ACM Transactions on Asian Language Information Processing 16(21).", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}, {"title": "An empirical comparison of goodness measures for unsupervised Chinese word segmentation with a unified framework", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Proceedings of the Third International Joint Conference on Natural Language Processing. Hyder-", "citeRegEx": "Zhao and Kit.,? 2008a", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Exploiting unlabeled text with different unsupervised segmentation criteria for Chinese word segmentation", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "Research in Computing Science 33:93\u2013104.", "citeRegEx": "Zhao and Kit.,? 2008b", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition", "author": ["Hai Zhao", "Chunyu Kit."], "venue": "The Sixth SIGHAN Workshop on Chinese Language Processing. Hyderabad, India,", "citeRegEx": "Zhao and Kit.,? 2008c", "shortCiteRegEx": "Zhao and Kit.", "year": 2008}, {"title": "Integrating unsupervised and supervised word segmentation", "author": ["Hai Zhao", "Chunyu Kit"], "venue": null, "citeRegEx": "Zhao and Kit.,? \\Q2011\\E", "shortCiteRegEx": "Zhao and Kit.", "year": 2011}, {"title": "Deep learning for Chinese word segmentation and POS tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, Washington, USA, pages 647\u2013", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": ", (Zhao et al., 2017).", "startOffset": 2, "endOffset": 21}, {"referenceID": 30, "context": "Since (Xue, 2003), most methods formalize this task as a sequence labeling problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 16, "context": "In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) (Low et al., 2005) and Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 106, "endOffset": 124}, {"referenceID": 14, "context": ", 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004).", "startOffset": 44, "endOffset": 86}, {"referenceID": 21, "context": ", 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004).", "startOffset": 44, "endOffset": 86}, {"referenceID": 6, "context": "(2013) first adapted the sliding-window based sequence labeling (Collobert et al., 2011) with character embeddings", "startOffset": 64, "endOffset": 88}, {"referenceID": 13, "context": ", 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004). However, these models rely heavily on hand-crafted features. \u2217Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), Key Project of National Society Science Foundation of China (No. 15-ZDA041), and the joint research project with Youtu Lab of Tencent. To minimize the efforts in feature engineering, neural word segmentation has been actively studied recently. Zheng et al. (2013) first adapted the sliding-window based sequence labeling (Collobert et al.", "startOffset": 45, "endOffset": 874}, {"referenceID": 43, "context": "A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity.", "startOffset": 73, "endOffset": 93}, {"referenceID": 18, "context": "Pei et al. (2014) introduced tag embeddings.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "Chen et al. (2015a) proposed to model n-", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "(2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context.", "startOffset": 52, "endOffset": 86}, {"referenceID": 3, "context": "Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN", "startOffset": 0, "endOffset": 151}, {"referenceID": 30, "context": "Besides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework.", "startOffset": 35, "endOffset": 55}, {"referenceID": 14, "context": "Liu et al. (2016) used a zero-order semiCRF based model.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "Most closely related to this work, Cai and Zhao (2016) proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word representation generation and an LSTM scoring model for segmentation result evaluation.", "startOffset": 35, "endOffset": 55}, {"referenceID": 12, "context": "Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007).", "startOffset": 67, "endOffset": 89}, {"referenceID": 30, "context": "(Xue, 2003) was the first to cast it as a characterbased tagging problem.", "startOffset": 0, "endOffset": 11}, {"referenceID": 27, "context": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", "startOffset": 55, "endOffset": 172}, {"referenceID": 37, "context": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", "startOffset": 55, "endOffset": 172}, {"referenceID": 41, "context": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", "startOffset": 55, "endOffset": 172}, {"referenceID": 38, "context": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", "startOffset": 55, "endOffset": 172}, {"referenceID": 24, "context": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", "startOffset": 55, "endOffset": 172}, {"referenceID": 32, "context": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", "startOffset": 55, "endOffset": 172}, {"referenceID": 11, "context": "Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion.", "startOffset": 68, "endOffset": 184}, {"referenceID": 0, "context": "Andrew (2006) proposed a semi-CRF model.", "startOffset": 0, "endOffset": 14}, {"referenceID": 15, "context": "Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 33, "context": ", 2016) and (Zhang et al., 2016) respectively.", "startOffset": 12, "endOffset": 32}, {"referenceID": 11, "context": "and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al.", "startOffset": 26, "endOffset": 78}, {"referenceID": 23, "context": "and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al.", "startOffset": 26, "endOffset": 78}, {"referenceID": 28, "context": "and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al.", "startOffset": 26, "endOffset": 78}, {"referenceID": 31, "context": ", 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013).", "startOffset": 36, "endOffset": 102}, {"referenceID": 32, "context": ", 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013).", "startOffset": 36, "endOffset": 102}, {"referenceID": 2, "context": "Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding.", "startOffset": 88, "endOffset": 108}, {"referenceID": 17, "context": "Another notable exception of embedding based methods is (Ma and Hinrichs, 2015), which used character-specified tags matching for fast decoding and resulted in a character-based greedy segmenter.", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "The most straightforward solution is to use a lookup table for word vectors (Bengio et al., 2003).", "startOffset": 76, "endOffset": 97}, {"referenceID": 2, "context": "Figure 2: The difference between (Cai and Zhao, 2016) (left) and our model (right).", "startOffset": 33, "endOffset": 53}, {"referenceID": 2, "context": "In contrast, the model in (Cai and Zhao, 2016) further combined COMP(\u00b7) and character embeddings ci via an update gate z (As in Figure 2), which has been shown helpless to the performance but requires huge computational cost according to our empirical study.", "startOffset": 26, "endOffset": 46}, {"referenceID": 25, "context": "Linking To capture word interactions within a word sequence, the resulted word vectors are then linked sequentially via an LSTM (Sundermeyer et al., 2012).", "startOffset": 128, "endOffset": 154}, {"referenceID": 26, "context": "ods (Taskar et al., 2005) where the structured margin loss is defined as \u03bc times the number of incorrectly segmented characters (Cai and Zhao, 2016).", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": ", 2005) where the structured margin loss is defined as \u03bc times the number of incorrectly segmented characters (Cai and Zhao, 2016).", "startOffset": 110, "endOffset": 130}, {"referenceID": 13, "context": "However, according to (Huang et al., 2012), standard parameter update cannot guarantee conver-", "startOffset": 22, "endOffset": 42}, {"referenceID": 5, "context": "Early update This strategy proposed in (Collins and Roark, 2004) can be simplified into \u201cupdate once the golden answer is unreachable\u201d.", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "We conduct experiments on two popular benchmark datasets, namely PKU and MSR, from the second international Chinese word segmentation bakeoff (Emerson, 2005) (Bakeoff-2005).", "startOffset": 142, "endOffset": 157}, {"referenceID": 8, "context": "3 We follow (Dyer et al., 2015) to train model parameters.", "startOffset": 12, "endOffset": 31}, {"referenceID": 18, "context": "are either randomly initialized or pre-trained by word2vec (Mikolov et al., 2013) toolkit on Chinese Wikipedia corpus (which will be indicated by +pre-train in tables.", "startOffset": 59, "endOffset": 81}, {"referenceID": 17, "context": "In fact, similar phenomenon was observed at character-level (Ma and Hinrichs, 2015).", "startOffset": 60, "endOffset": 83}, {"referenceID": 41, "context": ") (Zhao and Kit, 2008c) 95.", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "6 (Chen et al., 2015a) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 4, "context": "1* 100 120 (Chen et al., 2015b) 94.", "startOffset": 11, "endOffset": 31}, {"referenceID": 17, "context": "0* 117 120 (Ma and Hinrichs, 2015) 95.", "startOffset": 11, "endOffset": 34}, {"referenceID": 33, "context": "6 3 28 (Zhang et al., 2016) 95.", "startOffset": 7, "endOffset": 27}, {"referenceID": 15, "context": "0 - 13 125 (Liu et al., 2016) 93.", "startOffset": 11, "endOffset": 29}, {"referenceID": 2, "context": "21 (Cai and Zhao, 2016) 95.", "startOffset": 3, "endOffset": 23}, {"referenceID": 2, "context": "Results with * are from (Cai and Zhao, 2016).", "startOffset": 24, "endOffset": 44}, {"referenceID": 40, "context": "We also show the state-of-the-art results in (Zhao and Kit, 2008b) of traditional methods.", "startOffset": 45, "endOffset": 66}, {"referenceID": 41, "context": "Finally, we present the results on all four Bakeoff-2005 datasets compared to (Zhao and Kit, 2008c) in Table 5.", "startOffset": 78, "endOffset": 99}, {"referenceID": 41, "context": "Note (Zhao and Kit, 2008c) used AV features, which are derived from global", "startOffset": 5, "endOffset": 26}, {"referenceID": 33, "context": "To distinguish the performance improvement from model optimization, we especially list the results of standalone neural models in (Zhang et al., 2016) and (Liu et al.", "startOffset": 130, "endOffset": 150}, {"referenceID": 15, "context": ", 2016) and (Liu et al., 2016).", "startOffset": 12, "endOffset": 30}, {"referenceID": 33, "context": "All the running time results are from our runs of released implementations on a single CPU (Intel i7-5960X) with two threads only, except for those of (Zhang et al., 2016) which are from personal communication.", "startOffset": 151, "endOffset": 171}, {"referenceID": 29, "context": "The results of (Xu and Sun, 2016) are not listed due to their use of external Chinese idiom dictionary.", "startOffset": 15, "endOffset": 33}, {"referenceID": 41, "context": "Models PKU MSR CityU AS (Zhao and Kit, 2008c) 95.", "startOffset": 24, "endOffset": 45}, {"referenceID": 39, "context": "way of unsupervised segmentation (Zhao and Kit, 2008a), for performance enhancement.", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "To our knowledge, none of previous neural models has ever performed a complete evaluation over all four segmentation corpora of Bakeoff-2005, in which only two, PKU and MSR, are used since (Pei et al., 2014).", "startOffset": 189, "endOffset": 207}], "year": 2017, "abstractText": "Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-toend, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.", "creator": "LaTeX with hyperref package"}}}