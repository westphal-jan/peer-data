{"id": "1704.07069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Evaluating and Modelling Hanabi-Playing Agents", "abstract": "initial agent modelling involves considering how other agents inherently will behave, in order to influence measuring your participants own actions. in repeating this previous paper, later we explore the use of agent modelling in the concept hidden - information, targeting collaborative card data game designer hanabi. we implement instead a number options of incomplete rule - based agents, both from measuring the application literature and of our own independently devising, in obvious addition to an underlying information gap set monte algebra carlo mesh tree search ( is - \u611b mcts ) agent. we would observe poor results from is - mcts, & so construct a new, composite predictor version that uses developing a model of choosing the agents equally with which it ultimately is paired. we collectively observe : a significant marked improvement in game - playing strength derives from this controlling agent in strong comparison to is - ai mcts, resulting from assessing its consideration aspect of what the other causal agents in a game theory would presumably do. in his addition, tonight we essentially create often a flawed anonymous rule - based agent to uniquely highlight matching the predictor's capabilities with assessing such flaws an agent.", "histories": [["v1", "Mon, 24 Apr 2017 07:44:10 GMT  (128kb,D)", "http://arxiv.org/abs/1704.07069v1", "Proceedings of the IEEE Conference on Evolutionary Computation (2017)"]], "COMMENTS": "Proceedings of the IEEE Conference on Evolutionary Computation (2017)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joseph walton-rivers", "piers r williams", "richard bartle", "diego perez-liebana", "simon m lucas"], "accepted": false, "id": "1704.07069"}, "pdf": {"name": "1704.07069.pdf", "metadata": {"source": "CRF", "title": "Evaluating and Modelling Hanabi-Playing Agents", "authors": ["Joseph Walton-Rivers", "Piers R. Williams", "Richard Bartle", "Diego Perez-Liebana", "Simon M. Lucas"], "emails": ["sml}@essex.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTION Hanabi is a co-operative, partially-observable [1] board game which in 2013 won the prestigious Spiel des Jahres award for best board game of the year. For the reasons outlined below, it has featured in a number of recent academic publications. This paper explores whether the use of agent modelling can lead to an improvement in strength for agents playing the game.\nHanabi has a number of interesting features that make it a good choice for research in the field of agent modelling. Firstly, the domain is a co-operative one, in that the agents must work together to achieve a shared goal. This disfavours agents that behave greedily: for example, helping another player score a point is better than playing a risky card that might end the game. Secondly, its rules build in well-defined communication actions. These use a resource that regulates communication and must be managed by the agents. Finally, the game has hidden information, with no one player able to see the entire game state. This is a source of complexity for agents, because imperfect information needs to be reasoned about intelligently. Note that Hanabi has been proven to be NP-Complete even when players have perfect information [2].\nA number of rule-based approaches for designing agents that can play Hanabi have been presented in the literature, however there have been few attempts to employ more general strategies. In this paper, we go some way towards redressing this imbalance. Furthermore, because the use of information about other players\u2019 strategies can help to inform human players in co-operative games, we also explore whether or not such information could help guide our general agents\u2019 decisions.\nSection I-A describes the rules of the game of Hanabi. Section II describes the agents that were implemented to\nplay Hanabi under these rules.\nSection III describes how the agents were tested and evaluated.\nSection IV presents the results of the tests. Section V discusses and explains our findings in the results. Section VII discusses potential future AI-related work in-\nvolving Hanabi."}, {"heading": "A. Hanabi", "text": "Hanabi is a co-operative game in which a team of two to five players attempts to complete five stacks of sequentiallynumbered cards (one for each of the game\u2019s five suits).\nThe game is played with a deck of 50 cards, each possessing a suit and a rank. The suits are coloured white, yellow, green, blue and red. Within each suit, there are three cards of rank 1, two cards each of ranks 2, 3 and 4, and one card of rank 5. The game additionally features two types of token: an information token and a life token. The players collectively start the game with 3 life tokens and 8 information tokens.\nEvery player begins with a randomly-dealt hand of five cards. Cards are held facing away, such that players can\u2019t see the suit or rank of their own cards but can see the suit and rank of the cards held by the other players. The cards not dealt out at the start are placed face down as a draw deck, which will be accessed during play.\nPlay proceeds with each player taking it in turn to perform an action of their choice. There are three different types of action available:\nTell Select a player and point to all their cards of a given number or suit. This costs one information token. Play Choose a card from the player\u2019s own hand and play it. Discard Choose a card from the player\u2019s own hand and add it to the discard pile\nIn a single Tell action, a set of cards can only be identified either by their suit or by their rank \u2014 not by both. Furthermore, cards must be present in the hand to be identified: it is not permitted to state that another player has no cards of a given suit or rank.\nPlaying a card means adding it to the stack with matching suit. It is not required that the player know which stack it belongs to - for example, at the beginning of the game it is acceptable to blindly play a 1 that has been indicated to you. Each card in the stack must be of the correct suit and have a\n978-1-5090-4601-0/17/$31.00 c\u00a92017 IEEE\nar X\niv :1\n70 4.\n07 06\n9v 1\n[ cs\n.A I]\n2 4\nA pr\n2 01\n7\nrank one greater than the card below (except for 1 cards, which are used to start a stack). If a card is played out of sequence, the group loses one life token. Completing a stack of cards associated with a given suit grants an additional information token (if the team does not already have the maximum number, eight).\nDiscarding is only permitted if there is at least one information token to be gained. This means that either a Tell action or a Discard action is always possible.\nAfter either discarding or playing a card, the player draws a replacement card from the draw deck. Discarded cards are visible to all players. Discarding a card increments the number of information tokens up to the maximum. Once all cards in the draw deck have been drawn, all players get one more turn and then the game is considered to be over. The game also ends if the team uses up all of the life tokens.\nScoring is achieved by summing the top card of each stack that has been correctly played. The maximum possible score for the standard game is therefore 25, obtained by completing the stacks for all five suits. Remaining life or information tokens are not counted towards score in the standard game."}, {"heading": "B. Multi-agent domains", "text": "Multi-agent domains can be categorised as either centralised or distributed. A centralised system features a single controller controlling multiple agents; a distributed system has each agent in the world controlled by a separate controller. In this paper, we consider only the distributed approach.\nExisting work in this space includes: attempting to reason about what the other agent knows using answer set programming [3]; iterating on a plan that is communicated between agents [4]; and attempting to use plan recognition to allow one agent to assist another in a planning task [5].\nAnother possibility involves co-operative, multi-agent learning. Within this area, there have been attempts to learn models of teammates in order to make more informed decisions about which action to take. For a review of the literature, see Panait & Luke [6].\nThe use of embedding agent models into Monte Carlo Tree Search (MCTS) has previously been looked at by Barrett et al in the pursuit domain [7]. They made the assumption that all agents except for their modelling agent would be using the same, fixed strategy, and embedded perfect knowledge of this strategies into their agent. One of their findings was that the system did not perform well with models that didn\u2019t represent the behaviour of the agent.\nThe use of Theory of Mind (ToM) (reasoning about what the other agents know and will do in a given situation) has proven useful in competitive games such as Rock Paper Scissors [8]. In these games, higher-order ToM agents were able to outperform lower-order ToM players."}, {"heading": "C. Co-ordination in Hanabi", "text": "In Hanabi all agents have access to different information; because of this, a centralised approach to multi-agent planning would not make sense in this domain as private information must not be shared between agents.\nThe fact that the Tell action has an associated cost (an information token) means that information about a player\u2019s hand needs to be communicated efficiently. Also, because Hanabi players are limited to a set of well-defined communication actions, communication between them is very limited. This makes using communication between agents to co-ordinate their actions a challenging prospect \u2014 which is one reason why Hanabi is increasingly becoming the object of research.\nThe understanding of other players\u2019 strategies forms a core component of a great number of games and has been studied widely [8]. Existing Hanabi research assumes that all agents are playing the same pre-agreed strategy. The ability to reason about the actions that a player would take and their reasons for taking these actions can be used as part of the reasoning process of an agent.\nOur approach is to assume that we have access to a model which, given a state, will be able to return a possible action that an agent would perform in that state; if the agent may make multiple moves, then a single action from the set of possible actions will be returned. Given this model, we are able to incorporate the behaviour of the other agent into our model without understanding of that agent\u2019s reasoning process.\nA point to note is that Tell actions can convey more information than just the obvious: because all cards of a given suit or rank must be identified, cards which are not identified therefore must not satisfy the criterion. This negative information can be used to inform the possible values for a given card. Negative information can add up over a few turns, providing enough information to determine what a card is \u2014 or at least that it is playable. In the end game, such knowledge becomes very powerful.\nHuman players of Hanabi often make additional use of Tell actions. In particular, they can restrict their Tell actions by convention only to identify certain cards as playable. For example, suppose that Player 2 had the hand {(R, 1), (B, 1), . . .} and the current stacks on the table were {(R, 1), (B, 0), (G, 0), (W, 0), (Y, 0)}. Player 1 may elect to tell Player 2 about the suit rather than the number, to avoid identifying the non-playable red card. Player 2 could then infer that the card being identified was indeed a playable card, as they would know that Player 1 would not have identified a non-playable card. As they were told the suit rather than the number, they could further infer that they have a non-playable 1 in their hand (although they would not know the location of this card). The use of information in this way requires an understanding of how the player will use the provided information as part of their policy."}, {"heading": "D. Monte Carlo Tree Search", "text": "MCTS [9] is a widely-used tree-search algorithm that can operate without domain-specific knowledge. This gives MCTS the anytime property: the algorithm can be stopped at any time and can provide an answer for the next move. Given more time, it will typically produce a more accurate answer.\nMCTS proceeds using multiple iterations of the four main stages shown in Figure 1. The iterations typically continue until a predetermined end condition is met, such as running\nout of time. In the selection stage, the current tree is traversed using the tree policy to select the best child of each node. In the expansion stage, a new node is added to the tree. In the main, simulation phase, a simulation (rollout) of future moves is undertaken from the state represented by the new node until an end condition is met. Moves are selected according to the default policy (which is often to select at random from all possible moves). In the backpropagation phase, nodes in the tree that were selected are updated with the result of the rollout."}, {"heading": "E. Monte Carlo Tree Search and Theory of Mind", "text": "Zero-order theory of mind [8] agents are capable of using an agent\u2019s history in order to inform future actions. A firstorder theory of mind agent is capable of using a model of a zero-order theory of mind agent to inform its own future decisions. Our selection of MCTS for use in this domain came from a particular desire to find an algorithm that could be easily modified to operate with predictions of what other agents would do. This makes it a zero-order agent.\nThis approach has been tested before in the Tiny Co-op domain [11] by Walton-Rivers, who found that prediction worked best with a deterministic agent that did as it was instructed [12]. The Tiny Co-op domain is a simple, grid-based world containing a number of agents, goals, doors and buttons. Each agent must visit each goal individually for successful completion. Doors separate different areas in which the agents can move, and each door will only open if an agent is standing on its associated button. This forces the agents to co-operate to succeed overall.\nWhile MCTS was a good performer in Tiny Co-op when paired with itself (and even with random agents), it struggled when trying to co-operate with a particular agent that was designed to follow direction indications. Essentially, this follower agent moved to where it was instructed to move, but MCTS didn\u2019t pick up on this. The root cause was that it didn\u2019t model such behaviour in its search tree, leading to inaccurate states in the majority of the search space. The author added agent modelling to MCTS and found that the performance of MCTS when paired with the follower agent improved significantly. In this paper we used this approach to create a Hanabi-playing agent to assess the effectiveness of agent modelling in this domain."}, {"heading": "F. Previous research", "text": "1) Imperfect Information AI: Games with imperfect information are a complex challenge for AI. Poker is often chosen as an application, because it is a game that many people are familiar with on some level. Poker contains an unusual dynamic for games, as a strong player doesn\u2019t so much play the game as play the opponents. Winning requires a player to understand their opponents and to adopt a strategy that will counter their strengths while exploiting their weaknesses. Rule-based agents feature strongly in this, as do simulationbased agents such as MCTS. Poker has been extensively studied \u2014 see the review conducted by Rubin & Watson [13];\none of their notable finds was that a simulation-based approach is inferior to the formula-based approach, despite expectations.\nWhitehouse et al [14] looked into using MCTS for the card game Dou Di Zhu, which (like Hanabi) also features imperfect information. Here, they apply determinisation and IS-MCTS to the problem and conclude while the IS-MCTS is superior in some cases, no overall difference was observed.\n2) Hanabi AI: There has been a small amount of research into using artificial intelligence techniques to play Hanabi. Osawa [15] devised a number of rule-based agents for the 2-player version of the game, the mechanisms for which are described in Sections II-A2 and II-A3. Osawa found that the incorporation of consideration of the other agent\u2019s strategy and why they did what they did allows an agent to perform better than do the other non-cheating agents.\nCox [16] derives strategies for the game of Hanabi using the hat guessing game as inspiration. The agents all use an agreed encoding strategy to indicate what any particular Tell action specifically means, enabling them to co-operate so as to work around the limited view of their own hands. The encoding strategy does require the 5 player version of the game, however, as it won\u2019t work unless the hand size matches the number of other players in the game. We considered using this agent in the tests, as its unique strategy could have been the perfect test for agent modelling. However, there is an issue with the encoding strategy: every agent must know what every other agent has in their hands. This is cannot be used in agent modelling. If the Predictor IS-MCTS is agent 1, then it has access to the hands of agents 2, 3, 4 and 5 \u2014 which in Hanabi it does indeed have. Unfortunately, its internal copy of agent 2 needs access to the hands of agents 1, 3, 4 and 5. Agent 1 cannot give this information without breaking the rules of Hanabi. For this reason, we did not run tests with this agent.\nVan den Bergh et al [17] analyse Hanabi and define a number of rules for the game. The amount of time it would take to test every possible combination of these rules was too large, however, so they used an iterative approach to explore the search space intelligently. They note that some rules are far more effective than others, as well as observing that a risktaking rule does have some value. They found that the use of a Discard action when there is a possible hint is not optimal. In a follow-up paper [18], the authors present their best rule-based agent along with one using a Monte Carlo search."}, {"heading": "II. AI", "text": "A number of the controllers used in this experiment were implemented as production rule agents. Many of these share individual rules, so each rule will be described here independently. All rules have additional pre-conditions that ensure they can only fire if it is legal to do so within the game rules (for example, a Discard action would necessitate a check that an information token was available). To avoid verbosity, we assume that the rules of Hanabi will be properly followed (so, for example, if the rule says to inform a player about a card, then the player will also be informed about other cards that satisfy the Tell\u2019s stated criterion).\n\u2022 PlaySafeCard: Plays a card only if it is guaranteed that it is playable\n\u2022 OsawaDiscard: Discards a card if it cannot be played at the end of the turn. This will discard cards that we know enough about to disqualify them from being playable. For example, a card with an unknown suit but a rank of 1 will not be playable if all the stacks have been started. This rule also considers cards that can not be played because their pre-requisite cards have already been discarded. \u2022 TellPlayableCard: Tells the next player a random fact about any playable card in their hand. \u2022 TellRandomly: Tells the next player a random fact about any card in their hand. \u2022 DiscardRandomly: Randomly discards a card from the hand. \u2022 TellPlayableCardOuter: Tells the next player an unknown (to that player) fact about any playable card in their hand. \u2022 TellUnknown: Tells the next player an unknown fact about any card in their hand. \u2022 PlayIfCertain: Plays a card if we are certain about which card it is and that it is playable. \u2022 DiscardOldestFirst: Discards the card that has been held in the hand the longest amount of time. \u2022 IfRule(\u03bb) Then (Rule) Else (Rule): Takes a Boolean \u03bb expression and either one or two rules. The first rule will be used if the \u03bb evaluates to true. If it is false, and a second rule was provided, then that will be used instead. \u2022 PlayProbablySafeCard(Threshold \u2208 [0, 1]): Plays the card that is the most likely to be playable if it is at least as probable as Threshold. \u2022 DiscardProbablyUselessCard(Threshold \u2208 [0, 1]): Discards the card that is most likely to be useless if it is at least as probable as Threshold. \u2022 TellMostInformation(New? \u2208 [True, False]): Tells whatever reveals the most information, whether this is the most information in total or the most new information. \u2022 TellDispensable: Tells the next player with an unknown dispensible card the information needed to correctly identify that the card is dispensible. This rule will only target cards that can be identified to the holder as dispensible\nwith the addition of a single piece of information. \u2022 TellAnyoneAboutUsefulCard: Tells the next player\nwith a useful card either the remaining unknown suit of the card or the rank of the card. \u2022 TellAnyoneAboutUselessCard: Tells the next player with a useless card either the remaining unknown suit of the card or the rank of the card."}, {"heading": "A. Agents", "text": "1) Legal Random: This agent makes a move at random from the set of legal actions available to it at any given time step.\n2) Internal: This is a clone of the agent presented by Osawa that shares the same name. It features memory of the information it has been told about its own hand but does not remember information about what other players have been told. The rules used in order are:\n\u2022 PlaySafeCard \u2022 OsawaDiscard \u2022 TellPlayableCard \u2022 TellRandomly \u2022 DiscardRandomly 3) Outer: This is a clone of the agent presented by Osawa with the same name. It features knowledge of what the other agents have been told already, to avoid repeating Tell actions. The rules used in order are:\n\u2022 PlaySafeCard \u2022 OsawaDiscard \u2022 TellPlayableCardOuter \u2022 TellUnknown \u2022 DiscardRandomly 4) Cautious: This is an agent derived from human gameplay. The agent plays cautiously, never losing a life. The rules used in order are:\n\u2022 PlayIfCertain \u2022 PlaySafeCard \u2022 TellAnyoneAboutUsefulCard \u2022 OsawaDiscard \u2022 DiscardRandomly\n5) IGGI: This agent is a modification of Cautious. The alteration to a deterministic Discard function greatly aids the predictability of this player. The rules used in order are:\n\u2022 PlayIfCertain \u2022 PlaySafeCard \u2022 TellAnyoneAboutUsefulCard \u2022 OsawaDiscard \u2022 DiscardOldestFirst 6) Piers: This is an agent designed to use IfRules to improve the overall score. Otherwise, it is similar to IGGI. The rules used in order are:\n\u2022 IfRule (lives > 1 \u2227 \u00acdeck.hasCardsLeft) Then (PlayProbablySafeCard(0.0)) \u2022 PlaySafeCard \u2022 IfRule (lives > 1) Then (PlayProbablySafeCard(0.6)) \u2022 TellAnyoneAboutUsefulCard \u2022 IfRule (information < 4) Then (TellDispensable) \u2022 OsawaDiscard \u2022 DiscardOldestFirst \u2022 TellRandomly \u2022 DiscardRandomly The first IfRule is designed as a hail Mary in the end game: if there is nothing left to lose, try to gain a point. This derives from human play, when typically during the end game we make random plays if we know there is a playable card somewhere in our hand. This rule is more accurate, as it uses all the information it has gathered to calculate probabilities.\nThe second IfRule simply risks playing a card if there is a reasonable chance of its being safe.\nThe third IfRule is designed to try to provide more intelligent Tell conditions. If there is nothing useful to Tell and we are low on information, we set another agent up to be able to discard cards that are not needed. This means that the agents can burn through cards that are not helpful so as to try to obtain useful cards from the deck.\n7) Flawed: This is an agent designed to be intelligent but with some flaws: it does not possess intelligent Tell rules, and has a risky Play rule as well. Understanding this agent is the key to playing well with it, because other agents can give it the information it needs to prevent it from playing poorly. The rules used in order are:\n\u2022 PlaySafeCard \u2022 PlayProbablySafeCard(0.25) \u2022 TellRandomly \u2022 OsawaDiscard \u2022 DiscardOldestFirst \u2022 DiscardRandomly Giving information is the key to getting this agent to work intelligently. Without information, the intelligent rules can\u2019t fire, thereby leaving this agent to Tell randomly and Discard randomly \u2014 not a great strategy.\n8) Van den Bergh Rule: This is the best rule-based agent from [18]. It was created by observing from human play that there are four main tasks:\n\u2022 If I\u2019m certain enough that a card is playable, Play it. \u2022 If I\u2019m certain enough that a card is useless, Discard it. \u2022 Give a hint if possible.\n\u2022 Discard a card.\nVan den Bergh et al used a Genetic Algorithm (GA) to evolve the best options for each section, resulting in the following rules as an implementation:\n\u2022 IfRule (lives > 1) Then (PlayProbablySafeCard(.6)) Else (PlaySafeCard) \u2022 DiscardProbablyUselessCard(1.0) \u2022 TellAnyoneAboutUsefulCard \u2022 TellAnyoneAboutUselessCard \u2022 TellMostInformation \u2022 DiscardProbablyUselessCard(0.0)\n9) MCS: This agent is a simple Monte Carlo Search (MCS) that uses a provided agent for the rollout phase. MCS is a technique that uses the Upper Confidence Bound (UCB) equation to select actions in a single step lookahead, with policy informed rollouts to evaluate those positions. It is essentially MCTS with a tree depth limit of one turn. In this paper, we name the agent MCS-[agent] to indicate which agent provided the rollout policy. For example, a MCS agent using IGGI as a policy would be named MCS-IGGI. The agent has a one-second time limit to return a move.\n10) IS-MCTS: This agent uses a MCTS technique for handling games with partial observability as described in the paper by Cowling et al [19].\nIS-MCTS is a modification to MCTS in which, on each iteration through the tree, the partially-observable game state is determinised into a possible fully-observable state. This state remains consistent for the selection, expansion, rollout and backpropagation phases before being replaced by a new determinisation. The implementation uses a time limit for returning moves of one second per move.\n11) Predictor IS-MCTS: This agent was provided with a copy of each of the agents that it was paired with to use in its prediction. The predicted agents were initialised with random seeds: this corresponds to the predictor\u2019s having knowledge of each agent\u2019s overall strategy but no knowledge of its internal workings.\nThe Predictor IS-MCTS agent modifies the selection, expansion and rollout phases of MCTS when considering nodes for other agents turns. The modifications remove Upper Confidence bound for Trees (UCT) for other agents\u2019 turns and replaces it with a query to the agent model to discover what that agent would do in that situation. The rollout phase is similarly modified. When making moves for its own turn, the predictor agent defaults to the legal random selection method used by IS-MCTS. The implementation maintains the onesecond-per-move limit of IS-MCTS."}, {"heading": "III. METHOD", "text": "A. Validation\nIn order first to validate our framework and AI implementations, we performed experiments using reimplimentations of the Osawa and Van den Bergh agents. This involved recreating the experiments that they described in their papers and checking that we obtained similar results."}, {"heading": "B. Full Test", "text": "The set of agents under test contained a mix of current research on Hanabi as well as some rule-based agents of our own. There is also a mix of strong and poor agents for balance. We tested all the agents from this list:\n\u2022 Legal Random \u2022 Outer \u2022 IGGI \u2022 Piers \u2022 Flawed \u2022 Van den Bergh Rule \u2022 MCS-Legal Random \u2022 MCS-IGGI \u2022 MCS-Flawed \u2022 IS-MCTS \u2022 Predictor IS-MCTS In each experiment, one of the agents was selected from the list above and the remaining agents were selected as a group from the list below. For example, in the first experiment the Legal Random agent would be alone among four IGGI agents \u2014 a concept we call pairing. The agents above were all paired in turn with:\n\u2022 IGGI \u2022 Internal \u2022 Outer \u2022 Legal Random \u2022 Van den Bergh Rule \u2022 Flawed \u2022 Piers 200 random seeds were chosen, and for each seed every agent under test played two games with every agent with which it was paired. It did this for standard Hanabi rules with 2, 3, 4 and 5 players. Each agent under test played from a randomised position (first, second, third, fourth or fifth) determined by the seed. This ensured that each agent under test was in the same position for the same seed. Every agent therefore played 200(nSeeds) \u2217 4(2, 3, 4or5Players) \u2217 7(nAgentPaired) \u2217 2(reruns) = 11200 games.\nThe configuration, final score and other basic state information were logged to a file upon completion of the game. The results were collated per agent and the mean score and number of turns taken were calculated. We also stored additional information about the final state of each game including the number of lives remaining and the information tokens remaining. When there are no lives remaining at the end of the game, this indicates that the players ran out of life tokens.\nThe full (human readable) game traces for each game are also stored, for evaluating agent behaviour and the effectiveness of strategies.\nFinally, the configuration and results of each game are processed to obtain the mean score, mean number of moves per game and the mean remaining life and information tokens."}, {"heading": "IV. RESULTS", "text": "A. Validation\nThe validation results are in Table I. The two Osawa agents obtained similar results in our system to those reported in\nthe original paper. The Van den Bergh Rule agent performed differently, appearing to be somewhat improved in our system."}, {"heading": "B. Full Test", "text": "Table II shows the full results for this test. Predictor ISMCTS outperformed IS-MCTS in this experiment, with an average score of 10.74 versus IS-MCTS\u2019s score of 5.9. MCS typically performed very similarly to the agent it was provided with for its rollouts; little benefit was apparent from using MCS with these agents over simply using their rules in the first place. Overall, Piers performed the best by a slim margin over MCS-IGGI, IGGI and Van den Bergh Rule. The Flawed agent was only a little better than Legal Random."}, {"heading": "V. DISCUSSION", "text": "The Predictor IS-MCTS agent outperformed the IS-MCTS agent. This is mostly due to its better being able to take advantage of the effect of communication actions. As agents cannot see their own hands, the only way they gain information about their hands is via Tell actions; this then informs their decision process. When IS-MCTS appraises the moves of other agents in its tree, it considers all possible outcomes from that state. Some of these states will never occur in the real game because the paired agent would never select that action. The model that is available to Predictor IS-MCTS prunes the search to branches that are likely to occur in the game, resulting in more accurate statistics for the same number of iterations (Figure 2). The more deterministic the model, the lower the branching factor for the tree will be. Smaller branching factors concentrate the rollouts, resulting in potentially more accurate statistics regarding those positions. More accurate statistics should result in more intelligent game play.\nThe Predictor MCTS really shows its benefit with the Flawed agent as its partner. Table III shows each agent when paired with Flawed, with Predictor IS-MCTS in the clear lead ahead of other agents.\nInterestingly, Predictor MCTS\u2019s poor overall score appears to come largely from two-player games, for which it scores significantly lower than usual. This can be explained by the decreased rollout length present in these games. The more players in the game, the fewer random moves will be made in the rollouts (selecting random moves tends to end games very quickly with low scores, as exemplified by Legal Random).\nTable IV shows all the agents\u2019 average scores over each player count. Most agents tend to follow one of two trends: either performing better when there are more players in the game, or performing worse. Those that improve are typically poor players, with each new player added to the game on average being better than them. Those that decline are the opposite: more players added means more poorer players in the team. Predictor IS-MCTS isn\u2019t the only agent to exhibit trouble with two player games, with Outer experiencing some difficulty (despite having been designed for two-player games) and Van den Bergh Rule displaying a more prominent drop in performance. In 3, 4 and 5 player games, the Predictor ISMCTS is the best player from the set of agents."}, {"heading": "VI. CONCLUSION", "text": "In conclusion, we found that agent modelling improves playing strength for tree search algorithms such as MCTS in the game of Hanabi. These results are consistent with the findings of [7]."}, {"heading": "VII. FUTURE WORK", "text": "There is a lot of scope for future work in this area. Hanabi has some additional variants in its rules that focus on the addition of a multi-coloured suit of cards. This suit also contains 3 1\u2019s, 2 2\u2019s, 3\u2019s and 4\u2019s as well as a single 5. The different variants are:\nVariant 1 Add the multi-coloured suit as a sixth suit to the game. Maximum score is boosted to 30.\nVariant 2 Same as Variant 1, but only a single tile of each number from the multi-coloured suit is added to the game.\nVariant 3 The multi-coloured suit now functions as a wild card in Tell actions, and cannot be directly called out. For example, if Player 1 tells Player 2 {(M, 2), (Y, 2), (B, 5), (B, 3)} about all the blues, then cards 1, 3 and 4 will be indicated. With this setup, the multi-coloured cards can only be identified by contradicting information given, requiring 3 pieces of information to fully identify one.\nVariant 1 would be simple to implement and test, but was omitted from this paper as being too off-topic. Variant 2 adds a little extra strategy, but is very similar to Variant 1. Variant 3 would require some additional work to implement, as well as appropriate modifications to the AI agents.\nThe Predictor IS-MCTS has a number of limitations that we aim to address. The agent requires access to an accurate model of the co-operators in advance. It would be better if the agent could instead attempt to learn agent strategies based on observations in the game state. This would lead naturally to a more complicated agent that started with a more generic capability but was able to build models of its team members and update those models as games go on. Testing how much information is needed to learn enough to significantly improve the scores that a team achieve would then need to be done."}, {"heading": "VIII. ACKNOWLEDGEMENTS", "text": "This work was funded by the EPSRC Centre for Doctoral Training in Intelligent Games & Game Intelligence (IGGI) [EP/L015846/1]\nThe authors would like to thank members of the IGGI CDT for their assistance and support playing regular games of Hanabi with us."}], "references": [{"title": "Hanabi is np-complete, even for cheaters who look at their cards", "author": ["J.-F. Baffier", "M.-K. Chiu", "Y. Diez", "M. Korman", "V. Mitsou", "A. van Renssen", "M. Roeloffzen", "Y. Uno"], "venue": "arXiv preprint arXiv:1603.01911, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1911}, {"title": "Using answer set programming to model multi-agent scenarios involving agents\u2019 knowledge about other\u2019s knowledge", "author": ["C. Baral", "G. Gelfond", "T.C. Son", "E. Pontelli"], "venue": "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1, ser. AAMAS \u201910. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems, 2010, pp. 259\u2013266. [Online]. Available: http://dl.acm.org/ citation.cfm?id=1838206.1838243", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Fmap: a heuristic approach to cooperative multi-agent planning", "author": ["A. Torreno", "E. Onaindia", "O. Sapena"], "venue": "Proc. of DMAP Workshop of ICAPS, vol. 13, 2013, pp. 84\u201392.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems, vol. 11, no. 3, pp. 387\u2013434, 2005. [Online]. Available: http: //dx.doi.org/10.1007/s10458-005-2631-2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Empirical evaluation of ad hoc teamwork in the pursuit domain", "author": ["S. Barrett", "P. Stone", "S. Kraus"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2. International Foundation for Autonomous Agents and Multiagent Systems, 2011, pp. 567\u2013574.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "How much does it help to know what she knows you know? an agent-based simulation study", "author": ["H. De Weerd", "R. Verbrugge", "B. Verheij"], "venue": "Artificial Intelligence, vol. 199, pp. 67\u201392, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Progressive strategies for monte-carlo tree search", "author": ["G.M.J. Chaslot", "M.H. Winands", "H.J.V.D. HERIK", "J.W. Uiterwijk", "B. Bouzy"], "venue": "New Mathematics and Natural Computation, vol. 4, no. 03, pp. 343\u2013357, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Monte Carlo Tree Search Applied to Co-operative Problems", "author": ["P.R. Williams", "J. Walton-Rivers", "D. Perez-Liebana", "S.M. Lucas"], "venue": "CEEC2015 - IEEE Conference on Computer Science and Electronic Engineering, ser. IEEE CEEC. IEEE Computer Society, September 2015, pp. 219\u2013224.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Computer poker: A review", "author": ["J. Rubin", "I. Watson"], "venue": "Artificial Intelligence, vol. 175, no. 5, pp. 958\u2013987, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Determinization and information set monte carlo tree search for the card game dou di zhu", "author": ["D. Whitehouse", "E.J. Powley", "P.I. Cowling"], "venue": "2011 IEEE Conference on Computational Intelligence and Games (CIG\u201911). IEEE, 2011, pp. 87\u201394.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Solving hanabi: Estimating hands by opponent\u2019s actions in cooperative game with incomplete information", "author": ["H. Osawa"], "venue": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "How to make the perfect fireworks display: Two strategies for hanabi", "author": ["C. Cox", "J. De Silva", "P. Deorsey", "F.H. Kenter", "T. Retter", "J. Tobin"], "venue": "Mathematics Magazine, vol. 88, no. 5, pp. 323\u2013336, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hanabi, a co-operative game of fireworks", "author": ["M. Van Den Bergh", "F.S. MI", "W. Kosters"], "venue": "2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Information set monte carlo tree search", "author": ["P.I. Cowling", "E.J. Powley", "D. Whitehouse"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 2, pp. 120\u2013143, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Note that Hanabi has been proven to be NP-Complete even when players have perfect information [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "Existing work in this space includes: attempting to reason about what the other agent knows using answer set programming [3]; iterating on a plan that is communicated between agents [4]; and attempting to use plan recognition to allow one agent to assist another in a planning task [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Existing work in this space includes: attempting to reason about what the other agent knows using answer set programming [3]; iterating on a plan that is communicated between agents [4]; and attempting to use plan recognition to allow one agent to assist another in a planning task [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "For a review of the literature, see Panait & Luke [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "The use of embedding agent models into Monte Carlo Tree Search (MCTS) has previously been looked at by Barrett et al in the pursuit domain [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "The use of Theory of Mind (ToM) (reasoning about what the other agents know and will do in a given situation) has proven useful in competitive games such as Rock Paper Scissors [8].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "The understanding of other players\u2019 strategies forms a core component of a great number of games and has been studied widely [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "MCTS [9] is a widely-used tree-search algorithm that can", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Zero-order theory of mind [8] agents are capable of using an agent\u2019s history in order to inform future actions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "This approach has been tested before in the Tiny Co-op domain [11] by Walton-Rivers, who found that prediction worked best with a deterministic agent that did as it was instructed [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "Poker has been extensively studied \u2014 see the review conducted by Rubin & Watson [13]; one of their notable finds was that a simulation-based approach is inferior to the formula-based approach, despite expectations.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Whitehouse et al [14] looked into using MCTS for the card game Dou Di Zhu, which (like Hanabi) also features imperfect information.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Osawa [15] devised a number of rule-based agents for the 2-player version of the game, the mechanisms for which are described in Sections II-A2 and II-A3.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "Cox [16] derives strategies for the game of Hanabi using the hat guessing game as inspiration.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "Van den Bergh et al [17] analyse Hanabi and define a number of rules for the game.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "1: The four steps of Monte Carlo Tree Search [10]", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "10) IS-MCTS: This agent uses a MCTS technique for handling games with partial observability as described in the paper by Cowling et al [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "These results are consistent with the findings of [7].", "startOffset": 50, "endOffset": 53}], "year": 2017, "abstractText": "Agent modelling involves considering how other agents will behave, in order to influence your own actions. In this paper, we explore the use of agent modelling in the hiddeninformation, collaborative card game Hanabi. We implement a number of rule-based agents, both from the literature and of our own devising, in addition to an Information Set-Monte Carlo Tree Search (IS-MCTS) agent. We observe poor results from IS-MCTS, so construct a new, predictor version that uses a model of the agents with which it is paired. We observe a significant improvement in game-playing strength from this agent in comparison to IS-MCTS, resulting from its consideration of what the other agents in a game would do. In addition, we create a flawed rule-based agent to highlight the predictor\u2019s capabilities with such an agent.", "creator": "LaTeX with hyperref package"}}}