{"id": "1704.07221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM", "abstract": "reading this paper describes audit team assess turing's psychological submission undertaken to semeval task 2017 at rumoureval : when determining perceived rumour veracity and locating support points for rumours ( semeval report 2017 task evaluation 8, subtask a ). your subtask construct a addresses the challenge complexity of rumour stance classification, potentially which involves potentially identifying the critical attitude of twitter users towards proving the truthfulness of the internal rumour they actively are discussing. stance angle classification method is considered to'be an externally important step up towards rumour verification, \" therefore currently performing well in this analytical task he is expected to be locally useful successfully in debunking false official rumours. particularly in this statistical work we classify a set of twitter posts and discussing rumours into either seemingly supporting, denying, questioning or refusing commenting agreement on supplying the reason underlying incorrect rumours. we collectively propose enabling a lstm - wide based sequential model prediction that, through smoothly modelling the conversational output structure of specific tweets, which achieves an accuracy of ^ 0. 784 months on compiling the concurrent rumoureval test set weights outperforming all other other systems in subtask a.", "histories": [["v1", "Mon, 24 Apr 2017 13:41:25 GMT  (149kb,D)", "http://arxiv.org/abs/1704.07221v1", "SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A)"]], "COMMENTS": "SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["elena kochkina", "maria liakata", "isabelle augenstein"], "accepted": false, "id": "1704.07221"}, "pdf": {"name": "1704.07221.pdf", "metadata": {"source": "CRF", "title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM", "authors": ["Elena Kochkina", "Maria Liakata", "Isabelle Augenstein"], "emails": ["M.Liakata}@warwick.ac.uk,", "I.Augenstein@ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In stance classification one is concerned with determining the attitude of the author of a text towards a target (Mohammad et al., 2016). Targets can range from abstract ideas, to concrete entities and events. Stance classification is an active research area that has been studied in different domains (Ranade et al., 2013; Chuang and Hsieh, 2015). Here we focus on stance classification of tweets towards the truthfulness of rumours circulating in Twitter conversations in the context of breaking news. Each conversation is defined by a tweet that initiates the conversation and a set of nested replies to it that form a conversation thread. The goal is to classify each of the tweets in the\nconversation thread as either supporting, denying, querying or commenting (SDQC) on the rumour initiated by the source tweet. Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (Zhao et al., 2015). For instance, t has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b).\nHere we focus on exploiting the conversational structure of social media threads for stance classification and introduce a novel LSTM-based approach to harness conversations."}, {"heading": "2 Related Work", "text": "Single Tweet Stance Classification Stance classification for rumours was pioneered by Qazvinian et al. (2011) as a binary classification task (support/denial). Zeng et al. (2016) perform stance classification for rumours emerging during crises. Both works use tweets related to the same rumour during training and testing.\nA model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval2016 task 6 dataset (Augenstein et al., 2016). However the RumourEval task is different as it addresses conversation threads.\nSequential Stance Classification Lukasik et al. (2016) and Zubiaga et al. (2016a) consider the sequential nature of tweet threads in their works. Lukasik et al. (2016) employ Hawkes processes to classify temporal sequences of tweets. They show the importance of using both the textual content and temporal information about the tweets, disregarding the discourse structure. Zubiaga et\nar X\niv :1\n70 4.\n07 22\n1v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nal. (2016a) model the conversational structure of source tweets and subsequent replies: as a linear chain and as a tree. They use linear- and tree- versions of a CRF classifier, outperforming the approach by Lukasik et al. (2016)."}, {"heading": "3 Dataset", "text": "The dataset provided for this task contains Twitter conversation threads associated with rumours around ten different events in breaking news, including the Paris shootings in Charlie Hebdo, the Ferguson unrest, the crash of a Germanwings plane. These events include 325 conversation threads consisting of 5568 underlying tweets annotated for stance at the tweet level (breakdown between training, testing and development sets is shown in Table 1) (Derczynski et al., 2017).\nEach thread includes a source tweet that initiates a conversation and nested tweets responding to either the source tweet or earlier replies. The thread can be split into linear branches of tweets, where a branch is defined as a chain of tweets that starts with a leaf tweet including its direct parent tweets, all the way up to the source tweet. Figure 1 shows an example of a conversation along with its annotations represented as a tree structure with highlighted branches. The depth of a tweet is the number of its parents starting from the root node. Branches 1 and 2 in Figure 1 have depth one whereas branch 3 has depth three. There is a clear class imbalance in favour of commenting tweets (66%) and supporting tweets (18%), whereas the denying (8%) and querying classes (8%) are under-represented (see Table 2). While this imbalance poses a challenge, it is also indicative of the realistic scenario where only a few users question the veracity of a statement."}, {"heading": "4 System Description", "text": ""}, {"heading": "4.1 Features", "text": "Prior to generating features for the tweets, we perform a pre-processing step where we remove nonalphabetic characters, convert all words to lower case and tokenise texts.1 Once tweet texts are preprocessed, we extract the following features: \u2022 Word vectors: we use a word2vec (Mikolov\net al., 2013) model pre-trained on the Google News dataset (300d) 2 using the gensim package (R\u030cehu\u030ar\u030cek and Sojka, 2010). \u2022 Tweet lexicon: (1) count of negation words3\nand (2) count of swear words.4\n1For implementation of all pre-processing routines we use Python 2.7 with the NLTK package.\n2We have also tried using Glove word embeddings trained on Twitter dataset, but it lead to a decrease in performance on both development and testing sets comparing to the Google News word vectors\n3A presence of any of the following words would be considered as a presence of negation: not, no, nobody, nothing, none, never, neither, nor, nowhere, hardly, scarcely, barely, don\u2019t, isn\u2019t, wasn\u2019t, shouldn\u2019t, wouldn\u2019t, couldn\u2019t, doesn\u2019t\n4A list of 458 bad words was taken from http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/\n\u2022 Punctuation: (1) presence of a period, (2) presence of an exclamation mark, (3) presence of a question mark, (4) ratio of capital letters. \u2022 Attachments: (1) presence of a URL and (2)\npresence of images. \u2022 Relation to other tweets (1) Word2Vec\ncosine similarity wrt source tweet, (2) Word2Vec cosine similarity wrt preceding tweet, and (3) Word2Vec cosine similarity wrt thread \u2022 Content length: (1) word count and (2) char-\nacter count. \u2022 Tweet role: whether the tweet is a source\ntweet of a conversation. Tweet representations are obtained by averaging word vectors in a tweet and then concatenating with the additional features into a single vector, at the preprocessing step. This set of features have shown to be the best comparing to using word2vec features on their own or any of the reduced combinations of these features."}, {"heading": "4.2 Branch - LSTM Model", "text": "To tackle the task of rumour stance classificaiton, we propose branch-LSTM, a neural network architecture that uses layers of LSTM units (Hochreiter and Schmidhuber, 1997) to process the whole branch of tweets, thus incorporating structural information of the conversation (see the illustration of the branch-LSTM on the Figure 3). The input at each time step i of the LSTM layer is the representation of the tweet as a vector. We record the\noutput of each time step so as to attach a label to each tweet in a branch5. This output is fed through several dense ReLU layers, a 50% dropout layer, and then through a softmax layer to obtain class probabilities. We use zero-padding and masks to account for the varying lengths of tweet branches. The model is trained using the categorical cross entropy loss function. Since there is overlap between branches originating from the same source tweet, we exclude the repeating tweets from the loss function using a mask at the training stage. The model uses tweet representation as the mean average of word vectors concatenated with extra features described above. Due to the short length of tweets, using more complex models for learning tweet representations, such as an LSTM that takes each word as input at each time step and returns the representation at the final time step, does not lead to a noticeable difference in the performance based on cross-validation experiments on the training and development sets, while taking significantly longer to train. We experimented with replacing the unidirectional LSTMs with bidirectional LSTMs but could observe no improvements in accuracy (using cross-validation results on the training and development set)."}, {"heading": "5 Experimental Setup", "text": "The dataset is split into training, development and test sets by the task organisers. We determined the optimal set of hyperparameters via testing the performance of our model on the development set for different parameter combinations. We used the\n5For implementation of all models we used Python libraries Theano (Bastien et al., 2012) and Lasagne (Dieleman et al., 2015).\nTree of Parzen Estimators (TPE) algorithm 6 to search the parameter space, which is defined as follows: the number of dense ReLU layers varies from one to four; the number of LSTM layers is one or two; the mini-batch size is either 32 or 64; the number of units in the ReLU layer is one of {100, 200, 300, 400, 500}, and in the LSTM layer one of {100, 200, 300}; the strength of the L2 regularisation is one of {0.0, 1e-4, 3e-4, 1e-3} and the number of epochs is selected from {30, 50, 70, 100}. We performed 100 trials of different parameter combinations optimising for accuracy on the development set in order to choose the best combination. We fixed hyperparameters to train the model on combined training and development sets and evaluated on the held out test set."}, {"heading": "6 Results", "text": "The performance of our model on the testing and development set is shown in Table 3. Together with the accuracy we show macro-averaged Fscore and per-class macro-averaged F-scores as these metrics account for the class imbalance. The difference in accuracy between testing and development set is minimal, however we see significant difference in Macro-F score due to different class balance in these sets. Macro-F score could be improved if we used it as a metric for optimising hyper-parameters. The branch-LSTM model predicts commenting, the majority class well, however it is unable to pick out any denying, the mostchallenging under-represented class. Most deny-\n6We used the implementation of the TPE algorithm in the hyperopt package (Bergstra et al., 2013)\ning instances get misclassified as commenting (see Table 5), with only one tweet misclassified as querying and two as supporting (Figure 2). An increased amount of labelled data would be helpful to improve performance of this model. As we were considering conversation branches, it is interesting to analyse the performance distribution across different tweet depths (see Table 4). Maximum depth/branch length in the testing set is 13 with most tweets concentrated at depths from 0 to 3. Source tweets (depth zero) are usually supporting and the model predicts these very well, but performance of supporting tweets at other depths decreases. The model does not show a noticeable difference in performance on tweets of varying lengths."}, {"heading": "7 Conclusions", "text": "This paper describes the Turing system entered in the SemEval-2017 Task 8 Subtask A. Our method decomposes the tree structure of conversations into linear sequences and achieves accuracy 0.784 on the testing set and sets the state-of-the-art for rumour stance classification. In future work we plan to explore different methods for modelling tree-structured conversations."}, {"heading": "Acknowledgments", "text": "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Cloud computing resource were kindly provided through a Microsoft Azure for Research Award. Work by Elena Kochkina was partially supported by the Leverhulme Trust through the Bridges Programme."}], "references": [{"title": "Stance Detection with Bidirectional Conditional Encoding", "author": ["Isabelle Augenstein", "Tim Rockt\u00e4schel", "Andreas Vlachos", "Kalina Bontcheva."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Augenstein et al\\.,? 2016", "shortCiteRegEx": "Augenstein et al\\.", "year": 2016}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5590 .", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures", "author": ["James Bergstra", "Daniel Yamins", "David D Cox."], "venue": "ICML (1) 28:115\u2013123.", "citeRegEx": "Bergstra et al\\.,? 2013", "shortCiteRegEx": "Bergstra et al\\.", "year": 2013}, {"title": "Stance classification on ptt comments", "author": ["Ju-han Chuang", "Shukai Hsieh."], "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation.", "citeRegEx": "Chuang and Hsieh.,? 2015", "shortCiteRegEx": "Chuang and Hsieh.", "year": 2015}, {"title": "Semeval-2017 task 8: Rumoureval: Determining rumour veracity and support for rumours", "author": ["Leon Derczynski", "Kalina Bontcheva", "Maria Liakata", "Arkaitz Zubiaga", "Rob Procter", "Geraldine Wong Sak Hoi."], "venue": "Proceedings of the International Work-", "citeRegEx": "Derczynski et al\\.,? 2017", "shortCiteRegEx": "Derczynski et al\\.", "year": 2017}, {"title": "Pheme: computing veracity: the fourth challenge of big social data", "author": ["Leon Derczynski", "Kalina Bontcheva", "Michal Lukasik", "Thierry Declerck", "Arno Scharl", "Georgi Georgiev", "Petya Osenova", "Toms Pariente Lobo", "Anna Kolliakou", "Robert Stewart"], "venue": null, "citeRegEx": "Derczynski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Derczynski et al\\.", "year": 2014}, {"title": "Lasagne: First release", "author": ["Gbor Tak\u00e1cs", "Peter de Rivaz", "Jon Crall", "Gregory Sanders", "Kashif Rasul", "Cong Liu", "Geoffrey French", "Jonas Degrave."], "venue": "https://doi.org/10.5281/zenodo.27878.", "citeRegEx": "Tak\u00e1cs et al\\.,? 2015", "shortCiteRegEx": "Tak\u00e1cs et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Hawkes processes for continuous time sequence classification: an application to rumour stance classification in twitter", "author": ["Michal Lukasik", "P.K. Srijith", "Duy Vu", "Kalina Bontcheva", "Arkaitz Zubiaga", "Trevor Cohn."], "venue": "Proceedings of the", "citeRegEx": "Lukasik et al\\.,? 2016", "shortCiteRegEx": "Lukasik et al\\.", "year": 2016}, {"title": "Twitter under crisis: Can we trust what we RT? In 1st Workshop on Social Media Analytics", "author": ["Marcelo Mendoza", "Barbara Poblete", "Carlos Castillo."], "venue": "SOMA\u201910, pages 71\u201379.", "citeRegEx": "Mendoza et al\\.,? 2010", "shortCiteRegEx": "Mendoza et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semeval-2016 task 6: Detecting stance in tweets", "author": ["Saif M Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry."], "venue": "Proceedings of the International Workshop on Semantic Evaluation, SemEval. volume 16.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "Reading the riots on twitter: methodological innovation for the analysis of big data", "author": ["Rob Procter", "Farida Vis", "Alex Voss."], "venue": "International journal of social research methodology 16(3):197\u2013214.", "citeRegEx": "Procter et al\\.,? 2013", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Rumor has it: Identifying misinformation in microblogs", "author": ["Vahed Qazvinian", "Emily Rosengren", "Dragomir R Radev", "Qiaozhu Mei."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Qazvinian et al\\.,? 2011", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "Stance classification in online debates by recognizing users\u2019 intentions", "author": ["Sarvesh Ranade", "Rajeev Sangal", "Radhika Mamidi."], "venue": "Proceedings of the SIGDIAL 2013 Conference. pages 61\u201369.", "citeRegEx": "Ranade et al\\.,? 2013", "shortCiteRegEx": "Ranade et al\\.", "year": 2013}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA, Valletta, Malta, pages 45\u201350. http://is.muni.cz/", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "unconfirmed: Classifying rumor stance in crisisrelated social media messages", "author": ["Li Zeng", "Kate Starbird", "Emma S Spiro."], "venue": "Tenth International AAAI Conference on Web and Social Media.", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}, {"title": "Enquiring minds: Early detection of rumors in social media from enquiry posts", "author": ["Zhe Zhao", "Paul Resnick", "Qiaozhu Mei."], "venue": "Proceedings of the 24th International Conference on World Wide Web. ACM, pages 1395\u20131405.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Stance classification in rumours as a sequential task exploiting the tree structure of social media conversations", "author": ["Arkaitz Zubiaga", "Elena Kochkina", "Maria Liakata", "Rob Procter", "Michal Lukasik."], "venue": "Proceedings of COLING, the International Conference", "citeRegEx": "Zubiaga et al\\.,? 2016a", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}, {"title": "Analysing how people orient to and spread rumours in social media by looking at conversational threads", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie."], "venue": "PloS one 11(3):e0150989.", "citeRegEx": "Zubiaga et al\\.,? 2016b", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "In stance classification one is concerned with determining the attitude of the author of a text towards a target (Mohammad et al., 2016).", "startOffset": 113, "endOffset": 136}, {"referenceID": 14, "context": "Stance classification is an active research area that has been studied in different domains (Ranade et al., 2013; Chuang and Hsieh, 2015).", "startOffset": 92, "endOffset": 137}, {"referenceID": 3, "context": "Stance classification is an active research area that has been studied in different domains (Ranade et al., 2013; Chuang and Hsieh, 2015).", "startOffset": 92, "endOffset": 137}, {"referenceID": 17, "context": "Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (Zhao et al., 2015).", "startOffset": 187, "endOffset": 206}, {"referenceID": 9, "context": "For instance, t has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b).", "startOffset": 188, "endOffset": 280}, {"referenceID": 12, "context": "For instance, t has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b).", "startOffset": 188, "endOffset": 280}, {"referenceID": 5, "context": "For instance, t has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b).", "startOffset": 188, "endOffset": 280}, {"referenceID": 19, "context": "For instance, t has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b).", "startOffset": 188, "endOffset": 280}, {"referenceID": 13, "context": "Single Tweet Stance Classification Stance classification for rumours was pioneered by Qazvinian et al. (2011) as a binary classification task (support/denial).", "startOffset": 86, "endOffset": 110}, {"referenceID": 13, "context": "Single Tweet Stance Classification Stance classification for rumours was pioneered by Qazvinian et al. (2011) as a binary classification task (support/denial). Zeng et al. (2016) perform stance classification for rumours emerging during crises.", "startOffset": 86, "endOffset": 179}, {"referenceID": 0, "context": "A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval2016 task 6 dataset (Augenstein et al., 2016).", "startOffset": 155, "endOffset": 180}, {"referenceID": 8, "context": "Sequential Stance Classification Lukasik et al. (2016) and Zubiaga et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 8, "context": "Sequential Stance Classification Lukasik et al. (2016) and Zubiaga et al. (2016a) consider the sequential nature of tweet threads in their works.", "startOffset": 33, "endOffset": 82}, {"referenceID": 8, "context": "Sequential Stance Classification Lukasik et al. (2016) and Zubiaga et al. (2016a) consider the sequential nature of tweet threads in their works. Lukasik et al. (2016) employ Hawkes processes to classify temporal sequences of tweets.", "startOffset": 33, "endOffset": 168}, {"referenceID": 8, "context": "They use linear- and tree- versions of a CRF classifier, outperforming the approach by Lukasik et al. (2016).", "startOffset": 87, "endOffset": 109}, {"referenceID": 4, "context": "These events include 325 conversation threads consisting of 5568 underlying tweets annotated for stance at the tweet level (breakdown between training, testing and development sets is shown in Table 1) (Derczynski et al., 2017).", "startOffset": 202, "endOffset": 227}, {"referenceID": 10, "context": "1 Once tweet texts are preprocessed, we extract the following features: \u2022 Word vectors: we use a word2vec (Mikolov et al., 2013) model pre-trained on the Google News dataset (300d) 2 using the gensim package (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 106, "endOffset": 128}, {"referenceID": 15, "context": ", 2013) model pre-trained on the Google News dataset (300d) 2 using the gensim package (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 87, "endOffset": 112}, {"referenceID": 7, "context": "To tackle the task of rumour stance classificaiton, we propose branch-LSTM, a neural network architecture that uses layers of LSTM units (Hochreiter and Schmidhuber, 1997) to process the whole branch of tweets, thus incorporating structural information of the conversation (see the illustration of the branch-LSTM on the Figure 3).", "startOffset": 137, "endOffset": 171}, {"referenceID": 1, "context": "For implementation of all models we used Python libraries Theano (Bastien et al., 2012) and Lasagne (Dieleman et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 2, "context": "We used the implementation of the TPE algorithm in the hyperopt package (Bergstra et al., 2013) Label Prediction C D Q S", "startOffset": 72, "endOffset": 95}], "year": 2017, "abstractText": "This paper describes team Turing\u2019s submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.", "creator": "LaTeX with hyperref package"}}}