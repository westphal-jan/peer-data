{"id": "1704.07329", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "A Trie-Structured Bayesian Model for Unsupervised Morphological Segmentation", "abstract": "in this paper, lets we essentially introduce into a trie - parallel structured quantitative bayesian model for calculating unsupervised novel morphological segmentation. we adopt prior information from different sources in the computational model. we use linguistic neural diffusion word embeddings to discover words that together are morphologically derived and from each unrelated other and so thereby predict that vowels are semantically similar.... we use letter successor derived variety counts frequently obtained principally from lexi tries entities that are correctly built by these neural word embeddings. our results show that using different information across sources properties such as neural word medial embeddings and letter successor variety as prior information improves reading morphological segmentation helpful in obtaining a phylogenetic bayesian model. our model outperforms amongst other modern unsupervised vocabulary morphological segmentation models present on contemporary turkish and serbian gives increasingly promising technical results obtained on english and german ways for recovering scarce local resources.", "histories": [["v1", "Mon, 24 Apr 2017 17:07:26 GMT  (400kb,D)", "http://arxiv.org/abs/1704.07329v1", "12 pages, accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics"]], "COMMENTS": "12 pages, accepted and presented at the CICLING 2017 - 18th International Conference on Intelligent Text Processing and Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["murathan kurfal{\\i}", "ahmet \\\"ust\\\"un", "burcu can"], "accepted": false, "id": "1704.07329"}, "pdf": {"name": "1704.07329.pdf", "metadata": {"source": "CRF", "title": "A Trie-Structured Bayesian Model for Unsupervised Morphological Segmentation", "authors": ["Murathan Kurfal\u0131", "Ahmet \u00dcst\u00fcn", "Burcu Can"], "emails": ["kurfali@metu.edu.tr", "ustun.ahmet@metu.edu.tr", "burcucan@cs.hacettepe.edu.tr"], "sections": [{"heading": null, "text": "Keywords: unsupervised learning, morphology, morphological segmentation, Bayesian learning"}, {"heading": "1 Introduction", "text": "Morphological segmentation is the task of segmenting words into their meaningful units called morphemes. For example, the word transformations is split into trans, form, ation, and s. This process serves mainly as a preprocessing task in many natural language processing (NLP) applications such as information retrieval, machine translation, question answering, etc. This process is essential because sparsity becomes crucial in those NLP applications due to morphological generation that produces various word forms from a single root. It is infeasible to build a dictionary that involves all possible word forms in a language in order to use in an NLP application. Hankamer [14] suggests that the number of possible word forms in an agglutinative language such as Turkish is infinite. Therefore, instead of building a model based on word forms, morphological segmentation is applied to reduce the sparsity principally in any NLP application. ar X\niv :1\n70 4.\n07 32\n9v 1\n[ cs\n.C L\n] 2\nVarious features have been used for morphological segmentation. Many approaches use orthographic features. However, morphology is tightly connected with syntax and semantics. Syntactic and semantic features have also been used for the segmentation task.\nFeatures are normally used in Bayesian models in the form of a prior distribution. For example, [7] utilize frequency and length information of morphemes as prior information, which provide some orthographic features.\nIn this paper, we aggregate prior information from different sources in morphological segmentation within a Bayesian framework. We use orthographic features such as letter successor variety (LSV) counts obtained from tries, semantic information obtained from the neural word embeddings [17] to measure the semantic relatedness between substrings of a word, and we use the presence information of a stem in a dataset after its suffixes are stripped off assuming a concatenative morphology. Our results show that combining prior information from different sources give promising results in unsupervised morphological segmentation.\nIn this study, we learn tries based on semantic and orthographic features. Therefore, the output of our model is not only segmentation, but also tries that are composed of semantically and morphologically related words.\nThe paper is organized as follows: Section 2 presents the previous work on unsupervised morphological segmentation, section 3 defines the mathematical model, section 5 describes the inference algorithm to learn the mathematical model, section 7 presents the experimental results, and finally section 8 concludes the paper with a discussion and potential future work."}, {"heading": "2 Related Work", "text": "Morphological segmentation, as one of the oldest fields in NLP, has been excessively studied. Deterministic methods are the oldest ones used in morphological segmentation. Harris [15] defines the distributional characteristics of letters in a word for the first time for unsupervised morphological segmentation. LSV model is named after Harris, which defines the morpheme boundaries based on letter successor counts. If words are inserted into a trie, branches correspond to potential morpheme boundaries. An example is given in Figure 1. In the example, re- is a potential prefix, and -s, -ed and -ing are potential suffixes in the trie due to branching that emerges before those morphemes. LSV model has been applied in various works [13,11,1,2,3]. In our study, we also use a LSV-inspired prior information, but this time in a Bayesian framework.\nStochastic methods have also been extensively used in unsupervised morphological segmentation. Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9]. Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].\nNeural-inspired features are used in the recent studies. Narasimhan et al. [18] use semantic similarity obtained from neural word embeddings by word2vec [17].\nNarasimhan et al. [18] adopt the semantic similarity as a feature in a log-linear model. Soricut and Och [20] use word embeddings to learn morphological rules in an unsupervised setting.\nIn this work, we are both inspired by the oldest works and the recent works in terms of various features used. Thus our model has inspirations from the LSV methods, stochastic methods, and neural-based models in a combined framework."}, {"heading": "3 Building Neural Word Embedding-based Tries", "text": "Our model is based on neural word embedding-based tries that are built by using two different methods:"}, {"heading": "3.1 Tries Structured from the Same Stem", "text": "These tries contain semantically related (morphologically derived or inflected from each other) words having the same stem. In order to find the stem of a given word in the training set, we used the algorithm which is introduced in [21]. In the algorithm, all potential prefixes of a word are extracted. For example, fe, fea, fear, fearf, fearfu, fearful, fearfull, fearfully are the prefixes of fearfully. The rightmost segmentation point where the cosine similarity between the word and the first prefix (from the right of the word; i.e. fearful) is higher than a manually set threshold3 gives the first valid prefix, which refers to the first segmentation point.\nOther segmentation points are found by repeating the process towards the head of the word by checking the cosine similarity between the just detected valid prefix and the subsequent prefix to the left of the word. The final detected\n3 We assign 0.25 as the threshold following [21].\nprefix with the leftmost segmentation point in the word becomes the stem of the word.\nAmong the nearest 50 neighbors of the stem which are obtained from word2vec [17], the ones that begin with the same stem are inserted to the same trie. This process is repeated for each word that is inserted on the trie recursively until all the words that are semantically similar which share the same stem (detected by using the same algorithm described above) are covered. An example trie that is built with the words having the same stem is given in Figure 2."}, {"heading": "3.2 Tries Based on Semantic Relatedness", "text": "Semantically related 50 words are retrieved for each word in the training set by using word2vec [17]. For each word, a trie is built and 50 similar words are inserted on the word\u2019s trie. Eventually, a trie that consists of 51 words is created for each word in the training set. A portion of a trie that involves semantically related words is given in Figure 3."}, {"heading": "4 Bayesian Model Definition", "text": "We define a Bayesian model in order to find the morpheme boundaries on the tries: p(Model|Corpus) \u221d p(Corpus|Model)p(Model) (1) where Corpus is a list of raw words and Model denotes the segmentation of the corpus. The Model that maximizes the given posterior probability will be searched for the segmentation task. We apply a unigram model for the likelihood:\np(Corpus|Model) = |W |\u220f i p(wi = (mi1 +mi2 + \u00b7 \u00b7 \u00b7+miti |Model)\n= |W |\u220f i ti\u220f j=1 p(mij |Model) (2)\nwhere wi is the ith word in Corpus = {w1, \u00b7 \u00b7 \u00b7 , w|W |}, mij is the jth morpheme in wi, ti is the number of morphemes in word wi, and |W | is the number of words in the corpus. Here, morphemes are generated by a Dirichlet Process (DP) as follows:\nmij \u221d DP (\u03b1,H) (3)\nwith the concentration parameter \u03b1 and the base distribution H that is formed with a geometric distribution:\nH(mij) = \u03b3 |mij |+1 (4)\nHere, |mij | is the length of mij and \u03b3 is the parameter of the geometric distribution. We assume that each letter is uniformly distributed. Therefore, we assign \u03b3 = 1/L where L denotes the size of the alphabet in the language. Shorter morphemes will be favored with the usage of length-inspired base distribution in the DP. From the Chinese Restaurant Process (CRP) perspective, each morpheme is generated proportionally to the number of morphemes of the same type that have already been generated (i.e. customers having the same dish):\np(mij = k|Model) = nk + \u03b1H(k)\nN + \u03b1 (5)\nThis computes the probability of mij being of type k where k refers to a distinct morpheme (i.e. morpheme type). Here, nk is the number of morphemes of type k and N is the total number of morpheme tokens in the model. We generate each morpheme regardless of its type, such as stem, prefix, or suffix.\nAs for the prior information, we model the morpheme boundaries:\np(Model) = |W |\u220f i ti\u220f j=1 p(bij) (6)\nHere, bij refers to the jth morpheme boundary in wi = mi1 + mi2 + \u00b7 \u00b7 \u00b7 + miti where wi = {bi1, bi2, \u00b7 \u00b7 \u00b7 , biti}.\nThe probability of each bij is decomposed in terms of the number of branches leaving that node (when inserted on the trie), semantic similarity that is introduced between the two word forms that is split with bij , and the presence of the word form once the suffix is stripped off from the word:\np(bij) = p(bijbranch).p(bijsemantics).p(bijpresence)\nwhere p(bijbranch) denotes the probability of bij being a morpheme boundary based on the branches leaving the trie node, p(bijsemantics) is based on the semantic similarity of the two word forms where bij separates the two forms, and p(bijpresence) is estimated based on the word form whether it exists in the corpus once the suffix after bij is stripped off.\nBased on the LSV, the branching on the tries corresponds to the potential morpheme boundaries. We model the branching with a Poisson distribution:\np(bijbranch) = p(zij = l|\u03bb) (7)\n= \u03bble\u2212\u03bb\nl! (8)\nwhere zij = l denotes the number of branches leaving the node below bij and \u03bb is the parameter of the Poisson distribution4.\nWe use the cosine similarity (which is always between 0 and 1) between the neural word embeddings of the two word forms that are separated by bij for the semantic distribution:\np(bijsemantics) = cos(xmi1+\u00b7\u00b7\u00b7+mij , xmi1+\u00b7\u00b7\u00b7+mij+1) (9)\nHere, xmi1+\u00b7\u00b7\u00b7+mij corresponds to the word vector of the word formmi1+\u00b7 \u00b7 \u00b7+mij obtained from word2vec. It is the full word vector and not the compositional vector obtained from morpheme vectors.\nAs for the presence of the word form in the word list, we compute the likelihood of the word form mi1 + \u00b7 \u00b7 \u00b7+mij :\np(bijpresence) = f(mi1 + \u00b7 \u00b7 \u00b7+mij)\u2211|Corpus|\ni=1 f(wi) (10)\nwhere f(mi1 + \u00b7 \u00b7 \u00b7+mij) denotes the frequency of the word form in the corpus."}, {"heading": "5 Inference", "text": "We use Gibbs sampling [6] for the inference. In each iteration, a word is uniformly selected from any trie and removed from the corpus. A binary segmentation of the word is sampled from the given posterior distribution:\np(wi = mi1 +mi2|Corpus\u2212wi ,Model\u2212wi , \u03b1, \u03bb, \u03b3) \u221d p(mi1|Model\u2212wi , \u03b1, \u03b3)p(mi2|Model\u2212wi , \u03b1, \u03b3)p(bi1) (11)\nOnce a binary segmentation is sampled, another binary segmentation is sampled for mi1. Therefore, a left-recursion is applied for the left part of the word. This is because of the cosine similarity that is computed between neural word embeddings of word forms and not suffixes by the original word2vec.\nThis process is repeated recursively until having at least 4 letters in the stem or having sampled the word itself from the posterior distribution (i.e. when the word is not segmented). An illustration is given in Figure 4.\n4 In the experiments, we assign \u03bb = 4."}, {"heading": "6 Segmentation", "text": "Once the model is learned, any unseen word can be segmented by using the learned model. Each word is split based on the maximum likelihood in the learned model:\narg max mi1,\u00b7\u00b7\u00b7 ,miti p(wi = mi1 + \u00b7 \u00b7 \u00b7+miti |Model, \u03b1, \u03b3) (12)\nFor the segmentation, we apply two different strategies. In both methods, we select the segmentation with the maximum likelihood, however the set of possible segmentations for the given word differs. In the first method, we only consider the segmentations learned by the model. Since the same word can exist in multiple tries, a word may have more than one different segmentation. In the second method, we consider all possible segmentations of a word and choose the one with the maximum likelihood."}, {"heading": "7 Experiments and Results", "text": "We did experiments on Turkish, English and German. For each language, we built two sets of tries based on the methods described in Section 3.1 and Section 3.2. We aggregated the publicly available training and development sets provided by Morpho Challenge 2010 [16] for English, Turkish and German for training. Although gold segmentations are provided in the datasets, we only used the raw words in training. Gold segmentations were only used for evaluation purposes.\nWe began with 1686 English words, 1760 Turkish words, and 1779 German words obtained from the aggregated sets. Once the tries have been built by recursively augmenting the tries by using word2vec[17], eventually we obtained 2560 English word types, 43884 Turkish word types, and 13747 German word types in the tries structured from similar stems (see Section 3.1). Additionally, we obtained 34594 English word types, 67292 Turkish word types, and 23875 German types in the tries that were built based on the semantic relatedness (see Section 3.2).\nWe used 200-dimensional word embeddings that were obtained by training word2vec [17] on 361 million word tokens and 725.000 word types in Turkish, 129 million word tokens and 218.000 word types in English, and 651 million word\ntokens and 608.000 word types in German. The size of all datasets used in the experiments are given in Table 1.\nWe compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18]. For that purpose, we trained these models on the same training sets. We obtained the frequency information from the full word lists provided by Morpho Challenge which was need by other systems. The evaluation was performed on the aggregated training and development sets of Morpho Challenge 2010 using the Morpho Challenge evaluation method [16]. All word pairs that have a common morpheme are extracted from the results and checked whether they really share a common morpheme in the gold standard data. One point is given for each correct pair. The Precision is the proportion of the collected points to the total number of words. Recall is computed analogously. This time all word pairs that share a common morpheme are extracted from the gold standard data and checked whether they have a common morpheme in the results. For each correct pair, one point is given. Finally, the Recall is the proportion of the collected points is to the total number of words.\nThe results are given in Table 2 and Table 3 for tries that are composed of words structured from the same stem (see Section 3.1) and for tries that are based on semantic relatedness (see Section 3.2). According to the results, tries that contain semantically similar words achieve a better performance on morphological segmentation proving that semantically similar words also manifest similar syntactic and thus similar morphological features.\nOur trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.16 on the tries based on semantic relatednesss. We obtained a F-measure of %39.89 for Turkish from the tries structured from the same stem, which is poorer than the other method. This shows that for morphologically rich languages, semantic relatedness plays a more important role in segmentation. That is because of the sparseness of the word forms in morphologically rich languages. Here we overcome the sparsity problem with semantic information that is used in semantically built tries.\nOur TST model performs better on the tries structured from the same stem on English with a F-measure of %54.70 compared to the tries based on semantic relatedness, which has a F-measure of %51.81. Since English is not a morphologically rich language, obtaining the correct stem plays an important role in segmenting the word. Words usually do not have more than one suffix and therefore finding the stem is normally sufficient to do morphological segmentation in morphologically poor languages such as English.\nOur German results are close to each other obtained from two types of tries. We obtain a F-measure of %38.61 from the tries based on semantic relatedness and it performs better than Morfessor CatMAP and Morphological Chain. The F-measure is %37.33 on German, which is obtained from the tries structured from the same stem.\nThe results also show that Morfessor CatMAP suffers from sparsity in small datasets (especially in English), whereas our trie-structured model learns also well in small datasets."}, {"heading": "8 Conclusion and Future Work", "text": "We propose a Bayesian model that utilizes semantically built trie structures that are built by using neural word embeddings (i.e. obtained from word2vec [17]) for morphological segmentation in an unsupervised setting. The current study constitutes the first part of the on-going project which in the end aims to learn part-of-speech tags and morphological segmentation jointly. To this end, the fact that the tries having semantically related words achieves the best performance paves the way of using semantically similar words in learning syntactic features.\nMoreover, considering the resource-scarce languages like Turkish, our triestructured model shows a good performance on comparably smaller datasets. In comparison to other available systems, our model outperforms them in spite of the limited training data. This shows that the small size of data can be compensated to a certain extent with structured data, that is the main contribution of this paper."}, {"heading": "Acknowledgments", "text": "This research is supported by the Scientific and Technological Research Council of Turkey (TUBITAK) with the project number EEEAG-115E464."}], "references": [{"title": "Unsupervised knowledge-free morpheme boundary detection", "author": ["S. Bordag"], "venue": "Proceedings of the RANLP 2005", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Two-step approach to unsupervised morpheme segmentation", "author": ["S. Bordag"], "venue": "Proceedings of 2nd Pascal Challenges Workshop. pp. 25\u201329", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised and knowledge-free morpheme segmentation and analysis", "author": ["S. Bordag"], "venue": "Advances in Multilingual and Multimodal Information Retrieval pp. 881\u2013891", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Statistical Models for Unsupervised Learning of Morphology and POS tagging", "author": ["B. Can"], "venue": "Ph.D. thesis, Department of Computer Science, The University of York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic hierarchical clustering of morphological paradigms", "author": ["B. Can", "S. Manandhar"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. pp. 654\u2013663. EACL \u201912, Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Explaining the Gibbs sampler", "author": ["G. Casella", "E.I. George"], "venue": "The American Statistician 46(3), 167\u2013174", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised segmentation of words using prior distributions of morph length and frequency", "author": ["M. Creutz"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. pp. 280\u2013287. Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised discovery of morphemes", "author": ["M. Creutz", "K. Lagus"], "venue": "Proceedings of the ACL-02 workshop on Morphological and phonological learning. pp. 21\u201330. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["M. Creutz", "K. Lagus"], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR 2005). pp. 106\u2013113", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Transactions on Speech Language Processing 4, 1\u201334", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Morphemes as necessary concept for structures discovery from untagged corpora", "author": ["H. D\u00e9jean"], "venue": "Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning. pp. 295\u2013298. Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["S. Goldwater", "M. Johnson", "T.L. Griffiths"], "venue": "Advances in Neural Information Processing Systems 18, pp. 459\u2013466. MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Word segmentation by letter successor varieties", "author": ["M.A. Hafer", "S.F. Weiss"], "venue": "Information Storage and Retrieval 10(11-12), 371 \u2013 385", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1974}, {"title": "Finite state morphology and left to right phonology", "author": ["J. Hankamer"], "venue": "Proceedings of the West Coast Conference on Formal Linguistics. vol. 5, pp. 41\u201352", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "From phoneme to morpheme", "author": ["Z.S. Harris"], "venue": "Language 31(2), 190\u2013222", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1955}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "An unsupervised method for uncovering morphological chains", "author": ["K. Narasimhan", "R. Barzilay", "T.S. Jaakkola"], "venue": "Transactions of the Association for Computational Linguistics 3, 157\u2013167", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["B. Snyder", "R. Barzilay"], "venue": "Proceedings of ACL-08: HLT. pp. 737\u2013745. Association for Computational Linguistics", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["R. Soricut", "F. Och"], "venue": "Proceedings of the Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL. pp. 1627\u20131637. Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised morphological segmentation using neural word embeddings", "author": ["A. \u00dcst\u00fcn", "B. Can"], "venue": "Statistical Language and Speech Processing: 4th International Conference, SLSP 2016, Pilsen, Czech Republic, October 11-12, 2016, Proceedings. pp. 43\u201353. Springer International Publishing", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Hankamer [14] suggests that the number of possible word forms in an agglutinative language such as Turkish is infinite.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "For example, [7] utilize frequency and length information of morphemes as prior information, which provide some orthographic features.", "startOffset": 13, "endOffset": 16}, {"referenceID": 15, "context": "We use orthographic features such as letter successor variety (LSV) counts obtained from tries, semantic information obtained from the neural word embeddings [17] to measure the semantic relatedness between substrings of a word, and we use the presence information of a stem in a dataset after its suffixes are stripped off assuming a concatenative morphology.", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "Harris [15] defines the distributional characteristics of letters in a word for the first time for unsupervised morphological segmentation.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 10, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 0, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 1, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 2, "context": "LSV model has been applied in various works [13,11,1,2,3].", "startOffset": 44, "endOffset": 57}, {"referenceID": 7, "context": "Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9].", "startOffset": 123, "endOffset": 131}, {"referenceID": 9, "context": "Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9].", "startOffset": 123, "endOffset": 131}, {"referenceID": 8, "context": "Morfessor is the name of the family of a group of unsupervised morphological segmentation systems which are all stochastic [8,10,9].", "startOffset": 123, "endOffset": 131}, {"referenceID": 11, "context": "Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].", "startOffset": 84, "endOffset": 93}, {"referenceID": 17, "context": "Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].", "startOffset": 84, "endOffset": 93}, {"referenceID": 4, "context": "Non-parametric Bayesian models have also been applied in morphological segmentation [12,19,5].", "startOffset": 84, "endOffset": 93}, {"referenceID": 16, "context": "[18] use semantic similarity obtained from neural word embeddings by word2vec [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] use semantic similarity obtained from neural word embeddings by word2vec [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "Potential morpheme boundaries on a trie used in LSV model [4]", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "[18] adopt the semantic similarity as a feature in a log-linear model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Soricut and Och [20] use word embeddings to learn morphological rules in an unsupervised setting.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "In order to find the stem of a given word in the training set, we used the algorithm which is introduced in [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "25 as the threshold following [21].", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Among the nearest 50 neighbors of the stem which are obtained from word2vec [17], the ones that begin with the same stem are inserted to the same trie.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "Semantically related 50 words are retrieved for each word in the training set by using word2vec [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "We use Gibbs sampling [6] for the inference.", "startOffset": 22, "endOffset": 25}, {"referenceID": 15, "context": "Once the tries have been built by recursively augmenting the tries by using word2vec[17], eventually we obtained 2560 English word types, 43884 Turkish word types, and 13747 German word types in the tries structured from similar stems (see Section 3.", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "We used 200-dimensional word embeddings that were obtained by training word2vec [17] on 361 million word tokens and 725.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "We compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18].", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "We compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18].", "startOffset": 81, "endOffset": 84}, {"referenceID": 16, "context": "We compared our model with Morfessor Baseline [8] (M-Baseline), Morfessor CatMap [9] (M-CatMAP) and MorphoChain System [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "Our trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Our trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.", "startOffset": 94, "endOffset": 97}, {"referenceID": 16, "context": "Our trie-structured model (TST) performs better than Morfessor Baseline [8], Morfessor CatMAP [9] and Morphological Chain [18] on Turkish with a Fmeasure of %44.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "obtained from word2vec [17]) for morphological segmentation in an unsupervised setting.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "In this paper, we introduce a trie-structured Bayesian model for unsupervised morphological segmentation. We adopt prior information from different sources in the model. We use neural word embeddings to discover words that are morphologically derived from each other and thereby that are semantically similar. We use letter successor variety counts obtained from tries that are built by neural word embeddings. Our results show that using different information sources such as neural word embeddings and letter successor variety as prior information improves morphological segmentation in a Bayesian model. Our model outperforms other unsupervised morphological segmentation models on Turkish and gives promising results on English and German for scarce resources.", "creator": "LaTeX with hyperref package"}}}