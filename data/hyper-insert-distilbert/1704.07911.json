{"id": "1704.07911", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car", "abstract": "as part time of a nationally complete software stack for autonomous driving, nvidia has created a neural - network - network based system, again known as passive pilotnet, which outputs steering sway angles given sequential images of the remote road ahead. pilotnet is effectively trained with using road location images paired with the steering angles generated predominantly by shooting a human, driving a data - collection car. it indirectly derives all the necessary domain knowledge obtained by observing human towing drivers. computing this eliminates the need for modern human engineers today to readily anticipate what mapping is uniquely important in an image and thoroughly foresee all the detailed necessary minimum rules for safe trailer driving. prototype road control tests demonstrated demonstrated that pilotnet can successfully digitally perform smart lane lock keeping mapping in case a wide varying variety of driving conditions, normally regardless level of whether lane markings are present or not.", "histories": [["v1", "Tue, 25 Apr 2017 21:25:41 GMT  (3943kb,D)", "http://arxiv.org/abs/1704.07911v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE cs.RO", "authors": ["mariusz bojarski", "philip yeres", "anna choromanska", "krzysztof choromanski", "bernhard firner", "lawrence jackel", "urs muller"], "accepted": false, "id": "1704.07911"}, "pdf": {"name": "1704.07911.pdf", "metadata": {"source": "CRF", "title": "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car", "authors": ["Mariusz Bojarski"], "emails": ["64@1x18", "64@3x20", "48@5x22", "36@14x47", "24@31x98", "3@66x200"], "sections": [{"heading": "1 Introduction", "text": "A previous report [1] described an end-to-end learning system for self-driving cars in which a convolutional neural network (CNN) [2] was trained to output steering angles given input images of the road ahead. This system is now called PilotNet. The training data were images from a front-facing camera in a data collection car coupled with the time-synchronized steering angle recorded from a human driver. The motivation for PilotNet was to eliminate the need for hand-coding rules and instead create a system that learns by observing. Initial results were encouraging, although major improvements are required before such a system can drive without the need for human intervention. To gain insight into how the learned system decides what to do, and thus both enable further system improvements and create trust that the system is paying attention to the essential cues for safe steering, we developed a simple method for highlighting those parts of an image that are most salient\nar X\niv :1\n70 4.\n07 91\n1v 1\n[ cs\n.C V\nin determining steering angles. We call these salient image sections the salient objects. A detailed report describing our saliency detecting method can be found in [3]\nSeveral methods for finding saliency have been described by other authors. Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9]. We believe the simplicity of our method, its fast execution on our test car\u2019s NVIDIA DRIVETM PX 2 AI car computer, along with its nearly pixel level resolution, makes it especially advantageous for our task."}, {"heading": "1.1 Training the PilotNet Self-Driving System", "text": "PilotNet training data contains single images sampled from video from a front-facing camera in the car, paired with the corresponding steering command (1/r), where r is the turning radius of the vehicle. The training data is augmented with additional image/steering-command pairs that simulate the vehicle in different off-center and off-orientationpoistions. For the augmented images, the target steering command is appropriately adjusted to one that will steer the vehicle back to the center of the lane.\nOnce the network is trained, it can be used to provide the steering command given a new image."}, {"heading": "2 PilotNet Network Architecture", "text": "The PilotNet architecture is shown in Figure 1. The network consists of 9 layers, including a normalization layer, 5 convolutional layers and 3 fully connected layers. The input image is split into YUV\nplanes and passed to the network. The first layer of the network performs image normalization. The normalizer is hard-coded and is not adjusted in the learning process.\nThe convolutional layers were designed to perform feature extraction and were chosen empirically through a series of experiments that varied layer configurations. Strided convolutions were used in the first three convolutional layers with a 2\u00d72 stride and a 5\u00d75 kernel and a non-strided convolution with a 3\u00d73 kernel size in the last two convolutional layers. The five convolutional layers are followed with three fully connected layers leading to an output control value that is the inverse turning radius. The fully connected layers are designed to function as a controller for steering, but note that by training the system end-to-end, there is no hard boundary between which parts of the network function primarily as feature extractors and which serve as the controller."}, {"heading": "3 Finding the Salient Objects", "text": "The central idea in discerning the salient objects is finding parts of the image that correspond to locations where the feature maps, described above, have the greatest activations.\nThe activations of the higher-level maps become masks for the activations of lower levels using the following algorithm:\n1. In each layer, the activations of the feature maps are averaged. 2. The top most averaged map is scaled up to the size of the map of the layer below. The\nup-scaling is done using deconvolution. The parameters (filter size and stride) used for the\ndeconvolution are the same as in the convolutional layer used to generate the map. The weights for deconvolution are set to 1.0 and biases are set to 0.0.\n3. The up-scaled averaged map from an upper level is then multiplied with the averaged map from the layer below (both are now the same size). The result is an intermediate mask.\n4. The intermediate mask is scaled up to the size of the maps of layer below in the same way as described Step 2.\n5. The up-scaled intermediate map is again multiplied with the averaged map from the layer below (both are now the same size). Thus a new intermediate mask is obtained.\n6. Steps 4 and 5 above are repeated until the input is reached. The last mask which is of the size of the input image is normalized to the range from 0.0 to 1.0 and becomes the final visualization mask.\nThis visualization mask shows which regions of the input image contribute most to the output of the network. These regions identify the salient objects. The algorithm block diagram is shown in Figure 2.\nThe process of creating the visualization mask is illustrated in Figure 3. The visualization mask is overlaid on the input image to highlight the pixels in the original camera image to illustrate the salient objects.\nResults for various input images are shown in Figure 4. Notice in the top image the base of cars as well as lines (dashed and solid) indicating lanes are highlighted, while a nearly horizontal line from a crosswalk is ignored. In the middle image there are no lanes painted on the road, but the parked cars, which indicate the edge of the road, are highlighted. In the lower image the grass at the edge of the road is highlighted. Without any coding, these detections show how PilotNet mirrors the way human drivers would use these visual cues.\nFigure 5 show a view inside our test car. At the top of the image we see the actual view through the windshield. A PilotNet monitor is at the bottom center displaying diagnostics.\nFigure 6 is a blowup of the PilotNet monitor. The top image is captured by the front-facing camera. The green rectangle outlines the section of the camera image that is fed to the neural network. The bottom image displays the salient regions. Note that PilotNet identifies the partially occluded construction vehicle on the right side of the road as a salient object. To the best of our knowledge, such a vehicle, particularly in the pose we see here, was never part of the PilotNet training data."}, {"heading": "4 Analysis", "text": "While the salient objects found by our method clearly appear to be ones that should influence steering, we conducted a series of experiments to validate that these objects actually do control the steering. To perform these tests, we segmented the input image that is presented to PilotNet into two classes.\nClass 1 is meant to include all the regions that have a significant effect on the steering angle output by PilotNet. These regions include all the pixels that correspond to locations where the visualization mask is above a threshold. These regions are then dilated by 30 pixels to counteract the increasing span of the higher-level feature map layers with respect to the input image. The exact amount of dilation was determined empirically. The second class includes all pixels in the original image minus the pixels in Class 1. If the objects found by our method indeed dominate control of the output steering angle, we would expect the following: if we create an image in which we uniformly translate only the pixels in Class 1 while maintaining the position of the pixels in Class 2 and use this new image as input to PilotNet, we would expect a significant change in the steering angle output. However, if we instead translate the pixels in Class 2 while keeping those in Class 1 fixed and feed this image into PilotNet, then we would expect minimal change in PilotNet\u2019s output.\nFigure 7 illustrates the process described above. The top image shows a scene captured by our data collection car. The next image shows highlighted salient regions that were identified using the method of Section 3. The next image shows the salient regions dilated. The bottom image shows a test image in which the dilated salient objects are shifted.\nThe above predictions are indeed born out by our experiments. Figure 8 shows plots of PilotNet steering output as a function of pixel shift in the input image. The blue line shows the results when we shift the pixels that include the salient objects (Class 1). The red line shows the results when we shift the pixels not included in the salient objects. The yellow line shows the result when we shift all the pixels in the input image.\nShifting the salient objects results in a linear change in steering angle that is nearly as large as that which occurs when we shift the entire image. Shifting just the background pixels has a much smaller effect on the steering angle. We are thus confident that our method does indeed find the most important regions in the image for determining steering."}, {"heading": "5 Conclusions", "text": "We describe a method for finding the regions in input images by which PilotNet makes its steering decisions, i. e., the salient objects. We further provide evidence that the salient objects identified by this method are correct. The results substantially contribute to our understanding of what PilotNet learns.\nExamination of the salient objects shows that PilotNet learns features that \u201cmake sense\u201d to a human, while ignoring structures in the camera images that are not relevant to driving. This capability is derived from data without the need of hand-crafted rules. In fact, PilotNet learns to recognize subtle\nfeatures which would be hard to anticipate and program by human engineers, such as bushes lining the edge of the road and atypical vehicle classes."}], "references": [{"title": "End to end learning for self-driving", "author": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D. Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang", "Xin Zhang", "Jake Zhao", "Karol Zieba"], "venue": "cars, April", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "VisualBackProp: visualizing CNNs for autonomous driving, November 16 2016", "author": ["Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Bernhard Firner", "Larry Jackel", "Urs Muller", "Karol Zieba"], "venue": "URL: https://arxiv.org/abs/1611.05418,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "How to explain individual classification decisions", "author": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M.i Kawanabe", "K. Hansen", "K.-R. M\u00fcller"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In Workshop Proc. ICLR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Visualization of nonlinear classification models in neuroimaging - signed sensitivity", "author": ["P.M. Rasmussen", "T. Schmah", "K.H. Madsen", "T.E. Lund", "S.C. Strother", "L.K. Hansen"], "venue": "maps. BIOSIGNALS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "In ICCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "author": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W Samek"], "venue": "PLOS ONE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A previous report [1] described an end-to-end learning system for self-driving cars in which a convolutional neural network (CNN) [2] was trained to output steering angles given input images of the road ahead.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "A previous report [1] described an end-to-end learning system for self-driving cars in which a convolutional neural network (CNN) [2] was trained to output steering angles given input images of the road ahead.", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "A detailed report describing our saliency detecting method can be found in [3] Several methods for finding saliency have been described by other authors.", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 44, "endOffset": 53}, {"referenceID": 4, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 44, "endOffset": 53}, {"referenceID": 5, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 44, "endOffset": 53}, {"referenceID": 6, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 80, "endOffset": 86}, {"referenceID": 7, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 80, "endOffset": 86}, {"referenceID": 8, "context": "Among them are sensitivity based approaches [4, 5, 6], deconvolution based ones [7, 8], or more complex ones like layer-wise relevance propagation (LRP) [9].", "startOffset": 153, "endOffset": 156}], "year": 2017, "abstractText": "As part of a complete software stack for autonomous driving, NVIDIA has created a neural-network-based system, known as PilotNet, which outputs steering angles given images of the road ahead. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the necessary domain knowledge by observing human drivers. This eliminates the need for human engineers to anticipate what is important in an image and foresee all the necessary rules for safe driving. Road tests demonstrated that PilotNet can successfully perform lane keeping in a wide variety of driving conditions, regardless of whether lane markings are present or not. The goal of the work described here is to explain what PilotNet learns and how it makes its decisions. To this end we developed a method for determining which elements in the road image most influence PilotNet\u2019s steering decision. Results show that PilotNet indeed learns to recognize relevant objects on the road. In addition to learning the obvious features such as lane markings, edges of roads, and other cars, PilotNet learns more subtle features that would be hard to anticipate and program by engineers, for example, bushes lining the edge of the road and atypical vehicle classes.", "creator": "LaTeX with hyperref package"}}}