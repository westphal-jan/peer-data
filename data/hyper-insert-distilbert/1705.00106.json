{"id": "1705.00106", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "abstract": "still we directly study precise automatic question generation calculations for sentences arising from text linguistic passages sampled in reading comprehension. we clearly introduce yourself an elegant attention - based spoken sequence fuzzy learning software model created for recording the task and investigate the effect of encoding sentence - vs. paragraph - level vocabulary information. in contrast to all previous work, nowadays our model evaluation does definitely not rely heavily on utilizing hand - manually crafted rules or boasts a sophisticated learning nlp lesson pipeline ; furthermore it is presumably instead trainable - end - to - end via verbal sequence - coupled to - sequence learning. automatic evaluation search results show broadly that our system significantly outperforms the actual state - chain of - the - art rule - based system. in human evaluations, key questions generated by our system are heavily also rated as being too more consciously natural ( i. e., overall grammaticality, word fluency ) ( and as more internally difficult skills to answer ( expected in terms expected of purely syntactic and lexical divergence from just the original logical text and reasoning as needed to answer ).", "histories": [["v1", "Sat, 29 Apr 2017 01:08:48 GMT  (227kb,D)", "http://arxiv.org/abs/1705.00106v1", "Accepted to ACL 2017, 11 pages"]], "COMMENTS": "Accepted to ACL 2017, 11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["xinya du", "junru shao", "claire cardie"], "accepted": true, "id": "1705.00106"}, "pdf": {"name": "1705.00106.pdf", "metadata": {"source": "CRF", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "authors": ["Xinya Du", "Junru Shao", "Claire Cardie"], "emails": ["cardie}@cs.cornell.edu", "yz_sjr@sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Question generation (QG) aims to create natural questions from a given a sentence or paragraph. One key application of question generation is in the area of education \u2014 to generate questions for reading comprehension materials (Heilman and Smith, 2010). Figure 1, for example, shows three manually generated questions that test a user\u2019s understanding of the associated text passage. Question generation systems can also be deployed as chatbot components (e.g., asking questions to start a conversation or to request feedback (Mostafazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971).\nIn addition to the above applications, question generation systems can aid in the development of\nannotated data sets for natural language processing (NLP) research in reading comprehension and question answering. Indeed the creation of such datasets, e.g., SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas.\nFor the most part, question generation has been tackled in the past via rule-based approaches (e.g., Mitkov and Ha (2003); Rus et al. (2010). The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge.\nTo improve over a purely rule-based system, Heilman and Smith (2010) introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them using a supervised learning-based ranker. Although the ranking algorithm helps to produce more ac-\nar X\niv :1\n70 5.\n00 10\n6v 1\n[ cs\n.C L\n] 2\n9 A\npr 2\n01 7\nceptable questions, it relies heavily on a manually crafted feature set, and the questions generated often overlap word for word with the tokens in the input sentence, making them very easy to answer.\nVanderwende (2008) point out that learning to ask good questions is an important task in NLP research in its own right, and should consist of more than the syntactic transformation of a declarative sentence. In particular, a natural sounding question often compresses the sentence on which it is based (e.g., question 3 in Figure 1), uses synonyms for terms in the passage (e.g., \u201cform\u201d for \u201cproduce\u201d in question 2 and \u201cget\u201d for \u201cproduce\u201d in question 3), or refers to entities from preceding sentences or clauses (e.g., the use of \u201cphotosynthesis\u201d in question 2). Othertimes, world knowledge is employed to produce a good question (e.g., identifying \u201cphotosynthesis\u201d as a \u201clife process\u201d in question 1). In short, constructing natural questions of reasonable difficulty would seem to require an abstractive approach that can produce fluent phrasings that do not exactly match the text from which they were drawn.\nAs a result, and in contrast to all previous work, we propose here to frame the task of question generation as a sequence-to-sequence learning problem that directly maps a sentence from a text passage to a question. Importantly, our approach is fully data-driven in that it requires no manually generated rules.\nMore specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al., 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al., 2015a). We investigate several variations of this model, including one that takes into account paragraph- rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings.\nIn evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al., 2007), and the overgenerate-and-rank approach of Heil-\nman and Smith (2010). Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart Heilman and Smith (2010) system.\nIn the sections below we discuss related work (Section 2), specify the task definition (Section 3) and describe our neural sequence learning based models (Section 4). We explain the experimental setup in Section 5. Lastly, we present the evaluation results as well as a detailed analysis."}, {"heading": "2 Related Work", "text": "Reading Comprehension is a challenging task for machines, requiring both understanding of natural language and knowledge of the world (Rajpurkar et al., 2016). Recently many new datasets have been released and in most of these datasets, the questions are generated in a synthetic way. For example, bAbI (Weston et al., 2016) is a fully synthetic dataset featuring 20 different tasks. Hermann et al. (2015) released a corpus of cloze style questions by replacing entities with placeholders in abstractive summaries of CNN/Daily Mail news articles. Chen et al. (2016) claim that the CNN/Daily Mail dataset is easier than previously thought, and their system almost reaches the ceiling performance. Richardson et al. (2013) curated MCTest, in which crowdworker questions are paired with four answer choices. Although MCTest contains challenging natural questions, it is too small for training data-demanding question answering models.\nRecently, Rajpurkar et al. (2016) released the Stanford Question Answering Dataset1 (SQuAD), which overcomes the aforementioned small size and (semi-)synthetic issues. The questions are posed by crowd workers and are of relatively high quality. We use SQuAD in our work, and similarly, we focus on the generation of natural questions for reading comprehension materials, albeit via automatic means.\nQuestion Generation has attracted the attention of the natural language generation (NLG) community in recent years, since the work of Rus et al. (2010).\nMost work tackles the task with a rule-based approach. Generally, they first transform the input sentence into its syntactic representation, which\n1https://stanford-qa.com\nthey then use to generate an interrogative sentence. A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014). Labutov et al. (2015) use crowdsourcing to collect a set of templates and then rank the relevant templates for the text of another domain. Generally, the rule-based approaches make use of the syntactic roles of words, but not their semantic roles.\nHeilman and Smith (2010) introduce an overgenerate-and-rank approach: their system first overgenerates questions and then ranks them. Although they incorporate learning to rank, their system\u2019s performance still depends critically on the manually constructed generating rules. Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connection between language and vision. Serban et al. (2016) propose generating simple factoid questions from logic triple (subject, relation, object). Their task tackles mapping from structured representation to natural language text, and their generated questions are consistent in terms of format and diverge much less than ours.\nTo our knowledge, none of the previous works has framed QG for reading comprehension in an end-to-end fashion, and nor have them used deep sequence-to-sequence learning approach to generate questions."}, {"heading": "3 Task Definition", "text": "In this section, we define the question generation task. Given an input sentence x, our goal is to generate a natural question y related to information in the sentence, y can be a sequence of an arbitrary length: [y1, ..., y|y|]. Suppose the length of the input sentence is M , x could then be represented as a sequence of tokens [x1, ..., xM ]. The QG task is defined as finding y, such that:\ny = argmax y\nP (y|x) (1)\nwhere P (y|x) is the conditional log-likelihood of the predicted question sequence y, given the input x. In section 4.1, we will elaborate on the global attention mechanism for modeling P (y|x)."}, {"heading": "4 Model", "text": "Our model is partially inspired by the way in which a human would solve the task. To ask a natural question, people usually pay attention to certain parts of the input sentence, as well as associating context information from the paragraph. We model the conditional probability using RNN encoder-decoder architecture (Bahdanau et al., 2015; Cho et al., 2014), and adopt the global attention mechanism (Luong et al., 2015a) to make the model focus on certain elements of the input when generating each word during decoding.\nHere, we investigate two variations of our models: one that only encodes the sentence and another that encodes both sentence and paragraphlevel information."}, {"heading": "4.1 Decoder", "text": "Similar to Sutskever et al. (2014) and Chopra et al. (2016), we factorize the the conditional in equation 1 into a product of word-level predictions:\nP (y|x) = |y|\u220f t=1 P (yt|x, y<t)\nwhere probability of each yt is predicted based on all the words that are generated previously (i.e., y<t), and input sentence x.\nMore specifically,\nP (yt|x, y<t) = softmax (Wstanh (Wt[ht; ct])) (2) with ht being the recurrent neural networks state variable at time step t, and ct being the attentionbased encoding of x at decoding time step t (Section 4.2). Ws and Wt are parameters to be learned.\nht = LSTM1 (yt\u22121,ht\u22121) (3)\nhere, LSTM is the Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997). It generates the new state ht, given the representation of previously generated word yt\u22121 (obtained from a word look-up table), and the previous state ht\u22121.\nThe initialization of the decoder\u2019s hidden state differentiates our basic model and the model that incorporates paragraph-level information.\nFor the basic model, it is initialized by the sentence\u2019s representation s obtained from the sentence encoder (Section 4.2). For our paragraphlevel model, the concatenation of the sentence\nencoder\u2019s output s and the paragraph encoder\u2019s output s\u2032 is used as the initialization of decoder hidden state. To be more specific, the architecture of our paragraph-level model is like a \u201cY\u201dshaped network which encodes both sentenceand paragraph-level information via two RNN branches and uses the concatenated representation for decoding the questions."}, {"heading": "4.2 Encoder", "text": "The attention-based sentence encoder is used in both of our models, while the paragraph encoder is only used in the model that incorporates paragraph-level information.\nAttention-based sentence encoder: We use a bidirectional LSTM to encode the sentence,\n\u2212\u2192 bt = \u2212\u2212\u2212\u2212\u2192 LSTM2 ( xt, \u2212\u2212\u2192 bt\u22121 ) \u2190\u2212 bt = \u2190\u2212\u2212\u2212\u2212 LSTM2 ( xt, \u2190\u2212\u2212 bt+1\n) where \u2212\u2192 bt is the hidden state at time step t for the\nforward pass LSTM, \u2190\u2212 bt for the backward pass.\nTo get attention-based encoding of x at decoding time step t, namely, ct, we first get the context dependent token representation by bt = [ \u2212\u2192 bt; \u2190\u2212 bt], then we take the weighted average over bt (t = 1, ..., |x|),\nct = \u2211\ni=1,..,|x|\nai,tbi (4)\nThe attention weight are calculated by the bilinear scoring function and softmax normalization,\nai,t = exp\n( hTt Wbbi )\u2211 j exp ( hTt Wbbj\n) (5) To get the sentence encoder\u2019s output for initialization of decoder hidden state, we concatenate last hidden state of the forward and backward pass, namely, s = [ \u2212\u2212\u2192 b|x|; \u2190\u2212 b1].\nParagraph encoder: Given sentence x, we want to encode the paragraph containing x. Since in practice the paragraph is very long, we set a length thresholdL, and truncate the paragraph at theLth token. We call the truncated paragraph \u201cparagraph\u201d henceforth.\nDenoting the paragraph as z, we use another bidirectional LSTM to encode z,\n\u2212\u2192 dt = \u2212\u2212\u2212\u2212\u2192 LSTM3 ( zt, \u2212\u2212\u2192 dt\u22121 ) \u2190\u2212 dt = \u2190\u2212\u2212\u2212\u2212 LSTM3 ( zt, \u2190\u2212\u2212 dt+1\n) With the last hidden state of the forward and backward pass, we use the concatenation [ \u2212\u2192 d|z|; \u2190\u2212 d1] as the paragraph encoder\u2019s output s\u2032."}, {"heading": "4.3 Training and Inference", "text": "Giving a training corpus of sentence-question pairs: S = {( x(i),y(i) )}S i=1\n, our models\u2019 training objective is to minimize the negative loglikelihood of the training data with respect to all the parameters, as denoted by \u03b8,\nL = \u2212 S\u2211\ni=1\nlogP ( y(i)|x(i); \u03b8 )\n= \u2212 S\u2211\ni=1 |y(i)|\u2211 j=1 logP ( y (i) j |x (i), y (i) <j ; \u03b8 ) Once the model is trained, we do inference using beam search. The beam search is parametrized by the possible paths number k.\nAs there could be many rare words in the input sentence that are not in the target side dictionary, during decoding many UNK tokens will be output. Thus, post-processing with the replacement of UNK is necessary. Unlike Luong et al. (2015b), we use a simpler replacing strategy for our task. For the decoded UNK token at time step t, we replace it with the token in the input sentence with the highest attention score, the index of which is argmaxi ai,t."}, {"heading": "5 Experimental Setup", "text": "We experiment with our neural question generation model on the processed SQuAD dataset. In this section, we firstly describe the corpus of the task. We then give implementation details of our neural generation model, the baselines to compare, and their experimental settings. Lastly, we introduce the evaluation methods by automatic metrics and human raters."}, {"heading": "5.1 Dataset", "text": "With the SQuAD dataset (Rajpurkar et al., 2016), we extract sentences and pair them with the ques-\ntions. We train our models with the sentencequestion pairs. The dataset contains 536 articles with over 100k questions posed about the articles. The authors employ Amazon Mechanical Turks crowd-workers to create questions based on the Wikipedia articles. Workers are encouraged to use their own words without any copying phrases from the paragraph. Later, other crowd-workers are employed to provide answers to the questions. The answers are spans of tokens in the passage.\nSince there is a hidden part of the original SQuAD that we do not have access to, we treat the accessible parts (\u223c90%) as the entire dataset henceforth.\nWe first run Stanford CoreNLP (Manning et al., 2014) for pre-processing: tokenization and sentence splitting. We then lower-case the entire dataset. With the offset of the answer to each question, we locate the sentence containing the answer and use it as the input sentence. In some cases (< 0.17% in training set), the answer spans two or more sentences, and we then use the concatenation of the sentences as the input \u201csentence\u201d.\nFigure 2 shows the distribution of the token overlap percentage of the sentence-question pairs. Although most of the pairs have over 50% overlap rate, about 6.67% of the pairs have no nonstop-words in common, and this is mostly because of the answer offset error introduced during annotation. Therefore, we prune the training set based on the constraint: the sentence-question pair must have at least one non-stop-word in common. Lastly we add <SOS> to the beginning of the sen-\ntences, and <EOS> to the end of them. We randomly divide the dataset at the articlelevel into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set.\nTable 1 provides some statistics on the processed dataset: there are around 70k training samples, the sentences are around 30 tokens, and the questions are around 10 tokens on average. For each sentence, there might be multiple corresponding questions, and, on average, there are 1.4 questions for each sentence."}, {"heading": "5.2 Implementation Details", "text": "We implement our models 2 in Torch7 3 on top of the newly released OpenNMT system (Klein et al., 2017).\nFor the source side vocabulary V , we only keep the 45k most frequent tokens (including <SOS>, <EOS> and placeholders). For the target side vocabulary U , similarly, we keep the 28k most frequent tokens. All other tokens outside the vocabulary list are replaced by the UNK symbol. We choose word embedding of 300 dimensions and use the glove.840B.300d pre-trained embeddings (Pennington et al., 2014) for initialization. We fix the word representations during training.\nWe set the LSTM hidden unit size to 600 and set the number of layers of LSTMs to 2 in both the encoder and the decoder. Optimization is performed using stochastic gradient descent (SGD), with an initial learning rate of 1.0. We start halving the learning rate at epoch 8. The mini-batch size for the update is set at 64. Dropout with probability\n2The code is available at https://github.com/ xinyadu/nqg.\n3http://torch.ch/\n0.3 is applied between vertical LSTM stacks. We clip the gradient when the its norm exceeds 5.\nAll our models are trained on a single GPU. We run the training for up to 15 epochs, which takes approximately 2 hours. We select the model that achieves the lowest perplexity on the dev set.\nDuring decoding, we do beam search with a beam size of 3. Decoding stops when every beam in the stack generates the <EOS> token.\nAll hyperparameters of our model are tuned using the development set. The results are reported on the test set."}, {"heading": "5.3 Baselines", "text": "To prove the effectiveness of our system, we compare it to several competitive systems. Next, we briefly introduce their approaches and the experimental setting to run them for our problem. Their results are shown in Table 2.\nIR stands for our information retrieval baselines. Similar to Rush et al. (2015), we implement the IR baselines to control memorizing questions from the training set. We use two metrics to calculate the distance between a question and the input sentence, i.e., BM-25 (Robertson and Walker, 1994) and edit distance (Levenshtein, 1966). According to the metric, the system retrieves the training set to find the question with the highest score.\nMOSES+ (Koehn et al., 2007) is a widely used phrase-based statistical machine translation system. Here, we treat sentences as source language text, we treat questions as target language text, and we perform the translation from sentences to ques-\ntions. We train a tri-gram language model on target side texts with KenLM (Heafield et al., 2013), and tune the system with MERT on dev set. Performance results are reported on the test set.\nDirectIn is an intuitive yet meaningful baseline in which the longest sub-sentence of the sentence is directly taken as the predicted question. 4 To split the sentence into sub-sentences, we use a set of splitters, i.e., {\u201c?\u201d, \u201c!\u201d, \u201c,\u201d, \u201c.\u201d, \u201c;\u201d}.\nH&S is the rule-based overgenerate-and-rank system that was mentioned in Section 2. When running the system, we set the parameter just-wh true (to restrict the output of the system to being only wh-questions) and set max-length equal to the longest sentence in the training set. We also set downweight-pro true, to down weight questions with unresolved pronouns so that they appear towards the end of the ranked list. For comparison with our systems, we take the top question in the ranked list.\nSeq2seq (Sutskever et al., 2014) is a basic encoder-decoder sequence learning system for machine translation. We implement their model in Tensorflow. The input sequence is reversed before training or translating. Hyperparameters are tuned with dev set. We select the model with the lowest perplexity on the dev set.\n4We also tried using the entire input sentence as the prediction output, but the performance is worse than taking subsentence as the prediction, across all the automatic metrics except for METEOR."}, {"heading": "5.4 Automatic Evaluation", "text": "We use the evaluation package released by Chen et al. (2015), which was originally used to score image captions. The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts. BLEU measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. BLEU-n is BLEU score that uses up to n-grams for counting co-occurrences. METEOR is a recall-oriented metric, which calculates the similarity between generations and references by considering synonyms, stemming and paraphrases. ROUGE is commonly employed to evaluate n-grams recall of the summaries with goldstandard sentences as references. ROUGEL (measured based on longest common subsequence) results are reported."}, {"heading": "5.5 Human Evaluation", "text": "We also perform human evaluation studies to measure the quality of questions generated by our system and the H&S system. We consider two modalities: naturalness, which indicates the grammaticality and fluency; and difficulty, which measures the sentence-question syntactic divergence and the reasoning needed to answer the question. We randomly sampled 100 sentence-question pairs. We ask four professional English speakers to rate the pairs in terms of the modalities above on a 1\u20135 scale (5 for the best). We then ask the human raters to give a ranking of the questions according to the overall quality, with ties allowed."}, {"heading": "6 Results and Analysis", "text": "Table 2 shows automatic metric evaluation results for our models and baselines. Our model which only encodes sentence-level information achieves\nthe best performance across all metrics. We note that IR performs poorly, indicating that memorizing the training set is not enough for the task. The baseline DirectIn performs pretty well on BLEU and METEOR, which is reasonable given the overlap statistics between the sentences and the questions (Figure 2). H&S system\u2019s performance is on a par with DirectIn\u2019s, as it basically performs syntactic change without paraphrasing, and the overlap rate is also high.\nLooking at the performance of our three models, it\u2019s clear that adding the pre-trained embeddings generally helps. While encoding the paragraph causes the performance to drop a little, this makes sense because, apart from useful information, the paragraph also contains much noise.\nTable 3 shows the results of the human evaluation. We see that our system outperforms H&S in all modalities. Our system is ranked best in 38.4% of the evaluations, with an average ranking of 1.94. An inter-rater agreement of Krippendorff\u2019s Alpha of 0.236 is achieved for the overall ranking. The results imply that our model can generate questions of better quality than the H&S system. An interesting phenomenon here is that human raters gave higher score for our system\u2019s outputs than the human questions. One potential explanation for this is that our system is trained on all sentence-question pairs for one input sentence, while we randomly select one question among the several questions of one sentence as the human generated question, for the purpose of rating. Thus our system\u2019s predictions tend to be more diverse.\nFor our qualitative analysis, we examine the sample outputs and the visualization of the alignment between the input and the output. In Figure 3, we present sample questions generated by H&S and our best model. We see a large gap between our results and H&S\u2019s. For example, in\nthe first sample, in which the focus should be put on \u201cthe largest.\u201d Our model successfully captures this information, while H&S only performs some syntactic transformation over the input without paraphrasing. However, outputs from our system are not always \u201cperfect\u201d, for example, in pair 6, our system generates a question about the reason why birds still grow, but the most related question would be why many species still grow. But from a different perspective, our question is more challenging (readers need to understand that birds are one kind of species), which supports our system\u2019s performance listed in human evaluations (See Table 3). It would be interesting to further investigate how to interpret why certain irrelavant words are generated in the question. Figure 4 shows the attention weights (\u03b1i,t) for the input sentence when generating each token in the question. We see that the key words in the output (\u201cintroduced\u201d, \u201cteletext\u201d, etc.) aligns well with those in the input sentence.\nFinally, we do a dataset analysis and fine-\ngrained system performance analysis. We randomly sampled 346 sentence-question pairs from the dev set and label each pair with a category. 5 The four categories are determined by how much information is needed to ask the question. To be specific, \u201cw/ sentence\u201d means it only requires the sentence to ask the question; \u201cw/ paragraph\u201d means it takes other information in the paragraph to ask the question; \u201cw/ article\u201d is similar to \u201cw/ paragraph\u201d; and \u201cnot askable\u201d means that world knowledge is needed to ask the question or there is mismatch of sentence and question caused by annotation error.\nTable 4 shows the per-category performance of the systems. Our model which encodes paragraph information achieves the best performance on the questions of \u201cw/ paragraph\u201d category. This verifies the effectiveness of our paragraph-level model on the questions concerning information outside the sentence."}, {"heading": "7 Conclusion and Future Work", "text": "We have presented a fully data-driven neural networks approach to automatic question generation for reading comprehension. We use an attentionbased neural networks approach for the task and investigate the effect of encoding sentence- vs. paragraph-level information. Our best model achieves state-of-the-art performance in both automatic evaluations and human evaluations.\nHere we point out several interesting future research directions. Currently, our paragraph-level model does not achieve best performance across all categories of questions. We would like to explore how to better use the paragraph-level information to improve the performance of QG system regarding questions of all categories. Besides this, it would also be interesting to consider to incorporate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue generation) in our model to further improve the quality of generated questions."}, {"heading": "Acknowledgments", "text": "We thank the anonymous ACL reviewers, Kai Sun and Yao Cheng for their helpful suggestions. We thank Victoria Litvinova for her careful proofreading. We also thank Xanda Schofield, Wil Thoma-\n5The IDs of the questions examined will be made available at https://github.com/xinyadu/nqg/ blob/master/examined-question-ids.txt.\nson, Hubert Lin and Junxian He for doing the human evaluations."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations Workshop (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325 .", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Artificial paranoia", "author": ["Kenneth Mark Colby", "Sylvia Weber", "Franklin Dennis Hilf."], "venue": "Artificial Intelligence 2(1):1\u201325. https://doi.org/10.1016/00043702(71)90002-6.", "citeRegEx": "Colby et al\\.,? 1971", "shortCiteRegEx": "Colby et al\\.", "year": 1971}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Association for Computational Lin-", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Good question! statistical ranking for question generation", "author": ["Michael Heilman", "Noah A. Smith."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Heilman and Smith.,? 2010", "shortCiteRegEx": "Heilman and Smith.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Summarizing source code using a neural attention model", "author": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Luke Zettlemoyer."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Iyer et al\\.,? 2016", "shortCiteRegEx": "Iyer et al\\.", "year": 2016}, {"title": "Opennmt: Open-source toolkit for neural machine translation", "author": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush."], "venue": "ArXiv e-prints .", "citeRegEx": "Klein et al\\.,? 2017", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Deep questions without deep understanding", "author": ["Igor Labutov", "Sumit Basu", "Lucy Vanderwende."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Con-", "citeRegEx": "Labutov et al\\.,? 2015", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["Vladimir I Levenshtein."], "venue": "Soviet physics doklady. volume 10, page 707.", "citeRegEx": "Levenshtein.,? 1966", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Com-", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Generating natural language questions to support learning on-line", "author": ["David Lindberg", "Fred Popowich", "John Nesbit", "Phil Winne."], "venue": "Proceedings of the 14th European Workshop on Natural Language Generation. Association for Computa-", "citeRegEx": "Lindberg et al\\.,? 2013", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny F.", "Steven B.", "David M."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: Sys-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Linguistic considerations in automatic question generation", "author": ["Karen Mazidi", "Rodney D. Nielsen."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational", "citeRegEx": "Mazidi and Nielsen.,? 2014", "shortCiteRegEx": "Mazidi and Nielsen.", "year": 2014}, {"title": "Computeraided generation of multiple-choice tests", "author": ["Ruslan Mitkov", "Le An Ha."], "venue": "Jill Burstein and Claudia Leacock, editors, Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using", "citeRegEx": "Mitkov and Ha.,? 2003", "shortCiteRegEx": "Mitkov and Ha.", "year": 2003}, {"title": "Generating natural questions about", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende"], "venue": null, "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Generating instruction automatically for the reading strategy of selfquestioning", "author": ["Jack Mostow", "Wei Chen."], "venue": "Proceedings of the 2nd Workshop on Question Generation (AIED 2009). pages 465\u2013472.", "citeRegEx": "Mostow and Chen.,? 2009", "shortCiteRegEx": "Mostow and Chen.", "year": 2009}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Associ-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["Stephen E. Robertson", "Steve Walker."], "venue": "Proceedings of the 17th Annual International ACM SIGIR Conference on Re-", "citeRegEx": "Robertson and Walker.,? 1994", "shortCiteRegEx": "Robertson and Walker.", "year": 1994}, {"title": "The first question generation shared task", "author": ["Vasile Rus", "Brendan Wyse", "Paul Piwek", "Mihai Lintean", "Svetlana Stoyanchev", "Cristian Moldovan"], "venue": null, "citeRegEx": "Rus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Iulian Vlad Serban", "Alberto Garc\u00eda-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS). pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The importance of being important: Question generation", "author": ["Lucy Vanderwende."], "venue": "Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge, Arlington, VA.", "citeRegEx": "Vanderwende.,? 2008", "shortCiteRegEx": "Vanderwende.", "year": 2008}, {"title": "Eliza&mdash;a computer program for the study of natural language communication between man and machine", "author": ["Joseph Weizenbaum."], "venue": "Commun. ACM 9(1):36\u201345. https://doi.org/10.1145/365153.365168.", "citeRegEx": "Weizenbaum.,? 1966", "shortCiteRegEx": "Weizenbaum.", "year": 1966}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "International Conference on Learning Represen-", "citeRegEx": "Weston et al\\.,? 2016", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "One key application of question generation is in the area of education \u2014 to generate questions for reading comprehension materials (Heilman and Smith, 2010).", "startOffset": 131, "endOffset": 156}, {"referenceID": 22, "context": ", asking questions to start a conversation or to request feedback (Mostafazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al.", "startOffset": 66, "endOffset": 93}, {"referenceID": 35, "context": ", 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971).", "startOffset": 84, "endOffset": 122}, {"referenceID": 5, "context": ", 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971).", "startOffset": 84, "endOffset": 122}, {"referenceID": 27, "context": ", SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 24, "context": ", 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas.", "startOffset": 21, "endOffset": 42}, {"referenceID": 20, "context": ", Mitkov and Ha (2003); Rus et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": ", Mitkov and Ha (2003); Rus et al. (2010). The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge.", "startOffset": 2, "endOffset": 42}, {"referenceID": 8, "context": "To improve over a purely rule-based system, Heilman and Smith (2010) introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them using a supervised learning-based ranker.", "startOffset": 44, "endOffset": 69}, {"referenceID": 33, "context": "More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al.", "startOffset": 80, "endOffset": 127}, {"referenceID": 0, "context": "More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al.", "startOffset": 80, "endOffset": 127}, {"referenceID": 31, "context": ", 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al.", "startOffset": 23, "endOffset": 61}, {"referenceID": 11, "context": ", 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al.", "startOffset": 23, "endOffset": 61}, {"referenceID": 37, "context": ", 2016), and image caption generation (Xu et al., 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 17, "context": ", 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al., 2015a).", "startOffset": 115, "endOffset": 136}, {"referenceID": 27, "context": "In evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 29, "context": ", 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al.", "startOffset": 182, "endOffset": 210}, {"referenceID": 8, "context": ", 2007), and the overgenerate-and-rank approach of Heilman and Smith (2010). Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart Heilman and Smith (2010) system.", "startOffset": 51, "endOffset": 76}, {"referenceID": 8, "context": ", 2007), and the overgenerate-and-rank approach of Heilman and Smith (2010). Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart Heilman and Smith (2010) system.", "startOffset": 51, "endOffset": 325}, {"referenceID": 36, "context": "For example, bAbI (Weston et al., 2016) is a fully synthetic dataset featuring 20 different tasks.", "startOffset": 18, "endOffset": 39}, {"referenceID": 1, "context": "Chen et al. (2016) claim that the CNN/Daily Mail dataset is easier than previ-", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "ously thought, and their system almost reaches the ceiling performance. Richardson et al. (2013) curated MCTest, in which crowdworker questions are paired with four answer choices.", "startOffset": 54, "endOffset": 97}, {"referenceID": 27, "context": "Recently, Rajpurkar et al. (2016) released the Stanford Question Answering Dataset1 (SQuAD), which overcomes the aforementioned small size and (semi-)synthetic issues.", "startOffset": 10, "endOffset": 34}, {"referenceID": 27, "context": "Recently, Rajpurkar et al. (2016) released the Stanford Question Answering Dataset1 (SQuAD), which overcomes the aforementioned small size and (semi-)synthetic issues. The questions are posed by crowd workers and are of relatively high quality. We use SQuAD in our work, and similarly, we focus on the generation of natural questions for reading comprehension materials, albeit via automatic means. Question Generation has attracted the attention of the natural language generation (NLG) community in recent years, since the work of Rus et al. (2010).", "startOffset": 10, "endOffset": 551}, {"referenceID": 23, "context": "A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014).", "startOffset": 126, "endOffset": 198}, {"referenceID": 16, "context": "A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014).", "startOffset": 126, "endOffset": 198}, {"referenceID": 20, "context": "A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014).", "startOffset": 126, "endOffset": 198}, {"referenceID": 13, "context": "Labutov et al. (2015) use crowdsourcing to collect a set of templates and then rank the relevant templates for the text of another domain.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connection between language and vision.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connection between language and vision. Serban et al. (2016) propose generating simple factoid ques-", "startOffset": 0, "endOffset": 151}, {"referenceID": 0, "context": "We model the conditional probability using RNN encoder-decoder architecture (Bahdanau et al., 2015; Cho et al., 2014), and adopt the global attention mechanism (Luong et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 3, "context": "We model the conditional probability using RNN encoder-decoder architecture (Bahdanau et al., 2015; Cho et al., 2014), and adopt the global attention mechanism (Luong et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 17, "context": ", 2014), and adopt the global attention mechanism (Luong et al., 2015a) to make the model focus on certain elements of the input when generating each word during decoding.", "startOffset": 50, "endOffset": 71}, {"referenceID": 32, "context": "Similar to Sutskever et al. (2014) and Chopra et al.", "startOffset": 11, "endOffset": 35}, {"referenceID": 4, "context": "(2014) and Chopra et al. (2016), we factorize the the conditional in equa-", "startOffset": 11, "endOffset": 32}, {"referenceID": 10, "context": "here, LSTM is the Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997).", "startOffset": 56, "endOffset": 90}, {"referenceID": 17, "context": "Unlike Luong et al. (2015b), we use a simpler replacing strategy for our task.", "startOffset": 7, "endOffset": 28}, {"referenceID": 27, "context": "With the SQuAD dataset (Rajpurkar et al., 2016), we extract sentences and pair them with the ques-", "startOffset": 23, "endOffset": 47}, {"referenceID": 19, "context": "We first run Stanford CoreNLP (Manning et al., 2014) for pre-processing: tokenization and sentence splitting.", "startOffset": 30, "endOffset": 52}, {"referenceID": 12, "context": "We implement our models 2 in Torch7 3 on top of the newly released OpenNMT system (Klein et al., 2017).", "startOffset": 82, "endOffset": 102}, {"referenceID": 26, "context": "300d pre-trained embeddings (Pennington et al., 2014) for initialization.", "startOffset": 28, "endOffset": 53}, {"referenceID": 29, "context": ", BM-25 (Robertson and Walker, 1994) and edit distance (Levenshtein, 1966).", "startOffset": 8, "endOffset": 36}, {"referenceID": 14, "context": ", BM-25 (Robertson and Walker, 1994) and edit distance (Levenshtein, 1966).", "startOffset": 55, "endOffset": 74}, {"referenceID": 14, "context": "IR stands for our information retrieval baselines. Similar to Rush et al. (2015), we implement the IR baselines to control memorizing questions from the training set.", "startOffset": 44, "endOffset": 81}, {"referenceID": 7, "context": "get side texts with KenLM (Heafield et al., 2013), and tune the system with MERT on dev set.", "startOffset": 26, "endOffset": 49}, {"referenceID": 33, "context": "Seq2seq (Sutskever et al., 2014) is a basic encoder-decoder sequence learning system for machine translation.", "startOffset": 8, "endOffset": 32}, {"referenceID": 25, "context": "The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts.", "startOffset": 52, "endOffset": 75}, {"referenceID": 6, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts.", "startOffset": 16, "endOffset": 43}, {"referenceID": 15, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts.", "startOffset": 55, "endOffset": 66}], "year": 2017, "abstractText": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentencevs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequenceto-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).", "creator": "LaTeX with hyperref package"}}}