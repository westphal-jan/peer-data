{"id": "1705.00744", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "A Strategy for an Uncompromising Incremental Learner", "abstract": "so multi - class virtual supervised learning systems currently require solely the knowledge of observing the observed entire range stream of labels they regularly predict. less often seen when learnt incrementally, they suffer from perceived catastrophic forgetting. impossible to avoid this, although generous leeways policies have seems to be successfully made to explain the philosophy of incremental robust learning behaviour that either seemingly forces a part of on the machine to merely not typically learn, or to retrain with the machine again even with a selection of the historic data. additionally while these tricks work to both various degrees, they increasingly do mostly not adhere to simply the spirit of incremental pattern learning. in this article, we further redefine their incremental shock learning with stringent conditions alone that usually don't allow for reaching any truly undesirable relaxations and assumptions. we design a strategy involving generative models \u2014 and the distillation procedure of various dark phantom knowledge concepts as a means point of hallucinating all data digitally along with appropriate targets generated from previous past distributions. we currently call to this technique optimal phantom danger sampling. we show firstly that fast phantom sampling capability helps completely avoid persistent catastrophic forgetting during incremental learning. presented using an experimental implementation based on independent deep analytic neural networks, we only demonstrate that phantom catastrophe sampling even dramatically avoids catastrophic forgetting. rather we each apply these strategies generally to promote competitive multi - class rapid incremental random learning of deep digital neural networks. using various benchmark evaluation datasets adapted through our strategy, we demonstrate that strict incremental learning could be achieved.", "histories": [["v1", "Tue, 2 May 2017 00:17:54 GMT  (2638kb,D)", "http://arxiv.org/abs/1705.00744v1", null], ["v2", "Mon, 17 Jul 2017 07:30:18 GMT  (4920kb,D)", "http://arxiv.org/abs/1705.00744v2", "Under review at IEEE Transactions of Neural Networks and Learning Systems"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ragav venkatesan", "hemanth venkateswara", "sethuraman panchanathan", "baoxin li"], "accepted": false, "id": "1705.00744"}, "pdf": {"name": "1705.00744.pdf", "metadata": {"source": "CRF", "title": "A Strategy for an Uncompromising Incremental Learner", "authors": ["Ragav Venkatesan", "Hemanth Venkateswara", "Sethuraman Panchanathan", "Baoxin Li"], "emails": ["ragav.venkatesan@asu.edu,", "hemanthv@asu.edu,", "panch@asu.edu,", "baoxin.li@asu.edu"], "sections": [{"heading": "1. Introduction", "text": "Animals and humans learn incrementally. A child grows its vocabulary of identifiable concepts as different concepts are presented, without forgetting the concepts with which they are already familiar. Antithetically, most supervised learning systems work under the omniscience of the existence of all classes to be learned, prior to training. This is crucial for learning systems that produce an inference as a conditional probability distribution over all known categories.\nIncremental supervised learning though reasonably studied, lacks a formal and structured definition. One of the earliest formalization of incremental learning comes from the work of Jantke [10]. In this article the author defines in-\ncremental learning roughly as systems that \u201chave no permission to look back at the whole history of information presented during the learning process\u201d. Immediately following this statement though is the relaxation of the definition: \u201cOperationally incremental learning algorithms may have permission to look back, but they are not allowed to use information of the past in some effective way\u201d, with the terms information and effective not being sufficiently well-defined. Subsequently, other studies made conforming or divergent assumptions and relaxations thereby adopting their own characteristic definitions. Following suit, we redefine a more fundamental and rigorous incremental learning system using two fundamental philosophies: data membrane and domain agnosticism.\nConsider there are two sites: the base site Sb and the incremental site Si each with ample computational resources. Sb possesses the base dataset Db = {(xbl , ybl ), l \u2208 {1, 2, . . . n}}, where xbl \u2208 Rd,\u2200l and ybl \u2208 {1, 2, . . . j},\u2200l. Si possesses the increment dataset Di = {(xil, yil), l \u2208 {1, 2, . . .m}}, where xil \u2208 Rd,\u2200l and yil \u2208 {j + 1, j + 2, . . . c},\u2200l and yil 6\u2208 {0, 1, . . . j},\u2200l.\nProperty 1. Db is only available at Sb andDi is only available at Si. Neither set can be transferred either directly or as features extracted by any deterministic encoder, either in whole or in part to the other site, respectively.\nSb is allowed to trai n a discriminative learner Nb using Db and make Nb available to the world. Once broadcast, Sb does not maintain Nb and will therefore not support queries regarding Nb. Property 1 is referred to as the data membrane. Data membrane ensures that Si does not query Sb and that no data is transferred either in original form or in any encoded fashion (say as feature vectors). The generalization set at Si contains labels in the space of y \u2208 {1 . . . c}. This implies that though Si, has no data for training the labels 1 . . . j, the discriminator Ni trained at Si withDi alone is expected to generalize on the combined label space in the range 1 . . . c. Si can acquire Nb and other models from Sb and infer the existence of the classes y \u2208 {1, 2, . . . j} that Nb can distinguish. Therefore incremental learning differs from the problem of zero-shot novel class identification.\n1\nar X\niv :1\n70 5.\n00 74\n4v 1\n[ cs\n.C V\n] 2\nM ay\n2 01\n7\nA second property of multi-class incremental learning is domain agnosticism, which can be defined as follows:\nProperty 2. No priors shall be established as to the dependencies of classes or domains between Db and Di.\nProperty 2 implies that we cannot presume to gain any knowledge about the label space of Db ({0 . . . j}) by simply studying the behaviour of Nb using Di. In other words, the predictions of the network Nb does not provide us meaningful enough information regarding Di. This implies that the conditional probability distribution across the labels in y \u2208 {0 . . . j}, PNb(y|x) for (x, y) \u2208 Di produced by Nb, cannot provide any meaningful inference to the conditional probability distribution across the labels y \u2208 {j + 1 . . . c} when generalizing on the incremental data. For any samples x \u2208 Di, the conditional probability over the labels of classes y \u2208 {0 . . . j} are meaningless. Property (2) is called domain agnosticism.\nFrom the above definition it is implied that sites must train independently. The training at Si of labels y \u2208 {j + 1 . . . c} could be at any state when Sb triggers site Si by publishing its models, which marks the beginning of incremental training at Si. To keep experiments and discussions simpler, we assume the worst case scenario where the site 2 does not begin training by itself, but we will generalize to all chronology in the later sections.\nWe live in a world of data abundance. Even in this environment of data affluence, we may still encounter cases of scarcity of data. Data is a valuable commodity and is often jealously guarded by those who posses it. Most large institutions and organizations that deploy trained models, do not share the data with which the models are trained. A consumer who wants to add additional capability is faced with\nan incremental learning problem as defined. In other cases, such as in military or medicine, data may be protected by legal, intellectual property and privacy restrictions. A medical facility that wants to add the capability of diagnosing a related-but-different pathology to an already purchased model also faces a similar problem and often has to expend large sums of money to purchase an instrument with this incrementally additional capability. All these scenarios are plausible contenders for strict incremental learning following the above definition. The data membrane property ensures that even if data could be transferred, we are restricted by means other than technological, be it legal or privacy-related that prevents the sharing of data across sites. The domain agnosticism property implies that we should be able to add the capability of predicting labels to the network, without making any assumptions that the new labels may or may not hold any tangible relationship to the old labels.\nA trivial baseline: Given this formalism, the most trivial incremental training protocol would be to train a machine at Sb with Db, transfer this machine (make it available in some fashion) to Si. At Si, initialize a new machine with the parameters of the transferred machine, while alerting the new machine to the existence of classes j + 1, . . . c and simply teach it to model an updated conditional probability distribution over classes {1, 2, . . . c}. A quick experiment can demonstrate to us that such a system is afflicted by a well-studied problem called catastrophic forgetting. Figure 1 demonstrates this effect using neural networks. This demonstrates that without supplying samples from Db, incremental training without catastrophic forgetting at Si is difficult without relaxing our definition.\nTo avoid this, we propose that the use of generative models trained at Sb, be deployed at Si to hallucinate sam-\nples from Db. The one-time broadcast from Sb could include this generator along with the initializer machine that is transferred. While this system could generate sampleson-demand, we still do not have targets for the generated samples to learn classification with. To solve this problem, we propose the generation of supervision from the initializer network itself using a temperature-raised softmax. A temperature raised softmax was previously proposed as a means of distilling knowledge in the context of neural network compression [8]. Not only does this provide supervision for generated samples, but will also serve as a regularizer while training a machine at Si, similar to the fashion described in [8].\nIn summary this paper provides two major contributions: 1. A novel, uncompromising and practical definition of incremental learning and 2. a strategy to attack the defined paradigm through a novel sampling process called phantom sampling. The rest of this article is organized as follows: section 2 outlines the proposed method, section 3 discusses related works on the basis of the properties we have presented, section 4 presents the design of our experiments along with the results and section 5 provides concluding remarks."}, {"heading": "2. Proposed method", "text": "Our design begins at Sb. Although Sb and Si may train at various speeds and begin at various times, in this presentation we focus on the systems that mimic the following chronology of events:\n1. Sb trains a generative model Gb and a discriminative model Nb for P (xb) and PNb(y|xb) using (xb, yb) \u2208 Db, respectively.\n2. Sb broadcasts Gb and Nb.\n3. Si collects the models Gb and Nb and initializes new model Ni with the parameters of Nb adding new random parameters as appropriate. Expansion using new random parameters is required since, Ni should make predictions on a larger range of labels.\n4. Using Di together with phantom sampling from Gb and Nb, Si trains the model Ni until convergence.\nThis is an asymptotic special case of the definition established in the previous section and is therefore considered. Other designs could also be established and we will describe briefly a generalized approach in the latter part of this section. While the strategy we propose could be generalized to any discriminatory multi-class classifier, for the sake of clarity and being precise, in this article we restrict our discussions to the context of deep neural networks.\nThe generative model, Gb models P (x|Db). In this article we considered networks that are trained as simple generative adversarial networks (GAN) for our generative models. GANs have recently become very popular for approximating and sampling from distributions of data. GAN was originally proposed by Goodfellow et. al, in 2014 and has since seen many advances [5]. We consider the GANs proposed in the original article by Goodfellow et. al, for the sake of convenience. We use a simple convolutional neural network model as the discriminator Nb. Figure 2 shows the overall architecture of our strategy with Gb and Nb within the Sb capsule. As can be seen, Gb attempts to produce samples that are similar to the data and Nb learns a classifier using the softmax layer that is capable of producing PNb(y\nb|xb) as follows:PNb(y = 1|x b)\n... PNb(y = j|xb)  = 1\u2211j p=1 e w (p) b N \u2032 b(x)  ew (1) b N \u2032 b(x) ... ew (j) b N \u2032 b(x)  , (1) where, wb is the weight matrix of the last softmax layer with w(p)b representing the weight vector that produces the output of the class p and N \u2032b(x) is the output of the layer in Nb, immediately preceding the softmax layer. Once this network is trained, Sb broadcasts these models.\nAt Si, a new discriminative model Ni is initialized with the parameters of Nb. Nb is trained (and has the ability) to only make predictions on the label space of Db, i.e. {1 . . . j}. The incremental learner model Ni therefore, cannot be initialized with the same weights in the softmax layer of Nb alone. Along with the weights for the first j classes, Ni should also be initialized with random parameters as necessary to allow for the prediction on a combined incremental label space of {1 . . . c}. We can simply do the following assignment to get the desired arrangement:\nw (p) i =\n{ w\n(p) b , if p \u2208 {1 . . . j} N (0, 1), if p \u2208 {j + 1 . . . c} . (2)\nEquation 2 describes a simple strategy where the weight vectors are carried over to the first j classes and random weight vectors are assigned to the rest of the c \u2212 j classes. In figure 2, the gray weights in Ni represent those that are copied and the red weights represent the newly initialized weights.\nWe now have at Si, a network that will generate samples from the distribution of P (xb) and an initialized network Ni whose layers are setup with the weights from Nb. To train this network on Di, if we simply ignore Gb and train the network with samples (xi, yi) \u2208 Di, we will run into the catastrophic forgetting problem as discussed in figure 1. To avoid this, we can use samples queried from Gb (such samples are notationally represented as Gb(z) to indicate\nsampling using a random vector z) and use these samples to avoid forgetting. However we do not have targets for these samples to estimate an error with. Phantom sampling will help us to acquire targets.\nDefinition 1. A phantom sampler is a process of the following form:\nP : (z, T,Nb, Gb)\u2192 {Gb(z), PNb(y|Gb(z), T )}. (3)\nwhere, y \u2208 {0 . . . j} and T is a temperature parameter which will be described below. Using Nb and Gb, we can use this sampling process to generate sets of sampletarget pairs that simulate samples from the datasetDb. Simply using PNb(y\nb|xb) is not possible as we do not have access to xb at Si, and Si is not allowed to communicate with Sb regarding the data due to the data membrane condition described in property 1. We can however replace xb with\nGb(z) and use the generated samples to produce targets from this network for the generated samples itself. This is justifiable since Gb(z) is learnt to hallucinate samples from P (xb). However, given that we only use a simple GAN and that the samples are expected to be noisy, we might get corrupted and untrustworthy targets. GANs have not advanced sufficiently to a degree where perfect sampling is possible at the image level, at the moment of writing this article. As GAN technology improves, much better sampling could be achieved using this process.\nGiven that GANs (and any other similar generative models) are imperfect, often samples can have properties that are blended from two or more classes. In these cases, the targets generated from Nb might also be too high for only one of these classes, which is not optimal. To avoid this problem, we use a replacement for the softmax layer of Nb\nwith a new temperature-raised softmax layer,\nPNb(y = 1|x b, T )\n... PNb(y = j|xb, T )  = 1\u2211j p=1 e w (p) b N\u2032 b (x) T  e w (1) b N\u2032b(x) T ...\ne w\n(j) b N\u2032b(x) T  . (4)\nThis temperature-raised softmax for T > 1 (T = 1 is simply the softmax described in equation 1) provides a softer target which is smoother across the labels. It reduces the probability of the most probable label and provides rewards for the second and third most probable labels also, by equalizing the distribution. Soft targets such as the one described and their use in producing ambiguous targets exemplifying the relationships between classes were proposed in [8]. In this context, the use of soft targets for Gb(z) helps us get appropriate labels for the samples that may be poorly generated. For instance, a generated sample could be in between classes 8 and 0. The soft target for this will not be a strict 8 or a strict 0, but a smoother probability distribution over the two (all the) classes.\nWhile learning Ni, with a batch of samples from Di, we may simply use a negative log-likelihood with the softmax layer for the labels. To be able to back-propagate samples from phantom sampling, we require a temperature softmax layer at Ni as well. For this, we simply create a temperature softmax layer that share the weights wi, of the softmax layer of Ni, just as we did for Nb. This implies that Ni will have c\u2212j+1 additional units for which we would not have targets as phantom sampling will only provide us with targets for the first j classes. Given that the samples themselves are hallucinated from Gb(z), the optimal targets to assign for the output units [j+1 . . . c] of the temperature softmax layer are zero. Equivalently, we could simply avoid sharing the extra weights. Therefore along with the phantom sample\u2019s targets, we concatenate a zero vector of length [j + 1 . . . c]. This way, we could simply back-propagate the errors for the phantom samples also. The error for data from Di is,\ne(wi, x i \u2208 Di) = L(yi, argmax y PNi(y|xi)), (5)\nwhere, L represents an error function. The error for phantom samples is,\ne(wi, Gb(z)) = L(PNb(y|Gb(z), T ), PNi(y|Gb(z), T )). (6) Typically, we use a categorical-cross-entropy for learning labels and a root mean-squared error for learning softtargets.\nWhile both samples fromDi and from the phantom sampler are fed-forward through the same network, the weights are updated for two different errors. If the samples come from the phantom sampler, we estimate the error from the\ntemperature softmax layer and if the samples come fromDi, we estimate the errors from the softmax layer. For every k iterations of Db, we train with 1 iteration of phantom samples G(z). k is decided based on the number of classes that are in each set Db and Di.\nThus far we have assumed a certain chronology of events where Si begins training only after Sb is finished training. We could generalize this strategy of using phantom sampling when Si is already, partially trained by the time Sb finishes and triggers the incremental learning. In this case, we will not be able to re-initialize the network Ni with new weights, but as long as we have phantom samples, we can use a technique similar to mentor nets or fitnets, using embeded losses between Nb and Ni and transfer knowledge about Db to Ni [27] [30]. This strategy could also be extended to more than one increment of data in a straightforward manner. Using the same phantom sampling technique we could continue training the GAN to update it with the distributions of the new classes. Once trained, we can pass on this GAN and the newly trained net Ni to the next incremental site."}, {"heading": "3. Related Work", "text": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26]. In recent years, this problem has been tackled using special activation functions and dropout regularization. Srivastava et al. demonstrated that the choice of activation function affects catastrophic forgetting and introduced the Hard Winner Take All (HWTA) activation [29]. Goodfellow et al. argued that increased dropout works better at minimizing catastrophic forgetting compared to activation functions [6]. All these studies were made in regards to unavailability of data for particular classes, rather than in terms of incremental learning.\nWe find that most previous works in incremental learning, relaxes or violates the rigorous constraints that we have proposed for an incremental learner. While this may satisfy certain case studies, pertaining to each article, we find no work that has addressed our definition sufficiently. In this section, we organize our survey of existing literature in terms of the conditions they violate. Relaxing the data membrane: The following approaches relax property (1) to varying degrees. Mensink et al. develop a metric learning method to estimate the similarity (distance) between test samples and the nearest class mean (NCM) [18, 19]. The class mean vectors represent the centers of data samples belonging to different classes. The learned model is a collection class center vectors and a metric for distance measurement that is determined using the training data. The NCM approach has also been successfully applied to random forest based models for incremental learning in [25]. The nodes and leaves of the trees in\nthe NCM forest are dynamically grown and updated when trained with data from new classes. A tree of deep convolutional networks (DCNN) for incremental learning was proposed by Xiao et al. [32]. The leaves of this tree are CNNs with a subset of class outputs and the nodes of the tree are CNNs which split the classes. With the input of new data and classes, the DCNN grows hierarchically to accommodate the new classes. The clustering of classes, branching and tree growth is guided by an error-driven preview process and their results indicate that the incremental learning strategy performs better than a network trained from scratch.\nThe Learn++ is an ensemble based approach for incremental learning [23] [20]. Based on the Adaboost, the algorithm weights the samples to achieve incremental learning. The procedure, however requires every data batch to have examples from all the previously seen classes. In [14], Kuzborskij et al. develop a least squares SVM approach to incrementally update a N-category classifier to recognize N+1 classes. The results indicate that the model performs well only when the N+1 classifier model is also trained with some data samples from the previous N classes.\niCaRL is an incremental representation based learning method by Rebuffi et al. [24]. It progressively learns to recognize classes from a stream of labeled data with a limited budget for storing exemplars. The iCaRL classification is based on the nearest-mean-of-exemplars. The number of exemplars for each class is determined by a budget and the best representation for the exemplars is updated with existing exemplars and newly input data. The exemplars are chosen based on a herding mechanism that creates a representative set of samples based on a distribution [31]. This method while being very successful, violates the membrane property by transferring well-chosen exemplar samples. In our results section we address this idea by demonstrating that significant amount of (randomly chosen) samples are required to out-perform our strategy, which violates the budget criteria of the iCaRL methods. Relaxing data agnosticism: Incremental learning procedures that draw inference regarding previously trained data based on current batch of training data, can be viewed as violating this constraint. Li et al. use the base classifier Nb to estimate the conditional probabilities P (y\u0302|x) for x : (x, y) \u2208 Di. When training Ni with Di, they use these conditional probabilities to guide the output probabilities for classes y \u2208 [1, . . . , j] [16]. In essence, the procedure assumes that if Ni is trained in such a manner that P (y\u0302|x) for x : (x, y) \u2208 Di is the same for both classifier Nb and Ni, this ensures that P (y\u0302|x) for x : (x, y) \u2208 Db will also be the same. This is a strong assumption relatingDb andDi violating agnosticism. The authors Furlanello et al. develop a closely related procedure to in [4]. They train neural networks for the incremental classifier Ni by making sure the\nconditional probabilities P (y\u0302|x) for x : (x, y) \u2208 Di is the same for both Nb and Ni. The only difference compared to [16] is in the regularization of network parameters using weight decay and the network initialization. In another procedure based on the same principles, Jung et al. constrain the feature representations forDi to be similar to the feature representations for Db [11].\nOther models assume that the parameters of the classifiers wb for Nb and wi for Ni are related. Kirkpatrick et al. model the probability P (wb|Db) and get an estimate for the important parameters in wb [12]. When training Ni initialized with parameters wb, they make sure not to offset the important parameters in wb. This compromises the training of Ni under the assumption that important parameters in wb for Db are not important for Di.\nClosely related to the previous idea is pseudo-rehearsal proposed by Robins in 1995 [26]. Neuro-biological underpinnings of this work was also studied by French et. al, [3]. This method is a special case of ours if, the GAN was untrained and produces random samples. In other words, they used Nb to produce targets for random samples Gb(z) = z \u2192 N (0, 1), instead of using a generative model, similar to phantom sampling. This might partly be due to the fact that sophisticated generative models were not available at the time. This article also does not use soft targets such as those that we use because, for samples that are generated randomly, T = 1 is a better target. This article does not violate any of the properties that we required for our uncompromising incremental learner."}, {"heading": "4. Experiments and Results", "text": "To demonstrate our strategy we conduct thorough experiments on three benchmark datasets: MNIST dataset of handwritten character recognition, Street view housing numbers (SVHN) dataset and the CIFAR10 10-class visual object categorization dataset [15, 22, 13]. In all our experiments1 we train the Sb\u2019s GAN, Gb and base networks Nb using independent processes. The network parameters of all these models are written to drive, which simulates broadcasting the networks. Once trained, the datasets that are used to train and test these methods are deleted, simulating the data membrane and the process are killed.\nWe then begin Si as an independent process in keeping with site independence. This uses a new dataset which is setup in accordance with property 1. Networks Gb and Nb\u2019s parameters are loaded but only in their feed-forward operations. Two identical columns of networks N1i and N 2 i that share weights are built. These are initialized with the parameters of Nb, one with temperature and one without. By virtue of the way they are setup, updating the weights on\n1Our implementations are in theano and our code is available at https://github.com/ragavvenkatesan/ Incremental-GAN.\none, updates both the networks correspondingly. We feed forward k mini batches of data fromDb through the column that connects to the softmax layer and use the error generated here to update the weights for each mini batch. For every k updates of weights from the data, we update one mini batch of phantom samples from (Gb(z), PNb(y|Gb(z), T )). This is run until early termination or until a pre-determined number of epochs. Since we save the parameters of Gb after every epoch, we can load the corresponding GAN for our\nexperiments. In lieu of being fair, we train our baselines with twice as many iterations as phantom sampling. We use the same learning rate schedules, optimizers and momentums across all the architectures. We fix our temperature values using a simple grid search."}, {"heading": "4.1. MNIST dataset", "text": "For the MNIST dataset, for Gb we used a GAN Gb that samples 10 samples from a uniform 0-mean Gaussian. The\ngenerator part of the network has three fully-connected layers of 1200, 1200 and 784 neurons with ReLU activations for the first two and tanh activation for the last layers, respectively [21]. The discriminator part of Gb has two layers of 240 maxout-by-5 neurons [7]. This is an architecture that mimics the architecture used by Goodfellow et. al, closely [5]. All our discriminator networks across both sites Sb and Si are the same architecture which for the MNIST dataset is, two convolutional layers of 20 and 50 neurons each with filter sizes of 5 \u00d7 5 and 3 \u00d7 3 respectively, with max pooling by 2 on both layers. These are followed by two full-connected layers of 800 neurons each. All the layers in the discriminators are trained with batch normalization and weight decay with the fully-connected layers trained with a dropout of 0.5 [28, 9]. Results of the MNIST dataset are discussed in figure 3."}, {"heading": "4.2. CIFAR 10 and SVHN datasets", "text": "For both these datasets we used a generator model that samples 64 random samples. The number or neurons in subsequent fully-connected layers are 1200 and 5408 respectively. This is followed by two fractionally-strided or transposed convolution layers with filter sizes 3 \u00d7 3 and 5 \u00d7 5 respectively. Apart from the last layer that generates the\n32X32 image, every layer has a ReLU activation. The last layer uses a tanh activation. Our discriminator networks including the discriminator part of the GANs have six convolutional layers with neurons 20, 50, 50, 100, 100 and 250 respectively. Excepting the first layer, which has a filter size of 5\u00d75, every layer has filter sizes of 3\u00d73. Every third layer maxpools by 2. These are followed by two fully-connected layers of 1024 nodes each. All activations are ReLU.Results or the CIFAR 10 dataset are discussed in figure 4 and that of SVHN are discussed in figure 5. CIFAR 10 and SVHN contain three channel full-color images that are sophisticated. GANs as proposed by Goodfellow et. al, are not sophisticated enough to generate reasonably good looking samples. Without complicating our generator models, we generated images as best as we could, therefore the results proposed here could be improved significantly with the invention (or adoption) of better generative models. Some of the images that we synthesized using our GANs are shown in figure 6 for reference."}, {"heading": "5. Conclusions", "text": "In this paper, we redefined the problem of incremental learning, in its most rigorous form so that it can be a more realistic model for important real-world applications. Using a novel sampling technique involving generative models and the distillation technique, we implemented a strategy to hallucinate samples with appropriate targets using models that were previously trained and broadcast. Without having access to historic data, we demonstrated that we could still implement an uncompromising incremental learning system without relaxing any of the constraints of our definitions. We show strong and conclusive results on three benchmark datasets in support of our strategy."}], "references": [{"title": "Theano: A cpu and gpu math compiler in python", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proc. 9th Python in Science Conf, pages 1\u20137", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Catastrophic interference in connectionist networks: Can it be predicted", "author": ["R.M. French"], "venue": "can it be prevented? In Proceedings of the 6th International Conference on Neural Information Processing Systems, pages 1176\u20131177. Morgan Kaufmann Publishers Inc.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Pseudo-recurrent connectionist networks: An approach to the\u2019sensitivity-stability\u2019dilemma", "author": ["R.M. French"], "venue": "Connection Science, 9(4):353\u2013380", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Active long term memory networks", "author": ["T. Furlanello", "J. Zhao", "A.M. Saxe", "L. Itti", "B.S. Tjan"], "venue": "arXiv preprint arXiv:1606.02355", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2672\u20132680", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "ICML (3), 28:1319\u20131327", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Types of incremental learning", "author": ["P. Jantke"], "venue": "AAAI Symposium on Training Issues in Incremental Learning, pages 23\u201325", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Less-forgetting learning in deep neural networks", "author": ["H. Jung", "J. Ju", "M. Jung", "J. Kim"], "venue": "arXiv preprint arXiv:1607.00122", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["J. Kirkpatrick", "R. Pascanu", "N. Rabinowitz", "J. Veness", "G. Desjardins", "A.A. Rusu", "K. Milan", "J. Quan", "T. Ramalho", "A. Grabska-Barwinska", "D. Hassabis", "C. Clopath", "D. Kumaran", "R. Hadsell"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "From n to n+1: Multiclass transfer incremental learning", "author": ["I. Kuzborskij", "F. Orabona", "B. Caputo"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 3358\u20133365", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning without forgetting", "author": ["Z. Li", "D. Hoiem"], "venue": "Proceedings of the European Conf. on Computer Vision (ECCV), pages 614\u2013629. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "Psychology of learning and motivation, 24:109\u2013165", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "Proceedings of the European Conf. on Computer Vision (ECCV), pages 488\u2013501. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance-based image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, 35(11):2624\u20132637", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learn++", "author": ["M.D. Muhlbaier", "A. Topalis", "R. Polikar"], "venue": "nc: Combining ensemble of classifiers with dynamically weighted consult-and-vote for efficient incremental learning of new classes. IEEE Trans. on Neural Networks, 20(1):152\u2013 168", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Learn++: An incremental learning algorithm for supervised neural networks", "author": ["R. Polikar", "L. Upda", "S.S. Upda", "V. Honavar"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, part C (Applications and Reviews), 31(4):497\u2013508", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "iCaRL: Incremental classifier and representation learning", "author": ["S.-A. Rebuffi", "A. Kolesnikov", "C.H. Lampert"], "venue": "accepted to the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Incremental learning of NCM forests for large-scale image classification", "author": ["M. Ristin", "M. Guillaumin", "J. Gall", "L. Van Gool"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 3654\u20133661", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Catastrophic forgetting", "author": ["A. Robins"], "venue": "rehearsal and pseudorehearsal. Connection Science, 7(2):123\u2013146", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, pages 2310\u20132318", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Diving deeper into mentee networks", "author": ["R. Venkatesan", "B. Li"], "venue": "arXiv preprint arXiv:1604.08220", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Herding dynamical weights to learn", "author": ["M. Welling"], "venue": "Proceedings of the ACM Intl. Conf. on Machine Learning (ICML), pages 1121\u20131128", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Errordriven incremental learning in deep convolutional neural network for large-scale image classification", "author": ["T. Xiao", "J. Zhang", "K. Yang", "Y. Peng", "Z. Zhang"], "venue": "Proceedings of the ACM Intl. Conf. on Multimedia (ACM-MM), pages 177\u2013 186", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "One of the earliest formalization of incremental learning comes from the work of Jantke [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "A temperature raised softmax was previously proposed as a means of distilling knowledge in the context of neural network compression [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Not only does this provide supervision for generated samples, but will also serve as a regularizer while training a machine at Si, similar to the fashion described in [8].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "al, in 2014 and has since seen many advances [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Soft targets such as the one described and their use in producing ambiguous targets exemplifying the relationships between classes were proposed in [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 25, "context": "In this case, we will not be able to re-initialize the network Ni with new weights, but as long as we have phantom samples, we can use a technique similar to mentor nets or fitnets, using embeded losses between Nb and Ni and transfer knowledge about Db to Ni [27] [30].", "startOffset": 259, "endOffset": 263}, {"referenceID": 28, "context": "In this case, we will not be able to re-initialize the network Ni with new weights, but as long as we have phantom samples, we can use a technique similar to mentor nets or fitnets, using embeded losses between Nb and Ni and transfer knowledge about Db to Ni [27] [30].", "startOffset": 264, "endOffset": 268}, {"referenceID": 15, "context": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26].", "startOffset": 89, "endOffset": 100}, {"referenceID": 1, "context": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26].", "startOffset": 89, "endOffset": 100}, {"referenceID": 24, "context": "Catastrophic Forgetting: Early works by McCloskey, French and Robins outlines this issue [17, 2, 26].", "startOffset": 89, "endOffset": 100}, {"referenceID": 27, "context": "demonstrated that the choice of activation function affects catastrophic forgetting and introduced the Hard Winner Take All (HWTA) activation [29].", "startOffset": 142, "endOffset": 146}, {"referenceID": 5, "context": "argued that increased dropout works better at minimizing catastrophic forgetting compared to activation functions [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 16, "context": "develop a metric learning method to estimate the similarity (distance) between test samples and the nearest class mean (NCM) [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "develop a metric learning method to estimate the similarity (distance) between test samples and the nearest class mean (NCM) [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 23, "context": "The NCM approach has also been successfully applied to random forest based models for incremental learning in [25].", "startOffset": 110, "endOffset": 114}, {"referenceID": 30, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The Learn++ is an ensemble based approach for incremental learning [23] [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "The Learn++ is an ensemble based approach for incremental learning [23] [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "In [14], Kuzborskij et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The exemplars are chosen based on a herding mechanism that creates a representative set of samples based on a distribution [31].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": ", j] [16].", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "develop a closely related procedure to in [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "The only difference compared to [16] is in the regularization of network parameters using weight decay and the network initialization.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "constrain the feature representations forDi to be similar to the feature representations for Db [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "model the probability P (wb|Db) and get an estimate for the important parameters in wb [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Closely related to the previous idea is pseudo-rehearsal proposed by Robins in 1995 [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "al, [3].", "startOffset": 4, "endOffset": 7}, {"referenceID": 20, "context": "To demonstrate our strategy we conduct thorough experiments on three benchmark datasets: MNIST dataset of handwritten character recognition, Street view housing numbers (SVHN) dataset and the CIFAR10 10-class visual object categorization dataset [15, 22, 13].", "startOffset": 246, "endOffset": 258}, {"referenceID": 12, "context": "To demonstrate our strategy we conduct thorough experiments on three benchmark datasets: MNIST dataset of handwritten character recognition, Street view housing numbers (SVHN) dataset and the CIFAR10 10-class visual object categorization dataset [15, 22, 13].", "startOffset": 246, "endOffset": 258}, {"referenceID": 24, "context": "Even a GAN that is not trained (implying temperature softmax gained for samples that are purely noise), also produce some recognition performance as was already demonstrated in the work by Robins [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "generator part of the network has three fully-connected layers of 1200, 1200 and 784 neurons with ReLU activations for the first two and tanh activation for the last layers, respectively [21].", "startOffset": 187, "endOffset": 191}, {"referenceID": 6, "context": "The discriminator part of Gb has two layers of 240 maxout-by-5 neurons [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "al, closely [5].", "startOffset": 12, "endOffset": 15}, {"referenceID": 26, "context": "5 [28, 9].", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": "5 [28, 9].", "startOffset": 2, "endOffset": 9}], "year": 2017, "abstractText": "Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these tricks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique, phantom sampling.We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets through our strategy, we demonstrate that strict incremental learning could be achieved.", "creator": "LaTeX with hyperref package"}}}