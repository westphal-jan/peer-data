{"id": "1705.02077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews", "abstract": "true argumentation ann mining optimization aims exclusively at automatically extracting down the ubiquitous premises - claim logic discourse structures in automated natural language linguistics texts. there is is a great scholarly demand for stable argumentation input corpora for customer external reviews. however, due to the controversial structural nature of the primary argumentation annotation reclamation task, meaning there exist very few large - scale argumentation corpora for customer reviews. early in pursuing this hypothetical work, modern we practically novelly use the data crowdsourcing aggregator technique to collect accumulated argumentation text annotations results in one chinese website hotel reviews. credited as the first chinese argumentation dataset, truly our corpus includes 4814 argument certificate component annotations and 411 258 argument relation annotations, and its 46 annotations qualities are more comparable slightly to some seven widely used argumentation corpora in twelve other classical languages.", "histories": [["v1", "Fri, 5 May 2017 03:43:35 GMT  (285kb,D)", "http://arxiv.org/abs/1705.02077v1", "6 pages,3 figures,This article has been submitted to \"The 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC2017)\""]], "COMMENTS": "6 pages,3 figures,This article has been submitted to \"The 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC2017)\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mengxue li", "shiqiang geng", "yang gao", "haijing liu", "hao wang"], "accepted": false, "id": "1705.02077"}, "pdf": {"name": "1705.02077.pdf", "metadata": {"source": "CRF", "title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews", "authors": ["Mengxue Li", "Shiqiang Geng", "Yang Gao", "Haijing Liu", "Hao Wang"], "emails": ["mengxue2015@iscas.ac.cn,", "gsqwxh@163.com,", "wanghao}@iscas.ac.cn"], "sections": [{"heading": "1. Introduction", "text": "In customer reviews, users usually not only give their opinions on the products/services, but also provide reasons supporting their opinions. For example, consider the following review excerpt posted on Tripadvisor.com:\nExample 1: 1\u00a9\u623f\u95f4\u7684\u7535\u5668\u8bbe\u65bd\u8ba9\u4eba\u5f88\u5931\u671b\u3002 2\u00a9 \u6709\u4e00\u53f0\u5f88\u8001\u5f88\u5c0f\u7684\u9ed1\u767d\u7535\u89c6\u3002 3\u00a9 \u7a7a\u8c03\u4e5f\u662f \u574f\u7684\u3002 1\u00a9 Appalling in room electrical facilities. 2\u00a9 There was an old, small, black TV. 3\u00a9 Air conditioner did not work.\nClause 1\u00a9 gives the customer\u2019s opinion (or claim) on the appliances in the room, and clauses 2\u00a9 and 3\u00a9 are reasons/evidences (or premises) supporting the claim. Such discourse structures are known as arguments [11], and the techniques for automatically extracting arguments and their relations (e.g. support/attack) from natural language texts are known as argumentation mining [10]. Performing argumentation mining on customer reviews can reveal the reasons behind users\u2019 opinions, thus can greatly facilitate the product producers and service providers to figure out their weaknesses and hence has huge commercial potentials.\nThere exist a great demand for reliably annotated corpora on customer reviews, since they are required for training supervised-learning-based argumentation mining techniques. Existing argumentation corpora are mostly constructed from highly professional genres, e.g. legal documents [12], persuasive essays [16], newspapers and court cases [13]. Compared to these genres, customer reviews\n\u2217The first two authors contributed equally to this work and should be considered co-first authors.\nare written by ordinary people in casual scenarios, thus their linguistic complexities are usually lower and do not contain much domain knowledge; as a result, we believe that even novice people are able to identify the argumentation structures in customer reviews. Crowdsourcing has been widely recognised as a reliable and economic method for some annotating tasks [14]. In this work, we investigate the applicability of crowdsourcing for argumentation annotation in Chinese hotel reviews. Specifically, the contributions of this work are threefold:\n\u2022 We propose a novel argumentation model1 for hotel reviews, which extends the classic \u201cpremise-claim\u201d model, and can be potentially used for defining argumentation structures in other types of customer reviews and in other languages. \u2022 We novelly employ crowdsourcing to annotate argumentation structures (i.e. argument components and their relations) in Chinese hotel reviews, design some mechanisms so as to help the workers reduce their chances of making mistakes, and use a clustering algorithm to aggregate collected annotations. \u2022 The aggregated annotations are published as a publicly available corpus, and the annotating quality of the corpus is comparable to the state-of-the-art English argumentation corpora. Furthermore, because of the controversial nature of the argumentation annotation task, we provide a confidence score to each label, so as to help users understand the controversy degree of each annotation. To the best of our knowledge, this is the first Chinese argumentation corpus, and the first use of confidence score in argumentation corpora."}, {"heading": "2. Related Work", "text": "We first review existing argumentation corpora for customer reviews; in particular, we highlight the argumentation models they used to define arguments. After that, we review works on crowdsourcing for argumentation annotation and some related tasks, e.g. annotating discourse structures.\n1. An argumentation model gives the definition of arguments, e.g. what components an argument is consisting of, what kinds of relations are allowed between different argument and argument components.\nar X\niv :1\n70 5.\n02 07\n7v 1\n[ cs\n.C L\n] 5\nM ay\n2 01\n7"}, {"heading": "2.1. Argumentation Corpora", "text": "A comprehensive review on argumentation corpora is beyond the scope of this paper; good overviews can be found in e.g. [5], [10], [16]. Here we only review argumentation corpora constructed from customer reviews.\nWachsmuth et al. [19] built the ArguAna corpus, consisting of 2.1k hotel reviews posted on Tripadvisor.com. Instead of directly labelling arguments, they annotate statements and sentiments polarities. A statement is \u201cat least a clause and at most a sentence that is meaningful on its own\u201d. They designed a rule-based tool to segment statements, and employed crowdsourcing to annotate the sentiment and features (e.g. location, services, facilities) in each statement. Results suggested that crowdsourcing workers can reliably identify the sentiments of statements (approval rate 72.8%) but have controversies for identifying features (rejection rate 43.3%). We view ArguAna as an intermediate resource for building argumentation corpora, because each argument usually contains several statements (e.g. one positive/negative statement serving as the claim, and several neutral statements serving as premises). Thus, ArguAna cannot be directly used for training argumentation mining techniques.\nGarcia Villalba and Saint-Dizier [18] investigated suitable argumentation models for customer reviews. They viewed many different type of expressions (e.g. illustrations, elaborations and reformulations) as argument components, and built a corpus consisting of 50 customer reviews in French and English in the domains of hotels and restaurants, hifi products, and the French political campaign. Wyner et al. [20] built a corpus consisting of 84 reviews (posted on Amazon.com) for one specific camera model. They considered one specific argumentation model: an argument consists of two premises (premise 1 gives \u201ccamera X has property P\u201d and premise 2 gives \u201cproperty P promotes value V\u201d) and one claim (the customer should perform action ACT; possible ACT include \u201cbuy the camera\u201d, \u201cavoid using the flashlight\u201d, etc.). However, for both these corpora, their inter-rater agreement (IRA)2 were not reported, and they were not publicly available."}, {"heading": "2.2. Crowdsourcing for Argumentation Annotation and Related Tasks", "text": "Ghosh et al. [3] proposed an annotation mechanism to annotate arguments and their relations in blog comments: they hired crowdsourcing workers to label claims-premises relations. Note that the arguments segments were provided a priori (annotated by domain experts), and the crowdsourcing workers were asked to only label the argument component types and relations. They reported that crowdsourcing workers achieved 0.45-0.55 IRA score (in terms of multi-\u03c0 [2]), suggesting the agreement is moderate; also, they suggested\n2. IRA is a widely used metric to evaluate the annotation quality. There exist multiple methods for computing the IRA score; in this paper, for each IRA score, we will point out the computation method it uses. Larger IRA values suggest higher agreement between the annotators, thus suggest higher reliability of the obtained annotations.\nthat the agreement scores of the crowdsourcing workers is highly correlated to those of the expert annotators.\nCrowdsourcing has been used to annotate discourse structures. Kawahara et al. [7] designed a two-stage crowdsourcing mechanism to annotate two levels of discourse relations in Japanese texts crawled from multiple online genres. In their work, discourse relations include contrast, concession, cause-effect, etc., and these relations closely resemble some relations in argumentation structures (e.g. the attack relation between arguments can be viewed as contrast, and the premise-claim relation is closely related to the cause-effect relation). They did not report the IRA of the crowdsourcing workers, but instead, testified the quality of their discourse corpus by training a discourse parser on their corpus. Results suggested that the quality of their corpus is comparable to the state-of-the-art English discourse corpus, indicating that crowdsourcing can be reliably used for annotating relations between clauses."}, {"heading": "3. Pre-Study", "text": "In this section, we describe how we design our annotation guideline. In particular, we perform some preliminary annotating experiments to decide i) how to segment clauses (e.g. by rule-based automatic methods or by crowdsourcing workers), and ii) which argument model to use, i.e. which argument components constitute an argument, and what relations are permitted between argument components. Since most existing annotation guidelines are for English texts, we annotate twenty English and ten Chinese hotel reviews (without titles) from Tripadvisor.com to draft our guideline. Five annotators participate in the experiments; they are all Chinese native speakers fluent in English. The annotation is performed on the brat [17] open-source annotation platform.\nAs for clause segmenting, we test two approaches: subsentence based segmenting [19] (i.e. viewing each subsentence as a clause), and free segmenting (i.e. any span of texts can be viewed as a clause). We find that the free segmenting strategy is more suitable for Chinese hotel reviews, because punctuations are often missing or misused. In addition, the free segmenting strategy enables the annotators to label the exact boundary of each argument component, avoiding including some connecting words in argument components (e.g. \u201c\u800c\u4e14\u201d(in addition)).\nAs for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents. Our experiments suggested that:\n\u2022 In both Chinese and English hotel reviews, users often give their overall impression on hotels (e.g. \u5f3a\u70c8\u63a8\u8350(strongly recommend), \u6211\u89c9\u5f97\u662f\u5f88\u4e0d\u9519 \u7684\u9152\u5e97(I think the hotel is quite good), \u5f88\u7f8e\u597d \u7684\u56de\u5fc6(living here leaves me beautiful memories)). We believe the MajorClaim component in the PCM model is the most suitable argument component type for labelling these clauses.\n\u2022 In hotel reviews, users often give evidences supporting some implicit claims: consider the clause \u201c\u79bb\u5e02 \u4e2d\u5fc3\u8d70\u8def\u4e0d\u5230\u4e94\u5206\u201d (just 5 minutes\u2019 walk to the city center), we believe this clause is not only for stating the location of the hotel, but also supports some claims (e.g. the location of the hotel is good), although these claims are not explicitly presented in the hotel reviews. We thus believe that the argument component premise supporting implicit claim (PSIC) should be used in our argumentation model. \u2022 Distinguishing different kinds of premises (e.g. grounds warrants, refutation) in hotel reviews is error-prone even for expert annotators; thus we decide not to use the ETM model, although ETM is proposed for short user-generated documents.\nOur argumentation model is an extension and integration of the PCM and ECP models; it includes four argument components: MajorClaim, Claim, Premise and PSIC; texts not labelled as any argument component are non-argumentative (NA for short). Premises are allowed to support/attack claims, but claims are not allowed to support other claims, because this will lead to cascading support, which overcomplicates the annotating process [4]. In addition, annotators are also asked to annotate the sentiment polarity (positive, negative or neutral) for MajorClaims and Claims, because these two argument components are subjective. Fig. 1 illustrates our argumentation model, and some examples are given in Table 1. Note that each premise must support/attack some claim, but a claim may have no premises supporting/attacking it.\nThe final annotation guideline is in Chinese, including detailed explanations of the argument model, segmenting rules as well as considerable illustrative examples. To test the readability and applicability of our annotation guideline, we ask additional two students (Chinese native speakers) to independently annotate five Chinese hotel reviews using our annotation guideline. The average IRA in terms of Krippendorff\u2019s \u03b1U [9] for their annotations is 0.715, suggesting that the agreement is substantial."}, {"heading": "4. Crowdsourcing Experiments", "text": "Our crowdsourcing experiment is performed as an optional assignment in the social media mining course in University of Chinese Academy of Sciences in 2017. Over fourhundred MSc students are registered for this course, above 90% are Chinese native speakers. We call for volunteers\nto participate in our experiment, and inform them that the participated students can obtain the corpus and its statistics in return, which they can use in their final project to train some machine learning algorithms. At last, 388 students participate and we give an one-hour tutorial to help them go through the guideline and to illustrate some examples.\nThe crowdsourcing is performed on the brat [17] platform. Each student can only view the hotel reviews assigned to him/herself. To help the annotators reduce mistakes, we customise and extend the original brat system so that i) only legal relations (see Fig. 1) are allowed to annotate, ii) the annotator is reminded if the sentiments for some MajorClaims or Claims are not labelled, and iii) the annotator is reminded if there exist some premises that do not support/attack any claims (this is not allowed; see Sect. 3).\nEach student receives twenty-five hotel reviews to annotate; one in these twenty-five reviews is a gold standard review, which has been annotated by our expert annotators in our pre-study and is used to evaluate the devotedness of the annotator. Each non-gold-standard hotel review is allocated to 4 students to independently annotate. All hotel reviews are crawled from Tripadvisor.com.cn, and their length are all between 100 to 150 words. Students are asked to finish all labelling in one week. At last, we collect 2332 hotel reviews\u2019 annotations. Because around 10% students (38 students) fail to annotate all document assigned to them, some documents receive only one student\u2019s annotation, and we remove these documents. In addition, we remove the annotations that violate our annotation guideline. In total, 7 hotel reviews and their annotations are removed.\n< 0 0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 number of reviews614 216 266 283 248 205 181 137\nWe compute \u03b1U for each hotel review3 so as to evaluate the quality of annotations. The distribution of \u03b1U scores is presented in Fig. 2. We find that only 21% documents receive \u03b1U \u2265 0.5, suggesting that the annotations for most documents are quite diverging and cannot be directly used to build the corpus. We believe that two factors may have resulted in the low annotation agreement: i) the low devotedness of some annotators, and ii) the controversial nature of argumentation annotation in some hotel reviews. In the next section, we will give a specific analysis of these two reasons and attempt to improve the annotation quality along these two lines, i.e. by removing less-devoted students\u2019 annotations and by identifying controversial sentences, so as to produce a high-quality Chinese hotel review corpus."}, {"heading": "5. Post-Processing and Corpora Generation", "text": "We identify the less-devoted students and remove their annotations in Sect. 5.1, compute the confidence score for annotations and generate the final corpora in Sect. 5.2, and perform the error analysis in Sect. 5.3."}, {"heading": "5.1. Remove Less-Devoted Students\u2019 Annotations", "text": "We use the gold standard annotations to evaluate each student\u2019s devotedness. The gold standard texts are similar to the examples in the guideline, thus we believe that the students whose annotations diverge widely from the gold standard annotations are less devoted. For each sentence in a gold standard text, we compute all students\u2019 agreement (in terms of \u03b1U ) against the gold standard annotation on this sentence, and rank these agreement scores. If a student\u2019s agreement score on this sentence falls into the bottom 10%, we increase this student\u2019s less-devotedness degree by 1. Students whose less-devotedness degrees are equal or larger than 2 are labelled as less-devoted, and\n3. At the moment, the \u03b1U for a hotel review considers only the annotations for component types, and ignores the agreement for annotations on sentiment and relations, because we observe that the agreement scores for sentiment and relations are highly correlated to the \u03b1U for argument component. Detailed agreement for sentiment and relations will be presented in Sect. 5.2.\nall of their annotations are removed. In total, we found 38 less-devoted students, and we additionally remove 39 hotel reviews because they receive fewer than two students\u2019 annotations after deleting the less-devoted annotations.\nWe find that removing the less-devoted students\u2019 annotations can indeed improve the annotation quality as a whole. For example, after removal, the percentage of reviews whose \u03b1U \u2265 0.5 increases from 21% to 24%. In addition, the average agreement score for each gold standard text has also be increased thanks to the removal (see Table 2). However, even after the removal, there still exist considerable controversial annotations, due to the controversial nature of our guideline and the argument annotation task itself. Next, we will identify the controversial texts, remove their annotations and so as to obtain high-quality corpora."}, {"heading": "5.2. Dealing With Controversial Annotations and Obtain the Final Corpora", "text": "By manually reading and analysing the annotations, we find that the hotel reviews generally fall into two categories: i) easy reviews, in which the argument component type of each sentence is quite clear and their relations are easy to identify; and ii) controversial reviews, in which a high percentage (over 30%) of sentences meet multiple argument component types\u2019 definitions, and their labels are heavily dependent on their contexts. For the easy reviews, we can obtain the annotations for the argument component, sentiments and relations; for the controversial reviews, although many sentences have controversial annotations, we may still be able to find some less-controversial sentences and obtain their annotations. Thus, we build two corpora based on the annotations we have collected: one consists of easy reviews, and the other consists of the relatively less-controversial sentences in controversial reviews, so as to extract as much useful information from the annotations as possible.\nReviews whose \u03b1U scores are equal or larger than 0.6 are marked as easy reviews; we find that the agreement for relations and sentiments annotations are also high among these reviews (details are given in Sect. 5.2). In total, 316 hotel reviews are marked as easy reviews. The remaining 1911 hotel reviews are marked as controversial, and we segment them by sentences, so as to find the less-controversial sentences therein. In total, the controversial hotel reviews include 5212 sentences; among them, sentences whose \u03b1U is equal or larger than 0.7 are marked as less-controversial sentences, accounting for around one-fourth sentences (1452/5212).\n5.2.1. Easy Reviews Corpus. We first need to aggregate the annotations for argument components, which lays the\nfoundation for aggregating sentiments and relations. As a concrete example, consider a sentence and its annotations presented in Fig. 3. We can see that three annotations are different in their boundaries annotations as well as on their argument component types annotations; to obtain a converged annotation from these diverging annotations, two sub-tasks are involved: i) resolve the conflicts between argument component boundaries, so as to obtain clause texts, and ii) decide the argument component type for the obtained clause.\nWe employ the K-means clustering technique [6] to perform the argument component aggregation, which can perform the above two subtasks as a whole. Specifically, we vectorise each student\u2019s annotation in one-hot manner: each character is represented by 5 digits, representing the student\u2019s annotation for this character. We perform 1-cluster clustering on the annotation vectors, and the centroid of the clustering gives the boundary as well as the argument component type. Still consider the example in Fig. 3: the first 5 digits in the centroid is (0.33,0,0,0,0.67), where the first digit corresponds to MajorClaim and the last digit corresponds to Non-Argumentative, and thus we label it as Non-argumentative, and the confidence of this label is 0.67. By computing the centroid, we not only aggregate the component annotations, but also obtain the confidence score for each label. Some statistics of the argument components annotations in easy reviews are presented in Table 3, and according to our statistics, the average of each review is annotated by three students; as for IRA metrics, besides \u03b1U , we also use percentage agreement, multi-\u03c0 [2] and Krippendorff\u2019s \u03b1 [8].\nBased on the converged annotations for components, we first evaluate the quality of annotations for sentiments and reviews in the easy hotel reviews. Table 4 presents the agreement scores for sentiments and relations. We can see that almost all scores are over 0.5, suggesting the agreement is substantial. Thus, we directly employ the majority voting technique to aggregate the annotations for relations and sentiments, and obtain the easy review corpus.\n5.2.2. Less-Controversial Sentences Corpus. Again, we use K-means to aggregate the annotations for argument component in the less-controversial sentences. Some statistics of argument component annotations are presented in Table 5, and each sentence has an average of three students to annotate. We can see that the size of the less-controversial\nsentence corpus is even larger than the less-controversial review corpus, indicating that much useful information can be extracted even from the highly diverging annotations. As for the relations annotations in controversial hotel reviews, among all pairs of less-controversial sentences, only 5% have been annotated (by as least one annotator) as having relations; thus, we do not aggregate the relation annotations for the less-controversial sentences. The sentiment scores for these sentences are presented in Table 6."}, {"heading": "5.3. Error Analysis", "text": "In order to study the disagreements in the annotations, we created confusion probability matrices (CPM) [1] for\nargument components annotations. A CPM contains the conditional probabilities that an annotator gives a certain label (column) given that another annotator has chosen the label in the row for a specific item. For example, CPM for the easy review corpus and the less-controversial sentences corpus are presented in Table 7, and the upper-left cell in Table 7 means that, when some other annotators have labelled a clause as MajorClaim, an annotator will have 0.481 probability to label this clause also as MajorClaim.\nFrom Table 7 we can see that there exist no significant confusion between annotations in the less-controversial sentences corpus, and we believe the reason is the strict criterion for selecting less-controversial sentences (\u03b1U \u2265 0.7; see Sect. 5.2). However, in the easy reviews corpus (see Table 7), we find that the confusion between NA and PSIC is significant. As a concrete example, consider the following sentence obtained from the easy reviews corpus: \u201c\u9152\u5e97\u65c1 \u8fb9\u6709\u4e2a\u5f88\u6709\u7279\u8272\u7684\u9152\u5427\uff0c\u975e\u5e38\u559c\u6b22\u90a3\u91cc\u3002\u201d (\u201cThere is a very special bar next to the hotel; I like the bar very much.\u201d). Some annotators label this sentence as a PSIC, as they believe that this sentence supports an implicit claim \u201chotel locates at a convenient place\u201d; however, some other annotators label this sentence as NA, because the sentence says nothing about the hotel itself. As we do not provide a candidate list of implicit claims, different annotators naturally have different understandings of PSIC, resulting in the high confusion between PSIC and NA. A possible solution is to provide a candidate list of implicit claims; we leave this as future work."}, {"heading": "6. Conclusion", "text": "In this work, we present the first Chinese argumentation corpus, and present the crowdsourcing technique we used to build this corpus. The argumentation model used in corpus extends some classic models, and we believe it is suitable for product reviews in general. In particular, we novelly use the clustering technique to aggregate annotations, which can not only resolve annotation conflicts, but also provide a confidence score at the same time. The annotation quality of our corpus is comparable to some widely used argumentation\ncorpora in other languages. To stimulate further research, we make the corpus publicly available4."}], "references": [{"title": "Managing uncertainty in semantic tagging. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840\u2013850", "author": ["Silvie Cinkov\u00e1", "Martin Holub", "Vincent Kr\u0131\u0301\u017e"], "venue": "Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Measuring nominal scale agreement among many raters", "author": ["J.L. Fleiss"], "venue": "Psychological Bulletin,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1971}, {"title": "Analyzing argumentative discourse units in online interactions", "author": ["D. Ghosh", "S. Muresan", "N. Wacholder", "M. Aakhus", "M. Mitsui"], "venue": "In Proc. of Workshop on Argumentation Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Argumentation mining on the web from information seeking perspective", "author": ["I. Habernal", "J. Eckle-Kohler", "I. Gurevych"], "venue": "In Proc. of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Argumentation mining in user-generated web discourse", "author": ["I. Habernal", "I. Gurevych"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Algorithm as 136: A kmeans clustering algorithm", "author": ["John A Hartigan", "Manchek A Wong"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1979}, {"title": "Rapid development of a corpus with discourse annotations using two-stage crowdsourcing", "author": ["D. Kawahara", "Y. Machida", "T. Shibata", "S. Kurohashi"], "venue": "In Proc. of COLING,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Content analysis: An introduction to its methodology", "author": ["Krippendorff Klaus"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1980}, {"title": "Measuring the reliability of qualitative text analysis data", "author": ["Klaus Krippendorff"], "venue": "Quality & quantity,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Argumentation mining: State of the art and emerging trends", "author": ["M. Lippi", "P. Torroni"], "venue": "ACM Transactions on Internet Technology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Argumentation mining: Where are we now, where do we want to be and how do we get there", "author": ["Marie-Francine Moens"], "venue": "In Proc. of Forum on Information Retrieval Evaluation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Argumentation mining: the detection, classification and structure of arguments in text", "author": ["R.M. Palau", "M.-F. Moens"], "venue": "In Proc. of ICAIL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Language resources for studying argument", "author": ["C. Reed", "R. Mochales-Palau", "G. Rowe", "M.-F. Moens"], "venue": "In Proc. of LREC,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proc. of EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "In Proc. of COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "arXiv preprint,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "brat: a web-based tool for nlp-assisted text annotation", "author": ["P. Stenetorp", "S. Pyysalo", "G. Topic", "T. Ohta", "S. Ananiadou", "J. Tsujii"], "venue": "In Proc. of ECAL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Some facets of argument mining for opinion analysis", "author": ["M.P. Garcia Villalba", "P. Saint-Dizier"], "venue": "In Proc. of COMMA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A review corpus for argumentation analysis", "author": ["H. Wachsmuth", "M. Trenkmann", "B. Stein", "G. Engels", "T. Palakarska"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Semiautomated argumentative analysis of online product reviews", "author": ["A. Wyner", "J. Schneider", "K. Atkinson", "T. JM Bench-Capon"], "venue": "In Proc. of COMMA,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Such discourse structures are known as arguments [11], and the techniques for automatically extracting arguments and their relations (e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "support/attack) from natural language texts are known as argumentation mining [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "legal documents [12], persuasive essays [16], newspapers and court cases [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "legal documents [12], persuasive essays [16], newspapers and court cases [13].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "legal documents [12], persuasive essays [16], newspapers and court cases [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Crowdsourcing has been widely recognised as a reliable and economic method for some annotating tasks [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "[5], [10], [16].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[5], [10], [16].", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "[5], [10], [16].", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "[19] built the ArguAna corpus, consisting of 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Garcia Villalba and Saint-Dizier [18] investigated suitable argumentation models for customer reviews.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "[20] built a corpus consisting of 84 reviews (posted on Amazon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed an annotation mechanism to annotate arguments and their relations in blog comments: they hired crowdsourcing workers to label claims-premises relations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "55 IRA score (in terms of multi-\u03c0 [2]), suggesting the agreement is moderate; also, they suggested", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "[7] designed a two-stage crowdsourcing mechanism to annotate two levels of discourse relations in Japanese texts crawled from multiple online genres.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "The annotation is performed on the brat [17] open-source annotation platform.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "As for clause segmenting, we test two approaches: subsentence based segmenting [19] (i.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "As for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents.", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "As for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents.", "startOffset": 177, "endOffset": 180}, {"referenceID": 3, "context": "As for the argumentation model, we consider three candidate models: the Premises-Claim-MajorClaim (PCM) model [15] for persuasive essays, the extended ClaimPremises (ECP) model [4] for long Web documents and the extended Toulmin\u2019s (ETM) model [4] for short Web documents.", "startOffset": 243, "endOffset": 246}, {"referenceID": 3, "context": "Premises are allowed to support/attack claims, but claims are not allowed to support other claims, because this will lead to cascading support, which overcomplicates the annotating process [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "The average IRA in terms of Krippendorff\u2019s \u03b1U [9] for their annotations is 0.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "The crowdsourcing is performed on the brat [17] platform.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "We employ the K-means clustering technique [6] to perform the argument component aggregation, which can perform the above two subtasks as a whole.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Some statistics of the argument components annotations in easy reviews are presented in Table 3, and according to our statistics, the average of each review is annotated by three students; as for IRA metrics, besides \u03b1U , we also use percentage agreement, multi-\u03c0 [2] and Krippendorff\u2019s \u03b1 [8].", "startOffset": 264, "endOffset": 267}, {"referenceID": 7, "context": "Some statistics of the argument components annotations in easy reviews are presented in Table 3, and according to our statistics, the average of each review is annotated by three students; as for IRA metrics, besides \u03b1U , we also use percentage agreement, multi-\u03c0 [2] and Krippendorff\u2019s \u03b1 [8].", "startOffset": 289, "endOffset": 292}, {"referenceID": 0, "context": "In order to study the disagreements in the annotations, we created confusion probability matrices (CPM) [1] for", "startOffset": 104, "endOffset": 107}], "year": 2017, "abstractText": "Argumentation mining aims at automatically extracting the premises-claim discourse structures in natural language texts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.", "creator": "LaTeX with hyperref package"}}}