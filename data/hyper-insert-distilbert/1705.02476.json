{"id": "1705.02476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2017", "title": "PANFIS++: A Generalized Approach to Evolving Learning", "abstract": "implementing the online concept ii of evolving semantic intelligent feedback system ( eis ) provides an effective avenue name for data pipeline stream mining practitioners because hopefully it is capable of desperately coping himself with two clearly prominent issues : locating online learning competition and rapidly selecting changing environments. we note typically at, least perhaps three currently uncharted critical territories of both existing eiss : embedded data item uncertainty, static temporal social system dynamic, redundant data transmission streams. hence this book project chapter mainly aims at delivering fully a unique concrete solution of removing this problem with showcasing the actual algorithmic development of a novel extended learning trajectory algorithm, whilst namely learning panfis + +. panfis + + \u2212 is inherently a generalized version of the revised panfis specification by aggressively putting forward three important components : 1 ) an easier online active learning scenario platform is developed to overcome redundant data streams. this module greatly allows student to actively select data sequence streams for the training process, thereby expediting execution preparation time and enhancing generalization performance, volume 2 ) intelligent panfis + + is built manually upon at an interval type - form 2 strict fuzzy distributed system reference environment, which immediately incorporates the so - called intrinsic footprint of uncertainty. utilizing this component provides a degree measurement of predictable tolerance for data item uncertainty. 3 ) panfis + + is structured under a recurrent redundant network architecture with a self - evident feedback loop. accordingly this is meant to tackle adjusting the temporal system ecological dynamic. realizing the efficacy of the panfis + + challenge has been numerically validated through completing numerous popular real - world data and significant synthetic case studies, for where it delivers the highest predictive curve accuracy curve while retaining the marginal lowest complexity.", "histories": [["v1", "Sat, 6 May 2017 12:02:15 GMT  (635kb)", "http://arxiv.org/abs/1705.02476v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mahardhika pratama"], "accepted": false, "id": "1705.02476"}, "pdf": {"name": "1705.02476.pdf", "metadata": {"source": "CRF", "title": "PANFIS++: A Generalized Approach to Evolving Learning", "authors": ["Mahardhika Pratama"], "emails": [], "sections": [{"heading": null, "text": "data stream mining because it is capable of coping with two prominent issues: online learning and rapidly changing environments. We note at least three uncharted territories of existing EISs: data uncertainty, temporal system dynamic, redundant data streams. This book chapter aims at delivering a concrete solution of this problem with the algorithmic development of a novel learning algorithm, namely PANFIS++. PANFIS++ is a generalized version of the PANFIS by putting forward three important components: 1) An online active learning scenario is developed to overcome redundant data streams. This module allows to actively select data streams for the training process, thereby expediting execution time and enhancing generalization performance; 2) PANFIS++ is built upon an interval type-2 fuzzy system environment, which incorporates the so-called footprint of uncertainty. This component provides a degree of tolerance for data uncertainty. 3) PANFIS++ is structured under a recurrent network architecture with a self-feedback loop. This is meant to tackle the temporal system dynamic. The efficacy of the PANFIS++ has been numerically validated through numerous real-world and synthetic case studies, where it delivers the highest predictive accuracy while retaining the lowest complexity.\nIndex Terms\u2014fuzzy neural network, type-2 fuzzy system, online learning, metacognitive learning, evolving fuzzy system\nI. Introduction\nData streams present unique problems which are too costly to be handled by the conventional approach in machine learning. The problem of data streams is frequently encountered in the online time-critical applications, which calls for an efficient algorithm with low computational and storage requirement [1]. Data streams often pertain to the concept drift issue, because it is generated in non-stationary environments. For instance, the manufacturing process runs continuously without any stoppage at a high feed rate. This imposes data streams being generated at a fast sampling rate. Furthermore, the tool condition is monitored at rapidly changing environments, associated with variations of machining parameters, cutter profiles, etc. Tool degradation and changing surface integrity also cause gradual concept drift [2]. To this end, Evolving Intelligent System (EIS) has been proposed and features two prominent characteristics: open structure, online learning [3]. The open structure property allows EIS to start its learning process from scratch with an empty rule base. Its components are selfstructured and organized from data streams. This trait provides an answer to non-stationary\nsystem behaviors, because it can automatically grow when a change occurs in data distribution. The EIS also characterizes a strict online learning procedure, which can process data streams with high efficiency. A sample is directly discarded once learned, which renders economical computational and memory burdens. This is capable of dealing with a possible infinite nature of data stream and satisfying a life-long learning requirement [3].\nIn the past decade, the EIS has grown to be a very active research area and drawn considerable research interest of the computational intelligence community. The area of EIS was pioneered by Juang and Lin [4] with SONFIN, although the term \u201cEvolving\u201d has not been formalized until the development of DENFIS [5] and eTS [6]. Since then the area of EIS has attracted various contributions [7] - [15]. Nevertheless, the EIS area deserves in-depth investigation, because three issues, namely data uncertainty, temporal system dynamic, and redundant data streams, are unsolved. Although the concept of metacognitive learning incorporating the what-to-learn component aims at addressing redundant data streams [16]- [20], most of them are designed for classification problems, while a regression problem is an open issue. They make use of the so-called hinge error function to approximate data stream contribution. It is at risk of the bias-and-variance dilemma because system error can be high in the case of overfitting. Some attempt has been devoted to actualise the evolving concept in the recurrent network structure and the interval-type 2 fuzzy system [21]-[25]. We argue however that these works are over-dependent on the KM iterative procedure, incurring costly computational burden. These works are built upon a distance-based clustering method, which is prone to outliers. In addition, these methods do not possess the self-regulatory mechanism that controls data streams to train the model. As a result, all incoming data streams are subject to the training process. It slows down the training process and even brings down the generalization capability because redundant data streams are perhaps utilized to train the model.\nThis paper presents a novel evolving learning algorithm, called Parsimonious Network Based on Fuzzy Inference System++ (PANFIS++). PANFIS++ presents an extended version of PANFIS [26], which is not only capable of mining data streams efficiently but also selecting data streams to be learned on the fly. The salient components of PANFIS++ are elaborated as follows: 1) The online active learning for regression problem, namely ESEM, is proposed in the PANFIS++ and is capable of ruling out inconsequential samples, thereby improving the generalization\u2019s performance and expediting the training process; 2) PANFIS++ is constructed under a recurrent network structure, which features a local recurrent loop. The recurrent\nnetwork architecture aims at capturing temporal system dynamics and alleviating the need for time-delayed input attributes. Although the local recurrent connection has been put into perspective in [21], [22], the novelty of our work can be found in the rule layer of the PANFIS++, which features the interval type-2 multivariate Gaussian function with the intervalvalued centroids; 3) PANFIS++ realises a generalized interval type-2 Takagi-Sugeno-Kang (TSK) fuzzy rule. That is, the first-order polynomial is embedded in the rule consequent, while the multivariate Gaussian function with uncertain Centroids is incorporated in the rule premise; 4) As with other EISs, the PANFIS++ is capable of automatically generating its fuzzy rules in the single-pass learning mode using the Generalized Type-2 Datum Significance (GT2DS) method. The GT2DS is a generalized version of the T2DS method [7], which is designed under a strict condition of uniformly distributed training data. It adopts a strategy of [27], [28], which estimates a complex probability density function with the Gaussian Mixture Model (GMM). It is worth noting that [27], [28] are all designed for the type-1 fuzzy system; 5) PANFIS++ integrates the rule pruning scenario, which is capable of discarding an outdated rule \u2013 no longer relevant to current learning context and an inconsequential rule \u2013 plays little role during its lifespan. This is carried out using the Type-2 Relative Mutual Information (T2RMI) method; 6) PANFIS++ is equipped with a rule recall scenario, which makes possible to reactivate a previously pruned rule. This scenario is needed to deal with the cyclic concept drift, which presents a previously learned concept. 7) The parameter learning scenario is undertaken using a combination of the Fuzzily Weighted Generalized Recursive Least Square (FWGRLS) method and the Zero Error Density Maximization (ZEDM) principle to adapt other free parameters. The efficacy of the PANFIS++ has been numerically validated with various realworld and synthetic data streams. PANFIS++ has been benchmarked with prominent algorithms, recently published in the literature where PANFIS++ demonstrates more encouraging performance in attaining a balance between accuracy and simplicity than its counterparts.\nThe remainder of this paper is organized as follows: Section 2 outlines the network architecture of the PANFIS++; Section 3 discusses the rule base management of the PANFIS++; Section 4 details our numerical study and comparisons with state-of-the-art algorithms. Concluding remarks are drawn in the last section of this paper.\nII. Network Architecture of PANFIS++\nPANFIS++ is built upon a recurrent network structure using a local recurrent connection, which is meant to tackle the temporal system dynamic and to reduce the demand of time-\ndelayed input variables. Furthermore, the hidden layer of the PANFIS++ is composed of the interval-valued multivariate Gaussian function. The multivariate Gaussian function evolves a flexible ellipsoidal cluster in any direction, which is compatible to cover irregular data, which are not distributed in the main axes. This trait is capable of decreasing the fuzzy rule demand. Furthermore, the multivariate Gaussian function features the scale-invariant property, which can deal with different ranges of input attributes. Another appealing property is its nondiagonal covariance matrix, which retains the interrelation among input variables. The interrelation among input variables vanishes under a conventional fuzzy rule with the t-norm operator. Unlike the standard TSK fuzzy rule, the PANFIS++ extends the degree of freedom in the rule consequent with the Chebyshev function. It is well-known that the zero or first-order polynomial do not fully exploit the local mapping capabilities. This drawback is usually addressed by putting forward a higher order polynomial in the rule consequent but this strategy also increases the variance of the model, which increases the likelihood of overfitting. The Chebyshev function is chosen to map the original input space here because it outperforms the mapping capability of other polynomial function. The final output of the PANFIS++ is written as follows:\n \n \n R\ni\ni\niio\nR\ni i\niio o\nqq foy\n11\n6 )1(\n\n\n\n (1)\nwhere q stands for the design coefficient and R stands for the number of fuzzy rules. The qdesign factor is used to perform a type reduction mechanism and is dynamically adapted to be well-suited to the current training context. It controls the proportion of upper and lower fuzzy variables. ],[~   is an interval-valued temporal firing strength, which results from a local\nrecurrent connection. The interval-valued temporal firing strength is expressed as follows:\n\ud835\udf13 \ud835\udc56 = \ud835\udf06\ud835\udc56\ud835\udc45\ud835\udc56 + (1 \u2212 \ud835\udf06\ud835\udc56)\ud835\udc45\ud835\udc56, \ud835\udf13\ud835\udc56 = \ud835\udf06\ud835\udc56\ud835\udc45\ud835\udc56 + (1 \u2212 \ud835\udf06\ud835\udc56)\ud835\udc45\ud835\udc56 (2)\nwhere \ud835\u0303\udc45\ud835\udc56 = [\ud835\udc45\ud835\udc56, \ud835\udc45\ud835\udc56]denotes an interval-valued spatial firing strength, which is resulted from the interval-valued multivariable Gaussian function. It is worth noting that the uncertainty elements in terms of an interval is embedded in the Centroid of Gaussian function. In other words, the centroid of the Gaussian function is not a crisp vector rather an interval-valued vector. The interval-valued multivariable Gaussian function is formalised as follows:\n\ud835\udc45\ud835\udc56 = exp(\u2212(\ud835\udc4b \u2212 \ud835\udc36\ud835\udc56)\u03a3\ud835\udc56 \u22121(\ud835\udc4b \u2212 \ud835\udc36\ud835\udc56) \ud835\udc47),\ud835\udc45\ud835\udc56 = exp(\u2212(\ud835\udc4b \u2212 \ud835\udc36\ud835\udc56)\u03a3\ud835\udc56 \u22121(\ud835\udc4b \u2212 \ud835\udc36\ud835\udc56) \ud835\udc47) (3)\nwhere \ud835\u0303\udc36\ud835\udc56 = [\ud835\udc36\ud835\udc56, \ud835\udc36\ud835\udc56] \u2208 \u211c 1\u00d7\ud835\udc5dis an interval-valued Centroid of the i-th rule, \u03a3\ud835\udc56 \u22121 \u2208 \u211c\ud835\udc5d\u00d7\ud835\udc5d is a nondiagonal covariance matrix and p is the number of input dimension. Nevertheless, (3) should\nnot be implemented directly, because \ud835\udc45\ud835\udc56 > \ud835\udc45\ud835\udc56does not always hold. This problem is resolved by transforming a high dimensional cluster to its one-dimensional equivalent to satisfy the basic principle of interval arithmetic. In other words, a fuzzy set representation of a high-dimensional cluster needs to be obtained before performing the fuzzy inference scheme. This is done by simply enumerating radii of the Gaussian fuzzy set from a high dimensional ellipsoidal cluster. The radii of the fuzzy set are formulated as a distance between the centres to the cutting points of the ellipsoidal cluster [26] as follows:\n\ud835\udf0e\ud835\udc56 = (\ud835\udc5f\ud835\udc56+\ud835\udc5f\ud835\udc56)\n2 \u221a\u03a3\ud835\udc56,\ud835\udc57,\ud835\udc57\n\u22121 (4)\nwhere \u03a3\ud835\udc56,\ud835\udc57,\ud835\udc57 \u22121 is diagonal elements of the inverse covariance matrix of the i-th rule. It is worth noting although this strategy is relatively simple to use and fast, it is rather inaccurate provided that an ellipsoidal cluster is rotated around 45 degrees. The centre of the Gaussian fuzzy set adopts a centre of a multidimensional ellipsoidal cluster without any modification. After obtaining the fuzzy set representation of the Gaussian function, the firing strength is calculated as follows:\n\ud835\udf07\ud835\udc56,\ud835\udc57 = exp(\u2212 (\ud835\udc65\ud835\udc57\u2212\ud835\udc50\ud835\udc56,\ud835\udc57)\n2\n\ud835\udf0e\ud835\udc56,\ud835\udc57 2 ) = \ud835\udc41(\ud835\udc65\ud835\udc57; \ud835\u0303\udc50\ud835\udc56,\ud835\udc57, \ud835\udf0e\ud835\udc56,\ud835\udc57), \ud835\u0303\udc50\ud835\udc56,\ud835\udc57 = [\ud835\udc50\ud835\udc56,\ud835\udc57, \ud835\udc50\ud835\udc56,\ud835\udc57] (5)\n\ud835\udf07 \ud835\udc56,\ud835\udc57 = {\n\ud835\udc41(\ud835\udc65\ud835\udc57; \ud835\udc50\ud835\udc56,\ud835\udc57, \ud835\udf0e\ud835\udc56,\ud835\udc57), \ud835\udc65\ud835\udc57 < \ud835\udc50\ud835\udc56,\ud835\udc57 1, \ud835\udc50\ud835\udc56,\ud835\udc57 \u2264 \ud835\udc65\ud835\udc57 \u2264 \ud835\udc50\ud835\udc56,\ud835\udc57 \ud835\udc41(\ud835\udc65\ud835\udc57; \ud835\udc50\ud835\udc56,\ud835\udc57, \ud835\udf0e\ud835\udc56,\ud835\udc57), \ud835\udc65\ud835\udc57 > \ud835\udc50\ud835\udc56,\ud835\udc57\n(6)\n\ud835\udf07\ud835\udc56,\ud835\udc57 = { \ud835\udc41(\ud835\udc65\ud835\udc57; \ud835\udc50\ud835\udc56,\ud835\udc57 , \ud835\udf0e\ud835\udc56,\ud835\udc57), \ud835\udc65\ud835\udc57 \u2264\n(\ud835\udc50\ud835\udc56,\ud835\udc57+\ud835\udc50\ud835\udc56,\ud835\udc57)\n2\n\ud835\udc41(\ud835\udc65\ud835\udc57; \ud835\udc50\ud835\udc56,\ud835\udc57 , \ud835\udf0e\ud835\udc56,\ud835\udc57), \ud835\udc65\ud835\udc57 \u2265 (\ud835\udc50\ud835\udc56,\ud835\udc57+\ud835\udc50\ud835\udc56,\ud835\udc57)\n2\n(7)\nThe membership degree of all dimensions are combined with a conjunction operator, namely the product t-norm operator as follows:\n\ud835\udc45\ud835\udc56 = \u220f \ud835\udf07\ud835\udc56,\ud835\udc57 \ud835\udc5d \ud835\udc57=1 , \ud835\udc45\ud835\udc56 = \u220f \ud835\udf07\ud835\udc56,\ud835\udc57 \ud835\udc5d \ud835\udc57=1 (8)\nThe firing strength is widely used to measure a compatibility of existing rules in the rule growing scenario. The flaw of this approach is, however, outliers, because it is functionally equivalent to the distance-based clustering approach. The product t-norm operator suffers from\nthe curse of dimensionality problem, where (8) diminishes proportionally to the number of input features.\nThe rule consequent of the PANFIS++ is built upon a nonlinear mapping through up to the second order of the Chebyshev polynomial \ud835\udefd\ud835\udc56 = \ud835\udc65\ud835\udc52\ud835\udc4a\ud835\udc56, where \ud835\udc65\ud835\udc52 \u2208 \u211c 1\u00d7(2\ud835\udc5d+1)denotes an extended input vector based on the Chebyshev polynomial, while \ud835\udc4a\ud835\udc56 \u2208 \u211c (2\ud835\udc5d+1)\u00d71stands for the weight vector of the i-th rule. The Chebyshev polynomial is defined as follows:\n\ud835\udc34\ud835\udc5b+1 = 2\ud835\udc65\ud835\udc57\ud835\udc34\ud835\udc5b(\ud835\udc65\ud835\udc57) \u2212 \ud835\udc34\ud835\udc5b\u22121(\ud835\udc65\ud835\udc57) (10)\nSuppose that we deal with 2D-case \ud835\udc4b \u2208 \u211c1\u00d72, the extended input vector \ud835\udc65\ud835\udc52is formulated as \ud835\udc65\ud835\udc52 = [1, \ud835\udc341(\ud835\udc651), \ud835\udc342(\ud835\udc651), \ud835\udc341(\ud835\udc652), \ud835\udc342(\ud835\udc652)]. The term 1 is inserted in the extended input vector as an intercept to provide flexibility and to avoid an untypical gradient case.\nIII. Learning Policy of PANFIS++\nThis section elaborates the rule base management of the PANFIS++: Section III.A outlines the online active learning procedure of the PANFIS++, Section III.B discusses the rule growing scenario of the PANFIS++, Section III.C covers the rule pruning scenario of the PANFIS++, Section III.D describes the rule recall scenario of the PANFIS++, Section III.D details the parameter learning scenario of the PANFIS++. An overview of overall training procedures of PANFIS++ is depicted in Algorithm 1. III.A An Online Active Learning Scenario\nThe online active learning scenario is capable of expediting the training process because it can reduce the number of training samples to be seen. It also prevents redundant samples to be learned. This contributes to the significant improvement of generalization power. Although the active learning concept has been well-established in the literature, most of them work in the pool-based approach, which assumes all data are available in the pool for an iterative selection procedure [29]. Recently, some initiatives were devoted to proposing an online active learning scenarios to mine data streams [30], [31]. An open research is, however, the regression case, where a target variable is a continuous function. The regression problem requires more in-depth investigation than the classification problem because the sample contribution cannot be examined with respect to its position to a decision surface. One technique has been put forward in [31] but it is still based on the hinge loss function, which is sensitive to the system\u2019s error. As a matter of fact, the system error does not necessarily reflect a true system condition, because a high variance case also results in a high system error.\nThe PANFIS++ utilizes the ESEM to steer the online active learning strategy. The ESEM relies on the entropy of neighborhood probability, which studies uncertainties of existing structure with respect to an incoming sample. The ESEM is a generalization of SEM [32] which forms an incremental version of the SEM. The probability of a data stream to sit within influence zone of existing rules \u2013 the neighborhood probability - is formalized as follows:\n\ud835\udc43(\ud835\udc4b\ud835\udc5b \u2208 \ud835\udc41\ud835\udc56) = \u2211 \ud835\udc40(\ud835\udc4b\ud835\udc41,\ud835\udc65\ud835\udc5b) \ud835\udc41\ud835\udc56 \ud835\udc5b=1\n\u2211 \u2211 \ud835\udc40(\ud835\udc4b\ud835\udc41,\ud835\udc65\ud835\udc5b) \ud835\udc41\ud835\udc56 \ud835\udc5b=1 \ud835\udc45 \ud835\udc56=1\n(11)\nwhere \ud835\udc4b\ud835\udc41 , \ud835\udc65\ud835\udc5b, \ud835\udc40( ),\ud835\udc41\ud835\udc56 are respectively the newly seen sample, the n-th sample of i-th rule, a similarity measure, and the number of populations of i-th cluster. (11) is intractable for the online scenario, because it revisits all samples seen thus far. It is obvious that \u2211 \ud835\udc40(\ud835\udc4b\ud835\udc41 , \ud835\udc65\ud835\udc5b) \ud835\udc41\ud835\udc56 \ud835\udc5b=1 is akin to the local density since it looks at spatial proximities between a sample and all populations of the i-th rule. It also signifies a robust approach against uncertainty, since it is resulted from all collected samples. A recursive expression of \u2211 \ud835\udc40(\ud835\udc4b\ud835\udc41, \ud835\udc65\ud835\udc5b) \ud835\udc41\ud835\udc56 \ud835\udc5b=1 can be defined:\n\u2211 \ud835\udc40(\ud835\udc4b\ud835\udc41,\ud835\udc65\ud835\udc5b) \ud835\udc41\ud835\udc56 \ud835\udc5b=1\n\ud835\udc41\ud835\udc56 =\n\u2211 (\ud835\udc41\ud835\udc56\u22121)\ud835\udc65\ud835\udc57 \ud835\udc41\u22122\u2211 \ud835\udc65\ud835\udc57 \ud835\udc41\ud835\udf17 \ud835\udc57 \ud835\udc41\ud835\udc56+\ud835\udf17\ud835\udc41\ud835\udc56 \ud835\udc43 \ud835\udc57=1 \ud835\udc5d \ud835\udc57=1\n(\ud835\udc41\ud835\udc56\u22121)\ud835\udc62 (12)\n\ud835\udf17\ud835\udc57 \ud835\udc41\ud835\udc56 = \ud835\udf17\ud835\udc57 \ud835\udc41\ud835\udc56\u22121 + \ud835\udc65\ud835\udc57 \ud835\udc41\ud835\udc56\u22121,\ud835\udf17\ud835\udc41\ud835\udc56 = \ud835\udf17\ud835\udc41\ud835\udc56\u22121 + \u2211 \ud835\udc65\ud835\udc41\ud835\udc56\u22121,\ud835\udc57 2\ud835\udc62 \ud835\udc57=1 The final formula of the ESEM is exhibited as an entropy of the neighborhood probability:\n\ud835\udc3b(\ud835\udc41|\ud835\udc4b\ud835\udc41) = \u2212\u2211 \ud835\udc43(\ud835\udc4b\ud835\udc41 \u2208 \ud835\udc41\ud835\udc56)\ud835\udc59\ud835\udc5c\ud835\udc54 \ud835\udc45 \ud835\udc56=1 \ud835\udc43(\ud835\udc4b\ud835\udc41 \u2208 \ud835\udc41\ud835\udc56) (13)\nAn attention should be paid to a sample incurring a high level of uncertainty, whereas a sample with low uncertainty is negligible to the training process. Such sample can be ignored without loss of generalization, thus speeding up the training process. Hence, the condition to accept a sample for the training process is shown as follows:\n\ud835\udc3b(\ud835\udc41|\ud835\udc4b\ud835\udc41) \u2265 \ud835\udeff1 (14)\nwhere \ud835\udeff1is a predefined threshold. In non-stationary environments, \ud835\udeff1should not be fixed during the training process rather be adjustable to adapt to system\u2019s behaviour. This strategy is confirmed with the fact that shifts in the data distribution entail different levels of the threshold. A heuristic approach is adopted to fine-tune the threshold where the threshold should augment when a new training sample is accepted \ud835\udeff1 \ud835\udc41+1 = \ud835\udeff1 \ud835\udc41(1 + \ud835\udc60). This aims to reduce possibility of next training samples to be used in the training process. It should on the other hand decline when a new data point is admitted for model updates \ud835\udeff1 \ud835\udc41+1 = \ud835\udeff1 \ud835\udc41(1 \u2212 \ud835\udc60). The goal is to boost the chance of next samples to be next training patterns. In other words, this adjustment aims to achieve tradeoff between complexity and accuracy. We set s as its default value [33] s=0.01. III.B Rule Growing Mechanism\nPANFIS++ is capable of automatically evolving its fuzzy rules on demands in accordance with inherent data distributions using the GT2DQ method. The DQ method relies on the idea of statistical contribution estimation of a data stream, which includes the possible future contribution of a data point. The original version of DQ method [7] is derived with the p-fold numerical integration, which is only suitable for a small input dimension. Moreover, it assumes that data are uniformly distributed. This fact renders the DQ method less accurate when data samples are sparsely distributed. The GT2DQ method is proposed here to overcome these drawbacks of T2DQ. It is inspired by the works of [27], [28] taking advantage of the GMM as a probability density function to deal with a complex and irregular data distribution. In light of the rule significance definition [7], the significance of interval-valued multivariate Gaussian function is defined as the Lu-norm of the error function weighted by the input density function. This leads to the final expression of the GT2DQ method as follows:\n\ud835\udc38\ud835\udc56 = \u2016\ud835\udefd\ud835\udc56\u2016(1 \u2212 \ud835\udc5e) {( 2\ud835\udf0b \ud835\udc62\u2044 ) \ud835\udc5d 2\u2044 \ud835\udc51\ud835\udc52\ud835\udc61(\u03a3\ud835\udc56) \u22121 2\u2044 \ud835\udc41\ud835\udc56\ud835\udefe \ud835\udc47}\n1 \ud835\udc62\n+ \u2016\ud835\udefd\ud835\udc56\u2016\ud835\udc5e {( 2\ud835\udf0b \ud835\udc62\u2044 ) \ud835\udc5d 2\u2044 \ud835\udc51\ud835\udc52\ud835\udc61(\u03a3\ud835\udc56) \u22121 2\u2044 \ud835\udc41\ud835\udc56\ud835\udefe \ud835\udc47}\n1 \ud835\udc62\n )/,0;(),..,/,0;(),...,/,0;(),/,0;( 11212111 MiMimimiiiiii uvcuvcuvcuvcNN   ,  )/,0;(),..,/,0;(),...,/,0;(),/,0;( 11212111 MiMimimiiiiii uvcuvcuvcuvcNN   (15) where \ud835\udc63\ud835\udc5a, \u03a3\ud835\udc5a, \ud835\udefc\ud835\udc5aare respectively the parameters of GMM \u2211 \ud835\udefc\ud835\udc5c\ud835\udc41(\ud835\udc65; \ud835\udc63\ud835\udc5c , \u03a3\ud835\udc5c) \ud835\udc5a \ud835\udc5c=1 and the mixing coefficients\u2211 \ud835\udefc\ud835\udc5c = 1, \ud835\udefc\ud835\udc5c > 0 \ud835\udc40 \ud835\udc5c=1 . It is worth mentioning that the GMM parameters can be elicited using pre-recorded samples \ud835\udc41\u210e\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc5f\ud835\udc66. The use of pre-recorded samples still makes sense in practise especially in the big data era, where access to pre-recorded samples is not difficult to be obtained. The number of pre-recorded samples in addition is significantly smaller than the number of training samples \ud835\udc41\u210e\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc5f\ud835\udc66 \u226a \ud835\udc41. We find also that \ud835\udc41\u210e\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc5f\ud835\udc66is not problem-specific and is fixed at 30 for all simulations in this paper.\nThe rule growing procedure is undertaken first of all by creating a hypothetical rule\n\ud835\udc36\ud835\udc45+1, \ud835\udc36\ud835\udc45+1, \u03a3\ud835\udc56 \u22121from a newly seen sample as follows:\n\ud835\u0303\udc36\ud835\udc45+1 = \ud835\udc4b\ud835\udc41 \u00b1\u25b3, \u03a3\ud835\udc45+1 = max((\ud835\udc36\ud835\udc56\u2212\ud835\udc36\ud835\udc56\u22121),(\ud835\udc36\ud835\udc56\u2212\ud835\udc36\ud835\udc56+1))\n\u221aln(1/\ud835\udf16) \ud835\udc3c (16)\nwhere \ud835\udf16is a \ud835\udf16 factor set in respect to the desired completeness degree. We here follow the commonly used setting in the literature \ud835\udf16 = 0.5. \u0394 is an uncertainty threshold, which governs the footprint of uncertainty of the interval-valued multivariate Gaussian function. We set \u0394 = 0.1for all simulations in this paper for simplicity. The initialization of the covariance matrix (16) has been proven mathematically to meet the \ud835\udf16 \u2013 completeness condition [34]. The\nhypothetical rule is added to be a new rule when it offers a substantial statistical contribution over existing rules. A rule growing condition is formalised as follows:\n\ud835\udc38\ud835\udc45+1 \u2265 max \ud835\udc56=1,\u2026,\ud835\udc45 \ud835\udc38\ud835\udc56 (17)\nThis rule growing condition differs from its predecessors [7], [27], [28], which are reliant on a user-defined threshold. The threshold is often problem-dependent and entails substantial expert knowledge to arrive at a correct value. (17) is a plausible condition for a rule generation procedure, because the statistical contribution of the hypothetical rule exceeds existing rule. Furthermore, the GT2DQ is relatively robust against outliers because the statistical contribution is derived from the whole region defined by a complex input density function set as the GMM.\nThe GT2DQ method cannot be standalone in the rule growing procedure of the PANFIS++, because it yet analyses the position of a sample in the input space. This is necessary to illustrate the significance of a data sample to a current network structure and to arrive at a proper input space clustering. We to remedy this problem insert a compatibility measure, which delves a spatial proximity of a sample to current cluster prototypes. The compatibility measure is realized using the interval-valued spatial firing strength as depicted in(3) executed with the qdesign factor to produce its crisp values as follows:\n\ud835\udc39\ud835\udc46 \u2264 \ud835\udeff2, \ud835\udc39\ud835\udc46 = \ud835\udc5e\ud835\udc45\ud835\udc56 + (1 \u2212 \ud835\udc5e)\ud835\udc45\ud835\udc56 (18)\nwhere \ud835\udeff2 is a threshold, which determines a minimum conflict level and is assigned as the critical value of the chi-square distributed \ud835\udf122with p degree of freedom and a significance level of \ud835\udefc. \ud835\udeff2is then allocated as \ud835\udeff2 = exp(\u2212\ud835\udf12 2) with the significance level \ud835\udefc = 5%. This condition assures that a new data point is sufficiently remote from the zone of influence of existing rules. Because PANFIS++ makes use of the multivariate Gaussian function, the chi-square distribution can be referred as a basis of a threshold selection, because it statistically formulates a condition when a sample is out of coverage area. If (17) and (18) are complied, a hypothetical rule is introduced as a new rule and its rule premise is specified as (16), while its rule consequence is set as follows:\n\ud835\udc4a\ud835\udc45+1 = \ud835\udc4a\ud835\udc64\ud835\udc56\ud835\udc5b, \u03a8\ud835\udc45+1 = \ud835\udf14\ud835\udc3c (19)\nwhere \ud835\udf14 = 105is a positive large value and heuristically fixed at 105. This setting has been analytically proven to be a close approximation of a batched learning scheme. Furthermore, a\nnew rule consequent is akin to that of the winning rule, because the winning rule should represent similar output trend of the winning rule. There are several ways to choose the winning rule: the distance-based method, the Bayesian concept [35]. The Bayesian concept is applied in the PANFIS++ because of its prior probability. A winning rule is selected as the one with the highest posterior probability. It is not recounted here and interested reader is advised to go to [35] for further details of the Bayesian winning rule selection.\nIf (17) and (18) are not satisfied, a data sample does not feature sufficient statistical contribution to trigger the rule growing scenario. This sample should be absorbed to refine the position of the fuzzy rule in the data space. Specifically, the rule antecedent is fine-tuned using this sample as follows:\n1\n) ~ (~\n1\n~ 1\n1 1\n1\n1\n\n \n \n\n \n\n\nN win\nN winNN\nwinN win\nN winN\nwin N\nCX C\nN\nN C , ],[ ~ winwinwin CCC  (20)\nTN win C N Xold win N win C N X\nTN win C N XN win N win C N XN win\nN winN\nwin )1\u02c6(1)()1\u02c6(1\n))1\u02c6(1)1())(1\u02c6(1)1((\n11\n1)1( 1)(\n\n\n  \n \n\n\n\n(21)\n1 1  N win N win NN (22)\nwhere \ud835\udefc = 1 (\ud835\udc41\ud835\udc64\ud835\udc56\ud835\udc5b + 1)\u2044 and \ud835\u0302\udc36\ud835\udc64\ud835\udc56\ud835\udc5b= (\ud835\udc36\ud835\udc64\ud835\udc56\ud835\udc5b + \ud835\udc36\ud835\udc64\ud835\udc56\ud835\udc5b) 2\u2044 . The midpoint of upper and lower bounds of the interval-valued cluster prototypes to adjust the crisp covariance matrix. Moreover, a direct adaptation of the inverse covariance matrix is put forward where no reinversion step is required after performing any adaptation phase. The reinversion step incurs computationally prohibitive cost notably in the high input dimension case. It tends to be unstable in the light of reinversion step when a covariance matrix is ill-defined."}, {"heading": "III.C Parameter Learning Scenario of PANFIS++", "text": "The PANFIS++ puts forward a synergy between the zero-error density maximization method (ZEDM) and the FWGRLS method to perform the parameter learning scenario of the PANFIS++. The ZEDM is used to adjust the q-design factor of PANFIS++ and constitutes a generalized version of the gradient descent method [35]. It substitutes the Mean Square Error (MSE) with the error entropy as the cost function because it turns out to be more thorough to capture the high order statistical behavior than the MSE. Minimizing the error entropy is equivalent to reducing the distance between the probability distribution of the target function and the predictive output. Since the model of the error entropy is extremely difficult to formalize with the first principal, it is approximated with the Parzen Window technique here as follows:\n\ud835\udc53(0) = 1\n\ud835\udc41\ud835\udf0f\u221a2\ud835\udf0b \u2211 exp(\u2212\n\ud835\udc52\ud835\udc5b 2\n2\ud835\udf0f2 )\ud835\udc41\ud835\udc5b=1 (23)\nwhere \ud835\udf0f is a smoothing parameter, simply set as 1 in our case and \ud835\udc52\ud835\udc5bis the system error, while N is the number of training samples seen by far. The optimisation procedure follows the stochastic gradient descent approach where a gradient computation and adaptation are carried out per sample. This is defined as follows:\n\ud835\udc5e\ud835\udc5c \ud835\udc41 = \ud835\udc5e\ud835\udc5c \ud835\udc41\u22121 \u2212 \ud835\udf02\ud835\udc5e 1\n\ud835\udc41\u221a2\ud835\udf0b\n\ud835\udf15\ud835\udc38 \ud835\udf15\ud835\udc5e0 \u2211 exp(\u2212\n\ud835\udc52\ud835\udc5b 2\n2 )\ud835\udc41\ud835\udc5b=1 (24)\n\ud835\udf06\ud835\udc56 \ud835\udc41 = \ud835\udf06\ud835\udc56 \ud835\udc41\u22121 \u2212 \ud835\udf02\ud835\udf06 1\n\ud835\udc41\u221a2\ud835\udf0b\n\ud835\udf15\ud835\udc38 \ud835\udf15\ud835\udf06\ud835\udc56 \u2211 exp(\u2212\n\ud835\udc52\ud835\udc5b 2\n2 )\ud835\udc41\ud835\udc5b=1 (25)\nwhere \ud835\udf02\ud835\udc5edenotes a learning rate. \ud835\udf15\ud835\udc38\n\ud835\udf15\ud835\udc5e is the gradient of the squared error\n(\ud835\udc66\u2212\ud835\udc61)2\n2 in respect to the q\ndesign coeffient as follows:\n\ud835\udf15\ud835\udc38 \ud835\udf15\ud835\udc5e = \ud835\udc52\ud835\udc5b(\n\u2211 \u03a8\ud835\udc56\ud835\udefd\ud835\udc56 \ud835\udc45 \ud835\udc56=1\n\u2211 \u03a8\ud835\udc56 \ud835\udc45 \ud835\udc56=1\n\u2212 \u2211 \u03a8\ud835\udc56\ud835\udefd\ud835\udc56 \ud835\udc45 \ud835\udc56=1\n\u2211 \u03a8\ud835\udc56 \ud835\udc45 \ud835\udc56=1\n) (26)\n\ud835\udf15\ud835\udc38 \ud835\udf15\ud835\udf06\ud835\udc56 = \ud835\udc52\ud835\udc5b (((\ud835\udc45\ud835\udc56 \u2212\u03a8\ud835\udc56) (1\u2212\ud835\udc5e)\ud835\udefd\ud835\udc56 \u2211 \u03a8\ud835\udc56 \ud835\udc45 \ud835\udc56=1 )) + ((\ud835\udc45\ud835\udc56 \u2212\u03a8\ud835\udc56) \ud835\udc5e\ud835\udefd\ud835\udc56 \u2211 \u03a8\ud835\udc56 \ud835\udc45 \ud835\udc56=1 )) (27)\nIt is observed that \u2211 exp( \u2212\ud835\udc52\ud835\udc5b\n2\n2 )\ud835\udc41\ud835\udc5b=1 in (24), (25) requires revisiting all preceding samples, which\ndo not fit the online learning scenario. Its recursive version is to remedy this bottleneck developed as \ud835\udc34\ud835\udc41 = \ud835\udc34\ud835\udc41\u22121 + exp( \u2212\ud835\udc52\ud835\udc5b 2\n2 ). The learning rate plays vital role to the success of the\nadaptation process. A too small learning rate leads to slow convergence, whereas a too large learning rate causes instability. Hence, it should be adjusted in respect to the real system behaviour to expedite the convergence as follows:\n\ud835\udf02 = { \ud835\udeff3\ud835\udf02, \ud835\udc53\ud835\udc41(0) \u2265 \ud835\udc53\ud835\udc41\u22121(0)\n\ud835\udeff4\ud835\udf02, \ud835\udc53\ud835\udc41(0) < \ud835\udc53\ud835\udc41\u22121(0) (28)\nwhere \ud835\udeff3 \u2208 (1,1.5], \ud835\udeff4 \u2208 (0.5,1]stand for adjustment factors, which navigate the dynamic of learning rates. We set these parameters as \ud835\udeff3 = 1.1,\ud835\udeff4 = 0.9. The adjustment should be carried out with care because the learning rate should hover around a stable region where convergence is guaranted. The stable range can be derived using the Lyapunov stability concept and results in the condition0 < \ud835\udf02 < 2\ud835\udc41\u221a2\ud835\udf0b\n(\ud835\udc430)2\ud835\udc34\ud835\udc41 . The proof is left to reader here, because its derivation is well-\nknown in the literature [36].\nThe FWGRLS method is utilized to adapt the rule consequent of the PANFIS++. The FWGRLS presents a local learning version of the GRLS method [37] which incorporates the weight decay term to improve model\u2019s generalization and robustness of the adaptation process. The weight decay term prevents the rule consequent to be too large, which is the main cause of overfitting. In addition, it maintains the weight vector to be in a small and bounded range.\nTherefore, an inconsequential rule will be in very small values, which make them easy to detect and to prune. The local learning scenario is compatible with the evolving learning framework because it adapts per rule separately. Hence, convergence and stability of other rules are not affected by the tuning of a rule.\nIV. Proof of Concepts\nThis section discusses the proof of concepts of the PANFIS++. The PANFIS++ was numerically validated through two real-world case studies: valuation of residential premise price and tool wear prediction of a high-speed milling process. The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters."}, {"heading": "IV.A An Appraisal of Residential Premise Price", "text": "This numerical study aims to numerically validate the efficacy of the PANFIS+ to perform a valuation of residential premise price (Courtesy of Dr. Lughofer, Linz). As prevalent characteristic of real-world financial data, this problem is highly volatile and varies overtime, because it is highly influenced by various external parameters: infrastructures, economic activities in the surrounding area, etc. Data were collected from one of the large cities in Poland with 980 K populations. 50 K historical data were recorded during an 11 years period from 1998 until 2008. 5 input features were extracted from property market expert domain knowledge: usable area of premises, the age of a building, number of rooms in a flat, floor on which a flat is located, and distance from the city center [41]. The experimental procedure follows the predictive hold-out process, where 5 consecutive-years data formed the training set, while the testing set was compiled with the subsequent one-year data. The experiment was conducted in the absence of time-lagged input attributes. Table 2 sums up averaged numerical results across the whole periodic hold-out process.\nFrom Table 2, it is observed that the PANFIS++ produced the most encouraging performance in reaching tradeoff between accuracy and complexity. It underwent the fastest execution time because it is fitted by the online active learning strategy. This learning mechanism is capable of suppressing the number of training samples for model updates where in total only 5% of total training data were utilized. In terms of accuracy, the PANFIS++ delivered accurate predictive accuracy in which it came second after the GDFNN. It is worth noting the GDFNN is not suitable for online real-time deployment because it incurs prohibitive complexity as a result of revisiting already seen samples.\nIV.B Tool Wear Prediction of A High-Speed Milling Process\nThis case study is a tool condition monitoring problem of a high-speed milling process, namely the ball nose end milling process (Courtesy of Dr. X. Li, Singapore). It aims to predict the tool wear of the ball nose end milling cutter during the dry machining of hardened tool steel with a hardness of 52-54 HRC. The tool condition monitoring problem of a high-speed milling problem, in general, remains a very complex issue for both academic and industrial practitioners because of the use of multi-point cutting tools at high speed, varying machining parameters, inconsistency, and variability of cutter geometry/dimensions. The experiment moreover took place in non-stationary fashions because cutter degradation often leads to the gradual concept drift. The machining process applied varying spindle rates which causes variations of the data trends. The CNC milling process (R\u0151ders Tech RFM760) was used and raw data were collected using the 7-channels DAQ which correspond to the cutting and force signals in three different cutting axes (X, Y, Z) in addition to the AE signal. The predictive task was carried out using 12 time-domain features extracted from the force signal [1]. It is wellknown that the force signal conveys the most informative indicators of the tool wear in respect to the other two signals. A total of 630 data points were obtained from a real manufacturing process and the 10-fold cross validation (CV) scheme was followed as the experimental procedure. Table 3 tabulates an average of numerical results across the 10-fold CV procedure.\nFrom Table 3, it is obvious that the PANFIS++ attained the highest predictive accuracy while retaining the lowest complexity. The PANFIS++ outperformed its counterparts in three\nevaluation criteria, namely RMSE, the number of rules, the number of samples and runtime. Note that the characteristic of the tool condition monitoring problem is highly dynamic and this result confirms adaptive and evolving traits of the PANFIS++. The recurrent network structure increases adaptivity of a model because it memorizes previous learning actions, combined with the most recent observation.\nV. Conclusion\nA generalized version of PANFIS, namely PANFIS++, is proposed in this paper. PANFIS++ introduces a new paradigm of the EIS by providing concrete solutions against three prominent issues: temporal system dynamic, data uncertainty and absence of system order. The PANFIS++ puts forward three novel learning components: 1) A new online active learning strategy for regression problems is put forward where it is capable of expediting the training process while improving model\u2019s generalization; 2) The PANFIS++ is constructed under the type-2 fuzzy system environment which incorporates the so-called footprint of uncertainty. This component provides an effective avenue in dealing with the data uncertainty issue; 3) The PANFIS++ makes use a self-feedback loop at the rule layer to cope with the temporal system dynamic and the absence of system order. The self-feedback loop functions as an internal memory component and generates the spatiotemporal firing strength. The efficacy of the PANFIS++ was numerically validated using two real-world data streams containing nonstationary components. It was compared against prominent algorithms in the literature and it is shown that the PANFIS++ attained state-of-the-art performance.\nReferences\n[1] M. Pratama, M. J. Er, X. Li, R. J. Oentaryo, E. Lughofer, and I. Arifin, \"Data driven modeling based on dynamic parsimonious fuzzy neural network,\" Neurocomputing, vol. 110, pp. 18-28, 2013 [2] M. Sayed-Mouchaweh and E. Lughofer, Learning in Non-Stationary Environments: Methods and Applications, Springer, New York, 2012 [3] P.Angelov and D. Filev, \"Simpl_eTS: A simplified method for learning evolving Takagi-Sugeno fuzzy models,\" in IEEE International Conference on Fuzzy Systems (FUZZ), pp. 1068-1073.(2005) [4] C. F. Juang and C. T. Lin, \u201cAn on-line self-constructing neural fuzzy inference network and its applications,\u201d IEEE Transactions on Fuzzy Systems., vol. 6,no. 1, pp. 12\u201332, Feb. 1998 [5] N. Kasabov, and Q. Song, DENFIS: dynamic evolving neural-fuzzy inference system and its application for time series prediction, IEEE Transactions on Fuzzy Systems .vol10 (2).pp. 144\u2013154. (2002) [6] P.Angelov and D. Filev, \"An approach to online identification of Takagi-Sugeno fuzzy models,\" IEEE Transactions on Systems, Man, and Cybernetics, Part B.vol. 34, pp. 484-498.(2004) [7] C. F. Juang and Y. W. Tsao, \u201cA self-evolving interval type-2 fuzzy neural network with online structure and parameter learning,\u201d IEEE Transactions on Fuzzy Systems, vol. 16, no. 6, pp. 1411\u20131424, (2008) [8] M.Pratama, J.Lu, S.Anavatti,\u201d Recurrent Classifier based on An Incremental Meta-Cognitive-based Scaffolding Algorithm\u201d, IEEE Transactions on Fuzzy Systems, Vol.23(6), pp.2048-2066, (2015) [9] M. Pratama, J. Lu, G.Zhang, \u201c Evolving Type-2 Fuzzy Classifier\u201d, online and in press, IEEE Transactions on Fuzzy Systems, Vol. 24(3), pp. 574-589, (2016) [10] M.Pratama, S.Anavatti, E.Lughofer, \u201c pClass:An Effective Classifier to Streaming Examples\u201d, IEEE Transactions on Fuzzy Systems, Vol.23(2), pp.369-386, (2014) [11] M.Pratama, S.Anavatti, E.Lughofer, \u201c GENEFIS:Towards An Effective Localist Network\u201d, IEEE Transactions on Fuzzy Systems, Vol.25, no.3, pp.547-562, (2014) [12] E. Lughofer, C. Cernuda, S. Kindermann and M. Pratama, Generalized Smart Evolving Fuzzy Systems, Evolving Systems, Vol 6(4), pp.269-292, (2015) [13] P. Angelov, Ronald R. Yager: A new type of simplified fuzzy rule-based system. International Journal of General Systems vol.41(2),pp. 163-185 (2012) [14] M.Pratama, S.Anavatti, E.Lughofer, \u201c Evolving fuzzy rule-based classifier based on GENEFIS\u201d, Proceedings of the IEEE Conference on Fuzzy Systems, Hyderabad, India, 2013\n[15] M Pratama, J Lu, S Anavatti, E Lughofer, CP Lim, \u201cAn incremental meta-cognitive-based scaffolding fuzzy neural network \u201d, Vol. 171, pp. 89-105, (2016) [16] M. Pratama, M-J. Er, S.G Anavatti., E, Lughofer., N, Wang., I, Arifin., \u201c A novel meta-cognitive-based scaffolding classifier to sequential non-stationary classification problems\u201d, In proceeding of 2014 International Conference on Fuzzy Systems, 369-376, (2014) [17] M, Pratama., J, Lu., E, Lughofer., G, Zhang., S, Anavatti. \u201c Scaffolding type-2 classifier for incremental learning under concept drifts\u201d, Neurocomputing, Vol. 191, pp. 304-329, 2016 [18] K.Subramanian, A.K.Das, S.Sundaram, S.Ramasamy, \u201c A Meta-Cognitive Interval Type-2 Fuzzy Inference System and Its Projectionbased Learning Algorithm\u201d, Evolving Systems, DOI:10.1007/s12530-013-9102-9 [19] K.Subramanian, S.Suresh, N.Sundararajan, \u201c A metacognitive neuro-fuzzy inference system (mcfis) for sequential classification problems\u201d. IEEE Transactions on Fuzzy Systems, Vol.21, no.6, pp.1080\u20131095, (2013) [20] S. Suresh., K. Dong., H. Kim, \u201cA sequential learning algorithm for self-adaptive resource allocation network classifier,\u201d Neurocomputing, 73(16), 3012\u20133019, (2010) [21] Y. Y. Lin, J. Y. Chang, and C. T. Lin, \u201cA TSK-type based self-evolving compensatory interval type-2 fuzzy neural network (TSCIT2FNN) and its applications,\u201d IEEE Transactions on Industrial Electronics., vol. 61, no. 1, pp. 447\u2013459, (2014) [22] C.F. Juang and C.T. Lin, \u201cA recurrent self-organizing neural fuzzy inference network,\u201d IEEE Transactions on Neural Networks, vol.10, pp.828-845, (1999) [23] C. F. Juang, Y. Y. Lin, and C. C. Tu, \u201cA recurrent self-evolving fuzzy neural network with local feedbacks and its application to dynamic system processing,\u201d Fuzzy Sets and Systems, vol.161,no.19, (2010) [24] C.F. Juang, \u201cA TSK-type recurrent fuzzy network for dynamic systems processing by neural network and genetic algorithms,\u201d IEEE Transactions on Fuzzy Systems, vol.10,no.2, pp.155-170, (2002) [25] Yang-Yin Lin, Jyh-Yeong Chang, Chin-Teng Lin,\u201d Identification and Prediction of Dynamic Systems Using an Interactively Recurrent Self-Evolving Fuzzy Neural Network\u201d, IEEE Transactions on Neural Networks and Learning Systems, Vol.24,no.2,pp.310-321, (2013) [26] M. Pratama, S. Anavatti, P. Angelov, E. Lughofer, PANFIS: A Novel Incremental Learning, IEEE Transactions on Neural Networks and Learning Systems, Vol.25, no.1, pp.55-68,(2014) [27] N. Vukovic., Z. Miljkovic, \u201dA growing and pruning sequential learning algorithm of hyper basis function neural network for function approximation\u201d, Neural Networks, Vol. 46, pp. 210-226, (2013) [28] Bose,R.P.J.C., Bortman, M., & Aladjem, M,\u201d A growing and pruning method for radial basis function networks\u201d, IEEE Transactions on Neural Networks, Vol. 20(6), pp. 1039\u20131045, (2009) [29] Lughofer, E. (2011(b)). Evolving Fuzzy Systems --- Methodologies, Advanced Concepts and Applications, Springer, Heidelberg. [30] Savitha,R.,Suresh,S.,Sundararajan,N. Metacognitive Learning in a Fully Complex-Valued Radial Basis Function Neural Network. Neural computation, Vol. 24(5), 1297-1328, (2012). [31] Das, A.K., Subramanian, K., Suresh, S., An Evolving Interval Type-2 Neurofuzzy Inference System and Its Metacognitive Sequential Learning Algorithm. IEEE Transactions on Fuzzy Systems, Vol. 23(6), pp. 2080-2093, (2015) [32] Xiong,S.,Azimi,J.,Fern,X.Z. Active Learning of Constraints for Semi-Supervised Clustering. . IEEE Transactions on Knowledge and Data Engineering. Vol. 26(1),pp. 43-54, (2014). [33] Zliobaite,I., Bifet,A., Pfahringer.B., Holmes,B. Active Learning with Drifting Streaming Data. IEEE Transactions on Neural Networks and Learning Systems, 25(1), 27-39, (2014). [34] S.-Q. Wu, M-J. Er, and Y. Gao, A fast approach for automatic generation of fuzzy rules by generalized dynamic fuzzy neural networks, IEEE Transaction on Fuzzy System,Vol.9(4),pp.578\u2013594,(2003). [35] B. Vigdor and B. Lerner, \u201cThe Bayesian ARTMAP,\u201d IEEE Transactions on Neural Networks, vol. 18, no. 6, pp. 1628\u20131644, (2007). [36] R. H. Abiyev and O. Kaynak, \u201cType-2 fuzzy neural structure for identification and control of time-varying plants,\u201d IEEE Transactions on Industrial Electronics , vol. 57, no. 12, pp. 4147\u20134159,(2010) [37] Y.Xu, K.W.Wong, C.S.Leung, \u201c Generalized Recursive Least Square to The Training of Neural Network\u201d, IEEE Transactions on Neural Networks, Vol.17,no.1, (2006) [38] R. J. Oentaryo, et al, \"Online probabilistic learning for fuzzy inference systems,\" Expert Systems with Applications, vol. 41, no. 11, pp. 5082-5096, (2014) [39] S. Wu, and M-J. Er, Dynamic fuzzy neural networks\u2014a novel approach to function approximation, IEEE Transaction on Systems Man Cybernetics, part b: Cybernetics,vol.30,pp. 358\u2013364,(2000) [40] J.-S. R. Jang, \u201cANFIS: Adaptive-network-based fuzzy inference system\u201d, IEEE Transaction on System. Man. Cybernetic, part b: cybernetics, vol. 23, pp. 665\u2013684,(1993) [41] Edwin Lughofer, Bogdan Trawinski, Krzysztof Trawinski, Olgierd Kempa, Tadeusz Lasota, On Employing Fuzzy Modeling Algorithms for the Valuation of Residential Premises, Information Sciences, vol. 181 (23), pp. 5123--5142, (2011)"}], "references": [{"title": "Data driven modeling based on dynamic parsimonious fuzzy neural", "author": ["M. Pratama", "M.J. Er", "X. Li", "R.J. Oentaryo", "E. Lughofer", "I. Arifin"], "venue": "network,\" Neurocomputing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Learning in Non-Stationary Environments: Methods and Applications", "author": ["M. Sayed-Mouchaweh", "E. Lughofer"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Simpl_eTS: A simplified method for learning evolving Takagi-Sugeno fuzzy models,", "author": ["P.Angelov", "D. Filev"], "venue": "IEEE International Conference on Fuzzy Systems (FUZZ),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "An on-line self-constructing neural fuzzy inference network and its applications,", "author": ["C.F. Juang", "C.T. Lin"], "venue": "IEEE Transactions on Fuzzy Systems.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "DENFIS: dynamic evolving neural-fuzzy inference system and its application for time series prediction", "author": ["N. Kasabov", "Q. Song"], "venue": "IEEE Transactions on Fuzzy Systems .vol10 (2).pp. 144\u2013154. ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "An approach to online identification of Takagi-Sugeno fuzzy models,", "author": ["P.Angelov", "D. Filev"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B.vol", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A self-evolving interval type-2 fuzzy neural network with online structure and parameter learning,", "author": ["C.F. Juang", "Y.W. Tsao"], "venue": "IEEE Transactions on Fuzzy Systems, vol. 16,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Recurrent Classifier based on An Incremental Meta-Cognitive-based Scaffolding Algorithm", "author": ["M.Pratama", "J.Lu", "S.Anavatti"], "venue": "IEEE Transactions on Fuzzy Systems, Vol.23(6),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "J", "author": ["M. Pratama"], "venue": "Lu, G.Zhang, \u201c Evolving Type-2 Fuzzy Classifier\u201d, online and in press, IEEE Transactions on Fuzzy Systems, Vol. 24(3), pp. 574-589, ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized Smart Evolving Fuzzy Systems", "author": ["E. Lughofer", "C. Cernuda", "S. Kindermann", "M. Pratama"], "venue": "Evolving Systems, Vol 6(4), pp.269-292, ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Ronald R", "author": ["P. Angelov"], "venue": "Yager: A new type of simplified fuzzy rule-based system. International Journal of General Systems vol.41(2),pp. 163-185 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "An incremental meta-cognitive-based scaffolding fuzzy neural network ", "author": ["M Pratama", "J Lu", "S Anavatti", "E Lughofer", "CP Lim"], "venue": "Vol. 171, pp. 89-105, ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "M-J", "author": ["M. Pratama"], "venue": "Er, S.G Anavatti., E, Lughofer., N, Wang., I, Arifin., \u201c A novel meta-cognitive-based scaffolding classifier to sequential non-stationary classification problems\u201d, In proceeding of 2014 International Conference on Fuzzy Systems, 369-376, ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaffolding type-2 classifier for incremental learning under concept drifts", "author": ["J Pratama", "E Lu", "G Lughofer", "S Zhang", "Anavatti"], "venue": "Neurocomputing, Vol. 191,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "A sequential learning algorithm for self-adaptive resource allocation network classifier,", "author": ["S. Suresh", "K. Dong", "H. Kim"], "venue": "Neurocomputing, 73(16),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "and C", "author": ["Y.Y. Lin", "J.Y. Chang"], "venue": "T. Lin, \u201cA TSK-type based self-evolving compensatory interval type-2 fuzzy neural network (TSCIT2FNN) and its applications,\u201d IEEE Transactions on Industrial Electronics., vol. 61, no. 1, pp. 447\u2013459, ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A recurrent self-organizing neural fuzzy inference network,", "author": ["C.F. Juang", "C.T. Lin"], "venue": "IEEE Transactions on Neural Networks, vol.10,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "and C", "author": ["C.F. Juang", "Y.Y. Lin"], "venue": "C. Tu, \u201cA recurrent self-evolving fuzzy neural network with local feedbacks and its application to dynamic system processing,\u201d Fuzzy Sets and Systems, vol.161,no.19, ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A TSK-type recurrent fuzzy network for dynamic systems processing by neural network and genetic algorithms,", "author": ["C.F. Juang"], "venue": "IEEE Transactions on Fuzzy Systems, vol.10,no.2,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Identification and Prediction of Dynamic Systems Using an Interactively Recurrent Self-Evolving Fuzzy Neural Network", "author": ["Yang-Yin Lin", "Jyh-Yeong Chang", "Chin-Teng Lin"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "PANFIS: A Novel Incremental Learning", "author": ["M. Pratama", "S. Anavatti", "P. Angelov", "E. Lughofer"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, Vol.25, no.1, pp.55-68,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A growing and pruning sequential learning algorithm of hyper basis function neural network for function approximation", "author": ["N. Vukovic", "Z. Miljkovic"], "venue": "Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "A growing and pruning method for radial basis function networks", "author": ["Bose", "R.P.J.C", "M. Bortman", "M Aladjem"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Evolving Fuzzy Systems --- Methodologies, Advanced Concepts and Applications", "author": ["E. Lughofer"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "An Evolving Interval Type-2 Neurofuzzy Inference System and Its Metacognitive Sequential Learning Algorithm", "author": ["A.K. Das", "K. Subramanian", "S. Suresh"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Active Learning with Drifting Streaming Data", "author": ["I. Zliobaite", "A. Bifet", "Pfahringer.B", "B. Holmes"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "A fast approach for automatic generation of fuzzy rules by generalized dynamic fuzzy neural networks", "author": ["S.-Q. Wu", "M-J. Er", "Y. Gao"], "venue": "IEEE Transaction on Fuzzy System,Vol.9(4),pp.578\u2013594,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "The Bayesian ARTMAP,", "author": ["B. Vigdor", "B. Lerner"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "et al", "author": ["R.J. Oentaryo"], "venue": "\"Online probabilistic learning for fuzzy inference systems,\" Expert Systems with Applications, vol. 41, no. 11, pp. 5082-5096, ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic fuzzy neural networks\u2014a novel approach to function approximation", "author": ["S. Wu", "M-J. Er"], "venue": "IEEE Transaction on Systems Man Cybernetics, part b: Cybernetics,vol.30,pp. 358\u2013364,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "ANFIS: Adaptive-network-based fuzzy inference system", "author": ["J.-S.R. Jang"], "venue": "IEEE Transaction on System. Man. Cybernetic, part b: cybernetics, vol. 23, pp. 665\u2013684,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "The problem of data streams is frequently encountered in the online time-critical applications, which calls for an efficient algorithm with low computational and storage requirement [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 1, "context": "Tool degradation and changing surface integrity also cause gradual concept drift [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "To this end, Evolving Intelligent System (EIS) has been proposed and features two prominent characteristics: open structure, online learning [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "This is capable of dealing with a possible infinite nature of data stream and satisfying a life-long learning requirement [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "The area of EIS was pioneered by Juang and Lin [4] with SONFIN, although the term \u201cEvolving\u201d has not been formalized until the development of DENFIS [5] and eTS [6].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "The area of EIS was pioneered by Juang and Lin [4] with SONFIN, although the term \u201cEvolving\u201d has not been formalized until the development of DENFIS [5] and eTS [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "The area of EIS was pioneered by Juang and Lin [4] with SONFIN, although the term \u201cEvolving\u201d has not been formalized until the development of DENFIS [5] and eTS [6].", "startOffset": 161, "endOffset": 164}, {"referenceID": 6, "context": "Since then the area of EIS has attracted various contributions [7] - [15].", "startOffset": 63, "endOffset": 66}, {"referenceID": 11, "context": "Since then the area of EIS has attracted various contributions [7] - [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "Although the concept of metacognitive learning incorporating the what-to-learn component aims at addressing redundant data streams [16][20], most of them are designed for classification problems, while a regression problem is an open issue.", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Although the concept of metacognitive learning incorporating the what-to-learn component aims at addressing redundant data streams [16][20], most of them are designed for classification problems, while a regression problem is an open issue.", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "Some attempt has been devoted to actualise the evolving concept in the recurrent network structure and the interval-type 2 fuzzy system [21]-[25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "Some attempt has been devoted to actualise the evolving concept in the recurrent network structure and the interval-type 2 fuzzy system [21]-[25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "PANFIS++ presents an extended version of PANFIS [26], which is not only capable of mining data streams efficiently but also selecting", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Although the local recurrent connection has been put into perspective in [21], [22], the novelty of our work can be found in the rule layer of the PANFIS++, which features the interval type-2 multivariate Gaussian function with the intervalvalued centroids; 3) PANFIS++ realises a generalized interval type-2 Takagi-Sugeno-Kang (TSK) fuzzy rule.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Although the local recurrent connection has been put into perspective in [21], [22], the novelty of our work can be found in the rule layer of the PANFIS++, which features the interval type-2 multivariate Gaussian function with the intervalvalued centroids; 3) PANFIS++ realises a generalized interval type-2 Takagi-Sugeno-Kang (TSK) fuzzy rule.", "startOffset": 79, "endOffset": 83}, {"referenceID": 6, "context": "The GT2DS is a generalized version of the T2DS method [7], which is designed under a strict condition of uniformly distributed training data.", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "It adopts a strategy of [27], [28], which estimates a complex probability density function with the Gaussian Mixture Model (GMM).", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "It adopts a strategy of [27], [28], which estimates a complex probability density function with the Gaussian Mixture Model (GMM).", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "It is worth noting that [27], [28] are all designed for the type-1 fuzzy system; 5) PANFIS++ integrates the rule pruning scenario, which is capable of discarding an outdated rule \u2013 no longer relevant to current learning context and an inconsequential rule \u2013 plays little role during its lifespan.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "It is worth noting that [27], [28] are all designed for the type-1 fuzzy system; 5) PANFIS++ integrates the rule pruning scenario, which is capable of discarding an outdated rule \u2013 no longer relevant to current learning context and an inconsequential rule \u2013 plays little role during its lifespan.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "The radii of the fuzzy set are formulated as a distance between the centres to the cutting points of the ellipsoidal cluster [26] as follows:", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "pool-based approach, which assumes all data are available in the pool for an iterative selection procedure [29].", "startOffset": 107, "endOffset": 111}, {"referenceID": 24, "context": "scenarios to mine data streams [30], [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "in [31] but it is still based on the hinge loss function, which is sensitive to the system\u2019s error.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "We set s as its default value [33] s=0.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "The original version of DQ method [7] is derived with the p-fold numerical integration, which is only suitable for a small input dimension.", "startOffset": 34, "endOffset": 37}, {"referenceID": 21, "context": "It is inspired by the works of [27], [28] taking advantage of the GMM as a probability density function to deal with a complex and irregular data distribution.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "It is inspired by the works of [27], [28] taking advantage of the GMM as a probability density function to deal with a complex and irregular data distribution.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "In light of the rule significance definition [7], the significance of interval-valued multivariate Gaussian function is defined as the Lu-norm of the error function weighted by the input density function.", "startOffset": 45, "endOffset": 48}, {"referenceID": 26, "context": "The initialization of the covariance matrix (16) has been proven mathematically to meet the \u03b5 \u2013 completeness condition [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "This rule growing condition differs from its predecessors [7], [27], [28], which are reliant on a user-defined threshold.", "startOffset": 58, "endOffset": 61}, {"referenceID": 21, "context": "This rule growing condition differs from its predecessors [7], [27], [28], which are reliant on a user-defined threshold.", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "This rule growing condition differs from its predecessors [7], [27], [28], which are reliant on a user-defined threshold.", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "There are several ways to choose the winning rule: the distance-based method, the Bayesian concept [35].", "startOffset": 99, "endOffset": 103}, {"referenceID": 27, "context": "It is not recounted here and interested reader is advised to go to [35] for further details of the Bayesian winning rule selection.", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "generalized version of the gradient descent method [35].", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 100, "endOffset": 103}, {"referenceID": 28, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 151, "endOffset": 155}, {"referenceID": 26, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 163, "endOffset": 167}, {"referenceID": 30, "context": "The PANFIS++ was compared against 10 prominent learning algorithms: eT2Class [9], Simp_eTS [3], eTS [6], BARTFIS [38], PANFIS [26], GENEFIS [11], DFNN [39], GDFNN [34], ANFIS [40] and benchmarked algorithms were compared against five evaluation criteria: predictive accuracy, fuzzy rule, input attribute, runtime, training sample, and network parameters.", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "The predictive task was carried out using 12 time-domain features extracted from the force signal [1].", "startOffset": 98, "endOffset": 101}], "year": 2017, "abstractText": "Mahardhika Pratama Abstract \u2013 the concept of evolving intelligent system (EIS) provides an effective avenue for data stream mining because it is capable of coping with two prominent issues: online learning and rapidly changing environments. We note at least three uncharted territories of existing EISs: data uncertainty, temporal system dynamic, redundant data streams. This book chapter aims at delivering a concrete solution of this problem with the algorithmic development of a novel learning algorithm, namely PANFIS++. PANFIS++ is a generalized version of the PANFIS by putting forward three important components: 1) An online active learning scenario is developed to overcome redundant data streams. This module allows to actively select data streams for the training process, thereby expediting execution time and enhancing generalization performance; 2) PANFIS++ is built upon an interval type-2 fuzzy system environment, which incorporates the so-called footprint of uncertainty. This component provides a degree of tolerance for data uncertainty. 3) PANFIS++ is structured under a recurrent network architecture with a self-feedback loop. This is meant to tackle the temporal system dynamic. The efficacy of the PANFIS++ has been numerically validated through numerous real-world and synthetic case studies, where it delivers the highest predictive accuracy while retaining the lowest complexity.", "creator": "Microsoft\u00ae Word 2016"}}}