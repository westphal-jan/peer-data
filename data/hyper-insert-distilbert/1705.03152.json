{"id": "1705.03152", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Phone-aware Neural Language Identification", "abstract": "pure acoustic hybrid neural models, particularly implementing the lstm - coupled rnn imaging model, have shown great potential in automated language information identification ( dual lid ). however, the essential phonetic information content has been largely immediately overlooked by most of nine existing 2d neural lid models, thus although significantly this information also has been used in reviewing the myriad conventional formal phonetic link lid systems with equally a great success. likewise we commonly present comparatively a powerful phone - aware neural lid architecture, at which is a notably deep lstm - rnn resonance lid system but generally accepts output recordings from an rnn - based robust asr system. by utilizing all the phonetic node knowledge, measure the lid performance can be dramatically significantly improved. interestingly, it even like if the test language atlas is not involved in the asr training, measuring the phonetic knowledge still presents not a large contribution. performing our acoustic experiments conducted on recognizing four adjacent languages within scanning the babel corpus demonstrated instead that choosing the phone - aware resonance approach is highly effective.", "histories": [["v1", "Tue, 9 May 2017 02:47:22 GMT  (1007kb,D)", "http://arxiv.org/abs/1705.03152v1", "Submitted to Interspeech 2017"], ["v2", "Mon, 22 May 2017 11:43:40 GMT  (1007kb,D)", "http://arxiv.org/abs/1705.03152v2", "arXiv admin note: text overlap witharXiv:1705.03151"]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["zhiyuan tang", "dong wang", "yixiang chen", "ying shi", "lantian li"], "accepted": false, "id": "1705.03152"}, "pdf": {"name": "1705.03152.pdf", "metadata": {"source": "CRF", "title": "Phone-aware Neural Language Identification", "authors": ["Zhiyuan Tang", "Dong Wang", "Yixiang Chen", "Ying Shi", "Lantian Li"], "emails": ["wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID models, although this information has been used in the conventional phonetic LID systems with a great success. We present a phone-aware neural LID architecture, which is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR system. By utilizing the phonetic knowledge, the LID performance can be significantly improved. Interestingly, even if the test language is not involved in the ASR training, the phonetic knowledge still presents a large contribution. Our experiments conducted on four languages within the Babel corpus demonstrated that the phone-aware approach is highly effective. Index Terms: language identification, long short-term memory"}, {"heading": "1. Introduction", "text": "Language identification (LID) lends itself to a wide range of applications, e.g., mix-lingual (code-switching) speech recognition. Early methods are based on statistical models of phonetic or acoustic units [1, 2, 3]. Recent methods are based on probabilistic acoustic modeling, among which the i-vector model is perhaps the most successful [4, 5].\nRecently, deep neural models have attracted much attention in LID. Lopez-Moreno et al. [6] proposed a DNN-based approach which uses a DNN to discriminate different languages at the frame-level, and the language posteriors of an utterance are generated by a simple average of all the frame-level posteriors of the utterance. An RNN-based approach was later proposed by Gonzalez-Dominguez et al. [7], and better performance was obtained with much less parameters compared to the DNN-based model. Due to the advantage in temporal modeling, the RNN approach has been followed by a number of researchers, e.g., [8, 9]. Other neural model structures were also investigated, e.g., CNN [10, 11] and TDNN [12, 13]. Compared to the i-vector approach that is based on a probabilistic model, these pure neural methods show clear advantage in short utterances (e.g., 2\u223c3 seconds) [6, 7, 9]. The main advantage of the neural-based methods, compared to the i-vector model, is that they are discriminative and can learn complex decision bounds between languages, provided that sufficient data is provided. Moreover, the power of feature learning associated with deep neural nets often provides better robustness against noise and speaker variation, which is highly important for LID.\nDeep neural models are also used in a hybrid way, i.e., to generate features [14] or alignment [15, 16] for an i-vector model. In this case, the phonetic DNN/RNN model is trained for phone discrimination as in automatic speech recognition (ASR). This model is then used to produce bottle-neck features or acoustic alignment for constructing the i-vector model. By using the phonetic information that is directly related to LID, the i-vector model can be consistently improved.\nThe above two-approaches have their own disadvantages. For the pure neural approach, the entire system relies on raw features, ignoring any phonetic information that is known to be important from the beginning of LID research [2]; for the hybrid system, it is still based on a probabilistic model that (1) involves a strong Gaussian assumption that is not suitable for dealing with complex class (here, language) boundaries; (2) requires relatively more speech frames to estimate a reliable i-vector, which is not applicable to many real applications that require quick identification, e.g., code-switch ASR.\nIn this paper, we follow the pure neural model scheme, and enhance the existing models by introducing phonetic information as an auxiliary feature. Due to the clear advantage of the LSTM-RNN in both ASR and LID, we adopt this model in the study, though the idea of leveraging phonetic information is applicable to any neural models. The architecture is illustrated in Figure 1, which involves a phonetic RNN that is trained to discriminate phones as in ASR and produces phonetic features once the training is done, and an LID RNN which receives the phonetic features and uses them together with the raw acoustic feature to perform LID. This model has the following properties:\n\u2022 The phonetic RNN can be trained with flexible objectives. It can be discriminant for phones (as in ASR) or for both phones and languages, following the multi-task learning principle [17].\n\u2022 The phonetic RNN can be trained independently from the LID RNN. This means that it can be trained using data of any languages that are totally different from the target languages of the LID task. This is particularly attractive when the LID task is to discriminate lowresource languages.\n\u2022 The phonetic feature extraction and propagation is flexible. It can be extracted from any place of the phonetic RNN, and can be propagated to any place of the LID RNN.\nThis architecture is a reminiscence of the early phonetic recognition and language modeling (PRLM) approach [18], where a phone recognizer is used as a front-end to decode phonetic units, followed by a phonetic LM to perform scoring. The two RNNs in our architecture can be regarded as corresponding to the phone recognizer and the LM respectively, although the structure is much more flexible than the historic model. In fact, if the phonetic features are derived from the output layer of the phonetic RNN and are propagated to the input layer of the LID RNN, and if the raw feature is omitted, we obtain a PRLM system where the LID RNN is essentially an RNN-based phone LM. This architecture was recently studied by Salamea et al. [19].\nThe rest of the paper is organized as follows: the model structure is described in Section 2 and the experiments are re-\nar X\niv :1\n70 5.\n03 15\n2v 1\n[ cs\n.C L\n] 9\nM ay\n2 01\n7\nMultilingual input\nLID output\nLSTM LSTM\nASR output\nported in Section 3, followed by some conclusions and future work in Section 4."}, {"heading": "2. Model structure", "text": "We choose the LSTM-RNN as the phonetic RNN and the LID RNN components in the study. One reason for the choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural approach [7] and the hybrid approach [16]. Another reason is that this structure (phonetic feature plus RNN LID) is in accordance with our motivation to model the phonetic dynamics as in the old PRLM approach [18]. This section first describes the LSTM-RNN structure used in the study, and then presents the phone-aware LID system."}, {"heading": "2.1. LSTM structure", "text": "The LSTM model proposed in [20] is used in the study, as shown in Figure 2.\nThe associated computation is given as follows:\nit = \u03c3(Wixxt +Wirrt\u22121 +Wicct\u22121 + bi)\nft = \u03c3(Wfxxt +Wfrrt\u22121 +Wfcct\u22121 + bf )\nct = ft ct\u22121 + it g(Wcxxt +Wcrrt\u22121 + bc) ot = \u03c3(Woxxt +Worrt\u22121 +Wocct + bo)\nmt = ot h(ct) rt = Wrmmt\npt = Wpmmt\nyt = Wyrrt +Wyppt + by\nIn the above equations, theW terms denote weight matrices and those associated with the cells were constrained to be diagonal in our implementation. The b terms denote bias vectors. xt and yt are the input and output symbols respectively; it, ft, ot represent respectively the input, forget and output gates; ct is the cell and mt is the cell output. rt and pt are two output components derived from mt, where rt is recurrent and fed to the next time step, while pt is not recurrent and contributes to the present output only. \u03c3(\u00b7) is the logistic sigmoid function, and g(\u00b7) and h(\u00b7) are non-linear activation functions, often chosen to be hyperbolic. denotes element-wise multiplication."}, {"heading": "2.2. Phone-aware LID system", "text": "As a preliminary work to demonstrate the concept, we design a simple phone-aware LID system as shown in Figure 3, where both the phonetic RNN and the LID RNN involve a single LSTM layer. Although the phonetic features can be extracted from any places of the phonetic RNN, we choose to use the output of the recurrent projection layer. Similarly, the receiver of the phonetic features is also flexible and we will investigate the performance of difference choices. The configure shown in Figure 3 uses the non-linear function g(\u00b7) as the receiver. With this configure, most computation of the LID RNN remains the same, except that the cell value should be updated as follows:\nct = ft ct\u22121 + it g(Wcxxt +Wcrrt\u22121 +W \u2032crr\u2032t + bc)\nwhere r\u2032t is the phonetic feature propagated from the phonetic RNN.\nIn this study, all the RNN models use the same configuration that the LSTM layer consists of 1, 024 cells, and the dimensionality of both the recurrent and non-recurrent projections is set to 256. The natural stochastic gradient descent (NSGD) algorithm [21] is employed to train the model."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Database and configurations", "text": "The experiments were conducted with the Babel corpus. This corpus was collected as part of the IARPA (Intelligence Advanced Research Projects Activity) Babel program, with aim\nto develop speech technologies for low-resource languages. In this paper, we chose speech data of four languages from the Babel corpus to conduct the study: Assamese, Georgian, Bengali, and Turkish. For each language, a training dataset and a development dataset were officially provided. Training dataset contains both conversational speech and scripted speech and development dataset only has conversational speech. We used the entire training set of each language in the model training, but randomly selected 2, 000 utterances from the development set of each language to perform the test. The training data sets from the four languages are as follows: Assamese1 187 hours, Georgian2 159 hours, Bengali3 217 hours, and Turkish4 268 hours. The average length of the test utterances is 9.50 seconds, ranging from 0.48 seconds to 56.78 seconds.\nThe raw feature used for the RNN models is 23- dimensional Fbanks, with a symmetric 2-frame window to splice neighboring frames. All the experiments were conducted with Kaldi [22]. The default configurations of the Kaldi WSJ s5 nnet3 recipe were used to train the phonetic RNN and the LID RNN."}, {"heading": "3.2. Baseline results", "text": "As the first step, we build three RNN baseline systems using the speech data of two languages: Assamese and Georgian. The three RNN baselines are: multilingual ASR system (AG-ASR), LID system (AG-LID), ASR-LID multi-task system (AG-MLT). For the AG-ASR, the phone sets of the two languages are merged and the softmax group involves all the state targets, which is 3, 349 in our experiment. The ASR performance in terms of word error rate (WER) is 68.2% and 65.5% for Assamese and Georgian on the whole development dataset, respectively. The training and decoding follow the standard WSJ s5 nnet3 recipe of Kaldi. For the AG-LID, the output layer consists of two units, corresponding to the two languages respectively. The training procedure is similar to the one used for training the AG-ASR model. The AG-MLT model involves two groups of targets, and the training utilizes the labels of both phones and languages.\nThe LID for Assamese and Georgian can be conducted by either AG-LID or AG-MLT, using the language posteriors they produce. The performance results with these two systems, in terms of Cavg and equal error rate (EER), are shown in Table 1. Both the frame-level performance and the utterance-level performance are reported. For the utterance-level results, the frame-level posteriors are averaged to produce the utterancelevel posterior, with which the LID is conducted.\nThe results in Table 1 indicates that both the LID RNN and the multi-task LID RNN are capable of language discrimination, and the multi-task RNN performs better. This is expected\n1Language collection release IARPA-babel102b-v0.5a. 2Language collection release IARPA-babel404b-v1.0a. 3Language collection release IARPA-babel103b-v0.4b. 4Language collection release IARPA-babel105b-v0.5.\nas the ASR targets can help to regularize the model training and alleviate the impact of variability factors such as noise and speaker."}, {"heading": "3.3. Phonetic feature", "text": "The three baseline RNNs will be used as the candidates of the phonetic RNN. We visualize the discriminative power of the phonetic features produced by these RNNs using PCA. Specifically, 20 test utterances are randomly selected from the test set for each language, and these utterances are fed into the phonetic RNN frame by frame. For each frame, the phonetic feature is read from the recurrent projection layer of the tested RNN, and then is projected into the 2-dimensional space by PCA. Figure 4 presents the distribution of the features for Assamese and Georgian, the two languages \u2018known\u2019 in the model training. Figure 5 shows the distribution for Bengali and Turkish, two language that are \u2018unknown\u2019 in the model training. Figure 6 shows the distribution of the features of all the four languages. It can be observed that all the three RNNs possess certain discriminative capability for both the known and unknown languages. Comparing the three models, the features generated by the ASRbased RNN is clearly worse, and the features generated by the multi-task RNN looks more discriminative. Note that the phonetic features of Assamese and Bengali are highly overlapped, no matter which phonetic RNN is used. This means that the four-language LID task will be highly difficult, as we will see shortly."}, {"heading": "3.4. Phone-aware LID on known languages", "text": "Due to the clear advantage of the AG-MLT in language discrimination, we first choose this model to be the candidate of the\nphonetic RNN to produce phonetic features. The LID RNNs are trained to discriminate the two known languages: Assamese and Georgian. The results are shown in Table 2, where four configurations for the \u2018receiver\u2019 of the phonetic feature are tested: the input gate, the forget gate, the output gate and the g function. Compared to the results with the baseline RNNs (Table 1), introducing the phonetic feature leads to clear performance improvement, on both the frame-level and the utterance-level, in terms of both Cavg and EER.\nThen we use the AG-ASR and AG-LID models as the phonetic RNNs with only the best configurations above, that is, the g function or output gate as the receiver. The AG-ASR results in better performance than both the baseline and AG-LID, which further confirms our conjecture that phonetic information is valuable for neural-based LID.\nTable 2: Results of phone-aware LID on known languages (Assamese and Georgian).\nCavg EER% Phonetic RNN Inf. receiver Fr. Utt. Fr. Utt.\nAG-MLT input gate 0.0762 0.0298 7.99 3.25 AG-MLT forget gate 0.0765 0.0280 8.01 3.20 AG-MLT output gate 0.0752 0.0286 7.91 3.15 AG-MLT g function 0.0726 0.0264 7.57 2.92 AG-ASR output gate 0.0801 0.0298 8.38 3.28 AG-ASR g function 0.0795 0.0283 8.38 3.08 AG-LID output gate 0.1310 0.0638 13.67 7.00 AG-LID g function 0.1279 0.0640 13.45 6.95"}, {"heading": "3.5. Phone-aware LID on unknown languages", "text": "We now test the generalizability of the phonetic feature. Specifically, we use the feature to help discriminate two new languages, i.e., the languages that are unknown during the phonetic RNN training, which are Bengali and Turkish in our experiment. To test the gain with the phonetic feature, the LID RNN trained with the two target languages, Bengali and Turkish, denoted by BT-LID, is used as the baseline. For simplicity, we only test the scenario where the AG-MLT is used as the phonetic RNN, and the g function is used as the feature receiver.\nThe results are shown in Table 3. It can be seen that although the phonetic RNN has no knowledge of the two target languages, the phonetic feature it produces is still highly valuable for the LID task. This is understandable as the phonetic units are often shared by human languages, and so the phonetic\ninformation the phonetic RNN provides is generally valuable. This in fact demonstrates that the phonetic RNN can be trained very flexibly, by using speech data of any languages. This is particulary interesting for languages with too little training data to obtain a reasonable phonetic RNN."}, {"heading": "3.6. Phone-aware LID on four languages", "text": "The final experiment tests the LID performance on all the four languages. The baseline system is the RNN LID model trained with the data of the four languages, denoted by AGBT-LID. For the phone-aware system, the AG-MLT is used to produce the phonetic feature, and the g function is used as the feature receiver. The results are shown in Table 4. Again, the performance is greatly improved by involving the phonetic feature.\nTable 4: Results of phone-aware LID on four languages (Assamese, Georgian, Bengali and Turkish).\nCavg EER% Phonetic RNN LID RNN Fr. Utt. Fr. Utt.\n- AGBT-LID 0.2131 0.1499 21.34 15.75 AG-MLT AGBT-LID 0.1531 0.0833 15.43 9.27"}, {"heading": "4. Conclusions", "text": "We presented a phone-aware LSTM-RNN model for language identification. Our argument is that phonetic information is important for LID. This information has been successfully used in the historical phonetic models such as the famous PRLM system, but it has been largely ignored by the present pure acoustic methods, either the i-vector model or the pure neural model. Particularly with the LSTM-RNN model, the inherent power on modeling temporal dynamics with this model has been largely wasted without phonetic information involved. The phoneaware architecture we proposed in the paper employs a deep neural model to produce phonetic features and these features are propagated to the vanilla LSTM-RNN LID system. Our experiments conducted on the data of four languages of the Babel corpus demonstrated that the phone-aware model can dramatically improve performance of the LSTM-RNN LID system. In the future, we will test the phone-aware approach on more languages and under more complex conditions. Particularly, we expect that with the phonetic information, the RNN-based LID may be significantly improved on long utterances, by providing the phonetic normalization."}, {"heading": "5. References", "text": "[1] L. F. Lamel and J.-L. Gauvain, \u201cLanguage identification using\nphone-based acoustic likelihoods,\u201d in Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on, vol. 1. IEEE, 1994, pp. I\u2013293.\n[2] M. A. Zissman et al., \u201cComparison of four approaches to automatic language identification of telephone speech,\u201d IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.\n[3] H. Li, B. Ma, and C.-H. Lee, \u201cA vector space modeling approach to spoken language identification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 271\u2013284, 2007.\n[4] N. Dehak, A.-C. Pedro, D. Reynolds, and R. Dehak, \u201cLanguage recognition via i-vectors and dimensionality reduction,\u201d in Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 857\u2013860.\n[5] D. Mart\u0131nez, O. Plchot, L. Burget, O. Glembek, and P. Matejka, \u201cLanguage recognition in ivectors space,\u201d Proceedings of Interspeech, Firenze, Italy, pp. 861\u2013864, 2011.\n[6] I. Lopez-Moreno, J. Gonzalez-Dominguez, O. Plchot, D. Martinez, J. Gonzalez-Rodriguez, and P. Moreno, \u201cAutomatic language identification using deep neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5337\u20135341.\n[7] J. Gonzalez-Dominguez, I. Lopez-Moreno, H. Sak, J. GonzalezRodriguez, and P. J. Moreno, \u201cAutomatic language identification using long short-term memory recurrent neural networks.\u201d in Interspeech, 2014, pp. 2155\u20132159.\n[8] G. Gelly, J.-L. Gauvain, V. Le, and A. Messaoudi, \u201cA divide-andconquer approach for language identification based on recurrent neural networks,\u201d Interspeech 2016, pp. 3231\u20133235, 2016.\n[9] R. Zazo, A. Lozano-Diez, J. Gonzalez-Dominguez, D. T. Toledano, and J. Gonzalez-Rodriguez, \u201cLanguage identification in short utterances using long short-term memory (LSTM) recurrent neural networks,\u201d PloS one, vol. 11, no. 1, p. e0146917, 2016.\n[10] A. Lozano-Diez, R. Zazo Candil, J. Gonza\u0301lez Dom\u0131\u0301nguez, D. T. Toledano, and J. Gonzalez-Rodriguez, \u201cAn end-to-end approach to language identification in short utterances using convolutional neural networks,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. International Speech and Communication Association, 2015.\n[11] M. Jin, Y. Song, I. Mcloughlin, L.-R. Dai, and Z.-F. Ye, \u201cLIDsenone extraction via deep neural networks for end-to-end language identification,\u201d in Proc. of Odyssey, 2016.\n[12] M. Kotov and M. Nastasenko, \u201cLanguage identification using time delay neural network d-vector on short utterances,\u201d in Speech and Computer: 18th International Conference, SPECOM 2016, Budapest, Hungary, August 23-27, 2016, Proceedings, vol. 9811. Springer, 2016, p. 443.\n[13] D. Garcia-Romero and A. McCree, \u201cStacked long-term tdnn for spoken language recognition,\u201d Interspeech 2016, pp. 3226\u20133230, 2016.\n[14] Y. Song, B. Jiang, Y. Bao, S. Wei, and L.-R. Dai, \u201cI-vector representation based on bottleneck features for language identification,\u201d Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.\n[15] L. Ferrer, Y. Lei, M. McLaren, and N. Scheffer, \u201cStudy of senonebased deep neural network approaches for spoken language recognition,\u201d IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 24, no. 1, pp. 105\u2013116, 2016.\n[16] Y. Tian, L. He, Y. Liu, and J. Liu, \u201cInvestigation of senone-based long-short term memory rnns for spoken language recognition,\u201d Odyssey 2016, pp. 89\u201393, 2016.\n[17] R. Caruana, \u201cMultitask learning,\u201d Machine Learning, vol. 28, no. 1, pp. 41\u201375, 1997.\n[18] P. Matejka, L. Burget, P. Schwarz, and J. Cernocky, \u201cBrno university of technology system for nist 2005 language recognition evaluation,\u201d in Speaker and Language Recognition Workshop, 2006. IEEE Odyssey 2006: The. IEEE, 2006, pp. 1\u20137.\n[19] C. Salamea, L. F. D\u2019Haro, R. de Co\u0301rdoba, and R. San-Segundo, \u201cOn the use of phone-gram units in recurrent neural networks for language identification,\u201d Odyssey 2016, pp. 117\u2013123, 2016.\n[20] H. Sak, A. W. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014, pp. 338\u2013342.\n[21] D. Povey, X. Zhang, and S. Khudanpur, \u201cParallel training of deep neural networks with natural gradient and parameter averaging,\u201d arXiv preprint arXiv:1410.7455, 2014.\n[22] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFLCONF-192584. IEEE Signal Processing Society, 2011."}], "references": [{"title": "Language identification using phone-based acoustic likelihoods", "author": ["L.F. Lamel", "J.-L. Gauvain"], "venue": "Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on, vol. 1. IEEE, 1994, pp. I\u2013293.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Comparison of four approaches to automatic language identification of telephone speech", "author": ["M.A. Zissman"], "venue": "IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "A vector space modeling approach to spoken language identification", "author": ["H. Li", "B. Ma", "C.-H. Lee"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 271\u2013284, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Language recognition via i-vectors and dimensionality reduction", "author": ["N. Dehak", "A.-C. Pedro", "D. Reynolds", "R. Dehak"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 857\u2013860.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Language recognition in ivectors space", "author": ["D. Mart\u0131nez", "O. Plchot", "L. Burget", "O. Glembek", "P. Matejka"], "venue": "Proceedings of Interspeech, Firenze, Italy, pp. 861\u2013864, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5337\u20135341.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic language identification using long short-term memory recurrent neural networks.", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno"], "venue": "in Interspeech,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A divide-andconquer approach for language identification based on recurrent neural networks", "author": ["G. Gelly", "J.-L. Gauvain", "V. Le", "A. Messaoudi"], "venue": "Interspeech 2016, pp. 3231\u20133235, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification in short utterances using long short-term memory (LSTM) recurrent neural networks", "author": ["R. Zazo", "A. Lozano-Diez", "J. Gonzalez-Dominguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "PloS one, vol. 11, no. 1, p. e0146917, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "An end-to-end approach to language identification in short utterances using convolutional neural networks", "author": ["A. Lozano-Diez", "R. Zazo Candil", "J. Gonz\u00e1lez Dom\u0131\u0301nguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTER- SPEECH. International Speech and Communication Association, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "LIDsenone extraction via deep neural networks for end-to-end language identification", "author": ["M. Jin", "Y. Song", "I. Mcloughlin", "L.-R. Dai", "Z.-F. Ye"], "venue": "Proc. of Odyssey, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification using time delay neural network d-vector on short utterances", "author": ["M. Kotov", "M. Nastasenko"], "venue": "Speech and Computer: 18th International Conference, SPECOM 2016, Budapest, Hungary, August 23-27, 2016, Proceedings, vol. 9811. Springer, 2016, p. 443.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked long-term tdnn for spoken language recognition", "author": ["D. Garcia-Romero", "A. McCree"], "venue": "Interspeech 2016, pp. 3226\u20133230, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Study of senonebased deep neural network approaches for spoken language recognition", "author": ["L. Ferrer", "Y. Lei", "M. McLaren", "N. Scheffer"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 24, no. 1, pp. 105\u2013116, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigation of senone-based long-short term memory rnns for spoken language recognition", "author": ["Y. Tian", "L. He", "Y. Liu", "J. Liu"], "venue": "Odyssey 2016, pp. 89\u201393, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Brno university of technology system for nist 2005 language recognition evaluation", "author": ["P. Matejka", "L. Burget", "P. Schwarz", "J. Cernocky"], "venue": "Speaker and Language Recognition Workshop, 2006. IEEE Odyssey 2006: The. IEEE, 2006, pp. 1\u20137.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "On the use of phone-gram units in recurrent neural networks for language identification", "author": ["C. Salamea", "L.F. D\u2019Haro", "R. de C\u00f3rdoba", "R. San-Segundo"], "venue": "Odyssey 2016, pp. 117\u2013123, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014, pp. 338\u2013342.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Early methods are based on statistical models of phonetic or acoustic units [1, 2, 3].", "startOffset": 76, "endOffset": 85}, {"referenceID": 1, "context": "Early methods are based on statistical models of phonetic or acoustic units [1, 2, 3].", "startOffset": 76, "endOffset": 85}, {"referenceID": 2, "context": "Early methods are based on statistical models of phonetic or acoustic units [1, 2, 3].", "startOffset": 76, "endOffset": 85}, {"referenceID": 3, "context": "Recent methods are based on probabilistic acoustic modeling, among which the i-vector model is perhaps the most successful [4, 5].", "startOffset": 123, "endOffset": 129}, {"referenceID": 4, "context": "Recent methods are based on probabilistic acoustic modeling, among which the i-vector model is perhaps the most successful [4, 5].", "startOffset": 123, "endOffset": 129}, {"referenceID": 5, "context": "[6] proposed a DNN-based approach which uses a DNN to discriminate different languages at the frame-level, and the language posteriors of an utterance are generated by a simple average of all the frame-level posteriors of the utterance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7], and better performance was obtained with much less parameters compared to the DNN-based model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": ", [8, 9].", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [8, 9].", "startOffset": 2, "endOffset": 8}, {"referenceID": 9, "context": ", CNN [10, 11] and TDNN [12, 13].", "startOffset": 6, "endOffset": 14}, {"referenceID": 10, "context": ", CNN [10, 11] and TDNN [12, 13].", "startOffset": 6, "endOffset": 14}, {"referenceID": 11, "context": ", CNN [10, 11] and TDNN [12, 13].", "startOffset": 24, "endOffset": 32}, {"referenceID": 12, "context": ", CNN [10, 11] and TDNN [12, 13].", "startOffset": 24, "endOffset": 32}, {"referenceID": 5, "context": ", 2\u223c3 seconds) [6, 7, 9].", "startOffset": 15, "endOffset": 24}, {"referenceID": 6, "context": ", 2\u223c3 seconds) [6, 7, 9].", "startOffset": 15, "endOffset": 24}, {"referenceID": 8, "context": ", 2\u223c3 seconds) [6, 7, 9].", "startOffset": 15, "endOffset": 24}, {"referenceID": 13, "context": ", to generate features [14] or alignment [15, 16] for an i-vector model.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": ", to generate features [14] or alignment [15, 16] for an i-vector model.", "startOffset": 41, "endOffset": 49}, {"referenceID": 15, "context": ", to generate features [14] or alignment [15, 16] for an i-vector model.", "startOffset": 41, "endOffset": 49}, {"referenceID": 1, "context": "For the pure neural approach, the entire system relies on raw features, ignoring any phonetic information that is known to be important from the beginning of LID research [2]; for the hybrid system, it is still based on a probabilistic model that (1) involves a strong Gaussian assumption that is not suitable for dealing with complex class (here, language) boundaries; (2) requires relatively more speech frames to estimate a reliable i-vector, which is not applicable to many real applications that require quick identification, e.", "startOffset": 171, "endOffset": 174}, {"referenceID": 16, "context": "It can be discriminant for phones (as in ASR) or for both phones and languages, following the multi-task learning principle [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "This architecture is a reminiscence of the early phonetic recognition and language modeling (PRLM) approach [18], where a phone recognizer is used as a front-end to decode phonetic units, followed by a phonetic LM to perform scoring.", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "One reason for the choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural approach [7] and the hybrid approach [16].", "startOffset": 114, "endOffset": 117}, {"referenceID": 15, "context": "One reason for the choice is that LSTM-RNN has been demonstrated to perform well in both the pure neural approach [7] and the hybrid approach [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "Another reason is that this structure (phonetic feature plus RNN LID) is in accordance with our motivation to model the phonetic dynamics as in the old PRLM approach [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 19, "context": "The LSTM model proposed in [20] is used in the study, as shown in Figure 2.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "The picture is reproduced from [20].", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "The natural stochastic gradient descent (NSGD) algorithm [21] is employed to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "All the experiments were conducted with Kaldi [22].", "startOffset": 46, "endOffset": 50}], "year": 2017, "abstractText": "Pure acoustic neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID models, although this information has been used in the conventional phonetic LID systems with a great success. We present a phone-aware neural LID architecture, which is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR system. By utilizing the phonetic knowledge, the LID performance can be significantly improved. Interestingly, even if the test language is not involved in the ASR training, the phonetic knowledge still presents a large contribution. Our experiments conducted on four languages within the Babel corpus demonstrated that the phone-aware approach is highly effective.", "creator": "LaTeX with hyperref package"}}}