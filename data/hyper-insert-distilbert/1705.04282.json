{"id": "1705.04282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Learning to see people like people", "abstract": "often humans make complex inferences on hidden faces, ranging loosely from objective properties ( gender, ethnicity, expression, age, bodily identity, status etc ) to subjective judgments ( mainly facial attractiveness, trustworthiness, sociability, friendliness, etc ). while maintaining the objective aspects of ordinary face perception modelling have been extensively studied, data relatively with fewer computational thought models have effectively been developed save for the social context impressions of seen faces. bridging outside this gap, we collectively develop simply a method paradigm to predict human impressions perceived of faces together in a 40 % subjective perception social data dimensions, using deep representations from state - organization of - the - as art neural cognitive networks. therefore we also find that model recognizing performance grows as the human consensus on knowing a whole face trait increases, and again that model predictions constantly outperform human sized groups in correlation with human spatial averages. this illustrates describes the learnability bias of subjective social self perception properties of imagined faces, displayed especially when demonstrating there is as high human consensus. our system can essentially be used to decide regardless which prospective photographs from a standard personal collection will generally make the best individual impression. the available results are significant for the field model of social social robotics, demonstrating claims that rational robots can thus learn the appropriate subjective attitude judgments defining the underlying fabric of human technological interaction.", "histories": [["v1", "Fri, 5 May 2017 05:47:15 GMT  (4744kb,D)", "http://arxiv.org/abs/1705.04282v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["amanda song", "linjie li", "chad atalla", "garrison cottrell"], "accepted": false, "id": "1705.04282"}, "pdf": {"name": "1705.04282.pdf", "metadata": {"source": "CRF", "title": "Learning to see people like people", "authors": ["Amanda Song", "Linjie Li", "Chad Atalla", "Garrison Cottrell"], "emails": ["feijuejuanling@gmail.com", "li2477@purdue.edu", "catalla@ucsd.edu", "gary@ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "Recent advances in deep convolutional networks have driven tremendous progress in a variety of challenging face processing tasks including face recognition[27], face\nalignment[39], and face detection[25]. However, humans not only read objective properties from a face, such as gender, expression, race, age and identity, but also form subjective impressions of the social aspects of a face[31, 32], such as facial attractiveness[28], friendliness, trustworthiness[29], sociability, dominance[18], and typicality. Despite the relative less attention received by the social perception of faces, social judgment is an important part of people\u2019s daily interactions, and it has significant impact on social outcomes, ranging from electoral success to sentencing decisions[19, 35]. Whereas current computer vision techniques exceed human abilities at recognizing a face and identifying the objective properties of a face [27, 25], awareness of human subjective judgments is important for social robotics theory-of-mind inferences. Accurate predictions of social aspects of faces can help robots better understand how humans interact with and perceive each other, and can make a robot aware of inherent human biases, as these judgments rarely correspond to reality (except, perhaps, attractiveness) [32].\nIn this paper, we teach a machine to infer social impressions, that match human judgments, from faces. We examine a list of 20 pairs of social features that are typically studied by social psychologists, and that are relevant to social interactions between people [33, 32, 19]. Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]). Although social perceptions of faces are subjective, there is often a consensus among human raters in how they perceive facial attractiveness, trustworthiness and dominance[6, 5]. This indicates that faces contain high-level visual cues for social interactions, and therefore it is possible to model this process with machine learning techniques. We take advantage of the state-of-the-art neural network models trained for ob-\n1\nar X\niv :1\n70 5.\n04 28\n2v 1\n[ cs\n.C V\n] 5\nject recognition and face recognition tasks and use their internal representations for social perception learning. In all 40 social dimensions, our model correlates with human averaged ratings better than the humans correlate with each other.\nThe contributions of the paper are summarized below:\n\u2022 To the best of our knowledge, this work is the first attempt to systematically examine the consistency of human social perceptions of faces, to explore the landscape of social feature semantic space, and to predict human judgments of 40 social attributes of faces;\n\u2022 We adapted 6 state of the art neural network algorithms trained for various visual tasks to make social judgment predictions on faces and achieve high correlations with human ratings in all 40 dimensions;\n\u2022 We evaluate the tuning properties of nodes in the best network and visualize the patterns that maximally ignite the perceptions for each specific social dimensions to facilitate a better understanding of the neural networks\u2019 behavior in face processing.\nThe rest of the paper is organized as follows. In Section 2, we review related work on social perception modeling. Section 3 and 4 summarize the methodology and the experimental framework. The experimental results and visualizations are presented in Section 5 and 6. Section 7 concludes the paper."}, {"heading": "2. Related work", "text": "The focus of our paper is to infer as much social judgment information as possible from a face image and to predict the subjective impression of faces by learning from human group data. We review related work in terms of the visual features they use, the dataset they choose, the evaluation metric they adopt, and the social attributes they examine.\nVisual features Since the early 1990s, psychologists have identified that high level visual features, such as the averageness of a face[14, 21] and the symmetry of the face [23] can explain why certain faces look more attractive.\nMachine learning researchers have developed various computer vision features and models to predict social perceptions of faces, especially facial attractiveness. Yael et al.[5] used geometric ratios and distances between facial features based on facial landmarks to build an attractiveness predictor (0.65 correlation with human raters, face database size=184). End-to-end neural networks were applied to predict facial attractiveness in 2010[8] (correlation 0.458, face database size=2056, young female faces only). Amit Kagian and his colleagues have used a combination of landmark-derived features along with global\nfeatures to obtain a high correlation with human group averages on facial attractiveness [10](0.82 Pearson correlation, face database size=91). Traditional computer vision features such as SIFT, HoG, Gabor filters have been blended to predict the relative ranking of facial attractiveness in [1](rank order correlation 0.63, face database size=200). Rothe et al. incorporate collaborative filtering techniques with visual features extracted from pretrained VGG networks[24] to achieve individual-level prediction of facial attractiveness[22](correlation 0.671 on female face queries, database size = 13,000). McCurrie et al. [17] build a model based on a pretrained VGG network to predict trustworthiness, dominance and IQ in faces (R2 values on trustworthiness, dominance and IQ are 0.5687, 0.4601, 0.3548 respectively, face database size=6000). Previous papers have achieved correlations with human performance between 0.458 to 0.82 in attractiveness predictions, depending on the dataset and method used. However, to date, there is no standard dataset that has been used to compare these approaches.\nDataset Earlier studies employ datasets with relatively small numbers of faces (a few hundred) and most face datasets use young Caucasian faces only, as pointed out by [15]. In contrast, the MIT dataset[2] we use contains 2,222 high quality color images that vary in ethnicity, gender, age and expression, with ratings on 40 attributes. This dataset is smaller than two of the ones mentioned above. The first is collected from howhot.io, an online dating website[22] and contains 13,000 face images, but that work focused on personalized prediction of facial attractiveness, rather than average ratings. There are only binary choices (like or dislike) indicating implicit preference of facial attractiveness. The second one is collected from testmybrain.com, contains 6,000 grayscale face images [17], and includes just three social features: dominance, IQ and trustworthiness.\nEvaluation metric Social perceptions of faces are collected from human participants in various ways. The most common way is to ask for a discrete rating, say from 1-9 [2], or 1-7 [5] from a number of raters, and then use the group average as the score for a face in the specified feature dimension (e.g. attractiveness). The consistency of ratings between humans is checked by repeatedly randomly splitting human participants into two subgroups and then computing the correlation between the two groups\u2019 mean ratings. To compare model predictions with human ratings, Pearson correlation[10, 3], Spearman rank correlation[2] and R-squared values[17] are used, depending on the nature of the data. Another method is to present a pair of faces or multiple faces and ask for a relative ranking in a particular dimension (e.g., attractiveness). Prediction accuracy is measured using Kendall\u2019s Tau and the Gamma Test [1]. In Rothe et al. [22], a person indicates his/ her preference by choosing to like or dislike another user\u2019s face photo. In this\npaper, since our goal is to predict a continuous score of human average ratings, and our raters do not all rate the same faces, we also use Pearson correlation with average human ratings on a per-face basis.\nSocial attributes Although social perceptions are a subjective judgment, and may not reflect a person\u2019s actual traits or mental states, humans tend to share consensus on their first impressions. Kiapour et al.[12] and Wang et al.[34] find that the social styles of people (bikers vs. hipsters, for example) can be identified and classified from image features. Dhar et al. (2011) show that the interestingness of an image can be quantified and predicted [4]. Bainbridge et al. (2013) prove that the memorability of a face image can be predicted and modified to make it more memorable [2]. Todorov et al. [30, 31, 32] used synthesized faces to study the perception of competence, dominance, extroversion, likeability, threat, trustworthiness and attractiveness in faces [29]. However their face photos lack realism compared to real-world photos and therefore cannot predict human\u2019s social perceptions of real faces in a more natural environment. McCurrie et al. [17] have worked toward removing this limitation by using real human faces to make predictions of trustworthiness and dominance ratings. 12 From the literature, we can observe two trends: (i) Besides McCurrie et al. [17] and Todorov. et al[29], most machine learning work on social perception of faces focuses on attractiveness prediction, leaving the prediction of other social perceptions largely unstudied. We aim to bridge this gap in our paper. (ii) As summarized by Laurentini et al[15] usually small datasets are used, with few variations on expression, gender, ethnicity and age. The dataset we chose overcomes the above limitations and has comprehensive coverage of a list of 40 social feature ratings.\nThe papers closest to ours are McCurrie et al. [17] and Todorov. et al[29]. Our paper differs from theirs in three major ways: (1) Todorov. et al\u2019s work is on synthesized faces whereas ours is on realistic photos; (2) McCurrie et al. [17] predict three social features, dominance, trustworthiness, and IQ, whereas we look at 40 social features including trustworthiness, aggressiveness (a term close to their dominance), and intelligence (close to their IQ term), so our feature set can be considered to be a superset of theirs; and (3) we compared various feature extraction methods, including traditional geometric features and 6 neural networks pretrained for various tasks (face identification, face localization, object recognition). We also examine the effect of fine-tuning the network compared with directly applying ridge regression on extracted features from higher layer of the networks."}, {"heading": "3. Method", "text": "In this section we first describe the dataset used in our experiments. Next, we introduce our method for predict-\ning social perceptions of faces. Finally, we explain how we visualize the features that contribute most to social trait predictions."}, {"heading": "3.1. Dataset", "text": "To predict how human evaluate social traits of a face at a glance, we use the dataset collected by Aude Oliva\u2019s group [2]. The dataset consists of 2,222 images of faces sampled from the 10k US Adult Face Database and annotated for 20 pairs of social attributes. Each attribute is rated on a scale of 1-9 (1 means not at all, 9 means extremely) and each image is rated by 15 subjects. We take the average rating across all raters as a collective estimation of the social feature for every face.\nThe 20 pairs of social traits are: (attractive, unattractive), (happy, unhappy), (friendly, unfriendly), (sociable, introverted), (kind, mean), (caring, cold), (calm, aggressive), (trustworthy, untrustworthy), (responsible, irresponsible), (confident, uncertain), (humble, egotistical),(emotionally stable, emotionally unstable), (normal, weird), (intelligent, unintelligent), (interesting, boring), (emotional, unemotional), (memorable, forgettable), (typical, atypical), (familiar, unfamiliar) and (common, uncommon).\nClearly, some of these traits will be highly correlated, and are predictable from the others. We compute the Spearman\u2019s rank correlation between every pair of social features and show their correlations in a heatmap (see the left figure in Figure 1). We put features together in the map based on similarity and positive/negativeness. From the figure, we can see that negative social features such as untrustworthy, aggressiveness, cold, introverted, irresponsible form a correlated block, while most positive features such as attractive, sociable, caring, friendly, happy, intelligent, interesting, confident are highly correlated with each other. Although we chose 20 pairs of opposite features, they are not completely complementary and redundant. Principal component analysis shows that it takes 24 principal components to cover 95% of the variance."}, {"heading": "3.2. Regression Model for Social Attributes", "text": "After we average human ratings on each face, each face receives a continuous score from 1 to 9 in all social dimensions. We model these social scores with a regression model. Our proposed algorithm is a ridge regression model on features extracted from deep convolutional neural networks (CNN). Since CNN features are usually highdimensional, we first perform Principal Component Analysis (PCA) on the extracted features of the training set to reduce the dimensionality. The PCA dimensionality is chosen by cross-validation on a validation set, separately for each trait. The PCA weights are saved and further used in fine-tuning our CNN-regression model."}, {"heading": "3.3. Feature Visualization", "text": "Attempting to understand what features are most helpful in social attribute prediction, we visualize the features extracted from the CNN. Two different methods were proposed in past feature visualization studies: dataset-centric methods [36, 38], and a network-centric method[36, 37].\nThe dataset-centric method we employed is to display image patches from the training set that cause high activation for the feature units and use the deconvolution method to highlight the portions of the image that are responsible for firing the important feature neurons [36, 38].\nThe network-centric approach is usually used in classification networks. This method produces an image that is based on adapting the input by maximizing the output category activation using gradient ascent, i.e., it is mainly a function of the network. [36, 37]. The key idea is to optimize the input image so that the target neuron can be highly activated. We apply this idea to the output (regression) neuron as well as the top nine neurons that influence that output individually."}, {"heading": "4. Experimental framework", "text": "In this section, we report our experimental framework using 6 CNN-based regression models with respect to two baselines, human correlation between groups of raters, and a baseline model using the geometric features."}, {"heading": "4.1. Baseline I: Human Correlations", "text": "Since these social attributes are all subjective perceptions rated by people, it is informative to examine to what extent people agree with each other upon those social judgments. We performed the following procedure 50 times for each attribute and then averaged the results:\n1. For each face, we randomly split the 15 raters evenly into two groups of 7 and 8. (Note: the raters for each face will, in general, be different sets).\n2. We calculate the two group\u2019s average ratings for each face, obtaining two vectors of length 2,222 (there are 2,222 faces in the dataset).\n3. We calculate the correlation between the two vectors.\nThe results are shown in the second column of Table 1. For every social attribute, the averaged correlation between human subgroups serves as an index of the rating consistency."}, {"heading": "4.2. Baseline II: Regression on geometric features", "text": "Past studies on facial attractiveness have found that attractiveness can be inferred from the geometric ratios and configurations of a face[5, 10]. We suggest that other social attributes can also be inferred from geometric features. We compute 29 geometric features based on definitions described in [16] and further extract a \u201dsmoothness\u201d feature and skin color features according to the procedure in [5, 10]. The \u201dsmoothness\u201d of a face was evaluated by applying a\nCanny edge detector to windows from the cheek/forehead area [5]. The more edges detected by edge detectors within the window, the less smooth the skin is. The regions we chose to compute smoothness and skin color are highlighted in the right subplot of Figure 2). The \u201dskin color\u201d feature is extracted from the same window as \u201dsmoothness\u201d, converted from RGB to HSV. Regressing on these handcrafted features alone are not enough to capture the richness of geometric details about a face, we therefore use a computer vision library (dlib, C++) to automatically label 68 face landmarks (see Figure 2) for each face and compute distances and slopes between any two landmarks. Combining 29 handcrafted geometric features, smoothness, color and the distance-slope features, we obtain 4592 features in total. Since the features are highly correlated, we apply PCA to reduce dimensionality. Again, the PCA dimensionality is chosen by cross-validating on the hold out set separately for each facial attribute. Then a ridge regression model is applied to predict social attribute ratings of a face. The hyperparameter of ridge regression is selected by leave-one-out validation within the training set."}, {"heading": "4.3. CNN-based Regression Model", "text": "We initially compared six neural network architectures: (1) VGG16, (2) VGG-Face from the Oxford Visual Geometry Groups VGG networks[24], (3) AlexNet (the publicly available CaffeNet reference model) [13] (4) Inception from Google [26] (5) a shallow face identification Siamese neural network that we trained from scratch: Face-SNN and (6) a state of the art VGG-derived network trained for the face landmark localization task: Face-LandmarkNN. These comparisons were performed with the Caffe deep learning framework [9].\nTo find the best CNN to predict social attribute ratings among all six networks, we first find the best-performing feature layers of each network (with the ridge regression model), and then we compare the results among the networks to select the best network. For each layer of each network, before the ridge regression, we performed PCA and picked the PCA dimension that gave the best results on the validation set."}, {"heading": "5. Results", "text": "Surprisingly, we found that features from conv5 2 layer of VGG16 trained for object classification slightly outperformed the AlexNet and Inception networks, while the three networks trained solely on faces, VGG-Face, FaceLandmarkNN and Face-SNN did not achieve performance as competitive as the other three for most of the social attributes. The best performing VGG16 layer was conv5 2.\nWe speculate that the reason for the relatively poorer performance of the face recognition networks is that they are optimized either to learn differences between faces which define identity or to learn the face landmark configurations, whereas for this task at hand, we are looking for commonalities behind certain social features which go beyond identity. The landmark network presumably should give results similar to the geometric features, but did not learn features corresponding to all of the features we used in that model. These speculations need to be checked, of course, for example by trying to predict all of our measured features, using the landmark network, but we did not do that here.\nWe tried fine-tuning the model as follows. We used backpropagation to fine tune the weights into the conv5 2 layer, the weights to the PCA layer from conv5 2 (initialized by the PCA weights), and the weights from the PCA layer to the output regression unit. However, this fine-tuning did not improve performance, so the results reported in Table 1 are without fine-tuning.\nWe evaluate the performance on 50 random train / validation / test splits of the data with a 64/16/20 percent split for training, cross-validation and testing, respectively. The prediction performance of our model is evaluated using Pearson\u2019s correlation with the human ratings on the test set. For each social attribute, we report its human consistency as described in Section 4.1.\nTable 1 summarizes the prediction performance of our model for all the social attributes compared to Baseline I and II. The table is organized in a descending order of human agreement on the putative positive attribute of the paired attributes. The three attributes where there is greater agreement among humans for the negative component of the pair are bolded.\nAmong all the social attributes, human subjects agree most with each other about \u201dhappy\u201d and disagree most about \u201dunfamiliar.\u201d For both regression models (Baseline II and our model), model performance grows as the consensus on a social trait increases and human correlations with each other are consistently lower than the models\u2019 correlations with the average human ratings. Normally, one might consider the human correlations to be an upper bound on performance, but here they are different kinds of correlations.\nSince the change in expression would produce a change in landmark locations, it is not surprising that landmark-\nbased geometric features (Baseline II) achieve comparable or slightly higher correlation as our model for predicting those social attributes that are highly related to expressions (such as \u201dhappy\u201d, \u201dunhappy\u201d, \u201dcold\u201d and \u201dfriendly\u201d etc.). While for other social attributes, our model slightly outperforms landmark-based geometric features by about 0.04 correlation on average and significantly outperforms human correlation by about 0.12 correlation on average. This implies that CNN features encode much more information than just landmark-based features. It is essential to visualize those features and understand what features extracted from CNN make our model powerful enough to predict social attributes.\nTo quantitatively compare the face social features perceived by humans and those predicted by our best performing model, we take the model predictions on all social features, and compute the Spearman correlation between every pair in the set (see the right figure in Figure 1). Not surprisingly, this has very similar patterns compared the heatmap generated from human ratings (see the right panel in Figure 1). Pearson Correlation between the upper triangle of the two similarity matrices (human and model prediction) is 0.9836. However, note that each predictor was trained independently."}, {"heading": "6. Feature Visualization", "text": "In this section, we visualize the features that are of importance to social perceptions. We choose facial attractiveness as an example. The same method can be applied to the other social features. We employ the two methods described in section 3.3 to visualize features learned by our model."}, {"heading": "6.1. Data-centric Visualization", "text": "To identify visual features that ignite attractiveness perception, we find the top 9 units of highest influence on attractiveness at conv5 2 as follows. First, we compute a product of three terms: (1) A unit\u2019s activation from conv5 2, (2) that unit\u2019s weight to the following fc pca layer, (3) the fc pca unit\u2019s weight to the output unit. We then sort all conv5 2 units\u2019 average products of the three terms and identify the top 9 neurons as the ones that contribute most to the output neuron for the corresponding social feature. Then we employ the method described in [36, 38] to find top-9 input images that cause high activations in each of the top-9 conv5 2 neurons. Also we further produce the deconvolutional images by projecting each activation separately down to pixel space.\nFigure 3 captures the features that are important to predict the attractiveness of a face. The feature importance descends from left to right and top to bottom. The important features identified by our model are related to eyes, hair\nwith bangs, high nose-bridge, high cheeks, dark eyebrows, strong commanding jawline, chin and red lips. Note that among the 9 cropped input image patches, not all the faces are perceived as attractive or rated as attractive. An attractive face needs to activate more than one feature in order to be considered attractive. This observation agrees with our intuition that attractiveness is a kind of holistic judgment, requiring a combination of multiple features.\nIt also seems to be the case that several of the features\ninclude relationships between the parts. For example, while the first feature in the upper left of the figure emphasizes the eye, it also includes the nose. This is also true of the upper right feature. Smiling is also important in order to be perceived attractive, as emphasized by the feature in the lower left of the figure."}, {"heading": "6.2. Network-centric Visualization", "text": "In section 6.1, we have identified the top-9 units and their feature maps from the con5 2 layer that maximally activates the attractiveness neuron. Here, we use the gradient-ascent method to optimize the input image that would highly activate a specific neuron of the network. This method is also performed on the pretrained-VGG16 regression model, which is trained to predict attractiveness.\nFigure 4a shows the optimized image corresponding to the output neuron from a random input image. Optimizing the input image for the output neuron of a regression model does not result in a particularly interpretable figure, although it does appear to emphasize the eyes. Our second approach is to optimize the input image with respect to the top-9 contributing neurons from conv5 2 layer that have been identified in section 6.1. Figure 4b presents 9 optimized images with respect to the corresponding top-9 feature maps of the top-9 neurons from conv5 2 layer. Since we use a pretrained-VGG16 network for visualization, it is not surprising that the corresponding top-9 feature maps at conv5 2 layer are not particularly encoding facial patterns.\nWe also present the optimized image initialize with a face image, along with the original face image for comparison in Figure 5. The optimized image tends to highlight the eyes, nose, cheeks and the contour of the face, which is consistent with the features identified by data-centric method."}, {"heading": "7. Conclusion", "text": "We have shown that a deep network can be used to predict human social judgments with high correlation with the average human ratings. As far as we know, this is the widest exploration of social judgment predictions, showing human-like perceptions on 40 social dimensions. Unsurprisingly, given previous work recognizing facial expressions, where happiness is the easiest to recognize, our highest correlation is on the happy feature. However, previous work in this area tended to classify a face as happy or not, rather than the degree of rated happiness.\nWe find that for attributes that correspond to elements of the face that require muscle movement, or a lack of it (such as happy, unhappy, cold, aggressive, unemotional) a simple regression model based on the placement of facial landmarks works well. For ones that don\u2019t appear to suggest emotions, such as friendly, note that friendly and happy are highly correlated (see Figure 1, and the red block indexed\nby happy and friendly). Similarly, aggressive and mean are highly correlated, which presumably requires not smiling.\nPerhaps of more significance are the correlations with judgments of traits, such as trustworthiness, responsibleness, confidence, and intelligence, which would correspond to more static features of the face. In this area, the deep network, which responds to facial textures as well as shape, has superior performance. While these judgments do not correspond to a notion of \u201dground truth,\u201d they are things for which humans have a fair amount of agreement, suggesting that there is a signal to be recognized.\nOf further note is that we have shown, yet again, that a machine can recognize attractiveness, presumably without any hormonal influences. For this dataset, our deep network correlates with human ratings at 0.75. This provides a benchmark for this dataset.\nFinally, it is of note that we can see that some of the traits considered to be \u201dopposite\u201d in this list are not simply the reverse of one another. For example, there is a large difference in human agreements on \u201dsociable\u201d (0.74) versus \u201dintroverted\u201d (0.50), suggesting they are not opposites.\nThese results are significant for the field of social robotics. While a robot should not judge a human based completely on their appearance, it can be useful knowledge that humans might judge a person to be trustworthy, while the robot can be more objective. Similarly, a robot need not treat an attractive and unattractive person differently, but this knowledge could affect how the robot interacts with the unattractive person, knowing in advance that this person may have had many negative experiences interacting with people.\nIn this paper, we train each social feature separately, due to their varied consistency and reliability. In the future, it is worth trying to train one single convnet to learn multiple tasks simultaneously and evaluate whether shared representation may further improve the model performance.\nIn summary, we have provided the first machine learning system to learn subjective human judgments of a wide spectrum of traits. We found that the more humans agree on such subjective judgments, the more the system could pick up on the features driving those judgments. It will be of interest to investigate further what those features are, beyond the attractiveness features we displayed here.\nOne step further from predicting the value in a certain social feature is to move faces on the social manifold and to increase a face\u2019s elicited social perceptions in positive ways (e.g. to make a face look more sociable/ trustworthy/ attractive). Although the images generated by our current visualization method are still far away from being photorealistic, it may be a fruitful area in the future to develop generative models that can achieve this goal."}], "references": [{"title": "Relative ranking of facial attractiveness", "author": ["H. Altwaijry", "S. Belongie"], "venue": "In Applications of Computer Vision (WACV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The intrinsic memorability of face photographs", "author": ["W.A. Bainbridge", "P. Isola", "A. Oliva"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Female facial aesthetics based on soft biometrics and photo-quality", "author": ["A. Dantcheva", "J. Dugelay"], "venue": "In Proceedings of ICME,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "High level describable attributes for predicting aesthetics and interestingness", "author": ["S. Dhar", "V. Ordonez", "T.L. Berg"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Facial attractiveness: Beauty and the machine", "author": ["Y. Eisenthal", "G. Dror", "E. Ruppin"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "The robustness of learning about the trustworthiness of other people", "author": ["V. Falvello", "M. Vinson", "C. Ferrari", "A. Todorov"], "venue": "Social Cognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Human (homo sapiens) facial attractiveness and sexual selection: the role of symmetry and averageness", "author": ["K. Grammer", "R. Thornhill"], "venue": "Journal of comparative psychology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Predicting facial beauty without landmarks", "author": ["D. Gray", "K. Yu", "W. Xu", "Y. Gong"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A machine learning predictor of facial attractiveness revealing human-like psychophysical biases", "author": ["A. Kagian", "G. Dror", "T. Leyvand", "I. Meilijson", "D. Cohen-Or", "E. Ruppin"], "venue": "Vision research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Modifying the memorability of face photographs", "author": ["A. Khosla", "W.A. Bainbridge", "A. Torralba", "A. Oliva"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Hipster wars: Discovering elements of fashion styles", "author": ["M.H. Kiapour", "K. Yamaguchi", "A.C. Berg", "T.L. Berg"], "venue": "In European conference on computer vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Attractive faces are only average", "author": ["J.H. Langlois", "L.A. Roggman"], "venue": "Psychological science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1990}, {"title": "Computer analysis of face beauty: A survey", "author": ["A. Laurentini", "A. Bottino"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "The chicago face database: A free stimulus set of faces and norming data", "author": ["D.S. Ma", "J. Correll", "B. Wittenbrink"], "venue": "Behavior research methods,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Predicting first impressions with deep learning", "author": ["M. McCurrie", "F. Beletti", "L. Parzianello", "A. Westendorp", "S. Anthony", "W. Scheirer"], "venue": "arXiv preprint arXiv:1610.08119,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "The many faces of a neutral face: Head tilt and perception of dominance and emotion", "author": ["A. Mignault", "A. Chaudhuri"], "venue": "Journal of nonverbal behavior,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "The functional basis of  (a) Original input image  (b) Optimized image Figure 5: Visualization of the optimized image with a input face image: Figure 5a is the original face image before optimization. Figure 5b is produced by performing optimization with respect to the output unit. face evaluation", "author": ["N.N. Oosterhof", "A. Todorov"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Familiarity breeds attraction: Effects of exposure on the attractiveness of typical and distinctive faces. PERCEPTION-LONDON", "author": ["M. Peskin", "F.N. Newell"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Fitting the mind to the world face adaptation and attractiveness aftereffects", "author": ["G. Rhodes", "L. Jeffery", "T.L. Watson", "C.W. Clifford", "K. Nakayama"], "venue": "Psychological science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Some like it hotvisual guidance for preference prediction", "author": ["R. Rothe", "R. Timofte", "L. Van Gool"], "venue": "arXiv preprint arXiv:1510.07867,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Facial attractiveness, symmetry and cues of good genes", "author": ["J.E. Scheib", "S.W. Gangestad", "R. Thornhill"], "venue": "Proceedings of the Royal Society of London. Series B: Biological Sciences,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "End-to-end people detection in crowded scenes", "author": ["R. Stewart", "M. Andriluka", "A.Y. Ng"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Facial attractiveness", "author": ["R. Thornhill", "S.W. Gangestad"], "venue": "Trends in cognitive sciences,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Evaluating face trustworthiness: a model based approach", "author": ["A. Todorov", "S.G. Baron", "N.N. Oosterhof"], "venue": "Social cognitive and affective neuroscience,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Validation of data-driven computational models of social perception", "author": ["A. Todorov", "R. Dotsch", "J.M. Porter", "N.N. Oosterhof", "V.B. Falvello"], "venue": "of faces. Emotion,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Social judgments from faces", "author": ["A. Todorov", "P. Mende-Siedlecki", "R. Dotsch"], "venue": "Current opinion in neurobiology,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Social attributions from faces: Determinants, consequences, accuracy, and functional significance", "author": ["A. Todorov", "C.Y. Olivola", "R. Dotsch", "P. Mende- Siedlecki"], "venue": "Psychology, 66(1):519,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Understanding evaluation of faces on social dimensions", "author": ["A. Todorov", "C.P. Said", "A.D. Engell", "N.N. Oosterhof"], "venue": "Trends in cognitive sciences,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Bikers are like tobacco shops, formal dressers are like suits: Recognizing urban tribes with caffe", "author": ["Y. Wang", "G.W. Cottrell"], "venue": "In 2015 IEEE Winter Conference on Applications of Computer Vision,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "First impressions making up your mind after a 100-ms exposure to a face", "author": ["J. Willis", "A. Todorov"], "venue": "Psychological science,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Understanding neural networks through deep visualization", "author": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "A deep neural net trained for person categorization develops both detailed local features and broad contextual specificities", "author": ["S. Yu", "K. Zipser"], "venue": "Journal of Vision,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In European Conference on Computer Vision, pages 818\u2013833", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Face alignment across large poses: A 3d solution", "author": ["X. Zhu", "Z. Lei", "X. Liu", "H. Shi", "S.Z. Li"], "venue": "arXiv preprint arXiv:1511.07212,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Recent advances in deep convolutional networks have driven tremendous progress in a variety of challenging face processing tasks including face recognition[27], face alignment[39], and face detection[25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 38, "context": "Recent advances in deep convolutional networks have driven tremendous progress in a variety of challenging face processing tasks including face recognition[27], face alignment[39], and face detection[25].", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "Recent advances in deep convolutional networks have driven tremendous progress in a variety of challenging face processing tasks including face recognition[27], face alignment[39], and face detection[25].", "startOffset": 199, "endOffset": 203}, {"referenceID": 30, "context": "However, humans not only read objective properties from a face, such as gender, expression, race, age and identity, but also form subjective impressions of the social aspects of a face[31, 32], such as facial attractiveness[28], friendliness, trustworthiness[29], sociability, dominance[18], and typicality.", "startOffset": 184, "endOffset": 192}, {"referenceID": 31, "context": "However, humans not only read objective properties from a face, such as gender, expression, race, age and identity, but also form subjective impressions of the social aspects of a face[31, 32], such as facial attractiveness[28], friendliness, trustworthiness[29], sociability, dominance[18], and typicality.", "startOffset": 184, "endOffset": 192}, {"referenceID": 27, "context": "However, humans not only read objective properties from a face, such as gender, expression, race, age and identity, but also form subjective impressions of the social aspects of a face[31, 32], such as facial attractiveness[28], friendliness, trustworthiness[29], sociability, dominance[18], and typicality.", "startOffset": 223, "endOffset": 227}, {"referenceID": 28, "context": "However, humans not only read objective properties from a face, such as gender, expression, race, age and identity, but also form subjective impressions of the social aspects of a face[31, 32], such as facial attractiveness[28], friendliness, trustworthiness[29], sociability, dominance[18], and typicality.", "startOffset": 258, "endOffset": 262}, {"referenceID": 17, "context": "However, humans not only read objective properties from a face, such as gender, expression, race, age and identity, but also form subjective impressions of the social aspects of a face[31, 32], such as facial attractiveness[28], friendliness, trustworthiness[29], sociability, dominance[18], and typicality.", "startOffset": 286, "endOffset": 290}, {"referenceID": 18, "context": "Despite the relative less attention received by the social perception of faces, social judgment is an important part of people\u2019s daily interactions, and it has significant impact on social outcomes, ranging from electoral success to sentencing decisions[19, 35].", "startOffset": 253, "endOffset": 261}, {"referenceID": 34, "context": "Despite the relative less attention received by the social perception of faces, social judgment is an important part of people\u2019s daily interactions, and it has significant impact on social outcomes, ranging from electoral success to sentencing decisions[19, 35].", "startOffset": 253, "endOffset": 261}, {"referenceID": 26, "context": "Whereas current computer vision techniques exceed human abilities at recognizing a face and identifying the objective properties of a face [27, 25], awareness of human subjective judgments is important for social robotics theory-of-mind inferences.", "startOffset": 139, "endOffset": 147}, {"referenceID": 24, "context": "Whereas current computer vision techniques exceed human abilities at recognizing a face and identifying the objective properties of a face [27, 25], awareness of human subjective judgments is important for social robotics theory-of-mind inferences.", "startOffset": 139, "endOffset": 147}, {"referenceID": 31, "context": "Accurate predictions of social aspects of faces can help robots better understand how humans interact with and perceive each other, and can make a robot aware of inherent human biases, as these judgments rarely correspond to reality (except, perhaps, attractiveness) [32].", "startOffset": 267, "endOffset": 271}, {"referenceID": 32, "context": "We examine a list of 20 pairs of social features that are typically studied by social psychologists, and that are relevant to social interactions between people [33, 32, 19].", "startOffset": 161, "endOffset": 173}, {"referenceID": 31, "context": "We examine a list of 20 pairs of social features that are typically studied by social psychologists, and that are relevant to social interactions between people [33, 32, 19].", "startOffset": 161, "endOffset": 173}, {"referenceID": 18, "context": "We examine a list of 20 pairs of social features that are typically studied by social psychologists, and that are relevant to social interactions between people [33, 32, 19].", "startOffset": 161, "endOffset": 173}, {"referenceID": 6, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 28, "endOffset": 45}, {"referenceID": 27, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 28, "endOffset": 45}, {"referenceID": 4, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 28, "endOffset": 45}, {"referenceID": 9, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 28, "endOffset": 45}, {"referenceID": 7, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 28, "endOffset": 45}, {"referenceID": 5, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 63, "endOffset": 70}, {"referenceID": 28, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 63, "endOffset": 70}, {"referenceID": 17, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 153, "endOffset": 157}, {"referenceID": 1, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "Examples are attractiveness [7, 28, 5, 10, 8], trustworthiness [6, 29], sociability, aggressiveness [18], friendliness, kindness, happiness, familiarity [20], and memorability [2, 11]).", "startOffset": 176, "endOffset": 183}, {"referenceID": 5, "context": "Although social perceptions of faces are subjective, there is often a consensus among human raters in how they perceive facial attractiveness, trustworthiness and dominance[6, 5].", "startOffset": 172, "endOffset": 178}, {"referenceID": 4, "context": "Although social perceptions of faces are subjective, there is often a consensus among human raters in how they perceive facial attractiveness, trustworthiness and dominance[6, 5].", "startOffset": 172, "endOffset": 178}, {"referenceID": 13, "context": "Visual features Since the early 1990s, psychologists have identified that high level visual features, such as the averageness of a face[14, 21] and the symmetry of the face [23] can explain why certain faces look more attractive.", "startOffset": 135, "endOffset": 143}, {"referenceID": 20, "context": "Visual features Since the early 1990s, psychologists have identified that high level visual features, such as the averageness of a face[14, 21] and the symmetry of the face [23] can explain why certain faces look more attractive.", "startOffset": 135, "endOffset": 143}, {"referenceID": 22, "context": "Visual features Since the early 1990s, psychologists have identified that high level visual features, such as the averageness of a face[14, 21] and the symmetry of the face [23] can explain why certain faces look more attractive.", "startOffset": 173, "endOffset": 177}, {"referenceID": 4, "context": "[5] used geometric ratios and distances between facial features based on facial landmarks to build an attractiveness predictor (0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "End-to-end neural networks were applied to predict facial attractiveness in 2010[8] (correlation 0.", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "Amit Kagian and his colleagues have used a combination of landmark-derived features along with global features to obtain a high correlation with human group averages on facial attractiveness [10](0.", "startOffset": 191, "endOffset": 195}, {"referenceID": 0, "context": "Traditional computer vision features such as SIFT, HoG, Gabor filters have been blended to predict the relative ranking of facial attractiveness in [1](rank order correlation 0.", "startOffset": 148, "endOffset": 151}, {"referenceID": 23, "context": "incorporate collaborative filtering techniques with visual features extracted from pretrained VGG networks[24] to achieve individual-level prediction of facial attractiveness[22](correlation 0.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "incorporate collaborative filtering techniques with visual features extracted from pretrained VGG networks[24] to achieve individual-level prediction of facial attractiveness[22](correlation 0.", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "[17] build a model based on a pretrained VGG network to predict trustworthiness, dominance and IQ in faces (R values on trustworthiness, dominance and IQ are 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Dataset Earlier studies employ datasets with relatively small numbers of faces (a few hundred) and most face datasets use young Caucasian faces only, as pointed out by [15].", "startOffset": 168, "endOffset": 172}, {"referenceID": 1, "context": "In contrast, the MIT dataset[2] we use contains 2,222 high quality color images that vary in ethnicity, gender, age and expression, with ratings on 40 attributes.", "startOffset": 28, "endOffset": 31}, {"referenceID": 21, "context": "io, an online dating website[22] and contains 13,000 face images, but that work focused on personalized prediction of facial attractiveness, rather than average ratings.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "com, contains 6,000 grayscale face images [17], and includes just three social features: dominance, IQ and trustworthiness.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "The most common way is to ask for a discrete rating, say from 1-9 [2], or 1-7 [5] from a number of raters, and then use the group average as the score for a face in the specified feature dimension (e.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "The most common way is to ask for a discrete rating, say from 1-9 [2], or 1-7 [5] from a number of raters, and then use the group average as the score for a face in the specified feature dimension (e.", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "To compare model predictions with human ratings, Pearson correlation[10, 3], Spearman rank correlation[2] and R-squared values[17] are used, depending on the nature of the data.", "startOffset": 68, "endOffset": 75}, {"referenceID": 2, "context": "To compare model predictions with human ratings, Pearson correlation[10, 3], Spearman rank correlation[2] and R-squared values[17] are used, depending on the nature of the data.", "startOffset": 68, "endOffset": 75}, {"referenceID": 1, "context": "To compare model predictions with human ratings, Pearson correlation[10, 3], Spearman rank correlation[2] and R-squared values[17] are used, depending on the nature of the data.", "startOffset": 102, "endOffset": 105}, {"referenceID": 16, "context": "To compare model predictions with human ratings, Pearson correlation[10, 3], Spearman rank correlation[2] and R-squared values[17] are used, depending on the nature of the data.", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "Prediction accuracy is measured using Kendall\u2019s Tau and the Gamma Test [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 21, "context": "[22], a person indicates his/ her preference by choosing to like or dislike another user\u2019s face photo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] and Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] find that the social styles of people (bikers vs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "(2011) show that the interestingness of an image can be quantified and predicted [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "(2013) prove that the memorability of a face image can be predicted and modified to make it more memorable [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 29, "context": "[30, 31, 32] used synthesized faces to study the perception of competence, dominance, extroversion, likeability, threat, trustworthiness and attractiveness in faces [29].", "startOffset": 0, "endOffset": 12}, {"referenceID": 30, "context": "[30, 31, 32] used synthesized faces to study the perception of competence, dominance, extroversion, likeability, threat, trustworthiness and attractiveness in faces [29].", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "[30, 31, 32] used synthesized faces to study the perception of competence, dominance, extroversion, likeability, threat, trustworthiness and attractiveness in faces [29].", "startOffset": 0, "endOffset": 12}, {"referenceID": 28, "context": "[30, 31, 32] used synthesized faces to study the perception of competence, dominance, extroversion, likeability, threat, trustworthiness and attractiveness in faces [29].", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "[17] have worked toward removing this limitation by using real human faces to make predictions of trustworthiness and dominance ratings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] and Todorov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "et al[29], most machine learning work on social perception of faces focuses on attractiveness prediction, leaving the prediction of other social perceptions largely unstudied.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "(ii) As summarized by Laurentini et al[15] usually small datasets are used, with few variations on expression, gender, ethnicity and age.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "[17] and Todorov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "et al[29].", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "[17] predict three social features, dominance, trustworthiness, and IQ, whereas we look at 40 social features including trustworthiness, aggressiveness (a term close to their dominance), and intelligence (close to their IQ term), so our feature set can be considered to be a superset of theirs; and (3) we compared various feature extraction methods, including traditional geometric features and 6 neural networks pretrained for various tasks (face identification, face localization, object recognition).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "To predict how human evaluate social traits of a face at a glance, we use the dataset collected by Aude Oliva\u2019s group [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 35, "context": "Two different methods were proposed in past feature visualization studies: dataset-centric methods [36, 38], and a network-centric method[36, 37].", "startOffset": 99, "endOffset": 107}, {"referenceID": 37, "context": "Two different methods were proposed in past feature visualization studies: dataset-centric methods [36, 38], and a network-centric method[36, 37].", "startOffset": 99, "endOffset": 107}, {"referenceID": 35, "context": "Two different methods were proposed in past feature visualization studies: dataset-centric methods [36, 38], and a network-centric method[36, 37].", "startOffset": 137, "endOffset": 145}, {"referenceID": 36, "context": "Two different methods were proposed in past feature visualization studies: dataset-centric methods [36, 38], and a network-centric method[36, 37].", "startOffset": 137, "endOffset": 145}, {"referenceID": 35, "context": "The dataset-centric method we employed is to display image patches from the training set that cause high activation for the feature units and use the deconvolution method to highlight the portions of the image that are responsible for firing the important feature neurons [36, 38].", "startOffset": 272, "endOffset": 280}, {"referenceID": 37, "context": "The dataset-centric method we employed is to display image patches from the training set that cause high activation for the feature units and use the deconvolution method to highlight the portions of the image that are responsible for firing the important feature neurons [36, 38].", "startOffset": 272, "endOffset": 280}, {"referenceID": 35, "context": "[36, 37].", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[36, 37].", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Past studies on facial attractiveness have found that attractiveness can be inferred from the geometric ratios and configurations of a face[5, 10].", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "Past studies on facial attractiveness have found that attractiveness can be inferred from the geometric ratios and configurations of a face[5, 10].", "startOffset": 139, "endOffset": 146}, {"referenceID": 15, "context": "We compute 29 geometric features based on definitions described in [16] and further extract a \u201dsmoothness\u201d feature and skin color features according to the procedure in [5, 10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "We compute 29 geometric features based on definitions described in [16] and further extract a \u201dsmoothness\u201d feature and skin color features according to the procedure in [5, 10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 9, "context": "We compute 29 geometric features based on definitions described in [16] and further extract a \u201dsmoothness\u201d feature and skin color features according to the procedure in [5, 10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 4, "context": "Canny edge detector to windows from the cheek/forehead area [5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 23, "context": "We initially compared six neural network architectures: (1) VGG16, (2) VGG-Face from the Oxford Visual Geometry Groups VGG networks[24], (3) AlexNet (the publicly available CaffeNet reference model) [13] (4) Inception from Google [26] (5) a shallow face identification Siamese neural network that we trained from scratch: Face-SNN and (6) a state of the art VGG-derived network trained for the face landmark localization task: Face-LandmarkNN.", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "We initially compared six neural network architectures: (1) VGG16, (2) VGG-Face from the Oxford Visual Geometry Groups VGG networks[24], (3) AlexNet (the publicly available CaffeNet reference model) [13] (4) Inception from Google [26] (5) a shallow face identification Siamese neural network that we trained from scratch: Face-SNN and (6) a state of the art VGG-derived network trained for the face landmark localization task: Face-LandmarkNN.", "startOffset": 199, "endOffset": 203}, {"referenceID": 25, "context": "We initially compared six neural network architectures: (1) VGG16, (2) VGG-Face from the Oxford Visual Geometry Groups VGG networks[24], (3) AlexNet (the publicly available CaffeNet reference model) [13] (4) Inception from Google [26] (5) a shallow face identification Siamese neural network that we trained from scratch: Face-SNN and (6) a state of the art VGG-derived network trained for the face landmark localization task: Face-LandmarkNN.", "startOffset": 230, "endOffset": 234}, {"referenceID": 8, "context": "These comparisons were performed with the Caffe deep learning framework [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 35, "context": "Then we employ the method described in [36, 38] to find top-9 input images that cause high activations in each of the top-9 conv5 2 neurons.", "startOffset": 39, "endOffset": 47}, {"referenceID": 37, "context": "Then we employ the method described in [36, 38] to find top-9 input images that cause high activations in each of the top-9 conv5 2 neurons.", "startOffset": 39, "endOffset": 47}, {"referenceID": 37, "context": "For conv5 2 layer, we show the top 9 activations of the top 9 neurons that maximally activate the attractiveness neuron across the training data, projected down to pixel space using the deconvolutional network approach [38] and their corresponding cropped image patches.", "startOffset": 219, "endOffset": 223}], "year": 2017, "abstractText": "The human perceptual system can make complex inferences on faces, ranging from the objective evaluations regarding gender, ethnicity, expression, age, identity, etc. to subjective judgments on facial attractiveness, trustworthiness, sociability, friendliness, etc. Whereas the objective aspects have been extensively studied, less attention has been paid to modeling the subjective perception of faces. Here, we adapt 6 state-of-the-art neural networks pretrained on various image tasks (object classification, face identification, face localization) to predict human ratings on 40 social judgments of faces in the 10k US Adult Face Database. Supervised ridge regression on PCA of the conv5 2 layer in VGG-16 network gives best predictions on the average human ratings. Human group agreement was evaluated by repeatedly randomly splitting the raters into two halves for each face, and calculating the Pearson correlation between the two sets of averaged ratings. Due to this methodology, the models correlations with the average human ratings can exceed this score. We find that 1) model performance grows as the consensus on a face trait increases, and 2) model correlations are always higher than human correlations with each other. These results illustrate the learnability of the subjective perception of faces, especially when there is consensus, and the striking versatility and transferability of representations learned for object recognition. This work has strong applications to social robotics, allowing robots to infer human judgments of each other.", "creator": "LaTeX with hyperref package"}}}