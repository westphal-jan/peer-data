{"id": "1705.05229", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2017", "title": "Modeling of the Latent Embedding of Music using Deep Neural Network", "abstract": "while ignoring both the data volume and heterogeneity of the digital music content components is also huge, it has become increasingly enabling important and convenient behavior to ultimately build a recommendation or search system to facilitate successful surfacing these content to the consumer user or larger consumer aggregator community. besides most portions of the creative recommendation web models fall into base two primary coding species, collaborative ensemble filtering value based and content based content approaches. robust variants of agile instantiations varieties of noisy collaborative filtering approach suffer from the common issues of some so called \" cold fat start \" searches and \" paper long tail \" selection problems forming where acknowledging there inevitably is not much specific user interaction sufficient data to reveal better user opinions - or affinities on rendering the content and partly also alleviate the distortion towards implementing the previously popular content. content - based hierarchical approaches are sometimes limited by facilitating the richness of representing the lowest available content from data resources resulting in enabling a notably heavily biased and coarse recommendation result. in recent years, the deep learning neural network has especially enjoyed been a great great success playing in administering large - network scale image and video recognitions. in this paper, we confidently propose technique and experiment using discrete deep listening convolutional filtering neural network tools to imitate how human brain processes hierarchical structures in enhancing the overall auditory signals, independently such those as music, speech, etc., at various timescales. on this approach can be used consistently to anal discover largely the discrete latent factor models of the underlying music based upon 2d acoustic vibration hyper - images that are extracted from the raw audio stream waves of music. atop these automated latent functional embeddings findings can be used either successively as features to feed to interpret subsequent content models, such things as collaborative musical filtering, or to build similarity response metrics between streaming songs, or programs to classify selective music filters based on fitting the labels applicable for training such as genre, mood, sentiment, etc.", "histories": [["v1", "Fri, 12 May 2017 01:08:31 GMT  (3815kb,D)", "http://arxiv.org/abs/1705.05229v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhou xing", "eddy baik", "yan jiao", "nilesh kulkarni", "chris li", "gautam muralidhar", "marzieh parandehgheibi", "erik reed", "abhishek singhal", "fei xiao", "chris pouliot"], "accepted": false, "id": "1705.05229"}, "pdf": {"name": "1705.05229.pdf", "metadata": {"source": "CRF", "title": "Modeling of the Latent Embedding of Music using Deep Neural Network", "authors": ["Zhou Xing", "Eddy Baik", "Yan Jiao", "Nilesh Kulkarni", "Chris Li", "Gautam Muralidhar", "Marzieh Parandehgheibi", "Erik Reed", "Abhishek Singhal", "Fei Xiao", "Chris Pouliot"], "emails": ["chris.pouliot@nio.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "While both the data volume and heterogeneity in the digital music market is huge 1, it has become increasingly important and convenient to build automated systems of recommendation and search in order to facilitate the users to locate as well as discover the relevant content. For recommendation systems, most of the models are formulated following the concept of collaborative filtering and content based methodologies. While collaborative filtering model requires\n1Spotify has hosted more than 30 million songs coupling with the 20,000 new songs being released every day, Apple music also streams 40 million songs or more presently.\nsubstantial user feedback signals to learn or capture the usercontent latent embedding, it is difficult to precisely model the latent space in a so called \u201ccold start\u201d scenario when there is little signal collected from the users. Conventional content-based approaches try to solve this problem using explicit features associated with music, but limitation resides in the diversity and the level of fineness of recommendations.\nMusic, as well as speech, is a complex auditory signal that contains structures at multiple timescales. Neuroscience studies [1] has shown that the human brain integrates these complex streams of audio information through various levels of voxel, auditory cortex (A1+), superior temporal gyrus (STG), and inferior frontal gyrus (IFG), etc., as shown in Fig. 1. The temporal structure of the auditory signals can be well recognized by the neural network in human brain.\nIn recent years, deep neural network modeling has been shown to perform successful image classification and recognition tasks. The methodology of the localized convolution of images can also be used to recognize patterns, intensities in the music spectrogram 2. In this paper, we propose and experiment using deep convolutional neural network (CNN) to learn the latent models of the music based upon the acoustic data. The rationale behind this approach is that most of the\n2There are various types of spectrogram for sound or music, we combine them together to generate a so called audio \u201chyper-image\u201d\nar X\niv :1\n70 5.\n05 22\n9v 1\n[ cs\n.S D\n] 1\n2 M\nay 2\nexplicit features associated with music, such as genre, type, rhythm, pitch, loudness, timbre, as well as latent features, for example mood, sentiment of the song, can all be embedded by various filters in the neural network. These latent embeddings of songs can be used either as features to feed to subsequent recommendation models, such as collaborative filtering, to alleviate the issues mentioned previously, or to build similarity metrics between songs, or simply to classify music into the targeted training classes such as genre, mood, etc [2]. The deep learning model is experimented under CUDA computation framework using NVIDIA GTX1080 GPU infrastructures."}, {"heading": "2. DESCRIPTION OF THE DATASET", "text": "We use several Music Information Retrieval (MIR) benchmark dataset [3; 4; 5] to train and test our deep learning model. These datasets contain raw audio files of songs that are labeled either by genre or emotion. The music genre dataset contains 1886 songs all being encoded in mp3 format. The codec information such as sampling frequency and bitrate are 44,100 Hz and 128 kb, respectively."}, {"heading": "3. EXTRACTION OF THE ACOUSTIC FEATURES", "text": "We use a tool, FFmpeg [6], to decode audio files such as mp3, wav files, thus generate time series data of the sound wave. An example of the extracted sound wave signal for song \u201cMy Humps\u201d by \u201cBlack Eye Peas\u201d is shown in Fig. 2, only 5 seconds of snippet is shown here for illustration purpose.\nSubsequent feature engineering involves generating time frequency representations of the sound wave. Fourier transform and constant Q-transform (CQT) can be used to generate power spectrums of the sound as shown in the top two rows of Fig. 3, in both linear and logarithmic scale, frequency and chromatic scale (music notes). Chroma-gram of the 12 pitch classes can also be constructed using short-time Fourier transforms in combination with binning strategies [7] , as shown in the third row on the left. The extraction of the local tempo and beat information is implemented using a mid-level representation of cyclic tempograms, where tempi differing by a power of two are identified [8]. This cyclic tempogram is shown in the third row on the right.\nPerceptual scale of pitches such as melody spectrogram (MELspectrogram), mel-frequency cepstrum consisting of various Mel-frequency cepstral coefficients (MFCC) can be generated by taking discrete cosine transform of the mel logarithmic powers [9] as shown in the fourth row. Especially the\nMFCC has been used as the dominant features in the speech recognition for some time [10].\nWe also investigate other acoustic signals such as spectral contrast [11] and music harmonics such as tonal centroid features (tonnetz) [12]. In the end, we choose to combine all these the acoustic signals of chromogram, tempogram, mel-spectogram, MFCC, spectral contrast and tonnetz into a normalized \u201chyper-image\u201d, as shown in Fig. 4, to feed into the deep neural network for training."}, {"heading": "4. ARCHITECTURE OF THE CONVOLUTIONAL NEURAL NETWORK", "text": "The deep neural network has been shown to perform a successful job on image classification and recognitions [13], the localized convolution to the 3-dimensional (3D) neurons has been able to capture latent features as well as other explicit features on the image such as patterns, colors, brightness, etc. Every entry in the 3D output volume, axon, can also be interpreted as an output of a neuron that picks up stimulus\nfrom only a small region in the input, and that information is shared with all neurons spatially in the same layer. All these dendrite signal, through synapse, are integrated together to the other axon.\nWe have been experimenting with various architectures of convolutional neural network in terms of the input layer dimension, convolutional layer, pooling layer, receptive filed of the neuron, filter depth, strides, as well as other implementation choices such as activation functions, local response normalization, etc. An example of the CNN architecture is illustrated in Fig. 5. We use stochastic gradient decent (SGD) to optimize for the weights in our neural network.\nWe use rectified linear units (ReLUs) to activate neurons where non-saturating nonlinearity is supposed to run faster than the saturating ones. Local response normalization (LRN) is then performed integrating over the kernel map which fulfills a form of lateral constraint for big activities between neuron outputs computed using different kernel. Max pooling is used to synchronize the outputs of neighboring groups of neurons in the same kernel map. A linear mapping is used to generate the last two fully connected layers. A softmax cross-entropy function is used to build the overall loss function."}, {"heading": "5. TRAINING AND CROSS VALIDATION", "text": "OF THE MODEL\nA 10-fold cross validation is performed in terms of the predictions of music labels such as genres. There are 9 genres in total, alternative, blues, electronic, folk/country, funk/soul/rnb, jazz, pop, rap/hip-hop and rock, in the MIR dataset. Precisions of the predicted music label at rank 1 and 3 are shown in Table 1, where the network architecture follows the notation of I for input layer, C for convolutional layer with stride, receptive field and filter depth, L for local response normalization layer, P for max pooling layer, F for fully connected layer with number of fully connected neurons, and O for the output layer. For the way to split the training and testing sample, we have tried both selecting random snippets across all the songs for training and testing sample, as well as evenly ordered by the song title to make sure the snippets from the same song do not enter both training and testing sample simultaneously. Both methods of splitting the dataset give similar cross validation results."}, {"heading": "6. MUSIC EMBEDDING AND COMPOSING MUSIC USING TRAINED NET", "text": "We take the values of the neurons on the last fully connected layer, just before the output layer, as the latent embedding for each song. After a dimensionality reduction using PCA we can visualize, in the 3D eigen-space, the distribution of song embedding as shown in Fig. 6.\nThese latent vectors can be used to provide content features of each song, construct song-to-song similarity thus providing content-based recommendation. The trained weights of the neural network can also be used to compose a song, where music labels or characteristics are provided in advance, and we can perform backward propagation to artificially construct the audio hyper-image, thus the auditory signals of song. Using Artificial Intelligence (AI) techniques to compose songs, and even lyrics, would be a promising application of this particular study."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we have proposed and experimented to use acoustic features of music to compose audio hyper-images, thus utilize the deep convolutional neural network to find a nonlinear mapping between these audio images and song labels. Predictions of the music labels have been tested using a 10-fold cross validation approach. We use the neurons in\nthe last fully connected layer to embed all the songs, and these latent features can be fed to subsequent models such as collaborative filtering, factorization machine, etc., to build a recommendation system. Further extensions of this work can be to use similar features and neural network models for voice or speech recognition, where the long-time scale of temporal structures of the auditory signals need to be captured perhaps by a recurrrent neural network structure, with the intent of the speech as the latent state variable."}, {"heading": "8. REFERENCES", "text": "[1] Morwaread M. Farbood, David J. Heeger, Gary Marcus, Uri Hasson, and Yulia Lerner. The neural processing of hierarchical structure in music and speech at different timescales. Front. Neurosci., 9, 2015.\n[2] Zhou Xing, Marzieh Parandehgheibi, Fei Xiao, Nilesh Kulkarni, and Chris Pouliot. Content-based recommendation for podcast audio-items using natural language processing techniques. 2016 IEEE International Conference on Big Data, 2016.\n[3] T. Eerola and J. K. Vuoskoski. A comparison of the discrete and dimensional models of emotion in music. Psychology of Music, 39(1), pages 18-49, 2011.\n[4] Helge Homburg, Ingo Mierswa, Bulent Moller, Katharina Morik, and Michael Wurst. A benchmark dataset for audio classification and clustering. Proc. of the International Symposium on Music Information Retrieval, pages 528\u2013531, 2005.\n[5] Ingo Mierswa and Katharina Morik. Automatic feature extraction for classifying audio data. Machine Learning Journal, Vol. 58, pages 127\u2013149, 2005.\n[6] FFmpeg. https://ffmpeg.org.\n[7] Mark A. Bartsch and Gregory H. Wakefield. Audio thumbnailing of popular music using chroma-based representations. IEEE Transactions on Multimedia, 7(1), p96-104, 2005.\n[8] Peter Grosche, Meinard Mller, and Frank Kurth. Cyclic tempogram - a mid-level tempo representation for music signals. ICASSP, 2010.\n[9] Beth Logan. Mel frequency cepstral coefficients for music modeling. Proc. Int. Symp. Music Information Retrieval (ISMIR), 2000.\n[10] S. J. Young, P. C. Woodland, and W. J. Bryne. Htk: Hidden markov model toolkit v 1.5, cambridge university engineering department speech group and entropic research laboratories inc.\n[11] Dan Ning Jiang, Lu Lie, Hong Jiang Zhang, Jian Hua Tao, and Lian Hong Cai. Music type classification by spectral contrast feature. Multimedia and Expo, 2002. ICME02. Proceedings, 2002.\n[12] C. Harte, M. Sandler, and M. Gasser. Detecting harmonic change in musical audio. Proceedings of the 1st ACM Workshop on Audio and Music Computing Multimedia, pp. 21-26, 2006.\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 2012, 2012."}], "references": [{"title": "The neural processing of hierarchical structure in music and speech at different timescales", "author": ["Morwaread M. Farbood", "David J. Heeger", "Gary Marcus", "Uri Hasson", "Yulia Lerner"], "venue": "Front. Neurosci.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Content-based recommendation for podcast audio-items using natural language processing techniques", "author": ["Zhou Xing", "Marzieh Parandehgheibi", "Fei Xiao", "Nilesh Kulkarni", "Chris Pouliot"], "venue": "IEEE International Conference on Big Data,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Vuoskoski. A comparison of the discrete and dimensional models of emotion in music", "author": ["J.K.T. Eerola"], "venue": "Psychology of Music,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A benchmark dataset for audio classification and clustering", "author": ["Helge Homburg", "Ingo Mierswa", "Bulent Moller", "Katharina Morik", "Michael Wurst"], "venue": "Proc. of the International Symposium on Music Information Retrieval,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Automatic feature extraction for classifying audio data", "author": ["Ingo Mierswa", "Katharina Morik"], "venue": "Machine Learning Journal,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Audio thumbnailing of popular music using chroma-based representations", "author": ["Mark A. Bartsch", "Gregory H. Wakefield"], "venue": "IEEE Transactions on Multimedia, 7(1),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Cyclic tempogram - a mid-level tempo representation for music", "author": ["Peter Grosche", "Meinard Mller", "Frank Kurth"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["Beth Logan"], "venue": "Proc. Int. Symp. Music Information Retrieval (ISMIR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Music type classification by spectral contrast feature", "author": ["Dan Ning Jiang", "Lu Lie", "Hong Jiang Zhang", "Jian Hua Tao", "Lian Hong Cai"], "venue": "Multimedia and Expo,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Detecting harmonic change in musical audio", "author": ["C. Harte", "M. Sandler", "M. Gasser"], "venue": "Proceedings of the 1st ACM Workshop on Audio and Music Computing Multimedia,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Neuroscience studies [1] has shown that the human brain integrates these complex streams of audio information through various levels of voxel, auditory cortex (A1+), superior temporal gyrus (STG), and inferior frontal gyrus (IFG), etc.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "Figure 1: Re-printed from [1].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "These latent embeddings of songs can be used either as features to feed to subsequent recommendation models, such as collaborative filtering, to alleviate the issues mentioned previously, or to build similarity metrics between songs, or simply to classify music into the targeted training classes such as genre, mood, etc [2].", "startOffset": 322, "endOffset": 325}, {"referenceID": 5, "context": "Chroma-gram of the 12 pitch classes can also be constructed using short-time Fourier transforms in combination with binning strategies [7] , as shown in the third row on the left.", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "The extraction of the local tempo and beat information is implemented using a mid-level representation of cyclic tempograms, where tempi differing by a power of two are identified [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Perceptual scale of pitches such as melody spectrogram (MELspectrogram), mel-frequency cepstrum consisting of various Mel-frequency cepstral coefficients (MFCC) can be generated by taking discrete cosine transform of the mel logarithmic powers [9] as shown in the fourth row.", "startOffset": 244, "endOffset": 247}, {"referenceID": 8, "context": "We also investigate other acoustic signals such as spectral contrast [11] and music harmonics such as tonal centroid features (tonnetz) [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "We also investigate other acoustic signals such as spectral contrast [11] and music harmonics such as tonal centroid features (tonnetz) [12].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "The deep neural network has been shown to perform a successful job on image classification and recognitions [13], the localized convolution to the 3-dimensional (3D) neurons has been able to capture latent features as well as other explicit features on the image such as patterns, colors, brightness, etc.", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "While both the data volume and heterogeneity of the digital music content is huge, it has become increasingly important and convenient to build a recommendation or search system to facilitate surfacing these content to the user or consumer community. Most of the recommendation models fall into two primary species, collaborative filtering based and content based approaches. Variants of instantiations of collaborative filtering approach suffer from the common issues of so called \u201ccold start\u201d and \u201clong tail\u201d problems where there is not much user interaction data to reveal user opinions or affinities on the content and also the distortion towards the popular content. Content-based approaches are sometimes limited by the richness of the available content data resulting in a heavily biased and coarse recommendation result. In recent years, the deep neural network has enjoyed a great success in large-scale image and video recognitions. In this paper, we propose and experiment using deep convolutional neural network to imitate how human brain processes hierarchical structures in the auditory signals, such as music, speech, etc., at various timescales. This approach can be used to discover the latent factor models of the music based upon acoustic hyper-images that are extracted from the raw audio waves of music. These latent embeddings can be used either as features to feed to subsequent models, such as collaborative filtering, or to build similarity metrics between songs, or to classify music based on the labels for training such as genre, mood, sentiment, etc.", "creator": "LaTeX with hyperref package"}}}