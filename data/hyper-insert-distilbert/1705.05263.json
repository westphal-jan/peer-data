{"id": "1705.05263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs", "abstract": "we train preparing a generator metaphor by blending maximum likelihood data and we also train creating the same generator architecture inspired by approximate wasserstein gan. we just then compare the adequately generated squared samples, exact log - probability densities and independently approximate wasserstein distances. first we show that providing an individual independent critic trained to approximate wasserstein distance fluctuations between entering the kernel validation set and the generator empirical distribution helps detect bias overfitting. finally, we always use similar ideas from the introductory one - shot learning literature available to develop consider a specifically novel fast learning curve critic.", "histories": [["v1", "Mon, 15 May 2017 14:24:01 GMT  (3581kb,D)", "http://arxiv.org/abs/1705.05263v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ivo danihelka", "balaji lakshminarayanan", "benigno uria", "daan wierstra", "peter dayan"], "accepted": false, "id": "1705.05263"}, "pdf": {"name": "1705.05263.pdf", "metadata": {"source": "META", "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs", "authors": ["Ivo Danihelka", "Balaji Lakshminarayanan", "Benigno Uria", "Daan Wierstra", "Peter Dayan"], "emails": ["<danihelka@google.com>."], "sections": [{"heading": "1. Introduction", "text": "There has been a substantial recent interest in deep generative models. Broadly speaking, generative models can be classified into explicit models where we have access to the model likelihood function, and implicit models which provide a sampling mechanism for generating data, but do not need to explicitly define a likelihood function. Examples of explicit models are variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and PixelCNN (Oord et al., 2016). Examples of implicit generative models are generative adversarial networks (GANs) (Goodfellow et al., 2014) and stochastic simulator models. Explicit models are typically trained by maximizing the likelihood or its lower bound; they are popular in probabilistic modeling as the training procedure optimizes a well-defined quantity and the likelihood can be used for model comparison and selection. However likelihood is not about human perception (Theis et al., 2015).\nRecently, GAN-based training has emerged as a promising approach for learning implicit generative models. In particular, generators trained using GAN-based approaches have shown to be capable of generating photo-realistic samples. There is a lot of interest in understanding the the properties of the learned generators in GANs and comparing them with traditional likelihood-based methods. One step\n1Google DeepMind, London, United Kingdom 2CoMPLEX, Computer Science, UCL 3Gatsby Unit, UCL. Correspondence to: Ivo Danihelka <danihelka@google.com>.\nInvertible transformation\nz0 \u21e0 N(0, 1)\nz1\nInvertible transformation\nFigure 1. The generator is a real NVP with multiple invertible transformations.\nin this direction was the quantitative analysis by Wu et al. (2016) who evaluate the (approximate) likelihood of decoder based models using annealed importance sampling (AIS). We use a different approach to provide additional insights. We use a tractable generator architecture for which the log-probability densities can be computed exactly. We train the generator architecture by maximum likelihood and we also train the same generator architecture by GAN. We then compare the properties of the learned generators.\nConcretely, we use real non-volume preserving transformation (real NVP) (Dinh et al., 2016) as the generator. We compare training by maximum likelihood to training by Wasserstein GAN (WGAN) (Arjovsky et al., 2017). In WGAN, a critic learns to approximate the Wasserstein distance between the data and the generator distribution. The generator learns to minimize the critic\u2019s approximation of the Wasserstein distance. WGAN has a well defined training procedure and allowed us to train a non-traditional GAN generator. Another advantage of WGAN is that the critic\u2019s approximation of the Wasserstein distance can be used to detect overfitting as well as to compare different models. We discuss this more in Section 3.5.\nWGAN worked for us better than other GAN methods (Jensen-Shannon divergence or \u2212 logD(x)). We intend to communicate the found interesting differences from training by maximum likelihood. After describing the generator and the critic in Section 2, we report some perhaps surprising findings in Section 3:\n1. Generators trained by WGAN produce more globally coherent samples even from a relatively shallow gen-\nar X\niv :1\n70 5.\n05 26\n3v 1\n[ cs\n.L G\n] 1\n5 M\nay 2\n01 7\nerator.\n2. Minimization of the approximate Wasserstein distance does not correspond to minimization of the negative log-probability density. The negative log-probability densities became worse than densities from a uniform distribution.\nWe also report a few findings about the approximate Wasserstein distance:\n1. An approximation of the Wasserstein distance ranked correctly generators trained by maximum likelihood.\n2. The approximate Wasserstein distance between the training data and the generator distribution became smaller than the distance between the test data and the generator distribution. This overfitting was observed for generators trained by maximum likelihood and also for generators trained by WGAN.\nFinally, we use the ability to compare different models when evaluating a novel fast learning critic in Section 4."}, {"heading": "2. Setup", "text": ""}, {"heading": "2.1. Generator", "text": "In all our experiments, the generator is a real-valued non-volume preserving transformation (NVP) (Dinh et al.,\n2016). The generator is schematically represented in Figure 1. The generator starts with latent noise z0 from the standard normal distribution. The noise is transformed by a sequence of invertible transformations to a generated image z1. Like other generators used in GANs, the generator does not add independent noise to the generated image.\nUnlike other GAN generators, we are able to compute the log-probability density of the generated image z1 by:\nlog pZ1(z1) = log pZ0(z0)\u2212 log |det \u2202z1 \u2202z0 | (1)\nwhere log pZ0(z0) is the log-probability density of the standard normal distribution. The determinant of the \u2202z1\u2202z0 Jacobian matrix can be computed efficiently, if each transformation has a triangular Jacobian (Dinh et al., 2016).\nWe can also compute the log-probability density assigned to an image by running the inverse of the transformations. The inferred z0 can be then plugged to Equation 1.\nWe use a generator with the convolutional multi-scale architecture designed by Dinh et al. (2016). Concretely, we use 3 different generator architectures with 1, 2 and 3 multiscale levels. In figures, these generators are labeled NVP1, NVP2, NVP3 and have 7, 13 and 19 non-volume preserving transformations. Each transformation uses 4 residual blocks. In total, the generator NVP3 has 19 \u00d7 4 = 76 layers. It is a very deep generator. To have the same training and testing conditions, we use no batch normalization in the generator."}, {"heading": "2.2. Critic", "text": "Wasserstein GAN (WGAN) (Arjovsky et al., 2017) uses a critic instead of a discriminator. The critic is trained to provide an approximation of the Wasserstein distance between the real data distribution Pr and the generator distribution Pg . The approximation is based on the KantorovichRubinstein duality:\nW (Pr , Pg) \u221d max f Ex\u223cPr [f(x)]\u2212 Ex\u223cPg [f(x)] (2)\nwhere f : RD \u2192 R is a Lipschitz continuous function. In practice, the critic f is a neural network with clipped weights to have bounded derivatives. The critic is trained to produce high values at real samples and low values at generated samples. The difference of the expected critic values then approximates the Wasserstein distance. The approximation is scaled by the Lipschitz constant for the critic (Arjovsky et al., 2017). After training the critic for the current generator distribution, the generator can be trained to minimize the approximate Wasserstein distance.\nWe follow closely the original WGAN implementation1 and use a DCGAN (Radford et al., 2015) based critic. The implementation uses learning rate 0.00005, batch size 64 and clips the critic weights to a fixed box [\u22120.01, 0.01]. The critic is updated 100 times in the first 25 generator steps and also after every 500 generator steps, otherwise the critic is updated 5 times per generator step.\n1https://github.com/martinarjovsky/ WassersteinGAN\nWe rescale the images to [\u22121, 1] range before feeding them to the critic."}, {"heading": "2.3. Datasets", "text": "We train on 28 \u00d7 28 images from MNIST (LeCun et al., 1998) and on 32 \u00d7 32 images from the CelebFaces Attributes Dataset (CelebA) (Liu et al., 2015). The CelebA dataset contains over 200,000 celebrity images. During training we augment the CelebA dataset to also include horizontal flips of the training examples as done by Dinh et al. (2016)."}, {"heading": "2.4. Data Preprocessing", "text": "We convert the discrete pixel intensities from {0, . . . , 255}D to continuous noisy images from [0, 256]D by adding a uniform noise u \u2208 [0, 1]D to each pixel intensity. The computed log-probability density will be then a lower bound for the log-probability mass (Theis et al., 2015):\n\u222b [0,1]D log p(x+ u) du \u2264 log \u222b [0,1]D p(x+ u) du (3)\nFinally, before feeding the pixel intensities to a network, we divide the pixel intensities by 256. This is a simple nonvolume preserving transformation. If the noisy image is z2 = x+u and the image with scaled intensities is z1 = z2256\nthen the log-probability density of the noisy image is:\nlog pZ2(z2) = log pZ1(z1)\u2212 log |det \u2202z2 \u2202z1 | (4)\n= log pZ1(z1)\u2212D log 256 (5)\nwhere D is the number of dimensions in the image (e.g., D = 32 \u00d7 32 \u00d7 3). For example, if an uninformed model assigns uniform probability density pZ1(z1) = 1 to all z1 \u2208 [0, 1]D, the model would require log2(256) = 8 bits/dim to describe an image. The reported negative logprobability density then corresponds to an upper bound on compression loss in bits/dim."}, {"heading": "3. Results", "text": "We trained the same generator architecture by maximum likelihood and by WGAN. We will now compare the effect of these two different objectives."}, {"heading": "3.1. Generated Samples", "text": "Figure 2 shows samples from a generator trained by maximum likelihood and from another generator trained by WGAN. Both generators have the same number of parameters and the same architecture.\nThe generator trained by WGAN seems to produce more coherent faces. The effect is more apparent when looking at samples produced from a shallower generator in Figure 3. The shallow generator NVP1 learned to produce only locally coherent image patches if trained by maximum likelihood."}, {"heading": "3.2. Log-probability Density", "text": "Figure 4 shows the negative log-probability density for generators trained by maximum likelihood estimation (MLE). The shallowest generator NVP1 obtained the worse performance there. The generators show only a small amount of overfitting to the training set. The validation loss is slightly higher than the training loss.\nGenerators trained by WGAN have their negative logprobability densities shown in Figure 5. The figure has a completely different y-axis. The negative log-probability densities get worse when training by WGAN. The training and validation losses overlap.\nA generator can get infinite negative log-probability if it assigns zero probability to an example. To check this possibility, we analysed the negative log-probabilities assigned to all validation examples. Figure 6 shows the histogram of negative log-probabilities. We see that all 19867 validation examples have low log-probabilities. A similar histogram was obtained also for the training set. So the bad average log-probability is not caused by a single outlier.\nWe further inspected the negative log-probability density of generated samples. We observed that the average negative log-probability density of generated samples is -1.12 bits/dim. The negative sign is not a typo. The negative log-probability density can be negative, if the probability density is bigger than 1. In contrast, the NVP3 generator trained by maximum likelihood assigned on average 4.27 bits/dim to the validation examples and 4.35 bits/dim to its own generated samples.\nThe problematic negative log-probability densities from WGAN were not limited to the CelebA dataset. Generators trained by WGAN had high negative log-probability densities also on MNIST (Figure 13). A deep generator trained by WGAN learns a distribution lying on a low dimensional manifold (Figure 17 in the appendix). The generator is then putting the probability mass only to a space with a near-zero volume. We may need a more powerful critic to recognize the excessively correlated pixels. Approximating the likelihood by annealed importance sampling (Wu et al., 2016) would not discover this problem, as their analysis assumes a Gaussian observation model with a fixed variance.\nThe problem is not unique to WGAN. We also obtained near-infinite negative log-probability densities when training GAN to minimize the Jensen-Shannon divergence (Goodfellow et al., 2014)."}, {"heading": "3.3. Distribution of Latent Variables", "text": "One of the advantages of real NVPs is that we can infer the original latent z0 for a given generated sample. We know that the distribution of the latent variables is the prior N(0, 1), if the given images are from the generator. We are curious to see the distribution of the latent variables, if the given images are from the validation set.\nIn Figure 7, we display a 2D histogram of the first 2 latent variables z0[1], z0[2]. The histogram was obtained by inferring the latent variables for all examples from the validation set. When the generator was trained by maxi-\nmum likelihood, the inferred latent variables had the following means and standard deviations: \u00b51 = 0.05, \u00b52 = 0.05, \u03c31 = 1.06, \u03c32 = 1.03. In contrast, the generator trained by WGAN had inferred latent variables with significantly larger standard deviations: \u00b51 = 0.02, \u00b52 = 1.62, \u03c31 = 3.95, \u03c32 = 8.96.\nWhen generating the latent variables from the N(0, 1) prior, the samples from the generator trained by WGAN would have a different distribution than the validation set."}, {"heading": "3.4. Partial Reconstructions", "text": "Real NVPs are invertible transformations and have perfect reconstructions. We can still visualize reconstructions from a partially resampled latent vector. Gregor et al. (2016) and Dinh et al. (2016) visualized \u2018conceptual compression\u2019 by inferring the latent variables and then resampling a part of the latent variables from the normalN(0, 1) prior. The subsequent reconstruction should still form a valid image. If the original image was generated by the generator, the partially resampled latent vector would still have the normal N(0, 1) distribution.\nFigure 8 shows the reconstructions if resampling the first half or the second half of the latent vector. The generator trained by maximum likelihood (MLE) has partial reconstructions similar to generated samples. In comparison,\nthe partial reconstructions from the generator trained by WGAN do not resemble samples from WGAN. This again indicates that the validation examples have a different distribution than WGAN samples."}, {"heading": "3.5. Wasserstein Distance", "text": "We will now look at the approximate Wasserstein distance between the validation data and the generator distribution. To approximate the Wasserstein distance, we will use the duality in Equation 2. We will train another critic to assign high values to validation samples and low values to generated samples. This independent critic will be used only for evaluation. The generator will not see the gradients from the independent critic.\nThe independent critic is trained to maximize:\nW\u0302 (xvalid , xg) = 1\nN N\u2211 i=1 f\u0302(xvalid [i])\u2212 1 N N\u2211 i=1 f\u0302(xg[i]))\n(6)\nwhere xvalid is a batch of samples from the validation set, xg is a batch of generated samples and f\u0302 is the independent critic.\nWe will keep the Lipschitz constant of the independent critic approximately constant by always using same architecture and the same weight clipping for the independent critic. We can then compare the approximate Wasserstein distances from different experiments.\nFigure 9-left shows the approximate Wasserstein distance between the validation set and the generator distribution. The first thing to notice is the correct ordering of generators trained by maximum likelihood. The deepest generator NVP3 has the smallest approximate distance from the validation set, as indicated by the thick solid lines.\nWe also display an approximate distance between the training set and generator distribution:\nW\u0302 (xtrain , xg) (7)\nand the approximate distance between the test set and the generator distribution:\nW\u0302 (xtest , xg) (8)\nwhere xtrain is a batch of training examples, xtest is a batch of test examples and W\u0302 is the approximate Wasserstein distance computed by Equation 6. We are misusing the independent critic here. We are asking the independent critic to assign values to training and test examples. The independent critic was trained only to assign values to validation examples and to generated samples. This leads to a desirable effect: We can detect whether the generator overfits the training data. If the training examples have the same distribution as the test examples, we should observe:\nE [ W\u0302 (xtrain , xg) ] = E [ W\u0302 (xtest , xg) ] (9)\nIn practice, the empirical distribution of the training data is not the same as the distribution of the test data. The generated samples xg can be more similar to the training data than to the test data.\nFigure 9 clearly shows that:\nW\u0302 (xtrain , xg) < W\u0302 (xtest , xg) (10)\nfor the trained generators. The approximate distance between the test set and the generator distribution is a little bit smaller than the approximate distance between the validation set and the generator distribution. The approximate distance between the training set and the generator distribution is much smaller. The generators are overfitting the training set.\nAlgorithm 1 Fast Learning Critic Require: batch size N , conditional critic f . Sample xr \u223c Pr a batch from the real data. Sample xg \u223c Pg a batch of generator samples. Sample xe \u223c Pg extra generator samples. Compute the approximate Wasserstein distance: W := 1N \u2211N i=1 f(xr[i], xe)\u2212 1N \u2211N i=1 f(xg[i], xe)\nIn multiple experiments, we found that the performance of the generators is heavily influenced by the WGAN critic used for training. Figure 9-right shows that W\u0302 (xvalid , xg) is roughly the same for NVP1 and NVP3 (blue and red thick solid lines). Both generators were trained with the same WGAN critic architecture. The generators can also overfit and gradually degrade the performance on the validation set. Visual inspection of the samples can be then misleading.\nThe approximate Wasserstein distance introduced by Arjovsky et al. (2017) is a very useful tool for model selection. If we use the independent critic, we can compare generators trained by other GAN methods or by different approaches. MNIST results are in Figure 14. In the next section, we will use the independent critic to compare different critic architectures."}, {"heading": "4. Fast Learning Critic", "text": "To obtain good results, WGAN requires to have a good critic. The critic needs to be retrained when the generator changes. It would be nice to reduce the number of needed retraining steps. We will take ideas from supervised setting where people used a short-term memory to provide oneshot learning (Santoro et al., 2016; Vinyals et al., 2016).\nIf we look at the optimization problem for the critic (Equation 2), we see that the optimal critic would need to be based on the data distribution and on the current generator distribution. The data distribution is stationary, so the\nStridedConv2D\nLeakyReLU\nBatchNorm\n+\nStridedConv2D\nLeakyReLU\nShape: [batch_size, 32, 32, 3] Shape: [num_extra_samples, 32, 32, 3]\nLeakyReLU\nConv2D\nf(x, xe)\nx xe\nStridedConv2D\nShape: [batch_size, 1, 1, 1]\nStridedConv2D\nLeakyReLU\nBatchNorm\nGatedLayer\n+\nConv2D\nAvgConv2D\nEmbedding Shape: [1, 8, 8, num_channels]\nStridedConv2D\nLeakyReLU\nGatedLayer\nFigure 11. The fast learning critic f(x, xe) outputs the value for samples x, conditioned on an extra batch of samples xe from the generator.\nknowledge of the data distribution can be kept stored in the critic weights. The generator distribution is changing quickly. So we should give the critic a lot of information about the current generator distribution.\nFor example, it would be helpful to generate extra samples from the generator and to compute moments of the generator distribution. The moments can be then passed as extra information to the critic. We will do something more powerful. We will allow the critic to extract features from the extra generator samples.\nConcretely, we will allow the critic to condition on the extra samples from the generator. The extra samples are processed to produce a learned embedding of the generator distribution (Muandet et al., 2016). The embedding is then used to bias the critic."}, {"heading": "4.1. Architecture", "text": "The implemented critic architecture is depicted in Figure 11. The architecture looks like a DCGAN discriminator (Radford et al., 2015) conditioned on an embedding. The distribution embedding is produced by a network with\ngated activations (Figure 10) on the residual connections (Oord et al., 2016). The features from the batch of extra generator samples are averaged over the batch dimension to produce the distribution embedding.\nAlgorithm 1 shows the usage of the critic. The embedding remains the same when running the critic on real or generated samples. The critic would become the original WGAN critic, if using zeros instead of the distribution embedding. We use batch size 64 and we also use 64 extra generator samples.\nThe weights used to produce the embedding of the distribution do not need to be clipped and we do not clip them.\nWhen training a generator, we do not use the gradients with respect to the extra generator samples. The generator is only trained with the gradient of the approximate Wasserstein distance with respect to xg ."}, {"heading": "4.2. Fast Learning Results", "text": "Figure 12 compares training without and with the fast learning critic. The critic was intentionally updated less frequently by gradient descent to demonstrate the benefits of the fast learning. The critic was updated 100 times in the first 25 generator steps and also after every 500 generator steps, otherwise the critic is updated only 2 times per generator step. The independent critic was still updated at least 5 times per generator step to keep all measurements comparable.\nWithout the fast learning critic, the generator failed to produce samples similar to the data examples. The fast learning critic may be important for conditional models and for video modeling. We do not have multiple real samples for a situation there."}, {"heading": "5. Discussion", "text": "In our experiments, we used two tools for the evaluation of generators. First, we used a real NVP to compute the exact log-probability densities. Second, we used an independent critic to compare the approximate Wasserstein distances on the validation set. The independent critic is very generic. The critic only needs samples from two distributions.\nThe approximate Wasserstein distance from the independent critic allows us to compare different generator and critic architectures. If we care about Wasserstein distances, we should be comparing generators based on the approximate Wasserstein distance to the validation set.\nThe log-probability densities are less useful for generator comparison when the generators are generating only a subset of the data. On the other hand, when doing lossless compression, we care about log-probabilities (Theis et al., 2015). When using real NVPs we can even jointly optimize both objectives (Figure 15 and Figure 16).\nWe show one additional usage of real NVPs for Adversarial Variational Bayes (Mescheder et al., 2017) evaluation in the appendix."}, {"heading": "Acknowledgments", "text": "We want to thank Mihaela Rosca, David Warde-Farley, Shakir Mohamed, Edward Lockhart, Arun Nair and Chris Burgess for many inspiring discussions. We also thank the very helpful anonymous reviewer for suggesting to inspect the rank of the Jacobian matrix."}, {"heading": "A. Log-probability Density Ratio Evaluation", "text": "Real NVPs are not limited to the computation of negative log-probability densities of visible variables. For example, a real NVP can be used as an encoder in Adversarial Variational Bayes (Husza\u0301r, 2017; Mescheder et al., 2017). We were able to measure the gap between the unbiased KL estimate log q(z|x) \u2212 log p(z) and its approximation from GAN. Figure 18 shows that Adversarial Variational Bayes underestimates the KL divergence. The discriminator would need to output logit(D(x)) = \u2212KL to represent the KL.\nAfter measuring the problem, we can start thinking how to mitigate it. It would be possible to use auto-regressive discriminators (Oord et al., 2016) to decompose the large KL divergence to multiple smaller terms:\nlog q(z|x) p(z) = \u2211 i log q(zi|x, z1:i\u22121)\u2212 log p(zi) (11)\nwhere p(zi) is the independent Gaussian prior."}], "references": [{"title": "Towards Principled Methods for Training Generative Adversarial Networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.04862,", "citeRegEx": "Arjovsky and Bottou,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky and Bottou", "year": 2017}, {"title": "Density estimation using real NVP", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards conceptual compression", "author": ["Gregor", "Karol", "Besse", "Frederic", "Rezende", "Danilo Jimenez", "Danihelka", "Ivo", "Wierstra", "Daan"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2016}, {"title": "Improved training of wasserstein gans", "author": ["Gulrajani", "Ishaan", "Ahmed", "Faruk", "Arjovsky", "Martin", "Dumoulin", "Vincent", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1704.00028,", "citeRegEx": "Gulrajani et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2017}, {"title": "Variational inference using implicit models, part I: Bayesian logistic regression", "author": ["Husz\u00e1r", "Ferenc"], "venue": null, "citeRegEx": "Husz\u00e1r and Ferenc.,? \\Q2017\\E", "shortCiteRegEx": "Husz\u00e1r and Ferenc.", "year": 2017}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Deep learning face attributes in the wild", "author": ["Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks", "author": ["L. Mescheder", "S. Nowozin", "A. Geiger"], "venue": "arXiv preprint arXiv:1701.04722,", "citeRegEx": "Mescheder et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mescheder et al\\.", "year": 2017}, {"title": "Kernel mean embedding of distributions: A review and beyonds", "author": ["Muandet", "Krikamol", "Fukumizu", "Kenji", "Sriperumbudur", "Bharath", "Sch\u00f6lkopf", "Bernhard"], "venue": "arXiv preprint arXiv:1605.09522,", "citeRegEx": "Muandet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2016}, {"title": "Conditional image generation with PixelCNN decoders", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Vinyals", "Oriol", "Espeholt", "Lasse", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "arXiv preprint arXiv:1605.06065,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "On the quantitative analysis of decoder-based generative models", "author": ["Wu", "Yuhuai", "Burda", "Yuri", "Salakhutdinov", "Ruslan", "Grosse", "Roger"], "venue": "arXiv preprint arXiv:1611.04273,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Examples of explicit models are variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and PixelCNN (Oord et al.", "startOffset": 65, "endOffset": 111}, {"referenceID": 11, "context": ", 2014) and PixelCNN (Oord et al., 2016).", "startOffset": 21, "endOffset": 40}, {"referenceID": 2, "context": "Examples of implicit generative models are generative adversarial networks (GANs) (Goodfellow et al., 2014) and stochastic simulator models.", "startOffset": 82, "endOffset": 107}, {"referenceID": 15, "context": "However likelihood is not about human perception (Theis et al., 2015).", "startOffset": 49, "endOffset": 69}, {"referenceID": 1, "context": "Concretely, we use real non-volume preserving transformation (real NVP) (Dinh et al., 2016) as the generator.", "startOffset": 72, "endOffset": 91}, {"referenceID": 16, "context": "in this direction was the quantitative analysis by Wu et al. (2016) who evaluate the (approximate) likelihood of decoder based models using annealed importance sampling (AIS).", "startOffset": 51, "endOffset": 68}, {"referenceID": 4, "context": "The WGAN samples would look better if using the improved training of WGAN-GP (Gulrajani et al., 2017).", "startOffset": 77, "endOffset": 101}, {"referenceID": 1, "context": "In all our experiments, the generator is a real-valued non-volume preserving transformation (NVP) (Dinh et al., 2016).", "startOffset": 98, "endOffset": 117}, {"referenceID": 1, "context": "The determinant of the \u2202z1 \u2202z0 Jacobian matrix can be computed efficiently, if each transformation has a triangular Jacobian (Dinh et al., 2016).", "startOffset": 125, "endOffset": 144}, {"referenceID": 1, "context": "We use a generator with the convolutional multi-scale architecture designed by Dinh et al. (2016). Concretely, we use 3 different generator architectures with 1, 2 and 3 multiscale levels.", "startOffset": 79, "endOffset": 98}, {"referenceID": 12, "context": "We follow closely the original WGAN implementation1 and use a DCGAN (Radford et al., 2015) based critic.", "startOffset": 68, "endOffset": 90}, {"referenceID": 7, "context": "Datasets We train on 28 \u00d7 28 images from MNIST (LeCun et al., 1998) and on 32 \u00d7 32 images from the CelebFaces Attributes Dataset (CelebA) (Liu et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 8, "context": ", 1998) and on 32 \u00d7 32 images from the CelebFaces Attributes Dataset (CelebA) (Liu et al., 2015).", "startOffset": 78, "endOffset": 96}, {"referenceID": 1, "context": "During training we augment the CelebA dataset to also include horizontal flips of the training examples as done by Dinh et al. (2016).", "startOffset": 115, "endOffset": 134}, {"referenceID": 15, "context": "The computed log-probability density will be then a lower bound for the log-probability mass (Theis et al., 2015): \u222b", "startOffset": 93, "endOffset": 113}, {"referenceID": 17, "context": "Approximating the likelihood by annealed importance sampling (Wu et al., 2016) would not discover this problem, as their analysis assumes a Gaussian observation model with a fixed variance.", "startOffset": 61, "endOffset": 78}, {"referenceID": 2, "context": "We also obtained near-infinite negative log-probability densities when training GAN to minimize the Jensen-Shannon divergence (Goodfellow et al., 2014).", "startOffset": 126, "endOffset": 151}, {"referenceID": 2, "context": "Gregor et al. (2016) and Dinh et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "(2016) and Dinh et al. (2016) visualized \u2018conceptual compression\u2019 by inferring the latent variables and then resampling a part of the latent variables from the normalN(0, 1) prior.", "startOffset": 11, "endOffset": 30}, {"referenceID": 11, "context": "The gated convolutional layer from Oord et al. (2016). The channels are split and passed to the element-wise multiplicative interaction.", "startOffset": 35, "endOffset": 54}, {"referenceID": 14, "context": "We will take ideas from supervised setting where people used a short-term memory to provide oneshot learning (Santoro et al., 2016; Vinyals et al., 2016).", "startOffset": 109, "endOffset": 153}, {"referenceID": 16, "context": "We will take ideas from supervised setting where people used a short-term memory to provide oneshot learning (Santoro et al., 2016; Vinyals et al., 2016).", "startOffset": 109, "endOffset": 153}, {"referenceID": 10, "context": "The extra samples are processed to produce a learned embedding of the generator distribution (Muandet et al., 2016).", "startOffset": 93, "endOffset": 115}, {"referenceID": 12, "context": "The architecture looks like a DCGAN discriminator (Radford et al., 2015) conditioned on an embedding.", "startOffset": 50, "endOffset": 72}, {"referenceID": 11, "context": "gated activations (Figure 10) on the residual connections (Oord et al., 2016).", "startOffset": 58, "endOffset": 77}, {"referenceID": 15, "context": "On the other hand, when doing lossless compression, we care about log-probabilities (Theis et al., 2015).", "startOffset": 84, "endOffset": 104}, {"referenceID": 9, "context": "We show one additional usage of real NVPs for Adversarial Variational Bayes (Mescheder et al., 2017) evaluation in the appendix.", "startOffset": 76, "endOffset": 100}, {"referenceID": 9, "context": "For example, a real NVP can be used as an encoder in Adversarial Variational Bayes (Husz\u00e1r, 2017; Mescheder et al., 2017).", "startOffset": 83, "endOffset": 121}, {"referenceID": 11, "context": "It would be possible to use auto-regressive discriminators (Oord et al., 2016) to decompose the large KL divergence to multiple smaller terms:", "startOffset": 59, "endOffset": 78}], "year": 2017, "abstractText": "We train a generator by maximum likelihood and we also train the same generator architecture by Wasserstein GAN. We then compare the generated samples, exact log-probability densities and approximate Wasserstein distances. We show that an independent critic trained to approximate Wasserstein distance between the validation set and the generator distribution helps detect overfitting. Finally, we use ideas from the one-shot learning literature to develop a novel fast learning critic.", "creator": "LaTeX with hyperref package"}}}