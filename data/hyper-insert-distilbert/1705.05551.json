{"id": "1705.05551", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "New Reinforcement Learning Using a Chaotic Neural Network for Emergence of \"Thinking\" - \"Exploration\" Grows into \"Thinking\" through Learning -", "abstract": "expectation for the continued emergence of their higher functions is slowly getting larger exponential in the framework of quantum end - to - conclusion end dependent reinforcement mediated learning using a specific recurrent quantum neural network. \u201d however, obtaining the dynamic emergence factor of \" thinking \" that is a typical for higher output function is difficult to realize correctly because \" concurrent thinking \" needs essentially non fixed - point, especially flow - limiting type analytic attractors assisting with both convergence and transition dynamics. furthermore, knowing in your order to introduce \" appropriate inspiration \" process or \" discovery \" in \" thinking \", not completely random choices but unexpected transition periods should be also required.", "histories": [["v1", "Tue, 16 May 2017 06:54:04 GMT  (2015kb)", "http://arxiv.org/abs/1705.05551v1", "The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2017, 5 pages, 6 figures"]], "COMMENTS": "The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2017, 5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["katsunari shibata", "yuki goto"], "accepted": false, "id": "1705.05551"}, "pdf": {"name": "1705.05551.pdf", "metadata": {"source": "CRF", "title": "New Reinforcement Learning Using a Chaotic Neural Network for Emergence of \u201cThinking\u201d \u2014 \u201cExploration\u201d Grows into \u201cThinking\u201d through Learning \u2014", "authors": ["Katsunari Shibata", "Yuki Goto"], "emails": ["katsunarishibata@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n05 55\n1v 1\n[ cs\n.A I]\n1 6\nM ay\nBy analogy to \u201cchaotic itinerancy\u201d, we have hypothesized that \u201cexploration\u201d grows into \u201cthinking\u201d through learning by forming flow-type attractors on chaotic random-like dynamics. It is expected that if rational dynamics are learned in a chaotic neural network (ChNN), coexistence of rational state transition, inspiration-like state transition and also random-like exploration for unknown situation can be realized.\nBased on the above idea, we have proposed new reinforcement learning using a ChNN. The positioning of exploration is completely different from the conventional one. Here, the chaotic dynamics inside the ChNN produces exploration factors by itself. Since external random numbers for stochastic action selection are not used, exploration factors cannot be isolated from the output. Therefore, the learning method is also completely different from the conventional one.\nOne variable named causality trace is put at each connection, and takes in andmaintains the input through the connection according to the change in its output. Using these causality traces and TD error, the connection weights except for the feedback connections are updated in the actor ChNN.\nIn this paper, as the result of a recent simple task to see whether the new learning works appropriately or not, it is shown that a robot with two wheels and two visual sensors reaches a target while avoiding an obstacle after learning though there are still many rooms for improvement.\nKeywords: thinking, chaotic neural network (ChNN), exploration, reinforcement learning (RL), function emergence"}, {"heading": "Acknowledgements", "text": "This research has been supported by JSPS KAKENHI Grant Numbers JP23500245, JP15K00360 and many our group members\n\u2217http://shws.cc.oita-u.ac.jp/\u02dcshibata/home.html"}, {"heading": "1 Introduction", "text": "Expectation for the emergence of artificial intelligence is growing these days triggered by the recent results in reinforcement learning (RL) using a deep neural network (NN)[1, 2]. Our group has propounded for around 20 years that end-toend RL from sensors to motors using a recurrent NN (RNN) plays an important role for the emergence[3, 4]. Especially, different from \u201crecognition\u201d whose inputs are given as sensor signals or \u201ccontrol\u201d whose outputs are given as motor commands, higher functions are very difficult to design by human hands, and the function emergence approach through end-to-end RL is highly expected. Our group has shown that not only recognition and motion, but also memory, prediction, individuality, and also similar activities to those in monkey brain at tool use emerge[4]. We have also shown that a variety of communications emerge in the same framework[5]. However, the emergence of what can be called \u201cthinking\u201d that is one of the typical higher functions has not been shown yet. In this paper, the difficulty of emergence of \u201cthinking\u201d is discussed at first. Then our hypothesis that \u201cexploration\u201d grows into \u201cthinking\u201d through learning is introduced[6]. To realize the hypothesis, the use of a chaotic NN (ChNN) in RL and new deterministic RL for it are introduced[6]. Finally, it is shown that the new RL works in a simple task[7] though that cannot be called \u201cthinking\u201d yet and there are still many rooms for improvement. No other works with a similar direction to ours have not been found."}, {"heading": "2 Difficulty in Emergence of \u201cThinking\u201d", "text": "The definition of \u201cThinking\u201d must be varied depending on the person. However, we can \u201cthink\u201d even when we close our eyes and ears, and what we think does not change randomly, but logically or rationally. Therefore, we hope many ones can agree that in order to realize \u201cthinking\u201d, rational multi-stage or flow-type state transition should be formed.\nAs a kind of dynamic functions, we have shown that a variety of memory-required functions emerge in an RNN in simple tasks[4]. It is not so difficult to form memories as fixed-point convergence dynamics if the initial feedback connection weights are set such that the transition matrix for the connection is the identity matrix or close to it when being linearly approximated. That can also solve the vanishing gradient problem in error back propagation. However, state transition needs not only convergence dynamics for association, but also transition dynamics from one state to another.\nThen, we employed a multi-room task in which an agent moves and when it pushes a button, one of the doors opens and then the agent can move to another room. The sensor signals are the inputs of the agent\u2019s RNN, and the RNN was trained based on RL from a reward when it reached the goal. We expected that the internal state changed drastically between before and after the door open though the difference in the sensor signals was not so large. After learning, a large change in the internal state could be observed in some degree, but the learning was very difficult[8].\nFurthermore, when we \u201cthink\u201d, \u201cinspiration\u201d or \u201cdiscovery\u201d, which is a kind of unexpected but not completely random and rational transition, must be also essential. The convergence and transition dynamics seem contradict at a glance, and it seems very difficult to form both dynamics from scratch in a regular RNN."}, {"heading": "3 Chaos and Hypothesis: Growth from \u201cExploration\u201d to \u201cThinking\u201d through Learning", "text": "Suppose we are standing at a forked road as shown in Fig. 1. Usually, we choose one from two options: going right and going left. We do not care many other possible actions such as going straight and dancing. It is not motor(actuator)-level lower exploration, but higher exploration supported by some prior or learned knowledge[9]. Furthermore, at a fork, we may wonder such that \u201cthis path looks more rough, but that way looks to go away from the destination\u201d. This can be considered as a kind of \u201cexploration\u201d and also as a kind of \u201cthinking\u201d. The place where the wave in mind occurs is inside the process before making a decision, and learning should be reflected largely on the exploration. The author\u2019s group has thought that exploration should be generated inside of a recurrent NN (RNN) that generates motion commands[10]. \u201cExploration\u201d and \u201cthinking\u201d are both generated as internal dynamics as shown in Fig. 2. \u201cExploration\u201d is more random-\nlike state transitions. On the other hand, \u201cthinking\u201d is more rational or logical state transition, and sometimes higher exploration or unexpected but rational state transition such as inspiration or discovery occurs in it.\nAnalogy to such dynamics can be found in chaotic dynamics. In regular associative memory, fixed-point attractor (basin) is formed around each memorized or learned pattern as shown in the upper left part in Fig. 3. However, when a ChNN is used in it, transition dynamics among the memorized patters called \u201cChaotic Itinerancy\u201d[11] can be seen as the green arrows in the lower left part in Fig. 3. If rational or logical state transitions are learned, it is expected that flow-type attractors are formed as the red or black arrows in the lower right part in Fig. 3. It is also expected that as the green arrows, (1)inspiration or discovery emerges as irregular transitions among the attractors but reflecting the distance, (2)higher exploration emerges at branches of the flow, and (3)in unknown situations, no attractor is formed, and remaining random-like chaotic dynamics appears until return to a known situation. Skarda et al. reported that the activities on the olfactory bulb in rabbits become chaotic for unknown stimuli[12]. Osana et al. have shown the difference of chaotic property between known and unknown patterns on an associative memory using a ChNN, and also after an unknown pattern is learned, association to the pattern is formed as well as the other known patterns[13]. From the above discussion, we have hypothesized that \u201cexploration\u201d grows into \u201cthinking\u201d through learning by forming flow-type attractors on chaotic random-like dynamics and that can be realized on reinforcement learning using a ChNN."}, {"heading": "4 New Reinforcement Learning (RL) Using a Chaotic Neural Network (ChNN)", "text": "In order to realize the above idea, our group has proposed new reinforcement learning using a ChNN[6]. The positioning of exploration in learning is completely different from the conventional one. Here, the chaotic dynamics inside the ChNN produces exploration factors by itself. Since external random numbers for stochastic action selection are not used, exploration factors cannot be isolated from the output. Then the learning method has to be completely different from the conventional one.\nAssuming that the motions are continuous, actor-critic type reinforcement learning architecture is employed. Here, to isolate the chaotic dynamics from the critic, the actor is implemented in a ChNN and the critic is implemented in another regular layered NN as shown in Fig. 5. The inputs are the sensor signals for the both networks. Here, only the learning of actor ChNN that is largely different from the conventional reinforcement learning is explained comparing with the conventional one using Fig. 4. In our conventional works, as shown in Fig. 4(a), by adding a random number (noise) rndj,t to each actor output, the agent explores. The actor network is trained by BPTT (Back Propagation Through Time) using the product of the random number and TD error as the error for each output of the RNN.\nIn the proposedmethod, there is no external random numbers added to the actor outputs. The network is a kind of RNN, but by setting each feedback connection to a large random value, it can produce chaotic dynamics internally. Because the learning of recurrent connections does not work well, only the connections from inputs to hidden neurons and from hidden neurons to output neurons are trained. One variable Cji named causality trace is put on each connection, and takes in and maintains the input through the connection according to the change in its output as\nC [l] ji,t = (1\u2212 |\u2206x [l] j,t|)C [l] ji,t\u22121 +\u2206x [l] j,tx [l\u22121] i,t (1)\nwhere x [l] j,t: output of the j-th neuron in the l-th layer at time t,\u2206xt = xt \u2212 xt\u22121.\nUsing the causality trace Cji and TD error r\u0302t, the weight w [l] ji from the i-th neuron in (l\u22121)-th layer to the j-th neuron in the l-th layer is updated with a learning rate \u03b7 as\n\u2206w [l] ji,t = \u03b7r\u0302tC [l] ji,t. (2)\n5 Learning of Obstacle Avoidance\nSince the learning is completely different from the conventional one, it is necessary to show whether the learning works appropriately in a variety of tasks or not. It was already applied to several tasks[6][9]. In this paper, the result of a recent task in which a robot has two wheels and two visual sensors is shown[7]. As shown in Fig. 5, there is a 20\u00d720 size of field, and its center is the origin of the field. What the robot has to do is to reach a goal while avoiding an obstacle. The goal is put on the location (0, 8) with radius r=1.0, and a robot (r=0.5) and an obstacle (r=1.5) are put randomly at each episode. The orientation of the robot is also decided randomly. Each of the two omnidirectional visual sensors has 72 cells, and catches only goal or obstacle respectively. Total of 144 sensor signals are the inputs of both critic and actor networks, and the right and left wheels rotate according to the two actor outputs. When the robot comes in the goal area, a reward is given, and when it collides with the obstacle, a small penalty is given. One episode is defined as until arrival to the goal or 1000 steps from start. The both NNs have three layers including input layer. The number of hidden neurons is 10 for critic network and 100 for actor ChNN.\nThe learning results are shown in Fig. 6. Fig. 6(a) shows learning curve and the vertical axis indicates the number of steps to the goal. The blue line shows its average over each 100 episodes. It can be seen that according to the number of episodes, the number of steps decreases. Fig. 6(b) and (c) show the robot trajectories before and after learning respectively. Actually, the goal location\nis fixed and initial robot location is varied. But to see the initial robot orientation, the robot is on the center of the field, and its orientation is for upper-side initially on the figure. Relatively, the goal location is varied. The size of the obstacle in the figure shows the area where the robot collides with the obstacle and then is larger than the actual size. Before learning, the robot explored almost randomly, but after learning, it could reach the goal while avoiding the obstacle. In conventional learning, randomness of action selection is often decreased as learning progresses, but here, it can be seen that exploration factors decreased autonomously according to learning. However, there are some strange behaviors such\nthat the robot changes its direction suddenly. It was observed that the actor hidden neurons were likely to have a value around the maximum 0.5 or minimum -0.5. There is still a large space to improve the learning method.\nFig. 6(d) shows Lyapunov exponent to see the chaotic property of this system including the environment. The robot is located at one of the 8 locations in Fig. 6(c), and the obstacle is also located at one of 8 locations. For each of the 8 \u00d7 8 = 64 combinations, a small perturbation vector with the size dbefore is added to the internal states of hidden neurons, and the distance dafter in the internal states at the next time between the cases of no perturbation and addition of the perturbation is compared. The average of ln(dafter/dbefore) is used as the exponent here. From the figure, it can be seen that Lyapunov exponent is gradually decreased, but since the value is more than 0.0, it is considered that the chaotic property is maintained. In [6], it was observed that when the environment changed, Lyapunov exponent increased again."}], "references": [{"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A Huang"], "venue": "search, Nature,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Emergence of Intelligence through Reinforcement Learning ..., Advances in Reinforcement Learning, Intech, 99\u2013120", "author": ["K. Shibata"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Functions that Emerge through End-to-end Reinforcement Learning, RLDM 2017", "author": ["K. Shibata"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "A Variety of Communications that Emerge through Reinforcement Learning Using ..., RLDM 2017", "author": ["K. Shibata"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Reinforcement Learning with Internal-Dynamics-based Exploration Using a Chaotic Neural Network", "author": ["K. Shibata", "Y. Sakashita"], "venue": "Proc. of IJCNN (Int\u2019l Joint Conf. on Neural Networks)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Significance of Function Emergence Approach based on End-to-end Reinforcement Learning as suggested by Deep Learning, and Novel Reinforcement Learning Using a Chaotic ..", "author": ["K. Shibata", "Y. Goto"], "venue": "Cognitive Studies,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Emergence of Discrete and Abstract State ..", "author": ["Y Sawatsubashi"], "venue": "Robot Intelligence Tech. and App", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Emergence of Higher Exploration in Reinforcement Learning Using a Chaotic Neural Network", "author": ["Y. Goto", "K. Shibata"], "venue": "Neural Information Processing, Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Learning of Deterministic Exploration and Temporal Abstraction", "author": ["K. Shibata"], "venue": "Proc. of SICE-ICCAS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "How brains make chaos in order to make sense ..", "author": ["C.A. Skarda", "W.J. Freeman"], "venue": "Behavioral and Brain Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Successive Learning in hetero-associative memory using chaotic neural networks", "author": ["Y. Osana", "M. Hagiwara"], "venue": "Int. J. Neural Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Expectation for the emergence of artificial intelligence is growing these days triggered by the recent results in reinforcement learning (RL) using a deep neural network (NN)[1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 1, "context": "Our group has propounded for around 20 years that end-toend RL from sensors to motors using a recurrent NN (RNN) plays an important role for the emergence[3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 2, "context": "Our group has propounded for around 20 years that end-toend RL from sensors to motors using a recurrent NN (RNN) plays an important role for the emergence[3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 2, "context": "Our group has shown that not only recognition and motion, but also memory, prediction, individuality, and also similar activities to those in monkey brain at tool use emerge[4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "We have also shown that a variety of communications emerge in the same framework[5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "Then our hypothesis that \u201cexploration\u201d grows into \u201cthinking\u201d through learning is introduced[6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "To realize the hypothesis, the use of a chaotic NN (ChNN) in RL and new deterministic RL for it are introduced[6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 5, "context": "Finally, it is shown that the new RL works in a simple task[7] though that cannot be called \u201cthinking\u201d yet and there are still many rooms for improvement.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "As a kind of dynamic functions, we have shown that a variety of memory-required functions emerge in an RNN in simple tasks[4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "After learning, a large change in the internal state could be observed in some degree, but the learning was very difficult[8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "It is not motor(actuator)-level lower exploration, but higher exploration supported by some prior or learned knowledge[9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "The author\u2019s group has thought that exploration should be generated inside of a recurrent NN (RNN) that generates motion commands[10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "reported that the activities on the olfactory bulb in rabbits become chaotic for unknown stimuli[12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "have shown the difference of chaotic property between known and unknown patterns on an associative memory using a ChNN, and also after an unknown pattern is learned, association to the pattern is formed as well as the other known patterns[13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 4, "context": "In order to realize the above idea, our group has proposed new reinforcement learning using a ChNN[6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Figure 4: Comparison of conventional RL and proposed RL(only actor network) [6]", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "It was already applied to several tasks[6][9].", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "It was already applied to several tasks[6][9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "In this paper, the result of a recent task in which a robot has two wheels and two visual sensors is shown[7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "Figure 6: Learning results[7].", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "In [6], it was observed that when the environment changed, Lyapunov exponent increased again.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "Expectation for the emergence of higher functions is getting larger in the framework of end-to-end comprehensive reinforcement learning using a recurrent neural network. However, the emergence of \u201cthinking\u201d that is a typical higher function is difficult to realize because \u201cthinking\u201d needs non fixed-point, flow-type attractors with both convergence and transition dynamics. Furthermore, in order to introduce \u201cinspiration\u201d or \u201cdiscovery\u201d in \u201cthinking\u201d, not completely random but unexpected transition should be also required. By analogy to \u201cchaotic itinerancy\u201d, we have hypothesized that \u201cexploration\u201d grows into \u201cthinking\u201d through learning by forming flow-type attractors on chaotic random-like dynamics. It is expected that if rational dynamics are learned in a chaotic neural network (ChNN), coexistence of rational state transition, inspiration-like state transition and also random-like exploration for unknown situation can be realized. Based on the above idea, we have proposed new reinforcement learning using a ChNN. The positioning of exploration is completely different from the conventional one. Here, the chaotic dynamics inside the ChNN produces exploration factors by itself. Since external random numbers for stochastic action selection are not used, exploration factors cannot be isolated from the output. Therefore, the learning method is also completely different from the conventional one. One variable named causality trace is put at each connection, and takes in andmaintains the input through the connection according to the change in its output. Using these causality traces and TD error, the connection weights except for the feedback connections are updated in the actor ChNN. In this paper, as the result of a recent simple task to see whether the new learning works appropriately or not, it is shown that a robot with two wheels and two visual sensors reaches a target while avoiding an obstacle after learning though there are still many rooms for improvement.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}