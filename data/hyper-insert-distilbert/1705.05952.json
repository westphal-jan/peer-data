{"id": "1705.05952", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing", "abstract": "we present a novel neural virtual network model program that continuously learns pos tagging and graph - based dependency parsing steps jointly. our numerical model uses bidirectional lstms hardware to learn mutual feature representations shared for cisco both pos tagging and dependency spanning parsing software tasks, thus handling precisely the feature - engineering problem. after our ongoing extensive experiments, conducted on issue 19 modeling languages from springer the ie universal dependencies project, show that inherently our collaborative model outperforms the state - of - exactly the - same art hierarchical neural network - sensor based stack - path propagation collision model for binary joint threaded pos tagging implementations and uses transition - vertex based double dependency concurrency parsing, resulting in a promising new state ahead of mastering the computational art. our code is made open - source and available at :", "histories": [["v1", "Tue, 16 May 2017 23:09:00 GMT  (53kb,D)", "http://arxiv.org/abs/1705.05952v1", null], ["v2", "Thu, 8 Jun 2017 18:25:57 GMT  (58kb,D)", "http://arxiv.org/abs/1705.05952v2", "v2: also include universal POS tagging, UAS and LAS accuracies w.r.t gold-standard segmentation on Universal Dependencies 2.0 - CoNLL 2017 shared task test data; in CoNLL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dat quoc nguyen", "mark dras", "mark johnson"], "accepted": false, "id": "1705.05952"}, "pdf": {"name": "1705.05952.pdf", "metadata": {"source": "CRF", "title": "A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing", "authors": ["Dat Quoc Nguyen", "Mark Dras"], "emails": ["dat.nguyen@students.mq.edu.au,", "mark.johnson}@mq.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006 and 2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada and Matsumoto, 2003; Nivre, 2003). Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013). Recent work shows that using deep learning in dependency parsing has obtained state-of-the-art\n(SOTA) performances. Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural networkbased classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016). In addition, others propose novel neural architectures for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).\nPart-of-speech (POS) tags are essential features used in most dependency parsers. In real-world parsing, those parsers rely heavily on the use of automatically predicted POS tags, thus encountering error propagation problems. For example, Li et al. (2011), Straka et al. (2016) and Nguyen et al. (2016) show that parsing accuracies drop by 5+% when utilizing automatic POS tags instead of gold ones. Some attempts have been conducted to avoid using POS tags during dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), however, these approaches still additionally use the automatic POS tags to achieve the best accuracy. Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).\nIn this paper, we propose a novel neural architecture for joint POS tagging and graphbased dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM\u2014the bidirectional LSTM (Schuster\nar X\niv :1\n70 5.\n05 95\n2v 1\n[ cs\n.C L\n] 1\n6 M\nay 2\n01 7\nand Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than SOTA dependency parsers and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art."}, {"heading": "2 Our joint model", "text": "In this section, we describe our new model for joint POS tagging and dependency parsing, which we call jPTDP. Figure 1 illustrates the architecture of our new model. We learn shared latent feature vectors representing word tokens in an input sentence by using BiLSTMs. Then these shared feature vectors are further used to make the prediction of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs.\nBiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1, w2, ..., wn, we represent each word wi in s by an embedding e(\u2022)wi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So,\nwe also use a sequence BiLSTM (BiLSTMseq) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1c2...ck, the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the jth character cj in w; and the output is the character-based embedding e(\u2217)w of the word type w, computed as:\ne(\u2217)w = BiLSTMseq(c1:k)\nFor the ith word wi in the input sentence s, we create an input vector ei which is a concatenation (\u25e6) of the corresponding word embedding and character-based embedding vectors:\nei = e (\u2022) wi \u25e6 e (\u2217) wi\nThen, we feed the sequence of input vectors e1:n with an addition index i corresponding to a context position into another BiLSTM (BiLSTMctx), resulting in shared feature vectors vi representing the ith words wi in the sentence s:\nvi = BiLSTMctx(e1:n, i)\nPOS tagging: Using shared BiLSTM-based feature vectors, then we follow a common approach to compute the cross-entropy objective loss LPOS(t\u0302, t), in which t\u0302 and t are the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s, respectively (Goldberg, 2016; Plank et al., 2016).\nArc-factored graph-based parsing: Dependency trees can be formalized as directed graphs. An arc-factored parsing approach learns the scores of the arcs in a graph (Ku\u0308bler et al., 2009). Then, using an efficient decoding algorithm such as Eisner (1996)\u2019s algorithm, we can find a maximum spanning tree\u2014the highest scoring parse tree\u2014of the graph from those arc scores:\nscore(s) = argmax y\u0302\u2208Y(s) \u2211 (h,m)\u2208y\u0302 scorearc(h,m)\nwhere Y(s) is the set of all possible dependency trees for the input sentence s while scorearc(h,m) measures the score of the arc between the head hth word and the modifier mth word in s. Following Kiperwasser and Goldberg (2016b), we score an arc by using a MLP with one-node output layer (MLParc) on top of the BiLSTMctx:\nscorearc(h,m) = MLParc(vh \u25e6 vm) where vh and vm are the shared BiLSTM-based feature vectors representing the hth and mth words in s, respectively. We now compute a marginbased hinge loss Larc with loss-augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree (Kiperwasser and Goldberg, 2016b).\nDependency relation types are predicted in a similar manner. We use another MLP on top of the BiLSTMctx for predicting relation type of an head-modifier arc. Here, the number of the nodes in the output layer of this MLP (MLPrel) is the number of relation types. Given an arc (h,m), we compute a corresponding output vector as:\nv(h,m) = MLPrel(vh \u25e6 vm) Then, based on MLP output vectors v(h,m), we also compute another margin-based hinge loss Lrel for relation type prediction, using only the gold labeled parse tree.\nJoint model training: The final training objective function of our joint model is the sum of the POS tagging loss LPOS, the structure loss Larc and the relation labeling loss Lrel. The model parameters, including word embeddings, character embeddings, two BiLSTMs and two MLPs, are learned to minimize the sum of the losses.\nDiscussion: Prior neural network-based joint models for POS tagging and dependency parsing are feed-forward network- and transition-based approaches (Alberti et al., 2015; Zhang and Weiss,\n2016), while our model is BiLSTM- and graphbased method. Our model can be considered as a two-component mixture of a tagging component and a parsing component. Here, the tagging component can be viewed as a simplified version without the additional auxiliary loss for rare words of the BiLSTM-based POS tagging model proposed by Plank et al. (2016). The parsing component can be viewed as an extension of the graph-based dependency model proposed by Kiperwasser and Goldberg (2016b), where we replace the input POS tag embeddings by the character-based representations of words."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental setup", "text": "Following Zhang and Weiss (2016) and Plank et al. (2016), we conduct multilingual experiments on 19 languages from the Universal Dependencies (UD) treebanks1 v1.2, taking the universal POS tagset (Petrov et al., 2012) into account instead of the language specific POS tagset.2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS). The LAS score is the percentage of words which are correctly assigned both dependency arc and relation type."}, {"heading": "3.2 Implementation details", "text": "Our jPTDP is implemented using DYNET v2.0 (Neubig et al., 2017).3 We optimize the objective function using Adam (Kingma and Ba, 2014) with default DYNET parameter settings and no mini-batches. We use a fixed random seed, and we do not utilize pre-trained embeddings in any experiment. Following Kiperwasser and Goldberg (2016b) and Plank et al. (2016), we apply a word dropout rate of 0.25 and Gaussian noise with \u03c3 = 0.2. For training, we run for 30 epochs, and evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch. We perform a minimal grid search of hyper-parameters on English. We find that the highest mixed accuracy on the English development set is when using 64-dimensional character embeddings, 128-dimensional word embeddings,\n1http://universaldependencies.org/ 2 Zhang and Weiss (2016) and Plank et al. (2016) experimented on 19 and 22 languages, respectively. For consistency, we use 19 languages as in Zhang and Weiss (2016).\n3https://github.com/clab/dynet\nar bg da de\u2022 en es eu\u2022 fa fi\u2022 fr hi id it iw nl no pl\u2022 pt sl\u2022 AVG\n128-dimensional BiLSTM states, 2 BiLSTM layers and 100 hidden nodes in MLPs with one hidden layer.4 We then apply those hyper-parameters to all 18 remaining languages."}, {"heading": "3.3 Main results", "text": "Table 1 compares the POS tagging and dependency parsing results of our model jPTDP with results reported in prior work, using the same experimental setup.\nRegarding POS tagging, our joint model jPTDP generally obtains similar POS tagging accuracies to the SOTA BiLSTM-aux model (Plank et al., 2016). Our model also achieves higher averaged POS tagging accuracy than the SOTA joint model Stack-propagation (Zhang and Weiss, 2016). Note that there are slightly higher tagging results ob-\n4On English, it took 6 hours for training with these hyperparameters, and then obtaining a speed at 700 words/second, evaluated on a computer with 2.2 GHz Core i7 processor.\ntained by BiLSTM-aux when utilizing pre-trained word embeddings for initialization, as presented in Plank et al. (2016). However, for a fair comparison to both Stack-propagation and our jPTDP, we only compare to the results reported without using the pre-trained word embeddings.\nIn terms of dependency parsing, in most cases, our model jPTDP outperforms Stack-propagation. It is somewhat unexpected that our model produces about 7% absolute lower LAS score than Stack-propagation on Dutch (nl). A possible reason is that the hyper-parameters we selected on English are not optimal for Dutch. Another reason is due to a large number of non-projective trees in Dutch test set (106/386 \u2248 27.5%), while we use the Eisner\u2019s decoding algorithm, producing only projective trees (Eisner, 1996). Without taking \u201cnl\u201d into account, our averaged LAS score over all remaining languages is 1.1% absolute higher than Stack-propagation\u2019s.\nOne reason for our better LAS is probably because jPTDP uses character-based representations of words, while Stack-propagation uses latent feature representations for suffixes and prefixes which might not be as useful as character-based representations for capturing unknown words. The last row in Table 1 shows an absolute LAS improvement of 4.4% on average when comparing our jPTDP with its simplified version of not using character-based representations: specifically, morphologically rich languages get an averaged improvement of 9.3 %, vice versa 2.6% for others. So, our jPDTP is particularly good for morphologically rich languages, with 1.7% higher averaged LAS than Stack-propagation over these languages."}, {"heading": "4 Conclusion", "text": "In this paper, we describe our novel model for joint POS tagging and graph-based dependency parsing, using bidirectional LSTM-based feature representations. Multilingual experiments show that our model obtains state-of-the-art results in both POS tagging and dependency parsing. Our code is open-source and available together with pretrained models at: https://github.com/ datquocnguyen/jPTDP.\nAcknowledgments: The first author is supported by an International Postgraduate Research Scholarship\u2014which is an Australian Government Research Training Program Scholarship\u2014and a NICTA NRPA Top-Up Scholarship."}], "references": [{"title": "Improved Transition-Based Parsing and Tagging with Neural Networks", "author": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov."], "venue": "Proceedings of EMNLP. pages 1354\u20131359.", "citeRegEx": "Alberti et al\\.,? 2015", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Globally Normalized Transition-Based Neural Networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of ACL. pages 2442\u20132452.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of EMNLP. pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Very high accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proceedings of COLING. pages 89\u201397.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "A TransitionBased System for Joint Part-of-Speech Tagging", "author": ["Bernd Bohnet", "Joakim Nivre"], "venue": null, "citeRegEx": "Bohnet and Nivre.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "TnT: A Statistical Part-ofSpeech Tagger", "author": ["Thorsten Brants."], "venue": "Proceedings of ANLP. pages 224\u2013231.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of CoNLL. pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional Attention with Agreement for Dependency Parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "Proceedings of EMNLP. pages 2204\u20132214.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Transition-based dependency parsing with selectional branching", "author": ["Jinho D. Choi", "Andrew McCallum."], "venue": "Proceedings of ACL. pages 1052\u20131062.", "citeRegEx": "Choi and McCallum.,? 2013", "shortCiteRegEx": "Choi and McCallum.", "year": 2013}, {"title": "Deep Biaffine Attention for Neural Dependency Parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "Proceedings of ICLR.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL-IJCNLP. pages 334\u2013343.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three New Probabilistic Models for Dependency Parsing: An Exploration", "author": ["Jason M. Eisner."], "venue": "Proceedings of COLING. pages 340\u2013345.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "A Primer on Neural Network Models for Natural Language Processing", "author": ["Yoav Goldberg."], "venue": "Journal of Artificial Intelligence Research 57:345\u2013420.", "citeRegEx": "Goldberg.,? 2016", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Incremental Joint POS Tagging and Dependency Parsing in Chinese", "author": ["Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of IJCNLP", "citeRegEx": "Hatori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Joint part-of-speech and dependency projection from multiple sources", "author": ["Anders Johannsen", "\u017deljko Agi\u0107", "Anders S\u00f8gaard."], "venue": "Proceedings of ACL (Volume 2: Short Papers). pages 561\u2013566.", "citeRegEx": "Johannsen et al\\.,? 2016", "shortCiteRegEx": "Johannsen et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "EasyFirst Dependency Parsing with Hierarchical Tree LSTMs", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of ACL 4:445\u2013461.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016a", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of ACL 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016b", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Efficient ThirdOrder Dependency Parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of ACL. pages 1\u201311.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "Dependency Parsing", "author": ["Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre."], "venue": "Synthesis Lectures on Human Language Technologies, Morgan & cLaypool publishers.", "citeRegEx": "K\u00fcbler et al\\.,? 2009", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "Low-Rank Tensors for Scoring Dependency Structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of ACL (Volume 1: Long Papers). pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Joint Models for Chinese POS Tagging and Dependency Parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li."], "venue": "Proceedings of EMNLP. pages 1180\u20131191.", "citeRegEx": "Li et al\\.,? 2011", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Neural Probabilistic Model for Non-projective MST Parsing", "author": ["Xuezhe Ma", "Eduard H. Hovy."], "venue": "CoRR abs/1701.00874.", "citeRegEx": "Ma and Hovy.,? 2017", "shortCiteRegEx": "Ma and Hovy.", "year": 2017}, {"title": "Turning on the Turbo: Fast Third-Order NonProjective Turbo Parsers", "author": ["Andre Martins", "Miguel Almeida", "Noah A. Smith."], "venue": "Proceedings of ACL (Volume 2: Short Papers). pages 617\u2013622.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of ACL. pages 91\u201398.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Analyzing and integrating dependency parsers", "author": ["Ryan McDonald", "Joakim Nivre."], "venue": "Computational Linguistics 37(1):197\u2013230.", "citeRegEx": "McDonald and Nivre.,? 2011", "shortCiteRegEx": "McDonald and Nivre.", "year": 2011}, {"title": "Online Learning of Approximate Dependency Parsing Algorithms", "author": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of EACL. pages 81\u201388.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "An empirical study for Vietnamese dependency parsing", "author": ["Dat Quoc Nguyen", "Mark Dras", "Mark Johnson."], "venue": "Proceedings of ALTA.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of IWPT .", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "The CoNLL 2007 Shared Task on Dependency Parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.", "citeRegEx": "Nivre et al\\.,? 2007a", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "MaltParser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "G\u00fclsen Eryigit", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi."], "venue": "Natural Language Engineering", "citeRegEx": "Nivre et al\\.,? 2007b", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "An effective neural network model for graph-based", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": null, "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Deep Multitask Learning for Semantic Dependency Parsing", "author": ["Hao Peng", "Sam Thomson", "Noah A. Smith."], "venue": "Proceedings of ACL. page to appear.", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "A Universal Part-of-Speech Tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan T. McDonald."], "venue": "Proceedings of LREC. pages 2089\u20132096.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Adapting taggers to Twitter with not-so-distant supervision", "author": ["Barbara Plank", "Dirk Hovy", "Ryan McDonald", "Anders S\u00f8gaard."], "venue": "Proceedings of COLING: Technical Papers. pages 1783\u20131792.", "citeRegEx": "Plank et al\\.,? 2014", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of ACL (Volume 2: Short Papers). pages 412\u2013418.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Joint Chinese Word Segmentation, POS Tagging and Parsing", "author": ["Xian Qian", "Yang Liu."], "venue": "Proceedings of EMNLP-CoNLL. pages 501\u2013511.", "citeRegEx": "Qian and Liu.,? 2012", "shortCiteRegEx": "Qian and Liu.", "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal."], "venue": "IEEE Transactions on Signal Processing 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "UDPipe: Trainable Pipeline for Processing CoNLLU Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing", "author": ["Milan Straka", "Jan Hajic", "Jana Strakov."], "venue": "Proceedings of LREC.", "citeRegEx": "Straka et al\\.,? 2016", "shortCiteRegEx": "Straka et al\\.", "year": 2016}, {"title": "Graph-based Dependency Parsing with Bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proceedings of ACL (Volume 1: Long Papers). pages 2306\u20132315.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "Proceedings of ACL (Volume 1: Long Papers). pages 733\u2013742.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of ACLIJCNLP. pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings IWPT .", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "Randomized greedy inference for joint segmentation, pos tagging and dependency parsing", "author": ["Yuan Zhang", "Chengtao Li", "Regina Barzilay", "Kareem Darwish."], "venue": "Proceedings of NAACL-HLT .", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Stackpropagation: Improved Representation Learning for Syntax", "author": ["Yuan Zhang", "David Weiss."], "venue": "Proceedings of ACL (Volume 1: Long Papers). pages 1557\u20131566.", "citeRegEx": "Zhang and Weiss.,? 2016", "shortCiteRegEx": "Zhang and Weiss.", "year": 2016}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings ACL-HLT . pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}, {"title": "Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network", "author": ["Zhisong Zhang", "Hai Zhao", "Lianhui Qin."], "venue": "Proceedings of ACL (Volume 1: Long Papers). pages 1382\u20131392.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006 and 2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a).", "startOffset": 176, "endOffset": 223}, {"referenceID": 32, "context": "Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006 and 2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a).", "startOffset": 176, "endOffset": 223}, {"referenceID": 12, "context": "McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada and Matsumoto, 2003; Nivre, 2003).", "startOffset": 121, "endOffset": 181}, {"referenceID": 26, "context": "McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada and Matsumoto, 2003; Nivre, 2003).", "startOffset": 121, "endOffset": 181}, {"referenceID": 20, "context": "McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada and Matsumoto, 2003; Nivre, 2003).", "startOffset": 121, "endOffset": 181}, {"referenceID": 45, "context": ", 2005; Koo and Collins, 2010) and transition-based approaches (Yamada and Matsumoto, 2003; Nivre, 2003).", "startOffset": 63, "endOffset": 104}, {"referenceID": 31, "context": ", 2005; Koo and Collins, 2010) and transition-based approaches (Yamada and Matsumoto, 2003; Nivre, 2003).", "startOffset": 63, "endOffset": 104}, {"referenceID": 28, "context": "Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013).", "startOffset": 155, "endOffset": 288}, {"referenceID": 33, "context": "Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013).", "startOffset": 155, "endOffset": 288}, {"referenceID": 3, "context": "Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013).", "startOffset": 155, "endOffset": 288}, {"referenceID": 48, "context": "Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013).", "startOffset": 155, "endOffset": 288}, {"referenceID": 25, "context": "Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013).", "startOffset": 155, "endOffset": 288}, {"referenceID": 9, "context": "Most traditional graph- or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013).", "startOffset": 155, "endOffset": 288}, {"referenceID": 7, "context": "Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural networkbased classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016).", "startOffset": 137, "endOffset": 219}, {"referenceID": 44, "context": "Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural networkbased classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016).", "startOffset": 137, "endOffset": 219}, {"referenceID": 34, "context": "Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural networkbased classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016).", "startOffset": 137, "endOffset": 219}, {"referenceID": 1, "context": "Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural networkbased classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016).", "startOffset": 137, "endOffset": 219}, {"referenceID": 4, "context": "Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006 and 2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al.", "startOffset": 177, "endOffset": 251}, {"referenceID": 11, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 8, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 49, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 42, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 10, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 24, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 35, "context": "for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017).", "startOffset": 42, "endOffset": 221}, {"referenceID": 23, "context": "For example, Li et al. (2011), Straka et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 23, "context": "For example, Li et al. (2011), Straka et al. (2016) and Nguyen", "startOffset": 13, "endOffset": 52}, {"referenceID": 11, "context": "Some attempts have been conducted to avoid using POS tags during dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), however, these approaches still additionally use the automatic POS tags to achieve the best accuracy.", "startOffset": 84, "endOffset": 129}, {"referenceID": 2, "context": "Some attempts have been conducted to avoid using POS tags during dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), however, these approaches still additionally use the automatic POS tags to achieve the best accuracy.", "startOffset": 84, "endOffset": 129}, {"referenceID": 14, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 23, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 4, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 39, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 43, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 46, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 0, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 16, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 47, "context": "Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS ambiguities (Hatori et al., 2011; Li et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016).", "startOffset": 259, "endOffset": 450}, {"referenceID": 47, "context": "and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art.", "startOffset": 40, "endOffset": 63}, {"referenceID": 36, "context": "Plank et al. (2016) and Ballesteros et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "(2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances.", "startOffset": 11, "endOffset": 37}, {"referenceID": 13, "context": "POS tagging: Using shared BiLSTM-based feature vectors, then we follow a common approach to compute the cross-entropy objective loss LPOS(t\u0302, t), in which t\u0302 and t are the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s, respectively (Goldberg, 2016; Plank et al., 2016).", "startOffset": 280, "endOffset": 316}, {"referenceID": 38, "context": "POS tagging: Using shared BiLSTM-based feature vectors, then we follow a common approach to compute the cross-entropy objective loss LPOS(t\u0302, t), in which t\u0302 and t are the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s, respectively (Goldberg, 2016; Plank et al., 2016).", "startOffset": 280, "endOffset": 316}, {"referenceID": 21, "context": "An arc-factored parsing approach learns the scores of the arcs in a graph (K\u00fcbler et al., 2009).", "startOffset": 74, "endOffset": 95}, {"referenceID": 12, "context": "Then, using an efficient decoding algorithm such as Eisner (1996)\u2019s algorithm, we can find a maximum spanning tree\u2014the highest scoring parse tree\u2014of the graph from those arc scores:", "startOffset": 52, "endOffset": 66}, {"referenceID": 13, "context": "Following Kiperwasser and Goldberg (2016b), we score an arc by using a MLP with one-node output layer", "startOffset": 26, "endOffset": 43}, {"referenceID": 19, "context": "We now compute a marginbased hinge loss Larc with loss-augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree (Kiperwasser and Goldberg, 2016b).", "startOffset": 175, "endOffset": 208}, {"referenceID": 0, "context": "Discussion: Prior neural network-based joint models for POS tagging and dependency parsing are feed-forward network- and transition-based approaches (Alberti et al., 2015; Zhang and Weiss, 2016), while our model is BiLSTM- and graphbased method.", "startOffset": 149, "endOffset": 194}, {"referenceID": 47, "context": "Discussion: Prior neural network-based joint models for POS tagging and dependency parsing are feed-forward network- and transition-based approaches (Alberti et al., 2015; Zhang and Weiss, 2016), while our model is BiLSTM- and graphbased method.", "startOffset": 149, "endOffset": 194}, {"referenceID": 0, "context": "Discussion: Prior neural network-based joint models for POS tagging and dependency parsing are feed-forward network- and transition-based approaches (Alberti et al., 2015; Zhang and Weiss, 2016), while our model is BiLSTM- and graphbased method. Our model can be considered as a two-component mixture of a tagging component and a parsing component. Here, the tagging component can be viewed as a simplified version without the additional auxiliary loss for rare words of the BiLSTM-based POS tagging model proposed by Plank et al. (2016). The parsing component can be viewed as an extension of the graph-based dependency model proposed by Kiperwasser and Goldberg (2016b), where we replace the input POS tag embeddings by the character-based representations of words.", "startOffset": 150, "endOffset": 538}, {"referenceID": 0, "context": "Discussion: Prior neural network-based joint models for POS tagging and dependency parsing are feed-forward network- and transition-based approaches (Alberti et al., 2015; Zhang and Weiss, 2016), while our model is BiLSTM- and graphbased method. Our model can be considered as a two-component mixture of a tagging component and a parsing component. Here, the tagging component can be viewed as a simplified version without the additional auxiliary loss for rare words of the BiLSTM-based POS tagging model proposed by Plank et al. (2016). The parsing component can be viewed as an extension of the graph-based dependency model proposed by Kiperwasser and Goldberg (2016b), where we replace the input POS tag embeddings by the character-based representations of words.", "startOffset": 150, "endOffset": 672}, {"referenceID": 47, "context": "Following Zhang and Weiss (2016) and Plank", "startOffset": 10, "endOffset": 33}, {"referenceID": 36, "context": "2, taking the universal POS tagset (Petrov et al., 2012) into account instead of the language specific POS tagset.", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": "3 We optimize the objective function using Adam (Kingma and Ba, 2014) with default DYNET parameter settings and no mini-batches.", "startOffset": 48, "endOffset": 69}, {"referenceID": 13, "context": "Following Kiperwasser and Goldberg (2016b) and Plank et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 13, "context": "Following Kiperwasser and Goldberg (2016b) and Plank et al. (2016), we apply a word dropout rate of 0.", "startOffset": 26, "endOffset": 67}, {"referenceID": 45, "context": "org/ 2 Zhang and Weiss (2016) and Plank et al.", "startOffset": 7, "endOffset": 30}, {"referenceID": 37, "context": "org/ 2 Zhang and Weiss (2016) and Plank et al. (2016) experimented on 19 and 22 languages, respectively.", "startOffset": 34, "endOffset": 54}, {"referenceID": 37, "context": "org/ 2 Zhang and Weiss (2016) and Plank et al. (2016) experimented on 19 and 22 languages, respectively. For consistency, we use 19 languages as in Zhang and Weiss (2016). https://github.", "startOffset": 34, "endOffset": 171}, {"referenceID": 41, "context": "UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016).", "startOffset": 62, "endOffset": 83}, {"referenceID": 5, "context": "TnT denotes the second order HMM-based TnT tagger (Brants, 2000).", "startOffset": 50, "endOffset": 64}, {"referenceID": 38, "context": "BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTM-based POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016).", "startOffset": 129, "endOffset": 149}, {"referenceID": 47, "context": "Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016).", "startOffset": 116, "endOffset": 139}, {"referenceID": 2, "context": "B\u201915 denotes the character-based stack LSTM model for transition-based dependency parsing (Ballesteros et al., 2015).", "startOffset": 90, "endOffset": 116}, {"referenceID": 3, "context": "TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTM-based POS tagging model with an additional auxiliary loss for rare words (Plank et al.", "startOffset": 51, "endOffset": 155}, {"referenceID": 3, "context": "TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTM-based POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew \u201ciw\u201d is referred to as \u201che\u201d as in Plank et al. (2016). [\u2295]: Results are reported in Plank et al.", "startOffset": 51, "endOffset": 406}, {"referenceID": 3, "context": "TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTM-based POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew \u201ciw\u201d is referred to as \u201che\u201d as in Plank et al. (2016). [\u2295]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016).", "startOffset": 51, "endOffset": 456}, {"referenceID": 0, "context": "PipelinePtag refers to a greedy version of the approach proposed by Alberti et al. (2015). RBGParser refers to the SOTA graph-based dependency parser with tensor decomposition, presented in Lei et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 0, "context": "PipelinePtag refers to a greedy version of the approach proposed by Alberti et al. (2015). RBGParser refers to the SOTA graph-based dependency parser with tensor decomposition, presented in Lei et al. (2014). [*]: Results are reported in Zhang and Weiss (2016).", "startOffset": 68, "endOffset": 208}, {"referenceID": 0, "context": "PipelinePtag refers to a greedy version of the approach proposed by Alberti et al. (2015). RBGParser refers to the SOTA graph-based dependency parser with tensor decomposition, presented in Lei et al. (2014). [*]: Results are reported in Zhang and Weiss (2016).", "startOffset": 68, "endOffset": 261}, {"referenceID": 38, "context": "Regarding POS tagging, our joint model jPTDP generally obtains similar POS tagging accuracies to the SOTA BiLSTM-aux model (Plank et al., 2016).", "startOffset": 123, "endOffset": 143}, {"referenceID": 47, "context": "Our model also achieves higher averaged POS tagging accuracy than the SOTA joint model Stack-propagation (Zhang and Weiss, 2016).", "startOffset": 105, "endOffset": 128}, {"referenceID": 37, "context": "tained by BiLSTM-aux when utilizing pre-trained word embeddings for initialization, as presented in Plank et al. (2016). However, for a fair comparison to both Stack-propagation and our jPTDP, we only compare to the results reported without using the pre-trained word embeddings.", "startOffset": 100, "endOffset": 120}, {"referenceID": 12, "context": "5%), while we use the Eisner\u2019s decoding algorithm, producing only projective trees (Eisner, 1996).", "startOffset": 83, "endOffset": 97}], "year": 2017, "abstractText": "We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural networkbased Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art. Our code is open-source and available at: https://github.com/ datquocnguyen/jPTDP.", "creator": "LaTeX with hyperref package"}}}