{"id": "1705.06106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models", "abstract": "we present a semi - supervised complexity way used of training a character - graph based encoder - decoder coupled recurrent neural telegraph network for morphological reinflection, the task of generating one single inflected word word form sequence from possibly another. moreover this activity is easily achieved independently by using unlabeled tokens or collecting random string as descriptive training data for an easier autoencoding task, adapting itself a network for morphological morphological reinflection, and performing advanced multi - method task turing training. we only thus use successively limited domain labeled data more visually effectively, obtaining rates up to 9. 9 %, improvement % over state - systems of -'the - year art artificial baselines for 8 progressively different everyday languages.", "histories": [["v1", "Wed, 17 May 2017 11:48:15 GMT  (192kb,D)", "http://arxiv.org/abs/1705.06106v1", null], ["v2", "Fri, 21 Jul 2017 13:02:20 GMT  (460kb,D)", "http://arxiv.org/abs/1705.06106v2", "Accepted at SCLeM 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katharina kann", "hinrich sch\\\"utze"], "accepted": false, "id": "1705.06106"}, "pdf": {"name": "1705.06106.pdf", "metadata": {"source": "CRF", "title": "Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models", "authors": ["Katharina Kann"], "emails": ["kann@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "Morphologically rich languages use inflection\u2014 the adaptation of a surface form to its syntactic context\u2014to mark the properties of a word, e.g., gender or number of nouns or tense of verbs. This drastically increases the type-token ratio, and thus negatively effects natural language processing (NLP), making morphological analysis and generation an important field of research.\nIn this work, we focus on morphological reinflection (MRI), the task of mapping one inflected form of a lemma to another, given the morphological properties of the target, e.g., (smiling, PastPart) \u2192 smiled. The lemma does not have to be known. Recently, there have been some advances on the topic, motivated by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016). Neural sequence-to-sequence models, specifically attention-based encoder-decoder models, outperformed all other approaches (Faruqui et al., 2016; Kann and Schu\u0308tze, 2016). However, those models require a lot of training data, while in con-\ntrast many morphologically rich languages are low-resource, and little work has been done so far on neural models for morphology in settings with limited training data. This makes sequenceto-sequence models not applicable to morphological generation in most languages. An abundance of unlabeled data, in contrast, can be assumed available for each language in the focus of NLP. Thus, we propose a semi-supervised training method for a state-of-the-art encoder-decoder network for MRI using both labeled and unlabeled data, mitigating the need for time-expensive annotations. We achieve this by treating unlabeled words as training samples for an autoencoding (Vincent et al., 2010) task and multi-task training (cf. Figure 1). We intuit the following reasons why this should be beneficial: (i) The decoder\u2019s character language model can be trained using unlabeled data. (ii) Training on a second task reduces the problem of overfitting. (iii) By forcing the model to additionally learn autoencoding, we give it a strong prior to copy the input string. This might be advantageous as often many forms of a paradigm share the same stem, e.g., smiling and smiled. In order to investigate the importance of the latter, we further experiment with autoencoding of random strings and find that for our experimental settings and non-templatic languages the performance gain is comparable to using corpus words."}, {"heading": "2 Model Description", "text": "The log-likelihood for joint training on the tasks of MRI and autoencoding is:\nar X\niv :1\n70 5.\n06 10\n6v 1\n[ cs\n.C L\n] 1\n7 M\nay 2\n01 7\nL(\u03b8)= \u2211\n(l,src,trg)\u2208T\nlog p\u03b8 (ftrg(l) | fsrc(l)) (1)\n+ w \u2211 w\u2208W log p\u03b8(w | w),\nwith T being the MRI training data, w a weighting factor, fsrc(l) and ftrg(l) being the source form and target form of a given lemma l, respectively, andW being a set of words in the language of the system. The parameters \u03b8 are shared across the two tasks, resulting in a share of information. We obtain this by giving our model data from both sets at the same time, and marking each sample with a task-specific input symbol, cf. Figure 1. Following (Kann and Schu\u0308tze, 2016), we employ a neural encoder-decoder model.\nEncoder. For the input of the encoder, we adapt the format by Kann and Schu\u0308tze (2016), but modify it to be able to handle unlabeled data: Given the set of morphological subtags M each target tag is composed of (e.g., the tag 1SgPresInd contains the subtags 1, Sg, Pres and Ind), and the alphabet \u03a3 of the language of application, our input is of the form B[A/M\u2217]\u03a3\u2217E, i.e., it consists of either a sequence of subtags or the symbol A signaling that the input is not annotated and should be autoencoded, and (in both cases) the character sequence of the input word. B and E are start and end symbols. Each part of the input is represented by an embedding.\nWe then encode the input x = x1, x2, . . . , xTx using a bidirectional gated recurrent neural network (GRU) (Cho et al., 2014b), i.e., \u2212\u2192 h i =\nf (\u2212\u2192 h i\u22121, xi ) and \u2190\u2212 h i = f (\u2190\u2212 h i+1, xi ) , with f being the update function of the hidden layer. Forward and backward hidden states are concatenated to obtain the input hi for the decoder.\nDecoder. The decoder is an attentionbased GRU, defining a probability distribution over strings in \u03a3\u2217:\np(y | x) = Ty\u220f t=1 p(yt | y1, . . . , yt\u22121, st, ct),\nwith st being the decoder hidden state for time t and ct being a context vector, calculated using the encoder hidden states together with attention weights. A detailed description of the model can be found in Bahdanau et al. (2015)."}, {"heading": "3 Experiments", "text": "Dataset. We experiment on the Task 3 dataset of the SIGMORPHON 2016 shared task on MRI (Cotterell et al., 2016). and all standard languages provided: Arabic, Finnish, Georgian, German, Navajo, Russian, Spanish and Turkish. German, Spanish and Russian are suffixing and exhibit stem changes. Russian differs from the other two in that those stem changes are consonantal and not vocalic. Finnish and Turkish are agglutinating, almost exclusively suffixing and have vowel harmony systems. Georgian uses both prefixiation and suffixiation. In contrast, Navajo mainly makes use of prefixes with consonant harmony among its sibilants. Finally, Arabic is a templatic, nonconcatenative language.\nFor each language, we further add randomly sampled words from the respective Wikipedia dumps. We exclude tokens that are not exclusively composed from characters of the language\u2019s alphabet, e.g., digits, or do not appear at least 2 times in the corpus. The exact amount of unlabaled data added is treated as a hyperparameter depending on the number of available annotated samples and optimized on the development set, cf. Section 4.1. Evaluation is done on the official shared task test set.\nTraining, hyperparameters and evaluation. We mainly adopt the hyperparameters of (Kann and Schu\u0308tze, 2016). Embedding are 300- dimensional, the size of all hidden layers is 100 and for training we use ADADELTA (Zeiler, 2012) with a batch size of 20. We train all models which use 18 or more of the labeled data for 200 epochs, and models that see 116 and 1 32 of the original data for 400 and 800 epochs, respectively. In all cases, we apply the last model for testing.\nWe evaluate using two metrics: accuracy and edit distance. Accuracy reports the percentage of completely correct solutions, while the edit distance between the system\u2019s guess and the gold solution gives credit to systems that produce forms that are close to the right form.\nBaselines. We compare our system to 2 baselines: The first is MED1, the winning system of the 2016 shared task. The network architecture is the same as for our system, but is is trained exclusively on labeled data. Thus, we expect it to suffer stronger from a lack of ressources.\n1http://cistern.cis.lmu.de/med/\nThe second baseline is the official baseline 2016 SIGMORPHON shared task baseline (SIG16) (Cotterell et al., 2016), which is similar in spirit to the system described by Nicolai et al. (2015). The system treats the prediction of edit operations to be performed on the input string as a sequential decision-making problem, greedily choosing each edit action given the previously chosen actions. The selection of operations is made by an averaged perceptron, using the binary features described in (Cotterell et al., 2016).2\nThird, we compare to the baseline system of the 2017 CoNLL Shared Task on Universal Morphological Reinflection3 (SIG17), which is extremely suitable for low-ressource settings. It splits all training lemmas and training forms into prefix, middle part and suffix, and uses those to find prefix or suffix substitution rules. Every evaluation sample is searched for the longest contained prefix or suffix and the rule belonging to the affix and given target tag is applied to obtain the output.\nResults and discussion. As shown in Table 1, additionally training on unlabeled samples improves the performance of the encoder-decoder network for nearly all settings and languages, especially for the very low-resource scenarios with 1 16 and 1 32 of the training data. Biggest increase in accuracy can be seen for Russian and Spanish, both in the 132 setting, with 0.9630 (0.5023 \u2212 0.4060) and 0.9920 (0.7564 \u2212 0.6572), respectively. For the settings with bigger amounts of training data available, the unlabeled data does not change performance a lot. This was expected, as the model already gets enough information from the annotated data. However, semi-supervised training never hurts performance, and can thus al-\n2Note that our use of the system differs from the official baseline in that we perform a direct form-to-form mapping. The shared task system predicts first form-to-lemma and then lemma-to-form. However, we assume no lemmas to be given, and thus are unable to train such a system.\n3https://github.com/sigmorphon/conll2017\nways be employed. Overall, our semi-supervised training method shows to be a useful extension of the original system.\nFurthermore, there is only one case\u2014Georgian, 1 16\u2014where any of the SIGMORPHON baselines outperforms the neural methods. This clearly shows the superiority of neural networks for the task and emphasizes the need to reduce the amount of labeled training data required for their training."}, {"heading": "4 Analyses", "text": ""}, {"heading": "4.1 Amount of Unlabeled Data", "text": "We now consider the amount of unlabeled samples a function of the number of annotated samples. Data and training regime are the same as in Section 3. This analysis is performed on the development set and we report the highest accuracy obtained during training.\nThe resulting accuracies for Arabic and German can be seen in Figure 2. The other languages behave similarly to German. The loss of performance for reducing the training data varies a lot between languages, depending on how regular and thus \u201deasy to learn\u201d those are. Concerning the amount of unlabeled samples, it seems that even though in single cases other ratios are slightly better, using 4 unlabeled samples for each labeled sample seems to be a good general rule. The only exception is Arabic in the 132 setting, where using half as many unlabeled as labeled samples ob-\ntains much better results. We explain this with the Semitic language being templatic. Since words in Arabic paradigms do not share a connected stem, we expect that giving the model too much bias to copy might be harming performance in lowresource settings. However, even for low-resource Arabic using a ratio of 1:4 of labeled and unlabeled samples still yields a better performance than not using unlabeled samples at all. Thus, we can conclude that if aiming for a languageindependent setup, this is a good ratio."}, {"heading": "4.2 Autoencoding of Random Strings", "text": "We expect the network to benefit from a bias to copy strings. This suggests that any random combination of characters from the language\u2019s alphabet could be autoencoded in order to improve the performance in low-resource settings. To verify this, we train the model on new datasets with 132 of the labeled samples from the SIGMORPHON task 3 and the optimal number of unlabeled samples for each language, cf. \u00a74.1. However, the unlabeled samples are now random strings of a length between 3 and 20. All models are trained as before. Accuracies on the official test set are shown in Table 2, and compared to (i) training without unlabeled samples and (i) the data enhanced by corpus words. Several aspects of the results are eyecatching. First, for Arabic the gap to the performance with corpus words is the biggest, showing that indeed the tendency of languages to copy the stem when inflecting is playing an important role. Second, for some languages the performance gains for corpus words and random words are comparable. This shows that random string can be very effective as well. Third, the performance of random strings is closer to the performance of corpus words the higher the overall accuracy is. The additional unlabeled samples might be acting as regularizers in this case.\nOverall, this experiment shows clearly that giving a bias to the model to copy strings helps for inflection in non-templatic languages, and that autoencoding random strings can improve the performance of a network for MRI."}, {"heading": "5 Related Work", "text": "For the SIGMORPHON 2016 shared task (Cotterell et al., 2016), multiple MRI systems have been developed, e.g., (Nicolai et al., 2016; Taji et al., 2016; Kann and Schu\u0308tze, 2016; Aharoni et al., 2016; O\u0308stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al. (2016). Those approaches differ from ours, due to a fundamental difference between the two tasks: For MRI, the source vocabulary and the target vocabulary are mostly the same. This makes it intuitive for MRI to train the final model jointly on MRI and autoencoding. One case where this has been done is the work by Ha et al. (2016). However, they apply their method exclusively to MT."}, {"heading": "6 Conclusion", "text": "We presented a way of semi-supervised training of a state-of-the-art model for low-resource MRI, using words from an unlabeled corpus. We found that the best ratio of labeled and unlabeled data depends of the morphological typology of the language. Finally, we showed that autoencoding random strings also increases performance, for some languages as much as using corpus words."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the financial support of Siemens for this research."}], "references": [{"title": "Improving sequence to sequence learning for morphological inflection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinflection", "author": ["Roee Aharoni", "Yoav Goldberg", "Yonatan Belinkov."], "venue": "SIGMORPHON.", "citeRegEx": "Aharoni et al\\.,? 2016", "shortCiteRegEx": "Aharoni et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1606.04596 .", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "SSST .", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The SIGMORPHON 2016 shared task\u2014 morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "SIGMORPHON.", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "TSLP 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Graphical models over multiple strings", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "EMNLP.", "citeRegEx": "Dreyer and Eisner.,? 2009", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2009}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "EMNLP.", "citeRegEx": "Dreyer and Eisner.,? 2011", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2011}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "NAACL.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "NAACL.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["John Goldsmith."], "venue": "Computational linguistics 27(2):153\u2013198.", "citeRegEx": "Goldsmith.,? 2001", "shortCiteRegEx": "Goldsmith.", "year": 2001}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "author": ["Thanh-Le Ha", "Jan Niehues", "Alexander Waibel."], "venue": "arXiv preprint arXiv:1611.04798 .", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "One-shot neural cross-lingual transfer for paradig completion", "author": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Kann et al\\.,? 2017", "shortCiteRegEx": "Kann et al\\.", "year": 2017}, {"title": "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "ACL.", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak."], "venue": "NAACL.", "citeRegEx": "Nicolai et al\\.,? 2015", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Morphological reinflection via discriminative string transduction", "author": ["Garrett Nicolai", "Bradley Hauer", "Adam St Arnaud", "Grzegorz Kondrak."], "venue": "SIGMORPHON.", "citeRegEx": "Nicolai et al\\.,? 2016", "shortCiteRegEx": "Nicolai et al\\.", "year": 2016}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert \u00d6stling."], "venue": "SIGMORPHON.", "citeRegEx": "\u00d6stling.,? 2016", "shortCiteRegEx": "\u00d6stling.", "year": 2016}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Hoifung Poon", "Colin Cherry", "Kristina Toutanova."], "venue": "NAACL.", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Unsupervised pretraining for sequence to sequence learning", "author": ["Prajit Ramachandran", "Peter J Liu", "Quoc V Le."], "venue": "arXiv preprint arXiv:1611.02683 .", "citeRegEx": "Ramachandran et al\\.,? 2016", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2016}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ACL.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The columbia university - new york university abu dhabi SIGMORPHON 2016 morphological reinflection shared task submission", "author": ["Dima Taji", "Ramy Eskander", "Nizar Habash", "Owen Rambow."], "venue": "SIGMORPHON.", "citeRegEx": "Taji et al\\.,? 2016", "shortCiteRegEx": "Taji et al\\.", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Journal of Machine Learning", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Recently, there have been some advances on the topic, motivated by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016).", "startOffset": 129, "endOffset": 153}, {"referenceID": 10, "context": "Neural sequence-to-sequence models, specifically attention-based encoder-decoder models, outperformed all other approaches (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016).", "startOffset": 123, "endOffset": 169}, {"referenceID": 14, "context": "Neural sequence-to-sequence models, specifically attention-based encoder-decoder models, outperformed all other approaches (Faruqui et al., 2016; Kann and Sch\u00fctze, 2016).", "startOffset": 123, "endOffset": 169}, {"referenceID": 24, "context": "We achieve this by treating unlabeled words as training samples for an autoencoding (Vincent et al., 2010) task and multi-task training (cf.", "startOffset": 84, "endOffset": 106}, {"referenceID": 14, "context": "Following (Kann and Sch\u00fctze, 2016), we employ a neural encoder-decoder model.", "startOffset": 10, "endOffset": 34}, {"referenceID": 14, "context": "For the input of the encoder, we adapt the format by Kann and Sch\u00fctze (2016), but modify it to be able to handle unlabeled data: Given the set of morphological subtags M each target tag is composed of (e.", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "work (GRU) (Cho et al., 2014b), i.", "startOffset": 11, "endOffset": 30}, {"referenceID": 1, "context": "A detailed description of the model can be found in Bahdanau et al. (2015). 3 Experiments", "startOffset": 52, "endOffset": 75}, {"referenceID": 5, "context": "We experiment on the Task 3 dataset of the SIGMORPHON 2016 shared task on MRI (Cotterell et al., 2016).", "startOffset": 78, "endOffset": 102}, {"referenceID": 14, "context": "We mainly adopt the hyperparameters of (Kann and Sch\u00fctze, 2016).", "startOffset": 39, "endOffset": 63}, {"referenceID": 25, "context": "Embedding are 300dimensional, the size of all hidden layers is 100 and for training we use ADADELTA (Zeiler, 2012) with a batch size of 20.", "startOffset": 100, "endOffset": 114}, {"referenceID": 5, "context": "The second baseline is the official baseline 2016 SIGMORPHON shared task baseline (SIG16) (Cotterell et al., 2016), which is similar in spirit to the system described by Nicolai et al.", "startOffset": 90, "endOffset": 114}, {"referenceID": 5, "context": "The selection of operations is made by an averaged perceptron, using the binary features described in (Cotterell et al., 2016).", "startOffset": 102, "endOffset": 126}, {"referenceID": 5, "context": "The second baseline is the official baseline 2016 SIGMORPHON shared task baseline (SIG16) (Cotterell et al., 2016), which is similar in spirit to the system described by Nicolai et al. (2015). The system treats the prediction of edit operations to be performed on the input string as a sequential decision-making problem, greedily choosing each edit action given the previously chosen actions.", "startOffset": 91, "endOffset": 192}, {"referenceID": 14, "context": "Table 2: Accuracies for MED (Kann and Sch\u00fctze (2016)), MED+corpus and MED+random.", "startOffset": 29, "endOffset": 53}, {"referenceID": 5, "context": "For the SIGMORPHON 2016 shared task (Cotterell et al., 2016), multiple MRI systems have been developed, e.", "startOffset": 36, "endOffset": 60}, {"referenceID": 16, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 23, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 14, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 0, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 17, "context": ", (Nicolai et al., 2016; Taji et al., 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016).", "startOffset": 2, "endOffset": 104}, {"referenceID": 3, "context": "Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work.", "startOffset": 32, "endOffset": 98}, {"referenceID": 22, "context": "Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work.", "startOffset": 32, "endOffset": 98}, {"referenceID": 1, "context": "Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work.", "startOffset": 32, "endOffset": 98}, {"referenceID": 10, "context": "Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013).", "startOffset": 45, "endOffset": 115}, {"referenceID": 15, "context": "Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013).", "startOffset": 45, "endOffset": 115}, {"referenceID": 9, "context": "Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013).", "startOffset": 45, "endOffset": 115}, {"referenceID": 7, "context": ", (Dreyer and Eisner, 2009).", "startOffset": 2, "endOffset": 27}, {"referenceID": 8, "context": ", Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008).", "startOffset": 140, "endOffset": 211}, {"referenceID": 18, "context": ", Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008).", "startOffset": 140, "endOffset": 211}, {"referenceID": 20, "context": ", Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008).", "startOffset": 140, "endOffset": 211}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al.", "startOffset": 32, "endOffset": 550}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al.", "startOffset": 32, "endOffset": 571}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language.", "startOffset": 32, "endOffset": 909}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al.", "startOffset": 32, "endOffset": 1310}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al.", "startOffset": 32, "endOffset": 1333}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al.", "startOffset": 32, "endOffset": 1355}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al. (2016). Those approaches differ from ours, due to a fundamental difference between the two tasks: For MRI, the source vocabulary and the target vocabulary are mostly the same.", "startOffset": 32, "endOffset": 1383}, {"referenceID": 0, "context": ", 2016; Kann and Sch\u00fctze, 2016; Aharoni et al., 2016; \u00d6stling, 2016). Encoder-decoder neural networks (Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2015) perform best, such that we extend them for this work. Earlier work on paradigm completion includes (Faruqui et al., 2016; Nicolai et al., 2015; Durrett and DeNero, 2013). Work directly tackling MRI is more rare, e.g., (Dreyer and Eisner, 2009). Our work relates to the line of research on minimally supervised and unsupervised methods for morphology, e.g., Creutz and Lagus (2007) and Goldsmith (2001) presenting the unsupervised morphological segmentation systems Morfessor and Linguistica, or (Dreyer and Eisner, 2011; Poon et al., 2009; Snyder and Barzilay, 2008). However, none of those focus directly on MRI or on training neural networks for morphology. The only case we know of where this has been done is work by Kann et al. (2017). They leverage morphologically annotated data in a closely related high-resource language to reduce the need for labeled data in the target language. This works well for similar languages, but has the shortcoming to require annotations in such a language to be at hand. Unlabeled corpora have been used for semi-supervised training of models for machine translation (MT), e.g., by Cheng et al. (2016); Vincent et al. (2010); Socher et al. (2011); Ramachandran et al. (2016). Those approaches differ from ours, due to a fundamental difference between the two tasks: For MRI, the source vocabulary and the target vocabulary are mostly the same. This makes it intuitive for MRI to train the final model jointly on MRI and autoencoding. One case where this has been done is the work by Ha et al. (2016). However, they apply their method exclusively to MT.", "startOffset": 32, "endOffset": 1708}], "year": 2017, "abstractText": "We present a semi-supervised way of training a character-based encoderdecoder recurrent neural network for morphological reinflection, the task of generating one inflected word form from another. This is achieved by using unlabeled tokens or random string as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.9% improvement over state-of-the-art baselines for 8 different languages.", "creator": "LaTeX with hyperref package"}}}