{"id": "1705.06338", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Distributed Vector Representation Of Shopping Items, The Customer And Shopping Cart To Build A Three Fold Recommendation System", "abstract": "the single main preliminary idea of this this paper is to positively represent shopping items through vectors therefore because utilizing these vectors may act also as the base for brands building simple em - compatible beddings for target customers and shop shopping carts. also, furthermore these error vectors are input related to the mathematical trend models as that simply act as either a transaction recommendation routing engine or help people in finding targeting larger potential brand customers. nowadays we have used exponential family embeddings as constructing the tool bundle to parallel construct two basic production vectors - product vector embeddings and context vectors. using the basic vectors, we build combined embeddings, supermarkets trip design embeddings again and dynamic customer embeddings. combined food embeddings mix linguistic properties devoid of product names with their shopping patterns. like the other customer embeddings actually establish an understand - ing of the buying dress pattern expression of customers specializing in predicting a group trend and help in building higher customer profile. \u2022 for particular example a customer profile expression can represent customers frequently stop buying grocery pet - food. identifying such profiles can help us companies bring far out offers and discounts. similarly, trip embeddings they are used to build trip profiles. people therefore happen again to buy similar set of customized products in implementing a common trip and hence constructing their trip embeddings can better be used to predict the next product type they need would appear like selling to commercially buy. even this idea is a beautiful novel technique and the first of others its kind happens to consistently make recommendation using parallel product, grocery trip and restaurant customer embeddings.", "histories": [["v1", "Wed, 17 May 2017 20:28:14 GMT  (2259kb,D)", "http://arxiv.org/abs/1705.06338v1", "Cicling 2017"]], "COMMENTS": "Cicling 2017", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["bibek behera", "manoj joshi", "abhilash kk", "mohammad ansari ismail"], "accepted": false, "id": "1705.06338"}, "pdf": {"name": "1705.06338.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bibek Behera", "Manoj Joshi", "Abhilash KK", "Mohammad Ansari Ismail"], "emails": ["bibek.behera@searshc.com", "manoj.joshi@searshc.com", "Abhilash.KK@searshc.com", "Ansari.MohammedIsmail@searshc.com"], "sections": [{"heading": null, "text": "Keywords: Product Embedding, shopping cart, vector representation"}, {"heading": "1 Introduction", "text": "Word embeddings have changed the research world in natural language processing. The word to vector model proposed by Mikolov et al. [9] took the field of AI and NLP by storm. Later, word2vec model was extended to sentences and documents as sent2vec and doc2vec models [4] and found their ways into myriad of applications.\nIn recent times, there has been research to bring similar concepts into other domains. Rudoplh et al. [10] has shown how to apply word2vec model in various domains like movies, grocery and medical science using the concept of family embeddings. In case of grocery, we obtain embeddings for each product by iterating over transactions that is available in huge quantities from the transaction logs of retail companies. Product embedding is the manifold space of products\nar X\niv :1\n70 5.\n06 33\n8v 1\n[ cs\n.I R\n] 1\n7 M\nay 2\n01 7\nwhere their abstract representation makes semantic sense. For example cat-food and cat-accessories are nearby in the product embedding space. Secondly the vector difference between cat-food and cat-accessories is same as the vector difference between dog-food and dog-accessories. These kind of semantic structure hidden in the product embeddings speak of the quality of the representations and lead us to build the trip and customer embeddings on the foundation of product embeddings\nIn this paper we begin with the mathematical model called exponential family embedding (Ef-emb) [10] and the characteristics of product embeddings and context vectors. Then we construct a recommendation system that finds similar and co-occurring items. Here similar items mean items that can replace each other. For example \u201dBeverage A\u201d can be a replacement of \u201dBeverage B\u201d. By co-occurring products, we mean products that can be bought together.\nWe then deploy a sent2vec model on the labels of products. This gives the product a linguistic prospective. We combine the sentence embeddings with the product embeddings and obtain combined embeddings. We investigate the properties of all three embeddings.\nThen we extend the concept of product embedding to trips and customers. A trip consists of all the products bought by a customer in one single transaction. The motivation is that we find cluster amongst trips so that we can recommend products to customers based on their current cart. Similarly we find customer embeddings and then find meaningful customer clusters using K-means algorithm based on Lloyd\u2019s algorithm [6]. These groupings lead us to departments of products that they buy frequently. The frequently observed departments make the profile of the customer group. For example a profile of customers can consist of girl tops, preschool and tod boy sportswear probably because the customers represent parents whose kids are in the age range of 3 to 5. Having done such profiling, offers and discount can be targeted to a customer if she happens to be similar to a particular group based on the profile of the group.\nThe road map for the rest of the paper is as follows. First we lay down the specifics of the theory behind family embeddings and their extension from word2vec model but with their own peculiarities in section 2. Then we demonstrate the recommendation engine in section 3. Henceforth, we show a sent2vec model and a combined model in section 5. Afterwards, we explain the process behind construction of trip embeddings and analysis of trip in section 7 followed by similar construction of customer embeddings and customer profiling in section 8. The description of entire pipeline is explained in section 9. This is followed by conclusion and future work in section 10."}, {"heading": "2 Exponential family embeddings", "text": "Exponential family embeddings (Ef-emb) [2] are statistical tools that generalise the technique to capture contextual information for various data sets and their varying distributions. They have been employed by Rudolph et al. [10] to extend the word2vec model to different domains like grocery, movies, etc. This model\nrequires two inputs -the data point and the context. For example, in grocery, the item bought is the data point while context is all other items bought in that cart or trip. In case of movies, the movie being watched is the data point while the context is all the movies rated by the same person. Thus Ef-emb gives the user the flexibility to choose their context function.\nThe motivation behind using an Ef-emb is to derive useful features of the data. Context vector, distribution of the data points and their embeddings are the key ingredients that are part of a loss function which has to be minimized iteratively. By distribution of data points, we mean to say the nature of data points can be continuous or discrete values. For example in grocery data we can have product quantities which are numerical in nature hence we can use Bernoulli or Poisson distribution [8]. In case of word2vec the words are binary in nature hence we can use Bernoulli distribution. In case of neural activity, their time of activity is a continuous variable wherein we use the Gaussian distribution. Thus, Ef-emb can effectively consume any kind of data and produce distribution of vector representation of data points. We also get to know the semantic structure e.g. movies of the same genre are clubbed together or some neurons have same time of activity even if they are spatially distant."}, {"heading": "2.1 Similar and co-occuring products", "text": "In case of shopping data, the motivation is to get similar and co-occurring products [5]. According to the concept described in Rudolph et. al 2016 [10], product embeddings can help in determining similar products by using cosine distance in the vector space to generate nearest neighbors as shown in equation 1 where \u03c1 stands for product embeddings. In other words if the product embeddings of a pair of products have high cosine similarity, then they can replace each other.\nsimilarity score = cosine distance (\u2212\u2192\u03c1x,\u2212\u2192\u03c1y) (1)\nCo-occurring items are obtained by calculating inner product between all pairs of product embeddings and context vectors. It has been found if a pair of product co-occur the inner product of product embedding of x (\u03c1x) and context vector of y (\u03b1y), as shown in equation 2 is higher. If they do not co-occur their inner products tend to be negative.\ncooccurrence score = cosine distance (\u2212\u2192\u03c1x,\u2212\u2192\u03b1y) (2)\nUsing the two models for similar and co-occuring products we build a basic recommendation engine. Some of the examples from the recommendation system has been shown in Table 1."}, {"heading": "3 Recommendation engine", "text": "We have used six month\u2019s transaction data of Sears Holdings Corporation. Firstly, we found all products that were bought in a single transaction henceforth called a trip. We consider those trips with 5 or more products, so that we\nhave a context for each product. Each product is a data point and the products bought in the transaction become the context word. Now this data is fed to the Ef-emb model which assume a Bernoulli distribution since we just account for the presence or absence of each product. We employ the algorithm implemented by Rudolph et. al 2016 [10]. The algorithm runs for 1000 iterations and the data has approximately 20 million trips and 500 thousand products. After completion, it creates two arrays - \u03c1 for product embeddings and \u03b1 for context vector for each product. Each vector has a length of 100. These vectors are fed to Annoy which is a fast c++ library with python bindings. Annoy generates an approximate nearest neighbour model as proposed by Arya et. al 1998 [1] for a million products . This model can be employed in a realtime recommendation system [11]. To find similar items, we provide product embedding and labels. Given a product id of the reference item, the model generates similar items as per equation 1. We find co-occuring items as discussed in section 2.1."}, {"heading": "4 Visualisation of product embedding and context vectors", "text": "We used TSNE to visualise the 100 dimension vector. TSNE is available as a python or R package and was developed by Maaten et. al. 2008 [7]. It converts n-dimensional vector space to m-dimensional vector space using a probabilistic approach and minimizes the KL divergence between the 2 data sets. The TSNE algorithm works well for data that is difficult to classify at higher dimension. The beauty of TSNE is that even after converting to lower dimension it retains the clusters in the higher dimensions to a high degree of precision. So we can interpret patterns in data by actually visualising them in 2D or 3D. In reality we see a multiple number of interpretable clusters and find relation easier to interpret. For example, cat-food and cat-accessories are placed closeby. Similarly dog-food and cat-food are clumped nearby. We also see semantic relation as show in 3\n# \u00bb catfood\u2212 # \u00bbcataccessories = # \u00bbdogfood\u2212 # \u00bbdogaccessories (3)\nIn the 2D visualisation we found cluster that represents beverage like coke, pepsi, dew, etc and also the cluster of water products is nearer to that of beverage.\nFor both vectors, we ran TSNE for 1000 iteration and converted a 100 dimension vector space to 2 dimension vector space. The visualisation of product\nembedding is shown in figure 1. We verify the cluster mathematically by running the recommendation system. The results returned by recommendation system which works on the 100 dimensional vector concurred with the 2D representation using TSNE."}, {"heading": "5 Sent2Vec model and combined model", "text": "The motivation is to find linguistic patterns in products and then combine them with product embeddings to form a vector representation that is an amalgamation of linguistic and shopping patterns of products.\nWe generate the labels of around five hundred thousand products and give it as input to the sent2vec model. The sent2vec model then creates vectors based on the context of every word present in those product names. The characteristic of this model is that it finds clusters of products bearing similar names while the characteristic of product embeddings is that they are capable of finding clusters of products that are conceptually similar. To bridge this gap between the two models, we build a combined model that takes vector representation of product embeddings and sentence embeddings and concatenate them.\nWe use TSNE to project them to a 2D map. In the projection of sentence embeddings, Swimwear and Beachwear appear far apart but in the projection of combined embeddings they appear close. Another example is that of the cat food\nand dog food that are distant in the sent2vec model come closer in the combined model. This is probably due to the properties that are brought by the product embeddings. Although combined model is merely a merger of sentence embeddings and product embeddings but some examples show that the properties of product embeddings seem to improve the quality of clusters created in sent2vec model.\nThe combination of sentence embeddings and product embeddings space can be seen as a merger of natural language vectors and shopping data patterns. Thus by combining these spaces together we have embeddings that are more useful conceptually. We have not verified mathematically whether the combined model is an improvement over product embeddings. That restrains us from using the combined model for recommendation."}, {"heading": "6 Application of product embedding", "text": "Having obtained the product embeddings of all products, we use them to find out patterns in trips and customers. By patterns we mean any information which could help us profile the trips and customers. By profiling we mean to summarise the products that are being bought by a group of trips or a cluster of customers. This information is particularly helpful in targetting customers for offers and discounts. The only information we have prior to obtaining product embeddings are the departments of the product and we will use these departments to profile a cluster of trips and customers. Everytime a new product appears in a department, we could find a potential customer and then target her by giving offers and discounts in that department category. Some of the department categories have been listed in table 2."}, {"heading": "7 Trip Embedding", "text": "The idea is to convert each trip into vector by taking mean of the sum of the product embeddings \u03c1 of all the n products bought in one single trip as shown in equation 5.\ntrip embedding[i] = 1\nn n\u2211 i=1 \u03c1[i] (4)\nIn all there were 22 million trips and we converted them to vectors. We took a sub sample of 10k vectors after filtering all trips that had 5 or more products. This was done because we needed to extract buying patterns in trips. If the number of products in a trip are lesser than a threshold (here 5) then we may not get a significant pattern. After employing TSNE we project them onto a 2d map where we could actually see clusters as shown in figure 2."}, {"heading": "7.1 Trip analysis", "text": "After obtaining the projection onto 2d map, we ran the KNN over the trip embedding space and extracted 5 clusters. The noticeable point is that despite having no common items some trip embeddings were paired together. We call such trip fake pairs. To estimate number of fake pairs, we find nearest neighbours using Annoy and then if they do not have similar items we call them fake pairs. For each cluster we made the trip analysis as shown in table 3."}, {"heading": "7.2 Trip Profiling", "text": "The second analysis was to profile each cluster based on the frequency of departmental tags assosciated with product bought in the trips. The cluster and their top tags have been shown in table 4."}, {"heading": "8 Customer Embedding", "text": "The idea is to represent each customer through a vector by averaging the product embeddings of all the n products bought in all trips over a period of six months by a customer i as shown in equation 5.\ncustomer embedding[i] = 1\nn n\u2211 j=1 \u03c1[i] (5)\nIn all, there were 16 million trips and we converted them to vectors. We took a subsample of 10k vectors after filtering all trips that had 5 or more products. This was done because we needed to extract cluster of customers. In smaller trip, appearance of a pattern is not significant. After employing TSNE we project them onto a 2d map where we could actually see clusters as shown in figure 3."}, {"heading": "8.1 Customer analysis", "text": "After obtaining the projection onto 2d map, we ran the KNN over the customer embedding space and extracted 5 clusters. The customer analysis is similar to trip analysis as shown in table 5."}, {"heading": "8.2 Customer Profiling", "text": "Customer profiling was done in the same manner as trip profiling and has been shown in table 6. As shown in customer profiles, we can identify that cluster 1 reflects sportwear, cluster 2 is about tops, cluster 3 about chemicals, cluster 4 has more items related to hygiene and cluster 5 is about beverages. This way we can find clusters and give them profiles."}, {"heading": "9 Schematic representation", "text": "Figure 4 shows the semantic data model, which includes the abstracted information of the embedding module. The module contains mainly two layers, which include Data and Model. The data layer contains the instances and features for the further processing. On top of the Model layer data go through the preprocessing stage.\nThe model layer converts the features in to vectors representations by using exponential family embedding as the tool. These vector representations, as discussed earlier are embeddings for products, trips and customers. The basic two vectors are product embedding and context vectors. After performing the filter embedding all the embedding will be in N dimensions, which is converted to M-dimension (2D) before visualization. The visualization can be consider as the final layer of the module, which is responsible for the graphical representation of the all embeddings by the help of required plugins."}, {"heading": "10 Conclusion", "text": "We have proposed a recommendation system with a novel approach that uses product embeddings, trip embeddings and customer embeddings to recommend products. We begin with Ef-emb to generate product embeddings and recommend products that are similar or co-occurring. We also created combined embeddings by combining product embeddings with sentence embeddings to bring linguistic patterns into shopping patterns. Then we use product embedding to generate trip and customer embeddings. Basically, we tried generalising the patterns at the product level to the higher understanding at the trip(shopping cart) level and customer level. We also demonstrate visually the presence of meaningful clusters in all embeddings and propose ways to recommend products by doing cluster analysis.\nThe recommendation of combined embeddings model needs to be validated mathematically whether they are better in terms of quality when compared to recommendations using product embeddings and sentence embeddings. Currently, we are evaluating the quality manually.\nWe have proposed a recommendation system based on customer, trips and products. But we have not proposed techniques to validate them. The problem with recommendation systems is that they have online verification algorithms [3] i.e. if a user clicks on one of the recommendations then the score is given inversely proportional to the rank of that recommendation which was clicked. We need to build a front end where user clicks can be recorded and our recommendation system gets a score."}], "references": [{"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Sunil Arya", "David M Mount", "Nathan S Netanyahu", "Ruth Silverman", "Angela Y Wu"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Fundamentals of statistical exponential families with applications in statistical decision theory", "author": ["Lawrence D Brown"], "venue": "Lecture Notes-monograph series,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Itemrank: A random-walk based scoring algorithm for recommender engines", "author": ["Marco Gori", "Augusto Pucci", "V Roma", "I Siena"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Amazon. com recommendations: Item-to-item collaborative filtering", "author": ["Greg Linden", "Brent Smith", "Jeremy York"], "venue": "IEEE Internet computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Least squares quantization in pcm", "author": ["Stuart Lloyd"], "venue": "IEEE transactions on information theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Generalized linear models", "author": ["Peter McCullagh"], "venue": "European Journal of Operational Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1984}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Exponential family embeddings", "author": ["Maja Rudolph", "Francisco Ruiz", "Stephan Mandt", "David Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Recommender systems in ecommerce", "author": ["J Ben Schafer", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 1st ACM conference on Electronic commerce,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}], "referenceMentions": [{"referenceID": 8, "context": "[9] took the field of AI and NLP by storm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Later, word2vec model was extended to sentences and documents as sent2vec and doc2vec models [4] and found their ways into myriad of applications.", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "[10] has shown how to apply word2vec model in various domains like movies, grocery and medical science using the concept of family embeddings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "These kind of semantic structure hidden in the product embeddings speak of the quality of the representations and lead us to build the trip and customer embeddings on the foundation of product embeddings In this paper we begin with the mathematical model called exponential family embedding (Ef-emb) [10] and the characteristics of product embeddings and context vectors.", "startOffset": 300, "endOffset": 304}, {"referenceID": 5, "context": "Similarly we find customer embeddings and then find meaningful customer clusters using K-means algorithm based on Lloyd\u2019s algorithm [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Exponential family embeddings (Ef-emb) [2] are statistical tools that generalise the technique to capture contextual information for various data sets and their varying distributions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "[10] to extend the word2vec model to different domains like grocery, movies, etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "For example in grocery data we can have product quantities which are numerical in nature hence we can use Bernoulli or Poisson distribution [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "In case of shopping data, the motivation is to get similar and co-occurring products [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "al 2016 [10], product embeddings can help in determining similar products by using cosine distance in the vector space to generate nearest neighbors as shown in equation 1 where \u03c1 stands for product embeddings.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "al 2016 [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "al 1998 [1] for a million products .", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "This model can be employed in a realtime recommendation system [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "2008 [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "The problem with recommendation systems is that they have online verification algorithms [3] i.", "startOffset": 89, "endOffset": 92}], "year": 2017, "abstractText": "The main idea of this paper is to represent shopping items through vectors because these vectors act as the base for building embeddings for customers and shopping carts. Also, these vectors are input to the mathematical models that act as either a recommendation engine or help in targeting potential customers. We have used exponential family embeddings as the tool to construct two basic vectors product embeddings and context vectors. Using the basic vectors, we build combined embeddings, trip embeddings and customer embeddings. Combined embeddings mix linguistic properties of product names with their shopping patterns. The customer embeddings establish an understanding of the buying pattern of customers in a group and help in building customer profile. For example a customer profile can represent customers frequently buying pet-food. Identifying such profiles can help us bring out offers and discounts. Similarly, trip embeddings are used to build trip profiles. People happen to buy similar set of products in a trip and hence their trip embeddings can be used to predict the next product they would like to buy. This is a novel technique and the first of its kind to make recommendation using product, trip and customer embeddings.", "creator": "LaTeX with hyperref package"}}}