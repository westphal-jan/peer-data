{"id": "1705.06922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Spectral-graph Based Classifications: Linear Regression for Classification and Normalized Radial Basis Function Network", "abstract": "spectral graph theory has been widely applied in unsupervised and semi - supervised learning. in this paper, we find for the first time, to our knowledge, that it also plays a concrete role in supervised classification. it turns out that two classifiers are necessarily inherently related somewhat to evolving the coding theory : linear regression for approximation classification ( lrc ) filtering and normalized radial basis function network ( nrbfn ), corresponding to linear and nonlinear kernel respectively. the essential spectral graph theory thesis provides us with a new insight tool into a peculiar fundamental intuitive aspect of algorithms classification : the tradeoff reaction between fitting error repair and overfitting target risk. assist with exploring the theory, ideal working conditions applicable for simultaneous lrc and adaptive nrbfn are presented, which ensure not entirely only zero adequate fitting error but also low overfitting risk. unfortunately for pure quantitative interval analysis, only two concepts, the fitting transition error constant and precisely the spectral risk ( all indicating mutual overfitting ), have been defined. their bounds for initial nrbfn and vector lrc operations are uniquely derived. a special spectral result criterion shows that the minimum spectral risk of nrbfn is lower now bounded by the number of fault classes shown and still upper bounded by exactly the size of radial regular basis. alternatively when the worked conditions are uniformly not met exactly, the classifiers together will pursue below the possible minimum fitting defect error, running backwards into half the intrinsic risk of local overfitting. it just turns out that $ \\ ell _ 2 $ - expected norm regularization can be applied to prevent control overfitting. its systematic effect generally is explored under the spectral product context. it seems is likewise found unexpectedly that applying the two terms in either the $ \\ 1st ell _ 4th 2 $ - regularized network objective parameter are relatively one - one correspondent similar to the unique fitting normal error approximation and the inherent spectral risk, revealing a tradeoff between the two quantities. somewhat concerning practical performance, we each devise precisely a matched basis selection transition strategy to address almost the main problem hindering the applications of ( n ) rbfn. complete with the strategy, greatest nrbfn behavior is a easy to totally implement yet flexible. experiments calculated on 14 benchmark data binding sets consistently show the efficient performance of maximal nrbfn is comparable just to perhaps that of functional svm, whereas the parameter constrained tuning of nrbfn graphs is again much easier, further leading to reduction of model operator selection time.", "histories": [["v1", "Fri, 19 May 2017 10:35:37 GMT  (154kb)", "https://arxiv.org/abs/1705.06922v1", null], ["v2", "Tue, 13 Jun 2017 15:18:08 GMT  (154kb)", "http://arxiv.org/abs/1705.06922v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhenfang hu", "gang pan", "zhaohui wu"], "accepted": false, "id": "1705.06922"}, "pdf": {"name": "1705.06922.pdf", "metadata": {"source": "CRF", "title": "Spectral-graph Based Classifications: Linear Regression for Classification and Normalized Radial Basis Function Network", "authors": ["Zhenfang Hu", "Gang Pan", "Zhaohui Wu"], "emails": ["fancij@zju.edu.cn,", "gpan@zju.edu.cn,", "wzh@zju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n06 92\n2v 2\n[ cs\n.L G\n] 1\n3 Ju\nSpectral graph theory has been widely applied in unsupervised and semi-supervised learning. It is still unknown how it can be exploited in supervised learning. In this paper, we find for the first time, to our knowledge, that it also plays a concrete role in supervised classification. It turns out that two classifiers are inherently related to the theory: linear regression for classification (LRC) and normalized radial basis function network (nRBFN), corresponding to linear and nonlinear kernel respectively. The spectral graph theory provides us with a new insight into a fundamental aspect of classification: the tradeoff between fitting error and overfitting risk. With the theory, ideal working conditions for LRC and nRBFN are presented, which ensure not only zero fitting error but also low overfitting risk. For quantitative analysis, two concepts, the fitting error and the spectral risk (indicating overfitting), have been defined. Their bounds for nRBFN and LRC are derived. A special result shows that the spectral risk of nRBFN is lower bounded by the number of classes and upper bounded by the size of radial basis. When the conditions are not met exactly, the classifiers will pursue the minimum fitting error, running into the risk of overfitting. It turns out that \u21132-norm regularization can be applied to control overfitting. Its effect is explored under the spectral context. It is found that the two terms in the \u21132-regularized objective are one-one correspondent to the fitting error and the spectral risk, revealing a tradeoff between the two quantities. Concerning practical performance, we devise a basis selection strategy to address the main problem hindering the applications of (n)RBFN. With the strategy, nRBFN is easy to implement yet flexible. Experiments on 14 benchmark data sets show the performance of nRBFN is comparable to that of SVM, whereas the parameter tuning of nRBFN is much easier, leading to reduction of model selection time.\nKeywords: classification, spectral graph, radial basis function network, linear regression, regularization, over-\nfitting."}, {"heading": "1 Introduction", "text": "Spectral graph theory is a theory that centers around the graph Laplacian matrix [14]. On the one hand, it can reveal underlying cluster structure of data by the eigenvectors of the Laplacian matrix, on the other hand, the eigenvectors can serve as dimensionally reduced codes that preserve pair-wise data relation. The theory has found wide applications in unsupervised learning, including clustering [65] (generally named spectral clustering, including, e.g., ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.g., Laplacian eigenmap (LE) [2] and locality preserving projections (LPP) [24]). Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71]. In semisupervised learning, in an attempt to impose pair-wise data relation, the role of spectral graph usually appears as a \u201cgraph-regularization\u201d term added to the other objectives.\nRecently it has been discovered that, in the scope of unsupervised learning, spectral graph theory unifies a series of elementary methods of machine learning into a complete framework [25]. The methods cover dimensionality reduction, cluster analysis, and sparse representation. They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25]. It is revealed that these methods share inherent relations, they even become equivalent under an ideal graph condition. The framework\nalso incorporates extended relations to conventional over-complete sparse representations [19], e.g., [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.g., kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.g., sparse subspace clustering (SSC) [20], low-rank representation (LRR) [36].\nHowever, as far as we know, spectral graph theory has not yet found applications in supervised classification, except in hybrid-model way where the relation is not inherent, e.g., the addition of certain classification objective with a graph-regularization term. It is interesting to know whether the theory plays a concrete role in supervised classification and which classifiers are related.\nIn supervised learning, linear regression and radial basis function network (RBFN) are two basic methods. Both of them are devoted to function fitting, including classification as special case. It is well-known that linear regression can be interpreted with Bayesian probability (see, e.g., [6]), and when used for classification (LRC), its link to linear discriminant analysis was already discovered [68]. RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4]. Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67]. However, the above methods are hardly related to spectral graph theory before.\nIn classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15]. Especially, in a comprehensive evaluation involving 179 classifiers and 121 data sets by [22], RBFN ranks third, immediately following SVM.1 However, despite of bearing additional advantages, (n)RBFN did not receive wide applications as SVM. This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning. In this paper, in addition to exploring stronger theoretical background for (n)RBFN, to make (n)RBFN become practical tool, we propose a solution scheme for the basis selection and parameter setting problems."}, {"heading": "1.1 Our Work", "text": "In this paper, we uncover the concrete role of spectral graph theory in supervised classification, and find that LRC and nRBFN are inherently related to the theory. The tradeoff between fitting error and overfitting risk is a fundamental problem of classification. The theory provides us with a new insight into this problem under the context of LRC and nRBFN. With the theory, we establish the ideal working conditions for the two classifiers, which ensure not only zero fitting error but also low overfitting risk. When the conditions are not met exactly, the \u21132-norm regularization can be applied to control overfitting, its effect is revealed under the spectral context. As a benefit, the regularization weight can be set in a principled and easy way. The followings are more detailed introduction.\nIn spectral clustering, we directly extract the cluster information from the eigenvectors of Laplacian matrix, i.e., recover the indicator vectors from the eigenvectors. However, in classification, the partition of data is assigned, and the indicator vectors are given, it seems not straightforward to see how spectral graph theory will work in this case. It turns out that we are to find the closest components in the eigenspace of Laplacian matrix to approximate the given indicator vectors. If the data is well-behaved, i.e., the given classes match the underlying clusters, then unsupervised clustering and supervised classification become consistent, and they unify under the spectral graph framework.\nIn this paper, we find that LRC and nRBFN are inherently related to spectral graph theory. Firstly, LRC will lay down some theoretical foundation, then nRBFN is derived via applying kernel trick on LRC. Broadly speaking, the data/feature matrix used by LRC/nRBFN shares the same eigenvectors with the Laplacian matrix, and we are to find the closest components in the eigenvectors to approximate the given indicator vectors. When an ideal graph condition is satisfied, which requires the classes being totally separated, the indicator vectors appear in the leading eigenspace of largest eigenvalues, and consequently zero fitting error is achieved. That is the inherent relation of LRC and nRBFN to spectral graph theory.\nAlthough, zero fitting error is desirable, classification is more concerned with generalization performance. Striking a balance between low fitting error and low overfitting risk is a critical problem. Under the ideal graph\n1In that evaluation, the classifier ranking third is a kernel version of extreme learning machine (ELM) [27], named \u201celm kernel m\u201d in the evaluation, but we carefully checked the codes of \u201celm kernel m\u201d and found that it is actually a classical RBFN that uses the whole training set as the basis. The only difference is that the target indicators are converted to be of values 1 (true class) and -1 (other classes). This RBFN is a special case of ELM.\ncondition, things are perfect, the fitting error is zero and the overfitting risk is low. From qualitative point of view, this is because the indicator vectors are found in the principal subspace. The principal subspace corresponds to stable features, as contrary to the minor subspace that corresponds to noisy features, especially when sampling is insufficient. For quantitative analysis, we define two concepts: the fitting error and the spectral risk. The spectral risk measures the deviation of the found components to the principal subspace, therefore it signals a warning of overfitting. The bounds of the two quantities for nRBFN and LRC are derived. A special result shows that the spectral risk of nRBFN is lower bounded by the number of classes\u2013a quantity representing \u201cproblem complexity\u201d, and upper bounded by the size of basis\u2013a quantity representing \u201cmodel complexity\u201d. The upper bound indicates a tradeoff between fitting error and overfitting risk: larger basis implies lower error but higher risk.\nIn practice, the ideal condition cannot be met exactly, the leading eigenspace will deviate from the target indicator vectors. The found closest components may lie in minor subspace of small singular values. It is easily prone to noise, giving rise to increment of overfitting risk. It will be shown that the \u21132-norm regularization can alleviate this problem. Its effect is explored under the spectral context. First, qualitatively, it drives the classifier to find the closest components in the principal subspace and discourages the opposite direction. Second, quantitatively, the two terms in the \u21132-regularized objective are in one-one correspondence to the fitting error and the spectral risk, showing a tradeoff between the two quantities.\nnRBFN is more powerful than LRC for its nonlinear kernel and significant risk bounds, we thus focus on nRBFN. To make nRBFN work in practice, we devise a basis selection strategy to address the main problem that hinders the wide applications of (n)RBFN. The strategy is based on soft K-nearest neighbors. It is easy to implement and the result is deterministic, in contrast to traditional K-means based strategy that depends on random initialization. Traditionally, setting the basis size is a troublesome problem. In our scheme, it is implicitly determined via a user-friendly threshold within range (0, 1]. With this threshold, the basis size can be automatically determined according to the complexity of data distribution. We can also flexibly control the tradeoff between accuracy and computational cost via this threshold. Besides, when using \u21132-norm regularization, the regularization weight can be set in a more principled way. In all, the parameter tuning is easy, leading to significant reduction of model selection time.\nThe contributions of the paper are as follows:\n1. We have extended the spectral graph framework to the supervised classification domain. We found for the\nfirst time, to our knowledge, that spectral graph theory plays a concrete role in supervised classification. Two classifiers, LRC and nRBFN, corresponding to linear and nonlinear kernel respectively, turn out to be inherently related to the theory. With the theory, ideal working conditions for LRC and nRBFN are presented, which ensure not only zero fitting error but also low overfitting risk.\n2. With the spectral graph theory, new insights into the overfitting problem as well as the effect of \u21132-norm regularization are obtained. For quantitative analysis, two concepts, the fitting error and the spectral risk\nhave been defined. The bounds of them for nRBFN and LRC have been derived. One result states that the spectral risk of nRBFN is lower bounded by the number of classes and upper bounded by the size of radial basis. In addition, it turns out that the two terms in the \u21132-regularized objective are one-one correspondent to the fitting error and the spectral risk, revealing a tradeoff between the two quantities.\n3. We have devised a basis selection strategy for (n)RBFN, so that nRBFN becomes easy to implement yet\nflexible. The performance of nRBFN is comparable to that of SVM, whereas the parameters of nRBFN are much easier to set, leading to significant reduction of model selection time.\nThe rest of the paper is organized as follows. Section 2 briefly introduces spectral graph theory and reviews its rationale for clustering. Section 3 presents spectral-graph based classifications, including linear version LRC and kernel version nRBFN. Meanwhile, the ideal working conditions are introduced. In the cases of the conditions are not met exactly, Section 4 introduces the \u21132-norm regularization for LRC and nRBFN. Section 5 defines the fitting error and spectral risk, derives their bounds for nRBFN and LRC, and reveals the effect of \u21132-norm regularization. Section 6 proposes the basis selection strategy. Section 7 demonstrates the performance of nRBFN and empirically evaluates the fitting error and spectral risk. Section 8 introduces some related work. The paper is ended with further work in Section 9.\nNotations. A = [A1, . . . , An] \u2208 Rp\u00d7n: data matrix with n samples of dimension p. F = [F1, . . . , Fn] \u2208 R\nK\u00d7n: indicator matrix for n samples of K classes. If the ith sample belongs to class k, then the kth entry of Fi is one and the others are zero. G = [G1, . . . , Gr] \u2208 Rp\u00d7r: basis vectors of RBFN and nRBFN. 1: a vector of uniform value 1. diag(v): a diagonal matrix formed by vector v."}, {"heading": "2 Spectral-graph Based Clustering: a Review", "text": "Given an undirected graph of n vertices (data points), with the adjacency matrix defined to be a similarity matrix W \u2208 Rn\u00d7n, measuring the pairwise similarities between data points, Wij = Wji \u2265 0, the Laplacian matrix is defined as L . = S \u2212W , where S is a diagonal degree matrix with the diagonal being the sum of weights of each vertex, i.e., S = diag(1TW ). The Laplacian matrix has the following properties [65].2\n1. It is positive semi-definite.\n2. Vector 1 is always an eigenvector with eigenvalue zero.\n3. Assume there are K connected components in the graph, then the indicator vectors of these components (row vectors of F ) span the eigenspace of eigenvalue zero.\nThese properties are exploited for clustering purpose [65]. Assume we are to findK clusters, if the ideal graph condition for clustering (Definition 1) [25] holds (the condition implies the between-cluster weights are all zero: Wij = 0, if the ith and jth points are of different clusters), then we can compute the K eigenvectors of L with the smallest eigenvalues (zero), and then postprocess these eigenvectors to finish clustering. In practice, the K components of the graph may not be completely disconnected. In this noisy case, the same procedure can still be applied, since the K eigenvectors with the smallest eigenvalues become rotated noisy indicators, which may not differ much to their ideal ones. This is the working rationale of spectral clustering.\nDefinition 1. (ideal graph condition for clustering) Targeting for K clusters, if there are exactly K connected components in the graph, then the graph (or similarity matrix) is called ideal (with respect to K clusters).\nFinally, the eigenvectors of Lwith eigenvalue zero (smallest) are the eigenvectors of S\u22121W , called normalized Laplacian matrix, with eigenvalue one (largest) [65]."}, {"heading": "3 Spectral-graph Based Classifications", "text": "In spectral clustering, we directly extract the cluster information from the eigenvectors of Laplacian matrix, i.e., recover the indicator vectors from the eigenvectors. However, in classification, the indicator vectors are given. It will be shown that we are to find the closest components in the eigenspace to approximate the indicator vectors. When an ideal condition is satisfied, the indicator vectors appear in the leading eigenspace, achieving zero fitting error and low overfitting risk."}, {"heading": "3.1 Linear Version: Linear Regression for Classification (LRC)", "text": "We will show that the singular vectors of the data matrix are the eigenvectors of a Laplacian matrix. Thus the link to spectral graph theory is established. The Laplacian matrix is built by the inner product between data, i.e., linear kernel. In the following, we first introduce the basic formulation of LRC, then analyze it from the row-space view, this leads to the relation to spectral graph theory. Based on the theory, an ideal working condition for LRC is presented. Finally, we analyze LRC from the column-space view, which paves the way to nRBFN."}, {"heading": "3.1.1 Basic Formulation", "text": "Given data matrixA (assume mean-removed,A1 = 0) and the corresponding class labels, we convert the labels to\nan indicator matrix F , and define an augmented data matrix A\u0303 =\n[\u221a \u03b21T\nA\n]\n[25], where \u03b2 is a constant scalar that\nwill be introduced later. The objective of LRC is to find a weight matrix D so that the linear combinations of the columns ofD and the samples approximate the indicator vectors:3\nmin D\nn \u2211\ni=1\n\u2016Fi \u2212DA\u0303i\u201622 = \u2016F \u2212DA\u0303\u20162F . (1)\n2These properties are not shared by the similarity matrix. 3It is equivalent to the classical LRC where \u03b2 = 1 [6], since \u221a \u03b2 can be absorbed into the first column of D. In implementation, we indeed\nuse \u03b2 = 1.\nProvided rank(A\u0303) = p+1, there is a unique closed-form solution: D\u2217 = FA\u0303T (A\u0303A\u0303T )\u22121 [6]. AfterD\u2217 is obtained, given a test sample b (with mean removed as A), its label is determined by the maximum entry ofD\u2217b\u0303:\nargmax k\n(D\u2217b\u0303)k. (2)\nIt can be shown that the sum of D\u2217b\u0303 is always one.4 Besides, D\u2217b\u0303 is an approximation to the indicator vector. These endowD\u2217b\u0303 with a quasi-probability interpretation (may include negative values)."}, {"heading": "3.1.2 Row-space View", "text": "It will be shown that LRC are to find the closest components in the data row-space to approximate the indicator vectors, and this relates to the spectral graph.\nSubstitutingD\u2217 into (1), we obtain \u2016F \u2212FA\u0303T (A\u0303A\u0303T )\u22121A\u0303\u20162F . Note that A\u0303T (A\u0303A\u0303T )\u22121A\u0303 is a projection matrix, and the projection subspace is spanned by the rows of A\u0303. In this view, to approximate F , LRC projects F onto the row-space of A\u0303 and reconstructs. These facts are well-known. However, a natural question arises: does the row-space contain \u201cingredients\u201d close to F , so that the reconstruction error is small? With spectral graph theory, we present an ideal condition under which the row-space of A\u0303 contains F . Define a similarity matrixW . = A\u0303T A\u0303, and\n\u03b2 . = \u2212min\nij (ATA)ij . (3)\nSince the data is mean-removed, we have minij (A TA)ij < 0, 5 \u03b2 thus defined makes W become nonnegative [25]. The condition and theorem are as follows:6\nDefinition 2. (ideal graph condition for classification) If the weights between vertices of different classes are all zero, i.e., \u2200i, j, Wij = 0 if Fi 6= Fj , then the graph (or similarity matrix) is called ideal (with respect to the class labels).\nTheorem 1. Given indicator matrix F , if W satisfies the ideal graph condition for classification, then the row vectors of F lie in the row-space of A\u0303 corresponding to the largest singular value ( \u221a \u03b2n), and therefore zero fitting error is achieved: F = D\u2217A\u0303.\nProof. W such defined is a nonnegative symmetric matrix, hence it is a qualified similarity matrix. Because A is mean-free, the degree matrix is S = diag(1TW ) = n\u03b2I , and the Laplacian matrix L . = S \u2212 W becomes n\u03b2I \u2212 A\u0303T A\u0303. Assume the thin SVD [23] of A to be U\u03a3V , where the singular values are arranged in descending order, then [25]\nA\u0303 = U\u0303 \u03a3\u0303V\u0303 =\n[\n1 U\n] [\u221a \u03b2n\n\u03a3\n] [\n1\u221a n 1 T\nV\n]\n, (4)\nFurther, by L = n\u03b2I \u2212 A\u0303T A\u0303, we obtain the spectral decomposition [23] of L:\nL = [\n1\u221a n 1 V T V\u0302 T\n]\n\n 0 \u03b2nI \u2212 \u03a32\n\u03b2nI\n\n\n\n\n1\u221a n 1 T\nV\nV\u0302\n\n , (5)\nwhere V\u0302 is the complement of V\u0303 . If the condition holds, by the third property of Laplacian matrix in Section 2, the row vectors of F lie in the eigenspace of L with eigenvalue zero. In view of (5) and (4), the eigenvectors of L with the smallest eigenvalues are the right singular vectors of A\u0303 with the largest singular values. Thus, the row vectors of F lie in the row-space of A\u0303 with the largest singular values (all equal \u221a \u03b2n).\nNote that the condition ensures not only perfect reconstruction but also that the target lies in the principal row-subspace of data, or principal components (PCs) in the language of PCA [28]. This is important, because the PCs correspond to stable features, whereas the minor components usually correspond to noise, especially when sampling is insufficient. In some cases, zero training error can be achieved, however, the target may be found in the minor subspace, then generalization error can be large. That is the overfitting problem. We will return to this issue in later sections.\n4Referring to the proof of Lemma 2 in Section 3.2.4, it can be shown that 1TD\u2217 = [1, 0, . . . , 0]. 5Otherwise ATA1 6= 0 violating A1 = 0. 6Note that the theory requires the data to be mean-removed, which may be ignored by traditional LRC."}, {"heading": "3.1.3 Column-space View", "text": "Finally, we take a closer look at the vector D\u2217b\u0303, and understand the voting mechanism of LRC. The facts are routine. With (4), we haveD\u2217b\u0303 = FV\u0303 T V\u0303b = \u2211n i=1 Fi(V\u0303 T i V\u0303b), where V\u0303b = \u03a3\u0303 \u22121U\u0303T b\u0303 are the normalized full PCs of b\u0303 and V\u0303i are those of A\u0303i. The mechanism of class prediction becomes clear: the class of a sample is determined by the votes of the training set, where the indicator vectors Fi\u2019s play the role of the votes, and the similarities between the training set and the sample serve as the weights assigned to the votes. Here, the similarity is measured by the inner product of PCs. For general data, this is not a good choice, and that is one of the limitations of LRC. We now introduce the more powerful kernel version, which measures the similarity based on Euclidean distance."}, {"heading": "3.2 Kernel Version: Normalized RBF Network (nRBFN)", "text": "We will apply the kernel trick to LRC in two ways: a traditional way leads to RBFN,7 the other way leads to nRBFN. The function matrix used by RBFN is the similarity matrix, while that used by nRBFN is the normalized Laplacian matrix. Since the similarity matrix does not share the properties of Laplacian matrix, we cannot directly analyze RBFN by spectral graph theory, whereas the link of nRBFN to the theory is straightforward. In a following subsection, we introduce the routine basis reduction to reduce the size of the networks. After that, we interpret nRBFN from the row-space and column-space views, and analyze it with spectral graph theory. An ideal working condition in the context of basis reduction is introduced, and some properties of nRBFN are shown."}, {"heading": "3.2.1 RBFN", "text": "We derive RBFN by applying kernel trick to LRC. The solution of (1) can be rewritten asD\u2217 = (FA\u0303T (A\u0303A\u0303T )\u22122A\u0303)A\u0303T , which is a linear combination of the training data. If we assumeD = XA\u0303T , then (1) turns into another objective\nmin X\n\u2016F \u2212XA\u0303T A\u0303\u20162F . (6)\nApplying kernel trick on A\u0303T A\u0303, we get min X \u2016F \u2212XW\u20162F . (7)\nW is a kernel matrix defined by some kernel function Wij = \u03c6(Ai, Aj). In LRC case, it is a linear function \u03c6(Ai, Aj) = A T i Aj + \u03b2. Among nonlinear ones, Gaussian kernel is most frequently used \u03c6(Ai, Aj) = exp{\u2212\u2016Ai \u2212 Aj\u201622/(2\u03c32)}. (7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03d5(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi. Here, the basis G consists of the whole training set, Gi = Ai. AssumeW has full rank, the solution of (7) is X\n\u2217 = FW\u22121. Given a sample b, sinceD\u2217b\u0303 = X\u2217A\u0303T b\u0303, applying the same trick, the class of b is decided by\nargmax k\n(X\u2217Wb)k, (8)\nwhere (Wb)i = \u03c6(Ai, b). For RBFN, the ideal graph condition does not work, since the kernel matrix when served as similarity matrix does not possess the same properties as its Laplacian matrix. We are not sure whether F lies in the principal subspace or minor subspace."}, {"heading": "3.2.2 nRBFN", "text": "We restart the derivation with kernel trick from another way, which will lead to nRBFN. By absorbing \u03b2n intoX , (6) is equivalent to\nmin X \u2016F \u2212XA\u0303T A\u0303(\u03b2n)\u22121\u20162F = min X \u2016F \u2212XA\u0303T A\u0303 diag(1T A\u0303T A\u0303)\u22121\u20162F . (9)\n7There is another standard way based on the reproducing kernel Hilbert space and representor theorem that leads to kernel classifier, see e.g., [53], but it cannot lead to nRBFN. 8Compared with some traditional RBFNs, the RBFN here, derived through kernel trick, does not append a constant vector 1T to W and a bias vector to X .\nNote that A\u0303T A\u0303 diag(1T A\u0303T A\u0303)\u22121 = WS\u22121, which is the transpose of the normalized Laplacian matrix. Applying kernel trick and using Gaussian kernel, we obtain nRBFN:\nmin X\n\u2016F \u2212XWS\u22121\u20162F . (10)\nAssumeW has full rank, the solution isX\u2217 = FSW\u22121. Given a sample b, sinceX\u2217A\u0303T b\u0303(\u03b2n)\u22121 = X\u2217A\u0303T b\u0303(1T A\u0303T b\u0303)\u22121, applying the same kernel trick, the class of b is decided by\nargmax k\n(X\u2217Wb(1 TWb) \u22121)k = argmax k (X\u2217Wb/sb)k, (11)\nwhere sb is the sum ofWb. Note that WS\u22121 is a normalized similarity matrix. Each column of it sums to one, so is Wb/sb. nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67]. It is also closely related to Gaussian mixture model [63]. However, the underlying spectral graph background seems not yet be discovered. Before the exploration, we deal with the basis reduction problem."}, {"heading": "3.2.3 Basis Reduction", "text": "In above, the bases of RBFN and nRBFN consist of the whole training set, which will lead to expensive computation. Traditionally, basis reduction is applied [52]. A smaller basis is chosen by some strategy (discussed in Section 6). For the moment, we assume the basis G = [G1, . . . , Gr], r < n, is given. Now, W is of size r \u00d7 n, andWij = \u03c6(Gi, Aj). Denoting\nW\u0303 . = WS\u22121 \u2208 Rr\u00d7n,\nthe formulation of basis-reduced nRBFN becomes\nmin X\n\u2016F \u2212XW\u0303\u20162F . (12)\nnRBFN and RBFN are special cases of linear regression, with sample vectors replaced by similarity vectors. Assume W is of full rank, the solution of nRBFN is X\u2217 = FW\u0303T (W\u0303W\u0303T )\u22121 (that of RBFN is X\u2217 = FWT (WWT )\u22121). Given a sample b, in nRBFN its class is decided by\nargmax k\n(X\u2217W\u0303b)k, (13)\nwhere W\u0303b . = Wb/sb,Wb are the similarities between b and the basis, and sb is the sum ofWb.\nHereafter, we focus on basis-reduced nRBFN."}, {"heading": "3.2.4 Row-space View and Column-space View to nRBFN", "text": "We will show the spectral graph theory underlying nRBFN, and introduce some basic properties as well as interpretations concerning nRBFN.\n1. From column-space view, we will show that, as LRC, the class prediction of nRBFN is also via voting mechanism. First, besides W\u0303 has a probability interpretation, the weight matrix X\u2217 also has a quasi-probability interpretation. We have\nLemma 2. Each column of the weight matrixX\u2217 sums to one: 1TX\u2217 = 1TFW\u0303T (W\u0303W\u0303T )\u22121 = 1T .\nProof. The row-space of W\u0303 contains 1T \u2208 R1\u00d7n, because 1T W\u0303 = 1T . Thus, from the projection point of view, 1 T W\u0303T (W\u0303 W\u0303T )\u22121W\u0303 = 1T . By this, 1TX\u2217W\u0303 = 1T . On the other hand, since W\u0303 has full rank, the solution of xW\u0303 = 1T is unique. However, both 1T \u2208 R1\u00d7r and 1TX\u2217 are the solutions, so we conclude 1TX\u2217 = 1T .\nNow considering (13), the class of sample b is decided by the voting of the basis. Each basis vector keeps a vote X\u2217i , and the weight assigned to the vote is the normalized similarity of b to that basis vector Gi. In contrast to LRC, the votes are not indicator vectors, and the weights are not computed by inner product. Rather, the votes of the basis are gathered from another voting of the training data, X\u2217i = F (W\u0303 T (W\u0303 W\u0303T )\u22121)i. Note that 1T W\u0303T (W\u0303 W\u0303T )\u22121 = 1T , since 1TX\u2217 = 1T . We can expect that when a basis vector is more \u201creliable\u201d,\ne.g., lying in the center of a class, the vote it keeps would concentrate in its class, whereas when lying in the overlapping region, the vote would be distributed more evenly. Generally, except under the ideal condition below, X\u2217 will include negative value, which represents objection. 2. From row-space view, similar to LRC, nRBFN finds the closest subspace in the row-space of W\u0303 to approximate F . Again, an ideal condition ensuring perfect reconstruction exists. However, in the context of basis reduction, the graph should be generalized to a bipartite graph: one side of the vertices consists of the basis, the other side consists of the training set. We should assume the basis is a subset of the training set, and each class is represented by at least one basis vector. The original case where the basis consists of the whole training set is a special case of bipartite graph. Denoting FGi to be the indicator vector of Gi, the condition is as follows:\nDefinition 3. (ideal bipartite-graph condition for classification) If the weights between basis vertices and data vertices of different classes are all zero, i.e., \u2200i, j, Wij = 0 if FGi 6= Fj , then the bipartite-graph (or similarity matrix) is called ideal (with respect to the class labels).\nWith these prerequisites, we have the following theorem indicating zero fitting error:\nTheorem 3. Given indicator matrix F , if W satisfies the ideal bipartite-graph condition for classification, then the row vectors of F lie in the row-space of W\u0303 , and zero fitting error is achieved: F = X\u2217W\u0303 .\nThe proof of the theorem is manifest: the rows of W\u0303 corresponding to the same class sum to an indicator vector of that class.\nWhen the condition holds, it can be proved that the votesX\u2217 become an indicator matrix. In this case the votes are ideal and very confident, since they concentrate in one class.\nProposition 4. If the ideal bipartite-graph condition for classification holds, X\u2217 becomes an indicator matrix. For basis vector Gi, X \u2217 i = FGi .\nProof. Without loss of generality, assume the training samples of the same class are arranged consecutively, then W\u0303 is block-diagonal, so is (W\u0303W\u0303T )\u22121. Further, W\u0303T (W\u0303 W\u0303T )\u22121 has the same nonzero blocks as W\u0303T . Since X\u2217i = F (W\u0303\nT (W\u0303W\u0303T )\u22121)i, we see that X\u2217i is a linear combination of the indicator vectors of the same class as Gi, which means there is only one nonzero in X \u2217 i . By Lemma 2, we conclude that the nonzero value is one. Therefore,X\u2217i is an indicator vector of Gi.\nWhen the condition does not hold exactly, that is when the classes have some overlapping but not heavy, negative values may present in X\u2217, but their magnitude should be small, because X\u2217 is a continuous function of W .\nUnlike LRC case, whether F lies in the leading row-subspace is less obvious. When the basis consists of the whole training set and the ideal condition is satisfied, by property of the normalized Laplacian matrix (Section 2), rows of F are the left eigenvectors with the largest eigenvalues. We expect they are close to the leading rowsubspace that corresponds to the largest singular values. For the general bipartite-graph case, please refer to Appendix A for detailed investigation. We present the main result below.\nTheorem 5. Under the ideal bipartite-graph condition for classification, the row vectors of F become the right singular vectors of W\u0303 corresponding to the largest singular values, if and only if the row sums of W\u0303 are even within each class: \u2211\nk W\u0303ik = \u2211 k W\u0303jk , \u2200i, j, FGi = FGj .\nThe theorem suggests that when designing the model, a balanced system, which means the row sums of W\u0303 are as even as possible within each class, is preferred.\nHereafter, without confusion, we will simply refer the above three conditions as ideal graph condition."}, {"heading": "4 Regularization for LRC and nRBFN", "text": "In practice, the ideal graph conditions are not easy to meet exactly. In this case, zero fitting error may not be achieved, and part of the found closest components may lie in the minor subspace. This will lead to the increment of overfitting risk. In this section, we introduce the traditional \u21132-norm regularization, and qualitatively show its effect on controlling the overfitting risk from the spectral view. The regularized versions of LRC and nRBFN are what we will really apply in real world."}, {"heading": "4.1 Regularized LRC", "text": "For LRC, the ideal graph condition is hard to meet, except perhaps for high-dimensional data: by the construction of W , the condition essentially requires that, after translating along a new dimension, different classes become orthogonal. Thus in general case, the leading row-subspace of A\u0303may deviate much from F . LRC then searches the entire row-space to find a closest subspace to approximateF . The found subspacemay correspond to small singular values, which may represent discriminative features or, more frequently, noise (e.g., due to insufficient sampling). In other words, LRC may deem the noisy components of data as the discriminative features for classification. Poor generalization ability can be expected.\nIn this regard, we would like to encourage LRC to search within the principal row-subspace. This can be\nachieved via \u21132-norm regularization:\nmin D\n\u2016F \u2212DA\u0303\u20162F + \u03bb\u2032\u2016D\u20162F , (14)\nwhere \u03bb\u2032 > 0 is a scalar weight. The unique closed-from solution isD\u2217 = FA\u0303T (A\u0303A\u0303T +\u03bb\u2032I)\u22121, regardless of the rank of A. The label of a new sample is decided as (2).\nIn this case, the reconstruction of the training set is D\u2217A\u0303 = FA\u0303T (A\u0303A\u0303T + \u03bb\u2032I)\u22121A\u0303. Assume the SVD of A\u0303 to be U\u0303\u03a3\u0303V\u0303 , then A\u0303T (A\u0303A\u0303T + \u03bb\u2032I)\u22121A\u0303 = V\u0303 T\u039bV\u0303 , where \u039b is a diagonal matrix with \u039bii = \u03c3\u03032i /(\u03c3\u0303 2 i + \u03bb\n\u2032) < 1. For those large singular values \u03c3\u0303i \u226b \u03bb\u2032, \u039bii \u2248 1 so the principal subspace is preserved, while for those small ones \u03c3\u0303i \u226a \u03bb\u2032, \u039bii \u2248 0 so the minor components are suppressed. In implementation, for the ease of setting \u03bb\u2032, we rewrite it to be \u03bb\u2032 = \u03bb\u2016A\u0303\u20162F . Note that \u2016A\u0303\u20162F = \u2211 i \u03c3\u0303 2 i , so the contrast between \u03c3\u0303 2 i and \u03bb \u2032 is easier to control. Back to the objective, \u2016F \u2212D\u2217A\u0303\u20162F = \u2016F \u2212 FV\u0303 T\u039bV\u0303 \u20162F . Now LRC projects F onto the principal row-subspace and reconstructs. The reconstruction error may increase slightly, but overfitting is alleviated. A quantitative analysis will be conducted in Section 5."}, {"heading": "4.2 Regularized nRBFN", "text": "The ideal graph condition for nRBFN does not require orthogonality between the classes. Nevertheless, when the condition is not exactly met, to prevent nRBFN seeking the closest components in the minor subspace, we introduce regularization (the analysis follows LRC, and we omit):\nmin X\n\u2016F \u2212XW\u0303\u20162F + \u03bb\u2032\u2016X\u20162F , (15)\nwhere \u03bb\u2032 = \u03bb\u2016W\u0303\u20162F , and \u03bb > 0 is a scalar weight. The solution becomes X\u2217 = FW\u0303T (W\u0303W\u0303T + \u03bb\u2032I)\u22121, regardless of the rank of W\u0303 . The label of a sample b is decided as (13)."}, {"heading": "5 Error and Risk Analysis", "text": "The tradeoff between fitting error and overfitting risk is an important problem of classification. In this section, from the spectral point of view, we quantitatively analyze this problem for LRC and nRBFN, and reveal the effect of \u21132-regularization further. First, we define a quantitative criterion, the spectral risk, for the measurement of overfitting risk. The fitting error is also formally defined. Next, we analyze the un-regularized nRBFN and LRC in mainly ideal cases. The bounds of the two quantities will be derived. Finally, we investigate the \u21132-regularization (independent of the ideal graph condition). We will show that the two terms in the \u21132-regularized objective are one-one correspondent to the fitting error and spectral risk, and study the effect of \u21132-regularization on trading off the error and risk."}, {"heading": "5.1 Definitions of Spectral Risk and Fitting Error", "text": "The definitions apply to linear regression, including LRC and nRBFN as special cases. We will define an absolute measure and a relative measure for both spectral risk and fitting error, the reasons will be clear later. The relative measures will be used as default definitions.\nLet the linear regression problem be formulated as\nmin D\n\u2016F \u2212DA\u20162F , (16)\nwhere F \u2208 RK\u00d7n is any target matrix not limited to indicators, A \u2208 Rr\u00d7n (r \u2264 n) is any data matrix of full rank. The solution isD\u2217 = FAT (AAT )\u22121. To exclude meaningless case, we assumeD\u2217 6= 0, which means FAT 6= 0, i.e., the data is not orthogonal to the target."}, {"heading": "5.1.1 Spectral Risk", "text": "The spectral risk measures the deviation of the found components to the principal subspace. First, we define an absolute measure.\nDefinition 4. (absolute spectral risk) The absolute spectral risk is defined as\n\u03b1 . = \u2016D\u2217\u20162F . (17)\nThe justification can be understood by the following spectral expression. Assume the SVD of A to be A = U\u03a3V , then for problem (16), D\u2217 = FV T\u03a3\u22121UT , and we have\nProposition 6. For linear regression problem (16),\n\u03b1 =\nr \u2211\ni=1\na2i \u03c32i , (18)\nwhere a2i is the projection of the target onto Vi: a 2 i\n. = \u2211K\nk=1(FkV T i ) 2, and Fk is the kth row of F .\nIt implies that if the projections concentrate in the leading singular vectors, that is the closest components lie in the principal subspace, \u03b1 will be small. Conversely, if they concentrate in the rear singular vectors, \u03b1 will be large. Thus, as an absolute measure, \u2016D\u2217\u20162F is reasonable.\nHowever, a meaningful range of the absolute measure cannot be determined. In order to cancel out the volume of data so that the measures between different data of the same model can be compared, we now define a relative measure by normalizing the projections and singular values.\nDefinition 5. (spectral risk) The relative spectral risk, simply called spectral risk, is defined as\n\u03b3 . = \u2016D\u2217\u20162F \u2016A\u20162F \u2016D\u2217A\u20162F . (19)\nFor problem (16), by noting \u2016A\u20162F = \u2211r i=1 \u03c3 2 i and \u2016D\u2217A\u20162F = \u2016FV TV \u20162F = \u2211r i=1 a 2 i , we have\nProposition 7. For linear regression problem (16),\n\u03b3 =\nr \u2211\ni=1\na\u03032i \u03c3\u03032i , (20)\nwhere a\u03032i . = a2i / \u2211r i=1 a 2 i and \u03c3\u0303 2 i . = \u03c32i / \u2211r i=1 \u03c3 2 i .\nWe can easily obtain the following range and bounds of \u03b3:\nProposition 8. The range of the spectral risk is \u03b3 \u2265 1, and\n1 \u2264 1 \u03c3\u030321 \u2264 \u03b3 \u2264 1 \u03c3\u03032r . (21)\nThe minimum value 1 is achieved, if and only if the data dimension is one, i.e., r = 1.\nThe minimum bound implies that when we use the smallest basis having only one vector, the smallest risk is achieved. However, in this case, the fitting error can be quite large. There is a tradeoff between the fitting error and the spectral risk."}, {"heading": "5.1.2 Fitting Error", "text": "First of all, it should be made clear that the fitting error is distinct from the error rate of classification. The absolute measure of fitting error is defined straightforwardly:\nDefinition 6. (absolute fitting error) The absolute fitting error is defined as\nf . = \u2016F \u2212D\u2217A\u20162F . (22)\nFor problem (16), we have f = \u2016F\u20162F \u2212 \u2016D\u2217A\u20162F = n \u2212 \u2211r i=1 a 2 i . For reason that will be clear later, we\ndefine the relative measure to be:\nDefinition 7. (fitting error) The relative fitting error, simply called fitting error, is defined as\n\u01eb . = \u2016F \u2212D\u2217A\u20162F \u2016D\u2217A\u20162F + 1. (23)\nFor problem (16), the definition is equivalent to \u01eb . = \u2016F\u20162F/\u2016D\u2217A\u20162F . It is easy to see that\nProposition 9. For linear regression problem (16), \u01eb = n/ \u2211r i=1 a 2 i . Its range is \u01eb \u2265 1. The minimum value 1 is achieved if and only if the row-space of data completely covers the target, i.e.,D\u2217A = A."}, {"heading": "5.2 Error and Risk Bounds of nRBFN", "text": "The ideal graph condition ensures zero fitting error and low overfitting risk, we will calculate the specific bounds for nRBFN (un-regularized), and then extend to the perturbation case where the condition is not satisfied exactly. We assume FW\u0303T 6= 0, so thatX\u2217 6= 0."}, {"heading": "5.2.1 Ideal Case", "text": "Let rk denote the size of basis for the kth class, and nk the number of training samples of that class. We have the following result:\nTheorem 10. For nRBFN problem (12), when the ideal graph condition is satisfied, the fitting error achieves the minimum value, \u01eb = 1, and the spectral risk has the bounds\nr\nn\nK \u2211\nk=1\nnk rk \u2264 \u03b3 \u2264 r. (24)\nThe maximum risk is achieved when there is only one nonzero entry in each column of W\u0303 . The minimum risk is approached when the entries in each column approach distributing uniformly within the corresponding class, it is achieved if and only if rk = 1 for all k.\nProof. By Theorem 3, when the condition holds, perfect reconstruction is achieved, i.e.,X\u2217W\u0303 = F , so \u2016X\u2217W\u0303\u20162F = n, f = 1. Besides, by Proposition 4, X\u2217 becomes an indicator matrix, so \u2016X\u2217\u20162F = r. The spectral risk then equals r\u2016W\u0303\u20162F /n, depending only on \u2016W\u0303\u20162F . Recall that each column of W\u0303 sums to one. For a vector x \u2208 Rp with \u2016x\u20161 = 1, 1/ \u221a p \u2264 \u2016x\u20162 \u2264 1, the upper bound is obtained when there is only one nonzero entry in x, while the lower bound is obtained when the entries distribute uniformly, i.e., xi = 1/p. Therefore, we get \u2211K\nk=1 nk/rk \u2264 \u2016W\u0303\u20162F \u2264 n. The lower bound cannot be reached except when rk = 1 for all k, otherwise, the rank of W\u0303 will not be full, violating the assumption.\nCorollary 11. If both nk and rk are uniform among the classes, i.e., nk = n/K , rk = r/K , for all k, (24) becomes\nK \u2264 \u03b3 \u2264 r. (25)\nThe corollary conveys clear implications. It tells that in the ideal case the spectral risk is lower bounded by the number of classes\u2013a quantity representing \u201cproblem complexity\u201d, and upper bounded by the size of basis\u2013a quantity representing \u201cmodel complexity\u201d. First, the lower bound implies that however the model is the spectral risk will never be lower than the problem complexity. As the number of classes increases, the spectral risk increases too. By Theorem 10, the risk approaches the minimum when the system is balanced, i.e., when the entries are uniform. This is a stronger condition than the uniform row-sum condition in Theorem 5. Although they analyze the deviation of F to the principal subspace by different manners, Theorem 10 by divisive manner while Theorem 5 by subtractive manner, the results are consistent. They both prefer a balanced system. Second, the upper bound implies a tradeoff between the fitting error and the overfitting risk: larger basis means lower error but higher risk. In addition, the size of basis also makes us recall the VC-dimension [64]\u2013a classical measure of model complexity. Although the definitions are different, it happens that the VC-dimension of nRBFN (assuming the basis size is fixed, not a parameter) is also r. This shows some coincidence between the two concepts. Detailed comparisons are beyond the scope of the paper and we omit.\nAlthough the conclusions hold for the ideal case, the theorem provides a foundation for the analysis of noisy\ncase. We now explore."}, {"heading": "5.2.2 Perturbation Case", "text": "We can deem the noisy case as perturbed from an ideal case, and then analyze it with matrix perturbation theory. As will be shown the noise is required to be only tiny.\nDenote the noisy normalized similarity matrix to be W\u0303 \u2032, it can be decomposed as W\u0303 \u2032 = W\u0303 + \u2206W , where W\u0303 is an ideal normalized similarity matrix and\u2206W is noise. Given W\u0303 \u2032, for the purpose of perturbation analysis, we do not need to know the true W\u0303 . Constructing one will suffice. Among many choices, the simplest one is as follows: set the between-class entries of W\u0303 \u2032 to zero, and then normalize each column, this leads to a qualified W\u0303 . Subsequently, the noise is determined, \u2206W = W\u0303 \u2032 \u2212 W\u0303 . It can be proved that, among all the choices of W\u0303 , the noise induced by this manner is the minimum in \u21131-norm sense. Details are omitted.\nBefore presenting the results, we introduce some notations. Let\n\u03be . = \u2016W\u0303 \u2020\u20162\u2016\u2206W\u20162,\nwhere \u2016 \u00b7 \u20162 for a matrix denotes the spectral norm, W\u0303 \u2020 denotes the pseudo-inverse of W\u0303 , which is equivalent to W\u0303T (W\u0303W\u0303T )\u22121 in our case. Let\n\u03b4 . = \u2016\u2206W\u2016F/\u2016W\u0303\u2016F ,\nn\u03c1 . =\n\u221a\nnp/nq, r\u03c1 . = \u221a ra/rb,\nwhere np = maxk nk, nq = mink nk, ra = maxk rk , and rb = mink rk. Finally, let \u01eb \u2032 and \u03b3\u2032 be the fitting error and spectral risk of the noisy case respectively, and \u03b3 the spectral risk of the case W\u0303 . We have the following bounds for \u01eb\u2032 and \u03b3\u2032.\nTheorem 12. For nRBFN problem (12), assume W\u0303 \u2032 = W\u0303 +\u2206W , and W\u0303 is of full rank satisfying the ideal graph condition, if\n\u03be < 1/n\u03c1, (26)\nthen\n\u01eb\u2032 \u2264 n2\u03c1\u03be 2\n(1\u2212 n\u03c1\u03be)2 + 1, (27)\n\u03b3\u2032 \u2264 \u03b3(1 + \u03b4)2 ( 1 + r\u03c1\u03be 1\u2212 n\u03c1\u03be )2 . (28)\nProof. Denote the solution of the noisy case to be X \u2032, and the ideal case X . For simplicity, denote X \u2032W\u0303 \u2032 by Y \u2032, andXW\u0303 by Y . Let the difference denote by\u2206, e.g.,\u2206X = X \u2032\u2212X ,\u2206Y = Y \u2032 \u2212Y . We will apply the following results from matrix perturbation theory ([17] Theorem 18.1):\n\u2016\u2206y\u20162/\u2016y\u20162 \u2016\u2206W\u20162/\u2016W\u0303\u20162 \u2264 \u03ba cos \u03b8 , \u2016\u2206x\u20162/\u2016x\u20162 \u2016\u2206W\u20162/\u2016W\u0303\u20162 \u2264 \u03ba+ \u03ba 2 tan \u03b8 \u03b7 ,\nwhere x is any row ofX , and y is the corresponding row of Y . \u03ba is the condition number of W\u0303 , \u03ba . = \u2016W\u0303\u20162\u2016W\u0303 \u2020\u20162. \u03b8 is the included angle between the target vector and its reconstruction y. \u03b7 . = \u2016x\u20162\u2016W\u0303\u20162/\u2016y\u20162. In our context, W\u0303 satisfies the ideal condition, therefore \u03b8 = 0. The above results can be reduced to much simpler forms:\n\u2016\u2206y\u20162/\u2016y\u20162 \u2016\u2206W\u20162/\u2016W\u0303\u20162 \u2264 \u03ba, \u2016\u2206x\u20162/\u2016x\u20162 \u2016\u2206W\u20162/\u2016W\u0303\u20162 \u2264 \u03ba.\nBy the definitions of \u03ba and \u03b4, they lead to \u2016\u2206y\u20162/\u2016y\u20162 \u2264 \u03be, (29) \u2016\u2206x\u20162/\u2016x\u20162 \u2264 \u03be. (30)\nWe now begin the proof.\nFirst, we consider \u2016Y \u2032\u2016F , since both \u01eb\u2032 and \u03b3\u2032 involve this denominator.\n\u2016Y \u2032\u2016F \u2265 \u2016Y \u2016F \u2212 \u2016\u2206Y \u2016F = \u2016Y \u2016F (1\u2212 \u2016\u2206Y \u2016F /\u2016Y \u2016F ). (31)\nTo apply (29), we have to convert the Frobenius norm of Y to the \u21132 norm of its rows Yk\u2019s.\n\u2016\u2206Y \u2016F/\u2016Y \u2016F = \u221a \u2211 k \u2016\u2206Yk\u201622 \u2211\nk \u2016Yk\u201622\n\u2264 \u221a\nK\u2016\u2206Yu\u201622 K\u2016Yl\u201622\n= \u2016\u2206Yu\u20162/\u2016Yl\u20162 = (\u2016Yu\u20162/\u2016Yl\u20162)(\u2016\u2206Yu\u20162/\u2016Yu\u20162),\nwhere subscript u = argmaxk \u2016\u2206Yk\u20162, and l = argmink \u2016Yk\u20162. Note that W\u0303 satisfies the ideal condition, by Theorem 3, Y = F , and we have\n\u2016\u2206Y \u2016F /\u2016Y \u2016F \u2264 (\u2016Fu\u20162/\u2016Fl\u20162)(\u2016\u2206Yu\u20162/\u2016Yu\u20162)\n\u2264 \u221a\nnp/nq(\u2016\u2206Yu\u20162/\u2016Yu\u20162) = n\u03c1(\u2016\u2206Yu\u20162/\u2016Yu\u20162) \u2264 n\u03c1\u03be,\n(32)\nwhere the last line invokes (29). Substituting (32) into (31), we obtain\n\u2016Y \u2032\u2016F \u2265 \u2016Y \u2016F (1\u2212 n\u03c1\u03be). (33)\nBy the assumption, \u03be < 1/n\u03c1, we are sure 1\u2212 n\u03c1\u03be > 0 so that \u2016Y \u2032\u2016F > 0. Second, using (33) and then (32), the fitting error can be estimated\n\u01eb\u2032 = \u2016F \u2212 Y \u2032\u20162F \u2016Y \u2032\u20162F + 1 \u2264 \u2016F \u2212 Y \u2212\u2206Y \u2016 2 F \u2016Y \u20162F (1\u2212 n\u03c1\u03be)2 + 1 = \u2016\u2206Y \u20162F \u2016Y \u20162F (1\u2212 n\u03c1\u03be)2 + 1 = n2\u03c1\u03be 2 (1\u2212 n\u03c1\u03be)2 + 1.\nThird, we consider \u2016X \u2032\u2016F .\n\u2016X \u2032\u2016F \u2264 \u2016X\u2016F + \u2016\u2206X\u2016F = \u2016X\u2016F (1 + \u2016\u2206X\u2016F/\u2016X\u2016F ). (34)\nBy Proposition 4,X is an indicator matrix. Following the skill of the case \u2016\u2206Y \u2016F/\u2016Y \u2016F , we get\n\u2016\u2206X\u2016F/\u2016X\u2016F \u2264 (\u2016Xu\u2032\u20162/\u2016Xl\u2032\u20162)(\u2016\u2206Xu\u2032\u20162/\u2016Xu\u2032\u20162) \u2264 \u221a\nra/rb(\u2016\u2206Xu\u2032\u20162/\u2016Xu\u2032\u20162) = r\u03c1(\u2016\u2206Xu\u2032\u20162/\u2016Xu\u2032\u20162) \u2264 r\u03c1\u03be.\n(35)\nwhere the last line invokes (30). Substituting (35) into (34), we obtain\n\u2016X \u2032\u2016F \u2264 \u2016X\u2016F (1 + r\u03c1\u03be). (36)\nForth, using (33) and (36), we can get the bound of \u221a \u03b3\u2032.\n\u221a \u03b3\u2032 = \u2016W\u0303 \u2032\u2016F \u2016X \u2032\u2016F \u2016Y \u2032\u2016F\n\u2264 \u2016W\u0303 \u2032\u2016F \u2016X\u2016F (1 + r\u03c1\u03be) \u2016Y \u2016F (1\u2212 n\u03c1\u03be) = \u2016X\u2016F\u2016W\u0303\u2016F\n\u2016Y \u2016F \u2016W\u0303 \u2032\u2016F \u2016W\u0303\u2016F ( 1 + r\u03c1\u03be 1\u2212 n\u03c1\u03be )\n\u2264 \u221a\u03b3 \u2016W\u0303\u2016F + \u2016\u2206W\u2016F \u2016W\u0303\u2016F ( 1 + r\u03c1\u03be 1\u2212 n\u03c1\u03be ) = \u221a \u03b3(1 + \u03b4) ( 1 + r\u03c1\u03be\n1\u2212 n\u03c1\u03be ) .\nFinally, (28) is obtained.\nMoreover, we have the following two corollaries.\nCorollary 13. \u03b4 \u2264 \u03be, and \u03b3\u2032 \u2264 \u03b3(1 + \u03b4)2 ( 1 + r\u03c1\u03be\n1\u2212 n\u03c1\u03be )2 \u2264 \u03b3 (1 + r\u03c1\u03be) 4 (1\u2212 n\u03c1\u03be)2 . (37)\nProof. Similar to the relation between the Frobenius norm and the \u21132 norm in the proof of Theorem 12, we can relate \u03b4 to \u03be. Let the singular values of \u2206W and W\u0303 in descending order to be \u03c4i\u2019s and \u03c3i\u2019s respectively. By assumption, W\u0303 is of full rank, so \u03c3r 6= 0. Note that \u2016W\u0303 \u2020\u20162 = 1/\u03c3r. We have\n\u03b4 = \u2016\u2206W\u2016F/\u2016W\u0303\u2016F = \u221a \u2211 i \u03c4 2 i \u2211\ni \u03c3 2 i\n\u2264 \u03c41/\u03c3r = \u2016\u2206W\u20162\u2016W\u0303 \u2020\u20162 = \u03be.\nFurther, r\u03c1 \u2265 1, so \u03b4 \u2264 r\u03c1\u03be, and (37) is obtained.\nCorollary 14. If the classes and basis are even, i.e., n\u03c1 = 1, r\u03c1 = 1, then the condition (26) becomes \u03be < 1, and (27), (28) become\n\u01eb\u2032 \u2264 \u03be 2\n(1\u2212 \u03be)2 + 1, (38)\n\u03b3\u2032 \u2264 \u03b3(1 + \u03b4)2 ( 1 + 2\u03be 1\u2212 \u03be )2 \u2264 \u03b3 (1 + \u03be) 4 (1\u2212 \u03be)2 . (39)\nThe above results suggest that, comparing with the ideal case, the fitting error and spectral risk increase by a factor of n2\u03c1\u03be 2/(1 \u2212 n\u03c1\u03be)2 + 1 \u2265 1 and (1 + r\u03c1\u03be)4/(1 \u2212 n\u03c1\u03be)2 \u2265 1 respectively. When \u03be \u2192 0, they approach 1. \u03be < 1 is a condition frequently appeared in the perturbation analysis of linear system [61]. We have reproduced it in our context (stemming from (33)). \u03be measures the noise magnitude \u2016\u2206W\u20162 (i.e., the largest singular value of \u2206W ) relative to the smallest singular value of W\u0303 . In practice, \u03be < 1 requires the noise to be only tiny."}, {"heading": "5.3 Error and Risk Bounds of LRC", "text": "We derive the bounds for un-regularized LRC in the ideal case. The results are not as significant as those of nRBFN, so less emphasis is put. Denote \u03b6\u0304 . = \u2016A\u20162F /n to be the mean squared length of original data, and \u03b6\u03c1 . = maxi \u2016Ai\u201622/mini \u2016Ai\u201622 to be the ratio between the maximum and minimum length, and \u03b8u to be the maximum included angle between original data pairs. We have\nTheorem 15. For LRC problem (1), when the ideal graph condition is satisfied, the fitting error achieves the minimum value, \u01eb = 1, and the spectral risk\n\u03b3 = 1 + \u03b6\u0304\n\u03b2 , (40)\n1 + 1 \u03b6\u03c1| cos \u03b8u| \u2264 \u03b3 \u2264 1 + \u03b6\u03c1| cos \u03b8u| . (41)\nProof. We calculate \u03b3 using the spectral expression \u03b3 = \u2211r\ni=1 a\u0303 2 i /\u03c3\u0303 2 i . By Theorem 1, rows of F are the leading\nsingular vectors of A\u0303, corresponding to singular value \u221a \u03b2n. It implies a\u030321 + \u00b7 \u00b7 \u00b7+ a\u03032K = 1 and a\u0303i = 0 for all i > K+1, and \u03c3\u030321 = \u00b7 \u00b7 \u00b7 = \u03c3\u03032K . Thus, we have \u03b3 = 1/\u03c3\u030321 . Since \u03c3\u030321 = \u03b2n/\u2016A\u0303\u20162F = \u03b2n/(\u03b2n+\u2016A\u20162F ), (40) is obtained. Next, by the definition of \u03b2 (3), assume Ai\u2217 , Aj\u2217 achievemini,j A T i Aj , then \u03b2 = \u2212\u2016Ai\u2217\u20162\u2016Aj\u2217\u20162 cos \u03b8u, where \u03b8u > \u03c0/2. Hence, \u03b6\u0304/\u03b2 = \u03b6\u0304/(\u2016Ai\u2217\u20162\u2016Aj\u2217\u20162| cos \u03b8u|), and (41) is easy to verify."}, {"heading": "5.4 Effect of \u21132-norm Regularization", "text": "First, we extend the previous definitions of fitting error and spectral risk to regularized linear regression, then we show the one-one correspondence between the two quantities and the regularized objective, and finally we study the effect of the regularization.\nLet the regularized linear regression problem be\ng(D\u0303) = min D\u0303\n\u2016F \u2212 D\u0303A\u20162F + \u03bb\u2032\u2016D\u0303\u20162F , (42)\nwhere \u03bb\u2032 > 0. The previous four definitions are extended trivially by replacing with the new D\u0303\u2217 of problem (42). The spectral expressions are changed to be:\nProposition 16. For regularized linear regression problem (42),\n\u03b1 =\nr \u2211\ni=1\na2i\u03c3 2 i\n(\u03c32i + \u03bb \u2032)2\n; \u03b3 =\nr \u2211\ni=1\na\u0303\u2032 2\ni\n\u03c3\u03032i ,\nwhere a\u20322i . = a2i\u03c3 4 i /(\u03c3 2 i + \u03bb \u2032)2, and a\u0303\u2032 2 i . = a\u20322i / \u2211 i a \u20322 i ;\nf = n\u2212 r \u2211\ni=1\na2i (1 \u2212 \u03bb\u20322\n(\u03c32i + \u03bb \u2032)2\n); \u01eb = n\u2212\u2211i 2a2i\u03c32i \u03bb\u2032/(\u03c32i + \u03bb\u2032)2 \u2211\ni a 2 i\u03c3 4 i /(\u03c3 2 i + \u03bb\n\u2032)2 .\nThe range of relative spectral risk is \u03b3 \u2265 1, and 1 \u2264 1/\u03c3\u030321 \u2264 \u03b3 \u2264 1/\u03c3\u03032r . The minimum value 1 is achieved, if and only if r = 1. The range of relative fitting error is \u01eb > 1. If the row-space of data completely covers the target, then lim\u03bb\u2032\u21920 \u01eb = 1, but the minimum value 1 cannot be achieved unless \u03bb\u2032 = 0.\nProof. We prove the four spectral expressions, the remaining results are apparent. First, we have D\u0303\u2217 = FAT (AAT+ \u03bb\u2032I)\u22121 = FV T\u2206UT , where \u2206 is a diagonal matrix with \u2206ii = \u03c3i/(\u03c32i + \u03bb\n\u2032), and D\u0303\u2217A = FV T\u039bV , where \u039bii = \u03c3 2 i /(\u03c3 2 i + \u03bb\n\u2032). \u03b1 is easy to obtained by the expression of D\u0303\u2217, and then\n\u03b3 = \u2016D\u0303\u2217\u20162F \u2016A\u20162F \u2016D\u0303\u2217A\u20162F =\n(\n\u2211\ni a 2 i\u03c3 2 i /(\u03c3 2 i + \u03bb\n\u2032)2 )( \u2211\ni \u03c3 2 i\n)\n\u2211\ni a 2 i\u03c3 4 i /(\u03c3 2 i + \u03bb\n\u2032)2 =\n(\n\u2211 i a \u20322 i /\u03c3 2 i\n)(\n\u2211\ni \u03c3 2 i\n)\n\u2211 i a \u20322 i\n= \u2211\ni\na\u0303\u2032 2\ni\n\u03c3\u03032i .\nNext, f = \u2016F \u2212FV T\u039bV \u20162F = \u2016FV T\u039b\u2032V \u20162F +\u2016FV\u0304 T V\u0304 \u20162F , where\u039b\u2032ii = \u03bb\u2032/(\u03c32i +\u03bb\u2032) and V\u0304 is the complement of V . By \u2016FV T\u039b\u2032V \u20162F = \u2211 i a 2 i\u03bb \u20322/(\u03c32i + \u03bb \u2032)2 and \u2016FV\u0304 T V\u0304 \u20162F = \u2016F\u20162F \u2212 \u2016FV TV \u20162F = n \u2212 \u2211 i a 2 i , f is obtained. Finally, based on the expressions of D\u0303\u2217A and f , \u01eb can be obtained.\nUn-regularized linear regression is a special case of regularized linear regression. When setting \u03bb\u2032 = 0, all the above expressions reduce to the forms in Section 5.1.\nNext, it is not hard to demonstrate the correspondence relationship.\nTheorem 17. The regularized linear regression achieves a tradeoff between fitting error and spectral risk:\ng(D\u0303\u2217) = f + \u03bb\u2032\u03b1, (43)\ng(D\u0303\u2217)\n\u2016D\u2217A\u20162F = \u01eb+ \u03bb\u03b3 \u2212 1, (44)\nwhere \u03bb\u2032 = \u03bb\u2016A\u20162F .\nRemember that in Section 4.1, \u03bb\u2032 = \u03bb\u2016A\u20162F is set for the ease of parameter tuning. Here, coincidentally, it plays another role.9 The relationship (44) is more intuitive, since both \u01eb and \u03b3 have the same normalized range, and are traded off via a weight \u03bb. The value of (44) can be compared between different data of the same model, while (43) cannot.\nFinally, we rigorously study the tradeoff effect from another perspective: how the fitting error and spectral risk\nchange when we impose regularization?\nTheorem 18. Compared with linear regression (16), the fitting error of regularized linear regression (42) is increased, while the spectral risk is reduced. In precise, 1. f(D\u0303\u2217) > f(D\u2217), 2. \u01eb(D\u0303\u2217) > \u01eb(D\u2217), 3. \u03b1(D\u0303\u2217) < \u03b1(D\u2217), 4. \u03b3(D\u0303\u2217) < \u03b3(D\u2217) if {\u03c3i}i\u2208\u2126 are not uniform (|\u2126| > 1), where \u2126 is the index set of all nonzero projections \u2126 = {j|aj 6= 0, j = 1, . . . , r}, \u03b3(D\u0303\u2217) = \u03b3(D\u2217) otherwise.\nProof. Assertions 1 and 3 are easy to verify by comparing the spectral expressions in Proposition 16 with those of when setting \u03bb\u2032 = 0. Assertion 2 can be established by noting \u01eb(D\u0303\u2217) = f(D\u0303\u2217)/\u2016D\u0303\u2217A\u20162F + 1, and the numerator increases while the denominator decreases. The final assertion 4 is less obvious, since both \u2016D\u0303\u2217\u20162F and \u2016D\u0303\u2217A\u20162F decrease. Some efforts have to be made.\nFirst of all, if {\u03c3i}i\u2208\u2126 are uniform, in particular \u2126 has only one member or r = 1, it is clear that \u03b3(D\u0303\u2217) = \u03b3(D\u2217). Next, we deal with the remaining case. Although a\u2032i < ai for all i, the ratios of decrement are different. Denote di = \u03c3 4 i /(\u03c3 2 i + \u03bb\n\u2032)2, observe that d1 \u2265 d2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 dr and there is at least one \u201c>\u201d among {di}i\u2208\u2126. Focusing on a\u0303\u2032 2\ni \u2212 a\u03032i , a\u0303\u2032 2 i \u2212 a\u03032i = a2i di \u2211\ni a 2 i di\n\u2212 a 2 i \u2211\ni a 2 i\n= a2i ( di \u2211\ni a 2 i di \u2212 1\u2211 i a 2 i ).\ndi/ \u2211 i a 2 i di\u22121/ \u2211 i a 2 i , i = 1, . . . , r is a descending sequence. In the first term, d1/(a 2 1d1+a 2 2d2+ \u00b7 \u00b7 \u00b7+a2rdr) = 1/(a21 + a 2 2(d2/d1) + \u00b7 \u00b7 \u00b7+ a2r(dr/d1)) > 1/ \u2211 i a 2 i , since d1 \u2265 di for all i > 1 and {di}i\u2208\u2126 are not uniform, thus d1/ \u2211 i a 2 i di\u2212 1/ \u2211 i a 2 i > 0. Likewise, we can prove the last term dr/ \u2211 i a 2 i di\u2212 1/ \u2211 i a 2 i < 0. Assume the first k terms are positive, and the remaining terms negative, finally we have\n\u03b3(D\u0303\u2217)\u2212 \u03b3(D\u2217) = \u2211\ni\na\u0303\u2032 2\ni \u2212 a\u03032i \u03c3\u03032i\n= k \u2211\np=1\na2p \u03c3\u03032p ( dp \u2211 i a 2 i di \u2212 1\u2211 i a 2 i ) + r \u2211\nq=k+1\na2q \u03c3\u03032q ( dq \u2211 i a 2 i di \u2212 1\u2211 i a 2 i )\n< 1\n\u03c3\u03032k\nk \u2211\np=1\na2p( dp \u2211\ni a 2 i di \u2212 1\u2211 i a 2 i ) + 1 \u03c3\u03032k+1\nr \u2211\nq=k+1\na2q( dq \u2211\ni a 2 i di \u2212 1\u2211 i a 2 i )\n= c \u03c3\u03032k \u2212 c \u03c3\u03032k+1 < 0,\nwhere c = \u2211k\np=1 a 2 p( dp\u2211 i a 2 i di \u2212 1\u2211 i a 2 i ) > 0. Note that \u2211r q=k+1 a 2 q( dq\u2211 i a 2 i di \u2212 1\u2211 i a 2 i ) = \u2212c, since \u2211 i a\u0303 \u20322 i \u2212 a\u03032i =\n0.\nThe assertion 4 has some indications. If the singular values are totally different, the nonuniform condition must hold, and the spectral risk must strictly decrease. Conversely, if the singular values are totally uniform, including\n9The correspondence is unexpected beforehand, especially (44). When we conceiving the definitions of both absolute and relative spectral risk, they are actually inspired by the structure of the solution D\u2217 of regularized LRC, rather than by the objective. Only the relative fitting error is designed after the observation of the correspondence.\nAlgorithm 1 Soft KNN for finding basis (SKNN)\n1: Input: data A \u2208 Rp\u00d7n, labels Y \u2208 Rn, number of neighbors k, confidence threshold t 2: Output: basis G \u2208 Rp\u00d7r 3: Find the k nearest neighbors of each sample, record their indices, and compute their distances Q \u2208 Rk\u00d7n,\nwhere Qij is the Euclidian distance of the jth sample to its ith neighbor 4: Set the width of Gaussian kernel \u03c3 = \u03a3i,jQij/(kn) 5: Build the similarity matrixWij = exp{\u2212Q2ij/(2\u03c32)}, \u2200i, j 6: Normalize the columns ofW so that it becomes probability matrix W\u0303 = W diag(1TW )\u22121 7: For each sample, aggregate the probability of its neighbors that have the same label as it, resulting in a confi-\ndence vector T \u2208 Rn 8: Choose the samples whose Tj < t as the basis G 9: If there is any class missed, add the sample with the lowest confidence in this class to the basis\nAlgorithm 2 The training of nRBFN\n1: Input: training data A \u2208 Rp\u00d7n, labels Y \u2208 Rn, regularization weight \u03bb, number of neighbors k, confidence threshold t 2: Output: basis G \u2208 Rp\u00d7r, width of Gaussian kernel \u03c3, weight matrixX \u2208 RK\u00d7r 3: Find the basis via soft KNN G =SKNN(A, Y, k, t) 4: Compute pairwise distance Qij = \u2016Gi \u2212Aj\u20162, \u2200i, j 5: Set the width of Gaussian kernel \u03c3 = \u03a3i,jQij/(rn) 6: Build the kernel matrixWij = exp{\u2212Q2ij/(2\u03c32)}, \u2200i, j 7: Normalize the columns of the kernel matrix W\u0303 = W diag(1TW )\u22121 8: Convert the label vector Y to indicator matrix F 9: Compute the weight matrixX = FW\u0303T (W\u0303W\u0303T + \u03bb\u2016W\u0303\u20162F I)\u22121\nthe special case r = 1, the spectral risk will not decrease. It indicates that, not in all cases, employing regularization will help to improve the generalization performance. In a balanced system, where the singular values are uniform, regularization is not necessary. Although it is an exceptional case, this point is not easily observed by the traditional Bayesian view. A practical implication of the result is that if the fitting error is under control, designing a balanced system is preferable.\nWe highlight the spectral risk of the uniform case in the following corollary.\nCorollary 19. If the singular values of the data matrix are uniform, \u03c31 = \u03c32 = \u00b7 \u00b7 \u00b7 = \u03c3r, then \u03b3(D\u0303\u2217) = \u03b3(D\u2217) = r, which goes linearly with the size of basis. In this case, the spectral risk is independent of the target vectors so long as they are not orthogonal to the data.\nWe encounter the spectral risk equaling to the basis size again. But the context here is different to that of\nTheorem 10. This subsection\u2019s results are general, independent of the ideal graph condition."}, {"heading": "6 Basis Selection Strategy", "text": "To make nRBFN work, we have to settle the basis selection problem and associated parameter setting. Unfortunately, it is still an open question to find the optimal basis for (n)RBFN. In this paper, we are contented with a\nAlgorithm 3 The testing of nRBFN\n1: Input: test data B \u2208 Rp\u00d7m, basis G \u2208 Rp\u00d7r, width of Gaussian kernel \u03c3, weight matrixX \u2208 RK\u00d7r 2: Output: predicted labels l \u2208 Rm 3: Build the kernel matrix (WB)ij = exp{\u2212\u2016Gi \u2212Bj\u201622/(2\u03c32)}, \u2200i, j 4: Normalize the columns of the kernel matrix W\u0303B = WB diag(1 TWB) \u22121 5: Compute the predicted indicator matrix F\u0302 = XW\u0303B 6: Obtain the predicted labels lj = argmaxk F\u0302kj , \u2200j\nstrategy that is easy to use yet can deliver good performance.\nTraditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one. There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods. Generally, the gradient descent methods are time-consuming and lose one of the main advantages of RBFN compared with traditional neural networks. The incremental learning methods are also expensive and complex. The most frequently applied basis selection strategy is the clustering based method, especially K-means, due to its efficiency. The Gaussian width can be set via some heuristics [39, 57, 18], e.g., the maximum distance between basis vectors, or learnt by gradient descent, or searched via model selection. It was reported that the performance is not so sensitive to this parameter [59], especially for nRBFN [9].\nIn this paper, we devise a strategy that chooses the samples near the boundaries of different classes as the basis. The basis size is determined by a confidence parameter that is much easier to set. The Gaussian width is set as the mean distance of the training set to the basis.\nRBFN has its origin in function approximation. The basis is regarded as templates or stereotypical patterns. It is this view that leads to the clustering heuristics [55]. However, the classification problem is different from the general regression problem that is not concerned with the separability of classes. For classification, samples near the boundaries may deliver more crucial information for the separation of classes than those in the inner part. This has been investigated by [55, 9, 47]. Usually, the idea is borrowed from SVM [15], where the boundary points are called support vectors.\nThe boundary points can be identified by their classification confidence. That is, for each sample, if the probability belonging to its labeled class is known, then a sample can be identified as boundary point if this probability is below some preassigned threshold. Soft KNN may be the simplest tool that meets this demand. The detailed algorithm is presented in Algorithm 1. Note the basis size is implicitly determined by the confidence threshold (within range (0,1]), which is easy to set due to clear interpretation. In this way, on the one hand, the basis size can be determined according to the complexity of data distribution: when the classes overlap more, more samples are recruited as basis, vice versa. On the other hand, we can flexibly control the tradeoff between accuracy and resource burden: larger t implies better accuracy but higher computational cost, vice versa. Finally, the training and testing algorithms of nRBFN are shown in Algorithm 2 and Algorithm 3 respectively. The time complexity of SKNN is O(n2(p + logn)), dominated by the computation of Euclidean distance and search of nearest neighbors. The time complexity of training nRBFN excluding SKNN is O(prn + r2n), and the total isO(n2(p+logn)+r2n), generally dominated by SKNN. Lastly, the complexity of testing isO((p+K)rm)."}, {"heading": "7 Experiments", "text": "The experiments consist of two parts: demonstrating the performance of nRBFN, and evaluating the error and risk of nRBFN and LRC.\nThe experiments are carried out on a set of benchmark data sets, shown in Table 1.10 The data sets include classical small data sets of UCIMachine Learning Repository [35]: iris, wdbc, glass, sonar, wine; high-dimensional and small-sample-size gene data: colon, leukemia; human face images: ORL, AR, YaleB; high-dimensional and large-sample-size text data: TDT2, 20news; and large-sample-size hand-written digit images: USPS, MNIST. If the original data set does not have a training-testing split, we use the first half of each class as the training set, except glass, sonar, and YaleB, where random splits have been performed to avoid particular sample sequences. Following common practice, each face image of ORL, AR, and YaleB is normalized to unit length. The procedures are run on a server with 32GB memory and 24 cores CPU of 2.93GHz.11 In the results below, test error refers to the classification error on test set.\n10The data sets come from http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/ and\nhttp://www.cad.zju.edu.cn/home/dengcai/Data/data.html. 11The high-performance machine is used only for the purpose of convenience rather than necessity for the experiments."}, {"heading": "7.1 Performance of nRBFN", "text": "We first evaluate the parameters of nRBFN, then compare the performance of nRBFN with some other algorithms."}, {"heading": "7.1.1 Evaluation of the Parameters", "text": "We evaluate the influence of the three parameters of nRBFN, \u03bb, t, and k, on the classification performance. The results on five representative data sets are shown in Figure 1. Three default values \u03bb = 10\u221213, t = 0.9, and k = 20 are used. When one parameter varies, the others are fixed with the default values. We find that:\n(1) The test error generally decreases as \u03bb decreases, and reaches a plateau after 10\u22128. The exceptional case is iris, on which, due to insufficient sampling, larger \u03bb is needed to avoid noisy subspace. Nevertheless, the difference is not large. Considering methods using regularization are usually plagued by the problem of tuning the regularization weight, nRBFN shows a desirable feature: as a rule of thumb, \u03bb < 10\u22128 generally delivers nearoptimal result of nRBFN. Our experience showed that it also holds for the other data sets we have tested. This rule will be justified further from the error-and-risk perspective in Section 7.2.3. Remember the actual regularization weight is \u03bb\u2032 = \u03bb\u2016W\u0303\u20162F . It is adaptive to the data. The principles of this setting are provided by the row-space projection view and error-and-risk analysis. If \u03bb\u2032 is set as a whole, the above rule no long holds, and we have to search for the optimal value in a wide range.\n(2) The error steadily decreases as t increases, as expected. Note, in essence, t is not merely a parameter to be tuned, it is also a choice of ours. It virtually controls the tradeoff between accuracy and resource burden. Our experience suggests t = 0.9 strikes a good balance for general data sets. It will be justified further in below experiments.\n(3) The performance is not sensitive to k. It becomes stable after k \u2265 8. The experiments show that the parameter setting of nRBFN is easy. t is a matter of choice. k has minor impact on the performance. There is only one parameter, \u03bb, needed to be tuned, but its determination is not difficult. In the following, we fix t = 0.9, k = 20, and for each data set, \u03bb will be selected via 5-fold cross validation over three values {10\u22125, 10\u22129, 10\u221213}."}, {"heading": "7.1.2 nRBFN v.s. Other Classification Algorithms", "text": "In this subsection, we compare the classification performance of nRBFN with some other algorithms. The results are shown in Table 1. The involved algorithms include: (1) KNN (k=20), (2) LRC (\u03bb is selected via 5-fold cross validation over 10{\u221213,\u221212,...,\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,...,12} and \u03c3 is selected over 1.4{\u22124,\u22123,...,4} \u00d7 \u03c3 of nRBFN via 5-fold cross validation). Except RBFNnl and SVM, the others are implemented by us using MATLAB.\ndiverse nature. However, when we compare them pair-wise, the advantage of nRBFN becomes prominent.\n(1) As a linear model, the performance of LRC is limited, as the ideal graph condition implies. It mainly preforms well on high-dimensional data where p \u2265 n, since in this case the rank of row space is full. Results on TDT2 and 20news are absent, since it fails to run on such big data.\n(2) The results of nRNwr are inferior to nRBFN, confirming the importance of regularization. (3) nRBFN generally outperformsROLS, RBFNnl, and nRNrb, showing the effectiveness of the basis selection strategy of nRBFN. ROLS is resource-consuming, it is unable to run on big data sets. Note that it is hard for RBFNnl and nRNrb themselves to determine a suitable basis size. The strategy of nRBFN, which implicitly controls the size by a user-friendly confidence threshold, makes it much easier.\n(4) nRBFN generally obtains better results than SVM on these data sets. Since performance depends on the basis sizes, we list them in Table 2. The support vectors of SVM are found automatically, we note on the first ten smaller data sets, the bases found by nRBFN when setting t = 0.9 have sizes roughly consistent with those of SVM. It means the basis size has been determined properly according to the complexity of data distribution: when the classes overlap more, more points will be selected as basis, and vice versa. When setting t = 0.9, most of the uncertain points helpful for determining the classification boundaries have been included. For human face images, it is well-known that the data are clustered according to lightening, expressions, poses, rather than identity. For example, a left lightening cluster may include images from all identities. Thus, almost all points serve as basis. On the four larger sets, the bases obtained by nRBFN are more economical than those of SVM, however, the test error are not necessarily worse."}, {"heading": "7.1.3 nRBFN v.s. SVM with the Same Basis Size", "text": "Next, for a fair comparison between nRBFN and SVM, we let the basis size of nRBFN to be equal to the size of support vectors. This is done by choosing the specific number of samples with the lowest confidence as the basis. The other parameters are set as before. The results are shown in Table 3. In this test, nRBFN performs even better, and the time cost is comparable to that of SVM. Note that, the time cost does not include the part of cross validation. SVM usually needs to run hundreds of times during cross validation, while nRBFN only runs a dozen times."}, {"heading": "7.1.4 nRBFN with fixed parameters v.s. SVM", "text": "Finally, we test the performance of nRBFN with a fixed set of parameters (\u03bb = 10\u221213, t = 0.9, k = 20). In this case, nRBFN involves no model selection, but the results are still not far from those of SVM, as Table 4 shows."}, {"heading": "7.2 Empirical Evaluation of Error and Risk", "text": "In this section, we empirically evaluate the fitting error and spectral risk of nRBFN and LRC, and investigate their influence on the performance."}, {"heading": "7.2.1 The Effect of Regularization: nRNwr v.s. nRBFN", "text": "We evaluate the effect of regularization through comparing nRNwr (nRBFN without regularization) and nRBFN. The results are shown in Table 5. Due to rank deficiency (rank(W\u0303 ) < r), results of nRNwr on glass, wine, TDT2, and 20news are erroneous and not shown. According to the table, on the sets marked with bold face, regularization significantly reduces the spectral risk, mostly by orders of magnitude, while the fitting error is increased slightly, but not yet sacrificed much. On all these sets, the test error have been reduced. This demonstrates the effectiveness of regularization. On the other data sets, regularization affects not much. Since \u03bb has been selected for optimal performance, it implies that on these data sets the found row-subspaces by nRNwr probably contain discriminative features rather than noise. Note that, most of these data sets have large sample sizes, indicating sufficient sampling. Even though, regularization does not provide better result in this case, it does not undermine the performance\u2013so long as \u03bb is set properly. Considering its capability in dealing with both insufficient and sufficient sampling cases, regularization should be applied."}, {"heading": "7.2.2 LRC v.s. nRBFN", "text": "We compare the fitting error (\u01eb), spectral risk (\u03b3), the tradeoff (\u01eb+\u03bb\u03b3), and the test error between LRC and nRBFN, and study their influence on the performance. The results are shown in Table 6.\nTheoretically, two factors act together contributing to the performance of nRBFN/LRC. One is the rank of basis/data row-space. Despite of the idea graph condition, larger rank implies better fitting capacity. For nRBFN, this can be controlled via t. In the extreme case of full rank, zero fitting error can be achieved (although spectral risk is not guaranteed). The other factor is the ideal graph condition. Despite that the rank may be low, so long as the condition is nearly met, both the fitting error and spectral risk will be low.\nWe analyze Table 6 by two parts. For the data sets from colon to AR, LRC performs equally or better than nRBFN. These data sets are distinct from the others in that the ranks of LRC and nRBFN are exactly the same, and are full. Since both the fitting error and spectral risk depend on the rank, the comparison between LRC and nRBFN is thus fair. The lower test error of LRC can be attributed to the lower spectral risk. Although nRBFN has lower fitting error, it has higher risk of overfitting. Note that, these data sets consist of the gene data and face images. For nRBFN, on these sets all samples are recruited as basis, indicating the classes overlap heavily. Consequently the ideal graph condition will not be met well and the spectral risk can be high. In addition, although the fitting error of LRC seem higher, we found that on these four data sets (plus YaleB) the classification error of both LRC and nRBFN are uniformly zero. All these support the lower test error of LRC.\nFor the data sets marked with boldface, nRBFN performs better. This can be attributed to the higher ranks and lower fitting error. As to the higher spectral risk, first, the spectral risk is proportionally related to the rank. Second, higher value of spectral risk does not mean error actually happened, but a warning is signalled. The found subspace may be noise or discriminative features. Third, the cross validation determined a much smaller weight to offset the spectral risk, so that the product \u03bb\u03b3 is generally below the magnitude of 10\u22122 for nRBFN (10\u22121 for LRC). In contrast to the fitting error that is above 1, the weighted risk is quite small, and the tradeoff \u01eb + \u03bb\u03b3 \u2248 \u01eb. On the one hand, it implies that the performance is overwhelmed by the fitting error. On the other hand, the large offset of nRBFN suggests that the risk warning has been ignored. The cross validation learnt that, after regularization, the warning does no harm, the found subspace was judged to be discriminative features. These analyses also apply to the previous four data sets, since the margins of test error are not large. We conclude nRBFN performs better overall. This is due to the consistently lower fitting error and not severe overfitting."}, {"heading": "7.2.3 How Error and Risk of nRBFN Change as \u03bb and t Vary", "text": "The setting is the same as Section 7.1.1, now we focus on the fitting error and spectral risk. The results are shown in Figure 2 and Figure 3.\nWe observe from Figure 2 that: (a) As \u03bb decreases, the fitting error steadily decrease. This is as expected. What deserves notice is that the error of the five data sets coincidentally converge at the point \u03bb = 10\u22128, and reach a plateau of around 1 thereafter. This is consistent with the test error in Figure 1(a). The convergences confirm the law of \u03bb: as a rule of thumb, setting \u03bb < 10\u22128 generally delivers near-optimal result of nRBFN. (b) For most data sets, after \u03bb < 10\u221212, there exists a fairly stable converging range of spectral risk: 108\u223c10. This also holds\nfor most other data sets not shown, especially large data sets. The cases of the two gene data are abnormal, they inherently have low spectral risk, variations can be observed only when \u03bb becomes large. (c), (d) The weighted spectral risk (\u03bb\u03b3) and its proportion to the tradeoff converge after some points of \u03bb. They are not comparable to the fitting error.\nNext, Figure 3 shows that: (a) As t increases, the fitting error steadily decrease, as expected. At t = 1, all error converge close to 1, due to full ranks of the similarity matrices. The fitting capacity of nRBFN is a matter of choice. At t = 0.9, the error have reached a suitable level, so setting t = 0.9 as the default value meets general situation. (b) We observe that the spectral risk again converges to the range of 108\u223c10. The convergences of big data sets are smoother than those of smaller ones. (c), (d) The weighted spectral risk is again not comparable to the fitting error.\nIt should be noted that the definitions of relative measures are essential, the above laws would disappear, if we\nsimply use the absolute measures."}, {"heading": "8 Related Work", "text": "We discuss the related work of nRBFN, regularization, and generalization error.\n1. nRBFN. nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression. It is also closely related to Gaussian mixture model [63]. Comparing with RBFN, nRBFN has a distinctive feature: in regions far from the samples, the output of RBFN will vanish due to the localized property of radial basis function, while that of nRBFN will not due to the normalization [9]. Consequently, nRBFN provides better smoothness. Meanwhile, the universal approximation capacity is preserved [67, 4]. However, the connection of nRBFN to spectral graph theory is absent before.\n2. Regularization. The most classical viewpoint to regularization is from the Bayesian probability, see e.g., [6]. The regularization term corresponds to a prior distribution of the weights, while the error term corresponds to the conditional distribution of the target. However, in the Bayesian view, the effect of regularization on reducing overfitting is not as obvious as the spectral view. Moreover, how to set the regularization weight is unclear, and the uniform case that regularization does not help is not easily observed. Another viewpoint to regularization is from the Tikhonov regularization theory that is based on functional analysis, see e.g., [50]. In this view, the regularization term corresponds to a constraint imposing some smoothness on the approximating function.\n3. Generalization error. Even though we are concerned with the generalization problem, this paper is limited to the study of overfitting risk. We did not investigate the problem of generalization error or expected risk.12 Both of these two concepts relate to the error of the approximating function with respect to the underlying data distribution. In classification application, they indicate the error of a classifier when dealing with new data. Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31]. A typical result states that with probability greater than 1 \u2212 \u03b4, the generalization error of RBFN is upper bounded by O(1/r) + O( \u221a (pr log(nr)\u2212 log \u03b4)/n) [44]."}, {"heading": "9 Future Work", "text": "We mention some limitations of the paper as well as future work worthwhile to do.\nConcerning the performance improvements of nRBFN: (1) The relation between the ideal graph condition and the basis selection is not yet investigated. Our motivation to develop the basis selection strategy and parameter setting scheme is to demonstrate the practical performance of nRBFN and provide a baseline algorithm that is easy to use. Searching the optimal basis that is consistent with the theory is an important direction of future work. (2) Optimized or approximated search of nearest neighbors, e.g., [40], can be applied to address the bottleneck of speed improvement. (3) The basis can be further reduced, for there may be many boundary points highly overlapping. (4) Online basis learning can be considered [49], where the basis can be increased, updated, or pruned. It will enable (n)RBFN to handle large scale data.\nConcerning the theoretical investigations: (1) The error-and-risk analysis of nRBFN in the perturbation case does not depend on the normality of the columns of similarity matrix. This implies that the analysis can serve as a foundation for the analysis of the other models, e.g., RBFN [52], ELM [27]. Empirically, we found that when RBFN uses our basis selection strategy, it performs similarly to nRBFN and frequently even better. The perturbation analysis in this paper is limited to tiny noise. Extending the analysis from perturbation case to normal noise case has great practical significance. (2) How the two factors of basis size and ideal graph condition interact and contribute to the performance of nRBFN deserves further study. Many of the empirical laws observed in the experiments require explanations. (3) It will be interesting to compare the spectral risk with the VC dimension [64], the error-and-risk tradeoff with the structural risk minimization [64]."}, {"heading": "A The Gap between F and the Leading Row-subspace of W\u0303", "text": "Under the ideal graph condition, we know the row-space of W\u0303 contains F . Now we study how much F deviates from the leading row-subspace and when the gap is closed.\nFor simplicity, we consider one class, since under the ideal graph condition the blocks of matrices of different classes are independent of each other, the largest singular vectors of W\u0303 consist of the largest singular vectors of each block. We use lower case symbols w \u2208 Rrk\u00d7nk , f , x to denote the nonzero blocks of the kth class. Note, f is a uniform row vector of 1. The deviation can be measured using the idea of the \u21132 operator norm [23]:\n\u03c8 . = max\ny 6=0 \u2016yw\u0303\u201622 \u2016y\u201622 \u2212 \u2016x \u2217w\u0303\u201622 \u2016x\u2217\u201622 \u2265 0,\nOn the one hand, by property of the \u21132 operator norm,maxy 6=0 \u2016yw\u0303\u201622/\u2016y\u201622 = \u2016u1w\u0303\u201622/\u2016u1\u201622 = \u2016\u03c31v1\u201622/\u2016u1\u201622 = \u03c321 , where u1, v1, and \u03c31 denote the largest singular vectors and value of w\u0303. On the other hand, by Proposition 4, x\u2217 is a uniform row vector of 1, and x\u2217w\u0303 = f . Thus,\n\u03c8 = \u03c321 \u2212 nk/rk.\n\u03c8 = 0 if and only if \u03c321 = nk/rk, i.e., x \u2217 and f become the largest singular vectors. The remaining effort focuses on the estimation of \u03c31. We have the following proposition:\nProposition 20. For any nonnegative matrix w\u0303 \u2208 Rrk\u00d7nk (rk \u2264 nk) with each column sum normalized to 1, assume w\u0303 is of full rank, then\n12In the literature, see, e.g., [44], the \u201crisk\u201d of expected risk actually means error. A similar concept is empirical risk [64], actually it is\ntraining/fitting error. It should not be confused with the \u201crisk\u201d of overfitting/spectral risk in this paper.\n1. nk/rk \u2264 \u03c321 \u2264 zmax, where zmax is the maximal row sum of w\u0303.\n2. nk/rk = \u03c3 2 1 if and only if the row sums are even, i.e., zmax = nk/rk.\nProof. 1) The problem is resolved with the help of a closely related matrix, W\u0302 . = Z\u2212 1\n2 W\u0303 , where Z is a diagonal matrix of the row sums of W\u0303 . W\u0302 is the component of a reduced Laplacian matrix, L\u0302 . = I \u2212 W\u0302T W\u0302 , that is also for dealing with the scalable problem of large graph construction [37]. Under the ideal graph condition, W\u0303 is blockwise, so is the reduced similarity matrix W\u0302T W\u0302 , which implies W\u0302T W\u0302 is ideal too. Therefore, F is the smallest eigenvectors of L\u0302 (eigenvalue 0), and equivalently, the largest right singular vectors of Z\u2212 1\n2 W\u0303 (singular value 1). Denote w\u0302 and z to be the corresponding blocks of class k. We have\n\u03c321 = sup y 6=0 \u2016yw\u0303\u201622 \u2016y\u201622 = sup y 6=0 \u2016yz 12 w\u0302\u201622 \u2016y\u201622 .\nDenote y\u2032 = yz 1 2 , it turns into\n\u03c321 = sup y\u2032 6=0 \u2016y\u2032w\u0302\u201622 \u2016y\u2032z\u2212 12 \u201622 \u2264 sup y\u2032 6=0 \u2016y\u2032w\u0302\u201622 z\u22121max\u2016y\u2032\u201622 \u2264 zmax.\nzmax . = maxi zii. In the last inequality, we have used the fact that the largest singular value of w\u0302 is 1.\n2) First, note that nk/rk is also the mean of the row sums of w\u0303, since the sum of row sums is equal to the sum of column sums, which is nk. Thus, if the row sums are even, then zmax = nk/rk. Consequently nk/rk = \u03c3 2 1 . Conversely, if nk/rk = \u03c3 2 1 , then it means x\n\u2217 and f are the largest singular vectors, since \u2016x\u2217w\u0303\u201622/\u2016x\u2217\u201622 = nk/rk. In this case, again by the property of operator norm,\n\u03c321 = \u2016w\u0303f\u201622 \u2016f\u201622 =\n\u2211\ni z 2 ii nk \u2265 (\n\u2211\ni zii) 2/rk\nnk = nk rk .\nThe equality holds, if and only if zii\u2019s are even.\nWith this proposition, we immediately have\nProposition 21. \u03c8 \u2264 zmax \u2212 nk/rk. \u03c8 = 0 if and only if the row sums are even, i.e., zmax = nk/rk.\nBy this proposition, we finally arrive at Theorem 5."}], "references": [{"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["Michal Aharon", "Michael Elad", "Alfred Bruckstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On functional approximation with normalized gaussian units", "author": ["M Benaim"], "venue": "Neural Computation, 6(2):319\u2013 333", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Improving the generalization properties of radial basis function neural networks", "author": ["Christopher M Bishop"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Theoretical interpretations and applications of radial basis function networks", "author": ["Enrico Blanzieri"], "venue": "Technical Report No. DIT-03-023,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Multivariable functional interpolation and adaptive networks", "author": ["D. S Broomhead", "David Lowe"], "venue": "Complex Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Normalized gaussian radial basis function", "author": ["Guido Bugmann"], "venue": "networks. Neurocomputing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["Pak K Chan", "Martine DF Schlag", "Jason Y Zien"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih Chung Chang", "Chih Jen Lin"], "venue": "Acm Transactions on Intelligent Systems and Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Regularized orthogonal least squares algorithm for constructing radial basis function networks", "author": ["S. Chen", "E.S. Chng", "K. Alkadhimi"], "venue": "International Journal of Control, 64(5):829\u2013837", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Orthogonal least squares learning algorithm for radial basis function networks", "author": ["S Chen", "C.F.N Cowan", "P.M Grant"], "venue": "IEEE Transactions on Neural Networks, 2(2):302 \u2013 309", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Spectral Graph Theory", "author": ["Fan RK Chung"], "venue": "American Mathematical Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Multidimensional scaling", "author": ["Trevor F Cox", "Michael AA Cox"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Neural Networks and Statistical Learning", "author": ["Kelin Du", "M.N.S. Swamy"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing", "author": ["Michael Elad"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["Ehsan Elhamifar", "Rene Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J. Hakon Husoy"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 5, pages 2443\u20132446", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Do we need hundreds of classifiers to solve real world classification problems", "author": ["Manuel Fern\u00e1ndez-Delgado", "Eva Cernadas", "Sen\u00e1n Barro", "Dinani Amorim"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Locality preserving projections", "author": ["Xiaofei He", "ParthaNiyogi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Spectral sparse representation for clustering: Evolved from pca, k-means, laplacian eigenmap, and ratio cut", "author": ["Zhenfang Hu", "Gang Pan", "Yueming Wang", "Zhaohui Wu"], "venue": "Eprint Arxiv,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "An efficient sequential learning algorithm for growing and pruning rbf (gap-rbf) networks", "author": ["Guang-Bin Huang", "P Saratchandran", "Narasimhan Sundararajan"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Principal Component Analysis", "author": ["IT Jolliffe"], "venue": "Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Reformulated radial basis neural networks trained by gradient descent", "author": ["N.B. Karayiannis"], "venue": "IEEE Transactions on Neural Networks, 10(3):657\u2013671", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "AKrzy\u017cak", "author": ["B Kegl"], "venue": "and HNiemann. Radial basis function networks and complexity regularization in function learning and classification. In International Conference on Pattern Recognition, pages 81\u201386", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Radial basis function networks and complexity regularization in function learning", "author": ["A Krzy\u017cak", "T Linder"], "venue": "IEEE Transactions on Neural Networks, 9(2):247\u2013256", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonparametric estimation and classification using radial basis function nets and empirical risk minimization", "author": ["A Krzy\u017cak", "T Linder", "C Lugosi"], "venue": "IEEE Transactions on Neural Networks, 7(2):475\u201387", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "Semi-supervised graph clustering: a kernel approach", "author": ["Brian Kulis", "Sugato Basu", "Inderjit Dhillon", "Raymond Mooney"], "venue": "In International conference on Machine learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["Wei Liu", "Junfeng He", "Shih Fu Chang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["James MacQueen"], "venue": "In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1967}, {"title": "Fast learning in networks of locally-tuned processing units", "author": ["John Moody", "Christian J. Darken"], "venue": "Neural Computation,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1989}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["Marius Muja", "David G. Lowe"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "On the training of radial basis function classifiers", "author": ["M.T. Musavi", "W. Ahmed", "K.H. Chan", "K.B. Faris", "D.M. Hummels"], "venue": "Neural Networks, 5(05):595\u2013603", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1992}, {"title": "Netlab: Algorithms for Pattern Recognition", "author": ["Ian T. Nabney"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2003}, {"title": "On spectral clustering: analysis and an algorithm", "author": ["AndrewYNg", "Michael I Jordan", "Yair Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "On the relationship between generalization error", "author": ["P Niyogi", "F Girosi"], "venue": "hypothesis complexity, and sample complexity for radial basis functions. Neural Computation, 8(4):819\u2013842", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Regularization in the selection of radial basis function centers", "author": ["Mark J.L. Orr"], "venue": "Neural Computation,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1995}, {"title": "Data classification with radial basis function networks based on a novel kernel density estimation algorithm", "author": ["Yen Jen Oyang", "Shien Ching Hwang", "Yu Yen Ou", "Chien Yu Chen", "Zhi Wei Chen"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Universal approximation using radial-basis-function networks", "author": ["J Park", "I Sandberg"], "venue": "Neural Computation, 3(2):246\u2013257", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1991}, {"title": "A resource-allocating network for function interpolation", "author": ["John Platt"], "venue": "Neural Computation,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1991}, {"title": "Networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Proceedings of the IEEE, 78(9):1481\u2013 1497", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1990}, {"title": "Radial basis functions for multivariable interpolation: a review", "author": ["M.J.D. Powell"], "venue": "Algorithms for Approximation, pages 143\u2013167", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1987}, {"title": "Back to the future: radial basis function networks revisited", "author": ["Qichao Que", "Mikhail Belkin"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2016}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2000}, {"title": "Comparing support vector machines with gaussian kernels to radial basis function classifiers", "author": ["B. Sch\u00f6lkopf", "K. Sung", "C. Burges", "F. Girosi", "P. Niyogi", "T. Poggio", "V. Vapnik"], "venue": "IEEE Transactions on Signal Processing, 45(11):2758\u20132765", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Bernhard Sch\u00f6lkopf", "Alexander Smola", "Klaus-Robert M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1998}, {"title": "Three learning phases for radial-basis-function networks", "author": ["Friedhelm Schwenker", "Hans A. Kestler", "G\u00fcnther Palm"], "venue": "Neural Networks,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2001}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2000}, {"title": "Probabilistic neural networks", "author": ["Donald F. Specht"], "venue": "Neural Networks,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1990}, {"title": "A general regression neural network", "author": ["Donald F. Specht"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1991}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji Guang Sun"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1990}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2000}, {"title": "Network structuring and training using rule-based knowledge", "author": ["Volker Tresp", "J\u00fcrgen Hollatz", "Subutai Ahmad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1993}, {"title": "The Nature of Statistical Learning", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2007}, {"title": "Fast and efficient second-order method for training radial basis function networks", "author": ["Tiantian Xie", "Yu Hao", "J. Hewlett", "P. Rozycki", "B. Wilamowski"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "On radial basis function nets and kernel regression: Statistical consistency, convergence rates, and receptive field size", "author": ["Lei Xu", "Adam Krzy\u017cak", "Alan Yuille"], "venue": "Neural Networks,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1994}, {"title": "Least squares linear discriminant analysis", "author": ["Jieping Ye"], "venue": "International Conference on Machine Learning,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2007}, {"title": "An incremental design of radial basis function networks", "author": ["H. Yu", "P.D. Reiner", "T. Xie", "T Bartczak", "B.M. Wilamowski"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 25(10):1793\u20131803", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with local and global consistency", "author": ["Dengyong Zhou", "Olivier Bousquet", "Thomas Navin Lal", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2004}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": "Computer Science,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2008}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John D. Lafferty"], "venue": "International Conference on Machine Learning,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2003}], "referenceMentions": [{"referenceID": 13, "context": "Spectral graph theory is a theory that centers around the graph Laplacian matrix [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 59, "context": "The theory has found wide applications in unsupervised learning, including clustering [65] (generally named spectral clustering, including, e.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": ", ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 52, "context": ", ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.", "startOffset": 50, "endOffset": 58}, {"referenceID": 37, "context": ", ratio cut (Rcut) [10] and normalized cut (Ncut) [58, 43]), and dimensionality reduction (e.", "startOffset": 50, "endOffset": 58}, {"referenceID": 1, "context": ", Laplacian eigenmap (LE) [2] and locality preserving projections (LPP) [24]).", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": ", Laplacian eigenmap (LE) [2] and locality preserving projections (LPP) [24]).", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 107, "endOffset": 115}, {"referenceID": 66, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 64, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 2, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 65, "context": "Later, it develops as a popular paradigm in semi-supervised learning, including semi-supervised clustering [29, 34] and semi-supervised classification [72, 70, 3, 71].", "startOffset": 151, "endOffset": 166}, {"referenceID": 22, "context": "Recently it has been discovered that, in the scope of unsupervised learning, spectral graph theory unifies a series of elementary methods of machine learning into a complete framework [25].", "startOffset": 184, "endOffset": 188}, {"referenceID": 24, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "They range from principal component analysis (PCA) [28], K-means [38], LE [2], Rcut [10], and a new spectral sparse representation (SSR) [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "also incorporates extended relations to conventional over-complete sparse representations [19], e.", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": ", [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": ", [45], method of optimal directions (MOD) [21], KSVD [1]; manifold learning, e.", "startOffset": 54, "endOffset": 57}, {"referenceID": 50, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 50, "endOffset": 54}, {"referenceID": 56, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": ", kernel PCA [56], multidimensional scaling (MDS) [16], Isomap [62], locally linear embedding (LLE) [54]; and subspace clustering, e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": ", sparse subspace clustering (SSC) [20], low-rank representation (LRR) [36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": ", sparse subspace clustering (SSC) [20], low-rank representation (LRR) [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": ", [6]), and when used for classification (LRC), its link to linear discriminant analysis was already discovered [68].", "startOffset": 2, "endOffset": 5}, {"referenceID": 62, "context": ", [6]), and when used for classification (LRC), its link to linear discriminant analysis was already discovered [68].", "startOffset": 112, "endOffset": 116}, {"referenceID": 45, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 5, "endOffset": 16}, {"referenceID": 7, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 5, "endOffset": 16}, {"referenceID": 33, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 5, "endOffset": 16}, {"referenceID": 33, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 52, "endOffset": 60}, {"referenceID": 54, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 52, "endOffset": 60}, {"referenceID": 42, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 178, "endOffset": 189}, {"referenceID": 61, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 178, "endOffset": 189}, {"referenceID": 3, "context": "RBFN [51, 8, 39] and its normalized variant (nRBFN) [39, 60] are classical neural networks well-known for their simple structures and universal function approximation capacities [48, 67, 4].", "startOffset": 178, "endOffset": 189}, {"referenceID": 6, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 41, "endOffset": 44}, {"referenceID": 44, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 105, "endOffset": 109}, {"referenceID": 54, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 165, "endOffset": 173}, {"referenceID": 61, "context": "Among broad connections to many theories [7], RBFN can be interpretedwith Tikhonov regularization theory [50], while nRBFN can be interpreted with kernel regression [60, 67].", "startOffset": 165, "endOffset": 173}, {"referenceID": 47, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 41, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 19, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 46, "context": "In classification application, it has been repeatedly demonstrated that [53, 47, 22, 52] the performance of RBFN can be comparable to support vector machine (SVM) [15].", "startOffset": 72, "endOffset": 88}, {"referenceID": 19, "context": "Especially, in a comprehensive evaluation involving 179 classifiers and 121 data sets by [22], RBFN ranks third, immediately following SVM.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 40, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 49, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 41, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 46, "context": "This situation may be attributed to: (1) the less sound theoretical background compared with SVM, (2) the basis selection problem involved in (n)RBFN design [13, 46, 55, 47, 52], and (3) the difficulty of parameter tuning.", "startOffset": 157, "endOffset": 177}, {"referenceID": 59, "context": "The Laplacian matrix has the following properties [65].", "startOffset": 50, "endOffset": 54}, {"referenceID": 59, "context": "These properties are exploited for clustering purpose [65].", "startOffset": 54, "endOffset": 58}, {"referenceID": 22, "context": "Assume we are to findK clusters, if the ideal graph condition for clustering (Definition 1) [25] holds (the condition implies the between-cluster weights are all zero: Wij = 0, if the ith and jth points are of different clusters), then we can compute the K eigenvectors of L with the smallest eigenvalues (zero), and then postprocess these eigenvectors to finish clustering.", "startOffset": 92, "endOffset": 96}, {"referenceID": 59, "context": "Finally, the eigenvectors of Lwith eigenvalue zero (smallest) are the eigenvectors of S\u22121W , called normalized Laplacian matrix, with eigenvalue one (largest) [65].", "startOffset": 159, "endOffset": 163}, {"referenceID": 22, "context": "[25], where \u03b2 is a constant scalar that will be introduced later.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "It is equivalent to the classical LRC where \u03b2 = 1 [6], since \u221a \u03b2 can be absorbed into the first column of D.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "Provided rank(\u00c3) = p+1, there is a unique closed-form solution: D\u2217 = F\u00c3 (\u00c3\u00c3 )\u22121 [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "Since the data is mean-removed, we have minij (A A)ij < 0, 5 \u03b2 thus defined makes W become nonnegative [25].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Assume the thin SVD [23] of A to be U\u03a3V , where the singular values are arranged in descending order, then [25] \u00c3 = \u0168 \u03a3\u0303\u1e7c = [", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "Assume the thin SVD [23] of A to be U\u03a3V , where the singular values are arranged in descending order, then [25] \u00c3 = \u0168 \u03a3\u0303\u1e7c = [", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "Further, by L = n\u03b2I \u2212 \u00c3 \u00c3, we obtain the spectral decomposition [23] of L:", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Note that the condition ensures not only perfect reconstruction but also that the target lies in the principal row-subspace of data, or principal components (PCs) in the language of PCA [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 45, "context": "(7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03c6(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi.", "startOffset": 15, "endOffset": 26}, {"referenceID": 7, "context": "(7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03c6(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi.", "startOffset": 15, "endOffset": 26}, {"referenceID": 33, "context": "(7) is an RBFN [51, 8, 39], where the kernel function is viewed as radial basis function \u03c6(\u2016Gi \u2212 x\u2016):8 each column of W corresponds to a sample x, while each row a basis vector Gi.", "startOffset": 15, "endOffset": 26}, {"referenceID": 47, "context": ", [53], but it cannot lead to nRBFN.", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 33, "endOffset": 37}, {"referenceID": 53, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 117, "endOffset": 129}, {"referenceID": 54, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 117, "endOffset": 129}, {"referenceID": 61, "context": "nRBFN was initially mentioned by [39] and later derived from probability density estimation and kernel regression by [59, 60, 67].", "startOffset": 117, "endOffset": 129}, {"referenceID": 57, "context": "It is also closely related to Gaussian mixture model [63].", "startOffset": 53, "endOffset": 57}, {"referenceID": 46, "context": "Traditionally, basis reduction is applied [52].", "startOffset": 42, "endOffset": 46}, {"referenceID": 58, "context": "In addition, the size of basis also makes us recall the VC-dimension [64]\u2013a classical measure of model complexity.", "startOffset": 69, "endOffset": 73}, {"referenceID": 55, "context": "\u03be < 1 is a condition frequently appeared in the perturbation analysis of linear system [61].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 8, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 23, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 63, "context": "Traditionally, there are three problems (n)RBFN needs to deal with: (1) how to decide the basis size? (2) how to select the basis? (3) how to set the Gaussian width? Usually, the size of the basis is manually assigned, except for some incremental learning methods [13, 9, 26, 69] which learn the basis vectors one by one.", "startOffset": 264, "endOffset": 279}, {"referenceID": 44, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 83, "endOffset": 95}, {"referenceID": 25, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 83, "endOffset": 95}, {"referenceID": 60, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 83, "endOffset": 95}, {"referenceID": 33, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 4, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 35, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 46, "context": "There are two kinds of methods for basis selection: one is gradient descent method [50, 30, 66], the other is sample-selection based method, including: (1) random selection of a subset of the training set, (2) clustering the data and using the cluster centers as the basis [39, 5, 41, 52], and (3) incremental learning methods.", "startOffset": 273, "endOffset": 288}, {"referenceID": 33, "context": "The Gaussian width can be set via some heuristics [39, 57, 18], e.", "startOffset": 50, "endOffset": 62}, {"referenceID": 51, "context": "The Gaussian width can be set via some heuristics [39, 57, 18], e.", "startOffset": 50, "endOffset": 62}, {"referenceID": 15, "context": "The Gaussian width can be set via some heuristics [39, 57, 18], e.", "startOffset": 50, "endOffset": 62}, {"referenceID": 53, "context": "It was reported that the performance is not so sensitive to this parameter [59], especially for nRBFN [9].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "It was reported that the performance is not so sensitive to this parameter [59], especially for nRBFN [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 49, "context": "It is this view that leads to the clustering heuristics [55].", "startOffset": 56, "endOffset": 60}, {"referenceID": 49, "context": "This has been investigated by [55, 9, 47].", "startOffset": 30, "endOffset": 41}, {"referenceID": 8, "context": "This has been investigated by [55, 9, 47].", "startOffset": 30, "endOffset": 41}, {"referenceID": 41, "context": "This has been investigated by [55, 9, 47].", "startOffset": 30, "endOffset": 41}, {"referenceID": 11, "context": ",\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,.", "startOffset": 74, "endOffset": 78}, {"referenceID": 36, "context": ",\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,.", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": ",\u22122}), (3) ROLS (regularized orthogonal least squares algorithm for RBFN) [12] (a classical incremental learning method for basis selection, \u03bb is selected using the same scheme as nRBFN, \u03c3 and basis size are provided by nRBFN), (4) RBFNnl (RBFN from Netlab toolbox) [42] (a traditional RBFN that finds the basis via clustering\u2013gaussian mixture model, without regularization, the width of Gaussian is set as the maximum distance between the basis vectors, bias parameters are included, basis size follows nRBFN), (5) nRNwr (nRBFN without regularization, using the same basis as nRBFN), (6) nRNrb (nRBFN with basis chosen randomly from the training set, basis size follows nRBFN), (7) SVM [11] (Gaussian kernel, the weight C is selected over 2{\u22121,0,.", "startOffset": 687, "endOffset": 691}, {"referenceID": 33, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 32, "endOffset": 36}, {"referenceID": 53, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 58, "endOffset": 70}, {"referenceID": 54, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 58, "endOffset": 70}, {"referenceID": 61, "context": "nRBFN is initially mentioned by [39] and later derived by [59, 60, 67] from probability density estimation and kernel regression.", "startOffset": 58, "endOffset": 70}, {"referenceID": 57, "context": "It is also closely related to Gaussian mixture model [63].", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "Comparing with RBFN, nRBFN has a distinctive feature: in regions far from the samples, the output of RBFN will vanish due to the localized property of radial basis function, while that of nRBFN will not due to the normalization [9].", "startOffset": 228, "endOffset": 231}, {"referenceID": 61, "context": "Meanwhile, the universal approximation capacity is preserved [67, 4].", "startOffset": 61, "endOffset": 68}, {"referenceID": 3, "context": "Meanwhile, the universal approximation capacity is preserved [67, 4].", "startOffset": 61, "endOffset": 68}, {"referenceID": 5, "context": ", [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 44, "context": ", [50].", "startOffset": 2, "endOffset": 6}, {"referenceID": 38, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 28, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 27, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 46, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 59, "endOffset": 75}, {"referenceID": 61, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 86, "endOffset": 94}, {"referenceID": 26, "context": "Results on this problem had been established both for RBFN [44, 33, 32, 52] and nRBFN [67, 31].", "startOffset": 86, "endOffset": 94}, {"referenceID": 38, "context": "A typical result states that with probability greater than 1 \u2212 \u03b4, the generalization error of RBFN is upper bounded by O(1/r) + O( \u221a (pr log(nr)\u2212 log \u03b4)/n) [44].", "startOffset": 156, "endOffset": 160}, {"referenceID": 34, "context": ", [40], can be applied to address the bottleneck of speed improvement.", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "(4) Online basis learning can be considered [49], where the basis can be increased, updated, or pruned.", "startOffset": 44, "endOffset": 48}, {"referenceID": 46, "context": ", RBFN [52], ELM [27].", "startOffset": 7, "endOffset": 11}, {"referenceID": 58, "context": "(3) It will be interesting to compare the spectral risk with the VC dimension [64], the error-and-risk tradeoff with the structural risk minimization [64].", "startOffset": 78, "endOffset": 82}, {"referenceID": 58, "context": "(3) It will be interesting to compare the spectral risk with the VC dimension [64], the error-and-risk tradeoff with the structural risk minimization [64].", "startOffset": 150, "endOffset": 154}, {"referenceID": 20, "context": "The deviation can be measured using the idea of the l2 operator norm [23]:", "startOffset": 69, "endOffset": 73}, {"referenceID": 38, "context": ", [44], the \u201crisk\u201d of expected risk actually means error.", "startOffset": 2, "endOffset": 6}, {"referenceID": 58, "context": "A similar concept is empirical risk [64], actually it is training/fitting error.", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "= I \u2212 \u0174 \u0174 , that is also for dealing with the scalable problem of large graph construction [37].", "startOffset": 91, "endOffset": 95}], "year": 2017, "abstractText": "Spectral graph theory has been widely applied in unsupervised and semi-supervised learning. It is still unknown how it can be exploited in supervised learning. In this paper, we find for the first time, to our knowledge, that it also plays a concrete role in supervised classification. It turns out that two classifiers are inherently related to the theory: linear regression for classification (LRC) and normalized radial basis function network (nRBFN), corresponding to linear and nonlinear kernel respectively. The spectral graph theory provides us with a new insight into a fundamental aspect of classification: the tradeoff between fitting error and overfitting risk. With the theory, ideal working conditions for LRC and nRBFN are presented, which ensure not only zero fitting error but also low overfitting risk. For quantitative analysis, two concepts, the fitting error and the spectral risk (indicating overfitting), have been defined. Their bounds for nRBFN and LRC are derived. A special result shows that the spectral risk of nRBFN is lower bounded by the number of classes and upper bounded by the size of radial basis. When the conditions are not met exactly, the classifiers will pursue the minimum fitting error, running into the risk of overfitting. It turns out that l2-norm regularization can be applied to control overfitting. Its effect is explored under the spectral context. It is found that the two terms in the l2-regularized objective are one-one correspondent to the fitting error and the spectral risk, revealing a tradeoff between the two quantities. Concerning practical performance, we devise a basis selection strategy to address the main problem hindering the applications of (n)RBFN. With the strategy, nRBFN is easy to implement yet flexible. Experiments on 14 benchmark data sets show the performance of nRBFN is comparable to that of SVM, whereas the parameter tuning of nRBFN is much easier, leading to reduction of model selection time.", "creator": "LaTeX with hyperref package"}}}