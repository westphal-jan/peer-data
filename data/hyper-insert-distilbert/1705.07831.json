{"id": "1705.07831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Stabilizing GAN Training with Multiple Random Projections", "abstract": "citizen training generative directed adversarial flow networks efficiently is unstable in several high - finite dimensions given when the true data distribution direction lies on a lower - dimensional target manifold. the discriminator is still then apparently easily practically able practically to separate nearly all appropriately generated samples leaving the generator without meaningful spectral gradients. we consider propose thus training a normal single generator competing simultaneously against an online array of discriminators, each of eight which looks typically at a different random low - and dimensional projection values of the data. yet we show ensuring that individual discriminators then somehow provide stable gradients appropriately to assist the generator, and check that the generator learns to partially produce matching samples consistent with the local full data distribution geometry to satisfy all independent discriminators. we demonstrate the relative practical utility of putting this approach experimentally, honestly and completely show that now it is potentially able precisely to produce image sequence samples with higher quality components than traditional user training with defining a single discriminator.", "histories": [["v1", "Mon, 22 May 2017 16:23:26 GMT  (5559kb,D)", "http://arxiv.org/abs/1705.07831v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["behnam neyshabur", "srinadh bhojanapalli", "ayan chakrabarti"], "accepted": false, "id": "1705.07831"}, "pdf": {"name": "1705.07831.pdf", "metadata": {"source": "CRF", "title": "Stabilizing GAN Training with Multiple Random Projections", "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "Ayan Chakrabarti"], "emails": ["bneyshabur@ttic.edu", "srinadh@ttic.edu", "ayanc@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "Generative adversarial networks (GANs), introduced by [1], endow neural networks with the ability to express distributional outputs. The framework includes a generator network that is tasked with producing samples from some target distribution, given as input a (typically low dimensional) noise vector drawn from a simple known distribution, and possibly conditional side information. The generator learns to generate such samples, not by directly looking at the data, but through adversarial training with a discriminator network that seeks to differentiate real data from those generated by the generator. To satisfy the objective of \u201cfooling\u201d the discriminator, the generator eventually learns to produce samples with statistics that match those of real data.\nIn regression tasks where the true output is ambiguous, GANs provide a means to simply produce an output that is plausible (with a single sample), or to explicitly model that ambiguity (through multiple samples). In the latter case, they provide an attractive alternative to fitting distributions to parametric forms during training, and employing expensive sampling techniques at the test time. In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting [2], and super-resolution [3]. Recently, [4] demonstrated that GANs can be used to produce plausible mappings between a variety of domains\u2014including sketches and photographs, maps and aerial views, segmentation masks and images, etc. GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].\nDespite their success, training GANs to generate high-dimensional data (such as large images) is challenging. Adversarial training between the generator and discriminator involves optimizing a min-max objective. This is typically carried out by gradient-based updates to both networks, and the generator is prone to divergence and mode-collapse as the discriminator begins to successfully distinguish real data from generated samples with high confidence. Researchers have tried to address this instability and train better generators through several techniques. [7] proposed explicitly factorizing generating an image into a sequence of conditional generations of levels of a Laplacian\nar X\niv :1\n70 5.\n07 83\n1v 1\n[ cs\n.L G\n] 2\n2 M\nay 2\npyramid, while [6] demonstrated that specific architecture choices and parameter settings led to higher-quality samples. Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].\nIn this work, we seek to provide a framework that addresses a key source of instability [9] when training GANs\u2014when the true distribution of high-dimensional data is concentrated in a small fraction of the ambient space. During training in such settings, the generator\u2019s samples have very little overlap with the true distribution. This allows the discriminator to easily separate real and generated data perfectly. The generator then no longer receives any significant gradients from the discriminator to update its model, irrespective of its distance from the real data distribution.\nTo address this, we propose training the generator against an array of discriminators, each of which looks at a different, randomly-chosen, low-dimensional projection of the data. Each discriminator is unable to perfectly separate real and generated samples since it only gets a partial view of these samples. At the same time, to satisfy its objective of fooling all discriminators, the generator learns to match the true full data distribution. We formalize this intuition by providing results that show the marginals of the true data distribution along the lower-dimensional projections are less concentrated, effectively alleviating the problem of vanishing gradients for each discriminator. Furthermore, we show that with discriminators trained on different random projections, the generator will learn a distribution that well approximates the true data distribution. Finally, we demonstrate the efficacy of our approach by using it to train generators on standard image datasets, and find that these produce higher-quality samples than generators trained against a single discriminator."}, {"heading": "1.1 Related Work", "text": "Researchers have explored several approaches to improve the stability of GANs for training on higher dimensional images. Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN [12] and Wasserstein GAN [11] show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity. [10] further extended GAN training to any choice of f -divergence as objective. [8] proposed several heuristics to improve the stability of training. These include modifying the objective function, virtual batch-normalization, historical averaging of parameters, semi-supervised learning, etc.. All of these methods are designed to improve the quality of gradients, and provide additional supervision to the generator. They are therefore complementary to, and can likely be used in combination with, our approach.\nNote that prior methods have involved training ensembles of GANs [13], or ensembles of discriminators [14]. However, their goal is different from ours. In our framework, each discriminator is shown only a low-dimensional projection of the data, with the goal of preventing it from being able to perfectly reject generated samples. Crucially, we do not combine the outputs of discriminators directly, but rather compute losses on individual discriminators and average these losses.\nIndeed, the idea of combining multiple simple discriminators, e.g., with boosting [15], or mixtureof-experts [16], is a common approach to learning. While these combinations are aimed at building a single strong discriminator, our objective is to maintain a flow of gradients from individual discriminators to the generator. It is also worth noting the work of Bonneel et al. [17], who also use low-dimensional projections to deal with high-dimensional probability distributions. However, while they use such projections to efficiently compute Wasserstein distances and optimal transport between two distributions, our goal is to enable stable GAN training in high dimensions."}, {"heading": "2 Learning with Multiple Discriminators on Random Projections", "text": "GANs [1] traditionally comprise of a generator G that learns to generate samples from a data distribution Px, through adversarial training against a single discriminator D as:\nmin G max D V (D,G) = Ex\u223cPx [logD(x)] + Ez\u223cPz [log(1\u2212D(G(z)))], (1)\nfor x \u2208 Rd, Px : Rd \u2192 R+, \u222b Px = 1, and with Pz a fixed distribution (typically uniform or Gaussian) for noise vectors z \u2208 Rd\u2032 , d\u2032 d. The optimization in (1) is carried out using stochastic gradient descent (SGD), with alternating updates to the generator and the discriminator. While this approach works surprisingly well, instability is common during training, especially when generating high-dimensional data.\nAs described in [9], this instability is due to the fact that the distributions Px corresponding to natural data often have very limited support in the ambient domain of x and G(z). Then, the generator G is unable to learn quickly enough to generate samples from distributions that have significant overlap with this support. This in turn makes it easier for the discriminator D to learn to perfectly separate the generator\u2019s samples, at which point the latter is left without useful gradients for further training.\nThe key idea of our approach is to train a single generator against an array of discriminators. Each discriminator operates on a different low-dimensional linear projection, that is set randomly prior to training. Formally, we train a generator G against multiple discriminators {Dk}Kk=1 as:\nmin G max {Dk} K\u2211 i=k V (Dk, G) = K\u2211 i=k Ex\u223cPx [logDk(WTk x)] + Ez\u223cPz [log(1\u2212Dk(WTk G(z)))], (2)\nwhere Wk, k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} is a random matrix in Rd\u00d7m. As we will show in the following results, the above formulation addresses the issue of instability by ensuring the true marginal distributions of the low-dimensional projections {WTk x} have comparatively larger support. Consequently, each discriminator Dk is unable to perfectly separate real and generated samples, and continues to provide gradients to the generator. Moreover, we will also show that since the generator must simultaneously match the marginals of Px along all projections {WTk } to minimize the loss in (2), it will learn to generate samples under the true distribution Px, given a sufficiently large number of projections K."}, {"heading": "2.1 Stability", "text": "If the support of the true distribution Px occupies a low volume relative to the range of possible values of x, it is hard for the generator to produce samples within that support, which makes it easy for a discriminator to quickly learn to classify all such samples as fake. Here, we argue that this problem is ameliorated by training discriminators on lower-dimensional projections of x and G(z).\nFor simplicity, we assume that the range of x is the d-dimensional ball Bd of radius 1 centered at 0. We define the support supp(Px) \u2282 Bd of a distribution Px to be the set where the density is greater than some small threshold . Let W \u2208 Rd\u00d7m be an entry-wise random Gaussian projection matrix. We denote as BdW the projection of the range on W , and PWT x as the marginal of Px along W .\nTheorem 2.1. Assume Px = \u2211 j \u03c4jN (x|\u00b5j ,\u03a3j) is a mixture of Gaussians, such that the individual components are sufficiently well separated (in the sense that there is no overlap between their supports or the projections thereof, see [18]). If supp(Px) \u2282 Bd and Vol(supp(Px)) > 0, then Vol(supp(PWT x))/Vol(BdW ) > Vol(supp(Px))/Vol(Bd) with high probability.\nThe above result, proved in the supplementary, implies that the projection of the support of Px occupies a higher fraction of the volume of the projection of the range of x. This aids in stability because a larger fraction of the generator\u2019s samples (which lie within the range of x), after projection, will overlap with the projected support supp(Px), and can not be rejected absolutely by the discriminator."}, {"heading": "2.2 Consistency", "text": "The following two results, proved in the supplementary, show that with discriminators acting on a sufficiently large number K of random projections, the generator G will learn to produce samples under a distribution that matches the true data distribution Px. Theorem 2.2. Let Pg denote the distribution of the generator outputs G(z), where z \u223c Pz , and let PWTk g be the marginals of Pg along Wk. For fixed G, the optimal {Dk} are given by\nDk(y) = PWTk x(y)\nPWTk x(y) + PWTk g(y) , (3)\nfor all k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}. The optimal G that minimizes (2) is achieved iff PWTk x = PWTk g , \u2200k.\nThe above result implies that the marginals of the distribution of the optimal generator match those of the data distribution along all projections {WTk }. The next theorem states that for any smooth data distribution Px, the generator distribution Pg will be close to Px once it matches marginals along a sufficiently large number of projections. Theorem 2.3. A distribution Px : Rd \u2192 R+ is L-Lipschitz, if \u2200 x1, x2 \u2208 Rd, |Px(x1)\u2212 Px(x2)| \u2264 L \u00b7 d(x1, x2). Let Px and Pg be L-Lipschitz and compact distributions with support of radius B, such that PWTk x = PWTk g,\u2200k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K}. Let {Wk} be entrywise random Gaussian matrices in Rd\u00d7m. IfK > O((B\u00b7L ) d\u2212m), then for any x \u2208 Rd, |Px(x)\u2212Pg(x)| \u2264 O( ), with high probability.\nNote that when the true distribution Px is low dimensional, and does not have exponential degrees of freedom as assumed above, a far smaller number of projectionsK may suffice. As described later in Sec. 3, we find setting K to two to four times d/m to be adequate in our experiments."}, {"heading": "2.3 Projections for Generating Image Data", "text": "Prior work [6] has shown that using convolutional architectures for both the generator and discriminator networks is key to generating images with GANs. While individual discriminators in our framework see projected inputs that are lower-dimensional than the full image, their dimensionality is still large enough (a very small m would require a large number of discriminators) to make it hard to train discriminators with only fully-connected layers. Therefore, it is desirable to employ convolutional architectures for each discriminator.\nTo do so, the projection matrices WTk must be chosen to produce \u201cimage-like\u201d data. Accordingly, we use downsampled convolutions with random filters to embody the projections WTk (as illustrated in Fig. 1). The elements of these convolution filters are drawn i.i.d. from a Gaussian distribution, and the filters are then scaled to have unit `2 norm. Note that this imposes a block-Toeplitz structure on WTk , and they are no longer purely random as assumed in above. We partially address this by choosing filter sizes that are larger than the downsampling rate to promote more \u201cmixing\u201d of the input coordinates (e.g., we use 8\u00d78 filters when using a stride of 2). We find that our strategy works well in practice, and that the benefits of using convolutional architectures for each discriminator Dk outweigh the drawbacks of not using projections that are purely random."}, {"heading": "3 Experimental Results", "text": "In this section, we evaluate our approach with experiments comparing to generators trained against a single discriminator that sees the entire image as input, and demonstrate that it leads to higher stability during training, and ultimately yields generators that produce higher quality samples.\nDataset and Architectures. For evaluation, we primarily use the dataset of celebrity faces collected by [19]\u2014we use the cropped and aligned 64\u00d7 64-size version of the images\u2014and the DC-GAN [6] architectures for the generator and discriminator. We make two minor modifications to the DC-GAN implementation that we find empirically to yield improved results (for both the standard single discriminator setting, as well as our multiple discriminator setting). Firstly, we use different batches of generated images to update the discriminator and generator\u2014this increases the amount of computation required, but yields better quality results and faster training. Secondly, we employ batch normalization in the generator but not in the discriminator (the original DC-GAN implementation normalized real and generated batches separately, which we found yielded poorer generators).\nFor our approach, we train a generator againstK discriminators, each operating on a different singlechannel 32\u00d7 32 projected version of the input, i.e., d/m = 12. The projected images are generated through convolution with 8\u00d78 filters and a stride of two. The filters are generated randomly and kept constant throughout training. We compare this to the standard DC-GAN setting of a single discriminator that looks at the full-resolution 64\u00d7 64 color image. We use identical generator architectures in both settings\u2014that map a 100 dimensional uniformly distributed noise vector to a full resolution image. The discriminators also have similar architectures\u2014but each of the discriminator in our setting has one less layer as it operates on a lower resolution input (we map the number of channels in the first layer in our setting to those of the second layer of the full-resolution single-discriminator, thus matching the size of the final feature vector used for classification).\nAs is standard practice, we compute generator gradients in both settings with respect to minimizing the \u201cincorrect\u201d classification loss of the discriminator\u2014in our setting, this loss is given by \u2212 1K \u2211 k logDk(G(z)). As suggested in [6], we use Adam [20] with learning rate 2 \u00d7 10\u22124, \u03b21 = 0.5, and a batch size of 64.\nStability. We begin by analyzing the evolution of generators in both settings through training. Figure 2 shows the generator training loss for traditional DC-GAN with a single discriminator, and\ncompares it the proposed framework withK = 48 discriminators\u2014displaying both the average loss, and the loss with respect to individual discriminators. In both settings, the generator losses increase through much of training after decreasing in the initial iterations (i.e., the discriminators eventually \u201cwin\u201d). However, DC-GAN\u2019s generator loss remains higher than ours in absolute terms throughout training, and after crossing a certain threshold, begins to exhibit high-magnitude oscillations.\nFigure 2 also includes examples of generated samples from both generators across iterations (from the same noise vectors). We observe that DC-GAN\u2019s samples improve mostly in the initial iterations while the training loss is still low, in line with our intuition that generator gradients become less informative as discriminators get stronger. Indeed, we find that as training progresses, the quality of samples from traditional DC-GAN actually begins to decrease after around 40k iterations. In contrast, the generator trained in our framework improves continually throughout training.\nConsistency. Beyond stability, Fig. 2 also demonstrates the consistency of our framework. While the average loss in our framework is lower (which could, after all, have been achieved simply with a lower learning rate for the discriminator), we see that this does not impede our generator\u2019s ability to\nlearn the data distribution quickly as it collates feedback from multiple discriminators. Indeed, our generator produces higher-quality samples than traditional DC-GAN even in early iterations.\nFigure 3 includes a larger number of (random) samples from generators trained with traditional DC-GAN and our setting. For DC-GAN, we include samples from both roughly the end (100k iterations) of training, as well as from roughly the middle (40k iterations) where the sample quality are approximately the best. For our approach, we show results from training with different numbers of discriminators with K = 12, 24, and 48\u2014selecting the generator models from the end of training for all. We see that our generators produce face images with higher-quality detail and far fewer distortions than traditional DC-GAN. We also note the effect of the number of discriminators on sample quality. Specifically, we find that settingK to be equal only to the projection ratio d/m = 12 leads to subtle high-frequency noise in the generator samples, suggesting these many projections do not sufficiently constrain the generator to learn the full data distribution. However, increasing K diminishes these artifacts, with both K = 24 and K = 48 yielding similar, high-quality samples.\nTraining Time. The improved generator model quality comes at the expense of increased computation during training. While training traditional DC-GAN with a single discriminator takes only 0.6 seconds per iteration (on an NVIDIA Titan X), the iteration time in our framework goes up to 3.2 seconds for K = 12, 5.8 seconds for K = 24, and 11.2 seconds for K = 48. However, since our generator architecture is identical to that in the traditional setting, the time taken to produce samples is the same once training is complete.\nLatent Embedding. Next, we explore the quality of the embedding induced by our generator (trained with K = 48) in the latent space of noise vectors z. In Fig. 4, we consider selected pairs of randomly generated faces, and generate samples by linear interpolation between their corresponding noise vectors. We find that each of these generated samples is also a plausible face image\u2014this confirms both that our generator is not simply memorizing training samples, and that it is densely packing the latent space with face images. We also find that the generated samples smoothly interpolate between semantically meaningful face attributes\u2014gender, race, age, hair-style, expression, and so on. Note that in all rows, the sample for \u03b1 = 0.5 appears to be a clearly different individual than the ones represented by \u03b1 = 0 and \u03b1 = 1.\nResults on Imagenet-Canines. Finally, we also use our framework to train a generator on a subset of the Imagenet-1K database [21]. We train on 128 \u00d7 128 image crops (formed by scaling the smaller side to 128, and then taking a random crop along the other dimension), over a subset of Imagenet classes corresponding roughly to canines\u2014from classes 152 (chihuahua) to 281 (grey fox), with 160k images. We largely use the same settings as for faces. However, we feed a higher 200-dimensional noise vector to the generator, which also begins by mapping this to a feature vector that is twice as large (2048), and which has an extra transpose-convolution layer to go upto the 128\u00d7128 resolution. We again use convolutional 8\u00d78 filters with stride two to form our projection matrices {WTk }, but in this case, these projections map to 64 \u00d7 64 single channel images. Due to time constraints, we use onlyK = 12 discriminators. Each discriminator also has an additional layer because of the higher-resolution, but we reduce the number of channels in the first layer such that the feature vector used for classification is the same dimensionality as that in our face experiments.\nFigure 5 shows selected samples after 100k iterations of training (note that these are manually selected samples, please see the supplementary material for a larger random set). We see that since it is trained on a more diverse and unaligned image content, the generated images are not globally plausible photographs. Nevertheless, we find that the produced images are sharp, and that generator learns to reproduce realistic low-level textures as well as some high-level composition."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed a new framework for training GANs with high-dimensional outputs. Our approach employs multiple discriminators on random projections of the data to stabilize training, with enough projections to ensure that the generator learn the true data distribution. Experimental results demonstrate that generators trained using our method are in fact more stable\u2014they continue to learn throughout training while those trained against a single discriminator often diverge, and produce higher-quality samples that match the true data distribution. Source code and trained models for our implementation is available at the project page http://www.ttic.edu/chakrabarti/rpgan/.\nIn our current framework, the number of discriminators is limited by computational cost. In future work, we plan to investigate training with a much larger set of discriminators, employing only a small subset of them at each iteration, or every set of iterations. We are also interested in using multiple discriminators with modified and regularized objectives (e.g., [10\u201312]). Such modifications are complementary to our approach, and deploying them together will likely be beneficial.\nAcknolwedgments: AC was supported by the National Science Foundation under award no. IIS:1618021, and also thanks NVIDIA corporation for the donation of a Titan X GPU that was used in this research."}, {"heading": "A Proofs", "text": "Proof of Theorem 2.1. We first show that we can assume that the columns of the projection W are orthonormal. Since W \u2208 Rd\u00d7m is entry-wise Gaussian distributed, it has rank m with high probability. Then, there exists a square invertible matrix A such that W \u2032 = AW where W \u2032 is orthonormal. In that case, Vol(supp(PWT x))/Vol(BdW ) = Vol(supp(PW \u2032Tx ))/Vol(B d W \u2032) because the numerator and denominator terms for both can be related by det(A) for the change of variables, which cancels out. Note that under this orthonormal assumption, BdW = B m.\nNext, we consider the case of an individual Gaussian distribution Px = N (x|\u00b5,\u03a3), and prove that the ratio of supports (defined with respect to a threshold ) does not decrease with the projection. The expression for these ratios is given by:\nVol(supp(Px)) = Vol(Bd)\u00d7 det(\u03a3)\u00d7 [ log 1\n2 \u2212 d log 2\u03c0 \u2212 log det(\u03a3) ] \u21d2 Vol(supp(Px))\nVol(Bd) = det(\u03a3)\u00d7\n[ log 1\n2 \u2212 d log 2\u03c0 \u2212 log det(\u03a3)\n] . (4)\nVol(supp(PWT x)) Vol(BdW )\n= det(WT\u03a3W )\u00d7 [ log 1\n2 \u2212m log 2\u03c0 \u2212 log det(WT\u03a3W )\n] . (5)\nFor sufficiently small , the volume ratio of a single Gaussian will increase with projection if det(WT\u03a3W ) > det(\u03a3). Note that all eigenvalues of \u03a3 \u2264 1, with at-least one eigenvalue strictly < 1 (since supp(Px) \u2282 Bd). First, we consider the case when \u03a3 is not strictly positive definite and one of the eigenvalues is 0. Then, Vol(supp(Px)) = 0 and Vol(supp(PWT x)) \u2265 0, i.e., the volume ratio either stays the same or increases.\nFor the case when all eigenvalues are strictly positive, consider a co-ordinate transform where the first m co-ordinates of x correspond to the column vectors of W , such that\n\u03a3 =\n[ \u03a3W \u03a3WW \u2032\n\u03a3TWW \u2032 \u03a3W \u2032\n] , (6)\nwhere \u03a3W = WT\u03a3W . Then,\ndet(\u03a3) = det(\u03a3W ) det(\u03a3W \u2032 \u2212 \u03a3TWW \u2032\u03a3\u22121W \u03a3WW \u2032) \u2264 det(\u03a3W ) det(\u03a3W \u2032),\n\u21d2 det(\u03a3W ) \u2265 det(\u03a3)/ det(\u03a3W \u2032). (7)\nNote that det(\u03a3W \u2032) \u2264 1, since all eigenvalues of \u03a3 are \u2264 1, with equality only when W is completely orthogonal to the single eigenvector whose eigenvalue is strictly < 1, which has probability zero under the distribution for W . Therefore, we have that det(\u03a3W \u2032) < 1, and\ndet(WT\u03a3W ) = det(\u03a3w) > det(\u03a3). (8)\nThe above result shows that the volume ratio of individual components never decrease, and always increase when their co-variance matrices are full rank (no zero eigenvalue). Now, we consider the case of the Gaussian mixture. Note that the volume ratio of the mixture equals the sum of the ratios of individual components, since the denominator Vol(Bm) is the same, where the support volume in these ratios for component j is defined with respect to a threshold /\u03c4j . Also, note that since mixture distribution has non-zero volume, at least one of the Gaussian components must have all non-zero eigenvalues. Therefore, the volume ratios of Px and PWT x are both sums of individual Gaussian component terms, and each term for PWT x is greater than or equal to the corresponding term for Px, and at least one term is strictly greater. Therefore, the support volume ratio of PWT x is strictly greater than that of Px.\nProof of Theorem 2.2. The proof of follows along the same steps as that of Theorem 1 in [1].\nV (Dk, G)\n= Ex\u223cPx [logDk(WTk x)] + Ex\u223cPg [log(1\u2212Dk(WTk x))] = EY\u223cP\nWT k x [logDk(y)] + Ey\u223cP WT k g [log(1\u2212Dk(y))]. (9)\nFor any point y \u2208 supp(PWTk x) \u222a supp(PWTk g), differentiating V (Dk, G) w.r.t. Dk and setting to 0 gives us:\nDk(y) = PWTk x(y)\nPWTk x(y) + PWTk g(y) . (10)\nNotice we can rewrite V (Dk, G) as\nV (Dk, G) = \u22122 log(2) +KL ( PWTk x|| PWTk x + PWTk g 2 ) +KL ( PWTk g|| PWTk x + PWTk g 2 ) . (11)\nHere KL is the Kullback Leibler divergence, and it is easy to see that the above expression achieves the minimum value when PWTk x = PWTk g .\nProof of Theorem 2.3. We first prove this result for discrete distributions supported on a compact set S with \u03b3 points along each dimension. Let P\u0303 denote such a discretization of a distribution P.\nEach of the marginal equation P\u0303WTk x = P\u0303WTk g is equivalent to \u03b3 m linear equations of the distribution P\u0303x of the form, \u2211 x:WTk x=y P\u0303x(x) = P\u0303WTk g(y). Note that we have \u03b3 d choices for x and \u03b3m choices for y. Let Ak \u2208 R\u03b3 m\u00d7\u03b3d denote the coefficient matrix AkP\u0303x = P\u0303WTk g , such that Ak(i, j) = 1 if WTk xi = yj , and 0 otherwise.\nThe rows of Ak for different values of yj are clearly orthogonal. Further, since different Wk are independent Gaussian matrices, rows of Ak corresponding to different Wk are linearly independent. In particular let A \u2208 R\u03b3mK\u00d7\u03b3d denote the vertical concatenation of Ak. Then, A has full row rank of \u03b3m \u00b7K with probability \u2265 1\u2212 c \u00b7m \u00b7 e\u2212d [22]. Here c is some arbitrary positive constant.\nSince P\u0303x is a vector of dimension \u03b3d, that many linearly independent equations, uniquely determine it. Hence \u03b3m \u00b7K \u2265 \u03b3d, guarantees that P\u0303x = P\u0303g . Now we extend the results to the continuous setting. Without loss of generality, let the compact support S of the distributions be contained in a sphere of radius B. Let N\nL be an L net of S, with\n\u03b3d points (see Lemma 5.2 in [22]), where \u03b3 = 2B \u00b7 L/ . Then for every point x1 \u2208 S, there exists a x2 \u2208 N L such that, d(x1, x2) \u2264 L .\nFurther for any x1, x2 with d(x1, x2) \u2264 L , from the Lipschitz assumption of the distributions we know that,\n|Px(x1)\u2212 Px(x2)| \u2264 L \u00b7 L = . (12)\nFinally, notice that the marginal constraints do not guarantee that the distributions P\u0303WTk x and P\u0303WTk g match exactly on the -net, but only that they are equal upto an additive factor of . Hence, combining this with equation 12 we get, |Px(x)\u2212 Pg(x)| \u2264 O( ), for any x with probability \u2265 1\u2212 c \u00b7m \u00b7K \u00b7 e\u2212d."}, {"heading": "B Additional Experimental Results", "text": "Face Images: Proposed Method (K = 48)\nFace Images: Proposed Method (K = 24)\nFace Images: Proposed Method (K = 12)\nFace Images: Traditional DC-GAN (Iter. 40k)\nFace Images: Traditional DC-GAN (Iter. 100k)\nRandom Imagenet-Canine Images: Proposed Method"}], "references": [{"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Semi-supervised learning with context-conditional generative adversarial networks", "author": ["Emily Denton", "Sam Gross", "Rob Fergus"], "venue": "[cs.CV],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "venue": "[cs.CV],", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "[cs.CV],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumit Chintala"], "venue": "In ICLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["Martin Arjovsky", "L\u00e9on Bottou"], "venue": "In NIPS Workshop on Adversarial Training,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Wasserstein gan", "author": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "venue": "[stat.ML],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "[cs.LG],", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Ensembles of generative adversarial networks", "author": ["Yaxing Wang", "Lichao Zhang", "Joost van de Weijer"], "venue": "[cs.CV],", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Generative multi-adversarial networks", "author": ["Ishan Durugkar", "Ian Gemp", "Sridhar Mahadevan"], "venue": "[cs.LG],", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Experiments with a new boosting algorithm", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Methods for combining experts\u2019 probability assessments", "author": ["Robert A Jacobs"], "venue": "Neural computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Sliced and radon wasserstein barycenters of measures", "author": ["Nicolas Bonneel", "Julien Rabin", "Gabriel Peyr\u00e9", "Hanspeter Pfister"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In ICCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Generative adversarial networks (GANs), introduced by [1], endow neural networks with the ability to express distributional outputs.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting [2], and super-resolution [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting [2], and super-resolution [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Recently, [4] demonstrated that GANs can be used to produce plausible mappings between a variety of domains\u2014including sketches and photographs, maps and aerial views, segmentation masks and images, etc.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].", "startOffset": 189, "endOffset": 198}, {"referenceID": 4, "context": "GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].", "startOffset": 189, "endOffset": 198}, {"referenceID": 5, "context": "GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks [2, 5, 6].", "startOffset": 189, "endOffset": 198}, {"referenceID": 6, "context": "[7] proposed explicitly factorizing generating an image into a sequence of conditional generations of levels of a Laplacian", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "pyramid, while [6] demonstrated that specific architecture choices and parameter settings led to higher-quality samples.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 190, "endOffset": 197}, {"referenceID": 10, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 190, "endOffset": 197}, {"referenceID": 11, "context": "Other techniques include providing additional supervision [8], adding noise to the discriminator input [9], as well as modifying or adding regularization to the training objective functions [10\u201312].", "startOffset": 190, "endOffset": 197}, {"referenceID": 8, "context": "In this work, we seek to provide a framework that addresses a key source of instability [9] when training GANs\u2014when the true distribution of high-dimensional data is concentrated in a small fraction of the ambient space.", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN [12] and Wasserstein GAN [11] show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity.", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN [12] and Wasserstein GAN [11] show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "[10] further extended GAN training to any choice of f -divergence as objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] proposed several heuristics to improve the stability of training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Note that prior methods have involved training ensembles of GANs [13], or ensembles of discriminators [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Note that prior methods have involved training ensembles of GANs [13], or ensembles of discriminators [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": ", with boosting [15], or mixtureof-experts [16], is a common approach to learning.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": ", with boosting [15], or mixtureof-experts [16], is a common approach to learning.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "[17], who also use low-dimensional projections to deal with high-dimensional probability distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "GANs [1] traditionally comprise of a generator G that learns to generate samples from a data distribution Px, through adversarial training against a single discriminator D as:", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "As described in [9], this instability is due to the fact that the distributions Px corresponding to natural data often have very limited support in the ambient domain of x and G(z).", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "Assume Px = \u2211 j \u03c4jN (x|\u03bcj ,\u03a3j) is a mixture of Gaussians, such that the individual components are sufficiently well separated (in the sense that there is no overlap between their supports or the projections thereof, see [18]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 5, "context": "Prior work [6] has shown that using convolutional architectures for both the generator and discriminator networks is key to generating images with GANs.", "startOffset": 11, "endOffset": 14}, {"referenceID": 18, "context": "For evaluation, we primarily use the dataset of celebrity faces collected by [19]\u2014we use the cropped and aligned 64\u00d7 64-size version of the images\u2014and the DC-GAN [6] architectures for the generator and discriminator.", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "For evaluation, we primarily use the dataset of celebrity faces collected by [19]\u2014we use the cropped and aligned 64\u00d7 64-size version of the images\u2014and the DC-GAN [6] architectures for the generator and discriminator.", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "As suggested in [6], we use Adam [20] with learning rate 2 \u00d7 10\u22124, \u03b21 = 0.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "As suggested in [6], we use Adam [20] with learning rate 2 \u00d7 10\u22124, \u03b21 = 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "Finally, we also use our framework to train a generator on a subset of the Imagenet-1K database [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": ", [10\u201312]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [10\u201312]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [10\u201312]).", "startOffset": 2, "endOffset": 9}], "year": 2017, "abstractText": "Training generative adversarial networks is unstable in high-dimensions when the true data distribution lies on a lower-dimensional manifold. The discriminator is then easily able to separate nearly all generated samples leaving the generator without meaningful gradients. We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. We show that individual discriminators then provide stable gradients to the generator, and that the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.", "creator": "LaTeX with hyperref package"}}}