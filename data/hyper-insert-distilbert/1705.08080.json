{"id": "1705.08080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Visual Semantic Planning using Deep Successor Representations", "abstract": "implementing a crucial cognitive capability of real - world performing intelligent agents here is enhancing their hypothetical ability to partially plan a natural sequence of everyday actions to achieve achieve their goals in the visual puzzle world. in this work, we primarily address the problem vision of visual semantic link planning : establishing the task of predicting interpreting a sequence of actions from visual mirror observations that transform creating a dynamic reasoning environment from an innate initial state strategy to realize a goal pursuit state. doing so entails knowledge recovery about perceived objects planning and understanding their affordances, as well act as influencing actions and their preconditions and effects. informally we propose learning these scenarios through interacting images with a visual and dynamic environment. our proposed solution platform involves bootstrapping reinforcement learning framework with conceptual imitation learning. to potentially ensure cross - task generalization, we develop developing a deep predictive model based greatly on successor brain representations. furthermore our future experimental results frequently show near optimal results across a wildly wide range choice of tasks in the challenging math thor human environment. the completed supplementary prediction video can be accessed here at the following link :", "histories": [["v1", "Tue, 23 May 2017 05:22:47 GMT  (7454kb,D)", "https://arxiv.org/abs/1705.08080v1", null], ["v2", "Tue, 15 Aug 2017 21:13:49 GMT  (7305kb,D)", "http://arxiv.org/abs/1705.08080v2", "ICCV 2017 camera ready"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["yuke zhu", "daniel gordon", "eric kolve", "dieter fox", "li fei-fei", "abhinav gupta", "roozbeh mottaghi", "ali farhadi"], "accepted": false, "id": "1705.08080"}, "pdf": {"name": "1705.08080.pdf", "metadata": {"source": "CRF", "title": "Visual Semantic Planning using Deep Successor Representations", "authors": ["Yuke Zhu", "Daniel Gordon", "Eric Kolve", "Dieter Fox", "Li Fei-Fei", "Abhinav Gupta", "Roozbeh Mottaghi", "Ali Farhadi"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Humans demonstrate levels of visual understanding that go well beyond current formulations of mainstream vision tasks (e.g. object detection, scene recognition, image segmentation). A key element to visual intelligence is the ability to interact with the environment and plan a sequence of actions to achieve specific goals; This, in fact, is central to the survival of agents in dynamic environments [2, 37].\nVisual semantic planning, the task of interacting with a visual world and predicting a sequence of actions that achieves a desired goal, involves addressing several challenging problems. For example, imagine the simple task of putting the bowl in the microwave in the visual dynamic environment depicted in Figure 1. A successful plan involves first finding the bowl, navigating to it, then grabbing it, followed by finding and navigating to the microwave, opening the microwave, and finally putting the bowl in the microwave.\nThe first challenge in visual planning is that performing\n\u2217indicates equal contribution.\neach of the above actions in a visual dynamic environment requires deep visual understanding of that environment, including the set of possible actions, their preconditions and effects, and object affordances. For example, to open a microwave an agent needs to know that it should be in front of the microwave, and it should be aware of the state of the microwave and not try to open an already opened microwave. Long explorations that are required for some tasks imposes the second challenge. The variability of visual observations and possible actions makes na\u0131\u0308ve exploration intractable. To find a cup, the agent might need to search several cabinets one by one. The third challenge is emitting a sequence of actions such that the agent ends in the goal state and the effects of the preceding actions meet the preconditions of the proceeding ones. Finally, a satisfactory solution to visual planning should enable cross task transfer; previous knowledge about one task should make it easier to learn the next one. This is the fourth challenge.\nIn this paper, we address visual semantic planning as a policy learning problem. We mainly focus on high-level actions and do not take into account the low-level details of motor control and motion planning. Visual Semantic Planning (VSP) is the task of predicting a sequence of semantic actions that moves an agent from a random initial state in a\nar X\niv :1\n70 5.\n08 08\n0v 2\n[ cs\n.C V\n] 1\n5 A\nug 2\n01 7\nvisual dynamic environment to a given goal state. To address the first challenge, one needs to find a way to represent the required knowledge of objects, actions, and the visual environment. One possible way is to learn these from still images or videos [12, 51, 52]. But we argue that learning high-level knowledge about actions and their preconditions and effects requires an active and prolonged interaction with the environment. In this paper, we take an interaction-centric approach where we learn this knowledge through interacting with the visual dynamic environment. Learning by interaction on real robots has limited scalability due to the complexity and cost of robotics systems [39, 40, 49]. A common treatment is to use simulation as mental rehearsal before real-world deployment [4, 21, 26, 53, 54]. For this purpose, we use the THOR framework [54], extending it to enable interactions with objects, where an action is specified as its pre- and post-conditions in a formal language.\nTo address the second and third challenges, we cast VSP as a policy learning problem, typically tackled by reinforcement learning [11, 16, 22, 30, 35, 46]. To deal with the large action space and delayed rewards, we use imitation learning to bootstrap reinforcement learning and to guide exploration. To address the fourth challenge of cross task generalization [25], we develop a deep predictive model based on successor representations [7, 24] that decouple environment dynamics and task rewards, such that knowledge from trained tasks can be transferred to new tasks with theoretical guarantees [3].\nIn summary, we address the problem of visual semantic planning and propose an interaction-centric solution. Our proposed model obtains near optimal results across a spectrum of tasks in the challenging THOR environment. Our results also show that our deep successor representation offers crucial transferability properties. Finally, our qualitative results show that our learned representation can encode visual knowledge of objects, actions, and environments."}, {"heading": "2. Related Work", "text": "Task planning. Task-level planning [10, 13, 20, 47, 48] addresses the problem of finding a high-level plan for performing a task. These methods typically work with highlevel formal languages and low-dimensional state spaces. In contrast, visual semantic planning is particularly challenging due to the high dimensionality and partial observability of visual input. In addition, our method facilitates generalization across tasks, while previous methods are typically designed for a specific environment and task.\nPerception and interaction. Our work integrates perception and interaction, where an agent actively interfaces with the environment to learn policies that map pixels to actions. The synergy between perception and interaction has drawn\nan increasing interest in the vision and robotics community. Recent work has enabled faster learning and produced more robust visual representations [1, 32, 39] through interaction. Some early successes have been shown in physical understanding [9, 26, 28, 36] by interacting with an environment.\nDeep reinforcement learning. Recent work in reinforcement learning has started to exploit the power of deep neural networks. Deep RL methods have shown success in several domains such as video games [35], board games [46], and continuous control [30]. Model-free RL methods (e.g., [30, 34, 35]) aim at learning to behave solely from actions and their environment feedback; while model-based RL approaches (e.g., [8, 44, 50]) also estimate a environment model. Successor representation (SR), proposed by Dayan [7], can be considered as a hybrid approach of model-based and model-free RL. Barreto et al. [3] derived a bound on value functions of an optimal policy when transferring policies using successor representations. Kulkarni et al. [24] proposed a method to learn deep successor features. In this work, we propose a new SR architecture with significantly reduced parameters, especially in large action spaces, to facilitate model convergence. We propose to first train the model with imitation learning and fine-tune with RL. It enables us to perform more realistic tasks and offers significant benefits for transfer learning to new tasks.\nLearning from demonstrations. Expert demonstrations offer a source of supervision in tasks which must usually be learned with copious random exploration. A line of work interleaves policy execution and learning from expert demonstration that has achieved good practical results [6, 43]. Recent works have employed a series of new techniques for imitation learning, such as generative adversarial networks [19, 29], Monte Carlo tree search [17] and guided policy search [27], which learn end-to-end policies from pixels to actions.\nSynthetic data for visual tasks. Computer games and simulated platforms have been used for training perceptual tasks, such as semantic segmentation [18], pedestrian detection [33], pose estimation [38], and urban driving [5, 41, 42, 45]. In robotics, there is a long history of using simulated environments for learning and testing before real-world deployment [23]. Several interactive platforms have been proposed for learning control with visual inputs [4, 21, 26, 53, 54]. Among these, THOR [54] provides high-quality realistic indoor scenes. Our work extends THOR with a new set of actions and the integration of a planner."}, {"heading": "3. Interactive Framework", "text": "To enable interactions with objects and with the environment, we extend the THOR framework [54], which has been used for learning visual navigation tasks. Our extension in-\ncludes new object states, and a discrete description of the scene in a planning language [13]."}, {"heading": "3.1. Scenes", "text": "In this work, we focus on kitchen scenes, as they allow for a variety of tasks with objects from many categories. Our extended THOR framework consists of 10 individual kitchen scenes. Each scene contains an average of 53 distinct objects with which the agent can interact. The scenes are developed using the Unity 3D game engine."}, {"heading": "3.2. Objects and Actions", "text": "We categorize the objects by their affordances [15], i.e., the plausible set of actions that can be performed. For the tasks of interest, we focus on two types of objects: 1) items that are small objects (mug, apple, etc.) which can be picked up, held, and moved by the agent to various locations in the scene, and 2) receptacles that are large objects (table, sink, etc.) which are stationary and can hold a fixed capacity of items. A subset of receptacles, such as fridges and cabinets, are containers. These containers have doors that can be opened and closed. The agent can only put an item in a container when it is open. We assume that the agent can hold at most one item at any point. We further define the following actions to interact with the objects:\n1. Navigate {receptacle}: moving from the current location of the agent to a location near the receptacle; 2. Open {container}: opening the door of a container in front of an agent; 3. Close {container}: closing the door of a container in front of an agent; 4. Pick Up {item}: picking up an item in field of view; 5. Put {receptacle}: putting a held item in the receptacle; 6. Look Up and Look Down: tilting the agent\u2019s gaze 30\ndegrees up or down. In summary, we have six action types, each taking a corresponding type of action arguments. The combination of actions and arguments results in a large action set of 80 per\nscene on average. Fig. 2 illustrates example scenes and the six types of actions in our framework. Our definition of action space makes two important abstractions to make learning tractable: 1) it abstracts away from navigation, which can be tackled by a subroutine using existing methods such as [54]; and 2) it allows the model to learn with semantic actions, abstracting away from continuous motions, e.g., the physical movement of a robot arm to grasp an object. A common treatment for this abstraction is to \u201cfill in the gaps\u201d between semantic actions with callouts to a continuous motion planner [20, 47]. It is evident that not all actions can be performed in every situation. For example, the agent cannot pick up an item when it is out of sight, or put a tomato into fridge when the fridge door is closed. To address these requirements, we specify the pre-conditions and effects of actions. Next we introduce an approach to declaring them as logical rules in a planning language. These rules are only encoded in the environment but not exposed to the agent. Hence, the agent must learn them through interaction."}, {"heading": "3.3. Planning Language", "text": "The problem of generating a sequence of actions that leads to the goal state has been formally studied in the field of automated planning [14]. Planning languages offer a standard way of expressing an automated planning problem instance, which can be solved by an off-the-shelf planner. We use STRIPS [13] as the planning language to describe our visual planning problem.\nIn STRIPS, a planning problem is composed of a description of an initial state, a specification of the goal state(s), and a set of actions. In visual planning, the initial state corresponds to the initial configuration of the scene. The specification of the goal state is a boolean function that returns true on states where the task is completed. Each action is defined by its precondition (conditions that must be satisfied before the action is performed) and postcondition (changes caused by the action). The STRIPS formulation enables us to define the rules of the scene, such as object affordances and causality of actions."}, {"heading": "4. Our Approach", "text": "We first outline the basics of policy learning in Sec. 4.1. Next we formulate the visual semantic planning problem as a policy learning problem and describe our model based on successor representation. Later we propose two protocols of training this model using imitation learning (IL) and reinforcement learning (RL). To this end, we use IL to bootstrap our model and use RL to further improve its performance."}, {"heading": "4.1. Successor Representation", "text": "We formulate the agent\u2019s interactions with an environment as a Markov Decision Process (MDP), which can be specified by a tuple (S,A, p, r, \u03b3). S and A are the sets of\nstates and actions. For s \u2208 S and a \u2208 A, p(s\u2032|s, a) defines the probability of transiting from the state s to the next state s\u2032 \u2208 S by taking action a. r(s, a) is a real-value function that defines the expected immediate reward of taking action a in state s. For a state-action trajectory, we define the future discounted return R = \u2211\u221e i=0 \u03b3\nir(si, ai), where \u03b3 \u2208 [0, 1] is called the discount factor, which trades off the importance of immediate rewards versus future rewards.\nA policy \u03c0 : S \u2192 A defines a mapping from states to actions. The goal of policy learning is to find the optimal policy \u03c0\u2217 that maximizes the future discounted return R starting from state s0 and following the policy \u03c0\u2217. Instead of directly optimizing a parameterized policy, we take a value-based approach. We define a state-action value function Q\u03c0 : S \u00d7A \u2192 R under a policy \u03c0 as\nQ\u03c0(s, a) = E\u03c0[R|s0 = s, a0 = a], (1)\ni.e., the expected episode return starting from state s, taking action a, and following policy \u03c0. The Q value of the optimal policy \u03c0\u2217 obeys the Bellman equation [49]:\nQ\u03c0 \u2217 (s, a) = E\u03c0 \u2217 [r(s, a) + \u03b3max a\u2032 Q(s\u2032, a\u2032)] (2)\nIn deep Q networks [35], Q functions are approximated by a neural networkQ(s, a|\u03b8), and can be trained by minimizing the `2-distance between both sides of the Bellman equation in Eq. (2). Once we learn Q\u03c0 \u2217 , the optimal action at state s can be selected by a\u2217 = argmaxaQ\u03c0 \u2217 (s, a).\nSuccessor representation (SR), proposed by Dayan [7], uses a similar value-based formulation for policy learning. It differs from traditional Q learning by factoring the\nvalue function into a dot product of two components: a reward predictor vector w and a predictive successor feature \u03c8(s, a). To derive the SR formulation, we start by factoring the immediate rewards such that\nr(s, a) = \u03c6(s, a)Tw, (3)\nwhere \u03c6(s, a) is a state-action feature. We expand Eq. (1) using this reward factorization:\nQ\u03c0(s, a) = E\u03c0[ \u221e\u2211 i=0 \u03b3ir(si, ai)|s0 = s, a0 = a]\n= E\u03c0[ \u221e\u2211 i=0 \u03b3i\u03c6(si, ai) Tw|s0 = s, a0 = a]\n= E\u03c0[ \u221e\u2211 i=0 \u03b3i\u03c6(si, ai)|s0 = s, a0 = a]Tw = \u03c8\u03c0(s, a)Tw (4)\nWe refer to \u03c8(s, a)\u03c0 = E\u03c0[ \u2211\u221e i=0 \u03b3\ni\u03c6s,a|s0 = s, a0 = a] as the successor features of the pair (s, a) under policy \u03c0.\nIntuitively, the successor feature \u03c8\u03c0(s, a) summarizes the environment dynamics under a policy \u03c0 in a state-action feature space, which can be interpreted as the expected future \u201cfeature occupancy\u201d. The reward predictor vector w induces the structure of the reward functions, which can be considered as an embedding of a task. Such decompositions have been shown to offer several advantages, such as being adaptive to changes in distal rewards and apt to option discovery [24]. A theoretical result derived by Barreto et al. implies a bound on performance guarantee when the agent\ntransfers a policy from a task t to a similar task t\u2032, where task similarity is determined by the `2-distance of the corresponding w vectors between these two tasks t and t\u2032 [3]. Successor representation thus provides a generic framework for policy transfer in reinforcement learning."}, {"heading": "4.2. Our Model", "text": "We formulate the problem of visual semantic planning as a policy learning problem. Formally, we denote a task by a Boolean function t : S \u2192 {0, 1}, where a state s completes the task t iff t(s) = 1. The goal is to find an optimal policy \u03c0\u2217, such that given an initial state s0, \u03c0\u2217 generates a state-action trajectory T = {(si, ai) | i = 0 . . . T} that maximizes the sum of immediate rewards \u2211T\u22121 i=0 r(si, ai), where t(s0...T\u22121) = 0 and t(sT ) = 1. We parameterize such a policy using the successor representation (SR) model from the previous section. We develop a new neural network architecture to learn \u03c6, \u03c8 and w. The network architecture is illustrated in Fig. 3. In THOR, the agent\u2019s observations come from a first-person RGB camera. We also pass the agent\u2019s internal state as input, expressed by one-hot encodings of the held object in its inventory. The action space is described in Sec. 3.2. We start by computing embedding vectors for the states and the actions. The image is passed through a 3-layer convolutional encoder, and the internal state through a 2-layer MLP, producing a state embedding \u00b5s = f(s; \u03b8cnn, \u03b8int). The action a = [atype, aarg] is encoded as one-hot vectors and passed through a 2-layer MLP encoder that produces an action embedding \u00b5a = g(atype, aarg; \u03b8mlp). We fuse the state and action embeddings and generate the stateaction feature \u03c6s,a = h(\u00b5s, \u00b5a; \u03b8r) and the successor feature \u03c8s,a = m(\u00b5s, \u00b5a; \u03b8q) in two branches. The network predicts the immediate reward rs,a = \u03c6Ts,aw and the Q value under the current policy Qs,a = \u03c8Ts,aw using the decomposition from Eq. (3) and (4)."}, {"heading": "4.3. Imitation Learning", "text": "Our SR-based policy can be learned in two fashions. First, it can be trained by imitation learning (IL) under the supervision of the trajectories of an optimal planner. Second, it can be learned by trial and error using reinforcement learning (RL). In practice, we find that the large action space in THOR makes RL from scratch intractable due to the challenge of exploration. The best model performance is produced by IL bootstrapping followed by RL fine-tuning. Given a task, we generate a state-action trajectory:\nT = {(s0, a0), {(s1, a1), . . . , (sT\u22121, aT\u22121), (sT , \u2205)} (5)\nusing the planner from the initial state-action pair (s0, a0) to the goal state sT (no action is performed at terminal states). This trajectory is generated on a low-dimensional\nInput Remapping\nstate representation in the STRIPS planner (Sec. 3.3). Each low-dimensional state corresponds to an RGB image, i.e., the agent\u2019s visual observation. During training, we perform input remapping to supervise the model with image-action pairs rather than feeding the low-dimensional planner states to the network. To fully explore the state space, we take planner actions as well as random actions off the optimal plan. After each action, we recompute the trajectory. This process of generating training data from a planner is illustrated in Fig. 4. Each state-action pair is associated with a true immediate reward r\u0302s,a. We use the mean squared loss function to minimize the error of reward prediction:\nLr = 1\nT T\u22121\u2211 i=0 (r\u0302s,a \u2212 \u03c6Ts,aw)2. (6)\nFollowing the REINFORCE rule [49], we use the discounted return along the trajectory T as an unbiased estimate of the true Q value: Q\u0302s,a \u2248 \u2211T\u22121 i=0 \u03b3\nir\u0302s,a. We use the mean squared loss to minimize the error of Q prediction:\nLQ = (Q\u0302s,a \u2212 \u03c8Ts,aw)2 (7)\nThe final loss on the planner trajectory T is the sum of the reward loss and the Q loss: LT = Lr+LQ. Using this loss signal, we train the whole SR network on a large collection of planner trajectories starting from random initial states."}, {"heading": "4.4. Reinforcement Learning", "text": "When training our SR model using RL, we can still use the mean squared loss in Eq. (6) to supervise the learning of reward prediction branch for \u03c6 and w. However, in absence of expert trajectories, we would need an iterative way to learn the successor features \u03c8. Rewriting the Bellman equation in Eq. (2) with the SR factorization, we can obtain an equality on \u03c6 and \u03c8:\n\u03c8\u03c0 \u2217 (s, a) = E\u03c0 \u2217 [\u03c6(s, a) + \u03b3\u03c8(s\u2032, a\u2032)] (8)\nwhere a\u2032 = argmaxa \u03c8(s\u2032, a)Tw. Similar to DQN [35], we minimize the `2-loss between both sides of Eq. (8):\nLSR = E\u03c0[(\u03c6s,a + \u03b3\u03c8s\u2032,a\u2032 \u2212 \u03c8\u03c0s,a)2] (9)\nWe use a similar procedure to Kulkarni et al. [24] to train our SR model. The model alternates training between the reward branch and the SR branch. At each iteration, a minibatch is randomly drawn from a replay buffer of past experiences [35] to perform one SGD update."}, {"heading": "4.5. Transfer with Successor Features", "text": "A major advantage of successor features is its ability to transfer across tasks by exploiting the structure shared by the tasks. Given a fixed state-action representation \u03c6, let M\u03c6 be the set of all possible MDPs induced by \u03c6 and all instantiations of the reward prediction vectors w. Assume that \u03c0\u2217i is the optimal policy of the i-th task in the set {Mi \u2208 M\u03c6|i = 1, . . . n}. Let Mn+1 to be a new task. We denote Q\u03c0 \u2217 i n+1 as the value function of executing the optimal policy of the task Mi on the new task Mn+1, and Q\u0303 \u03c0\u2217i n+1 as an approximation ofQ\u03c0 \u2217 i n+1 by our SR model. Given a bound on the approximations such that\n|Q\u03c0 \u2217 i n+1(s, a)\u2212Q\u0303 \u03c0\u2217i n+1(s, a)| \u2264 \u2200s \u2208 S, a \u2208 A, i = 1, . . . , n,\nwe define a policy \u03c0\u2032 for the new task Mn+1 using Q\u03031,...,n, where \u03c0\u2032(s) = argmaxamaxi Q\u0303 \u03c0\u2217i n+1(s, a). Theorem 2 in Barreto et al. [3] implies a bound of the gap between value functions of the optimal policy \u03c0\u2217n+1 and the policy \u03c0 \u2032:\nQ \u03c0\u2217n+1 n+1 (s, a)\u2212Q\u03c0 \u2032 n+1(s, a) \u2264 2\u03c6m 1\u2212 \u03b3 (min i ||wi\u2212wn+1||+ ),\nwhere \u03c6m = maxs,a ||\u03c6(s, a)||. This result serves the theoretical foundation of policy transfer in our SR model. In practice, when transferring to a new task while the scene dynamics remain the same, we freeze all model parameters except the single vector w. This way, the policy of the new task can be learned with substantially higher sample efficiency than training a new network from scratch."}, {"heading": "4.6. Implementation Details", "text": "We feed a history of the past four observations, converted to grayscale, to account for the agent\u2019s motions. We use a time cost of \u22120.01 to encourage shorter plans and a task completion reward of 10.0. We train our model with imitation learning for 500k iterations with a batch size of 32, and a learning rate of 1e-4. We also include the successor loss in Eq. (9) during imitation learning, which helps learn better successor features. We subsequently fine-tune the network with reinforcement learning with 10,000 episodes."}, {"heading": "5. Experiments", "text": "We evaluate our model using the extended THOR framework on a variety of household tasks. We compare our method against standard reinforcement learning techniques as well as with non-successor based deep models. The tasks compare the different methods\u2019 abilities to learn across varying time horizons. We also demonstrate the SR network\u2019s ability to efficiently adapt to new tasks. Finally, we show that our model can learn a notion of object affordance by interacting with the scene."}, {"heading": "5.1. Quantitative Evaluation", "text": "We examine the effectiveness of our model and baseline methods on a set of tasks that require three levels of planning complexity in terms of optimal plan length.\nExperiment Setup We explore the two training protocols introduced in Sec. 4 to train our SR model:\n1. RL: we train the model solely based on trial and error, and learn the model parameters with RL update rules.\n2. IL: we use the planner to generate optimal trajectories starting from a large collection of random initial stateaction pairs. We use the imitation learning methods to train the networks using supervised losses.\nEmpirically, we find that training with reinforcement learning from scratch cannot handle the large action space. Thus, we report the performance of our SR model trained with imitation learning (SR IL) as well as with additional reinforcement learning fine-tuning (SR IL + RL).\nWe compare our SR model with the state-of-the-art deep RL model, A3C [34], which is an advantage-based actorcritic method that allows the agent to learn from multiple copies of simulation while updating a single model in an asynchronous fashion. A3C establishes a strong baseline for reinforcement learning. We further use the same architecture to obtain two imitation learning (behavior cloning) baselines. We use the same A3C network structure to train a softmax classifier that predicts the planner actions given an input. The network predicts both the action types (e.g., Put) and the action arguments (e.g., apple). We call this baseline CLS-MLP. We also investigate the role of memory in these models. To do this, we add an extra LSTM layer to the network before action outputs, called CLS-LSTM. We also include simple agents that take random actions and take random valid actions at each time step.\nLevels of task difficulty We evaluate all of the models with three levels of task difficulty based on the length of the optimal plans and the source of randomization:\n1. Level 1 (Easy): Navigate to a container and toggle its state. A sample task would be go to\nthe microwave and open it if it is closed, close it otherwise. The initial location of the agent and all container states are randomized. This task requires identifying object states and reasoning about action preconditions.\n2. Level 2 (Medium): Navigate to multiple receptacles, collect items, and deposit them in a receptacle. A sample task here is pick up three mugs from three cabinets and put them in the sink. Here we randomize the agent\u2019s initial location, while the item locations are fixed. This task requires a long trajectory of correct actions to complete the goal.\n3. Level 3 (Hard): Search for an item and put it in a receptacle. An example task is find the apple and put it in the fridge. We randomize the agent\u2019s location as well as the location of all items. This task is especially difficult as it requires longerterm memory to account for partial observability, such as which cabinets have previously been checked.\nWe evaluate all of the models on 10 easy tasks, 8 medium tasks, and 7 hard tasks, each across 100 episodes. Each episode terminates when a goal state is reached. We consider an episode fails if it does not reach any goal state within 5,000 actions. We report the episode success rate and mean episode length as the performance metrics. We exclude these failed episodes in the mean episode length metric. For the easy and medium tasks, we train the imitation learning models to mimic the optimal plans. However for the hard tasks, imitating the optimal plan is infeasible, as the location of the object is uncertain. In this case, the target object is likely to hide in a cabinet or a fridge which the agent cannot see. Therefore, we train the models to imitate a plan which searches for the object from all the receptacles in a fixed order. For the same reason, we do not perform RL fine-tuning for the hard tasks.\nTable 1 summarizes the results of these experiments. Pure RL-based methods struggle with the medium and hard tasks because the action space is so large that na\u0131\u0308ve exploration rarely, if ever, succeeds. Comparing CLS-MLP and CLS-LSTM, adding memory to the agent helps improving\nsuccess rate on medium tasks as well as completing tasks with shorter trajectories in hard tasks. Overall, the SR methods outperform the baselines across all three task difficulties. Fine-tuning the SR IL model with reinforcement learning further reduces the number of steps towards the goal. More qualitative results can be found in the video.1"}, {"heading": "5.2. Task Transfer", "text": "One major benefit of the successor representation decomposition is its ability to transfer to new tasks while only retraining the reward prediction vector w, while freezing the successor features. We examine the sample efficiency of adapting a trained SR model on multiple novel tasks in the same scene. We examine policy transfer in the hard tasks, as the scene dynamics of the searching policy retains, even when the objects to be searched vary. We evaluate the speed at which the SR model converges on a new task by finetuning the w vector versus training the model from scratch. We take a policy for searching a bowl in the scene and substituting four new items (lettuce, egg, container, and apple) in each new task. Fig. 5 shows the episode success rates (bar chart) and the successful action rate (line plot). By fine-\n1Link to supplementary video: https://goo.gl/vXsbQP\ntuning w, the model quickly adapts to new tasks, yielding both high episode success rate and successful action rate. In contrast, the model trained from scratch takes substantially longer to converge. We also experiment with fine-tuning the entire model, and it suffers from similar slow convergence."}, {"heading": "5.3. Learning Affordances", "text": "An agent in an interactive environment needs to be able to reason about the causal effects of actions. We expect our SR model to learn the pre- and post-conditions of actions through interaction, such that it develops a notion of affordance [15], i.e., which actions can be performed under a circumstance. In the real world, such knowledge could help prevent damages to the agent and the environment caused by unexpected or invalid actions.\nWe first evaluate each network\u2019s ability to implicitly learn affordances when trained on the tasks in Sec. 5.1. In these tasks, we penalize unnecessary actions with a small time penalty, but we do not explicitly tell the network which actions succeed and which fail. Fig. 6 illustrates that a standard reinforcement learning method cannot filter out unnecessary actions especially given delayed rewards. Imitation learning methods produce significantly fewer failed actions because they can directly evaluate whether each action gets them closer to the goal state.\nWe also analyze the successor network\u2019s capability of explicitly learning affordances. We train our SR model with reinforcement learning, by executing a completely random policy in the scene. We define the immediate reward of issuing a successful action as +1.0 and an unsuccessful one as \u22121.0. The agent learns in 10,000 episodes. Fig. 7 shows a t-SNE [31] visualization of the state-action features \u03c6s,a. We see that the network learns to cluster successful state ac-\ntion pairs (shown in green) separate from unsuccessful pairs (orange). The network achieves an ROC-AUC of 0.91 on predicting immediate rewards over random state-action actions, indicating that the model can differentiate successful and unsuccessful actions by performing actions and learning from their outcomes."}, {"heading": "6. Conclusions", "text": "In this paper, we argue that visual semantic planning is an important next task in computer vision. Our proposed solution shows promising results in predicting a sequence of actions that change the current state of the visual world to a desired goal state. We have examined several different tasks with varying degrees of difficulty and show that our proposed model based on deep successor representations achieves near optimal results in the challenging THOR environment. We also show promising cross-task knowledge transfer results, a crucial component of any generalizable solution. Our qualitative results show that our learned successor features encode knowledge of object affordances, and action pre-conditions and post-effects. Our next steps involve exploring knowledge transfer from THOR to realworld environments as well as examining the possibilities of more complicated tasks with a richer set of actions."}, {"heading": "Acknowledgements", "text": "This work is in part supported by ONR N00014-13-10720, ONR MURI N00014-16-1-2007, NSF IIS-1338054, NSF-1652052, NRI-1637479, NSF IIS-1652052, a Siemens\ngrant, the Intel Science and Technology Center for Pervasive Computing (ISTC-PC), Allen Distinguished Investigator Award, and the Allen Institute for Artificial Intelligence."}, {"heading": "A. Experiment Details", "text": "A.1. Experiment Setup\nWe used the Adam optimizer from (Kingma and Ba) for learning our Successor Representation (SR) model with a learning rate of 1e-4 and a mini-batch size of 32. For the reinforcement learning experiments, we use the discounted factor \u03b3 = 0.99 and a replay buffer size of 100,000. The exploration term is annealed from 1.0 to 0.1 during the training process. We run an -greedy policy ( = 0.1) during evaluation. We use soft target updates (\u03c4 = 0.1) after every episode. For the easy and medium tasks, we assign +10.0 immediate reward for task completion, \u22125.0 for invalid actions, and \u22121.0 for other actions (to encourage a shorter plan). For the hard task, we train our SR model to imitate a plan that searches all the receptacles for an object in a fixed order of visitation based on the spatial locations of the receptacles. We assign +1.0 immediate reward for task completion, and an episode terminates as failure if the agent does not follow the order of visitation in the plan.\nA.2. Network Inputs\nThe input to the SR model consists of three components: action (action type and argument), agent\u2019s observation (image), and agent\u2019s internal state. The action type is encoded by a 7-dimensional one-hot vector, indicating one of the seven action types (Navigate, Open, Close, Pick Up, Put, Look Up, and Look Down). The action argument is encoded by a one-hot vector that has the same dimension as the number of interactable objects plus one. The first dimension denotes null argument used for Look Up and Look Down actions, and the other dimensions correspond to the index of each object. RGB images from the agent\u2019s firstperson camera are preprocessed to 84 \u00d7 84 grayscale images. We stack four history frames to make an 84\u00d7 84\u00d7 4 tensor as the image input to the convolutional networks. The agent\u2019s internal state is expressed by the agent\u2019s inventory, rotation, and viewpoint. The inventory is a one-hot vector that represents the index of the held item, with an extra dimension for null. The rotation is a 4-dimensional one-hot vector that represents the rotation of the agent (90 degree turns). The viewpoint is a 3-dimensional one-hot vector that represents the tiling angle of the agent\u2019s camera (\u221230\u25e6, 0\u25e6, and 30\u25e6).\nA.3. Network Architecture\nHere we describe the network architecture of our proposed SR model. The convolutional image encoder \u03b8cnn takes an 84 \u00d7 84 \u00d7 4 image as input. The three convolutional layers are 32 filters of size 8 \u00d7 8 with stride 4, 64 filters of size 4 \u00d7 4 with stride 2, 64 filters of size 3 \u00d7 3 with stride 1. Finally a fully-connected layer maps the outputs from the convolutional encoder into a 512-d feature.\nThe actions encoder \u03b8mlp and internal state encoder \u03b8int are both 2-layer MLPs with 512 hidden units. A concatenated vector of action, internal state, and image encodings is fed into two 2-layer MLPs \u03b8r and \u03b8q with 512 hidden units to produce the 512-d state-action feature \u03c6s,a and the successor feature \u03c8s,a. We take the dot product of the 512-d reward predictor vector w and state-action features (successor features) to compute the immediate rewards (Q values). All the hidden layers use ReLU non-linearities. The final dot product layers of the immediate reward and the Q value produce raw values without any non-linearity."}, {"heading": "B. Algorithm Details", "text": "We describe the reinforcement learning procedure of the SR model in Algorithm 1. This training method follows closely with previous work on deep Q-learning [35] and deep SR model [24]. Similar to these two works, replay buffer and target network are used to stabilize training."}, {"heading": "C. Action Space", "text": "The set of plausible actions in a scene is determined by the variety of objects in the scene. On average each scene has 53 objects (a subset of them are interactable) and the agent is able to perform 80 actions. Here we provide an example scene to illustrate the interactable objects and the action space.\nScene #9: 16 items, 23 receptacles (at 11 unique locations), and 15 containers (a subset of receptacles)\nitems: apple, bowl, bread, butter knife, glass bottle, egg, fork, knife, lettuce, mug 1-3, plate, potato, spoon, tomato receptacles: cabinet 1-13, coffee machine, fridge, garbage can, microwave, sink, stove burner 1-4, table top containers: cabinet 1-13, fridge, microwave actions: 80 actions in total, including 11 Navigation actions, 15 Open actions, 15 Close actions, 14 Pick Up ac-\nAlgorithm 1 Reinforcement Learning for Successor Representation Model 1: procedure RL-TRAINING 2: Initialize replay buffer D to size N 3: Initialize an SR network \u03b8 with random weights \u03b8 = [\u03b8int, \u03b8cnn, \u03b8mlp, \u03b8r, \u03b8q,w] 4: Make a clone of \u03b8 as the target network \u03b8\u0303 5: for i = 1 : #episodes do: 6: Initialize an environment with random configuration 7: Reset exploration term = 1.0 8: while not terminal do 9: Get agent\u2019s observation and internal state st from the environment 10: Compute Qst,a = f(st, a; \u03b8) for every action a in action space 11: With probability select a random action at; otherwise, select at = argmaxaQst,a 12: Execute action at to obtain the immediate reward rt and the next state st+1 13: Store transition (st, at, rt, st+1) in D 14: Sample a random mini-batch of transitions (sj , aj , rj , sj+1) from D 15: Compute r\u0303j , \u03c6sj ,aj , and \u03c8sj ,aj using \u03b8 for every transition j 16: Compute gradients that minimize the mean squared error between rj and r\u0303j 17: Compute \u03c6sj+1,a, \u03c8sj+1,a, and Q\u0303sj+1,a using \u03b8\u0303 for every transition j and every action a 18: if sj+1 is a terminal state then: 19: Compute gradients that minimize the mean squared error between \u03c8sj ,aj and \u03c6sj ,aj 20: else: 21: Compute gradients that minimize the mean squared error between \u03c8sj ,aj and \u03c6sj ,aj + \u03b3\u03c8sj+1,a\u2032 22: where a\u2032 = argmaxa Q\u0303sj+1,a 23: end if 24: Perform a gradient descend step to update \u03b8 25: end while 26: Anneal exploration term 27: Soft-update target network \u03b8\u0303 using \u03b8 28: end for 29: end procedure\ntions, 23 Put actions, Look Up and Look Down.\nWe have fewer Navigation and Pick Up actions than the number of receptacles and items respectively, as we merge some adjacent receptacles to one location (navigation destination). We also merge picking up items from the same object category into one action. This reduces the size of the action space and speeds up learning. An important simplification that we made is to treat the Navigation actions as \u201cteleports\u201d, which abstracts away from visual navigation of the agent. The actual visual navigation problem can be solved as an independent subroutine from previous work [54]. As discussed in Sec. 3.2, not all actions in the set can be issued given a certain circumstance based on affordance. We use the PDDL language to check if the preconditions of an\naction are satisfied before the action is sent to THOR for execution."}, {"heading": "D. Tasks", "text": "We list all the tasks that we have evaluated in the experiments in Table 2. In summary, we evaluated tasks from three levels of difficulty, with 10 easy tasks, 8 medium tasks, and 7 hard tasks."}], "references": [{"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recent work has enabled faster learning and produced more robust visual representations [1, 32, 39] through interaction.", "startOffset": 88, "endOffset": 99}, {"referenceID": 0, "context": "For a state-action trajectory, we define the future discounted return R = \u2211\u221e i=0 \u03b3 r(si, ai), where \u03b3 \u2208 [0, 1] is called the discount factor, which trades off the importance of immediate rewards versus future rewards.", "startOffset": 104, "endOffset": 110}], "year": 2017, "abstractText": "A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.", "creator": "LaTeX with hyperref package"}}}