{"id": "1705.08321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Increasing Papers' Discoverability with Precise Semantic Labeling: the sci.AI Platform", "abstract": "the anticipated number of regularly published cited findings in biomedicine increases continually. here at the same successive time, web specifics search of the citations domain's terminology presentation complicates the specific task of relevant publications retrieval. simultaneously in the current google research, nowadays we investigate influence of citation terms'specific variability size and ambiguity effect on a small paper's likelihood of publish being retrieved. we obtained statistics that demonstrate significance of managing the issue category and its challenges, followed successively by algorithms presenting the corresponding sci. ai platform, a which allows precise and terms retrieval labeling as a resolution.", "histories": [["v1", "Tue, 2 May 2017 17:04:42 GMT  (743kb)", "http://arxiv.org/abs/1705.08321v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["roman gurinovich", "alexander pashuk", "yuriy petrovskiy", "alex dmitrievskij", "oleg kuryan", "alexei scerbacov", "antonia tiggre", "elena moroz", "yuri nikolsky"], "accepted": false, "id": "1705.08321"}, "pdf": {"name": "1705.08321.pdf", "metadata": {"source": "CRF", "title": "Increasing Papers\u2019 Discoverability with Precise Semantic Labeling: the sci.AI Platform", "authors": ["Roman GURINOVICH", "Alexander PASHUK", "Yuriy PETROVSKIY", "Alex DMITRIEVSKIJ", "Oleg KURYAN", "Alexei SCERBACOV", "Antonia TIGGRE", "Elena MOROZ", "Yuri NIKOLSKY"], "emails": ["roman.gurinovich@xpansa.com"], "sections": [{"heading": null, "text": "Keywords. Supervised semanticization, word sense disambiguation, paper influence and citation, biomedical text processing, named entity recognition"}, {"heading": "1. Key objectives of the study and its significance", "text": "Over the last two decades, life sciences articles have become substantially more complex, reflecting technological evolution, particularly OMICs experimentation, increasing cooperation between multiple institutions, and involving more advanced math and statistics applied to the data. In many publications, plain unstructured text is supported by algorithms, code, and multiple files of processed and raw datasets with annotated metadata and graphs. With such enhancements in place, experimental articles, per se, might become a driving force of the Literature Based Discoveries (LBD) [1]. Recently, the whole field of \u201cmeta-analysis\u201d has arose to describe \u201cdry lab\u201d studies on normalization, unification, and analysis of many similar datasets derived from different labs and projects. However, a number of experimental papers are missed, because they cannot be retrieved from the body of literature by keywords search. Needless to say that scientists are keenly interested in higher discoverability of their published research and referencing to their findings, as citation index becomes an increasingly prevalent metrics in evaluation of their work. The issue can be addressed with proper semantic labeling of the texts as the very first step in global analysis of the research reports.\n1 Corresponding Author: roman.gurinovich@xpansa.com\nThe state-of-the-art section reflects the ways published papers are algorithmically processed in text mining applications. Such computations rely on preexisting, statistically supported information, while text mining of the scientific literature targets novel findings. This leads to Information Retrieval (IR) and then Information Extraction (IE) underperformance when applied to scientific literature.\nThe objective of the paper is to consider just one issue of many in biomedical texts processing: false terms recognition caused by ambiguity of the concepts\u2019 names and multiple-terms spelling variants. This leads to at least two undesirable effects:\n1. Lower recall rate when search engines and aggregators retrieve articles, so the target audience does not receive a full set of relevant papers.\n2. Retrieving a paper that is irrelevant to the sought-for concept. For example, reader can query \u2018cat\u2019 with the \u2018cat\u2019 animal in mind but receive texts about the \u2019CAT\u2019 gene.\nWe address: a. A global need of initial transformation of the plain text to a machine-readable format; and b. Uncertainty issue mentioned above; by releasing the sci.AI system. This system combines automatic metatagging and manual validation of the results by the author or reader and supports generating semantic structures during writing and editorial processing. Human validation eliminates almost any possibility of term misinterpretation in the following IR and IE tasks, because authors can be expected to have a comprehensive understanding of the concepts being mentioned in their papers and can supervise the machine\u2019s results."}, {"heading": "2. State of the art in the field of biomedical texts semanticization", "text": "Computational Linguistics is one of the most dynamic fields with innovations being released almost monthly. Unfortunately, there is no solid state-of-the-art solution for biomedical text labeling yet that unites all the latest advances in each subfield into a single package.\nThe first subfield is metatagging standards and paradigms. Semantic Web\u2019s objects, concepts, knowledge association, and data representation utilize schema.org vocabularies and W3C RDF/XML [2]. A current limitation is the lack of a similar single schema for the life sciences. Former related initiatives here are W3C Scholarly HTML [11] and JATS4R [12]. Still both schemas do not provide a standard namespace for biomedical concepts labeling.\nThe second subfield is terms labeling or Named Entities Recognition. Just as in many other areas, deep learning and neural networks (NN) methods are increasingly popular for extracting information from professional texts [7, 8]. NN algorithms are rather generic and can be applied for the text analysis in unsupervised fashion (i.e., to a variety of texts without establishing prior rules generation). However, its precision and recall hardy depend on statistical data and cannot be considered as stable solution for concepts and challenges that have appeared recently. Still, NN demonstrates the highest recognition rates [16, 17] among automatic methods.\nThen there are methods of increasing precision by reducing concepts ambiguity by connecting the same concepts in various ontologies.\nUMLS (Unified Medical Language System), in combination with MetaMap, provide graph-like links between objects from various ontologies, as widely-accepted\nsolution for the Word Sense Disambiguation (WSD) task in the biomedical domain. Essentially, UMLS represents a metaontology of biomedical terms and concepts. UMLS is extensive and well supported by NIH, and it is in constant development. Future considerations include possibly connecting this data to the sci.AI application. Currently, there are several limitations:\n1. Lack of details for specialised ontologies, such as Uniprot and ChEBI. 2. Focus on the indexing task for the NCBI. This leads to the same dropdown in precision and recall of post-publication text processing. 3. It is not a simple plug-and-play solution for the publishing industry [6]. SciGraph by the Neo4j [13] framework allows objects to be interconnected and can be used as a technical basis for future metaontologies."}, {"heading": "3. Design and Methodology", "text": "Resolving terms\u2019 ambiguity and variability represents a significant challenge in text processing. Here, we investigated how these factors affect the paper\u2019s influence. Such causality is assumed based on the logic that findings described in the paper can be reused and cited\u2014only if the paper will be discovered by the readers first. To model paper\u2019s influence potential mathematically, we defined Paper\u2019s Influence as a function of a variable we called Discoverability.\n\ud835\udc43\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\u2032\ud835\udc60 \ud835\udc3c\ud835\udc5b\ud835\udc53\ud835\udc59\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52 = \ud835\udc53(\ud835\udc43\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\u2032\ud835\udc60 \ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66) (1)\nThe paper\u2019s potential for influence greatly depends on how accurately search engines and aggregators solve the IR task. For further explanation, we will continue with our query scenario from above. A reader is discovering the paper about animal \u2018cat\u2019 after querying string \ud835\udc44: \u2018genes of cat\u2019, including term \ud835\udc61:; \u2018cat\u2019 while meaning biomedical concept \ud835\udc50: \u2018Felis catus\u2019, corresponding to object \u20189685\u2019 in the ontology [14]. Concept \ud835\udc50: then can be referenced with any term (spelling variant) \ud835\udc61:; of the set \ud835\udc47:\n\ud835\udc47: = {\ud835\udc61:>, \ud835\udc61:@, . . . , \ud835\udc61:B} \u2208 \ud835\udc50: (2)\nIf we assume that a reader will read the paper if the search engine returned it in response to the query \ud835\udc44:, then \u201cdiscoverability\u201d is a synonym of \u201cretrieval\u201d. We can then apply two major IR metrics, recall and precision:\n\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 = \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51\n\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc4e\ud835\udc59\ud835\udc59 \ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc61\u210e\ud835\udc52 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc5d\ud835\udc61 =\n= \ud835\udc43(\ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \u2287 \ud835\udc47: | \ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \u2287 \ud835\udc50:) (3)\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 =\n= \ud835\udc43(\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc5b\ud835\udc54 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \u2287 \ud835\udc50: | \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \u2287 \ud835\udc47:) (4)\nAs long as such cases could be found across biomedical terminology, when concept can have several synonyms (variable terms) or single term can refer to several concepts (ambiguous terms), probabilistic precision and recall can be calculated based\non the numbers of possible outcomes when querying \ud835\udc44: \u2287 \ud835\udc61:;. For example, texts \u201cTNF alpha\u201d, \u201cTNFa\u201d and \u201cTNF \u03b1\u201d are variants of the object Uniprot [P01375]. This means that if search engine was queried with \u201cTNF alpha\u201d, an ideal result would return all documents that contain all three variants. Still, due to existence of the several variants, the there is a probability \u2265 0 that some of them will not be considered.\nWe can estimate chances of such event using a basic definition of the probability as the ratio of the number of favorable outcomes to the total number of possible outcomes. Term\u2019s ambiguity and variability define those numbers of possible outcomes. Finally, when we know precision and recall of the paper\u2019s retrieving while searching for the concept \ud835\udc50:, we can answer specific questions about discoverability of the paper in some kind of progression order.\nQuestion 1. How many papers out of all existing literature about concept \ud835\udc50: can be retrieved, when there is set \ud835\udc47:, all terms of which refer to this concept \ud835\udc50: ?\n\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59(\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66)\n= \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc61:> + . . . + \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc61:B\n\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50: =\n= \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc47: \ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udef4 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc50:\n(5)\nQuestion 2. How many papers out of retrieved and containing terms from \ud835\udc47: mention concept \ud835\udc50:specifically? As long as only recorded synonyms are proved to exist, we can assume that all synonyms from the ontology and generated variants constitute a full dictionary of the concept, and\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66)\n= \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50:\n\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc61:> + . . . + \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc61:B =\n= \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50:\n\ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc47: ,\n\u2200\ud835\udc61:;, \ud835\udc47: \u2208 \ud835\udc50: ,\n\u21d2 \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc50: = \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc47: \u21d2\n\u21d2 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b(\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66) = 1\n(6)\nThis means that precision for the specific concept does not depend on the number of variants, as long as we assume that all variants are describing the same concept in the event.\nQuestion 3. There is term \ud835\udc61:;which refers to the concept \ud835\udc50: or another concept \ud835\udc50R. How many papers out of retrieved and containing term \ud835\udc61:;, are talking about the concept \ud835\udc50:exactly?\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66 =\n= \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50:\n\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50: + . . . + \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53\ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50R =\n(7)\n= \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50:\n\ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc50: \u2229. . .\u2229 \ud835\udc50R = {\ud835\udc61:;}\nQuestion 4. Did we receive all papers containing term \ud835\udc61:;? (Answer: Yes, obviously. Continuing to ask this question is important for keeping track of the general recall and precision derivation)\n\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59(\ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66) = \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc61:;\n\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc52\ud835\udc65\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc61:; = 1 (8)\nQuestion 5. What is the overall probability of retrieving a relevant paper for the concept \ud835\udc50: that has many variants {\ud835\udc61:>, \ud835\udc61:@, . . . , \ud835\udc61:B} and some of them \ud835\udc50: \u2229. . .\u2229 \ud835\udc50R = {\ud835\udc61:;} are also found in the other concepts?\nThis means a probability of two independent events: A = the concept has spelling variants, and B = those variants can be found in several concepts. Therefore P (A and B) will be multiples of the probabilities above:\n\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66 = \ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 \u2217 \ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66 =\n= \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc47: \ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \ud835\udef4 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc50: \u2217 1 (9)\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66 =\n= \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 \u2217 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66 =\n= \ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc61 \ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc5c\ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc4e\ud835\udc4f\ud835\udc5c\ud835\udc62\ud835\udc61 \ud835\udc50:\n\ud835\udef4 \ud835\udc5f\ud835\udc52\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc51 \ud835\udc5d\ud835\udc4e\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc60 \ud835\udc50: \u2229. . .\u2229 \ud835\udc50R = {\ud835\udc61:;} \u2217 1\n(10)\nIf operating only with the number of variants per concept, then prior probability of variants occurrence can be approximated as uniformly distributed, as long as actual frequency of terms occurrence in the papers will be retrieved in the next steps. This means that, in the first approximation, occurrence = {True, False} of the term can be sufficient variable to estimate the minimum expected probabilities:\n\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5c\ud835\udc5f \ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59(\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66) = 1\n\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc5a\ud835\udc60 \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc60 \ud835\udc5d\ud835\udc52\ud835\udc5f \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc5d\ud835\udc61 (11)\n\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5c\ud835\udc5f \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc4f\ud835\udc56\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 \ud835\udc4e\ud835\udc5b\ud835\udc51 \ud835\udc4e\ud835\udc5a\ud835\udc4f\ud835\udc56\ud835\udc54\ud835\udc62\ud835\udc56\ud835\udc61\ud835\udc66 =\n= 1\n\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f \ud835\udc5c\ud835\udc53 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc5d\ud835\udc61\ud835\udc60 \ud835\udc5d\ud835\udc52\ud835\udc5f \ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 \ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc5a \ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc61\n(12)\nAs long as the Number of the terms per concept \u2265 1 (each object has at least a main name) and Number of concepts per single term variant \u2265 1 (each term is related to at least one object) and they are in the denominator of the retrieving probabilities above\u2014terms\u2019 variability and ambiguity will always reduce (at least, will not increase) recall and precision, respectively, when searching for the paper.\nIn order to estimate influence of the existing terms\u2019 uncertainty on the papers discoverability, we have searched for:\n1. homographs across Uniprot, ICD-10, ChEBI, MeSH, Drugbank, and Gene Ontology databases;\n2. possible spelling variants for the same objects; 3. actually used terms\u2019 variants in the 26782464 Pubmed, 26404 Bioline and 5426 eLife papers. MeSH Categories G\u2013Z were not analysed because they contain generic objects, such as countries\u2019 names, which are out of scope of sci.AI semanticization for now. Our research is ongoing and the latest results can be found on the sci.AI webpage [20].\nWe had not only considered synonyms that exist in the ontologies but also created a rules-based term variant generator (TVG) to cover a case when the same object, Uniprot [P01375], might be written as \u201cTNF alpha\u201d, \u201cTNFa\u201d, or \u201cTNF \u03b1\u201d in a paper. Next generating techniques groups were utilized:\n- orthographic; - abbreviations and acronyms; - inflectional variations; - morphological variations; - structural recombinations [4, 5, 6]. Table 1 shows average number of original terms\u2019 synonyms and how much variants were generated. Then we\u2019ve searched for them in the papers. There is increase of the concept detection of 2.03 - 3 times more when searching for all variants.\nTable 2 shows how much objects has terms with identical spellings, i.e. ambiguous terms. Higher overlap within the same ontology than across other ontologies makes algorithmic recognition even more challenging tasks, because algorithms have to distinguish objects within the same class.\nFig.1 shows overall influence of the variability and ambiguity of the terminology\non paper\u2019s discoverability.\na) Uniprot\nb) ChEBI\nc) GeneOntology\nWhen searching by original synonyms only, average likelihood of finding papers is lower than searching by all possible variants. Retrieving higher amount of the papers can be done at the cost of their relevance. Increasing amount of variants leads to the drop of the probabilistic precision. Relevance can be guaranteed only in case of labeling terms and searching by exact ID instead of a string. As long as current literature is not labeled, exact recall and precision can\u2019t be calculated for. We used\nrelative changes instead, to visualize scale of the issue across most of the available literature.\nFig.2 shows the distribution of the variability across ontologies.\na) Uniprot\nb) ChEBI\nc) GeneOntology\nd) DrugBank\ne) ICD10\nf) MeSH\nFigure 2. Number of objects that have specific amounts of original synonyms, generated variants and variants found in the papers.\nThere are original synonyms in the ontologies, generated variants and how much of them were found in the papers. It shows that there are significant chances to found term\u2019s spelling that was never mentioned in the ontology. Thus, it reduces recall of the relevant papers.\nWe were going to use the MSH WSD Data Set [15] initially for the ambiguity testing purposes, but it turned out to contain generic words only. So, we performed a generic wide search across all ontologies and variants to obtain low-level detalization.\nThere is also a case of \u201cartificial\u201d ambiguity in ontologies. It is caused by intersection of alternative names and terms\u2019 descriptions in attempt of extending variants to increase recall. \u201cCarbon monoxide\u201d example is provided in the Table 3.\nDOC, CHS\nUniprot Cheilanthifoline synthase (C7195_ESCCA)\nCH, CHS, cytochrome P450 719A5, cheilanthifoline synthase\nCanavanine hydrolase (CANHY_HELVI)\nCH, canavanine hydrolase\nChEBI methanylylidene group (CHEBI:29432)\nCH, methanylylidene group\nmethylidyne group (CHEBI:29429)\nCH, methylidyne group\nmethanetriyl group (CHEBI:29433)\nCH, methanetriyl group\nDrugbank N-Cyclohexyltaurine (DB03309) CHES, n-cyclohexyltaurine, CH\nIMP MeSH = ChEBI Inosine Monophosphate (D007291)\nribosylhypoxanthine monophosphate, inosinic acid, IMP, inosinate, sodium, sodium inosinate, inosine monophosphate, acids, inosinic, monophosphate, ribosylhypoxanthine, inosinic acids, monophosphate, inosine, acid, inosinic\nDrugBank Imipenem (DB01598)\nimipemide, imipenem anhydrou, imipenem anhydrous, n-formimidoylthienamycin, imipenem, IMP, imipenem and cilastatin for injection, USP, ran-imipenem-cilastatin, imipenem, nformimidoyl thienamycin, imipenem and cilastatin, imipenemum, imipenem and cilastatin for injection, -USP, primaxin 250, imipenem and cilastatin for injection USP, (5R,6S)-6-((R)-1Hydroxyethyl)-3-(2-(iminomethylamino) ethylthio)-7-oxo-1-azabicyclo(3.2.0) hept-2-ene-2carbonsaeure, primaxin IV 500, primaxin 500, primaxin IV 250/250 add-vantage vial, imipenem and cilastatin for injection, usp, imipenem and cilastatin for injection,-usp, (5R,6S)-3-(2formimidoylamino-ethylsulfanyl)-6-((R)-1hydroxy-ethyl)-7-oxo-1-aza-bicyclo[3.2.0] hept-2ene-2-carboxylic acid, imipenem and cilastatin for injection, tienamycin, imipenem and cilastatin for injection usp, imipenem and cilastatin for injection-USP, imipenem and cilastatin for injection-usp, primaxin IV, primaxin-iv, nformimidoyl thienamycin, (5R,6S)-3-((2(formimidoylamino) ethyl) thio)-6-((R)-1hydroxyethyl)-7-oxo-1-azabicyclo(3.2.0) hept-2ene-2-carboxylic acid\nGeneOntology. The same obsolete mitochondrial inner\nIMP, obsolete mitochondrial inner membrane peptidase activity, mitochondrial inner membrane\nmembrane peptidase activity (GO:0004244) peptidase activity\nmitochondrial inner membrane peptidase complex (GO:0042720) IMP, mitochondrial inner membrane peptidase complex\nChEBI IMP (CHEBI:17202) IMP, C10H13N4O8P\nUniprot. The same for various organisms\nInositol monophosphatase (IMPA1_DICDI)\nIMPase, IMP, inositol-1(or 4)-monophosphatase, inositol monophosphatase, d-galactose 1- phosphate phosphatase\nInositol monophosphatase (IMPP_MESCR)\nIMPase, IMP, inositol-1(or 4)-monophosphatase, inositol monophosphatase\nInositol monophosphatase ttx-7 (IMPA1_CAEEL)\nIMPase, IMP, inositol monophosphatase ttx 7, abnormal thermotaxis protein vii, inositol-1(or 4)- monophosphatase, abnormal thermotaxis protein7, inositol monophosphatase ttx vii, inositol monophosphatase ttx-vii, abnormal thermotaxis protein7, d-galactose 1-phosphate phosphatase, abnormal thermotaxis protein 7, inositol monophosphatase ttx-7, abnormal thermotaxis protein-vii, inositol monophosphatase ttx7\nThis leads to the necessity of human validation of the same concepts identification. Such functionality exists in sci.AI to validate several ID\u2019s from the various ontologies for the same term (Fig. 3)."}, {"heading": "4. Decreasing retrieving uncertainty with the precise semantic labeling feature of the sci.AI platform", "text": "Formalization and statistics above show that uncertainty is not an exception but basic feature of the biomedical text mining. This uncertainty might lead to significant deviations when interpreting academic papers with unsupervised methods only. While it might be acceptable for fiction literature mining, because the major task there is context and sentiments analysis that acts as a smoothing function\u2014such uncertainty might contradict goals of mining STEM research communication, where we are looking for the anomalistic or novel discoveries, exact objects interactions, verification of facts, and relations between statements in various texts. This is why accepting uncertainty might have significant negative consequences on the LBD.\nIn order to address this issue, we implemented the sci.AI platform that has supervised labeling functionality on top of the text mining framework. After initial automatic terms recognition, no matter whether precision is 70% or 99%, users can make final verifications to level up recognition precision to 100%. From the perspective of the search and text mining algorithms, this means removing any uncertainty, which, in turn, leads to exact papers extraction in an SQL-like querying manner. Thus, assuming that author will always label terms correctly, maximum precision and recall will be achieved.\nHuman-made corrections will be used as training data for the next processings of a text. Such learning with human feedback provides steady path to gradient growth of text mining quality.\nCurrent version of the sci.AI allows to upload text, then performs Named Entities Recognition (NER) task automatically. Author or annotator can validate labeling results via interface and export final structured text to the XML file (Fig.4).\nDevelopment roadmap includes release of the next features: - Machine Learning based analysis to provide the most likeable variants in the first place. This feature will be based on the (a) logs of the terms validation events and (b) statistical co-occurrences of the terms in all available texts. We expect that it will provide approximately 90% recognition rate, as reported by NN researchers [7, 8]. Introduction of the NN prioritisation is expected to reduce cases of the necessary authors intervention to the reasonable minimum. Internal time tracking done by our team members suggest that 10 pages validation time will be reduced from 1 h to 15 min, approximately. Current statistics will be corrected after actual feature release and making more measures for the various annotators and texts with different density of the terms;\n- Graph based WSD connections between objects and existing metaontologies data;\n- Generating JATS, RDF/XML and RDFa files; - Validation by the readers, not only by the authors; - Concepts interactions labeling, for example, protein-protein. The system can be embedded into the publishing process directly. Both authors and editors can create semanticized versions during submission of even publish new version of the digital paper. Key features of the current production version are as follows:\n1. Automated metatagging of biomedical concepts, (named-entity recognition, with further context-dependant semanticization of terms). Current tags contain links to the related objects in the ontology;\n2. User-friendly web preprint for tags editing and recognition supervising; 3. Web application for easy integration into the existing publishing process.\nsci.AI allows labeling a term with several term-2-ontologyId relationships. For example, \u2018serotonin\u2019 is the object UID=D012701 in MeSH and UID=28790 in ChEBI ontologies simultaneously. Author or reader can suggest additional UIDs too or can correct existing one. This functionality contributes to the global connectivity between terms and might support UMLS Word Sense Disambiguation (WSD) works.\nWe expect another possible positive effect for the future papers too. Authors might come to the same single spelling variant of the concept, like \u201cClNa\u201d only and not \u201csalt\u201d or \u201cNaCl\u201d.\nAs of the end of 2016, sci.AI is in the first phases of its long-term development roadmap from being a semanticization tool to becoming a full-fledged artificial intelligence (AI) tool applied to the life sciences. Wide adoption of this application will extend the publisher's role even further into research results delivery to the intended target audience."}, {"heading": "5. Discussion", "text": "This paper is the first in our series of researchers about precise semantic labelling of life sciences texts. Our goal was to focus on dependency of the paper\u2019s influence on two fundamental factors: ambiguity and variability of terms. In order to avoid excessive complication, we made several assumptions which may bias the results. These simplifications will be addressed in follow-up studies:\n1. Prior precision estimation is calculated with assumption that probability of retrieving a paper with concept \ud835\udc50: when searching for ambiguous term \ud835\udc61: might have uniform distribution. In fact, it has a nonlinear distribution, as shown in statistics in Tables 1, 2 and Fig. 2.\n2. Prior recall estimation is calculated with assumption that probability of retrieving a paper with concept \ud835\udc50: when searching for term \ud835\udc61: with multiple variants might have uniform distribution. In fact, it has a nonlinear distribution as shown in statistics in Table 1, 2 and Fig. 2.\n3. Simplified dependency of recall from ambiguity and precision from variability. 4. Categories of terms variability and implementation of the terms variant generator deserve full comprehensive description in the following research. 6. We intended to show fundamental specifics of biomedical language that makes it is challenging to achieve 100% recognition of terms with unsupervised methods only. Still, there are various NLP approaches including metaontologies like UMLS based disambiguation and statistical methods that significantly improve terms recognition. Those methods are integrated by sci.AI development team and performance of each of them will be evaluated in separate paper.\n7. We assumed that there is the same number of concepts and objects within single ontology.\n8. \u201cHuman factor\u201d was removed from consideration by assuming that author can always correctly label every biomedical concept in own manuscript. Under \u201cprecise labeling\u201d we mean \u201clabeling verified by the actual text\u2019s author\u201d.\n9. There are several studies, where researchers propose models of the future paper\u2019s success, for example, [19]. Future analysis might take into consideration ambiguity and variability as variables in the prediction models.\n10. Part of speech tagging might improve precision of the variants validation. This functionality exists in the sci.AI but was not applied for the statistics calculation.\n11. We assume that all possible spelling variants were generated. Further validation is required."}, {"heading": "Appendix A. Examples of the objects\u2019 synonyms and generated variants", "text": "Variants, that were found in actual papers are marked with * and DOI of one of the retrieved papers. Primary term: Peroxisome proliferator-activated receptor gamma coactivator 1-alpha\nOntology: Uniprot [PRGC1_HUMAN] Synonyms: PGC-1-alpha, PPAR-gamma coactivator 1-alpha, PPARGC-1-alpha, Ligand effect modulator 6 Variants: *PGC-1alpha [10.1186/1750-1326-4-10], PPARGC 1-alpha, *PPAR \u03b3-coactivator 1\u03b1 [10.1038/nutd.2011.3], *peroxisome proliferator-activated receptor-\u03b3-coactivator 1-\u03b1 [10.1016/j.molmet.2015.09.003], PPARGC-i-\u03b1, *PPAR-gamma coactivator 1\u03b1 [10.1186/1476-511X-10246], *PPARGC1-\u03b1 [10.1186/1743-7075-7-88], *PPAR-gamma coactivator 1alpha [10.1155/2008/418765], *peroxisome proliferator-activated receptor \u03b3 coactivator 1-alpha [10.1038/srep18011], *peroxisome proliferator-activated receptor-\u03b3-coactivator 1\u03b1 [10.1210/me.2014-1164], *PPAR \u03b3 coactivator 1 \u03b1 [10.1074/jbc.M115.636878], *peroxisome proliferator-activated receptor \u03b3 coactivator 1-\u03b1 [10.1016/j.molmet.2015.09.003], PGC-i-\u03b1, *PGC-1 \u03b1 [10.1038/ncomms10210], *PPAR-\u03b3-coactivator 1- alpha [10.1371/journal.pone.0055940], *PGC-1\u03b1 [10.7554/eLife.03245], *ligand effect modulator-6, PPARGC i-\u03b1, *PPARGC-1 \u03b1 [10.1074/jbc.M113.512483], PPARGC ialpha, *PPAR-gamma coactivator 1 \u03b1 [10.3892/mmr.2013.1714], *PPAR-\u03b3 coactivator 1 \u03b1 [10.1074/jbc.M115.636878], PPAR \u03b3 coactivator 1alpha, *ligand effect modulator 6, PGC-i alpha, *peroxisome proliferator-activated receptor-\u03b3-coactivator 1 alpha [10.1038/srep18011], *PPAR-\u03b3 coactivator 1\u03b1 [10.1038/nutd.2011.3], *PPARGC 1alpha [10.1038/nm.2049], PPARGC-ialpha, *peroxisome proliferator-activated receptor \u03b3 coactivator 1alpha [10.1111/jnc.12089], PGC i\u03b1, ligand effect modulator6, *peroxisome proliferator-activated receptor gamma coactivator 1 alpha [10.1152/ajpgi.00270.2015], *PPARGC 1 \u03b1 [10.1074/jbc.M113.512483], PGC-i \u03b1, *PPAR-\u03b3-coactivator 1 \u03b1 [10.1074/jbc.M115.636878], *PGC1-\u03b1 [10.7150/ijbs.7972], *PGC 1\u03b1 [10.7554/eLife.03245], PPARGC i\u03b1, *PPARGC1 alpha [10.1016/j.jnutbio.2009.03.012], *PGC1-alpha [10.2527/jas.2009-1896], *PPARGC1-alpha [10.1016/j.jnutbio.2009.03.012], *PGC1\u03b1 [10.1016/j.molmet.2015.08.002], *peroxisome proliferator-activated receptor gamma coactivator 1-alpha [10.1152/ajpgi.00270.2015], *PPAR-\u03b3 coactivator 1 alpha [10.1371/journal.pone.0055940], ligand effect modulator vi, *peroxisome proliferator-activated receptor \u03b3 coactivator 1 alpha [10.1038/srep18011], *PPARGC 1\u03b1 [10.1371/journal.pgen.1005062], PPARGC-i \u03b1, PPARGC i-alpha, *peroxisome proliferatoractivated receptor gamma coactivator 1alpha [10.1007/s00125-006-0268-6], *peroxisome proliferatoractivated receptor gamma coactivator 1\u03b1 [10.1038/ncomms3906], *PPARGC-1-\u03b1 [10.1074/jbc.M113.512483], *PPAR-\u03b3 coactivator 1-\u03b1 [10.1074/jbc.M115.636878], PPARGC-i-alpha, *peroxisome proliferator-activated receptor \u03b3 coactivator 1 \u03b1 [10.1016/j.molmet.2015.09.003], PPAR-\u03b3 coactivator 1alpha, *PPAR \u03b3-coactivator 1-\u03b1 [10.1074/jbc.M115.636878], *PGC 1-alpha [10.1038/sj.ijo.0803567], *PGC-1-\u03b1 [10.1038/ncomms10210], *PPAR \u03b3-coactivator 1-alpha [10.1371/journal.pone.0055940], PPARGC-i\u03b1, *PGC1 alpha [10.2527/jas.2009-1896], *peroxisome proliferator-activated receptor \u03b3-coactivator 1-\u03b1 [10.1016/j.molmet.2015.09.003], *PPARGC 1-\u03b1 [10.1074/jbc.M113.512483], *PGC 1 \u03b1 [10.1038/ncomms10210], PGC-i-alpha, PPARGC i \u03b1, *PPARGC-1\u03b1 [10.1371/journal.pgen.1005062], *peroxisome proliferator-activated receptor \u03b3 coactivator 1\u03b1 [10.1210/me.2014-1164], *PPAR-gamma coactivator 1 alpha [10.1152/japplphysiol.00780.2009], *PPAR-\u03b3coactivator 1\u03b1 [10.1038/nutd.2011.3], *PGC 1alpha [10.1186/1750-1326-4-10], *PGC-1 alpha [10.1038/sj.ijo.0803567], *PPAR-\u03b3 coactivator 1-alpha [10.1371/journal.pone.0055940], PPARGC-1-alpha, *peroxisome proliferator-activated receptor-\u03b3-coactivator 1alpha [10.1111/jnc.12089], PGC i \u03b1, PPARGC i alpha, *PPARGC1 \u03b1 [10.1186/1743-7075-7-88], *PPAR \u03b3 coactivator 1\u03b1 [10.1038/nutd.2011.3], PGC-i\u03b1, PPARGC 1 alpha, *PPAR-\u03b3-coactivator 1-\u03b1 [10.1074/jbc.M115.636878], *PGC1 \u03b1 [10.7150/ijbs.7972], *peroxisome proliferator-activated receptor gamma coactivator 1 \u03b1 [10.3168/jds.2015-9847], *PPARGC1\u03b1 [10.7554/eLife.18206], *PPAR \u03b3 coactivator 1 alpha [10.1371/journal.pone.0055940], *PGC 1 alpha [10.1038/sj.ijo.0803567], PGC i-\u03b1, PGC i-alpha, *peroxisome proliferator-activated receptor \u03b3-coactivator 1 \u03b1 [10.1016/j.molmet.2015.09.003], PPAR-\u03b3-coactivator 1alpha, *peroxisome proliferator-activated receptor\u03b3-coactivator 1 \u03b1 [10.1016/j.molmet.2015.09.003], *PPAR \u03b3 coactivator 1-alpha [10.1371/journal.pone.0055940], *PPARGC1alpha [10.1186/s13395-016-0083-9], ligand effect modulatorvi, *PGC-1-alpha [10.1038/sj.ijo.0803567], *peroxisome proliferator-activated receptor \u03b3-coactivator 1\u03b1 [10.1210/me.2014-1164], PGC i alpha, *PPAR-gamma coactivator 1-\u03b1 [10.3892/mmr.2013.1714], PPAR \u03b3coactivator 1alpha, PGC ialpha, PPARGC-i alpha, *PPAR \u03b3 coactivator 1-\u03b1 [10.1074/jbc.M115.636878], *PGC 1-\u03b1 [10.1038/ncomms10210], *peroxisome proliferator-activated receptor gamma coactivator 1-\u03b1 [10.3168/jds.2015-9847], *PPAR \u03b3-coactivator 1 alpha [10.1371/journal.pone.0055940], PPARGC-1 alpha, *PPAR \u03b3-coactivator 1 \u03b1 [10.1074/jbc.M115.636878], *PPAR-gamma coactivator 1-alpha [10.1152/japplphysiol.00780.2009], *peroxisome proliferator-activated receptor \u03b3-coactivator 1alpha [10.1111/jnc.12089], *PGC1alpha [10.1677/jme.1.01499], *peroxisome proliferator-activated receptor \u03b3coactivator 1-alpha [10.1038/srep18011], PGC-ialpha, *PPAR-\u03b3-coactivator 1 alpha [10.1371/journal.pone.0055940], *peroxisome proliferator-activated receptor \u03b3-coactivator 1 alpha [10.1038/srep18011], *PPARGC-1alpha [10.1038/nm.2049], *peroxisome proliferator-activated receptor-\u03b3coactivator 1-alpha [10.1038/srep18011]\nPrimary term: Interleukin-1 receptor type 2 Ontology: Uniprot [IL1R2_HUMAN] Synonyms: IL-1R-2, IL-1RT-2, IL-1RT2,CD121 antigen-like family member B, CDw121b, IL-1 type II receptor, Interleukin-1 receptor beta, Interleukin-1 receptor type II Variants: interleukin1 receptor type 2, *interleukin-i receptor type-ii [10.1021/ac800928z], *interleukin 1 receptor type-ii [10.1021/ja043466g], *interleukin i-receptor type II [10.1021/ac800928z], interleukin1receptor type ii, *interleukin-1 receptor type-ii [10.1021/ja043466g], *interleukin-i-receptor type II [10.1021/ac800928z], interleukin-i receptor type-2, interleukin1 receptor type2, interleukin1 receptor type-ii, interleukin-1-receptor type2, *interleukin-1 receptor type ii [10.1021/ja043466g], *interleukin i-receptor type ii [10.1021/ac800928z], interleukin1 receptor type-2, *interleukin i receptor type-ii [10.1021/ac800928z], interleukin1-receptor type II, interleukin-i receptor type 2, *interleukin-i-receptor type-ii [10.1021/ac800928z], interleukin i receptor type 2, *interleukin 1-receptor type ii [10.1021/ja043466g], *interleukin-1-receptor type-ii [10.1021/ja043466g], interleukin1-receptor type-ii, *interleukin 1-receptor type II [10.1021/ja043466g], interleukin1 receptor type ii, interleukin1-receptor type 2, interleukin-i-receptor type2, *interleukin-1 receptor type 2 [10.1038/mi.2015.108], interleukin1 receptor type II, *interleukin i receptor type II [10.1021/ac800928z], interleukin-i-receptor type-2, *interleukin i receptor type ii [10.1021/ac800928z], interleukin1receptor type-ii, interleukin 1-receptor type2, interleukin-i-receptor type 2, *interleukin-1-receptor type II [10.1021/ja043466g], interleukin-1 receptor type2, interleukin1receptor type2, interleukin 1 receptor type2, interleukin1receptor type 2, *interleukin-1-receptor type ii [10.1021/ja043466g], *interleukin 1 receptor type ii [10.1021/ja043466g], *interleukin 1 receptor type-2 [10.1038/mi.2015.108], interleukin i receptor type-2, *interleukin-i-receptor type ii [10.1021/ac800928z], *interleukin-1-receptor type-2 [10.1038/mi.2015.108], *interleukin-1 receptor type-2 [10.1038/mi.2015.108], interleukin1-receptor type2, interleukin i-receptor type 2, *interleukin 1-receptor type 2 [10.1038/mi.2015.108], interleukin1-receptor type-2, interleukin1receptor type ii, interleukin i receptor type2, interleukin i-receptor type-2, interleukin1receptor type2, *interleukin 1 receptor type II [10.1021/ja043466g], *interleukin 1 receptor type 2 [10.1038/mi.2015.108], *interleukin-1-receptor type 2 [10.1038/mi.2015.108], interleukin1receptor type II, *interleukin 1-receptor type-2 [10.1038/mi.2015.108], *interleukin-i receptor type ii [10.1021/ac800928z], *interleukin-i receptor type II [10.1021/ac800928z], *interleukin i-receptor type-ii [10.1021/ac800928z], *interleukin-1 receptor type II [10.1021/ja043466g], interleukin i-receptor type2, *interleukin 1-receptor type-ii [10.1021/ja043466g], interleukin-i receptor type2, interleukin 1-receptor\u03b2, CD121 antigen-like family member B, interleukin i-receptor-beta, interleukin1 receptor type-II, *interleukin-i receptor type-II [10.1021/ac800928z], interleukin-i-receptor beta, interleukin i receptor-beta, interleukin 1-receptor-\u03b2, interleukin i-receptor\u03b2, interleukin1receptor-\u03b2, interleukin1receptor type-II, *interleukin-1-receptor-beta [10.1186/1471-2164-11-545], interleukin i receptor \u03b2, IL1type II receptor, interleukin-1 receptorbeta, interleukin-1 receptor-\u03b2, *IL-1RT2 [10.3389/fnint.2013.00061], interleukin 1 receptor-\u03b2, *interleukin 1-receptor-beta [10.1186/1471-2164-11-545], interleukin1-receptor beta, interleukin 1-receptorbeta, *IL 1-type II receptor [10.1084/jem.20020906], *IL-1R2 [10.1038/mi.2015.108], interleukin1receptor-beta, *IL-1R-2, interleukin-1-receptor \u03b2, interleukin-i-receptorbeta, *IL-1RT ii, interleukin i receptor beta, *IL-1-type II receptor [10.1084/jem.20020906], *interleukin i receptor type-II [10.1021/ac800928z], interleukin 1-receptor \u03b2, *interleukin-1 receptor beta [10.1186/1471- 2164-11-545], *interleukin 1-receptor beta [10.1186/1471-2164-11-545], interleukin-1-receptor\u03b2, IL i-type II receptor, interleukin 1 receptor\u03b2, *interleukin-i-receptor type-II [10.1021/ac800928z], interleukin i-receptor beta, interleukin1-receptor \u03b2, interleukin1 receptor \u03b2, *IL1-type II receptor [10.3389/fcell.2016.00072], interleukin-i-receptor\u03b2, interleukin-i receptor-\u03b2, *IL1 type II receptor [10.3389/fcell.2016.00072], *IL-1RTii, interleukin i-receptor \u03b2, interleukin1-receptor-beta, interleukin-i-receptor-\u03b2, interleukin-1 receptor \u03b2, interleukin-1 receptor\u03b2, interleukin1-receptor type-II, interleukin-1-receptor-\u03b2, *CDw121b, interleukin 1 receptorbeta, interleukin1 receptorbeta, interleukin-i receptor beta, interleukin1 receptor\u03b2, interleukin-1receptorbeta, interleukin1-receptor-\u03b2, interleukin i receptorbeta, CD121 antigen-like family member-b, *IL1R-ii [10.1177/039463200601900204], *IL-1R 2, interleukin1 receptor-\u03b2, interleukin i receptor-\u03b2, IL-1RT-2, *interleukin i-receptor type-II [10.1021/ac800928z], IL-i type II receptor, interleukin1-receptor\u03b2, interleukini receptorbeta, interleukin-i-receptorbeta, *interleukin 1 receptor beta [10.1186/1471-2164-11-545], interleukin i-receptorbeta, interleukin 1 receptor \u03b2, interleukin1 receptor beta, *IL 1 type II receptor [10.1084/jem.20020906], *interleukin 1 receptor type-II [10.1021/ja043466g], *interleukin-1 receptor type-II [10.1021/ja043466g], IL-i-type II receptor, *IL-1R ii [10.1177/039463200601900204], interleukin i receptor\u03b2, interleukin1receptor\u03b2, *interleukin-1-receptor type-II [10.1021/ja043466g], *IL-1 type II receptor [10.1084/jem.20020906], interleukin i-receptor-\u03b2, IL-1RT 2, interleukin-i-receptor \u03b2, *interleukin 1 receptorbeta [10.1186/1471-2164-11-545], interleukin1 receptor-beta, interleukin-i receptor \u03b2, interleukin1receptor \u03b2, IL i type II receptor, interleukin1receptorbeta, *interleukin 1-receptor type-II [10.1021/ja043466g], *interleukin-1 receptor-beta [10.1186/1471-2164-11-545], interleukin1receptor beta, interleukin-i receptorbeta, interleukin-i receptor\u03b2, interleukin1-receptorbeta, *interleukin-1-receptor beta [10.1186/1471-2164-11- 545]"}], "references": [{"title": "The effect of word sense disambiguation accuracy on literature based discovery", "author": ["J. Preiss", "M. Stevenson"], "venue": "In Proceedings of the ACM Ninth International Workshop on Data and Text Mining in Biomedical Informatics: Medical Informatics and decision making,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Normalizing biomedical terms by minimizing ambiguity and variability", "author": ["Y. Tsuruoka", "J. McNaught", "S. Ananiadou"], "venue": "BMC Bioinformatics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Term Identification in the Biomedical Literature", "author": ["M. Krauthammer", "G. Nenadi\u0107"], "venue": "J. Biomed. Inform., vol. 6, no. 37, pp. 512\u201326", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "An Analysis of Biomedical Tokenization: Problems and Strategies", "author": ["N.P. Cruz D\u00edaz"], "venue": "Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis (Louhi)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program", "author": ["A.R. Aronson"], "venue": "Proceedings. AMIA Symp., pp. 17\u201321", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts", "author": ["S. Zhang", "N. Elhadad"], "venue": "J. Biomed. Inform.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Deep Neural Network Language Models", "author": ["E. Ar\u0131soy", "T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-Gram Model? On the Future of Language Modeling for HLT", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Ontology-based annotations and semantic relations in large-scale (epi)genomics data", "author": ["E. Galeota", "M. Pelizzola"], "venue": "Brief. Bioinform.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Disambiguation of entities in MEDLINE abstracts by combining MeSH terms with knowledge", "author": ["A. Siu", "P. Ernst", "G. Weikum"], "venue": "Proceedings of the 15th Workshop on Biomedical Natural Language Processing", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Boosting Named Entity Recognition with Neural Character Embeddings", "author": ["C.N. d. Santos", "V. Guimar\u00e3es"], "venue": "[Online]. Available: https://arxiv.org/abs/1505.05008", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian"], "venue": "Proceedings of NAACL-HLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "With such enhancements in place, experimental articles, per se, might become a driving force of the Literature Based Discoveries (LBD) [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Just as in many other areas, deep learning and neural networks (NN) methods are increasingly popular for extracting information from professional texts [7, 8].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "Just as in many other areas, deep learning and neural networks (NN) methods are increasingly popular for extracting information from professional texts [7, 8].", "startOffset": 152, "endOffset": 158}, {"referenceID": 9, "context": "Still, NN demonstrates the highest recognition rates [16, 17] among automatic methods.", "startOffset": 53, "endOffset": 61}, {"referenceID": 10, "context": "Still, NN demonstrates the highest recognition rates [16, 17] among automatic methods.", "startOffset": 53, "endOffset": 61}, {"referenceID": 4, "context": "It is not a simple plug-and-play solution for the publishing industry [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Next generating techniques groups were utilized: - orthographic; - abbreviations and acronyms; - inflectional variations; - morphological variations; - structural recombinations [4, 5, 6].", "startOffset": 178, "endOffset": 187}, {"referenceID": 3, "context": "Next generating techniques groups were utilized: - orthographic; - abbreviations and acronyms; - inflectional variations; - morphological variations; - structural recombinations [4, 5, 6].", "startOffset": 178, "endOffset": 187}, {"referenceID": 4, "context": "Next generating techniques groups were utilized: - orthographic; - abbreviations and acronyms; - inflectional variations; - morphological variations; - structural recombinations [4, 5, 6].", "startOffset": 178, "endOffset": 187}, {"referenceID": 5, "context": "We expect that it will provide approximately 90% recognition rate, as reported by NN researchers [7, 8].", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": "We expect that it will provide approximately 90% recognition rate, as reported by NN researchers [7, 8].", "startOffset": 97, "endOffset": 103}], "year": 2017, "abstractText": "The number of published findings in biomedicine increases continually. At the same time, specifics of the domain\u2019s terminology complicates the task of relevant publications retrieval. In the current research, we investigate influence of terms\u2019 variability and ambiguity on a paper\u2019s likelihood of being retrieved. We obtained statistics that demonstrate significance of the issue and its challenges, followed by presenting the sci.AI platform, which allows precise terms labeling as a resolution.", "creator": "Word"}}}