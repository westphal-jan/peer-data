{"id": "1705.08982", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Modeling the Intensity Function of Point Process Via Recurrent Neural Networks", "abstract": "event sequence, asynchronously generated typically with random timestamp, nowadays is ubiquitous among applications. the typically precise order and distinctly arbitrary frequency timestamp can carry important simple clues about the underlying dynamics, and has essentially lent the event data itself fundamentally different from generating the individual time - series whereby individual series parameter is indexed sequential with repeated fixed entries and distributed equal time interval. usually one expressive mechanical mathematical tool described for modeling event is point process. the spontaneous intensity generating functions method of performing many point processes might involve examining two components : the background timing and recording the effect by the history. due first to controlling its inherent spontaneousness, the background can be individually treated as a time transition series variable while calculating the other need to perfectly handle matching the history events. in processing this paper, we might model the history background by observing a recurrent persistent neural navigation network ( simply rnn ) path with its units aligned with time series input indexes while the virtual history effect is modeled by identifying another rnn frame whose output units are aligned upwards with asynchronous events to capture the long - range chaotic dynamics. the whole model with unique event type and timestamp prediction output capture layers independently can be trained end - to - later end. our analysis approach takes behind an rnn perspective to represent point process, appropriately and well models its total background and history effect. presumably for utility, our method allows in a black - box treatment for simpler modeling, the intensity which is often a better pre - defined parametric form in discrete point time processes. meanwhile reliable end - to - post end neural training opens the venue for rapidly reusing existing rich techniques in managing deep network topology for point process memory modeling. we apply only our model to defeat the direct predictive maintenance balance problem using assuming a typical log value dataset by installing more seldom than 1000 atms controlled from a nationwide global bank or headquartered only in north northern america.", "histories": [["v1", "Wed, 24 May 2017 22:23:14 GMT  (1292kb,D)", "http://arxiv.org/abs/1705.08982v1", "Accepted at Thirty-First AAAI Conference on Artificial Intelligence (AAAI17)"]], "COMMENTS": "Accepted at Thirty-First AAAI Conference on Artificial Intelligence (AAAI17)", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["shuai xiao", "junchi yan", "xiaokang yang", "hongyuan zha", "stephen m chu"], "accepted": true, "id": "1705.08982"}, "pdf": {"name": "1705.08982.pdf", "metadata": {"source": "CRF", "title": "Modeling The Intensity Function Of Point Process Via Recurrent Neural Networks", "authors": ["Shuai Xiao", "Junchi Yan", "Stephen M. Chu", "Xiaokang Yang", "Hongyuan Zha"], "emails": ["benjaminforever@sjtu.edu.cn,", "xkyang@sjtu.edu.cn,", "jcyan@sei.ecnu.edu.cn,", "schu@us.ibm.com,", "zha@cc.gatech.edu"], "sections": [{"heading": "Introduction", "text": "Event sequence is becoming increasingly available in a variety of applications such as e-commerce transactions, social network activities, conflicts, and equipment failures etc. The event data can carry rich information not only about the event attribute (e.g. type, participator) but also the timestamp {zi, ti}Ni=1 indicating when the event occurs. Being treated as a random variable when the event is stochastically generated in an asynchronous manner, the timestamp makes the\nCorrespondence author is Junchi Yan. This research was partially supported by The National Key Research and Development Program of China (2016YFB1001003), NSFC (61602176, 61672231, 61527804, 61521062), STCSM (15JC1401700, 14XD1402100), China Postdoctoral Science Foundation Funded Project (2016M590337), the 111 Program (B07022) and NSF (IIS-1639792, DMS-1620345). Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nevent sequence fundamentally different from the time series (Montgomery, Jennings, and Kulahci 2015) with equal and fixed time interval, whereby the time point only serves as a role for index {yt}Tt=1. A major line of research (Aalen, Borgan, and Gjessing 2008) has been devoted to study event sequence, especially exploring the timestamp information to model the underlying dynamics of the system, whereby point process (Snyder and Miller 2012) has been a powerful and compact framework in this direction.\nRecently there are many machine learning based models for scalable point process modeling. We attribute the progressions in this direction in part to the smart mathematical reformulations and optimization techniques e.g. (Lewis and Mohler 2011; Zhou, Zha, and Song 2013b; Zhou, Zha, and Song 2013a), as well as novel parametric forms for the conditional intensity function (Shen et al. 2014; Ertekin, Rudin, and McCormick 2015; Xu et al. 2016) as carefully designed by researchers\u2019 prior knowledge to capture the character of the dataset in study. However, one major limitation of the parametric forms of point process is due to their specialized and restricted expression capability for arbitrary distributed event data which trends to be oversimplified or even infeasible for capturing the problem complexity in real applications. Moreover, it runs at the risk of model underfitting due to the misjudgement on model choice. Recent works e.g. (Zhou, Zha, and Song 2013b) start to turn to non-parametric form to fit the structure of a point process, but their method is under the Hawkes process formulation,while this formulation runs at the risk of model mischoice.\nIn this paper, we view the conditional intensity of a point process as a nonlinear mapping between the predicted transient occurrence intensity of events with different types, and the model input information of event participators, event profile and the system history. Such a nonlinear mapping is expected to be complex and flexible enough to model various characters of real event data for its application utility.\nIn fact, deep learning models, such as Convolutional Neural Networks (CNNs) (LeCun et al. 1998), Recurrent Neural Networks (RNNs) (Pascanu, Mikolov, and Bengio 2013) have attracted wide attention in recent vision, speech and language communities, and many of them has dominated the competing results on perceptual benchmark tasks e.g. (Russakovsky et al. 2015). In particular, we turn to RNNs as a\nar X\niv :1\n70 5.\n08 98\n2v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\n01 7\nnatural way to encode such nonlinear and dynamic mapping, in an effort for modeling an end-to-end nonlinear intensity mapping without any prior knowledge.\nKey idea and highlights Our model interprets the conditional intensity function of a point process as a nonlinear mapping, which is synergetically established by a composite neural network with two RNNs as its building blocks. As illustrated in Fig.1, time series (top row) and event sequence (bottom row) are distinct to each other in that time series is more suitable to carry the synchronously (i.e. in a fixed pace) and regularly updated or constant profile features, while the event sequence can compactly catch event driven, more abrupt information, which can affect the condition intensity function over longer time period. More specifically, the highlights of this paper are:\n1) We first make an observation that many conditional intensity functions can be viewed as an integration of two effects: i) spontaneous background component inherently affected by the internal (time-varying) attributes of the individual and the event type; ii) effects from history events. Meanwhile, most information in real world can also be covered by continuously updated features like age, temperature, and asynchronous event data such as clinical records, failures. This motivates us to devise a general approach.\n2) Then we use an RNN whose units are aligned with the time points of a time series as its units, and an RNN whose units are aligned with events. The time series RNN can timely track the spontaneous background while the event sequence RNN is used to efficiently capture the long-range dependency over history with arbitrary time intervals. This allows to fit arbitrary dynamics of point process which otherwise will be very difficult or often impossible to be specified by a parameterized model under certain assumptions.\n3) To our best knowledge, this is the first work to fully in-\nterpret and instantiate the conditional intensity function with fused time series and event sequence RNNs. This opens up the room for connecting the neural network techniques to traditional point process that emphasizes more on specific model driven by domain knowledge. More importantly the introduction of a full RNN treatment lessen the efforts for the design of (semi-)parametric point process model and its complex learning algorithms which often call for special tricks that prohibiting the wide use for practitioners. In contrast, neural networks and specifically RNN, is becoming off-the-shelf tools and getting widely used recently.\n4) Our model is simple, general and can be end-to-end trained. We target to a predictive maintenance problem. The data is from a global bank headquartered in North America, consisting decades of thousands of event logs for a large number of Automated Teller Machines (ATMs). The stateof-the-art performance on failure type and timestamp prediction corroborates its suitability to real-world applications."}, {"heading": "Related Work and Motivation", "text": "We view the related concepts and work in this section, which is mainly focused on Recurrent Neural Networks (RNNs) and their applications in time series and sequences data, respectively. Then we give our point of view on existing point process methods and their connection to RNNs. All these observations indeed motivate the work of this paper.\nRecurrent neural network The building blocks of our model are the Recurrent Neural Networks (RNNs) (Elman 1990; Pascanu, Mikolov, and Bengio 2013) and its modern variant Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber 1997; Graves 2013). RNNs are dynamical systems whose next state and output depend on the present network state and input, which are more general models than the feed-forward networks. RNNs have long been explored in perceptual applications for many decades, however it can be very difficult to train RNNs to learn long-range dynamics perhaps in part due to the vanishing and exploding gradients problem. LSTMs provide a solution by incorporating memory units that allow the network to learn when to forget previous hidden states and when to update hidden states given new information. Recently, RNNs and LSTMs have been successfully applied in large-scale vision (Gregor et al. 2015), speech (Graves, rahman Mohamed, and Hinton 2014) and language (Sutskever, Vinyals, and Le. 2014) problems.\nRNNs for series data From application perspective, we view RNNs works by two scenarios as particularly considered in this paper: i) RNNs for synchronized series with evenly spaced interval e.g. time series or indexed sequence with pure order information e.g. language; ii) asynchronous sequence with random timestamp e.g. event data.\ni) Synchronized series: RNNs have been a long time a natural tool for standard time series modeling and prediction (Connor, Martin, and Atlas 1994; Han et al. 2004; Chandra and Zhang 2012; Chen, Chang, and Chang 2013), whereby the indexed series data point is fed as input to an (unfold) RNN. In a broader sense, video frames can also be treated as time series and RNN are widely used in recent visual analytics works (Jain et al. 2016; Tripathi et al. 2016) and so for speech (Graves, rahman Mohamed, and Hinton\n2014). RNNs are also intensively adopted for sequence modeling tasks (Chung et al. 2014; Bengio et al. 2015) when only order information is considered.\nii) Asynchronous event: In contrast, event sequence with timestamp about their occurrence, which are asynchronously and randomly distributed over the continuous time space, is another typical input type for RNNs (Du et al. 2016; Choi et al. 2016; Esteban et al. 2016) and (Che et al. 2016) (despite its title for \u2019time series\u2019). One key differentiation against the first scenario is that the timestamp or time duration between events (together with other features) is taken as input to the RNNs. By doing so, (long-range) event dependency can be effectively encoded.\nPoint process Point process has been a principled framework for modeling event data (Aalen, Borgan, and Gjessing 2008). The dynamics of the point process can be well captured by its conditional intensity function whose definition is briefly reviewed here: for a short time window [t, t+ dt), \u03bb(t) represents the rate for the occurrence of a new event conditioned on the historyHt = {zi, ti|ti < t}:\n\u03bb(t) = lim \u2206t\u21920 E(N(t+ \u2206t)\u2212N(t)|Ht) \u2206t = E(dN(t)|Ht) dt\nwhere E(dN(t)|Ht) is the expectation of the number of events happened in the interval (t, t + dt] given the historical observations Ht. The conditional intensity function has played a central role in point processes and many popular processes vary on how it is parameterized.\n1) Poisson process (Kingman 1992): the homogeneous Poisson process has a very simple form for its intensity function: \u03bb(t) = \u03bb0. Poisson process and its time-varying generalization are both assumed to be independent of the history.\n2) Reinforced poisson processes (Pemantle 2007; Shen et al. 2014): the model captures the \u2018rich-get-richer\u2019 mechanism characterized by a compact intensity function, which is recently used for popularity prediction (Shen et al. 2014).\n3) Hawkes process (Hawkes 1971): Hawkes process has received wide attention recently in social network analysis (Zhou, Zha, and Song 2013a), viral diffusion(Yang and Zha 2013) and criminology (Lewis et al. 2010) etc. It explicitly uses a triggering term to model the excitation effect from history events and is originally motivated to analyze the earthquake and its aftershocks(Ogata 1988).\n4) Reactive point process (Ertekin, Rudin, and McCormick 2015): it can be regarded as a generalization for the Hawkes process by adding a self-inhibiting term to account for the inhibiting effects from history events.\n5) Self-correcting process (Isham and Westcott 1979): its background part increases steadily, while it is decreased by a constant e\u2212\u03b1 < 1 every time a new event appears.\nWe reformulate these intensity functions in their general form in Table 1. It tries to separate the spontaneous background component and history event effect explicitly.\nPredictive maintenance Predictive maintenance (Mobley 2002) is a sound testbed for our model which refers to a practice that involves equipment risk prediction to allow for proactive scheduling of corrective maintenance. Such an early identification of potential concerns helps deploy limited resources more cost effectively, reduce operations costs\nand maximize equipment uptime (Grall et al. 2002). Predictive maintenance is adopted in a wide variety of applications such as fire inspection (Madaio et al. 2016), data center (Sirbu and Babaoglu 2015) and electrical grid (Ertekin, Rudin, and McCormick 2015) management. For its practical importance in different scenarios and relative rich event data for modeling, we target our model to a real-world dataset of more than 1,000 automated teller machines (ATMs) from a global bank headquartered in North America."}, {"heading": "Network Structure and End-to-End Learning", "text": "Taking a sequence {x}Tt=1 as input, the RNN generates the hidden states {h}Tt=1 and outputs a sequence (Elman 1990; Pascanu, Mikolov, and Bengio 2013). Specifically, we implement our RNN with Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997; Graves 2013) for its popularity and well-known capability for efficient longrange dependency learning. In fact other RNN variant e.g. Gated Recurrent Units (GRU) (Chung et al. 2014) can also be alternative choice. We reiterate the formulation of LSTM:\nit = \u03c3(Wixt + Uiht\u22121 + Vict\u22121 + bi), ft = \u03c3(Wfxt + Ufht\u22121 + Vfct\u22121 + bf ), ct = ftct\u22121 + it tanh(Wcxt + Ucht\u22121 + bc), ot = \u03c3(Woxt + Uoht\u22121 + Voct + bo), ht = ot tanh(ct)\nwhere denotes element-wise multiplication and the recurrent activation \u03c3 is the Logistic Sigmod function. The above system can be reduced into an LSTM equation:\n(ht, ct) = LSTM(xt,ht\u22121 + ct\u22121)\nWe consider two types of input: i) continuously and evenly distributed time-series data e.g. temperature; ii) event data whose occurrence time interval is random. The network is comprised by two RNNs using evenly spaced time series {yt}Tt=1 to model the background intensity of events occurrence and event sequence {zi, ti}Ni=1 to capture long-range event dependency. As a result, we have:\n(hyt , c y t ) = LSTMy(yt,h y t\u22121 + c y t\u22121), (1) (hzt , c z t ) = LSTMz(zt,h z t\u22121 + c z t\u22121), (2)\net = tanh(Wf [hyt ,h z t ] + bf ), (3)\nUt = softMax(WUet + bU ), (4) ut = softMax(Wu[et,Ut] + bu) (5) st = Wset + bs, (6)\nwhere U and u denotes the main type and subtype of events respectively. s is the timestamp associated with each event. The total loss is the sum of the time prediction loss and the cross-entropy loss for event type:\nN\u2211 j=1 ( \u2212W jU log(U j t )\u2212 w j u log(u j t)\u2212 log ( f(sjt |h j t\u22121) )) (7)\nwhere N is the number of training samples indexed by j, and sjt is the timestamp for the coming event, while h j t\u22121 is the history information. The underlying rationale for the third term is that we not only encourage correct classification of the coming event type, but also reinforce the corresponding timestamp of the event shall be close to the ground truth. We adopt a Gaussian penalty function with a fixed \u03c32 = 10:\nf(sjt |h j t\u22121) = 1\u221a 2\u03c0\u03c3 exp\n( \u2212(sjt \u2212 s\u0303 j t ) 2\n2\u03c32 ) The output s\u0303jt from the timestamp prediction layer is fed to the classification loss layer to compute the above penalty given the actual timestamp sjt for sample i.\nFollowing the importance weighting methodology for skewed data of model training (Rosenberg 2012), the weight parameters W,w for both main-type and subtype are set as the inverse of the sample number ratio in that type against the total size of samples, in order to weight more on those classes with fewer training samples. For the loss of independent main-type or subtype prediction as shown in Fig.3(b), we set the weight parameter w and W to zero respectively.\nWe adopt RMSprop gradients (Dauphin et al. 2015) which have been shown to work well on training deep networks to learn these parameters."}, {"heading": "Experiments on Real-world Data", "text": "We use failure prediction for predictive ATMs maintenance as a typical example of event based point process modeling. We have no prior knowledge on the dynamics of the complex system and the task can involve arbitrarily working schedules and heterogeneous mix of conditions. It takes much cost or even impractical to devise specialized models.\nProblem and real data description In maintenance support services, when a device fails, the equipment owner raises a maintenance service ticket and technician will be assigned to repair the failure. In fact, the history log and relevant profile information about the equipment can be indicative signals for the coming failures.\nThe studied dataset is comprised of the event logs involving error reporting and failure tickets, which is originally collected from a large number of ATMs owned by an anonymous global bank headquartered in North America. The bank is also a customer of the technical support service department of a Fortune 500 IT company.\nATM models The training data consists of 1085 ATMs and testing data has 469 ATMs, in total 1557 Wincor ATMs that cover 5 ATM machine models: ProCash 2100 RL (980, 430), 1500 RL (19, 5), 2100 FL (53, 21), 1500 FL (26, 10),\nand 2250XE RL (7, 3). The numbers in the bracket indicate the number of machines for training and testing.\nEvent type There are two main types \u2018ticket\u2019 and \u2018error\u2019 from Sep. 2014 to Mar. 2015. Statistics is presented in Table 2. Moreover \u2018error\u2019 is divided into 6 subtypes regarding in which component the error occurs: 1) printer (PRT), 2) cash dispenser module (CNG), 3) internet data center (IDC), 4) communication part (COMM), 5) printer monitor (LMTP), 6) miscellaneous e.g. hip card module, usb (MISC).\nFeatures The input features for the two RNNs are: 1) Time series RNN: For each sub-window of length 7 days,\nfor the time series RNN, we extract features including: i) the inventory information: ATM models, age, location, etc; ii) Event statistics, including tickets events from maintenance records, and errors from system log. Their occurrence frequencies are used as features. The concatenation of the above two categories of features serves as the features for each sub-window i.e. time series point. 2) Event sequence RNN: event type and the time interval between two events.\nModel setting We use a single layer LSTM of size 32 with Sigmoid gate activations and tanh activation for hidden representation. The embedding layer is fully connected and it uses tanh activation and outputs a 16 dimensional vector. One-hot or embedding can be used for event type representation. For a large number of types, embedding representation is compact and efficient. For time series RNN, we set the length of each sub-window (i.e. the evenly spaced time interval) to be 7 days and the number of sub-window to be 5. In this way, our observation length is 35 days for time series. For event-dependency, the length of event sequence can be arbitrarily long. Here we take it by 7.\nWe also test degraded versions of our model as follows: 1) Time series RNN: the input is event sequence (the right half in the yellow part of Fig.2) is removed. Note this design is in spirit similar to many LSTM models (Jain et al. 2016; Tripathi et al. 2016) used for video analytics, whereby the frame sequence can be treated as time series as the input to LSTMs. 2) Event (sequence) RNN: the RNN whose input is time series (the left half in the yellow part of Fig.2) is removed; 3) Intensity RNN: two RNN are fused as shown in Fig.2. For the above three methods, the output layer is directly the fine-grained subtype of events with no hierarchical structure as shown in the top left part of Fig.2) in dark green. We also term three \u2018hierarchical\u2019 versions whose two hierarchical prediction layers in Fig.2 are used: 4) Time series hRNN, 5) Event (sequence) hRNN, 6) Intensity hRNN.\nIn addition, we compare three major peer methods. For Logistic model, the input are the concatenation of feature vectors for all active time series RNN sub-windows (set to 5 in this paper). For RMTPP and Hawkes process, we train the model on the event sequences with associated information. In fact, RMTPP will further process the event data into the similar input information to our event RNN.\n1) Logistic model: we use Logistic regression for event timestamp prediction and use another independent Logistic classification model for event type prediction.\n2) Recurrent Marked Temporal Point Processes (RMTPP): (Du et al. 2016) uses neural network to model the event dependency flexibly. The method can only sample transient time series features when an event happens and use partially parametric form for the base intensity.\n3) Hawkes Process: To enable multi-type event prediction, we use a Multi-dimensional Hawkes process. Similar to (Zhou, Zha, and Song 2013a), we also add a sparsity regularization term on the mutual infection matrix but the lowrank assumption is removed as we only have 6 subtypes.\nEvaluation metrics We use several popular prediction metrics for performance evaluation. For the coming event type prediction, we adopt Precision, Recall, F1 Score and Confusion matrix over 2 main types (\u2018error\u2019, \u2018ticket\u2019) as well\nas Confusion matrix over 6 subtypes under \u2018error\u2019. Note all these metrics are computed for each type, and then are averaged over all types. For event time prediction, we use the Mean Absolute Error (MAE) which measures the absolute difference between the predicted time point and the actual one. These settings are similar to (Du et al. 2016).\nTo evaluate the type and timestamp prediction jointly, we devise two more strict metrics. For type prediction, we narrow down to test the samples whose timestamp prediction error MAE< 3 days and we compute the new F1 score+. For timestamp, we recompute the new MAE+ only for the samples whose coming event is correctly predicted.\nPlatform The code is based on Theano running on a Linux server with 32G memory, 2 CPUs with 6 cores for each: Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz. We also use 4 GPU:GeForce GTX TITAN X for acculturation."}, {"heading": "Results and discussion", "text": "All evaluations are performed on the testing dataset distinctive to training set whose statistics are shown in Table 2.\nAveraged performance Table 3 shows the averaged performance among various types of events. As shown in Fig.3, we test two architectures of the event type prediction layer, i.e. hierarchical predictor (Fig.3(a)) and flat independent predictors (Fig.3(b)). The main type includes \u2018ticket\u2019 and \u2018error\u2019 and the subtype include \u2018ticket\u2019 and the other six subtypes under \u2018error\u2019 as we describe earlier in the paper.\nConfusion matrix The confusion matrix for the six subtypes under \u2018error\u2019 event, as well as for the two main types \u2018ticket\u2019 and \u2018error\u2019 are shown in Fig.4 by various methods.\nWe make observations and analysis based on the results: 1) As shown in 3, for main-type, the flat architecture that directly predicts the main types outperforms the hierarchical one in different settings of the input RNN as well as varying evaluation metrics. This can be explained that the loss function focuses on the main-type misclassification only. While for the subtype prediction, the hierarchical layer performs better since it fuses the output from the main-type prediction layer and the embedding layer as shown in Fig.3(a).\n2) No surprisingly, for both event type and timestamp prediction, our main approach, i.e. intensity RNN that fuses two RNNs outperforms its counterparts time series RNN and event sequence RNN by a notable margin. While the event RNN also often performs better than the time series counterpart. This suggests at least in the studied dataset, history event effects are important for the future event occurrence.\n3) Our main method intensity RNN is almost always superior against other methods except for the main-type pre-\ndiction task, whereby the Logistic classification model performs better. However for more challenging tasks i.e. subtype prediction and event timestamp prediction, our method significantly outperforms especially for subtype prediction task. Interestingly, all point process based models obtain better results on this task which suggests the point process models are more promising compared with classical classification models. Indeed, our methodology provides an endto-end learning mechanism without any pre-assumption in modeling point process. All these empirical results on realworld tasks suggest the efficacy of our approach."}, {"heading": "Conclusion", "text": "We use Fig.5 to conclude and further position our model in the development of (implicit and explicite) modeling the intensity function of point process. In fact, Hawkes process uses a full explicit parametric model and RMTPP misses the dense time series features to model time-varying base intensity and assumes a partially parametric form for it. We make a further step by a full implicit mapping model. Our model\n(see Fig.2) is simple, general and can be learned end-to-end with standard backpropagation and opens up new possibilities for borrowing the advances in neural network learning to the area of point process modeling and applications. The representative study in this paper has clearly suggests its high potential to real-world problems, even we have no domain knowledge on the problem at hand. This is in contrast to existing point process models where an assumption about the dynamics is often need to be specified beforehand."}], "references": [{"title": "Survival and event history analysis: a process point of view", "author": ["Borgan Aalen", "O. Gjessing 2008] Aalen", "O. Borgan", "H. Gjessing"], "venue": null, "citeRegEx": "Aalen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aalen et al\\.", "year": 2008}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bengio", "year": 2015}, {"title": "and Zhang", "author": ["R. Chandra"], "venue": "M.", "citeRegEx": "Chandra and Zhang 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Che"], "venue": null, "citeRegEx": "Che,? \\Q2016\\E", "shortCiteRegEx": "Che", "year": 2016}, {"title": "F", "author": ["P.A. Chen", "L.C. Chang", "Chang"], "venue": "J.", "citeRegEx": "Chen. Chang. and Chang 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "W", "author": ["E. Choi", "M.T. Bahadori", "A. Schuetz", "Stewart"], "venue": "F.; and Sun, J.", "citeRegEx": "Choi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung"], "venue": null, "citeRegEx": "Chung,? \\Q2014\\E", "shortCiteRegEx": "Chung", "year": 2014}, {"title": "L", "author": ["J.T. Connor", "R.D. Martin", "Atlas"], "venue": "E.", "citeRegEx": "Connor. Martin. and Atlas 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Y", "author": ["Dauphin"], "venue": "N.; de Vries, H.; Chung, J.; and Bengio, Y.", "citeRegEx": "Dauphin et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent marked temporal point processes: Embedding event history to vectore", "author": ["Du"], "venue": null, "citeRegEx": "Du,? \\Q2016\\E", "shortCiteRegEx": "Du", "year": 2016}, {"title": "J", "author": ["Elman"], "venue": "L.", "citeRegEx": "Elman 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "T", "author": ["S. Ertekin", "C. Rudin", "McCormick"], "venue": "H.", "citeRegEx": "Ertekin. Rudin. and McCormick 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting clinical events by combining static and dynamic information using recurrent neural networks", "author": ["Esteban"], "venue": null, "citeRegEx": "Esteban,? \\Q2016\\E", "shortCiteRegEx": "Esteban", "year": 2016}, {"title": "Continuous-time predictive-maintenance scheduling for a deteriorating system", "author": ["Grall"], "venue": "IEEE transactions on Reliability", "citeRegEx": "Grall,? \\Q2002\\E", "shortCiteRegEx": "Grall", "year": 2002}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["rahman Mohamed Graves", "A. Hinton 2014] Graves", "A. rahman Mohamed", "G. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor"], "venue": null, "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}, {"title": "Prediction of chaotic time series based on the recurrent predictor neural network", "author": ["Han"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "Han,? \\Q2004\\E", "shortCiteRegEx": "Han", "year": 2004}, {"title": "A", "author": ["Hawkes"], "venue": "G.", "citeRegEx": "Hawkes 1971", "shortCiteRegEx": null, "year": 1971}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Westcott", "author": ["V. Isham"], "venue": "M.", "citeRegEx": "Isham and Westcott 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "H", "author": ["A. Jain", "A. Singh", "Koppula"], "venue": "S.; Soh, S.; and Saxena, A.", "citeRegEx": "Jain et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "J", "author": ["Kingman"], "venue": "F. C.", "citeRegEx": "Kingman 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324", "citeRegEx": "LeCun,? \\Q1998\\E", "shortCiteRegEx": "LeCun", "year": 1998}, {"title": "and Mohler", "author": ["E. Lewis"], "venue": "E.", "citeRegEx": "Lewis and Mohler 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["E. Lewis", "G. Mohler", "Brantingham"], "venue": "J.; and Bertozzi, A.", "citeRegEx": "Lewis et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Firebird: Predicting fire risk and prioritizing fire inspections in atlanta", "author": ["Madaio"], "venue": null, "citeRegEx": "Madaio,? \\Q2016\\E", "shortCiteRegEx": "Madaio", "year": 2016}, {"title": "R", "author": ["Mobley"], "venue": "K.", "citeRegEx": "Mobley 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "C", "author": ["Montgomery, D.C.", "Jennings"], "venue": "L.; and Kulahci, M.", "citeRegEx": "Montgomery. Jennings. and Kulahci 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Mikolov Pascanu", "R. Bengio 2013] Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "Berg"], "venue": "C.; and Fei-Fei, L.", "citeRegEx": "Russakovsky et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling and predicting popularity dynamics via reinforced poisson processes", "author": ["Shen"], "venue": null, "citeRegEx": "Shen,? \\Q2014\\E", "shortCiteRegEx": "Shen", "year": 2014}, {"title": "and Babaoglu", "author": ["A. Sirbu"], "venue": "O.", "citeRegEx": "Sirbu and Babaoglu 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["D.L. Snyder", "Miller"], "venue": "I.", "citeRegEx": "Snyder and Miller 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le."], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Z", "author": ["Tripathi, S.", "Lipton"], "venue": "C.; Belongie, S.; and Nguyen, T.", "citeRegEx": "Tripathi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Patient flow prediction via discriminative learning of mutuallycorrecting processes", "author": ["Xu"], "venue": "IEEE transactions on Knowledge and Data Engineering", "citeRegEx": "Xu,? \\Q2016\\E", "shortCiteRegEx": "Xu", "year": 2016}, {"title": "and Zha", "author": ["Yang", "S.-h."], "venue": "H.", "citeRegEx": "Yang and Zha 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processe", "author": ["Zha Zhou", "K. Song 2013a] Zhou", "H. Zha", "L. Song"], "venue": "In AISTATS", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Learning triggering kernels for multi-dimensional hawkes processes", "author": ["Zha Zhou", "K. Song 2013b] Zhou", "H. Zha", "L. Song"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "Event sequence, asynchronously generated with random timestamp, is ubiquitous among applications. The precise and arbitrary timestamp can carry important clues about the underlying dynamics, and has lent the event data fundamentally different from the time-series whereby series is indexed with fixed and equal time interval. One expressive mathematical tool for modeling event is point process. The intensity functions of many point processes involve two components: the background and the effect by the history. Due to its inherent spontaneousness, the background can be treated as a time series while the other need to handle the history events. In this paper, we model the background by a Recurrent Neural Network (RNN) with its units aligned with time series indexes while the history effect is modeled by another RNN whose units are aligned with asynchronous events to capture the long-range dynamics. The whole model with event type and timestamp prediction output layers can be trained end-to-end. Our approach takes an RNN perspective to point process, and models its background and history effect. For utility, our method allows a black-box treatment for modeling the intensity which is often a pre-defined parametric form in point processes. Meanwhile end-to-end training opens the venue for reusing existing rich techniques in deep network for point process modeling. We apply our model to the predictive maintenance problem using a log dataset by more than 1000 ATMs from a global bank headquartered in North America.", "creator": "LaTeX with hyperref package"}}}