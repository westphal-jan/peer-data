{"id": "1705.09054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment", "abstract": "successfully recognizing textual entailment is a hugely fundamental task in a total variety of of text mining algorithms or natural language reconstruction processing applications. this paper proposes a simple artificial neural model for rte problem. it first matches each inconsistent word in the hypothesis description with its two most - similar sequential word meanings in documenting the given premise, now producing an autonomous augmented representation process of the hypothesis image conditioned properly on displaying the premise as executing a predicted sequence of correct word pairs. the lstm model is ideally then used to formally model this augmented sequence, and afterward the final false output result from the lstm hypothesis is periodically fed into a special softmax layer tree to make the prediction. besides besides the base model, studied in order supposedly to structurally enhance its performance, we also proposed : three techniques : the integration of multiple vector word - structure embedding correlation library, layered bi - way parallel integration, loop and ensemble based on model combination averaging. substantial experimental numerical results on the snli dataset have shown that eliminating the three approximation techniques simultaneously are effective coupled in financially boosting the predicative experimental accuracy and that our overall method outperforms produces several state - of - errors the - state ones.", "histories": [["v1", "Thu, 25 May 2017 05:45:42 GMT  (14kb)", "http://arxiv.org/abs/1705.09054v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhipeng xie", "junfeng hu"], "accepted": false, "id": "1705.09054"}, "pdf": {"name": "1705.09054.pdf", "metadata": {"source": "CRF", "title": "Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment", "authors": ["Zhipeng Xie", "Junfeng Hu"], "emails": ["15210240075}@fudan.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n09 05\n4v 1\n[ cs\n.C L\n] 2\n5 M\nay 2\n01 7\nKeywords: Textual Entailment, Recurrent Neural Networks, LSTM"}, {"heading": "1 Introduction", "text": "In natural language text, there are always different ways to express the same meaning. This surface-level variability of semantic expressions is fundamental in tasks related to natural language processing and text mining. Textual entailment recognition (or RTE in short) is a specific semantic inference approach to model surface-level variability. As formulated by Dagan and Glickman [8], the task of Recognizing Textual Entailment is to decide whether the meaning of a text fragment Y (called the Hypothesis) can be inferred (is inferred) from another text fragment X (called the Premise). Giampiccolo et al. [10] extended the task to include the additional requirement that systems identify when the Hypothesis contradicts the Premise. The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction. Given a pair of premise X and hypothesis Y , the relation between them may be: Entailment (Y can be inferred from X), Contradiction (Y is inferred to contradict X), or Neutral (X and Y are unrelated to each other). Table 1 presents a simple example to illustrate these three relations.\nA lot of research work has been devoted to the RTE problem in the last decade. The mainstream methods for recognizing textual entailment can be roughly divided into two categories:\n\u2013 The first category attempts to provide a sequence of transformations allowing to derive the hypothesis Y from the premise X , by applying one transformation rule at each step. \u2013 The second category simply thinks of RTE problem as a classification problem, where features (manually defined or automatically constructed) are extracted from the premise-hypothesis pairs.\nIn transformation-based RTE methods (also called rule-based methods), the underlying idea is to make use of inference rules (or entailment rules) for making transformation. However, the lack of such knowledge has been a major obstacle to improving the performance on RTE problem. The acquisition of entailment rules can be done either by learning algorithms which extract entailment rules from large text corpora, or by methods which extract rules from manually constructed knowledge resources.\nSome research works have focused on extraction of entailment rules from manually constructed knowledge resources. WordNet [9] is the most prominent resource to extract entailment rules from. The synonymy and hypernymy relations (called substitutable relations) can be exploited to do direct substitution. To make use of the other non-substitutable relations (such as entailment and cause relations), Szpektor and Dagan [27] populated these non-substitutable relations with argument mapping which are extracted various resource, and thus extended WordNet\u2019s inferential relations at the syntactic representation level. FrameNet [2] is another manually constructed lexical knowledge base for entailment rule extraction. Aharon et al. [1] detected the entailment relations implied in FrameNet, and utilized FrameNet\u2019s annotated sentences and relations between frames to extract both the entailment relations and their argument mappings.\nAlthough the entailment rules extracted from manually constructed knowledge resources have achieved sufficiently accuracy, their coverage is usually severely limited. A lot of research work has been devoted to learning entailment rules from a given text corpus. The DIRT algorithm proposed by Lin and Pantel [18] was based on the so-called Extended Distributional Hypothesis which states that\nphrases occurring in similar contexts are similar. An inference rule extracted by DIRT algorithm is actually a paraphrase rule, which is a pair of language patterns that can replace each other in a sentence. In DIRT, the language patterns are chains in dependency trees, with placeholders for nouns at the end of this chain. Different from the Extended Distributional Hypothesis adopted by DIRT, Glickman and Dagan [11] proposed an instance-based approach, which uses linguistic filters to identify paraphrase instances that describe the same fact and then rank the candidate paraphrases based on a probabilistically motivated paraphrase likelihood measure. Sekine [25] extracted the phrase between two named entities as candidate linear pattern, then identified a keyword in each phrase and joined phrases with the same keyword into sets, and finally linked sets which involve the same pairs of individual named entities. The sets or the links can be treated as paraphrases.\nBesides paraphrase rules (which can be thought of as a specific case of entailment rules), a more general notion needed for RTE is that of entailment rules [8]. An entailment rule is a directional relation between two language patterns, where the meaning of one can be entailed or inferred from the meaning of the other, but not vice versa. Pekar [22] proposed a three-step method: it first identifies pairs of discourse-related clauses, and then creates patterns by extracting pairs of verbs along with relevant information as to their syntactic behaviour, and finally scores each verb pair in terms of plausibility of entailment. Recently, Szpektor et al. [28] presented a fully unsupervised learning algorithm for Web-based extraction of entailment rules. The algorithm takes as its input a lexical-syntactic template and searches the Web for syntactic templates that participate in an entailment relation with the input template.\nRecently, with the availability of large high-quality dataset, especially with the Stanford Natural Language Inference (SNLI) corpus published in 2015 by Bowman et al. [4], there comes an upsurge of end-to-end neural models for RTE, where the fundamental problem is how to model a sentence pair (X,Y ). The first and simplest model, proposed by Bowman et al. [4], encodes the premise and the hypothesis with two separate LSTMs, and then feeds the concatenation of their final outputs into a MLP for classification. This model does not take the interaction between the premise and the hypothesis into consideration. Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6]. These models treat sentences as word sequences, but some others adopt more principled choice to work on the tree-structured sentences, by explicitly model the compositionality and the recursive structure of natural language over trees. Such kind of work includes the Stack-augmented Parser-Interpreter Neural Network (SPINN) [5] and Tree-based Convolutional Neural Network (TBCNN) [21]. In the section 2, we shall have a look at all these neural models in more detail.\nIn this paper, we propose a simple neural method, called MaxConsineLSTM,based on max-cosine matching for natural language inference. It first matches each word in the hypothesis (or the premise) with its most-similar word in the premise (or the hypothesis), and obtains a representation of hypothesis (or the\npremise conditioned on the premise (or the hypothesis). Then, LSTM is used to model the enhanced representations of hypothesis and premise into dense vectors. And finally, we concatenate the two dense vectors and feed it into a softmax layer to make the final decision about the relation between them. Experimental results have shown that our method achieves better or comparable performance when compared with state-of-the-art methods."}, {"heading": "2 Related Work", "text": "In this section, we review some neural models that work for recognizing textual entailment.\nThe first and simplest neural model to RTE was proposed by Bowman et al. [4] in 2015, which uses separate LSTMs [15] to encode the premise and the hypothesis as dense fixed-length vectors and then feeds their concatenation into a multi-layer perceptron (MLP) or other classifiers for classification. It learns the sentence representation of premise and hypothesis independently, and does not take their interaction into consideration.\nThis first neural model suffer from the fact that the hypothesis and the premise are modeled independently, and thus the information cannot flow between them. To solve this problem, a sequential LSTM model is proposed in [24]. An LSTM reads the premise, and a second LSTM with different parameters reads a delimiter and the hypothesis, but its memory state is initialized with the final cell state of the previous LSTM. In this way, information from the premise can flow to the encoding of hypothesis.\nTo further facilitate the information flow between the premise and the hypothesis, Rockta\u0308schel et al. [24] applied a neural attention model which can achieve better performance. When the second LSTM processes the hypothesis one word at a time, the first LSTM\u2019s output vectors are attended over, generating attention weights over all output vectors of the premise for every word in the hypothesis. The final sentence-pair representation is obtained from the last attention-weighted representation of the premise and the last output vector of the hypothesis. It outperforms Bowman et al. (2015) in that it checks for entailment or contradiction of individual word- and phrase-pairs.\nWang and Jiang [29] used an LSTM to perform word-by-word matching of the hypothesis with the premise. It is expected that the matching results that are critical for the final prediction will be \u201cremembered\u201d by the LSTM while less important matching results will be \u201cforgotten\u201d.\nThe attentive mechanisms used in [24] and [29] are both between the hypothesis and the premise. It is sometimes helpful to exploit the attentive mechanism within the hypothesis or the premise. the long short-term memory-network (LSTMN) proposed by Cheng et al. [6] induces undirected relations among tokens as an intermediate step of learning representations, which can be thought of as an intra-attention mechanism. It has also been manifested that the intraattention mechanism can lead to representations of higher quality.\nThe algorithms described above all deal with sentences as sequences of word vectors, and learn sequence-based recurrent neural networks to map them to sentence vectors. Another more principled choice is to learn the tree-structured recursive networks. Recursive neural networks explicitly model the compositionality and the recursive structure of natural language over tree. Bowman et al. [5] introduced the Stack-augmented Parser-Interpreter Neural Network (or SPINN in short) to combine parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser.\nMou et al. [21] proposed a tree-based convolutional neural network (TBCNN) to capture sentence-level semantics. TBCNN is more robust than sequential convolution in terms of word order distortion introduced by determinators, modifiers, etc. In TBCNN, a pooling layer aggregates information along tree, serving as a way of semantic compositionality. Finally, two sentences\u2019 information is combined and fed into a softmax layer for output."}, {"heading": "3 Base Method", "text": "Recognizing textual entailment is concerned about the relation between two sequences - the premise X and the hypothesis Y . The commonly used encoderdecoder architecture processes the second sequence conditioned on the first one.\nIn this paper, we establish the connection of the hypothesis to the premise at the word level, where each word in the hypothesis is matched to and paired with its most-similar word in the premise. It leads to a simple base method called MaxCosineLSTM , consisting of three steps as illustrated in Figure 1:\n\u2013 Step 1: (Word Matching) Each word yt in the hypothesis Y is matched to its most-similar word (denoted as \u03b3(yt) in X , where the similarity between two words is measured as the cosine similarity between their embeddings. Such a match strategy can be thought of as a conditional representation of the hypothesis on the given premise; \u2013 Step 2: (Sequence Modeling) For each time step 1 \u2264 t \u2264 m, the concatenation of the word embedding of yi and that of \u03b3(yi) is fed into an LSTM layer, yield a vector representation of the hypothesis conditional on the premise; \u2013 Step 3: (Decision Making) The final output of the LSTM layer is fed into a softmax layer to get the final decision about the relation between X and Y : Entailment, Contradiction, or Neutral."}, {"heading": "3.1 Word Matching with Cosine Similarity", "text": "To judge whether a hypothesis Y can be inferred from a given premise X , it is of importance to get to know whether each word in Y expresses a similar meaning as one word in the premise X . Distributional Hypothesis proposed by Harris [12] has provided a guiding principle, which states that words appearing\nin similar contexts tend to have similar meanings. This principle has led to a variety of distributional semantic models (DSM) that use multidimensional vectors as word-sense representation. Latent semantic analysis [17] is a representative method of this kind, which applies truncated Singular Value Decomposition to a matrix of word-context co-occurrence matrix. Recently, neural network-based methods, such as Skip-Gram [20] and Glove [23], have been proposed to represent words as low-dimensional vectors called word embeddings. Compared with traditional DSM methods, these word embeddings have shown superior performance in similarity measurement between words.\nLet D \u2208 Rd\u00d7|V | be a learned embedding matrix for a finite vocabulary V of |V | words, with d denoting the dimensionality of word embeddings. The i-th column, (D(i) \u2208 Rd, represents the embedding of the i-th word in the vocabulary V . Given two words x and y in the vocabulary, we can measure their semantic similarity as the most commonly-used cosine similarity between their word embeddings x = D(x) and y = D(y):\nsim(x, y) = cosine(x,y) = \u3008x,y\u3009\n\u2016x\u2016 \u00b7 \u2016y\u2016 (1)\nTherefore, for each word yt(1 \u2264 t \u2264 m) in the hypothesis Y = y1y2...ym, we use \u03b3(yt) to denote the word in premise X = x1x2...xn that is of highest semantic similarity with yt:\n\u03b3(yt) = argmax xs sim(xs, yt) (2)\nSuch a mapping \u03b3 can build the connection from the hypothesis to the premise, at the word level. Each sentence pair (X,Y ) can then be represented as a sequence Z = z1z2...zm where zt = (yt, \u03b3(yt)), 1 \u2264 t \u2264 m, denotes the t-th word in hypothesis Y paired with its most-similar word \u03b3(yt) in the premise Y . This pairing process do associate the most relevant words from the hypothesis Y to the premise X . We use zt \u2208 R\n2d to denote the concatenation of word embeddings of yt and \u03b3(yt) for zt = (yt, \u03b3(yt)):\nzt =\n[\nD(yt) D(\u03b3(yt))\n]\n(3)\nThus, we can get a sequence Z = (z1, z2, . . . , zm) of vectors in R 2d, which can be thought of as an augmented representation of the hypothesis with reference to the premise."}, {"heading": "3.2 Sequence Modeling with LSTM", "text": "Next, we would like to transform the sequence Z into a vector, as the representation of the hypothesis conditioned on the premise. Recurrent neural networks are naturally suited for modeling variable-length sequences, which can recursively compose each (2d)-dimensional vector zt with its previous memory. Traditional recurrent neural networks often suffer from the problem of vanishing and exploding gradients [14] [3], making it hard to train models. In this paper, we adopt the Long Short-Term Memory (LSTM) model [15] which partially solves the problem by using gated activation function.\nThe LSTM maintains a memory state c through all the time steps, in order to save the information over long time periods. Concretely, at each time step t, the concatenation of two word embeddings, zt, is fed into the LSTM as the input. The LSTM updates the memory state from the previous ct\u22121 to the current ct, by adding new content that should be memorized and erasing old content that should be forgotten. The LSTM also outputs current content that should be exposed. relying on memory is updated by partially forgetting the previous memory ct\u22121 and adding a new memory content tanh(W\ncH+ bc). The output gates ot modulates the amount of memory content exposure.\nH =\n[\nzt ht\u22121\n]\n(4)\nit = \u03c3(W iH+ bi) (5)\nft = \u03c3(W fH+ bf ) (6)\not = \u03c3(W oH+ bo) (7) ct = ft \u2299 ct\u22121 + it \u2299 tanh(W cH+ bc) (8)\nht = ot \u2299 tanh(ct) (9)\nTo prevent overfitting, dropout is applied to regularize the LSTMs. Dropout has shown a great success when working with feed-forward networks [26]. As indicated in [30], our method drops the input and the output of the LSTM layer, with the same dropout rate."}, {"heading": "3.3 Decision Making with Softmax Layer", "text": "The final output, hm, generated by the LSTM on the enhanced representation Z of the hypothesis Y conditioned on the premise X , is fed into a softmax layer which performs the following two steps.\nAs the first step, hm goes through a linear transformation to get a 3-dimensional vector p:\np = Wshm + b s (10)\nwhere the weight matrix Ws \u2208 R3\u00d7k , and bias vector bs \u2208 R3 are the parameters of the softmax layer.\nThe p = (p1, p2, p3 is then transformed by a nonlinear softmax function, resulting in a probabilistic prediction (t\u03021, t\u03022, t\u03023) over the three possible labels (Entailment=1, Contradiction=2, or Neutral=3):\nt\u0302i = exp pi\n\u22113 j=1 exp pj\n(11)\nDuring the training phase, the cross-entropy error function is used as the cost function. At test time, the label with the highest probability, argmax1\u2264i\u22643 pi, is output as the predicted label."}, {"heading": "4 Improvements over the Base Method", "text": "To obtain better predictive performance, three optional techniques are applied on the base method described above:\n\u2013 Multiple word-embedding libraries, which improves the vector representations of words; \u2013 Biway-LSTM integration, which enhances the representations of relations between text pairs; \u2013 Ensemble based on model averaging, which produces more accurate predictions."}, {"heading": "4.1 Mutliple Embeddings", "text": "Word2vec and Glove are two popular software for learning word embeddings from text corpus. We use Dw2v and Dglove to denote their induced word embedding libraries, respectively. For each word x, we can represent it as a vector D(x) as the concatenation of its embedding from Dw2v and that from Dglove, that is,\nD(x) = [Dw2v(x)Dglove(x)]\n. Or equivalently, a new embedding matrix D \u2208 R(2d)\u00d7|V | is constructed by concatenating Dw2v with Dglove. The semantic similarity between words from the hypothesis and the premise is calculated with regards to this new embedding matrix. The aim of using this technique is to integrated the potentially complementary information provided by different word embedding libraries. As another possible solution, we can also make use of canonical correlation analysis (CCA) to project the two embedding libraries into a common semantic space, and thus induce a new embedding library. However, we do not make any further exploration, because it is out of the scope of this paper."}, {"heading": "4.2 Biway-LSTM Integration", "text": "In RTE problem, the premise should also play an important role as the hypothesis. In the previous two subsections, we have described how to model the hypothesis conditioned on the premise. This idea can also be applied the other way round, i.e. to model the premise conditioned on the hypothesis.\nTo justify this statement, let us consider the following simple example. Let X(1)=\u201cJohn failed to pass the exam.\u201d, X(2)=\u201cJohn succeeded in passing the exam.\u201d, and Y=\u201cJohn passed the exam.\u201d It is clear that X(2) entails Y , but X(1) contradicts Y . However, the enhanced representation of Y conditioned on X(1) is the same as that of Y conditioned on X(2). Therefore, we cannot discriminate these two situations based on only the enhanced representation of Y .\nTo do a remedy, we extend our base model to the biway architecture illustrated in Figure 2. Two LSTMs are used to separately model the enhanced representation of the premise and that of the hypothesis. Their final output vectors are then concatenated and fed into a softmax layer to do the final decision."}, {"heading": "4.3 Ensemble by Model Averaging", "text": "Combining multiple models generally leads to better performance. The component models are expected to be diverse and accurate, in order to produce an ensemble of high quality. In this paper, all the components are homogeneous, that is, all of them are induced by our base method (optionally enhanced with bi-embedding integration and/or biway integration). The diversity of these components comes from random initialization of the model parameters, with different random seeds. The predictions from these component models are averaged to make the final decision."}, {"heading": "5 Experiments", "text": "In the experimental part, we evaluate our method on the Stanford Natural Language Inference (SNLI) dataset [4] which consists of about 570K sentence pairs. After filtering sentence pairs with unknown class labels, we get a train data of 549,367 pairs, a validation data of 9,842 pairs, a test data of 9,824 pairs. This dataset has been commonly used by previous state-of-the-art neural models.\nTo train our model, we use cross-entropy loss J(\u03b8) in Equation (12), the B is the mini-batch size, t(i) is the true label of sample i, {t(i) = j} is 1 if t(i) equals j else 0. We use stochastic mini-batch gradient descent with the ADAM optimizer [16], we set ADAM\u2019s hyperparameters \u03b21 = 0.9 and beta2 = 0.999 and the initial learning rate to 0.001. We use both the pre-trained Glove [23] model glove.840B.300d and Word2vec [20] model GoogleNews-vectorsnegative300 to initialize word embeddings. We don\u2019t tune the word embeddings when train, OOV words\u2019 vectors are set to be the average of their window words\u2019 vectors, follow same setting in the Match-LSTM paper [29]. We fix the length of LSTM hidden states k to 300D, and apply various dropout rate on the input layer. We don\u2019t apply any regularization to the network weights, and use batch\nsize B = 128 when training. We use Lasagne1 to implement these models.\nJ(\u03b8) = \u2212 1\nB\nB \u2211\ni=1\n3 \u2211\nj=1\n1{t(i) = j} log t\u0302j (12)\nWe compare our approach with the following state-of-art methods:\n\u2013 Separate-LSTM: the first neural method proposed in [4], which encodes the premise and the hypothesis with two separate LSTMs independently. \u2013 Sequential-LSTM method: this method [24] makes use of two LSTMs, where an LSTM reads the premise, and a second LSTM initialized with the final cell state of the first LSTM reads a delimiter and the hypothesis. \u2013 Attention-LSTM: the method in [24] that attends over output vectors of the premise only for the final output of the hypothesis. \u2013 Word-by-Word Attention-LSTM: the method in [24] that attends over output vectors of the premise for every word in the hypothesis. \u2013 matchLSTM with word embedding: the method in [29] that performs wordby-word matching of the hypothesis with the premise.\nWe implemented our versions of Attention-LSTM, Word-by-Word AttentionLSTM, and matchLSTM, because the codes of the original papers are not made publicly available.\nResults on the SNLI corpus are summarized in Table 2. Our method MaxCosineLSTM that uses all three improvement techniques has achieved the best performance when compared with these state-of-the-art methods.\nWe also conduct experiments to study the effectiveness of the three techniques in Section 4. With the dropout ratio set as 0.3, 0.4, and 0.5, Table 3 shows the results of using different combinations of the three techniques. The accuracies of all the non-ensemble methods (MaxCosine-LSTM, MaxCosine-LSTM-biEmb, MaxCosine-LSTM-biWay, MaxCosine-LSTM-biEmb-biWay) are reported as the average over 5 runs.\n1 http://lasagne.readthedocs.io/en/latest/\nIt can be easily observed that all the three techniques have their own contributions to the final predictive ability. Discarding any technique would lead to some decrease in the prediction accuracy. It is also evident that the ensemble technique is the most important one, the biway integration goes next, and the bi-embedding integration is relatively less significant.\nIn addition, we can also observe the following two facts:\n\u2013 The effect of ensemble technique on MaxCosine-LSTM-biWay is more significant than its effect on MaxCosine-LSTM or MaxCosine-LSTM-biEmb. \u2013 The effect of ensemble is more significant with smaller dropout ratio.\nThese observations can be explained by the fact that the ensemble technique based on model averaging works better on the method of higher model complexity which usually has higher variance in bias-variance decomposition. Intuitively, the model complexity of MaxCosine-LSTM-biWay is much higher than that of MaxCosine-LSTM and that of MaxCosine-LSTM-biEmb. Another fact is that the dropout ratio can control the model complexity: lower dropout ratio means higher model complexity, because dropout is one kind of regularization."}, {"heading": "6 Conclusion and Outlook", "text": "We proposed a simple neural method to determine the relation between a hypothesis and a premise. It relies on word semantic matching from the hypothesis to the premise (or vice versa), then makes use the LSTM to do the sequence modeling, and finally feed the final output from the LSTM into a softmax layer to make the classification decision. After equipped with three techniques to improve its performance, experimental results have shown that our method has achieved better accuracies than state-of-the-art systems. In addition, it is also shown that the three techniques all have their own contribution to the accuracy obtained."}, {"heading": "Acknowledgments", "text": "This work is supported by National High-Tech R&D Program of China (863 Program) (No. 2015AA015404), and Science and Technology Commission of Shanghai Municipality (No. 14511106802). We are grateful to the anonymous reviewers for their valuable comments."}], "references": [{"title": "Generating entailment rules from framenet", "author": ["R.B. Aharon", "I. Szpektor", "I. Dagan"], "venue": "Proceedings of the ACL 2010 Conference Short Papers. pp. 241\u2013246. Association for Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 36 Annual Meeting of the Association for Computational Linguistics and 17 International Conference on Computational Linguistics-Volume 1. pp. 86\u2013 90. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks 5(2), 157\u2013166", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632\u2013642", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["S.R. Bowman", "J. Gauthier", "A. Rastogi", "R. Gupta", "C.D. Manning", "C. Potts"], "venue": "arXiv preprint arXiv:1603.06021", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["J. Cheng", "L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches", "author": ["I. Dagan", "B. Dolan", "B. Magnini", "D. Roth"], "venue": "Natural Language Engineering 15(4), i\u2013xvii", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic textual entailment: Generic applied modeling of language variability", "author": ["I. Dagan", "O. Glickman"], "venue": "Learning Methods for Text Understanding and Mining. pp. 26\u201329", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Wordnet: An electronic lexical database", "author": ["Fellbaum", "C. (ed."], "venue": "MIT Press Cambridge", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "The third pascal recognizing textual entailment challenge", "author": ["D. Giampiccolo", "B. Magnini", "I. Dagan", "B. Dolan"], "venue": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. pp. 1\u20139. Association for Computational Linguistics", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Acquiring lexical paraphrases from a single corpus", "author": ["O. Glickman", "I. Dagan"], "venue": "Recent Advances in Natural Language Processing III. John Benjamins Publishing, Amsterdam, Netherlands pp. 81\u201390", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word 10(2-3), 146\u2013162", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1954}, {"title": "Methods for using textual entailment in open-domain question answering", "author": ["S. Harabagiu", "A. Hickl"], "venue": "Proceedings of the 21 International Conference on Computational Linguistics and 44 Annual Meeting of the ACL. pp. 905\u2013912", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["S. Hochreiter"], "venue": "Diploma, Technische Universit\u00e4t M\u00fcnchen", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8), 1735\u20131780", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review 104(2), 211\u2013240", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Dirt - discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 323\u2013328", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "A Text Summarization Approach under the Influence of Textual Entailment", "author": ["E. Lloret", "O. Ferr\u00e1ndez", "R. Munoz", "M. Palomar"], "venue": "NLPCS. pp. 22\u201331", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems (NIPS). pp. 3111\u20133119", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["L. Mou", "R. Men", "G. Li", "Y. Xu", "L. Zhang", "R. Yan", "Z. Jin"], "venue": "arXiv preprint arXiv:1512.08422", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Acquisition of verb entailment from text", "author": ["V. Pekar"], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. pp. 49\u201356. Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Conference on Empirical Methods on Natural Language Processing (EMNLP). pp. 1532\u20131543", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u00fd", "P. Blunsom"], "venue": "International Conference on Learning Representations", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic paraphrase discovery based on context and keywords between NE pairs", "author": ["S. Sekine"], "venue": "Proceedings of IWP. vol. 2005, pp. 4\u20136", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Improving neural networks with dropout", "author": ["N. Srivastava"], "venue": "Ph.D. thesis, University of Toronto", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Augmenting wordnet-based inference with argument mapping", "author": ["I. Szpektor", "I. Dagan"], "venue": "Proceedings of the 2009 Workshop on Applied Textual Inference. pp. 27\u201335. Association for Computational Linguistics", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised acquisition of entailment relations from the web", "author": ["I. Szpektor", "H. Tanev", "I. Dagan", "B. Coppola", "M. Kouylekov"], "venue": "Natural Language Engineering 21(01), 3\u201347", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning natural language inference with LSTM", "author": ["S. Wang", "J. Jiang"], "venue": "Proceedings of the 15 Annual Conference of the North American Chapter of the Association for Computational Linguistics. pp. 700\u2013704", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "As formulated by Dagan and Glickman [8], the task of Recognizing Textual Entailment is to decide whether the meaning of a text fragment Y (called the Hypothesis) can be inferred (is inferred) from another text fragment X (called the Premise).", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "[10] extended the task to include the additional requirement that systems identify when the Hypothesis contradicts the Premise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction.", "startOffset": 91, "endOffset": 94}, {"referenceID": 12, "context": "The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction.", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "The semantic inference needs are pervasive in a variety of NLP or text mining applications [7], inclusive of but not limited to, questionanswering [13], text summarization [19], and information extraction.", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "WordNet [9] is the most prominent resource to extract entailment rules from.", "startOffset": 8, "endOffset": 11}, {"referenceID": 26, "context": "To make use of the other non-substitutable relations (such as entailment and cause relations), Szpektor and Dagan [27] populated these non-substitutable relations with argument mapping which are extracted various resource, and thus extended WordNet\u2019s inferential relations at the syntactic representation level.", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "FrameNet [2] is another manually constructed lexical knowledge base for entailment rule extraction.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "[1] detected the entailment relations implied in FrameNet, and utilized FrameNet\u2019s annotated sentences and relations between frames to extract both the entailment relations and their argument mappings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "The DIRT algorithm proposed by Lin and Pantel [18] was based on the so-called Extended Distributional Hypothesis which states that", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Different from the Extended Distributional Hypothesis adopted by DIRT, Glickman and Dagan [11] proposed an instance-based approach, which uses linguistic filters to identify paraphrase instances that describe the same fact and then rank the candidate paraphrases based on a probabilistically motivated paraphrase likelihood measure.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "Sekine [25] extracted the phrase between two named entities as candidate linear pattern, then identified a keyword in each phrase and joined phrases with the same keyword into sets, and finally linked sets which involve the same pairs of individual named entities.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Besides paraphrase rules (which can be thought of as a specific case of entailment rules), a more general notion needed for RTE is that of entailment rules [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 21, "context": "Pekar [22] proposed a three-step method: it first identifies pairs of discourse-related clauses, and then creates patterns by extracting pairs of verbs along with relevant information as to their syntactic behaviour, and finally scores each verb pair in terms of plausibility of entailment.", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "[28] presented a fully unsupervised learning algorithm for Web-based extraction of entailment rules.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], there comes an upsurge of end-to-end neural models for RTE, where the fundamental problem is how to model a sentence pair (X,Y ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], encodes the premise and the hypothesis with two separate LSTMs, and then feeds the concatenation of their final outputs into a MLP for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 5, "context": "Several follow-ups have been proposed to solve this problem by modeling their interaction with a variety of attentive mechanisms [24] [29] [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Such kind of work includes the Stack-augmented Parser-Interpreter Neural Network (SPINN) [5] and Tree-based Convolutional Neural Network (TBCNN) [21].", "startOffset": 89, "endOffset": 92}, {"referenceID": 20, "context": "Such kind of work includes the Stack-augmented Parser-Interpreter Neural Network (SPINN) [5] and Tree-based Convolutional Neural Network (TBCNN) [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "[4] in 2015, which uses separate LSTMs [15] to encode the premise and the hypothesis as dense fixed-length vectors and then feeds their concatenation into a multi-layer perceptron (MLP) or other classifiers for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[4] in 2015, which uses separate LSTMs [15] to encode the premise and the hypothesis as dense fixed-length vectors and then feeds their concatenation into a multi-layer perceptron (MLP) or other classifiers for classification.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "To solve this problem, a sequential LSTM model is proposed in [24].", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "[24] applied a neural attention model which can achieve better performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Wang and Jiang [29] used an LSTM to perform word-by-word matching of the hypothesis with the premise.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "The attentive mechanisms used in [24] and [29] are both between the hypothesis and the premise.", "startOffset": 33, "endOffset": 37}, {"referenceID": 28, "context": "The attentive mechanisms used in [24] and [29] are both between the hypothesis and the premise.", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "[6] induces undirected relations among tokens as an intermediate step of learning representations, which can be thought of as an intra-attention mechanism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] introduced the Stack-augmented Parser-Interpreter Neural Network (or SPINN in short) to combine parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] proposed a tree-based convolutional neural network (TBCNN) to capture sentence-level semantics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Distributional Hypothesis proposed by Harris [12] has provided a guiding principle, which states that words appearing", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "Latent semantic analysis [17] is a representative method of this kind, which applies truncated Singular Value Decomposition to a matrix of word-context co-occurrence matrix.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "Recently, neural network-based methods, such as Skip-Gram [20] and Glove [23], have been proposed to represent words as low-dimensional vectors called word embeddings.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Recently, neural network-based methods, such as Skip-Gram [20] and Glove [23], have been proposed to represent words as low-dimensional vectors called word embeddings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Traditional recurrent neural networks often suffer from the problem of vanishing and exploding gradients [14] [3], making it hard to train models.", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "Traditional recurrent neural networks often suffer from the problem of vanishing and exploding gradients [14] [3], making it hard to train models.", "startOffset": 110, "endOffset": 113}, {"referenceID": 14, "context": "In this paper, we adopt the Long Short-Term Memory (LSTM) model [15] which partially solves the problem by using gated activation function.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Dropout has shown a great success when working with feed-forward networks [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "As indicated in [30], our method drops the input and the output of the LSTM layer, with the same dropout rate.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "In the experimental part, we evaluate our method on the Stanford Natural Language Inference (SNLI) dataset [4] which consists of about 570K sentence pairs.", "startOffset": 107, "endOffset": 110}, {"referenceID": 15, "context": "We use stochastic mini-batch gradient descent with the ADAM optimizer [16], we set ADAM\u2019s hyperparameters \u03b21 = 0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "We use both the pre-trained Glove [23] model glove.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "300d and Word2vec [20] model GoogleNews-vectorsnegative300 to initialize word embeddings.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "We don\u2019t tune the word embeddings when train, OOV words\u2019 vectors are set to be the average of their window words\u2019 vectors, follow same setting in the Match-LSTM paper [29].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "\u2013 Separate-LSTM: the first neural method proposed in [4], which encodes the premise and the hypothesis with two separate LSTMs independently.", "startOffset": 53, "endOffset": 56}, {"referenceID": 23, "context": "\u2013 Sequential-LSTM method: this method [24] makes use of two LSTMs, where an LSTM reads the premise, and a second LSTM initialized with the final cell state of the first LSTM reads a delimiter and the hypothesis.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "\u2013 Attention-LSTM: the method in [24] that attends over output vectors of the premise only for the final output of the hypothesis.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "\u2013 Word-by-Word Attention-LSTM: the method in [24] that attends over output vectors of the premise for every word in the hypothesis.", "startOffset": 45, "endOffset": 49}, {"referenceID": 28, "context": "\u2013 matchLSTM with word embedding: the method in [29] that performs wordby-word matching of the hypothesis with the premise.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones.", "creator": "LaTeX with hyperref package"}}}