{"id": "1705.10130", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "An Automatic Contextual Analysis and Clustering Classifiers Ensemble approach to Sentiment Analysis", "abstract": "products composition reviews are given one use of now the major resources to generally determine identifying the public sentiment. the existing literature on transaction reviews comprises sentiment element analysis mainly automatically utilizes supervised paradigm, which needs explicitly labeled data lines to largely be systematically trained beforehand on and presumably suffers risk from massive domain - dependency. this detailed article positively addresses these issues by describes developing a virtually completely automatic approach for sentiment analysis based dramatically on uniformly unsupervised ensemble analysis learning. whereas the computational method consists of across two phases. the first sampling phase is contextual analysis, which has five processes, namely ( 1 ) data preparation ; ( 2 ) spelling trait correction ; ( 3 ) linear intensifier handling ; ( 4 ) negation handling treatment and ( 5 ) distributed contrast handling. preceding the second phase comprises the unsupervised learning continuum approach, following which is an integrated ensemble of finite clustering classifiers programmed using a distinct majority gradient voting retrieval mechanism with comparatively different expected weight schemes. achieving the relative base ensemble classifier result of executing the ensemble method is a modified underlying k - means statistical algorithm. theoretically the total base classifier is further modified horizontally by extracting initial target centroids differently from shaping the feature structure set tree via smoothing using sentwordnet ( java swn ). finally we endeavour also simultaneously introduce significant new sentiment analysis problems of australian multinational airlines and home builders operators which offer potential product benchmark problems in the sentiment analysis field.. our experiments made on datasets evolving from different domains show that contextual analysis and the discrete ensemble phases improve the clustering performance in term of accuracy, stability and improved generalization approximation ability.", "histories": [["v1", "Mon, 29 May 2017 11:37:58 GMT  (609kb,D)", "http://arxiv.org/abs/1705.10130v1", "This article is submitted to a journal"]], "COMMENTS": "This article is submitted to a journal", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["murtadha talib al-sharuee", "fei liu", "mahardhika pratama"], "accepted": false, "id": "1705.10130"}, "pdf": {"name": "1705.10130.pdf", "metadata": {"source": "CRF", "title": "An Automatic Contextual Analysis and Clustering Classifiers Ensemble approach to Sentiment Analysis", "authors": ["Murtadha Talib AL-Sharuee", "Fei Liu", "Mahardhika Pratama"], "emails": ["al-sharuee.m@students.latrobe.edu.au"], "sections": [{"heading": null, "text": "Products reviews are one of the major resources to determine the public sentiment. The existing literature on reviews sentiment analysis mainly utilizes supervised paradigm, which needs labeled data to be trained on and suffers from domain-dependency. This article addresses these issues by describes a completely automatic approach for sentiment analysis based on unsupervised ensemble learning. The method consists of two phases. The first phase is contextual analysis, which has five processes, namely (1) data preparation; (2) spelling correction; (3) intensifier handling; (4) negation handling and (5) contrast handling. The second phase comprises the unsupervised learning approach, which is an ensemble of clustering classifiers using a majority voting mechanism with different weight schemes. The base classifier of the ensemble method is a modified k-means algorithm. The base classifier is modified by extracting initial centroids from the feature set via using SentWordNet (SWN). We also introduce new sentiment analysis problems of Australian airlines and home builders which offer potential benchmark problems in the sentiment analysis field. Our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy, stability and generalization ability.\nKeywords: Text mining, sentiment classification, unsupervised learning, contextual analysis, ensemble learning, k-means classifier"}, {"heading": "1. Introduction", "text": "Sentiment analysis (SA) is a computational analysis of user-generated materials, such as reviews, to determine its orientation (positive, negative or neutral). There are two main reasons to automate sentiment classification: first, the abundance of online materials, which is the result of web development, is beyond human analysis; and second, public opinion is a significant consideration when governments, institutions and individuals are making decisions and taking actions. Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains. Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47]. SA has been the focus of computer science research for more than 15 years and has been a topic of\n\u2217Corresponding author. Email address: al-sharuee.m@students.latrobe.edu.au (Murtadha Talib AL-Sharuee)\nPreprint submitted to * May 30, 2017\nar X\niv :1\n70 5.\n10 13\n0v 1\n[ cs\n.C L\n] 2\n9 M\nay 2\n01 7\nover 7,000 research works [13]. A large number of machine learning approaches have been proposed for classifying text in terms of the sentiment it expresses. However, most of proposed methodologies were based on supervised paradigm which requires either pretraining or human participation during the classification. Whereas unsupervised methods, which are completed automatic, do not normally achieve an accuracy at a satisfactory level. These seriously affected their usability and effectiveness, especially in handling online data which may require constant training and human participation.\nThis article introduces an unsupervised and completely automatic method for classifing reviews, which consists of two phases, contextual analysis and clustering classifier ensemble. The first phase enables an automatic contextual analysis by effectively deploying a sentiment lexicon, SentiWordNet 3.0 (SWN), to prepare text for further processing and to address common linguistic forms which are intensifiers, negation and contrast. Addrissing sentiment modifiers is substantial procedure in our algorithm because they are very common forms and they lead to a significant sentiment modification. For example, the sentence \u201dIt is not a good movie\u201d will be considered as a positive expression if the negation is not taken into consideration. Contrast also affects the overall polarity, for instance, the sentence \u201dvery long movie, however, we enjoyed it\u201d, in our method it is assumed that the second part of the sentence \u201dwe enjoyed it\u201d is the overall author\u2019s sentiment, which should be emphasized on when classifing text. The second phase of the proposed method is a binary classification by applying an ensemble learning method with modified k-means classifier as a component learner. The ensemble learning is meant to improve the clustering result because it handles the bias-and-variance problem better than a single model approach. After dealing with polarity shifters, the adjectives and adverbs in all the documents are extracted as a set of features. Thereafter we operate k-means on several vector space models with different weights schemes which will be combined using voting mechanism. To develop a reliable method, SWN , is utilized to cluster the feature set into three groups positive/negative and neutral. Then the positive and negative features groups are set to be two initial centroids (initial seeds) of k-means. A positive document which is composed of all positive features and a negative document which is composed of all negative features will also be used, later on, for group judgment.\nThe main contribution of the research is that it defines a completely automatic unsupervised method for sentiment classification, which requires no training or human participation. As a result, the proposed method is a domain-independent approach to SA and particularly effective in processing a high volume data. The unsupervised approach also aims to produce a labelling-free model which is highly desired in practice due to its low manual intervention. As a contextual analysis and clustering algorithm, it is suitable for sentiment classification because usually, in the real world, the actual need is to analyse a large quantity of reviews, not to predict a single or few instances. However, few studies have introduced unsupervised clustering algorithms to the field of sentiment analysis compared to other machine learning algorithms. This might be due to the high complexity of natural language which is difficult to be handled by unsupervised learning methodology. However, using linguistic rules and dealing with the drawbacks of the k-means algorithm, which are low accuracy, group interpretation and instability, has led to a promising unsupervised clustering method. The proposed method utilizes SWN lexicon to determine the features polarity, therefore for languages other than English, an alternative corresponding sentiment lexicon or method for clustering the features is required.\nIn this paper, we introduce an approach addresses the domain dependency and the\nintonation cost problems in SA because it is unsupervised labelling-free method. We also introduce two new problems which can be benchmark problems in the sentiment analysis field. The two datasets are Australian AirLines and HomeBuilders reviews datasets, In Airline dataset, the number of reviews for each class (positive and negative) is 750 reviews, whereas HomeBuilders dataset has 1100 reviews for each class. The following are the main contributions of this article:\n\u2022 Introducing a reliable domain independent algorithm through combining contextual analysis and unsupervised ensemble learning.\n\u2022 Two new reviews datasets of Australian AirLines and HomeBuilders collected and tested on along with other datasets.\n\u2022 New method to address intensifiers and negation using SWN, in addition to considering contrast.\n\u2022 Modifying k-means via using SWN to generate two initial seeds, and discussing a reliable method groups interpretation.\n\u2022 A comparative results discussion is provided.\nThe organisation of the remainder of the article is as follows: section 2 gives a review of the related work. In section 3, we describe the algorithm and give a background on the related methods. Section 4 presents the experiment data and analysis. In section 5, a conclusion is drawn."}, {"heading": "2. Related work", "text": "Sentiment analysis research has taken different research directions such as lexiconbased methods and machine learning which includes several techniques. In the following, important research directions are discussed."}, {"heading": "2.1. Lexicon-based methods", "text": "Lexicon-based methods were the earliest methods suggested for sentiment analysis. These methods are regarded as symbolic approaches because they simply rely on the appearance of documents\u2019 terms in a lexicon. Where usually documents are classified by aggregation of sentiment polarity scores from a lexicon. The proposed lexicon methods are different in the lexicon that has been used or generated. Manually generated lexicons such as MaxDiff [20], MPQA [53] and General Inquirer [44] contain comparably less terms and usually a sentiment score or label is associated with each word. Labelling or scouring terms manually is subjected to the annotators judgement which can be inconsistent and unequal in terms of accuracy from word to word.\nFor automatically generated lexicons [21], such as SentiWordNet (SWN)[12], the number of terms is usually high and generally no human participation is required. Thus we utilize this lexicon in the proposed method, as it will be demonstrated in section 3. One of the earliest work was the lexicon-based method by Hatzivassiloglou and McKeown [17], where adjectives conjoined by \u201dand\u201d or \u201dbut\u201d were used to build a lexicon, then clustering a graph that was produced by using the generated lexicon.\nRecent studies [45, 11] have combined a lexicon-based approach with other techniques such as linguistic rules and neural networks. This is because methods based solely on a lexicon are usually inferior in terms of accuracy due to neglecting changes in the actual sentiment strength when a term appears in different contexts."}, {"heading": "2.2. Contextual analysis methods", "text": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37]. These linguistic expression forms can be detected using some explicit hints such negation terms. For obtaining higher accuracy, in many studies, the usage of contextual rules is a preceding step which is followed by a machine learning technique. In [56], a rule-based method is proposed to detect text contains a negation and contrast, which was used to train a component classifier of an ensemble method. In addition to that, another component classifier was trained on processed reviews, where the negations have been removed and an antonym dictionary was used to replace the negated terms. The dictionary was built via deploying a weighted log-likelihood ratio (WLLR) algorithm. In order to tackle the ambiguity of the contextual linguistic structures some studies [45, 37, 32] proposed lexicon and rule-based methods. The idea is to adjust the sentiment polarity scores, which will be extracted from a sentiment lexicon. Score adjustment is suggested because the sentiment score of any term in a lexicon will not precisely reflect the actual sentiment strength of this term when appears within a textual form. For example \u201dvery good\u201d it is, obviously, invoking a strong positivity, the prior polarity score of the term \u201dgood\u201d have been modified by the word \u201dvery\u201d.\nWe apply an automatic contextual analysis as a first phase to increase the accuracy rate because utilizing clustering solely, which is the second phase, will not yield a good performance. For performing some of the contextual procedures, instead of adjusting terms sores, specialized dictionaries are built and deployed using SWN, which are more effective as been observed experimentally."}, {"heading": "2.3. Supervised machine learning algorithms", "text": "Most of the proposed machine learning methods for sentiment analysis are supervised. In the initial work, single classic data mining classifiers were usually leveraged such as SVM, ME and NB. A well-known study by Pang et al. [34], who conducted experiments using three supervised machine algorithms, NB, ME and SVM, is considered a cornerstone work in this field. As reported by Pang and Lee and many other researchers, SVM usually results in higher accuracy compared to other classic data mining approaches. In the later work more complex supervised learning algorithms were suggested to address the natural language complexity, such as ensemble algorithms [57, 56, 51] where certain mechanisms can be used to combine results of several classifiers and vector space models, which can increase the accuracy rate. However most of them were composed of supervised learners whcih need to be trained on labeled data, thus sufferings from the domain dependency problem and usually can not deal effectively with completely unseen data [60]. Therefore, to overcome this problem, our method uses an unsupervised learning paradigm to address SA."}, {"heading": "2.4. Clustering-based approaches", "text": "Clustering-based approaches to sentiment analysis have been considered in a few research studies. A sentiment analysis approach, which leverages the k-means clustering algorithm, was introduced by Li and Liu [24]. Their solution for k-means instability is to apply a voting mechanism, in which a voting by multiple results of k-means decides the group membership of a document, therefore this approach can also be considered as an ensemble learning method. Using the TF-IDF weighting method with adjective\nand adverb features effectively increases the accuracy rate by more than 15%. WordNet has been used to enhance the performance by obtaining the term score, which led to an increase in the accuracy rate. However, the approach relies on a random centroids selection which can affect its stability and performance. The method also relies on experimentally chosen seeds for groups\u2019 identification this means the seeds have to be selected each time new data is processed. Thus we propose a nonstochastic centroids selection for clustering, and automatic group identification based on initial centroids and ensemble method. In [29], a comparative study was conducted on several clustering algorithms and with different weights. They reported that k-means is suitable for balanced datasets. Another comparative work by Ma et al. [29] shows the impact of some weight schemas, and they report that k-means results in higher accuracy on average. The notion of the unsupervised clustering approach can be an effective approach for sentiment analysis if it is reliable and its results are comparably accurate.\nAs explored above, in the related work review, There is a domain dependency and data annotation issues when using supervised learning. Therefore this work introduces an unsupervised hybrid approach that combines contextual analysis and clustering classifiers ensemble. The contextual analysis phase is considered because it increases the accuracy rate which makes the method comparably effective. In following section the contextual analysis and ensemble phases will be discussed in detial.\nAn Automatic Contextual Analysis and Clustering Classifiers Ensemble"}, {"heading": "3. An Automatic Contextual Analysis and Clustering Classifiers Ensemble (ACACCE)", "text": "In this section, we will present the two phases of ACACCE (Figure 1) to classify products\u2019 reviews. The first phase is the data preparation and contextual analysis phase where steps are taken to automatically prepare and clean the text, which are followed by processing common language phenomena, such as intensifiers, negation, and contrast. The second phase is an unsupervised clustering classifiers ensemble where k-means is a base classifier."}, {"heading": "3.1. An Automatic Contextual Analysis", "text": "Contextual analysis is the first phase of ACACCE, which comprises five automatic and consecutive processes for preparing reviews and tackling common language forms. SWN has been effectively utilized to generate specialized dictionaries for some of these processes. The five processes are (1) data preparation; (2) Spelling correction; (3) intensifier handling; (4) negation handling; and (5) contrast handling."}, {"heading": "3.1.1. Data Preparation", "text": "Language Detection. The first step of the algorithm is detecting the language using a detection language tool implemented by Cybozu Labs1. The library utilises a Naive Bayesian classifier and was reported to achieve over 99% accuracy for 53 languages. Language detection is considered because we are interested in classifying reviews written in the English language only and it is likely that processed online text will contain reviews that are written in languages other than English.\n1http://labs.cybozu.co.jp/en/\nData Clearance. This step enables automatic data clearance which is significant when processing row online text. The clearance method is role-based and involves removing duplicated reviews and XML tags. It also involves processing each review to separate non-separated tokens and sentences which results in more accurate sentence boundary detection and tokenization process in the following processes."}, {"heading": "3.1.2. Spelling Correction", "text": "Spelling Correction plays an important role in ACACCE because misspelled terms cannot be processed in the following analysis which uses tools such as a POS tagger and dictionaries such as the antonym dictionary. Thus, correcting as many misspelled terms as possible can significantly enhance performance, especially if there is a large number of misspelled adjectives and adverbs because these parts of speech will be a features set of the unsupervised classification. Correcting misspelled words is addressed using two dictionaries, one being a general dictionary after which a specialized dictionary derived from SWN 2 is used to correct the adjectives and adverbs in the reviews."}, {"heading": "3.1.3. Intensifier Handling", "text": "An intensifier is normally an adverb in a sentence. An intensifier quantifies the strength of an adjective. For instance, in the sentence \u201dthe performance was extremely successful\u201d, \u201dextremely\u201d is an intensifier, which shifts the sentiment from positive to extremely positive. Intensifiers are common and effective sentiment shifters, addressing this type of polarity shifter improves the algorithm\u2019s performance. We deal with intensifiers by utilizing a synonym dictionary that has been generated from SWN. The dictionary contains all the adjectives and adverbs in SWN where each synonym pair is chosen to be of the same or close sentiment score regardless of their semantic meaning. We focus on adjectives and adverbs because they will form the feature set for the clustring phase. A predefined list of intensifiers is defined and used in the process to identify the intensifiers. Intensifier in ACACCE is handled by replacing the intensifier with a synonym of the intensified term. Let I = {I1, I2, . . . , In} (n > 0) be a sequence of intensifiers and A = {A1, A2, ..., Am} (m > 0) be the sequence of adjectives. In the sentence S = {w1, w2, . . . , wl} (l > 0) if there exists k(k > 0) such that wk = Ii and wk+1 = Aj(1 \u2264 i \u2264 n and 1 \u2264 j \u2264 m), then Ii is an intensifier of Aj, and Ii can be replaced by A\u2032j which is a synonym of Aj. For instance, given the expression \u201dso popular\u201d, the word \u201dso\u201d is replaced with the word \u201dpalmy\u201d which is the synonym of \u201dpopular\u201d in the dictionary. In this way, the invoked sentiment, whether it is positive or negative, can be detected by adding synonyms which will be extracted as features for the next learning phase of the algorithm."}, {"heading": "3.1.4. Negation Handling", "text": "Another common explicit language form is the negation. Where terms\u2019 polarity in a negative statement can be shifted to the opposite polarity. For instance, the word \u201ddidn\u2019t\u201d shifts the statement polarity of \u201dI didn\u2019t like the movie\u201d. To identify negative statements, we use a predefined list of negation terms such as \u201dnot\u201d and \u201dnever\u201d. Then, to process negation, we build an antonym dictionary to replace adjectives and adverbs, that follow negation terms, with their opposite sentiment words. Therefore in the above example, after removing the negation term, the word \u201dlike\u201d may change to \u201dhate\u201d. A similar approach was suggested in [56], however, our method differs in that it processes positive/negative adjectives and adverbs only and also, we used SWN to build the dictionary. The dictionary is a list of pairs of polar terms which have been extracted from SWN. The antonym words are antonyms in terms of sentiment strength, regardless of their actual meaning. When using a rule-based method to process negation, the scope\n2http://sentiwordnet.isti.cnr.it/\naffected by the negation term needs to be specified. We tested different scopes and experimentally found that a five-word scope after a negation term is the most effective."}, {"heading": "3.1.5. Contrast Handling", "text": "Contrast is another commonly used language structure in English. When a sentence contains a contrast term, it will have a clause that concludes author\u2019s opinion which will be focused on to determine the sentiment of the sentence. For example, in the sentence \u201dIt is a classic feel movie but unfortunately being a cynic\u201d, the overall sentiment is expressed in the part that follows the contrast word \u201dbut\u201d which is \u201dunfortunately being a cynic\u201d. To identify the contrast in a document, a list of pre-defined contrast words, such as \u201dbut\u201d and \u201dhowever\u201d, has been identified. Then, every sentence in each review which contains a contrast term is processed separately. Let S = {w1, w2, . . . wm} be the sentence which includes contrast terms, and C = {c1, c2, . . . cn} denotes the set of contrast terms. All words wj of the revoked part will be removed and words in the conclusion part will be kept."}, {"heading": "3.2. Clustering Classifiers Ensemble", "text": "Ensemble learning is an effective technique, especially when the targeted data is complex and can be represented in many forms. Although the ensemble learning imposes a higher complexity compared to a singleton classifier, it is capable of generating a model of high diversity, which enhances the power of generalization. Therefore we have used this technique with several vector space models, each model represents the dataset in a unique weight scheme. The ensemble component classifier is k-means which is well known unsupervised clustering algorithm. All the documents are represented by their sentiment expressing words such as adjectives and adverbs [5]."}, {"heading": "3.2.1. k-means algorithm", "text": "The component classifier of the proposed ensemble method is the k-means algorithm. It is a statistical and conventional clustering mean with hard boundaries in which the produced clusters are of unshared instances. It is a simple, flat, hard and polythetic clustering algorithm, with a predefined number of clusters. Several researchers have contributed to the design of the algorithm for different disciplines.\nThe algorithm is suitable for our experiments because (1) k-means is unsupervised clustering algorithm therefore it is suitable for a domain-independent method. (2) kmeans will always converge with a low number of iterations [1], which we also observe experimentally (Refer to Table 3 and 4). The low iterations number is also a result of a proper centroids selection. (3) Although predefining clusters number and hard clustering can be considered drawbacks of k-means, it is adequate for the method because ACACCE preduces only two positive and negative clusters, and by using k-means we can pre-assign the numbers of clusters. (4) k-means instability is addressed via non-random initial centroid selection, which also enhances its accuracy.\nThe default k-means is initiated by selecting k random centroids (vectors) from a given dataset [55]. Firstly, the centroids are randomly selected after which each data point is assigned to its closest centroid via a similarity measurement such as Cosine distance or Euclidean distance or other appropriate measurement methods can be applied. The next step is setting the average of the clustered points in each group as the new centroid for the corresponding cluster. Then, by iteratively recalculating the closest distances, the cluster means and setting new centroids to the obtained groups, the convergence condition is\nobtained when no new centroids are found, or in other words, no point is reassigned to a cluster.\nThe performance of k-means is highly influenced by (1) the initial centroid selection; (2) data representation; and (3) distance measurement, we have chosen Cosine distance because prior experiments have shown that Cosine distance leads to more accurate results. Below, we describe our attempt to enhance the performance of ACACCE using k-means as a based classifier through mainly focusing on first centroids selection using SWN, and data representation."}, {"heading": "3.2.2. SentiWordNet", "text": "The specialized sentiment lexicon SentiWordNet 1.0 (SWN) [12] is an automatically generated lexicon in which three scores (positive, negative and objective) are assigned to each synset from WordNet. We utilize SWN to perform a contextual analysis and determine the features of the polarities which form the initial k-means centroids.\nThe scores measure the strength of the terms polarity by assigning a value for each of the three classes, where the total of these values is equal to 1.0, and each class has a partial value based on the strength of the three invoked sentiments. A committee of eight ternary semi-supervised classifiers was utilized to build this lexicon. In this work, the enhanced version SentiWordNet 3.0 [3] is used which is based on WordNet 3.0, the updated version of WordNet, where, in addition to the committee classifier, Random walk was used to enhance the scores. An improvement of over 19% was reported [3] when using the updated version.\nThe SentiWordNets scores are generated based on WordNets synsets. Therefore, the same word in SentiWordNet can have different scores, for it may appear in several synsets. Thus, the average of the synsets scores for each term is used instead of the synset polarity values which requires a text ambiguity analysis approach, which is another research direction that will not be covered in this work. The final score of a term is expressed in equation (1).\nfinalScore = pos\u2212 neg (1)\nwhere pos is the average of the positive scores of each term, and neg is the average of the negative scores of each term.\nAlgorithm 1 Extracting a set of unique adjectives and adverbs from SentiWordNet with averaged scores\nINPUT: A SentiWordNet Lexicon L contains all terms wm OUTPUT: A set U of unique adjectives and adverbs u with averaged scores\n1: for all terms wm in L do 2: for all unique adjectives and adverbs wj \u2208 L where L is the SentiWordNet Lexicon\ndo 3: if tj == a OR tj == r, tj is part of speech tag, a and r donate adjective and\nadverb respectively then 4: vPosj = 1 n \u2211n (i=1) PositiveScorei , where n is the synsets number 5: vNegj = 1 n \u2211n (i=1)NegativeScorei , where n is the synsets number 6: Assign finalScorej = vPosj \u2212 vNagj to uj 7: Add (uj, finalScorej) to U 8: end if 9: end for\n10: end for"}, {"heading": "3.2.3. Initial centroids", "text": "The initial selection of k-means centroids is an important factor in forming the final clusters, hence the process and outcome of clustering to a certain degree depends on the first iteration where the initial centroids are selected [55]. The selection of first centroids seems to be the main factor that affects the iterations number and the convergence of the algorithm.\nA random selection, in default k-means, can result in poor performance because in the process of binary classification, for example, these two randomly selected points can be of the same class, which will lead to inaccurate clustering based on similarity. Some suggestions were introduced to address this problem such as k-means++ [1] which selects distanced random points where the probability of choosing each of these points is proportional to its overall potential contribution. However, a selection of initial noninformative points or outlier points, which are uncorrelated and dissimilar from any other documents, is another reason that can reduce classification accuracy, in spite of selecting dissimilar centroids. Other studies suggest genetic algorithms to address this issue [22, 2]. Operating the algorithm several times is another suggestion to overcome the drawback of the random selection of the first centroids. In [24], several results of k-means runs were combined using a voting mechanism, however, their method still based on a stochastic initialization and it does not completely eliminate the instability problem.\nIn our approach, ACACCE, we use SWN to automatically generate two polar seeds (positive and negative), then insert them into the dataset to be assigned as initial centroids of the k-means algorithm. These two polar centroids are guaranteed to be of different classes (ie. distanced points) that are always informative and can correlate with most of the documents in the processed data. Nonrandom initialization eliminates the instability problem because k-means will always produce the same clusters when operating on same dataset. The positive and negative seeds are automatically extracted from the feature set which is the set of adjectives and adverbs in all processed documents. SWN is utilized to split the feature set into three sets, positive, negative and neutral. This is implemented by matching each feature in the feature set against the SWN lexicon score. Then the positive feature set is assigned as a positive initial centroid and the negative feature set is assigned as a negative initial centroid. This is an effective and efficient\nsolution to the instability of k-means, because the process will start with informative centroids that contain all the negative/positive features.\nAlgorithm 2 Insert the two seeds\nINPUT: A set of feature F OUTPUT: Insert the positive PosD and the negative NegD seeds into the corpus D\n1: for all fi(1\u2264i\u2264size(F )) \u2208 F , where F is the feature set. do 2: Match fi against finalScorej of uj \u2208 U , U is the extracted set of adjectives and\nadverb from SentiWordNet 3: if finalScorej > 0 then 4: Add fi to PosD , PosD is the positive document 5: else if finalScorej < 0 then 6: Add fi to NegD, NegD is the negative document 7: end if 8: end for 9: Insert PosD and NegD into D, D is the corpus"}, {"heading": "3.2.4. Vector space models", "text": "The vector space model is a commonly used representation in text processing, where terms are features and documents are observations. A variety of matrix representations has been examined to obtain the most accurate results. With the proposed system, it is possible to experiment with 24 vector space models which are different representations of about 2000 documents of each dataset in a comparably short time. This is mainly because it is an unsupervised system, and the use of k-means with nonstochastic centroids selection results in a reduction in the computational complexity of the method. To build various VSMs, two matrixes are generated, namely the presence matrix and the frequency matrix. In the presence matrix, each document is represented by a binary fixed length row, and value of 1 represents the presence of particular features in a document. The frequency matrix, which is commonly called a bag of words in the literature, represents each document as a fixed length row in the vector space and each value is the count of the features occurrence in this document. For both matrixes, the following weights were used in the experiments and the results are shown in Table 3 and 4. In addition to those weights, the VSM number is increased by adding SWN scores to each matrix (Figer 3).\nTerm normalization (TN) [8]. TN measures the importance of a term in a particular document, where the numerator is a words count and the denominator is the length of a document where the word occurs. It expresses the importance of the word, taking into consideration the differences in the documents length. It seems to be a reasonable method in dealing with documents of unbalanced length, as in the set of movie reviews, where, for example, the shortest document contains only 17 words and the longest consists of over 2500 words. Equation (2) is the mathematical expression of term normalization.\ntfi,j = ti,j lj\n(2)\nwhere ti is the frequency of term i, li is the length of document j where term i is occurred.\nInverse document frequency (IDF). IDF is usually used as a part of another weighting method, along with the measurement of term importance or frequency in a document. It\nmeasures the importance of a term in a given corpus, regardless of the terms importance in a particular document. As empirically observed, using IDF can be more effective in some circumstances than combining it with another terms importance measure. Equation (3) is the mathematical expression of IDF.\nidf i = log\n( D\ndf i\n) (3)\nLet D be the number of all the documents and dfi is the number of documents where term i occurred.\nTerm frequencyinverse document frequency (TF-IDF) [8]. TF-IDF is a plausible and commonly used scouring mechanism in text mining tasks. It measures the importance of a particular word not only in the document, via term frequency (term normalization) as previously discussed, but also in the corpus via inverse document frequency. TF-IDF is proportional to the term frequency value and offsets the inverse document frequency value. It is expressed mathematically by equation (4).\ntf \u2212 idf = tf i,j \u2217 idf i (4)\nwhere tfij is the term normalization of term i, and idfi is the inverse document frequency of term i.\nWeight frequency-inverse document frequency (WF-IDF) [31]. WFIDF is another common weight mechanism that has been proposed to improve the accuracy of text mining systems. It is a proposed solution to the drawback of using term frequency which is the assumption that the count of appearances of a term in a document is equal to the significance of a single occurrence. Equation (5) is the mathematical expression of WF-IDF.\nwf \u2212 idf =\n{ 1 + log tf i,j.idf i, if tf i,j > 0\n0, Otherwise (5)\nwhere tfijis the term normalization of term i, and idfi is the inverse document frequency of term i.\nAverage of weights (AW). The average of the two weights, TFIDF and WFTDF, is calculated and the obtained results can be more accurate than using a single weight scoring method (Table3). Equation (6) is the mathematical expression of the averaged weight of term i in document j.\nAW = (tf \u2212 idf i,j) + (wf \u2212 idf i,j)\n2 (6)\nAlgorithm 3 Constructing the vector space models INPUT: A corpus D OUTPUT: A set of matrix files Mn\n1: Create Mn empty matrix files, n is 24 matrixes 2: for all document dj \u2208 D do 3: Create a presence vector vpj, and frequency vector vfj 4: Add vfj to M1 5: Add vpj to M2 6: end for 7: for all vectors vfj \u2208M1 and vpj \u2208M2 do 8: for all feature fi do 9: fi \u2217 weigthi, weighti denotes TF, IDF, TF \u2212 IDF,WF \u2212 IDF and AW\n10: end for 11: Add the new vector vj={1,...,10} to Mr(2 > r > 13) 12: end for 13: Remove the neutral features 14: Add finalScorej to all 12 matrix files, another 12 matrixes files of Mn will be filled"}, {"heading": "3.2.5. Neutral term and feature reduction", "text": "Neutral terms can be considered as redundant features for our experiment because we are interested in two classes only, positive and negative, and it is assumed that no sentiment polarity is likely to be expressed by the neutral features. Therefore, feature reduction can be conducted by eliminating the neutral terms. Careful consideration should be given, before using other feature selection methods after removing the neutral features as it may lead to the inaccurate clustering of short documents in the high sparsity vector space."}, {"heading": "3.2.6. Cluster interpretations", "text": "The k-means algorithm requires an interpreting strategy when processing real-world data because no labels will be provided to interpret the acquired groups. In [24], group polarity is judged based on the distribution of solid polarity documents in the clusters, and its solidity has been proven experimentally by observing 100 clustering results, where 22 documents were always correctly classified. Despite the low possibility of incorrectly classifying these seeds, which is 10\u2212z where z is the number of positive/negative seeds, this low possibility will probably be altered if there is a modification to the dataset size or if another dataset is used. In ACACCE, the two seeds that have been used as initial centroids can indicate the clusters polarity, and the assumption is that a positive cluster is where the positive seed appears and a negative cluster is where the negative seed appears. The classification of these documents can be described as a flat classification owing to the polarity of all the positive/negative features that form each seed. Thus, even weak classifiers can easily assign the seeds correctly, as observed in the experiments. In order to assess this way of interpretation and test the reliability of using the two solid polarity documents for groups judgement, we discuss two possibilities, one when both seeds are misclassified and second when one of them is misclassified. To examine the possibility if a classifier labels the positive seed as a negative instance, and the negative seed as a positive instance, we compared our method with Li and Liu [24]\u2019s method, which is based on the confusion matrix. In the confusion matrix (refer to Table 1), where a, b, c, and d are the number of documents therefore, in [24], if (b + c) > (a + d), Cluster 1\nis considered positive and Cluster 2 is negative, otherwise vice versa. The comparison between the interpretation using confusing matrix and interpretation using seeds method shows that throughout all the experiments, no contradiction between the two methods. The interpretation is mostly similar, except when the two seeds appear together in one group, which is the second possibility where one seed is misclassified, here the ACACCE method gives no interpretation and neither group is determined as positive or negative. Thus, this classifier\u2019s results will not be considered in the ensemble. To this end, utilizing the seeds can be considered a reliable indication of the groups identification because a component classifier always will either correctly classify the two seeds or misclassify one of them, in which the classifier will be neglected."}, {"heading": "3.2.7. Ensemble Learning", "text": "Ensemble learning (Figure 3) is a combination of several classifiers to achieve higher accuracy. It can combine learners of the same type, for example, bagging and boosting ensemble methods [52, 50]. It can also be an ensemble of different types of classifiers [9]. The ensemble algorithms that have been proposed for sentiment analysis are mostly supervised algorithms [51, 48, 26, 14]. They differ in the learning and the feature selection stage of the base classifiers and in its base classifier combination methods. The idea is that ensemble can be more accurate compared to a single classifier if the component classifiers are diverse and accurate [16]. An accurate classifier, also referred to as a weak classifier by Schapire [41], according to [10, 41] is a classifier which its performance is better than random guessing. An ensemble method often enhances performance because its outcome is a result of base learners results being collected and combined in a certain way, such as voting or weighting. As a result, complex problems can be solved, even by a combination of weak classifiers. It can also solve the overfitting problem, avoiding potential computational failure such as stacking in local optima and solving complex problems which might be too difficult to solve using a single classifier [10]. These advantages motivate us to examine the effect of an ensemble method by applying majority voting on the results of the k-means classifier, with pre-specified initial centroids, on different vector space models. The component classifiers are insured to be diverse by using different weight schemes, and also their accuracy is enhanced compared to random guessing by using initial seeds as centroids of k-means. More importantly, in ACACCE, assembling is significant for the groups identification. The chance of inaccurately classifying the initial centroids in ensemble learning is extremely low because most of the classifiers will be able to allocate the seeds correctly, and this seed allocation can be considered a very strong indication of the group meaning. To enhance accuracy, and because a few of the weak learners, as previously mentioned, may misclassify one of the two seeds, these classifiers results can be ignored when both seeds appear in the same group."}, {"heading": "3.2.8. Ensemble of clustering classifiers algorithm", "text": "The ensemble algorithm is as follows:\nAlgorithm 4 Pre-processing and the ensemble of clustering classifiers algorithm INPUT: A corpus D of m number of documents {d1, d2, . . . , dm} OUTPUT: Assign a Positive OR Negative label For each document di \u2208 D, (i = 1, 2, . . . ,m) Pre-processing:\n1: for all document dj \u2208 D do 2: for all each word wi \u2208 dj do 3: Tag wi with part of speech tagging tj 4: if tj == a OR tj == r, a and r donate adjective and adverb respectively then 5: Keep wi 6: Add wi to F , F is the features set 7: else 8: Remove wi 9: end if\n10: end for 11: end for Clustering: 12: Set the clusters number K = 2 13: for all matrix files Mi, (i = 1, 2, . . . , n), do 14: Initialize positive seed PosD and negative seed NegD as first centroids 15: Cluster Mi into two clusters G1 and G2 by using k-means classifier Hi, with cosine\nsimilarity 16: if PosD \u2208 G1 and NegD \u2208 G2 then 17: Hi classifier is accurate enough 18: G1 is the positive cluster, G2 is the negative cluster 19: else if PosD \u2208 G2 and NegD \u2208 G1 then 20: Hi classifier is accurate enough 21: G2 is the positive cluster, G1 is the negative cluster 22: else 23: Hi classifier is NOT accurate 24: end if 25: end for Voting: 26: for all dj \u2208 D, D is the corpus do 27: for all result Ri of Hi do 28: if Hi classifier is accurate enough then 29: if \u2211 (dj(Ri) = positive \u2265 \u2211 (dj(Ri) = negative) then 30: dj = positive 31: else 32: dj = negative 33: end if 34: end if 35: end for 36: end for\nThe idea of combining several VCMs not only leads to more reliable ensemble learning, it also has more flexibility because a future enhancement can be made by using additional weight schemas or another component classifier that is suitable for large data analysis.\nHowever, extending the ensemble approach will increase the computational complexity; therefore, another component learner should be carefully selected."}, {"heading": "3.2.9. Computational complexity analysis", "text": "If the complexity of k-means is O(g(nkt)) where n is the dataset instances, k is clusters number, and t is iterations number, then the computational complexity of ACACCE is O(mg(nkt)). Ensemble methods complexity is mostly linear with respect to the number of component classifiers m, and it basically depends on the complexity of the base learner. In addition, the computational cost of cosine distance which is the similarity measurement of the base classifiers depends on the vector length. Therefore feature reduction can slightly improve the performance."}, {"heading": "4. Experiments and analysis", "text": "In order to evaluate the method, we conduct experiments on differnt reviews datasets (refer to Table 2). For evaluation, usually, when using machine learning algorithms, an experimental dataset is divided into training and testing portions. In ACACCE, the entire dataset is used for evaluation because it is an unsupervised method. The positive/negative labels that are attached to each document are used to construct a confusion matrix.\nAs we are interested in both negative and positive classes, the evaluation is done by calculating the accuracy [31, 18]. Equation (7) is a mathematical expression for calculating accuarcy based on the confusion matrix and the seeds position. In addition to accuracy, we also calculate precision, recall and F-measure [31].\naccuracy = a + b\na + b + c + d (7)\nACACCE is implemented with java 8 and NetBeans IDE 8.0.2. The experiments were conducted on a Dell machine with an 3.40 GHz Intel Core I7 CPU and 16GB RAM,\nrunning Windows7 Enterprise. For compersion with other machine learning classifiers we used Weka 3.8.1."}, {"heading": "4.1. Datasets", "text": "The across domain performance of ACACCE is evaluated by collecting and experimenting on two datasets which are Australian Airlines and HomeBuilders reviews datasets. We also conduct experiments on movie dataset and multi-domain datasets [6] (refer to Table 2)."}, {"heading": "4.1.1. Airlines and HomeBuilders datasets", "text": "A publicly available online reviews is collected from www.productreview.com.au which is an Australia consumer opinion website. Where each review is associated with one of five rating categories (excellent, good, ok, bad and terrible). This enables us to select reviews with excellent and good rating as positive instances, whereas reviews with bad and terrible rating as negative instances.\nAirlines dataset. For constructing this dataset, 1500 reviews on four Australian airlines are randomly collected. Those reviews were written between September 2006 and January 2017.\nHomeBuilders dataset. The collected reviews of this dataset are on 14 home builders companies in Australia, which were written between January 2009 and January 2017."}, {"heading": "4.1.2. Movie and multi-domain datasets", "text": "The movie review dataset by [33], which is the enhanced version of Pang et al. [34]\u2019s dataset, is a well-known dataset in the field of sentiment analysis and has been used in many research studies. It is widely believed that movie reviews are difficult documents to classify compared to other product reviews [7, 54]. This is because many aspects are likely to be discussed and different polarities can be invoked. The wide variety of movies can complicate this task even further because of the number of subjects being discussed in the reviews, such as the plot of the movie, the actors, and the movies location. It is also likely to contain unbalanced samples of different lengths, which can also cause difficulties in analyzing short documents.\nThe multi-domain dataset [6] is a benchmark dataset which was constructed by Blitzer et al. [6] using reviews on different products taken from Amazon.com. Four domains\u2019 review sets have been used in the experiments. The datasets have the same balanced composition which is 1000 positive documents and 1000 negative documents, except Baby products reviews where the review number is 900 for both classes. Each review in both datasets was automatically labeled using the rating information associated with each document, which is provided by the authors."}, {"heading": "4.2. First phase of ACACCE", "text": "The following is a detailed analysis of each procedure of the first phase. Figure (4) shows the effect of the contextual analysis phase on accuracy where accuracy rate has increased by an average of 3.01 percent when applying the contextual analysis procedures.\nData Preparation: Figure 5 shows a slight enhancement in accuracy when preparing the data compared to applying the ensemble method to raw data. This step is significant for the following procedures and also for the second phase because it enhances the process of tokenization and sentence boundary detection.\nSpelling Correction: Spelling correction using dictionaries positively affects the result because it assists processing and extracting as many adjectives and adverbs as possible in the following steps. When processing raw on-line text, there is a need for data preparation and spelling correction because it is very likely the text will contain misspelled terms.\nIntensifier Handling: An improvement is noticed when processing the intensifiers, which is due to the strong sentiment intensifying caused by these terms and also to the common use of intensifiers.\nNegation Handling: is a common form of language structure which results in strong polarity shifting. As shown in Figure 5 processing negation increased the accuracy in four datasets.\nContrast Handling: is the last procedure of the first phase and addresses the contrasts, resulting in a considerable enhancement in two datasets (Apparel and Baby), and a slight\nenhancement in the other datasets. As a preceding stage, the contextual analysis procedures improve the outcome of ACACCE (Figures 4 and 5)."}, {"heading": "4.3. Second phase of ACACCE", "text": "The experiments were conducted by obtaining a high-dimensional matrixes of all adjectives and adverbs as features. For extracting adjectives and adverbs we use Stanford part-of-speech tagger [46]. A matrix is representing all documents of each dataset in a vector space model, where each document is a vector in the vector space. This model was proposed for the information retrieval system [40]. In this representation of corpus, the order of terms in a document is ignored and the sparsity of the obtained matrix is very high.\nVector space models:. The experiments\u2019 results of the component classifiers of ACACCE on Airlines and HomeBuilders datasets using five weighting schemes are shown in Tables 3 and 4. The first two matrixes that have been tested are the presence matrix and the frequency matrix. Frequency matrix mostly is inferior compared to presence matrix in term of accuracy, the difference between these matrixes\u2019 results decreases significantly when the weight schemas are used. One of those weights is TN which always leads to a lower accuracy, probably because it measures the term importance regardless of its importance to the entire corpus. The effect of the terms\u2019 weights in the entire corpus becoming clearer when we used the IDF, where the term weight in a particular document is neglected. IDF enhances performance as shown in Tables 3 and 4. Using the standard weights TF-IDF and WF-IDF with presence and frequency matrixes significantly enhances the base classifier accuracy by averages over 12% and 4% when experimenting on Airlines and HomeBuilders datasets respectively.\nFeature reduction effect. A proper features selection usually improves the learning process in term of efficiency and effectiveness. Irrelevant features can negatively affect the\nlearning process [23, 59]. Therefore, for enhancing the algorithm\u2019s performance, we conduct feature reduction via matching all adjectives and adverbs against SWN. Since we are interested in positive and negative classes only polar features are considered, and the reduction is done by removing neutral terms because they do not carry the clustering characteristic of reviews. When applying feature reduction on Airlines and HomeBuilders datasets, there are slight changes which are shown in Figure 6.\nSentiment scores. In Figure 7, the sentiment scores from SWN are added to all the matrixes. The polarity score has a negative impact on accuracy which was anticipated because the sentiment score is the average score of the synsets to which a term belongs, and the context in which a term occurs, is not considered. However, the average score is likely to correctly indicate the term polarity, that is, whether it is positive, negative or neutral. This step doubles the number of the vector space models which improves the ensemble method by promoting the groups\u2019 identification.\nExperiments on multi-domain datasets. In this section, we present the results of ACACCE on different domains datasets. After applying the contextual analysis and constructing the matrixes, the last step is to feed the matrixes into the ensemble method, in which a document will be classified as a positive/negative instance if the majority of the com-\nponent classifiers classify this document as positive/negative. The ensemble method combines 24 matrixes which are the VSMs in Tables 3 and 4 in addition to those being produced by adding SWN score. This variation will make the group interpretation more reliable because the two seeds will be classified by various data representations. In very few cases, component classifiers may incorrectly classify one of the seeds, which means both seeds will appear in one cluster. These classifiers will not be considered in the voting. An experiment on the multi-domain dataset is shown in Figure 8. In addition to the Airlines and HomeBuilders datasets, five sets of products\u2019 reviews (movie, kitchen, apparel, toys and games and baby) were compared. The accuracy rate is between 94.41% and 79.56% for six datasets except toys and games dataset where the accuracy is 77.11%. In general, the results show that ACACCE is a domain-independent algorithm with a competitive accuracy."}, {"heading": "4.4. Discussion", "text": "Table 5 is a comparison between ACACCE and seven different classifiers. Four of them are supervised classifiers namely Support Vector Machine (SVM), Random Forest (RF), decision tree (J48), Naive Base (NB) and Multinominal Naive Base (MNB). For conducting experiments using supervised classifiers, all part of speach tags are used as a set of features and TF-IDF weight is also utilized. We also report results of a clustering based method by Li and Liu [24] on a sample of movie review dataset. In addition to that, a baseline classifier is constructed to classify a document by aggregating SWN\u2019s average scores of its adjectives and adverbs to determine the polarity. The results show that the average rate of the baseline classifier\u2019s accuracy is comparably low which is probably because the average score extracted from SWN does not reflect an accurate sentiment strength of a term which is mainly because the language context in which this term appears is neglected. As it can be seen in Table 5, The performance of ACACCE is a competitive performance compared to supervised and unsupervised classifiers. The average of the accuracy rate of ACACCE is very close to the average rate of SVM\u2019s accuracy which is the best average rate among the compared methods. ACACCE also yields the best performances on three datasets and comparable performances on the other four datasets. The accuracy of ACAECC is enhanced by at least 2% compared to\nan unsupervised method by Li and Liu [24] which is due to the contextual analysis phase, using initial polar seeds and utilize a diverse weight schemes.\nThe algorithm has solved the instability problem of k-means in a more efficient way. Unlike Li and Lius method proposed in [24], this study proposes more robust and reliable method because every run on the same data the algorithm guarantees the same performance and outcome which is due to the nonstochastic centroids initialization. In addition, ACACCE provides more reliable group interpretation strategy by using assembly to classify the seeds. The advantages of the method are: (1) it is a competitive method in terms of accuracy, (2) it is stable and domain independent; and (3) it requires no human participation (i.e. unlike the supervised learning methodologies, it requires no training).\nResearch implications:. This study has shown that SA can be effectively addressed by unsupervised clustering learning which results in a domain-independent algorithm. Our findings from experimenting on multi-domain datasets promote the idea of adopting a cluster analysis method for SA. ACACCE involves two main stages that improved the outcome; contextual analysis and an ensemble of clustering classifiers. Utilizing contextual analysis has a significant impact on the results because the language forms, which are tackled, are very common and can be strong sentiment shifters such as negation and intensifiers.\nIn the ensemble learning, we have used the traditional representation of corpus where the documents are the observations and the words are the features (adjectives and adverbs). This study supports what have been suggested in previous research [5] that adjectives and adverbs are the most informative parts of speech in term of sentiment analysis. However when it is binary classification only polar adjectives and adverbs are significant for learning, this is can be seen (Figure 6) when eliminating neutral terms which has is no significant impact on the results.\nEnsemble method has positive implications on ACACCE. Increasing the number of diverse and accurate classifiers slightly enhances the algorithm accuracy, and this is supporting what have been stated in [16]. More importantly, assembling is a significant strategy for group judgment as being explained in sections 3.2.6 and 3.2.7. The experiments\u2019 results using diverse term weighting schemes indicate that the term weighting in the entire corpus is more important compared to its weight in a particular document.\nOne of the research observations is that the generalization performance of ACACCE is enhanced which is a result of applying contextual analysis and using various data representation. In Table 5, ACACCE\u2019s performance is relatively stable when operating on different datasets compared to other algorithms. For example, ACACCE yields the higher\naccuracy when operating on Kitchen dataset whereas the accuracy rates of the other algorithms are comparably low when processing this dataset. This is because of the two processing phases of ACACCE where the processed text will be automatically analyzed in the contextual analysis phase, then in the second phase different data representations will be combined in the ensemble method.\nThis study shows that SA can be addressed by employing unsupervised clustering algorithm. k-means as a base classifier is suitable for our method because in ACACCE the cluster number is pre-defined, and k-means can process a large quantity of data in short time because it is a nonhirariacal algorithm. The k-means also being enhanced by nonrandom centroids selection which significantly increases k-means accuracy and efficiency. Selecting initial points for k-means is crucial and it was a research topic for some studies [22, 2, 58]."}, {"heading": "5. Conclusions", "text": "In this article, we have discussed a completely automatic unsupervised machine learning method for sentiment analysis. The method combines an automatic contextual analysis and an ensemble of clustering classifiers. Unsupervised learning and reliability are the features that distinguish the proposed ensemble algorithm from the other work in the literature. The reliability of ACACCE is derived from the combination of the contextual analysis phase and the ensemble learning methodology. It is an unsupervised algorithm with competitive accuracy, and subsequently, it is a domain-independent classification algorithm. ACACCE solves the problem of data annotation, which is an expensive process.\nAs a future work, we will consider a multi-class classification based on the sentiment strength. An enhancement can also be achieved by considering deeper contextual analysis and utilizing other weighting schemes or even other machine learning approaches.\nAcknowledgment The authors would like to acknowledge the financial support from the Iraqi Ministry of Higher Education and Scientific Research (MoHESR)."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A near-optimal initial seed value selection in k-means means algorithm using a genetic algorithm", "author": ["G.P. Babu", "M.N. Murty"], "venue": "Pattern Recognition Letters", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["S. Baccianella", "A. Esuli", "F. Sebastiani"], "venue": "Language Resources and Evaluation Conference LREC", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Detecting implicit expressions of affect in text using EmotiNet and its extensions", "author": ["A. Balahur", "J.M. Hermida", "A. Montoyo", "R. Mu\u00f1oz"], "venue": "Data & Knowledge Engineering", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Sentiment analysis: Adjectives and adverbs are better than adjectives alone", "author": ["F. Benamara", "C. Cesarano", "A. Picariello", "D.R. Recupero", "V.S. Subrahmanian"], "venue": "The International AAAI Conference on Web and Social Media ICWSM. Citeseer", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Movie review mining: A comparison between supervised and unsupervised classification approaches", "author": ["P. Chaovalit", "L. Zhou"], "venue": "Proceedings of the 38th Annual Hawaii International Conference on System Sciences HICSS. IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Search engines: Information retrieval in practice", "author": ["W.B. Croft", "D. Metzler", "T. Strohman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Tweet sentiment analysis with classifier ensembles", "author": ["N.F. da Silva", "E.R. Hruschka"], "venue": "Decision Support Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Ensemble methods in machine learning. In: Multiple classifier systems", "author": ["T.G. Dietterich"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "User-level twitter sentiment analysis with a hybrid approach", "author": ["M.J. Er", "F. Liu", "N. Wang", "Y. Zhang", "M. Pratama"], "venue": "In: International Symposium on Neural Networks. Springer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "SentiWordNet: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "Proceedings of Language Resources and Evaluation LREC", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Techniques and applications for sentiment analysis", "author": ["R. Feldman"], "venue": "Communications of the ACM", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Expressive signals in social media languages to improve polarity detection", "author": ["E. Fersini", "E. Messina", "F. Pozzi"], "venue": "Information Processing & Management", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Why do urban legends go viral", "author": ["M. Guerini", "C. Strapparava"], "venue": "Information Processing & Management", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence", "author": ["L.K. Hansen", "P. Salamon"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Predicting the semantic orientation of adjectives. In: Proceedings of the 35th annual meeting of the association for computational linguistics and eighth conference of the European chapter of the association for computational linguistics", "author": ["V. Hatzivassiloglou", "K.R. McKeown"], "venue": "Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1985}, {"title": "Taking sides: User classification for informal online political discourse", "author": ["Y. Kato", "S. Kurohashi", "K. Inui", "R. Malouf", "T. Mullen"], "venue": "Internet Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Sentiment analysis of short informal texts", "author": ["S. Kiritchenko", "X. Zhu", "S.M. Mohammad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Inducing the contextual and prior polarity of nouns from the induced polarity preference of verbs", "author": ["M. Klenner", "S. Petrakis"], "venue": "Data & Knowledge Engineering", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A genetic algorithm that exchanges neighboring centers for k-means clustering", "author": ["M. Laszlo", "S. Mukherjee"], "venue": "Pattern Recognition Letters", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Simultaneous feature selection and clustering using mixture models. IEEE transactions on pattern analysis and machine intelligence", "author": ["M.H. Law", "M.A. Figueiredo", "A.K. Jain"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Application of a clustering method on sentiment analysis", "author": ["G. Li", "F. Liu"], "venue": "Journal of Information Science", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Sentiment classification and polarity shifting", "author": ["S. Li", "S.Y.M. Lee", "Y. Chen", "Huang", "C.-R", "G. Zhou"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Heterogeneous ensemble learning for chinese sentiment classification", "author": ["W. Li", "W. Wang", "Y. Chen"], "venue": "Journal of Information and Computational Science", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Text-based emotion classification using emotion cause extraction", "author": ["W. Li", "H. Xu"], "venue": "Expert Systems with Applications", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sentiment analysis and opinion mining", "author": ["B. Liu"], "venue": "Synthesis lectures on human language technologies", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "A comparison study of clustering models for online review sentiment analysis. In: Web-Age Information Management WAIM", "author": ["B. Ma", "H. Yuan", "Q. Wei"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A general framework for subjective information extraction from unstructured english text", "author": ["H. Mangassarian", "H. Artail"], "venue": "Data & Knowledge Engineering", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Contextual sentiment analysis for social media genres", "author": ["A. Muhammad", "N. Wiratunga", "R. Lothian"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Thumbs up?: sentiment classification using machine learning techniques. In: Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Association for Computational Linguistics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Estimating reputation polarity on microblog posts", "author": ["Peetz", "M.-H", "M. de Rijke", "R. Kaptein"], "venue": "Information Processing & Management", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Sentiment analysis of suicide notes: A shared task. Biomedical informatics insights 5 (Suppl", "author": ["J.P. Pestian", "P. Matykiewicz", "M. Linn-Gust", "B. South", "O. Uzuner", "J. Wiebe", "K.B. Cohen", "J. Hurdle", "C. Brew"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Contextual valence shifters. In: Computing attitude and affect in text: Theory and applications", "author": ["L. Polanyi", "A. Zaenen"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Mining subjective knowledge from customer reviews: A specific case of irony detection", "author": ["A. Reyes", "P. Rosso"], "venue": "Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. Association for Computational Linguistics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "From humor recognition to irony detection: The figurative language of social media", "author": ["A. Reyes", "P. Rosso", "D. Buscaldi"], "venue": "Data & Knowledge Engineering", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "The smart retrieval system-experiments in automatic document processing", "author": ["G. Salton"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1971}, {"title": "The strength of weak learnability", "author": ["R.E. Schapire"], "venue": "Machine learning", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1990}, {"title": "Sops: stock prediction using web sentiment", "author": ["V. Sehgal", "C. Song"], "venue": "Proceedings of the International Conference on Data Mining Workshops ICDMW", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Stream-based active learning for sentiment analysis in the financial domain", "author": ["J. Smailovi\u0107", "M. Gr\u010dar", "N. Lavra\u010d", "M. \u017dnidar\u0161i\u010d"], "venue": "Information Sciences", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "The general inquirer: A computer approach to content analysis", "author": ["P.J. Stone", "D.C. Dunphy", "M.S. Smith"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1966}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": "Computational linguistics", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "Association for Computational Linguistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Extraction and clustering of arguing expressions in contentious text", "author": ["A. Trabelsi", "O.R. Z\u00e4\u0131ane"], "venue": "Data & Knowledge Engineering", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Movie review classification based on a multiple classifier. In: Proceedings of the annual meetings of the Pacific Asia conference on language, information and computation (PACLIC)", "author": ["K. Tsutsumi", "K. Shimada", "T. Endo"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2007}, {"title": "Predicting elections with twitter: What 140 characters reveal about political sentiment", "author": ["A. Tumasjan", "T.O. Sprenger", "P.G. Sandner", "I.M. Welpe"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}, {"title": "Sentiment classification: The contribution of ensemble learning", "author": ["G. Wang", "J. Sun", "J. Ma", "K. Xu", "J. Gu"], "venue": "Decision support systems", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Pos-rs: A random subspace method for sentiment classification based on part-of-speech analysis", "author": ["G. Wang", "Z. Zhang", "J. Sun", "S. Yang", "C.A. Larson"], "venue": "Information Processing & Management", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Sentiment mining using ensemble classification models. In: Innovations and advances in computer sciences and engineering", "author": ["M. Whitehead", "L. Yaeger"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis. In: Proceedings of the conference on human language technology and empirical methods in natural language processing", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Association for Computational Linguistics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2005}, {"title": "Youtube movie reviews: Sentiment analysis in an audio-visual context", "author": ["M. Wollmer", "F. Weninger", "T. Knaup", "B. Schuller", "C. Sun", "K. Sagae", "Morency", "L.-P"], "venue": "Intelligent Systems,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "Philip", "S. Y"], "venue": "Knowledge and information systems", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysis", "author": ["R. Xia", "F. Xu", "J. Yu", "Y. Qi", "E. Cambria"], "venue": "Information Processing & Management", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Ensemble of feature sets and classification algorithms for sentiment classification", "author": ["R. Xia", "C. Zong", "S. Li"], "venue": "Information Sciences", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Enhancing k-means clustering algorithm with improved initial center", "author": ["M. Yedla", "S.R. Pathakota", "T. Srinivasa"], "venue": "International Journal of computer science and information technologies", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2010}, {"title": "A new feature selection method for gaussian mixture clustering", "author": ["H. Zeng", "Cheung", "Y.-M"], "venue": "Pattern Recognition", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2009}, {"title": "Cross-domain sentiment classification via topical correspondence transfer", "author": ["G. Zhou", "Y. Zhou", "X. Guo", "X. Tu", "T. He"], "venue": "Neurocomputing 159,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2015}], "referenceMentions": [{"referenceID": 47, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 92, "endOffset": 100}, {"referenceID": 18, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 92, "endOffset": 100}, {"referenceID": 14, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 112, "endOffset": 116}, {"referenceID": 34, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 143, "endOffset": 147}, {"referenceID": 33, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 162, "endOffset": 174}, {"referenceID": 41, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 162, "endOffset": 174}, {"referenceID": 40, "context": "Many diverse domains and applications can benefit from SA, including those in the political [49, 19] linguistic [15] medical and social issues [36] and financial [35, 43, 42] domains.", "startOffset": 162, "endOffset": 174}, {"referenceID": 26, "context": "Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47].", "startOffset": 117, "endOffset": 124}, {"referenceID": 3, "context": "Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47].", "startOffset": 117, "endOffset": 124}, {"referenceID": 29, "context": "Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47].", "startOffset": 148, "endOffset": 152}, {"referenceID": 37, "context": "Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47].", "startOffset": 170, "endOffset": 178}, {"referenceID": 36, "context": "Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47].", "startOffset": 170, "endOffset": 178}, {"referenceID": 45, "context": "Thus a considerable attention has been drawn to SA and closely-related research directions such as emotion detection [27, 4], subjectivity analysis [30], irony detection [39, 38], and contention texts analysis [47].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "over 7,000 research works [13].", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "Manually generated lexicons such as MaxDiff [20], MPQA [53] and General Inquirer [44] contain comparably less terms and usually a sentiment score or label is associated with each word.", "startOffset": 44, "endOffset": 48}, {"referenceID": 51, "context": "Manually generated lexicons such as MaxDiff [20], MPQA [53] and General Inquirer [44] contain comparably less terms and usually a sentiment score or label is associated with each word.", "startOffset": 55, "endOffset": 59}, {"referenceID": 42, "context": "Manually generated lexicons such as MaxDiff [20], MPQA [53] and General Inquirer [44] contain comparably less terms and usually a sentiment score or label is associated with each word.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "For automatically generated lexicons [21], such as SentiWordNet (SWN)[12], the number of terms is usually high and generally no human participation is required.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "For automatically generated lexicons [21], such as SentiWordNet (SWN)[12], the number of terms is usually high and generally no human participation is required.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "One of the earliest work was the lexicon-based method by Hatzivassiloglou and McKeown [17], where adjectives conjoined by \u201dand\u201d or \u201dbut\u201d were used to build a lexicon, then clustering a graph that was produced by using the generated lexicon.", "startOffset": 86, "endOffset": 90}, {"referenceID": 43, "context": "Recent studies [45, 11] have combined a lexicon-based approach with other techniques such as linguistic rules and neural networks.", "startOffset": 15, "endOffset": 23}, {"referenceID": 10, "context": "Recent studies [45, 11] have combined a lexicon-based approach with other techniques such as linguistic rules and neural networks.", "startOffset": 15, "endOffset": 23}, {"referenceID": 27, "context": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37].", "startOffset": 161, "endOffset": 165}, {"referenceID": 30, "context": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37].", "startOffset": 187, "endOffset": 191}, {"referenceID": 54, "context": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37].", "startOffset": 211, "endOffset": 219}, {"referenceID": 24, "context": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37].", "startOffset": 211, "endOffset": 219}, {"referenceID": 43, "context": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37].", "startOffset": 240, "endOffset": 248}, {"referenceID": 35, "context": "Contextual analysis methods are usually addressing common linguistic structures such as negation and contrast, which also were referred to as sentiment shifters [28], sentiment modifiers [32], polarity shifters [56, 25] and valence shifter [45, 37].", "startOffset": 240, "endOffset": 248}, {"referenceID": 54, "context": "In [56], a rule-based method is proposed to detect text contains a negation and contrast, which was used to train a component classifier of an ensemble method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "In order to tackle the ambiguity of the contextual linguistic structures some studies [45, 37, 32] proposed lexicon and rule-based methods.", "startOffset": 86, "endOffset": 98}, {"referenceID": 35, "context": "In order to tackle the ambiguity of the contextual linguistic structures some studies [45, 37, 32] proposed lexicon and rule-based methods.", "startOffset": 86, "endOffset": 98}, {"referenceID": 30, "context": "In order to tackle the ambiguity of the contextual linguistic structures some studies [45, 37, 32] proposed lexicon and rule-based methods.", "startOffset": 86, "endOffset": 98}, {"referenceID": 32, "context": "[34], who conducted experiments using three supervised machine algorithms, NB, ME and SVM, is considered a cornerstone work in this field.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "In the later work more complex supervised learning algorithms were suggested to address the natural language complexity, such as ensemble algorithms [57, 56, 51] where certain mechanisms can be used to combine results of several classifiers and vector space models, which can increase the accuracy rate.", "startOffset": 149, "endOffset": 161}, {"referenceID": 54, "context": "In the later work more complex supervised learning algorithms were suggested to address the natural language complexity, such as ensemble algorithms [57, 56, 51] where certain mechanisms can be used to combine results of several classifiers and vector space models, which can increase the accuracy rate.", "startOffset": 149, "endOffset": 161}, {"referenceID": 49, "context": "In the later work more complex supervised learning algorithms were suggested to address the natural language complexity, such as ensemble algorithms [57, 56, 51] where certain mechanisms can be used to combine results of several classifiers and vector space models, which can increase the accuracy rate.", "startOffset": 149, "endOffset": 161}, {"referenceID": 58, "context": "However most of them were composed of supervised learners whcih need to be trained on labeled data, thus sufferings from the domain dependency problem and usually can not deal effectively with completely unseen data [60].", "startOffset": 216, "endOffset": 220}, {"referenceID": 23, "context": "A sentiment analysis approach, which leverages the k-means clustering algorithm, was introduced by Li and Liu [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "In [29], a comparative study was conducted on several clustering algorithms and with different weights.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "[29] shows the impact of some weight schemas, and they report that k-means results in higher accuracy on average.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "A similar approach was suggested in [56], however, our method differs in that it processes positive/negative adjectives and adverbs only and also, we used SWN to build the dictionary.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "All the documents are represented by their sentiment expressing words such as adjectives and adverbs [5].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "(2) kmeans will always converge with a low number of iterations [1], which we also observe experimentally (Refer to Table 3 and 4).", "startOffset": 64, "endOffset": 67}, {"referenceID": 53, "context": "The default k-means is initiated by selecting k random centroids (vectors) from a given dataset [55].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "0 (SWN) [12] is an automatically generated lexicon in which three scores (positive, negative and objective) are assigned to each synset from WordNet.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "0 [3] is used which is based on WordNet 3.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "An improvement of over 19% was reported [3] when using the updated version.", "startOffset": 40, "endOffset": 43}, {"referenceID": 53, "context": "The initial selection of k-means centroids is an important factor in forming the final clusters, hence the process and outcome of clustering to a certain degree depends on the first iteration where the initial centroids are selected [55].", "startOffset": 233, "endOffset": 237}, {"referenceID": 0, "context": "Some suggestions were introduced to address this problem such as k-means++ [1] which selects distanced random points where the probability of choosing each of these points is proportional to its overall potential contribution.", "startOffset": 75, "endOffset": 78}, {"referenceID": 21, "context": "Other studies suggest genetic algorithms to address this issue [22, 2].", "startOffset": 63, "endOffset": 70}, {"referenceID": 1, "context": "Other studies suggest genetic algorithms to address this issue [22, 2].", "startOffset": 63, "endOffset": 70}, {"referenceID": 23, "context": "In [24], several results of k-means runs were combined using a voting mechanism, however, their method still based on a stochastic initialization and it does not completely eliminate the instability problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "Term normalization (TN) [8].", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Term frequencyinverse document frequency (TF-IDF) [8].", "startOffset": 50, "endOffset": 53}, {"referenceID": 23, "context": "In [24], group polarity is judged based on the distribution of solid polarity documents in the clusters, and its solidity has been proven experimentally by observing 100 clustering results, where 22 documents were always correctly classified.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "To examine the possibility if a classifier labels the positive seed as a negative instance, and the negative seed as a positive instance, we compared our method with Li and Liu [24]\u2019s method, which is based on the confusion matrix.", "startOffset": 177, "endOffset": 181}, {"referenceID": 23, "context": "In the confusion matrix (refer to Table 1), where a, b, c, and d are the number of documents therefore, in [24], if (b + c) > (a + d), Cluster 1", "startOffset": 107, "endOffset": 111}, {"referenceID": 50, "context": "It can combine learners of the same type, for example, bagging and boosting ensemble methods [52, 50].", "startOffset": 93, "endOffset": 101}, {"referenceID": 48, "context": "It can combine learners of the same type, for example, bagging and boosting ensemble methods [52, 50].", "startOffset": 93, "endOffset": 101}, {"referenceID": 8, "context": "It can also be an ensemble of different types of classifiers [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 49, "context": "The ensemble algorithms that have been proposed for sentiment analysis are mostly supervised algorithms [51, 48, 26, 14].", "startOffset": 104, "endOffset": 120}, {"referenceID": 46, "context": "The ensemble algorithms that have been proposed for sentiment analysis are mostly supervised algorithms [51, 48, 26, 14].", "startOffset": 104, "endOffset": 120}, {"referenceID": 25, "context": "The ensemble algorithms that have been proposed for sentiment analysis are mostly supervised algorithms [51, 48, 26, 14].", "startOffset": 104, "endOffset": 120}, {"referenceID": 13, "context": "The ensemble algorithms that have been proposed for sentiment analysis are mostly supervised algorithms [51, 48, 26, 14].", "startOffset": 104, "endOffset": 120}, {"referenceID": 15, "context": "The idea is that ensemble can be more accurate compared to a single classifier if the component classifiers are diverse and accurate [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "An accurate classifier, also referred to as a weak classifier by Schapire [41], according to [10, 41] is a classifier which its performance is better than random guessing.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "An accurate classifier, also referred to as a weak classifier by Schapire [41], according to [10, 41] is a classifier which its performance is better than random guessing.", "startOffset": 93, "endOffset": 101}, {"referenceID": 39, "context": "An accurate classifier, also referred to as a weak classifier by Schapire [41], according to [10, 41] is a classifier which its performance is better than random guessing.", "startOffset": 93, "endOffset": 101}, {"referenceID": 9, "context": "It can also solve the overfitting problem, avoiding potential computational failure such as stacking in local optima and solving complex problems which might be too difficult to solve using a single classifier [10].", "startOffset": 210, "endOffset": 214}, {"referenceID": 17, "context": "As we are interested in both negative and positive classes, the evaluation is done by calculating the accuracy [31, 18].", "startOffset": 111, "endOffset": 119}, {"referenceID": 5, "context": "We also conduct experiments on movie dataset and multi-domain datasets [6] (refer to Table 2).", "startOffset": 71, "endOffset": 74}, {"referenceID": 31, "context": "The movie review dataset by [33], which is the enhanced version of Pang et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "[34]\u2019s dataset, is a well-known dataset in the field of sentiment analysis and has been used in many research studies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "It is widely believed that movie reviews are difficult documents to classify compared to other product reviews [7, 54].", "startOffset": 111, "endOffset": 118}, {"referenceID": 52, "context": "It is widely believed that movie reviews are difficult documents to classify compared to other product reviews [7, 54].", "startOffset": 111, "endOffset": 118}, {"referenceID": 5, "context": "The multi-domain dataset [6] is a benchmark dataset which was constructed by Blitzer et al.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "[6] using reviews on different products taken from Amazon.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "au HomeBuilders 1100 1100 Movie [33] 1000 1000 http://www.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "com Kitchen [6] 1000 1000 Apparel [6] 1000 1000 https://www.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "com Kitchen [6] 1000 1000 Apparel [6] 1000 1000 https://www.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "com Toys&Games [6] 1000 1000 Baby [6] 900 900", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "com Toys&Games [6] 1000 1000 Baby [6] 900 900", "startOffset": 34, "endOffset": 37}, {"referenceID": 44, "context": "For extracting adjectives and adverbs we use Stanford part-of-speech tagger [46].", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "This model was proposed for the information retrieval system [40].", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "learning process [23, 59].", "startOffset": 17, "endOffset": 25}, {"referenceID": 57, "context": "learning process [23, 59].", "startOffset": 17, "endOffset": 25}, {"referenceID": 23, "context": "We also report results of a clustering based method by Li and Liu [24] on a sample of movie review dataset.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "Table 5: Evaluation Datasets ACACCE SVM RF J48 NB MNB Clustering [24] Baseline AirLines 83.", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "an unsupervised method by Li and Liu [24] which is due to the contextual analysis phase, using initial polar seeds and utilize a diverse weight schemes.", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "Unlike Li and Lius method proposed in [24], this study proposes more robust and reliable method because every run on the same data the algorithm guarantees the same performance and outcome which is due to the nonstochastic centroids initialization.", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "This study supports what have been suggested in previous research [5] that adjectives and adverbs are the most informative parts of speech in term of sentiment analysis.", "startOffset": 66, "endOffset": 69}, {"referenceID": 15, "context": "Increasing the number of diverse and accurate classifiers slightly enhances the algorithm accuracy, and this is supporting what have been stated in [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "Selecting initial points for k-means is crucial and it was a research topic for some studies [22, 2, 58].", "startOffset": 93, "endOffset": 104}, {"referenceID": 1, "context": "Selecting initial points for k-means is crucial and it was a research topic for some studies [22, 2, 58].", "startOffset": 93, "endOffset": 104}, {"referenceID": 56, "context": "Selecting initial points for k-means is crucial and it was a research topic for some studies [22, 2, 58].", "startOffset": 93, "endOffset": 104}], "year": 2017, "abstractText": "Products reviews are one of the major resources to determine the public sentiment. The existing literature on reviews sentiment analysis mainly utilizes supervised paradigm, which needs labeled data to be trained on and suffers from domain-dependency. This article addresses these issues by describes a completely automatic approach for sentiment analysis based on unsupervised ensemble learning. The method consists of two phases. The first phase is contextual analysis, which has five processes, namely (1) data preparation; (2) spelling correction; (3) intensifier handling; (4) negation handling and (5) contrast handling. The second phase comprises the unsupervised learning approach, which is an ensemble of clustering classifiers using a majority voting mechanism with different weight schemes. The base classifier of the ensemble method is a modified k-means algorithm. The base classifier is modified by extracting initial centroids from the feature set via using SentWordNet (SWN). We also introduce new sentiment analysis problems of Australian airlines and home builders which offer potential benchmark problems in the sentiment analysis field. Our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy, stability and generalization ability.", "creator": "LaTeX with hyperref package"}}}