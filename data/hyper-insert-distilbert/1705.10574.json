{"id": "1705.10574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Multi-Focus Image Fusion Via Coupled Sparse Representation and Dictionary Learning", "abstract": "formally we address establishing the delayed multi - mirror focus image fusion problem, where physically multiple contrast images captured with rapidly different focal data settings are to be periodically fused into giving an unreliable all - in - focus image of higher quality quality. algorithms required for this problem necessarily admit the targeted source image characteristics along with discrete focused and blurred feature. however, most sequential sparsity - resolution based approaches incorrectly use a single simultaneous dictionary in focused feature space to safely describe distorted multi - focus images, and gradually ignore the representations in blurred concentrated feature space. further here, we propose applying a multi - focus supplementary image fusion approach virtually based on coupled descriptive sparse representation. explaining the approach exploits the difficult facts that ( i ) forming the localized patches in sparse given training set can largely be sparsely frequently represented essentially by a privileged couple of seemingly overcomplete dictionaries related to employing the focused and related blurred categories analysis of images ; addition and ( ii ) merging such representations leads to implementing a more locally flexible and therefore better fusion strategy than the previous one based on just one selecting the fairly sparsest representation in the identical original image estimate. by nearly jointly learning the coupled comprehensive dictionary, we progressively enforce enhancing the similarity of sparse angular representations in locating the focused and blurred feature source spaces, and then introduce a fusion approach to combine these homogeneous representations potentially for densely generating creating an all - oriented in - focus image. we also discuss the principal advantages of lacking the fusion crossover approach entirely based on using coupled additive sparse representation methods and present an efficient algorithm for learning the coupled dictionary. for extensive practical experimental comparisons with related state - of - motion the - cross art functional multi - lens focus image digital fusion alignment algorithms accurately validate the effectiveness challenge of approaching the proposed mixed approach.", "histories": [["v1", "Tue, 30 May 2017 12:20:26 GMT  (2869kb,D)", "http://arxiv.org/abs/1705.10574v1", "27 pages, 8 figures, 1 table"]], "COMMENTS": "27 pages, 8 figures, 1 table", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rui gao", "sergiy a vorobyov"], "accepted": false, "id": "1705.10574"}, "pdf": {"name": "1705.10574.pdf", "metadata": {"source": "CRF", "title": "Multi-Focus Image Fusion Via Coupled Sparse Representation and Dictionary Learning", "authors": ["Rui Gao", "Sergiy A. Vorobyov"], "emails": ["rui.gao@aalto.fi", "svor@ieee.org"], "sections": [{"heading": null, "text": "We address the multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. Algorithms for this problem necessarily admit the source image characteristics along with focused and blurred feature. However, most sparsity-based approaches use a single dictionary in focused feature space to describe multi-focus images, and ignore the representations in blurred feature space. Here, we propose a multi-focus image fusion approach based on coupled sparse representation. The approach exploits the facts that (i) the patches in given training set can be sparsely represented by a couple of overcomplete dictionaries related to the focused and blurred categories of images; and (ii) merging such representations leads to a more flexible and therefore better fusion strategy than the one based on just selecting the sparsest representation in the original image estimate. By jointly learning the coupled dictionary, we enforce the similarity of sparse representations in the focused and blurred feature spaces, and then introduce a fusion approach to combine these representations for generating an all-in-focus image. We also discuss the advantages of the fusion approach based on coupled sparse representation and present an efficient algorithm for learning the coupled dictionary. Extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate the effectiveness of the proposed approach.\nIndex Terms\nCoupled sparse representations, dictionary learning, image fusion, multi-focus image.\nR. Gao is with Aalto University, Dept. Signal Processing and Acoustics, FI-00076, AALTO, Finland. She is also with\nNortheastern University, Dept. Computer Application Technology, Shenyang 110819, China. E-mail: rui.gao@aalto.fi\nS. A. Vorobyov is with Aalto University, Dept. Signal Processing and Acoustics, FI-00076, AALTO, Finland. E-mail:\nsvor@ieee.org\nMay 31, 2017 DRAFT\nar X\niv :1\n70 5.\n10 57\n4v 1\n[ cs\n.C V\n] 3\n0 M\nay 2\n01 7\n2 I. INTRODUCTION\nOver the last several decades, considerable attention has been given to the multi-focus image fusion problem [1]\u2013[5]. Multi-focus image fusion is an effective post-processing technique for combining multiple images captured with different focal distances into an all-in-focus image, without sacrificing image quality, and at the same time without using specialized optic sensors [6]\u2013[8]. The problem is of high importance in many fields, ranging from remote sensing to medical imaging [9]\u2013[12], especially for addressing the demand for cost minimization of optical sensors/cameras. The majority of the existing literature on the topic can be categorized into two basic classes of approaches [13]: the spatial frequency-based and transform domainbased approaches. In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images. The main drawback of these methods is that they may produce blocking artifacts because of misaligned decision map in focused boundary and wrong selection in sub-focused regions. Fusion methods from the second class suggest to apply multiscale transforms to decompose source images, and construct an all-in-focus image in the inverse transform domain. Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others. However, all aforementioned traditional methods are still sensitive to image misregistration, and they may suffer from undesirable artifacts.\nLooking at recent approaches, sparsity and overcompleteness have been successfully used for computational image fusion [19]\u2013[27]. The methods exploit the fact that patches of natural image can be compactly represented using an overcomplete dictionary as a linear combination of only few atoms. It means that the vector of weighting coefficients for the atoms is sparse. Formally, the basic synthesis model suggests that the signal x can be described as being a linear combination of few atoms over an overcomplete dictionary D, and the problem of seeking such sparse representation can be formulated as\nmin \u03b1 \u2016\u03b1\u20160 s.t. x \u2248D\u03b1 (1)\nwhere \u03b1 is the sparse vector of coefficients applied to atoms in D and \u2016 \u00b7 \u20160 denotes the operator that counts the number of non-zero entries in a vector. As problem (1) is known to be NP-hard because of its combinational nature, a suboptimal strategy for addressing the above problem is based on replacing the operator \u2016 \u00b7 \u20160 with l1-norm and learning adaptive\nMay 31, 2017 DRAFT\n3 dictionary from the training data. Many image processing applications have benefited remarkably by using such approach with learned overcomplete dictionary. Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants. \u201cGood\u201d dictionaries are expected to be highly adaptive to the observed images/signals and to contribute accurate sparse representations. While the single dictionary model has been extensively studied, there is an advanced coupled dictionary viewpoint to sparsity and overcompleteness that has tracked the double feature space representation problem [31]\u2013[34]. Corresponding approaches explicitly learn a pair of dictionaries from training sets of double image patches, and formulate sparse and redundant representations for various applications and scenarios. The combination of learned coupled dictionary and sparse approximation is shown to be superior for representing double feature spaces [35]\u2013[38].\nThe aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features. Hence, these methods ignore the sparse representations in blurred feature space, and set limits on sparsity of the vector of coefficients. The latter consequently leads to less accurate fusing coefficients. These disadvantages, which are associated with working in single feature spaces, motivate us to perform fusion instead in double feature spaces. In particular, instead of learning a single overcomplete dictionary from the focused features only, this paper suggests to learn two dictionaries over focused and blurred feature spaces, and then use the pair of dictionaries to perform fusing via coupled sparse representation. In this way, we exploit the existing structure shared by all available multi-focus images, correlate the representations over double feature spaces, and significantly improve the fusion performance. Our approach based on dictionaries learned from double feature spaces and fusion in the space of sparse representations proves to be more accurate than traditional methods based on single learned dictionary."}, {"heading": "A. Contributions", "text": "In this paper, we propose to extend the coupled dictionary learning approach based on coupled sparse and redundant representations to the problem of fusing multi-focus images. Coupled overcomplete dictionaries are expected to lead to more compact representation of the focused and blurred categories of images. Based on the coupled dictionary, a simple fusion of sparse coefficients based on plain averaging can be then used while seeking for optimal fusion coefficients\nMay 31, 2017 DRAFT\n4 needed to reconstruct an all-in-focus image. The paper presents both algorithmic developments and simulation results for multi-focus image fusion. Compared to the existing prior works, the main difference of the proposed approach is that the coupled dictionary is exploited for more flexible and accurate representation of multi-focus images. Such sparse representation then lead to higher quality results for image fusion. Towards realizing this approach, this paper makes the following main contributions.\n\u2022 We formulate the multi-focus image fusion problem as a problem of obtaining an all-in-focus\nimage representation based on sparse representation over a couple (focused and blurred) of dictionaries. Sparse representations from different feature spaces are fused by obtaining joint sparse representation. The main difficulties to be addressed in such formulation are the coupled dictionary learning and fusion of corresponding sparse representations. In our conference paper [39], some initial results for the first contribution have been presented. \u2022 We develop a fusion procedure for finding and merging sparse vectors of coefficients, which\nrepresent focused and blurred image feature spaces defined by learned coupled dictionary. Such procedure forces to produce a higher quality all-in-focus image by using the plain averaging. This contribution differs from the previous approaches on the basis of the sparsity model used, i.e., a coupled dictionary is used for fusion. Furthermore, we introduce a local sparsity in a global Bayesian reconstruction framework of our fusion procedure. \u2022 We develop a K-SVD-based coupled dictionary learning algorithm by extending the well-\nknown K-SVD algorithm [28]. It explicitly enforces the requirement that sparse approximations evaluated for double feature spaces must generate an all-in-focus image using corresponding pair of dictionaries. Our optimization algorithm simultaneously learns the focused and blurred dictionaries under the joint sparse coding requirement in order to enforce correlation between the dictionaries. Such coupled dictionary learning algorithm alternates between the updates of dictionary atoms and sparse coding."}, {"heading": "B. Organization of the Paper and Notation", "text": "The remainder of the paper is organized as follows. Section II gives the problem description and summarizes some general assumptions. Section III gives a detailed explanation of our fusion procedure. Section IV focuses on the problem of coupled dictionary learning. Specifically, a K-SVD-based coupled dictionary learning algorithm is presented. Simulation results are provided in Section V. Finally, we conclude the paper in Section VI with summary of this work.\nMay 31, 2017 DRAFT\n5 We use bold capital letters for matrices, for example, we denote k-th multi-focus and all-infocus images as the following matrices of pixels Ik and IF, respectively. Similarly, the matrices DF and DB denote the focus and blurred dictionaries, respectively. Bold lowercase letters stand for vectors. Two-dimensional image patches ordered lexicographically as column vectors are denoted, for example, as the following vectors ik and iF for patches of k-th multi-focus and all-in-focus images, respectively. The notation [X]i,j is used to specify an element located on the intersection of i-th row and j-th column of matrix X . We use p(x) to denote the probability density function (pdf) of a random quantity x and p(x|y) to denote the conditional probability of a random quantity x conditioned to another quantity y. The vector norm \u2016 \u00b7 \u2016p for p \u2265 1 is the standard lp-norm, and the meaning of the notation \u2016 \u00b7 \u20160 has been already introduced above.\nFor a matrix X , we define the Frobenius norm as \u2016X\u2016F = ( \u2211 i,j([X]i,j) 2) 1 2 . The symbol represents element-wise product of matrices, \u2207(\u00b7) denotes the forward finite difference operator on the vertical and horizontal directions, and (\u00b7)T stands for the transpose operation."}, {"heading": "II. PROBLEM DESCRIPTION", "text": "Consider the problem of constructing/reconstructing a high quality all-in-focus image IF from a set of multi-focus source images {Ik}Kk=1, which can be abstractly written in the form of the following fusing process\nIF = F {Ik}Kk=1 + V (2)\nwhere F{\u00b7} stands for a fusing operator and V is a zero-mean independent and identically distributed (i.i.d.) Gaussian additive noise matrix, with entries drawn at random from the normal distribution N (0, \u03c32). The fusion goal is to obtain IF from {Ik}Kk=1. Some further assumptions as to the capture of multiple images are the following. Each multi-focus image Ik is captured for the same scene and all multi-focus images are properly aligned. Note that the latter assumption is typical in the literature with a focus on image fusion algorithms design, but proper alignment of images is also an important practical issue. The problem of finding optimal fusing operator F {Ik}Kk=1 in (2) is an ill-posed problem in general. Therefore, different types of priors and regularizations have to be enforced and used.\nPatch-Sparsity Prior: Images are typically processed by patches.1 Image patches are easy to obtain from, for example, the all-in-focus image IF by multiplying the image with matrix Wi,j\n1It is because adapting a dictionary to large size images is impractical.\nMay 31, 2017 DRAFT\n6 that extracts (i, j)-th patch from an image by means of sliding window (moving thrugh an image staring from the left-top corner of an image and moving to the right-bottom corner) technique. Formally, by multiplying IF with Wi,j , we obtain IFi,j = Wi,jI F. Then the image patch of the size d \u00d7 d pixels IFi,j \u2208 Rd\u00d7d is ordered lexicographically as a column vector and denoted as iFi,j \u2208 Rd 2 . For notation simplicity and without loss of generality, we drop hereafter the indices (i, j) marking the patch position in an image and denote patches of k-th multi-focus image Ik and all-in-focus image IF just as ik \u2208 Rd 2 and iF \u2208 Rd2 , respectively.\nBase on the synthesis model, the patch iF can be represented as a linear combination of few atoms over an overcomplete dictionary D. Then the problem of seeking a sparse representation of iF over the dictionary D, i.e., the problem of finding sparse vector of coefficients \u03b1 for atoms of D, is the same as problem (1). It can be written in the regularized form as\nmin \u03b1 \u2225\u2225D\u03b1\u2212 iF\u2225\u22252 2 + \u03bb\u2016\u03b1\u20161 (3)\nwhere \u03bb is the regularization parameter and \u03bb\u2016\u03b1\u20161 is a regularization term enforcing sparsity of \u03b1. The dictionary here has to be learned in advance by using a training set of image patches or multiple training sets of image patches for images with different focuses.\nDouble Sparse Representation: Image patches in the multi-focus training sets can be sparsely represented by more than one overcomplete dictionary D. Specifically, for the problem of constructing an all-in-focus image from a set of available multi-focus images, it makes sense to consider a couple of overcomplete dictionaries, DF, DB \u2208 Rd2\u00d7N of N atoms (N > d2), corresponding to focus and blurred image features, respectively. Then, sparse representations can be found for each dictionary using patch-sparsity prior.\nThe benefit of using a coupled dictionary {DF, DB} versus using a single focused dictionary DF only, as in the existing fusion approaches [19]\u2013[25], [27], is the following. The use of a singly dictionary implies that the coupled features in each multi-focus patch ik are described by one indiscriminate linear combination of rows from DF. This description lacks flexibility in characterizing the blurred image structures that negatively reflects on reconstruction quality. The coupled dictionary {DF, DB} is expected to overcome this limitation by enabling a more accurate reconstruction in terms of directly considering the coupled features in each multi-focus patch. Merging multiple sparse vectors of coefficients for image patch representations over the dictionaries corresponding to focused and blurred feature spaces is then better than just selecting the sparsest representation over a single dictionary alone, because a combination of focused and\nMay 31, 2017 DRAFT\n7\nblurred feature spaces gives a larger space and more degrees of freedom to the problem [41]. Of course, the problem of fusing such sparse representation of more than one dictionary then appears, and will be addressed in this paper.\nGlobal Reconstruction: After merging multiple sparse vectors of coefficients for image patch representation over the dictionaries corresponding to focused and blurred feature spaces and finding corresponding patches of the desired all-in-focus image, the global reconstruction still has to be performed to ensure proximity between the whole reconstructed all-in-focus image and the available multi-focus images {Ik}Kk=1. It is needed to make the entire reconstructed all-in-focus image consistent, and can be enforced by applying another type of regularization, which is the total variation regularization.\nSummarizing, Fig. 1 shows the block-diagram of the procedure for constructing/reconstructing an all-in-focus image IF from the given set of multi-focus images {Ik}Kk=1. In this block-diagram, the block \u201cPatch extraction\u201d represents the above described simple process of extracting patches\nMay 31, 2017 DRAFT\n8 applied to the available multi-focus images {Ik}Kk=1. The input to the block \u201cCoupled dictionary learning\u201d is a the set of two available subsets of M training patches, i.e., {XF, XB}. HereXF ,[ xF1 ,x F 2 , \u00b7 \u00b7 \u00b7 ,xFM ] \u2208 Rd2\u00d7M and XB , [ xB1 ,x B 2 , \u00b7 \u00b7 \u00b7 ,xBM ] \u2208 Rd2\u00d7M are the subsets of available in-focus and blurred training patches, which can be extracted from actual images. The output of the block \u201cCoupled dictionary learning\u201d is the coupled dictionary {DF, DB} representing the in-focus and blurred image feature spaces. Section IV will be devoted to the coupled dictionary learning problem, which is otherwise decoupled from the image fusion problem addressed in the next section.\nThe procedure of fusing multi-focus images is presented in the block-diagram in Fig. 1 by three blocks. The block \u201cLocal optimal fusion with double sparsity prior\u201d represents a procedure for finding and fusing vectors of sparse representations of patches of multi-focus images over the coupled dictionary {DF, DB}. Knowing the dictionaries and the fused sparse vector of coefficients, we can reconstruct all patches of the all-in-focus image and combine them together to an initial estimate of the all-in-focus image denoted as IF0 , which corresponds in Fig. 1 to the block \u201cCombining patches to initial image estimate IF0 \u201d. Finally, the block \u201cGlobal reconstruction with total variation regularization\u201d represents a procedure of global all-in-focus image reconstruction."}, {"heading": "III. FUSION VIA COUPLED SPARSE REPRESENTATION", "text": ""}, {"heading": "A. The Problem of Finding Fusing Operator", "text": "The problem of constructing/reconstructing a high quality clean all-in-focus image IF can be expressed in general as the following maximum a-posteriori probability (MAP) estimator design problem IF = argmax\nIF p ( IF|{Ik}Kk=1 ) = argmax\nIF exp { \u2212 1 2\u03c32 \u2225\u2225\u2225IF \u2212F {Ik}Kk=1\u2225\u2225\u22252 F } = argmin\nIF \u2225\u2225\u2225IF \u2212F {Ik}Kk=1\u2225\u2225\u22252 F .\n(4)\nwhere the second row follows from model (2) and the assumption that the noise is Gaussian distributed, while the third row expresses the equivalence between the maximization of exponential function of a Frobenius norm square and minimization of the exponent, which is a Frobenius norm square, of exponential function. The MAP estimator (4) is equivalent to minimizing a welldefined global penalty and recovering/constructing IF in terms of finding the most probable set\nMay 31, 2017 DRAFT\n9 of observations {Ik}Kk=1. Note that the main difficulty of solving (4) is that the fusing operator F{\u00b7} is unknown and (4) is ill-posed.\nUsing the image patch double sparse representation discussed in the previous section for finding local optimal fusion and (4) for global reconstruction, the overall fusion via coupled sparse representation can be described in terms of the following optimization problem\nmin IF,\u03b1F \u2211 i,j {\u2225\u2225DF\u03b1F \u2212 iF\u2225\u22252 2 + \u03bb\u2016\u03b1F\u20161 } +\n\u2225\u2225\u2225\u2225IF \u2212F {[DF DB]; {Ik}Kk=1}\u2225\u2225\u2225\u22252 F + \u03b7\u03c1(IF)\n(5)\nwhere \u03b1F denotes the local sparse vector of coefficients needed for reconstructing the allin-focus image patch iF over the focused dictionary DF, the summation \u2211\ni,j means that\nsparse reconstruction error for all image patches is considered, \u03c1(IF ) stands for the penalty function that enforces prior knowledge about the entire all-in-focus image, and \u03bb and \u03b7 is the regularization parameters controlling the local (for image patch) sparsity penalty and the global (for the whole image) prior knowledge enforcement penalty, respectively. The notation\nF {[ DF DB ] ; {Ik}Kk=1 } is used here to reflect the fact that fusion is performed for sparse representations of the images {Ik}Kk=1 over the coupled dictionary {D F, DB}.\nProblem (5) is separable to local optimal fusion and global reconstruction subproblems. Indeed, the first row in (5) includes image patches only, and therefore, the corresponding optimization problem is the local optimal fusion. The second row in (5) includes the whole image, and therefore, the corresponding optimization problem is the global reconstruction by which the proximity between the whole reconstructed all-in-focus image and the available multi-focus images {Ik}Kk=1 is ensured to make the entire all-in-focus image consistent. Then the penalty function \u03c1(IF ) takes here the form of generic regularization applied to the whole image, e.g., total variation regularization."}, {"heading": "B. Local Optimal Fusion", "text": "Seeking the local optimal all-in-focus patch iF, we first need to find and consider the collection of sparse vectors of coefficients {\u03b1k}Kk=1 representing the patches {ik} K k=1 of the corresponding multi-focus images {Ik}Kk=1 over the coupled dictionary {D F, DB}. Due to the fact that visible artifacts may occur on patch boundaries, overlapping patches that include pixels of neighboring patches are typically used to suppress such artifacts.\nMay 31, 2017 DRAFT\n10\nFusing operator applied to the set of image patches {ik}Kk=1 can be formulated as iF , F { {ik}Kk=1 } =DF \u00b7 R { {\u03b1k}Kk=1 } \u00b7 L {\u03b1k; ik}\ufe38 \ufe37\ufe37 \ufe38\noptimal \u03b1F\n(6)\nwhere the operator L{\u00b7} yields the collections of sparse vectors of coefficients {\u03b1k}Kk=1 representing corresponding patches {ik}Kk=1, and the operator R { {\u03b1k}Kk=1 } describes selection of the optimal sparse vector, i.e., \u03b1F, from the collection of individual sparse vectors of coefficients for patches {ik}Kk=1 obtained by L{\u00b7}, i.e., from {\u03b1k} K k=1.\nIn (6), we fist need to find the operator L{\u00b7}. This operator must generate a collection of sparse representations of image patches over the coupled dictionary {DF, DB}, and it can be expressed then as the following l1-norm minimization problem\nL{\u03b1k; ik} =argmin \u03b1k \u2016\u03b1k\u20161\ns.t. \u2225\u2225DF \u00b7 C {\u03b1Fk ,\u03b1Bk}\u2212 ik\u2225\u222522 \u2264 (7)\nwhere C { \u03b1Fk ,\u03b1 B k } is the operator applied to the sparse representations \u03b1Fk and \u03b1 B k over the\ncorresponding focused and blurred components of the coupled dictionary { DF,DB } and is a tolerance parameter of user\u2019s choice that defines an acceptable representation accuracy. The\noperator C { \u03b1Fk ,\u03b1 B k } must provide the sparse vector of coefficients \u03b1k, and it can be, for example, the plain averaging of the sparse vectors \u03b1Fk and \u03b1 B k . Since the plain averaging is a separable function in DF and DB, problem (7) can be decomposed and separately solved with respect to \u03b1Fk and \u03b1 B k , and then followed by finding the plain average of \u03b1 F k and \u03b1 B k . In other words, instead of (7), we first solve the problems min \u03b1Fk \u2225\u2225\u03b1Fk\u2225\u22251 s.t. \u2225\u2225DF\u03b1Fk \u2212 ik\u2225\u222522 \u2264 min \u03b1Bk\n\u2225\u2225\u03b1Bk \u2225\u22251 s.t. \u2225\u2225DB\u03b1Bk \u2212 ik\u2225\u222522 \u2264 (8) and then find the plain average of \u03b1Fk and \u03b1 B k .\nProblems (8) can be solved efficiently by many existing greedy methods, e.g., conventional orthogonal matching pursuit (OMP) algorithm [40]. However, specifically for our fusing process, each coefficient from \u03b1Fk and \u03b1 B k represents its own significant salient structure, and merging these coefficients must form a more accurate fusion result, which can be achieved by randomization. In particular, the two vectors of competitive representations \u03b1Fk and \u03b1 B k in (8) can be found by randomized orthogonal matching pursuit (RandOMP) method [41], [42]. According\nMay 31, 2017 DRAFT\n11\nto the RandOMP method as applied to (8), instead of adding the coefficients corresponding to the atoms (columns of DF and DB), denoted hereafter as dFt and d B t , respectively, to the\ncorresponding support sets Support { \u03b1Fk } and Support { \u03b1Bk } by checking the minimum errors \u2016dFt \u03b1Fk (t)\u2212 ik(t)\u201622 and \u2016d B t \u03b1 B k (t)\u2212 ik(t)\u201622, respectively, for all candidates indexed with t, the RandOMP method makes a random choice of candidates dFt and d B t . Here \u03b1 F k (t) and \u03b1 B k (t) stand for t-th elements of the vectors \u03b1Fk and \u03b1 B k , respectively, and ik(t) is the t-th pixel of ik. The method starts with assigning empty support sets to the supports of \u03b1Fk and \u03b1 B k , i.e.,\nSupport { \u03b1Fk } = Support { \u03b1Bk } = \u2205; and assigning the residuals as r(0)F = r (0) B = ik. Then at each i-th iteration, atoms dFt and d B t are randomly and independently selected with probabilities proportional to exp{ c2 2\u03c32 |(dFt )Tr (l\u22121) F |2/\u2016d F t \u201622} and exp{ c 2 2\u03c32 |(dBt )Tr (l\u22121) B |2/\u2016d B t \u201622}, respectively. Here c2 = \u03c32k/(\u03c3 2 k + \u03c3 2) and \u03c3k is the variance of the non-zero entries of ik representation.\nAfter running through these steps L times, we obtain randomized solutions L\u22121 \u2211L\nl=1\u03b1 F(l) k and L\u22121 \u2211L\nl=1\u03b1 B(l) k , respectively. Then by applying the plain averaging operator, we find the desirable\nsparse vector of coefficients\n\u03b1k = C { \u03b1Fk ,\u03b1 B k } = 1\n2L ( L\u2211 l=1 \u03b1 F(l) k + L\u2211 l=1 \u03b1 B(l) k ) . (9)\nAfter obtaining each vector of sparse representation \u03b1k for the corresponding patch ik, the operator R { {\u03b1k}Kk=1 } is applied in accordance with (6) to find the optimal sparse vector of coefficients \u03b1F. According to the general rule [23], the sparse vector of coefficients \u03b1F is the one from the set of vectors {\u03b1k}Kk=1 that has the largest l1-norm, that is,\n\u03b1F = R { {\u03b1k}Kk=1 } = argmax\nk 6=m {\u2016\u03b1k\u20161, \u2016\u03b1m\u20161}\n1 6 k, m 6 n. (10)\nFinally, the reconstructed all-in-focus patch iF can be found as\niF =DF\u03b1F. (11)\nGoing across the whole image, all reconstructed all-in-focus image patches iF can be found separately.\nBy focusing on the image patches, we have enhanced so far local details of the all-in-focus image, i.e., spatial edges, local textures. The global reconstruction then has to be performed according to the second row of (5).\nMay 31, 2017 DRAFT\n12"}, {"heading": "C. Global Reconstruction", "text": "Given all reconstructed all-in-focus image patches iF, we can concatenate them trivially by placing to the corresponding positions to form the initial estimate of the all-in-focus image, which we denote as IF0 . In order to remove possible artifacts and improve spatial smoothness, the global reconstruction problem needs to be solved for ensuring the consistency between the initial estimation IF0 and the final outcome.\nWith the prior commonly used in the natural image analysis on the image gradients magnitude,\nthe global reconstruction problem can be written as\nIF=argmin IF\n1 2 \u2225\u2225IF \u2212 IF0\u2225\u222522 + \u03b7\u2211 i,j \u03c1 (\u2225\u2225[\u2207IF]i,j\u2225\u22252) (12)\nwhere \u03c1(\u00b7) takes the form of total variation, [\u2207IF]i,j denotes the discretization of the gradient for (i, j)-th element, defined as \u2225\u2225[\u2207IF]i,j\u2225\u22252 =\u221a[Dh\u2207IF]2i,j + [Dv\u2207IF]2i,j (13) with linear operators Dh and Dv representing finite difference approximations of the first-order horizontal and vertical partial derivatives [43].\nSimilar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems. The result IF from the above optimization is then taken as the final estimate of the all-in-focus image."}, {"heading": "D. Summary of the Fusion Via Sparse Representation Algorithm", "text": "In summary, when the underlying dictionaries DF and DB are known, the optimal fusion via local sparse representations is first calculated. After that, the global reconstruction is employed for improving spatial smoothness of the reconstructed all-in-focus image. The overall multi-focus image fusion via sparse representation algorithm is summarized as Algorithm 1. Note that before reconstructing the all-in-focus image IF from the available multi-focus source images {Ik}Kk=1 using given/learned dictionaries DF and DB, we need to ensure that all images {Ik}Kk=1 are of the same intensity, which can be achieved by removing the mean intensity for source images as reflected in step 1 of Algorithm 1. At the end the mean intensity needs to be added to the reconstructed all-in-focus image as reflected in step 10 of Algorithm 1.\nMay 31, 2017 DRAFT\n13\nAlgorithm 1 Image fusion via sparse representation. Input: Multi-focus source images {Ik}Kk=1 and learned dictionaries D F and DB.\n1: Remove the mean intensity for each source image. 2: for each d2 (e.g., d = 8) image patch ik, taken starting from the left-top corner with p (e.g.,\np = 1) pixel overlap between neighboring patches,\n3: Solve (8) to find \u03b1Fk and \u03b1 B k ; 4: Compute \u03b1k using (9); 5: Apply the operator R{{\u03b1k}nk=1} using (10); 6: Find each patch iF using (11); 7: end for 8: Form the initial estimate of the all-in-focus image IF0 . 9: Perform the global reconstruction using (12).\n10: Put the mean intensity into the reconstructed IF.\nOutput: The all-in-focus image IF."}, {"heading": "IV. COUPLED DICTIONARY LEARNING", "text": "The description of the fusing process in the previous section has been based on the assumption that the coupled dictionary {DF, DB} is known. However, the dictionary also has to be learned as shown in the block-diagram in Fig. 1. In this section, the problem of the coupled dictionary {DF, DB} learning from a set of available in-focus and blurred image patches is addressed with the objective of designing a computationally efficient algorithm."}, {"heading": "A. Coupled Dictionary Learning Problem Formulation", "text": "We aim to find a coupled dictionary {DF,DB} best representing a given available sets of training image patches for focused XF and blurred XB feature spaces. Specifically, we aim to find the best possible representation \u0393 for XF and XB in the coupled dictionary {DF, DB}. Then the problem can be formulated as the following optimization problem\nmin DF,DB,\u0393\n\u2225\u2225XF \u2212DF\u0393\u2225\u22252 F + \u2225\u2225XB \u2212DB\u0393\u2225\u22252 F\ns.t. \u2225\u2225\u03b3Tt \u2225\u22251 6 T0, \u2225\u2225dFt \u2225\u222522 6 1,\u2225\u2225dBt \u2225\u222522 6 1,\u2200t (14)\nMay 31, 2017 DRAFT\n14\nwhere the matrix \u0393 is the joint sparse coding matrix for both XF and XB, dFt and d B t are as before t-th columns of DF and DB, respectively, \u03b3Tt is t-th row of \u0393, and T0 is the parameter controlling the sparsity.\nMany numerical algorithms [31]\u2013[34] have been developed to address problems of type (14) in alternating manner. Specifically for problem (14), the optimization variables can be split into two subsets of variables, where one subset includes the coupled dictionary {DF, DB}, and the other consists of the sparse coding matrix \u0393. By addressing (14) in alternating manner, we first initialize/update and fix {DF, DB} and optimize (14) over \u0393. Then \u0393 is fixed and (14) is optimized over {DF, DB} until a stopping criterion is satisfied. Although the algorithms in [31]\u2013 [34] can be used for addressing (14) after some necessary modifications, their computational efficiency is not satisfactory.\nThe K-SVD algorithm [28] is another well-known alternating approach to overcomplete dictionary learning. It alternates between the so-called sparse coding of the training set (when the coupled dictionary is fixed) and dictionary update (when the matrix of sparse coefficients is fixed) steps. The K-SVD algorithm is computationally more efficient than the above mentioned methods, however, its computational efficiency can be further improved by using some simplifying structures present in (14). Specifically, in the dictionary update step, while updating each atom, the standard K-SVD algorithm has to update all non-zero coefficients of a corresponding column of \u0393. Moreover, \u0393 is updated again at the sparse coding step. To avoid some redundant updates, we can introduce a so-called mask matrix [48] that consists of zeros and ones, and aims to keep all the non-zero column elements intact. It can be done by adding the following constraint\n\u0393 M = 0 (15)\nto problem (14). Here, (i, j)-th element of the mask matrix M is defined as\nmi,j ,  1, if [\u0393]i,j = 0 0, if [\u0393]i,j 6= 0 . (16)\nThe dictionaries DF and DB capture the coherent structures of each corresponding individual feature space, and at the same time, describe correlation characteristics between the focused and blurred feature spaces. It facilitates the fusing process as explained in the previous section.\nMay 31, 2017 DRAFT\n15\nOur algorithm for addressing problem (14) with additional constraint (15) in alternating matter will contain two stages: (i) coupled dictionary {DF,DB} update (atom by atom updates) for fixed \u0393; and (ii) sparse coding for optimizing the matrix of joint sparse coefficients \u0393 for fixed coupled dictionary {DF, DB}."}, {"heading": "B. Dictionary Update", "text": "For fixed \u0393, we solve (14) for finding the coupled dictionary {DF, DB}. Since the objective function of (14) is separable with respect to the dictionaries DF and DB, and different sets of constraints are applied to the atoms of DF and DB, (14) can be split into two subproblems of finding updates to the dictionaries DF and DB separately. The corresponding optimization problems are given as\nDF = argmin DF \u2225\u2225\u2225\u2225\u2225XF \u2212 N\u2211 t=1 dFt \u03b3 T t \u2225\u2225\u2225\u2225\u2225 2\nF\n(17)\nDB = argmin DB \u2225\u2225\u2225\u2225\u2225XB \u2212 N\u2211 t=1 dBt \u03b3 T t \u2225\u2225\u2225\u2225\u2225 2\nF\n(18)\nsubject to the constraints in (14) applicable to corresponding atoms.\nIn (17) and (18), we rewrite the products DF\u0393 and DB\u0393 as the sums of vector outer products. After such modification, it appears to be not necessary to update all atoms simultaneously. Instead each atom can be updated disjoint from the others. Thus, to update the atoms dFt and d B t , we fix the remaining atoms, and rewrite optimization problems (17) and (18) as\ndt F=argmin\ndFt\n\u2225\u2225\u2225\u2225\u2225 ( XF \u2212 \u2211 s 6=t dFs\u03b3 T s ) Mt \u2212 dFt \u03b3Tt \u2225\u2225\u2225\u2225\u2225 2\nF\n(19)\ndt B=argmin\ndBt\n\u2225\u2225\u2225\u2225\u2225 ( XB \u2212 \u2211 s 6=t dBs \u03b3 T s ) Mt \u2212 dBt \u03b3Tt \u2225\u2225\u2225\u2225\u2225 2\nF\n(20)\nwhere Mt is built by repeating d times t-th row of the mask matrix M, i.e., the row mTt . Using the mask matrix, all the columns of \u0393, except t-th one, are removed, that is, XF \u2212\u2211 s 6=t d F t \u03b3 T t and X B \u2212 \u2211 s 6=t d B t \u03b3 T t , and only the atoms of D B and DF involved in the sparse representation picked by M are updated. Since redundant and unnecessary updates of irrelevant atoms is avoided, the computational complexity can be significantly reduced. Then optimization\nMay 31, 2017 DRAFT\n16\nproblems (19) and (20) can be further rewritten as the following simple rank-1 approximation problems\ndFt = argmin dFt \u2225\u2225EFt \u2212 dFt \u03b3Tt \u2225\u22252F (21) dBt = argmin\ndBt \u2225\u2225EBt \u2212 dBt \u03b3Tt \u2225\u22252F (22) where EFt , ( XF \u2212 \u2211 s 6=t d F s\u03b3 T s ) Mt and EBt , ( XB \u2212 \u2211 s 6=t d B s \u03b3 T s ) Mt are the error matrices which do not include t-th atoms.\nProblems (21) and (22) can be then easily solved by singular value decomposition (SVD) [28] of the corresponding error matrices. In particular, we first perform SVD for EFt and E B t , that is,\nEFt = U F t \u2206 F t V F t\n(23)\nEBt = U B t \u2206 B t V B t . (24)\nThen the updated atoms dFt and d B t are given by the principal left singular vectors (first columns) of UBt and U B t , respectively. Also the product of the first column of V F t with the largest/first singular value in \u2206Ft and the product of the first column of V B t with the first singular value in \u2206Bt can be viewed as two non-sparse versions of \u03b3 T t ."}, {"heading": "C. Sparse Coding", "text": "We yet need to find optimal sparse representation matrix \u0393. The problems for finding optimal \u03b3Tt that represents the image patches from X F and XB over the dictionaries DF and DB are, respectively, given as\n\u03b3\u0303Tt = argmin \u03b3Tt \u2225\u2225\u2225\u2225\u2225XF \u2212 N\u2211 t=1 dFt \u03b3 T t \u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb \u2225\u2225\u03b3Tt \u2225\u22251 (25)\n\u03b3\u0303Tt = argmin \u03b3Tt \u2225\u2225\u2225\u2225\u2225XB \u2212 N\u2211 t=1 dBt \u03b3 T t \u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb \u2225\u2225\u03b3Tt \u2225\u22251 . (26)\nThe objective functions in problems (25) and (26) can be rewritten in terms of the correspond-\ning error matrices EFt and E B t as \u2225\u2225EFt \u2212 dFt \u03b3Tt \u2225\u22252F + \u03bb\u2225\u2225\u03b3Tt \u2225\u22251 and \u2225\u2225EBt \u2212 dBt \u03b3Tt \u2225\u22252F + \u03bb\u2225\u2225\u03b3Tt \u2225\u22251, respectively. Moreover, both atoms from the focused and blurred dictionaries, i.e., dFt and d B t ,\nMay 31, 2017 DRAFT\n17\nmust share the same sparse representation, i.e., the same \u03b3Tt , which means that problems (25) and (26) have to be solved jointly. Then the resulting optimization problem is\nmin \u03b3t\n1 2 \u2225\u2225EFt \u2212 dFt \u03b3Tt \u2225\u22252F + 12 \u2225\u2225EBt \u2212 dBt \u03b3Tt \u2225\u22252F + \u03bb\u2225\u2225\u03b3Tt \u2225\u22251 . (27) Let us denote the whole objective function of (27) as f(\u03b3t),2 and split it into the following\ntwo parts f1(\u03b3t) , 0.5 (\u2225\u2225EFt \u2212 dFt \u03b3Tt \u2225\u22252F + \u2225\u2225EBt \u2212 dBt \u03b3Tt \u2225\u22252F) = \u2225\u2225Et \u2212 dt\u03b3Tt \u2225\u22252F /2 and f2(\u03b3t) ,\n\u03bb \u2225\u2225\u03b3Tt \u2225\u22251, where Et , EFt +EBt and dt , dFt +dBt . Then (27) can be efficiently addressed using block successive minimization method [49], which is a general framework for many large-scale optimization methods. Specifically, (27) is solved iteratively by finding a sequence {\u03b3(q)t } where the superscript q stands for the iteration index. At q-th iteration, the following optimization problem is solved\n\u03b3 (q+1) t = argmin \u03b3t G(\u03b3t,\u03b3 (q) t ) (28)\nwhere G(\u03b3t,\u03b3 (q) t ) is a surrogate function for the function f(\u03b3t) at the point \u03b3 (q) t , for which the conditions G(\u03b3(q)t ,\u03b3 (q) t ) = f(\u03b3 (q) t ) and G(\u03b3t,\u03b3 (q) t ) \u2265 f(\u03b3t) are satisfied.\nSelecting G(\u03b3t,\u03b3 (q) t ) as\nG(\u03b3t,\u03b3(q)t ),\u2207T\u03b3tf1(\u03b3 (q) t )\u03b3t+\n1 2 \u2225\u2225\u2225\u03b3t \u2212 \u03b3(q)t \u2225\u2225\u22252 2 +f2(\u03b3t) (29)\nthe update rule for \u03b3t can be found to be\n\u03b3 (q+1) t =\u03b3 (q) t \u2212S \u03bb\nT 2(dt)\n{ \u03b3 (q) t \u2212\n1\nT 2(dt) dTt\n( dt(\u03b3 (q) t ) T \u2212Et )} . (30)\nwhere the operator S \u03bb T 2(dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T 2(dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]). Since problem (27) is convex, the convergence to global optimum is guaranteed. Also note that with respect to f1(\u03b3t) only, the selection of G(\u03b3t,\u03b3 (q) t ) as (29) is the same as in the well-known mirror descent method. Therefore, (30) can be interpreted as a modified mirror descent for a composite objective function containing convex smooth, i.e., f1(\u03b3t), and convex non-smooth, i.e., f2(\u03b3t), parts, where we have the proximal soft thresholding operator due to the sparsity enforcing part f2(\u03b3t).\n2We drop for simplicity the transpose of \u03b3t, but always mean that we work with rows of \u0393.\nMay 31, 2017 DRAFT\n18"}, {"heading": "D. Summary of the Coupled Dictionary Learning Algorithm", "text": "Alternating between the dictionary update and sparse coding steps, the overall algorithm for coupled dictionary learning can be summarized as in Algorithm 2. The final update rules for both the dictionary update and sparse coding are obtained by using the most computationally efficient approaches from the state-of-the art approaches. Also the fast convergence for updates of type (30) is well documented fact (see the summary in [49]).\nAlgorithm 2 Coupled Dictionary Learning. Input: Training sets of vectorized focus and blurred image patches XF and XB, the initial\ndictionaries DF0 and D B 0 , and the initial sparse coding matrix \u03930.\n1: Initialization: Set DF :=DF0 , D B :=DB0 , \u0393 := \u03930. 2: for t = 1 \u00b7 \u00b7 \u00b7N do 3: Compute the error matrices EFt and E B t for atoms d F t and d B t , respectively; 4: Update the atoms dFt and d B t using (21) and (22) for all t; 5: Define the surrogate function as given in (29); 6: for q = 1: maximum iterations do 7: Update rows of the sparse coding matrix \u0393, i.e., \u03b3Tt , using (30) for all t; 8: end for 9: end for\nOutput: The coupled dictionary {DF, DB}."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Experimental Setup", "text": "In this section, we first evaluate the proposed approach and compare it to existing state-of-theart approaches in terms of visual comparison, and then discuss also quantitative assessments. We then discuss various factors that influence the performance including patch size, tolerance error, and number of overlapping pixels between neighboring patches. In addition, we test different approaches in terms of robustness to noise for different noise levels.\nThe quantitative assessments are based on two state-of-the-art fusion performance metrics.\n\u2022 The measure of how well the mutual information from the source images is preserved in\nthe fused image, denoted as QMI [52];\nMay 31, 2017 DRAFT\n19\n\u2022 The measure of how well the success of edge information transfers from the source images\nto the fused image, denored as, QAB/F [53].\nThe proposed approach is compared to the following existing state-of-the-art multi-focus image\nfusion algorithms.\n\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.\nThroughout all experiments, the parameter used in the methods tested are set as follows. For the LP and DWT methods, the source images are decomposed to 3 levels. The wavelet basis \u201cdb1\u201d is applied in the DWT algorithm. For the NSCT approach, the direction numbers at 4 decomposition levels from coarse to fine are selected as 4, 8, 8, and 16 for levels 1 to 4, respectively. For fair comparison, all algorithms based on sparsity model are implemented using the same dictionary size. For the proposed method, the tolerance error is set to = 0.1, the patch size is equal to 82, and the overlapping length between neighboring patches is 1. For the coupled dictionary learning, the training sets of focused image patches of size 82 consisting of 50,000 patches taken from the USC-SIPI image database [55] of 40 natural images, and 50,000 blurred patches of the same size, which are created from focused patches using Gaussian blur function, are used. The coupled dictionaries is then learned by Algorithm 2. Randomly selected patches from the training sets of focused and blurred sets of patches are used as initialization. The coupled dictionary size is selected to be 64\u00d7 256 as a reasonable tradeoff between fusion quality and computation, while in general larger size dictionary results in heavier computation, but better fusion quality. Also for coupled dictionary learning, we execute 6 multiple dictionary update cycles (DUC) and 30 iterations per DUC. All our experiments are performed on a PC running a Inter(R) Xeon(R) 3.40GHz CPU.\nB. Visual Comparison\nWe conduct our fusion experiments over the standard multi-focus dataset [54], and show some representative fusion results in Figs. 2 and 3. All the figures also include the magnified details of\nMay 31, 2017 DRAFT\n20\nthe images in the lower right corners. Figs. 2(a) and (b) show the source images, and the fused images obtained by different fusion methods tested are presented in Figs. 2(c)-(j). The same order of images and reconstructions obtained by different methods is also used in the Fig. 3. By inspecting the results visually, we can see that the LP method does not produce continuous\nMay 31, 2017 DRAFT\n21\nedges (see Fig. 2(c)) and the DWT method results in blocking artifacts (see Fig. 2(e)). The same results for these two methods can be more clearly seen in the magnified sections of the images. The fusion methods based on MWG (see Fig. 2(d)) and NSCT (see Fig. 2(f)) show circle blurring effect around strong boundaries, while Fig. 2(f) appears visually better than Fig. 2(d). In Figs. 2(g) and (i), some artificial distortions can be seen, while Fig. 2(h) obviously lacks edge information. Our approach provides the best visual appearance (see Fig. 2(j)). Note that\nMay 31, 2017 DRAFT\n22\nsimilar visual subjective results are consistently obtained in our experiments with all the images tested including the other representative images shown in Fig. 3. Therefore, it can be concluded that the proposed approach yields better all-in-focus image quality than other tested methods, including state-of-the-art methods.\nIn addition to the visual comparisons, Table I summarizes the quantitative evaluations of different methods tested on the above grayscale multi-focus images in terms of QMI , QAB/F , and the running time (in seconds). The performance measures QMI and QAB/F range from 0 to 1, with 1 representing the ideal fusion. The one bold value per each column of the table depicts the best result for a specific performance measure by a corresponding method. It can be seen from Table I that the proposed approach generally produces better quantitative results in terms of QMI and QAB/F . The value of QMI for the proposed approach is always larger than for the LP, MWG, DWT, NSCT, and SR-CM methods, and it is also better than for the\nMay 31, 2017 DRAFT\n23\nmethods based on RPCA and SR-KSVD. Therefore, the proposed approach as well as RPCA and SR-KSVD preserve well the mutual information from different source images. The values of QAB/F demonstrate that the proposed approach reduces the blocking artifacts and artificial distortions, and combines the significant edge information into the fused image. Comparing the running time, it can be seen that the SR-CM, SR-KSVD, and proposed methods require more time than the other methods tested."}, {"heading": "C. Effects of Main Parameters", "text": "The following three main parameters influence the fusion performance: patch size d2, tolerance error , and overlapping length between neighboring patches p. We compare here the best\nMay 31, 2017 DRAFT\n24\nperforming methods, which are the SR-CM, SR-KSVD and the proposed methods, on the grayscale multi-focus image dataset which contains 10 pairs of grayscale images and 30 pairs of artificial source images with 256 level grey scales. Figs. 4\u2013 6 show respectively the average values of QMI , QAB/F , and the running time for different values of pitch size, tolerance error, and overlapping length. We also give the corresponding parameter values for all the methods in the caption.\nIt can be seen in Figs. 4(a) and (b) that both QMI and QAB/F are slightly better when the patch size is larger. As the patch size increases, the running time also increases (see Fig. 4(c)) for the proposed as well as for the SR-CM and SR-KSVD methods at about the same rate, while the SR-KSVD shows the worst time. Therefore, by trying to balance the computation time and fusion quality, we have set the patch size to 82 in our above reported experiments. It can be also seen in Fig. 5 that as the tolerance error increases, all the three methods tested here show degradation, as expected, in QMI and QAB/F , while the running time slightly reduces. Yet, the proposed method shows better results than the other two methods tested here. The results in Fig. 6 show that with the increase of the overlapping length between neighboring patches, QMI decreases, while the running time increases for all three methods tested here. The proposed method still show the best results. Therefore, we have set the overlapping size as p = 1 in our above experiments."}, {"heading": "D. Robustness to Noise", "text": "To test how robust is the proposed method to noise, we add Gaussian noise to source images, and conduct fusion experiments for different noise levels \u03c3 = {0, 5, 10, 15, 20}. The results are shown in Fig. 7. It can be seen that with the increase of the noise level, QMI and QAB/F gradually decrease.\nFor comparison, we also show the results of using the SR-CM and SR-KSVD methods for noisy images with \u03c3 = 15. As it can be seen in the fused images shown in Fig. 8, there are noticeable differences in the pores on the man\u2019s face. In particular, Figs. 8(c) and (d) show over-smoothing effect. Moreover, some details also disappear. Our approach performs the best as it visually appears in Fig. 8(e). Therefore, the proposed approach is capable of providing fusion and restoration simultaneously.\nMay 31, 2017 DRAFT\n25"}, {"heading": "VI. CONCLUSION", "text": "We have presented a fusion algorithm for combing multiple images with different focal settings into one all-in-focus image. We first have formalized the physical process of capturing multifocus images, and then developed a basic model based on coupled sparse representation of all-in-focus image. We have introduced a K-SVD-based coupled dictionary learning algorithm that enforce the sparse approximations for double feature spaces (focused and blurred). Using the coupled dictionary from the focused and blurred feature spaces, we have develop an efficient and accurate fusing approach, and have demonstrated that the proposed approach well preserves the edge and structural information of source images; drastically reduces the blocking artifacts, circle blurring, and artificial distortions; and shows in general better results than the existing fusion methods including state-of-the-art methods."}], "references": [{"title": "Multifocus image fusion based on robust principal component analysis, \u201dPattern", "author": ["T. Wan", "C. Zhu", "Z. Qin"], "venue": "Recognit. Lett., vol. 34,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Multi-focus image fusion with dense SIFT", "author": ["Y. Liu", "S. Liu", "Z. Wang"], "venue": "Inf. Fusion, vol. 23, pp. 139\u2013155, May 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Generation of all-in-focus images by noise-robust selective fusion of limited depth-of-field images", "author": ["S. Pertuz", "D. Puig", "M.A. Garcia", "A. Fusiello"], "venue": "IEEE Trans. Image Process., vol. 22, no. 3, pp. 1242\u20131251, Mar. 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-focus image fusion using wavelet-domain statistics", "author": ["J. Tian", "L. Chen"], "venue": "Proc. IEEE Int. Conf. Image Process., Hong Kong, 2010, pp. 1205\u20131208.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-focus image fusion using a bilateral gradient-based sharpness criterion", "author": ["J. Tian", "L. Chen", "L. Ma", "W. Yu"], "venue": "Opt. Commun., vol. 284, no. 1, pp. 80\u201387, Jan. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Focusing techniques", "author": ["M. Subbarao", "T. Choi", "A. Nikzad"], "venue": "Opt. Eng., vol. 32, pp. 2824\u20132836, Mar. 1993.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Multifocus image fusion using the nonsubsampled contourlet transform", "author": ["Q. Zhang", "B.L. Guo"], "venue": "Signal Process., vol. 89, pp. 1334\u20131346, Jul. 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Remote sensing image fusion using the curvelet transform", "author": ["F. Nencini", "A. Garzelli", "S. Baronti", "L. Alparone"], "venue": "Inf. Fusion, vol. 8, no. 2, pp. 143\u2013156, Apr. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A wavelet-based image fusion tutorial", "author": ["G. Pajares", "J. Cruz"], "venue": "Pattern Recognit., vol. 37, no. 9, pp. 1855\u20131872, Sep. 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1855}, {"title": "Image sequence fusion using a shift-invariant wavelet transform", "author": ["O. Rockinger"], "venue": "Proc. IEEE Int. Conf. Image Process., Santa Barbara, CA, 1997, pp. 288\u2013291.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Feature-based fusion of medical imaging data", "author": ["V.D. Calhoun", "T. Adali"], "venue": "IEEE Trans. Inf. Technol. Biomedicine, vol. 13, no. 5, pp. 711\u2013720, Sep. 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Fusion of multi-focus images using differential evolution algorithm", "author": ["V. Aslantas", "R. Kurban"], "venue": "Expert Syst. Appl., vol. 37, no. 12, pp. 8861\u20138870, Dec. 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The laplacian pyramid as a compact image code", "author": ["P. Burt", "E. Adelson"], "venue": "IEEE Trans. Commun., vol. 31, no. 4, pp. 532\u2013 540, Apr. 1983. May 31, 2017  DRAFT  26", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1983}, {"title": "Multifocus image fusion using region segmentation and spatial frequency", "author": ["S. Li", "B. Yang"], "venue": "Inf. Fusion, vol. 26, no. 7, pp. 971\u2013979, Jul. 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-scale weighted gradient-based fusion for multi-focus images", "author": ["Z. Zhou", "S. Li", "B. Wang"], "venue": "Image Vision Comput., vol. 20, pp. 60\u201372, Nov. 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive multi-focus image fusion using a wavelet-based statistical sharpness measure", "author": ["J. Tian", "L. Chen"], "venue": "Signal Process., vol. 92, no. 9, pp. 2137\u20132146, Sep. 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Multifocus image fusion using the nonsubsampled contourlet transform", "author": ["Q. Zhang", "B. Guo"], "venue": "Signal Process., vol. 89, no. 7, pp. 1334\u20131346, Jul. 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Compressive image fusion", "author": ["T. Wan", "N. Canagarajah", "A. Achim"], "venue": "Proc. IEEE Int. Conf. Image Process., San Diego, CA, 2008, pp. 1308\u20131311.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Fusion of algorithms for compressed sensing", "author": ["S. Ambat", "S. Chatterjee", "K. Hari"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 14, pp. 3699\u20133704, May. 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A robust fusion scheme for multifocus images using sparse features", "author": ["T. Wan", "Z. Qin", "C. Zhu", "R. Liao"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., British Columbia, Canada, 2013, pp. 1957\u20131961.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-focus image fusion based on sparse feature matrix decomposition and morphological filtering", "author": ["H. Li", "L. Li", "J. Zhang"], "venue": "Opt. Commun., vol. 342, pp. 1\u201311, May. 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Multifocus image fusion and restoration with sparse representation", "author": ["B. Yang", "S. Li"], "venue": "IEEE Trans. Instrum. Meas., vol. 59, no. 4, pp. 884\u2013892, Apr. 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-focus image fusion using dictionary-based sparse representation", "author": ["M. Nejati", "S. Samavi", "S. Hirani"], "venue": "Inf. Fusion, vol. 25, pp. 72\u201384, Sep. 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust multi-focus image fusion using multi-task sparse representation and spatial context", "author": ["Q. Zhang", "M.D. Levine"], "venue": "IEEE Trans. Image Process., vol. 25, no. 5, pp. 2045\u20132058, Mar. 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Image fusion with cosparse analysis operator", "author": ["R. Gao", "S.A. Vorobyov", "H. Zhao"], "venue": "IEEE Signal Process. Lett., vol. 24, no. 7, pp. 943\u2013947, July 2017.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-focus image fusion based on spatial frequency in discrete cosine transform domain", "author": ["L. Cao", "L. Jin", "H. Tao", "G. Li", "Z. Zhuang", "Y. Zhang"], "venue": "IEEE Signal Process. Lett., vol. 22, no. 2, pp. 220\u2013224, Sep. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311\u20134322, Nov. 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J.H. Husoy"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., Phoenix, AZ, USA, 1999, pp. 2443\u20132446.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Proc. ACM Int. Conf. Mach. Learn., Montreal, QC, Canada, 2009, pp. 689\u2013696.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Trans. Image Process., vol. 19, no. 11, pp. 2861\u20132873, May. 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Coupled dictionary training for image super-resolution", "author": ["J. Yang", "Z. Wang", "Z. Lin", "S. Cohen", "T. Huang"], "venue": "IEEE Trans. Image Process., vol. 21, no. 8, pp. 3467\u20133478, Aug. 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint dictionary training for bandwidth extension of speech signals", "author": ["J. Sadasivan", "S. Mukherjee", "C.S. Seelamantula"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., Shanghai, China, 2016, pp. 5925\u20135929.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis", "author": ["S. Wang", "L. Zhang", "Y. Liang", "Q. Pan"], "venue": "Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., Rhode Island, USA, 2012, pp. 2216\u20132223.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "A statistical prediction model based on sparse representations for single image super-resolution", "author": ["T. Peleg", "M. Elad"], "venue": "IEEE Trans. Image Process., vol. 23, no. 6, pp. 2569\u20132582, Jun. 2014. May 31, 2017  DRAFT  27", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust multiframe super-resolution employing iteratively re-weighted minimization", "author": ["T. Kohler", "X. Huang", "F. Schebesch", "A. Aichert", "A. Maier", "J. Hornegger"], "venue": "IEEE Trans. Comput. Imag., vol. 2, no. 1, pp. 42\u201358, Mar. 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["A. Mesaros", "T. Heittola", "O. Dikmen", "T. Virtanen"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., South Brisbane, Australia, 2015, pp. 151\u2013155.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Trans. Signal Process., vol. 58, no. 3, pp. 1553\u20131564, Mar. 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-focus image fusion via coupled dictionary training", "author": ["R. Gao", "S.A. Vorobyov", "H. Zhao"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., Shanghai, China, 2016, pp. 1666\u20131670.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "CS Technion, vol. 40, no. 8, pp. 1\u201315, Apr. 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "A plurality of sparse representations is better than the sparsest one alone", "author": ["M. Elad", "I. Yavneh"], "venue": "IEEE Trans. Inf. Theory, vol. 55, no. 10, pp. 4701\u20134714, Oct. 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting statistical dependencies in sparse representations for signal recovery", "author": ["T. Peleg", "Y. Eldar", "M. Elad"], "venue": "IEEE Trans. Signal Process., vol. 60, no. 5, pp. 2286\u20132303, Feb. 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Convex image denoising via non-convex regularization", "author": ["A. Lanza", "S. Morigi", "F. Sgallari"], "venue": "Scale Sp. Var. Methods Comput. Vis., vol. 9087, Springer, pp. 666\u2013677, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for finding global minimizers of image segmentation and denoising models", "author": ["M. Nikolova", "S. Esedoglu", "T.F. Chan"], "venue": "SIAM J. Appl. Math., vol. 66, no. 5, pp. 1632\u20131648, Jun. 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex denoising using non-convex tight frame regularization,\u201dIEEE", "author": ["A. Parekh", "I.W. Selesnick"], "venue": "Signal Process. Lett., vol. 22,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D. Bertsekas"], "venue": "Math. Program., vol. 55, no. 3, pp. 293\u2013318, Nov. 1992.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1992}, {"title": "Alternating direction method for balanced image restoration", "author": ["S. Xie", "S. Rahardja"], "venue": "IEEE Trans. Image Process., vol. 21, no. 11, pp. 4557\u20134567, Nov. 2012.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving dictionary learning: multiple dictionary updates and coefficient reuse", "author": ["L.N. Smith", "M. Elad"], "venue": "IEEE Signal Process. Lett., vol. 20, no. 1, pp. 79\u201382, Jan. 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.Q. Luo"], "venue": "SIAM J. Optim., vol. 23, no. 2, pp. 1126\u20131153, Jan. 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Ideal spatial adaptation by wavelet shrinkage", "author": ["D. Donoho", "J. Johnstone"], "venue": "Biometrika, vol. 81, no. 3, pp. 425\u2013455, 1994.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1994}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics, vol. 57, no. 11, pp. 1413\u20131457, 2004.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "Comments on information measure for performance of image fusion", "author": ["M. Hossny", "S. Nahavandi", "D. Creighton"], "venue": "Electron. Lett., vol. 44, no. 18, pp. 1066\u20131067, Aug. 2008.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}, {"title": "Objective image fusion performance measure", "author": ["C. Xydeas", "V. Petrovi\u0107"], "venue": "Electron. Lett., vol. 36, no. 4, pp. 308\u2013309, Feb. 2000.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-focus image fusion using dictionary-based sparse representation", "author": ["M. Nejati", "S. Samavi", "S. Shirani"], "venue": "Inf. Fusion, vol. 25, pp. 72\u201384, Sep. 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "The USC-SIPI Image Database [Online]. Available: http://sipi.usc.edu/database", "author": ["A. Weber"], "venue": "May 31,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1981}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Over the last several decades, considerable attention has been given to the multi-focus image fusion problem [1]\u2013[5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "INTRODUCTION Over the last several decades, considerable attention has been given to the multi-focus image fusion problem [1]\u2013[5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Multi-focus image fusion is an effective post-processing technique for combining multiple images captured with different focal distances into an all-in-focus image, without sacrificing image quality, and at the same time without using specialized optic sensors [6]\u2013[8].", "startOffset": 261, "endOffset": 264}, {"referenceID": 6, "context": "Multi-focus image fusion is an effective post-processing technique for combining multiple images captured with different focal distances into an all-in-focus image, without sacrificing image quality, and at the same time without using specialized optic sensors [6]\u2013[8].", "startOffset": 265, "endOffset": 268}, {"referenceID": 7, "context": "The problem is of high importance in many fields, ranging from remote sensing to medical imaging [9]\u2013[12], especially for addressing the demand for cost minimization of optical sensors/cameras.", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "The problem is of high importance in many fields, ranging from remote sensing to medical imaging [9]\u2013[12], especially for addressing the demand for cost minimization of optical sensors/cameras.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "The majority of the existing literature on the topic can be categorized into two basic classes of approaches [13]: the spatial frequency-based and transform domainbased approaches.", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "In the first class, the methods such as image fusion based on Laplacian pyramid (LP) [14], spatial frequency (SF) [15], multi-scale weighted gradient (MWG) [16], and variance [13] aim to directly select the best pixels or regions to fuse multiple images.", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others.", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "Corresponding algorithms include discrete wavelet transform (DWT) [17], curvelet transform (CVT) [9], non-subsampled contourlet transform (NSCT) [18] among others.", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "Looking at recent approaches, sparsity and overcompleteness have been successfully used for computational image fusion [19]\u2013[27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 25, "context": "Looking at recent approaches, sparsity and overcompleteness have been successfully used for computational image fusion [19]\u2013[27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants.", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants.", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Representative examples include the K-SVD method [28], the method of optimal directions (MOD) [29], the online dictionary learning (OLD) method [30], and their variants.", "startOffset": 144, "endOffset": 148}, {"referenceID": 29, "context": "While the single dictionary model has been extensively studied, there is an advanced coupled dictionary viewpoint to sparsity and overcompleteness that has tracked the double feature space representation problem [31]\u2013[34].", "startOffset": 212, "endOffset": 216}, {"referenceID": 32, "context": "While the single dictionary model has been extensively studied, there is an advanced coupled dictionary viewpoint to sparsity and overcompleteness that has tracked the double feature space representation problem [31]\u2013[34].", "startOffset": 217, "endOffset": 221}, {"referenceID": 33, "context": "The combination of learned coupled dictionary and sparse approximation is shown to be superior for representing double feature spaces [35]\u2013[38].", "startOffset": 134, "endOffset": 138}, {"referenceID": 36, "context": "The combination of learned coupled dictionary and sparse approximation is shown to be superior for representing double feature spaces [35]\u2013[38].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "The aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "The aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "The aforementioned image fusion methods [19]\u2013[25], [27] directly learn and exploit a single overcomplete dictionary in a single feature space in order to describe multiple images which contain both the focused and blurred categories of image features.", "startOffset": 51, "endOffset": 55}, {"referenceID": 37, "context": "In our conference paper [39], some initial results for the first contribution have been presented.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "\u2022 We develop a K-SVD-based coupled dictionary learning algorithm by extending the wellknown K-SVD algorithm [28].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "The benefit of using a coupled dictionary {D, D} versus using a single focused dictionary D only, as in the existing fusion approaches [19]\u2013[25], [27], is the following.", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "The benefit of using a coupled dictionary {D, D} versus using a single focused dictionary D only, as in the existing fusion approaches [19]\u2013[25], [27], is the following.", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "The benefit of using a coupled dictionary {D, D} versus using a single focused dictionary D only, as in the existing fusion approaches [19]\u2013[25], [27], is the following.", "startOffset": 146, "endOffset": 150}, {"referenceID": 39, "context": "blurred feature spaces gives a larger space and more degrees of freedom to the problem [41].", "startOffset": 87, "endOffset": 91}, {"referenceID": 38, "context": ", conventional orthogonal matching pursuit (OMP) algorithm [40].", "startOffset": 59, "endOffset": 63}, {"referenceID": 39, "context": "In particular, the two vectors of competitive representations \u03b1k and \u03b1 B k in (8) can be found by randomized orthogonal matching pursuit (RandOMP) method [41], [42].", "startOffset": 154, "endOffset": 158}, {"referenceID": 40, "context": "In particular, the two vectors of competitive representations \u03b1k and \u03b1 B k in (8) can be found by randomized orthogonal matching pursuit (RandOMP) method [41], [42].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "According to the general rule [23], the sparse vector of coefficients \u03b1 is the one from the set of vectors {\u03b1k}k=1 that has the largest l1-norm, that is, \u03b1 = R { {\u03b1k}k=1 } = argmax k 6=m {\u2016\u03b1k\u20161, \u2016\u03b1m\u20161}", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "with linear operators Dh and Dv representing finite difference approximations of the first-order horizontal and vertical partial derivatives [43].", "startOffset": 141, "endOffset": 145}, {"referenceID": 41, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 43, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 34, "endOffset": 38}, {"referenceID": 44, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 151, "endOffset": 155}, {"referenceID": 45, "context": "Similar to the approaches in [43]\u2013[45], optimization problem (12) can be efficiently solved by the alternating directions method of multipliers (ADMM) [46], [47], which decomposes a large global problem into a series of smaller local subproblems.", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "Many numerical algorithms [31]\u2013[34] have been developed to address problems of type (14) in alternating manner.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "Many numerical algorithms [31]\u2013[34] have been developed to address problems of type (14) in alternating manner.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Although the algorithms in [31]\u2013 [34] can be used for addressing (14) after some necessary modifications, their computational efficiency is not satisfactory.", "startOffset": 27, "endOffset": 31}, {"referenceID": 32, "context": "Although the algorithms in [31]\u2013 [34] can be used for addressing (14) after some necessary modifications, their computational efficiency is not satisfactory.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "The K-SVD algorithm [28] is another well-known alternating approach to overcomplete dictionary learning.", "startOffset": 20, "endOffset": 24}, {"referenceID": 46, "context": "To avoid some redundant updates, we can introduce a so-called mask matrix [48] that consists of zeros and ones, and aims to keep all the non-zero column elements intact.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "Problems (21) and (22) can be then easily solved by singular value decomposition (SVD) [28] of the corresponding error matrices.", "startOffset": 87, "endOffset": 91}, {"referenceID": 47, "context": "Then (27) can be efficiently addressed using block successive minimization method [49], which is a general framework for many large-scale optimization methods.", "startOffset": 82, "endOffset": 86}, {"referenceID": 48, "context": "where the operator S \u03bb T (dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T (dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 49, "context": "where the operator S \u03bb T (dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T (dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 47, "context": "where the operator S \u03bb T (dt) {\u00b7} is the soft thresholding operator with the threshold \u03bb/T (dt) applied on the entries of its argument [50], [51] and T (dt) = \u2016dt\u2016 to guarantee the convergence (see details in [49]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 47, "context": "Also the fast convergence for updates of type (30) is well documented fact (see the summary in [49]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 50, "context": "\u2022 The measure of how well the mutual information from the source images is preserved in the fused image, denoted as QMI [52];", "startOffset": 120, "endOffset": 124}, {"referenceID": 51, "context": "\u2022 The measure of how well the success of edge information transfers from the source images to the fused image, denored as, Q [53].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 199, "endOffset": 203}, {"referenceID": 16, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 277, "endOffset": 281}, {"referenceID": 0, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 355, "endOffset": 358}, {"referenceID": 21, "context": "\u2022 LP: Laplacian pyramid-based image fusion approach [14]; \u2022 MWG: multi-scale weighted gradient-based spatial image fusion approach [16]; \u2022 DWT: discrete wavelet transform-based image fusion approach [17]; \u2022 NSCT: non-subsampled contourlet transform-based image fusion approach [18]; \u2022 RPCA: robust principal component analysis-based image fusion approach [1]; \u2022 SR-CM: sparse representation \u201cchoose-max\u201d-based image fusion approach [23]; \u2022 SR-KSVD: sparse representation K-SVD-based image fusion approach.", "startOffset": 432, "endOffset": 436}, {"referenceID": 53, "context": "For the coupled dictionary learning, the training sets of focused image patches of size 8 consisting of 50,000 patches taken from the USC-SIPI image database [55] of 40 natural images, and 50,000 blurred patches of the same size, which are created from focused patches using Gaussian blur function, are used.", "startOffset": 158, "endOffset": 162}, {"referenceID": 52, "context": "Visual Comparison We conduct our fusion experiments over the standard multi-focus dataset [54], and show some representative fusion results in Figs.", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "We address the multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. Algorithms for this problem necessarily admit the source image characteristics along with focused and blurred feature. However, most sparsity-based approaches use a single dictionary in focused feature space to describe multi-focus images, and ignore the representations in blurred feature space. Here, we propose a multi-focus image fusion approach based on coupled sparse representation. The approach exploits the facts that (i) the patches in given training set can be sparsely represented by a couple of overcomplete dictionaries related to the focused and blurred categories of images; and (ii) merging such representations leads to a more flexible and therefore better fusion strategy than the one based on just selecting the sparsest representation in the original image estimate. By jointly learning the coupled dictionary, we enforce the similarity of sparse representations in the focused and blurred feature spaces, and then introduce a fusion approach to combine these representations for generating an all-in-focus image. We also discuss the advantages of the fusion approach based on coupled sparse representation and present an efficient algorithm for learning the coupled dictionary. Extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}