{"id": "1705.10610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition", "abstract": "this overview paper presents a streamlined state - product of - the -! art modeling system for vietnamese airlines named entity language recognition ( chinese ner ). by fully incorporating automatic syntactic searching features grouped with overlapping word embeddings as input for bidirectional finite long array short - program term memory ( bi - lstm ), our delivery system, although progressively simpler than some deep learning architectures, achieves a overall much dramatically better result for indigenous vietnamese vendor ner. the initial proposed method achieves, an overall f1 score pool of 92. 05 % on selecting the user test acceptance set of china an evaluation media campaign, organized in late june 2016 by consulting the indigenous vietnamese language and speech processing ( ie vlsp ) multimedia community. our proprietary named collaborative entity recognition retrieval system outperforms the best previous interactive systems considered for using vietnamese ner users by allowing a large margin.", "histories": [["v1", "Mon, 29 May 2017 08:10:15 GMT  (153kb)", "http://arxiv.org/abs/1705.10610v1", "7 pages, 3 figures. arXiv admin note: substantial text overlap witharXiv:1705.04044"], ["v2", "Wed, 31 May 2017 16:23:40 GMT  (153kb)", "http://arxiv.org/abs/1705.10610v2", "7 pages, 3 figures, resolve text overlapping. arXiv admin note: text overlap witharXiv:1705.04044"], ["v3", "Thu, 8 Jun 2017 16:17:49 GMT  (152kb)", "http://arxiv.org/abs/1705.10610v3", "7 pages, 3 figures, submitted to PACLIC 2017. arXiv admin note: text overlap witharXiv:1705.04044"], ["v4", "Mon, 28 Aug 2017 02:32:16 GMT  (153kb)", "http://arxiv.org/abs/1705.10610v4", "7 pages, 9 tables, 3 figures, accepted to PACLIC 2017"]], "COMMENTS": "7 pages, 3 figures. arXiv admin note: substantial text overlap witharXiv:1705.04044", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thai-hoang pham", "phuong le-hong"], "accepted": false, "id": "1705.10610"}, "pdf": {"name": "1705.10610.pdf", "metadata": {"source": "CRF", "title": "The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 61\n0v 1\n[ cs\n.C L\n] 2\n9 M\nThis paper presents a state-of-the-art system for Vietnamese Named Entity Recognition (NER). By incorporating automatic syntactic features with word embeddings as input for bidirectional Long Short-Term Memory (BiLSTM), our system, although simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall F1 score of 92.05% on the test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our named entity recognition system outperforms the best previous systems for Vietnamese NER by a large margin."}, {"heading": "1 Introduction", "text": "Named entity recognition (NER) is a fundamental task in natural language processing (NLP) and understanding. The purposes of this task are identifying noun phrases and classifying each of them into a predefined class. NER is a crucial preprocessing step used in some NLP applications such as information extraction, question answering, automatic translation, speech processing, and biomedical science. In two shared-tasks, CoNLL 20021 and CoNLL 20032, language independent NER systems were evaluated for English, German, Spanish, and Dutch. These systems considered four named entity\n1 http://www.cnts.ua.ac.be/conll2002/ner/ 2 http://www.cnts.ua.ac.be/conll2003/ner/\ntypes, including names of persons, organizations, locations, and names of miscellaneous entities that do not belong to these three types.\nMore recently, the Vietnamese Language and Speech Processing (VLSP)3 community has organized an evaluation campaign to systematically compare NER systems for the Vietnamese language. Similar to the CoNLL 2003 share task, four named entity types are evaluated: person (PER), organization (ORG), location (LOC), and miscellaneous entity (MISC). The data are collected from electronic newspapers published on the web.\nIn this paper, we present a state-of-the-art NER system for Vietnamese language that uses automatic syntactic features with word embedding in BiLSTM. Our system outperforms the first-rank system of the VLSP campaign that used many syntactic and hand-crafted features and an end-to-end system described in (Pham and Le-Hong, 2017) that is a combination of Bi-LSTM, Convolutional Neural Network (CNN), and Conditional Random Field (CRF) about 3%. In summary, the overall F1 score of our system is 92.05% on the standard test set provided by the organizing committee of the evaluation campaign. The contributions of this work include:\n\u2022 We propose a deep learning model which\ngives the state-of-the-art performance on a standard NER data set for Vietnamese. By incorporating automatic syntactic features, our system (Bi-LSTM), although simpler than (Bi-LSTM-CNN-CRF) model described in (Pham and Le-Hong, 2017), achieves a\n3 http://vlsp.org.vn/\nmuch better result on Vietnamese NER dataset. The simple architecture also contributes to the feasibility of our system in practice because it requires less time for inference stage. Our best system utilizes part-of-speech, chunk, and regular expression type features with word embeddings as an input for two-layer Bi-LSTM model, which achieves an F1 score of 92.05%.\n\u2022 We demonstrate the greater importance of syn-\ntactic features in Vietnamese NER compared to their impact in other languages. Those features help improve the F1 score of about 18%.\n\u2022 We give an extensive empirical study on us-\ning common deep learning models for Vietnamese NER, including Recurrent Neural Network (RNN), unidirectional and bidirectional LSTM. These models are also compared to conventional sequence labelling models such as Maximum Entropy Markov models (MEMM).\n\u2022 We make our NER system open source for re-\nsearch purpose, which is believed to be a good contribution to the future development of Vietnamese NER in particular and Vietnamese language processing research in general.\nThe remainder of this paper is structured as follows. Section 2 summarizes related work on NER. Section 3 describes features and model used in our system. Section 4 gives experimental results and discussions. Finally, Section 5 concludes the paper."}, {"heading": "2 Related Works", "text": "Within the large body of research on NER which have been published in the last two decades, we identify two main approaches. The first approach is characterized by the use of traditional sequence labelling models such as CRF, hidden markov model, support vector machine, maximum entropy that are heavily dependent on hand-crafted features (Florian et al., 2003; Lin and Wu, 2009; Durrett and Klein, 2014; Luo and Xiaojiang Huang, 2015). These systems attempted to use information other than the available training data such as gazetteers and unannotated data.\nIn the last few years, several neural architectures have been proposed for NER task. These methods have a long story, but they have been focused only recently by the advance of computational power and high-quality word embeddings. Firstly, (Collobert et al., 2011) used a CNN over a sequence of word embeddings with a conditional random field on the top. This model achieved near state-of-the-art results on some sequence labelling tasks such as POS tagging, chunking, and NER. From 2015 until now, the LSTM model has been the best approach for many sequence labelling tasks. (Huang et al., 2015) used Bi-LSTM with CRF layer for joint decoding. Instead of using hand-crafted feature as (Huang et al., 2015), (Chiu and Nichols, 2016) proposed a hybrid model that combined BiLSTM with CNN to learn both characterlevel and word-level representations. Unlike (Chiu and Nichols, 2016), (Lample et al., 2016) used BI-LSTM to model both character and word-level information.\nFor Vietnamese, VLSP community has organized an evaluation campaign that follows the rules of CoNLL 2003 shared task to systematically compare NER systems. The data are collected from electronic newspapers published on the web. Participating systems have approached this task by both traditional and deep learning architectures. In particular, the first-rank system of the VLSP campaign which achieved an F1 score of 88.78% used MEMM with many hand-crafted features (Le-Hong, 2016). Meanwhile, (Nguyen et al., 2016) applied deep learning approach for this task. They used the implementation provided by (Lample et al., 2016). There are two types of LSTM models in this open source software: Bi-LSTM-CRF and Stack-LSTM. Their best system achieved an F1 score of 83.80%. More recently, (Pham and Le-Hong, 2017) used an end-toend system that is a combination of Bi-LSTM-CNNCRF for Vietnamese NER. The F1 score of this system is 88.59% that is competitive with the accuracy of (Le-Hong, 2016)."}, {"heading": "3 Methodology", "text": ""}, {"heading": "3.1 Feature Engineering", "text": "Word Embeddings To create skip-gram vectors for Vietnamese word, we use a dataset consisting of 7.3GB of text from 2 million articles collected through a Vietnamese news portal.4 The text is first normalized to lower case and all special characters are removed. Common symbols such as comma, semicolon, colon, full stop and percentage sign are replaced with a special token punct . All numeral sequences are replaced with a special token number, so that correlations between certain words and numbers are correctly recognized by the neural network or the log-bilinear regression model.\nEach word in the Vietnamese language may consist of more than one syllables with spaces in between, which could be regarded as multiple words by the unsupervised models. Hence it is necessary to replace the spaces within each word with underscores to create full word tokens. The tokenization process follows the method described in (Le-Hong et al., 2008).\nAutomatic Syntactic Features To ameliorate a performance of our system, we incorporate some syntactic features with word embeddings as input for Bi-LSTM model. These syntactic features are generated automatically by some public tools so the actual input of our system is only raw texts. These additional features consist of part-of-speech (POS) and chunk tags that are available in the dataset, and regular expression types that capture common organization and location names. These regular expressions over tokens described particularly in (Le-Hong, 2016) are shown to provide helpful features for classifying candidate named entities, as shown in the experiments."}, {"heading": "3.2 Long Short-Term Memory", "text": "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of Recurrent Neural Network (RNN) which is designed to deal with gradient vanishing and exploding problems (Bengio et al., 1994; Pascanu et al., 2013) when learning with long-range sequences. LSTM networks are the same as RNN,\n4 http://www.baomoi.com\nexcept that the hidden layer updates are replaced by memory cells. Basically, a memory cell unit is composed of three multiplicative gates that control the proportions of information to forget and to pass on to the next time step. As a result, it is better for exploiting long-range dependency data. The memory cell is computed as follows:\nit = \u03c3(Wiht\u22121 + Uixt + bi)\nft = \u03c3(Wfht\u22121 + Ufxt + bf )\nct = ft \u2299 ct\u22121 + it \u2299 tanh(Wcht\u22121 + Ucxt + bc)\not = \u03c3(Woht\u22121 + Uoxt + bo)\nht = ot \u2299 tanh(ct)\nwhere \u03c3 is the element-wise sigmoid function and\u2299 is the element-wise product, i, f, o and c are the input gate, forget gate, output gate and cell vector respectively. Ui,Uf ,Uc,Uo are connection weight matrices between input x and gates, and Ui,Uf ,Uc,Uo are connection weight matrices between gates and hidden state h. bi,bf ,bc,bo are the bias vectors. Figure 1 illustrates a single LSTM memory cell."}, {"heading": "3.3 Bidirectional Long Short-Term Memory", "text": "The original LSTM uses only past features. For many sequence labelling tasks, it is beneficial when accessing both past and future contexts. For this reason, we utilize the bidirectional LSTM (Bi-LSTM) (Graves and Schmidhuber, 2005; Graves et al., 2013) for NER task. The basic idea is running both forward and backward passes to\ncapture past and future information, respectively, and concatenate two hidden states to form a final representation. Figure 2 illustrates the backward and forward passes of Bi-LSTM."}, {"heading": "3.4 Our Deep Learning Model", "text": "For Vietnamese named entity recognition, we use a 2-layer Bi-LSTM with softmax layer on the top to detect named entities in sequence of sentences. The inputs are the combination of word and syntactic features, and the outputs are the probability distributions over named entity tags. Figure 3 describes the details of our deep learning model. In the next sections, we present our experimental results."}, {"heading": "4 Results And Discussions", "text": ""}, {"heading": "4.1 VLSP Corpus", "text": "We evaluate our system on the VSLP NER shared task 2016 corpus. This corpus consists of electronic newspapers published on the web. There are four named entity types in this corpus, names of person, location, organization and other named entities. Four types of named entities are compatible with their descriptions in the CoNLL shared task 2003.\nThe automatic POS and chunking tags are available in this dataset. The format of this corpus follows that of the CoNLL 2003 shared task. It consists of five columns. The order of these columns are word, POS tag, chunking tag, named entity label, and nested named entity label. Our system focuses on only named entity without nesting, so we do not use the fifth column. Named entity labels are annotated using the IOB notation as in the CoNLL shared tasks. There are 9 labels: B-PER and I-PER are used for persons, B-ORG and I-ORG are used\nfor organizations, B-LOC and I-LOC are used for locations, B-MISC and I-MISC are used for other named entities and O is used for other elements. Table 1 shows the quantity of named entity annotated in the training set and the test set.\nMoreover, we take one part of training data for validation. The number of sentences of each data set used is described in Table 2."}, {"heading": "4.2 Evaluation Method", "text": "The performance is measured with F1 score:\nF1 = 2 \u2217 precision \u2217 recall\nprecision+ recall\nPrecision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file. The performance of our system is evaluated by the automatic evaluation script of the CoNLL 2003 shared task.5"}, {"heading": "4.3 Results", "text": "In this section, we analyze the efficiency of word embeddings, bidirectional learning, model configuration, and especially automatic syntactic features.\n5 http://www.cnts.ua.ac.be/conll2003/ner/\nEmbeddings To evaluate the effectiveness of word embeddings, we compare the systems on three types of input: skip-gram, random vector, and onehot vector.\nThe number of dimensions we choose for word embedding is 300. For words that appear in VLSP corpus but not appear in word embeddings set, we create random vectors for these words by uniformly sampling from the range [\u2212 \u221a 3\ndim ,+\n\u221a\n3\ndim ] where\ndim is the dimension of embeddings. For random vector setting, we also sample vectors for all words from this distribution. The performances of the system with each input type are represented in Table 3.\nWe can conclude that word embedding is an important factor of our model. Skip-gram vector significantly improves our performance. The improvement is about 11% when using skip-gram vectors instead of random vectors. Thus, we use skip-gram vectors as inputs for our system.\nEffect of Bidirectional Learning In the second experiment, we examine the benefit of accessing both past and future contexts by comparing the per-\nformances of RNN, LSTM and Bi-LSTM models. In this task, RNN model fails because it faces the gradient vanishing/exploding problem when training with long-range dependencies (132 time steps), leading to the unstable value of the cost functions. For this reason, only performances of LSTM and BiLSTM models are shown in Table 4.\nWe see that learning both past and future contexts is very useful for NER. Performances of all of the entity types are increased, especially for ORG and MISC. The total accuracy is improved greatly, from 65.80% to 74.02%.\nNumber of Bi-LSTM Layers In the third experiment, we investigate the improvement when adding more Bi-LSTM layers. Table 5 shows the accuracy when using one or two Bi-LSTM layers. We observe a significant improvement when using two layers of Bi-LSTM. The performance is increased from 71.74% to 74.02%\nEffect of Dropout In the fourth experiment, we compare the results of our model with and without dropout layers. The optimal dropout ratio for our experiments is 0.5. The accuracy with dropout is 74.02%, compared to 68.27% without dropout. It proves the effectiveness of dropout for preventing overfitting.\nSyntactic Features Integration As shown in the previous experiments, using only word features in deep learning models is not enough to achieve the state-of-the-art result. In particular, the accuracy of this model is only 74.02%. This result is far lower in comparison to that of state-of-the-art systems for Vietnamese NER. In the following experiments, we add more useful features to enhance the performance of our deep learning model. Table 7 shows the improvement when adding part-of-speech, chunk, case-sensitive, and regular expression features.\nAs seen in this table, adding each of these syntactic features helps improve the performance significantly. The best result we get is adding part-ofspeech, chunk and regular expression features. The accuracy of this final system is 92.05% that is much higher than 74.02 of the system without using syntactic features. An explanation for this problem is possibly a characteristic of Vietnamese. In particular, Vietnamese named entities are often a noun\nphrase chunk.\nComparison with Previous Systems In VLSP 2016 workshop, several different systems have been proposed for Vietnamese NER. In this campaign, they have evaluated over three entities types LOC, ORG, PER. In all fairness, we also evaluate our performances over these tags on the same training and test set. The accuracy of our best model over three entity types is 92.02%, which is higher than the best participating system (Le-Hong, 2016) in that shared task about 3.2%. Moreover, (Pham and Le-Hong, 2017) used a combination of Bi-LSTM, CNN, and CRF that achieved the same performance with (Le-Hong, 2016). This system is end-to-end architecture that required only word embeddings while (Le-Hong, 2016) used many syntactic and hand-crafted features with MEMM. Our system surpasses both of these systems by using Bi-LSTM with automatically syntactic features that takes less time for training and inference than Bi-LSTMCNN-CRF model and does not depend on many hand-crafted features as MEMM. Table 8 presents the accuracy of each system."}, {"heading": "5 Conclusion", "text": "In this work, we have presented a state-of-the-art named entity recognition system for the Vietnamese language, which achieves an F1 score of 92.05% on the standard corpus published recently by the Vietnamese Language and Speech community. Our\nsystem outperforms the first-rank system of the related NER shared task with a large margin, 3.2% in particular. We have also shown the effectiveness of using automatic syntactic features for BiLSTM model that surpass the combination of BiLSTM-CNN-CRF models albeit requiring less time for computation."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2016] Jason P.C. Chiu", "Eric Nichols"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Chiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A joint model for entity analysis: Coreference", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": "typing, and linking. Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "Proceedings of CoNLL-2003,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Framewise phoneme classification with bidirectional lstm networks", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of 2005 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alex Graves", "Abdel rahmand Mohamed", "Geoffrey Hinton"], "venue": "In Proceedings of 2013 IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "A hybrid approach to word segmentation of Vietnamese texts", "author": ["Thi Minh Huyen Nguyen", "AzimRoussanaly", "TuongVinh Ho"], "venue": "In Language and Automata Theory and Applications,", "citeRegEx": "Le.Hong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Le.Hong et al\\.", "year": 2008}, {"title": "Vietnamese named entity recognition using token regular expressions and bidirectional inference", "author": ["Phuong Le-Hong"], "venue": "In Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Processing,", "citeRegEx": "Le.Hong.,? \\Q2016\\E", "shortCiteRegEx": "Le.Hong.", "year": 2016}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Joint entity recognition and disambiguation", "author": ["Luo", "Xiaojiang Huang2015] Gang Luo", "Zaiqing Nie Xiaojiang Huang", "Chin-Yew Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Vietnamese named entity recognition at vlsp 2016 evaluation campaign", "author": ["Le Minh Nguyen", "Xuan Chien Tran"], "venue": "In Proceedings of The Fourth International Workshop on Vietnamese Language and Speech Pro-", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "End-to-end recurrent neural network models for vietnamese named entity recognition: Word-level vs. character-level", "author": ["Pham", "Le-Hong2017] Thai-Hoang Pham", "Phuong Le-Hong"], "venue": "arXiv preprint arXiv:1705.04044", "citeRegEx": "Pham et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "The first approach is characterized by the use of traditional sequence labelling models such as CRF, hidden markov model, support vector machine, maximum entropy that are heavily dependent on hand-crafted features (Florian et al., 2003; Lin and Wu, 2009; Durrett and Klein, 2014; Luo and Xiaojiang Huang, 2015).", "startOffset": 214, "endOffset": 310}, {"referenceID": 2, "context": "Firstly, (Collobert et al., 2011) used a CNN over a sequence of word embeddings with a conditional random field on the top.", "startOffset": 9, "endOffset": 33}, {"referenceID": 8, "context": "(Huang et al., 2015) used Bi-LSTM with CRF layer for joint decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Instead of using hand-crafted feature as (Huang et al., 2015), (Chiu and Nichols, 2016) proposed a hybrid model that combined BiLSTM with CNN to learn both characterlevel and word-level representations.", "startOffset": 41, "endOffset": 61}, {"referenceID": 9, "context": "Unlike (Chiu and Nichols, 2016), (Lample et al., 2016) used BI-LSTM to model both character and word-level information.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "78% used MEMM with many hand-crafted features (Le-Hong, 2016).", "startOffset": 46, "endOffset": 61}, {"referenceID": 14, "context": "Meanwhile, (Nguyen et al., 2016) applied deep learning approach for this task.", "startOffset": 11, "endOffset": 32}, {"referenceID": 9, "context": "They used the implementation provided by (Lample et al., 2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 11, "context": "59% that is competitive with the accuracy of (Le-Hong, 2016).", "startOffset": 45, "endOffset": 60}, {"referenceID": 10, "context": "The tokenization process follows the method described in (Le-Hong et al., 2008).", "startOffset": 57, "endOffset": 79}, {"referenceID": 11, "context": "These regular expressions over tokens described particularly in (Le-Hong, 2016) are shown to provide helpful features for classifying candidate named entities, as shown in the experiments.", "startOffset": 64, "endOffset": 79}, {"referenceID": 0, "context": "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of Recurrent Neural Network (RNN) which is designed to deal with gradient vanishing and exploding problems (Bengio et al., 1994; Pascanu et al., 2013) when learning with long-range sequences.", "startOffset": 185, "endOffset": 228}, {"referenceID": 15, "context": "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of Recurrent Neural Network (RNN) which is designed to deal with gradient vanishing and exploding problems (Bengio et al., 1994; Pascanu et al., 2013) when learning with long-range sequences.", "startOffset": 185, "endOffset": 228}, {"referenceID": 6, "context": "For this reason, we utilize the bidirectional LSTM (Bi-LSTM) (Graves and Schmidhuber, 2005; Graves et al., 2013) for NER task.", "startOffset": 61, "endOffset": 112}, {"referenceID": 11, "context": "02%, which is higher than the best participating system (Le-Hong, 2016) in that shared task about 3.", "startOffset": 56, "endOffset": 71}, {"referenceID": 11, "context": "Moreover, (Pham and Le-Hong, 2017) used a combination of Bi-LSTM, CNN, and CRF that achieved the same performance with (Le-Hong, 2016).", "startOffset": 119, "endOffset": 134}, {"referenceID": 11, "context": "This system is end-to-end architecture that required only word embeddings while (Le-Hong, 2016) used many syntactic and hand-crafted features with MEMM.", "startOffset": 80, "endOffset": 95}, {"referenceID": 11, "context": "Models Types F1 (Le-Hong, 2016) ME 88.", "startOffset": 16, "endOffset": 31}], "year": 2017, "abstractText": "This paper presents a state-of-the-art system for Vietnamese Named Entity Recognition (NER). By incorporating automatic syntactic features with word embeddings as input for bidirectional Long Short-Term Memory (BiLSTM), our system, although simpler than some deep learning architectures, achieves a much better result for Vietnamese NER. The proposed method achieves an overall F1 score of 92.05% on the test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. Our named entity recognition system outperforms the best previous systems for Vietnamese NER by a large margin.", "creator": "LaTeX with hyperref package"}}}