{"id": "1705.10814", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages", "abstract": "we present a symmetric transition - based type dependency parser that periodically uses a special convolutional input neural network to compose word representations originally from output characters. the combined character composition complexity model shows great historical improvement over the word - lookup model, especially given for software parsing agglutinative tag languages. these theoretical improvements are even better than using pre - trained batch word embeddings harvested from storage extra data. on assuming the spmrl tree data support sets, although our system largely outperforms surpassing the popular previous most best greedy parser ( ballesteros martinez et in al., 2015 ) by exhibiting a weighted margin curve of 0 3 % reduction on average.", "histories": [["v1", "Tue, 30 May 2017 18:23:18 GMT  (28kb)", "http://arxiv.org/abs/1705.10814v1", "Accepted in ACL 2017 (Short)"]], "COMMENTS": "Accepted in ACL 2017 (Short)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiang yu", "ngoc thang vu"], "accepted": true, "id": "1705.10814"}, "pdf": {"name": "1705.10814.pdf", "metadata": {"source": "CRF", "title": "Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages", "authors": ["Xiang Yu"], "emails": ["xiangyu@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 81\n4v 1\n[ cs\n.C L\n] 3\n0 M\nay 2\n01 7"}, {"heading": "1 Introduction", "text": "As with many other NLP tasks, dependency parsing also suffers from the out-of-vocabulary (OOV) problem, and probably more than others since training data with syntactical annotation is usually scarce. This problem is particularly severe when the target is a morphologically rich language. For example, in the SPMRL shared task data sets (Seddah et al., 2013, 2014), 4 out of 9 treebanks contain more than 40% word types in the development set that are never seen in the training set.\nOne way to tackle the OOV problem is to pretrain the word embeddings, e.g., with word2vec (Mikolov et al., 2013), from a large set of unlabeled data. This comes with two main advantages: (1) more word types, which means that the vocabulary is extended by the unlabeled data, so that some of the OOVwords now have a learned representation; (2) more word tokens per type, which means that the syntactic and semantic similarities of the words are better modeled than only using the parser training data.\n1The parser is available at http://www.ims. uni-stuttgart.de/institut/mitarbeiter/ xiangyu/index.en.html\nPre-trained word embeddings can alleviate the OOV problem by expanding the vocabulary, but it does not model the morphological information. Instead of looking up word embeddings, many researchers propose to compose the word representation from characters for various tasks, e.g., part-of-speech tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimar\u00e3es, 2015), language modeling (Ling et al., 2015), machine translation (Costa-juss\u00e0 and Fonollosa, 2016). In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing. Kim et al. (2016) present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that \u201cit remains open as to which character composition model (i.e., LSTM or CNN) performs better\u201d.\nWe propose to apply the CNN model by Kim et al. (2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al., 2015). This model requires no extra unlabeled data but performs better than using pre-trained word embeddings. Furthermore, it can be combined with word embeddings from the lookup table since they capture different aspects of word similarities.\nExperimental results show that the CNN model works especially well on agglutinative languages, where the OOV rates are high. On other morphologically rich languages, the CNN model also performs at least as good as the word-lookup model.\nFurthermore, our CNNmodel outperforms both the original and our re-implementation of the bidirectional LSTMmodel by Ballesteros et al. (2015) by a large margin. It provides empirical evidence to the aforementioned open question, suggesting that the CNN is the better character composition model for dependency parsing."}, {"heading": "2 Parsing Models", "text": ""}, {"heading": "2.1 Baseline Parsing Model", "text": "As the baseline parsing model, we re-implement the greedy parser in Weiss et al. (2015) with some modifications, which brings about 0.5% improvement, outlined below.2\nSince most treebanks contain non-projective trees, we use an approximate non-projective transition system similar to Attardi (2006). It has two additional transitions (LEFT-2 and RIGHT-2) to the Arc-Standard system (Nivre, 2004) that attach the top of the stack to the third token on the stack, or vice versa. We also extend the feature templates in Weiss et al. (2015) by extracting the children of the third token in the stack. The complete transitions and feature templates are listed in Appendix A.\nThe feature templates consist of 24 tokens in the configuration, we look up the embeddings of the word forms, POS tags and dependency labels of each token.3 We then concatenate the embeddings Eword(ti), Etag(ti), Elabel(ti) for each token ti, and use a dense layer with ReLU non-linearity to obtain a compact representation f(ti) of the token:\nx(ti) = [Eword (ti);Etag(ti);Elabel(ti)] (1)\nf(ti) = max{0,Wfx(ti) + bf}\nWe concatenate the representations of the tokens and feed them through two hidden layers with ReLU non-linearity, and finally into the softmax layer to compute the probability of each transition:\nh0 = [f(t1); f(t2); ...; f(t24)]\nh1 = max{0,W1h0 + b1}\nh2 = max{0,W2h1 + b2}\np(\u22c5|t1, ..., t24) = sof tmax(W3h2 + b3)\nEword , Etag, Elabel, Wf , W1, W2, W3, bf , b1, b2, b3 are all the parameters that will be learned through back propagation with averaged stochastic gradient descent in mini-batches.\nNote that Weiss et al. (2015) directly concatenate the embeddings of the words, tags, and labels of all the tokens together as input to the hidden layer. Instead, we first group the embeddings of the word, tag, and label of each token and compute\n2We only experiment with the greedy parser, since this paper focuses on the character-level input features and is independent of the global training and inference as in Weiss et al. (2015); Andor et al. (2016).\n3The tokens in the stack and buffer do not have labels yet, we use a special label <NOLABEL> instead.\nan intermediate representation with shared parameters, then concatenate all the representations as input to the hidden layer."}, {"heading": "2.2 LSTM Character Composition Model", "text": "To tackle the OOV problem, wewant to replace the word-lookup table with a function that composes the word representation from characters.\nAs a baseline character model, we re-implement the bidirectional LSTM character composition model following Ballesteros et al. (2015). We replace the lookup table Eword in the baseline parser with the final outputs of the forward and backward LSTMs \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6lstm and \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d7lstm. Equation (1) is then replaced with\nx(ti) = [ \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6lstm(ti); \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d7lstm(ti);Etag(ti);Elabel(ti)].\nWe refer the readers to Ling et al. (2015) for the details of the bidirectional LSTM."}, {"heading": "2.3 CNN Character Composition Model", "text": "In contrast to the LSTM model, we propose to use a \u201cflat\u201d CNN as the character composition model, similar to Kim et al. (2016).4\nEquation (1) is thus replaced with\nx(ti) =[cnn l1 (ti); cnn l2 (ti); ...; cnn lk(ti);\nEtag(ti);Elabel(ti)]. (2)\nConcretely, the input of the model is a concatenated matrix of character embeddings C \u2208 \u211ddi\u00d7w, where di is the dimensionality of character embeddings (number of input channels) and w is the length of the padded word.5 We apply k convolutional kernels  \u2208 \u211ddo\u00d7di\u00d7lk with ReLU nonlinearity on the input, where do is the number of output channels and lk is the length of the kernel. The output of the convolution operation is Oconv \u2208 \u211d\ndo\u00d7(l\u2212k+1), and we apply a max-overtime pooling that takes the maximum activations of the kernel along each channel, obtaining the final output Ofinal \u2208 \u211d\ndo , which corresponds to the most salient n-gram representation of the word, denoted cnnlk in Equation (2). We then concatenate the outputs of several such CNNs with different lengths, so that the information from different n-grams are extracted and can interact with each other.\n4We do not use the highway networks since it did not improve the performance in preliminary experiments.\n5The details of the padding is in Appendix B."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Setup", "text": "We conduct our experiments on the treebanks from the SPMRL 2014 shared task (Seddah et al., 2013, 2014), which includes 9 morphologically rich languages: Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish. All the treebanks are split into training, development, and test sets by the shared task organizers. We use the fine-grained predicted POS tags provided by the organizers, and evaluate the labeled attachment scores (LAS) including punctuation.\nWe experiment with the CNN-based character composition model (CNN) along with several baselines. The first baseline (WORD) uses the wordlookup model described in Section 2.1 with randomly initialized word embeddings. The second baseline (W2V) uses pre-trained word embeddings by word2vec (Mikolov et al., 2013) with the CBOW model and default parameters on the unlabeled texts from the shared task organizers. The third baseline (LSTM) uses a bidirectional LSTM as the character composition model following Ballesteros et al. (2015). Appendix C lists the hyper-parameters of all the models.\nFurther analysis suggests that combining the character composition models with word-lookup models could be beneficial since they capture different aspects of word similarities (orthographic vs. syntactic/semantic). We therefore experiment with four combined models in two groups: (1) randomly initialized word embeddings (LSTM+WORD vs. CNN+WORD), and (2) pre-trained word embeddings (LSTM+W2V vs. CNN+W2V).\nThe experimental results are shown in Table 1, with Int denoting internal comparisons (with three groups) and Ext denoting external comparisons, the highest LAS in each group is marked in bold face."}, {"heading": "3.2 Internal Comparisons", "text": "In the first group, we compare the LAS of the four single models WORD, W2V, LSTM, and CNN. In macro average of all languages, the CNN model performs 2.17% higher than the WORD model, and 1.24% higher than the W2V model. The LSTM model, however, performs only 0.9% higher than the WORD model and 1.27% lower than the CNN model.\nThe CNN model shows large improvement in four languages: three agglutinative languages (Basque, Hungarian, Korean), and one highly inflected fusional language (Polish). They all have high OOV rate, thus difficult for the baseline parser that does not model morphological information. Also, morphemes in agglutinative languages tend to have unique, unambiguous meanings, thus easier for the convolutional kernels to capture.\nIn the second group, we observe that the additional word-lookup model does not significantly improve the CNN moodel (from 82.75% in CNN to 82.90% in CNN+WORD on average) while the LSTMmodel is improved by a much larger margin (from 81.48% in LSTM to 82.56% in LSTM+WORD on average). This suggests that the CNN model has already learned the most important information from the the word forms, while the LSTM model has not. Also, the combined CNN+WORD model is still better than the LSTM+WORD model, despite the large improvement in the latter.\nIn the third group where pre-trained word embeddings are used, combining CNN with W2V brings an extra 0.75% LAS over the CNN model. Combining LSTM with W2V shows similar trend but with much larger improvement, yet again, CNN+W2V outperforms LSTM+W2V both on average and individually in 8 out of 9 languages."}, {"heading": "3.3 External Comparisons", "text": "We also report the results of the two models from Ballesteros et al. (2015): B15-WORD with randomly initialized word embeddings and B15-LSTM as their proposed model. Finally, we report the best published results (BestPub) on this data set (Bj\u00f6rkelund et al., 2013, 2014).\nOn average, the B15-LSTM model improves their own baseline by 1.1%, similar to the 0.9% improvement of our LSTM model, which is much smaller than the 2.17% improvement of the CNN model. Furthermore, the CNN model is improved from a strong baseline: our WORDmodel performs already 2.22% higher than the B15-WORDmodel.\nComparing the individual performances on each language, we observe that the CNN model almost always outperforms the WORD model except for Hebrew. However, both LSTM and B15-LSTM perform higher than baseline only on the three agglutinative languages (Basque, Hungarian, and Korean), and lower than baseline on the other six.\nBallesteros et al. (2015) do not compare the effect of adding a word-lookup model to the LSTM model as in our second group of internal comparisons. However, Plank et al. (2016) show that combining the same LSTM character composition model with word-lookup model improves the performance of POS tagging by a very large margin. This partially confirms our hypothesis that the LSTMmodel does not learn sufficient information from the word forms.\nConsidering both internal and external comparisons in both average and individual performances, we argue that CNN is more suitable than LSTM as character composition model for parsing.\nWhile comparing to the best published results\n(Bj\u00f6rkelund et al., 2013, 2014), we have to note that their approach uses explicit morphological features, ensemble, ranking, etc., which all can boost parsing performance. We only use a greedy parser with much fewer features, but bridge the 6 points gap between the previous best greedy parser and the best published result bymore than one half."}, {"heading": "3.4 Discussion on CNN and LSTM", "text": "We conjecture that the main reason for the better performance of CNN over LSTM is its flexibility in processing sub-word information. The CNN model uses different kernels to capture ngrams of different lengths. In our setting, a kernel with a minimum length of 3 can capture short morphemes; and with a maximum length of 9, it can practically capture a normal word. With the flexibility of capturing patterns from morphemes up to words, the CNNmodel almost always outperforms the word-lookup model.\nIn theory, LSTM has the ability to model much longer sequences, however, it is composed step by step with recurrence. For such deep network architectures, more data would be required to learn the same sequence, in comparison to CNN which can directly use a large kernel to match the pattern. For dependency parsing, training data is usually scarce, this could be the reason that the LSTM has not utilized its full potential."}, {"heading": "3.5 Analyses on OOV and Morphology", "text": "The motivation for using character composition models is based on the hypothesis that it can address the OOV problem. To verify the hypothesis, we analyze the LAS improvements of the CNN and LSTMmodel on the development sets in two cases: (1) both the head and the dependent are in vocabu-\nlary or (2) at least one of them is out of vocabulary. Table 2 shows the results, where the two cases are denoted as \u0394IV and \u0394OOV. The general trend in the results is that the improvements of both models in the OOV case are larger than in the IV case, which means that the character composition models indeed alleviates the OOV problem. Also, CNN improves on seven languages in the IV case and eight languages in the OOV case, and it performs consistently better than LSTM in both cases.\nTo analyze the informativeness of the morphemes at different positions, we conduct an ablation experiment. We split each word equally into three thirds, approximating the prefix, stem, and suffix. Based on that, we construct six modified versions of the development sets, in which we mask one or two third(s) of the characters in each word. Then we parse them with the CNN models trained on normal data. Table 3 shows the degradations of LAS on the six modified data sets compared to parsing the original data, where the position of \u2663 signifies the location of the masks. The three agglutinative languages Basque, Hungarian, and Korean suffer the most with masked words. In particular, the suffixes are the most informative for parsing in these three languages, since they cause the most loss while masked, and the least loss while unmasked. The pattern is quite different on the other languages, in which the distinction of informativeness among the three parts is much smaller."}, {"heading": "4 Conclusion", "text": "In this paper, we propose to use a CNN to compose word representations from characters for dependency parsing. Experiments show that the CNN model consistently improves the parsing accuracy, especially for agglutinative languages. In an external comparison on the SPMRL data sets, our system outperforms the previous best greedy parser.\nWe also provide empirical evidence and analysis, showing that the CNNmodel indeed alleviates the OOV problem and that it is better suited than the LSTM in dependency parsing."}, {"heading": "Acknowledgements", "text": "This work was supported by the German Research Foundation (DFG) in project D8 of SFB 732. We also thank our collegues in the IMS, especially Anders Bj\u00f6rkelund, for valuable discussions, and the anonymous reviewers for the suggestions."}, {"heading": "A Transitions and Feature Templates", "text": ""}, {"heading": "B Character Input Preprocessing", "text": "For the CNN input, we use a list of characters with fixed length to for batch processing. We add some special symbols apart from the normal alphabets, digits, and punctuations: <SOW> as the start of the word, <EOW> as the end of the word, <MUL> as multiple characters in the middle of the word squeezed into one symbol, <PAD> as padding equally on both sides, and <UNK> as characters unseen in the training data.\nFor example, if we limit the input length to 9, a short word ein will be converted into <PAD><PAD>-<SOW>-e-i-n-<EOW>-<PAD>-<PAD>; a long word pr\u00e4chtiger will be <SOW>-p-r-\u00e4<MUL>-g-e-r-<EOW>. In practice, we set the length as 32, which is long enough for almost all the words."}, {"heading": "C Hyper-Parameters", "text": "The common hyper-parameters of all the models are tuned on the development set in favor of the WORD model:\n\u2022 100,000 training steps with random sampling of mini-batches of size 100; \u2022 test on the development set every 2,000 steps; \u2022 early stop if the LAS on the development does not improve for 3 times in a row; \u2022 learning rate of 0.1, with exponential decay rate of 0.95 for every 2,000 steps; \u2022 L2-regularization rate of 10\u22124; \u2022 averaged SGD with momentum of 0.9; \u2022 parameters are initialized following He et al. (2015); \u2022 dimensionality of the embeddings of each word, tag, and label are 256, 32, 32, respectively; \u2022 dimensionality of the hidden layers are 512, 256; \u2022 dropout on both hidden layers with rate of 0.1; \u2022 total norm constraint of the gradients is 10.\nThe hyper-parameters for the CNN model are:\n\u2022 dimensionality of the character embedding is 32; \u2022 4 convolutional kernels of lengths 3, 5, 7, 9; \u2022 number of output channels of each kernel is 64; \u2022 fixed length for the character input is 32.\nThe hyper-parameters for the LSTM model are:\n\u2022 128 hidden units for both LSTMs; \u2022 all the gates use orthogonal initialization; \u2022 gradient clipping of 10; \u2022 no L2-regularization on the parameters."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Experiments with a multilanguage non-projective dependency parser", "author": ["Giuseppe Attardi."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X). Association for Computational Linguistics, pages 166\u2013170.", "citeRegEx": "Attardi.,? 2006", "shortCiteRegEx": "Attardi.", "year": 2006}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "A. Noah Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "The imswroc\u0142aw-szeged-cis entry at the spmrl 2014 shared task: Reranking and morphosyntax meet unlabeled", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas M\u00fcller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "re)ranking meets morphosyntax: State-of-the-art results from the spmrl 2013 shared task", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker."], "venue": "Proceedings of the FourthWorkshop on Statistical Pars-", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2013", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["R. Marta Costa-juss\u00e0", "R. Jos\u00e9 A. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Cicero dos Santos", "Victor Guimar\u00e3es."], "venue": "Proceedings of the Fifth Named Entity Workshop. Association for Computational Linguistics, pages 25\u201333.", "citeRegEx": "Santos and Guimar\u00e3es.,? 2015", "shortCiteRegEx": "Santos and Guimar\u00e3es.", "year": 2015}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero dos Santos", "Bianca Zadrozny."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14). pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performanceon imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE international conference on computer vision. pages 1026\u20131034.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "AlexanderM. Rush."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA.. AAAI Press, pages", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "W. Alan Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceed-", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. http://aclweb.org/anthology/W04-0308.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Introducing the spmrl 2014 shared task on parsing morphologically-rich languages", "author": ["Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages", "citeRegEx": "Seddah et al\\.,? 2014", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Overview of the spmrl", "author": ["Habash", "Marco Kuhlmann", "WolfgangMaier", "Joakim Nivre", "Adam Przepi\u00f3rkowski", "Ryan Roth", "Wolfgang Seeker", "Yannick Versley", "Veronika Vincze", "Marcin Woli\u0144ski", "Alina Wr\u00f3blewska", "Villemonte Eric de la Clergerie"], "venue": null, "citeRegEx": "Habash et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2013}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceed-", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et al., 2015) by a margin of 3% on average.", "startOffset": 79, "endOffset": 105}, {"referenceID": 12, "context": ", with word2vec (Mikolov et al., 2013), from a large set of unlabeled data.", "startOffset": 16, "endOffset": 38}, {"referenceID": 14, "context": ", part-of-speech tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimar\u00e3es, 2015), language modeling (Ling et al.", "startOffset": 25, "endOffset": 76}, {"referenceID": 11, "context": ", 2016), named entity recognition (dos Santos and Guimar\u00e3es, 2015), language modeling (Ling et al., 2015), machine translation (Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 86, "endOffset": 105}, {"referenceID": 6, "context": ", 2015), machine translation (Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 29, "endOffset": 62}, {"referenceID": 2, "context": "In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing.", "startOffset": 15, "endOffset": 41}, {"referenceID": 2, "context": "In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing. Kim et al. (2016) present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that \u201cit remains open as to which character composition model (i.", "startOffset": 15, "endOffset": 149}, {"referenceID": 5, "context": "(2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 88, "endOffset": 132}, {"referenceID": 17, "context": "(2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 88, "endOffset": 132}, {"referenceID": 9, "context": "We propose to apply the CNN model by Kim et al. (2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 2, "context": "Furthermore, our CNNmodel outperforms both the original and our re-implementation of the bidirectional LSTMmodel by Ballesteros et al. (2015) by a large margin.", "startOffset": 116, "endOffset": 142}, {"referenceID": 13, "context": "It has two additional transitions (LEFT-2 and RIGHT-2) to the Arc-Standard system (Nivre, 2004) that attach the top of the stack to the third token on the stack, or vice versa.", "startOffset": 82, "endOffset": 95}, {"referenceID": 15, "context": "As the baseline parsing model, we re-implement the greedy parser in Weiss et al. (2015) with some modifications, which brings about 0.", "startOffset": 68, "endOffset": 88}, {"referenceID": 1, "context": "2 Since most treebanks contain non-projective trees, we use an approximate non-projective transition system similar to Attardi (2006). It has two additional transitions (LEFT-2 and RIGHT-2) to the Arc-Standard system (Nivre, 2004) that attach the top of the stack to the third token on the stack, or vice versa.", "startOffset": 119, "endOffset": 134}, {"referenceID": 1, "context": "2 Since most treebanks contain non-projective trees, we use an approximate non-projective transition system similar to Attardi (2006). It has two additional transitions (LEFT-2 and RIGHT-2) to the Arc-Standard system (Nivre, 2004) that attach the top of the stack to the third token on the stack, or vice versa. We also extend the feature templates in Weiss et al. (2015) by extracting the children of the third token in the stack.", "startOffset": 119, "endOffset": 372}, {"referenceID": 17, "context": "Note that Weiss et al. (2015) directly concatenate the embeddings of the words, tags, and labels of all the tokens together as input to the hidden layer.", "startOffset": 10, "endOffset": 30}, {"referenceID": 16, "context": "2We only experiment with the greedy parser, since this paper focuses on the character-level input features and is independent of the global training and inference as in Weiss et al. (2015); Andor et al.", "startOffset": 169, "endOffset": 189}, {"referenceID": 0, "context": "(2015); Andor et al. (2016). 3The tokens in the stack and buffer do not have labels yet, we use a special label <NOLABEL> instead.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "As a baseline character model, we re-implement the bidirectional LSTM character composition model following Ballesteros et al. (2015). We replace the lookup table Eword in the baseline parser with the final outputs of the forward and backward LSTMs \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6 lstm and \u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d6\u20d7 lstm.", "startOffset": 108, "endOffset": 134}, {"referenceID": 11, "context": "We refer the readers to Ling et al. (2015) for the details of the bidirectional LSTM.", "startOffset": 24, "endOffset": 43}, {"referenceID": 10, "context": "In contrast to the LSTM model, we propose to use a \u201cflat\u201d CNN as the character composition model, similar to Kim et al. (2016).4", "startOffset": 109, "endOffset": 127}, {"referenceID": 12, "context": "The second baseline (W2V) uses pre-trained word embeddings by word2vec (Mikolov et al., 2013) with the CBOW model and default parameters on the unlabeled texts from the shared task organizers.", "startOffset": 71, "endOffset": 93}, {"referenceID": 2, "context": "The third baseline (LSTM) uses a bidirectional LSTM as the character composition model following Ballesteros et al. (2015). Appendix C lists the hyper-parameters of all the models.", "startOffset": 97, "endOffset": 123}, {"referenceID": 2, "context": "We also report the results of the two models from Ballesteros et al. (2015): B15-WORD with randomly initialized word embeddings and B15-LSTM as their proposed model.", "startOffset": 50, "endOffset": 76}], "year": 2017, "abstractText": "We present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et al., 2015) by a margin of 3% on average.1", "creator": "LaTeX with hyperref package"}}}