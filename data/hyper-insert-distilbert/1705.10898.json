{"id": "1705.10898", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Towards Learned Clauses Database Reduction Strategies Based on Dominance Relationship", "abstract": "clause learning is one case of the possibly most important components critical of enabling a large conflict dynamic driven clause learning ( cdcl ) sat problem solver that is extensively effective on industrial instances. since now the actual number sequences of faulty learned relative clauses is proved merely to typically be exponential better in the continually worse case, it clearly is still necessary to identify locally the most relevant clauses to maintain closure and delete the irrelevant ones. as reported in the literature, several learned clauses deletion strategies have been repeatedly proposed. however ; the diversity in arguments both reflects the number of taught clauses appearing to be removed arrived at just each step of residue reduction operations and only the results obtained with each strategy creates confusion needed to normally determine regarding which criterion consolidation is better. thus, the smallest problem to routinely select in which learned key clauses again are tendency to be removed slightly during whatever the search step remains very challenging. in this paper, we propose as a novel approach mainly to identify the internally most critically relevant negatively learned clauses without favoring or excluding any of solely the proposed measures, formally but by adopting the notion of dominance recovery relationship among those measures. nevertheless our modern approach bypasses the problem of the diversity acquisition of results and reaches a general compromise between the various assessments of these measures. thereby furthermore, the newly proposed approach also finally avoids another non - trivial problem which is enables the greater amount of clauses to be automatically deleted significantly at each reduction of constructing the additional learned clause memory database.", "histories": [["v1", "Wed, 31 May 2017 00:05:26 GMT  (369kb)", "http://arxiv.org/abs/1705.10898v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jerry lonlac", "engelbert mephu nguifo"], "accepted": false, "id": "1705.10898"}, "pdf": {"name": "1705.10898.pdf", "metadata": {"source": "CRF", "title": "Towards Learned Clauses Database Reduction Strategies Based on Dominance Relationship", "authors": ["Jerry Lonlac", "Engelbert Mephu Nguifo"], "emails": ["nguifo}@uca.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 89\n8v 1\n[ cs\n.A I]\n3 1\nM ay"}, {"heading": "1 Introduction", "text": "The SAT problem, i.e., the problem of checking whether a Boolean formula in conjunctive normal form (CNF) is satisfiable or not, is central to many domains in computer science and artificial intelligence including constraint satisfaction problems (CSP), automated planning, non-monotonic reasoning, VLSI correctness checking, etc. Today, SAT has gained a considerable audience with the advent of a new generation of solvers able to solve large instances encoding real-world problems. These solvers, often called modern SAT solvers [17, 11] or CDCL (Conflict Driven Clause Learning) SAT solvers have been shown to be very efficient at solving real-world SAT instances. They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17]. Although a nice combination of these components contributes to improve the efficiency of modern SAT solvers [16], clause learning is known as the most important component [18]. The global idea\nof clause learning is that during the unit propagation process, when a current branch of the search tree leads to a conflict, moderns SAT solvers learn a conflict clause that helps unit propagation to discover one of the implications missed at an earlier level. This conflict clause expresses the causes of the conflict and is used to prune the search space. Clause learning, also known in the literature as Conflict Driven Clause Learning (CDCL), refers now to the most known and used First (UIP) learning scheme, first integrated in the SAT solver Grasp [19] and efficiently implemented in zChaff [17]. Most of the SAT solvers integrate this strong learning scheme. Since at each conflict, CDCL solvers learn a new clause that is added to the learned clauses database, and the number of learned clauses is proved to be exponential in the worse case, it is necessary to remove some learned clauses to maintain a database of polynomial size. Therefore, removing too many clauses can make learning inefficient, and keeping too many clauses also can alter the efficiency of unit propagation.\nManaging the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14]. These strategies were proposed with the objective to maintain a learned clause database of reasonable size by eliminating clauses deemed irrelevant to the subsequent search. The general principle of these strategies is that, at each conflict, an activity is associated to the learned clauses (static strategy). Such heuristic-based activity aims to weight each clause according to its relevance to the search process. In the case of dynamic strategies, such clauses activities are dynamically updated. The reduction of the learned clauses database consists in eliminating inactive or irrelevant clauses. Although all the learned clause deletion strategies proposed in the literature are shown to be empirically efficient, identifying the most relevant clause to maintain during the search process remains a challenging task. Our motivation in this work comes from the observation that the use of different relevant-based deletion strategies gives different performances. Our goal is to take advantage of several relevant learned clauses deletion strategies by seeking a compromise between them through a dominance relationship.\nIn this paper, we integrate a user-preference point of view in the SAT process. To this end, we integrate into the SAT process the idea of skyline queries [7], dominant patterns [21], undominated association rules [8] in order to learn clauses in a threshold-free manner. Such queries have attracted considerable attention due to their importance in multi-criteria decision making. Given a set of clauses, the skyline set contains the clauses that are not dominated by any other clause.\nSkyline processing does not require any threshold selection function, and the formal property of domination satisfied by the skyline clauses gives to the clauses a global interest with semantics easily understood by the user. This skyline notion has been developed for database and data mining applications, however it was unused for SAT purposes. In this paper, we adapt this notion to the learned clauses management process.\nThe paper is organized as follows. We first present some effective relevantbased learned clauses deletion strategies used in the literature. Then, our learned\nclauses deletion strategy based on the dominance relationship between different strategies is presented in section 3. Finally, before the conclusion, experimental results demonstrating the efficiency of our approach are presented."}, {"heading": "2 On the learned clauses database management strategies", "text": "In this section, we present some efficient learned clauses relevance measures exploited in the most SAT solvers of the literature.\nThe most popular CDCL SAT solver Minisat [11] considers as relevant the clauses the most involved in recent conflict analysis and removes the learned clauses whose involvement in recent conflict analysis is marginal. Another strategy called LBD for Literal Block Distance was proposed in [2]. LBD based measure is also exploited by most of the best state-of-the-art SAT solver (Glucose, Lingeling [6]) and whose efficiency has been proved empirically. LBD based measure uses the number of different levels involved in a given learned clause to quantify the quality of the learned clauses. Hence, the clauses with smaller LBD are considered as more relevant. In [3], a new dynamic management policy of the learned clauses database is proposed. It is based on a dynamic freezing and activation principle of the learned clauses. At a given search state, using a relevant selection function based on progress saving (PSM), it activates the most promising learned clauses while freezing irrelevant ones. In [14], a new criterion to quantify the relevance of a clause using its backtrack level called BTL for BackTrack Level was proposed. From experiments, the authors observed that the learned clauses with small BTL values are used more often in the unit propagation process than those with higher BTL values. More precisely, the authors observed that the learned clauses with BTL value less than 3 are always used much more than the remaining clauses. Starting from this observation, and motivated by the fact that a learned clause with smaller BTL contains more literals from the top of the search tree, the authors deduce that relevant clauses are those allowing a higher backtracking in the search tree (having small BTL value). More recently, several other learned clauses database strategies were proposed in [15, 1]. In [15], the authors explore a number of variations of learned clause database reduction strategies, and the performance of the different extensions of Minisat solver integrating their strategies is evaluated on the instances of the SAT competitions 2013/2014 and compared against other state-of-the-art SAT solvers (Glucose, Lingeling) as well as against default Minisat. From the performances obtained in [15], the authors have shown that size-bounded learning strategies proposed more than fifteenth years ago [19, 4, 5] is not over and remains a good measure to predict the quality of learned clauses. They show that adding randomization to size bounded learning is a nice way to achieve controlled diversification, allows to favor the short clauses, while maintaining a small fraction of large clauses necessary for deriving resolution proofs on some SAT instances. This study opens many discussions about the learned clauses database strategies and raises questions about the effectiveness proclaimed by other strategies of the state-of-the-art [11, 2]. In [1], the authors use the com-\nmunity structure of industrial SAT instances to identify a set of highly useful learned clauses. They show that augmenting a SAT instance with the clauses learned by the solver during its execution does not always mean to make the instance easy. However, the authors show that augmenting the formula with a set of clauses based on the community structure of the formula improves the performance of the solver in many cases. The different performances obtained by each strategy suggests that the question on how to predict efficiently the \u201dbest\u201d learned clauses is still open and deserves further investigation.\nOn the other hand, it is important to note that the efficiency of most of these state-of-the-art learned clauses management strategies heavily depends on the cleaning frequency and on the amount of clauses to be deleted each time. Generally, all the CDCL SAT solvers using these strategies exactly delete half of the learned clauses at each learned clauses database reduction step. For example, the CDCL SAT solver Minisat [11] and Glucose [2] delete half of the learned clauses at each cleaning. Therefore, the efficiency of this amount of learned clauses to delete (e.g the half) at each cleaning step of the learned clauses database has not been demonstrated theoretically, but instead experimentally. For our knowledge, there are not many studies in the literature on how to determine the amount of clauses to be deleted each time. This paper proposes an approach to identify the relevant learned clauses during the resolution process without favoring any of the best reported relevant measures and which frees itself of the amount of clauses to be removed at each time: the amount of learned clauses to delete corresponds at each time to the number of learned clauses dominated by one particular learned clause of the set of the current learned clauses which is called in the following sections, the reference learned clause."}, {"heading": "3 Detecting undominated learned Clauses", "text": "We present now our learned clauses relevant measure based on dominance relationship. We first motivate this approach with a simple example, and then propose an algorithm allowing to identify the relevant clauses with some technical details."}, {"heading": "3.1 Motivating example", "text": "Let us consider the following relevant strategies: LBD [2], SIZE (which consider as relevant the clause of the short size) and the relevant measure use by minisat [11] that we denote here CVSIDS. Suppose that we have in the learned clauses database, the clauses c1, c2 and c3 with:\n\u2013 SIZE(c1) = 8, LBD(c1) = 3, CV SIDS(c1) = 1e 100; \u2013 SIZE(c2) = 6, LBD(c2) = 5, CV SIDS(c2) = 1e 200; \u2013 SIZE(c3) = 5, LBD(c3) = 4, CV SIDS(c3) = 1e 300.\nThe question we ask is the following: which one is relevant? In [2], the authors consider the clause c1 which has the most smallest LBD measure as the most\nrelevant. In contrast, the authors of [15] and [12] prefer the clause c3 while the preference of the authors of Minisat [11] leads to the clause c3. Our approach copes with the particular preference at one measure by finding a compromise between the different relevant measures through the dominance relationship. Hence, for the situation described above, only the clause c2 is irrelevant because it is dominated by the clause c3 on the three given measures."}, {"heading": "3.2 Formalization", "text": "During the search process, the CDCL SAT solvers learn a set of clauses which are stored in the learned clauses database \u2206, \u2206 = {c1, c2, ..., cn}. At each cleaning step, we evaluate these clauses with respect to a set M = {m1,m2, ...,mk} of relevant measures. We denote m(c) the value of the measure m for the clause c, c \u2208 \u2206, m \u2208 M. Since the evaluation of learned clauses varies from a measure to another one, using several measures could lead to different outputs (relevant clauses with respect to a measure). For example, if we consider the motivating example, c1 is the best clause with respect to the LBD measure whereas it is not the case according to the evaluation of SIZE measure which favors c3 . This difference of evaluations is confusing for any process of learned clauses selection. Hence, we can utilize the notion of dominance between learned clauses to address the selection of relevant ones. Before, formulating the dominance relationship between learned clauses, we need to define it at the level of measure values. To do that, we define dominance value as follows:\nDefinition 1 (dominance value). Given a learned clauses relevant measure m and two learned clauses c and c\u2032, we say that m(c) dominates m(c\u2032), denoted by m(c) m(c\u2032), iff m(c) is preferred to m(c\u2032). If m(c) m(c\u2032) and m(c) 6= m(c\u2032) then we say that m(c) strictly dominates m(c\u2019), denoted m(c) \u227b m(c\u2032).\nDefinition 2 (dominance clause). Given two learned clauses c, c\u2032, the dominance relationship according to the set of learned clauses relevant measures M is defined as follows:\n\u2013 c dominates c\u2032, denoted c c\u2032, iff m(c) m(c\u2032), \u2200m \u2208 M. \u2013 If c dominates c\u2032 and \u2203m \u2208 M such that m(c) \u227b m(c\u2032), then c stritly domi-\nnates c\u2032 and we note c \u227b c\u2032.\nTo discover the relevant learned clauses a naive approach consists in comparing each clause with all other ones. However, the number of learned clauses is proved to be exponential which makes pairwise comparisons costly. In the following, we show how to overcome this problem by defining at each cleaning step of learned clauses database, a particular learned clause denoted by \u03c4 that we call here current reference learned clause which is an undominated clause of \u2206 according to the set of learned clauses relevant measures M. At each cleaning step, all the learned clauses dominated by \u03c4 will be considered as the irrelevant learned clauses and thus deleted from the learned clauses database.\nTo define current reference learned clause, we need a new relevant measure based on all the learned clauses relevant measures of M. We call this new measure Degree of compromise, in short DegComp defines as follows:\nDefinition 3 (Degree of compromise). Given a learned clause c, the degree of compromise of c with respect to the set of learned clauses relevant measures M is defined by DegComp(c) = \u2211 n i=1 m\u0302i(c)\n|M| , where m\u0302i(c) corresponds to the\nnormalized value of the clause c on the measure mi.\nIn fact, in practice, measures are heterogeneous and defined within different scales. For example the values of the learned clauses relevant measures in [11] are very high, in exponential order while the values of the relevant measures in [2] are smallest ones. Hence, in order to avoid that the measures with the higher values make marginal the measures with smallest values in the computation of the comprise degree of a given learned clauses, it is recommended to normalize the measures values. In our case here, we choose to normalize all the measures in the interval [0, 1]. More precisely, each value of measure m(c) of any learned\nclause c must be normalized into m\u0302(c) within [0, 1]. The normalization of a given measure m is performed depending on its domain and the statistical distribution of its active domain. We recall that the active domain of a measure m is the set of its possible values. It is worth mentioning, the normalization of a measure does not modify the dominance relationship between two given values. If we consider the learned clause c1 given in the motivating example in the section 3.1, with its three values : DegComp(c1) = \u0302CV SDIS(c1)+ \u0302LBD(c1)+ \u0302SIZE(c1)\n3 , then,\nwe have, DegComp(c1) = 1 1e100 + 3 nV ars() + 8 nV ars()\n3 , with nV ars() the number of variables of the Boolean formula.\nAfter giving the necessary definitions (current reference learned clause and Degree of compromise), the following lemma offers a swifter solution rather than pairwise comparisons, to find relevant clauses based on dominance relationship.\nLemma 1 Let c be a learned clause having the minimal degree of compromise with respect to the set of learned clauses relevant measures M, then c is an undominated clause.\nProof 1 Let c be a learned clause having the minimal degree of compromise with respect to the set of learned clauses relevant measures M, we suppose that there exists a learned clause c\u2032 that strictly dominates c, which means that \u2200m \u2208 M, m(c\u2032) m(c) and \u2203m\u2032 \u2208 M, m\u2032(c\u2032) \u227b m\u2032(c). Hence, we have DegComp(c\u2032) < DegComp(c). The latter inequality contradicts our hypothesis, since c has the minimal degree of compromise with respect to M.\nProperty 1 Let M the set of learned clauses relevant measures, \u2200c, c\u2032, c\u201d three learned clauses, if c \u227b c\u2032 and c\u2032 \u227b c\u201d then c \u227b c\u201d.\nDuring the search process, at each cleaning step of the learned clauses database, we first find the learned clause cMin having the minimal degree of compromise\nwith respect to M. Then, we delete from the learned clauses database all the clauses dominated by cMin.\nSearching for all undominated clauses during each cleaning step can be time consuming, such that we only compute the undominated clauses with respect to the reference learned clause during each reduction step."}, {"heading": "3.3 Algorithm", "text": "In this section, after presenting the general scheme of a deletion strategy of learned clauses (reduceDB(\u2206)) adopted by most of the reported solvers, we propose an algorithm allowing to discover relevant learned clauses by using dominance relationship.\nAlgorithm 1 depicts the general scheme of a learned clause deletion strategy (reduceDB(\u2206)). This algorithm first sorts the set of learned clauses according to the defined criterion and then deletes half of the learned clauses. In fact, this algorithm take a learned clauses database of size n and outputs a learned clauses database of size n/2. This is different from our approach which first searches the learned clause having the smallest degree of compromise (called reference learned clause) and then removes all the learned clauses that it dominates. The algorithm 2 depicts our learned clause deletion strategy. It is important to note that the clauses whose size (number of literals) and LBD are less than or equal to 2 are not concerned by the dominance relationship. These learned clauses are considered as more relevant and are maintained in the learned clauses database. Hence, the minDegComp function of our algorithm 2 looks the learned clause of minimal degree of compromise among the learned clauses of size and LBD greater than 3.\nAlgorithm 1: Deletion Strategy: reduceDB function\nInput: \u2206: The learned clauses database of size n Output: \u2206 The new learned clauses database of size n/2 sortLearntClauses() ; /* by the defined criterion */1 limit = n/2;2 ind = 0;3 while ind < limit do4 clause = \u2206[ind] ;5 if clause.size() > 2 and clause.lbd() > 2 then6 removeClause() ;7 else8 saveClause() ;9\nind++;10\nreturn \u2206 ;11\nAlgorithm 2: reduceDB-Dominance-Relationship\nInput: \u2206: The learned clauses database; M: a set of relevant measures Output: \u2206 The new learned clauses database cMin = minDegComp(M) ; /* cMin the clause having minimal1 degree of compromise according to M */ ind = 0;2 while ind < |\u2206| do3 c = \u2206[ind] ; /* a learned clause */4 if c.size() > 2 and c.lbd() > 2 and dominates(cMin, c, M) then5 removeClause() ;6 else7 saveClause() ;8 ind++;9\nreturn \u2206 ;10\nFunction dominates(cMin: a clause, c: a clause, M)11 i = 0;12 while i < |M| do13 m = M[i] ; /* a relevant measure */14 if m(c) m(cMin) then15 return FALSE ;16 i++;17\nreturn TRUE ;18"}, {"heading": "4 Experiments", "text": "For our experiments, we use three relevant measures for the dominance relationship to assess the efficiency of our approach. Notice that the user can choose to combine different other measures. We use SIZE [12], LBD [2] and CV SIDS [11] measures. All these measures have been proved effective in the literature [11, 2, 15]. It is possible to use more relevant measures, but it should be noted that by adding a measure to M, the number of relevant learned clauses maintained may decrease or increase. The decrease can be explained by the fact that a learned clause can be dominated with respect to a set of measures M and undominated with respect to M\u2032 , such that M \u2282 M\u2032. For example, if two learned clauses c and c\u2032 are undominated with respect to M, there is a possibility that one of them dominates the other by removing one measure. The increase can be explained by the fact that a learned clause can be dominated with respect to M and undominated with respect to M\u2032. For example, consider a learned clause c which dominates another learned clause c\u2032 with respect to M, by adding a measure m to M, such that m(c\u2032) \u227b m(c), then c\u2032 is no longer dominated by c.\nWe run the SAT solvers on the 300 instances taken from the last SAT-RACE 2015 and on the 300 instances taken from the last SAT competition 2016. All the instances are preprocessed by SatElite [10] before running the SAT solver.\nThe experiments are made using Intel Xeon quad-core machines with 32GB of RAM running at 2.66 Ghz. For each instance, we used a timeout of 1 hour of CPU time for the SAT-RACE, and 10000s for the SAT Competition. We integrate our approach in Glucose and made a comparison between the original solver and the one enhanced with the new deletion learned clause strategy using dominance relationship called DegComp-Glucose."}, {"heading": "4.1 Number of solved instances and CPU time", "text": "Table 1 presents results on SAT-RACE. We use the source code of Glucose 3.0 with the measure LBD (written LBD-Glucose or Glucose in what follows). We then replace LBD by each of the other measures : SIZE-Glucose that considers the shortest clauses as the most relevant, CV SIDS-Glucose that maintains the learned clauses most involved in recent conflict analysis, and finally our proposal DegComp-Glucose. Table 1 shows the comparative experimental evaluation of the four measures as well asMinisat 2.2. In the second column of Table 1, we give the total number of solved instances (#Solved). We also mention, the number of instances proven satisfiable (#SAT) and unsatisfiable (#UNSAT) in parenthesis. The third column shows the average CPU time in seconds (total time on solved instances divided by the number of solved instances). On the SAT-RACE 2015, our approachDegComp-Glucose is more efficient than the others in terms of the number of solved instances (see also Figure 1). In fact the original solver Glucose solves 236 instances while it is enhanced with our dominance approach as 12 more instances are solved. In fact, solving such additional number of instances is clearly significant in practical SAT solving. The CV SIDS-Glucose solver solves 4 more instances than Glucose 3.0. Minisat 2.2 is the worst solver among the five solvers.\nTable 2 shows 5 instances of the SAT-RACE 2015 solved by our approach but not solved by LBD-Glucose, SIZE-Glucose, nor CV SIDS-Glucose. The time used to solve those instances may also explain the increase of the average running time of DegComp-Glucose. In addition we also find that there is none instance solved by all the other solvers and not solved by our approach (as detailed later). This shows on the one hand that the application of dominance between different relevant measures does not degrade the performance of all the\nsolvers but instead takes advantage of the performance of each relevant measure, considering the SAT-RACE dataset.\nFigure 1 shows the cumulated time results i.e. the number of instances (xaxis) solved under a given amount of time in seconds (y-axis). This figure gives for each technique the number of solved instances (#instances) in less than t seconds. It confirms the efficiency of our dominance relationship approach. From this figure, we can observe thatDegComp-Glucose is generally faster than all the other solvers, even if the average running time of LBD-Glucose is the lowest one (see Table 1). Although DegComp-Glucose needs additionnal time to compute the dominance relationship, the quality of the remained clauses on SAT-RACE helps to improve the time needed to solved the instances.\nTable 3 presents results on the instances of the SAT Competition 2016. Here LBD-Glucose and CV SIDS-Glucose solve one more instance than DegCompGlucose which remains competitive, and solves the greatest number of satisfiable\ninstances. Figure 2 presents the cumulated time results on the instances of the SAT competition 2016. It comes out from this second dataset that LBD-Glucose is more efficient than the others including our approach which remains competitive wrt the number of solved instances.\nThis outcome gives credit to the NO FREE Lunch theorem [22]. We also think that the aggregated function may not be unique for all the datasets, such that it is necessary to explore the efficient combination of the prefered measures."}, {"heading": "4.2 Common solved instances", "text": "In table 4, the intersection between two relevant measures gives the number of common instances solved by each measure. For example, LBD and SIZE solved 219 instances in common, while 234 instances are solved by LBD and DegComp. We can see than our approach solves the largest number of instances in common with each of the aggregated measures. More precisely, the number of common instances solved with another measure is lower than the number of common instances solved with our approach.\nTo get more details, Table 5 gives the number of instances commonly solved by the considered relevant measures. This table allows to see the number of\ncommon instances solved by one, two, three or four measures. For example, there are 218 common instances solved by the four deletion strategies, while 44 instances are not solved by none of them. We can observe that 1, 1, 5, and 5 are the number of instances solved alone by respectively LBD and CV SIDS, SIZE and DegComp. Moreover, there is no instance solved by the three strategies (LBD, SIZE and CV SIDS) and not solved by our approach DegComp."}, {"heading": "4.3 Combined measures", "text": "Table 6 gives the number of instances solved with our dominance approach wrt the measures used in the dominance relations. From this table, we can see that the number of instances solved by using two measures (instead of three) in the dominance relationship is always lower than the number of instances solved (248) by using three measures."}, {"heading": "4.4 Percentage of deleted clauses", "text": "During our experiments, we compute at each reduction step of instance resolution, the percentage of deleted clauses i.e the number of dominated clauses (which are the removed) over the total number of learned clauses during this step. This allows to obtain an average percentage of deleted learned clauses per solved instance. By taking all the solved instances of the SAT-RACE 2015, the average of the average percentage of deleted learned clauses is equal to 0.36 with a standard deviation of 0.16.\nFigures 3 and 4 plot for each solved instance of the SAT-RACE 2015 (X-axis), the average percentage of deleted learned clauses (red curve with left-Y-axis) against respectively the total resolution time and the average resolution time (green curve with right-Y-axis). For each solved instance, the average resolution time is obtained by dividing its total resolution time by the number of reductions made before solving the instance.\nThe (red) curve of the average percentages of deleted learned clauses exhibits a high variation of the percentages of reduction from 0.11 to 0.89, with an average value equals to 0.36 with a standard deviation of 0.16. It comes out from this figure that the average percentage of deleted learned clauses is less than 50% on 200 instances among 248 solved instances. Our current strategy which uses only one undominated clause at each step is satisfactory wrt the running time, even if it can be possible to extend this stategy to a reduction with many undominated clauses. The curve of the average percentages of deleted learned clauses also shows 17 instances having the average percentage of deleted learned clauses equal to 0. These 17 instances correspond to the instances solved by the solver without having to reduce the learned clauses database.\nFigure 3 shows on the one hand, the instances whose resolution times are small but with a high average percentage of deleted learned clauses, and on the other hand, the instances whose resolution times are high but with a low average percentage of deleted learned clauses. The same remark is also valid with figure 4 where we use the average resolution time instead of the total resolution time.\nThis clearly shows that the number of deleted learned clauses at each reduction step is not the only component that impacts the resolution time. Other key components of modern CDCL SAT solver such as the restart policies [13] and the activity-based variable selection heuristics [17] also have an influence on the resolution time.\nIt should be noted that if the solver Glucose keeps all the learned clauses (no learned clauses deleted) throughout the resolution process, it solves only 203 instances on the SAT-RACE 2015 instances in 1 hour (33 instances less than the original solver and 45 instances of less than the solver integrating our dominance approach). On the instances of the 2016 SAT competition, keeping all the learned clauses during the resolution process, the solver glucose solves only 131 in 10000 seconds (34 instances less than the original solver and 33 instances of less than the solver integrating our approach).\nThis confirms the need to eliminate certain learned clauses (those deemed irrelevant) during the resolution process, and otherwise the interest of the learned clauses removal problem in CDCL SAT solvers."}, {"heading": "5 Conclusion and Future Works", "text": "In this paper, we propose an approach that addresses the learned clauses database management problem. We have shown that the idea of dominance relationship between relevant measures is a nice way to take profit of each measure. This\napproach is not hindered by the abundance of relevant measures which has been the issue of several works. The proposed approach avoids another non-trivial problem which is the amount of learned clauses to be deleted at each reduction step of the learned clauses database. The experimental results show that exploiting the dominance relationship improves the performance of CDCL SAT solver, at least on the SAT-RACE 2015. For the case of SAT-Competition, we still have to find a good dominance relation. The instances categories might also be an issue which should be explored.\nTo the best of our knowledge, this is the first time that dominance relationship has been used in the satisfiability domain to improve the performance of a CDCL SAT solver. Our approach opens interesting perspectives. In fact, any new relevant measure of learned clauses can be integrated into the dominance relationship."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Auvergne-Rho\u0302ne-Alpes region and European Union for their financial support through the European Regional Development Fund (ERDF). The authors would also like to thank CRIL (Lens Computer Science Research Lab) for providing them computing server and the authors of Glucose solver for making available the source code of their solver."}], "references": [{"title": "Using community structure to detect relevant learnt clauses", "author": ["Carlos Ans\u00f3tegui", "Jes\u00fas Gir\u00e1ldez-Cru", "Jordi Levy", "Laurent Simon"], "venue": "SAT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Predicting learnt clauses quality in modern sat solvers", "author": ["G. Audemard", "L. Simon"], "venue": "In IJCAI\u201909,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "On freezing and reactivating learnt clauses", "author": ["Gilles Audemard", "Jean-Marie Lagniez", "Bertrand Mazure", "Lakhdar Sais"], "venue": "In SAT\u20192011,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A complexity analysis of spacebounded learning algorithms for the constraint satisfaction problem", "author": ["Roberto J. Bayardo", "Daniel P. Miranker"], "venue": "Proceedings of the Thirteenth National Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Using CSP look-back techniques to solve real-world SAT instances", "author": ["Roberto J. Bayardo", "Jr.", "Robert C. Schrag"], "venue": "In AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Lingeling and friends entering the sat challenge", "author": ["A. Biere"], "venue": "Proceedings of SAT Challenge 2012: Solver and Benchmark Descriptions,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "The skyline operator", "author": ["Stephan B\u00f6rzs\u00f6nyi", "Donald Kossmann", "Konrad Stocker"], "venue": "In Proceedings of the 17th International Conference on Data Engineering, April", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Mining undominated association rules through interestingness measures", "author": ["Slim Bouker", "Rabie Saidi", "Sadok Ben Yahia", "Engelbert Mephu Nguifo"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A machine program for theorem-proving", "author": ["Martin Davis", "Gearge Logemann", "Donald W. Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1962}, {"title": "Effective preprocessing in sat through variable and clause elimination", "author": ["Niklas E\u00e9n", "Armin Biere"], "venue": "In SAT\u201905,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "An extensible sat-solver", "author": ["Niklas E\u00e9n", "Niklas S\u00f6rensson"], "venue": "SAT", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Berkmin: A fast and robust sat-solver", "author": ["Eugene Goldberg", "Yakov Novikov"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Boosting combinatorial search through randomization", "author": ["Carla P. Gomes", "Bart Selman", "Henry A. Kautz"], "venue": "In AAAI/IAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Diversification by clauses deletion strategies in portfolio parallel SAT solving", "author": ["Long Guo", "S\u00e4\u0131d Jabbour", "Jerry Lonlac", "Lakhdar Sais"], "venue": "ICTAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Revisiting the learned clauses database reduction strategies", "author": ["S\u00e4\u0131d Jabbour", "Jerry Lonlac", "Lakhdar Sais", "Yakoub Salhi"], "venue": "CoRR, abs/1402.1956,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Empirical study of the anatomy of modern sat solvers", "author": ["Hadi Katebi", "Karem A. Sakallah", "Jo\u00e3o P. Marques Silva"], "venue": "SAT", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Chaff: Engineering an efficient sat solver", "author": ["Matthew W. Moskewicz", "Conor F. Madigan", "Ying Zhao", "Lintao Zhang", "Sharad Malik"], "venue": "In 38th Design Automation Conference", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "On the power of clause-learning sat solvers with restarts", "author": ["Knot Pipatsrisawat", "Adnan Darwiche"], "venue": "In (CP\u201909),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Grasp - a new search algorithm for satisfiability", "author": ["Jo\u00e3o P. Marques Silva", "Karem A. Sakallah"], "venue": "In International Conference on Computer-Aided Design (ICCAD\u201996),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "GRASP: A search algorithm for propositional satisfiability", "author": ["Jo\u00e3o P. Marques Silva", "Karem A. Sakallah"], "venue": "IEEE Trans. Computers,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Mining dominant patterns in the sky", "author": ["Arnaud Soulet", "Chedy R\u00e4\u0131ssi", "Marc Plantevit", "Bruno Cr\u00e9milleux"], "venue": "ICDM", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "No free lunch theorems for optimization", "author": ["David Wolpert", "William G. Macready"], "venue": "IEEE Trans. Evolutionary Computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}], "referenceMentions": [{"referenceID": 16, "context": "These solvers, often called modern SAT solvers [17, 11] or CDCL (Conflict Driven Clause Learning) SAT solvers have been shown to be very efficient at solving real-world SAT instances.", "startOffset": 47, "endOffset": 55}, {"referenceID": 10, "context": "These solvers, often called modern SAT solvers [17, 11] or CDCL (Conflict Driven Clause Learning) SAT solvers have been shown to be very efficient at solving real-world SAT instances.", "startOffset": 47, "endOffset": 55}, {"referenceID": 8, "context": "They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17].", "startOffset": 86, "endOffset": 89}, {"referenceID": 16, "context": "They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17].", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17].", "startOffset": 199, "endOffset": 203}, {"referenceID": 19, "context": "They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17].", "startOffset": 225, "endOffset": 233}, {"referenceID": 16, "context": "They are built by integrating four major components to the classical (DPLL) procedure [9]: lazy data structures [17], activity-based variable selection heuristics (VSIDS-like) [17], restart policies [13], and clause learning [20, 17].", "startOffset": 225, "endOffset": 233}, {"referenceID": 15, "context": "Although a nice combination of these components contributes to improve the efficiency of modern SAT solvers [16], clause learning is known as the most important component [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Although a nice combination of these components contributes to improve the efficiency of modern SAT solvers [16], clause learning is known as the most important component [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "Clause learning, also known in the literature as Conflict Driven Clause Learning (CDCL), refers now to the most known and used First (UIP) learning scheme, first integrated in the SAT solver Grasp [19] and efficiently implemented in zChaff [17].", "startOffset": 197, "endOffset": 201}, {"referenceID": 16, "context": "Clause learning, also known in the literature as Conflict Driven Clause Learning (CDCL), refers now to the most known and used First (UIP) learning scheme, first integrated in the SAT solver Grasp [19] and efficiently implemented in zChaff [17].", "startOffset": 240, "endOffset": 244}, {"referenceID": 16, "context": "Managing the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14].", "startOffset": 73, "endOffset": 95}, {"referenceID": 18, "context": "Managing the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14].", "startOffset": 73, "endOffset": 95}, {"referenceID": 10, "context": "Managing the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14].", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "Managing the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14].", "startOffset": 73, "endOffset": 95}, {"referenceID": 2, "context": "Managing the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14].", "startOffset": 73, "endOffset": 95}, {"referenceID": 13, "context": "Managing the learned clauses database was the subject of several studies [17, 19, 11, 2, 3, 14].", "startOffset": 73, "endOffset": 95}, {"referenceID": 6, "context": "To this end, we integrate into the SAT process the idea of skyline queries [7], dominant patterns [21], undominated association rules [8] in order to learn clauses in a threshold-free manner.", "startOffset": 75, "endOffset": 78}, {"referenceID": 20, "context": "To this end, we integrate into the SAT process the idea of skyline queries [7], dominant patterns [21], undominated association rules [8] in order to learn clauses in a threshold-free manner.", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "To this end, we integrate into the SAT process the idea of skyline queries [7], dominant patterns [21], undominated association rules [8] in order to learn clauses in a threshold-free manner.", "startOffset": 134, "endOffset": 137}, {"referenceID": 10, "context": "The most popular CDCL SAT solver Minisat [11] considers as relevant the clauses the most involved in recent conflict analysis and removes the learned clauses whose involvement in recent conflict analysis is marginal.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Another strategy called LBD for Literal Block Distance was proposed in [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "LBD based measure is also exploited by most of the best state-of-the-art SAT solver (Glucose, Lingeling [6]) and whose efficiency has been proved empirically.", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "In [3], a new dynamic management policy of the learned clauses database is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In [14], a new criterion to quantify the relevance of a clause using its backtrack level called BTL for BackTrack Level was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "More recently, several other learned clauses database strategies were proposed in [15, 1].", "startOffset": 82, "endOffset": 89}, {"referenceID": 0, "context": "More recently, several other learned clauses database strategies were proposed in [15, 1].", "startOffset": 82, "endOffset": 89}, {"referenceID": 14, "context": "In [15], the authors explore a number of variations of learned clause database reduction strategies, and the performance of the different extensions of Minisat solver integrating their strategies is evaluated on the instances of the SAT competitions 2013/2014 and compared against other state-of-the-art SAT solvers (Glucose, Lingeling) as well as against default Minisat.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "From the performances obtained in [15], the authors have shown that size-bounded learning strategies proposed more than fifteenth years ago [19, 4, 5] is not over and remains a good measure to predict the quality of learned clauses.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "From the performances obtained in [15], the authors have shown that size-bounded learning strategies proposed more than fifteenth years ago [19, 4, 5] is not over and remains a good measure to predict the quality of learned clauses.", "startOffset": 140, "endOffset": 150}, {"referenceID": 3, "context": "From the performances obtained in [15], the authors have shown that size-bounded learning strategies proposed more than fifteenth years ago [19, 4, 5] is not over and remains a good measure to predict the quality of learned clauses.", "startOffset": 140, "endOffset": 150}, {"referenceID": 4, "context": "From the performances obtained in [15], the authors have shown that size-bounded learning strategies proposed more than fifteenth years ago [19, 4, 5] is not over and remains a good measure to predict the quality of learned clauses.", "startOffset": 140, "endOffset": 150}, {"referenceID": 10, "context": "This study opens many discussions about the learned clauses database strategies and raises questions about the effectiveness proclaimed by other strategies of the state-of-the-art [11, 2].", "startOffset": 180, "endOffset": 187}, {"referenceID": 1, "context": "This study opens many discussions about the learned clauses database strategies and raises questions about the effectiveness proclaimed by other strategies of the state-of-the-art [11, 2].", "startOffset": 180, "endOffset": 187}, {"referenceID": 0, "context": "In [1], the authors use the com-", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "For example, the CDCL SAT solver Minisat [11] and Glucose [2] delete half of the learned clauses at each cleaning.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "For example, the CDCL SAT solver Minisat [11] and Glucose [2] delete half of the learned clauses at each cleaning.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "Let us consider the following relevant strategies: LBD [2], SIZE (which consider as relevant the clause of the short size) and the relevant measure use by minisat [11] that we denote here CVSIDS.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "Let us consider the following relevant strategies: LBD [2], SIZE (which consider as relevant the clause of the short size) and the relevant measure use by minisat [11] that we denote here CVSIDS.", "startOffset": 163, "endOffset": 167}, {"referenceID": 1, "context": "The question we ask is the following: which one is relevant? In [2], the authors consider the clause c1 which has the most smallest LBD measure as the most", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "In contrast, the authors of [15] and [12] prefer the clause c3 while the preference of the authors of Minisat [11] leads to the clause c3.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "In contrast, the authors of [15] and [12] prefer the clause c3 while the preference of the authors of Minisat [11] leads to the clause c3.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "In contrast, the authors of [15] and [12] prefer the clause c3 while the preference of the authors of Minisat [11] leads to the clause c3.", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "For example the values of the learned clauses relevant measures in [11] are very high, in exponential order while the values of the relevant measures in [2] are smallest ones.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "For example the values of the learned clauses relevant measures in [11] are very high, in exponential order while the values of the relevant measures in [2] are smallest ones.", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "In our case here, we choose to normalize all the measures in the interval [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "clause c must be normalized into m\u0302(c) within [0, 1].", "startOffset": 46, "endOffset": 52}, {"referenceID": 11, "context": "We use SIZE [12], LBD [2] and CV SIDS [11] measures.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "We use SIZE [12], LBD [2] and CV SIDS [11] measures.", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "We use SIZE [12], LBD [2] and CV SIDS [11] measures.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "All these measures have been proved effective in the literature [11, 2, 15].", "startOffset": 64, "endOffset": 75}, {"referenceID": 1, "context": "All these measures have been proved effective in the literature [11, 2, 15].", "startOffset": 64, "endOffset": 75}, {"referenceID": 14, "context": "All these measures have been proved effective in the literature [11, 2, 15].", "startOffset": 64, "endOffset": 75}, {"referenceID": 9, "context": "All the instances are preprocessed by SatElite [10] before running the SAT solver.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "This outcome gives credit to the NO FREE Lunch theorem [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Other key components of modern CDCL SAT solver such as the restart policies [13] and the activity-based variable selection heuristics [17] also have an influence on the resolution time.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "Other key components of modern CDCL SAT solver such as the restart policies [13] and the activity-based variable selection heuristics [17] also have an influence on the resolution time.", "startOffset": 134, "endOffset": 138}], "year": 2017, "abstractText": "Clause Learning is one of the most important components of a conflict driven clause learning (CDCL) SAT solver that is effective on industrial instances. Since the number of learned clauses is proved to be exponential in the worse case, it is necessary to identify the most relevant clauses to maintain and delete the irrelevant ones. As reported in the literature, several learned clauses deletion strategies have been proposed. However the diversity in both the number of clauses to be removed at each step of reduction and the results obtained with each strategy creates confusion to determine which criterion is better. Thus, the problem to select which learned clauses are to be removed during the search step remains very challenging. In this paper, we propose a novel approach to identify the most relevant learned clauses without favoring or excluding any of the proposed measures, but by adopting the notion of dominance relationship among those measures. Our approach bypasses the problem of the diversity of results and reaches a compromise between the assessments of these measures. Furthermore, the proposed approach also avoids another non-trivial problem which is the amount of clauses to be deleted at each reduction of the learned clause database.", "creator": "gnuplot 4.6 patchlevel 4"}}}