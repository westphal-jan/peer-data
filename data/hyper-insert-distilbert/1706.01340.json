{"id": "1706.01340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "abstract": "using models supporting passive backchannel ( bc ) cues can only make efficient human - computer interaction networks more increasingly social. generally bcs provide both a feedback from giving the listener to the speaker indicating relevance to the speaker that he is typically still listened regularly to. bcs can be expressed physically in different discrete ways, explicitly depending solely on the modality nor of enabling the participant interaction, for example as text gestures or acoustic cues. more in this work, now we only considered existing acoustic semantic cues. we then are proposing an early approach that towards detecting arbitrary bc opportunities based on acoustic input input features like power and high pitch. while... other works in programming the field rely on permitting the use of a hand - delivered written rule containing set or specialized features, we ourselves made actually use few of our artificial neural networks. technically they are capable possibly of immediately deriving higher order architectural features remotely from musical input features unlike themselves. in surveying our setup, together we first used create a fully connected adaptive feed - forward network to establish upon an otherwise updated baseline in comparison to simply our previously proposed collaborative setup. we also has extended this computational setup partially by the use of long short - term adaptive memory ( lstm ) networks which have meanwhile shown to largely outperform better feed - in forward based numerical setups adequately on various tasks. our best system achieved an f1 - depth score maximum of 0. 37 using power capture and pitch features. adding linguistic information achieved using conventional word2vec, the score increased to : 0. 39.", "histories": [["v1", "Fri, 2 Jun 2017 17:05:26 GMT  (476kb,D)", "http://arxiv.org/abs/1706.01340v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.HC cs.LG cs.SD", "authors": ["robin ruede", "markus m\\\"uller", "sebastian st\\\"uker", "alex waibel"], "accepted": false, "id": "1706.01340"}, "pdf": {"name": "1706.01340.pdf", "metadata": {"source": "CRF", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "authors": ["Robin Ruede", "Markus M\u00fcller", "Sebastian St\u00fcker", "Alex Waibel"], "emails": ["robin.ruede@student.kit.edu", "m.mueller@kit.edu", "sebastian.stueker@kit.edu", "alexander.waibel@kit.edu"], "sections": [{"heading": "1 Introduction", "text": "With dialog speech technology increasingly entering the mainstream of our every day lives (Siri, Cortana, Alexa, . . . ), there is a growing interest in dialog systems that are not only utilitarian (to answer questions or carry out tasks), but also to entertain\nRobin Ruede \u00b7 robin.ruede@student.kit.edu Markus Mu\u0308ller \u00b7 m.mueller@kit.edu Sebastian Stu\u0308ker \u00b7 sebastian.stueker@kit.edu Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Germany\nAlex Waibel \u00b7 alexander.waibel@kit.edu Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Germany Carnegie Mellon University, InterACT, Pittsburgh, PA, USA\n1\nar X\niv :1\n70 6.\n01 34\n0v 1\n[ cs\n.C L\n] 2\nJ un\nand to be social. Humanoid robots, interactive toys, virtual assistants and even virtual psychiatrists and pets attempt to add an emotional and social dimension to human interaction that may go beyond improving the user experience of existing dialog systems, and thus require increasingly skillful and adept social interaction. Social dialogs are, however, much less well understood than goal directed ones. They do not aim for a particular outcome other than the more indirect goals of growing a mutual understanding, empathizing, bonding and entertaining between humans.\nIn the present paper, we are proposing a neural network based system to generate a social response. Our first attempt in this regard aims to predict a suitable social response, when human speakers take \u201cthe floor\u201d and are sharing thoughts and experiences. The so-called \u201cbackchannel\u201d (BC) involves short phrases (\u201cuh-huh\u201d, \u201chum\u201c, \u201cyeah\u201d, \u201cright\u201d, etc.) whose role is to signal to another speaker that one is listening and paying attention. Further extensions also empathize, confirm, approve or disapprove. In conversational speech, BCs complement turn taking where more rapid questions and responses are exchanged. Despite its simple function, however, the BC is surprisingly complex: It must be chosen properly, timed correctly and placed at appropriate intervals. It also responds to content, emotion and discourse state.\nIn this paper, we describe a neural network approach to learning the production of proper BC cues. We will focus on short phrasal BC cues during longer stretches of conversational speech, where another speaker has taken the floor. Appropriate prediction of backchanneling is learned from human conversation and includes acoustic and linguistic features. In our work, we use recurrent neural networks to learn the choice and placement of appropriate BC cues from conversational data (Switchboard). Special attention is given to producing \u201ccausal\u201d backchanneling, i.e., so that the generation of a BC can be produced in real-time systems with information of the past.\nThis paper is organized as follows: In the next Section, we provide an overview of related work. In Section 3, we describe our approach in detail, followed by an overview of the experimental setup in Section 4. The results of the experiments are presented in Section 5. This paper concludes with an outlook to future work in Section 6."}, {"heading": "2 Related Work", "text": "Different approaches towards BC prediction have been proposed in the past. They are based on different types of predictors and use a wide variety of input modalities. These modalities include acoustic features like pause and pitch, but also visual cues like head movement. In addition to these direct features, additional information sources like language models or part of speech tagging exist.\nMany approaches are rule based. [25] proposed a method that uses acoustic features. The authors state that the most important acoustic phenomena for BC prediction occur right before a BC. As features, they used pause information, as well\nas pitch (falling or rising slope). They conducted their experiments on a Dutch corpus and report that the most important feature in their work is the duration of the pause. [27] proposed a similar approach triggering BCs at low pitch and pause regions in English and Japanese. But building a rule based system might prove difficult as these rules have to be manually created, which is a time-consuming and difficult to generalize. Other works included data-driven methods in which a classifier is trained and the output of this classifier is then post-processed. [16] proposes an approach that incorporates sequential probabilistic models like Hidden Markov Models or Conditional Random Fields. They used a set of features including eye gaze and several features derived from the audio signal, e.g., downslopes in pitch or certain types of volume changes. In another approach, predicting different types of BC was attempted [8]. Detecting BCs in real-time was also proposed [20] in the past.\nThere exists another category of systems that make use of artificial neural networks (ANNs). Being a data-driven method, NNs do not require handwritten rules. They have shown to be a versatile tool with the ability to learn relevant features automatically. A first approach towards detecting speech acts (including BCs) was proposed by Ries [19]. He used an NN in combination with an HMM. Stolcke also proposed NN based methods for modelling dialogue acts [23, 22]. In the past, we also proposed an NN based approach [17] that was mainly data-driven, requiring only minimal post-processing of the network outputs. In this first approach, we used a very basic ANN based setup, which we now refined.\nThe objective evaluation of systems for BC prediction is difficult because BC behaviour is very speaker-dependent and subjective. As an objective measurement, the use of the F1-Score has been established. [11] provides a comparison of different approaches for evaluation. In addition to objective measures, user studies are also a possibility to evaluate BC systems, like we did in the past [17]. A general study about the occurrence of BCs with respect to their role in facilitating attentive listening also exists [7]."}, {"heading": "3 Backchannel Prediction", "text": ""}, {"heading": "3.1 BC Utterance Selection", "text": "There are different kinds of phrasal BCs, they can be non-committal, positive, negative, questioning, et cetera. To simplify the problem of predicting BCs, we only try to predict the trigger times for any type of BC, ignoring the distinction between different kinds of responses."}, {"heading": "3.2 Feature Selection", "text": "A neural network is able to learn advantageous feature representations on its own. Hence, feeding the absolute pitch and power (signal energy) values for a given time context enables the network to automatically extract the relevant information such as pitch slopes and pause triggers, as used in related research [16]. In addition to pitch and power, we also evaluated using other acoustic features such as the fundamental frequency variation (FFV) [12] and the Mel-frequency cepstral coefficients (MFCCs). Finally, we tried adding an encoding of the speakers\u2019 word history before the listener backchannel using word2vec [14] to assess whether our setup benefits from multimodal input features."}, {"heading": "3.3 Training and Neural Network Design", "text": "We assumed to have two separate, but synchronized audio channels and corresponding transcripts: One for the speaker and one for the listener. We needed to decide which areas of audio to use to train the network. As we wanted to predict the BCs in an online fashion without using future information, we needed to train the network to detect segments of audio from the speaker track that would potentially cause a BC in the listener track. We chose the beginning of the BC utterance as an anchor and used a fixed context before that as the positive prediction area. We also needed to choose negative examples, so the network would not be biased to always predict a BC. We did this by selecting the range a few seconds before each BC, because in that area the listener explicitly decided not to give a backchannel response yet. This resulted in a fully balanced training dataset.\nWe initially used a feed forward network architecture. The input layer consists of all the chosen features over the previously selected fixed time context. The output layer has two softmax neurons representing the \u201ccategories\u201d [BC, non-BC]. We used back-propagation to train the network on the outputs [1, 0] for BC and [0, 1] for non-BC prediction areas. We only need to consider one of these outputs because the softmax function guarantees that they add up to one. We evaluated multiple different combinations of network depths and neuron counts. An example of the architecture with two hidden layers can be seen in Figure 1.\nThe placement of future BCs is dependent on the timing of previous BCs. The probability of a BC increases with longer periods without any listener feedback. To accommodate for this, we want the network to also take its previous internal state or outputs into account. We do this by modifying the above architecture to use Long-short term memory (LSTM) layers instead of feed forward layers."}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Dataset", "text": "We used the Switchboard dataset [3], which consists of 2,438 English telephone conversations of five to ten minutes, 260 hours in total. Pairs of participants from across the United States were encouraged to talk about a specific topic chosen randomly from 70 possibilities. Conversation partners and topics were selected so two people would only talk once with each other, and every person would only discuss a specific topic once.\nThese telephone conversations are annotated with transcriptions and word alignments [4] with a total of 390k utterances or 3.3 million words. We split the dataset randomly into 2,000 conversations for training, 200 for validation and 238 for evaluation. We used annotations from the Switchboard Dialog Act Corpus (SwDA) [6] to decide which utterances to classify as BCs. The SwDA contains categorical annotations for the utterances of about half of the data of the Switchboard corpus."}, {"heading": "4.2 Extraction", "text": "We chose to use the top 150 most common unique utterances marked as BCs from the SwDA. Because the SwDA is incomplete, we had to identify utterances as BCs just by their text. We manually included some additional utterances that were missing from the SwDA transcriptions but present in the original transcriptions, by going through the most common utterances and manually selecting those that seemed relevant, such as \u2018um-hum yeah\u2019 and \u2018absolutely\u2019. The most common BCs in the data\nset are \u201cyeah\u201d, \u201cum-hum\u201d, \u201cuh-huh\u201d and \u201cright\u201d, adding up to 68% of all extracted BC phrases.\nTo select which utterances should be categorized as BCs and used for training, we first filtered noise and other markers such as laughter from the transcriptions. Some utterances such as \u201cuh\u201d can be both BCs and speech disfluencies, so we only chose those that have either silence or another BC before them. With this method a total of 15.7% of utterances or 2.21% of words were labelled as BCs.\nWe used the Janus Recognition Toolkit [13] for parts of the feature extraction (power, pitch tracking, FFV, MFCC). Features were extracted for 32 ms frame windows with a frame shift of 10 ms, resulting in 100 samples per feature dimension per second. Because most of the data does not change much every 10 ms, we also test different context strides by only extracting every n-th frame. As an example, 800 ms of context with a stride of 2 corresponds to 40 data frames. For word2vec, we chose to also emit one frame every 10 ms for consistency, containing the encoding of the last non-silent word that ended before or at the time of the frame."}, {"heading": "4.3 Training", "text": "We used Theano [24] with Lasagne [1] for rapid prototyping and testing of different parameters.1 To evaluate different hyperparameters, we trained multiple network configurations with various context lengths (500ms to 2000ms), context strides (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (tanh and relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and Glorot [2]), and layer types (feed forward and LSTM).\nThe LSTM networks we tested were prone to overfitting quickly. We tried two methods of regularization to overcome this. The first was Dropout training, where we randomly dropped a specific portion of neuron outputs in each layer for each training batch [21]. We evaluated dropout layer combinations from 0 to 50% while increasing layer sizes proportionately, but this did not improve the results. The second was adding L2-Regularization with a constant factor of 0.0001. This greatly reduced overfitting and slightly improved the results."}, {"heading": "4.4 Postprocessing", "text": "We interpret the output value of the neural networks as the probability of a BC occurring at a given time. As the output is very noisy, we first apply a low-pass filter. To ensure our prediction does not use any future information, we use a Gaussian filter\n1 Our code for extraction, training, postprocessing and evaluation will be available at https://github.com/phiresky/backchannel-prediction. The repository also contains a script to reproduce all of the results of this paper.\nwhich is asymmetrically cut off at some multiple c of the standard deviation \u03c3 for the side that would range into the future, and offset it so the last frame is at \u00b10ms from the prediction target time. This means the latency of our prediction increases by c \u00b7\u03c3 ms. If we choose c = 0, we cut off the complete right half of the bell curve, keeping the latency at 0 at the cost of accuracy of the filter.\nAfter the low-pass filter, we select every area for which the value exceeds a given threshold. We trigger either at the beginning of each of these areas or at their first local maximum, depending on the largest acceptable latency. This varies depending on the chosen allowed margin of error as defined in subsection 4.5. An example of this postprocessing process can be seen in Figure 2.\nWe determined the optimal postprocessing hyperparameters for each network configuration and allowed margin of error automatically using Bayesian optimization [15] with the validation F1-Score as the utility function. For a margin of error of [0 ms, +1000 ms], the resulting standard deviation \u03c3 ranged from 200 ms to 350 ms, and the filter cut-off ranged from 0.9\u03c3 to 1.4\u03c3 . With this margin of error, the prediction can happen with a delay of up to one second after the ground truth. When choosing a margin of error that only allows a smaller delay such as [-200 ms, +200 ms], the selected standard deviation ranged from 150 ms to 250 ms, and the filter cut-off ranged from 0.0\u03c3 to 1.0\u03c3 , causing the predicted trigger to happen earlier."}, {"heading": "4.5 Evaluation", "text": "The training data contains two-sided conversations. Because the output of our predictor is only relevant for segments with just one person talking, we run our evaluation on monologuing segments. We define a monologuing segment as the maximum\npossible time range in which one person is continuously talking and the other person is continuously not talking for at least five seconds. A person is continuously talking iff they are only emitting utterances that are not silence or BCs. We only consider segments of a minimum length of five seconds to exclude sections of alternating conversation. This way we get segments where one participant is talking, and the other is producing backchannels without taking the turn.\nWe define a prediction time as correct if it is within a given margin of error of the onset of a correct BC utterance. We did most of the testing of our predictions with an error margin of [0 ms, +1000 ms], but also provide results for other margins used in related research. For comparison, we also evaluated a random predictor as a baseline. This predictor knows the correct count of BCs for a audio file and returns a uniformly distributed set of trigger times."}, {"heading": "5 Results", "text": "We use \u201c70 : 35\u201d to denote a network layer configuration of input\u2192 70 neurons\u2192 35 neurons\u2192 output.\nWe tested different context widths. A context width of nms means we use the range [\u2212nms,0ms] from the beginning of the backchannel utterance. The results improved when increasing the context width from our initial value of 500 ms. Performance peaked with a context of about 1500 ms, as can be seen in Table 1a, longer contexts tended to cause the predictor to trigger too late. We tested using only every n-th frame of input data. Even though we initially did this for performance reasons, we noticed that training on every single frame has worse performance than skipping every second frame due to overfitting. Taking every fourth frame seems to miss too much information, so performance peaks at a context stride of 2, as seen in Table 1b.\nWe tested different combinations of features, with using solely power in a first approach. But adding prosodic features gives great improvements. Using FFV as the only prosodic feature performs worse than FFV together with the absolute pitch value. Adding MFCCs does not seem to improve performance in a meaningful way, when also using pitch, see Table 1c for more details. Note that using only word2vec performs reasonably well, because with our method it indirectly encodes the time since the last utterance, similar to the power feature. Table 1d shows a comparison between feed forward and LSTM networks. The parameter count is the number of connection weights the network learns during training. Note that LSTMs have higher performance, even with similar parameter counts. We compared different layer sizes for our LSTM networks, as shown in Table 1e. A network depth of two hidden layers worked best, but the results are adequate with a single hidden layer or three hidden layers.\nIn Table 2, our final results are given for the completely independent evaluation data set. We compared the results from [17] with our system. [17] used the same dataset, but focused on offline predictions, meaning their network had future information available, and they evaluated their performance on the whole corpus\nincluding segments with silence and with alternating conversation. We adjusted our baseline and evaluation system to match their setup by removing the monologuing constraint described in subsection 4.5 and changing the margin of error to [-200 ms, +200 ms]. As can be seen in Table 2a, our predictor performs better. All other related research used different languages, datasets or evaluation methods, rendering a direct comparison difficult because of slightly different tasks.\nTable 2b shows the results with our presented evaluation method. We provide scores for different margins of error used in other research. Subjectively, missing a BC trigger may be more acceptable than a false positive, so we also provide a result with balanced precision and recall. Note that a later margin center with the same margin width has higher performance because it allows more latency in the predictions, which means we can choose better postprocessing parameters as described in subsection 4.4."}, {"heading": "6 Conclusion and Future Work", "text": "We have presented a new approach to predict BCs using neural networks. With refined methods for network training as well as different network architectures, we could improve the F1-Score in contrast to our previous experiments. In addition to evaluating different hyperparameter configurations, we also experimented with LSTM networks, which lead to improved results. Our best system achieved an F1-Score of 0.388.\nWe used linguistic features via word2vec only in a very basic way, assuming the availability of an instant speech recognizer by using the reference transcripts. As low-latency speech recognition is possible [18], one of the next steps would be to combine both systems. Further work is needed to evaluate other methods for adding word vectors to the input features and to analyze problems with our approach. We only tested feed forward neural networks and LSTMs, other architectures like timedelay neural networks [26], also called convolutional neural networks, may also give interesting results. Our approach of choosing the training areas may not be optimal because the delay between the last utterance of the speaker and the backchannel can vary significantly. One possible solution would be to align the training area by the last speaker utterance instead of the backchannel start.\nBecause backchannels are a largely subjective phenomenon, a user study would be helpful to subjectively evaluate the performance of our predictor and to compare it with human performance in our chosen evaluation method. Another method would be to find consensus for backchannel triggers by combining the predictions of multiple human subjects for a single speaker channel as described by Huang et al. (2010) as \u201cparasocial consensus sampling\u201d [5].\nAcknowledgements This work has been conducted in the SecondHands project which has received funding from the European Union\u2019s Horizon 2020 Research and Innovation programme (call:H2020ICT-2014-1, RIA) under grant agreement No 643950.\n(b) Results with our evaluation method with various margins of error used in other research [10]. Performance improves with a wider margin width and with a later margin center.\nMargin of Error Constraint Precision Recall F1-Score\n\u2212200 ms to 200 ms 0.172 0.377 0.237 \u2212100 ms to 500 ms 0.239 0.406 0.301 \u2212500 ms to 500 ms 0.247 0.536 0.339\n0 ms to 1000 ms Baseline (random, correct BC count) 0.111 0.052 0.071 Baseline (random, 8x correct BC count) 0.079 0.323 0.127 Balanced Precision and Recall 0.342 0.339 0.341 Best F1-Score (only acoustic features) 0.294 0.488 0.367 Best F1-Score (acoustic and linguistic features) 0.312 0.511 0.388"}], "references": [{"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Aistats, vol. 9, pp. 249\u2013256", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "ISIP switchboard word alignments", "author": ["D Harkins"], "venue": "URL https://www.isip. piconepress.com/projects/switchboard/", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Learning backchannel prediction model from parasocial consensus sampling: a subjective evaluation", "author": ["L. Huang", "L.P. Morency", "J. Gratch"], "venue": "International Conference on Intelligent Virtual Agents, pp. 159\u2013172. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Switchboard discourse language modeling project", "author": ["D. Jurafsky", "C Van Ess-Dykema"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Toward Adaptive Generation of Backchannels for Attentive Listening Agents", "author": ["T. Kawahara", "M. Uesato", "K. Yoshino", "K. Takanashi"], "venue": "International Workshop Serien on Spoken Dialogue Systems Technology, pp. 1\u201310", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction and Generation of Backchannel Form for Attentive Listening Systems", "author": ["T. Kawahara", "T. Yamaguchi", "K. Inoue", "K. Takanashi", "N. Ward"], "venue": "Proc. INTERSPEECH, vol. 2016", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on evaluation metrics for backchannel prediction models", "author": ["I. de Kok", "D. Heylen"], "venue": "Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey on Evaluation Metrics for Backchannel Prediction Models", "author": ["Kok", "I.d.", "D. Heylen"], "venue": "Feedback Behaviors in Dialog", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "The fundamental frequency variation spectrum", "author": ["K. Laskowski", "M. Heldner", "J. Edlund"], "venue": "Proceedings of FONETIK 2008 pp. 29\u201332", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "The janus-iii translation system: Speech-to-speech translation in multiple domains", "author": ["L. Levin", "A. Lavie", "M. Woszczyna", "D. Gates", "M. Gavald\u00e1", "D. Koll", "A. Waibel"], "venue": "Machine Translation 15(1), 3\u201325", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "On bayesian methods for seeking the extremum", "author": ["J. Mockus"], "venue": "Proceedings of the IFIP Technical Conference, pp. 400\u2013404. Springer-Verlag, London, UK, UK", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1974}, {"title": "A Probabilistic Multimodal Approach for Predicting Listener Backchannels", "author": ["L.P. Morency", "I. de Kok", "J. Gratch"], "venue": "Autonomous Agents and Multi-Agent Systems 20(1), 70\u201384", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Using Neural Networks for Data-Driven Backchannel Prediction: A Survey on Input Features and Training Techniques", "author": ["M. M\u00fcller", "D. Leuschner", "L. Briem", "M. Schmidt", "K. Kilgour", "S. St\u00fcker", "A. Waibel"], "venue": "International Conference on Human-Computer Interaction, pp. 329\u2013 340. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic transcription for low-latency speech translation", "author": ["J. Niehues", "T.S. Nguyen", "E. Cho", "T.L. Ha", "K. Kilgour", "M. M\u00fcller", "M. Sperber", "S. St\u00fcker", "A. Waibel"], "venue": "Interspeech 2016 pp. 2513\u20132517", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "HMM and Neural Network Based Speech Act Detection", "author": ["K. Ries"], "venue": "Proceedings of the Acoustics, Speech, and Signal Processing, 1999. on 1999 IEEE International ConferenceVolume 01, pp. 497\u2013500. IEEE Computer Society", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Building Autonomous Sensitive Artificial Listeners", "author": ["M. Schroder", "E. Bevacqua", "R. Cowie", "F. Eyben", "H. Gunes", "D. Heylen", "M. Ter Maat", "G. McKeown", "S. Pammi", "M Pantic"], "venue": "IEEE Transactions on Affective Computing 3(2), 165\u2013183", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1), 1929\u20131958", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Dialogue act modeling for automatic tagging and recognition of conversational speech", "author": ["A. Stolcke", "K. Ries", "N. Coccaro", "E. Shriberg", "R. Bates", "D. Jurafsky", "P. Taylor", "R. Martin", "C. Van Ess-Dykema", "M. Meteer"], "venue": "Computational linguistics 26(3), 339\u2013373", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Dialog act modeling for conversational speech", "author": ["A Stolcke"], "venue": "AAAI Spring Symposium on Applying Machine Learning to Discourse Processing, pp. 98\u2013105", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints abs/1605.02688", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "A Rule-Based Backchannel Prediction Model Using Pitch and Pause Information", "author": ["K.P. Truong", "R.W. Poppe", "D.K.J. Heylen"], "venue": "Proceedings of Interspeech 2010, Makuhari, Chiba, Japan, pp. 3058\u20133061. International Speech Communication Association (ISCA)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Phoneme Recognition Using Time-Delay Neural Networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE transactions on acoustics, speech, and signal processing 37(3), 328\u2013339", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "Prosodic features which cue back-channel responses in english and japanese", "author": ["N. Ward", "W. Tsukahara"], "venue": "Journal of pragmatics 32(8), 1177\u20131207", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 22, "context": "[25] proposed a method that uses acoustic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] proposed a similar approach triggering BCs at low pitch and pause regions in English and Japanese.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] proposes an approach that incorporates sequential probabilistic models like Hidden Markov Models or Conditional Random Fields.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In another approach, predicting different types of BC was attempted [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "Detecting BCs in real-time was also proposed [20] in the past.", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "A first approach towards detecting speech acts (including BCs) was proposed by Ries [19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Stolcke also proposed NN based methods for modelling dialogue acts [23, 22].", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "Stolcke also proposed NN based methods for modelling dialogue acts [23, 22].", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "In the past, we also proposed an NN based approach [17] that was mainly data-driven, requiring only minimal post-processing of the network outputs.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "[11] provides a comparison of different approaches for evaluation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In addition to objective measures, user studies are also a possibility to evaluate BC systems, like we did in the past [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "A general study about the occurrence of BCs with respect to their role in facilitating attentive listening also exists [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "Hence, feeding the absolute pitch and power (signal energy) values for a given time context enables the network to automatically extract the relevant information such as pitch slopes and pause triggers, as used in related research [16].", "startOffset": 231, "endOffset": 235}, {"referenceID": 9, "context": "In addition to pitch and power, we also evaluated using other acoustic features such as the fundamental frequency variation (FFV) [12] and the Mel-frequency cepstral coefficients (MFCCs).", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "Finally, we tried adding an encoding of the speakers\u2019 word history before the listener backchannel using word2vec [14] to assess whether our setup benefits from multimodal input features.", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "These telephone conversations are annotated with transcriptions and word alignments [4] with a total of 390k utterances or 3.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "We used annotations from the Switchboard Dialog Act Corpus (SwDA) [6] to decide which utterances to classify as BCs.", "startOffset": 66, "endOffset": 69}, {"referenceID": 10, "context": "We used the Janus Recognition Toolkit [13] for parts of the feature extraction (power, pitch tracking, FFV, MFCC).", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "We used Theano [24] with Lasagne [1] for rapid prototyping and testing of different parameters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "1 To evaluate different hyperparameters, we trained multiple network configurations with various context lengths (500ms to 2000ms), context strides (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (tanh and relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and Glorot [2]), and layer types (feed forward and LSTM).", "startOffset": 325, "endOffset": 328}, {"referenceID": 0, "context": "1 To evaluate different hyperparameters, we trained multiple network configurations with various context lengths (500ms to 2000ms), context strides (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (tanh and relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and Glorot [2]), and layer types (feed forward and LSTM).", "startOffset": 387, "endOffset": 390}, {"referenceID": 18, "context": "The first was Dropout training, where we randomly dropped a specific portion of neuron outputs in each layer for each training batch [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "We determined the optimal postprocessing hyperparameters for each network configuration and allowed margin of error automatically using Bayesian optimization [15] with the validation F1-Score as the utility function.", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "We compared the results from [17] with our system.", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "[17] used the same dataset, but focused on offline predictions, meaning their network had future information available, and they evaluated their performance on the whole corpus", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "As low-latency speech recognition is possible [18], one of the next steps would be to combine both systems.", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "We only tested feed forward neural networks and LSTMs, other architectures like timedelay neural networks [26], also called convolutional neural networks, may also give interesting results.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "(2010) as \u201cparasocial consensus sampling\u201d [5].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "[17] did their evaluation without the constraints defined in subsection 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "(offline) [17] \u2013 \u2013 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "(b) Results with our evaluation method with various margins of error used in other research [10].", "startOffset": 92, "endOffset": 96}], "year": 2017, "abstractText": "Using supporting backchannel (BC) cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory (LSTM) networks which have shown to outperform feed-forward based setups on various tasks. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39.", "creator": "LaTeX with hyperref package"}}}