{"id": "1706.02501", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Unlocking the Potential of Simulators: Design with RL in Mind", "abstract": "repeatedly using reinforcement learning ( cs rl ) complexity in simulation to automatically construct policies useful ways in real scientific life is naturally challenging. solving this limitations is often attributed further to the naive sequential decision making aspect : inaccuracies in chaotic simulation accumulate over multiple calendar steps, hence hence the simulated trajectories all diverge mechanically from what would happen otherwise in reality.", "histories": [["v1", "Thu, 8 Jun 2017 10:10:44 GMT  (1799kb,D)", "http://arxiv.org/abs/1706.02501v1", "Extended abstract for RLDM17 (3rd Multidisciplinary Conference on Reinforcement Learning and Decision Making)"]], "COMMENTS": "Extended abstract for RLDM17 (3rd Multidisciplinary Conference on Reinforcement Learning and Decision Making)", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["rika antonova", "silvia cruciani"], "accepted": false, "id": "1706.02501"}, "pdf": {"name": "1706.02501.pdf", "metadata": {"source": "CRF", "title": "Unlocking the Potential of Simulators: Design with RL in Mind", "authors": ["Rika Antonova", "Silvia Cruciani"], "emails": ["cruciani}@kth.se"], "sections": [{"heading": null, "text": "In our work we show the need to consider another important aspect: the mismatch in simulating control. We bring attention to the need for modeling control as well as dynamics, since oversimplifying assumptions about applying actions of RL policies could make the policies fail on real-world systems.\nWe design a simulator for solving a pivoting task (of interest in Robotics) and demonstrate that even a simple simulator designed with RL in mind outperforms high-fidelity simulators when it comes to learning a policy that is to be deployed on a real robotic system. We show that a phenomenon that is hard to model \u2013 friction \u2013 could be exploited successfully, even when RL is performed using a simulator with a simple dynamics and noise model. Hence, we demonstrate that as long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even using a simple simulator.\nRL-compatible simulators could open the possibilities for applying a wide range of RL algorithms in various fields. This is important, since currently data sparsity in fields like healthcare and education frequently forces researchers and engineers to only consider sample-efficient RL approaches. Successful simulator-aided RL could increase flexibility of experimenting with RL algorithms and help applying RL policies to real-world settings in fields where data is scarce. We believe that lessons learned in Robotics could help other fields design RL-compatible simulators, so we summarize our experience and conclude with suggestions.\nKeywords: Reinforcement Learning in Robotics, Learning from Simulation"}, {"heading": "Acknowledgements", "text": "We thank Christian Smith and Danica Kragic for guidance in the \u201cPivoting Task\u201d work, which was supported by the European Union framework program H2020-645403 RobDREAM.\n\u2217Robotics, Perception and Learning Group: https://www.kth.se/en/csc/forskning/rpl . . at the School of Computer Science and Communication\nar X\niv :1\n70 6.\n02 50\n1v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\n7"}, {"heading": "1 Introduction", "text": "Using simulators to learn policies applicable to real-world settings is a challenge: important aspects of agent and environment dynamics might be hard and costly to model. Even if sufficient modeling knowledge and computational resources are available, inaccuracies present even in high-fidelity simulators can cause the learned policies to be useless in practice. In the case of sequential decision making even a small mismatch between the simulated and the real world could accumulate across multiple steps of executing a policy.\nDespite these challenges, there is potential if simulators are designed with RL in mind. In our work we aim to understand what aspects of simulators are particularly important for learning RL policies. We focus first on the field of Robotics and present an approach to learn robust policies in a simulator, designed to account for uncertainty in the dynamics as well as in control of the real robot. We discuss our simulation and hardware experiments for solving a pivoting task (a robotic dexterity problem of re-orienting an object after initial grasp). Solving this task is of interest to the Robotics community; our approach to solving this task is described in detail in [1]. Here we only include the aspects of the work relevant to an interdisciplinary RLDM audience and present it as an example of RL-compatible simulator design.\nWe then discuss the potential of building simulators with RL in mind for other fields. In a number of fields enough domain knowledge is available to construct a task-specific or setting-specific simulator for a particular problem. It would be useful to ensure that such simulators can facilitate learning RL policies that work when transferred to real environments. This would significantly broaden the range of RL algorithms that could be used to solve a given problem \u2013 eliminating the need to restrict attention to only sample-efficient approaches (as is often done, since real-world learning is costly)."}, {"heading": "2 Why Use Simulators for Learning?", "text": "Recent success of Reinforcement Learning in games (Atari, Go) and Robotics make RL potentially the next candidate to solve challenging decision making (under uncertainty) problems in a wide variety of fields. A number of attempts to apply RL algorithms to real-world problems have been successful, but the success was mostly limited to sample-efficient approaches. It is costly and time-consuming to search for optimal policies by obtaining real-world trajectories. Batch RL \u2013 a class of algorithms that learn only from previously collected samples \u2013 could be helpful. But batch RL constitutes only a small fraction of a variety of RL algorithms. Hence, to open up the potential for using a wider range of RL algorithms we can turn to simulation.\nA number of fields have already developed general-purpose simulators1, and task-specific simulators are frequently developed for solving concrete research problems. But there is a need to facilitate successful transfer of the policies learned in simulation to real-world environments. To achieve this we can either ensure a tight enough match between simulated and real environments and use the learned policies directly, or develop data-efficient \u201csecond-stage\u201d approaches to adjust the learned policies to the real world.\nAn additional consideration is current rapid advances in using deep neural networks for RL. Several recently successful algorithms, like Trust Region Policy Optimization (TRPO) [8] and Deep Deterministic Policy Gradient (DDPG) [6] are not designed to be particularly sample-efficient. Instead these allow to learn flexible control policies, and can use large neural networks as powerful policy and Q function approximators. In simulation, \u201cdeep RL\u201d algorithms can solve problems similar in principle to those considered, for example, in Robotics. However, significant practical problems remain before they can be routinely applied to the real-world. Algorithms might not be data-efficient enough to learn on real hardware, so (pre-)training in simulation might be the only feasible solution. Success in simplified simulated settings does not immediately imply the policies could be transferred to a real-world task \u201cas-is\u201d, but investigating the conditions necessary for successful transfer could enable approaches that succeed in real world environments."}, {"heading": "3 Challenges and Opportunities of Simulation-based Learning: Insights from Robotics", "text": "Robotics presents a great opportunity for research in simulation-to-reality transfer for RL agents. First of all, a wide variety of general and task-specific simulators have been built to simulate various robotic systems. These simulators can model certain aspects well (e.g. the dynamics of rigid bodies with known masses and shapes). However, other aspects are harder to model precisely (e.g. dynamics of non-rigid bodies, friction). Hence, so-called \u201chigh-fidelity\u201d simulators contain a combination of models that are precise enough to describe real-world interactions and models that are either imprecise or even misleading. This creates a challenging problem from the perspective of an uninformed RL agent. However, the simplifying aspect is that getting insights into RL algorithms is easier. Experienced roboticists can have a reasonable guess as to which aspect of the environment is hardest for the agent to learn correctly from simulation. This\n1Examples of general-purpose simulators from various fields: SimStudent in education (simstudent.org); NS network simulator used for research in data networks (en.wikipedia.org/wiki/Ns (simulator)); more than a dozen general-purpose simulators for robotics (en.wikipedia.org/wiki/Robotics simulator).\nis in contrast with other fields. For example, some leading approaches for modeling student\u2019s knowledge in the field of education have error rates not far from random guessing when applied to non-synthetic student data. So the modeling problem is hard, but it is not at all clear which aspect is at fault and what exactly is making the real student data so challenging. Another example: modeling traffic in data networks requires significant simplifications for large-scale (internet-scale) simulators. This causes difficulties if an RL agent is learning a policy for handling individual packets. By comparison, robotic simulators provide a more clear picture of what would or would not be modeled well, and there is a way to quickly test policies learned in simulation on the real environment. Such robotics experiments are frequently more accessible than running large-scale studies involving humans, where returns from the policy could be distant in time (e.g. learning outcome of a class lasting several months) or too high-stakes for experimentation (result of a treatment for a patient). Thus one strategy is to develop and test a variety of simulation-to-reality transfer approaches on robotic tasks, then use the promising approaches for costlier and longer-term studies in other fields.\nWe note that care should be taken to capture (even if only approximately) all the relevant environment and control dynamics in a simulator. Roboticists have looked into problems arising from under-modeling of dynamics and hardware wear-and-tear (e.g. [2]) and experimented with various approaches of injecting noise in the deterministic simulators to increase tolerance to slight modeling inaccuracies ( [5] gives a summary and further references). Nonetheless, one aspect that is frequently overlooked is the application of control actions to real systems. Simple systems built in the lab can be designed to support fast and direct control. However, as the field moves to more sophisticated robotic systems that are built for a variety of tasks \u2013 oversimplifying assumptions of control could render simulation useless. Consider, for example, the Baxter robot \u2013 a medium-to-low-cost system built for small businesses and research labs. This robot is built to be relatively inexpensive, safe and versatile. Hence, in its usual operating mode, the control of the robot differs from that of high-cost industrial robots, and from that of simpler custom-built research systems. Speed and precision of controlling Baxter is traded off for lower cost of components, safe and smooth motions, versatility.\nIn our work, in addition to incorporating previously proposed solutions to deal with policy transfer challenges, we also emphasize the need to model delays and inaccuracies in control. We envision that as more multi-purpose medium-tolow-cost robots become widely used, it should become a standard component in building simulators. With that, we demonstrate that even a simple simulator designed with this in mind can be suitable for simulation-based RL."}, {"heading": "4 Building RL-compatible Simulator for Solving a Pivoting Task", "text": "In this section we present a short summary of our research work described in [1], with an intent to give an example that is comprehensible to an interdisciplinary audience. We describe our approach to building a task-specific simulator for a pivoting task, then learning an RL policy in simulation and using it on the real robot2 (without further adjustments)."}, {"heading": "4.1 Dynamic and Friction Models", "text": "Our simulator system is composed of a parallel gripper attached to a link that can rotate around a single axis. This system is an under-actuated two-link planar arm, in which the under-actuated joint corresponds to the pivoting point. We assume that we can control the desired acceleration on the first joint. The dynamic model of the system is given by:\n(I+mr2+mlr cos(\u03c6tl))\u03c6\u0308grp + (I+mr2)\u03c6\u0308tl +mlr sin(\u03c6tl)\u03c6\u03072grp +mgr cos(\u03c6grp+\u03c6tl) = \u03c4f , (1)\nwhere the variables are as follows: \u03c6grp and \u03c6tl are the angles of the first and second link respectively; \u03c6\u0308grp and \u03c6\u0308tl are their angular acceleration and \u03c6\u0307grp is the angular velocity of the first link; l is the length of the first link; I is the tool\u2019s moment of inertia with respect to its center of mass; m is the tool\u2019s mass; r is the distance of its center of mass from the\n2Video summarizing hardware experiments is available at https://www.youtube.com/watch?v=LWSjYI9a9xw\npivoting point; g is the gravity acceleration; \u03c4f is the torsional friction at the contact point between the gripper\u2019s fingers and the tool. The second link represents the tool and \u03c6tl is the variable we aim to control. Figure 2 illustrates this model."}, {"heading": "4.2 Learning Robust Policies", "text": "We formulate the pivoting task as an MDP. The state space is comprised of states observed at each time step t: st = [\u03c6tl \u2212 \u03c6tgt , \u03c6\u0307tl , \u03c6grp , \u03c6\u0307grp , dfing], with notation as in the previous subsection. The actions are: at = {\u03c6\u0308grp, dfing}, where \u03c6\u0308grp is the rotational acceleration of the robot arm, and dfing is the direction of the desired change in distance between the fingers of the gripper. The state transition dynamics is implemented by the simulator, but the RL algorithm does not have an explicit access to these. Reward is given at each time step t such that higher rewards are given when the angle of the tool is closer to the target angle: rt =\n\u2212|\u03c6tl\u2212\u03c6tgt| \u03c6RNG\n\u2208 [\u22121, 1] (\u03c6RNG is a normalizing constant). A bonus reward of 1 is given when the tool is close to the target and stopped.\nWe aim to learn from simulated environment, while being robust to the discrepancies between the simulation and execution on the robot. For this purpose, we first built a simple custom simulator using equations from Section 4.1. To facilitate learning policies robust to uncertainty, we added 10% noise to friction values estimated for the tool modeled by the simulator. We also injected up to 10% randomized delay for arm and finger actions in simulation: used noisy linear increase in velocity (as response to acceleration action) and simulated changing fingers\u2019 distance with noisy steps.\nWe then trained a model-free deep RL policy search algorithm TRPO [8] on our simulated setting. TRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [3]. However, to our knowledge it has not yet been widely applied to real-world robotics tasks. While the background for the algorithm is well-motivated theoretically, the approximations made for practicality, along with challenges in achieving reasonable training results with a small-to-medium number of samples, could impair the applicability of the algorithm to learning on the robot directly. Hence we explore the approach of using TRPO for policy search in a simulated environment.\nOur model of delayed and noisy control matched the behavior of real Baxter better than assuming near-instantaneous high-precision control. In contrast, learning from a high-fidelity simulator (V-REP) yielded policies that failed completely on the real robot: the tool was swayed in random directions, dropped. V-REP\u2019s oversimplification of simulating control impaired the ability of RL to learn useful policies from what was considered to be a relatively high-fidelity simulation.\n4.3 Summary of Experiments\nWe trained TRPO (using rllab [3] implementation as a starting point and parameters as reported in [8] for simulated control tasks) with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators. The motion was constrained to be linear in a horizontal plane. However, since the learning was not at all informed of the physics of the task, any other plane could be chosen and implemented by the simulator if needed.\nTo simulate the dynamics of the gripper arm and the tool, we used the modeling approach from Section 4.1 and injected noise during training to help learn policies robust to mismatch between the simulator and the robot. While we allowed only up to 10% noise in the variable modeling Coulomb friction, we observed that the policy was still able to retain 40% success rate of reaching the target angle when tested on settings with 250% to 500% change. This level of robustness alleviates the need to estimate friction coefficients precisely.\nFigure 4: Distance to target vs time for experiments on Baxter averaged over all trials (30 trials for each tool).\nWe deployed the learned policies on a Baxter robot and ran experiments with two objects of different materials, hence different friction properties. The friction coefficients used for modeling the tool in the simulator have been estimated only for the first tool. Hence the policy was not explicitly trained using the parameters matching those of the second tool.\nWe obtained a \u224893% success rate with tool 1 and \u224883% with tool 2. As expected, the policy performs better with the tool whose parameters were used (in a noisy manner) during training in simulation. Nonetheless, the performance using the tool not seen during training was robust as well. Fig. 4 illustrates the performance averaged over all the trials. The target is reached faster when tool 1 is used, and a bit slower when tool 2 is used. The deviation from the goal region after reaching the goal is likely due to inaccuracies in visual tracking. After reporting that the tool is in the goal region, the tracker might later report a corrected estimate, indicating further tool adjustment is needed. We observe that in such cases the policy still succeeds in further pivoting the tool to the target angle."}, {"heading": "5 Conclusion & Implications for other Fields for Designing RL-compatible Simulators", "text": "Our experience of building an RL-compatible simulator for a particular robotics task yields a few general suggestions.\n1. Focused modeling of the dynamics: Since high-fidelity modeling could be expensive, one could consider modeling only the part of the dynamics relevant to the task. Domain expertise could be used to understand what is \u201crelevant\u201d. E.g. high-fidelity modeling of the robot arm dynamics might be unnecessary, if the task could be solved by fixing the motion to a particular plane where even simple dynamics equations are sufficient. In fields that already have general-purpose simulators such \u201cfocusing\u201d is not always easy. The design usually does not allow to make a general-purpose simulator more task-oriented. Even if an researcher can find a restriction to the case where dynamics should be simpler \u2013 there is frequently no option in the simulator to eliminate unnecessary computations. Hence, the suggestion is to keep in mind this design consideration when building general-purpose simulators: either give the users easy control of which simulation modules to turn on/off or run automatic internal optimization of the simulator.\n2. Not oversimplifying the model of executing actions: Neglecting to observe that the process of applying an action needs modeling could cause policies learned in even a high-fidelity simulator to fail on a real system. In contrast, including even a simple model of control application in the simulator could fix the problem. Another solution could be to reformulate an MDP/POMDP for the task to make primitive actions be something that is trivial to apply to the real system (and capture all the side-effects of applying a primitive action in the dynamics of the state transitions). This solution could work, though it might require unintuitive formulations. Regardless of the approach, the main insight is not to neglect this aspect altogether (control of real robots is not instantaneous; the action of \u201dgive medication A\u201d might not lead to state \u201ctook medication A\u201d on the patient\u2019s side; the action of \u201cshow advertisement B\u201d might be blocked/ignored by the users).\n2. Even simple noise models help if sources of variability/uncertainty are identified: Adding Gaussian noise to various parts of a deterministic dynamical system is an approach frequently used in robotics to \u201cmodel\u201d uncertainty. No doubt this could be too simplistic for some settings/tasks. However, since learning in simulation opens potential for cheaper experimentation \u2013 it is possible to try various ways of capturing uncertainty and identify those sufficient for learning successful policies. As we show in our experiments, a phenomenon that is hard to model \u2013 friction \u2013 could be still exploited successfully for control even with a simple dynamical model and a simple noise model. As long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even from a simple simulator."}], "references": [{"title": "Reinforcement learning for pivoting task", "author": ["Rika Antonova", "Silvia Cruciani", "Christian Smith", "Danica Kragic"], "venue": "arXiv preprint arXiv:1703.00472,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Using local trajectory optimizers to speed up global optimization in dynamic programming", "author": ["Christopher G Atkeson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "A general framework for open-loop pivoting", "author": ["A. Holladay", "R. Paolini", "M.T. Mason"], "venue": "In 2015 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J Andrew Bagnell", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Friction models and friction compensation", "author": ["H. Olsson", "K.J. strm", "M. Gfvert", "C. Canudas De Wit", "P. Lischinsky"], "venue": "Eur. J. Control,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Swing-up regrasping algorithm using energy control", "author": ["A. Sintov", "A. Shapiro"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "In-hand manipulation using gravity and controlled slip", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "K. Pauwels", "C. Smith", "D. Kragic"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Adaptive control for pivoting with visual and tactile feedback", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "C. Smith", "D. Kragic"], "venue": "In 2016 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Solving this task is of interest to the Robotics community; our approach to solving this task is described in detail in [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "Several recently successful algorithms, like Trust Region Policy Optimization (TRPO) [8] and Deep Deterministic Policy Gradient (DDPG) [6] are not designed to be particularly sample-efficient.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Several recently successful algorithms, like Trust Region Policy Optimization (TRPO) [8] and Deep Deterministic Policy Gradient (DDPG) [6] are not designed to be particularly sample-efficient.", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "[2]) and experimented with various approaches of injecting noise in the deterministic simulators to increase tolerance to slight modeling inaccuracies ( [5] gives a summary and further references).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[2]) and experimented with various approaches of injecting noise in the deterministic simulators to increase tolerance to slight modeling inaccuracies ( [5] gives a summary and further references).", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "In this section we present a short summary of our research work described in [1], with an intent to give an example that is comprehensible to an interdisciplinary audience.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "The limitations of the previous approaches include: only open loop control with no control of the gripping force [4], movement restricted to the vertical plane [9], gripper in a fixed position, thus motion of the tool is determined only by the gravitational torque and torsional friction [11].", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "The limitations of the previous approaches include: only open loop control with no control of the gripping force [4], movement restricted to the vertical plane [9], gripper in a fixed position, thus motion of the tool is determined only by the gravitational torque and torsional friction [11].", "startOffset": 160, "endOffset": 163}, {"referenceID": 10, "context": "The limitations of the previous approaches include: only open loop control with no control of the gripping force [4], movement restricted to the vertical plane [9], gripper in a fixed position, thus motion of the tool is determined only by the gravitational torque and torsional friction [11].", "startOffset": 288, "endOffset": 292}, {"referenceID": 6, "context": "When the tool moves with respect to the gripper, we model the friction torque \u03c4f as viscous friction and Coulomb friction [7]: \u03c4f =\u2212\u03bcv\u03c6\u0307tl\u2212\u03bccfn sgn(\u03c6\u0307tl), in which \u03bcv and \u03bcc are the viscous and Coulomb friction coefficients and sgn(\u00b7) is the signum function.", "startOffset": 122, "endOffset": 125}, {"referenceID": 9, "context": "Since most robots are not equipped with tactile sensors to measure the normal force fn at the contact point, as in [10] we express it as a function of the distance dfing between the fingers using a linear deformation model: fn=k(d0\u2212dfing), where k is a stiffness parameter, d0 is the distance at which fingers initiate contact with the tool.", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "We then trained a model-free deep RL policy search algorithm TRPO [8] on our simulated setting.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "TRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "We trained TRPO (using rllab [3] implementation as a starting point and parameters as reported in [8] for simulated control tasks) with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "We trained TRPO (using rllab [3] implementation as a starting point and parameters as reported in [8] for simulated control tasks) with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators.", "startOffset": 98, "endOffset": 101}], "year": 2017, "abstractText": "Using Reinforcement Learning (RL) in simulation to construct policies useful in real life is challenging. This is often attributed to the sequential decision making aspect: inaccuracies in simulation accumulate over multiple steps, hence the simulated trajectories diverge from what would happen in reality. In our work we show the need to consider another important aspect: the mismatch in simulating control. We bring attention to the need for modeling control as well as dynamics, since oversimplifying assumptions about applying actions of RL policies could make the policies fail on real-world systems. We design a simulator for solving a pivoting task (of interest in Robotics) and demonstrate that even a simple simulator designed with RL in mind outperforms high-fidelity simulators when it comes to learning a policy that is to be deployed on a real robotic system. We show that a phenomenon that is hard to model \u2013 friction \u2013 could be exploited successfully, even when RL is performed using a simulator with a simple dynamics and noise model. Hence, we demonstrate that as long as the main sources of uncertainty are identified, it could be possible to learn policies applicable to real systems even using a simple simulator. RL-compatible simulators could open the possibilities for applying a wide range of RL algorithms in various fields. This is important, since currently data sparsity in fields like healthcare and education frequently forces researchers and engineers to only consider sample-efficient RL approaches. Successful simulator-aided RL could increase flexibility of experimenting with RL algorithms and help applying RL policies to real-world settings in fields where data is scarce. We believe that lessons learned in Robotics could help other fields design RL-compatible simulators, so we summarize our experience and conclude with suggestions.", "creator": "LaTeX with hyperref package"}}}