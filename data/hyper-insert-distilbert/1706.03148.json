{"id": "1706.03148", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Trimming and Improving Skip-thought Vectors", "abstract": "while the skip - thought model has been proven to be effective improvement at fully learning sentence representations naturally and capturing sentence item semantics. in developing this 2013 paper, we have propose promising a suite of techniques seeking to strengthen trim and better improve it. first, we automatically validate a hypothesis that, having given a current descriptive sentence, between inferring automatically the previous category and inferring predict the correct next binary sentence provide similar supervision computation power, providing therefore only one decoder for predicting the next sentence is preserved tightly in our loosely trimmed skip - thought model. \u2014 second, we present a stable connection inversion layer inversion between encoder and decoder to help the model to comfortably generalize better on semantic conflict relatedness tasks. third, theories we independently found argue that a genetically good polynomial word embedding initialization is also potentially essential resources for learning better capturing sentence representations. overall we informally train programs our query model unsupervised on a large weighted corpus problem with eight contiguous component sentences, and then efficiently evaluate toward the partially trained item model on 7 standard supervised compilation tasks, which includes semantic cross relatedness, paraphrase detection, and text classification benchmarks. but we empirically show that, because our currently proposed model is then a faster, far lighter - weight and equally powerful quantitative alternative to the overall original smoothed skip - thought consistency model.", "histories": [["v1", "Fri, 9 Jun 2017 22:44:31 GMT  (511kb,D)", "http://arxiv.org/abs/1706.03148v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shuai tang", "hailin jin", "chen fang", "zhaowen wang", "virginia r de sa"], "accepted": false, "id": "1706.03148"}, "pdf": {"name": "1706.03148.pdf", "metadata": {"source": "CRF", "title": "Trimming and Improving Skip-thought Vectors", "authors": ["Shuai Tang", "Hailin Jin Chen", "Fang Zhaowen Wang"], "emails": ["shuaitang93@ucsd.edu,", "desa@ucsd.edu,", "zhawang}@adobe.com"], "sections": [{"heading": null, "text": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model."}, {"heading": "1 Introduction", "text": "Learning distributed sentence representations is an important and hard topic in both the deep learning and natural language processing communities, since it requires machines to encode a sentence with potential unlimited language content into a fixed-dimension vector filled with continuous values. We are interested in learning to build a distributed sentence encoder in an unsupervised fashion by exploring the structure and relationship in a large unlabelled corpus. Since humans understand sentences by composing from the meanings of the words, we define that learning a sentence encoder should be composed of two essential components, which are learning distributed word representations, and learning how to compose a sentence representation from the representations of words in the given sentence.\nWith the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data. [4] proposed LSTM-based sequence to sequence learning (seq2seq) model for machine translation. Later [5] applied the seq2seq model for unsupervised representation learning on language, and then finetuned the model for supervised tasks. The seq2seq model can be jointly trained to learn the word representation and the composition function on word representations, also it shows encouraging idea that knowledge learned from unsupervised training could be transferred to help other related supervised tasks.\n[6] proposed the skip-thought model, which is an encoder-decoder model for unsupervised distributed sentence representation learning. The paper exploits the semantic similarity within a tuple of adjacent sentences as a supervision, and successfully built a generic, distributed sentence encoder. Rather than applying the conventional autoencoder model, the skip-thought model tries to reconstruct the surrounding 2 sentences instead of itself. The learned sentence representation encoder outperforms\nar X\niv :1\n70 6.\n03 14\n8v 1\n[ cs\n.C L\n] 9\nJ un\n2 01\nprevious unsupervised pretrained models on the evaluation tasks with no finetuning, and the results are comparable to the models which were trained directly on the datasets in a supervised fashion.\nIn this paper, We aim to trim and improve the original skip-thought model by three techniques. First, given the neighborhood hypothesis first proposed in [7], we directly abandon one of the decoders in the skip-thought model, and leave only one encoder and one decoder for learning from inferring the next sentence given the current one. Second, we replace the plain connection used between the encoder and decoder with the Average+Max Connection, which is a non-linear non-parametric feature engineering method proposed by [8] for Stanford Natural Language Inference (SNLI) [9] challenge, and enhances the model to capture more complex interactions among the hidden states. Third, a good initialization for word embeddings boosts the transferability of the model trained in unsupervised fashion, which may raise the importance of the word embeddings in unsupervised learning algorithms. In addition, we show that increasing the dimension of the encoder improves the performance of our proposed model, but still keeps the number of parameters much smaller than the original skip-thought model. Detailed description of our model is described in Section 2."}, {"heading": "2 Approach", "text": "In this section, we present the trimmed skip-thought model. It includes a few simple yet effective modifications from the previously proposed skip-thought model [6]. We first briefly introduce the skip-thought model, and then discuss how to explicitly modify it by incorporating our proposed 3 techniques."}, {"heading": "2.1 Skip-thought Model", "text": "In skip-thought model, given a sentence tuple (si\u22121, si, si+1), the encoder computes a fixeddimension vector as the representation zi for the sentence si, which learns a distribution p(zi|si; \u03b8e), where \u03b8e refers to the set of parameters in the encoder. Then, conditioned on the representation zi, two separated decoders are applied to reconstruct the previous sentence si\u22121, and the next sentence si+1, respectively. We call them previous decoder p(si\u22121|zi; \u03b8p) and next decoder p(si+1|zi; \u03b8n), where \u03b8\u00b7 denotes the set of parameters in each decoder. An illustration is shown in Figure 1a.\nSince the two conditional distributions learned from the decoders are parameterized independently, they implicitly utilize the sentence order information within the sentence tuple. Intuitively, given the current sentence si, inferring the previous sentence si\u22121 is considered to be different from inferring the next sentence si+1.\nEncoder: The encoder is a recurrent neural network, which is composed of bi-directional gated recurrent unit (GRU) [10], or uni-directional GRU. Suppose sentence si contains N words, which are w1i , w 2 i , ..., w N i . At an arbitrary time step t, the encoder produces a hidden state h t i, and we regard it as the representation for the previous subsequence through time t. At time N , the hidden state hNi represents the given sentence si, which is zi.\nDecoder: The decoder is a single-layer recurrent network with conditional GRU. Specifically, compared to GRU, it takes the sentence representation zi as an additional input at each time step. The decoder needs to reconstruct the previous sentence si\u22121 and the next sentence si+1 given the representation zi. The computation flows for the GRU and the conditional GRU are presented in Table 1."}, {"heading": "2.2 Trimming Skip-thought Model by Neighborhood Hypothesis", "text": "The neighborhood hypothesis was first introduced in [7], and it pointed out that given the current sentence, inferring the previous sentence and inferring the next sentence both provide same supervision power.\nTo incorporate the neighborhood hypothesis into the model, we need to modify the skip-thought model. Given si, we assume that inferring si\u22121 is the same as inferring si+1. If we define si\u22121, si+1 are two neighbors of si, then the inferring process can be denoted as sj \u223c p(s|zi; \u03b8d), for any j in the neighborhood of si. The conditional distribution learned from the decoder is parameterized by \u03b8d. Figure 1b illustrates the neighborhood hypothesis.\nFurthermore, in our trimmed skip-thought model, for a given sentence si, the decoder needs to reconstruct the sentences in its neighborhood {si\u22121, si+1}, which are two targets. We denote the inference process as si \u2192 {si\u22121, si+1}. For the next sentence si+1, the inference process is si+1 \u2192 {si, si+2}. In other words, for a given sentence pair {si, si+1}, the inference process includes si \u2192 si+1 and si+1 \u2192 si. In the neighborhood hypothesis [7], the model doesn\u2019t distinguish between the sentences in a neighborhood. As a result, an inference process that includes both si \u2192 si+1 and si+1 \u2192 si is equivalent to an inference process with only one of them. Thus, we define a trimmed skip-thought model with only one target, presented in Figure 1c, and the target is always the next sentence. The objective at each time step is defined as the log-likelihood of the predicted word given the previous words, which is\n`ti,j(\u03b8e, \u03b8d) = log p(w t j |w<tj , zi; \u03b8e, \u03b8d) (1)\nwhere \u03b8e is the set of parameters in the encoder, and \u03b8d is the set of parameters in the decoder. The objective function is summed across the whole training corpus, then the objective during training is\nmax \u03b8e,\u03b8d \u2211 i \u2211 t `ti,i+1(\u03b8e, \u03b8d) (2)"}, {"heading": "2.3 Average+Max Connection", "text": "In skip-thought models [6], only the hidden state at the last time step produced from the RNN encoder is regarded as the vector representation for a given sentence, and serves as the conditional information for the decoder to reconstruct the adjacent 2 sentences.\nRecently, [9] collected a large corpus, which is SNLI, for textual entailment recognition. Given a sentence pair including premise and hypothesis, the task is to classify the relationship of the sentence pair, entailment, contradiction or neutral. [11] proposed to summarize the hidden states from all time steps computed from a RNN encoder as a sentence representation. While [8] proposed to concatenate the outputs from an average pooling function and a max pooling function, which both run over all time steps, to serve as a sentence representation, and showed a performance boost on the SNLI dataset.\nThe concatenation of an average pooling and a max pooling function is actually a non-parametric composition function, and the computation load is negligible compare to heavy matrix multiplication. Also, the non-linearity of the max pooling function augments the average pooling function for building a representation that captures more complex composition of the context information. Given\na sentence si, the encoder produces a set of hidden states [h1i ;h 2 i ; ...;h N i ], the composition function could be represented as zi = [ 1 N \u2211N n=1 h n i ; max N n=1 h n i ] .\nHere, since our goal is to simplify and accelerate the skip-thought model, and also get comparable results on the evaluation tasks, we consider comparing the 2 different composition functions, which are the original one used in the skip-thought model [6], denoted as Plain Connection, and the function proposed by [8], denoted as Average+Max Connection. We hypothesize that the composition function by concatenating two pooling functions will help the model perform better on tasks that involve judging the relationship of a sentence pair, while it is hard to say if the model would benefit from it on the classification benchmark. We will discuss the results in Section 4."}, {"heading": "2.4 Word Embeddings Initialization", "text": "Distributed word embedding matters in the deep learning models that deal with NLP-related tasks. The proposed training methods, such as continuous bag-of-words and skip-gram [12], always serves as strong baseline models for the supervised tasks in NLP. The pretrained word embeddings, including word2vec [13] and GloVe [14], also boosts the model performance on the supervised tasks.\nWe hypothesize that initializing the deep models with pretrained word embeddings is useful for transferring the knowledge from unsupervised learning to the supervised tasks. We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively."}, {"heading": "3 Experiment Settings", "text": "The large corpus that we used for unsupervised training is the BookCorpus dataset [15], which contains 74 million sentences from 7000 books in total.\nAll of our experiments were conducted in Torch7 [16]. To make the comparison fair, we follow the encoder design by [6]. Since the comparison among different recurrent units is not our main focus, we decide to work with GRU, which is fast and stable. In addition, [3] shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) [2]. We also reimplemented the skip-thought model under the same settings, according to [6], and the publicly available theano code 1. We adopted the multi-GPU training scheme from the Facebook\u2019s implementation of ResNet2.\nThe experiments with bi-directional encoder and unidirectional encoder were both conducted in [6], and we follow the design of these experiments. For training efficiency, we didn\u2019t follow the exact same dimensionality used. In [6], for bi-skip model, the encoder contains a bi-directional GRU with 1200 dimension of each, for uni-skip model, the encoder contains a uni-directional GRU with 2400 dimension, and the decoder is a one-layer with 2400 dimension.\nIn our experiments, except for Section 4.4, the bi-directional encoder contains a forward and a backward GRU of 300 dimension each, and the uni-directional encoder contains a forward GRU of 600 dimension. After training the 2 models with 2 different encoders separately, we concatenate the vectors produced from 2 encoders to form a sentence representation, and evaluate the performance on evaluation tasks. The decoder is a one-layer unidirectional RNN with GRU, and the dimension is 600. The dimension of word embedding is 300.\nFor stable training, we use ADAM [17] algorithm for optimization. The gradient will be cut off to make it within [\u22121, 1]. For the purpose of fast training, all the sentences were zero-padded or clipped to have the same length.\nThe vocabulary for unsupervised training is set to contain the top 20k most frequent words in BookCorpus. In order to generalize the model trained with relatively small, fixed vocabulary to a large amount of English words, [6] proposed a word expansion method that learns a linear projection from the pretrained word embeddings word2vec [13] to the learned RNN word embeddings. Thus, the model benefits from the generalization ability of the pretrained word embeddings.\n1https://github.com/ryankiros/skip-thoughts 2https://github.com/facebook/fb.resnet.torch"}, {"heading": "4 Quantitative Evaluation", "text": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24]. After unsupervised training on the BookCorpus dataset, we fix the parameters in the encoder, and apply it as a sentence representation extractor on the 7 tasks.\nFor SICK and MSRP tasks, we adopt the feature engineering idea proposed by [25]. For a given sentence pair, the encoder computes a pair of representations, denoted as u and v, and the concatenation of the component-wise product u \u00b7 v and the absolute difference |u \u2212 v| is regarded as the feature vector for the given sentence pair. Then we train logistic regression on the feature vector to predict the semantic relatedness score. The evaluation metrics for SICK are Pearson\u2019s r, Spearman\u2019s \u03c1, and mean squared error MSE, and the performance is reported as accuracy and F1-score (Acc/F1) for MSRP. The performance on TREC is presented as test accuracy, and 10-fold cross validation is applied to evaluate the model on the MR, CR, SUBJ, and MPQA classification benchmarks.\nTable 2 presents the models and results, where the model name is given by encoder type - model type. Three types of encoder are denoted as uni-, bi-, and C- in Table 2, and the C- refers to the concatenation of 2 vector representations computed from uni-encoder model and bi-encoder model. -T-skip refers to our trimmed skip-thought model, and -skip refers to the skip-thought model."}, {"heading": "4.1 Trimmed skip-thought vs. Skip-thought", "text": "We first compare our trimmed skip-thought model with our implemented skip-thought model, to check the neighborhood hypothesis. In this comparison, all the models use the plain connection, which means that, the sentence representation is the hidden state at the last time step.\nTable 2 presents the results of 3 trimmed skip-thought models, and 3 of our implemented skip-thought models. From the table, we can tell that our trimmed skip-thought models perform slightly better than the skip-thought models overall, yet not significantly, but the performance on the TREC dataset is worse than the skip-thought models. The general performance comparison between our trimmed skip-thought model and the skip-thought model proves that the neighborhood hypothesis is reasonable, which means that the neighborhood information is effective for distributed sentence representation learning. In addition, our trimmed skip-thought model runs faster in training, since our model only needs to reconstruct its next sentence while the skip-thought model needs to reconstruct its two surrounding sentences.\nUnlike the results in [7], these models presented in this paper contain word embeddings with lower dimension, which is half of that in [7], and GRUs with much smaller size. Also, our models presented here use word2vec [13] as word embeddings initialization, which is different from random initialization applied in [7].\nThe results of our implemented skip-thought model differ from those presented in [6], (also presented here in the last section in Table 2), since our implementation contains much fewer parameters than the original skip-thought model, and it has word2vec [13] initialization. Overall, our implementation reaches similar performance on all tasks except Sick. The comparison with the original skip-thought model shows that our implementation of skip-thought model is reasonable."}, {"heading": "4.2 Plain Connection vs. Average+Max Connection", "text": "We further compare the effect of two different connections between the encoder and the decoder. The results are presented in Table 2. As we expected, our proposed trimmed skip-thought model benefits from the Average+Max Connection on judging the relationship of a sentence pair. The performance on SICK task get improved. However, the performance on 2 classification benchmarks, MR and CR, slightly drops, compared to our model with plain connection. The overall performance on the evaluation tasks reaches the results reported in [6] except TREC, which shows that our model with Average+Max Connection could be a fast, lighter-weight alternative to the skip-thought model. See Table 3 for detailed parameter counts."}, {"heading": "4.3 Word Embedding Initialization", "text": "The second section in Table 2 presents the comparison among 3 different initializations. After training, we learn a linear mapping from the word2vec 3 embedding space to RNN word embedding space, regardless of what initialization was applied in the model as in [6].\n3https://code.google.com/archive/p/word2vec/\nGenerally, the models with pretrained word embeddings as initialization perform better on the evaluation tasks than those with random initialization, which shows that a good initialization for word embeddings helps the model to better transfer knowledge from unsupervised training.\nOne thing worth mentioning here, for the models initialized with GloVe 4, we also trained a linear projection from GloVe word embeddings to the RNN word embeddings. The performance on SICK and MSRP is as good as other models presented in Table 2, but the word expansion from GloVe embeddings gave bad performance on 5 classification benchmarks, so we didn\u2019t include the results."}, {"heading": "4.4 Doubling Encoder\u2019s Dimension", "text": "In our experiments above, the encoder is either a bi-directional GRU with 300 dimension each or a uni-directional GRU with 600 dimension. With the average+max connection, the dimension of a sentence representation is 1200. We hypothesized that a model with larger encoder size could also improve the performance on evaluation tasks. Hence, we double the dimension of the encoder, which is now either a bi-GRU with 600 dimension each or a uni-GRU with 1200 dimension. As a result, the sentence representation is a 2400-dimension vector, which matches the dimensionality of the representation reported in [6]. Table 2 represents the results.\nOur trimmed skip-thought models with doubled encoder performs better than the skip-thought models report in [6] on SICK and MSRP, and have comparable results on 4 classification benchmarks. The performance is worse than the original skip-thought model on TREC. The training time and inference time are significantly less than that for the original skip-thought model. The cut down on the training time comes from the neighborhood hypothesis[7] and many fewer parameters in the model."}, {"heading": "5 Qualitative Investigation", "text": "We conduct investigation on our trimmed skip-thought model qualitatively. The model being studied here contains bi-GRU as encoder with 300 dimension of each, one-layer GRU as decoder with 600 dimension, and average+max connection."}, {"heading": "5.1 Sentence Retrieval", "text": "For this task, 1000 sentences were selected as the query set, and 1 million sentences were picked up as the database. All the sentences come from the training corpus. Cosine distance is applied to\n4https://nlp.stanford.edu/projects/GloVe/\nmeasure the distance in the representation space. See Table 4 for several samples. Most of retrieved sentences look semantically related and can be viewed as the sentential contextual extension to the query sentences."}, {"heading": "5.2 Conditional Sentence Generation", "text": "The decoder in our trimmed skip-thought model was trained in language modeling fashion, it is reasonable to analyze the sentences generated from the decoder after training. Since the sentence generation process is conditional on the representations produced from the encoder, we first randomly pick up sentences from the training corpus, and forward the model to get the output from the decoder for each of them. Greedy decoding is applied for sentence generation. Table 5 presents the generated sentences.\nWe observe that, the generated sentences tend to start with i \u2019m not, and i do n\u2019t. It might be caused the corpus bias, since there exists a large amount of sentences that start with i \u2019m not, i do n\u2019t, etc. In addition, the decoder is trained to reconstruct the next sentence in the model, which could be think of as a sentential contextual extension of the input sentence, while the generated sentences rarely are related to the associated input sentences, which is same for the skip-thought models. More investigations are needed for the conditional sentence generation."}, {"heading": "6 Related Work", "text": "Previously, [12] proposed the continuous bag-of-words (CBOW) model and the skip-gram model for distributed word representation learning. The main idea is learn a word representation by discovering the context information from the surrounding words. [13] improved the skip-gram model, and empirically showed that additive composition of the learned word representations successfully captures contextual information of phrases and sentences, which is a strong baseline model for NLP tasks. Similarly, [26] proposed a method to learn a fixed-dimension vector for each sentence by predicting the words within the given sentence. However, after training, the representation for a new sentence is hard to derive, since it requires optimizing the sentence representation towards an objective.\nRecent research in deep learning shows that, the word representation and the their composition could be done at the same time in an end-to-end machine learning system. LSTM-based autoencoder model for language representation learning was proposed by [5]. For a specific dataset, the model first was trained in an unsupervised fashion and then finetuned for the supervised task. The model didn\u2019t outperform previous CBOW models significantly, but it shows that knowledge learned through unsupervised pretraining could be transfered to augment the performance on the supervised tasks.\nThe skip-thought model was proposed by [6] for learning a generic, distributed sentence encoder, and its key idea was inspired by the skip-gram model [12]. The results on 8 evaluation tasks are promising with no finetuning on the encoder, and some of the results reach other supervised trained models. In [27], they finetuned the skip-thought models on the SNLI corpus [9], which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.\n[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30]. Following the same hypothesis, [28] proposed FastSent model. It takes summation of the word embeddings in a sentence as the sentence representation, and predicts the words in both the previous sentence and the next sentence. It is an simplification of the skip-thought model, which\nassume the composition function of the words is summation. The results on SICK is comparable with the skip-thought model, while the skip-thought model still outperforms the FastSent model on the other six evaluation tasks. Later, Siamese CBOW [31] aimed to learn the word representations to make the cosine similarity of adjacent sentences in the representation space larger than that of sentences which are not adjacent. The reported comparison with the skip-thought and FastSent models on SICK dataset was convincing that the Siamese CBOW captures better sentence semantics, while no other comparisons on evaluation tasks were reported.\nInstead of learning to reconstruct the sentences which are adjacent to the current sentence, [32] proposed a model that learns to categorize the manually defined relationships of two input sentences. The model encodes two sentences in two representations, respectively, and the classifier on top of the representations judges 1) whether the two sentences are adjacent to each other, 2) whether the two sentences are in the correct order, and 3) whether the second sentence starts with a conjunction phrase. The proposed model runs faster than the skip-thought model, since it only contains an encoder and no decoder is required. However, only the result on microsoft paraphrase detection task is similar to that of the skip-thought model, and the results on other tasks are not as good."}, {"heading": "7 Conclusion", "text": "We proposed 3 techniques for trimming and also improving the skip-thought model[6], which includes dropping off one decoder, incorporating non-linear non-parametric connection, and initializing with pretrained word vectors. We empirically showed the effectiveness of our proposed techniques. In addition, our proposed trimmed skip-thought model contains much fewer parameters, which runs much faster than skip-thought model. Furthermore, our model could be facilitated by various connection methods between the encoder and the decoder, and benefit from a larger model size. Future research could make use of proposed techniques on unsupervised representation learning, and generalize to more sophisticated model types."}, {"heading": "Acknowledgments", "text": "We gratefully thank Jeffrey L. Elman, Benjamin K. Bergen, and Seana Coulson for insightful discussion, and thank Thomas Donoghue, and Reina Mizrahi for suggestive chatting. We also thank Adobe Research Lab for GPUs support, and thank NVIDIA for DGX-1 trial as well as support from NSF IIS 1528214 and NSF SMA 1041755."}], "references": [{"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, pp. 179\u2013211, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "NIPS, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["J.R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking skip-thought: A neighborhood based approach", "author": ["S. Tang", "H. Jin", "C. Fang", "Z. Wang", "V.R. de Sa"], "venue": "RepL4NLP, ACL Workshop, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Enhancing and combining sequential and tree lstm for natural language inference", "author": ["Q. Chen", "X. Zhu", "Z. Ling", "S. Wei", "H. Jiang"], "venue": "arXiv preprint arXiv:1609.06038, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "EMNLP, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014. 9", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A decomposable attention model for natural language inference", "author": ["A.P. Parikh", "O. Tackstrom", "D. Das", "J. Uszkoreit"], "venue": "EMNLP, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R.S. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "ICCV, pp. 19\u201327, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["M. Marelli", "S. Menini", "M. Baroni", "L. Bentivogli", "R. Bernardi", "R. Zamparelli"], "venue": "LREC, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["W.B. Dolan", "C. Quirk", "C. Brockett"], "venue": "COLING, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "COLING, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "ACL, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "KDD, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "ACL, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language Resources and Evaluation, vol. 39, pp. 165\u2013210, 2005.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "ACL, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards generalizable sentence embeddings", "author": ["E. Triantafillou", "J.R. Kiros", "R. Urtasun", "R. Zemel"], "venue": "RepL4NLP, ACL Workshop, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "HLT-NAACL, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, vol. 10, no. 2-3, pp. 146\u2013162, 1954.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1954}, {"title": "An exploration of discourse-based sentence spaces for compositional distributional semantics", "author": ["T. Polajnar", "L. Rimell", "S. Clark"], "venue": "Workshop on LSDSem, p. 1, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["T. Kenter", "A. Borisov", "M. de Rijke"], "venue": "ACL, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Discourse-based objectives for fast unsupervised sentence representation learning", "author": ["Y. Jernite", "S.R. Bowman", "D. Sontag"], "venue": "arXiv preprint arXiv:1705.00557, 2017. 10", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "With the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data.", "startOffset": 83, "endOffset": 92}, {"referenceID": 1, "context": "With the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data.", "startOffset": 83, "endOffset": 92}, {"referenceID": 2, "context": "With the development of deep learning techniques, recurrent neural networks (RNNs) [1, 2, 3] have shown encouraging results on natural language processing (NLP) tasks, and become the dominant methods in processing sequential data.", "startOffset": 83, "endOffset": 92}, {"referenceID": 3, "context": "[4] proposed LSTM-based sequence to sequence learning (seq2seq) model for machine translation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Later [5] applied the seq2seq model for unsupervised representation learning on language, and then finetuned the model for supervised tasks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "[6] proposed the skip-thought model, which is an encoder-decoder model for unsupervised distributed sentence representation learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "First, given the neighborhood hypothesis first proposed in [7], we directly abandon one of the decoders in the skip-thought model, and leave only one encoder and one decoder for learning from inferring the next sentence given the current one.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Second, we replace the plain connection used between the encoder and decoder with the Average+Max Connection, which is a non-linear non-parametric feature engineering method proposed by [8] for Stanford Natural Language Inference (SNLI) [9] challenge, and enhances the model to capture more complex interactions among the hidden states.", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "Second, we replace the plain connection used between the encoder and decoder with the Average+Max Connection, which is a non-linear non-parametric feature engineering method proposed by [8] for Stanford Natural Language Inference (SNLI) [9] challenge, and enhances the model to capture more complex interactions among the hidden states.", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "Figure 1: The comparison of the previously proposed skip-thought model [6], and our proposed trimmed skip-thought model.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "It includes a few simple yet effective modifications from the previously proposed skip-thought model [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Encoder: The encoder is a recurrent neural network, which is composed of bi-directional gated recurrent unit (GRU) [10], or uni-directional GRU.", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Table 1: Here presents the Gated Recurrent Unit (GRU) [3] and Conditional GRU, omitting the subscript i.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "The neighborhood hypothesis was first introduced in [7], and it pointed out that given the current sentence, inferring the previous sentence and inferring the next sentence both provide same supervision power.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "In the neighborhood hypothesis [7], the model doesn\u2019t distinguish between the sentences in a neighborhood.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "In skip-thought models [6], only the hidden state at the last time step produced from the RNN encoder is regarded as the vector representation for a given sentence, and serves as the conditional information for the decoder to reconstruct the adjacent 2 sentences.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "Recently, [9] collected a large corpus, which is SNLI, for textual entailment recognition.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "[11] proposed to summarize the hidden states from all time steps computed from a RNN encoder as a sentence representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "While [8] proposed to concatenate the outputs from an average pooling function and a max pooling function, which both run over all time steps, to serve as a sentence representation, and showed a performance boost on the SNLI dataset.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Here, since our goal is to simplify and accelerate the skip-thought model, and also get comparable results on the evaluation tasks, we consider comparing the 2 different composition functions, which are the original one used in the skip-thought model [6], denoted as Plain Connection, and the function proposed by [8], denoted as Average+Max Connection.", "startOffset": 251, "endOffset": 254}, {"referenceID": 7, "context": "Here, since our goal is to simplify and accelerate the skip-thought model, and also get comparable results on the evaluation tasks, we consider comparing the 2 different composition functions, which are the original one used in the skip-thought model [6], denoted as Plain Connection, and the function proposed by [8], denoted as Average+Max Connection.", "startOffset": 314, "endOffset": 317}, {"referenceID": 11, "context": "The proposed training methods, such as continuous bag-of-words and skip-gram [12], always serves as strong baseline models for the supervised tasks in NLP.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "The pretrained word embeddings, including word2vec [13] and GloVe [14], also boosts the model performance on the supervised tasks.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "The pretrained word embeddings, including word2vec [13] and GloVe [14], also boosts the model performance on the supervised tasks.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively.", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "We choose to initialize the word embedding matrix in the model with word2vec [13], GloVe [14], and the original method of [6] that uses random samples from a uniform distribution, respectively.", "startOffset": 122, "endOffset": 125}, {"referenceID": 14, "context": "The large corpus that we used for unsupervised training is the BookCorpus dataset [15], which contains 74 million sentences from 7000 books in total.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "All of our experiments were conducted in Torch7 [16].", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "To make the comparison fair, we follow the encoder design by [6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "In addition, [3] shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) [2].", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "In addition, [3] shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "We also reimplemented the skip-thought model under the same settings, according to [6], and the publicly available theano code 1.", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "The experiments with bi-directional encoder and unidirectional encoder were both conducted in [6], and we follow the design of these experiments.", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "In [6], for bi-skip model, the encoder contains a bi-directional GRU with 1200 dimension of each, for uni-skip model, the encoder contains a uni-directional GRU with 2400 dimension, and the decoder is a one-layer with 2400 dimension.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "For stable training, we use ADAM [17] algorithm for optimization.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "In order to generalize the model trained with relatively small, fixed vocabulary to a large amount of English words, [6] proposed a word expansion method that learns a linear projection from the pretrained word embeddings word2vec [13] to the learned RNN word embeddings.", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "In order to generalize the model trained with relatively small, fixed vocabulary to a large amount of English words, [6] proposed a word expansion method that learns a linear projection from the pretrained word embeddings word2vec [13] to the learned RNN word embeddings.", "startOffset": 231, "endOffset": 235}, {"referenceID": 5, "context": "Results reported by [6] bi-T-skip word2vec 0.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "4 bi-skip [6] random 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "4 uni-skip [6] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "4 C-skip [6] 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 179, "endOffset": 183}, {"referenceID": 19, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 220, "endOffset": 224}, {"referenceID": 20, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 320, "endOffset": 324}, {"referenceID": 21, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 356, "endOffset": 360}, {"referenceID": 22, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 409, "endOffset": 413}, {"referenceID": 23, "context": "We compared our proposed trimmed skip-thought model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness (SICK) [18], paraphrase detection (MSRP) [19], questiontype classification (TREC) [20], and 4 benchmark sentiment and subjective datasets, which includes movie review sentiment (MR) [21], customer product reviews (CR) [22], subjectivity/objectivity classification (SUBJ) [23], and opinion polarity (MPQA) [24].", "startOffset": 443, "endOffset": 447}, {"referenceID": 24, "context": "For SICK and MSRP tasks, we adopt the feature engineering idea proposed by [25].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Unlike the results in [7], these models presented in this paper contain word embeddings with lower dimension, which is half of that in [7], and GRUs with much smaller size.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "Unlike the results in [7], these models presented in this paper contain word embeddings with lower dimension, which is half of that in [7], and GRUs with much smaller size.", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": "Also, our models presented here use word2vec [13] as word embeddings initialization, which is different from random initialization applied in [7].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Also, our models presented here use word2vec [13] as word embeddings initialization, which is different from random initialization applied in [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "The results of our implemented skip-thought model differ from those presented in [6], (also presented here in the last section in Table 2), since our implementation contains much fewer parameters than the original skip-thought model, and it has word2vec [13] initialization.", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "The results of our implemented skip-thought model differ from those presented in [6], (also presented here in the last section in Table 2), since our implementation contains much fewer parameters than the original skip-thought model, and it has word2vec [13] initialization.", "startOffset": 254, "endOffset": 258}, {"referenceID": 5, "context": "The overall performance on the evaluation tasks reaches the results reported in [6] except TREC, which shows that our model with Average+Max Connection could be a fast, lighter-weight alternative to the skip-thought model.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "uni-skip [6] 69.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "4M 48M bi-skip [6] 51.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "After training, we learn a linear mapping from the word2vec 3 embedding space to RNN word embedding space, regardless of what initialization was applied in the model as in [6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "As a result, the sentence representation is a 2400-dimension vector, which matches the dimensionality of the representation reported in [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Our trimmed skip-thought models with doubled encoder performs better than the skip-thought models report in [6] on SICK and MSRP, and have comparable results on 4 classification benchmarks.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "The cut down on the training time comes from the neighborhood hypothesis[7] and many fewer parameters in the model.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "Previously, [12] proposed the continuous bag-of-words (CBOW) model and the skip-gram model for distributed word representation learning.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "[13] improved the skip-gram model, and empirically showed that additive composition of the learned word representations successfully captures contextual information of phrases and sentences, which is a strong baseline model for NLP tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Similarly, [26] proposed a method to learn a fixed-dimension vector for each sentence by predicting the words within the given sentence.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "LSTM-based autoencoder model for language representation learning was proposed by [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "The skip-thought model was proposed by [6] for learning a generic, distributed sentence encoder, and its key idea was inspired by the skip-gram model [12].", "startOffset": 39, "endOffset": 42}, {"referenceID": 11, "context": "The skip-thought model was proposed by [6] for learning a generic, distributed sentence encoder, and its key idea was inspired by the skip-gram model [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 26, "context": "In [27], they finetuned the skip-thought models on the SNLI corpus [9], which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [27], they finetuned the skip-thought models on the SNLI corpus [9], which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.", "startOffset": 67, "endOffset": 70}, {"referenceID": 27, "context": "[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30].", "startOffset": 102, "endOffset": 110}, {"referenceID": 29, "context": "[28] pointed out that the skip-thought model made use of the sentence-level distributional hypothesis [29, 30].", "startOffset": 102, "endOffset": 110}, {"referenceID": 27, "context": "Following the same hypothesis, [28] proposed FastSent model.", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "Later, Siamese CBOW [31] aimed to learn the word representations to make the cosine similarity of adjacent sentences in the representation space larger than that of sentences which are not adjacent.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "Instead of learning to reconstruct the sentences which are adjacent to the current sentence, [32] proposed a model that learns to categorize the manually defined relationships of two input sentences.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "We proposed 3 techniques for trimming and also improving the skip-thought model[6], which includes dropping off one decoder, incorporating non-linear non-parametric connection, and initializing with pretrained word vectors.", "startOffset": 79, "endOffset": 82}], "year": 2017, "abstractText": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model.", "creator": "LaTeX with hyperref package"}}}