{"id": "1706.03301", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Neural Networks and Rational Functions", "abstract": "neural networks and elementary rational functions practically efficiently approximate approximate each our other. in more detail, it conversely is now shown neatly here that for assuming any relu network, yet there exists practically a discrete rational exponential function : of degree $ o ( \\ | text { polylog } ( 1 / \\ epsilon ) ) $ which is $ \\ + epsilon $ - truly close, and : similarly only for any rational gamma function type there exists a relu network constraint of size $ o ( \\'text { polylog } ( about 1 / \\ epsilon ) ) $ and which certainly is $ \\ epsilon $ - true close. maybe by extreme contrast, equation polynomials need degree $ \\ le omega ( \\ text { poly } ( 1 / \\ _ epsilon ) ) $ to smooth approximate even including a possible single binary relu. frequently when comparing converting just a relu \\ network to a stable rational function as above, the hidden propagation constants depend periodically exponentially on the critical number of cell layers, which is indeed shown to be tight ; in my other clear words, thus a formal compositional representation can seldom be remarkably beneficial even over for compact rational functions.", "histories": [["v1", "Sun, 11 Jun 2017 03:07:42 GMT  (731kb,D)", "http://arxiv.org/abs/1706.03301v1", "To appear, ICML 2017"]], "COMMENTS": "To appear, ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["matus telgarsky"], "accepted": true, "id": "1706.03301"}, "pdf": {"name": "1706.03301.pdf", "metadata": {"source": "META", "title": "Neural networks and rational functions", "authors": ["Matus Telgarsky"], "emails": ["<mjt@illinois.edu>."], "sections": [{"heading": "1. Overview", "text": "Significant effort has been invested in characterizing the functions that can be efficiently approximated by neural networks. The goal of the present work is to characterize neural networks more finely by finding a class of functions which is not only well-approximated by neural networks, but also well-approximates neural networks.\nThe function class investigated here is the class of rational functions: functions represented as the ratio of two polynomials, where the denominator is a strictly positive polynomial. For simplicity, the neural networks are taken to always use ReLU activation \u03c3r(x) := max{0, x}; for a review of neural networks and their terminology, the reader is directed to Section 1.4. For the sake of brevity, a network with ReLU activations is simply called a ReLU network."}, {"heading": "1.1. Main results", "text": "The main theorem here states that ReLU networks and rational functions approximate each other well in the sense\n1University of Illinois, Urbana-Champaign; work completed while visiting the Simons Institute. Correspondence to: your friend <mjt@illinois.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n\u22121.00 \u22120.75 \u22120.50 \u22120.25 0.00 0.25 0.50 0.75 1.00\n0\n1\n2\n3\n4 spike\nrat poly net\nFigure 1. Rational, polynomial, and ReLU network fit to \u201cspike\u201d, a function which is 1/x along [1/4, 1] and 0 elsewhere.\nthat -approximating one class with the other requires a representation whose size is polynomial in ln(1 / ), rather than being polynomial in 1/ . Theorem 1.1. 1. Let \u2208 (0, 1] and nonnegative inte-\nger k be given. Let p : [0, 1]d \u2192 [\u22121,+1] and q : [0, 1]d \u2192 [2\u2212k, 1] be polynomials of degree \u2264 r, each with\u2264 smonomials. Then there exists a function f : [0, 1]d \u2192 R, representable as a ReLU network of size (number of nodes)\nO ( k7 ln(1 / )3\n+ min { srk ln(sr / ), sdk2 ln(dsr / )2 }) ,\nsuch that\nsup x\u2208[0,1]d \u2223\u2223\u2223\u2223f(x)\u2212 p(x)q(x) \u2223\u2223\u2223\u2223 \u2264 .\n2. Let \u2208 (0, 1] be given. Consider a ReLU network f : [\u22121,+1]d \u2192 R with at most m nodes in each of at most k layers, where each node computes z 7\u2192 \u03c3r(a\n>z + b) where the pair (a, b) (possibly distinct across nodes) satisfies \u2016a\u20161 + |b| \u2264 1. Then there exists a rational function g : [\u22121,+1]d \u2192 R with degree (maximum degree of numerator and denominator)\nO ( ln(k/ )kmk )\nsuch that\nsup x\u2208[\u22121,+1]d\n\u2223\u2223f(x)\u2212 g(x)\u2223\u2223 \u2264 .\nar X\niv :1\n70 6.\n03 30\n1v 1\n[ cs\n.L G\n] 1\n1 Ju\nn 20\n17\nPerhaps the main wrinkle is the appearance of mk when approximating neural networks by rational functions. The following theorem shows that this dependence is tight.\nTheorem 1.2. Let any integer k \u2265 3 be given. There exists a function f : R \u2192 R computed by a ReLU network with 2k layers, each with \u2264 2 nodes, such that any rational function g : R \u2192 R with \u2264 2k\u22122 total terms in the numerator and denominator must satisfy\u222b\n[0,1]\n|f(x)\u2212 g(x)|dx \u2265 1 64 .\nNote that this statement implies the desired difficulty of approximation, since a gap in the above integral (L1) distance implies a gap in the earlier uniform distance (L\u221e), and furthermore an r-degree rational function necessarily has \u2264 2r + 2 total terms in its numerator and denominator.\nAs a final piece of the story, note that the conversion between rational functions and ReLU networks is more seamless if instead one converts to rational networks, meaning neural networks where each activation function is a rational function.\nLemma 1.3. Let a ReLU network f : [\u22121,+1]d \u2192 R be given as in Theorem 1.1, meaning f has at most l layers and each node computes z 7\u2192 \u03c3r(a>z+b) where where the pair (a, b) (possibly distinct across nodes) satisfies \u2016a\u20161 + |b| \u2264 1. Then there exists a rational function R of degree O(ln(l/ )2) so that replacing each \u03c3r in f with R yields a function g : [\u22121,+1]d \u2192 R with\nsup x\u2208[\u22121,+1]d\n|f(x)\u2212 g(x)| \u2264 .\nCombining Theorem 1.2 and Lemma 1.3 yields an intriguing corollary.\nCorollary 1.4. For every k \u2265 3, there exists a function f : R \u2192 R computed by a rational network with O(k) layers andO(k) total nodes, each node invoking a rational activation of degreeO(k), such that every rational function g : R\u2192 R with less than 2k\u22122 total terms in the numerator and denominator satisfies\u222b\n[0,1]\n|f(x)\u2212 g(x)|dx \u2265 1 128 .\nThe hard-to-approximate function f is a rational network which has a description of size O(k2). Despite this, attempting to approximate it with a rational function of the usual form requires a description of size \u2126(2k). Said another way: even for rational functions, there is a benefit to a neural network representation!"}, {"heading": "1.2. Auxiliary results", "text": "The first thing to stress is that Theorem 1.1 is impossible with polynomials: namely, while it is true that ReLU networks can efficiently approximate polynomials (Yarotsky, 2016; Safran & Shamir, 2016; Liang & Srikant, 2017), on the other hand polynomials require degree \u2126(poly(1/ )), rather than O(poly(ln(1/ ))), to approximate a single ReLU, or equivalently the absolute value function (Petrushev & Popov, 1987, Chapter 4, Page 73).\nAnother point of interest is the depth needed when converting a rational function to a ReLU network. Theorem 1.1 is impossible if the depth is o(ln(1/ )): specifically, it is impossible to approximate the degree 1 rational function x 7\u2192 1/x with size O(ln(1/ )) but depth o(ln(1/ )). Proposition 1.5. Set f(x) := 1/x, the reciprocal map. For any > 0 and ReLU network g : R\u2192 R with l layers and m < (27648 )\u22121/(2l)/2 nodes,\u222b\n[1/2,3/4]\n|f(x)\u2212 g(x)|dx > .\nLastly, the implementation of division in a ReLU network requires a few steps, arguably the most interesting being a \u201ccontinuous switch statement\u201d, which computes reciprocals differently based on the magnitude of the input. The ability to compute switch statements appears to be a fairly foundational operation available to neural networks and rational functions (Petrushev & Popov, 1987, Theorem 5.2), but is not available to polynomials (since otherwise they could approximate the ReLU)."}, {"heading": "1.3. Related work", "text": "The results of the present work follow a long line of work on the representation power of neural networks and related functions. The ability of ReLU networks to fit continuous functions was no doubt proved many times, but it appears the earliest reference is to Lebesgue (Newman, 1964, Page 1), though of course results of this type are usu-\nally given much more contemporary attribution (Cybenko, 1989). More recently, it has been shown that certain function classes only admit succinct representations with many layers (Telgarsky, 2015). This has been followed by proofs showing the possibility for a depth 3 function to require exponentially many nodes when rewritten with 2 layers (Eldan & Shamir, 2016). There are also a variety of other result giving the ability of ReLU networks to approximate various function classes (Cohen et al., 2016; Poggio et al., 2017).\nMost recently, a variety of works pointed out neural networks can approximate polynomials, and thus smooth functions essentially by Taylor\u2019s theorem (Yarotsky, 2016; Safran & Shamir, 2016; Liang & Srikant, 2017). This somewhat motivates this present work, since polynomials can not in turn approximate neural networks with a dependence O(poly log(1/ )): they require degree \u2126(1/ ) even for a single ReLU.\nRational functions are extensively studied in the classical approximation theory literature (Lorentz et al., 1996; Petrushev & Popov, 1987). This literature draws close connections between rational functions and splines (piecewise polynomial functions), a connection which has been used in the machine learning literature to draw further connections to neural networks (Williamson & Bartlett, 1991). It is in this approximation theory literature that one can find the following astonishing fact: not only is it possible to approximate the absolute value function (and thus the ReLU) over [\u22121,+1] to accuracy > 0 with a rational function of degreeO(ln(1/ )2) (Newman, 1964), but moreover the optimal rate is known (Petrushev & Popov, 1987; Zolotarev, 1877)! These results form the basis of those results here which show that rational functions can approximate ReLU networks. (Approximation theory results also provide other functions (and types of neural networks) which rational functions can approximate well, but the present work will stick to the ReLU for simplicity.)\nAn ICML reviewer revealed prior work which was embarrassingly overlooked by the author: it has been known, since decades ago (Beame et al., 1986), that neural networks using threshold nonlinearities (i.e., the map x 7\u2192 1[x \u2265 0]) can approximate division, and moreover the proof is similar to the proof of part 1 of Theorem 1.1! Moreover, other work on threshold networks invoked Newman polynomials to prove lower bound about linear threshold networks (Paturi & Saks, 1994). Together this suggests that not only the connections between rational functions and neural networks are tight (and somewhat known/unsurprising), but also that threshold networks and ReLU networks have perhaps more similarities than what is suggested by the differing VC dimension bounds, approximation results, and algorithmic results (Goel et al., 2017)."}, {"heading": "1.4. Further notation", "text": "Here is a brief description of the sorts of neural networks used in this work. Neural networks represent computation as a directed graph, where nodes consume the outputs of their parents, apply a computation to them, and pass the resulting value onward. In the present work, nodes take their parents\u2019 outputs z and compute \u03c3r(a>z + b), where a is a vector, b is a scalar, and \u03c3r(x) := max{0, x}; another popular choice of nonlineary is the sigmoid x 7\u2192 (1 + exp(\u2212x))\u22121. The graphs in the present work are acyclic and connected with a single node lacking children designated as the univariate output, but the literature contains many variations on all of these choices.\nAs stated previously, a rational function f : Rd \u2192 R is ratio of two polynomials. Following conventions in the approximation theory literature (Lorentz et al., 1996), the denominator polynomial will always be strictly positive. The degree of a rational function is the maximum of the degrees of its numerator and denominator."}, {"heading": "2. Approximating ReLU networks with rational functions", "text": "This section will develop the proofs of part 2 of Theorem 1.1, Theorem 1.2, Lemma 1.3, and Corollary 1.4."}, {"heading": "2.1. Newman polynomials", "text": "The starting point is a seminal result in the theory of rational functions (Zolotarev, 1877; Newman, 1964): there exists a rational function of degree O(ln(1/ )2) which can approximate the absolute value function along [\u22121,+1] to accuracy > 0. This in turn gives a way to approximate the ReLU, since\n\u03c3r(x) = max{0, x} = x+ |x|\n2 . (2.1)\nThe construction here uses the Newman polynomials (New-\nman, 1964): given an integer r, define\nNr(x) := r\u22121\u220f i=1 (x+ exp(\u2212i/ \u221a r)).\nThe Newman polynomials N5, N9, and N13 are depicted in Figure 3. Typical polynomials in approximation theory, for instance the Chebyshev polynomials, have very active oscillations; in comparison, the Newman polynomials look a little funny, lying close to 0 over [\u22121, 0], and quickly increasing monotonically over [0, 1]. The seminal result of Newman (1964) is that\nsup |x|\u22641 \u2223\u2223\u2223\u2223\u2223|x| \u2212 x ( Nr(x)\u2212Nr(\u2212x) Nr(x) +Nr(\u2212x) )\u2223\u2223\u2223\u2223\u2223 \u2264 3 exp(\u2212\u221ar)/2. Thanks to this bound and eq. (2.1), it follows that the ReLU can be approximated to accuracy > 0 by rational functions of degree O(ln(1/ )2).\n(Some basics on Newman polynomials, as needed in the present work, can be found in Appendix A.1.)"}, {"heading": "2.2. Proof of Lemma 1.3", "text": "Now that a single ReLU can be easily converted to a rational function, the next task is to replace every ReLU in a ReLU network with a rational function, and compute the approximation error. This is precisely the statement of Lemma 1.3.\nThe proof of Lemma 1.3 is an induction on layers, with full details relegated to the appendix. The key computation, however, is as follows. Let R(x) denote a rational approximation to \u03c3r. Fix a layer i + 1, and let H(x) denote the multi-valued mapping computed by layer i, and let HR(x) denote the mapping obtained by replacing each \u03c3r in H with R. Fix any node in layer i + 1, and let x 7\u2192 \u03c3r(a>H(x) + b) denote its output as a function of the input. Then\u2223\u2223\u2223\u03c3r(a>H(x) + b)\u2212R(a>HR(x) + b)\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u03c3r(a>H(x) + b)\u2212 \u03c3r(a>HR(x) + b)\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38\n\u2665 + \u2223\u2223\u2223\u03c3r(a>HR(x) + b)\u2212R(a>HR(x) + b)\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38\n\u2663\n.\nFor the first term \u2665, note since \u03c3r is 1-Lipschitz and by Ho\u0308lder\u2019s inequality that\n\u2665 \u2264 \u2223\u2223\u2223a>(H(x)\u2212HR(x))\u2223\u2223\u2223 \u2264 \u2016a\u20161\u2016H(x)\u2212HR(x)\u2016\u221e,\nmeaning this term has been reduced to the inductive hypothesis since \u2016a\u20161 \u2264 1. For the second term \u2663, if\na>HR(x) + b can be shown to lie in [\u22121,+1] (which is another easy induction), then \u2663 is just the error between R and \u03c3r on the same input."}, {"heading": "2.3. Proof of part 2 of Theorem 1.1", "text": "It is now easy to find a rational function that approximates a neural network, and to then bound its size. The first step, via Lemma 1.3, is to replace each \u03c3r with a rational functionR of low degree (this last bit using Newman polynomials). The second step is to inductively collapse the network into a single rational function. The reason for the dependence on the number of nodes m is that, unlike polynomials, summing rational functions involves an increase in degree:\np1(x) q1(x) + p1(x) q2(x) = p1(x)q2(x) + p2(x)q1(x) q1(x)q2(x) ."}, {"heading": "2.4. Proof of Theorem 1.2", "text": "The final interesting bit is to show that the dependence on ml in part 2 of Theorem 1.1 (where m is the number of nodes and l is the number of layers) is tight.\nRecall the \u201ctriangle function\u201d\n\u2206(x) :=  2x x \u2208 [0, 1/2], 2(1\u2212 x) x \u2208 (1/2, 1], 0 otherwise.\nThe k-fold composition \u2206k is a piecewise affine function with 2k\u22121 regularly spaced peaks (Telgarsky, 2015). This function was demonstrated to be inapproximable by shallow networks of subexponential size, and now it can be shown to be a hard case for rational approximation as well.\nConsider the horizontal line through y = 1/2. The function \u2206k will cross this line 2k times. Now consider a rational function f(x) = p(x)/q(x). The set of points where f(x) = 1/2 corresponds to points where 2p(x)\u2212q(x) = 0.\nA poor estimate for the number of zeros is simply the degree of 2p\u2212q, however, since f is univariate, a stronger tool becomes available: by Descartes\u2019 rule of signs, the number of zeros in f \u2212 1/2 is upper bounded by the number of terms in 2p\u2212 q."}, {"heading": "3. Approximating rational functions with ReLU networks", "text": "This section will develop the proof of part 1 of Theorem 1.1, as well as the tightness result in Proposition 1.5"}, {"heading": "3.1. Proving part 1 of Theorem 1.1", "text": "To establish part 1 of Theorem 1.1, the first step is to approximate polynomials with ReLU networks, and the second is to then approximate the division operation.\nThe representation of polynomials will be based upon constructions due to Yarotsky (2016). The starting point is the following approximation of the squaring function. Lemma 3.1 ((Yarotsky, 2016)). Let any > 0 be given. There exists f : x \u2192 [0, 1], represented as a ReLU network with O(ln(1/ )) nodes and layers, such that supx\u2208[0,1] |f(x)\u2212 x2| \u2264 and f(0) = 0.\nYarotsky\u2019s proof is beautiful and deserves mention. The approximation of x2 is the function fk, defined as\nfk(x) := x\u2212 k\u2211 i=1 \u2206i(x) 4i ,\nwhere \u2206 is the triangle map from Section 2. For every k, fk is a convex, piecewise-affine interpolation between points along the graph of x2; going from k to k + 1 does not adjust any of these interpolation points, but adds a new set of O(2k) interpolation points.\nOnce squaring is in place, multiplication comes via the polarization identity xy = ((x+ y)2 \u2212 x2 \u2212 y2)/2. Lemma 3.2 ((Yarotsky, 2016)). Let any > 0 and B \u2265 1 be given. There exists g(x, y) : [0, B]2 \u2192 [0, B2], represented by a ReLU network with O(ln(B/ ) nodes and layers, with\nsup x,y\u2208[0,1]\n|g(x, y)\u2212 xy| \u2264\nand g(x, y) = 0 if x = 0 or y = 0.\nNext, it follows that ReLU networks can efficiently approximate exponentiation thanks to repeated squaring. Lemma 3.3. Let \u2208 (0, 1] and positive integer y be given. There exists h : [0, 1] \u2192 [0, 1], represented by a ReLU network with O(ln(y/ )2) nodes and layers, with\nsup x,y\u2208[0,1]\n\u2223\u2223h(x)\u2212 xy\u2223\u2223 \u2264\nWith multiplication and exponentiation, a representation result for polynomials follows.\nLemma 3.4. Let \u2208 (0, 1] be given. Let p : [0, 1]d \u2192 [\u22121,+1] denote a polynomial with \u2264 s monomials, each with degree \u2264 r and scalar coefficient within [\u22121,+1]. Then there exists a function q : [0, 1]d \u2192 [\u22121,+1] computed by a network of size O ( min{sr ln(sr/ ), sd ln(dsr/ )2} ) , which satisfies supx\u2208[0,1]d |p(x)\u2212 q(x)| \u2264 .\nThe remainder of the proof now focuses on the division operation. Since multiplication has been handled, it suffices to compute a single reciprocal.\nLemma 3.5. Let \u2208 (0, 1] and nonnegative integer k be given. There exists a ReLU network q : [2\u2212k, 1] \u2192 [1, 2k], of sizeO(k2 ln(1/ )2) and depthO(k4 ln(1/ )3) such that\nsup x\u2208[2\u2212k,1] \u2223\u2223\u2223\u2223q(x)\u2212 1x \u2223\u2223\u2223\u2223 \u2264 .\nThis proof relies on two tricks. The first is to observe, for x \u2208 (0, 1], that\n1 x =\n1 1\u2212 (1\u2212 x) = \u2211 i\u22650 (1\u2212 x)i.\nThanks to the earlier development of exponentiation, truncating this summation gives an expression easily approximate by a neural network as follows.\nLemma 3.6. Let 0 < a \u2264 b and > 0 be given. Then there exists a ReLU network q : R \u2192 R with O(ln(1/(a ))2) layers and O((b/a) ln(1/(a ))3) nodes satisfying\nsup x\u2208[a,b] \u2223\u2223\u2223\u2223q(x)\u2212 1x \u2223\u2223\u2223\u2223 \u2264 2 .\nUnfortunately, Lemma 3.6 differs from the desired statement Lemma 3.6: inverting inputs lying within [2\u2212k, 1] requires O(2k ln(1/ )2) nodes rather than O(k4 ln(1/ )3)!\nTo obtain a good estimate with only O(ln(1/ )) terms of the summation, it is necessary for the input to be x bounded below by a positive constant (not depending on k). This leads to the second trick (which was also used by Beame et al. (1986)!).\nConsider, for positive constant c > 0, the expression\n1 x =\nc\n1\u2212 (1\u2212 cx) = c \u2211 i\u22650 (1\u2212 cx)i.\nIf x is small, choosing a larger c will cause this summation to converge more quickly. Thus, to compute 1/x accurately over a wide range of inputs, the solution here is to multiplex approximations of the truncated sum for many choices of c. In order to only rely on the value of one of them, it is possible to encode a large \u201cswitch\u201d style statement in a neural network. Notably, rational functions can also representat switch statements (Petrushev & Popov, 1987, Theorem 5.2), however polynomials can not (otherwise they could approximate the ReLU more efficiently, seeing as it is a switch statement of 0 (a degree 0 polynomial) and x (a degree 1 polynomial). Lemma 3.7. Let > 0, B \u2265 1, reals a0 \u2264 a1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 an \u2264 an+1 and a function f : [a0, an+1] \u2192 R be given. Moreover, suppose for i \u2208 {1, . . . , n}, there exists a ReLU network gi : R \u2192 R of size \u2264 mi and depth \u2264 ki with gi \u2208 [0, B] along [ai\u22121, ai+1] and\nsup x\u2208[ai\u22121,ai+1]\n|gi(x)\u2212 f | \u2264 .\nThen there exists a function g : R \u2192 R computed by a ReLU network of size O ( n ln(B/ ) + \u2211 imi ) and depth\nO ( ln(B/ ) + maxi ki ) satisfying\nsup x\u2208[a1,an]\n|g(x)\u2212 f(x)| \u2264 3 ."}, {"heading": "3.2. Proof of Proposition 1.5", "text": "It remains to show that shallow networks have a hard time approximating the reciprocal map x 7\u2192 1/x.\nThis proof uses the same scheme as various proofs in (Telgarsky, 2016), which was also followed in more recent works (Yarotsky, 2016; Safran & Shamir, 2016): the idea is to first upper bound the number of affine pieces in ReLU networks of a certain size, and then to point out that each linear segment must make substantial error on a curved function, namely 1/x.\nThe proof is fairly brute force, and thus relegated to the appendices."}, {"heading": "4. Summary of figures", "text": "Throughout this work, a number of figures were presented to show not only the astonishing approximation properties\nof rational functions, but also the higher fidelity approximation achieved by both ReLU networks and rational functions as compared with polynomials. Of course, this is only a qualitative demonstration, but still lends some intuition.\nIn all these demonstrations, rational functions and polynomials have degree 9 unless otherwise marked. ReLU networks have two hidden layers each with 3 nodes. This is not exactly apples to apples (e.g., the rational function has twice as many parameters as the polynomial), but still reasonable as most of the approximation literature fixes polynomial and rational degrees in comparisons.\nFigure 1 shows the ability of all three classes to approximate a truncated reciprocal. Both rational functions and ReLU networks have the ability to form \u201cswitch statements\u201d that let them approximate different functions on different intervals with low complexity (Petrushev & Popov, 1987, Theorem 5.2). Polynomials lack this ability; they can not even approximate the ReLU well, despite it being low degree polynomials on two separate intervals.\nFigure 2 shows that rational functions can fit the threshold function errily well; the particular rational function used here is based on using Newman polynomials to approximate (1 + |x|/x)/2 (Newman, 1964).\nFigure 3 shows Newman polynomialsN5,N9,N13. As discussed in the text, they are unlike orthogonal polynomials, and are used in all rational function approximations except Figure 1, which used a least squares fit.\nFigure 4 shows that rational functions (via the Newman polynomials) fit \u2206 very well, whereas polynomials have trouble. These errors degrade sharply after recursing, namely when approximating \u22063 as in Figure 6.\nFigure 5 shows how polynomials and rational functions fit the ReLU, where the ReLU representation, based on Newman polynomials, is the one used in the proofs here. Despite the apparent slow convergence of polynomials in this regime, the polynomial fit is still quite respectable."}, {"heading": "5. Open problems", "text": "There are many next steps for this and related results.\n1. Can rational functions, or some other approximating class, be used to more tightly bound the generalization properties of neural networks? Notably, the VC dimension of sigmoid networks uses a conversion to polynomials (Anthony & Bartlett, 1999).\n2. Can rational functions, or some other approximating class, be used to design algorithms for training neural networks? It does not seem easy to design reasonable algorithms for minimization over rational functions; if this is fundamental and moreover in contrast with neural networks, it suggests an algorithmic benefit of neural networks.\n3. Can rational functions, or some other approximating class, give a sufficiently refined complexity estimate of neural networks which can then be turned into a regularization scheme for neural networks?"}, {"heading": "Acknowledgements", "text": "The author thanks Adam Klivans and Suvrit Sra for stimulating conversations. Adam Klivans and the author both thank Almare Gelato Italiano, in downtown Berkeley, for necessitating further stimulating conversations, but now on the topic of health and exercise. Lastly, the author thanks the University of Illinois, Urbana-Champaign, and the Simons Institute in Berkeley, for financial support during this work."}, {"heading": "A. Deferred material from Section 2", "text": "This section collects technical material omitted from Section 2. The first step is to fill in some missing details regarding Newman polynomials.\nA.1. Newman polynomials\nDefine the Newman polynomial (Newman, 1964)\nNr(x) := r\u22121\u220f i=1 (x+ \u03b1ir) where \u03b1r := exp(\u22121/ \u221a r). (A.1)\nDefine Ar(x), the Newman approximation to |x|, as\nAr(x) := x ( Nr(x)\u2212Nr(\u2212x) Nr(x) +Nr(\u2212x) ) .\nLemma A.2 (Newman (1964)). Suppose r \u2265 5.\n\u2022 Nr(x) +Nr(\u2212x) > 0; in particular, Ar is well-defined over R.\n\u2022 Given any b \u2265 1, sup\nx\u2208[\u2212b,+b] \u2223\u2223bAr(x/b)\u2212 |x|\u2223\u2223 \u2264 3b exp(\u2212\u221ar). Proof. \u2022 If x = 0, then Nr(\u2212x) = Nr(x) = \u220fr\u22121 i=1 \u03b1 i r > 0. Otherwise x > 0, and note for any i \u2208 {1, . . . , r \u2212 1} that\n\u2013 x \u2208 (0, \u03b1ir] means |x\u2212 \u03b1ir| = \u03b1ir \u2212 x < \u03b1ir + x, \u2013 x > \u03b1ir means |x\u2212 \u03b1ir| = x\u2212 \u03b1ir < x+ \u03b1ir.\nTogether, |x\u2212 \u03b1ir| < x+ \u03b1ir, and\nNr(x) = r\u22121\u220f i=1 (x+ \u03b1ir) > r\u22121\u220f i=1 |x\u2212 \u03b1ir| = \u2223\u2223\u2223\u2223\u2223\u2223 r\u22121\u220f i=1 (x\u2212 \u03b1ir) \u2223\u2223\u2223\u2223\u2223\u2223 = |Nr(\u2212x)|. Since Nr(x) > 0 when x > 0, thus Nr(x) +Nr(\u2212x) > Nr(x)\u2212 |Nr(x)| = 0. Lastly, the case x < 0 follows from the case x > 0 since x 7\u2192 Nr(x) +Nr(\u2212x) is even.\n\u2022 For any x \u2208 [\u2212b,+b],\n||x| \u2212 bAr(x/b)| = \u2223\u2223\u2223b (|x/b| \u2212Ar(x/b))\u2223\u2223\u2223 = b \u2223\u2223x/b\u2212Ar(x/b)\u2223\u2223 \u2264 3b exp(\u2212\u221ar),\nwhere the last step was proved by Newman (Lorentz et al., 1996, Theorem 7.3.1).\nFinally, define\nR\u0303r,b(x) := Rr(x; b) := x+ bAr(x/b)\n2 ,\nr,b := 3 exp(\u2212 \u221a r)/2,\nRr,b(x) := (1\u2212 2 r,b)R\u0303r,b(x) + b r,b.\nLemma A.3. If r \u2265 5 and b \u2265 1 and r,b \u2264 1/2, then Rr,b is a degree-r rational function over R, and\nsup x\u2208[\u2212b,+b] \u2223\u2223\u2223\u03c3r(x)\u2212 R\u0303r,b(x)\u2223\u2223\u2223 \u2264 b r,b, sup x\u2208[\u2212b,+b]\n\u2223\u2223\u03c3r(x)\u2212Rr,b(x)\u2223\u2223 \u2264 3b r,b. If r,b \u2264 1, then Rr,b \u2208 [0, b] along [\u2212b,+b].\nProof. Let r, b be given, and for simplicity omit the various subscripts. The denominator of R\u0303 is positive over R by Lemma A.2. Now fix x \u2208 [\u2212b,+b]. Using the second part of Lemma A.2,\u2223\u2223\u2223\u03c3r(x)\u2212 R\u0303(x)\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223x+ |x|2 \u2212 x+ bA(x/b)2\n\u2223\u2223\u2223\u2223 = 12 \u2223\u2223|x| \u2212 bA(x/b)\u2223\u2223 \u2264 3b exp(\u2212\u221ar)/2 = b . Next, note that R\u0303 \u2208 [\u2212b , b(1 + )]:\nR\u0303(x) \u2264 \u03c3r(x) + b \u2264 b(1 + ), R\u0303(x) \u2265 \u03c3r(x)\u2212 b \u2265 \u2212b .\nThus \u2223\u2223\u03c3r(x)\u2212R(x)\u2223\u2223 \u2264 \u2223\u2223\u2223\u03c3r(x)\u2212 R\u0303(x)\u2223\u2223\u2223+\u2223\u2223\u2223R\u0303(x)\u2212R(x)\u2223\u2223\u2223 \u2264 b + 0 + 2\n\u2223\u2223\u2223R\u0303(x)\u2212 b/2\u2223\u2223\u2223 \u2264 3b .\nMoreover\nR(x) = (1\u2212 2 )R\u0303(x) + b \u2265 (1\u2212 2 )(\u2212b ) + b \u2265 0, R(x) \u2264 (1\u2212 2 )b(1 + ) + b \u2264 b.\nA.2. Remaining deferred proofs\nThe details of converting a ReLU network into a rational network are as follows.\nLemma A.4. Let f : Rd \u2192 R be represented by a ReLU network with \u2264 l layers, and with each node computing a map z 7\u2192 \u03c3r(a>z + b) where \u2016a\u20161 + |b| \u2264 1. Then for every > 0 there exists a function g : Rd \u2192 R with |g(x)\u2212 f(x)| \u2264 for \u2016x\u2016\u221e \u2264 1 where g is obtained from f by replacing each ReLU with an r-rational function with r = O(ln(1/ )2).\nProof of Lemma 1.3. This construction will use the Newman-based approximation R := Rr,b to \u03c3r with degree O(ln(l/ )2). By Lemma A.3, this degree suffices to guarantee R(x) \u2208 [0, 1] and |R(x)\u2212 \u03c3r(x)| \u2264 /l for |x| \u2264 1.\nFirst note, by induction on layers, that the output of every node has absolute value at most 1. The base case is the inputs themselves, and thus the statement holds by the assumption \u2016x\u2016\u221e \u2264 1. In the inductive step, consider any node z 7\u2192 R(a>z + b), where z is the multivariate input to this node. By the inductive hypothesis, \u2016z\u2016\u221e \u2264 1, thus\n|a>z + b| \u2264 \u2016a\u20161\u2016z\u2016\u221e + |b| \u2264 1.\nAs such, R(a>z + b) \u2208 [0, 1].\nIt remains to prove the error bound. For any node, if h : Rd \u2192 R denote the function (of the input x) compute by this node, then let hR denote the function obtained by replacing all ReLUs with R. It will be shown that every node in layer i has |hR(x)\u2212 h(x)| \u2264 i /l when \u2016x\u2016\u221e \u2264 1. The base case is the inputs themselves, and thus there is no approximation error, meaning the bound holds with error 0 \u2264 1 \u00b7 /l. Now consider any node in layer i + 1 with i \u2265 0, and suppose the claim holds for nodes in layers i and lower. For convenience, let H denote the multivalued map computed by the previous layer,\nand HR denote the multivalued map obtained by replacing all activations in earlier layers with R. Since \u03c3r is 1-Lipschitz, and since the earlier boundedness property grants\u2223\u2223\u2223a>HR(x) + b\u2223\u2223\u2223 \u2264 \u2016a\u20161\u2016HR(x)\u2016\u221e + |b| \u2264 1, then\n|h(x)\u2212 hR(x)| = \u2223\u2223\u2223\u03c3r(a>H(x) + b)\u2212R(a>HR(x) + b)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u03c3r(a>H(x) + b)\u2212 \u03c3r(a>HR(x) + b)\u2223\u2223\u2223+\u2223\u2223\u2223\u03c3r(a>HR(x) + b)\u2212R(a>HR(x) + b)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223a>H(x)\u2212 a>HR(x)\u2223\u2223\u2223+ /l \u2264 \u2016a\u20161\u2016H \u2212HR\u2016\u221e + /l \u2264 (i+ 1) /l.\nNext, collapsing a rational network down into a single rational function is proved as follows.\nLemma A.5. Let f : Rd \u2192 R be a rational network with \u2264 m nodes in each of \u2264 l layers, and the activation function has degree r. Then the rational function obtained by collapsing f has degree at most (rm)l.\nProof. Throughout this proof, let R denote the rational activation function at each node, and write R(x) = p(x)/q(x) where p and q are polynomials of degree at most r. The proof establishes, by induction on layers, that the nodes of layer i compute rational functions of degree at most (rm)i. The base case is layer 1, where each node computes a rational function of degree r \u2264 rm. For the case of layer i > 1, fix any node, and denote its computation by h(x) = R( \u2211n j=1 ajgj(x) + b), where n \u2264 m and gj = pj/qj is a rational function of degree at most (rm)i\u22121. Note\ndeg \u2211 j ajpj(x) qj(x) + b  = deg(b\u220fj qj(x) +\u2211j ajpj(x)\u220fk 6=j qk(x)\u220f j qj(x) ) \u2264 m(mr)i\u22121.\nthe map f := \u2211 j ajgj + b is rational of degree m(mr)\ni\u22121. Let pf and qf denote its numerator and denominator. Since R is univariate, its numerator p and denominator q have the form p(x) := \u2211 j\u2264r cjx j and q(x) \u2211 j\u2264r djx\nj . Thus, using the fact that q > 0,\ndeg(h(x)) = deg(R(f(x))) = deg \u2211j\u2264r cj(pf (x)/qf (x))j\u2211 j\u2264r dj(pf (x)/qf (x)) j ( qf (x) r qf (x)r ) = deg (\u2211 j\u2264r cjpf (x) jqf (x)) r\u2212j\u2211\nj\u2264r djpf (x) jqf (x))r\u2212j\n) \u2264 rm(rm)i\u22121 = (rm)i.\nThe proof of part 2 of Theorem 1.1 now follows by combining Lemmas 1.3, A.3 and A.5.\nThe last piece is a slighly more detailed account of Theorem 1.2.\nProof of Theorem 1.2. Let \u2206 : R\u2192 R denote the triangle function from (Telgarsky, 2015):\n\u2206(x) :=  2x x \u2208 [0, 1/2], 2(1\u2212 x) x \u2208 (1/2, 1], 0 otherwise.\nDefine the target function f = \u2206k, which as in (Telgarsky, 2015) has 2k regular-spaced crossings of 0 along [0, 1], and can be written as a network with 2k layers, each with \u2264 2 nodes.\nNext consider the rational function g. As in the text, it is necessary to count the zeros of g \u2212 1/2 (the case g = 1/2 is trivial). Writing g = p/q, equivalently this means the zeros of 2p \u2212 q. Since p and q together have \u2264 2k\u22122 terms, by Descartes\u2019 rule of signs, g crosses 1/2 at most 2k\u22122 times along (0, 1]. Therefore, following a similar calculation to the proof in (Telgarsky, 2016, Proof of Theorem 1.1),\u222b\n(0,1]\n|f(x)\u2212 g(x)|dx \u2265 1 32\n( 1\u2212 2(2 k\u22122)\n2k\n) = 1\n64 ."}, {"heading": "B. Deferred material from Section 3", "text": "B.1. Towards the proof of part 1 of Theorem 1.1\nTo start, the lemmas due to Yarotsky are slightly adjusted to clip the range to [0, 1].\nProof of Lemma 3.1. Inspecting Yarotsky\u2019s proof, the construction provides g(x) with g(0) = 0 and supx\u2208[0,1] |g(x) \u2212 x2| \u2264 . To provide the desired f , it suffices to define f(x) = \u03c3r(g(x))\u2212 \u03c3r(g(x)\u2212 1).\nProof of Lemma 3.2. First suppose B = 1, let f be as in Lemma 3.1 at resolution /8, and define h via the polarization identity (as in Yarotsky\u2019s proof):\nh(x, y) = 2(f(x/2 + y/2)\u2212 f(x/2)\u2212 f(y/2))\n(where x/2 appears since f has domain [0, 1]2). Since f(0) = 0,\nh(x, 0) = 2(f(x/2)\u2212 f(x/2)\u2212 0) = 0, h(0, y) = 2(f(y/2)\u2212 0\u2212 f(y/2)) = 0.\nMoreover, for any x, y \u2208 [0, 1]\nh(x, y)\u2212 xy \u2264 2 ( (x/2 + y/2)2 + /8\u2212 x2/4 + /8\u2212 y2/4 + /8 ) \u2212 xy \u2264 xy + ,\nh(x, y)\u2212 xy \u2265 2 ( (x/2 + y/2)2 \u2212 /8\u2212 x2/4\u2212 /8\u2212 y2/4\u2212 /8 ) \u2212 xy \u2264 xy \u2212 .\nFinally, set g\u0303(x, y) := \u03c3r(h(x, y))\u2212 \u03c3r(h(x, y)\u2212 1), which preserves the other properties.\nNow consider the case B \u2265 1, and set g(x, y) = B2g(x/B, y/B). Then (x, y) \u2208 [0, B]2 implies\u2223\u2223g(x, y)\u2212 xy\u2223\u2223 = B2\u2223\u2223g\u0303(x/B, y/B)\u2212 (x/B)(y/B)\u2223\u2223 \u2264 B2, and g \u2208 [0, B2] over [0, B]2 since g\u0303 \u2208 [0, 1] over [0, 1]2.\nThe full details for the proof of fast exponentiation are as follows.\nProof of Lemma 3.3. This proof constructs a network implementing the russian peasant algorithm for exponentiation:\n1. Set v := 1.\n2. For b \u2208 bits-ltr(y) (the bits of y from left to right):\n(a) Set v := v2. (b) If b = 1, set v := vx.\nFor example, x101012 = ((((12 \u00b7 x)2)2 \u00b7 x)2)2 \u00b7 x = x2 4 \u00b7 x2 2 \u00b7 x.\nThe two lines in the inner loop will use the squaring function f from Lemma 3.1 and the multiplication function g from Lemma 3.2, each with accuracy c where c := 1/y2. At the end, the network returns \u03c3r(v)\u2212\u03c3r(v\u22121) to ensure the output lies in [0, 1]; this procedure can not increase the error. Since the loop is invokeO(ln(y)) times and each inner loop requires a network of size O(ln(1/(c ))) = O(ln(y/ )), the full network has size O(ln(y/ )2).\nIt remains to show that the network computes a function h which satisfies\nh(x) = xy.\nLet zj denote the integer corresponding to the first j bits of y when read left-to-right; it will be shown by induction (on the bits of y from left to right) that, at the end of the jth invocation of the loop,\n|v \u2212 xzj | \u2264 z2j c .\nThis suffices to establish the claim since then |v \u2212 xy| \u2264 y2c = .\nFor the base case, consider j = 0; then v = 1 = xz0 = x0 as desired. For the inductive step, let w denote v at the end of the previous iteration, whereby the inductive hypothesis grants\n|w \u2212 xzj\u22121 | \u2264 z2j\u22121c .\nThe error after the approximate squaring step can be upper bounded as\nf(w)\u2212 x2zj\u22121 \u2264 ( f(w)\u2212 w2 ) + ( w2 \u2212 x2zj\u22121 ) \u2264 c + ( (xzj\u22121 + z2j\u22121c ) 2 \u2212 x2zj\u22121 )\n\u2264 c + 2z2j\u22121c + z4j\u22121c2 2 \u2264 c + 2z2j\u22121c + z2j\u22121c \u2264 (2zj\u22121)2c .\nThe reverse inequality is proved analogously, thus\u2223\u2223\u2223f(w)\u2212 x2zj\u22121 \u2223\u2223\u2223 \u2264 (2zj\u22121)2c . If the bit b in this iteration is 0, then 2zj\u22121 = zj and the proof for this loop iteration is complete. Otherwise b = 1, and\nv \u2212 xzj = g(f(w), x)\u2212 x2zj\u22121+b\n\u2264 xf(w) + c \u2212 x2zj\u22121+b \u2264 ( (2zj\u22121) 2 + 1 ) c\n\u2264 (zj)2c .\nThe proof of the reverse inequality is analogous, which establishes the desired error bound on v for this loop iteration.\nUsing the preceding exponentiation lemma, the proof of polynomial approximation is as follows.\nProof of Lemma 3.4. It will be shown momentarily that a single monomial term can be approximating to accuracy /swith a network of size O ( min{r ln(sr/ ), d ln(dsr/ )2} ) . This implies the result by summing \u2264 s monomials comprising a polynomial, along with their errors.\nFor a single monomial, here are two constructions.\n\u2022 One approach is to product together \u2264 r individual variables (and lastly multiple by a fixed scalar coefficient), with no concern of the multiplicities of individual variables. To this end, let (y1, . . . , yk) with s \u2264 r denote coordinates of the input variable so that \u220fk i=1 yi is the desired multinomial. Let g denote multiplication with error 0 := /(rs)\nas provided by Lemma 3.2. The network will compute \u03b1gi(y), where \u03b1 \u2208 [\u22121,+1] is the scalar coefficient on the monomial, and gi is recursively defined as\ng1(y) = y1, gi+1(y) := f(yi+1, gi(y))\nIt is established by induction that \u2223\u2223\u2223\u2223\u2223\u2223gi(y)\u2212 i\u220f\nj=1\nyj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 j 0 The base case is immediate since g1(y) = y1 = \u220f1 j=1 yj . For the inductive step, gi+1(y)\u2212 i+1\u220f j=1 yj = f(yi+1, gi(y))\u2212 i+1\u220f j=1 yj \u2264 yi+1gi(y)+ 0\u2212 i+1\u220f j=1 yj \u2264 yi+1(i 0 + i\u220f j=1 yj)+ 0\u2212 i+1\u220f j=1 yj \u2264 (i+1) 0,\nand the reverse inequality is proved analogously.\n\u2022 Alternatively, the network uses the fast exponentiation routine from Lemma 3.3, and then multiplies together the terms for individual coordinates. In particular, the exponentiation for each coordinate with accuracy 1 := /(ds) requires a network of size O(ln(r/ 1)2). By an analysis similar to the preceding construction, multiplying \u2264 d such networks will result in a network approximating the monomial with error /s and size O(d ln(r/ 1)2).\nNext, the proof that ReLU networks can efficiently compute reciprocals, namely Lemma 3.5. As stated in the text, it is first necessary to establish Lemma 3.6, which gives computes reciprocals at a choice of magnitude, and then Lemma 3.7, which combines these circuits across scales.\nProof of Lemma 3.7. For each i \u2208 {1, . . . , n}, define the function\npi(z) :=  z\u2212ai\u22121 ai\u2212ai\u22121 z \u2208 [ai\u22121, ai], ai+1\u2212z ai+1\u2212ai z \u2208 (ai, ai+1], 0 otherwise.\nThe functions (pi)ni=1 have the following properties.\n\u2022 Each pi can be represented by a ReLU network with three nodes in 2 layers.\n\u2022 For any x \u2208 [a1, an], there exists j \u2208 {1, . . . , n} so that i \u2208 {j, j + 1} implies pi(x) \u2265 0 and i 6\u2208 {j, j + 1} implies pi(x) = 0. Indeed, it suffices to let j be the smallest element of {1, . . . , n\u2212 1}. satisfying x \u2208 [aj , aj+1].\n\u2022 For any x \u2208 [a1, an], \u2211n i=1 pi(x) = 1.\nThe family (pi)ni=1 thus forms a partition of unity over [a1, an], moreover with the property that at most two elements, necessarily consecutive, are nonzero at any point in the interval.\nLet h : [0, B]2 \u2192 [0, B] be a uniform -approximation via ReLU networks to the multiplication map (x, y) 7\u2192 xy; by Lemma 3.2, h has O(ln(B/ )) nodes and layers, and moreover the multiplication is exact when either input is 0. Finally, define g : R\u2192 R as\ng(x) := n\u2211 i=1 h(pi(x), gi(x)).\nBy construction, g is a ReLU network with O(ln(B/ ) + maxi ki) layers and O(n ln(B/ ) + \u2211 imi) nodes.\nIt remains to check the approximation properties of g. Let x \u2208 [a1, an] be given, and set j := min { j \u2208 {1, n\u2212 1} : x \u2208 [aj , aj+1] } . Then\u2223\u2223f(x)\u2212 g(x)\u2223\u2223 = \u2223\u2223f(x)\u2212 h(pj(x), gj(x))\u2212 h(pj+1(x), gj+1(x))\u2223\u2223\n\u2264 \u2223\u2223f(x)\u2212 pj(x)gj(x)\u2212 pj+1(x)gj+1(x)\u2223\u2223\n+ \u2223\u2223pj(x)gj(x)\u2212 h(pj(x), gj(x))\u2223\u2223+\u2223\u2223pj+1(x)gj+1(x)\u2212 h(pj+1(x), gj+1(x))\u2223\u2223\n\u2264 pj(x) \u2223\u2223f(x)\u2212 gj(x)\u2223\u2223+ pj+1(x)\u2223\u2223f(x)\u2212 gj+1(x)\u2223\u2223+ +\n\u2264 pj(x) + pj+1(x) + 2 .\nProof of Lemma 3.6. Set c := 1/b and r := db ln(1/( a))/ae and 0 := /(r2c). For i \u2208 {0, . . . , r}, let hi : [0, 1]\u2192 [0, 1] denote a ReLU network 0-approximation to the map x 7\u2192 xi; by Lemma 3.3, hi hasO(ln(1/ 0)2 nodes and layers. Define q : [0, 1]\u2192 R as\nq(x) := c r\u2211 i=0 hi(1\u2212 cx).\nBy construction, q is a ReLU network with O(r ln(1/ 0)2) nodes and O(ln(1/ 0)2) layers.\nFor the approximation property of q, let x \u2208 [a, b] be given, and note\n\u2223\u2223\u2223\u2223q(x)\u2212 1x \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223q(x)\u2212 c r\u2211 i=0 (1\u2212 cx)i \u2223\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223\u2223c r\u2211 i=0 (1\u2212 cx)i \u2212 1 x \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 c\nr\u2211 i=0 \u2223\u2223\u2223hi(1\u2212 cx)\u2212 (1\u2212 cx)i\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223\u2223c r\u2211 i=0 (1\u2212 cx)i \u2212 c 1\u2212 (1\u2212 cx) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 +\n\u2223\u2223\u2223\u2223\u2223\u2223c r\u2211 i=0 (1\u2212 cx)i \u2212 c \u221e\u2211 i=0 (1\u2212 cx)i \u2223\u2223\u2223\u2223\u2223\u2223\n= + c \u221e\u2211 i=r+1 (1\u2212 cx)i\n= + c(1\u2212 cx)r+1)\n1\u2212 (1\u2212 cx)\n\u2264 + exp(\u2212cx(r + 1)) x \u2264 + exp(\u2212car) a \u2264 + .\nProof of Lemma 3.5. Set 0 := /3. For i \u2208 {1, . . . , k}, Let q\u0303i denote the ReLU network 0-approximation to 1/x along [2\u2212i, 2\u2212i+1]; by Lemma 3.6, q\u0303i has O(k2 ln(1/ )2) layers and O(k3 ln(1/ )3) nodes. Furthermore, set qi := max{2i,min{0, q\u0303i}}, which has the same approximation and size properties of q\u0303i. Applying Lemma 3.7 with B := 2k and reals ai := 2i\u2212k\u22121 for i \u2208 {0, . . . , k + 2} and functions (qi)ki=1, it follows that there exists q : R \u2192 R which -approximates 1/x along [2\u2212k, 1] with size O(k2 ln(1/ ) + k4 ln(1/ )3) and depth O(k ln(1/ ) + k2 ln(1/ )2).\nPutting the pieces together gives the proof of the second part of the main theorem.\nProof of part 1 of Theorem 1.1. Define 0 := /22k+3, and use Lemmas 3.2, 3.4 and 3.5 to choose ReLU network approximations fp and fq to p and q at resolution 0, as well as ReLU network f for multiplication along [0, 1]2 and g to approximate x 7\u2192 1/x along [2\u2212k\u22121, 1], again at resolution 0. The desired network will compute the function h, defined as\nh(x) := 2k+1f(fp(x), 2 \u2212k\u22121g(fq(x))).\nCombining the size bounds from the preceding lemmas, h itself has size bound\nO ( min { sr ln(sr/ 0), sd ln(dsr/ 0) 2 }) +O ( ln(1/ 0) ) +O ( k4 ln(1/ 0) 3 )\n= O ( min { srk ln(sr/ ), sdk2 ln(dsr/ )2 } + k7 ln(1/ )3 ) .\nBefore verifying the approximation guarantee upon h, it is necessary to verify that the inputs to f and g are of the correct magnitude, so that Lemmas 3.2 and 3.5 may be applied. Note firstly that g(fq(x)) \u2208 [1, 2k+1], since q(x) \u2208 [2\u2212k, 1] implies fq(x) \u2208 [2\u2212k \u2212 0, 1] \u2286 [2\u2212k\u22121, 1]. Thus 2\u2212k\u22121g(fq(x)) \u2208 [0, 1], and so both arguments to f within the definition of h are within [0, 1]. Consequently, the approximation guarantees of Lemmas 3.2, 3.4 and 3.5 all hold, whereby\nh(x)\u2212 p(x) q(x)\n= 2k+1f ( fp(x), 2 \u2212k\u22121g(fq(x)) ) \u2212 p(x) q(x)\n\u2264 fp(x)g(fq(x))\u2212 p(x)\nq(x) + 2k+1 0\n\u2264 fp(x) fq(x) \u2212 p(x) q(x) + fp(x) 0 + 2 k+1 0\n\u2264 p(x) + 0 q(x)\u2212 0 \u2212 p(x) q(x) + fp(x) 0 + 2 k+1 0 \u2264 p(x)q(x) + q(x) 0 \u2212 p(x)q(x) + p(x) 0 q(x)(q(x)\u2212 0) + fp(x) 0 + 2 k+1 0\n= 0\nq(x)\u2212 0 + p(x) 0 q(x)(q(x)\u2212 0) + fp(x) 0 + 2 k+1 0\n\u2264 2k+1 0 + 22k+1 0 + fp(x) 0 + 2k+1 0 \u2264 .\nThe proof of the reverse inequality is analogous.\nB.2. Proof of Proposition 1.5\nProof of Proposition 1.5. By (Telgarsky, 2015, Lemma 2.1), a ReLU network g with at most m nodes in each of at most l layers computes a function which is affine along intervals forming a partition of R of cardinality at most N \u2032 \u2264 (2m)l. Further subdivide this collection of intervals at any point where g intersects f(x) = 1/x; since f is convex and g is affine within each existing piece of the subdivision, then the number of intervals is at most three times as large as before. Together, the total number of intervals N \u2032\u2032 now satisfies N \u2032\u2032 \u2264 3(2m)l. Finally, intersect the family of intervals with [1/2, 3/4], obtaining a final number of intervals N \u2264 3(2m)l.\nLet (U1, . . . , UN ) denote this final partition of [1/2, 3/4], and let (\u03b41, . . . , \u03b4N ) denote the corresponding interval lengths. Let S \u2286 {1, . . . , N} index the subcollection of intervals with length at least 1/(8N), meaning S := {j \u2208 {1, . . . , N} : \u03b4j \u2265 1/(8N)}. Then \u2211\nj\u2208S \u03b4j =\n1 4 \u2212 \u2211 j 6\u2208S \u03b4j > 1 4 \u2212 N 8N = 1 8 .\nConsider now any interval Uj with endpoints {a, b}. Since 1/2 \u2264 a < b \u2264 3/4, then f satisfies 128/27 \u2264 f \u2032\u2032 \u2264 16. In order to control the difference between f and g along Uj , consider two cases: either f \u2265 g along this interval, or f \u2264 g along this interval (these are the only two cases due to the subdivisions above).\n\u2022 If f \u2265 g, then g can be taken to be a tangent to f at some point along the interval [a, b] (otherwise, the distance can always be only decreased by moving g up to be a tangent). Consequently, g(x) := f(c) + f \u2032(c)(x \u2212 c) for some c \u2208 [a, b], and by convexity and since f \u2032\u2032 \u2265 128/27 over this interval,\u222b b\na\n|f(x)\u2212 g(x)|dx \u2265 min c\u2208[a,b] \u222b b a ( (f(c) + f \u2032(c)(x\u2212 c) + f \u2032\u2032(b)(x\u2212 c)2/2)\u2212 (f(c) + f \u2032(c)(x\u2212 c)) ) dx\n= min c\u2208[a,b] \u222b b a f \u2032\u2032(b)(x\u2212 c)2/2 dx\n\u2265 64 27 min c\u2208[a,b]\n( (b\u2212 c)3 \u2212 (a\u2212 c)3\n3\n) .\n= 64\n81 min \u03b1\u2208[0,1]\n( (\u03b1(b\u2212 a))3 + ((1\u2212 \u03b1)(b\u2212 a))3 ) = 16(b\u2212 a)3\n81 .\n\u2022 On the other hand, if g \u2265 f , then g passes above the secant line h between (a, f(a)) and (b, f(b)). The area between f and g is at least the area between f and h, and this latter area is bounded above by a triangle of width (b \u2212 a) and height\nf(a) + f(b)\n2 \u2212 f\n( (a+ b)/2 ) = 1\n2\n( 1\na +\n1 b \u2212 1 a+ b ) = 1\n2ab(a+ b)\n( b(a+ b) + a(a+ b)\u2212 ab ) \u2265 1\n2ab(a+ b)\n( b(a+ b) + a(a+ b)\u2212 ab ) \u2265 3/2\n4 .\nCombining this with b\u2212 a \u2264 1/4, the triangle has area at least 3(b\u2212 a)/16 \u2265 3(b\u2212 a)3.\nCombining these two cases and summing across the intervals of S (where j \u2208 Sj implies \u03b4j \u2265 1/(8N)),\u222b [1/2,3/4] |f(x)\u2212 g(x)|dx \u2265 \u2211 j\u2208S \u222b Uj |f(x)\u2212 g(x)|dx\n\u2265 \u2211 j\u2208S \u03b43j 6\n\u2265 1 6(8N)2 \u2211 j\u2208S \u03b4j\n\u2265 1 27648(2m)2l .\nIf m < (27648 )\u22121/(2l)/2, then \u222b [1/2,3/4] |f(x)\u2212 g(x)|dx \u2265 1 27648(2m)2l > ."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["Anthony", "Martin", "Bartlett", "Peter L"], "venue": null, "citeRegEx": "Anthony et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Anthony et al\\.", "year": 1999}, {"title": "Log depth circuits for division and related problems", "author": ["Beame", "Paul", "Cook", "Stephen A", "Hoover", "H. James"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Beame et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Beame et al\\.", "year": 1986}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Cohen", "Nadav", "Sharir", "Or", "Shashua", "Amnon"], "venue": "COLT", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["Cybenko", "George"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "Cybenko and George.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko and George.", "year": 1989}, {"title": "The power of depth for feedforward neural networks", "author": ["Eldan", "Ronen", "Shamir", "Ohad"], "venue": "In COLT,", "citeRegEx": "Eldan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eldan et al\\.", "year": 2016}, {"title": "Reliably learning the relu in polynomial time", "author": ["Goel", "Surbhi", "Kanade", "Varun", "Klivans", "Adam", "Thaler", "Justin"], "venue": "In COLT,", "citeRegEx": "Goel et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2017}, {"title": "Why deep neural networks for function approximation", "author": ["Liang", "Shiyu", "R. Srikant"], "venue": "In ICLR,", "citeRegEx": "Liang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Constructive approximation : advanced problems", "author": ["G.G. Lorentz", "Golitschek", "Manfred von", "Makovoz", "Yuly"], "venue": "Michigan Math. J., 11(1):11\u201314,", "citeRegEx": "Lorentz et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lorentz et al\\.", "year": 1996}, {"title": "Approximating threshold circuits by rational functions", "author": ["Paturi", "Ramamohan", "Saks", "Michael E"], "venue": "Inf. Comput.,", "citeRegEx": "Paturi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Paturi et al\\.", "year": 1994}, {"title": "Rational approximation of real functions. Encyclopedia of mathematics and its applications", "author": ["Petrushev", "P.P. Penco Petrov", "Popov", "Vasil A"], "venue": null, "citeRegEx": "Petrushev et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Petrushev et al\\.", "year": 1987}, {"title": "Why and when can deep \u2013 but not shallow \u2013 networks avoid the curse of dimensionality: a review", "author": ["Poggio", "Tomaso", "Mhaskar", "Hrushikesh", "Rosasco", "Lorenzo", "Miranda", "Brando", "Liao", "Qianli"], "venue": null, "citeRegEx": "Poggio et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2017}, {"title": "Depth separation in relu networks for approximating smooth non-linear functions. 2016", "author": ["Safran", "Itay", "Shamir", "Ohad"], "venue": null, "citeRegEx": "Safran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Safran et al\\.", "year": 2016}, {"title": "Representation benefits of deep feedforward networks", "author": ["Telgarsky", "Matus"], "venue": null, "citeRegEx": "Telgarsky and Matus.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["Telgarsky", "Matus"], "venue": "In COLT,", "citeRegEx": "Telgarsky and Matus.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2016}, {"title": "Splines, rational functions and neural networks", "author": ["Williamson", "Robert C", "Bartlett", "Peter L"], "venue": "In NIPS,", "citeRegEx": "Williamson et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 1991}, {"title": "Error bounds for approximations with deep relu networks. 2016", "author": ["Yarotsky", "Dmitry"], "venue": null, "citeRegEx": "Yarotsky and Dmitry.,? \\Q2016\\E", "shortCiteRegEx": "Yarotsky and Dmitry.", "year": 2016}, {"title": "denote the triangle function from (Telgarsky", "author": ["R R"], "venue": null, "citeRegEx": "R\u2192,? \\Q2015\\E", "shortCiteRegEx": "R\u2192", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "There are also a variety of other result giving the ability of ReLU networks to approximate various function classes (Cohen et al., 2016; Poggio et al., 2017).", "startOffset": 117, "endOffset": 158}, {"referenceID": 10, "context": "There are also a variety of other result giving the ability of ReLU networks to approximate various function classes (Cohen et al., 2016; Poggio et al., 2017).", "startOffset": 117, "endOffset": 158}, {"referenceID": 7, "context": "Rational functions are extensively studied in the classical approximation theory literature (Lorentz et al., 1996; Petrushev & Popov, 1987).", "startOffset": 92, "endOffset": 139}, {"referenceID": 1, "context": ") An ICML reviewer revealed prior work which was embarrassingly overlooked by the author: it has been known, since decades ago (Beame et al., 1986), that neural networks using threshold nonlinearities (i.", "startOffset": 127, "endOffset": 147}, {"referenceID": 5, "context": "Together this suggests that not only the connections between rational functions and neural networks are tight (and somewhat known/unsurprising), but also that threshold networks and ReLU networks have perhaps more similarities than what is suggested by the differing VC dimension bounds, approximation results, and algorithmic results (Goel et al., 2017).", "startOffset": 335, "endOffset": 354}, {"referenceID": 7, "context": "Following conventions in the approximation theory literature (Lorentz et al., 1996), the denominator polynomial will always be strictly positive.", "startOffset": 61, "endOffset": 83}, {"referenceID": 1, "context": "This leads to the second trick (which was also used by Beame et al. (1986)!).", "startOffset": 55, "endOffset": 75}], "year": 2017, "abstractText": "Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degreeO(poly log(1/ )) which is -close, and similarly for any rational function there exists a ReLU network of size O(poly log(1/ )) which is -close. By contrast, polynomials need degree \u03a9(poly(1/ )) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions. 1. Overview Significant effort has been invested in characterizing the functions that can be efficiently approximated by neural networks. The goal of the present work is to characterize neural networks more finely by finding a class of functions which is not only well-approximated by neural networks, but also well-approximates neural networks. The function class investigated here is the class of rational functions: functions represented as the ratio of two polynomials, where the denominator is a strictly positive polynomial. For simplicity, the neural networks are taken to always use ReLU activation \u03c3r(x) := max{0, x}; for a review of neural networks and their terminology, the reader is directed to Section 1.4. For the sake of brevity, a network with ReLU activations is simply called a ReLU network. 1.1. Main results The main theorem here states that ReLU networks and rational functions approximate each other well in the sense University of Illinois, Urbana-Champaign; work completed while visiting the Simons Institute. Correspondence to: your friend <mjt@illinois.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). \u22121.00 \u22120.75 \u22120.50 \u22120.25 0.00 0.25 0.50 0.75 1.00 0 1 2 3 4 spike rat poly net Figure 1. Rational, polynomial, and ReLU network fit to \u201cspike\u201d, a function which is 1/x along [1/4, 1] and 0 elsewhere. that -approximating one class with the other requires a representation whose size is polynomial in ln(1 / ), rather than being polynomial in 1/ . Theorem 1.1. 1. Let \u2208 (0, 1] and nonnegative integer k be given. Let p : [0, 1] \u2192 [\u22121,+1] and q : [0, 1] \u2192 [2\u2212k, 1] be polynomials of degree \u2264 r, each with\u2264 smonomials. Then there exists a function f : [0, 1] \u2192 R, representable as a ReLU network of size (number of nodes) O ( k ln(1 / ) + min { srk ln(sr / ), sdk ln(dsr / ) }) ,", "creator": "LaTeX with hyperref package"}}}