{"id": "1706.04719", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Effective Sequential Classifier Training for Multitemporal Remote Sensing Image Classification", "abstract": "the resultant explosive distance availability targeting of remote numerical sensing information images has challenged supervised adaptive classification algorithms on such firms as support vector machines ( svm ), as training samples tend to inherently be highly likelihood limited frequently due to the somewhat expensive delicate and laborious task of testing ground truthing. highlighting the extreme temporal correlation dependence and transient spectral similarity interaction between multiple multitemporal location images alike have opened this up an opportunity ahead to alleviate this taxonomic problem. in this study, a svm - - based sequential classifier training ( sct - adaptive svm ) approach iteration is increasingly proposed for multitemporal remote sensing image path classification. the timing approach vastly leverages the memory classifiers of repeating previous images to reduce the required number out of residual training development samples for the classifier training of identifying an outgoing incoming image. for each incoming image, placing a detailed rough classifier is firstly directly predicted vertically based heavily on solving the temporal probability trend of recognizing a set assortment of previous classifiers. the inaccurate predicted alignment classifier is then fine - tuned separately into a more accurate position with restoring current existing training samples. this approach can both be applied progressively to sequential image prediction data, therefore with only a small number hundreds of training software samples individually being required from each image. research experiments recently were conducted with prospective sentinel - 2a encoded multitemporal data over considering an agricultural conservation area in australia. results result showed clearly that ultimately the proposed sct - oriented svm methods achieved better classification accuracies compared with potentially two state - ownership of - the - land art model transfer algorithms. compared with those those obtained without the detailed assistance of from resolving previous images, the true overall posterior classification accuracy calculation was improved from around 76. 61 2 % points to 96 93. 8 %, well demonstrating that adapting the leverage of a priori grouping information from recalling previous prototype images can provide advantageous assistance for later reference images in multitemporal image classification.", "histories": [["v1", "Thu, 15 Jun 2017 02:01:44 GMT  (9728kb,D)", "http://arxiv.org/abs/1706.04719v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yiqing guo", "xiuping jia", "david paull"], "accepted": false, "id": "1706.04719"}, "pdf": {"name": "1706.04719.pdf", "metadata": {"source": "CRF", "title": "Effective Sequential Classifier Training for Multitemporal Remote Sensing Image Classification", "authors": ["Yiqing Guo", "Xiuping Jia", "David Paull"], "emails": ["Yiqing.Guo@student.adfa.edu.au;", "X.Jia@adfa.edu.au).", "D.Paull@adfa.edu.au)"], "sections": [{"heading": null, "text": "Index Terms\u2014Multitemporal, classifier training, classification, support vector machines\nI. INTRODUCTION\nLAND cover information extracted from remote sensingimages has enabled timely monitoring of land cover dynamics at a large spatial scale [1], [2]. For the generation of land cover maps, supervised classification algorithms have been widely adopted [3]. Ground reference data are required by these algorithms for the training of classifiers [3].\nHowever, traditional supervised classification algorithms such as the Support Vector Machines (SVM) have been challenged by the explosive availability of remote sensing images. While collecting a sufficient number of training samples is critical to achieve satisfactory classification results, it is often labour- and time-consuming and sometimes practically unfeasible [4]\u2013[8]. The problem is more severe for agricultural areas where the accuracy of ground reference data can be\nY. Guo and X. Jia are with the School of Engineering and Information Technology, The University of New South Wales, Canberra Campus, Canberra, ACT 2600, Australia (e-mail: Yiqing.Guo@student.adfa.edu.au; X.Jia@adfa.edu.au).\nD. Paull is with the School of Physical, Environmental and Mathematical Sciences, The University of New South Wales, Canberra Campus, Canberra, ACT 2600, Australia (e-mail: D.Paull@adfa.edu.au)\nquickly outdated, as a result of the frequent land cover changes and spectral characteristic shifts caused by crop phenology and anthropogenic practices such as irrigation and harvesting [9]. As a result, training data sufficiency is often hard to be guaranteed for every image.\nTemporal correlation and spectral similarity between multitemporal images have opened up an opportunity to alleviate the small sample size problem. When an incoming image needs to be classified, existing knowledge provided by previous images can be utilized as a priori information. However, it cannot be directly applied to the incoming image, because class data of different images may follow different probabilistic distributions in the feature space. This phenomenon is often referred to as cross-image dataset shifts [10]. The shifts can either originate from changes in the nature of land surface properties, for example, induced by the phenology of vegetation, or from the background noise caused by varied acquisition and atmospheric conditions [11]\u2013[13] and inconsistent sun-targetsenor geometries [14], [15]. Therefore, appropriate strategies need to be applied to tackle the dataset shifts [16].\nDomain adaptation is an emerging technique that is able to accommodate the dataset shifts among images [16]\u2013[21]. The technique aims to adapt a priori information extracted from previous images to an incoming image that has shifted spectral characteristics. Compared with a complete retraining of the incoming image, the required number of training samples can be considerably reduced if the assistance from previous images is leveraged [6]. Several domain adaptation algorithms have been proposed to accommodate the dataset shifts in multitemporal image classification. For example, the changedetection-driven algorithms [7], [22] train a classifier for an incoming image with the help of the unchanged samples from a previous image. A change detection step is firstly implemented to identify unchanged pixels, whose labels are then directly transferred to the incoming image. However, as erroneous labels may be transferred as well, the classification error will then propagate from the previous image to the incoming image and accumulate when the algorithm is applied progressively to multiple incoming images. The Bayesian-based algorithms [6] adapt a Bayesian classifier to an incoming image with the statistical parameters calculated from the previous image as an a priori estimate. Then an unsupervised retraining procedure is used to adjust the a priori estimate, making it right for the incoming image. However, the a priori estimate is made with only one previous image instead of a set of previous images, so the temporal trajectory of class spectral characteristics, which is useful information to make a more accurate a priori estimation, is not extracted and utilized in the algorithm. ar X iv :1 70 6. 04 71\n9v 1\n[ cs\n.C V\n] 1\n5 Ju\nn 20\n17\n2 This study provides a SVM-based sequential classifier training (SCT-SVM) approach for multitemporal remote sensing image classification. With the assistance from a set of previous classifiers, this approach is able to effectively train classifiers for an incoming image at the same location. This approach can be applied progressively to sequential image data, with only a small number of training samples being required from each image. A set of Sentinel-2A multitemporal images over an agricultural area in Australia have been used to demonstrate the effectiveness of the proposed approach."}, {"heading": "II. METHOD", "text": ""}, {"heading": "A. Proposed Framework", "text": "When an incoming image needs to be classified, the small sample size problem often arises if training samples are difficult to collect. In order to mitigate this problem, we propose to leverage a priori knowledge gained from the classification of previous images at the same location. The assistance from previous images is provided at the classifier-level instead of the data-level. Specifically, the parameters of classifiers instead of raw data (i.e., pixel values and/or their corresponding labels) from the previous images are used.\nAccording to the sequence of sensing date, previous images and their classifiers are indexed t\u22121, t\u22122, \u00b7 \u00b7 \u00b7 , t\u2212N , where N is the total number of previous images, and the incoming image and its classifier are indexed t.\nConsider that a classifier needs to be determined for Image t. We propose a sequential classifier training approach that consists of two steps: prediction and fine-tuning, as shown in Fig. 1. In the prediction step, a rough classifier (Classifier t\u2217) is firstly estimated based on a set of previous classifiers (Classifiers t \u2212 1, t \u2212 2, \u00b7 \u00b7 \u00b7 , t \u2212N ). In the fine-tuning step, the predicted classifier (Classifier t\u2217) is adjusted and finalized as Classifier t with current training samples. Because the finetuning is based on the predicted classifier, compared with the direct training of a classifier for Image t without any ancillary information, the required number of training samples can be smaller. After Classifier t is determined, it can be included into the set of previous classifiers, while the oldest classifier (Classifier t \u2212 N ) may be removed. The updated set is then used as a priori knowledge to assist the training of the next classifier (Classifier t+ 1).\nThe proposed approach is named as SCT-SVM as it is based on the well-established classification algorithm of Support Vector Machines (SVM). In a m-dimensional feature space, a linear SVM classifier f (x) for a binary classification problem can be expressed as [23]:\nf(x) = wTx + b, (1)\nwhere x = (x1 x2 \u00b7 \u00b7 \u00b7 xm)T is the feature vector; w = (w1 w2 \u00b7 \u00b7 \u00b7 wm)T is the weight vector and b is the bias factor. For convenience, we stack the classifier parameters w and b into a single vector p = (w1 w2 \u00b7 \u00b7 \u00b7 wm b)T = ( wT b )T . Because each classifier f(x) can be uniquely determined by its parameters p, the terms classifier and classifier parameters will be used synonymously in the following. The binary SVM\nclassifier can handle the multi-class case by applying the oneagainst-one or one-against-the-rest strategies [3].\nFigure 2 illustrates the classifier parameter space defined in two dimensions (e.g., p1 and p2), where the prediction/finetuning steps are detailed. Different classifiers often appear at different points in this parameter space, due to the temporal variations among multitemporal images. The temporal trend of previous classifiers (p(t\u22121), p(t\u22122), \u00b7 \u00b7 \u00b7 , p(t\u2212N)) is firstly utilized to predict a rough classifier (p(t)\u2217) for the current image (Image t), and it is then fine-tuned into an updated position (p(t)). The prediction and fine-tuning steps will be detailed below in the following Subsections II-B and II-C, respectively."}, {"heading": "B. Classifier Prediction", "text": "The aim of classifier prediction is to determine a rough classifier for the current image (Image t). The prediction is based on the temporal trend of a set of previous classifiers. The trend can be due to the background variations under different acquisition conditions or originated from changes in the nature of land surface properties such as the phenology of crops and forests, or both. The former factor causes random and unpredictable changes. The latter factor induces relatively smooth and predictable temporal transition, due to the continuity of the changes with time. We propose to perform a principal component transform, and use the first transformed component to extract the overall temporal trend and suppress the background effects. As shown in Fig. 3, the first principal component direction captures the maximum temporal dynamics of previous classifiers.\nThe classifier prediction is conducted with the following procedure. Firstly, the previous classifiers are mean-centralized in the classifier parameter space (Fig. 3):\np\u0304(i) = p(i)\u2212 1 N t\u2212N\u2211 j=t\u22121 p(j), i = t\u22121, t\u22122, \u00b7 \u00b7 \u00b7 , t\u2212N , (2)\nwhere p(i) and p\u0304(i) are the classifiers before and after meancentralization, respectively.\nThen a principal component transform is applied to transform the mean-centered classifiers from the classifier parameter space into the principal component space:\nq\u0304(i) = Gp\u0304(i), i = t\u2212 1, t\u2212 2, \u00b7 \u00b7 \u00b7 , t\u2212N, (3) where q\u0304(i) = ( q\u0304 (i) 1 , q\u0304 (i) 2 , \u00b7 \u00b7 \u00b7 , q\u0304 (i) m+1 )T are the transformed classifiers with q\u0304(i)1 being their first principal components, and G is a (m + 1)-by-(m + 1) matrix that provides the transformation coefficients.\nThe transformation matrix G is calculated by decomposing the matrix P\u0304 = ( p\u0304(t\u22121) p\u0304(t\u22122) \u00b7 \u00b7 \u00b7 p\u0304(t\u2212N) )T with singular value decomposition:\nP\u0304 = U\u039bGT, (4)\nwhere U and \u039b are N -by-N and N -by-(m + 1) matrices, respectively.\nThe next step is to map the maximum temporal dynamics direction of previous classifiers on a time scale. As shown in\n3\nFig. 4, the first principal components after the transformation,\nq\u0304 (t\u22121) 1 , q\u0304 (t\u22122) 1 , \u00b7 \u00b7 \u00b7 , q\u0304 (t\u2212N) 1 , are now associated with the image sensing dates, d(t\u22121), d(t\u22122), \u00b7 \u00b7 \u00b7 , d(t\u2212N), to show the intervals between each image collection, which can be irregular. A regression is then performed in order to predict a classifier for Image t. The polynomial fitting function is selected in this\nstudy:\nq\u0304 (i) 1 = a0 + a1d (i) + \u00b7 \u00b7 \u00b7+ar ( d(i) )r ,\ni = t\u2212 1, t\u2212 2, \u00b7 \u00b7 \u00b7 , t\u2212N, (5)\nwhere r is the order of the function, and a0, a1, \u00b7 \u00b7 \u00b7 , ar are the fitting coefficients. The sensing dates used here are integer counts of days starting from a consistent reference date.\nIt is worth noting that the order of the fitting function is selected according to the pattern of the temporal trend of classifiers. In order to achieve better fitting accuracies, higherorder polynomials may be adopted, but the overfitting risk will increase as well. So the balance between fitting accuracy and overfitting problem should be considered when choosing the fitting function.\nAfter determining the fitting coefficients a0 and a1 with the data of previous classifiers, the first principal component of\n4 the predicted classifier for Image t, q\u0304(t)\u22171 , can be calculated as:\nq\u0304 (t)\u2217 1 = a0. (6)\nThe rest of the principal components q\u0304(t)\u22172 , q\u0304 (t)\u2217 3 , \u00b7 \u00b7 \u00b7 , q\u0304 (t)\u2217 m+1 can be set to zero. The inverse of the principal component transform is then performed, followed by the mean-decentralization, to obtain the predicted classifier in the original classifier parameter space:\np(t)\u2217 = G\u22121q\u0304(t)\u2217 + 1\nN t\u2212N\u2211 i=t\u22121 p(i). (7)"}, {"heading": "C. Classifier Fine-Tuning", "text": "A domain adaptation algorithm is developed as the second stage of the proposed SCT-SVM. It fine-tunes the predicted classifier p(t)\u2217 to a more accurate position p(t) with training samples of the current image (Image t). The algorithm is designed with anticipation that the true classifier should be located not far from the predicted position. This is a reasonable expectation, given that the predicted classifier is based on the historical information extracted from previous images and the changes often have continuity with time. The aim of the finetuning is set as to achieve a higher classification accuracy on the training samples of Image t. In this way, contributions from the predicted classifier and the training samples are incorporated and balanced in the algorithm, as detailed in the following.\nThe a priori information provided by the predicted classifier is utilized by restricting the fine-tuned classifier from departing too far away from the predicted position. To achieve this goal, the distance between the first m elements of p(t) (i.e., w(t)) and p(t)\u2217 (i.e., w(t)\u2217) is minimized [24]. More specifically, a set of non-negative slack variables \u00b5 = {\u00b5j}mj=1 are introduced, where the jth element restricts the upper and lower bounds of the deviation of p(t)j from p (t)\u2217 j , resulting in the following constraints:\n\u2212 \u00b5j \u2264 p(t)j \u2212 p (t)\u2217 j \u2264 \u00b5j and \u00b5j \u2265 0, j = 1, 2, \u00b7 \u00b7 \u00b7 , m. (8) The information provided by the training samples is utilized by taking into account the classification error of the fine-tuned classifier on the training samples {xi, yi}ni=1, where xi is the feature vector of the ith training sample and yi \u2208 {+1, \u2212 1} is its corresponding label. Similar to those in the standard support vector machines, a set of non-negative slack variables \u03be = {\u03bei}ni=1 are used to allow soft-margin classification in order to tackle overlapping classes. Consequently, the following constraints are designed:\nyi  m\u2211 j=1 p (t) j xi,j + p (t) m+1  \u2265 1\u2212 \u03bei and \u03bei \u2265 0, i = 1, 2, \u00b7 \u00b7 \u00b7 , n,\n(9)\nwhere xi,j is the jth entry in xi.\nWith the constraints in Eqs. (8) and (9), the following optimization problem can be constructed:\nmin 1\n2 m\u2211 j=1 ( p (t) j )2 + C n\u2211 i=1 \u03bei + F m\u2211 j=1 \u00b5j\ns.t. yi m\u2211 j=1 p (t) j xi,j+p (t) m+1 \u22651\u2212 \u03bei and \u03bei\u22650, i=1,2,\u00b7 \u00b7 \u00b7,n, \u2212 \u00b5j \u2264 p(t)j \u2212 p (t)\u2217 j \u2264 \u00b5j and \u00b5j \u2265 0, j = 1,2, \u00b7 \u00b7 \u00b7 ,m.\n(10)\nThe first term in the objective function accounts for the margin space of the fine-tuned classifier. Like that in the standard support vector machines, the term aims to adjust the separating hyperplane to a position that generates the maximum margin space between classes. The second and third terms aim to minimize the degree of violation of the constraints. The positive constants C and F are regularisation parameters, which control the weights of the second and third terms relative to the first term. The values of C and F control the amounts of contributions from training samples and previous images, respectively.\nThe dual form of the optimization problem in Eq. (10) can be obtained by constructing the following Lagrangian function L:\nL = 1\n2 m\u2211 j=1 ( p (t) j )2 + C n\u2211 i=1 \u03bei + F m\u2211 j=1 \u00b5j \u2212 n\u2211\ni=1\n\u03b1i yi  m\u2211\nj=1\np (t) j xi,j + p (t) m+1 \u2212 (1\u2212 \u03bei) \n\u2212 n\u2211\ni=1 \u03b2i\u03bei \u2212 m\u2211 j=1 \u03b3j [ \u00b5j \u2212 ( p (t) j \u2212 p (t)\u2217 j )] \u2212\nm\u2211 j=1 \u03b4j [( p (t) j \u2212 p (t)\u2217 j ) + \u00b5j ] \u2212 m\u2211 j=1 \u03b5j\u00b5j ,\n(11)\nwhere {\u03b1i}ni=1, {\u03b2i} n i=1, {\u03b3j} m j=1, {\u03b4j} m j=1, and {\u03b5j} m j=1 are non-negative Lagrange multipliers. Equating the partial derivatives of L to zero with respect to p(t)j , \u03bei, and \u00b5j results in the following equations:\np (t) j \u2212 n\u2211 i=1 \u03b1iyixi,j + \u03b3j \u2212 \u03b4j = 0, j = 1, 2, \u00b7 \u00b7 \u00b7 , m, (12)\nn\u2211 i=1 \u03b1iyi = 0, (13)\nC \u2212 \u03b1i \u2212 \u03b2i = 0, i = 1, 2, \u00b7 \u00b7 \u00b7 , n, (14)\nF \u2212 \u03b3j \u2212 \u03b4j \u2212 \u03b5j = 0, j = 1, 2, \u00b7 \u00b7 \u00b7 , m. (15)\n5 Eliminating p(t)j , C, and F by substituting Eqs. (12\u201315) into Eq. (11) results in the following Lagrangian dual function Ld:\nLd =\u2212 1\n2 n\u2211 i=1 n\u2211 k=1 m\u2211 j=1 \u03b1i\u03b1kyiykxi,jxk,j \u2212 1 2 m\u2211 j=1 \u03b3j 2\n\u2212 1 2 m\u2211 j=1 \u03b4j 2 + n\u2211 i=1 m\u2211 j=1 \u03b3j\u03b1iyixi,j\n\u2212 n\u2211\ni=1 m\u2211 j=1 \u03b4j\u03b1iyixi,j + m\u2211 j=1 \u03b3j\u03b4j + n\u2211 i=1 \u03b1i\n\u2212 m\u2211 j=1 p (t)\u2217 j (\u03b3j \u2212 \u03b4j).\n(16)\nThen the following dual optimization problem can be constructed, which is equivalent to the primal problem in Eq. (10) but has a simpler form:\nmin \u03b1i,\u03b3j ,\u03b4j\n\u2212 Ld\ns.t. 0 \u2264 \u03b1i \u2264 C, i = 1, 2, \u00b7 \u00b7 \u00b7 , n, \u03b3j + \u03b4j \u2264 F, \u03b3j \u2265 0, and \u03b4j \u2265 0, j = 1, 2, \u00b7 \u00b7 \u00b7 , m, n\u2211 i=1 \u03b1iyi = 0.\n(17)\nThis is a quadratic programming problem and can be solved with standard quadratic programming algorithms [25]. After determining \u03b1i, \u03b3i, and \u03b4i, the parameters for the fine-tuned classifier p(t) can be calculated as:\np (t) j = n\u2211 i=1 \u03b1iyixi,j \u2212 \u03b3j + \u03b4j , j = 1, 2, \u00b7 \u00b7 \u00b7 , m, (18)\np (t) m+1 =\n1\nNS \u2211 i\u2208S  1 yi \u2212 m\u2211 j=1 p (t) j xi,j , (19) where S and NS are the index set and the total number of support vectors, respectively."}, {"heading": "D. Discussions", "text": "1) Advantages of Classifier-Level Approach: The proposed SCT-SVM is a classifier-level approach. It transfers the classifier-level knowledge, i.e. the classifier parameters of a set of previous classifiers, to the incoming image. Compared with the data-level approaches that require the availability of pixels and/or their corresponding labels (raw data) from previous images, such as the Domain Adaptation SVM (DA-SVM) [26] and the Domain Transfer SVM (DTSVM) [27], the proposed approach has the following two merits. Firstly, the required computational load can be reduced considering that the size of raw data is usually large. Secondly, the proposed algorithm is applicable even if the raw data of previous images are inaccessible (e.g., private or no longer stored). Therefore, compared with data-level approaches, the proposed classifier-level approach is more efficient and more feasible.\n2) Comparing Proposed Fine-tuning Algorithm to State-ofthe-Art Algorithms:\nThe second step of the proposed method is fine tunning. There are two existing domain adaptation algorithms in the literature that are able to handle the same task: the Adaptive SVM (A-SVM) [28], [29] and the Projective Model Transfer SVM (PMT-SVM) [30]. The A-SVM enables the estimation of a fine-tuned target classifier by augmenting a perturbation term to the predicted classifier. However, it has been found that the A-SVM tends to generate a small margin space for the finetuned classifier [30]. The other algorithm, PMT-SVM, is able to estimate a fine-tuned classifier without penalizing its margin maximization. The limitation, however, is that it restricts the included angle between the predicted and fine-tuned classifiers to a value equivalent to or less than 90\u25e6 in the feature space. This restriction leads to a decreased performance when the underlying optimal classifier is positioned at an angle greater than 90\u25e6 from the predicted position.\nThe proposed fine tunning algorithm considers a maximummargin-space term (i.e., the first term of the objective function in Eq. (10)), which can alleviate the small-margin-space problem that exists in the A-SVM. The proposed algorithm is also able to fine-tune a predicted classifier into a position larger than 90\u25e6 as no restriction is applied on the fine-tuning angle, which is an improvement from the PMT-SVM. The proposed domain adaptation algorithm is named as TA-SVM for convenience, and it is compared with A-SVM and PMTSVM in the experiments."}, {"heading": "III. EXPERIMENT AND RESULTS", "text": ""}, {"heading": "A. Study Area and Data Sets", "text": "The experimental area is located at the Coleambally Irrigation Area in the southwest of New South Wales, Australia (145\u25e640\u201927\u201d\u2212146\u25e608\u201912\u201d E, 34\u25e640\u201937\u201d\u221235\u25e603\u201928\u201d S), as shown in Fig. 5. A high water-consuming crop, rice, is widely planted in this area. Its growing period lasts for about six months, roughly from October of the first year to April of the second year. The area has a semiarid climate (Ko\u0308ppen-Geiger BSk). The annual rainfall is about 400 mm, distributed evenly throughout the year. Due to the semi-arid climate and high water consumption of the crop industry, water supplement in this area largely depends on irrigation from the Murrumbidgee River. Therefore, timely and accurate mapping of land cover distributions is critical for the local Irrigation Company to make appropriate decisions on water budgeting and allocation, and to conduct environmental assessments.\nFive cloud-free Sentinel-2A optical images were collected from the Google Earth Engine platform. The images were sensed on Jan. 28th, Feb. 07th, Feb. 27th, Mar. 09th, and Mar. 19th, respectively, in the year of 2017. Each image consists of 13 spectral bands covering the visible, near-infrared, and shortwave-infrared spectral regions. The images were provided in top of atmosphere (TOA) reflectance values with radiometric and geometric corrections applied (Level-1C data). The bands with 10 m (Bands 2, 3, 4, and 8) and 20 m (Bands 5, 6, 7, 8a, 11, and 12) spatial resolutions were reduced down to 60 m resolutions in order to make them consistent with the other bands (Bands 1, 9, and 10) that had 60 m resolutions.\n6\nThere were five land cover classes identified in the study area: crops, forests, water, soil, and buildings. A total of 80 pixels were selected for each class in each image, and labelled via photointerpretation and with the help of the farmland boundary data provided by the Coleambally Irrigation Company. In the following experiments, the first four images were used as previous images to assist the training and classification of the fifth image."}, {"heading": "B. Classifier Prediction Results", "text": "This section presents the classifier prediction results obtained with the proposed SCT-SVM approach. The temporal movements of classifiers, which were driven by the variations of class data, were firstly analyzed. Then fitting functions were used to fit the movement patterns of classifiers. Classifiers predicted with different fitting function were compared with the true classifiers to assess the prediction accuracies.\n1) Temporal Variations of Classifiers: The correlation between spectral features and sensing dates was firstly analyzed. Bands 4 (red) and 8 (near-infrared) were selected for the analysis. Linear correlation coefficients were calculated for the five land cover classes, with the results shown in Tab. I. The highest temporal correlations were found for crops. The coefficient for the near-infrared band was -0.8306, indicating a strong negative correlation with time. The red band showed a positive but weaker correlation with a coefficient of 0.5213. Another vegetative class, forests, showed similar but less significant temporal correlations with coefficients of 0.4209 and -0.4699 for the red and infrared bands, respectively. The spectral features of the three nonvegetative classes, water, soil, and buildings, showed insignificant temporal correlations, with all the coefficients lower than 0.1.\nMore detailed analysis of the temporal correlations for crops and soil is shown in Fig. 6. By examining the class data\nof different images, it was found that the means of crops gradually moved from the top left towards the bottom right in the red/near-infrared feature space. In contrast, the means of soil remained relatively stable. Figure 7 explains the movement patterns of these classes data from a phenological perspective. It was found that the five images covered the mid-late growing periods of crops. During these periods, the crop leaves turned from green to yellow, accompanied by a gradual decrease in the value of Normalized Differential Vegetation Index (NDVI). The NDVI of soil remained stable with time, suggesting an insignificant temporal variation.\nDriven by the temporal trends of class data, the position of the classifier separating crops and soil drifted with time as well. Figures 8(a) and 8(b) show the movement of the crops-against-soil classifier in the feature space and classifier parameter space, respectively. It was found that the classifier\n7\nposition moved consistently with time. Based on the first four classifier positions, a predicted position was estimated for the fifth classifier. Though the predicted classifier was not exactly located at the true position, it was a good a priori guess that could provide advantageous help in seeking the true classifier.\n2) Prediction Accuracies with Different Fitting Functions:\nDifferent orders of fitting functions were tested for classifier predictions. For each classifier separating two classes, the Euclidean distance between the predicted and true positions was calculated, in order to quantitatively measure the prediction accuracy. The results are shown in Tab. II. The highest accuracy for each classifier is shown in bold typeface. It was found that, for classifiers that involved one or more vegetative classes (crops and/or forests), second-order polynomial fitting produced the best prediction results. For classifiers that separated two non-vegetative classes, first-order polynomial fitting was the best choice.\nMore detailed analysis of the prediction accuracies for the crops-against-soil and soil-against-buildings classifiers are shown in Fig. 9. For the crops-against-soil classifier, compared with first-order polynomial, the second-order polynomial was more capable of describing the non-linear movement of classifier\u2019s positions. The third-order polynomial generated the worst result due to the overfitting effect. The non-linear movement was less obvious for the soil-against-buildings\nclassifier, so the first-order fitting produced the best accuracy in this case."}, {"heading": "C. Classifier Fine-Tuning Results", "text": "The proposed SCT-SVM approach includes a new domain adaptation algorithm, TA-SVM, to fine-tune classifiers from the predicted positions to more accurate positions. In this section, the performance of TA-SVM in classifier fine-tuning was firstly compared with two existing algorithms, A-SVM\n8 [28] and PMT-SVM [30]. Then the impacts of two TA-SVM parameters, F and C, on fine-tuning accuracies were analyzed and discussed.\n1) Comparisons with A-SVM and PMT-SVM Algorithms: The three fine-tuning algorithms, A-SVM [28], PMT-SVM [30] and the proposed TA-SVM, were compared. In this analysis, the Number of Training Samples (NT ) from the fifth image ranged from 5 to 50, in order to analyze and compare the algorithms under different levels of training data insufficiency. Classification accuracies achieved with the finetuned classifiers on the rest of the labelled samples were calculated as a measure to evaluate the three algorithms.\nThe results are shown in Fig. 10. It was found that the accuracies of A-SVM and TA-SVM increased with NT , but the TA-SVM generated relatively higher accuracies especially when training data were insufficient (NT <10). The other algorithm, PMT-SVM, produced low accuracies for all NT levels. The results indicated that the TA-SVM outperformed the other two algorithms in classifier fine-tuning.\nIn-depth comparisons of the three algorithms are given in Fig. 11, where the behaviours of these algorithms were analyzed with two classifier fine-tuning cases. In the first case shown in Fig. 11(a), it was found that the classifier fine-tuned with A-SVM generated a relatively small margin space. This made the classifier be located at a biased position and produce a low classification accuracy. Differently, the classifier finetuned with TA-SVM was positioned at the maximum-margin location where a higher classification accuracy was achieved. In the second case shown in Fig. 11(b), the classifier finetuned with PMT-SVM was restricted to 90\u25e6 with the predicted classifier, while the classifier fine-tuned with the proposed TASVM was able to be positioned at an obtuse angle where higher classification accuracy could be achieved.\n2) Impacts of Regularization Parameters: The two regularization parameters adopted in TA-SVM, F and C, control the contributions of information from previous images and current training samples, respectively. Here experiments were conducted to analyze the impacts of these parameters on the fine-tuning results. In the experiments, the Number of Training Samples (NT ) from the fifth image was set to 5, 20, and 50, corresponding to training sample cases that were insufficient, moderate and sufficient, respectively. The rest of the labelled samples were used to assess the\naccuracies of the classifiers fine-tuned with different settings of regularization parameters. The results are presented in Fig. 12.\nFig. 12a shows the results for F . It was found that, if training samples were limited (NT = 5), higher classification accuracy was achieved with a larger F value. Low classification accuracy was observed (76.4%) when F was set to a small value of 0.01, as the information from previous images played an insignificant role. When F was set to 10 or higher, it was able to provide enough weight for the information from previous images, and considerably increased classification accuracies were achieved (>92.6%). The importance of F was less critical if training samples were sufficient (NT = 50). When F was increased from 0.01 to 10, the increase in classification accuracy was from 91.9% to 96.0%. This increase was less significant compared with the training sample insufficiency case (i.e., NT = 5).\nFig. 12b shows the results for C. When C was set to\n9 Fig. 12. Impact of the regularization parameters F and C on overall classification accuracy with different number of training samples (NT ) in the incoming image.\na small value such as 0.01, the contribution of information from training samples was low. In this case, low classification accuracies were observed for all the three NT levels. With increased C values, higher classification accuracies were achieved. However, decreases in classification accuracy occurred when C was larger than 100, because the classifier training relied too heavily on the insufficient training samples, and the contribution from previous images was suppressed. This phenomenon was more obvious for a lower NT .\nThese results indicate that, in order to achieve satisfactory classification accuracy, it is necessary to adequately incorporate and balance the contributions from previous images and training samples by appropriately setting F and C."}, {"heading": "D. Classification Results", "text": "We used the proposed SCT-SVM approach to classify the fifth image. The one-against-one strategy was used to extend the binary classifiers to multiple classes. According to the results presented above, regularization parameters F and C were set to 20 and 50, respectively. The second-order polynomial fitting function was selected for classifiers that involved one or more vegetative classes, while for the rest of the classifiers the first-order polynomial was selected. With the assistance of the first four images, the fifth incoming image was trained and classified under the training sample insufficiency situation (NT = 5). The training and classification were repeated 10 times with different sets of training samples selected from the labelled data. The classification accuracies were compared with those obtained when SVM was applied directly to the\nfifth image without using the a priori knowledge from previous images (Dir-SVM).\nThe results are shown in Fig. 13. It was found that, with the a priori knowledge provided by previous images, the overall classification accuracy was improved from 76.2% to 93.8%. The greatest improvements were found for crops and forests, two vegetative land cover types that share similar spectral characteristics and have relatively high temporal dynamics. The remaining three land cover types showed improved classification accuracies as well, though the improvements were comparatively lower. These results demonstrate that, when the training samples are limited, the utilization of a priori knowledge from previous images is advantageous to enhance classification accuracy.\nFig. 13. Comparison of classification accuracies with and without the a priori knowledge from previous images. Error bars indicate the standard deviations of classification accuracies obtained with different sets of training samples."}, {"heading": "IV. CONCLUSIONS", "text": "A SVM-based Sequential Classifier Training (SCT-SVM) approach is proposed for multitemporal remote sensing image classification. The approach leverages the classifiers of previous images to reduce the required number of training samples for the classifier training of a new image. Based on the temporal trend of previous classifiers, classifiers are firstly predicted for the new image. Then, a newly developed domain adaptation algorithm, Temporal-Adaptive Support Vector Machines (TA-SVM), is used to fine-tune the predicted classifiers into more accurate positions. Experimental results showed that the TA-SVM outperformed state-of-the-art algorithms in classifier fine-tuning. Compared with those obtained without the assistance from previous images, the overall classification accuracy was improved from 76.2% to 93.8% with the proposed SCT-SVM, which demonstrated that the leverage of a priori information from previous images could provide advantageous assistance for the classification of later images."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Mr. Graham Parton and Mr. Bernard Star at the Coleambally Irrigation Co-operative Ltd for providing farmland boundary data of the Coleambally Irrigation Area.\n10"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The explosive availability of remote sensing im-<lb>ages has challenged supervised classification algorithms such as<lb>Support Vector Machines (SVM), as training samples tend to<lb>be highly limited due to the expensive and laborious task of<lb>ground truthing. The temporal correlation and spectral similarity<lb>between multitemporal images have opened up an opportunity<lb>to alleviate this problem. In this study, a SVM-based Sequen-<lb>tial Classifier Training (SCT-SVM) approach is proposed for<lb>multitemporal remote sensing image classification. The approach<lb>leverages the classifiers of previous images to reduce the required<lb>number of training samples for the classifier training of an<lb>incoming image. For each incoming image, a rough classifier<lb>is firstly predicted based on the temporal trend of a set of<lb>previous classifiers. The predicted classifier is then fine-tuned<lb>into a more accurate position with current training samples. This<lb>approach can be applied progressively to sequential image data,<lb>with only a small number of training samples being required<lb>from each image. Experiments were conducted with Sentinel-2A<lb>multitemporal data over an agricultural area in Australia. Results<lb>showed that the proposed SCT-SVM achieved better classification<lb>accuracies compared with two state-of-the-art model transfer<lb>algorithms. Compared with those obtained without the assistance<lb>from previous images, the overall classification accuracy was<lb>improved from 76.2% to 93.8%, demonstrating that the lever-<lb>age of a priori information from previous images can provide<lb>advantageous assistance for later images in multitemporal image<lb>classification.", "creator": "LaTeX with hyperref package"}}}