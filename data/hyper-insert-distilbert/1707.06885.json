{"id": "1707.06885", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "SGNMT -- A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "abstract": "all this groundbreaking paper officially introduces { sgnmt, our experimental study platform for machine translation semantic research. \u00bb sgnmt technologies provides using a substantial generic machine interface framework to illustrate neural and symbolic scoring coding modules ( predictors ) with other left - to - right semantic limitations such as various translation mathematical models \u2014 like nmt, language models, translation lattices, $ { n $ - best tag lists or other kinds independent of neural scores and syntax constraints. predictors can moreover be similarly combined with few other predictors employed to form complex decoding tasks. sgnmt method implements a number of various search aggregation strategies for fully traversing the space equally spanned by the underlying predictors which arise are appropriate objects for different predictor type constellations. adding new predictors or decoding strategies is yet particularly easy, making it also a very efficient user tool for extensively prototyping new research ideas. and sgnmt support is likewise actively being better used by students in the mphil program institutes in : machine language learning, speech and technical language technology at further the university of cambridge for selected course examination work and theses, as well as testing for most of now the initial research work in our group.", "histories": [["v1", "Fri, 21 Jul 2017 13:14:25 GMT  (194kb,D)", "http://arxiv.org/abs/1707.06885v1", "Accepted as EMNLP 2017 demo paper"]], "COMMENTS": "Accepted as EMNLP 2017 demo paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix stahlberg", "eva hasler", "danielle saunders", "bill byrne"], "accepted": true, "id": "1707.06885"}, "pdf": {"name": "1707.06885.pdf", "metadata": {"source": "CRF", "title": "SGNMT \u2013 A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "authors": ["Felix Stahlberg", "Eva Hasler", "Danielle Saunders", "Bill Byrne"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We are developing an open source decoding framework called SGNMT, short for Syntactically Guided Neural Machine Translation.1 The software package supports a number of well-known frameworks, including TensorFlow2 (Abadi et al., 2016), OpenFST (Allauzen et al., 2007), Blocks/Theano (Bastien et al., 2012; van Merrie\u0308nboer et al., 2015), and NPLM (Vaswani et al., 2013). The two central concepts in the\n1http://ucam-smt.github.io/sgnmt/html/ 2SGNMT relies on the TensorFlow fork available at\nhttps://github.com/ehasler/tensorflow\nSGNMT tool are predictors and decoders. Predictors are scoring modules which define scores over the target language vocabulary given the current internal predictor state, the history, the source sentence, and external side information. Scores from multiple, diverse predictors can be combined for use in decoding.\nDecoders are search strategies which traverse the space spanned by the predictors. SGNMT provides implementations of common search tree traversal algorithms like beam search. Since decoders differ in runtime complexity and the kind of search errors they make, different decoders are appropriate for different predictor constellations.\nThe strict separation of scoring module and search strategy and the decoupling of scoring modules from each other makes SGNMT a very flexible decoding tool for neural and symbolic models which is applicable not only to machine translation. SGNMT is based on the OpenFSTbased Cambridge SMT system (Allauzen et al., 2014). Although the system is less than a year old, we have found it to be very flexible and easy for new researchers to adopt. Our group has already integrated SGNMT into most of its research work.\nWe also find that SGNMT is very well-suited for teaching and student research projects. In the 2015-16 academic year, two students on the Cambridge MPhil in Machine Learning, Speech and Language Technology used SGNMT for their dissertation projects.3 The first project involved using SGNMT with OpenFST for applying subword models in SMT (Gao, 2016). The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in \u2018Bach\u2019 chorales (Tomczak, 2016). The LSTM provides the \u2018creativity\u2019 and the WFSA enforces constraints\n3http://www.mlsalt.eng.cam.ac.uk/Main/ CurrentMPhils\nar X\niv :1\n70 7.\n06 88\n5v 1\n[ cs\n.C L\n] 2\n1 Ju\nl 2 01\n7\nthat the chorales must obey. This second project in particular demonstrates the versatility of the approach. For the current, 2016-17 academic year, SGNMT is being used heavily in two courses."}, {"heading": "2 Predictors", "text": "SGNMT consequently emphasizes flexibility and extensibility by providing a common interface to a wide range of constraints or models used in MT research. The concept facilitates quick prototyping of new research ideas. Our platform aims to minimize the effort required for implementation; decoding speed is secondary as optimized code for production systems can be produced once an idea has been proven successful in the SGNMT framework. In SGNMT, scores are assigned to partial hypotheses via one or many predictors. One predictor usually has a single responsibility as it represents a single model or type of constraint. Predictors need to implement the following methods:\n\u2022 initialize(src sentence) Initialize the predictor state using the source sentence.\n\u2022 get state() Get the internal predictor state.\n\u2022 set state(state) Set the internal predictor state.\n\u2022 predict next() Given the internal predictor state, produce the posterior over target tokens for the next position.\n\u2022 consume(token)Update the internal predictor state by adding token to the current history.\nThe structure of the predictor state and the implementations of these methods differ substantially between predictors. Tab. 2 lists all predictors which are currently implemented. Tab. 1 summarizes the semantics of this interface for three very common predictors: the neural machine translation (NMT) predictor, the (deterministic) finite state transducer (FST) predictor for lattice rescoring, and the n-gram predictor for applying n-gram language models. We also included two examples (word count and UNK count) which do not have a natural left-to-right semantic but can still be represented as predictors."}, {"heading": "2.1 Example Predictor Constellations", "text": "SGNMT allows combining any number of predictors and even multiple instances of the same predictor type. In case of multiple predictors we combine the predictor scores in a linear model. The following list illustrates that various interesting decoding tasks can be formulated as predictor combinations.\n\u2022 nmt: A single NMT predictor represents pure NMT decoding.\n\u2022 nmt,nmt,nmt: Using multiple NMT predictors is a natural way to represent ensemble decoding (Hansen and Salamon, 1990; Sutskever et al., 2014) in our framework.\n\u2022 fst,nmt: NMT decoding constrained to an FST. This can be used for neural lattice rescoring (Stahlberg et al., 2016) or other kinds of constraints, for example in the context of source side simplification in MT (Hasler et al., 2016) or chord progressions in \u2018Bach\u2019 (Tomczak, 2016). The fst predictor can also be used to restrict the output of character-based or subword-unit-based NMT to a large word-level vocabulary encoded as FSA.\n\u2022 nmt,rnnlm,srilm,nplm: Combining NMT with three kinds of language models: An RNNLM (Zaremba et al., 2014), a Kneser-Ney n-gram LM (Heafield et al., 2013; Stolcke et al., 2002), and a feedforward neural network LM (Vaswani et al., 2013).\n\u2022 nmt,ngramc,wc: MBR-based NMT following Stahlberg et al. (2017) with n-gram posteriors extracted from an SMT lattice (ngramc) and a simple word penalty (wc)."}, {"heading": "3 Decoders", "text": "Decoders are algorithms to search for the highest scoring hypothesis. The list of predictors determines how (partial) hypotheses are scored by implementing the methods initialize(\u00b7), get state(), set state(\u00b7), predict next(), and consume(\u00b7). The Decoder class implements versions of these methods which apply to all predictors in the list. initialize(\u00b7) is always called prior to decoding a new sentence. Many popular search strategies can be described via the remaining methods get state(), set state(\u00b7), predict next(), and consume(\u00b7). Algs. 1 and 2 show how to define greedy and beam decoding in this way.45\nTab. 3 contains a list of currently implemented decoders. The UML diagram in Fig. 1 illustrates the relation between decoders and predictors.\n4Formally, predict next() in Algs. 1 and 2 returns pairs of tokens and their costs.\n5String concatenation is denoted with \u00b7.\nAlgorithm 1 Greedy(src sen) 1: initialize(src sen) 2: h\u2190 \u3008<s>\u3009 3: repeat 4: P \u2190predict next() 5: (t, c)\u2190 argmax(t\u2032,c\u2032)\u2208P c\u2032 6: h\u2190 h \u00b7 t 7: consume(t) 8: until t = </s> 9: return h\nNMT batch decoding The flexibility of the predictor framework comes with degradation in decoding time. SGNMT provides two ways of speeding up pure NMT decoding, especially on the GPU. The vanilla decoding strategy exposes the beam search implementation in Blocks (van Merrie\u0308nboer et al., 2015) which processes all active hypotheses in the beam in parallel. We also implemented a beam decoder version which decodes multiple sentences at once (batch decoding) rather than in a sequential order. Batch decoding is potentially more efficient since larger batches can make better use of GPU parallelism. The key concepts of our batch decoder implementation are:\n\u2022 We use a scheduler running on a separate CPU thread to construct large batches of computation (GPU jobs) from multiple sentences and feeding them to the jobs queue.\n\u2022 The GPU is operated by a single thread which communicates with the CPU scheduler thread via queues containing jobs. This thread is only responsible for retrieving jobs in the jobs queue, computing them, and putting them in the jobs results queue, minimizing the down-time of GPU computation.\n\u2022 Yet another CPU thread is responsible for processing the results computed on the GPU\nAlgorithm 2 Beam(n, src sen) 1: initialize(src sen) 2: H \u2190 {(\u3008<s>\u3009, 0.0,get state())} 3: repeat 4: Hnext \u2190 \u2205 5: for all (h, c, s) \u2208 H do 6: set state(s) 7: P \u2190predict next() 8: Hnext \u2190 Hnext\u222a\u22c3\n(t\u2032,c\u2032)\u2208P (h \u00b7 t\u2032, c+ c\u2032, s) 9: end for 10: H \u2190 \u2205 11: for all (h, c, s) \u2208 n-best(Hnext) do 12: set state(s) 13: consume(h|h|) 14: H \u2190 H \u222a {(h, c,get state())} 15: end for 16: until Best hypothesis in H ends with </s> 17: return Best hypothesis in H\nin the job results queue, e.g. by getting the n-best words from the posteriors. Processed jobs are sent back to the CPU scheduler where they are reassembled into new jobs.\nThis decoder is able to translate the WMT English-French test sets news-test2012 to newstest2014 on a Titan X GPU with 911.6 words per second with the word-based NMT model described in Stahlberg et al. (2016).6 This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al., 2016) with reported decoding speeds of 865 words per second.7 However, batch decoding with MarianNMT is much faster reaching over 4,500 words\n6Theano 0.9.0, cuDNN 5.1, Cuda 8 with CNMeM, Intel R\u00a9 Core i7-6700 CPU 7Note that the comparability is rather limited since even though we use the same beam size (5) and vocabulary sizes (30k), we use (a) a slightly slower GPU (Titan X vs. GTX\nper second.8 We think that these differences are mainly due to the limited multithreading support and performance in Python especially when using external libraries as opposed to the highly optimized C++ code in Marian-NMT. We did not push for even faster decoding as speed is not a major design goal of SGNMT. Note that batch decoding bypasses the predictor framework and can only be used for pure NMT decoding.\nEnsembling with models at multiple tokenization levels SGNMT allows masking predictors with alternative sets of modelling units. The conversion between the tokenization schemes of different predictors is defined with FSTs. This makes it possible to decode by combining scores from both a subword-unit (BPE) based NMT (Sennrich et al., 2016) and a word-based NMT model with character-based NMT, masking the BPE-based and word-based NMT predictors with FSTs which transduce character sequences to BPE or word sequences. Masking is transparent to the decoding strategy as predictors are replaced by a special wrapper (fsttok) that uses the masking FST to translate predict next() and consume() calls to (a series of) predictor calls with alternative tokens. The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token. This allows combining scores at the word level even when using models with multiple levels of tokenization. Joint decoding with different tokenization schemes has the potential of combining the benefits of the different schemes: character- and BPE-based models are able to address rare words, but word-based NMT can model long-range dependencies more efficiently.\nSystem-level combination We showed in Sec. 2.1 how to formulate NMT ensembling as a set of NMT predictors. Ensembling averages the individual model scores in each decoding step. Alternatively, system-level combination decodes the entire sentence with each model separately, and selects the best scoring complete hypothesis over all models. In our experiments, system-level combination is not as effective as en-\n1080), (b) a different training and test set, (c) a slightly different network architecture, and (d) words rather than subword units.\n8https://marian-nmt.github.io/ features/\nsembling but still leads to moderate gains for pure NMT. However, a trivial implementation which selects the best translation in a postprocessing step after separate decoding runs is slow. The sepbeam decoding strategy reduces the runtime of system-level combination to the single system level. The strategy applies only one predictor rather than a linear combination of all predictors to expand a hypothesis. The single predictor is linked by the parent hypothesis. The initial stack in sepbeam contains hypotheses for each predictor (i.e. system) rather than only one as in normal beam search. We report a moderate gain of 0.5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from Stahlberg et al. (2017) using the sepbeam decoder.\nIterative beam search Normal beam search is difficult to use in a time-constrained setting since the runtime depends on the target sentence length which is a priori not known, and it is therefore hard to choose the right beam size beforehand. The bucket search algorithm sidesteps the problem of setting the beam size by repeatedly performing small beam search passes until a fixed computational budget is exhausted. Bucket search produces an initial hypothesis very quickly, and keeps the partial hypotheses for each length in buckets. Subsequent beam search passes refine the initial hypothesis by iteratively updating these buckets. Our initial experiments suggest that bucket search often performs on a similar level as standard beam search with the benefit of being able to support hard time constraints. Unlike beam search, bucket search lends itself to risk-free (i.e. admissible) pruning since all partial hypotheses worse than the current best complete hypothesis can be discarded."}, {"heading": "4 Conclusion", "text": "This paper presented our SGNMT platform for prototyping new approaches to MT which involve both neural and symbolic models. SGNMT supports a number of different models and constraints via a common interface (predictors), and various search strategies (decoders). Furthermore, SGNMT focuses on minimizing the implementation effort for adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm. SGNMT is actively being used for teaching and research and we\nwelcome contributions to its development, for example by implementing new predictors for using models trained with other frameworks and tools."}, {"heading": "Acknowledgments", "text": "This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Pushdown automata in statistical machine translation", "author": ["Cyril Allauzen", "Bill Byrne", "Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Michael Riley."], "venue": "Computational Linguistics, 40(3):687\u2013723.", "citeRegEx": "Allauzen et al\\.,? 2014", "shortCiteRegEx": "Allauzen et al\\.", "year": 2014}, {"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri."], "venue": "International Conference on Implementation and Application of Automata, pages 11\u201323.", "citeRegEx": "Allauzen et al\\.,? 2007", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: New features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "NIPS.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Variable length word encodings for neural translation models", "author": ["Jiameng Gao."], "venue": "MPhil dissertation, University of Cambridge.", "citeRegEx": "Gao.,? 2016", "shortCiteRegEx": "Gao.", "year": 2016}, {"title": "Neural network ensembles", "author": ["Lars Kai Hansen", "Peter Salamon."], "venue": "IEEE transactions on pattern analysis and machine intelligence, 12(10):993\u2013 1001.", "citeRegEx": "Hansen and Salamon.,? 1990", "shortCiteRegEx": "Hansen and Salamon.", "year": 1990}, {"title": "Source sentence simplification for statistical machine translation", "author": ["Eva Hasler", "Adri\u00e0 de Gispert", "Felix Stahlberg", "Aurelien Waite", "Bill Byrne."], "venue": "Computer Speech & Language.", "citeRegEx": "Hasler et al\\.,? 2016", "shortCiteRegEx": "Hasler et al\\.", "year": 2016}, {"title": "A comparison of neural models for word ordering", "author": ["Eva Hasler", "Felix Stahlberg", "Marcus Tomalin", "Adri\u00e0 de Gispert", "Bill Byrne."], "venue": "INLG, Santiago de Compostela, Spain.", "citeRegEx": "Hasler et al\\.,? 2017", "shortCiteRegEx": "Hasler et al\\.", "year": 2017}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "ACL, pages 690\u2013696, Sofia, Bulgaria.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "arXiv preprint arXiv:1610.01108.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1506.00619.", "citeRegEx": "Merri\u00ebnboer et al\\.,? 2015", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "ASPEC: Asian scientific paper excerpt corpus", "author": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "LREC, pages 2204\u20132208, Portoroz, Slovenia.", "citeRegEx": "Nakazawa et al\\.,? 2016", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2016}, {"title": "Artificial Intelligence: A Modern Approach, 2 edition", "author": ["Stuart J. Russell", "Peter Norvig."], "venue": "Pearson Education.", "citeRegEx": "Russell and Norvig.,? 2003", "shortCiteRegEx": "Russell and Norvig.", "year": 2003}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL, pages 1715\u20131725, Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Efficient left-to-right hierarchical phrase-based translation with improved reordering", "author": ["Maryam Siahbani", "Baskaran Sankaran", "Anoop Sarkar."], "venue": "EMNLP, pages 1089\u20131099, Seattle, Washington, USA.", "citeRegEx": "Siahbani et al\\.,? 2013", "shortCiteRegEx": "Siahbani et al\\.", "year": 2013}, {"title": "Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne."], "venue": "EACL, pages 362\u2013368, Valencia, Spain.", "citeRegEx": "Stahlberg et al\\.,? 2017", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2017}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "ACL, pages 299\u2013305, Berlin, Germany.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Interspeech, volume 2002, page 2002.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS, pages 3104\u20133112. MIT Press.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Bachbot", "author": ["Marcin Tomczak."], "venue": "MPhil dissertation, University of Cambridge.", "citeRegEx": "Tomczak.,? 2016", "shortCiteRegEx": "Tomczak.", "year": 2016}, {"title": "Decoding with largescale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "EMNLP, pages 1387\u20131392, Seattle, Washington, USA.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "1 The software package supports a number of well-known frameworks, including TensorFlow2 (Abadi et al., 2016), OpenFST (Allauzen et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 2, "context": ", 2016), OpenFST (Allauzen et al., 2007), Blocks/Theano (Bastien et al.", "startOffset": 17, "endOffset": 40}, {"referenceID": 4, "context": ", 2007), Blocks/Theano (Bastien et al., 2012; van Merri\u00ebnboer et al., 2015), and NPLM (Vaswani et al.", "startOffset": 23, "endOffset": 75}, {"referenceID": 21, "context": ", 2015), and NPLM (Vaswani et al., 2013).", "startOffset": 18, "endOffset": 40}, {"referenceID": 5, "context": "3 The first project involved using SGNMT with OpenFST for applying subword models in SMT (Gao, 2016).", "startOffset": 89, "endOffset": 100}, {"referenceID": 20, "context": "The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in \u2018Bach\u2019 chorales (Tomczak, 2016).", "startOffset": 159, "endOffset": 174}, {"referenceID": 4, "context": "Supports Blocks/Theano (Bastien et al., 2012; van Merri\u00ebnboer et al., 2015) and TensorFlow (Abadi et al.", "startOffset": 23, "endOffset": 75}, {"referenceID": 0, "context": ", 2015) and TensorFlow (Abadi et al., 2016).", "startOffset": 23, "endOffset": 43}, {"referenceID": 17, "context": "fst Predictor for rescoring deterministic lattices (Stahlberg et al., 2016).", "startOffset": 51, "endOffset": 75}, {"referenceID": 1, "context": "rtn Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion.", "startOffset": 71, "endOffset": 94}, {"referenceID": 9, "context": "srilm n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit.", "startOffset": 55, "endOffset": 100}, {"referenceID": 21, "context": "nplm Neural n-gram language models based on NPLM (Vaswani et al., 2013).", "startOffset": 49, "endOffset": 71}, {"referenceID": 8, "context": "bow Restricts the search space to a bag of words with or without repetition (Hasler et al., 2017).", "startOffset": 76, "endOffset": 97}, {"referenceID": 15, "context": "lrhiero Experimental implementation of leftto-right Hiero (Siahbani et al., 2013) for small grammars.", "startOffset": 58, "endOffset": 81}, {"referenceID": 0, "context": "Predictor Description nmt Attention-based neural machine translation following Bahdanau et al. (2015). Supports Blocks/Theano (Bastien et al.", "startOffset": 79, "endOffset": 102}, {"referenceID": 0, "context": ", 2015) and TensorFlow (Abadi et al., 2016). fst Predictor for rescoring deterministic lattices (Stahlberg et al., 2016). nfst Predictor for rescoring nondeterministic lattices. rtn Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion. srilm n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit. nplm Neural n-gram language models based on NPLM (Vaswani et al., 2013). rnnlm Integrates RNN language models with TensorFlow as described by Zaremba et al. (2014). forced Forced decoding with a single reference.", "startOffset": 24, "endOffset": 568}, {"referenceID": 0, "context": ", 2015) and TensorFlow (Abadi et al., 2016). fst Predictor for rescoring deterministic lattices (Stahlberg et al., 2016). nfst Predictor for rescoring nondeterministic lattices. rtn Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion. srilm n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit. nplm Neural n-gram language models based on NPLM (Vaswani et al., 2013). rnnlm Integrates RNN language models with TensorFlow as described by Zaremba et al. (2014). forced Forced decoding with a single reference. forcedlst n-best list rescoring. bow Restricts the search space to a bag of words with or without repetition (Hasler et al., 2017). lrhiero Experimental implementation of leftto-right Hiero (Siahbani et al., 2013) for small grammars. wc Number of words feature. unkc Applies a Poisson model for the number of UNKs in the output. ngramc Integrates external n-gram posteriors, e.g. for MBR-based NMT according Stahlberg et al. (2017). length Target sentence length model using simple source sentence features.", "startOffset": 24, "endOffset": 1049}, {"referenceID": 6, "context": "ble decoding (Hansen and Salamon, 1990; Sutskever et al., 2014) in our framework.", "startOffset": 13, "endOffset": 63}, {"referenceID": 19, "context": "ble decoding (Hansen and Salamon, 1990; Sutskever et al., 2014) in our framework.", "startOffset": 13, "endOffset": 63}, {"referenceID": 17, "context": "This can be used for neural lattice rescoring (Stahlberg et al., 2016) or other kinds of constraints, for example in the context of source side simplification in MT (Hasler et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 7, "context": ", 2016) or other kinds of constraints, for example in the context of source side simplification in MT (Hasler et al., 2016) or chord progressions in \u2018Bach\u2019 (Tomczak, 2016).", "startOffset": 102, "endOffset": 123}, {"referenceID": 20, "context": ", 2016) or chord progressions in \u2018Bach\u2019 (Tomczak, 2016).", "startOffset": 40, "endOffset": 55}, {"referenceID": 22, "context": "\u2022 nmt,rnnlm,srilm,nplm: Combining NMT with three kinds of language models: An RNNLM (Zaremba et al., 2014), a Kneser-Ney n-gram LM (Heafield et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 9, "context": ", 2014), a Kneser-Ney n-gram LM (Heafield et al., 2013; Stolcke et al., 2002), and a feedforward neural network LM (Vaswani et al.", "startOffset": 32, "endOffset": 77}, {"referenceID": 21, "context": ", 2002), and a feedforward neural network LM (Vaswani et al., 2013).", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "(Bahdanau et al., 2015).", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "astar A* search (Russell and Norvig, 2003).", "startOffset": 16, "endOffset": 42}, {"referenceID": 16, "context": "\u2022 nmt,ngramc,wc: MBR-based NMT following Stahlberg et al. (2017) with n-gram posteriors extracted from an SMT lattice (ngramc) and a simple word penalty (wc).", "startOffset": 41, "endOffset": 65}, {"referenceID": 10, "context": "6 This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al., 2016) with reported decoding speeds of 865 words per second.", "startOffset": 126, "endOffset": 156}, {"referenceID": 15, "context": "6 words per second with the word-based NMT model described in Stahlberg et al. (2016).6 This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 14, "context": "This makes it possible to decode by combining scores from both a subword-unit (BPE) based NMT (Sennrich et al., 2016) and a word-based NMT model with character-based NMT, masking the BPE-based", "startOffset": 94, "endOffset": 117}, {"referenceID": 12, "context": "5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from Stahlberg et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 12, "context": "5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from Stahlberg et al. (2017) using the", "startOffset": 68, "endOffset": 160}], "year": 2017, "abstractText": "This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, n-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.", "creator": "LaTeX with hyperref package"}}}