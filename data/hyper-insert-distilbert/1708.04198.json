{"id": "1708.04198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "A scalable multi-core architecture with heterogeneous memory structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "abstract": "neuromorphic computing chip systems comprise net - wise works of neurons software that use asynchronous transient events for both computation time and semantic communication. specifically this intermediate type mode of performance representation simultaneously offers arguably several advantages in terms of bandwidth access and its power consumption in different neuromorphic electronic systems. however, managing twice the total traffic of asynchronous events in large scale systems normally is a terribly daunting task, appealing both in terms of circuit complexity and memory separation requirements. precisely here firstly we present a possible novel routing methodology that employs both structured hierarchical and mesh spatial routing improvement strategies \u2026 and concurrently combines heterogeneous memory structure structures for actually minimizing both memory requirements and latency, while likewise maximizing programming flexibility to support a locally wide range subset of event - dependence based neural network architectures, presumably through their parameter configuration. we validated the naturally proposed scheme in considering a prototype multi - core wired neuromorphic protocol processor and chip that continuously employs hybrid physical analog / digital circuits defined for safely emulating sequential synapse and neuron dynamics together with connected asynchronous digital circuits for managing concurrently the available address - event protocol traffic. visually we mainly present a pure theoretical static analysis of the proposed connectivity scheme, you describe specifically the methods proposed and circuits used to implement such scheme, designing and also characterize thus the simulation prototype chip. finally, we demonstrate the considerable use possible of coordinating the neuromorphic processor with basically a convolutional internal neural communication network for reducing the real - / time color classification of visual digital symbols being computer flashed to enable a special dynamic vision sensor ( dvs ) cpu at historically high startup speed.", "histories": [["v1", "Mon, 14 Aug 2017 16:28:02 GMT  (9526kb,D)", "http://arxiv.org/abs/1708.04198v1", "17 pages, 14 figures"], ["v2", "Wed, 16 Aug 2017 17:50:21 GMT  (7787kb,D)", "http://arxiv.org/abs/1708.04198v2", "17 pages, 14 figures"]], "COMMENTS": "17 pages, 14 figures", "reviews": [], "SUBJECTS": "cs.AR cs.AI", "authors": ["saber moradi", "ning qiao", "fabio stefanini", "giacomo indiveri"], "accepted": false, "id": "1708.04198"}, "pdf": {"name": "1708.04198.pdf", "metadata": {"source": "CRF", "title": "A scalable multi-core architecture with heterogeneous memory structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "authors": ["Saber Moradi", "Ning Qiao", "Fabio Stefanini", "Giacomo Indiveri"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Neuromorphic computing, routing architectures, asynchronous, circuits and systems\nI. Introduction In an effort to develop a new generation of brain-inspired non von Neumann computing systems, several neuromorphic computing platforms have been proposed in recent years, that implement spike-based re-configurable neural networks [1]\u2013 [8]. Despite being developed with different goals in mind and following different design strategies, most of these architectures share the same data representation and signal communication protocol: the Address-Event Representation (AER) [9], [10]. In this representation computational units (e.g., neurons) are assigned an address that is encoded as a digital word and transmitted as soon they produce an event (e.g., as soon as the neuron spikes) using asynchronous digital circuits. Information is therefore encoded in the timing of these address-events. In event-based neural networks the neurons inter-spike interval (the interval between successive address events produced by the same neuron) represent the analog data, and neural computation is achieved by connecting multiple neurons among each other with different types of connectivity schemes. Spike produced by source neurons are transmitted to one or more destination synapse circuits that integrate them with different gain factors and convey them to the post-synaptic neuron. Unlike classical digital logic circuits, these networks are typically characterized by very large fan-in and fan-out numbers. For example, in cortical networks neurons project on average to about 10000 destinations. The type of processing and functionality of these spiking neural networks is determined by their specific structure and parameters, such as the properties of the neurons or the weights of the synapses [11]. It is therefore important to design neuromorphic computing platforms that can be configured to support the construction of different network topologies, with different neuron and synapse properties. This requires the development of configurable neuron and synapse circuits and of programmable AER routing and communication schemes. The latter elements are particularly important, because the scalability of neuromorphic systems is mainly restricted by communication requirements. Indeed, some of the main bottlenecks in the construction of large-scale re-configurable neuromorphic computing platforms are the bandwidth, latency, and memory requirements for routing address-events among neurons. Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5]. In particular, most approaches proposed either make use of 2D mesh routing schemes, with maximum flexibility, but at the cost of large resource usage, or tree routing schemes which minimize latency and power, but are more restrictive in the types of networks that can be supported (see [13] for a comprehensive overview comparing most of the approaches that have been proposed in the literature). In [13] the authors proposed a hierarchical address event routing scheme (HiAER) that overcomes some of the limitations of previous flat treebased approaches. However, as with many of the previous approaches [5], the HiAER architecture stores the routing tables in external memory banks implemented using Dynamic Random Access Memory (DRAM). Within this context, these approaches do not represent a radical departure from the classical von Neumann architecture, as they are affected by the ar X iv :1 70 8. 04 19 8v 1 [ cs .A R ] 1 4 A\nug 2\n2 von Neumann bottleneck problem [14], [15]. In this paper we propose a radically new routing scheme that integrates within the arrays of neurons and synapses both asynchronous routing circuits and heterogeneous memory structures used to store the data as distributed programmable routing tables. The routing scheme has been obtained by analyzing all previous approaches, and carrying out a systematic study for minimizing memory resources and maximizing network programmability and flexibility. The approach we propose combines the advantages of all previous approaches proposed up to now by combining 2D-mesh with hierarchical routing, implementing a 2-stage routing scheme for minimizing memory usage, which employs a combination of point-to-point sourceaddress routing and multi-cast destination-address routing, and by using heterogeneous memory structures distributed within and across the neuron and synapse arrays which are optimally suited to exploit emerging memory technologies such as Resistive Random Access Memory (RRAM) [16].\nIn the Section II we present the theory that was developed to minimize memory requirements in multi-neuron architectures. In Section III we describe the mixed mode hierarchical-mesh routing scheme proposed, and the Quasi Delay Insensitive (QDI) circuits designed to implement them; in Section IV we present a multi-core neuromorphic chip that was designed and fabricated to validate this routing scheme. The chip comprises 1k VLSI neurons distributed among four neural cores and programmable routers for managing the AER traffic. The routing fabric implemented in this chip uses new designs of quasi-delay insensitive asynchronous circuits, synthesized following the Communicating Hardware Process (CHP) formalism, while the neural and synaptic dynamics are designed using ultralow power subthreshold neuromorphic circuits [17]. Finally, in Section V we demonstrate an application of this architecture by configuring the neuromorphic processor to implement a Convolutional Neural Network (CNN) designed to process fast sequences of visual patterns, represented by AddressEvents (AEs) that are generated by a Dynamic Vision Sensor (DVS) for low latency classification. We conclude the paper by comparing the features of the circuits and approach proposed to the current state-of-the-art and discuss the benefits of the proposed architecture."}, {"heading": "II. Memory optimized routing", "text": "Consider a generic spiking neural network with N neurons in which each neuron has a large fan-out of F . This is the case for many neural network models including biologically plausible models of cortical networks, recurrent neural networks, convolutional networks and deep networks. In standard routing schemes e.g., source or destination-based methods, each neuron would be assigned a unique address encoded with log2(N) bits. Therefore, to support F fan-out destinations per neuron, a storage of F log2(N) bits/neuron would be required. In total, the size of the connection memory the whole network would then be NF log2(N). While this scheme can support any type of connectivity, it is very wasteful in case of networks that have a specific structure, i.e., for most of the real and artificial neural networks [18], [19].\nF/M\nF/M\nbroadcast\nbroadcast\npoint to point\n1\nN\n1\nN/C\n1\nC\n1\nC\nTag Intermediatenodes log(K) log(N/C)\nSource memory\nTag\nlog(K)\nTarget memory\nM neurons subscribed\nK tags\nM neurons subscribed\nK tags\nFig. 1: Two-stage tag-based routing scheme. The connectivity of a network of N neurons with fan-out F is implemented by a scheme that uses N neurons with a reduced fan-out of F/M that transmit their tag data to N/C intermediate nodes, with point-to-point routing. Each intermediate node broadcasts its tag data to all neurons of its target cluster. Each cluster has C neurons, with a subset of M neurons subscribed to the incoming tag. The total unique number of tags used in each cluster is K. Note that the nodes on the right side represent the same neurons of the left side, but grouped into N/C clusters.\nInspired by the connectivity patterns in biological neural networks [20], [21] we developed a novel routing scheme that exploits their clustered connectivity structure to reduce memory requirements in large-scale neural networks. This novel method uses a mixed tag-based shared-addressing scheme, instead of plain source or destination addressing schemes. The main idea is to introduce different clusters with independent address spaces, so that it is possible to re-use the same tag ids for connecting neurons among each other, without loss of generality. Specifically, in the proposed routing method neurons are grouped in separate N/C clusters of size C, and the connectivity with destination nodes (i.e., the fan-out operation) is divided into two distinct stages [22]. In the first stage, the neurons use source-address routing for targeting a subset (F/M , with F > M ) of the intermediate nodes. The number of intermediate nodes is N/C (with F/M 6 N/C). In the second stage the intermediate nodes broadcast the incoming events for targeting a number M \u2264 C of destination neurons within each local domain (or cluster) C (see Fig. 1). Each neuron in the end-point cluster uses a set of tags (one of K tags for the cluster) to determine whether to accept or ignore the AER packet received. In addition to distributing the memory resources among multiple nodes (neurons) within the computing fabric, therefore eliminating the von Neumann bottleneck [14], [15], this two-stage routing scheme allows us to minimize the total digital memory for configuring the\n3 network connectivity1, still allowing for large M -way fan-out: the total memory MEM required per neuron can be separated into Source memory MEMS and Target memory MEMT . At the sender side each source neuron stores F/M entries. Each of these entries has two fields: the first field requires log2(K) bits for representing a source tag and the second requires log2(N/C) bits to represent the address of the target intermediate nodes (see also Fig. 1). Therefore the total Source memory required is MEMS = (F/M)(log2(K) + log2(N/C)) bits/neuron. At the destination side, assuming that the tags are uniformly distributed, each neuron needs to store M tags. This leads to using KM tag entries per cluster and KM/C tags per neuron, with each tag requiring log2(K) bits. As a consequence, a total Target memory of MEMT = (KM/C)(log2(K)) bits/neuron is required. Taking into account these values, we get:\nMEM = MEMS +MEMT (1)\nMEM = F M\nlog2 ( KN\nC\n) + KM\nC log2(K) (2)\nAs evident from eq. 2, larger clusters and fewer tags lead to reduced memory requirements. Reducing the number of tags reduces the routing flexibility. However this can be counter balanced by increasing the number of clusters. By considering the ratio \u03b1 = K/C it is possible to minimize for memory usage while maximizing for routing flexibility: if we substitute K = \u03b1C in eq.2, we get:\nMEM = F M log2(\u03b1N) + \u03b1M log2(\u03b1C) (3)\nThe parameter M determines the trade-off between point-topoint copying in the first stage versus broadcasting in the second stage of the routing scheme. The total memory requirement can therefore be minimized by differentiating eq. 3 with respect to M , and determining the optimal M\u2217:\n0 = \u03b1 log2(\u03b1C)\u2212 F\nM2 log2(\u03b1N) (4)\nM\u2217 =\n\u221a F\n\u03b1 log2(\u03b1N) log2(\u03b1C)\n(5)\nWith this choice of M , the total number of bits per neuron required are MEM = 2 \u221a \u03b1F log2(\u03b1C) log2(\u03b1N). If for example we set, as a design choice, \u03b1 = 1, then we obtain M\u2217 = \u221a F log2(N)/ log2(C). This leads to a total memory/neuron requirement of:\nMEM = 2 \u221a F log2(C) log2(N) (6)\nWe therefore developed a scheme in which the memory requirements scale with N in a way that is drastically lower than the scaling of standard routing schemes, that require F log2(N) bits/neuron (e.g. compare 160k bits/neuron of memory required for a network of approximately 1 million (220) neurons, with fan-out of almost 10000 (213) with the conventional routing approach, versus less than 1.2k bits/neuron required for our\n1provided that M \u2264 F and M \u2264 C \u2013 see Appendix VII-A for an in-depth analysis of the feasibility of these constraints.\nmemory-optimized scheme, for a network of same size, with same fan-out, and with cluster size of 256 neurons). Unlike basic source or destination based routing methods, the proposed scheme is a mixed one that makes use of tag addresses which are shared among source and destination neurons. This results in a smaller address space to encode the connections among neurons and fewer memory entries to support large fan-out in biologically plausible clustered neural networks."}, {"heading": "III. Mixed-mode hierarchical-mesh routing architecture", "text": "In this section we propose a multi-core routing architecture that deploys the memory optimization scheme of Section II: each cluster of Fig. 1 is mapped onto a \u201ccore\u201d, the intermediate nodes are implemented by asynchronous routers, and the tags are stored into asynchronous Content Addressable Memory (CAM) blocks. The CAM block (of each neuron) contains multiple tags representing the address of sources that this neuron is subscribed to. To optimize the event routing within the network, we adopted a mixed-mode approach that combines the advantages of mesh routing schemes (low bandwidth requirements, but high latency), with those of hierarchical routing ones (low latency, but high bandwidth requirements) [3]. Specifically, this architecture adopts a hierarchical routing scheme with three distinct levels of routers: at the lowest level an R1 is responsible for the local traffic, it either sends back the events to the present core or the next level of hierarchy. The events sent from the R1 to the core are broadcast to all nodes within the core; following the prescription of the scheme described in Section II, In other words, incoming events are presented to the CAM blocks of all neurons and consequently they will be accepted by the nodes as valid inputs if there is match in the CAM. The non-local events are sent to the second level router (R2) which has bidirectional channels to communicate with local cores and the next level router. Depending on the complexity of the network, this treebased routing structure can have multiple R2 levels (e.g., see Fig. 2 for an example with three R2 levels). For transmitting data packets across even longer distances, the highest level router (R3) is employed, which uses a relative 2D-mesh (also known as xy algorithm) routing strategy. A combination of R1, R2, and R3 routers represent the intermediate nodes of Fig. 1, the cores represent the clusters, the Source memory is stored in Static Random Access Memory (SRAM) cells in the R1 routers, and the Target memory in the synapse CAM cells. For the prototype described in this paper, we have chosen three level of routing. The routing network in our implementations is based on the use of ultra-low power and low-latency QDI asynchronous digital routing and communication circuits."}, {"heading": "A. Quasi Delay-Insensitive asynchronous circuit design methodology", "text": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most\n4 R1\nCo re\n0\nR1\nCore3\nCo re\n2\nR1\nCore1\nR1\nR1 Co re 16 R1\nCore19\nCo re\n18\nR1\nCore17\nR1\nR1\nCo re\n28\nR1\nCore31\nCo re\n30\nR1\nCore29\nR1\nR1\nCo re\n20\nR1 Core23 Co re 22 R1\nCore21\nR1\nR1 Co re 24 R1\nCore27\nCo re\n26\nR1\nCore25\nR1\nBranch level 1 Branch level 2\nChip\nR1\nCo re\n48\nR1\nCore51\nCo re\n50\nR1\nCore49\nR1\nR1\nCo re\n32\nR1\nCore35\nCo re\n34\nR1\nCore33\nR1\nR1\nCo re\n44\nR1\nCore47\nCo re\n46\nR1\nCore45\nR1\nR1\nCo re\n36\nR1\nCore39\nCo re\n38\nR1\nCore37\nR1\nR1\nCo re\n40\nR1\nCore43\nCo re\n42\nR1\nCore41\nR1\nR1\nCo re\n60\nR1\nCore63\nCo re\n62\nR1\nCore61\nR1\nR1\nCo re\n52\nR1\nCore55\nCo re\n54\nR1\nCore53\nR1\nR1\nCo re\n56\nR1\nCore59\nCo re\n58\nR1\nCore57\nR1\nR2 (level 2)\nR2 (level 2)\nR2 (level 2)\nR1\nCo re\n12\nR1\nCore15\nCo re\n14\nR1\nCore13\nR1\nR1\nCo re\n4\nR1\nCore7\nCo re\n6\nR1\nCore5\nR1\nR1\nCo re\n8\nR1\nCore11\nCo re\n10\nR1\nCore9\nR1\nR2 (level 3)\nR3\nR2 (level 1)\nR2 (level 1)\nR2 (level 1)\nR2 (level 1)\nR2 (level 1) R2 (level 1) R2 (level 1) R2 (level 1)\nR2 (level 1)\nR2 (level 1)\nR2 (level 1)\nR2 (level 1)\nR2 (level 1) R2 (level 1) R2 (level 1) R2 (level 1)\nR2 (level 2)\n5 c.t c.f\nin.t<0> in.f<0>\nin.t<9> in.f<9>\nin.v\nin.v\nen0\nout.v\nen0\nin.v\nin.a in.e en1\nen1\nin.a\nout.v\nout.a\nin.a\nen0\nin.v\nin.e\nin.e\nc.f\nen1\nout.e\nen0\nen0\nout.e\nin.t<9:0>\nc.t c.t\nin.f<9:0>\nout.e\nen0\nen0\nout.e\nout.t<9:0> out.f<9:0>\nHandshaking\nFunctionValidity\nout.eout.a"}, {"heading": "B. Asynchronous routing fabric", "text": "In the following we describe the block diagrams of the QDI routing architecture implemented following CHP design methodology. As an example design choice, we assign 256 neurons/core (i.e., C of Fig. 1 is set to 256), and four cores/tile (i.e., M is set to 4); we assume that each neuron can copy its output to four different destination cores; and, for sake of simplicity, we assume that one tile corresponds to a single VLSI chip, which requires only one level of R2 arbiters in the hierarchy. 1) The R1 router: Each core in the chip is paired to a R1 router, and each R1 router has an embedded SRAM of size proportional to the number of source nodes in the core. The block-diagram of the R1 router is shown in Fig. 4. Addressevents generated by neurons in the corresponding core \u201cx\u201d are sent to R1 (see corex.in signal) The data packet representing the address-event is extended, by appending two additional bits that encode the fan-out for the first-level point-to-point copy\n(equivalent to F/M in Section II). The extended packet then enters the \"memory address loop\" of Fig. 4, which comprises a QDI merge, a buffer, and a split process, as well as a controlledpass and a decrement process. The merge process is nondeterministic: it does arbitration on two input requests and allows the winner to send its message through to the output. Initially the input data packet is copied, via the merge process to a buffer and then split into two branches: the bottom output of the split process is used to address an asynchronous SRAM block, and read the content of the addressed memory; while the upper output of the split process is fed to the controlled-pass process of the feed-back loop. The controlled-pass checks the header bits (the two bits appended in previous stage) and passes the packet through if the header value is non-zero, otherwise it skips. When passed, the value of header bit is decremented and then merged back to the forward path, in order to read the next address of the SRAM. This \"memory loop address\" process continues until all memory information for the corresponding event is read. The content of each SRAM cell is a 20-bit word, which includes a 10-bit tag address, a 6-bit header information, and a 4-bit destination core id. The 6-bit header uses 2-bits to encode the \u2206X number of hops, 2-bits for the \u2206Y hops, one sign bit for the X direction (east or west) and one for the Y directions (north or south). A controlled split process after the SRAM checks the core and chip destination address bits, and decides whether to broadcast the data back to the same core, to send it point-to-point to a different core on the same chip, or to send it via mesh-routing to a core on a different chip through the R2 and R3 routers. The controlled split is a one-input to two-output process where control bits decide which output receives the input data, unlike a normal split process where incoming data are sent to both outputs.\n2) The R2 router: Each R2 router has four bidirectional links to R1 routers of the same level of the cores in the chip and one bidirectional link to the higher level router. This router manages both the inter-tile communication and the communication with longer distance targets (see Fig. 5). Based on a hierarchical 2D tree architecture, this router is at the heart of the proposed routing scheme. At each level of the tree hierarchy there is one bidirectional link between R2 and each core in the same level (total 4) and one bidirectional link between R2 and the higher-level router. With the design choices we made for these block diagrams, the next level in the hierarchy is directly represented by an R3 router. Data from all the incoming R1 channels (R1.core0.in \u2212 \u2212R1.core3.in) is first merged into a single stream by a merge tree. Then bychecking the 6-bit chip destination address entries of the packet, it determines whether to route events to the higher level inter-chip router via R3.out port, or to redirect events to one of the local R1 routers. In the latter case, a controlled split tree decides which local core to target. Similarly, on the downstream pathway, the R2 router receives events form the R3 router (see R3.in) and a controlled split tree decides which destination core to target, using the destination core id specified in the packet\u2019s header.\n3) The R3 router: This block routes events among multiple tiles with relative distance addressing along a 2D-mesh. The block diagram of R3 router is illustrated in Fig.6. On one\n6 BUFs CPASS\n0\n1\ncorex.in\nR2.in appends two additional bits for reading corresponing SRAMs\n\"MEMORY ADDRESS LOOP\": keep source address in the loop until finish reading corresponing SRAMs\nappend destination address stored in SRAM to source address\nmerge incoming events from R2\nR2.out\ncore.out\ncheck whether events belong to local core\nAppend\nDecrement\nSP\nCSP\nMG\nMG\nSRAM 20Kb\nMG: Merge SP: Split CSP: Controlled Split CPASS: Controlled Pass\ncorex R2R1router\nFig. 4: Block diagram of the R1 router. There is one R1 router/core. Each R1 router has a bidirectional link between itself and its local core, and a bidirectional link between itself and its R2 router.\nmerge events from all input channels\nBUFs\ndx=0&dy=0\nCSP\n0\n1\nR3.out check whether events are local (to be sent to cores) or global (to be sent to R3 router)\nCHKR1.core0.in\nInterface.in\nMGT\nR1.core3.in MG\nmerge events from R3 and local cores and deliver them to corresponding cores by two controlled split tree (one for local events and the other for R3 events) and four two-way merges.\nR3.in\nSPT R1.core3.out\nR1.core0.outSPT\n0\n0\n3\n3\nMG3\nMG0\nSPT: Split Tree MGT: Merge Tree CHK: Check\nInterface.in\nR3\nR2 router R1.core2\nR1.core1\nR1.core3\nR1.core0\nFig. 5: Block diagram of the R2 router. This router has bidirectional links with four R1 routers in the same chip, and one bidirectional link with higher level router for managing inter-chip traffic.\nside, this router communicates with its lower-level R2 router in same tile, and on other side it communicates with other four R3 routers of other chips/tiles in a global 2D-mesh network following an \u201cXY routing\u201d method. This method first routes events in X direction until \u2206X is decremented to zero, and then in the Y direction.\nOn the upstream pathway, the router buffers the signals R2.in, and performs a control split operation: it first checks \u2206X 6= 0, and if yes sends the data to the west port (if signX > 0) or to the east port (if signX < 0). If the value of \u2206X = 0 the events are sent to the north or south ports, depending on the value of \u2206Y . As events are passed through, the corresponding \u2206X and/or \u2206Y values are decremented by one. There are two bits signX and signY in the packet that specify the sign of \u2206X and \u2206Y changes. On the downstream pathway, events can arrive from the south/north ports, or from the east/west ones. In the first case, south/north input events trigger a check of the value of \u2206Y , done by a controlled split process. If this value is zero, the events are sent to the local R2 router (see R2.out in Fig. 6). Otherwise they are sent to the north/south link. In the case of incoming events from south/north ports,\nthe \u2206X value is not checked as it will always equal to zero. In the case of an event reaching the east/west port, this triggers a control split process to check the value of \u2206X . If this is not zero, it is passed onto the west/east port and processed as described above, otherwise a check is made on the \u2206Y value, via a second split control process. Depending on the value of \u2206Y the data is passed onto the south/north port, and \u2206Y is decremented.\n4) Input Interface: This additional QDI block is used for sending stimuli to the neurons in the cores from external sources of address-events, to program the distributed SRAM and CAM memory cells, to set additional network configuration parameters, and to configure different circuit biases (see Fig. 7). Once address-events reach this block (e.g., from a Field Programmable Gate Array (FPGA) device, via the FPGA.in signals), a first check is made to see if the data packet contains the correct chip/tile address. If so, the data packet is first checked via a control split process to see if the data represent programming or configuration commands, or normal address-events. In the latter case, the data is sent directly to the destination core. Otherwise the data packet\n7 deliver events to W/E if dx =! 0, otherwise deliver events to S/N dx=0? CSP 0 BUFs dx>0? CSP 0 1 dx-1 dx-1 dy>0?\nCSP\n0\n1\ndy-1\ndy-1\nwest.out\ndx=0?\nCSP\n0\n1\nBUFs east.in /west.in\ndy>0?\nCSP\n0\n1\ndy-1\ndy-1\nsouth.out\nnorth.out\ndy=0?\nCSP\n1\n0\nBUFs\ncheck whehter dx=0, if not, deliver events to x direction first\ndeliver events to W/E according to sign of dx\ndeliver events to S/N according to sign of dy\ndeliver events to south/north according to sign of dy\ndeliver events to N/S if dy =! 0, otherwise deliver events to R2\nR2.in\neast.out\nsouth.out\nnorth.out\nsouth.in /north.in\nCSP\n0\n1 R2.out\ndy=0?\nnorth.out /south.out\nwest.out /east.out 1\nR2.out\nnorth\nsouth\neastwest\nR2\nR3 Router\nCSP: Controlled Split\nFig. 6: Block diagram of the R3 router. There is one R3 router per chip/tile to manage global traffic in a 2D-mesh of chips/tiles.\ngoes through a second control split process to check if it represent control words for programming bias generators, or data for programming the memory cells. Additional control split processes keep on making checks on the data, until it is delivered to its final destination, which could be a register for programming on chip bias generators (for example), or an SRAM or CAM cell memory content."}, {"heading": "IV. A multi-core neuromorphic processor prototype", "text": "We validated the architecture proposed by designing, fabricating, and testing a prototype multi-core neuromorphic processor, made adopting the design choices used in Section III-B: the chip comprises four cores; each core comprises 256 neurons, and each neuron has a fan-out of 4k. It has hierarchical asynchronous routers (R1, R2, and R3 level), and embedded asynchronous SRAM and CAM memory cells, distributed across the cores and the routers. Following the memoryoptimizing scheme of Section II, the communication circuits combine point-to-point source-address routing with multi-cast destination-address routing, and use a mixed-mode hierarchicalmesh connectivity fabric. The chip was fabricated using a standard 0.18 um 1P6M CMOS technology, and occupies an area of 43.79mm2 (see Fig. 8 for the chip micro-graph). The memory optimization theory and the hierarchical routing fabric can be used with either pure digital logic approaches [4], or mixed mode analog/digital ones [17]. Here we demonstrate a neuromorphic processor designed following a mixed signal approach: we implemented the neuron and synapse functions and state-holding properties using parallel analog circuits, rather than time-multiplexed digital ones. The analog circuits are operated in the subthreshold domain to minimize the dynamic power consumption and to implement biophysically realistic neural and synaptic behaviors, with biologically plausible temporal dynamics [17]. The core area of the chip layout (excluding pad frame) measures 38.5mm2, of which approximately 30% is used for the memory circuits, and 20% for the neuron and synapse circuits (see Table. I for a detailed breakdown of the area usage). Neurons are implemented using Adaptive-Exponential Integrate\nand Fire (AdExp-I&F) neuron circuits of the type described and fully characterized in [2]. The circuit comprises a block implementing N-Methyl-D-Aspartate (NMDA) like voltagegating, a leak block implementing neuron\u2019s leak conductance, a negative feedback spike-frequency adaptation block, a positive feedback block which models the effect of Sodium activation and inactivation channels for spike generation, and a negative feedback block that reproduces the effect of Potassium channels to reset the neuron\u2019s activation and implement a refractory period. The negative feedback mechanism of the adaptation block and the tunable reset potential of the Potassium block introduce two extra variables in the dynamic equation of the neuron that endow it with a wide variety of dynamical behaviors [27], [28]. Synapses and biophysically realistic synapse dynamic are implemented using sub-threshold Differential Pair Integrator (DPI) log-domain filters, of the type proposed in [29], and described in [17]. These circuit can produce Excitatory PostSynaptic Currents (EPSCs) and Inhibitory Post-Synaptic Currents (IPSCs) with time constants that can range from few \u00b5s to hundreds of ms, using relatively small capacitors (of the value of about 1 pF), thus keeping the circuit footprint to a minimum size.\nThe analog circuit parameters governing the behavior and dynamics of the neurons and synapses are set by programmable onchip temperature compensated bias-generators [30]. There are two independent bias-generator blocks, to provide independent sets of biases for core pairs, to allow the modeling of different neural population types. The use of mixed-mode analog/digital\n8 check whehter evnets belong to this chip events for programming BiasGen 1/2 this chip? CPASS CSP 1 0 CS P 1 0 CSP 1 0 23 23 CSP 1 0 20 SPT 28\n28\n12\n12\nevents for configuring latches in coreX\nspiking events to chips via input-interface will be delivered to R2 for routing\nevents for programming CAMs/SRAMs in coreX\n34\nSPT\ncore0.conf\ncore3.conf\nR2.out\ncore0.prg\ncore3.prg\nFPGA.in\nBiasGen1.conf\nBiasGen2.conf\nFig. 7: Input Interface block for programming on-chip SRAM and CAM memories, and for configuring additional network parameters.\ncircuits, embedded in the asynchronous routing fabric proposed allowed us to distribute the memory elements across and within the computing modules, creating an in-memory computing architecture that makes use of distributed and heterogeneous memory elements (such as capacitors, CAM, and SRAM cells) and that is truly non-von Neumann. Alternative neuromorphic processors that use pure digital design styles typically timemultiplex the computing resources and are still faced with the problem of having to transfer state memory back and forth from areas devoted to computing to areas devoted to memory storage [4], [5]."}, {"heading": "A. The core memory/computing module", "text": "The block diagram of the circuits comprising the synapse memory and synapse dynamic circuits together with the neuron circuits is shown in Fig. 9. Each of these nodes implements at the same time the memory and computing operations of the architecture: there are 64 10-bit CAM words, 64 2-bit SRAM cells, \u201cfour\u201d synapse circuits, and \u201done\u201d leaky integrate and fire neuron circuit per node. The asynchronous CAM memory\nis used to store the tag of the source address that the neuron is connected to, while the 2-bit SRAM memories are used to program the type of synapse circuits to use. Depending on the content of the SRAM a synapse can be programmed to exhibit one of 4 possible behaviors: fast excitatory, slow excitatory, subtractive inhibitory, or shunting inhibitory. Each synapse behavior is modeled by a dedicated DPI circuit, each with globally shared biases that set time constants and weight values. When a synapse accepts an address-event that has been broadcast to the core (i.e., when there is a match between the address transmitted and the one stored in the synapse CAM cell), the event triggers a local pulse-generator circuit, which in turn drives a pulse-extender circuit to produce a square wave of tunable width, ranging from fractions of \u00b5s to tens of ms. These square waves are then fed into the addressed DPI circuit, which integrates them over time, and produces an EPSC or IPSC with corresponding weight and temporal dynamics. Eventually, once all synaptic currents integrated by the neuron make it cross its spiking threshold, the neuron will produce an output digital event and send it to its handshaking block (HS). This block will communicate with neuron address encoder which will transmit the address-event to the core associated R1 router."}, {"heading": "B. Asynchronous Content-Addressable Memory circuits", "text": "Figure 10 shows a simplified diagram of the 16 \u00d7 16 asynchronous CAM cells, as they are arranged in each core. Each CAM cell comprises 64 10 bit word CAM circuits, as described on the left side of Fig. 10, which contain the addresses of 64 source neurons connected to the corresponding destination neuron\u2019s synapses. The CAM cells make use of NOR-type 9T circuits and of a pre-charge-high Match-Line scheme [31]. In the pre-charge phase, no data is presented on the search bit lines (neutral state of data) and the signal PreB is asserted to low, in order to pre-charge all the Match-Lines in the core to high. In the search phase, PreB is asserted to high and all CAMs compare their contents with the data presented on the search lines simultaneously. A miss between any input data bit and\n9 SL <0 :9 > SL B< 0: 9>\nLeaky I&F Neuron\nFE PS\nP\nSE PS\nP\nFI PS\nP\nSI PS\nP\nHS y.a y.r\nx. a x. r\nDPI\n10-bit CAMs 2-bit SRAMs\nPE PG DEC\nMatch_line<0> VB\n10-bit CAMs 2-bit SRAMs\nPE PG DEC VB\nMatch_line<1>\n10-bit CAMs 2-bit SRAMs\nPE PG DEC\nMatch_line<63> VB\nFig. 9: Block diagram of one of the 256 computing nodes in each core. Each node comprises 64 mixed memory words, consisting of 10-bit CAM and 2-bit SRAM cells, 64 pulse generators circuits (PG), 64 pulse extenders circuits (PE), 64 pulse decoders (DECs), 64\u00d74 digital pulse to analog current converters, 4 DPI filters, one adaptive I&F neuron circuit, and one handshaking block (HS).\nCAM0\nCAM1\nSL < 0>\nSL B<\n0>\nSL < 9>\nSL B<\n9>\nM_line<0>\n01\n0\n0 1\n1 WENB PreB\nWENB PreB LTH\nWeak_up\n1 01 0\nWL<0>\nWENB PreB Check\nMatch\nMiss Match\nPulse Generator\nCAM63\nVC\nCAM_D\nEvent BUF\nPreB\nCheck\nin.a\ndata\nbuffers\nFig. 10: Asynchronous Content-Addressable Memory (CAM) circuit diagram. Left: a simplified 16\u00d7 16 CAM blocks in one core each having 64 words corresponding to 64 active synapses of a neuron. Right: Circuit details of one CAM word combined with one pulse generator circuit.\nthe content of the corresponding CAM bit circuit discharges the corresponding match line. Thus, only CAM words with a match on all bits keep the corresponding Match-Line high. A Low-Threshold (LTH) inverter together with a weak pull-up P-MOS transistor is used to compensate the leak from the NOR transistors during the search phase in order to guarantee that only a real miss will pull the Match-Line low. After all CAM comparisons are complete, a pulse signal Check is transmitted across the whole core and multiplied with the Match-Lines (i.e., via a logical AND operation) to generate pulses for all CAM words that are in a match state. The matched pulses are then transmitted to the pulse generation circuit of Fig. 9, and used to produce the synaptic currents.\nThe search operation of the asynchronous CAM array follows a standard four-phase handshaking protocol, to communicate with the QDI routing blocks. However, to minimize its area, the interface to the CAM array make timing assumptions and\ndoes not implement a QDI design style. The full description of the asynchronous CAM circuit behavior, and of the timing assumptions made are detailed in Appendix VII-D"}, {"heading": "V. Experimental results", "text": "The 0.18 um CMOS process used to fabricate the chip expects a core power-supply voltage of 1.8V, however thanks to a careful design of the analog and asynchronous digital blocks, we could reduce the core supply voltage down to 1.3V without any loss of functionality. The chip specifications, including speed and latency measurements, are outlined in Table II. As evident from this table, although the asynchronous routers deliver a high throughput within the chip, the overall system performance is limited by the speed of the I/O circuits. In larger designs this would not be a major problem, as the communication among neurons would mostly happen inside the chip. The other restricting factor is the broadcast time,\n10\ni.e., the time that the CAM requires to process the incoming events. Also in this case, and similar to most memories designs that make use of asynchronous interfaces, we made worst-case scenario timing assumptions that guarantee correct functionality of the CAM circuits block (see Appendix for details). In the current design, we set the broadcast time to be 27 ns. Also this will not be a critical limitation in large scale multi-core designs, as the CAM cells of different cores operate in parallel.\nFigure 11 shows the average power dissipation measures, for different power-supply voltage settings, in a worst-case scenario condition in which all neurons of the chip are firing, at different average firing rates. The power usage of the major circuits in the neuromorphic processor is reported in Table III.\nDespite the use of a 0.18 um CMOS process, and thanks to the mixed signal analog/digital and asynchronous design strategies adopted, the chip proposed achieves power consumption and latency figures that are comparable to analogous stateof-the-art architectures fabricated with more advanced scaled technology nodes. In Table IV we provide a detailed comparison between the specifications of this neuromorphic prototype chip and of recently proposed analogous neuromorphic systems.\nThe throughput of the local event-delivery circuitry and the routing networks are important factors for the overall scalability of the proposed architecture. At the on-chip core level, the throughput is determined by the broadcast time. In the current implementation the broadcast time is of \u224827 ns (leading to a bandwidth of \u224838Mevents/sec) and the number of neurons per core i.e. 256. This results in a throughput that allows us to have 7200 fan-in per neuron in a network with the\naverage firing rate of 20Hz and 1400 fan-in at 100Hz. As these are digital circuits, the throughput in the local core is expected to improve significantly when implemented in more advanced processes. At the large-scale network level, the \u201clatency across chip\u201d figure (which measures \u224815.4 ns in the current implementation) determines how many cores can be reliably integrated in a Printed Circuit Board (PCB) board. As this number is experimentally measured, it includes the latency of input pins, R3 router and output pins. In a many-core large chip design, the latency of the I/O pins would no longer impact the R3 throughput. In our 0.18\u00b5m prototype, R3 has the latency of 2.5 ns, delivering 400Mevent/sec. According to circuit simulation studies [32], the throughput of the R3 router can reach up to 1Gevents/sec in a 28 nm process. Thanks to the hierarchical structure of the routing architecture, a high percentage of local activity of a clustered network is sorted out by the R2 and R1 routers without need to involving the R3 routers. Therefore the traffic at the top level of hierarchy significantly decreases in comparison to that of a plain 2D mesh architecture. Furthermore, being implemented using only split/merge and decrements blocks, without any lookup table or feedback mechanism, the R3 router was carefully designed to have a simple and feed-forward structure. This structure provides more opportunity to increase the throughout by increasing the number of pipeline stages in future revision of this work.\nExample application: Convolutional Neural Networks The architecture we proposed, and the prototype chip implementation used to validate it can be applied to a wide range of network types, ranging from biophysically realistic\n11\nIBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs\nTechnology 28 nm 0.13 um 0.13 um 0.18 um 0.18 um Neuron Type Digital Digital Mixed Mixed Mixed In core computation Time multiplexing Time multiplexing Parallel Parallel Parallel Fan-in/out 256/256 /1k x/1k x 64/4k Routing on-chip on-chip on/off chip on/off chip on-chip Energy per hop 2.3 pJ@0.77V 1.11 nJ x 14 nJ 17 pJ@1.3V Avg. Distance* 2 \u221a N/3 \u221a N/2 x x \u221a N/3\nTABLE IV: Features of the multi-core neuromorphic processor presented in this work, and other recent analogous devices. *Average distance between any two nodes in the network of N neurons; HiAER and Neurogrid, hierarchical and tree-based network respectively, have utilized a combination of on-chip and off-chip resources for routing, therefore it is unclear what is the average distance between the nodes.\nmodels of cortical networks, to more conventional machine learning and classical artificial neural networks. Here we use the prototype chip designed to implement CNNs. CNNs are a class of models originally inspired by the visual system, that are typically used to extract information from input images for classification. They are multi-layered networks whereby low-level features, such as edge orientations extracted by early layers, are subsequently combined to produce high-level feature detectors in later stages. To support experiments with multilayer network structures, we developed a PCB board which hosts nine chips (of the type shown in Fig. 8). The interchip communication on the board is carried out by parallel input/output ports through direct wire/pin connections. The communication is directly managed by the programmable onchip routers (R3). An additional on-board FPGA device is used for programming the on-chip memories, configuring the analog parameters and monitoring the neuron activities from all chips. The board is extendable as it has connectors on the four sides for interfacing to other instances of the same PCB. This feature extracting process makes the classification problem of the later stages much simpler. Therefore neural networks that combine several hidden layers of this form, known as deep neural networks (DNNs), can be efficiently trained to achieve very high performance on a wide range of visual and non-visual tasks [19]. Recently, pre-trained CNNs and DNNs have been successfully mapped into spike-based hardware, achieving remarkable accuracy, while minimizing power consumption [33]\u2013[36]. Furthermore, there are growing efforts towards the direct use of asynchronous event-based data produced by spiking sensors such as the DVS [37]\u2013[42]. Here we tailored the design of the CNN for efficient real-time classification of event streams using an AER data-set recently made available [38]. This data-set consists of a list of timestamped events generated by an AER dynamic vision sensor while observing a very fast sequence of Poker cards flipped from a deck. All 52 cards of the deck are flipped in front of the vision sensor in about half a second producing a total of 0.5 million events with a peak rate of slightly above 8 Meps [43]. The events obtained in this way have been further processed to center the Poker card symbol in the center of 31\u00d731 image patches. Their weights have been set to detect vertical and horizontal edges as well as upward and downward vertices. The resulting 16x16x4 maps are then pooled into a layer comprising 4x8x8 maps. The activity of the silicon neurons in the pooling\nlayer is then used as input to four populations of 64 neurons in the output layer. The all-to-all connections between pooling layer and output layer are tuned using an off-line Hebbian-like learning rule such that, for each input symbol, the 64 most active pooling layer neurons are strongly connected with the corresponding output neurons subgroup. We used 4 populations of 64 neurons, rather than just 4 neurons in the output layer, to stabilize the performance, by using a majority rule. The class corresponding to the right Poker card suit is thus determined by the most active population, during the presentation of the input pattern (see Fig. 12). As evident from Fig. 12, the asynchronous event-based processing nature of this architecture allowed the system to produce output results with extreme low-latency figures: the classification result is available within less than 30 ms from the onset of the input stimulus. This figure was obtained without optimizing the network parameters for speed . The time constants in the analog circuits can be tuned to be much faster than the biologically plausible ones used in this demonstration (i.e., in the order of tens of milliseconds). The simplicity of the chosen problem was such that even Parameter Size Input image 32\u00d732 Conv. kernels 4\u00d78\u00d78, Stride=2 Conv. layer output 4\u00d716\u00d716 Subsampling kernels 2\u00d72 Subsampling output 4\u00d78\u00d78 Fully-connected layer 4\u00d764\nTABLE V: An example CNN for Poker card suit recognition experiment.\na simple architecture such as the one described here, that used merely 2560 neurons was sufficient to achieve a 100% performance on the test data set. Multi-chip boards, such as the 9-chip board used for this experiment, can support much larger and sophisticated networks (of up to 9k neurons). Indeed, it has been recently shown [33], [35] that if these networks are designed appropriately, they can tolerate the limited precision that affects the analog circuits in the chip, while preserving high accuracy figures that are comparable to those obtained by state-of-the-art Deep Neural Networks (DNNs) running on hardware consuming orders of magnitude more power. One important and novel aspect of the hardware and\nframework proposed in this work is the flexibility offered by the memory optimized programmable routing scheme, that\n12\nallows an efficient allocation of memory, synapse, and neural resources for a wide range of architectures, including CNNs. This is evident if one analyzes the amount of resources required by comparable architectures, such as the one proposed in [4], for different types of CNNs architectures, as recently reported in [36]. We analyzed the memory size for CNN models implemented on TrueNorth. The type of neuron and synapse models implemented in the two hardware platforms are very similar, therefore we\u2019d expect similar classification performance if those networks were to be implemented on the proposed platform. Here we consider the scaling that we would obtain if we were to implement those CNN networks on our hardware platform. For TrueNorth, we observed a roughly quadratic relation between the number of hardware cores used and the number of neurons required by the network, despite the filter size being fixed by design in agreement with the hardware constraints (Fig. 13). This scaling comes from the fact that in TrueNorth additional cores are allocated for implementing larger fan-out as required by the models. Instead, in our design it was possible to include enough fan-in and fan-out memory requirements per core as established by those models and therefore no additional \"routing core\" would be allocated. Hence, with the proposed architecture the scaling is only linear with the CNN size and so the advantage for larger networks, as required by increasingly complex tasks, is obvious since no additional \"routing\" cores are needed."}, {"heading": "VI. Discussion", "text": "One of the most appealing features of neuromorphic processors is their ability to implement massively parallel computing architectures with memory and computation co-localized within their computational nodes. In addition to enabling ultra lowpower data-driven processing, this feature allows for the construction of scalable systems for implementing very large\n13\nscale neural networks, without running into the von Neumann bottleneck problem [14], [15], and in other data communication bottleneck problems typical of centralized systems. However, distributed memory/computing systems come at the price of non-trivial routing overhead. Neuromorphic solutions that have optimized this overhead and, with it, the communication bandwidth, have sacrificed flexibility and programmability [3]. On the other hand, solutions that have maximized network configurability, have sacrificed silicon real-estate for on-chip memory [4], or resorted to using external memory banks [5] (and therefore eliminated the advantage of having memory and computation co-localized). In this work we presented a scalability study for neuromorphic systems that allows network configuration programmability and optimization of area and power consumption at the same time. This is especially important for the configuration of different classes of sensoryprocessing neural networks that are not restricted to classical CNNs and DNNs architectures, and that can be used to process real-time streaming data on the fly, without having to resort to saving input data in external memory structures (be it DRAM or other forms of solid-state storage. While there have been ad-hoc solutions proposed in the literature for utilizing off-chip memory more efficiently in neuromorphic systems [44], there is no consensus around a systematic approach that would explicitly allow to trade-off flexibility and memory to meet the requirements of specific applications. The approach we proposed allows to choose an optimal trade-off point between network configuration flexibility and routing memory demands. The trade-offs and design points we chose for the prototype device built to validate our approach were determined by the study of cortical networks in mammalian brains [45]\u2013[47]. In particular, our solution profits from the use of strategies observed in anatomically realistic topologies found in biology, namely to express dense clusters of connectivity that are interconnected by few long range connections. These choices resulted in a novel architecture that incorporates distributed and heterogeneous memory structures, with a flexible routing scheme that can minimize the amount of memory required for a given network topology, and that supports the most common feed-forward and recurrent neural networks. The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49]."}, {"heading": "VII. Conclusions", "text": "In this work we presented a two-stage routing scheme which minimizes the memory requirements needed to implement scalable and reconfigurable spiking neural networks with bounded connectivity. We used this scheme to solve analytically the trade-off between \u201cpoint-to-point\u201d connectivity, which increases memory budget in favor of connection specificity, and \u201cbroadcasting\u201d, which reduces memory budget by distributing the same signal across populations of neurons. We presented QDI circuits and building blocks for implementing this routing scheme in asynchronous VLSI technology, and presented a prototype neuromorphic processor that integrates such building\nblocks together with mixed signal analog/digital neuron and synapse circuits. We showed that even with the conservative 0.18 um VLSI technology used, the power consumption of the prototype neuromorphic processor chip fabricated is comparable to state-of-art digital solutions. To demonstrate the features of such architecture and validate the circuits presented, we tiled multiple neuromorphic processors together and configured the setup to implement a three layer CNN. We applied the network to an event-based data-set and showed how the system can produce reliable accurate results, using extremely low power and latency figures. As scaling studies have demonstrated that such architecture can outperform the current state-of-theart systems when implemented using a 28 nm Fully-Depleted Silicon on Insulator (FDSOI) process [32], we are confident that such approach can lead to the design of a new generation of neuromorphic processors for solving a wide range of practical applications that require real-time processing of eventbased sensory signals, using ultra low-power, low latency, and compact systems."}, {"heading": "Acknowledgments", "text": "We are grateful to Rajit Manohar, at Yale University, for providing us the tutorials and tips on the design of asynchronous circuits, as well as CAD tools for the synthesis and verification of the asynchronous circuits. We thank our colleagues at the Institute of Neuroinformatics in Zurich for stimulating discussions. This work was supported by the EU ERC Grant \u201cneuroP\u201d (257219)."}, {"heading": "A. Routing memory minimization constraints", "text": "Here we provide an intuitive explanation of why the twostage routing scheme presented in Section II minimizes memory usage. First, consider a simple case in which two subgroups of K neurons project only within their groups. In this case the two groups correspond in fact to two separate networks and there is no need to store more than K words (which we shall call tags herein) to identify the neurons and thus to implement the connectivity for this network. Obviously having a larger network with more groups that share the same property of local connectivity doesn\u2019t affect K, hence K is constant with the number of neurons in the network. It is clear that adding one connection from one group to an other may potentially cause an address collision, i.e., two neurons with the same tag project onto the same population, and therefore more than K\n15\ntags are required in this case in order to implement different connectivity patterns for those neurons. If few connections exist between different clusters, i.e., in the case of sparse longrange connectivity, the total number of tags still scales slowly with the number of neurons because the number of address collisions is low, furthermore it can be kept under control by tag re-assignment. Although the particular number of K tags for a network depends on the actual specific connectivity, its average value at least for a given class of networks scales nicely with network size for the above reasons. An analogous situation appears if the neurons in one group never project back to the same group, as for example in a feed-forward network. These are deliberately oversimplified cases but in many scenarios neural networks do show clustering and grouping properties. It is clear that in such situations a K-tag based scheme for implementing the connectivity can be harnessed to gain in memory efficiency over a traditional scheme that requires to store N different identities and their connectivity. Next,we show a more systematic derivation of the theoretical constraints for optimizing digital memory in neuromorphic hardware implementations of clustered neural networks.\nThe optimal design point must satisfy the following requirements:\n1) F \u2265M\u2217 2) C \u2265M\u2217 If the first requirement is not met and M\u2217 > F :\u221a F\n\u03b1 log2(\u03b1N) log2(\u03b1C) > F\n\u03b1=1\u21d2 N1/F > C\nTherefore, M\u2217 is a valid design point if the cluster size meet this condition: C \u2265 N1/F . This is a very safe constraint: for example even when the total neuron count is in the 1010 range, a fan-out as small as 10 would require a cluster size of C \u2265 10 to be able to have an optimal choice of M\u2217. Since typical fan-out values are actually in the 103\u2013104 range, this requirement imposes very few constraints on the cluster size. The total number of neurons N would have to be larger than 10103 before the right hand side of the constraint would be 10 or larger. The second requirement indicates that the cluster size must be greater than a minimum size (C \u2265 M\u2217) to support the anticipated fan-out.\nC \u2265\n\u221a F\n\u03b1 log2(\u03b1N) log2(\u03b1C)\nwhich leads to:\u221a F log2 \u03b1N \u2264 \u221a \u03b1C \u221a\nlog2 \u03b1C C \u221a log2(C) \u2265 \u221a F log2(N) for \u03b1 = 1\nThis constraint is much more restrictive than the first one. For example, if we take typical values of F = 5000, and N = 1010, then clusters need to be C \u2265 152. Conversely, if we pick a cluster size C = 256 with \u03b1 = 1 (i.e., with 256 tags), then the optimal value of M would be\nM\u2217 = 144. In this case, the network requires a first-level fan-out of 35, followed by a second cluster-level fan-out of 144 for a fan-out of 5040 an the storage per neuron would be 424.26 \u221a log2 N bits."}, {"heading": "B. Communicating Hardware Process (CHP)", "text": "Here is a list of the most common CHP commands that cover the design of all the blocks presented in this paper: \u2022 Send: X!e means send the value of e over channel X. \u2022 Receive: Y ?v means receive a value over channel Y and\nstore it in variable v. \u2022 Probe: The boolean expression X is true if a communica-\ntion over channel X can complete without suspending. \u2022 Sequential Composition: S; T \u2022 Parallel Composition: S \u2019 T or S, T \u2022 Assignment: a := b. This statement means \u201cassign the value of b to a.\u201d We also write a\u2191 for a := true, and a\u2193 for a := false. \u2022 Selection: [G1 \u2192 S1 [] ... [] Gn \u2192 Sn], where Gi\u2019s are boolean expressions (guards) and Si\u2019s are program parts. The execution of this command corresponds to waiting until one of the guards is true, and then executing one of the statements with a true guard. The notation [G] is shorthand for [G \u2192 skip], and denotes waiting for the predicate G to become true. If the guards are not mutually exclusive, we use the vertical bar \u201c|\u201d instead of \u201c[].\u201d \u2022 Repetition: *[G1 \u2192 S1 [] ... [] Gn \u2192 Sn]. The execution of this command corresponds to choosing one of the true guards and executing the corresponding statement, repeating this until all guards evaluate to false. The notation *[S] is short-hand for *[true \u2192 S].\nC. CHP, HSE, and PR examples or asynchronous circuits used in the routing scheme\nchp { \u2217 [ [ in1\u2192 i n1 ? s ; ou t ! s\n| in2\u2192 i n2 ? s ; ou t ! s ] ]\n}\nhse { \u2217 [ [ i n 1 a r b _ o u t \u2192 x +; ou t . b [ i ] . d [ j ] := i n1 . b [ i ] . d [ j ] ; i n1 . a + ; en1\u2212; ( [ ou t . a ] ; x\u2212; ou t . b [ i ] . d [ j ]\u2212; [\u223cou t . a ] ) , (\u223ci n 1 a r b _ o u t ] ; i n1 . a\u2212) ; en1+\n[ ] i n 2 a r b _ o u t \u2192 x +; ou t . b [ i ] . d [ j ] := i n2 . b [ i ] . d [ j ] ; i n2 . a + ; en2\u2212; ( [ ou t . a ] ; x\u2212; ou t . b [ i ] . d [ j ]\u2212; [\u223cou t . a ] ) , (\u223ci n 2 a r b _ o u t ] ; i n2 . a\u2212) ; en2+ ] ]\n| |\n\u2217 [ i n1 . v & en1 \u2192 i n 1 a r b _ i n + ; [\u223ci n1 . v & \u223cen1 ] ; i n1a rb \u2212]\n| |\n16\n\u2217 [ i n2 . v & en2 \u2192 i n 2 a r b _ i n + ; [\u223ci n2 . v & \u223cen2 ] ; i n2a rb \u2212]\n| |\n\u2217 [ [ i n 1 a r b _ i n \u2192 i n 1 a r b _ o u t + ; [\u223ci n 1 a r b _ i n ] ; i n 1 a r b _ o u t \u2212 | i n 2 a r b _ i n \u2192 i n 2 a r b _ o u t + ; [\u223ci n 2 a r b _ i n ] ; i n 2 a r b_ou t\u2212\n] ] }\nListing 4: merge\nchp { \u2217 [ [ v ( i n ) ] ; [ c t r l 0 \u2192 ou t0\u21d1\n[ ] c t r l 1 \u2192 ou t11\u21d1 ] ; [ n ( i n ) \u2227 n ( c t r l ) ] ; ou t0\u21d3 , ou t1\u21d3 ]\n}\nListing 5: controlled-split\nchp { \u2217 [ IN?x ; OUT! x ]\n}\nListing 6: buffer"}, {"heading": "D. Asynchronous Content-Addressable Memory (CAM) timing assumptions", "text": "In order to guarantee the correct handshaking communication between the local router (R1) and the CAM array, it is necessary to make appropriate timing assumptions. Here we describe the assumptions that were made in the design of the asynchronous CAM array. Initially, without the presence of input data, the output of Event Buffer (EB) is in neutral state and all the search lines are low: SL \u30089 : 0\u3009 = 0 and SLB \u30089 : 0\u3009 = 0. Therefore the Validity Check (VC) block, placed at the top-right-hand side of the array in the layout (see also Fig. 10, sets PreB to 0 and consequently, all the MLs are pre-charged to high. Upon receiving the events, the buffers start broadcasting and driving the data lines in entire array. The V C, the last element in the array to receive the valid data, eventually asserts PreB to high. This signal is in turn used to enable the comparison between the input (presented on data lines) and the CAM words content by sending the enable signal across the whole array from top-left to bottom-right. The duplicate CAM cell CAM_D, placed at the opposite corner to the V C block (at the bottom-right-hand side of the CAM array), is assumed to be the last one to get the enable signal PreB. This duplicate cell is designed to produce a miss signal with the worst case (that is when only one bit is a miss in its 10-bit CAM word) for any data presented on search lines. Once its ML M_line_D is discharged, which guarantees that the comparison between input data lines and all CAMs words have been finished. At this point Check signal is being driven to high and is sent across the array. This signal then lets the CAM words with active MLs produce a match(hit) signal. The Check signal eventually reaches the EB block as the acknowledgment from the array. The EB circuits then de-asserts data and sets data\nlines to their neutral state, which lowers the PreB. Again, the PreB signal is sent from top-left to bottom-right to reset all the Match lines to low as well as to reset all MLs. As soon as the ML of the dummy CAM_D, word is lowered, the Check signal is de-asserted and the handshaking is completed at this point. Although the local communication is not optimized for speed, it is sufficient to cover all the neurons in one core. The way we implemented the timing assumption guarantees small mismatch of pulse width of Match signals generated by different CAM words in the array. The pulse width is a critical parameter for the analog neural computations. It is ensures that the mismatch between width of Match pulses generated in different synapses across the core is as small as possible. Assuming PreB and Check signals drive similar size load capacitances, The same buffer is used for sending these signals from top left to bottom right-hand side of the array. For a particular CAM word, CAMx, td1x, the delay between transmitting the \u201cCheck\u201d signal and Match<x>, and td2x, the delay between PreB and Match<x> are about the same(see Fig. 14). The pulse width of a particular CAMx is tdxtux = (tpre + td2x)\u2212 (tck + td1x). And for td1x and td2x, the pulse width can be approximated to tpre\u2212 tck which is not related to the physical position of the CAM. In designing the asynchronous CAM array we made worstcase scenario timing assumptions to ensure the correct operation of the circuits. Lets consider the case in which the dual-rail data from the output of Event Buffer (EB) of Fig. 10 is neutral with all search lines: SL<9:0>=0 and SLB<9:0>=0. The Validity Check (VC) block at the top-right-hand side of the array in Fig. 10 checks the data state and assigns PreB to 0 for the neutral state. All MLs are consequently pre-charged to high.\nOnce EB gets events from the previous stage, i.e., from the core router R1, it pushes the new data to dual-rail data lines and broadcasts it to the whole array through broadcasting buffers. After the searching lines have successfully set-up the new data, VC is assumed to be the last one in this array to get the valid data. This is in turn used to enable the CAMs comparisons by transmitting the enable signal to the whole core from top-left to bottom-right. The duplicate CAM cell CAM_D, placed at the bottom-right-hand side of the CAM array, is assumed to be the last one to get the enable signal PreB for comparing. This duplicate cell is designed to always get a miss with the worst case (only one bit is a miss in its 10-bit CAM word for discharging the ML) for any data presented on search lines. Once its ML M_line_D is discharged, which guarantees that all CAMs in this array have finished comparing, an inverted signal Check is asserted to high and sent to left-top of the array. Then it is buffered to the whole array from left-top to right-bottom. All MLs in this array will apply an AND operation with Check, PreB and ML, and CAM words with match state will get a high Match. The Check signal will finally arrive at EB as the acknowledge signal of its valid output data. The EB will then de-assert the data and set the data lines to neutral again, which will cause the PreB from VC to be set to 0. Again, the high to low transition of PreB is transmitted from top-left to bottom-right of the layout, to reset all high Match to low as well as to reset all MLs. As soon as this happens the ML of the dummy cell CAM_D (which has been positioned physically\n17\nas the last in the array), is reset again and the signal Check is set to low, to finish the handshaking.Assuming that PreB and Check have similar load capacitances in the whole array and that the same buffer is used for transmitting from top left to bottom right-hand side of the array, then for a particular CAM word CAMx in a match state, the delay td1x between transmits of Check and Match<x> and the delay td2x between PreB and Match<x> will be similar.\nAnother critical circuit is the one responsible for extending input pulses. Since that pulse width is also one critical parameter of analog computing, the pulse width circuit has to minimize the effct of mismatch, such that pulses generated in different synapses across the core should be as similar as possible. Figure 14 shows the sequence of operations from the events received at the input of each to the signals routed to the synapses in the core. At the very beginning, dual-rail data from output of Event Buffer (EB) is \"neutral\" with all search lines, SL \u30089 : 0\u3009 = 0 and SLB \u30089 : 0\u3009 = 0. The Validity Check (VC) block at the right-top of the array will check the data state and assign PreB to \"0\" for the \"neutral\" state. All MLs are consequently pre-charged to \"high\". Once EB gets events from the previous stage, i.e. the local router R1, it will push the new data to dual-rail data lines and broadcast it to the whole array through broadcasting buffers. After searching lines have successfully set-up new data, VC is assumed to be the last one in this array to get the valid data, check dual-rail data presented on search lines and assert PreB to \"1\" for the \"valid\" state. This is in turn used to enable the CAMs comparisons by transmitting the \u201cenable\u201d signal the whole core from left-top to right-bottom. The duplicate CAM cell CAM_D placed at the right-bottom of the CAM array is assumed to be the last one to get the enable signal PreB for comparing. This duplicate cell is designed to always get \"miss\" with the worse case (only one bit is miss in its 10-bit CAM word for discharging the ML) for any data presenting on search lines. Once its ML M_line_D is discharged which can guarantee that all CAMs in this array have finished comparing, a inverted signal Check is asserted to \"high\" and sent to left-top of the array and then buffered to the whole array from left-top to right-bottom. All MLs in this array will do AND operation with Check, PreB and ML, CAM words with \"match\" state will get \"high\" Match. The Check signal will finally arrive at EB as the acknowledge signal of its valid output data. The EB will then dessert data and set data lines to \"neutral\" again, which will cause the PreB from VC to be \"0\". Again, the \"jumping from high to low\" of PreB is still transmitted from left-top to right-bottom to reset all \"high\" Match to \"low\" as well as reset all MLs. As soon as the ML of duplicate cell CAM_D which is assumed to be the last one in the array has been reset again, Check will go low and set in.a to low to finish the handshaking.\nThe local communication is not optimized for speed, however it minimizes the mismatch of the pulse width generated by different CAM words with match states distributed in the whole array. Due to the small number of neurons in each core, the speed of operations is already sufficient with this scheme. Assuming PreB and Check have similar load capacitances in the whole array and that the same buffer is used for transmitting from top-left to right-bottom of the array, then for a particular\nCAM word CAMx in a match state, the delay td1x between transmits of Check and Match < x > and the delay td2x between PreB and Match < x > will be similar (see Fig 14). The pulse weight of a particular CAMx is tdx\u2212 tux = (tpre + td2x)\u2212(tck+td1x). For similar td1x and td2x, the pulse weight can be approximated to tpre \u2212 tck which is no more related to the physical position of the CAM."}], "references": [{"title": "An event-based neural network architecture with an asynchronous programmable synaptic memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "Biomedical Circuits and Systems, IEEE Transactions on, vol. 8, no. 1, pp. 98\u2013107, February 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A re-configurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses", "author": ["N. Qiao", "H. Mostafa", "F. Corradi", "M. Osswald", "F. Stefanini", "D. Sumislawska", "G. Indiveri"], "venue": "Frontiers in Neuroscience, vol. 9, no. 141, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations", "author": ["B.V. Benjamin", "P. Gao", "E. McQuinn", "S. Choudhary", "A.R. Chandrasekaran", "J. Bussat", "R. Alvarez-Icaza", "J. Arthur", "P. Merolla", "K. Boahen"], "venue": "Proceedings of the IEEE, vol. 102, no. 5, pp. 699\u2013716, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "I. Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickner", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science, vol. 345, no. 6197, pp. 668\u2013673, Aug 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The SpiNNaker project", "author": ["S. Furber", "F. Galluppi", "S. Temple", "L. Plana"], "venue": "Proceedings of the IEEE, vol. 102, no. 5, pp. 652\u2013665, May 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A 32GBit/s communication SoC for a waferscale neuromorphic system", "author": ["S. Scholze", "H. Eisenreich", "S. H\u00f6ppner", "G. Ellguth", "S. Henker", "M. Ander", "S. H\u00e4nzsche", "J. Partzsch", "C. Mayr", "R. Sch\u00fcffny"], "venue": "INTEGRATION, the VLSI journal, vol. 45, no. 1, pp. 61\u201375, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Computing with networks of spiking neurons on a biophysically motivated floating-gate based neuromorphic integrated circuit.", "author": ["S. Brink", "S. Nease", "P. Hasler"], "venue": "Neural networks: the official journal of the International Neural Network Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A VLSI network of spiking neurons with an asynchronous static random access memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "Biomedical Circuits and Systems Conference (BioCAS), 2011. IEEE, November 2011, pp. 277\u2013280.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Address-event asynchronous local broadcast protocol", "author": ["S. Deiss", "T. Delbruck", "R. Douglas", "M. Fischer", "M. Mahowald", "T. Matthews", "A. Whatley"], "venue": "World Wide Web page, 1994, http://www.ini.uzh.ch/ \u0303amw/scx/aeprotocol.html.  14", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Point-to-point connectivity between neuromorphic chips using address-events", "author": ["K. Boahen"], "venue": "IEEE Transactions on Circuits and Systems II, vol. 47, no. 5, pp. 416\u201334, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems", "author": ["P. Dayan", "L. Abbott"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Multicasting mesh aer: a scalable assembly approach for reconfigurable neuromorphic structured aer systems. application to convnets", "author": ["C. Zamarre\u00f1o-Ramos", "A. Linares-Barranco", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Biomedical Circuits and Systems, IEEE Transactions on, vol. 7, no. 1, pp. 82\u2013102, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical address event routing for reconfigurable large-scale neuromorphic systems", "author": ["J. Park", "T. Yu", "S. Joshi", "C. Maier", "G. Cauwenberghs"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201315, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Can programming be liberated from the von neumann style?: a functional style and its algebra of programs", "author": ["J. Backus"], "venue": "Communications of the ACM, vol. 21, no. 8, pp. 613\u2013641, 1978.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "Memory and information processing in neuromorphic systems", "author": ["G. Indiveri", "S.-C. Liu"], "venue": "Proceedings of the IEEE, vol. 103, no. 8, pp. 1379\u20131397, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Resistive random access memory (ReRAM) based on metal oxides", "author": ["H. Akinaga", "H. Shima"], "venue": "Proceedings of the IEEE, vol. 98, no. 12, pp. 2237\u20132251, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Neuromorphic electronic circuits for building autonomous cognitive systems", "author": ["E. Chicca", "F. Stefanini", "C. Bartolozzi", "G. Indiveri"], "venue": "Proceedings of the IEEE, vol. 102, no. 9, pp. 1367\u20131388, 9 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Formation and maintenance of neuronal assemblies through synaptic plasticity", "author": ["A. Litwin-Kumar", "B. Doiron"], "venue": "Nature communications, vol. 5, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural circuits of the neocortex", "author": ["R. Douglas", "K. Martin"], "venue": "Annual Review of Neuroscience, vol. 27, pp. 419\u201351, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Embedding of cortical representations by the superficial patch system", "author": ["D. Muir", "N. Da Costa", "C. Girardin", "S. Naaman", "D. Omer", "E. Ruesch", "A. Grinvald", "R. Douglas"], "venue": "Cerebral Cortex, vol. 21, no. 10, pp. 2244\u20132260, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A memory-efficient routing method for large-scale spiking neural networks", "author": ["S. Moradi", "N. Imam", "R. Manohar", "G. Indiveri"], "venue": "Circuit Theory and Design, (ECCTD), 2013 European Conference on. IEEE, 2013, pp. 1\u20134.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "The limitations to delay-insensitivity in asynchronous circuits", "author": ["A.J. Martin"], "venue": "Proceedings of the Sixth MIT Conference on Advanced Research in VLSI, ser. AUSCRYPT \u201990. Cambridge, MA, USA: MIT Press, 1990, pp. 263\u2013278.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "Communicating Sequential Processes", "author": ["C.A.R. Hoare"], "venue": "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1985}, {"title": "Asynchronous techniques for system-on-chip design", "author": ["A. Martin", "M. Nystrom"], "venue": "Proceedings of the IEEE, vol. 94, pp. 1089\u20131120, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Reconfigurable asynchronous logic", "author": ["R. Manohar"], "venue": "Custom Integrated Circuits Conference. IEEE, 2006, pp. 13\u201320.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamical systems in neuroscience: The geometry of excitability and bursting", "author": ["E. Izhikevich"], "venue": "The MIT press,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Firing patterns in the adaptive exponential integrate-and-fire model", "author": ["R. Naud", "N. Marcille", "C. Clopath", "W. Gerstner"], "venue": "Biological Cybernetics, vol. 99, no. 4\u20135, pp. 335\u2013347, November 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Synaptic dynamics in analog VLSI", "author": ["C. Bartolozzi", "G. Indiveri"], "venue": "Neural Computation, vol. 19, no. 10, pp. 2581\u20132603, Oct 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "32-bit configurable bias current generator with sub-off-current capability", "author": ["T. Delbruck", "R. Berner", "P. Lichtsteiner", "C. Dualibe"], "venue": "International Symposium on Circuits and Systems, (ISCAS), 2010, IEEE. Paris, France: IEEE, 2010, pp. 1647\u20131650.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Content-addressable memory (CAM) circuits and architectures: A tutorial and survey", "author": ["K. Pagiamtzis", "A. Sheikholeslami"], "venue": "IEEE Journal of Solid-State Circuits, vol. 41, no. 3, pp. 712\u2013727, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Scaling mixed-signal neuromorphic processors to 28nm fd-soi technologies", "author": ["N. Qiao", "G. Indiveri"], "venue": "Biomedical Circuits and Systems Conference, (BioCAS), 2016. IEEE, 2016, pp. 552\u2013555.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing", "author": ["P.U. Diehl", "D. Neil", "J. Binas", "M. Cook", "S.-C. Liu", "M. Pfeiffer"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Spiking deep convolutional neural networks for energy-efficient object recognition", "author": ["Y. Cao", "Y. Chen", "D. Khosla"], "venue": "International Journal of Computer Vision, vol. 113, no. 1, pp. 54\u201366, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Energy-efficient neuromorphic classifiers", "author": ["D. Marti", "M. Rigotti", "M. Seok", "S. Fusi"], "venue": "Neural Computation, vol. 28, pp. 2011\u20132044, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. McKinstry", "T. Melano", "D.R. Barch", "C. di Nolfo", "P. Datta", "A. Amir", "B. Taba", "M.D. Flickner", "D.S. Modha"], "venue": "Proceedings of the National Academy of Science, vol. 113, no. 41, pp. 11 441\u20131446, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Mapping from framedriven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing\u2013application to feedforward convnets", "author": ["J.A. P\u00e9rez-Carrasco", "B. Zhao", "C. Serrano", "B. Acha", "T. Serrano- Gotarredona", "S. Chen", "B. Linares-Barranco"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2706\u20132719, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Poker-dvs and mnist-dvs. their history, how they were made, and other details.", "author": ["T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Spike event based learning in neural networks", "author": ["J.A. Henderson", "T.A. Gibson", "J. Wiles"], "venue": "arXiv preprint arXiv:1502.05777, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Feedforward categorization on aer motion events using cortex-like features in a spiking neural network", "author": ["B. Zhao", "R. Ding", "S. Chen", "B. Linares-Barranco", "H. Tang"], "venue": "IEEE transactions on neural networks and learning systems, vol. 26, no. 9, pp. 1963\u20131978, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1963}, {"title": "Hfirst: a temporal approach to object recognition", "author": ["G. Orchard", "C. Meyer", "R. Etienne-Cummings", "C. Posch", "N. Thakor", "R. Benosman"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 37, no. 10, pp. 2028\u20132040, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Hots: A hierarchy of event-based time-surfaces for pattern recognition", "author": ["X. Lagorce", "G. Orchard", "F. Gallupi", "B.E. Shi", "R. Benosman"], "venue": "2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "An eventdriven multi-kernel convolution processor module for event-driven vision sensors", "author": ["L. Camunas-Mesa", "C. Zamarreno-Ramos", "A. Linares-Barranco", "A. Acosta- Jimenez", "T. Serrano-Gotarredona", "B. Linares-Barranco"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 47, no. 2, pp. 504\u2013 517, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A forecast-based STDP rule suitable for neuromorphic implementation", "author": ["S. Davies", "F. Galluppi", "A.D. Rast", "S.B. Furber"], "venue": "Neural Networks, vol. 32, pp. 3\u201314, 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "A quantitative map of the circuit of cat primary visual cortex", "author": ["T. Binzegger", "R. Douglas", "K. Martin"], "venue": "Journal of Neuroscience, vol. 24, no. 39, pp. 8441\u201353, 2004.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Recurrent neuronal circuits in the neocortex", "author": ["R. Douglas", "K. Martin"], "venue": "Current Biology, vol. 17, no. 13, pp. R496\u2013R500, 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "The importance of being hierarchical", "author": ["N. Markov", "H. Kennedy"], "venue": "Current opinion in neurobiology, vol. 23, no. 2, pp. 187\u2013194, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Specifications of nanoscale devices and circuits for neuromorphic computational systems", "author": ["B. Rajendran", "Y. Liu", "J. sun Seo", "K. Gopalakrishnan", "L. Chang", "D. Friedman", "M. Ritter"], "venue": "Electron Devices, IEEE Transactions on, vol. 60, no. 1, pp. 246\u2013253, Jan 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Resistive Switching: From Fundamentals of Nanoionic Redox Processes to Memristive Device Applications", "author": ["G. Indiveri", "E. Linn", "S. Ambrogio"], "venue": "Weinheim, Germany: Wiley-VCH Verlag GmbH & Co. KGaA,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In an effort to develop a new generation of brain-inspired non von Neumann computing systems, several neuromorphic computing platforms have been proposed in recent years, that implement spike-based re-configurable neural networks [1]\u2013 [8].", "startOffset": 230, "endOffset": 233}, {"referenceID": 7, "context": "In an effort to develop a new generation of brain-inspired non von Neumann computing systems, several neuromorphic computing platforms have been proposed in recent years, that implement spike-based re-configurable neural networks [1]\u2013 [8].", "startOffset": 235, "endOffset": 238}, {"referenceID": 8, "context": "Despite being developed with different goals in mind and following different design strategies, most of these architectures share the same data representation and signal communication protocol: the Address-Event Representation (AER) [9], [10].", "startOffset": 233, "endOffset": 236}, {"referenceID": 9, "context": "Despite being developed with different goals in mind and following different design strategies, most of these architectures share the same data representation and signal communication protocol: the Address-Event Representation (AER) [9], [10].", "startOffset": 238, "endOffset": 242}, {"referenceID": 10, "context": "The type of processing and functionality of these spiking neural networks is determined by their specific structure and parameters, such as the properties of the neurons or the weights of the synapses [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 2, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 207, "endOffset": 210}, {"referenceID": 11, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 212, "endOffset": 216}, {"referenceID": 3, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 356, "endOffset": 359}, {"referenceID": 4, "context": "Most large-scale neuromorphic computing approaches followed up to now have either restricted the space of possible network connectivity schemes to optimize bandwidth usage while minimizing power and latency [3], [12], or have designed systems that use large amounts of memory, silicon real-estate, and/or power, to maximize flexibility and programmability [4], [5].", "startOffset": 361, "endOffset": 364}, {"referenceID": 12, "context": "In particular, most approaches proposed either make use of 2D mesh routing schemes, with maximum flexibility, but at the cost of large resource usage, or tree routing schemes which minimize latency and power, but are more restrictive in the types of networks that can be supported (see [13] for a comprehensive overview comparing most of the approaches that have been proposed in the literature).", "startOffset": 286, "endOffset": 290}, {"referenceID": 12, "context": "In [13] the authors proposed a hierarchical address event routing scheme (HiAER) that overcomes some of the limitations of previous flat treebased approaches.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "However, as with many of the previous approaches [5], the HiAER architecture stores the routing tables in external memory banks implemented using Dynamic Random Access Memory (DRAM).", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "von Neumann bottleneck problem [14], [15].", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "von Neumann bottleneck problem [14], [15].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "The approach we propose combines the advantages of all previous approaches proposed up to now by combining 2D-mesh with hierarchical routing, implementing a 2-stage routing scheme for minimizing memory usage, which employs a combination of point-to-point sourceaddress routing and multi-cast destination-address routing, and by using heterogeneous memory structures distributed within and across the neuron and synapse arrays which are optimally suited to exploit emerging memory technologies such as Resistive Random Access Memory (RRAM) [16].", "startOffset": 539, "endOffset": 543}, {"referenceID": 16, "context": "The routing fabric implemented in this chip uses new designs of quasi-delay insensitive asynchronous circuits, synthesized following the Communicating Hardware Process (CHP) formalism, while the neural and synaptic dynamics are designed using ultralow power subthreshold neuromorphic circuits [17].", "startOffset": 293, "endOffset": 297}, {"referenceID": 17, "context": ", for most of the real and artificial neural networks [18], [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": ", for most of the real and artificial neural networks [18], [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Inspired by the connectivity patterns in biological neural networks [20], [21] we developed a novel routing scheme that exploits their clustered connectivity structure to reduce memory requirements in large-scale neural networks.", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "Inspired by the connectivity patterns in biological neural networks [20], [21] we developed a novel routing scheme that exploits their clustered connectivity structure to reduce memory requirements in large-scale neural networks.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": ", the fan-out operation) is divided into two distinct stages [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "In addition to distributing the memory resources among multiple nodes (neurons) within the computing fabric, therefore eliminating the von Neumann bottleneck [14], [15], this two-stage routing scheme allows us to minimize the total digital memory for configuring the", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "In addition to distributing the memory resources among multiple nodes (neurons) within the computing fabric, therefore eliminating the von Neumann bottleneck [14], [15], this two-stage routing scheme allows us to minimize the total digital memory for configuring the", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "To optimize the event routing within the network, we adopted a mixed-mode approach that combines the advantages of mesh routing schemes (low bandwidth requirements, but high latency), with those of hierarchical routing ones (low latency, but high bandwidth requirements) [3].", "startOffset": 271, "endOffset": 274}, {"referenceID": 22, "context": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "The asynchronous circuits that implement the R1, R2, and R3 routers, and the overall routing architecture, were synthesized using the QDI approach [23], by following the Communicating Hardware Processes (CHP) [24] and Handshaking Expansion (HSE) formalism [25] (see also Appendix VII-C for the most", "startOffset": 256, "endOffset": 260}, {"referenceID": 25, "context": "The CHP language provides a set of formal constructs for synthesizing a large class of programs [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "Finally, HSE programs are transformed into a set of \u201cproduction rules\u201d which are abstract descriptions of digital Complementary Metal-Oxide-Semiconductor (CMOS) Very Large Scale Integration (VLSI) circuits [25].", "startOffset": 206, "endOffset": 210}, {"referenceID": 3, "context": "The memory optimization theory and the hierarchical routing fabric can be used with either pure digital logic approaches [4], or mixed mode analog/digital ones [17].", "startOffset": 121, "endOffset": 124}, {"referenceID": 16, "context": "The memory optimization theory and the hierarchical routing fabric can be used with either pure digital logic approaches [4], or mixed mode analog/digital ones [17].", "startOffset": 160, "endOffset": 164}, {"referenceID": 16, "context": "The analog circuits are operated in the subthreshold domain to minimize the dynamic power consumption and to implement biophysically realistic neural and synaptic behaviors, with biologically plausible temporal dynamics [17].", "startOffset": 220, "endOffset": 224}, {"referenceID": 1, "context": "and Fire (AdExp-I&F) neuron circuits of the type described and fully characterized in [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 26, "context": "The negative feedback mechanism of the adaptation block and the tunable reset potential of the Potassium block introduce two extra variables in the dynamic equation of the neuron that endow it with a wide variety of dynamical behaviors [27], [28].", "startOffset": 236, "endOffset": 240}, {"referenceID": 27, "context": "The negative feedback mechanism of the adaptation block and the tunable reset potential of the Potassium block introduce two extra variables in the dynamic equation of the neuron that endow it with a wide variety of dynamical behaviors [27], [28].", "startOffset": 242, "endOffset": 246}, {"referenceID": 28, "context": "Synapses and biophysically realistic synapse dynamic are implemented using sub-threshold Differential Pair Integrator (DPI) log-domain filters, of the type proposed in [29], and described in [17].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Synapses and biophysically realistic synapse dynamic are implemented using sub-threshold Differential Pair Integrator (DPI) log-domain filters, of the type proposed in [29], and described in [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 29, "context": "The analog circuit parameters governing the behavior and dynamics of the neurons and synapses are set by programmable onchip temperature compensated bias-generators [30].", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "Alternative neuromorphic processors that use pure digital design styles typically timemultiplex the computing resources and are still faced with the problem of having to transfer state memory back and forth from areas devoted to computing to areas devoted to memory storage [4], [5].", "startOffset": 274, "endOffset": 277}, {"referenceID": 4, "context": "Alternative neuromorphic processors that use pure digital design styles typically timemultiplex the computing resources and are still faced with the problem of having to transfer state memory back and forth from areas devoted to computing to areas devoted to memory storage [4], [5].", "startOffset": 279, "endOffset": 282}, {"referenceID": 30, "context": "The CAM cells make use of NOR-type 9T circuits and of a pre-charge-high Match-Line scheme [31].", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "According to circuit simulation studies [32], the throughput of the R3 router can reach up to 1Gevents/sec in a 28 nm process.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 28, "endOffset": 31}, {"referenceID": 12, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "IBM Truenorth [4] Spinnaker [5] HiAER [13] Neurogrid [3] This work\u2013DYNAPs", "startOffset": 53, "endOffset": 56}, {"referenceID": 18, "context": "Therefore neural networks that combine several hidden layers of this form, known as deep neural networks (DNNs), can be efficiently trained to achieve very high performance on a wide range of visual and non-visual tasks [19].", "startOffset": 220, "endOffset": 224}, {"referenceID": 32, "context": "Recently, pre-trained CNNs and DNNs have been successfully mapped into spike-based hardware, achieving remarkable accuracy, while minimizing power consumption [33]\u2013[36].", "startOffset": 159, "endOffset": 163}, {"referenceID": 35, "context": "Recently, pre-trained CNNs and DNNs have been successfully mapped into spike-based hardware, achieving remarkable accuracy, while minimizing power consumption [33]\u2013[36].", "startOffset": 164, "endOffset": 168}, {"referenceID": 36, "context": "Furthermore, there are growing efforts towards the direct use of asynchronous event-based data produced by spiking sensors such as the DVS [37]\u2013[42].", "startOffset": 139, "endOffset": 143}, {"referenceID": 41, "context": "Furthermore, there are growing efforts towards the direct use of asynchronous event-based data produced by spiking sensors such as the DVS [37]\u2013[42].", "startOffset": 144, "endOffset": 148}, {"referenceID": 37, "context": "Here we tailored the design of the CNN for efficient real-time classification of event streams using an AER data-set recently made available [38].", "startOffset": 141, "endOffset": 145}, {"referenceID": 42, "context": "5 million events with a peak rate of slightly above 8 Meps [43].", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "Indeed, it has been recently shown [33], [35] that if these networks are designed appropriately, they can tolerate the limited precision that affects the analog circuits in the chip, while preserving high accuracy figures that are comparable to those obtained by state-of-the-art Deep Neural Networks (DNNs) running on hardware consuming orders of magnitude more power.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "Indeed, it has been recently shown [33], [35] that if these networks are designed appropriately, they can tolerate the limited precision that affects the analog circuits in the chip, while preserving high accuracy figures that are comparable to those obtained by state-of-the-art Deep Neural Networks (DNNs) running on hardware consuming orders of magnitude more power.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "This is evident if one analyzes the amount of resources required by comparable architectures, such as the one proposed in [4], for different types of CNNs architectures, as recently reported in [36].", "startOffset": 122, "endOffset": 125}, {"referenceID": 35, "context": "This is evident if one analyzes the amount of resources required by comparable architectures, such as the one proposed in [4], for different types of CNNs architectures, as recently reported in [36].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "13: Memory scaling comparison between the TrueNorth architecture [4] and this work.", "startOffset": 65, "endOffset": 68}, {"referenceID": 35, "context": "For TrueNorth, 4 data points (black dots) are extrapolated from CNN benchmark models described in [36] and fitted with a quadratic function (red curve).", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "(2), but adding 2 extra bits per neuron for 4 synaptic weight types as in [36].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "scale neural networks, without running into the von Neumann bottleneck problem [14], [15], and in other data communication bottleneck problems typical of centralized systems.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "scale neural networks, without running into the von Neumann bottleneck problem [14], [15], and in other data communication bottleneck problems typical of centralized systems.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "Neuromorphic solutions that have optimized this overhead and, with it, the communication bandwidth, have sacrificed flexibility and programmability [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "On the other hand, solutions that have maximized network configurability, have sacrificed silicon real-estate for on-chip memory [4], or resorted to using external memory banks [5] (and therefore eliminated the advantage of having memory and computation co-localized).", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "On the other hand, solutions that have maximized network configurability, have sacrificed silicon real-estate for on-chip memory [4], or resorted to using external memory banks [5] (and therefore eliminated the advantage of having memory and computation co-localized).", "startOffset": 177, "endOffset": 180}, {"referenceID": 43, "context": "While there have been ad-hoc solutions proposed in the literature for utilizing off-chip memory more efficiently in neuromorphic systems [44], there is no consensus around a systematic approach that would explicitly allow to trade-off flexibility and memory to meet the requirements of specific applications.", "startOffset": 137, "endOffset": 141}, {"referenceID": 44, "context": "The trade-offs and design points we chose for the prototype device built to validate our approach were determined by the study of cortical networks in mammalian brains [45]\u2013[47].", "startOffset": 168, "endOffset": 172}, {"referenceID": 46, "context": "The trade-offs and design points we chose for the prototype device built to validate our approach were determined by the study of cortical networks in mammalian brains [45]\u2013[47].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49].", "startOffset": 166, "endOffset": 170}, {"referenceID": 47, "context": "The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49].", "startOffset": 172, "endOffset": 176}, {"referenceID": 48, "context": "The use of on-chip heterogeneous memory structures makes this architecture ideal for exploiting the emerging memory structures based on nano-scale resistive memories [16], [48], [49].", "startOffset": 178, "endOffset": 182}, {"referenceID": 31, "context": "As scaling studies have demonstrated that such architecture can outperform the current state-of-theart systems when implemented using a 28 nm Fully-Depleted Silicon on Insulator (FDSOI) process [32], we are confident that such approach can lead to the design of a new generation of neuromorphic processors for solving a wide range of practical applications that require real-time processing of eventbased sensory signals, using ultra low-power, low latency, and compact systems.", "startOffset": 194, "endOffset": 198}], "year": 2017, "abstractText": "Neuromorphic computing systems comprise networks of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multi-core neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor (DVS) at high speed.", "creator": "LaTeX with hyperref package"}}}