{"id": "1708.04776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network", "abstract": "nowadays, dynamic cross - modal interaction retrieval plays an indispensable secondary role solely to flexibly find information fit across different modalities of data. effectively measuring the information similarity between different modalities of existing data is the strategic key of combining cross - blinded modal intelligence retrieval. connecting different media modalities such examples as image and text styles have exhibited imbalanced and complementary hierarchical relationships, which notably contain virtually unequal minimum amount of associated information when erroneously describing the same preferred semantics. for example, images images often additionally contain more details that functionality cannot be jointly demonstrated by establishing textual page descriptions and metadata vice vale versa. existing works based all on comparative deep neural accountability network ( dnn ) mostly merely construct the one common consensus space for listing different modalities to find the deepest latent alignments between linking them, but which lose their user exclusive link modality - category specific consensus characteristics. visibly different from both the competing existing works, we propose more modality - specific vocabulary cross - modal similarity reliability measurement ( tc mcsm ) approach by partially constructing independent semantic space formats for performing each interaction modality, which itself adopts end - to - end framework procedures to directly generate their modality - domain specific cross - modal specific similarity value without creating explicit common representation. similarly for each semantic space, cognitive modality - specific link characteristics expressed within one modality resource are further fully exploited by using recurrent attention network, providing while comparing the data of another modality index is projected appropriately into this space with attention based relational joint, embedding to utilize encoding the learned attention weights for guiding among the fine - and grained cognitive cross - authored modal correlation correlation data learning, of which results can capture the deeply imbalanced composition and complementary relationships between different identical modalities.... finally, the complementarity effect between designing the semantic spaces for distinguishing different modalities is explored by utilizing adaptive fusion of the modality - specific cross - indexed modal similarities to perform cross - modal retrieval. experiments improved on coupling the shared widely - used wikipedia xml and pascal sentence datasets similarly as well as as on our constructed large - scale integrated xmedianet dataset systematically verify the effectiveness validity of our proposed approach, outperforming 9 constructed state - layers of - the - computer art adaptive methods.", "histories": [["v1", "Wed, 16 Aug 2017 05:43:54 GMT  (5195kb,D)", "http://arxiv.org/abs/1708.04776v1", "13 pages, submitted to IEEE Transactions on Image Processing"]], "COMMENTS": "13 pages, submitted to IEEE Transactions on Image Processing", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["yuxin peng", "jinwei qi", "yuxin yuan"], "accepted": false, "id": "1708.04776"}, "pdf": {"name": "1708.04776.pdf", "metadata": {"source": "CRF", "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network", "authors": ["Yuxin Peng", "Jinwei Qi", "Yuxin Yuan"], "emails": ["pengyuxin@pku.edu.cn)."], "sections": [{"heading": null, "text": "Modality-specific Cross-modal Similarity Measurement with Recurrent Attention Network\nYuxin Peng, Jinwei Qi and Yuxin Yuan\nAbstract\u2014Nowadays, cross-modal retrieval plays an indispensable role to flexibly find information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities such as image and text have imbalanced and complementary relationships, which contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on Deep Neural Network (DNN) mostly construct one common space for different modalities to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Different from the existing works, we propose modality-specific cross-modal similarity measurement (MCSM) approach by constructing independent semantic space for each modality, which adopts end-to-end framework to directly generate modalityspecific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding to utilize the learned attention weights for guiding the fine-grained crossmodal correlation learning, which can capture the imbalanced and complementary relationships between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modalityspecific cross-modal similarities to perform cross-modal retrieval. Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset verify the effectiveness of our proposed approach, outperforming 9 state-of-the-art methods.\nIndex Terms\u2014Modality-specific cross-modal similarity measurement, recurrent attention network, attention based joint embedding, adaptive fusion.\nI. INTRODUCTION\nNOWADAYS, multimodal data such as image, video, textand audio, has been widely available on the Internet, which is the fundamental component for promoting artificial intelligence to understand the real world. Some works have been done for breaking the boundaries between different modalities. Cross-modal retrieval has become a highlighted research topic to perform retrieval across different modalities of data, which has great demands with many practical applications, such as search engine and digital libraries. An example of cross-modal retrieval is shown in Figure 1. Different from the traditional single-modal retrieval, such as image retrieval [1] and video retrieval [2], which is limited in providing\nThis work was supported by National Natural Science Foundation of China under Grants 61371128 and 61532005.\nThe authors are with the Institute of Computer Science and Technology, Peking University, Beijing 100871, China. Corresponding author: Yuxin Peng (e-mail: pengyuxin@pku.edu.cn).\nretrieval results of the same single modality with query, cross-modal retrieval is more flexible and useful to retrieve relevant multimodal information by submitting one query of any modality [3].\nThe main challenge of cross-modal retrieval is to deal with the inconsistency between different modalities and learn the intrinsic correlation between them. For the fact that different modalities of data has diverse representations and distributions that usually span different feature spaces, this heterogeneous characteristic makes it hard to directly measure the similarity between different modalities, such as an image and an audio clip. To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different modalities into one common space to learn the common representation, by which the cross-modal similarity can be calculated to perform retrieval between different modalities. Traditional methods build the common space by learning mapping matrices to maximize the correlations of variables from different modalities, such as methods based on Canonical Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12]. Recently, the dramatic advances in deep learning have inspired researchers to bridge the ar X\niv :1\n70 8.\n04 77\n6v 1\n[ cs\n.C V\n] 1\n6 A\nug 2\n01 7\ngap between different modalities with Deep Neural Network (DNN). Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.\nThe aforementioned methods mostly project the data of different modalities from their own feature spaces into one single common space equally to find the latent alignments between them, which means equal amount of information is captured from the data of different modalities during cross-modal correlation learning. Generally speaking, different modalities such as image and text have imbalanced and complementary relationships that provide unequal amount of information in describing the same semantics, because some modality-specific characteristics within one modality cannot be exactly aligned with other modalities. For example, an image often co-occurs with its corresponding text descriptions on a web page to describe the same semantics such as objects or events. But the relationships between image and text instance are imbalanced, where not all fine-grained image details can be exactly aligned to the textual descriptions and vice versa. Furthermore, image often contains more details which cannot be demonstrated by textual descriptions, that is what we usually say \u201cA picture is worth a thousand words\u201d. While in other cases, the opposite would happen that textual descriptions contain more semantic information than image when describing some high-level semantics, such as historical events or literature works, where the textual description contains more background information that cannot be shown by its corresponding image. As shown in Figure 1, green boxes indicate the appropriate alignment between visual and textual fine-grained information, while the red boxes mean the misalignment. Therefore, treating different modalities equally to find fine-grained alignments for constructing one common space loses such exclusive and useful modality-specific characteristics, which cannot fully exploit the intrinsic information within each modality.\nFor addressing the above issues, we aim to preserve the modality-specific characteristics by fully exploiting the finegrained information within each modality when learning the cross-modal correlation. Thus, modality-specific measurement\nis required for each modality instead of only constructing one common space. Recently, attention mechanism has made great advances in DNN, which allows models to concentrate on the necessary fine-grained parts of visual or textual inputs, and has been successfully applied to various multimodal tasks, such as image caption [16] and visual question answering [17]. Inspired by these advances, we attempt to apply attention mechanism to fully exploit the intrinsic modality-specific finegrained information for cross-modal retrieval..\nIn this paper, we propose modality-specific cross-modal similarity measurement (MCSM) approach, which constructs independent semantic spaces for different modalities, where the modality-specific characteristics can be fully explored with attention mechanism during cross-modal correlation learning. The modality-specific cross-modal similarity is directly generated from each semantic space through the end-to-end framework without explicit common representation. Figure 2 shows an overview of our proposed approach. The main contributions of this paper are presented as follows:\n\u2022 Modality-specific cross-modal similarity measurement. We construct independent semantic space for each modality. In each semantic space, the modality-specific characteristics are fully exploited by modeling the fine-grained information within one modality, while the data of another modality is projected into this semantic space to capture the imbalanced and complementary relationships between different modalities during cross-modal correlation learning. \u2022 Recurrent attention network with joint embedding. We design recurrent attention network in each semantic space to capture the modality-specific characteristics including both fine-grained local and context information by recurrent network with attention mechanism. While an attention based joint embedding loss is proposed to utilize the learned attention weights for guiding the fine-grained cross-modal correlation learning. \u2022 End-to-end frameworks with adaptive fusion. We adopt an end-to-end framework in each semantic space to directly generate modality-specific cross-modal similarity, which integrates both representation learning and similarity measurement stages to benefit each other. Furthermore, adaptive fusion is proposed to obtain the final similarity for performing cross-modal retrieval, which can fully explore the complementarity between the semantic spaces for different modalities.\nExperiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset show that our proposed MCSM approach outperforms 9 state-of-the-art methods, which can verify the effectiveness of MCSM approach.\nThe remainder of this paper is organized as follows. We briefly review the related works in Section II. In Section III, our proposed MCSM approach is presented in detail. Then Section IV reports the experimental results as well as analyses. Finally, Section V concludes this paper."}, {"heading": "II. RELATED WORKS", "text": "In this section, we briefly review the representative crossmodal retrieval methods with common space learning, as well as recent advances of attention mechanism in DNN."}, {"heading": "A. Common Space Learning for Cross-modal Retrieval", "text": "The current mainstream of cross-modal retrieval methods is to learn an intermediate common space for the features of different modalities, then the cross-modal similarity can be directly measured in one common space. Figure 3 illustrates the main framework of such common space learning methods. As indicated in [3], we mainly introduce three categories of existing methods as follows, namely traditional statistical correlation analysis methods, cross-modal graph regularization methods and DNN-based methods.\n1) Traditional statistical correlation analysis methods: As the foundation of common space learning methods, these methods mainly optimize the statistical values to learn linear projection matrices, which project features of different modalities into the common space and obtain the common representation. Canonical Correlation Analysis (CCA) [18], as one of the most representative works, is a natural solution to maximize the pairwise correlation between the data of different modalities such as image/text pairs [4]. Furthermore, semantic information is incorporated to extend CCA for improving the accuracy of cross-modal retrieval. For example, Costa et al. [19] integrate semantic category labels to improve the performance of CCA. Multi-view CCA [8] is proposed to construct a third view for modeling the high-level semantics. While Ranjan et al. [20] propose multi-label CCA, which considers the high-level semantic information in the form of multi-label annotations. Besides, Cross-modal Factor Analysis (CFA) [21], as one of the alternative methods, is proposed to minimize the Frobenius norm between the data of different modalities after projecting them into one common space.\n2) Cross-modal graph regularization methods: Graph regularization [22] is widely used to construct a partially labeled graph for semi-supervised learning, which aims to enrich the training set and smooth the solution. Zhai et al. [23] are the first to integrate graph regularization into cross-modal retrieval and propose Joint Graph Regularized Heterogeneous Metric Learning (JGRHML), which constructs the joint graph regularization term in the learned metric space. Furthermore, Joint Representation Learning (JRL) [9] is proposed to construct several separate graphs for different modalities and learn projection matrices with the joint consideration of correlation and semantic information. Peng et al. [24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time. Besides, Wang et al. [10] also adopt multimodal graph regularization term to preserve inter-modality and intra-modality similarity relationships.\n3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27]. Researches also adopt DNN to perform common space learning to take\nthe advantage of its considerable ability on modeling highly nonlinear correlation. Most of the DNN-based methods construct two subnetworks for different modalities such as image and text, which are linked at joint layer as the common space to model cross-modal correlation. Bimodal Autoencoders (Bimodal AE) [28] is proposed as an extension of Restricted Boltzmann Machine (RBM) to model multiple modalities by minimizing the reconstruction error. Srivastava et al. [29] propose Multimodal Deep Belief Network (Multimodal DBN), which adopts two kinds of DBN for different modalities to model the distribution over their original features, while a joint RBM is adopted on the top of them to model the joint distribution and get the common representation. Correspondence Autoencoder (Corr-AE) [14] and Deep Canonical Correlation Analysis (DCCA) [30] also consist of two subnetworks, while Corr-AE jointly models correlation and reconstruction information, and DCCA combines DNN with CCA to maximize the correlation on the top of two subnetworks. Besides, Peng et al. [13] propose Cross-media Multiple Deep Networks (CMDN) to model the intra-modality and inter-modality correlation in a two-stage learning framework. The above works mainly take hand-crafted features as image inputs. Furthermore, Wei et al. [31] propose deep-SM to perform deep semantic matching with Convolutional Neural Network (CNN), which exploits the strong representation ability of CNN features to improve the retrieval accuracy. He et al. [32] adopt two convolution-based networks to model the matched and mismatched image/text pairs via deep and bidirectional representation learning.\nThe aforementioned methods mostly construct one common space for different modalities to find the latent alignments between them, which lose exclusive modality-specific characteristics. Therefore, we attempt to adopt modality-specific measurement to fully exploit such modality-specific characteristics by modeling the intrinsic fine-grained information within each modality."}, {"heading": "B. Attention Mechanism", "text": "Attention mechanism, as one of the recent advances in neural network, has been successfully applied to various multimodal tasks, which allows models to focus on the necessary\nfine-grained parts of visual or textual inputs. We mainly introduce two kinds of attention mechanism as follows, namely visual attention and textual attention.\n1) Visual attention: Recently, many methods have adopted visual attention model to promote several image processing tasks, which can pay more attention to fine-grained parts in an image. Mnih et al. [33] propose an attention based task-driven visual processing framework for image classification, which adopts Recurrent Neural Network (RNN) to adaptively select a sequence of regions. Gregor et al. [34] propose a spatial attention mechanism, which designs a sequential variational auto-encoding framework to perform image generation. Yang et al. [35] propose Stacked Attention Networks (SANs) for image question answering, which can locate relevant image regions to the question with stacked attention model.\n2) Textual attention: Some related works in Natural Language Processing (NLP) have adopted textual attention model to find semantic alignments with an encoder-decoder network. Rockta\u0308schel et al. [36] propose a word-by-word neural attention mechanism to reason over entailments of paired words or phrases. Hermann et al. [37] develop a class of attention based deep neural networks, which learn to read and answer complex questions. Rush et al. [38] propose a fully datadriven approach, which adopts a local attention based model to generate summarization according to the input sentence.\nInspired by the great progress of attention mechanism, we attempt to fully exploit the intrinsic fine-grained information within each modality by attention mechanism, which can preserve the modality-specific characteristics when learning the cross-modal correlation."}, {"heading": "III. OUR PROPOSED APPROACH", "text": "As shown in Figure 4, our proposed MCSM approach adopts modality-specific measurement to construct independent semantic spaces by an end-to-end framework for image and text respectively. In each semantic space, first, recurrent attention network is adopted to fully exploit the fine-grained modality-specific characteristics. Second, an attention based joint embedding is employed to capture the imbalanced and complementary relationships between different modalities and perform cross-modal correlation learning. Finally, adaptive fusion is proposed to explore the complementarity between different semantic spaces for cross-modal retrieval."}, {"heading": "A. Notation", "text": "In the beginning, the formal definition of cross-modal retrieval is presented as follows. The two modalities involved in cross-modal retrieval are denoted as I for image and T for text. The multimodal dataset consists of two parts, namely training set and testing set. The training set is denoted as Dtr = {Itr, Ttr}. Image training data Itr = {ip}ntrp=1 consists of totally ntr instances, and ip is the p-th image instance. Similarly, the text training data is denoted as Ttr = {tp}ntrp=1, which has the same number of instance with image for training. Besides, the training data also has its corresponding semantic category labels, which are denoted as {cIp} ntr p=1 and {cTp } ntr p=1. Then, the testing set, which is denoted as Dte = {Ite, Tte}, includes Ite = {iq}nteq=1 and Tte = {tq} nte q=1 containing nte testing instances. Finally, given a query of any modality, the goal of cross-modal retrieval is to calculate the cross-modal similarity sim(ia, tb), and retrieve the relevant instances of another modality in the testing data by the ranking of calculated\nsimilarity. In the following subsections, we first introduce the proposed image and text semantic space measurement methods respectively, then we demonstrate the proposed adaptive fusion approach on different semantic spaces.\nB. Image Semantic Space Measurement\n1) Recurrent attention network for image: To exploit intrinsic modality-specific characteristics within image data, we design a recurrent network with attention mechanism, which is adopted on the top of CNN hidden layer, to fully model the fine-grained local information and spatial context information jointly.\nFirst, each input image ip is resized to 256 \u00d7 256 and fed into CNNs for exploiting the fine-grained local information. Specifically, the network structure is configured the same as the layers before the last pooling layer (pool5) in 19-layer VGGNet [39]. The last pooling layer consists of 512 filters, and we can obtain separate feature vectors for different regions from the response of each filter over a 7 \u00d7 7 mapping of the image. Thus, each input image ip can be represented as {vi1, ..., vin}, where n denotes the total number of image regions and vin is the n-th region represented as a 512 dimensional feature vector.\nThen, we employ RNN to model the spatial context information among image regions. We compose these regions as a sequence, which are regarded as the results of eye movement when we glance at the image [40]. Specifically, Long Short Term Memory (LSTM) network [41] is adopted, which is a special kind of RNN, with strong ability to learn long-term dependencies through the memory cell and the update gates as well as preserve previous time-steps information at the same time. The architecture of LSTM unit is shown in Figure 5. Formally, taking a sequence of image regions as input, the LSTM is updated recursively with the following equations.\nit = \u03c3(Wixt + Uiht\u22121 + bi), (1) ft = \u03c3(Wfxt + Ufht\u22121 + bf ), (2) ot = \u03c3(Woxt + Uoht\u22121 + bo), (3) ut = tanh(Wuxt + Uuht\u22121 + bu), (4) ct = ut it + ct\u22121 ft, (5) ht = ot tanh(ct), (6)\nwhere the activation vectors of the input, forget, memory cell and output are denoted as i, f , c and o respectively. And x is the input region feature and the hidden unit output is denoted as h. While W , U and b are the weight matrices and bias term that need to be trained. denotes the element-wise multiplication. And \u03c3 is the sigmoid nonlinearity to activate the gate, which is defined as follows.\n\u03c3(x) = 1\n1 + e\u2212x . (7)\nAfter that, we apply attention mechanism which aims to allow models focusing on the necessary fine-grained regions within image. We have obtained the output sequence Hi = {hi1, ..., hin} from LSTM. The attention weights ai can be\ncalculated by a feed-forward neural network with softmax function as follows.\nM i = tanh(W iaHi), (8) ai = softmax(wTiaMi), (9)\nwhere W ia and wia are the weight parameters for respective layers. And ai is the generated attention probabilities for image regions. Thus, the final image vector for the n-th region can be obtained as ainh i n, which contains both fine-grained local information and spatial context information. 2) Visual Attention based joint embedding: Cross-modal correlation learning is performed to project the text data into the image semantic space, which utilizes the learned visual attention weights from the above recurrent attention network. We first need to generate the representation for text instance tp. Each word is represented by a k-dimensional vector extracted by Word2Vec [42] model, which is trained on billions of words in Google News and publicly available1. Thus, a sentence with n words can be represented as an n \u00d7 k matrix. And a Word CNN is adopted on the input matrix, which has the same configuration with [43]. While the text representation for each sentence is generated from the last fully connected layer, denoted as qtp.\nThen, we aim to project the text data from its feature space into the constructed semantic space for image. To fully explore the imbalanced and complementary relationships between different modalities, we design the cross-modal similarity simi for image semantic space between image ip and text tp as follows.\nsimi(ip, tp) = n\u2211 j=1 a ip j h ip j \u00b7 q t p, (10)\nwhere hipj is the j-th region in the image ip and a ip j is the attention weight for the corresponding image region. Finally, we design an attention based joint embedding loss to perform cross-modal correlation learning, which jointly considers both matched and mismatched image/text pairs with the\n1https://code.google.com/p/word2vec/\ndefined cross-modal similarity based on the learned attention weights. The objective function is defined as follows.\nLi = 1\nN N\u2211 n=1 li1(i + n , t + n , t \u2212 n ) + li2(t + n , i + n , i \u2212 n ), (11)\nand the two items in this formula are defined as:\nli1(i + n ,t + n , t \u2212 n ) =\nmax(0, \u03b1+ simi(i + n , t + n )\u2212 simi(i+n , t\u2212n )), (12)\nli2(t + n ,i + n , i \u2212 n ) =\nmax(0, \u03b1+ simi(i + n , t + n )\u2212 simi(i\u2212n , t+n )), (13)\nwhere (i+n , t + n ) denotes the matched image pair, while (i + n , t \u2212 n ) and (i\u2212n , t + n ) are the mismatched pairs. The margin parameter is set to be \u03b1. N is the number of triplet tuples sampled from training set. To train the model, stochastic gradient descent (SGD) is adopted. So far, we have obtained the modalityspecific cross-modal similarity simi for image semantic space, which integrates both representation learning and similarity measurement to benefit each other, and also fully captures the intrinsic fine-grained clues in image and imbalanced information across different modalities for correlation learning."}, {"heading": "C. Text Semantic Space Measurement", "text": "1) Recurrent attention network for text: For fully exploiting modality-specific characteristics within text data, we also design a recurrent network with attention mechanism, on the top of CNN hidden layer, which can model both the finegrained local and context information in textual descriptions.\nFirst, the input text instance tp, which consists of n words, is represented as an n\u00d7k matrix, where each word is also represented as a k-dimensional vector extracted by Word2Vec [42] model. Following [43], we design fast convolutional networks for text (Word CNN), which is built by several combinations of convolution layer, threshold activation function layer and pooling layer. The Word CNN is similar with the image CNN except the 2D convolution and spatial max-pooling of image CNN are replaced by temporal (1D) convolution and temporal max-pooling. Through the Word CNN network, CNN hidden activation of the last pooling layer is split to generate the features of text fragments.\nSecond, RNN is adopted on the top of CNN with the sequence of vectors to further model the context information along the input text sequence. Specifically, we also adopt LSTM network [41] to exploit such temporal dependency, which takes a sequence of text fragments as input. LSTM is updated following the equations (1) to (6), where x denotes the input feature of text fragment. Thus, we can obtain the output sequence from LSTM denoted as Ht = {ht1, ..., htm}.\nThen, the attention mechanism is applied to capture useful fine-grained fragments in text sequence. The attention weights are denoted as at, which are calculated by a feed-forward network with softmax function as follows.\nM t = tanh(W taHt), (14) at = softmax(wTtaMt), (15)\nwhere the weight parameters for respective layers are denoted as W ta and wia. a\nt is the generated attention probabilities for text fragments. Thus, the final text vector for the m-th fragment is calculated as atmh t m, which captures both finegrained local and context information in textual description. 2) Textual Attention based joint embedding: To perform cross-modal correlation learning, image data is projected into the text semantic space to utilize the learned textual attention weights from the above recurrent attention network for text. We still need to generate the image representation for each instance ip, which is also extracted from the last fully connected layer (fc7) in 19-layer VGGNet [39] with 4,096 dimensional number, and denoted as qip.\nThen, the image data is projected into the constructed semantic space for text from their own feature space. Thus, we design the cross-modal similarity simt for text semantic space between image ip and text tp as follows, which aims to explore the imbalanced and complementary relationships between different modalities.\nsimt(ip, tp) = m\u2211 j=1 a tp j h tp j \u00b7 q i p, (16)\nwhere the j-th fragment of text tp is denoted as h tp j , a tp j is the attention weight for the corresponding text fragment. Finally, an attention based joint embedding loss is designed similarly for performing cross-modal correlation learning in the text semantic space, which takes the consideration that the difference between the similarity of matched image/text pair and that of mismatched pair should be as large as possible. Thus, the objective function is defined as follows.\nLt = 1\nM M\u2211 n=1 lt1(t + n , i + n , i \u2212 n ) + lt2(i + n , t + n , t \u2212 n ), (17)\nwhere lt1(t+n , i + n , i \u2212 n ) and lt2(i + n , t + n , t \u2212 n ) are defined similarly as equations (12) and (13) with the cross-modal similarity simt as follows,\nlt1(t + n ,i + n , i \u2212 n ) =\nmax(0, \u03b2 + simt(i + n , t + n )\u2212 simt(i\u2212n , t+n )), (18)\nlt2(i + n ,t + n , t \u2212 n ) =\nmax(0, \u03b2 + simt(i + n , t + n )\u2212 simt(i+n , t\u2212n )). (19)\nAlso the number of triplet tuples sampled from training set is denoted as M . \u03b2 is the margin parameter. Therefore, the modality-specific cross-modal similarity simt for text semantic space can be obtained, with the fully modeling of fine-grained clues in text as well as imbalanced information across different modalities for correlation learning."}, {"heading": "D. Adaptive Fusion on Different Semantic Spaces", "text": "So far, we have obtained two kinds of modality-specific cross-modal similarity, namely simi and simt from the semantic spaces for image and text. Inspired by [44], we further attempt to explore the complementarity between different semantic spaces by adaptive fusion.\nFirst, the cross-modal similarity scores obtained from different semantic spaces are min-max normalized to [0, 1] respectively, which aims to overcome the influence of image/text pairs with too large similarity scores, and are defined as follows.\nri(ip, tp) = simi(ip, tp)\u2212min(simi(i, t))\nmax(simi(i, t))\u2212min(simi(i, t)) , (20)\nrt(ip, tp) = simt(ip, tp)\u2212min(simt(i, t))\nmax(simt(i, t))\u2212min(simt(i, t)) . (21)\nThen, the normalized scores obtained from one semantic space are used as the adaptive weights of the corresponding image/text pair in another semantic space during fusion stage, with the motivation that larger similarity in one semantic space leads to higher importance of the corresponding pair in another semantic space. Finally, the two kinds of modality-specific cross-modal similarity are fused with the following equation.\nSim(ip, tp) = (22) rt(ip, tp) \u00b7 simi(ip, tp) + ri(ip, tp) \u00b7 simt(ip, tp).\nThus, we can obtain the final cross-modal similarity score Sim(ip, tp) between image ip and text tp, which can fully explore the complementarity between different semantic spaces to boost the performance of cross-modal retrieval."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we conduct experiments on 3 cross-modal datasets, including widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset, taking 9 state-of-the-art methods for comparison to verify the effectiveness of our proposed approach. Besides, comprehensive experimental analyses are presented including convergence and parameter analysis, as well as baseline experiments to verify the contribution of each component in our approach."}, {"heading": "A. Datasets", "text": "Here we briefly introduce 3 cross-modal datasets adopted in the experiments, including Wikipedia, Pascal Sentence and our constructed large-scale XMediaNet datasets. Each dataset is divided into 3 subsets, namely training set, testing set and validation set.\n\u2022 Wikipedia dataset [4], as the most widely-used dataset for cross-modal retrieval, is selected from \u201cfeatured articles\u201d in Wikipedia2 with 10 most populated categories, including history, biology and so on. This dataset totally consists of 2,866 image/text pairs. For fair and objective comparison purpose, we exactly follow the dataset partition strategy of [13], [14] to divide the dataset into 3 subsets: 2,173 pairs in training set, 231 pairs in validation set and 462 pairs in testing set. \u2022 Pascal Sentence dataset [45] contains 1,000 images, which is generated from 2008 PASCAL development kit. Each image is annotated via Amazon Mechanical Turk by crowdsourcing to generate 5 independent sentences\n2https://en.wikipedia.org/wiki/Wikipedia:Featured articles\nfrom different annotators, which forms one document. This dataset is categorized into 20 categories, and also following [13], [14], 800 documents are selected as training set, while 100 documents for testing and 100 documents for validation. \u2022 XMediaNet dataset is our constructed large-scale crossmodal dataset, which consists of 5 modalities, namely text, image, video, audio and 3D model. We select 200 category nodes from WordNet3 to construct this dataset to ensure the semantic hierarchy structure. These categories can be divided into two main kinds: animals and artifacts. There are 47 species of animal such as elephant, owl, bee and frog as well as 153 types of artifact such as violin, airplane, shotgun, and camera. The total number of instances exceeds 100,000. It is noted that we also use image and text in the experiments, where text paragraphs are extracted from Wikipedia articles whose topics belong to the category, and images including the objects of the category are collected from Flickr4. This dataset totally consists of 40,000 image/pairs, and is also divided into 3 subsets, with 32,000 pairs in training set, 4,000 pairs in testing set and 4,000 pairs in validation set. Some examples are shown in Figure 6."}, {"heading": "B. Details of the Network", "text": "We implement the proposed network by Torch5, which is a widely-used scientific computing framework. And we\n3http://wordnet.princeton.edu/ 4http://www.flickr.com 5http://torch.ch/\nintroduce the details of the network including data preprocess strategy and network structure as following.\n1) Data preprocess: For image, we use the original images resized to 256\u00d7256 as inputs. For text, we convert each word in a document into a 300-dimension vector by Word2Vec [42] and generate vector sequences as text inputs. The maximum input length is set as the max sequence length in the dataset, and we adopt zero-padding for other sequences to beneath this limit.\n2) Recurrent attention network: The recurrent attention network of each semantic space mainly consists of three parts, namely convolutional network, Long Short-Term Memory (LSTM) network and attention network. For the semantic space for text, the convolutional network consists of three learnable temporal convolution layers, each of which is followed by a ReLU activation function layer and a temporal max-pooling layer. Taking Wikipedia dataset as an example, the first temporal convolution layer has 384 kernels whose widths are 15. The parameter combinations of the remaining convolution layers are (512, 9) and (256, 7). The first parameter of each combination means the number of convolution kernels while the second is the kernel width. The kernel step sizes of all convolution layers are 1. For the other two datasets, the number of kernels on each convolution layer is the same with Wikipedia dataset, but the length of text instances differs greatly between different datasets, thus the kernel widths change according to the lengths of the input sequences. For the semantic space for image, we use the pretrained convolution network in 19-layer VGGNet and treat the output of the last pooling layer as the input of LSTM. The LSTM network has two units in series, and the dimension of output keeps the same with input. The LSTM is followed by a fully connected layer which aims to project the output of LSTM into the target dimension, which is 4,096 dimensions in our case. The attention network is made up of a fully connected layer and a softmax layer."}, {"heading": "C. Compared Methods", "text": "Totally 9 state-of-the-art methods are compared in the experiments to verify the effectiveness of our proposed approach. There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47]. While the other 4 methods, Corr-AE [14], DCCA [15], CMDN [13] and Deep-SM [31] are DNN-based methods. The brief introductions of these 9 compared methods are presented as follows:\n\u2022 CCA [18] learns project matrices to maximize the correlation between the projected features of different modalities in one common space. \u2022 CFA [21] minimizes the Frobenius norm between the data of different modalities after projecting them into one common space. \u2022 KCCA [46] uses kernel function to project the features into a higher-dimensional space, and then learns a common space by CCA. In the experiments, we adopt Gaussian kernel as the kernel function.\n\u2022 JRL [9] learns a common space by using semantic information, with semi-supervised regularization and sparse regularization. \u2022 LGCFL [47] jointly learns basis matrices of different modalities, by using a local group based priori in the formulation to fully take advantage of popular block based features. \u2022 Corr-AE [14] consists of two autoencoder networks coupled at the code layer to simultaneously model the reconstruction error and correlation loss. It should be noted that Corr-AE has two extensions, namely CorrCross-AE and Cross-Full-AE, and in the experiments we compare with the best results of the three models. \u2022 DCCA [15] is a nonlinear extension of CCA. The correlation is maximized between the output layers of two separate subnetworks. \u2022 CMDN [13] adopts multiple deep networks to generate separate representations and learns the common representation with a stacked network. \u2022 Deep-SM [31] adopts convolutional neural network to perform deep semantic matching, which fully exploits the strong power of CNN image feature."}, {"heading": "D. Evaluation Metric", "text": "We perform cross-modal retrieval on 3 datasets mentioned above with two kinds of retrieval tasks, which are defined as follows.\n\u2022 Image query text (image\u2192text). Retrieving relevant text instances in the testing set ranked by calculated crossmodal similarity, using a query of image. \u2022 Text query image (text\u2192image). Retrieving relevant image instances in the testing set ranked by calculated cross-modal similarity, using a query of text.\nIt should be noted that our proposed MCSM approach integrates both common representation learning and distance metric, which takes original image and text as inputs to directly generate the cross-modal similarity score. While other compared methods only learn the common representation taking hand-crafted features as input. Thus, for fair comparison, all compared methods also adopt the same CNN features used in our proposed approach as input. Specifically, the CNN feature of image is extracted from fc7 layer in 19-layer VGGNet [39], while CNN feature of text is extracted by Word CNN with the same configuration of [43]. We directly adopt the source codes provided by their authors, to fairly evaluate the compared methods by the following steps in the experiments.\n1) Perform common representation learning using the data in training set to obtain the learned projections or deep models.\n2) Use the learned projections or deep models to convert the data in testing set into the common representation.\n3) Calculate the cross-modal similarity between image and text by cosine distance, and then perform cross-modal retrieval.\nMean Average Precision (MAP) score is adopted as the evaluation metric on Wikipedia, Pascal Sentence and our constructed XMediaNet datasets, which is the mean value of\nAverage Precision (AP) of each query. And AP is defined as follows.\nAP = 1\nR n\u2211 k=1 Rk k \u00d7 relk, (23)\nwhere the testing set contains n instances, which has R relevant instances. Rk is the number of relevant instances in the top k returned results. relk is set to be 1 when the k-th returned result is relevant, otherwise, relk is set to be 0. MAP score considers the ranking of returned retrieval results as well as precision simultaneously, which is extensively adopted in cross-modal retrieval tasks, such as [4], [13]. It should be noted that the MAP score is calculated on all returned results, not only top 50 as adopted in [14]."}, {"heading": "E. Comparisons with 9 State-of-the-art Methods", "text": "In this part, we present experimental results on our proposed approach as well as all the compared methods. Tables I to III show the MAP scores of two retrieval tasks and their average scores on 3 datasets, we can see that our proposed approach achieves the best retrieval accuracy compared with 9 state-of-the-art methods. As shown in Table I, our proposed approach has improved the average MAP score from 0.457 to 0.487 on Wikipedia dataset. On one hand, among the compared traditional methods, LGCFL achieves the best retrieval accuracy, which is closer to CMDN based on DNN. On the other hand, the accuracies of 4 DNN-based methods differ greatly, while some of them are outperformed by the traditional methods, such as the average MAP scores of DCCA and Corr-AE are lower than LGCFL and JRL. Besides, results of 2 retrieval tasks as well as their average results on Pascal Sentence and XMediaNet datasets are shown in Tables II and III, which have the similar trends as Wikipedia dataset, while our proposed MCSM approach still keeps the best. Some cross-modal retrieval results on XMediaNet dataset of our proposed MCSM approach and the best DNN-based compared method CMDN are shown in Figure 7.\nNext we give the in-depth analysis on the retrieval results of both our proposed MCSM approach and all compared methods. As shown in Tables I to III, our proposed MCSM approach shows advantage compared with 9 state-of-the-art methods on all 3 datasets. We can also observe that the accuracies of DNN-based methods fail to widen a clear gap with traditional methods. Among the traditional methods, the classical baseline CCA has the worst accuracy for it only models some statistical values, while KCCA extends CCA to achieve better accuracy by the adoption of kernel function with the better ability of modeling nonlinear correlation. CFA has similar results with KCCA, which minimizes the Frobenius norm in the learned common space. JRL and LGCFL are the best two methods among them, while the former utilizes the semi-supervised and sparse regularization, and the latter learns the basis matrices with a local group based priori.\nAs for DNN-based methods, DCCA and Corr-AE have close accuracies. DCCA maximizes correlation on the outputs of two separate networks to extend CCA, and Corr-AE not only considers the correlation error but also minimizes the reconstruction error. Deep-SM outperforms DCCA and Corr-AE by fully exploring the strong learning power of convolutional neural network with semantic category information. CMDN achieves the best accuracy among all the compared methods on 2 datasets, because it takes intra-modality and intermodality correlation into consideration during both separate representation learning and common representation learning stages. Compared with all state-of-the-art methods, our proposed MCSM approach has the best accuracy for the fact of following 3 aspects: (1) Independent semantic spaces for different modalities with recurrent attention network to fully exploit the modality-specific fine-grained context information. (2) Attention based joint embedding loss to utilize the imbalanced and complementary relationships between different modalities. (3) Adaptive fusion to explore the complementarity\nthese examples, the correct retrieval results are with green borders, and the wrong results are with red borders.numbers of iteration 1040 0.5 1 1.5 2 2.5 3 3.5 4loss 05101520253035404550\nFig. 8. Convergence experiments of our proposed MCSM approach conducted on the large-scale XMediaNet dataset, which show the curves of downtrend on the loss value.\nbetween different semantic spaces for cross-modal retrieval."}, {"heading": "F. Convergence and Parameter Analysis", "text": "First, we conduct convergence experiments on the relatively large-scale XMediaNet dataset. The curve of downtrend on the loss value is shown in Figure 8. We can observe that our proposed approach converges within 15K iterations on the\nXMediaNet dataset with relatively large scale, which shows its efficiency in training stage. Then, we also conduct parameter experiments on the effect of key parameters, including learning rate and margin parameters \u03b1 and \u03b2 in equations (12), (13), (18) and (19), which are implemented on all the 3 datasets. For the learning rate, we range the value from 1e-2 to 1e-5, and the results are shown in Figure 9, from which we can see that our proposed approach achieves the best accuracy at the learning rate of 1e-4 on all the 3 datasets, and the accuracy becomes lower at a higher learning rate. Then, for the margin parameter, it should be noted that we set the margin parameters \u03b1 and \u03b2 with the same value in all loss functions during each experiment. The value is ranged from 0.1 to 0.9. The results are shown in Figure 10, from which we can see that the accuracies are not sensitive to the margin parameters."}, {"heading": "G. Baseline Comparisons", "text": "In this part, we conduct two baseline experiments as follows, to verify the separate contribution of each component in our proposed MCSM approach. Tables IV and V show the accuracy of our proposed MCSM approach as well as the baseline approaches on the following two aspects.\n1) Performance of each semantic space: As shown in Table IV, MCSM-image means to perform cross-modal retrieval only by the cross-modal similarity simi in equation (10) generated from image semantic space, while MCSM-text means to use the cross-modal similarity simt only in equation (16) from text\nsemantic space. We can observe that MCSM-text has better accuracy than MCSM-image on Wikipedia dataset, while on Pascal Sentence dataset and XMediaNet dataset, MCSMimage outperforms MCSM-text in the average MAP score.\nThis is because of the imbalanced and complementary\nrelationships between different modalities that contain unequal amount of information. The respective result of each category is reported in Figure 11, taking Wikipedia and Pascal Sentence datasets as examples. Specifically, in Wikipedia dataset, the categories mostly lie in high-level semantics, such as\nhistory or literature, where the textual description contains more background information that cannot be presented by its corresponding image. As for the other two datasets, their categories mostly are specific objects, including animal such as elephant, or artifact such as airplane. Their corresponding textual descriptions are relatively simple, such as only 5 sentences to describe each image in Pascal Sentence dataset. Under this situation, the visual description contains more useful information than its corresponding text, which leads to the higher accuracy of MCSM-image. Besides, the final accuracy of MCSM stably outperforms MCSM-image and MCSM-text, which indicates that there exists reasonable complementarity between the two semantic spaces. Thus, from the above results, the motivation of this paper is fully verified.\n2) Performance of adaptive fusion on different semantic spaces: We also present the baseline experiments to verify the effectiveness of adaptive fusion on different semantic spaces. We compare our proposed adaptive fusion strategy with late fusion (MCSM-LF), which means to directly average the two kinds of cross-modal similarities generated from different semantic spaces by the following equation.\nsimlf (ip, tp) = 1\n2 (simi(ip, tp) + simt(ip, tp)). (24)\nThe results on 3 datasets are shown in Table V, where MCSM-LF means the accuracy calculated by late fusion. We can observe that adaptive fusion can further exploit the rich complementary information between the two semantic spaces to boost the accuracy of cross-modal retrieval.\nFrom the above baseline results, the separate contribution of each component in our proposed MCSM approach can be verified. First, the imbalanced and complementary relationships between different modalities are fully exploited\nin different semantic spaces. Second, the complementarity between different semantic spaces is fully captured by the adaptive fusion."}, {"heading": "V. CONCLUSION", "text": "In this paper, we have proposed a modality-specific crossmodal similarity measurement approach to construct independent semantic spaces for different modalities. First, recurrent attention network with attention based joint embedding loss is adopted to fully model the modality-specific characteristics within each modality, and utilize the imbalanced and complementary relationships between different modalities during correlation learning. Second, the end-to-end frameworks are implemented in different semantic spaces to directly generate the cross-modality similarity, integrating both common representation learning and distance metric learning to benefit each other. Third, the adaptive fusion is adopted to explore the complementarity between different semantic space. Experiments on 3 cross-modal datasets, including widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset, verify the effectiveness of our proposed approach compared with 9 state-of-the-art methods.\nAs for the future work, we attempt to extend the current framework to other modalities such as video, audio and so on, for exploring the imbalanced and complementary relationships across the data of multiple modalities. Besides, we attempt to transfer knowledge from external knowledge base to further boost the performance of cross-modal retrieval."}], "references": [{"title": "Neighborhood discriminant hashing for large-scale image retrieval", "author": ["J. Tang", "Z. Li", "M. Wang", "R. Zhao"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 24, no. 9, pp. 2827\u20132840, 2015.  IEEE TRANSACTIONS ON IMAGE PROCESSING  13", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Clip-based similarity measure for querydependent clip retrieval and video summarization", "author": ["Y. Peng", "C.-W. Ngo"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 16, no. 5, pp. 612\u2013627, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "An overview of cross-media retrieval: Concepts, methodologies, benchmarks and challenges", "author": ["Y. Peng", "X. Huang", "Y. Zhao"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM International Conference on Multimedia (ACM-MM), 2010, pp. 251\u2013260.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning discriminative binary codes for large-scale cross-modal retrieval", "author": ["X. Xu", "F. Shen", "Y. Yang", "H.T. Shen", "X. Li"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 26, no. 5, pp. 2494\u20132507, 2017.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "Multimodal discriminative binary embedding for large-scale cross-modal retrieval", "author": ["D. Wang", "X. Gao", "X. Wang", "L. He", "B. Yuan"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 25, no. 10, pp. 4540\u2013 4554, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-modal subspace learning via pairwise constraints", "author": ["R. He", "M. Zhang", "L. Wang", "Y. Ji", "Q. Yin"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 24, no. 12, pp. 5543\u20135556, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-view embedding space for modeling internet images, tags, and their semantics", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": "International Journal of Computer Vision (IJCV), vol. 106, no. 2, pp. 210\u2013233, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning cross-media joint representation with sparse and semi-supervised regularization", "author": ["X. Zhai", "Y. Peng", "J. Xiao"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 24, pp. 965\u2013 978, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint feature selection and subspace learning for cross-modal retrieval", "author": ["K. Wang", "R. He", "L. Wang", "W. Wang", "T. Tan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 38, no. 10, pp. 2010\u20132023, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Cross-modal learning to rank via latent joint representation", "author": ["F. Wu", "X. Jiang", "X. Li", "S. Tang", "W. Lu", "Z. Zhang", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 24, no. 5, pp. 1497\u20131509, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning of multimodal representations with random walks on the click graph", "author": ["F. Wu", "X. Lu", "J. Song", "S. Yan", "Z.M. Zhang", "Y. Rui", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing (TIP), vol. 25, no. 2, pp. 630\u2013642, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-media shared representation by hierarchical learning with multiple deep networks", "author": ["Y. Peng", "X. Huang", "J. Qi"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), 2016, pp. 3846\u20133853.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACM International Conference on Multimedia (ACM- MM), 2014, pp. 7\u201316.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep correlation for matching images and text", "author": ["F. Yan", "K. Mikolajczyk"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3441\u20133450.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 2048\u20132057.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2016, pp. 289\u2013297.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, pp. 321\u2013377, 1936.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1936}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J.C. Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 36, no. 3, pp. 521\u2013 535, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-label cross-modal retrieval", "author": ["V. Ranjan", "N. Rasiwasia", "C.V. Jawahar"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 4094\u20134102.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "ACM International Conference on Multimedia (ACM-MM), 2003, pp. 604\u2013611.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Regularization and semisupervised learning on large graphs", "author": ["M. Belkin", "I. Matveeva", "P. Niyogi"], "venue": "Annual Conference on Learning Theory (COLT), 2004, pp. 624\u2013638.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Heterogeneous metric learning with joint graph regularization for cross-media retrieval", "author": ["X. Zhai", "Y. Peng", "J. Xiao"], "venue": "AAAI Conference on Artificial Intelligence (AAAI), 2013, pp. 1198\u20131204.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised crossmedia feature learning with unified patch graph regularization", "author": ["Y. Peng", "X. Zhai", "Y. Zhao", "X. Huang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 26, no. 3, pp. 583\u2013596, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2012, pp. 1106\u20131114.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-stream multi-class fusion of deep networks for video classification", "author": ["Z. Wu", "Y. Jiang", "X. Wang", "H. Ye", "X. Xue"], "venue": "ACM International Conference on Multimedia (ACM-MM), 2016, pp. 791\u2013800.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML), 2011, pp. 689\u2013696.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning representations for multimodal data with deep belief nets", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning (ICML) Workshop, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J.A. Bilmes", "K. Livescu"], "venue": "International Conference on Machine Learning (ICML), 2013, pp. 1247\u20131255.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-modal retrieval with CNN visual features: A new baseline", "author": ["Y. Wei", "Y. Zhao", "C. Lu", "S. Wei", "L. Liu", "Z. Zhu", "S. Yan"], "venue": "IEEE Transactions on Cybernetics (TCYB), vol. 47, no. 2, pp. 449\u2013460, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Cross-modal retrieval via deep and bidirectional representation learning", "author": ["Y. He", "S. Xiang", "C. Kang", "J. Wang", "C. Pan"], "venue": "IEEE Transactions on Multimedia (TMM), vol. 18, no. 7, pp. 1363\u20131377, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2014, pp. 2204\u20132212.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 1462\u20131471.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 21\u201329.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Kocisk\u00fd", "P. Blunsom"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2015, pp. 1693\u20131701.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015, pp. 379\u2013 389.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR), 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-driven visual saliency and attention-based visual question answering", "author": ["Y. Lin", "Z. Pang", "D. Wang", "Y. Zhuang"], "venue": "arXiv preprint arXiv:1702.06700, 2017.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1997}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Conference on Neural Information Processing Systems (NIPS), 2013, pp. 3111\u20133119.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1746\u20131751.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Coarse2fine: Two-layer fusion for image retrieval", "author": ["G. Kong", "L. Dong", "W. Dong", "L. Zheng", "Q. Tian"], "venue": "arXiv preprint arXiv:1607.00719, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, 2010, pp. 139\u2013147.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedm\u00e1k", "J. Shawe-Taylor"], "venue": "Neural Computation, vol. 16, no. 12, pp. 2639\u20132664, 2004.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning consistent feature representation for cross-modal multimedia retrieval", "author": ["C. Kang", "S. Xiang", "S. Liao", "C. Xu", "C. Pan"], "venue": "IEEE Transactions on Multimedia (TMM), vol. 17, no. 3, pp. 370\u2013381, 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Different from the traditional single-modal retrieval, such as image retrieval [1] and video retrieval [2], which is limited in providing", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "Different from the traditional single-modal retrieval, such as image retrieval [1] and video retrieval [2], which is limited in providing", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "retrieval results of the same single modality with query, cross-modal retrieval is more flexible and useful to retrieve relevant multimodal information by submitting one query of any modality [3].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "To address this issue, some approaches [4], [5], [6], [7] have been proposed to project the features of different", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "Correlation Analysis (CCA) [4], [8], graph regularization [9], [10] and learning to rank [11], [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Such methods like [13], [14], [15] attempt to exploit the advantages of DNN in modeling nonlinear correlation to learn common representation with multilayer network.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Recently, attention mechanism has made great advances in DNN, which allows models to concentrate on the necessary fine-grained parts of visual or textual inputs, and has been successfully applied to various multimodal tasks, such as image caption [16] and visual question answering", "startOffset": 247, "endOffset": 251}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "As indicated in [3], we mainly introduce three categories of existing methods as follows, namely traditional statistical correlation analysis methods, cross-modal graph regularization methods and DNN-based methods.", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": "Canonical Correlation Analysis (CCA) [18], as one of the most representative works, is a natural solution to maximize the pairwise correlation between the data of different modalities such as image/text pairs [4].", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "Canonical Correlation Analysis (CCA) [18], as one of the most representative works, is a natural solution to maximize the pairwise correlation between the data of different modalities such as image/text pairs [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 18, "context": "[19] integrate semantic category labels to improve the performance of CCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Multi-view CCA [8] is proposed to construct a third view for modeling the high-level semantics.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "[20] propose multi-label CCA, which considers the high-level semantic information in the form of multi-label annotations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Besides, Cross-modal Factor Analysis (CFA) [21], as one of the alternative methods, is proposed to minimize the Frobenius norm between the data of different modalities after projecting them into one common space.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "2) Cross-modal graph regularization methods: Graph regularization [22] is widely used to construct a partially labeled graph for semi-supervised learning, which aims to enrich the training set and smooth the solution.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "[23] are the first to integrate graph regularization into cross-modal retrieval and propose Joint Graph Regularized Heterogeneous Metric Learning (JGRHML), which constructs the joint graph regularization term in the learned metric space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Furthermore, Joint Representation Learning (JRL) [9] is proposed to con-", "startOffset": 49, "endOffset": 52}, {"referenceID": 23, "context": "[24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "[24] further improve the previous works [9], [23] by constructing a unified hypergraph to learn the common space for up to five modalities, which also utilizes the fine-grained information at the same time.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "[10] also adopt multimodal graph regularization term to preserve inter-modality and intra-modality similarity relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 25, "context": "3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "3) DNN-based methods: Deep learning has made great advance in multimodal applications, such as image/video classification [25], [26] and object recognition [27].", "startOffset": 156, "endOffset": 160}, {"referenceID": 27, "context": "Bimodal Autoencoders (Bimodal AE) [28] is proposed as an extension of Restricted Boltzmann Machine (RBM) to model multiple modalities by minimizing the reconstruction error.", "startOffset": 34, "endOffset": 38}, {"referenceID": 28, "context": "[29] propose Multimodal Deep Belief Network (Multimodal DBN), which adopts two kinds of DBN for different modalities to model the distribution over their original features, while a joint RBM is adopted on the top of them to model the joint distribution and get the common representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Correspondence Autoencoder (Corr-AE) [14] and Deep Canonical Correlation Analysis (DCCA) [30] also consist of two subnetworks, while Corr-AE jointly models correlation and reconstruction information, and DCCA combines DNN with CCA to maximize the correlation on the top of two subnetworks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "Correspondence Autoencoder (Corr-AE) [14] and Deep Canonical Correlation Analysis (DCCA) [30] also consist of two subnetworks, while Corr-AE jointly models correlation and reconstruction information, and DCCA combines DNN with CCA to maximize the correlation on the top of two subnetworks.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "[13] propose Cross-media Multiple Deep Networks (CMDN) to model the intra-modality and inter-modality correlation in a two-stage learning framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] propose deep-SM to perform deep semantic matching with Convolutional Neural Network (CNN), which exploits the", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] adopt two convolution-based networks to model the matched and mismatched image/text pairs via deep and bidirectional representation learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] propose an attention based task-driven visual processing framework for image classification, which adopts Recurrent Neural Network (RNN) to adaptively select a sequence of regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] propose a spatial attention mechanism, which designs a sequential variational auto-encoding framework to perform image generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] propose Stacked Attention Networks (SANs) for image question answering, which can locate relevant image regions to the question with stacked attention model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] propose a word-by-word neural attention mechanism to reason over entailments of paired words or phrases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] develop a class of attention based deep neural networks, which learn to read and answer", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] propose a fully datadriven approach, which adopts a local attention based model to generate summarization according to the input sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Specifically, the network structure is configured the same as the layers before the last pooling layer (pool5) in 19-layer VGGNet [39].", "startOffset": 130, "endOffset": 134}, {"referenceID": 39, "context": "We compose these regions as a sequence, which are regarded as the results of eye movement when we glance at the image [40].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "Specifically, Long Short Term Memory (LSTM) network [41] is adopted, which is a special kind of RNN, with strong ability to learn long-term dependencies through the memory cell and the update gates as well as preserve previous time-steps information at the same time.", "startOffset": 52, "endOffset": 56}, {"referenceID": 41, "context": "Each word is represented by a k-dimensional vector extracted by Word2Vec [42] model, which is trained on billions of words in Google News and publicly available1.", "startOffset": 73, "endOffset": 77}, {"referenceID": 42, "context": "And a Word CNN is adopted on the input matrix, which has the same configuration with [43].", "startOffset": 85, "endOffset": 89}, {"referenceID": 41, "context": "First, the input text instance tp, which consists of n words, is represented as an n\u00d7k matrix, where each word is also represented as a k-dimensional vector extracted by Word2Vec [42] model.", "startOffset": 179, "endOffset": 183}, {"referenceID": 42, "context": "Following [43], we design fast convolutional networks for text (Word CNN), which is built by several combinations of convolution layer, threshold activation function layer and pooling layer.", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "Specifically, we also adopt LSTM network [41] to exploit such temporal dependency, which takes a sequence of text fragments as input.", "startOffset": 41, "endOffset": 45}, {"referenceID": 38, "context": "We still need to generate the image representation for each instance ip, which is also extracted from the last fully connected layer (fc7) in 19-layer VGGNet [39] with 4,096 dimensional number, and denoted as q p.", "startOffset": 158, "endOffset": 162}, {"referenceID": 43, "context": "Inspired by [44], we further attempt to explore the complementarity between different semantic spaces by adaptive fusion.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "ent semantic spaces are min-max normalized to [0, 1] respectively, which aims to overcome the influence of image/text pairs with too large similarity scores, and are defined as follows.", "startOffset": 46, "endOffset": 52}, {"referenceID": 3, "context": "\u2022 Wikipedia dataset [4], as the most widely-used dataset for cross-modal retrieval, is selected from \u201cfeatured articles\u201d in Wikipedia2 with 10 most populated categories, including history, biology and so on.", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "For fair and objective comparison purpose, we exactly follow the dataset partition strategy of [13], [14] to divide the dataset into 3 subsets: 2,173 pairs in training set, 231 pairs in validation set and 462 pairs in testing set.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "For fair and objective comparison purpose, we exactly follow the dataset partition strategy of [13], [14] to divide the dataset into 3 subsets: 2,173 pairs in training set, 231 pairs in validation set and 462 pairs in testing set.", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "\u2022 Pascal Sentence dataset [45] contains 1,000 images,", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "This dataset is categorized into 20 categories, and also following [13], [14], 800 documents are selected as training set, while 100 documents for testing and 100 documents for validation.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "This dataset is categorized into 20 categories, and also following [13], [14], 800 documents are selected as training set, while 100 documents for testing and 100 documents for validation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "For text, we convert each word in a document into a 300-dimension vector by Word2Vec [42] and generate vector sequences as text inputs.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 97, "endOffset": 100}, {"referenceID": 46, "context": "There are 5 traditional cross-modal retrieval methods, namely CCA [18], CFA [21], KCCA [46], JRL [9] and LGCFL [47].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "While the other 4 methods, Corr-AE [14], DCCA [15], CMDN", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "While the other 4 methods, Corr-AE [14], DCCA [15], CMDN", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "[13] and Deep-SM [31] are DNN-based methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[13] and Deep-SM [31] are DNN-based methods.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "\u2022 CCA [18] learns project matrices to maximize the correlation between the projected features of different modalities in one common space.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "\u2022 CFA [21] minimizes the Frobenius norm between the data of different modalities after projecting them into one", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": "\u2022 KCCA [46] uses kernel function to project the features into a higher-dimensional space, and then learns a common space by CCA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "\u2022 JRL [9] learns a common space by using semantic information, with semi-supervised regularization and sparse regularization.", "startOffset": 6, "endOffset": 9}, {"referenceID": 46, "context": "\u2022 LGCFL [47] jointly learns basis matrices of different modalities, by using a local group based priori in the", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "\u2022 Corr-AE [14] consists of two autoencoder networks coupled at the code layer to simultaneously model the reconstruction error and correlation loss.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "\u2022 DCCA [15] is a nonlinear extension of CCA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "\u2022 CMDN [13] adopts multiple deep networks to generate separate representations and learns the common representation with a stacked network.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "\u2022 Deep-SM [31] adopts convolutional neural network to perform deep semantic matching, which fully exploits the strong power of CNN image feature.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "of image is extracted from fc7 layer in 19-layer VGGNet [39], while CNN feature of text is extracted by Word CNN with the same configuration of [43].", "startOffset": 56, "endOffset": 60}, {"referenceID": 42, "context": "of image is extracted from fc7 layer in 19-layer VGGNet [39], while CNN feature of text is extracted by Word CNN with the same configuration of [43].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "MAP score considers the ranking of returned retrieval results as well as precision simultaneously, which is extensively adopted in cross-modal retrieval tasks, such as [4], [13].", "startOffset": 168, "endOffset": 171}, {"referenceID": 12, "context": "MAP score considers the ranking of returned retrieval results as well as precision simultaneously, which is extensively adopted in cross-modal retrieval tasks, such as [4], [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 13, "context": "It should be noted that the MAP score is calculated on all returned results, not only top 50 as adopted in [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "487 CMDN [13] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "457 Deep-SM [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "450 LGCFL [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "449 JRL [9] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "454 DCCA [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "422 Corr-AE [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "436 KCCA [46] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "414 CFA [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "318 CCA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "598 CMDN [13] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "535 Deep-SM [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "550 LGCFL [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "521 JRL [9] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "534 DCCA [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "539 Corr-AE [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "527 KCCA [46] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "467 CFA [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "473 CCA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "545 CMDN [13] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "501 Deep-SM [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "371 LGCFL [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "475 JRL [9] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "447 DCCA [15] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "429 Corr-AE [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "488 KCCA [46] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "261 CFA [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "326 CCA [18] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "Examples of the cross-modal retrieval results on XMediaNet dataset by our proposed MCSM approach as well as compared method CMDN [13].", "startOffset": 129, "endOffset": 133}], "year": 2017, "abstractText": "Nowadays, cross-modal retrieval plays an indispensable role to flexibly find information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities such as image and text have imbalanced and complementary relationships, which contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on Deep Neural Network (DNN) mostly construct one common space for different modalities to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Different from the existing works, we propose modality-specific cross-modal similarity measurement (MCSM) approach by constructing independent semantic space for each modality, which adopts end-to-end framework to directly generate modalityspecific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding to utilize the learned attention weights for guiding the fine-grained crossmodal correlation learning, which can capture the imbalanced and complementary relationships between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modalityspecific cross-modal similarities to perform cross-modal retrieval. Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset verify the effectiveness of our proposed approach, outperforming 9 state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}