{"id": "1708.05536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution", "abstract": "recent applications of neural language detection models have led to apparently an increased interest in revealing the automatic generation aspect of natural language. however impressive, nonetheless the intensity evaluation difficulty of neurally generated text has so it far remained though rather informal to and anecdotal. here, we help present an intense attempt at the systematic efficacy assessment evaluation of precisely one aspect of producing the quality improvement of transmitting neurally generated text. we focus deeply on a specific aspect of neural language generation : tracing its unique ability to automatically reproduce inconsistent authorial quality writing styles. using established models struggling for authorship attribution, additionally we empirically - assess remotely the structural stylistic qualities of typing neurally generated text. in comparison to any conventional dynamic language models, neural models repeatedly generate consistently fuzzier text recall that is relatively harder overall to automatically attribute correctly. nevertheless, our evolutionary results recently also suggest that neurally independently generated encoded text offers fewer more dramatically valuable perspectives for the augmentation qualities of training speech data.", "histories": [["v1", "Fri, 18 Aug 2017 08:43:52 GMT  (192kb,D)", "http://arxiv.org/abs/1708.05536v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["e manjavacas", "j de gussem", "w daelemans", "m kestemont"], "accepted": false, "id": "1708.05536"}, "pdf": {"name": "1708.05536.pdf", "metadata": {"source": "CRF", "title": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution", "authors": ["Enrique Manjavacas", "Jeroen De Gussem", "Walter Daelemans", "Mike Kestemont"], "emails": ["firstname@uantwerpen.be", "lastname@uantwerpen.be", "jedgusse.degussem@ugent.be"], "sections": [{"heading": "1 Introduction", "text": "In his landmark paper \u2018Computing Machinery and Intelligence\u2018, Turing (1950) quoted Jefferson\u2019s \u2018The Mind of Mechanical Man\u2019 (1949): \u2018Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain\u2019. Strikingly, these early pioneers of modern AI considered the conscious creation of literature as a significant milestone on the long road towards general AI. In recent years, the automated generation of text, such as literature, has received a significant impetus from research in the field of neural language modeling. A variety of recent studies have demonstrated that neural language models can be used to synthesize new (literary) text, even at the character-level.\nTo a surprising extent, neurally generated text seems to make an authentic impression on readers, due to its ability to mimic certain properties of the text on which it was trained, without it degrading into in a mere reproduction or patchwork of verbatim passages in it. In one particularly visible blog post, Karpathy (2015) demonstrated how a relatively simple character-level recurrent neural network, when trained on Shakespeare\u2019s oeuvre, was able to generate new, artificial text which, certainly in the eyes of non-experts, undeniably displayed some Shakespearean qualities. This blog has inspired a wide array of other applications \u2013 ranging from cooking recipes (Brewe, 2015) to Bach\u2019s sonatas (Feynman et al., 2016).\nMuch of this work has so far been published in the online blogosphere and the assessment of the quality of neurally generated text has often remained fairly informal and anecdotal, apart from a number of more empirically oriented studies, for instance in the field of hiphop lyric generation (Potash et al., 2015; Malmi et al., 2015). In this paper, we report an attempt at a systematic assessment of the properties of neurally generated text in the context of style-based authorship attribution in stylometry (Stamatatos, 2009). We address the following research questions: (1) To which extent is the text, neurally generated on the basis of a single author\u2019s oeuvre, still attributable to the original input author? and (2) To which extent is the neural generation of text useful for training data augmentation in stylometry, e.g. for authors for whom little reference data is available?\nBelow, we first present the model architectures underlying our text generation, comparing a modern neural architecture to a more conventional ngram-based language model. Next, we describe the Latin data set which we will use (Patrologia Latina) and discuss our experimental set-up (authorship attribution). We go on to present our attribution results; in the discussion section, we in-\nar X\niv :1\n70 8.\n05 53\n6v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 7\nterpret and visualize these results. We conclude by pointing out viable future improvements."}, {"heading": "2 Character-Level Text Generation", "text": "We approach the task of text generation with character-level Language Models (LM). In short, a LM is a probabilistic model of linguistic sequences that, at each step in a sequence, assigns a probability distribution over the vocabulary conditioned on the prefix sequence. More formally, a LM is defined by Equation 1,\nLM(wt) = P (wt|wt\u2212n, wt\u2212(n\u22121), ..., wt\u22121) (1)\nwhere n refers to the scope of the model \u2014i.e. the length of the prefix sequence taken into account to condition the output distribution at step t. By extension, a LM defines a generative model of sentences where the probability of a sentence is defined by the following equation:\nP (w1, w2, ..., wn) = n\u220f i P (wt|w1, ..., wt\u22121) (2)\nGiven its generative nature, a LM can easily be used for text generation. We start by sampling from the output distribution at step t and, then, we recursively feed back the sampled symbol, together with any other previous output, to condition the generative distribution at step t + 1. Equation 3 shows formally the text generation process for a symbol at step t where w\u2032t\u22121 is the generated symbol at step t\u22121 and S refers to any given sampling method.\nw\u2032t = S[P (wt|w\u2032t\u2212n, w\u2032t\u2212(n\u22121), ..., w \u2032 t\u22121)] (3)\nAn obvious approach towards sampling is to select the symbol that maximizes the probability of the entire generated sequence (argmax decoding). For a large vocabulary (e.g. in the case of a word-level LM), the search quickly becomes impractical and is usually approximated by means of beam search (including the extreme case of using a beamsize equal to 1, which corresponds to picking the most probable symbol at each step). However, when used for generation, the argmax decoding approach tends to yield repetitive and dull sentences, and eventually runs into dead-end loops. Therefore, we instead sample from the LM\u2019s output distribution at each step.\nThe sampling approaches discussed so far attempt to strike a trade-off between variability and\ncorrectness \u2013 in the sense of departure from regularities observed in the training data. Beam-search decoding will tend to generate sentences that are more formally correct (e.g. more similar to the sentences observed in the training corpus), while generating very similar and monotonous output in the presence of similar histories. Conversely, multinomial sampling will make the output diverge more from the original training data, and therefore produce a more varied output, but with a tendency towards more grammatically incorrect sentences. Focusing on multinomial sampling, the described trade-off can be operationalized in form of a parameter \u03c4 , mostly referred to as \u201ctemperature\", that is in charge of modifying the skewness of the parameters of the multinomial distribution to encourage more or less variability in exchange for potentially less or more formally correct output.1\nA further aspect of our LM approach to text generation is topical variation. In order to ensure that during generation the LM explores the topical distribution present in the training data, we implement the following procedure. After having generated a fixed number of sentences s, a sentence from the LM\u2019s training data is sampled uniformly and used to seed the generation of the next s sentences. Finally, we force the LM to generate fully terminated sentences by including endof-sentence symbols (EOS) during training time and discarding any output sentence that reaches a maximum number of characters m without having generated the EOS symbol \u2013 thus, we consider the generation of a single sentence finished whenever the EOS symbol is produced and we only generate sentences with a maximum number of charactersm. This is motivated by the fact that very long sentences tend to degenerate into poor-quality text. Our generative system displays a total of 3 generation hyper-parameters: \u03c4 (sampling temperature), s (reset seed every s sentences) and m (maximum m characters per sentence).\n1 Given the multinomial parameters p = {p1, p2, ..., pk} for a vocabulary size of V , the \u201cfreezing\u201d transformation p\u03c4i = p 1 \u03c4 i / \u2211V j p 1 \u03c4 j will flatten the original distribution for higher values of \u03c4 , thereby ensuring more variability in the output. Conversely, lower values of \u03c4 will skew it, thereby facilitating the outcome of the originally more probable symbol. For \u03c4 values approaching 0, we recover the simple argmax decoding procedure of picking the highest probability symbol at each step."}, {"heading": "2.1 Ngram-based Language Model", "text": "So far, we have kept the definition of the LM agnostic with respect to its concrete implementation. In the current study we compare two widely-used LM architectures \u2013 an ngram-based LM (NGLM) and a Recurrent Neural Networkbased LM (RNNLM). An NGLM is basically a conditional probability table for Equation 1 that is estimated on the basis of the count data for ngrams of a given length n. Typically, NGLMs suffer from a data sparsity problem since for a large enough value of n many possible conditioning prefixes will not be observed in the training data and the corresponding probability distribution will be missing. To alleviate the sparsity problem, two techniques\u2014smoothing and backoff models\u2014can be used that either reserve some probability mass and evenly redistribute it across unobserved ngrams (smoothing) or resort back to a lower-order model to provide an approximation to the conditional distribution of an unobserved ngram (backoff models). Here, however, we implement an unsmoothed LM since we only use the LM for generation, where it is not necessary to compute probabilities for unseen ngrams. An unsmoothed NGLM only has one model hyperparameter\u2014the ngram order."}, {"heading": "2.2 RNN-based Language Model", "text": "A RNNLM implements a language model using a Recurrent Neural Network (RNN) to allow left-toright information flow during sequence processing (Mikolov et al., 2010). As shown in (Bengio et al., 2003), at a given step t, a RNNLM (Elman, 1990) (i) first computes a distributed representation wt with dimensionality M of the input xt, (ii) it then feeds the resulting vector into an RNN layer that computes a hidden activation ht combining it with the hidden activation at the previous step ht\u22121, and (iii) it projects the hidden activation onto a space of dimensionality equal to the vocabulary size V , followed by a softmax function that turns the output vector into a valid probability distribution. More formally, given a binary column vector xt representing the input symbol at step t, we retrieve its corresponding embedding wt through wt = Wmxt, where Wm is the embedding matrix with dimensionality RMxV . The hidden state in the standard RNN is given by\nht = \u03c3(Wihwi +Whhht\u22121 + bh) (4)\nwhere Wih and Whh are respectively the input-tohidden and hidden-to-hidden projection matrices with dimensionality RMxH and RHxH , bh is a bias vector and \u03c3 is the sigmoid non-linear function. Finally, the probability assigned to each entry in the vocabulary at step t is defined by the softmax\nPt,j = eot,j\u2211V k e ot,k (5)\nwhere ot,j is the jth entry in the output vector ot = Whoht and Who is the hidden-to-output projection with dimensionality RHxV .\nIn practice, training an RNN is difficult due to the vanishing gradient problem (Hochreiter, 1998) that makes it hard to apply the back-propagation algorithm for parameter tuning over long sequences. Therefore, it is common to implement the recurrent layer using an enhanced RNN like, e.g. Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber, 1997). An LSTM-based RNNLM only differs from the previous RNNLM in the way the hidden activation ht is computed. An LSTM cell incorporates three learnable gates\u2014an input, forget and output gate\u2014of shape:\nit = \u03c3(W i ihwt +W i hh + b i h) (6)\nft = \u03c3(W f ihwt +W f hh + b f h) (7) ot = \u03c3(W o ihwt +W o hh + b o h) (8)\nwhereW i,W f andW o are, respectively, the gates parameters, and a writable memory cell ct that is updated following\nct = ft ct\u22121+it tanh(W cihwt+W chhht\u22121+bch) (9) (where is element-wise product and tanh is the hyperbolic tangent non-linear function). Finally, the memory cell is combined with the output gate to yield the hidden activation ht: ht = ot \u03c3(ct). As can be seen from the equations, the role of the gates is to learn to write to and delete from the memory cell based on the input (Equations 7, 6 and 9), as well as to use the memory cell to compute the hidden activation (Equation 2.2).\nA RNNLM has as parameter the embedding matrix Wm, the hidden-to-output projection Who, as well as the input-to-hidden and hidden-tohidden projections of the RNN/LSTM networks. Theoretically, what sets a RNNLM apart is that it consistently displays a much larger context awareness\u2014because of its ability to carry over information in the hidden state across very large\nspans\u2014and that it is therefore able to learn syntactic dependencies and structures from the training material. This is in stark contrast with a NGLM, which only reasons on the basis of a very local history and have little abstractive power.\nImportantly, however, it should be emphasized that most approaches to AA operate on very local features, such as lower-order character ngrams (Stamatatos, 2013; Sapkota et al., 2015; Kestemont, 2014). Most state-of-the-art models for AA indeed depend on document vectors containing normalized character ngram frequencies (typically in the range of 2-4), which are fed to a standard classifier, such as a support-vector machine with a linear kernel. The fact that the RNNLM might generate more realistic sentences than the NGLM does not necessarily entail that it would have an advantage in AA with respect to a conventional NGLM, which will stay closer to the original source documents. An important, if only secondary, question is therefore whether the use of an RNNLM in the context of AA would outperform a conventional NGLM, even if only very local features, such as character ngrams, are included in the model."}, {"heading": "3 Experimental setup", "text": ""}, {"heading": "3.1 Design", "text": "The Patrologia Latina (PL) is a corpus containing texts of Latin ecclesiastical writers in 221 volumes ranging a time span of 10 centuries, from Late Antiquity to the High Middle Ages (3rd13th century). It was first published in two series halfway the 19th century by Jacques-Paul Migne, who mainly based the texts off of 17th and 18thcentury prints. Its digitized version is available since 1993, and it has remained one of the most sizable Latin corpora online (\u00b1113M words).\nPerforming this experiment on the PL, and not on an English corpus, for instance, has been a conscious decision to raise the bar. It has been observed that state-of-the-art AA on an inflected language such as Latin yields poorer results when it is reliant on most frequent words (Eder and Rybicki, 2011). Moreover, the Latin that has come down to us from the 1st century AD onwards is an institutionalized literary language, hardly a natural language, showing only far resemblance, or occasionally no resemblance at all, to the writer\u2019s mother tongue (Maes, 2009). Tracing stylistic properties within a heavily formalized language,\nand attempting to resuscitate these through generation, is therefore challenging. An additional obstacle for both language generation as AA is that many of the PL\u2019s authors cite from similar, authoritative sources such as the Bible or the church fathers\u2019 precursory texts, thereby having in common an ecclesiastical vocabulary that could complicate the detection of stable writing style patterns.\nNot all authors in the PL have been equally prolific. These circumstances considerably limit the set of authors for whom our task is suited (Eder, 2015). We set the condition that our text data include only texts by authors who dispose of at least 20 authentic, individual documents each. As such we favored document counts over token counts, and lexical variety over mere word quantity. A list of the 18 most prolific authors, their number of documents and the respective average length of these documents is given in Fig. 1.\nIt is not trivial to design an experiment that allows us to study the behavior of generated text in the context of AA. Fig. 2 shows the experimental setup which we propose, and in which we attempt to maximize the comparability of both generated and authentic data. We start by splitting the full corpus into two equal-size document collections (stratified at the author level), \u03b1 and \u03c9. Only \u03b1 will be used to train a LM, which then generates a\nthird collection of synthetic documents. For each author in \u03b1 and \u03c9, we aggregate all documents into a list of sentences per sub-corpus. From these collections, we create 20 documents containing at least 5,000 words to create \u03b1 and \u03c9, through randomly sampling sentences (without replacement) from the author\u2019s sentence collection. For the creation of \u03b1\u0304, we would also create 20 artificial 5,000-word documents, but this time through sampling new sentences from the LM. This approach has its limitations, because we limit and balance the available data to a considerable extent. Furthermore, the sampling procedure implies an underestimation in attribution performance, since it strips away all supra-sentential information. Nevertheless, this setup guarantees that the authentic and generated corpora are maximally comparable in terms of number of documents, document length, topical diversity and style mixture\u2014which is our focus in the present study.\nSubsequently, 5 classification experiments are defined, where we train and and test on different 2-way combinations of the 3 datasets. In a first pair of experiments, < \u03b1,\u03c9 > and < \u03c9,\u03b1 >, we train and test a classifier on the authentic datasets to assess the classifier\u2019s performance under natural conditions. (Note that we apply the classifier in both directions to account for any directionality artifacts.) In a third experiment, we train and test a classifier on the generated data only (< \u03b1\u0304, \u03b1\u0304 >) to establish to which extent the generated data preserves the data\u2019s stylistic structure at the author level (i.e. auto-classification). Fourthly, we conduct an experiment where we train on the generated data in \u03b1\u0304 and test on the authentic data in \u03c9 (< \u03b1\u0304, \u03c9 >). This allows us to verify whether the generated documents retain enough stylistic information to correctly attribute authentic documents. Finally, we train a classifier on the authentic data in \u03c9 and test it on \u03b1\u0304: this setup (< \u03c9, \u03b1\u0304 >) allows to assess whether a classifier, trained on authentic data is still able to correctly attribute the generated materials.\nIn addition, we conduct a final experiment which can be characterized from the point of view of self-learning or co-learning (Mihalcea, 2004)\u2014 a semi-supervised learning technique where a core of training data is expanded with examples from a related but unlabeled dataset that can be classified with high confidence by a classifier trained in the original labeled dataset. In this experiment we\ncompare the NGLM and RNNLM models with respect to their capacity to boost attribution performance by adding synthetic examples to the original training set\u2014which might be a valuable strategy for real-life experiments. Specifically, we perform attribution on \u03c9 using a combination of\u03b1 and \u03b1\u0304 as training data (< \u03b1+ \u03b1\u0304, \u03c9 >)."}, {"heading": "3.2 Language Model Architectures for Text Generation", "text": "In Section 2, the text-generation and model parameters were defined. For the present experiments we generate 20 documents of 5000 words each using a \u03c4 value of 1 and am value estimated on each author\u2019s dataset. For the RNNLM we reset the seed (parameter s) every 10 successfully generated sentences, whereas for the NGLM we do it after every sentence. This asymmetry is motivated by the fact that NGLM the output distribution of an NGLM at each step is much more skewed and therefore sentences generated from the same seed tend to be be much less varied. For model fitting we set the NGLM order at 6, which, on a subjective evaluation, seemed a sufficiently large value for the comparatively small size of the datasets.\nFor the RNNLM models the following parameter settings were selected. Embedding dimen-\nsionality M was set to 24, the hidden layer dimension was 200 and we stacked up 2 LSTM layers to encourage the model to learn more abstract representations. Parameters were chosen based on common practice and reasonable defaults without further hyperparameter search. Each model was trained during 50 epochs using the adaptive variant of Stochastic Gradient Descent Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001. We set a small batch size of 50, preferring stability over speed during training. Moreover, we clip the gradients before each batch update to a maximum norm value of 5 to avoid the exploding gradients following (Pascanu et al., 2013) and truncate the gradient back-propagation after 50 recurrent steps. We also applied 30% dropout after each recurrent layer following (Zaremba et al., 2015) to avoid overfitting. For each RNNLM we held out a validation set using 10% of the data to monitor and evaluate training. We ensured that validation perplexity was always lower than train perplexity. Average validation perplexity was 4.015 with a standard deviation of 0.183.2"}, {"heading": "3.3 Attribution as Classification", "text": "For the AA classification as described in the experimental setup of section 3, we use a linear SVM classifier (Diederich, 2003). We extract shallow linguistic features in the form of Tfidfweighted character ngrams (from bigrams to fourgrams) as style markers by which to determine authorship. Note that the feature extraction of ngrams in the order of 2 to 4 might have important repercussions, since NGLM training fully focuses on capturing that particular distribution, whereas the more expressive RNNLM models full sequences. Furthermore, we refrain from using word-level features such as word ngrams or POS tags, since this would introduce a further asymmetry in the comparison given that the RNNLM can generate unseen words whereas the NGLM can not. The model accuracy of the SVM is finetuned by searching over different value ranges for the SVM\u2019s parameters. The number of features is set to range from 1,000 to 30,000 max features for each fit, more specifically in the following order: 5,000, 10,000, 15,000 and 30,000 features. For the C-parameter of the SVM we search over values of respectively 1, 10, 100 and 1,000.\n2 All software associated with this paper is available from https://www.github.com/jedgusse/ project_lorenzo."}, {"heading": "4 Results", "text": ""}, {"heading": "4.1 Examples of Generated Language", "text": "What follows are two short extracts from the respective outputs of an NGLM and RNNLM trained on Augustine (A.H.) (most prolific author of the dataset, see Table 1), which gives an anecdotal intuition of how the output of these language models differs.\nNgram-based LM (\u03b1\u0304)\n(1) * Sed Yet uis you wish uenire: to come quod since postridie, tomorrow\nascensiones ascensions honora honoured pastorem, the shepherd, nec and not sane completely reipublicos republican idem the same testis witness et also implebitur will be fulfilled tamen nevertheless mentiendum to be deceived sit it may be propitiaberis. you will be enriched.\nRNN-based LM (\u03b1\u0304)\n(2) * Et And idam that same (?) precepti, commandment, siue be it\nad towards sensum the feeling noui: I know: nonuulde not enough (?) sunt are enim after all Filius the Son Domini of our Lord substantia, our substance, sed but non none sunt are there qui who secururum amongst the untroubled superbia through pride et also perrectus righteous est, are, mortalis mortal includendi by including estus fire que and fiumus we were (?) propter because of illam this uideantur. may they be beheld.\nThe extract of RNNLM-generated text as compared to the NGLM demonstrates how the RNNLM is better at reproducing a syntactic logic (which moreover makes translation easier). Note, for instance, how the nominative of the relative pronoun qui is maintained towards the end of the subordinate clause in the participle perfect perrectus, and even seems to be carried on in the next clause as opposed to the awkwardly placed quod in the ngram-based extract. The RNNLM is also arguably better at positioning the verbs in the clauses. Compare, for instance, the NGLM\u2019s dense verbal sequence implebitur tamen mentiendum sit propitiaberis. Finally, the RNNLM is more apt at generating plausible neologisms. Examples include idam (cfr. idem and quidam), fiumus (cfr. fiemus), secururum (cfr. securus and the genitive ending -orum and -arum). To a human reader, the RNNLM produces superficially more convincing text."}, {"heading": "4.2 Attribution results", "text": "The results of the attribution experiments are presented in Table 1 in terms of recall, precision and F1-scores and the distributions are visualized in Fig. 3. We focus on the macro-averaged F1-scores in our discussion, although one should not forget that the scores vary considerably over individual authors (cf. Fig. 3). With respect to the authentic data, classifying \u03b1 on the basis of \u03c9 is slightly more difficult than the reverse direction, which seems a negligible directionality artifact. When we use the generated data as training material to classify authentic material < \u03b1\u0304, \u03c9 > , we see that the F1-scores drop significantly for both LMs, although the NGLM seems more robust in this re-\nspect. Interestingly, the drop is much less significant for the opposite situation, where we train on authentic material and classify generated material < \u03c9, \u03b1\u0304 >. This suggests that enough stylistic information is preserved in the generated text to attribute it to the original author, but that this information in isolation does not suffice to train a convincing attribution system on. When used in isolation, the NGLM outperforms the RNNLM in both setups. However, the situation is clearly different for the augmentation or self-learning setup (< \u03b1+ \u03b1\u0304, \u03c9 >)\u2014c.f. Section 3\u2014, where we train an attributor on the combination of \u03b1 and \u03b1\u0304, and test it on the authentic \u03c9 set. Here, we see that the RNNLM performs better than the NGLM in the corresponding experiment \u2013 the NGLM in fact even performs worse in this case than in the normal < \u03b1,\u03c9 > setup."}, {"heading": "4.3 Discussion", "text": "To understand the difference in behavior between both LMs, it is useful to inspect Fig. 4. Here, we use a Principal Components Analysis (Binongo and Smith, 1999) to visualize 2500-word samples for 3 three most prolific authors (Augustine of Hippo, Honorius of Autun, and Gregory the Great) using the 150 most common ngrams. We include a mixture of authentic \u03c9 data and generated \u03b1\u0304 data for each author, comparing the NGLM and the RNNLM. The plots shows that NGLM produces text samples which lie very close in ngram frequencies to the authentic data, whereas the texts produced by the RNNLM follow a markedly different distribution than \u03c9 \u2013 this difference is very outspoken for Augustine, for instance. As might be expected on the basis of the observation in sec-\ntion 3.3, the NGLM thus produces data that stays very close to the original input, whereas RNNLM yields fuzzier texts, that follow a slightly different distribution. This explains why it is, for example, easier to train an attributor on the data generated by an NGLM than an RNNLM.\nConversely, our results show that the situation is different in the data augmentation setup, where we train an attributor on the combination of \u03b1 and \u03b1\u0304 and test it on the authentic \u03c9 set. In this case, the NGLM performs worse than in the corresponding the non-augmented setup, whereas the performance of the RNNLM sensitively increases. Arguably, the fuzziness of the RNNLM-generated data adds an interesting complexity to the original\ncore of authentic data, which can be exploited by the classifier.\nB.C\nB\nA\nW.S\nH.S\nA.C\nA.H\nP.D\nA.M\nR.M\nWord-level Char-level\nB.C\nB\nA\nW.S\nH.S\nA.C\nA.H\nP.D\nA.M\nR.M\nR .M A .M P .D A .H A .C H .S W .S A B B .C\nB.C\nB\nA\nW.S\nH.S\nA.C\nA.H\nP.D\nA.M\nR.M\nR .M A .M P .D A .H A .C H .S W .S A B B .C\n0.02 0.00 0.02 0.05 0.00 0.05\nFigure 5: Mean-normalized Jaccard similarity scores between 10 most prolific authors using word (left column) and character (right column) bigrams to fourgrams, comparing real data (first row), RNNLM-synthetic data with real data (second row) and NGLM-synthetic data with real data (third row).\nWhile these results indirectly show that the RNNLM did not simply overfit on \u03b1, it is an interesting question to which extent \u03b1 and \u03b1\u0304 display (lexical) overlap in the case of both LMs. If the overlap would indeed be larger for the NGLM than the RNNLM, this would support our interpretation. In Fig. 5, we show mean-normalized, pairwise Jaccard similarities for the 10 most prolific authors in both \u03b1 and \u03b1\u0304 for each LM. The dark diagonals in the second row of the heatmaps visually support the observation that the NGLM displays a much more outspoken overlap between \u03b1 and \u03b1\u0304. Such an effect is much more faint in the case of\nthe RNNLM and in this respect it remains more faithful to the real data (first row for \u03b1 and \u03c9)."}, {"heading": "5 Conclusion", "text": "Our preliminary results confirm that the texts generated by a traditional NGLM are relatively \u2018dull\u2019 and \u2018conservative\u2019 in the sense that they stay relatively close to the local distribution of the source data on which they were trained. Conceptually, the RNNLM has a clear advantage in terms of expressiveness and capacity with respect to the NGLM. In practice, given the small size of AA datasets, an underfitted RNNLM yield fuzzier examples, which explains why the NGLM outperforms the RNNLM when the classifier is restricted to the generated data (< \u03b1\u0304, \u03c9 > and < \u03c9, \u03b1\u0304 >). At the same time, the training data augmentation setup (< \u03b1+ \u03b1\u0304, \u03c9 >) shows that whereas NGLMgenerated data adds comparatively little to the authentic data\u2014reproducing a subset of the original feature distribution, as shown in Fig. 5\u2014, the RNNLM-generated data presents a valuable data contribution which does result in an absolute increase in attribution performance with respect to the real classification setup < \u03b1,\u03c9 >. Although further research into the matter is needed, this clearly suggests that the complexity of the RNNLM data is useful for training data augmentation, arguably capturing stylistic nuances which a simpler LM cannot.\nIn the future, we will explore the flexibility of the general RNNLM framework to develop generative architectures that better capture the style of the training data. In particular, following (Linzen et al., 2016) we hypothesize that forcing the RNN to model more linguistic structure\u2014e.g. jointly modeling words and POS-tags\u2014, should result in better language generation and better style preservation. Furthermore, we plan on exhaustively testing the capabilities of author-specific generative models for self-learning in AA, investigating the effect of adding different amounts of synthetic data and selectively adding synthetic data based on the confidence with which it can be correctly classified by a classifier trained on real data.\nAdditionally, we would like to investigate pre-training in out-of-domain data as well as more compact ways of modelling author-specific language\u2014such as conditional language models (Tang et al., 2016)\u2014as means to alleviate underfitting of the RNN models on small datasets."}, {"heading": "A Author names with abbreviations", "text": "Abbrv Author\nH.S Hieronymus Stridonensis G.I Gregorius I A.H Augustinus Hipponensis A.M Ambrosius Mediolanensis B Beda H.C Hildebertus Cenomanensis H.d.S.V Hugo de S- Victore R.T Rupertus Tuitiensis W.S Walafridus Strabo T Tertullianus P.D Petrus Damianus H.A Honorius Augustodunensis H.R Hincmarus Rhemensis B.C Bernardus Claraevallensis A Alcuinus R.M Rabanus Maurus A.C Anselmus Cantuariensis R.S.V Richardus S- Victoris"}], "references": [{"title": "Pascal Vincent", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme"], "venue": "and Christian Janvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Wilfrid A", "author": ["M Jos\u00e9 Nilo G. Binongo"], "venue": "Smith.", "citeRegEx": "Binongo and Smith1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Do androids dream of cooking? https://gist.github.com/ nylki/1efbaa36635956d35bcc", "author": ["Tom Brewe"], "venue": null, "citeRegEx": "Brewe.,? \\Q2015\\E", "shortCiteRegEx": "Brewe.", "year": 2015}, {"title": "Authorship attribution with support vector machines", "author": ["Joachim Diederich"], "venue": "Applied Intelligence,", "citeRegEx": "Diederich.,? \\Q2003\\E", "shortCiteRegEx": "Diederich.", "year": 2003}, {"title": "Deeper delta across genres and languages: Do we really need the most frequent words", "author": ["Eder", "Rybicki2011] Maciej Eder", "Jan Rybicki"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Eder et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eder et al\\.", "year": 2011}, {"title": "Taking stylometry to the limits: Benchmark study on 5,281 texts from \"patrologia latina", "author": ["Maciej Eder"], "venue": "In Digital Humanities 2015: Conference Abstracts,", "citeRegEx": "Eder.,? \\Q2015\\E", "shortCiteRegEx": "Eder.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Mathew Johnson", "author": ["Liang Feynman", "Mark Gotham", "Marcin Tomczak"], "venue": "and Jaimie Shotton.", "citeRegEx": "Feynman et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "The mind of mechanical man", "author": ["Geoffrey Jefferson"], "venue": "British Medical Journal,", "citeRegEx": "Jefferson.,? \\Q1949\\E", "shortCiteRegEx": "Jefferson.", "year": 1949}, {"title": "The unreasonable effectiveness of recurrent neural networks. http://karpathy.github.io/ 2015/05/21/rnn-effectiveness", "author": ["Andrej Karpathy"], "venue": null, "citeRegEx": "Karpathy.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy.", "year": 2015}, {"title": "Function words in authorship attribution. from black magic to theory", "author": ["Mike Kestemont"], "venue": "In Proceedings of the 3rd Workshop on Computational Linguistics for Literature,", "citeRegEx": "Kestemont.,? \\Q2014\\E", "shortCiteRegEx": "Kestemont.", "year": 2014}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Emmanuel Dupoux", "author": ["Tal Linzen"], "venue": "and Yoav Goldberg.", "citeRegEx": "Linzen et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuity through appropriation", "author": ["Yanick Maes"], "venue": "Latinitas Perennis. Volume II: Appropriation and Latin Literature,", "citeRegEx": "Maes.,? \\Q2009\\E", "shortCiteRegEx": "Maes.", "year": 2009}, {"title": "Tapani Raiko", "author": ["Eric Malmi", "Pyry Takala", "Hannu Toivonen"], "venue": "and Aristides Gionis.", "citeRegEx": "Malmi et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Co-training and self-training for word sense disambiguation", "author": ["Rada Mihalcea"], "venue": "In CoNLL,", "citeRegEx": "Mihalcea.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea.", "year": 2004}, {"title": "Jan Cernock\u00fd", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget"], "venue": "and Sanjeev Khudanpur.", "citeRegEx": "Mikolov et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Alexey Romanov", "author": ["Peter Potash"], "venue": "and Anna Rumshisky.", "citeRegEx": "Potash et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Manuel Montes", "author": ["Upendra Sapkota", "Steven Bethard"], "venue": "and Thamar Solorio.", "citeRegEx": "Sapkota et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of modern authorship attribution methods", "author": ["Efstathios Stamatatos"], "venue": "Journal of the American Society For Information Science and Technology,", "citeRegEx": "Stamatatos.,? \\Q2009\\E", "shortCiteRegEx": "Stamatatos.", "year": 2009}, {"title": "On the robustness of authorship attribution based on character n-gram features", "author": ["Efstathios Stamatatos"], "venue": "Journal of Law and Policy,", "citeRegEx": "Stamatatos.,? \\Q2013\\E", "shortCiteRegEx": "Stamatatos.", "year": 2013}, {"title": "Ming Zhang", "author": ["Jian Tang", "Yifan Yang", "Sam Carton"], "venue": "and Qiaozhu Mei.", "citeRegEx": "Tang et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ilya Sutskever", "author": ["Wojciech Zaremba"], "venue": "and Oriol Vinyals.", "citeRegEx": "Zaremba et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.", "creator": "LaTeX with hyperref package"}}}