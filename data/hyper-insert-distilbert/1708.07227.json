{"id": "1708.07227", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Proportionate gradient updates with PercentDelta", "abstract": "deep neural networks fibers are generally trained manually using iterative gradient updates. magnitudes of gradients which are repeatedly affected independently by many macro factors, including choice of activation functions and initialization. more importantly, stationary gradient magnitudes can greatly or differ further across layers, dramatically with some layers receiving sufficiently much drastically smaller gradients than others. causing some stationary layers to temporarily train slower than others and corresponding therefore sometimes slowing down the intrinsic overall convergence. similarly we then analytically explain regarding this disproportionality. if then we propose way to explicitly train next all possible layers at the fundamental same critical speed, denoted by scaling the gradient w. r. t. every exactly trainable rotation tensor will to be proportional to its absolute current repetition value. in doing particular, glance at virtually every new batch, if we want to update on all equal trainable diagonal tensors, ideally such that the relative change of plus the reciprocal l1 - r norm of calculating the inverse tensors is the overall same, across all orthogonal layers areas of both the network, throughout training time. experiments on mnist show that our iteration method continuously appropriately scales gradients, such simply that if the actual relative change in trainable tensors is conserved approximately equal also across layers. studies in addition, measuring the improved test accuracy associated with careful training instruction time, shows that our method trains systems faster than our other methods, giving higher consistent test accuracy given same budget of proper training iteration steps.", "histories": [["v1", "Thu, 24 Aug 2017 00:20:16 GMT  (277kb,D)", "http://arxiv.org/abs/1708.07227v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sami abu-el-haija"], "accepted": false, "id": "1708.07227"}, "pdf": {"name": "1708.07227.pdf", "metadata": {"source": "CRF", "title": "Proportionate gradient updates with PercentDelta", "authors": ["Sami Abu-El-Haija"], "emails": ["haija@google.com"], "sections": [{"heading": "1 Introduction", "text": "Weights of Deep Neural Networks (DNNs) are commonly randomly initialized. The j-th layer\u2019s weight matrix Wj is initialized from a distribution that is generally conditioned on the shape of Wj .\nFeed-forward DNNs generally consist of a series of matrix multiplications and element-wise activation functions. For example, input vector x0 is transformed to the output vector xL by an L-layer neural network h\u03b8(x0) = xL, depicted as:\nx0\nW0\n\u00d7 \u03c3\nW1\n\u00d7 \u03c3 . . .\nWL\u22121\n\u00d7 \u03c3 xL\nWhere \u00d7 is a matrix-multply operator, \u03c3 is an element-wise activation (e.g. logistic or ReLu). The DNN parameters \u03b8 = {W0,W1, . . . ,WL\u22121} are optimized by a training algorithm, such as Stochastic Gradient Descent (SGD), which iteratively applies gradient updates:\nW (t+1) j := W (t) j \u2212 \u03b7 \u03b3(t)\n\u2202J\n\u2202Wj , (1)\nwhere W (t)j is the value of the j-th layer weight matrix Wj at timestep t, \u03b7 \u2208 R is the learning rate, the decay function \u03b3 : R\u2192 [0, 1], generally decreases with time t, and \u2202J\u2202Wj is the partial gradient of training objective J w.r.t. Wj , evaluated at time t for a batch of training examples. For notational convenience, we define the delta SGD as: \u2206SGDW = \u03b7 \u03b3(t) \u2202J\u2202W\nar X\niv :1\n70 8.\n07 22\n7v 1\n[ cs\n.L G\n] 2\n4 A\nug 2\n01 7\nUnder usual circumstances, the magnitudes of gradients can widely vary across layers of a DNN, causing some trainable tensors to change much slower than others, slowing down overall training. Several proposed methods metigate this problem, including utilizing: per-parameter adaptive learning rate such as AdaGrad (Duchi et al., 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010). These methods heuristically attack the disproportionate training problem, which we justify in section 2. In this paper, we propose to directly enforce proportionate training of layers. Specifically, We propose a gradient update rule that moves every weight tensor in the direction of the gradient, but with a magnitude that changes the tensor value by a relative amount. We use the same relative amount across all layers, to train them all at the same speed.\nThe remainder of the paper is organized as follows. In Section 2 we illustrate the disproportionate training phenomena on a (toy) hypothetical 4-layer neural network, by expanding gradient terms using BackProp Rumelhart et al. (1986). In Section 3, we summarize related work. Then, we introduce our algorithm in Section 4. We show experimental results on MNIST in Section 5. In Section 6, we discuss where PercentDelta could be useful and potential future direction. Finally, we conclude our findings in Section 7."}, {"heading": "2 Disproprtionate Training", "text": "We use a toy example to illustrate disproportionate training across layers. Assume a 4-layer network with trainable weight matrices {W0,W1,W2,W3} and output vector x4. The gradient of the objective w.r.t. output vector x4 can be directly calculated from the data e.g. using cross-entropy loss. We write-down the gradient of the objective J w.r.t. the last layer\u2019s weight matrix W3 as:\n\u2202J\n\u2202W3 =\n[ \u03c3\u2032(W3 \u00d7 x3) \u25e6 \u2202J\n\u2202x4\n] \u00d7 xT3 , (2)\nwhere \u03c3\u2032() is the derivative of \u03c3() w.r.t. its input, and \u25e6 is the Hadamard product. We also write-down the gradient w.r.t. W1 and W2, then expand the expressions using the Back Propagation Algorithm (Rumelhart et al., 1986):\n\u2202J\n\u2202W2 = \u03c3\u2032(W2 \u00d7 x2) \u25e6 \u2202J\u2202x3\ufe38\ufe37\ufe37\ufe38 = \u00d7 xT2\n= \u03c3\u2032(W2 \u00d7 x2) \u25e6 \ufe37 \ufe38\ufe38 \ufe37[ WT3 \u00d7 [ \u03c3\u2032(W3 \u00d7 x3) \u25e6 \u2202J\n\u2202x4 ]]\u00d7 xT2 \u2202J\n\u2202W1 = \u03c3\u2032(W1 \u00d7 x1) \u25e6 \u2202J\u2202x2\ufe38\ufe37\ufe37\ufe38 = \u00d7 xT1\n= \u03c3 \u2032(W1 \u00d7 x1) \u25e6 \ufe37 \ufe38\ufe38 \ufe37WT2 \u00d7 \u03c3\u2032(W2 \u00d7 x2) \u25e6 [ WT3 \u00d7 [ \u03c3\u2032(W3 \u00d7 x3) \u25e6 \u2202J \u2202x4 ]] \ufe38 \ufe37\ufe37 \ufe38\n=\u2202J/\u2202x3\n  \u00d7 x T 1\nNote the following in the above equations: First, the derivatives \u2202J\u2202W3 , \u2202J \u2202W2 , \u2202J\u2202W1 look similar. In fact, they are almost sub-expressions of one another, with the exception of the right-most row-vector xTj . Second, all quantities in brackets are column-vectors. The gradient matrix \u2202J\u2202Wj is determined by an outer product of a column-vector (in brackets) times a row-vector xTj .\nWhat can we conclude about the magnitudes (e.g. the L1 norm) of \u2202J\u2202W3 , \u2202J \u2202W2 , \u2202J\u2202W1 ? Each \u2202J \u2202Wj is calculated using three types of multiplicands: \u03c3\u2032(.), WT. , and x T j . Therefore, the magnitudes of \u2202J \u2202Wj are affected by:\n1. The value of the derivative of \u03c3\u2032(), which is evaluated element-wise. If \u03c3\u2032() is generally less than 1, then we can expect the gradient to be smaller for earlier layers than later ones, since they are multiplied by \u03c3\u2032() more times. If it is generally greater than 1, then we can expect the gradient to be larger for earlier layers. For very deep networks (recurrent or otherwise), the former situation can cause the gradients to vanish, while the latter can cause the gradients to explode. ReLu mitigates this problem, as its derivative = 1, in locations where input is positive.\n2. The magnitude of W \u2019s. In practice, all W \u2019s are initialized from the same distribution. If the L1-norms of rows in WTj are less than 1, then we should expect ||WTj \u00d7 \u03c3\u2032(z)|| < ||\u03c3\u2032(z)||, yielding smaller gradients for earlier layers. Otherwise, if the row L1 norms are greater than 1, then earleir layers should receive larger gradient magnitudes. Weight normalization (Salimans and Kingma, 2016) and intelligent initialization schemes (e.g. Glorot and Bengio, 2010) can mitigate this problem.\n3. The norm of the row-vector xj . If in the forward pass, the activations consistently grow (e.g. unbounded activation) with subsequent layers, then later layers will receive larger gradients. BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al., 2016) mitigate this problem."}, {"heading": "3 Related Work", "text": ""}, {"heading": "3.1 Adaptive Gradient", "text": "Duchi et al. (2011) proposed AdaGrad. A training algorithm that keeps a cumulative sum-of-squared gradients (i.e. second moment of gradients):\nS (t) j = S (t\u22121) j +\n( \u2202J\n\u2202Wj \u25e6 \u2202J \u2202Wj\n) , (3)\nThen divides (element-wise) the gradient by the square-root of the sum:\nW (t+1) j := W (t) j \u2212 \u03b7 \u03b3(t)\n( \u2202J \u2202Wj \u25e6 ( S (t) j )\u22121/2) , (4)\nor equivalently \u2206AdaGradWj = \u03b7 \u03b3(t) \u2202J\u2202Wj \u25e6 ( S (t) j )\u22121/2 , where the (.)\u22121/2 power operator is applied element-wise. In essense, if some layer receives large gradients, then they will be normalized to smaller gradients through the division. However, a weakness in AdaGrad is that at some point, the S will grow too large, effectively making (S)\u22121/2 \u2248 0 and therefore slowing or halting training. Ba and Kingma (2015) propose Adam, which keeps exponential decaying sums, of gradients and of square gradients, respectively known as first and second moments. Adam offers two benefits over AdaGrad. First, its decaying sums should not grow to infinity and therefore training should not halt. Second, the exponential-decay averaging was shown to speed up training (Momentum, Sutskever et al., 2013). For details, we point readers to the Adam paper (Ba and Kingma, 2015)."}, {"heading": "3.2 LARS", "text": "You et al. (2017) propose to normalize every layer\u2019s gradients by the ratio of L2 norms of the parameter and the gradient. Namely, they propose the gradient update rule:\n\u2206LARSWj = \u03b7 \u03b3(t)\n \u2223\u2223\u2223\u2223\u2223\u2223W (t)j \u2223\u2223\u2223\u2223\u2223\u2223 2\n||(\u2202J/\u2202Wj)||2  \u2202J \u2202Wj , (5)\nwhich effectively normalizes the gradient \u2202J\u2202Wj to be unit-norm. This setup is very similar to ours, with two differences: First, our norm operator is L1 rather than L2. Second, our norm operator is\napplied outside the division (i.e. our division is element-wise). Our proposed algorithm, PercentDelta, was used to train our work (Abu-El-Haija et al., 2017), before we where aware of the work of (You et al., 2017). In (Abu-El-Haija et al., 2017), PercentDelta gave us 1% improvement over Adam, over all datasets. Nonetheless, we only discuss MNIST experiments in this paper, and leave graph embedding experiments for follow-up work."}, {"heading": "4 PercentDelta", "text": "For every weight matrix Wj , and similarily bias vectors, we propose the gradient update rule:\n\u2206PercentDeltaWj = \u03b7 \u03b3(t)  size(Wj)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 (\u2202J/\u2202Wj)W (t)j \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1  \u2202J\u2202Wj , (6) where scalar size(W ) \u2208 Z+ is the number of entries1 in W , and the scalar:\nsize(Wj)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 (\u2202J/\u2202Wj)W (t)j \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1\n(7)\nnormalizes the gradient \u2202J\u2202Wj of the j-th layer, so that it is more proportional to its current parameter value W (t)j , and ||.||1 is the L1-norm. We avoid division-by-zero errors by adding an epsilon to the denominator2. The divide operator within the L1 norm is applied element-wise and the outer divide operator is scalar. Note that the \u201cgradient multiplier\u201d fraction (Equation 7) is \u2208 R+. Therefore, we only changes the gardient\u2019s magnitude, but not its direction.\nNow, we justify Equation 6. Assume that Ws is scalar. Therefore, size(Ws) = 1 and the L1-norm becomes an absolute value. Equation 6 simplifies to:\nW (t+1)s := W (t) s \u2212 \u03b7 \u03b3(t) \u2223\u2223\u2223\u2223\u2223 W (t)s(\u2202J/\u2202Ws) \u2223\u2223\u2223\u2223\u2223 \u2202J\u2202Ws\n:= W (t)s \u2212 \u03b7 \u03b3(t) \u2223\u2223\u2223W (t)s \u2223\u2223\u2223 sign( \u2202J\u2202Ws ) ,\n(8)\nwhere sign(z) = 1 if z > 0 and = \u22121 if z < 0. Therefore, Ws will change with a quantity proportional to its current value. In particular, the scalar \u03b7 \u03b3(t) determines the percentage at which W changes at timestep t, giving rise to the name: PercentDelta. For example, if we setup a decay schedule on \u03b3(t), such that \u03b7\u03b3(t) changes during training from 10% to 0.01%, then the network parameters will change at a rate of 10% in early mini-batches, and gradually decrease their PercentDelta updates to 0.01% towards end of training, consistently across all layers, regardless of the current parameter value or the gradient value, which are influenced by choice of activation function, initialization, and network architecture."}, {"heading": "5 Experiments", "text": "We run experiments on MNIST. We use the same model for all experiments and fix the batch size to 500. Our model code is copied from the TensorFlow tutorial3, and contains 4 trainable layers: 2D Convolution, Max-pooling, 2D Convolution, Max-pooling, Fully-connected, Fully-connected. The convolutional layers contain trainable tensors with dimensions: (5, 5, 1, 32), (5, 5, 32, 64), and bias vectors. The Fully-connected layers contain trainable tensors with dimensions: (3136, 1024), (1024, 10), and bias vectors. The model is trained with Softmax loss, uses ReLu for hidden activations, and does not use BatchNorm.\n1size(W ) is the product of W \u2019s dimensions. For a 2-D matrix, it is equal to # rows \u00d7 # columns, for a vector, it is equal to its length, etc. We also define size(scalar) = 1.\n2In practice, rather than converting a b to a b+ , we use a b+ \u00b7sign(b) , as our goal is to push b slightly away from\nzero while keeping its sign. 3https://www.tensorflow.org/get_started/mnist/pros"}, {"heading": "5.1 MNIST Network Gradient Magnitudes", "text": ""}, {"heading": "5.2 MNIST Test Accuracy Curves", "text": "We want to measure how fast can PercentDelta train MNIST. We compare training speed with other algorithms, including per-parameter adaptive learning rate algorithms, AdaGrad (Duchi et al., 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al., 2017), which also normalizes the gradient w.r.t. a weight tensor, by the current value of the weight tensor."}, {"heading": "6 Discussion", "text": ""}, {"heading": "6.1 Situations where PercentDelta is useful", "text": "While PercentDelta has outperforms other training algorithms on 4-layer MNIST, the space of modelsdatasets is enormous we leave it as future work to try PercentDelta under various models and datasets. Nonetheless, we speculate that PercentDelta (and similarily, LARS, You et al. (2017)) would be very useful in the following scenarios:\n1. Learning Embeddings. Consider the common setup of feeding word embeddings (or graph embeddings, Abu-El-Haija et al., 2017) into a shared Neural Network and jointly learning the embeddings and Neural Network for an upstream objective. In this setup, a certain\nembedding vector is only affected by a fraction of training examples, while the shared network parameters are affected by all training examples. The sum of gradients w.r.t. the shared network parameters over all training examples can be disproportionately larger than embedding gradients. PercentDelta ensures that the shared network is not being updated much faster than the emebddings.\n2. Soft-Attention Models on Bag-of-Words. It is common to convert from variable-length bag-of-words (xj [1],xj [2], . . . ) into fixed-length representation by a convex combination: xj+1 := \u2211 i \u03b1ixj [i], which can then be used for an upstream objective (e.g. event detection\nin videos, Ramanathan et al., 2016). Here, \u03b1i can be the i-th position of the softmax over all Words. The parameters of the softmax model would receive gradients from all words. PercentDelta ensures that, the otherwise disproportionately large, gradient updates of the softmax model are proportional to the remainder of the network.\n3. Matrix Factorization Models. For example, Koren et al. (2009) propose to factorize a user-movie rating matrix R \u2208 Ru\u00d7m into:\nR \u2248WU \u00d7WM + bU \u00d7 ~1T + ~1\u00d7 bM + b,\nwhere WU \u2208 Ru\u00d7d and WM \u2208 Rd\u00d7m are the user and movie embedding matrix; d is the size of the latent-space; bU \u2208 Ru and bM \u2208 Rm are the user and movie bias vectors, and b \u2208 R is the global bias scalar. In this setup, b would receive very large sum-of-gradients, and PercentDelta can ensure that all parameters are training at the same speed."}, {"heading": "6.2 Hyperparamters and Decay Function", "text": "It seems that PercentDelta has many knobs to tune. However, we can fix \u03b7 to some value and only change \u03b3(t) as their product determines the effective rate of change across all layers\u2019 trainable tensors. We can set \u03b3(t) to constant decay: \u03b3(t) = 1\u2212 t\u00d7m, (9) where 0 < m << 1 determines the decay slope. In addition, we can ensure that \u03b3(t) > 0 to allow training continue indefinitely, by modifying Equation 9 to:\n\u03b3(t) = max(\u03b2, 1\u2212 t\u00d7m), (10)\nwhere \u03b2 can be set to a small positive value, such as 0.01. In this case, if we fix \u03b7 = 0.03, then we are effectively changing each trainable tensor by 3% for every training batch initially, then gradually annealing this change-rate to 0.03% after 1m steps.\nMore importantly, we feel that \u03b7 and \u03b3(t) are a function of the dataset, and not the model. Experimentally, we observe the algorithm is insensitive to the choices ofm and \u03b7 as long as they are \u201creasonable\u201d (i.e. removing diverging setups that can be quickly detected). However, we do not yet have a formula to automatically set them. Nonetheless, with a wide range of \u03b7 and m, we experimentally show on MNIST that PercentDelta beats all training algorithms, given the same budget of training steps."}, {"heading": "7 Conclusion", "text": "We propose an algorithm that trains layers of a neural network, all at the same speed. Our algorithm, PercentDelta, is a simple modification over standard Gradient Descent. It divides the gradient w.r.t. a trainable tensor over the mean of ||gradient / tensor||1. The division over mean L1-norm is scalar, and only changes the gradient\u2019s magnitude but not its direction. Effectively, this updates the L1 norm of trainable layers, all at the same rate. We recommend a linear decaying change-rate schedule. Our modified gradients can be passed through a standard momentum accumulator (Sutskever et al., 2013). Overall, we show experimentally that our algorithm puts an upper envelop on all training algorithms, reaching higher test accuracy with fewer steps."}], "references": [{"title": "Learning edge representations via lowrank asymmetric projections", "author": ["S. Abu-El-Haija", "B. Perozzi", "R. Al-Rfou"], "venue": "ACM International Conference on Information and Knowledge Management (CIKM).", "citeRegEx": "Abu.El.Haija et al\\.,? 2017", "shortCiteRegEx": "Abu.El.Haija et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["J. Ba", "D. Kingma"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Ba and Kingma,? 2015", "shortCiteRegEx": "Ba and Kingma", "year": 2015}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arxiv.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910).", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Journal of Machine Learning Research (JMLR).", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R.M. Bell", "C. Volinsky"], "venue": "IEEE Computer.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Detecting events and key actors in multi-person videos", "author": ["V. Ramanathan", "J. Huang", "S. Abu-El-Haija", "A. Gorban", "K. Murphy", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Ramanathan et al\\.,? 2016", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2016}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "Neural Information Processing Systems.", "citeRegEx": "Salimans and Kingma,? 2016", "shortCiteRegEx": "Salimans and Kingma", "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "International Conference on Machine Learning.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Scaling sgd batch size to 32k for imagenet training", "author": ["Y. You", "I. Gitman", "B. Ginsburg"], "venue": "UC Berkeley Technical Report.", "citeRegEx": "You et al\\.,? 2017", "shortCiteRegEx": "You et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Several proposed methods metigate this problem, including utilizing: per-parameter adaptive learning rate such as AdaGrad (Duchi et al., 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 5, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 81, "endOffset": 106}, {"referenceID": 9, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.", "startOffset": 119, "endOffset": 146}, {"referenceID": 2, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010).", "startOffset": 161, "endOffset": 178}, {"referenceID": 4, "context": ", 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010).", "startOffset": 65, "endOffset": 90}, {"referenceID": 1, "context": ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier\u2019s (Glorot and Bengio, 2010). These methods heuristically attack the disproportionate training problem, which we justify in section 2. In this paper, we propose to directly enforce proportionate training of layers. Specifically, We propose a gradient update rule that moves every weight tensor in the direction of the gradient, but with a magnitude that changes the tensor value by a relative amount. We use the same relative amount across all layers, to train them all at the same speed. The remainder of the paper is organized as follows. In Section 2 we illustrate the disproportionate training phenomena on a (toy) hypothetical 4-layer neural network, by expanding gradient terms using BackProp Rumelhart et al. (1986). In Section 3, we summarize related work.", "startOffset": 17, "endOffset": 956}, {"referenceID": 8, "context": "W1 and W2, then expand the expressions using the Back Propagation Algorithm (Rumelhart et al., 1986):", "startOffset": 76, "endOffset": 100}, {"referenceID": 9, "context": "Weight normalization (Salimans and Kingma, 2016) and intelligent initialization schemes (e.", "startOffset": 21, "endOffset": 48}, {"referenceID": 5, "context": "BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al.", "startOffset": 10, "endOffset": 35}, {"referenceID": 2, "context": "BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al., 2016) mitigate this problem.", "startOffset": 50, "endOffset": 67}, {"referenceID": 3, "context": "1 Adaptive Gradient Duchi et al. (2011) proposed AdaGrad.", "startOffset": 20, "endOffset": 40}, {"referenceID": 1, "context": "For details, we point readers to the Adam paper (Ba and Kingma, 2015).", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": "Ba and Kingma (2015) propose Adam, which keeps exponential decaying sums, of gradients and of square gradients, respectively known as first and second moments.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "2 LARS You et al. (2017) propose to normalize every layer\u2019s gradients by the ratio of L2 norms of the parameter and the gradient.", "startOffset": 7, "endOffset": 25}, {"referenceID": 0, "context": "Our proposed algorithm, PercentDelta, was used to train our work (Abu-El-Haija et al., 2017), before we where aware of the work of (You et al.", "startOffset": 65, "endOffset": 92}, {"referenceID": 11, "context": ", 2017), before we where aware of the work of (You et al., 2017).", "startOffset": 46, "endOffset": 64}, {"referenceID": 0, "context": "In (Abu-El-Haija et al., 2017), PercentDelta gave us 1% improvement over Adam, over all datasets.", "startOffset": 3, "endOffset": 30}, {"referenceID": 3, "context": "We compare training speed with other algorithms, including per-parameter adaptive learning rate algorithms, AdaGrad (Duchi et al., 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al.", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": ", 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 11, "context": ", 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al., 2017), which also normalizes the gradient w.", "startOffset": 96, "endOffset": 114}, {"referenceID": 11, "context": "Nonetheless, we speculate that PercentDelta (and similarily, LARS, You et al. (2017)) would be very useful in the following scenarios:", "startOffset": 67, "endOffset": 85}, {"referenceID": 3, "context": "Top-to-bottom: PercentDelta (our algorithm), AdaGrad (Duchi et al., 2011), Adam (Ba and Kingma, 2015), LARS (You et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 1, "context": ", 2011), Adam (Ba and Kingma, 2015), LARS (You et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": ", 2011), Adam (Ba and Kingma, 2015), LARS (You et al., 2017), and the last row shows the best performer from all algorithms.", "startOffset": 42, "endOffset": 60}, {"referenceID": 6, "context": "For example, Koren et al. (2009) propose to factorize a user-movie rating matrix R \u2208 Ru\u00d7m into:", "startOffset": 13, "endOffset": 33}], "year": 2017, "abstractText": "Deep Neural Networks are generally trained using iterative gradient updates. Magnitudes of gradients are affected by many factors, including choice of activation functions and initialization. More importantly, gradient magnitudes can greatly differ across layers, with some layers receiving much smaller gradients than others. causing some layers to train slower than others and therefore slowing down the overall convergence. We analytically explain this disproportionality. Then we propose to explicitly train all layers at the same speed, by scaling the gradient w.r.t. every trainable tensor to be proportional to its current value. In particular, at every batch, we want to update all trainable tensors, such that the relative change of the L1-norm of the tensors is the same, across all layers of the network, throughout training time. Experiments on MNIST show that our method appropriately scales gradients, such that the relative change in trainable tensors is approximately equal across layers. In addition, measuring the test accuracy with training time, shows that our method trains faster than other methods, giving higher test accuracy given same budget of training steps.", "creator": "LaTeX with hyperref package"}}}