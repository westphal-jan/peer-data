{"id": "1708.07950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2017", "title": "MTIL17: English to Indian Langauge Statistical Machine Translation", "abstract": "english to ease indian language machine translation poses much the challenge upstream of structural dominance and morphological divergence. this paper describes english to employ indian language statistical machine translation as using pre - ordering consolidation and suffix separation. utilizing the pre - ordering differentiation uses unique rules set to consistently transfer the simple structure of defining the source tongue sentences prior to training and simultaneous translation. this syntactic project restructuring helps statistical precision machine translation to uniquely tackle the structural divergence and hence better communicate translation quality. using the internal suffix separation is actually used to tackle the further morphological divergence between between mainline english scholars and highly influenced agglutinative austro indian languages. we demonstrate that the use of pre - ordering and incomplete suffix separation helps excel in improving roughly the comparative quality of domestic english to alleviate indian domestic language machine translation.", "histories": [["v1", "Sat, 26 Aug 2017 08:12:35 GMT  (36kb,D)", "http://arxiv.org/abs/1708.07950v1", "11 pages journal paper"]], "COMMENTS": "11 pages journal paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raj nath patel", "prakash b pimpale", "sasikumar m"], "accepted": false, "id": "1708.07950"}, "pdf": {"name": "1708.07950.pdf", "metadata": {"source": "CRF", "title": "MTIL17: English to Indian Langauge Statistical Machine Translation", "authors": ["Raj Nath Patel", "Prakash B. Pimpale"], "emails": [], "sections": [{"heading": null, "text": "Keywords. Statistical Machine Translation, English, Indian Languages, Preprocessing, Reordering, Suffix and Compound Splitting, Transliteration.\n2010 Mathematics Subject Classification. 68T50."}, {"heading": "1 Introduction", "text": "In this paper, we present our Statistical Machine Translation (SMT) experiments from English to Tamil, Malayalam, Punjabi and Hindi. From the set of target languages involved, Hindi and Punjabi belong to the Indo-Aryan language family and Tamil and Malayalam belong to the Dravidian language family. All languages except English, have the same flexibility towards word order, canonically following the Subject-Object-Verb (SOV) structure, whereas English follows the SubjectVerb-Object (SVO) structure.\nEnglish The structural difference between source and target languages makes SMT difficult. It has been demonstrated that pre-ordering benefits SMT in such cases [32, 33]. Pre-ordering or reordering transforms the source sentence into a target-like order using syntactic parse of the source side. After reordering, training of the SMT system is performed using parallel corpus. Reordering also applies to the new source sentences prior decoding. The reordering system is generally developed using rich set of rules for the structural transformation of English sentence into SOV structure. These rules are manually extracted based on analysis ar X iv :1\n70 8.\n07 95\n0v 1\n[ cs\n.C L\n] 2\n6 A\nug 2\n01 7\nof source sentence tree and target translations. We have made use of a system developed by [23] to achieve the pre-ordering.\nWith reference to the morphology, Tamil and Malayalam are more agglutinative compared to English. It is also known that SMT produces more unknown words resulting in bad translation quality, if morphological divergence between source and target language is high. [14, 26, 30] and [25] have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system. To tackle the morphological divergence of English with Tamil we have used suffix separation developed by [24, 25] as a pre-processing step. The suffix separation tries to reduce the morphological divergence between source and agglutinative target language by splitting long words. Such words when generated in decoding are combined to form a single word in post-processing.\nA factored SMT with stem as an alignment factor [12] has been used to achieve better alignment. The target side transliteration is also applied to non translated words.\nThe rest of the paper is organized as follows. In section 2, we discuss the challenges of english to Indian language machine translation. Section 3 describes the dataset and experimental setup. Section 4 discusses experiments and results followed by a description of our submission to the shared task in section 5. In section 6, we report few early observations and conclusion and future work in section 7."}, {"heading": "2 Challenges of English to Indian Language SMT", "text": "As discussed briefly in the introduction, English to Indian language MT poses challenges of structural and morphological differences. In the subsection we discuss the difference of word order and morphology with examples."}, {"heading": "2.1 Word Order", "text": "The important structural difference in English and most of the Indian languages is that of word order. English uses the Subject-Verb-Object (SVO) order and most of the Indian languages, including the ones under study, primarily use SubjectObject-Verb (SOV). Sometimes, these languages exhibit free word order also. Prepositions in case of english come after the pronoun or noun they qualify and for hindi they succeed the noun or pronouns. Two representative examples of the same are given in table 1. There, we can see that word order \u2019ate mango\u2019 becomes\n\u2019mango ate\u2019 (1aama khaayaa ) in Hindi. In the next sentence, we can see that the preposition \u2019on\u2019 (para in Hindi) moves past the noun phrase, \u2019the table\u2019 which it qualifies."}, {"heading": "2.2 Morphology - Agglutination", "text": "We discuss here morphological divergence of Tamil and Malayalam with respect to English using analysis of the parallel corpus detailed in table 2. We know that the parallel corpus represents the same information in two different languages. In table 2, we can see that english makes use of more words to represent the same concept or information as compared to tamil and malayalam. If we look at the unique words for each language, we can conclude that english has much less vocabulary as compared to these two Indian languages and so english needs to make use of different combinations of available words to represent a concept. Whereas in case of tamil and malayalam there exist words to represent the same concepts. Examples of the same can be seen in table 3. The different average sentence lengths of these languages also establish the same fact. The significant difference in 2average word length shows that the words of tamil and malayalam are longer as compared to that of the english. Many words in these Indian languages are formed using compounding of multiple words. The phenomenon is called agglutination and so we say that tamil and malayalam are more agglutinative compared to english.\nThe difference in length of source and target sentences makes the word alignment difficult. The difficulty ultimately results into poor quality translation system. In our experiments we try to tackle the same by using suffix separation methods for english-tamil SMT.\n1 All Non-English (Hindi, Tamil) words have been written in Itrans using http://sanskritlibrary.org/transcodeText.html; For Tamil, we have written the word pronunciation in Devanagari and then trans-coded in Itrans 2 Average word length calculated on unique words, on total words, it is 4.8 for english and stays almost same for others."}, {"heading": "3 System Setup", "text": "In the following subsections we describe Data distribution followed by pre-processing, evaluation metrics, and SMT system setup for the experiments."}, {"heading": "3.1 Data set", "text": "For our experiments, we have used corpus shared by MTIL-2017 detailed in Table 4. We split the shared data into train, test, and development sets. We used publicly available 3Indian Language tokenizer and text normalizer for all the target languages. For English, we used tokenizer available with 4moses . Also, we removed the sentences having word count > 80 or source-target length ratio > 1:9. The details of other advanced preprocessing stages are described in the following subsections."}, {"heading": "3.2 Preprocessing", "text": "To handle the structural divergence between English and other languages, we have used source side reordering. Reordering is proven to improve the translation quality of the language pairs with high structural divergence.\n3 http://anoopkunchukuttan.github.io/indic_nlp_library/ 4 https://github.com/moses-smt/mosesdecoder\nTo tackle the morphological divergence between the source and target languages for the purpose of a better SMT system, we preprocessed the Tamil for suffix separation. A detailed description of both reordering and suffix separation is given in the following paragraphs.\nReordering (RO) is a preprocessing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase-based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality.\nFor English-Hindi SMT, earlier reordering is used by [23, 32, 33] and have shown significant improvements over baseline. [15] reported SMT results for English to 510 major Indian languages and showed that reordering helped for all of them.\nOther language pairs have also shown significant improvement when reordering is employed. [37] and [36] have observed improvement for French-English and Chinese-English language pairs respectively. [20] have proposed sentence restructuring whereas [4] have proposed clause restructuring to improve German-English SMT. [27, 28] have also reported the use of simple local transformation rules for Spanish-English and Serbian-English translation. Recently, [11] proposed a reordering technique using a deterministic approach for long distance reordering and non-deterministic approach for short distance reordering exploiting morphological information. Some reordering approaches are also presented exploiting the SMT itself [5, 9].\nSuffix Separation (SS) is the process where the words are split into stem and suffixes. For machine translation, the splitting of an unknown word into its parts\n5 Hindi, Urdu, Punjabi, Bengali, Gujarati, Marathi, Konkani, Tamil, Telugu, and Malayalam.\nenables the translation of the word by the translation of its parts. For example (Hindi-Marathi SMT), in Marathi, \u2019mahinyaaMnii\u2019 is translated as \u2019mahiine meM\u2019 (in the month) in Hindi. In this case, we split the word into \u2019mahiny + aaMnii\u2019. Here, the suffix \u2019aaMnii\u2019 corresponds to the word \u2019meM\u2019 in Hindi.\nWe considered only suffix from target language which corresponds to preposition in the source language (English). For this task, the list of suffixes is manually created with the linguistic expertise. When a word is subjected to SS, longest matching suffix from the list is considered for suffix separation. Suffix separation takes place only once for a word. We add a continuation symbol \"@@\" after the stem word (mahiny@@), which is used to combine the suffixes back after translation. Pseudocode for the suffix separation is detailed in Algorithms 1.\nAlgorithm 1 Suffix Separation 1: procedure SUFFIXSEP(word) 2: suffixSet\u2190 read file suffix list 3: splits\u2190 {word, \"NULL\"} 4: for suffix\u2190 suffixSet do 5: if then word.ENDSWITH = suffix & word.LENGTH > suffix.LENGTH 6: splits[0]\u2190 word.SUBSTRING(0, word.LASTINDEXOF(suffix)) + \u201d@@\u201d 7: splits[1]\u2190 suffix return splits 8: end if 9: end for 10: end procedure\nMany researchers have tried compound word splitting and suffix separation for SMT between morphologically rich languages. [2] has proposed an approach guided by a parallel corpus. The work is limited to breaking compounds into cognates and words found in a translation lexicon, but no results on translation performance are reported. [13] have demonstrated an empirical method of learning the compound splitting using monolingual and bilingual data and reported impact on performance of SMT. [24, 25] reported significantly improved translation quality for Indian languages SMT using suffix separation and compound word splitting."}, {"heading": "3.3 Transliteration", "text": "Out-of-Vocabulary (OOV) words occur in almost all Machine Translation (MT) systems. These words are mostly named entities, technical terms or foreign words that were not part of training corpus or were not added to the development dictionary. So, OOV words need to be translated to the target language using transliteration. Transliteration helps to improve the translation quality [24] and it has also been shown to be useful for translating closely related language pairs [6, 19]. For\nmost of the language pairs parallel corpus of transliterations isn\u2019t readily available and even if such a training data is made available, the arrangement to integrate transliterated words into MT pipelines are not available in SMT toolkits like phrasal [8] and joshua [31].\nGenerally, a transliteration system is trained separately outside of an MT pipeline using supervised training methods. It gives all possible target transliterations for a given source word. Generally, the 1-best output is selected as transliteration and is used to replace the OOV word in the translation, post decoding.\nThis paper uses unsupervised model [7] based on the Expectation Maximization (EM) to induce transliteration corpus from word aligned parallel data, which is then used to train a transliteration model. The implementation is available with the 6moses toolkit. We use top n-best transliteration output for OOV words. These candidates are plugged in and re-scored with the language model to get the best translation for source sentence."}, {"heading": "3.4 SMT system set up", "text": "The baseline system was setup by using the phrase-based model [1, 14, 17, 21] and [12] was used for factored model. We tuned the model parameters using minimum error rate training (MERT) [21]. The language model was trained using KenLM [10] toolkit with modified Kneser-Ney smoothing [3].We tried various n-gram language models and found that 5-gram performs best for the languages under study. For factored SMT training source and target side stem has been used as alignment factor. Stemming for Hindi, Punjabi, Tamil, and Malayalam, has been done using a modified version of lightweight stemmer [34]. For English we have used porter stemmer [18]."}, {"heading": "3.5 Evaluation metrics", "text": "The different experimental systems are being compared using, BLEU [22], PER [29], TER [35], and CDER [16]. For an MT system to be better, higher BLEU and PER scores with lower TER, and CDER are desired."}, {"heading": "4 Experiments and Results", "text": "We carried out various experiments to achieve better accuracy, using the data and setup described in previous sections. Table 5 details systems we tried. We also report BLEU, 1-TER, PER, and 1-CDER scores of those systems. It can be seen\n6 https://github.com/moses-smt/mosesdecoder\nthat the use of preprocessing and transliteration has contributed to the improvement of 1 to 1.5 BLEU points over the baseline for english-hindi, english-punjabi and english-tamil. For english-malayalam the BLEU has decreased and we plan to investigate the same in future. Also, investigation is needed to figure out why the BLEU score decreased on use of factors in english-punjabi, while it was useful for other language pairs.\nTable 6 describes with an example how reordering reduces the structural divergence and helps to achieve better translation quality. From the example, it can be seen that the translation of the system using S3 is better than the S1. The output of S3 is structurally more correct and conveys the same meaning as that of the reference translation."}, {"heading": "5 Submission to the Shared Task", "text": "As shown in table 7, we (C-DACM) submitted systems for all language pairs under the task. The submitted translations, of the unseen test set, were obtained using S4 and S4\u2032. The shared task organizers (MTIL17) evaluated the results manually and also used the BLEU metric. The manual evaluation used Adequacy, Fluency and Overall Rating as metrics. Evaluation results for the top three participating systems were published by MTIL17 and table 7 can be referred for the same. From the evaluation results, it is evident that our (CDAC-M) submissions significantly outperform the other submissions for english-hindi, english-tamil, and englishmalayalam. For english-punjabi our results stand at the second position."}, {"heading": "6 Error Analysis", "text": "A closer look at the performance of these systems to understand the utility of Reordering and Suffix Separation has been done. We report a few early observations."}, {"heading": "6.1 Reordering Errors", "text": "We have used Reodering system developed by [23]. It\u2019s based on manual observation and needs the addition of multiple rules. Table 8 details an example of the reordering error. In the example, the phrase sequence \u2019very useful for losing fat\u2019 is wrongly reordered and that has resulted into a wrong translation. The wrong reordering not just affects the structure of the output, but also badly affects the phrase translation."}, {"heading": "6.2 Bad Split", "text": "The suffix separation by [25] is used. For tamil it has limited list of manually observed suffixes and hence it doesn\u2019t work for many words. As suffixes are crudely chopped without consideration of validity of remaining part, the errors get introduced. Most of the errors belong to the category where the words which are not meant to split, get split because they end with a certain suffix. This causes sparsity of these genuine terms in the data and leads to a wrong translation of those. For example, a genuine word, say \u2019abcd\u2019 is getting split into \u2019ab\u2019 + \u2019cd\u2019 which is a wrong split. As, \u2019abcd\u2019 is a proper noun and hence should not have been split. Avoiding suffix separation of words in the NNP POS tag was tried, but that was stopping many other valid candidates from pre-processing. A word getting split at wrong position was also one of the major error cases. For example, a word with character sequence \u2019pqrstu\u2019 was getting split into \u2019pqr\u2019 and \u2019stu\u2019 instead of \u2019pqrs\u2019\nand \u2019tu\u2019. In such cases, both suffixes \u2019stu\u2019 and \u2019tu\u2019 are valid and so deciding on when it goes wrong is difficult."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we presented various systems for translation from English to Hindi, Malayalam, Punjabi, and Tamil. Factored SMT with suffix separation and reordering as pre-processing and transliteration as post-processing performs better. In the immediate future we plan to investigate the detailed impact of the methods devised. For example, failure of factored SMT for english-punjabi and english-malayalam and impact of methods on error rates. Further, we plan to work towards improving the preprocessing and post-processing techniques for better translation quality and extend the approach to other Indian languages."}], "references": [{"title": "Roossin, A Statistical Approach to Machine Translation, Computational linguistics", "author": ["Peter F Brown", "John Cocke", "Stephen A Della Pietra", "Vincent J Della Pietra", "Fredrick Jelinek", "John D Lafferty", "Robert L Mercer", "Paul S"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Corpus-driven splitting of compound", "author": ["Ralf D Brown"], "venue": "Proceedings of the 9th International Conference on Theoretical and Methodological Issues in Machine Translation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "An Empirical Study of Smoothing Techniques for Language Modeling, in: Proceedings of the 34th annual meeting on Association for Computational Linguistics, Association for Computational Linguistics", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Hindi-to-Urdu machine translation through transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid"], "venue": "in: Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Integrating an Unsupervised Transliteration Model into Statistical Machine Translation", "author": ["Nadir Durrani", "Hassan Sajjad", "Hieu Hoang", "Philipp Koehn"], "venue": "in: Proceedings of the 15th Conference of the European Chapter of the ACL (EACL", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Phrasal: A Toolkit for New Directions in Statistical Machine Translation", "author": ["Spence Green", "Daniel Cer", "Christopher D. Manning"], "venue": "Proceddings of the Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning Improved Reordering Models for Urdu, Farsi and Italian using SMT, in: Proceedings of the first workshop on Reordering for Statistical Machine Translation", "author": ["Rohit Gupta", "Raj N Patel", "Ritesh Shah"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["Kenneth Heafield"], "venue": "in: Proceedings of the Sixth Workshop on Statistical Machine Translation, Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Syntax-Based Reordering for Statistical Machine Translation, Computer speech & language", "author": ["Maxim Khalilov", "Jos\u00e9 AR Fonollosa"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Factored Translation Models", "author": ["Philipp Koehn", "Hieu Hoang"], "venue": "in: EMNLP-CoNLL, pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Empirical Methods for Compound Splitting, in: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1, Association for Computational Linguistics", "author": ["Philipp Koehn", "Kevin Knight"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Statistical Phrase-Based Translation, in: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, Association for Computational Linguistics", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Sata-anuvadak: Tackling multiway translation of indian languages, pan", "author": ["Anoop Kunchukuttan", "Abhijit Mishra", "Rajen Chatterjee", "Ritesh Shah", "Pushpak Bhattacharyya"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "CDER: Efficient MT Evaluation Using Block Movements", "author": ["Gregor Leusch", "Nicola Ueffing", "Hermann Ney"], "venue": "in: EACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "A Phrase-Based, Joint Probability Model for Statistical Machine Translation, in: Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, Association for Computational Linguistics", "author": ["Daniel Marcu", "William Wong"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages, in: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, Association for Computational Linguistics", "author": ["Preslav Nakov", "J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Statistical Machine Translation with Scarce Resources using Morpho-Syntactic Information, Computational linguistics", "author": ["Sonja Nie\u00dfen", "Hermann Ney"], "venue": "MTIL17: English to Indian Langauge Statistical Machine Translation", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Minimum error rate training in statistical machine translation, in: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, Association for Computational Linguistics", "author": ["Franz Josef Och"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation, in: Proceedings of the 40th annual meeting on association for computational linguistics", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Reordering rules for English-Hindi SMT", "author": ["Raj Nath Patel", "Rohit Gupta", "Prakash B Pimpale", "Sasikumar M"], "venue": "in: Proceedings of the Second Workshop on Hybrid Approaches", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Statistical Machine Translation for Indian Languages: Mission Hindi", "author": ["Raj Nath Patel", "Prakash B Pimpale", "Sasikumar M"], "venue": "in: Proceedings of NLP Tools contest,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "SMT from Agglutinative Languages: Use of Suffix Separation and Word Splitting", "author": ["Prakash B Pimpale", "Raj Nath Patel", "Sasikumar M"], "venue": "in: Proceedings of ICON- 2014: 11th International Conference on Natural Language Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Towards the Use of Word Stems and Suffixes for Statistical Machine Translation", "author": ["Maja Popovic", "Hermann Ney"], "venue": "in: LREC,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "POS-Based Word Reorderings for Statistical Machine Translation", "author": ["Maja Popovic", "Hermann Ney"], "venue": "in: International Conference on Language Resources and Evaluation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Statistical Machine Translation with a Small Amount of Bilingual Training Data", "author": ["Maja Popovic", "Hermann Ney"], "venue": "in: 5th LREC SALTMIL Workshop on Minority Languages,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Word error rates: decomposition over pos classes and applications for error analysis", "author": ["Maja Popovi\u0107", "Hermann Ney"], "venue": "in: Proceedings of the Second Workshop on Statistical Machine Translation, Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Statistical machine translation of German compound words, Advances in natural language processing", "author": ["Maja Popovi\u0107", "Daniel Stein", "Hermann Ney"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "A phrase-based and hierarchical statistical machine translation system", "author": ["Matt Post", "Yuan Cao", "Gaurav Kumar", "Joshua"], "venue": "The Prague Bulletin of Mathematical Linguistics", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Does Syntactic Knowledge help English-Hindi SMT", "author": ["Taraka Rama", "Karthik Gali", "Avinesh PVS"], "venue": "arXiv preprint arXiv:1401.4869", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Simple Syntactic and Morphological Processing Can Help English-Hindi Statistical Machine Translation", "author": ["Ananthakrishnan Ramanathan", "Jayprasad Hegde", "Ritesh M Shah", "Pushpak Bhattacharyya", "M Sasikumar"], "venue": "in: Proceedings of International Joint Conference on NLP (IJCNLP08),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "A Lightweight Stemmer for Hindi", "author": ["Ananthakrishnan Ramanathan", "Durgesh D Rao"], "venue": "in: The Proceedings of EACL,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation, in: Proceedings of association for machine translation in the Americas", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Chinese Syntactic Reordering for Statistical Machine Translation", "author": ["Chao Wang", "Michael Collins", "Philipp Koehn"], "venue": "in: EMNLP-CoNLL, pp", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Improving a Statistical MT System with Automatically Learned Rewrite Patterns, in: Proceedings of the 20th international conference on Computational Linguistics, Association for Computational Linguistics, p", "author": ["Fei Xia", "Michael McCord"], "venue": "E-mail: prakash@cdac.in Sasikumar M, KBCS Division,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}], "referenceMentions": [{"referenceID": 28, "context": "It has been demonstrated that pre-ordering benefits SMT in such cases [32, 33].", "startOffset": 70, "endOffset": 78}, {"referenceID": 29, "context": "It has been demonstrated that pre-ordering benefits SMT in such cases [32, 33].", "startOffset": 70, "endOffset": 78}, {"referenceID": 19, "context": "We have made use of a system developed by [23] to achieve the pre-ordering.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "[14, 26, 30] and [25] have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system.", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[14, 26, 30] and [25] have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system.", "startOffset": 0, "endOffset": 12}, {"referenceID": 26, "context": "[14, 26, 30] and [25] have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system.", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "[14, 26, 30] and [25] have demonstrated ways to handle this issue with morphological segmentation of words before training the SMT system.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "To tackle the morphological divergence of English with Tamil we have used suffix separation developed by [24, 25] as a pre-processing step.", "startOffset": 105, "endOffset": 113}, {"referenceID": 21, "context": "To tackle the morphological divergence of English with Tamil we have used suffix separation developed by [24, 25] as a pre-processing step.", "startOffset": 105, "endOffset": 113}, {"referenceID": 9, "context": "A factored SMT with stem as an alignment factor [12] has been used to achieve better alignment.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "For English-Hindi SMT, earlier reordering is used by [23, 32, 33] and have shown significant improvements over baseline.", "startOffset": 53, "endOffset": 65}, {"referenceID": 28, "context": "For English-Hindi SMT, earlier reordering is used by [23, 32, 33] and have shown significant improvements over baseline.", "startOffset": 53, "endOffset": 65}, {"referenceID": 29, "context": "For English-Hindi SMT, earlier reordering is used by [23, 32, 33] and have shown significant improvements over baseline.", "startOffset": 53, "endOffset": 65}, {"referenceID": 12, "context": "[15] reported SMT results for English to 510 major Indian languages and showed that reordering helped for all of them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37] and [36] have observed improvement for French-English and Chinese-English language pairs respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[37] and [36] have observed improvement for French-English and Chinese-English language pairs respectively.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "[20] have proposed sentence restructuring whereas [4] have proposed clause restructuring to improve German-English SMT.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27, 28] have also reported the use of simple local transformation rules for Spanish-English and Serbian-English translation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[27, 28] have also reported the use of simple local transformation rules for Spanish-English and Serbian-English translation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "Recently, [11] proposed a reordering technique using a deterministic approach for long distance reordering and non-deterministic approach for short distance reordering exploiting morphological information.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "Some reordering approaches are also presented exploiting the SMT itself [5, 9].", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "LASTINDEXOF(suffix)) + \u201d@@\u201d 7: splits[1]\u2190 suffix return splits 8: end if 9: end for 10: end procedure", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "[2] has proposed an approach guided by a parallel corpus.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[13] have demonstrated an empirical method of learning the compound splitting using monolingual and bilingual data and reported impact on performance of SMT.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24, 25] reported significantly improved translation quality for Indian languages SMT using suffix separation and compound word splitting.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[24, 25] reported significantly improved translation quality for Indian languages SMT using suffix separation and compound word splitting.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "Transliteration helps to improve the translation quality [24] and it has also been shown to be useful for translating closely related language pairs [6, 19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Transliteration helps to improve the translation quality [24] and it has also been shown to be useful for translating closely related language pairs [6, 19].", "startOffset": 149, "endOffset": 156}, {"referenceID": 15, "context": "Transliteration helps to improve the translation quality [24] and it has also been shown to be useful for translating closely related language pairs [6, 19].", "startOffset": 149, "endOffset": 156}, {"referenceID": 5, "context": "most of the language pairs parallel corpus of transliterations isn\u2019t readily available and even if such a training data is made available, the arrangement to integrate transliterated words into MT pipelines are not available in SMT toolkits like phrasal [8] and joshua [31].", "startOffset": 254, "endOffset": 257}, {"referenceID": 27, "context": "most of the language pairs parallel corpus of transliterations isn\u2019t readily available and even if such a training data is made available, the arrangement to integrate transliterated words into MT pipelines are not available in SMT toolkits like phrasal [8] and joshua [31].", "startOffset": 269, "endOffset": 273}, {"referenceID": 4, "context": "This paper uses unsupervised model [7] based on the Expectation Maximization (EM) to induce transliteration corpus from word aligned parallel data, which is then used to train a transliteration model.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "The baseline system was setup by using the phrase-based model [1, 14, 17, 21] and [12] was used for factored model.", "startOffset": 62, "endOffset": 77}, {"referenceID": 11, "context": "The baseline system was setup by using the phrase-based model [1, 14, 17, 21] and [12] was used for factored model.", "startOffset": 62, "endOffset": 77}, {"referenceID": 14, "context": "The baseline system was setup by using the phrase-based model [1, 14, 17, 21] and [12] was used for factored model.", "startOffset": 62, "endOffset": 77}, {"referenceID": 17, "context": "The baseline system was setup by using the phrase-based model [1, 14, 17, 21] and [12] was used for factored model.", "startOffset": 62, "endOffset": 77}, {"referenceID": 9, "context": "The baseline system was setup by using the phrase-based model [1, 14, 17, 21] and [12] was used for factored model.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "We tuned the model parameters using minimum error rate training (MERT) [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "The language model was trained using KenLM [10] toolkit with modified Kneser-Ney smoothing [3].", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "The language model was trained using KenLM [10] toolkit with modified Kneser-Ney smoothing [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 30, "context": "Stemming for Hindi, Punjabi, Tamil, and Malayalam, has been done using a modified version of lightweight stemmer [34].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "The different experimental systems are being compared using, BLEU [22], PER [29], TER [35], and CDER [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "The different experimental systems are being compared using, BLEU [22], PER [29], TER [35], and CDER [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": "The different experimental systems are being compared using, BLEU [22], PER [29], TER [35], and CDER [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "The different experimental systems are being compared using, BLEU [22], PER [29], TER [35], and CDER [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 19, "context": "We have used Reodering system developed by [23].", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "The suffix separation by [25] is used.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation.", "creator": "TeX"}}}