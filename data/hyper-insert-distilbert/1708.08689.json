{"id": "1708.08689", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization", "abstract": "a number programs of online services nowadays nowadays effectively rely upon machine learning computing to blindly extract valuable user information extracted from classified data collected in the wild. maintaining this knowledge exposes quantitative learning manipulation algorithms to exploits the threat strategy of internal data poisoning poisoning, i. that e., forming a pure coordinate memory attack in which a fraction of value the training available data is controlled by detecting the attacker and otherwise manipulated to subvert the learning process. concurrent to however date, these attacks have exclusively been primarily devised only as against a limited class of binary obstacle learning algorithms, noting due to limitations the largely inherent complexity of the gradient - based procedure used to highly optimize the data poisoning base points ( a. k. a. adversarial training examples ). in this comprehensive work, basically we exclude rst extend applying the de / nition of poisoning attacks devised to eliminate multiclass matching problems. we then strongly propose a novel poisoning assessment algorithm essentially based on the idea of of back - dynamic gradient memory optimization, i. e., intending to compute the squared gradient of interest computed through automatic posterior di erentiation, while also freely reversing the learning analytics procedure to drastically reduce the attack complexity. compared as to current poisoning strategies, our approach is able each to carefully target a lot wider applicable class of selective learning algorithms, trained with additional gradient - based searching procedures, including neural networks and deep network learning skill architectures. whilst we empirically delicately evaluate its e ectiveness on acquiring several application examples, including intentional spam victim ltering, malware filtering detection, and user handwritten adaptive digit language recognition. nonetheless we nally correctly show that, particularly similarly to adversarial evaluation test examples, adversarial analyst training performance examples can also however be transferred only across di erent learning preparation algorithms.", "histories": [["v1", "Tue, 29 Aug 2017 10:47:38 GMT  (1000kb,D)", "http://arxiv.org/abs/1708.08689v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luis mu\\~noz-gonz\\'alez", "battista biggio", "ambra demontis", "rea paudice", "vasin wongrassamee", "emil c lupu", "fabio roli"], "accepted": false, "id": "1708.08689"}, "pdf": {"name": "1708.08689.pdf", "metadata": {"source": "META", "title": "Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization", "authors": ["Luis Mu\u00f1oz-Gonz\u00e1lez", "Battista Biggio", "Ambra Demontis", "Andrea Paudice", "Vasin Wongrassamee", "Emil C. Lupu", "Fabio Roli"], "emails": [], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computing Methodologies\u2192Machine Learning;\nKEYWORDS Adversarial Machine Learning, Training Data Poisoning, Adversarial Examples, Deep Learning."}, {"heading": "1 INTRODUCTION", "text": "In recent years technology has become pervasive, enabling a rapid a disruptive change in the way society is organized. Our data is provided to third-party services which are supposed to facilitate and protect our daily work and activities. Most of these services leverage machine learning to extract valuable information from the overwhelming amount of input data received. Although this provides advantages to the users themselves, e.g., in terms of usability and functionality of such services, it is also clear that these\nservices may be abused, providing great opportunities for cybercriminals to conduct novel, illicit, and highly-profitable activities. Being one of the main components behind such services makes machine learning an appealing target for attackers, who may gain a significant advantage by gaming the learning algorithm. Notably, machine learning itself can be the weakest link in the security chain, as its vulnerabilities can be exploited by the attacker to compromise the whole system infrastructure. To this end, she may inject malicious data to poison the learning process, or manipulate data at test time to evade detection.1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339]. Such attacks have fuelled a growing interest in the research area of adversarial machine learning, at the intersection of cybersecurity and machine learning. This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].\nAmong the different attack scenarios envisaged against machine learning, poisoning attacks are considered one of the most relevant and emerging security threats for data-driven technologies, i.e., technologies relying upon the collection of large amounts of data in the wild [17]. In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39]. More practically, data poisoning is already a relevant threat in different application domains. For instance, some online services directly exploit users\u2019 feedback on their decisions to update the trained model. PDFRate2 is an online malware detection tool that analyzes the submitted PDF files to reveal the presence of embedded malware [34]. After classification, it allows the user to provide feedback on its decision, i.e., to confirm or not the classification result. A malicious user may thus provide wrong feedback to gradually poison the system and compromise its performance over time. Notably, there is a more general underlying problem related to the collection of large data volumes with reliable labels. This is a well-known problem\n1We refer to the attacker here as feminine due to the common interpretation as \u201cEve\u201d or \u201cCarol\u201d in cryptography and security. 2http://pdfrate.com\n1\nar X\niv :1\n70 8.\n08 68\n9v 1\n[ cs\n.L G\n] 2\n9 A\nug 2\nin malware detection, where malware samples are collected by means of compromised machines with known vulnerabilities (i.e., honeypots), or via other online services, like VirusTotal,3 in which labelling errors are often reported.\nPrevious work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39]. The main technical difficulty in devising a poisoning attack is the computation of the poisoning samples, also recently referred to as adversarial training examples [20]. This requires solving a bilevel optimization problem in which the outer optimization amounts to maximizing the classification error on an untainted validation set, while the inner optimization corresponds to training the learning algorithm on the poisoned data [23]. Since solving this problem with black-box optimization is too computationally demanding, previous work has exploited gradient-based optimization, along with the idea of implicit differentiation. The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39]. This approach however can only be used against a limited class of learning algorithms, excluding neural networks and deep learning architectures, due to the inherent complexity of the procedure used to compute the required gradient. Another limitation is that, to date, previous work has only considered poisoning of two-class learning algorithms.\nIn this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect. 2). We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack. The underlying idea is to compute the gradient of interest through reverse-mode (automatic) differentiation (i.e., back-propagation), while reversing the underlying learning procedure to trace back the entire sequence of parameter updates performed during learning, without storing it. In fact, storing this sequence in memory would be infeasible for learning algorithms that optimize a large set of parameters across several iterations. Our poisoning algorithm only requires the learning algorithm to update its parameters during training in a smooth manner (e.g., through gradient descent), to correctly trace these changes backwards. Accordingly, compared to previously proposed poisoning strategies, our approach is the first capable of targeting a wider class of learning algorithms, trainable with gradient-based procedures, like neural networks and deep learning architectures (Sect. 3).\nAnother important contribution of this work is to show how the performance of learning algorithms may be drastically compromised even by the presence of a small fraction of poisoning points in the training data, in the context of real-world applications like spam filtering, malware detection, and handwritten digit recognition (Sect. 4). We also investigate the transferability property of poisoning attacks, i.e., the extent to which attacks devised against a specific learning algorithm are effective against different ones. To our knowledge, this property has been investigated for evasion\n3https://virustotal.com\nattacks (a.k.a. adversarial test examples), i.e., attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks. We conclude our work by discussing related work (Sect. 5), the main limitations of our approach, and future research directions (Sect. 6)."}, {"heading": "2 THREAT MODEL", "text": "In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples. Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].\nThe framework characterizes the attacker according to her goal, knowledge of the targeted system, and capability of manipulating the input data. Based on these assumptions, it allows one to define an optimal attack strategy as an optimization problem whose solution amounts to the construction of the attack samples, i.e., of the adversarial examples.\nIn this work, we extend this framework, originally developed for binary classification problems, to multiclass classification. While this generalization holds for evasion attacks too, we only detail here the main poisoning attack scenarios.\nNotation. In a classification task, given the instance space X and the label space Y, the learner aims to estimate the underlying (possibly noisy) latent function f that maps X 7\u2192 Y. Given a training set Dtr = {x i ,yi }ni=1 with n i.i.d. samples drawn from the underlying probability distribution p(X,Y),4 we can estimate f with a parametric or non-parametric modelM trained by minimizing an objective function L(D,w) (normally, a tractable estimate of the generalization error), w.r.t. its parameters and/or hyperparameters w .5\nThus, while L denotes the learner\u2019s objective function (possibly including regularization), we use L(D,w) to denote only the loss incurred when evaluating the learner parameterized byw on the samples in D."}, {"heading": "2.1 Attacker\u2019s Goal", "text": "The goal of the attack is determined in terms of the desired security violation and attack specificity. In multiclass classification, misclassifying a sample does not have a unique meaning, as there is more than one class different from the correct one. Accordingly, we extend the current framework by introducing the concept of error specificity. These three characteristics are detailed below.\nSecurity Violation. This characteristic defines the high-level security violation caused by the attack, as normally done in security engineering. It can be: an integrity violation, if malicious activities evade detection without compromising normal system operation;\n4While normally the set notation {x i , yi }ni=1 does not admit duplicate entries, we admit our data sets to contain potentially duplicated points. 5For instance, for kernelized SVMs,w may include the dual variables\u03b1 , the bias b , and even the regularization parameterC . In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.\nan availability violation, if normal system functionality is compromised, e.g., by increasing the classification error; or a privacy violation, if the attacker obtains private information about the system, its users or data by reverse-engineering the learning algorithm. Attack Specificity. This characteristic ranges from targeted to indiscriminate, respectively, if the attack aims to cause misclassification of a specific set of samples (to target a given system user or protected service), or of any sample (to target any system user or protected service). Error Specificity.We introduce here this characteristic to disambiguate the notion of misclassification in multiclass problems. The error specificity can thus be: specific, if the attacker aims to have a sample misclassified as a specific class; or generic, if the attacker aims to have a sample misclassified as any of the classes different from the true class.6"}, {"heading": "2.2 Attacker\u2019s Knowledge", "text": "The attacker can have different levels of knowledge of the targeted system, including: (k .i) the training data Dtr; (k .ii) the feature set X; (k .iii) the learning algorithmM, along with the objective function L minimized during training; and, possibly, (k .iv) its (trained) parametersw . The attacker\u2019s knowledge can thus be characterized in terms of a space \u0398 that encodes the aforementioned assumptions (k .i)-(k .iv) as \u03b8 = (D,X,M,w). Depending on the assumptions made on each of these components, one can envisage different attack scenarios. Typically, two main settings are considered, referred to as attacks with perfect and limited knowledge. Perfect-Knowledge (PK) Attacks. In this case, the attacker is assumed to know everything about the targeted system. Although this setting may be not always representative of practical cases, it enables us to perform a worst-case evaluation of the security of learning algorithms under attack, highlighting the upper bounds on the performance degradation that may be incurred by the system under attack. In this case, we have \u03b8PK = (D,X,M,w). Limited-Knowledge (LK) Attacks. Although LK attacks admit a wide range of possibilities, the attacker is typically assumed to know the feature representationX and the learning algorithmM, but not the training data (for which surrogate data from similar sources can be collected). We refer to this case here as LK attacks with Surrogate Data (LK-SD), and denote it with \u03b8LK\u2212SD = (D\u0302,X,M, w\u0302) (using the hat symbol to denote limited knowledge of a given component). Notably, in this case, as the attacker is only given a surrogate data set D\u0302, also the learner\u2019s parameters have to be estimated by the attacker, e.g., by optimizing L on D\u0302.\nSimilarly, we refer to the case in which the attacker knows the training data (e.g., if the learning algorithm is trained on publiclyavailable data), but not the learning algorithm (for which a surrogate learner can be trained on the available data) as LK attacks with Surrogate Learners (LK-SL). This scenario can be denoted with \u03b8LK\u2212SL = (D,X, M\u0302, w\u0302), even though the parameter vector w\u0302 may belong to a different vector space than that of the targeted learner. Note that LK-SL attacks also include the case in which the 6In [28], the authors defined targeted and indiscriminate attacks (at test time) depending on whether the attacker aims to cause specific or generic errors. Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].\nattacker knows the learning algorithm, but she is not able to derive an optimal attack strategy against it (e.g., if the corresponding optimization problem is not tractable or difficult to solve), and thus uses a surrogate learning model to this end. Experiments on the transferability of attacks among learning algorithms, firstly demonstrated in [6] and then in subsequent work on deep learners [27], fall under this category of attacks."}, {"heading": "2.3 Attacker\u2019s Capability", "text": "This characteristic is defined based on the influence that the attacker has on the input data, and on the presence of data manipulation constraints. Attack Influence. In supervised learning, the attack influence can be causative, if the attacker can influence both training and test data, or exploratory, if the attacker can only manipulate test data. These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39]. Data Manipulation Constraints. Another aspect related to the attacker\u2019s capability is the presence of constraints on the manipulation of input data, which is however strongly dependent on the given practical scenario. For example, if the attacker aims to evade a malware classification system, she should manipulate the exploitation code embedded in the malware sample without compromising its intrusive functionality. In the case of poisoning, the labels assigned to the training samples are not typically under the control of the attacker. She should thus consider additional constraints while manipulating the poisoning samples to have them labelled as desired. Typically, these constraints can be nevertheless accounted for in the definition of the optimal attack strategy. In particular, we characterize them by assuming that an initial set of attack samples Dc is given, and that it is modified according to a space of possible modifications \u03a6(Dc )."}, {"heading": "2.4 Attack Strategy", "text": "Given the attacker\u2019s knowledge \u03b8 \u2208 \u0398 and a set of manipulated attack samples D \u2032c \u2208 \u03a6(Dc ), the attacker\u2019s goal can be characterized in terms of an objective function A(D \u2032c ,\u03b8 ) \u2208 R which evaluates how effective the attacks D \u2032c are. The optimal attack strategy can be thus given as:\nD\u22c6c \u2208 arg max D\u2032c \u2208\u03a6(Dc ) A(D \u2032c ,\u03b8 ) (1)\nWhile this high-level formulation encompasses both evasion and poisoning attacks, in both binary and multiclass problems, in the remainder of this work we only focus on the definition of some poisoning attack scenarios."}, {"heading": "2.5 Poisoning Attack Scenarios", "text": "We focus here on two poisoning attack scenarios of interest for multiclass problems, noting that other attack scenarios can be derived in a similar manner. Error-Generic Poisoning Attacks. The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service. This is an availability attack, and it could be targeted or indiscriminate, depending on whether it affects a specific system user or service, or\nany of them. In the multiclass case, it is thus natural to extend this scenario assuming that the attacker is not aiming to cause specific errors, but only generic misclassifications. As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem. This can be made explicit by rewriting Eq. (1) as:\nD\u22c6c \u2208 arg max D\u2032c \u2208\u03a6(Dc )\nA(D \u2032c ,\u03b8 ) = L(D\u0302val, w\u0302) , (2)\ns.t. w\u0302 \u2208 arg min w \u2032\u2208W\nL(D\u0302tr \u222a D \u2032c ,w \u2032) , (3)\nwhere the surrogate data D\u0302 available to the attacker is divided into two disjoint sets D\u0302tr and D\u0302val. The former, alongwith the poisoning points D \u2032c is used to learn the surrogate model, while the latter is used to evaluate the impact of the poisoning samples on untainted data, through the function A(D \u2032c ,\u03b8 ). In this case, the function A(D \u2032c ,\u03b8 ) is simply defined in terms of a loss function L(D\u0302val, w\u0302) that evaluates the performance of the (poisoned) surrogate model on D\u0302val. The dependency of A on D \u2032c is thus indirectly encoded through the parameters w\u0302 of the (poisoned) surrogate model.7 Note that, since the learning algorithm (even if convex) may not exhibit a unique solution in the feasible set W, the outer problem has to be evaluated using the exact solution w\u0302 found by the inner optimization. Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.g., using the hinge loss against SVMs [8]). In the multiclass case, one can use a multiclass loss function, like the log-loss with softmax activation, as done in our experiments. Error-Specific Poisoning Attacks. Here, we assume that the attacker\u2019s goal is to cause specific misclassifications \u2013 a plausible scenario only for multiclass problems. This attack can cause an integrity or an availability violation, and it can also be targeted or indiscriminate, depending on the desired misclassifications. The poisoning problem remains that given by Eqs. (2)-(3), though the objective is defined as:\nA(D \u2032c ,\u03b8 ) = \u2212L(D\u0302 \u2032val, w\u0302) , (4)\nwhere D\u0302 \u2032val is a set that contains the same data as D\u0302val, though with different labels, chosen by the attacker. These labels correspond to the desired misclassifications, and this is why there is a minus sign in front of L, i.e., the attacker effectively aims atminimizing the loss on her desired set of labels. Note that, to implement an integrity violation or a targeted attack, some of these labels may actually be the same as the true labels (such that normal system operation is not compromised, or only specific system users are affected)."}, {"heading": "3 POISONING ATTACKS WITH BACK-GRADIENT OPTIMIZATION", "text": "In this section, we first discuss how the bilevel optimization given by Eqs. (2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39]. As we will see, these attacks can only be used against a limited class of learning algorithms, excluding neural networks and deep learning architectures, due to the\n7Note that A can also be directly dependent on D\u2032c , as in the case of nonparametric models; e.g., in kernelized SVMs, when the poisoning points are support vectors [8].\ninherent complexity of the procedure used to compute the required gradient. To overcome this limitation, we exploit a recent technique called back-gradient optimization [14, 22], which allows computing the gradient of interest in a more computationally-efficient and stabler manner. Notably, this enables us to devise the first poisoning attack able to target neural networks and deep learning architectures (without using any surrogate model).\nBefore delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization. The poisoning problem can be thus simplified as:\nx\u22c6c \u2208 arg max x \u2032c \u2208\u03a6({x c ,yc })\nA({x \u2032c ,yc },\u03b8 ) = L(D\u0302val, w\u0302) , (5)\ns.t. w\u0302 \u2208 arg min w \u2032\u2208W\nL(x \u2032c ,w \u2032) . (6)\nThe function \u03a6 imposes constraints on the manipulation of xc , e.g., upper and lower bounds on its manipulated values. These may also depend on yc , e.g., to ensure that the poisoning sample is labelled as desired when updating the targeted classifier. Note also that, for notational simplicity, we only report x \u2032c as the first argument of L instead of D\u0302tr \u222a {x \u2032c ,yc }. Gradient-based Poisoning Attacks.We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39]. For some classes of loss functions L and learning objective functions L, this problem can be indeed solved through gradient ascent. In particular, provided that the loss function L is differentiable w.r.t.w and xc , we can compute the gradient \u2207x cA using the chain rule:\n\u2207x cA = \u2207x c L + \u2202w\u0302\n\u2202xc\n\u22a4 \u2207wL , (7)\nwhere L(D\u0302val, w\u0302) is evaluated on the parameters w\u0302 learned after training (including the poisoning point). The main difficulty here is computing \u2202w\u0302\u2202x c , i.e., understanding how the solution of the learning algorithm varies w.r.t. the poisoning point. Under some regularity conditions, this can be done by replacing the inner learning problem with its stationarity (KKT) conditions. For example, this holds if the learning problem L is convex, which implies that all stationary points are global minima [31]. In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39]. The trick here is to replace the inner optimizationwith the implicit function\u2207wL(Dtr\u222a{xc ,yc }, w\u0302) = 0, corresponding to its KKT conditions. Then, assuming that it is differentiable w.r.t. xc , one yields the linear system \u2207x c\u2207wL + \u2202w\u0302 \u2202x c\n\u22a4\u22072wL = 0. If \u22072wL is not singular, we can solve this system w.r.t. \u2202w\u0302\u2202x c , and substitute its expression in Eq. (7), yielding: \u2207x cA = \u2207x c L \u2212 (\u2207x c\u2207wL)(\u22072wL)\u22121\u2207wL . (8) This gradient is then iteratively used to update the poisoning point through gradient ascent, as shown in Algorithm 1.8 Recall that the 8Note that Algorithm 1 can be exploited to optimize multiple poisoning points too. As in [39], the idea is to perform several passes over the set of poisoning samples, using Algorithm 1 to optimize each poisoning point at a time, while keeping the other points fixed. Line searches can also be exploited to reduce complexity.\nAlgorithm 1 Poisoning Attack Algorithm\nInput: D\u0302tr, D\u0302val, L, L, the initial poisoning point x (0) c , its label yc , the learning rate \u03b7, a small positive constant \u03b5 . 1: i \u2190 0 (iteration counter) 2: repeat 3: w\u0302 \u2208 arg minw \u2032 L(x (i) c ,w\n\u2032) (train learning algorithm) 4: x (i+1)c \u2190 \u03a0\u03a6 ( xc (i) + \u03b7\u2207x cA({x (i) c ,yc }) ) 5: i \u2190 i + 1 6: until A({x (i)c ,yc }) \u2212 A({x (i\u22121) c ,yc }) < \u03b5\nOutput: the final poisoning point xc \u2190 x (i)c\nprojection operator \u03a0\u03a6 is used to map the current poisoning point onto the feasible set \u03a6 (cf. Eqs. 5-6).\nThis is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39]. The problem here is that computing and inverting \u22072wL scales in time as O(p3) and in memory as O(p2), being p the cardinality ofw . Moreover, Eq. (8) requires solving one linear system per parameter. These aspects make it prohibitive to assess the effectiveness of poisoning attacks in a variety of practical settings.\nTo mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq. (8). In particular, one can set (\u22072wL) v = \u2207wL, and compute \u2207xcA = \u2207x c L \u2212 \u2207xc\u2207wL v. The computation of the matrices \u2207x c\u2207wL and \u22072wL can also be avoided using Hessianvector products [30]:\n(\u2207x c\u2207wL) z = limh\u21920 1 h\n( \u2207x cL ( x \u2032c , w\u0302 + hz ) \u2212 \u2207x cL ( x \u2032c , w\u0302 ) ) ,\n(\u2207w\u2207wL) z = lim h\u21920 1 h\n( \u2207wL ( x \u2032c , w\u0302 + hz ) \u2212 \u2207wL ( x \u2032c , w\u0302 ) ) .\nAlthough this approach allows poisoning learning algorithms more efficiently w.r.t. previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly. From a practical perspective, this means that the KKT conditions have to be met with satisfying numerical accuracy. However, as these problems are always solved to a finite accuracy, it may happen that the gradient \u2207x cA is not sufficiently precise, especially if convergence thresholds are too loose [14, 22].\nIt is thus clear that such an approach can not be used, in practice, to poison learning algorithms like neural networks and deep learning architectures, as it may not only be difficult to derive proper stationarity conditions involving all parameters, but also as it may be too computationally demanding to train such learning algorithms with sufficient precision to correctly compute the gradient \u2207x cA. Poisoning with Back-gradient Optimization. In this work, we overcome this limitation by exploiting back-gradient optimization [14, 22]. This technique has been first exploited in the context of energybased models and hyperparameter optimization, to solve bilevel optimization problems similar to the poisoning problem discussed before. The underlying idea of this approach is to replace the inner optimization with a set of iterations performed by the learning\nAlgorithm 2 Gradient Descent\nInput: initial parametersw0, learning rate \u03b7, D\u0302tr, L. 1: for t = 0, . . . ,T \u2212 1 do 2: gt = \u2207wL(D\u0302tr,wt ) 3: wt+1 \u2190 wt \u2212 \u03b7 gt 4: end for\nOutput: trained parameterswT\nAlgorithm 3 Back-gradient Descent\nInput: trained parameterswT , learning rate \u03b7, D\u0302tr, D\u0302val, poisoning point x \u2032c , yc , loss function L, learner\u2019s objective L. initialize dxc \u2190 0, dw \u2190 \u2207wL(D\u0302val,wT ) 1: for t = T , . . . , 1 do 2: dxc \u2190 dx \u2032c \u2212 \u03b7 dw\u2207x c\u2207wL(x \u2032c ,wt ) 3: dw \u2190 dw \u2212 \u03b7 dw\u2207w\u2207wL(x \u2032c ,wt ) 4: gt\u22121 = \u2207w tL(x \u2032c ,wt ) 5: wt\u22121 = wt + \u03b1gt\u22121 6: end for\nOutput: \u2207x cA = \u2207x c L + dxc\nalgorithm to update the parametersw , provided that such updates are smooth, as in the case of gradient-based learning algorithms. According to [14], this technique allows to compute the desired gradients in the outer problem using the parameterswT obtained from an incomplete optimization of the inner problem (afterT iterations). This represent a significant computational improvement compared to traditional gradient-based approaches, since it only requires a reduced number of training iterations for the learning algorithm. This is especially important in large neural networks and deep learning algorithms, where the computational cost per iteration can be high. Then, assuming that the inner optimization runs for T iterations, the idea is to exploit reverse-mode differentiation, or back-propagation, to compute the gradient of the outer objective. However, using back-propagation in a na\u00efve manner would not work for this class of problems, as it requires storing the whole set of parameter updates w1, . . . ,wT performed during training, along with the forward derivatives. These are indeed the elements required to compute the gradient of the outer objective with a backward pass (we refer the reader to [22] for more details). This process can be extremely memory-demanding if the learning algorithm runs for a large number of iterations T , and especially if the number of parametersw is large (as in deep networks). Therefore, to avoid storing the whole training trajectoryw1, . . . ,wT and the required forward derivatives, Domke [14] and Maclaurin et al. [22] proposed to compute them directly during the backward pass, by reversing the steps followed by the learning algorithm to update them. Computing wT , . . . ,w1 in reverse order w.r.t. the forward step is clearly feasible only if the learning procedure can be exactly traced backwards. Nevertheless, this happens to be feasible for a large variety of gradient-based procedures, including gradient descent with fixed step size, and stochastic gradient descent with momentum.\nIn this work, we leverage back-gradient descent to compute \u2207x cA (Algorithm 3) by reversing a standard gradient-descent procedure with fixed step size that runs for a truncated training of the learning algorithm to T iterations (Algorithm 2). Notably, lines 2-3 in Algorithm 3 can be efficiently computed with Hessian-vector products, as discussed before. We exploit this algorithm to compute the gradient \u2207x cA in line 4 of our poisoning attack algorithm (Algorithm 1). In this case, line 3 of Algorithm 1 is replaced with the incomplete optimization of the learning algorithm, truncated to T iterations. Note finally that, as in [14, 22], the time complexity of our back-gradient descent is O(T ). This drastically reduces the complexity of the computation of the outer gradient, making it feasible to evaluate the effectiveness of poisoning attacks also against large neural networks and deep learning algorithms. Moreover, this outer gradient can be accurately estimated from a truncated optimization of the inner problem with a reduced number of iterations. This allows for a tractable computation of the poisoning points in Algorithm 1, since training the learning algorithm at each iteration can be prohibitive, especially for deep networks.\nWe conclude this section by noting that, in the case of errorspecific poisoning attacks (Sect. 2.5), the outer objective in Problem (5)-(6) is \u2212L(D\u0302 \u2032val , w\u0302). This can be regarded as a minimization problem, and it thus suffices to modify line 4 in Algorithm 1 to update the poisoning point along the opposite direction. We clarify this in Fig. 1, where we also discuss the different effect of errorgeneric and error-specific poisoning attacks in a multiclass setting."}, {"heading": "4 EXPERIMENTAL ANALYSIS", "text": "In this section, we first evaluate the effectiveness of the backgradient poisoning attacks described in Sect. 3 on spam andmalware detection tasks. In these cases, we also assess whether poisoning samples can be transferred across different learning algorithms. We then investigate the impact of error-generic and error-specific poisoning attacks in the well-known multiclass problem of handwritten digit recognition. In this case, we also report the first proofof-concept adversarial training examples computed by poisoning a convolutional neural network in an end-to-end manner (i.e., not just using a surrogate model trained on the deep features, as in [20])."}, {"heading": "4.1 Spam and Malware Detection", "text": "We consider here two distinct datasets, respectively representing a spam email classification problem (Spambase) and a malware detection task (Ransomware). The Spambase data [11] consists of a collection of 4, 601 emails, including 1, 813 spam emails. Each email is encoded as a feature vector consisting of 54 binary features, each denoting the presence or absence of a given word in the email. The Ransomware data [33] consists of 530 ransomware samples and 549 benign applications. Ransomware is a very recent kind of malware which encrypts the data on the infected machine, and requires the victim to pay a ransom to obtain the decryption key. This dataset has 400 binary features accounting for different sets of actions, API invocations, and modifications in the file system and registry keys during the execution of the software.\nWe consider the following leaning algorithms: (i)Multi-Layer Perceptrons (MLPs) with one hidden layer consisting of 10 neurons; (ii) Logistic Regression (LR); and (iii) Adaline (ADA). For MLPs, we have used hyperbolic tangent activation functions for the neurons in the hidden layer, and softmax activations in the output layer. Moreover, for MLPs and LR, we use the cross-entropy (or log-loss) as the loss function, while we use the mean squared error for ADA.\nWe assume here that the attacker aims to cause a denial of service, and thus runs a poisoning availability attack whose goal is simply to maximize the classification error. Accordingly, we run Algorithm 1 injecting up to 20 poisoning points in the training data. We initialize the poisoning points by cloning training points and flipping their label. We set the number of iterationsT for obtaining stable back-gradients to 200, 100, and 80, respectively for MLPs, LR and ADA. We further consider two distinct settings: PK attacks, in which the attacker is assumed to have full knowledge of the attacked system (for a worst-case performance assessment); and LK-SL attacks, in which she knows everything except for the learning algorithm, and thus uses a surrogate learner M\u0302. This scenario, as discussed in Sect. 2.2, is useful to assess the transferability property of the attack samples. To the best of our knowledge, this has been demonstrated in [6, 27] for evasion attacks (i.e., adversarial test examples) but never for poisoning attacks (i.e., adversarial training examples). To this end, we optimize the poisoning samples using alternatively MLPs, LR or ADA as the surrogate learner, and then evaluate the impact of the corresponding attacks against the other two algorithms.\nThe experimental results, shown in Figs. 2-3, are averaged on 10 independent random data splits. In each split, we use 100 samples for training and 400 for validation, i.e., to respectively construct Dtr and Dval. Recall indeed that in both PK and LK-SL settings, the attacker has perfect knowledge of the training set used to learn the true (attacked) model, i.e., D\u0302tr = Dtr. The remaining samples are used for testing, i.e., to assess the classification error under poisoning.9\nWe can observe from Fig. 2 that PK poisoning attacks can significantly compromise the performance of all the considered classifiers. In particular, on Spambase, they cause the classification error of ADA and LR to increase up to 30% even if the attacker only controls 15% of the training data. Although the MLP is more resilient to poisoning than these linear classifiers, its classification error also increases significantly, up to 25%, which is not tolerable in several practical settings. The results for PK attacks on Ransomware are similar, although the MLP seems as vulnerable as ADA and LR in this case. Transferability of Poisoning Samples. Regarding LK-SL poisoning attacks, we can observe from Fig. 3 that the attack points generated using a linear classifier (either ADA or LR) as the surrogate model have a very similar impact on the other linear classifier. In contrast, the poisoning points crafted with these linear algorithms have a lower impact against the MLP, although its performance is still noticeably affected. When the MLP is used as the surrogate model, instead, the performance degradation of the other algorithms is similar. However, the impact of these attacks is much\n9Note indeed that the validation error only provides a biased estimate of the true classification error, as it is used by the attacker to optimize the poisoning points [8].\nlower. To summarize, our results show that the attack points can be effectively transferred across linear algorithms and also have a noticeable impact on (nonlinear) neural networks. In contrast, transferring poisoning samples from nonlinear to linear models seems to be less effective."}, {"heading": "4.2 Handwritten Digit Recognition", "text": "We consider here the problem of handwritten digit recognition, which involves 10 classes (each corresponding to a digit, from 0 to 9), using theMNIST data [21]. Each digit image consists of 28\u00d728 = 784 pixels, ranging from 0 to 255 (images are in grayscale). We divide each pixel value by 255 and use it as a feature. We evaluate the effect of error-generic and error-specific poisoning strategies against a multiclass LR classifier using softmax activation and the log-loss as the loss function. Error-generic attack. In this case, the attacker aims to maximize the classification error regardless of the resulting kinds of error, as described in Sect. 2.5. This is thus an availability attack, aimed to cause a denial of service. We generate 10 independent random splits using 1000 samples for training, 1000 for validation, and 8000 for testing. To compute the back-gradients \u2207x cA required by our poisoning attack, we use T = 60 iterations. We initialize the poisoning points by cloning randomly-chosen training points and changing their label at random In addition, we compare our poisoning attack strategy here against a label-flip attack in which the attack points are drawn from the validation set and their labels are flipped at random. In both cases, we inject up to 60 attack points into the training set.\nThe results are shown in Fig. 4 (top row). Note first that our error-generic poisoning attack almost doubles the classification error in the absence of poisoning, with less than 6% of poisoning points. It is also much more effective than random label flips and, as expected, it causes a similar increase of the classification error over all classes (although some classes are easier to poison, like digit 5). This is even more evident from the difference between the\nconfusion matrix obtained under 6% poisoning and that obtained in the absence of attack.\nError-specific attack. Here, we assume that the attacker aims to misclassify 8s as 3s, while not having any preference regarding the classification of the other digits. This can be thus regarded as an availability attack, targeted to cause the misclassification of a specific set of samples. We generate 10 independent random splits with 1000 training samples, 4000 samples for validation, and 5000 samples for testing. Recall that the goal of the attacker in this scenario is described by Eq. (4). In particular, she aims at minimizing L(D\u0302 \u2032val , w\u0302), where the samples in the validation set D\u0302 \u2032 val are relabelled according to the attacker\u2019s goal. Here, the validation set thus only consists of digits of class 8 labelled as 3. We set T = 60 to compute the back-gradients used in our poisoning attack, and inject up to 40 poisoning points into the training set. We initialize the poisoning points by cloning randomly-chosen samples from the classes 3 and 8 in the training set, and flipping their label from 3 to 8, or vice-versa. We consider only these two classes here as they are the only two actively involved in the attack.\nThe results are shown in Fig. 4 (bottom row).We can observe that only the classification error rate for digit 8 is significantly affected, as expected. In particular, it is clear from the difference of the confusion matrix obtained under poisoning and the one obtained in the absence of attack that most of the 8s are misclassified as 3s. After adding less than 4% of poisoning points, in fact, the error rate for digit 8 increases approximately from 20% to 50%. Note that, as a side effect, the error rate of digit 3 also slightly increases, though not to a significant extent.\nPoisoning Deep Neural Networks. We finally report a proofof-concept experiment to show the applicability of our attack algorithm to poison a deep network in an end-to-end manner, i.e., accounting for all weight updates in each layer (instead of using\na surrogate model trained on a frozen deep feature representation [20]). To this end, we consider the convolutional neural network (CNN) proposed in [21] for classification of the MNIST digit data, which requires optimizing more than 450, 000 parameters.10 In this proof-of-concept attack, we inject 10 poisoning points into the training data, and repeat the experiment on 5 independent data splits, considering 1, 000 samples for training, and 2, 000 for validation and testing. For simplicity, we only consider the classes of digits 1, 5, and 6 in this case. We use Algorithm 1 to craft each single poisoning point, but, similarly to [39], we optimize them iteratively, making 2 passes over the whole set of poisoning samples. We also use the line search exploited in [39], instead of a fixed gradient step size, to reduce the attack complexity (i.e., the number of training updates to the deep network). Under this setting, however, we find that our attack points only slightly increase the classification error, though not significantly, while random label flips do not have any substantial effect. For comparison, we also attack a multiclass LR classifier under the same setting, yielding an increase of the error rate from 2% to 4.3% with poisoning attacks, and to only 2.1% with random label flips. This shows that, at least in this simple case, deep networks seem to be more resilient against (a very small fraction of) poisoning attacks (i.e., less than 1%). Some of the poisoning samples crafted against the CNN and the LR are shown in Figs. 5 and 6. We report the initial digit (and its true label y), its poisoned version (and its label yc ), and the difference between the two images, in absolute value (rescaled to visually appreciate the modified pixels). Notably, similarly to adversarial test examples, also poisoning samples against deep networks are visually indistinguishable from the initial image (as in [20]), while this is not the case when targeting the LR classifier. This might be due to the specific shape of the decision function learned by the deep network in the input space, as explained in the case of adversarial test examples [15, 36]. We\n10We use the implementation available at https://github.com/tflearn/tflearn/blob/ master/examples/images/convnet_mnist.py.\nhowever leave a more detailed investigation of this aspect to future work, along with a more systematic security evaluation of deep networks against poisoning attacks. We conclude this section with a simple transferability experiment, in which we use the poisoning samples crafted against the LR classifier to attack the CNN, and vice-versa. In the former case, the attack is totally ineffective, while in the latter case it has a similar effect to that of random label flips (as the minimal modifications to the CNN-poisoning digits are clearly irrelevant for the LR classifier)."}, {"heading": "5 RELATEDWORK", "text": "Seminal work on the analysis of supervised learning in the presence of omniscient attackers that can compromise the training data has been presented in [12, 18]. While their results show the infeasibility of learning in such settings, their analysis reports an overly-pessimistic perspective on the problem. The first practical poisoning attacks against two-class classification algorithms have been proposed in [19, 26], in the context of spam filtering and anomaly detection. However, such attacks do not easily generalize to different learning algorithms. More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39]. In particular, Biggio et al. [8] have been the first to demonstrate the vulnerability of SVMs to poisoning attacks. Following the same approach, Xiao et al. [39] have shown how to poison LASSO, ridge regression, and the elastic net. Finally, Mei and Zhu [23] has systematized such attacks under a unified framework to poison convex learning algorithms with Tikhonov regularizers,\nbased on the concept of machine teaching [29, 40]. The fact that these techniques require full re-training of the learning algorithm at each iteration (to fulfil the KKT conditions up to a sufficient finite precision), along with the intrinsic complexity required to compute\n0 10 20\n0\n5\n10\n15\n20\n25\nInitial sample (y=1)\n0 10 20\n0\n5\n10\n15\n20\n25\nPoisoning sample (y=5)\n0 10 20\n0\n5\n10\n15\n20\n25\nAttack changes\n0 10 20\n0\n5\n10\n15\n20\n25\nInitial sample (y=5)\n0 10 20\n0\n5\n10\n15\n20\n25\nPoisoning sample (y=6)\n0 10 20\n0\n5\n10\n15\n20\n25\nAttack changes\nInitial sample (y=6)\nPoisoning sample (y=5)\nAttack changes\nthe corresponding gradients, makes them too computationally demanding for several practical settings. Furthermore, this limits their applicability to a wider class of learning algorithms, including those based on gradient descent and subsequent variants, like deep neural networks, as their optimization is often truncated prior to meeting the stationarity conditions with the precision required to compute the poisoning gradients effectively. Note also that, despite recent work [20] has provided a first proof of concept of the existence of adversarial training examples against deep networks, this has been shown on a binary classification task using a surrogate model (attacked with standard KKT-based poisoning). In particular, the authors have generated the poisoning samples by attacking a logistic classifier trained on the features extracted from the penultimate layer of the network (which have been kept fixed). Accordingly, to our knowledge, our work is thus the first to show how to poison a deep neural network in an end-to-end manner, considering all its parameters and layers, and without using any surrogate model. Notably, our work is also the first to show (in a more systematic way) that poisoning samples can be transferred across different learning algorithms, using substitute (a.k.a. surrogate) models, as similarly demonstrated for evasion attacks (i.e., adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks."}, {"heading": "6 CONCLUSIONS, LIMITATIONS AND FUTUREWORK", "text": "Advances in machine learning have led to a massive use of datadriven technologies with emerging applications in many different fields, including cybersecurity, self-driving cars, data analytics, biometrics and industrial control systems. At the same time, the variability and sophistication of cyberattacks have tremendously increased, making machine learning systems an appealing target for cybercriminals [2, 16].\nIn this work, we have considered the threat of training data poisoning, i.e., an attack in which the training data is purposely manipulated to maximally degrade the classification performance of learning algorithms. While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings. To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.e., misclassifying samples as a specific class), or generic ones (i.e., misclassifying samples as any class different than the correct one).\nAnother important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39]. As discussed throughout this work, this requirement, as well as the intrinsic complexity of such attacks, limits their application only to a reduced class of learning algorithms. In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31]. Our approach can be applied to a wider class of learning algorithms, as it only requires the learning algorithm to update smoothly its parameters during training, without even necessarily fulfilling the optimality conditions with very high precision. Moreover, the gradients can be accurately estimated with the parameters obtained from an incomplete optimization of the learning algorithm truncated to a reduced number of iterations. This enables the efficient application of our attack strategy to large neural networks and deep learning architectures, as well as any other learning algorithm trained through gradient-based procedures. Our empirical evaluation on spam filtering, malware detection, and handwritten digit recognition has shown that neural networks can be significantly compromised even if the attacker only controls a small fraction of training points. We have also empirically shown that poisoning samples designed against one learning algorithm can be rather effective also in poisoning another algorithm, highlighting an interesting transferability property, as that shown for evasion attacks (a.k.a. adversarial test examples) [6, 27, 37].\nThe main limitation of this work is that we have not run an extensive evaluation of poisoning attacks against deep networks, to thoroughly assess their security to poisoning. Although our preliminary experiments seem to show that they can be more resilient against this threat than other learning algorithms, a more complete and systematic analysis remains to be performed. Therefore, we plan to more systematically investigate the effectiveness of our back-gradient poisoning attack against deep networks in the very near future. Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35]."}], "references": [{"title": "The security of machine learning", "author": ["Marco Barreno", "Blaine Nelson", "Anthony Joseph", "J. Tygar"], "venue": "Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Proc. ACM Symp. Information, Computer and Comm. Sec. (ASIACCS \u201906)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Gradient-based optimization of hyperparameters", "author": ["Y. Bengio"], "venue": "Neural Computation 12,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Poisoning complete-linkage hierarchical clustering", "author": ["Battista Biggio", "Samuel Rota Bul\u00f2", "Ignazio Pillai", "Michele Mura", "Eyasu Zemene Mequanint", "Marcello Pelillo", "Fabio Roli"], "venue": "In Joint IAPR Int\u2019l Workshop on Structural, Syntactic, and Statistical Pattern Recognition (Lecture Notes in Computer Science),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Bagging Classifiers for Fighting Poisoning Attacks in Adversarial Classification Tasks", "author": ["Battista Biggio", "Igino Corona", "Giorgio Fumera", "Giorgio Giacinto", "Fabio Roli"], "venue": "In 10th International Workshop on Multiple Classifier Systems (MCS) (Lecture Notes in Computer Science),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD)", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "Part III (LNCS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Security Evaluation of Pattern Classifiers Under Attack", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering 26,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Poisoning attacks against support vector machines", "author": ["Battista Biggio", "Blaine Nelson", "Pavel Laskov"], "venue": "In 29th Int\u2019l Conf. on Machine Learning, John Langford and Joelle Pineau (Eds.). Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Is Data Clustering in Adversarial Settings Secure", "author": ["Battista Biggio", "Ignazio Pillai", "Samuel Rota Bul\u00f2", "Davide Ariu", "Marcello Pelillo", "Fabio Roli"], "venue": "In Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security (AISec \u201913)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Poisoning Behavioral Malware Clustering", "author": ["Battista Biggio", "Konrad Rieck", "Davide Ariu", "ChristianWressnegger", "Igino Corona", "Giorgio Giacinto", "Fabio Roli"], "venue": "In 2014 Workshop on Artificial Intelligent and Security (AISec \u201914)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "PAC Learning with Nasty Noise", "author": ["NaderH. Bshouty", "Nadav Eiron", "Eyal Kushilevitz"], "venue": "In Algorithmic Learning Theory, Osamu Watanabe and Takashi Yokomori (Eds.). Lecture Notes in Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Efficient multiple hyperparameter learning for log-linear models", "author": ["C. Do", "C.S. Foo", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Generic Methods for Optimization-Based Modeling", "author": ["Justin Domke"], "venue": "In 15th Int\u2019l Conf. Artificial Intelligence and Statistics (Proceedings of Machine Learning Research),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adversarial Machine Learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "In 4th ACM Workshop on Artificial Intelligence and Security (AISec", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Machine Learning Methods for Computer Security (Dagstuhl Perspectives Workshop 12371)", "author": ["Anthony D. Joseph", "Pavel Laskov", "Fabio Roli", "J. Doug Tygar", "Blaine Nelson"], "venue": "Dagstuhl Manifestos", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Learning in the presence of malicious errors", "author": ["Michael Kearns", "Ming Li"], "venue": "SIAM J. Comput. 22,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Security Analysis of Online Centroid Anomaly Detection", "author": ["Marius Kloft", "Pavel Laskov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Understanding Black-box Predictions via Influence Functions", "author": ["P.W. Koh", "P. Liang"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Gradient- Based Learning Applied to Document Recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Gradient-based Hyperparameter Optimization Through Reversible Learning", "author": ["Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams"], "venue": "In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "In 29th AAAI Conf. Artificial Intelligence", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Universal adversarial perturbations", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "In CVPR", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Exploiting Machine Learning to Subvert your", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C.A. Sutton", "J.D. Tygar", "K. Xia"], "venue": "Spam Filter. LEET", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["Blaine Nelson", "Marco Barreno", "Fuching Jack Chi", "Anthony D. Joseph", "Benjamin I.P. Rubinstein", "Udam Saini", "Charles Sutton", "J.D. Tygar", "Kai Xia"], "venue": "In LEET\u201908: Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Practical Black-Box Attacks Against Machine Learning", "author": ["Nicolas Papernot", "PatrickMcDaniel", "Ian Goodfellow", "Somesh Jha", "Z. Berkay Celik", "Ananthram Swami"], "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security (ASIA CCS \u201917)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "The Limitations of Deep Learning in Adversarial Settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z. Berkay Celik", "Ananthram Swami"], "venue": "In Proc. 1st IEEE European Symposium on Security and Privacy", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Optimal teaching for limitedcapacity human learners", "author": ["K.R. Patil", "X. Zhu", "L. Kope\u0107", "B.C. Love"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Fast Exact Multiplication by the Hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation 6,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1994}, {"title": "Hyperparameter optimization with approximate gradient", "author": ["F. Pedregosa"], "venue": "In 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research), Maria Florina Balcan and Kilian Q. Weinberger (Eds.),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "ANTIDOTE: understanding and defending against poisoning of anomaly detectors", "author": ["Benjamin I.P. Rubinstein", "Blaine Nelson", "Ling Huang", "Anthony D. Joseph", "Shinghon Lau", "Satish Rao", "Nina Taft", "J.D. Tygar"], "venue": "In Proceedings of the 9th ACM SIGCOMM Internet Measurement Conference (IMC \u201909)", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Automated Dynamic Analysis of Ransomware: Benefits, Limitations and use for Detection", "author": ["D. Sgandurra", "L. Mu\u00f1oz-Gonz\u00e1lez", "R. Mohsen", "E.C. Lupu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Malicious PDF Detection Using Metadata and Structural Features", "author": ["Charles Smutz", "Angelos Stavrou"], "venue": "In Proceedings of the 28th Annual Computer Security Applications Conference (ACSAC \u201912)", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Certified Defenses for Data Poisoning Attacks", "author": ["J. Steinhardt", "P.W. Koh", "P. Liang"], "venue": "arXiv preprint arXiv:1706.03691", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In International Conference on Learning Representations. http://arxiv", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Practical Evasion of a Learning-Based Classifier: A Case Study", "author": ["Nedim \u0160rndic", "Pavel Laskov"], "venue": "In Proc. 2014 IEEE Symp. Security and Privacy (SP \u201914). IEEE CS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Man vs.Machine: Practical Adversarial Detection of Malicious Crowdsourcing Workers", "author": ["GangWang", "TianyiWang", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In 23rd USENIX Security Symposium (USENIX Security 14)", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Is Feature Selection Secure against Training Data Poisoning", "author": ["Huang Xiao", "Battista Biggio", "Gavin Brown", "Giorgio Fumera", "Claudia Eckert", "Fabio Roli"], "venue": "In JMLR W&CP - Proc. 32nd Int\u2019l Conf. Mach. Learning (ICML), Francis Bach and David Blei (Eds.),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Machine Teaching for Bayesian Learners in the Exponential Family", "author": ["X. Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 14, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 15, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 17, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 21, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 23, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 30, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 32, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 35, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 36, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 37, "context": "1 These kinds of attack have been reported against anti-virus engines, anti-spam filters, and systems aimed to detect fake profiles or news in social networks \u2013 all problems involving a well-crafted deployment of machine learning algorithms [8, 16, 17, 19, 23, 25, 32, 34, 37\u201339].", "startOffset": 241, "endOffset": 279}, {"referenceID": 6, "context": "This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].", "startOffset": 147, "endOffset": 158}, {"referenceID": 14, "context": "This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].", "startOffset": 147, "endOffset": 158}, {"referenceID": 15, "context": "This recent research field aims at understanding the security properties of current learning algorithms, as well as at developing more secure ones [7, 16, 17].", "startOffset": 147, "endOffset": 158}, {"referenceID": 15, "context": ", technologies relying upon the collection of large amounts of data in the wild [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 17, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 21, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 23, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 30, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 37, "context": "In a poisoning attack, the attacker is assumed to control a fraction of the training data used by the learning algorithm, with the goal of subverting the entire learning process, or facilitate subsequent system evasion [8, 19, 23, 25, 32, 39].", "startOffset": 219, "endOffset": 242}, {"referenceID": 32, "context": "PDFRate2 is an online malware detection tool that analyzes the submitted PDF files to reveal the presence of embedded malware [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 17, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 18, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 21, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 23, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 30, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 37, "context": "Previous work has developed poisoning attacks against popular learning algorithms like Support Vector Machines (SVMs), LASSO, logistic and ridge regression, in different applications, like spam and malware detection [8, 19, 20, 23, 25, 32, 39].", "startOffset": 216, "endOffset": 243}, {"referenceID": 18, "context": "The main technical difficulty in devising a poisoning attack is the computation of the poisoning samples, also recently referred to as adversarial training examples [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "This requires solving a bilevel optimization problem in which the outer optimization amounts to maximizing the classification error on an untainted validation set, while the inner optimization corresponds to training the learning algorithm on the poisoned data [23].", "startOffset": 261, "endOffset": 265}, {"referenceID": 7, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 18, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 21, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 37, "context": "The latter consists of replacing the inner optimization problem with its stationarity (Karush-Kuhn-Tucker, KKT) conditions to derive an implicit equation for the gradient [8, 20, 23, 39].", "startOffset": 171, "endOffset": 186}, {"referenceID": 0, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 1, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 6, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 14, "context": "In this work, we overcome these limitations by first extending the threat model proposed in [1, 2, 7, 16] to account for multiclass poisoning attacks (Sect.", "startOffset": 92, "endOffset": 105}, {"referenceID": 2, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 12, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 20, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 29, "context": "We then exploit a recent technique called back-gradient optimization, originally proposed for hyperparameter optimization [3, 14, 22, 31], to implement a much more computationally-efficient poisoning attack.", "startOffset": 122, "endOffset": 137}, {"referenceID": 5, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 22, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 25, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 35, "context": ", attacks aimed to evade a trained classifier at test time [6, 24, 27, 37], but never for poisoning attacks.", "startOffset": 59, "endOffset": 74}, {"referenceID": 0, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 82, "endOffset": 92}, {"referenceID": 1, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 82, "endOffset": 92}, {"referenceID": 14, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 82, "endOffset": 92}, {"referenceID": 6, "context": "2 THREAT MODEL In this section, we summarize the framework originally proposed in [1, 2, 16] and subsequently extended in [7], which enables one to envision different attack scenarios against learning algorithms (including deep learning ones), and to craft the corresponding attack samples.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 6, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 7, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 14, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 21, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 37, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 117, "endOffset": 134}, {"referenceID": 25, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 246, "endOffset": 258}, {"referenceID": 26, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 246, "endOffset": 258}, {"referenceID": 34, "context": "Remarkably, these include attacks at training and at test time, usually referred to as poisoning and evasion attacks [6\u20138, 16, 23, 39] or, more recently, as adversarial (training and test) examples (when crafted against deep learning algorithms) [27, 28, 36].", "startOffset": 246, "endOffset": 258}, {"referenceID": 7, "context": "In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.", "startOffset": 20, "endOffset": 31}, {"referenceID": 21, "context": "In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.", "startOffset": 20, "endOffset": 31}, {"referenceID": 37, "context": "In this work, as in [8, 23, 39], we however consider only the optimization of the model parameters, and not of its hyperparameters.", "startOffset": 20, "endOffset": 31}, {"referenceID": 26, "context": "6In [28], the authors defined targeted and indiscriminate attacks (at test time) depending on whether the attacker aims to cause specific or generic errors.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 1, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 3, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 6, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 8, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 9, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 14, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 37, "context": "Here we do not follow their naming convention, as it can cause confusion with the interpretation of targeted and indiscriminate attacks introduced in previous work [1, 2, 4, 7, 9, 10, 16, 39].", "startOffset": 164, "endOffset": 191}, {"referenceID": 5, "context": "Experiments on the transferability of attacks among learning algorithms, firstly demonstrated in [6] and then in subsequent work on deep learners [27], fall under this category of attacks.", "startOffset": 97, "endOffset": 100}, {"referenceID": 25, "context": "Experiments on the transferability of attacks among learning algorithms, firstly demonstrated in [6] and then in subsequent work on deep learners [27], fall under this category of attacks.", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 7, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 14, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 21, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 37, "context": "These settings are more commonly referred to as poisoning and evasion attacks [2, 6\u20138, 16, 23, 39].", "startOffset": 78, "endOffset": 98}, {"referenceID": 7, "context": "The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service.", "startOffset": 53, "endOffset": 64}, {"referenceID": 21, "context": "The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service.", "startOffset": 53, "endOffset": 64}, {"referenceID": 37, "context": "The most common scenario considered in previous work [8, 23, 39] considers poisoning twoclass learning algorithms to cause a denial of service.", "startOffset": 53, "endOffset": 64}, {"referenceID": 7, "context": "As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 21, "context": "As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 37, "context": "As in [8, 23, 39], this poisoning attack (as any other poisoning attack) requires solving a bilevel optimization, where the inner problem is the learning problem.", "startOffset": 6, "endOffset": 17}, {"referenceID": 7, "context": "Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.", "startOffset": 112, "endOffset": 123}, {"referenceID": 21, "context": "Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.", "startOffset": 112, "endOffset": 123}, {"referenceID": 37, "context": "Worth remarking, this formulation encompasses all previously-proposed poisoning attacks against binary learners [8, 23, 39], provided that the loss function L is selected accordingly (e.", "startOffset": 112, "endOffset": 123}, {"referenceID": 7, "context": ", using the hinge loss against SVMs [8]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 18, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 21, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 37, "context": "(2)-(3) has been solved in previous work to develop gradientbased poisoning attacks [8, 20, 23, 39].", "startOffset": 84, "endOffset": 99}, {"referenceID": 7, "context": ", in kernelized SVMs, when the poisoning points are support vectors [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "To overcome this limitation, we exploit a recent technique called back-gradient optimization [14, 22], which allows computing the gradient of interest in a more computationally-efficient and stabler manner.", "startOffset": 93, "endOffset": 101}, {"referenceID": 20, "context": "To overcome this limitation, we exploit a recent technique called back-gradient optimization [14, 22], which allows computing the gradient of interest in a more computationally-efficient and stabler manner.", "startOffset": 93, "endOffset": 101}, {"referenceID": 7, "context": "Before delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization.", "startOffset": 94, "endOffset": 105}, {"referenceID": 21, "context": "Before delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization.", "startOffset": 94, "endOffset": 105}, {"referenceID": 37, "context": "Before delving into the technical details, we make the same assumptions made in previous work [8, 23, 39] to reduce the complexity of Problem (2)-(3): (i) we consider the optimization of one poisoning point at a time, denoted hereafter with xc ; and (ii) we assume that its label yc is initially chosen by the attacker, and kept fixed during the optimization.", "startOffset": 94, "endOffset": 105}, {"referenceID": 7, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 18, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 21, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 37, "context": "We discuss here how Problem (5)-(6) has been solved in previous work [8, 20, 23, 39].", "startOffset": 69, "endOffset": 84}, {"referenceID": 29, "context": "For example, this holds if the learning problem L is convex, which implies that all stationary points are global minima [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 18, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 21, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 37, "context": "In fact, poisoning attacks have been developed so far only against learning algorithms with convex objectives [8, 20, 23, 39].", "startOffset": 110, "endOffset": 125}, {"referenceID": 37, "context": "As in [39], the idea is to perform several passes over the set of poisoning samples, using Algorithm 1 to optimize each poisoning point at a time, while keeping the other points fixed.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 18, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 21, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 37, "context": "This is the state-of-the-art approach used to implement current poisoning attacks [8, 20, 23, 39].", "startOffset": 82, "endOffset": 97}, {"referenceID": 11, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 12, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 18, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 20, "context": "To mitigate these issues, as suggested in [13, 14, 20, 22], one can apply conjugate gradient descent to solve a simpler linear system, obtained by a trivial re-organization of the terms in the second part of Eq.", "startOffset": 42, "endOffset": 58}, {"referenceID": 28, "context": "The computation of the matrices \u2207x c\u2207wL and \u22072 wL can also be avoided using Hessianvector products [30]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly.", "startOffset": 14, "endOffset": 25}, {"referenceID": 21, "context": "previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly.", "startOffset": 14, "endOffset": 25}, {"referenceID": 37, "context": "previous work [8, 23, 39], it still requires the inner learning problem to be solved exactly.", "startOffset": 14, "endOffset": 25}, {"referenceID": 12, "context": "However, as these problems are always solved to a finite accuracy, it may happen that the gradient \u2207x cA is not sufficiently precise, especially if convergence thresholds are too loose [14, 22].", "startOffset": 185, "endOffset": 193}, {"referenceID": 20, "context": "However, as these problems are always solved to a finite accuracy, it may happen that the gradient \u2207x cA is not sufficiently precise, especially if convergence thresholds are too loose [14, 22].", "startOffset": 185, "endOffset": 193}, {"referenceID": 12, "context": "In this work, we overcome this limitation by exploiting back-gradient optimization [14, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "In this work, we overcome this limitation by exploiting back-gradient optimization [14, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 12, "context": "According to [14], this technique allows to compute the desired gradients in the outer problem using the parameterswT obtained from an incomplete optimization of the inner problem (afterT iterations).", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "These are indeed the elements required to compute the gradient of the outer objective with a backward pass (we refer the reader to [22] for more details).", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": ",wT and the required forward derivatives, Domke [14] and Maclaurin et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "[22] proposed to compute them directly during the backward pass, by reversing the steps followed by the learning algorithm to update them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Note finally that, as in [14, 22], the time complexity of our back-gradient descent is O(T ).", "startOffset": 25, "endOffset": 33}, {"referenceID": 20, "context": "Note finally that, as in [14, 22], the time complexity of our back-gradient descent is O(T ).", "startOffset": 25, "endOffset": 33}, {"referenceID": 18, "context": ", not just using a surrogate model trained on the deep features, as in [20]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "The Ransomware data [33] consists of 530 ransomware samples and 549 benign applications.", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "To the best of our knowledge, this has been demonstrated in [6, 27] for evasion attacks (i.", "startOffset": 60, "endOffset": 67}, {"referenceID": 25, "context": "To the best of our knowledge, this has been demonstrated in [6, 27] for evasion attacks (i.", "startOffset": 60, "endOffset": 67}, {"referenceID": 7, "context": "9Note indeed that the validation error only provides a biased estimate of the true classification error, as it is used by the attacker to optimize the poisoning points [8].", "startOffset": 168, "endOffset": 171}, {"referenceID": 19, "context": "2 Handwritten Digit Recognition We consider here the problem of handwritten digit recognition, which involves 10 classes (each corresponding to a digit, from 0 to 9), using theMNIST data [21].", "startOffset": 187, "endOffset": 191}, {"referenceID": 18, "context": ", accounting for all weight updates in each layer (instead of using a surrogate model trained on a frozen deep feature representation [20]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "To this end, we consider the convolutional neural network (CNN) proposed in [21] for classification of the MNIST digit data, which requires optimizing more than 450, 000 parameters.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "We use Algorithm 1 to craft each single poisoning point, but, similarly to [39], we optimize them iteratively, making 2 passes over the whole set of poisoning samples.", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "We also use the line search exploited in [39], instead of a fixed gradient step size, to reduce the attack complexity (i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "Notably, similarly to adversarial test examples, also poisoning samples against deep networks are visually indistinguishable from the initial image (as in [20]), while this is not the case when targeting the LR classifier.", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "This might be due to the specific shape of the decision function learned by the deep network in the input space, as explained in the case of adversarial test examples [15, 36].", "startOffset": 167, "endOffset": 175}, {"referenceID": 34, "context": "This might be due to the specific shape of the decision function learned by the deep network in the input space, as explained in the case of adversarial test examples [15, 36].", "startOffset": 167, "endOffset": 175}, {"referenceID": 10, "context": "5 RELATEDWORK Seminal work on the analysis of supervised learning in the presence of omniscient attackers that can compromise the training data has been presented in [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 16, "context": "5 RELATEDWORK Seminal work on the analysis of supervised learning in the presence of omniscient attackers that can compromise the training data has been presented in [12, 18].", "startOffset": 166, "endOffset": 174}, {"referenceID": 17, "context": "The first practical poisoning attacks against two-class classification algorithms have been proposed in [19, 26], in the context of spam filtering and anomaly detection.", "startOffset": 104, "endOffset": 112}, {"referenceID": 24, "context": "The first practical poisoning attacks against two-class classification algorithms have been proposed in [19, 26], in the context of spam filtering and anomaly detection.", "startOffset": 104, "endOffset": 112}, {"referenceID": 7, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 18, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 21, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 37, "context": "More systematic attacks, based on the exploitation of KKT conditions to solve the bilevel problem corresponding to poisoning attacks have been subsequently proposed in [8, 20, 23, 39].", "startOffset": 168, "endOffset": 183}, {"referenceID": 7, "context": "[8] have been the first to demonstrate the vulnerability of SVMs to poisoning attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "[39] have shown how to poison LASSO, ridge regression, and the elastic net.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Finally, Mei and Zhu [23] has systematized such attacks under a unified framework to poison convex learning algorithms with Tikhonov regularizers, 0 10 20 0", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "based on the concept of machine teaching [29, 40].", "startOffset": 41, "endOffset": 49}, {"referenceID": 38, "context": "based on the concept of machine teaching [29, 40].", "startOffset": 41, "endOffset": 49}, {"referenceID": 18, "context": "Note also that, despite recent work [20] has provided a first proof of concept of the existence of adversarial training examples against deep networks, this has been shown on a binary classification task using a surrogate model (attacked with standard KKT-based poisoning).", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": ", adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks.", "startOffset": 32, "endOffset": 39}, {"referenceID": 35, "context": ", adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks.", "startOffset": 32, "endOffset": 39}, {"referenceID": 25, "context": ", adversarial test examples) in [6, 37] against SVMs and NNs, and subsequently in [27] against deep networks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "At the same time, the variability and sophistication of cyberattacks have tremendously increased, making machine learning systems an appealing target for cybercriminals [2, 16].", "startOffset": 169, "endOffset": 176}, {"referenceID": 14, "context": "At the same time, the variability and sophistication of cyberattacks have tremendously increased, making machine learning systems an appealing target for cybercriminals [2, 16].", "startOffset": 169, "endOffset": 176}, {"referenceID": 7, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 18, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 21, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 37, "context": "While previous work has shown the effectiveness of such attacks against binary learners [8, 20, 23, 39], in this work we have been the first to consider poisoning attacks in multiclass classification settings.", "startOffset": 88, "endOffset": 103}, {"referenceID": 0, "context": "To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.", "startOffset": 73, "endOffset": 83}, {"referenceID": 1, "context": "To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.", "startOffset": 73, "endOffset": 83}, {"referenceID": 14, "context": "To this end, we have extended the commonly-used threat model proposed in [1, 2, 16] by introducing the concept of error specificity, to denote whether the attacker aims to cause specific misclassification errors (i.", "startOffset": 73, "endOffset": 83}, {"referenceID": 7, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 18, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 21, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 37, "context": "Another important contribution of this work has been to overcome the limitations of state-of-the-art poisoning attacks, which require exploiting the stationarity (KKT) conditions of the attacked learning algorithms to optimize the poisoning samples [8, 20, 23, 39].", "startOffset": 249, "endOffset": 264}, {"referenceID": 12, "context": "In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31].", "startOffset": 126, "endOffset": 138}, {"referenceID": 20, "context": "In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31].", "startOffset": 126, "endOffset": 138}, {"referenceID": 29, "context": "In this work, we have overcome these limitations by proposing a novel poisoning algorithm based on back-gradient optimization [14, 22, 31].", "startOffset": 126, "endOffset": 138}, {"referenceID": 5, "context": "adversarial test examples) [6, 27, 37].", "startOffset": 27, "endOffset": 38}, {"referenceID": 25, "context": "adversarial test examples) [6, 27, 37].", "startOffset": 27, "endOffset": 38}, {"referenceID": 35, "context": "adversarial test examples) [6, 27, 37].", "startOffset": 27, "endOffset": 38}, {"referenceID": 13, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 384, "endOffset": 392}, {"referenceID": 22, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 384, "endOffset": 392}, {"referenceID": 4, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 539, "endOffset": 550}, {"referenceID": 30, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 539, "endOffset": 550}, {"referenceID": 33, "context": "Besides the extension and evaluation of this poisoning attack strategy to different deep learning architectures and nonparametric models, further research avenues include: the investigation of the existence of universal perturbations (not dependent on the initial attack point) for poisoning samples against deep networks, similarly to the case of universal adversarial test examples [15, 24]; and the evaluation of defense mechanisms against poisoning attacks, through the exploitation of data sanitization and robust learning algorithms [5, 32, 35].", "startOffset": 539, "endOffset": 550}], "year": 2017, "abstractText": "A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we first extend the definition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic differentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradientbased procedures, including neural networks and deep learning architectures. We empirically evaluate its effectiveness on several application examples, including spam filtering, malware detection, and handwritten digit recognition. We finally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across different learning algorithms. CCS CONCEPTS \u2022 Computing Methodologies\u2192Machine Learning;", "creator": "LaTeX with hyperref package"}}}