{"id": "1708.08712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Neural Machine Translation Training in a Multi-Domain Scenario", "abstract": "in this scientific paper, we explore six alternative ways to anal train effectively a sophisticated neural assisted machine translation system configured in a detailed multi - domain cognitive scenario. ideally we investigate data concatenation ( experimentation with fine scale tuning ), geometric model stacking ( and multi - region level fine tuning ), classical data selection and weighted ensemble. we constantly evaluate these interactive methods equally based strongly on three criteria : usually i ) evaluating translation quality, ii ) training ensemble time, and iii ) design robustness towards out - of - domain equivalence tests. our preliminary findings on arabic - english hybrid and german - ukrainian english language pairs simulation show that selecting the best translation quality can be achieved by building an initial system set on within a concatenation set of available out - of - data domain data stored and then fine - finishing tuning it on in - domain data. simulation model stacking also works somewhat best when conceptual training evaluation begins with the furthest out - sequence of - domain raw data and eventually the model unit is incrementally remarkably fine - tuned with the approximately next remaining furthest significant domain change and thereafter so on. data selection did undoubtedly not give the data best results, but can be roughly considered perhaps as a pretty decent linguistic compromise between specified training time values and translation quality. a weighted selection ensemble of approximately different individual models performed better than data selection. it therefore is beneficial in a numerical scenario setup when demonstrating there is no spare time for fine - end tuning.", "histories": [["v1", "Tue, 29 Aug 2017 11:56:41 GMT  (1738kb,D)", "http://arxiv.org/abs/1708.08712v1", "7 pages"], ["v2", "Sun, 3 Sep 2017 13:01:59 GMT  (1739kb,D)", "http://arxiv.org/abs/1708.08712v2", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hassan sajjad", "nadir durrani", "fahim dalvi", "yonatan belinkov", "stephan vogel"], "accepted": false, "id": "1708.08712"}, "pdf": {"name": "1708.08712.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Training in a Multi-Domain Scenario", "authors": ["Hassan Sajjad", "Nadir Durrani", "Fahim Dalvi", "Yonatan Belinkov", "Stephan Vogel"], "emails": ["svogel}@qf.org.qa", "belinkov@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) systems are sensitive to the data they are trained on. The available parallel corpora come from various genres and have different stylistic variations and semantic ambiguities. While such data is often beneficial for a general purpose machine translation system, a problem arises when building systems for specific domains such as lectures (Cettolo et al., 2014), patents (Fujii et al., 2010) or medical text\n(Bojar et al., 2014), where either the in-domain bilingual text does not exist or is available in small quantities.\nDomain adaptation aims to preserve the identity of the in-domain data while exploiting the out-ofdomain data in favor of the in-domain and avoiding possible drift towards out-of-domain jargon and style. The most commonly used approach to train a domain-specific neural MT system is to fine-tune an existing model (trained on generic data) with the new domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu et al., 2017) or to add domain-aware tags in building a concatenated system (Kobus et al., 2016).\nIn this paper we explore NMT in a multidomain scenario. Considering a small in-domain corpus and a number of out-of-domain corpora, we target questions like:\n\u2022 What are different ways to combine multiple domains during a training process?\n\u2022 What is the best strategy to build an optimal in-domain system?\n\u2022 Which training strategy results in a robust system?\n\u2022 Which strategy should be used to build a decent in-domain system given limited time?\nTo answer these, we try the following approaches: i) data concatenation: train a system by concatenating all the available in-domain and out-ofdomain data; ii) model stacking: build NMT in an online fashion starting from the most distant domain, fine-tune on the closer domain and finish by fine-tuning the model on the in-domain data; iii) data selection: select a certain percentage of the available out-of-domain corpora that is most\nar X\niv :1\n70 8.\n08 71\n2v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 7\nclosest to the in-domain data and use it for training the system; iv) multi-model ensemble: separately train models for each available domain and combine them during decoding using balanced or weighted averaging. We experiment with ArabicEnglish and German-English language pairs. Our results demonstrate the following findings:\n\u2022 A concatenated system fine-tuned on the indomain data achieves the most optimal indomain system.\n\u2022 Model stacking works best when starting from the furthest domain, fine-tuning on closer domains and then finally fine-tuning on the in-domain data.\n\u2022 A concatenated system on all available data results in the most robust system.\n\u2022 Data selection gives a decent trade-off between translation quality and training time.\n\u2022 Weighted ensemble is helpful when several individual models have been already trained and there is no time for retraining/fine-tuning.\nThe paper is organized as follows: Section 2 describes the adaptation approaches explored in this work. We present experimental design in Section 3. Section 4 summarizes the results and Section 5 concludes."}, {"heading": "2 Approaches", "text": "Consider an in-domain data Di and a set of outof-domain data Do = Do1 , Do2 , ..Don . We explore several methods to benefit from the available data with an aim to optimize translation quality on the in-domain data. Specifically, we try data concatenation, model stacking, data selection and ensemble. Figure 1 presents them graphically. In the following, we describe each approach briefly."}, {"heading": "2.1 Concatenation", "text": "A na\u0131\u0308ve yet commonly used method when training both statistical (Williams et al., 2016)1 and neural machine translation systems (Sennrich et al., 2016a) is to simply concatenate all the bilingual parallel data before training the system. During training an in-domain validation set is used to\n1State-of-the-art baselines are trained on plain concatenation of the data with MT feature functions (such as Language Model) skewed towards in-domain data, through interpolation.\nguide the training loss. The resulting system has an advantage of seeing a mix of all available data at every time interval, and is thus robust to handle heterogeneous test data."}, {"heading": "2.2 Fine Tuning and Model Stacking", "text": "Neural machine translation follows an online training strategy. It sees only a small portion of the data in every training step and estimates the value of network parameters based on that portion. Previous work has exploited this strategy in the context of domain adaptation. Luong and Manning (2015) trained an initial model on an out-ofdomain data and later extended the training on indomain data. In this way the final model parameters are tuned towards the in-domain data. The approach is referred as fine-tuning later on.\nSince in this work we deal with several domains, we propose a stacking method that uses multi-level fine-tuning to train a system. Figure 1 (second row) shows the complete procedure: first, the model is trained on the out-of-domain data Do1 for N epochs; training is resumed from N + 1-th epoch to the M -th epoch but using the next available out-of-domain data Do2 ; repeat the process till all of the available out-of-domain corpora have been used; in the last step, resume training on the in-domain data Di for a few epochs. The resulting model has seen all of the available data as in the case of the data concatenation approach. However, here the system learns from the data domain by domain. We call this technique model stacking.\nThe model stacking and fine-tuning approaches have the advantage of seeing the in-domain data in the end of training, thus making the system parameters more optimized for the in-domain data. They also provide flexibility in extending an existing model to any new domain without having to retrain the complete system again on the available corpora."}, {"heading": "2.3 Data Selection", "text": "Building a model, whether concatenated or stacked, on all the available data is computationally expensive. An alternative approach is data selection, where we select a part of the out-ofdomain data which is close to the in-domain data for training. The intuition here is two fold: i) the out-of-domain data is huge and takes a lot of time to train on, and ii) not all parts of the outof-domain data are beneficial for the in-domain data. Training only on a selected part of the out-ofdomain data reduces the training time significantly while at the same time creating a model closer to the in-domain.\nIn this work, we use the modified Moore-Lewis (Axelrod et al., 2011) for data selection. It trains in- and out-of-domain n-gram models and then ranks sequences in the out-of-domain data based on cross-entropy difference. The out-of-domain sentences below a certain threshold are selected for training. Since we are dealing with several out-of-domain corpora, we apply data selection separately on each of them and build a concatenated system using in-domain plus selected outof-domain data as shown in Figure 1. Data selection significantly reduces data size thus improving training time for NMT. However, finding the optimal threshold to filter data is a cumbersome process. We explore data selection as an alternative to the above mentioned techniques."}, {"heading": "2.4 Multi-domain Ensemble", "text": "Out-of-domain data is generally available in larger quantity. Training a concatenated system whenever a new in-domain becomes available is expensive in terms of both time and computation. An alternative to fine-tuning the system with new indomain is to do ensemble of the new model with the existing model. The ensemble approach brings the flexibility to use them during decoding without a need of retraining and fine-tuning.\nConsider N models that we would like to use to generate translations. For each decoding step, we\nuse the scores over the vocabulary from each of these N models and combine them by averaging. We then use these averaged scores to choose the output word(s) for each hypothesis in our beam. The intuition is to combine the knowledge of the N models to generate a translation. We refer to this approach as balanced ensemble later on. Since here we deal with several different domains, averaging scores of all the models equally may not result in optimum performance. We explore a variation of balanced ensemble called weighted ensemble that performs a weighted average of these scores, where the weights can be pre-defined or learned on a development set.\nBalanced ensemble using several models of a single training run saved at different iterations has shown to improve performance by 1-2 BLEU points (Sennrich et al., 2016a). Here our goal is not to improve the best available system but to benefit from individual models built using several domains during a single decoding process. We experiment with both balanced ensemble and weighted ensemble under the multi-domain condition only."}, {"heading": "3 Experimental Design", "text": ""}, {"heading": "3.1 Data", "text": "We experiment with Arabic-English and GermanEnglish language pairs using the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016 as our in-domain data. For Arabic-English, we take the UN corpus (Ziemski et al., 2016) and the OPUS corpus (Lison and Tiedemann, 2016) as out-of-domain corpora. For German-English, we use the Europarl (EP), and the Common Crawl (CC) corpora made available for the 1st Conference on Statistical Machine Translation2 as outof-domain corpus. We tokenize Arabic, German and English using the default Moses tokenizer. We did not do morphological segmentation of Arabic. Instead we apply sub-word based segmentation (Sennrich et al., 2016b) that implicitly segment as part of the compression process.3 Table 1 shows the data statistics after running the Moses tokenizer.\nWe use a concatenation of dev2010 and tst2010 sets for validation during training. Test sets tst2011 and tst2012 served as development sets to\n2http://www.statmt.org/wmt16/translation-task.html 3Sajjad et al. (2017) showed that using BPE performs as\nwell as morphological tokenization in Arabic-to-English.\nfind the best model for fine-tuning and tst2013 and tst2014 are used for evaluation. We use BLEU (Papineni et al., 2002) to measure performance."}, {"heading": "3.2 System Settings", "text": "We use the Nematus tool (Sennrich et al., 2017) to train a 2-layered LSTM encoder-decoder with attention (Bahdanau et al., 2015). We use the default settings: embedding layer size: 512, hidden layer size: 1000. We limit the vocabulary to 50k words using BPE (Sennrich et al., 2016b) with 50,000 operations."}, {"heading": "4 Results", "text": "In this section, we empirically compare several approaches to combine in- and out-of-domain data to train an NMT system. Figure 2 and Figure 3 show the learning curve on development sets using various approaches mentioned in this work. We will go through them individually later in this section."}, {"heading": "4.1 Individual Systems", "text": "We trained systems on each domain individually (for 10 epochs)4 and chose the best model using the development set. We tested every model on the in-domain testsets. Table 2 shows the results. On Arabic-English, the system trained on the out-ofdomain data OPUS performed the best. This is due to the large size of the corpus and its spoken nature which makes it close to TED in style and genre. However, despite the large size of UN, the system trained using UN performed poorly. The reason is\n4For German-English, we ran the models until they converged because the training data is much smaller compared to Arabic-English direction\nthe difference in genre of UN from the TED corpus where the former consists of United Nations proceedings and the latter is based on talks.\nFor German-English, the systems built using out-of-domain corpora performed better than the in-domain corpus. The CC corpus appeared to be very close to the TED domain. The system trained on it performed even better than the in-domain system by an average of 2 BLEU points."}, {"heading": "4.2 Concatenation and Fine-tuning", "text": "Next we evaluated how the models performed when trained on concatenated data. We mainly tried two variations: i) concatenating all the available data (ALL) ii) combine only the available outof-domain data (OD) and later fine-tune the model on the in-domain data. Table 3 shows the results. The fine-tuned system outperformed a full concatenated system by 1.8 and 2.1 average BLEU points in Arabic-English and German-English systems respectively.\nLooking at the development life line of these systems (Figures 2, 3), since ALL has seen all of the data, it is better than OD till the point OD is fine-tuned on the in-domain corpus. Interestingly, at that point ALL and OD\u2192TED have seen the same amount of data but the parameters of the latter model are fine-tuned towards the in-domain data. This gives it average improvements of up to 2 BLEU points over ALL.\nThe ALL system does not give any explicit weight to any domain 5 during training. In order to revive the in-domain data, we fine-tuned it on the in-domain data. We achieved comparable results to that of the OD\u2192TED model which means that one can adapt an already trained model on all the available data to a specific domain by fine tuning\n5other than the data size itself\nit on the domain of interest. This can be helpful in cases where in-domain data is not known beforehand."}, {"heading": "4.3 Model Stacking", "text": "Previously we concatenated all out-of-domain data and fine-tuned it with the in-domain TED corpus. In this approach, we picked one out-ofdomain corpus at a time, trained a model and finetuned it with the other available domain. We repeated this process till all out-of-domain data had been used. In the last step, we fine-tuned the model on the in-domain data. Since we have a number of out-of-domain corpora available, we experimented with using them in different permutations for training and analyzed their effect on the\ndevelopment sets. Figure 2 and Figure 3 show the results. It is interesting to see that the order of stacking has a significant effect on achieving a high quality system. The best combination for the Arabic-English language pair started with the UN data, fine-tuned on OPUS and then fine-tuned on TED. When we started with OPUS and finetuned the model on UN, the results dropped drastically as shown in Figure 2 (see OPUS\u2192UN). The model started forgetting the previously used data and focused on the newly provided data which is very distant from the in-domain data. We saw similar trends in the case of German-English language pair where CC\u2192EP dropped the performance drastically. We did not fine-tune CC\u2192EP and OPUS\u2192UN on TED since there was no better model to fine-tune than to completely ignore the second corpus i.e. UN and EP for Arabic and German respectively and fine-tune OPUS and CC on TED. The results of OPUS\u2192TED and CC\u2192TED are shown in Figures.\nComparing the OPUS\u2192TED system with the UN\u2192OPUS\u2192TED system, the result of OPUS\u2192TED are lowered by 0.62 BLEU points from the UN\u2192OPUS\u2192TED system. Similarly, we saw a drop of 0.4 BLEU points for GermanEnglish language pair when we did not use EP and directly fine-tuned CC on TED. There are two ways to look at these results, considering quality vs. time: i) by using UN and EP in model stacking, the model learned to remember only those parts of the data that are beneficial for achieving\nbetter translation quality on the in-domain development sets. Thus using them as part of the training pipeline is helpful for building a better system. ii) training on UN and EP is expensive. Dropping them from the pipeline significantly reduced the training time and resulted in a loss of 0.62 and 0.4 BLEU points only.\nTo summarize, model stacking performs best when it starts from the domain furthest from the in-domain data. In the following, we compare it with the data concatenation approach."}, {"heading": "4.4 Stacking versus Concatenation", "text": "We compared model stacking with different forms of concatenation. In terms of data usage, all models are exposed to identical data. Table 4 shows the results. The best systems are achieved using a concatenation of all of the out-of-domain data for initial model training and then fine-tuning the trained model on the in-domain data. The concatenated system ALL performed the lowest among all.\nALL learned a generic model from all the available data without giving explicit weight to any particular domain whereas model stacking resulted in a specialized system for the in-domain data. In order to confirm the generalization ability of ALL vs. model stacking, we tested them on a new domain, News. ALL performed 4 BLEU points better than model stacking in translating the news NIST MT04 testset. This concludes that a concatenation system is not an optimum solution for one particular domain but is robust enough to perform well in new testing conditions."}, {"heading": "4.5 Data Selection", "text": "Since training on large out-of-domain data is time inefficient, we selected a small portion of out-ofdomain data that is closer to the in-domain data. For Arabic-English, we selected 3% and 5% from the UN and OPUS data respectively which constitutes roughly 2M sentences. For German-English, we selected 20% from a concatenation of EP and CC, which roughly constitutes 1M training sentences.6\nWe concatenated the selected data and the indomain data to train an NMT system. Table 5 presents the results. The selected system is worse than the ALL system. This is in contrary to the results mentioned in the literature on phrase-based machine translation where data selection on UN improves translation quality (Sajjad et al., 2013).\n6These data-selection percentages have been previously found to be optimal when training phrase-based systems using the same data. For example see (Sajjad et al., 2013).\nThis shows that NMT is not as sensitive as phrasebased to the presence of the out-of-domain data.\nData selection comes with a cost of reduced translation quality. However, the selected system is better than all individual systems shown in Table 2. Each of these out-of-domain systems take more time to train than a selected system. For example, compared to individual UN system, the selected system took approximately 1/10th of the time to train. One can look at data selected system as a decent trade-off between training time and translation quality."}, {"heading": "4.6 Multi-domain Ensemble", "text": "We took the best model for every domain according to the average BLEU on the development sets and ensembled them during decoding. For weighted ensemble, we did a grid search and selected the weights using the development set. Table 6 presents the results of an ensemble on the Arabic-English language pair and compares them with the individual best model, OPUS, and a model built on ALL. As expected, balanced ensemble (ENSb) dropped results compared to the best individual model. Since the domains are very distant, giving equal weights to them hurts the overall performance. The weighted ensemble (ENSw) improved from the best individual model by 1.8 BLEU points but is still lower than the concatenated system by 1.7 BLEU points. The weighted ensemble approach is beneficial when individual domain specific models are already available for testing. Decoding with multiple models is more efficient compared to training a system from scratch on a concatenation of the entire data."}, {"heading": "4.7 Discussion", "text": "The concatenation system showed robust behavior in translating new domains. Kobus et al. (2016) proposed a domain aware concatenated system by introducing domain tags for every domain. We trained a system using their approach and com-\npared the results with simple concatenated system. The domain aware system performed slightly better than the concatenated system (up to 0.3 BLEU points) when tested on the in-domain TED development sets. However, domain tags bring a limitation to the model since it can only be tested on the domains it is trained on. Testing on an unknown domain would first require to find its closest domain from the set of domains the model is trained on. The system can then use that tag to translate unknown domain sentences."}, {"heading": "5 Conclusion", "text": "We explored several approaches to train a neural machine translation system under multi-domain conditions and evaluated them based on three metrics: translation quality, training time and robustness. Our results showed that an optimum indomain system can be built using a concatenation of the out-of-domain data and then fine-tuning it on the in-domain data. A system built on the concatenated data resulted in a generic system that is robust to new domains. Model stacking is sensitive to the order of domains it is trained on. Data selection and weighted ensemble resulted in a less optimal solution. The former is efficient to train in a short time and the latter is useful when different individual models are available for testing. It provides a mix of all domains without retraining or fine-tuning the system."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Edinburgh, United Kingdom, EMNLP \u201911.", "citeRegEx": "Axelrod et al\\.,? 2011", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR. http://arxiv.org/pdf/1409.0473v6.pdf.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2014 workshop", "author": ["Ondrej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-Amand", "Radu Soricut", "Lucia Specia", "Ale\u0161 Tamchyna"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "An Arabic-Hebrew parallel corpus of TED talks", "author": ["Mauro Cettolo."], "venue": "Proceedings of the AMTA Workshop on Semitic Machine Translation (SeMaT). Austin, US-TX.", "citeRegEx": "Cettolo.,? 2016", "shortCiteRegEx": "Cettolo.", "year": 2016}, {"title": "Report on the 11th IWSLT Evaluation Campaign", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "Proceedings of the International Workshop on Spoken Language Translation, Lake Tahoe, US .", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "An empirical comparison of simple domain adaptation methods for neural machine translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "CoRR abs/1701.03214. http://arxiv.org/abs/1701.03214.", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "CoRR abs/1612.06897. http://arxiv.org/abs/1612.06897.", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "Overview of the patent translation task at the ntcir-8 workshop", "author": ["Atsushi Fujii", "Masao Utiyama", "Mikio Yamamoto", "Takehito Utsuro."], "venue": "In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technolo-", "citeRegEx": "Fujii et al\\.,? 2010", "shortCiteRegEx": "Fujii et al\\.", "year": 2010}, {"title": "Domain control for neural machine translation", "author": ["Catherine Kobus", "Josep Maria Crego", "Jean Senellart."], "venue": "CoRR abs/1612.06140. http://arxiv.org/abs/1612.06140.", "citeRegEx": "Kobus et al\\.,? 2016", "shortCiteRegEx": "Kobus et al\\.", "year": 2016}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles", "author": ["Pierre Lison", "Jrg Tiedemann."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language", "citeRegEx": "Lison and Tiedemann.,? 2016", "shortCiteRegEx": "Lison and Tiedemann.", "year": 2016}, {"title": "Stanford Neural Machine Translation Systems for Spoken Language Domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the International Workshop on Spoken Language Translation. Da Nang, Vietnam.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the Association for Computational Linguistics (ACL\u201902). Philadelphia, PA, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic spoken language translation", "author": ["Hassan Sajjad", "Francisco Guzmn", "Preslav Nakov", "Ahmed Abdelali", "Kenton Murray", "Fahad Al Obaidli", "Stephan Vogel."], "venue": "Proceedings of the", "citeRegEx": "Sajjad et al\\.,? 2013", "shortCiteRegEx": "Sajjad et al\\.", "year": 2013}, {"title": "Nematus: a toolkit", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u201daubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 371\u2013", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Domain specialization: a post-training domain adaptation for neural machine translation", "author": ["Christophe Servan", "Josep Maria Crego", "Jean Senellart."], "venue": "CoRR abs/1612.06141. http://arxiv.org/abs/1612.06141.", "citeRegEx": "Servan et al\\.,? 2016", "shortCiteRegEx": "Servan et al\\.", "year": 2016}, {"title": "Edinburgh\u2019s statistical machine translation systems for wmt16", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Barry Haddow", "Ond\u0159ej Bojar."], "venue": "Proceedings of the First Conference on Machine Translation. Association for", "citeRegEx": "Williams et al\\.,? 2016", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["Michal Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen"], "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoroz\u030c,", "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "While such data is often beneficial for a general purpose machine translation system, a problem arises when building systems for specific domains such as lectures (Cettolo et al., 2014), patents (Fujii et al.", "startOffset": 163, "endOffset": 185}, {"referenceID": 7, "context": ", 2014), patents (Fujii et al., 2010) or medical text (Bojar et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 2, "context": ", 2010) or medical text (Bojar et al., 2014), where either the in-domain bilingual text does not exist or is available in small quantities.", "startOffset": 24, "endOffset": 44}, {"referenceID": 10, "context": "data) with the new domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu et al., 2017) or to add domain-aware tags in building a concatenated system (Kobus et al.", "startOffset": 26, "endOffset": 120}, {"referenceID": 6, "context": "data) with the new domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu et al., 2017) or to add domain-aware tags in building a concatenated system (Kobus et al.", "startOffset": 26, "endOffset": 120}, {"referenceID": 16, "context": "data) with the new domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu et al., 2017) or to add domain-aware tags in building a concatenated system (Kobus et al.", "startOffset": 26, "endOffset": 120}, {"referenceID": 5, "context": "data) with the new domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu et al., 2017) or to add domain-aware tags in building a concatenated system (Kobus et al.", "startOffset": 26, "endOffset": 120}, {"referenceID": 8, "context": ", 2017) or to add domain-aware tags in building a concatenated system (Kobus et al., 2016).", "startOffset": 70, "endOffset": 90}, {"referenceID": 17, "context": "A na\u0131\u0308ve yet commonly used method when training both statistical (Williams et al., 2016)1 and neural machine translation systems (Sennrich et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 14, "context": ", 2016)1 and neural machine translation systems (Sennrich et al., 2016a) is to simply concatenate all the bilingual parallel data before training the system.", "startOffset": 48, "endOffset": 72}, {"referenceID": 10, "context": "Luong and Manning (2015) trained an initial model on an out-ofdomain data and later extended the training on indomain data.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "In this work, we use the modified Moore-Lewis (Axelrod et al., 2011) for data selection.", "startOffset": 46, "endOffset": 68}, {"referenceID": 14, "context": "points (Sennrich et al., 2016a).", "startOffset": 7, "endOffset": 31}, {"referenceID": 3, "context": "We experiment with Arabic-English and GermanEnglish language pairs using the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016 as our in-domain data.", "startOffset": 93, "endOffset": 108}, {"referenceID": 18, "context": "For Arabic-English, we take the UN corpus (Ziemski et al., 2016) and the OPUS corpus (Lison and Tiedemann, 2016) as out-of-domain corpora.", "startOffset": 42, "endOffset": 64}, {"referenceID": 9, "context": ", 2016) and the OPUS corpus (Lison and Tiedemann, 2016) as out-of-domain corpora.", "startOffset": 28, "endOffset": 55}, {"referenceID": 15, "context": "Instead we apply sub-word based segmentation (Sennrich et al., 2016b) that implicitly segment as part of the compression process.", "startOffset": 45, "endOffset": 69}, {"referenceID": 12, "context": "html Sajjad et al. (2017) showed that using BPE performs as well as morphological tokenization in Arabic-to-English.", "startOffset": 5, "endOffset": 26}, {"referenceID": 11, "context": "We use BLEU (Papineni et al., 2002) to measure performance.", "startOffset": 12, "endOffset": 35}, {"referenceID": 13, "context": "We use the Nematus tool (Sennrich et al., 2017) to train a 2-layered LSTM encoder-decoder with attention (Bahdanau et al.", "startOffset": 24, "endOffset": 47}, {"referenceID": 1, "context": ", 2017) to train a 2-layered LSTM encoder-decoder with attention (Bahdanau et al., 2015).", "startOffset": 65, "endOffset": 88}, {"referenceID": 15, "context": "using BPE (Sennrich et al., 2016b) with 50,000 operations.", "startOffset": 10, "endOffset": 34}, {"referenceID": 12, "context": "This is in contrary to the results mentioned in the literature on phrase-based machine translation where data selection on UN improves translation quality (Sajjad et al., 2013).", "startOffset": 155, "endOffset": 176}, {"referenceID": 12, "context": "For example see (Sajjad et al., 2013).", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "Kobus et al. (2016) proposed a domain aware concatenated system by introducing domain tags for every domain.", "startOffset": 0, "endOffset": 20}], "year": 2017, "abstractText": "In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble. We evaluate these methods based on three criteria: i) translation quality, ii) training time, and iii) robustness towards out-of-domain tests. Our findings on Arabic-English and German-English language pairs show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning.", "creator": "LaTeX with hyperref package"}}}