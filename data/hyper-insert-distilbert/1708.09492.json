{"id": "1708.09492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Automatically Generating Commit Messages from Diffs using Neural Machine Translation", "abstract": "commit messages currently are a reasonably valuable resource in showing comprehension levels of software evolution, since they thereby provide completely a record coverage of actual changes such as significant feature additions requests and bug repairs. more unfortunately, hardware programmers must often merely neglect to write genuinely good timing commit messages. different techniques have been tentatively proposed originally to supposedly help programmers maintain by automatically directly writing nearly these messages. these techniques are effective at confidently describing literally what changed, but are often verbose items and lack context knowledge for understanding the theoretical rationale behind a change. whereas in contrast, humans write messages that naturally are short and summarize the usual high level rationale. in this paper, we criticized adapt guided neural machine simultaneous translation ( sr nmt ) approach to automatically \" easily translate \" diffs into commit messages. if we mainly trained independently an nmt algorithm. using a corpus of diffs and human - only written commit item messages coming from already the top fifty 1k github developers projects. we designed directly a filter to help ensure that content we actually only trained as the code algorithm software on higher - quality commit object messages. our evaluation paper uncovered a mixed pattern in which the messages tasks we used generate tend primarily to be either very high threshold or very low quality. therefore, sometimes we created a robust quality - assurance filter to detect cases in which we are also unable sufficiently to produce good hardcore messages, and return for a warning instead.", "histories": [["v1", "Wed, 30 Aug 2017 22:26:48 GMT  (1143kb,D)", "http://arxiv.org/abs/1708.09492v1", "Preprint version. Accepted in ASE 2017, the 32nd IEEE/ACM International Conference on Automated Software Engineering"]], "COMMENTS": "Preprint version. Accepted in ASE 2017, the 32nd IEEE/ACM International Conference on Automated Software Engineering", "reviews": [], "SUBJECTS": "cs.SE cs.CL", "authors": ["siyuan jiang", "ameer armaly", "collin mcmillan"], "accepted": false, "id": "1708.09492"}, "pdf": {"name": "1708.09492.pdf", "metadata": {"source": "CRF", "title": "Automatically Generating Commit Messages from Diffs using Neural Machine Translation", "authors": ["Siyuan Jiang", "Ameer Armaly", "Collin McMillan"], "emails": ["cmc}@nd.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION Commit messages are natural language descriptions of changes in source code. When a programmer updates code, a typical procedure is to upload the change to a version control system with a short commit message to describe the purpose of the change, e.g., \u201cadds support for 9 inch tablet screen size.\u201d The repository stores the message alongside a diff that represents the difference between the current and previous version of the affected files. The practice is extremely common: for this paper alone, we obtained over 2M diffs and messages from just 1k projects on GitHub.\nCommit messages are useful because they help programmers to understand the high level rationale for a change without reading the low level implementation details. They serve a valuable purpose in comprehension of software evolution, and act as a record of feature additions and bug repairs [7]. Unfortunately, programmers sometimes neglect commit messages [11], [36], likely due to the same time and market pressures that have been reported to affect many types of documentation [44], [12], [24]. In short, programmers use commit messages but often avoid writing them themselves.\nAutomated generation of commit messages has been proposed as an alternative to manual efforts by programmers. For example, Buse et al. [7] describe DeltaDoc, a tool that summarizes what changed in the control flow of a program between code versions. Likewise, Cortes-Coy et al. [32] built ChangeScribe, which summarizes changes such as method\nadditions. These and other existing techniques (see Section II) have been shown to be effective in answering questions about what changed and where from one code version to another.\nWhat is missing from existing approaches is a short, high level description of the purpose behind commits. Current approaches are effective at summarizing what changed and where, but do not answer the question why [7]. Questions of why traditionally require human insight since they involve synthesis of different, complex data sources and context. However, as Mockus et al. [38] observed, many commit messages are similar and can be broadly categorized as related to bug repair, feature additions, etc. Plus, they follow similar grammatical patterns such as verb-direct object structure (e.g. \u201cadds support for...\u201d) [23]. This observation leads us to believe that the text of commit messages can be learned and predicted if there is sufficient data. Our view is in line with the hypothesis of \u201cnaturalness\u201d of software [20], that software artifacts follow patterns that can be learned from sufficiently large datasets.\nIn this paper, we adapt a neural machine translation (NMT) algorithm to the problem of commit message generation. Several NMT algorithms have been designed to translate between natural languages by training a neural network on pairs of sentences that humans have already translated. The datasets required are enormous by typical software engineering research standards, involving up to tens of millions of pairs of sentences [49], [34]. We trained an NMT algorithm using pairs of diffs and commit messages from 1k popular projects on GitHub. While we were able to obtain quite large datasets (over 2M commits), we encountered many commit messages that were gibberish or very low quality (a problem others have observed [11], [36]), which if left in the training data could be reflected in the NMT\u2019s output. Therefore, we designed a filter to ensure that we only trained the algorithm using messages with a verb-direct object pattern.\nWe investigate and report the effectiveness of the predictions from the process. We found promising results as well as key constraints on the accuracy of the predictions. In short, the NMT process performed quite well under select conditions, but poorly in others. We report these results and promising and poor conditions as a guide to other researchers and platform for advancement in this research area. To further promote advancement of the area, we make our implementation and data freely available in an online replication package.\nOur approach has two key advantages that make it a\nar X\niv :1\n70 8.\n09 49\n2v 1\n[ cs\n.S E\n] 3\n0 A\nug 2\n01 7\nsupplement to, rather than a competitor of, existing automatic commit message generation techniques. First, we produce short summary messages rather than exhaustive descriptions of code changes. And second, our approach produces messages for changes to many types of software artifact in the repository, not solely source code."}, {"heading": "A. The Problem", "text": "In this paper we target the problem of automatically generating commit messages. Commit messages are useful in the long term for program comprehension and maintainability, but cost significant time and effort in the short term. These short term pressures lead programmers to neglect writing commit messages, like other types of documentation [11], [36], [44], [12], [24]. Buse et al. [7] point out that programmers use commit messages for two reasons: 1) to summarize what changed, and 2) to briefly explain why the change was necessary. To date, research into commit message generation has exclusively focused on the question what. In this paper, we seek to begin answering why.\nExisting commit message generation techniques produce relatively long messages that include details such as the methods that were added or the number of files changes (what information). While useful, these techniques are a complement to, rather than a replacement for, high level why information that humans write such as \u201cadds support for 9 inch tablet screens.\u201d Normally, this high level information requires human judgment. But we hypothesize that there are patterns of commits, and that these patterns can be detected and used to generate messages for similar commits later. Given a large number of pairs of diffs and messages, we believe we can train an algorithm to write new messages for new commits, based on the new commits\u2019 similarity to older ones.\nPlease note that we do not claim to generate new insights for completely new types of commits \u2013 that task is likely to remain in the hands of human experts. However, we do aim to write messages that reflect knowledge that can be learned from records of previous commits. In the long run, we hope that this technology will help reduce manual effort by programmers in reading and understanding code changes in repositories."}, {"heading": "B. Paper Overview", "text": "Figure 1 depicts an overview of our paper. We have divided the work into three segments: In Part A (Section IV), we present our approach to filtering for verb/direct-object (VDO) commit message patterns and training an NMT algorithm to produce messages with this pattern. The V-DO filter was introduced because a large percentage of the messages in the repositories we downloaded were very low quality, and we needed to ensure that we trained the NMT algorithm only with examples matching an acceptable pattern. We then trained an NMT algorithm on the pairs of diffs and commit messages where the messages followed the V-DO pattern.\nIn Part B (Sections V and VI), we evaluate the quality of the commit messages produced by the algorithm with an automated method and a human study with 2 Ph.D. students\nand 18 professional programmers. We observe that while there are a significant number of positive results, there are also a significant number of negative results. Therefore, in Part C (Sections VII), we design a quality assurance (QA) filter to detect cases in which the NMT algorithm is likely to produce a negative result. We then modify our approach to produce a warning message instead of a commit message in those cases, and update our evaluation to show the effects of our modification. In short, we reduce the number of poor predicted messages by 44% at a cost of also mistakenly reducing high quality predictions by 11%."}, {"heading": "II. RELATED WORK", "text": "We split the related work into three categories: 1) the work that generates commit messages; 2) the work that summarizes source code; and 3) the work that applies deep learning algorithms in software engineering."}, {"heading": "A. Commit Message Generation Techniques", "text": "We categorize the commit message generation techniques into three groups based on the inputs of the techniques. The first group uses code changes of a commit as an input, and summarizes the changes to generate the commit message. For example, Buse et al. have built DeltaDoc, which extracts path predicates of changed statements, and follows a set of predefined rules to generate a summary [7]. Similarly, Linares-Va\u0301squez et al. have built ChangeScribe, which extracts changes between two Abstract Syntax Trees and summarizes the changes based on predefined rules [32].\nSupplementing the first group, the second group is based on related software documents. For example, Le et al. have built RCLinker, which links a bug report to the corresponding commit message [31]. Rastkar and Murphy have proposed to summarize multiple related documents for commits [43]. Integrating the ideas of the first and the second groups, Moreno et al. have built ARENA, which summarizes changes and finds related issues to generate release notes [39].\nThe third group is our technique using diffs (generated by \u201cgit diff\u201d) as inputs. Our technique is to translate a\ndiff to a natural language sentence. Our technique supplements the first group in two ways. First, the techniques in the first group often generate multi-line summaries that contain pseudocode and template text. In contrast, our technique generates one-sentence descriptions, which can be used as a headline of the multi-line summaries. Second, our technique summarizes both code and non-code changes in diffs."}, {"heading": "B. Source Code Summarization", "text": "Source code summarization techniques generate descriptions of source code pieces. The algorithms of the techniques can be adapted to generate summaries for changes in commits. Code summarization can be categorized into two groups: extractive and abstractive. Extractive summarization extracts relevant parts of source code and uses the relevant parts as a summary [16]. Abstractive summarization includes information that is not explicitly in the source code. For example, Sridhara et al. has designed a Natural Language Generation (NLG) system to create summaries of Java methods [50]. First, the NLG system finds important statements of a Java method. Second, the system uses a text generation algorithm to transform a statement to a natural language description. This algorithm has predefined text templates for different statement types, such as return statements and assignment statements. Both DeltaDoc and ChangeScribe (discussed in Section II-A) follow the similar NLG design.\nBesides the NLG approach to generate abstractive summaries, Iyer et al. have built Code-NN, which uses an Neural Machine Translation (NMT) algorithm to summarize code snippets [22]. This work is similar to our technique because our technique also uses an NMT algorithm. There are two key differences between our technique and Code-NN. First, the goal of Code-NN is summarizing code snippets and the goal of our technique is summarizing changes. Second, CodeNN parses code snippets and removed all the comments. In contrast, our technique\u2019s input is an entire diff with code, comments, and diff marks (e.g., + denoting insertion)."}, {"heading": "C. Deep Learning in Software Engineering", "text": "Deep learning algorithms are becoming more prevalent in Software Engineering research. Deep learning algorithms, as applied to software, automatically learn representations of software artifacts. For example, to detect code clones, traditional approaches predefine the representations of code fragments (some techniques use token sequences to represent code [25]; others use graphs [29], [9]). In contrast, the deep learning approach introduced by White et al. [53] learns the representations of code automatically. Similarly, deep learning algorithms are introduced in bug localization [30], software traceability [15], and code suggestions [54].\nOur technique is similar to the work done by Gu et al. [14], because both our and their techniques use Neural Machine Translation (NMT). Gu et al. use NMT to translate natural language queries to API method sequences [14]. Similarly, several code generation techniques use NMT to translate nat-\nural language to programming language [33], [42]. In contrast, our technique translates diffs to natural language.\nOur technique is also similar to the work by Alexandru et al. [5], which investigates the suitability of NMT for program comprehension. Alexandru et al. use NMT for source code tokenization and token annotation. While Alexandru et al. target on lower-level source code understanding (token-level), we target on understanding higher-level of mixtures of code and text (diff-level)."}, {"heading": "III. BACKGROUND", "text": "We split the background section into three subsections. The first subsection is about the empirical studies on commit messages, which motivate us to generate short descriptions of commits. The second subsection describes RNN EncoderDecoder, a popular Neural Network Translation model, which is an important background for the third subsection. The third subsection describes attentional RNN Encoder-Decoder, which is used in our work."}, {"heading": "A. Commit Messages", "text": "Our work is motivated by the findings of the studies by Buse et al. [7] and by Jiang and McMillan [23]. The results of the two studies indicate three things. First, commit messages are pervasive and desired. Buse et al. examined 1k commits from five mature software projects and found that 99.1% of the commits have non-empty messages. Jiang and McMillan collected over 2M commit messages from 1k projects.\nSecond, human-written commit messages are short. In Buse et al.\u2019s study, the average size of the 991 non-empty commit messages is 1.1 lines. Similarly, the study of Jiang and McMillan shows that 82% of the commit messages have only one sentence.\nThird, commit messages contain various types of information not solely summaries of code changes. Buse et al. manually analyzed 375 commit messages and found that the messages are not only about what the changes are but also about why the changes are made. Supported by the three findings, our technique aims to generate one-sentence commit messages which mimic the human-written commit messages."}, {"heading": "B. RNN Encoder-Decoder Model", "text": "Neural Machine Translation (NMT) is neural networks that model the translation process from a source language sequence x = (x1, ..., xn) to a target language sequence y = (y1, ..., yn) with the conditional probability p(y|x) [5], [35]. Cho et al. introduced RNN Encoder-Decoder as an NMT model [10], which is commonly used and can produce state of the art translation performance [49], [34]. As a promising deep learning model, RNN Encoder-Decoder has been used in addressing other software engineering tasks [14], [5].\nRNN Encoder-Decoder has two recurrent neural networks (RNNs). One RNN is used to transform source language sequences into vector representations. This RNN is called the encoder. The other RNN is used to transform the vector representations to the target language sequences, which is called the decoder.\n1) Encoder: The input of the encoder is a variable-length sequence x = (x1, ..., xT ). The encoder takes one symbol at a time as shown in Figure 2. As an RNN, the encoder has a hidden state h, which is a fixed-length vector. At a time step t, the encoder computes the hidden state ht by:\nht = f(ht\u22121, xt) (1)\nwhere f is a non-linear function. Two common options for f are long short-term memory (LSTM) [21] and the gated recurrent unit (GRU) [10] (due to space limit, we do not describe these two unit types in detail here). For example, Bahdanau et al. use GRU [6] and Sutskever et al. use LSTM [51]. The last symbol of x should be an end-of-sequence (<eos>) symbol which notifies the encoder to stop and output the final hidden state hT , which is used as a vector representation of x.\n2) Decoder: Figure 3 shows the RNN of the decoder. The output of the decoder is the target sequence y = (y1, ..., yT \u2032). One input of the decoder is a <start> symbol denoting the beginning of the target sequence. At a time step t, the decoder computes the hidden state h\u2032t and the conditional distribution of the next symbol yt by:\nh\u2032t = f(h \u2032 t\u22121, yt\u22121, hT ) (2)\np(yt|yt\u22121, ..., y1, hT ) = g(h\u2032t, yt\u22121, hT ) (3)\nwhere hT (generated by the encoder) is called the context vector; f and g are non-linear functions. Function f here and f in Equation 1 are often the same. Function g must produce valid probabilities. For example, softmax can be used as g. The decoder finishes when it predicts an <eos> symbol.\n3) Training Goal: The encoder and the decoder are jointly trained to maximize the conditional log-likelihood:\nmax \u03b8\n1\nN N\u2211 i=1 log p(yi|xi; \u03b8) (4)\nwhere \u03b8 is the set of the model parameters; N is the size of the training set; and each (xi, yi) is a pair of a source sequence and a target sequence in the training set."}, {"heading": "C. Attentional RNN Encoder-Decoder and Nematus", "text": "Bahdanau et al. introduced the attentional RNN EncoderDecoder, in which attention mechanism is introduced to deal with long source sequences [6]. We use this mechanism in our\nwork because our source sequences, diffs, are much longer than natural language sentences. The attention mechanism includes several modifications in both the encoder and the decoder, which we describe in the following subsections.\n1) Encoder: The encoder in the attentional model is a bidirectional RNN, which has two RNNs: forward and backward. The two RNNs have the same architecture. The forward RNN is the same as the RNN in the original RNN Encoder-Decoder model (Figure 2), which reads the source sequence x as it is ordered, from x1 to xT . The forward RNN generates a sequence of the hidden states ( \u2212\u2192 h 1, ... \u2212\u2192 h T ). In contrast, the backward RNN reads x in the reversed order, and generates a sequence of the hidden states ( \u2190\u2212 h T , ... \u2190\u2212 h 1).\nIn the end, for each symbol xi in x, the encoder outputs hi = [ \u2212\u2192 hi ; \u2190\u2212 hi ], which is a concatenation of \u2212\u2192 hi and \u2190\u2212 hi .\n2) Decoder: The decoder computes the hidden state h\u2032t and the conditional distribution of the next symbol yt by:\nh\u2032t = f(h \u2032 t\u22121, yt\u22121, ct) (5)\np(yt|yt\u22121, ..., y1, ct) = g(h\u2032t, yt\u22121, ct) (6)\nwhere f and g are non-linear functions like f and g in Equations 2 and 3. ct is the distinct context vector for yt, and can be computed by\nct = T\u2211 i=1 \u03b1tihi (7)\nwhere T is the length of the input sequence; the weight \u03b1ti can be trained jointly with the other components in the model, and hi is generated by the encoder. Since ct is designed to introduce the context\u2019s impact to yt, attentional RNN EncoderDecoder works better on long source sequences. Therefore, we use this NMT model in this paper rather than the original one."}, {"heading": "IV. APPROACH", "text": "This section describes our approach, including the data set preparation and the NMT training procedure. This section corresponds to Part A in the paper overview Figure 1, and is detailed in Figure 4."}, {"heading": "A. Preparing a Data Set for NMT", "text": "We used the commit data set provided by Jiang and McMillan [23], which contains 2M commits. The data set includes commits from top 1k Java projects (ordered by the number of stars) in Github. We describe how we prepared the data set for NMT algorithms as follows.\n1) Preprocessing the Data Set: First, we extracted the first sentences from the commit messages. We used the first sentences as the target sequences because the first sentences often are the summaries of the entire commit messages. Similarly, Gu et al. used the first sentences of the API comments as their target sequences [14]. Second, we removed issue ids from the extracted sentences and removed commit ids from the diffs, because issue ids and commit ids are unique ids and\nincrease the vocabularies of the source and the target languages dramatically, which in turn cause large memory use of NMT.\nThird, we removed merge and rollback commits (the same practice done by Jiang and McMillan [23]). Merges and rollbacks are removed because the diffs of merges and rollbacks are often more than thousands of lines, which NMT is not suitable to translate. For the same reason, we also removed any diff that is larger than 1MB.\nAfter the above steps, we have 1.8M commits remaining. Finally, we tokenized the extracted sentences and the diffs by white spaces and punctuations. We did not split CamelCase so that identifiers (e.g., class names or method names) are treated as individual words in this study.\n2) Setting Maximum Sequence Lengths for NMT Training: A maximum sequence length for both source and target sequences need to be set for an RNN Encoder-Decoder [6], [46]. Since NMT is for translating natural language sentences, maximum sequence lengths for both source and target sequences are often set between 50 to 100 [6], [46]. Because the lengths of our source and target sequences are very different, we set the maximum sequence lengths separately.\nFor our target sequences, we set the maximum length at 30 tokens (including words and punctuations), because the first sentences from the commit messages tend to be short. In our data set, 98% of the first sentences have less than 30 tokens.\nFor our source sequences, we set the maximum length at 100 tokens because 100 is the largest maximum length used by NMT in natural language translation. Many configurations are possible, and optimizing the maximum diff length for generating commit messages is an area of future work. In pilot\nstudies, a maximum length of 100 outperformed lengths of 50 and 200.\nAfter applying the maximum lengths for source and target sequences (30 and 100), we have 75k commits remaining.\n3) V-DO Filter: We introduced Verb-Direct Object (VDO) filter because we found that the existing messages have different writing styles and some of the messages are poor written, which may affect the performance of NMT.\nTo obtain a set of commit messages that are in a similar format, we filtered the messages for verb/direct-object pattern. We chose this pattern because a previous study shows that 47% of commit messages follow this pattern [23]. To find the pattern, we used a Natural Language Processing (NLP) tool, Stanford CoreNLP [37], to annotate the sentences with grammar dependencies. Grammar dependencies are a set of dependencies between parts of a sentences. Considering a phrase, \u201cprogram a game\u201d, this phrase has a dependency, which is called \u201cdobj\u201d in Stanford CoreNLP, where the governor is \u201cprogram\u201d and the dependent is \u201cgame\u201d. For V-DO filter, we look for \u201cdobj\u201d dependencies which represent the verb/directobject pattern.\nFor each sentence, we checked whether the sentence is begun with a \u201cdobj\u201d dependency. If the sentence is begun with a \u201cdobj\u201d, we mark the sentence as a \u201cdobj\u201d sentence. In the end, we have 32k commit messages that are \u201cdobj\u201d sentences.\n4) Generating Training/Validation/Test Sets: We randomly selected 3k commits for testing, 3k commits for validation, and the rest 26k commits for training.\n5) Selecting Vocabularies: NMT needs predefined vocabularies for commit messages and diffs. In the training set, the commit messages have 16k distinct tokens (words or punctuations) and the diffs have 65k distinct tokens. We selected all the 16k tokens in the commit messages to be the vocabulary of commit messages. We used the most frequent 50k tokens in the diffs to be the vocabulary of diffs. All the tokens that are not in the diff vocabulary only occur once in the training set. Additionally, the vocabulary size of 50k is often used by other NMT models [34]."}, {"heading": "B. NMT Training and Testing", "text": "In this section, we describe how we trained and tested an NMT model for generating commit messages.\n1) Model: We used Nematus [47] in our work because it is robust, easy to use, and produced best constrained systems for seven translation directions (e.g., English to German, etc.) in WMT 2016 shared news translation task [49]. Nematus is based on Theano [52], and implements the attentional RNN encoder-decoder (see Section III-C) with several implementation differences [47].\n2) Training Setting: We borrowed the training setting that Sennrich et al. used to produce the best translation systems in WMT 2016 [49]. The training goal is cross-entropy minimization [45]. The learning algorithm is stochastic gradient descent (SGD) with Adadelta [55], which automatically adapts the learning rate. The size of minibatches is 80; the size of word embeddings is 512; the size of hidden layers is 1024.\nFor each epoch, the training set is reshuffled. The model is validated every 10k minibatches by BLEU [41], which is a commonly used similarity metric for machine translation. The maximum number of epochs is 5k; the maximum number of minibatches is 10M; and early stopping is used [47]. During the training, the model is saved every 30k minibatches. So after the training, a list of models are saved and the ensemble results of the last four models are used for evaluation.\nOne key difference between our and Sennrich et al.\u2019s training processes is that Sennrich et al. used maximum sentence length of 50 for all the languages; we used 30 for commit messages and 100 for diffs as explained in Section IV-A2.\n3) Training Details: We trained on the training set of 26k pairs of commit messages and diffs, with a validation set of 3k pairs. We conducted the training on an Nvidia GeForce GTX 1070 with 8GB memory. The learning algorithm stopped at 210k minibatches. Because a model is saved every 30k minibatches, seven models are saved from this training. The training process took 38 hours.\n4) Testing Details: While we describe our evaluation in the next section, certain technical details are relevant here. We ran Nematus with the last four saved models on the testing set and we obtained the ensemble result. We used the same GPU as we used in training. The testing process took 4.5 minutes. We note that we followed the standard evaluation procedure for NMT and used a test set of 3k [34], [48], [10]."}, {"heading": "V. EVALUATION USING AN AUTOMATIC METRIC", "text": "In this section, we evaluate the generated messages from our approach that we described in the last section. Our objective is to assess the similarity between the generated messages and the reference messages in the test set. This section corresponds to Part B in the paper overview Figure 1. Note that this evaluation is distinct from the experiment with human evaluators that we describe in Section VI, which is also a component of \u201cPart B.\u201d In this section we ask: RQ1 Compared to the messages generated by a baseline, are\nthe messages generated by the NMT model more or less similar to the reference messages?\nRQ2 Are the messages generated by the NMT model more or less similar to the reference messages when V-DO filter is enabled or disabled?\nWe ask RQ1 to evaluate the NMT model compared to a baseline, which we describe in the following subsection. We ask RQ2 in order to evaluate the impact of V-DO filter. In the following subsections, we first introduce the baseline for RQ1. Then, we introduce the metric for measuring similarity between two messages. Finally, we report our results for the research questions."}, {"heading": "A. Baseline: MOSES", "text": "We used MOSES [28] as the baseline in RQ1. MOSES is a popular statistical machine translation software, which is often used as a baseline in evaluating machine translation systems [8], [27]. For example, Iyer et al. used MOSES as a baseline when they evaluated Code-NN [22]. To run MOSES\nfor translating diffs to commit messages, we trained a 3- gram language model using KenLM [18], [19], which is the same procedure in the study of Iyer et al. [22]. We did not use Code-NN as a baseline, because, in our pilot study of running Code-NN [22] to generate commit messages, CodeNN did not generate comparable results. A possible reason is that Code-NN needs parsing source sequences and diffs are not suitable for parsing."}, {"heading": "B. Similarity Metric: BLEU", "text": "BLEU [41] is widely used to measure the similarity between two sentences in evaluation of machine translation systems [26], [34], [33]. Additionally, BLEU is recommended for assessing an entire test set instead of a sentence [41]. The calculation of BLEU needs the modified n-gram precisions. For any n, the modified n-gram precision is calculated by:\npn =\n\u2211 (gen,ref)\u2208test \u2211 ngram\u2208gen\nCntclip(ngram)\u2211 (gen,ref)\u2208test \u2211 ngram\u2208gen Cntgen(ngram) (8)\nCntclip(ngram) =\nmin(Cntgen(ngram), Cntref (ngram)) (9)\nwhere test is the set of pairs of the generated and the reference messages in the test set; gen is the set of distinct n-grams in a generated message; Cntclip is defined in Equation (9); Cntgen is the number of occurrences of an n-gram in a generated message; similarly, Cntref is the number of the occurrences of an n-gram in a reference message. Then, BLEU is:\nBLEU = BP \u00b7 exp( N\u2211 n=1 1 N log(pn)) (10)\nBP =\n{ 1 if c > r\ne(1\u2212r/c) if c \u2264 r (11)\nwhere N is the maximum number of grams; pn is defined in Equation (8); BP is defined in Equation (11); r is the sum of the lengths of all the reference messages; c is the sum of the lengths of the generated messages. BLEU scores range from 0 to 100 (in percent). The default value of N is 4, which is used in our evaluation and is commonly used in other evaluations [26], [48], [34], [22], [33], [14]."}, {"heading": "C. RQ1: Compared to the Baseline", "text": "The first two rows in Table I list the BLEU scores of MOSES and the NMT model we trained in Section IV-B, which we refer to as NMT1. The BLEU score of our model is 31.92 while the BLEU score of MOSES is 3.63, so according to the BLEU metric, the messages generated by the NMT model are more similar to the reference messages than the messages generated by the baseline. One key reason that the attentional NMT model outperforms MOSES is that MOSES does not handle well very long source sequences with short target sequences. Particularly, MOSES depends on Giza++ [40] for word alignments between source and target sequences, and\nGiza++ becomes very inefficient when a source sequence is 9 times longer than the target sequence or vice versa [4]. Table I shows that the total length of the generated messages (LenGen in Table I) of MOSES is much longer than the total length of the reference messages, which may cause the modified n-gram precisions (p1, p2, p3, and p4), of MOSES to be small.\nTo further examine the messages generated by our model, we split the test set by the lengths of the diffs into four groups and calculated BLEU scores separately for each group. Figure 5 shows the distribution of the lengths of diffs in the test set and Table II shows the BLEU scores for the diffs. This table shows that the diffs that have more than 75 tokens have the highest BLEU score. One possible reason is that there are many more diffs that have more than 75 tokens than the other smaller diffs. Figure 6 shows the distribution of the diff lengths in the training set. This figure shows that the training set is populated by larger diffs, which may cause the model to fit the larger diffs better.\nIn Table II, the modified 4-gram precision, p4, is 7.6 when diff lengths are between 25 and 50, and becomes 42.3 when diff lengths are larger than 75. This increase of p4 means that the number of the 4-grams that are shared by the generated and reference messages increase dramatically when the lengths of diffs increase to more than 75 tokens. In contrast, p4 changes much less (3.1 to 4.5, 4.5 to 7.6) in other cases."}, {"heading": "D. RQ2: Impact of V-DO Filter", "text": "Besides NMT1 (the NMT model trained with V-DO filter in Section IV), we trained another model without V-DO filter, which we refer to as NMT2. In this subsection, we compare NMT1 and NMT2 to see the impact of V-DO filter.\n1) Data Set and Training Process for NMT2: Without VDO filter, the data set has 75k commits. First, we extracted the test set that is used by NMT1 so that we can compare the test results. Then, from the remaining 72k commits, we randomly selected 3k commits to be another test set, which may contain messages that do not follow the V-DO pattern. We refer to the first test set as Test1 (with V-DO filter), and the second test set as Test2 (without V-DO filter).\nThen, we randomly selected 3k for validation and used the rest 66k commits for training. We note that the training set of NMT1 has only 26k commits, so NMT2 has 2.5 times more training data than NMT1. The training set includes 45k distinct tokens in commit messages and 110k distinct tokens in diffs. Similar to the vocabulary setting we used in Section IV-A4, we used all the 45k tokens to be the vocabulary of commit messages. We used the most frequent 100k tokens in diffs to be the vocabulary of diffs. All the tokens that are not included in the vocabulary only occur once in the training set. We followed the same process described in Section IV-B. The training process took 41 hours. The testing process for Test1 took 21.5 minutes and Test2 took 20 minutes.\n2) Results: The third and fourth rows in Table I show the BLEU scores of NMT2 on Test1 and Test2, which are 32.81 and 23.10 respectively. Comparing the BLEU scores of NMT1 and Test1, the result shows that the messages generated by NMT2 are more similar to the reference messages in Test1. This finding indicates that although the training set without VDO filter has low-quality messages, there are valuable commits that do not follow the V-DO pattern but help the NMT model improve over Test1 which follow the V-DO pattern.\nHowever, the BLEU score of Test2 is about 10 percent lower\nthan the BLEU score of Test1, which means that NMT2 does not perform well over the commits that do not follow the V-DO pattern. For example, a reference message in Test2 is \u201c7807cb6 ca7a229\u201d, which should be version numbers. For such reference messages in Test2, the NMT model cannot generate the same version numbers and is not meant to generate such numbers. However, similar messages in the training set cause the NMT model to try to generate such numbers for commit messages. For example, a generated message in Test2 is \u201cDd38b1cc2 92007d1d7\u201d while the reference message is \u201cRun only on jdk7 for the moment\u201d."}, {"heading": "VI. HUMAN EVALUATION", "text": "In this section, we ask human experts to evaluate the generated messages by the NMT model we described in Section IV. In Section V, we evaluated our model by the automatic metric, BLEU. Our human study complements the evaluation that uses BLEU in two ways. First, although BLEU is a widely used metric that enables us to compare our model with others and to deliver reproducibility, BLEU is not recommended for evaluating individual sentences [41]. Our human study can show how our model perform on individual messages. Second, BLEU calculates the textual similarity between the generated and the reference messages, while the human study can evaluate the semantic similarity.\nIn this study, we hired 20 participants for 30 minutes each to evaluate the similarity in a survey study. Two participants are computer science Ph.D. students and 18 participants are professional programmers with 2 to 14 years experience. In the rest of this subsection, we describe our survey design, the process of conducting the survey, and the survey results."}, {"heading": "A. Survey Design", "text": "We introduce our survey in the first page as: \u201cThis survey will ask you to compare two commit messages by their meaning. You will be able to select a score between 0 to 7, where 0 means there is no similarity and 7 means that two messages are identical.\u201d We permitted the participants to search the internet for unfamiliar concepts. Then, we gave three scoring examples with recommended scores of 6, 3, and 1. Due to space limit, we present only the first example in Figure 7 (all the other examples are available in our online appendix, Section XI). Then, in the remaining pages of the survey, each page has one pair of the messages, and we asked the participants to score the similarity by meaning. Note that the participants do not know who/what generated the messages. The order of the messages in every page is randomly decided. In the end of the page, there is an optional text box for the participants to enter their justifications. A formal qualitative study about the participants\u2019 comments will need to be performed in the future but is beyond the scope of this study. Figure 8 shows one page of the survey."}, {"heading": "B. Survey Procedure", "text": "First, the pairs of generated/reference messages are randomly ordered in a list. Then, for each participant, a survey is\nTiming\nExample 1 of 3 message 1: \"Added X to readme\" message 2: \"edit readme\" Recommended score: 6\nExplanation: The two messages have only one shared word, \"readme\". But the two messages are very similar in the meaning, because \"Added\" is a type of \"edit\".\nThese page timer metrics will not be displayed to the recipient.\nFirst Click 5.301 seconds Last Click 97.921 seconds Page Submit 0 seconds Click Count 6 clicks\nClose Preview Restart Survey   Place Bookmark \n(Optional) Justification:\nThese page timer metrics will not be displayed to the recipient.\nFirst Click 10.848 seconds Last Click 27.478 second Page Submit 0 seconds Click Count 3 clicks\n<< >>\nClose Preview Restart Survey   Place Bookmark \ngenerated with the messages in the list from a given starting point. For example, for the first three participants, the surveys are generated with the messages starting from the first pair in the list. In 30 minutes, the first participant was able to score 107 pairs; the second participant was able to score 61 pairs; the third participant was able to score 99 pairs. So the first 61 pairs of messages were evaluated by three participants. For the fourth participant, we generated a survey starting from the 62th pair and the participant stopped at 99th pair in 30 minutes. So after the first four participants, we have 99 pairs scored by three participants. Although it would be ideal if we obtain three scores for every pair, we did not enforce all the pairs being scored by three participants because we want to have more pairs scored with the limited number of participants. In the end, 226 pairs were scored by three participants, 522 pairs were scored by two participants, and 235 pairs were scored by one participant."}, {"heading": "C. Results", "text": "Figure 9 shows the distribution of the median scores of the semantic similarity of the generated/reference messages. To be conservative, we round down the median scores. For example, if a generated message has two scores, 1 and 2, and the median score is 1.5, we round down the median score to 1. In total, 983 generated commit messages have scores made by the participants. Zero and seven are the two most frequent scores. There are 248 messages scored 0 and 234 messages scored 7, which shows that the performance of our model tends to be either good or bad."}, {"heading": "VII. QUALITY ASSURANCE FILTER", "text": "Based on the results from our study with human evaluators (Section VI), we propose a quality assurance filter (QA filter) to automatically detect the diffs for which the NMT model does not generate good commit messages. By building this\nfilter, we investigate whether it is possible to automatically learn the cases where our NMT model does not perform well. In this section, we describe the method of our filter, how we evaluate the filter, and the performance of the filter. This section corresponds to Part C in the paper overview Figure 1."}, {"heading": "A. QA Filter", "text": "Our method of QA filter has three steps. First, we prepared the gold set. We used the evaluated messages and the corresponding diffs in the human study as our gold set. For each diff and the corresponding generated message, there is a score we obtained in the human study (Figure 9) that indicates whether the generated message for the diff is similar to the reference message (i.e., the actual human-written message). To be conservative, we labeled the diffs that have scores of zero or one as \u201cbad\u201d and all the other diffs as not \u201cbad\u201d.\nSecond, we extracted the features of the diffs. We used term frequency/inverse document frequency (tf/idf) for every word in a diff as the features. Tf/idf is widely used in machine learning for text processing [17], which is computed based on the frequency of a word in a diff and whether the word is common in the other diffs.\nFinally, we used the data set of diffs and their labels to train a linear SVM using stochastic gradient descent (SGD) as the learning algorithm. After we trained the SVM, to\npredict whether the NMT model will generate a \u201cbad\u201d commit message for a diff, we extract tf/idfs from the diff and run the trained SVM with the tf/idfs."}, {"heading": "B. Cross-Validation Evaluation", "text": "Figure 10 illustrates our 10-fold cross-validation process. We shuffled the gold set first, and split the gold set into 10 folds. For each fold, we trained a SVM model on the other 9 folds, and tested the SVM model on the one fold. In the end, we obtained the test results for every fold. Figure 11 shows the predicts of all the folds. In terms of detecting diffs for which the NMT model will generate \u201cbad\u201d messages, QA filter has 44.9% precision and 43.8% recall. Furthermore, if we label the messages with scores of 6 or 7 as \u201cgood\u201d, in this evaluation, QA filter reduced 44% of the \u201cbad\u201d messages at a cost of 11% of the \u201cgood\u201d messages."}, {"heading": "VIII. EXAMPLE RESULT", "text": "Table III shows a representative example of a generated message that was rated highly by the human experts. It includes the generated and reference messages, three scores made by three participants, and the corresponding diff. In this example, the reference message refers to a replacement of a call to a function called deactivate() with a call to a function close(). To a human reader, that is evident from the diff: a call to deactivate() is removed and a call to close() is added. The NMT algorithm also picked up on this change, generating text \u201cClose instead of mCursor.Deactivate.\u201d"}, {"heading": "IX. THREATS TO VALIDITY", "text": "One threat to validity is that our approach is experimented on only Java projects in Git repositories, so they may not be representative of all the commits. However, Java is a popular programming language [3], [1], [2], which is used in a large number of projects. In the future, we will extend our approach to other programming languages.\nAnother threat to validity is the quality of the commit messages. We collected actual human-written commit messages from Github, and used V-DO filter to obtain a set of relatively good-quality commit messages. But the humanwritten messages may not contain all the useful information\nthat should be in a commit message. However, our objective in this paper is to generate commit messages that can be learned from the history of the repositories. Further improvement on human-written messages falls outside the scope of this paper.\nAnother threat to validity is about the human study because of the limited number of the participants. We cannot guarantee that every final score for a generated commit message is fair. We tried to mitigate this threat by hiring as many professional programmers as we can, and having 23% of the evaluated messages scored by three participants and 53% of the evaluated messages scored by two participants."}, {"heading": "X. DISCUSSION AND CONCLUSION", "text": "The key advancement that this paper makes to the state-ofthe-art is a technique to generate short commit messages that summarize the high-level rationale for a change to software. As we note in Section I-A, we do not claim to be able to provide new insights for completely novel changes to software \u2013 that task is likely to remain in human hands for the foreseeable future. Instead, we learn from knowledge stored in a repository of changes that have already been described in commit messages. Several authors in the related literature have observed that many code changes follow similar patterns, and have a similar high-level rationale (e.g., [38], [23]). Traditionally programmers still need to manually write commit messages from scratch, even in cases where a commit has a rationale that has been described before. What this paper does is automate writing commit messages based on knowledge in a repository of past changes.\nOur strategy was, in a nutshell, to 1) collect a large repository of commits from large projects, 2) filter the commits to ensure relatively high-quality commit messages, and 3) train a Neural Machine Translation algorithm to \u201ctranslate\u201d from diffs to commit messages using the filtered repository. We then evaluated the generated commit messages in two ways. First we conducted an automated evaluation using accepted metrics and procedures from the relevant NMT literature\n(Section V). Second, as a verification and for deeper analysis, we also conducted an experiment with human evaluators (Section VI).\nWhat we discovered is that the NMT algorithm succeeded in identifying cases where the commit had a similar rationale to others in the repository. The evidence for this is the large bar for item 7 in Figure 9 \u2013 it means that the human evaluators rated a large number of the generated messages as very closely matching the reference messages. However, the algorithm also generated substantial noise in the form of low quality messages (note the large bar for item 0). A likely explanation is that these include the cases that involve new insights which the NMT algorithm is unable to provide. While creating these new insights from the data is currently beyond the power of existing neural network-based machine learning (a problem observed across application domains [13]), at a minimum we would like to return a warning message to the programmer to indicate that we are unable to generate a message, rather than return a low quality message. Therefore we created a Quality Assurance filter in Section VII. This filter helped reduce the number of low quality predictions, as evident in the reduced bar for item 0 in Figure 11.\nWhile we do view our work as meaningfully advancing the state-of-the-art, we by no means claim this work is definitive or completed. We release our complete data set and implementation via an online appendix, noted at the end of Section IV. Our hope is that other researchers will use this data set and implementation for further research efforts. Generally speaking, future improvements are likely to lie in targeted training for certain types of commits, combined with detection of change types. It is probable that very high quality predictions are possible for some types of software changes, but not others. This work provides a foundation for those and other future developments."}, {"heading": "XI. REPRODUCIBILITY", "text": "Our data sets, scripts, and results are accessible via: https://sjiang1.github.io/commitgen/"}, {"heading": "XII. ACKNOWLEDGMENTS", "text": "This work was partially supported by the NSF CCF1452959 and CNS-1510329 grants, and the Office of Naval Research grant N000141410037. Any opinions, findings, and conclusions expressed herein are the authors and do not necessarily reflect those of the sponsors."}], "references": [{"title": "Replicating parser behavior using neural machine translation", "author": ["C.V. Alexandru", "S. Panichella", "H.C. Gall"], "venue": "IEEE 25th International Conference on Program Comprehension (ICPC),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automatically documenting program changes", "author": ["R.P. Buse", "W.R. Weimer"], "venue": "In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Findings of the 2011 workshop on statistical machine translation", "author": ["C. Callison-Burch", "P. Koehn", "C. Monz", "O.F. Zaidan"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Achieving accuracy and scalability simultaneously in detecting application clones on android markets", "author": ["K. Chen", "P. Liu", "Y. Zhang"], "venue": "In Proceedings of the 36th International Conference on Software Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Boa: A language and infrastructure for analyzing ultra-large-scale software repositories", "author": ["R. Dyer", "H.A. Nguyen", "H. Rajan", "T.N. Nguyen"], "venue": "In 2013 35th International Conference on Software Engineering (ICSE),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Do code and comments coevolve? on the relation between source code and comment changes", "author": ["B. Fluri", "M. Wursch", "H.C. Gall"], "venue": "In Proceedings of the 14th Working Conference on Reverse Engineering,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Deep api learning", "author": ["X. Gu", "H. Zhang", "D. Zhang", "S. Kim"], "venue": "In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Semantically enhanced software traceability using deep learning techniques", "author": ["J. Guo", "J. Cheng", "J. Cleland-Huang"], "venue": "In Proceedings of the 2017 International Conference on Software Engineering,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "On the use of automated text summarization techniques for summarizing source code", "author": ["S. Haiduc", "J. Aponte", "L. Moreno", "A. Marcus"], "venue": "In Proceedings of the 2010 17th Working Conference on Reverse Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "On the use of automated text summarization techniques for summarizing source code", "author": ["S. Haiduc", "J. Aponte", "L. Moreno", "A. Marcus"], "venue": "In 2010 17th Working Conference on Reverse Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["K. Heafield"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On the naturalness of software", "author": ["A. Hindle", "E.T. Barr", "Z. Su", "M. Gabel", "P. Devanbu"], "venue": "In Proceedings of the 2012 International Conference on Software Engineering,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["S. Iyer", "I. Konstas", "A. Cheung", "L. Zettlemoyer"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Towards automatic generation of short summaries of commits", "author": ["S. Jiang", "C. McMillan"], "venue": "IEEE 25th International Conference on Program Comprehension (ICPC),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "A survey of documentation practice within corrective maintenance", "author": ["M. Kajko-Mattsson"], "venue": "Empirical Softw. Engg.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Ccfinder: a multilinguistic token-based code clone detection system for large scale source code", "author": ["T. Kamiya", "S. Kusumoto", "K. Inoue"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Factored translation models", "author": ["P. Koehn", "H. Hoang"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Identifying similar code with program dependence graphs", "author": ["J. Krinke"], "venue": "In Proceedings Eighth Working Conference on Reverse Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Combining deep learning with information retrieval to localize buggy files for bug reports (n)", "author": ["A.N. Lam", "A.T. Nguyen", "H.A. Nguyen", "T.N. Nguyen"], "venue": "In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Rclinker: Automated linking of issue reports and commits leveraging rich contextual information", "author": ["T.D.B. Le", "M. Linares-Vasquez", "D. Lo", "D. Poshyvanyk"], "venue": "IEEE 23rd ICPC,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Changescribe: A tool for automatically generating commit messages", "author": ["M. Linares-V\u00e1squez", "L.F. Cort\u00e9s-Coy", "J. Aponte", "D. Poshyvanyk"], "venue": "In 2015 IEEE/ACM 37th IEEE ICSE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Latent predictor networks for code generation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["W. Ling", "P. Blunsom", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u00fd", "F. Wang", "A. Senior"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["M. Luong", "C.D. Manning"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C.D. Manning"], "venue": "CoRR, abs/1508.04025,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Can development work describe itself", "author": ["W. Maalej", "H.J. Happel"], "venue": "In 2010 7th IEEE Working Conference on Mining Software Repositories (MSR", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Association for Computational Linguistics (ACL) System Demonstrations,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Identifying reasons for software changes using historic databases", "author": ["A. Mockus", "L.G. Votta"], "venue": "In Proceedings 2000 International Conference on Software Maintenance,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "ARENA: An approach for the automated generation of release notes", "author": ["L. Moreno", "G. Bavota", "M.D. Penta", "R. Oliveto", "A. Marcus", "G. Canfora"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "A syntactic neural model for general-purpose code generation", "author": ["G.N. Pengcheng Yin"], "venue": "In Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}, {"title": "Why did this code change", "author": ["S. Rastkar", "G.C. Murphy"], "venue": "In Proceedings of the 2013 ICSE,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "How do professional developers comprehend software", "author": ["T. Roehm", "R. Tiarks", "R. Koschke", "W. Maalej"], "venue": "In Proceedings of the 2012 International Conference on Software Engineering,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning", "author": ["R.Y. Rubinstein", "D.P. Kroese"], "venue": "Springer Science & Business Media,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Nadejde. Nematus: a toolkit for neural machine translation", "author": ["R. Sennrich", "O. Firat", "K. Cho", "A. Birch", "B. Haddow", "J. Hitschler", "M. Junczys-Dowmunt", "S. L\u201daubli", "A.V. Miceli Barone", "J. Mokry"], "venue": "In Proceedings of the Software Demonstrations of the 15th Conference of  the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Nadejde. Nematus: a Toolkit for Neural Machine Translation", "author": ["R. Sennrich", "O. Firat", "K. Cho", "A. Birch", "B. Haddow", "J. Hitschler", "M. Junczys-Dowmunt", "S. L\u00e4ubli", "A.V. Miceli Barone", "J. Mokry"], "venue": "In Proceedings of the Demonstrations at the 15th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2017}, {"title": "Linguistic input features improve neural machine", "author": ["R. Sennrich", "B. Haddow"], "venue": "translation. CoRR,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Edinburgh neural machine translation systems for", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "WMT 16. CoRR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Towards automatically generating summary comments for java methods", "author": ["G. Sridhara", "E. Hill", "D. Muppaneni", "L. Pollock", "K. Vijay-Shanker"], "venue": "In Proceedings of the IEEE/ACM international conference on Automated software engineering,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Deep learning code fragments for code clone detection", "author": ["M. White", "M. Tufano", "C. Vendome", "D. Poshyvanyk"], "venue": "In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Toward deep learning software repositories", "author": ["M. White", "C. Vendome", "M. Linares-Vasquez", "D. Poshyvanyk"], "venue": "In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "They serve a valuable purpose in comprehension of software evolution, and act as a record of feature additions and bug repairs [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "Unfortunately, programmers sometimes neglect commit messages [11], [36], likely due to the same time and market pressures that have been reported to affect many types of documentation [44], [12], [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 30, "context": "Unfortunately, programmers sometimes neglect commit messages [11], [36], likely due to the same time and market pressures that have been reported to affect many types of documentation [44], [12], [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 38, "context": "Unfortunately, programmers sometimes neglect commit messages [11], [36], likely due to the same time and market pressures that have been reported to affect many types of documentation [44], [12], [24].", "startOffset": 184, "endOffset": 188}, {"referenceID": 7, "context": "Unfortunately, programmers sometimes neglect commit messages [11], [36], likely due to the same time and market pressures that have been reported to affect many types of documentation [44], [12], [24].", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "Unfortunately, programmers sometimes neglect commit messages [11], [36], likely due to the same time and market pressures that have been reported to affect many types of documentation [44], [12], [24].", "startOffset": 196, "endOffset": 200}, {"referenceID": 2, "context": "[7] describe DeltaDoc, a tool that summarizes what changed in the control flow of a program between code versions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[32] built ChangeScribe, which summarizes changes such as method additions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Current approaches are effective at summarizing what changed and where, but do not answer the question why [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 32, "context": "[38] observed, many commit messages are similar and can be broadly categorized as related to bug repair, feature additions, etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "\u201d) [23].", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Our view is in line with the hypothesis of \u201cnaturalness\u201d of software [20], that software artifacts follow patterns that can be learned from sufficiently large datasets.", "startOffset": 69, "endOffset": 73}, {"referenceID": 43, "context": "The datasets required are enormous by typical software engineering research standards, involving up to tens of millions of pairs of sentences [49], [34].", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": "The datasets required are enormous by typical software engineering research standards, involving up to tens of millions of pairs of sentences [49], [34].", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "While we were able to obtain quite large datasets (over 2M commits), we encountered many commit messages that were gibberish or very low quality (a problem others have observed [11], [36]), which if left in the training data could be reflected in the NMT\u2019s output.", "startOffset": 177, "endOffset": 181}, {"referenceID": 30, "context": "While we were able to obtain quite large datasets (over 2M commits), we encountered many commit messages that were gibberish or very low quality (a problem others have observed [11], [36]), which if left in the training data could be reflected in the NMT\u2019s output.", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "These short term pressures lead programmers to neglect writing commit messages, like other types of documentation [11], [36], [44], [12], [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 30, "context": "These short term pressures lead programmers to neglect writing commit messages, like other types of documentation [11], [36], [44], [12], [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 38, "context": "These short term pressures lead programmers to neglect writing commit messages, like other types of documentation [11], [36], [44], [12], [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "These short term pressures lead programmers to neglect writing commit messages, like other types of documentation [11], [36], [44], [12], [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "These short term pressures lead programmers to neglect writing commit messages, like other types of documentation [11], [36], [44], [12], [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "[7] point out that programmers use commit messages for two reasons: 1) to summarize what changed, and 2) to briefly explain why the change was necessary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "have built DeltaDoc, which extracts path predicates of changed statements, and follows a set of predefined rules to generate a summary [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 26, "context": "have built ChangeScribe, which extracts changes between two Abstract Syntax Trees and summarizes the changes based on predefined rules [32].", "startOffset": 135, "endOffset": 139}, {"referenceID": 25, "context": "have built RCLinker, which links a bug report to the corresponding commit message [31].", "startOffset": 82, "endOffset": 86}, {"referenceID": 37, "context": "Rastkar and Murphy have proposed to summarize multiple related documents for commits [43].", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "have built ARENA, which summarizes changes and finds related issues to generate release notes [39].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Extractive summarization extracts relevant parts of source code and uses the relevant parts as a summary [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 44, "context": "has designed a Natural Language Generation (NLG) system to create summaries of Java methods [50].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "have built Code-NN, which uses an Neural Machine Translation (NMT) algorithm to summarize code snippets [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "For example, to detect code clones, traditional approaches predefine the representations of code fragments (some techniques use token sequences to represent code [25]; others use graphs [29], [9]).", "startOffset": 162, "endOffset": 166}, {"referenceID": 23, "context": "For example, to detect code clones, traditional approaches predefine the representations of code fragments (some techniques use token sequences to represent code [25]; others use graphs [29], [9]).", "startOffset": 186, "endOffset": 190}, {"referenceID": 4, "context": "For example, to detect code clones, traditional approaches predefine the representations of code fragments (some techniques use token sequences to represent code [25]; others use graphs [29], [9]).", "startOffset": 192, "endOffset": 195}, {"referenceID": 46, "context": "[53] learns the representations of code automatically.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Similarly, deep learning algorithms are introduced in bug localization [30], software traceability [15], and code suggestions [54].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Similarly, deep learning algorithms are introduced in bug localization [30], software traceability [15], and code suggestions [54].", "startOffset": 99, "endOffset": 103}, {"referenceID": 47, "context": "Similarly, deep learning algorithms are introduced in bug localization [30], software traceability [15], and code suggestions [54].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "[14], because both our and their techniques use Neural Machine Translation (NMT).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "use NMT to translate natural language queries to API method sequences [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Similarly, several code generation techniques use NMT to translate natural language to programming language [33], [42].", "startOffset": 108, "endOffset": 112}, {"referenceID": 36, "context": "Similarly, several code generation techniques use NMT to translate natural language to programming language [33], [42].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "[5], which investigates the suitability of NMT for program comprehension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[7] and by Jiang and McMillan [23].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[7] and by Jiang and McMillan [23].", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": ", yn) with the conditional probability p(y|x) [5], [35].", "startOffset": 46, "endOffset": 49}, {"referenceID": 29, "context": ", yn) with the conditional probability p(y|x) [5], [35].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "introduced RNN Encoder-Decoder as an NMT model [10], which is commonly used and can produce state of the art translation performance [49], [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 43, "context": "introduced RNN Encoder-Decoder as an NMT model [10], which is commonly used and can produce state of the art translation performance [49], [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "introduced RNN Encoder-Decoder as an NMT model [10], which is commonly used and can produce state of the art translation performance [49], [34].", "startOffset": 139, "endOffset": 143}, {"referenceID": 9, "context": "As a promising deep learning model, RNN Encoder-Decoder has been used in addressing other software engineering tasks [14], [5].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "As a promising deep learning model, RNN Encoder-Decoder has been used in addressing other software engineering tasks [14], [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 16, "context": "Two common options for f are long short-term memory (LSTM) [21] and the gated recurrent unit (GRU) [10] (due to space limit, we do not describe these two unit types in detail here).", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Two common options for f are long short-term memory (LSTM) [21] and the gated recurrent unit (GRU) [10] (due to space limit, we do not describe these two unit types in detail here).", "startOffset": 99, "endOffset": 103}, {"referenceID": 1, "context": "use GRU [6] and Sutskever et al.", "startOffset": 8, "endOffset": 11}, {"referenceID": 45, "context": "use LSTM [51].", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "introduced the attentional RNN EncoderDecoder, in which attention mechanism is introduced to deal with long source sequences [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 18, "context": "We used the commit data set provided by Jiang and McMillan [23], which contains 2M commits.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "used the first sentences of the API comments as their target sequences [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "Third, we removed merge and rollback commits (the same practice done by Jiang and McMillan [23]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "A maximum sequence length for both source and target sequences need to be set for an RNN Encoder-Decoder [6], [46].", "startOffset": 105, "endOffset": 108}, {"referenceID": 40, "context": "A maximum sequence length for both source and target sequences need to be set for an RNN Encoder-Decoder [6], [46].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "Since NMT is for translating natural language sentences, maximum sequence lengths for both source and target sequences are often set between 50 to 100 [6], [46].", "startOffset": 151, "endOffset": 154}, {"referenceID": 40, "context": "Since NMT is for translating natural language sentences, maximum sequence lengths for both source and target sequences are often set between 50 to 100 [6], [46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 18, "context": "We chose this pattern because a previous study shows that 47% of commit messages follow this pattern [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 31, "context": "To find the pattern, we used a Natural Language Processing (NLP) tool, Stanford CoreNLP [37], to annotate the sentences with grammar dependencies.", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "Additionally, the vocabulary size of 50k is often used by other NMT models [34].", "startOffset": 75, "endOffset": 79}, {"referenceID": 41, "context": "1) Model: We used Nematus [47] in our work because it is robust, easy to use, and produced best constrained systems for seven translation directions (e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 43, "context": ") in WMT 2016 shared news translation task [49].", "startOffset": 43, "endOffset": 47}, {"referenceID": 41, "context": "Nematus is based on Theano [52], and implements the attentional RNN encoder-decoder (see Section III-C) with several implementation differences [47].", "startOffset": 144, "endOffset": 148}, {"referenceID": 43, "context": "used to produce the best translation systems in WMT 2016 [49].", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "The training goal is cross-entropy minimization [45].", "startOffset": 48, "endOffset": 52}, {"referenceID": 48, "context": "The learning algorithm is stochastic gradient descent (SGD) with Adadelta [55], which automatically adapts the learning rate.", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "The model is validated every 10k minibatches by BLEU [41], which is a commonly used similarity metric for machine translation.", "startOffset": 53, "endOffset": 57}, {"referenceID": 41, "context": "The maximum number of epochs is 5k; the maximum number of minibatches is 10M; and early stopping is used [47].", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "We note that we followed the standard evaluation procedure for NMT and used a test set of 3k [34], [48], [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 42, "context": "We note that we followed the standard evaluation procedure for NMT and used a test set of 3k [34], [48], [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "We note that we followed the standard evaluation procedure for NMT and used a test set of 3k [34], [48], [10].", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "We used MOSES [28] as the baseline in RQ1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "MOSES is a popular statistical machine translation software, which is often used as a baseline in evaluating machine translation systems [8], [27].", "startOffset": 137, "endOffset": 140}, {"referenceID": 21, "context": "MOSES is a popular statistical machine translation software, which is often used as a baseline in evaluating machine translation systems [8], [27].", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "used MOSES as a baseline when they evaluated Code-NN [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "To run MOSES for translating diffs to commit messages, we trained a 3gram language model using KenLM [18], [19], which is the same procedure in the study of Iyer et al.", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "To run MOSES for translating diffs to commit messages, we trained a 3gram language model using KenLM [18], [19], which is the same procedure in the study of Iyer et al.", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "We did not use Code-NN as a baseline, because, in our pilot study of running Code-NN [22] to generate commit messages, CodeNN did not generate comparable results.", "startOffset": 85, "endOffset": 89}, {"referenceID": 35, "context": "BLEU [41] is widely used to measure the similarity between two sentences in evaluation of machine translation systems [26], [34], [33].", "startOffset": 5, "endOffset": 9}, {"referenceID": 28, "context": "BLEU [41] is widely used to measure the similarity between two sentences in evaluation of machine translation systems [26], [34], [33].", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "BLEU [41] is widely used to measure the similarity between two sentences in evaluation of machine translation systems [26], [34], [33].", "startOffset": 130, "endOffset": 134}, {"referenceID": 35, "context": "Additionally, BLEU is recommended for assessing an entire test set instead of a sentence [41].", "startOffset": 89, "endOffset": 93}, {"referenceID": 42, "context": "The default value of N is 4, which is used in our evaluation and is commonly used in other evaluations [26], [48], [34], [22], [33], [14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "The default value of N is 4, which is used in our evaluation and is commonly used in other evaluations [26], [48], [34], [22], [33], [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "The default value of N is 4, which is used in our evaluation and is commonly used in other evaluations [26], [48], [34], [22], [33], [14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "The default value of N is 4, which is used in our evaluation and is commonly used in other evaluations [26], [48], [34], [22], [33], [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 9, "context": "The default value of N is 4, which is used in our evaluation and is commonly used in other evaluations [26], [48], [34], [22], [33], [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 34, "context": "Particularly, MOSES depends on Giza++ [40] for word alignments between source and target sequences, and", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "First, although BLEU is a widely used metric that enables us to compare our model with others and to deliver reproducibility, BLEU is not recommended for evaluating individual sentences [41].", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Tf/idf is widely used in machine learning for text processing [17], which is computed based on the frequency of a word in a diff and whether the word is common in the other diffs.", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": ", [38], [23]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [38], [23]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "While creating these new insights from the data is currently beyond the power of existing neural network-based machine learning (a problem observed across application domains [13]), at a minimum we would like to return a warning message to the programmer to indicate that we are unable to generate a message, rather than return a low quality message.", "startOffset": 175, "endOffset": 179}], "year": 2017, "abstractText": "Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically \u201ctranslate\u201d diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.", "creator": "LaTeX with hyperref package"}}}