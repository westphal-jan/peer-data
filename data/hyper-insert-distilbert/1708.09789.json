{"id": "1708.09789", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Learning Lexico-Functional Patterns for First-Person Affect", "abstract": "informal nonfiction first - person speech narratives predictions are a uniquely unique quantitative resource for discovering computational models of everyday events and people'life s ongoing affective reactions submitted to them. even people blogging about their day alone tend perhaps not to explicitly say i am happy. instead they describe situations often from which other humans can readily correctly infer their affective physiological reactions. however considerable current mainstream sentiment manipulation dictionaries capabilities are missing where much of the information analysis needed historically to independently make similar inferences. we build increasingly on recent work here that models affect in terms of lexical noun predicate stat functions and affect on examining the predicate's arguments. tools we present by a refined method to roughly learn proxies correctly for these functions from first - person narratives. we continually construct a yet novel nonlinear fine - grained validation test set, and thereby show that dramatically the patterns we shall learn improve as our ability continuously to appropriately predict first - degree person \u2013 affective narrative reactions to everyday events, estimated from a standardized stanford sentiment detection baseline of. 67f to. 69 75f.", "histories": [["v1", "Thu, 31 Aug 2017 16:04:26 GMT  (140kb)", "http://arxiv.org/abs/1708.09789v1", "7 pages, Association for Computational Linguistics (ACL) 2017"]], "COMMENTS": "7 pages, Association for Computational Linguistics (ACL) 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lena reed", "jiaqi wu", "shereen oraby", "pranav anand", "marilyn a walker"], "accepted": true, "id": "1708.09789"}, "pdf": {"name": "1708.09789.pdf", "metadata": {"source": "CRF", "title": "Learning Lexico-Functional Patterns for First-Person Affect", "authors": ["Lena Reed", "Jiaqi Wu", "Shereen Oraby", "Pranav Anand"], "emails": ["lireed@ucsc.edu", "jwu64@ucsc.edu", "soraby@ucsc.edu", "panand@ucsc.edu", "mawalker@ucsc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n09 78\n9v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\n01 7\nunique resource for computational models of everyday events and people\u2019s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate\u2019s arguments. We present a method to learn proxies for these functions from firstperson narratives. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F."}, {"heading": "1 Introduction", "text": "Across social media, thousands of posts daily take the form of informal FIRST-PERSON NARRATIVES. These narratives provide a rich resource for computational modeling of how people feel about the events they report on. Being able to reliably predict the affect a person may feel towards events they encounter has a range of potential applications, including monitoring mood and mental health (Isaacs et al., 2013) and getting conversational assistants to respond appropriately (Bowden et al., 2017). Moreover, as these narratives are told from the perspective of a protagonist, this research could be used to understand other types of protagonist-framed narratives, like those in fiction.\nWe are interested in the opinions that a protagonist has, not the author per se. This is sometimes referred to as internal sentiment or self reflective sentiment. While in many situations that is overlaid with the author\u2019s opinions, in first-personal narratives, because the author is the protagonist, the two perspectives align. Here, we use the term affect to reference this protagonist-centered notion of opinion.\nA central obstacle to reliable affect prediction is that that people tend not to explicitly flag their affective state, by saying I am happy. Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005). But people tend to describe situations, such as My friend bought me flowers, or I got a parking ticket, from which other humans can readily infer their implicit affective reactions.\nOne approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).\nAnother approach aims to model the speaker\u2019s affect to an event compositionally, e.g. Anand and Reschke (2010) (A&R) proposed that the affect a lexical predicate communicates should be modeled as an n-ary function, taking as inputs the affect that the speaker bears towards\neach participant. Table 1 contains A&R\u2019s functions for verbs of possession: a state in which X has Y or X lacks Y does not convey a clear affect unless we know what the speaker thinks of both X and Y . If the speaker has positive affect toward both X and Y (Row 1), then we infer that her attitude toward the event is positive, but if either is negative, then we infer that the speaker is negative toward the event. Similarly, Rashkin et al. (2016) represent the typical affect communicated by particular predicates via connotation frames. Here we are finding the internal sentiment of the speaker, or, as Rashkin et al. refer to it, the \u201dmental state\u201d of the speaker.\nInspired by A&R\u2019s framework, our work learns lexico-functional patterns (patterns involving lexical items or pairs of lexical items in specific grammatical relations that we show to capture functorargument relations in A&R\u2019s sense), about the effects of combining particular arguments with particular verbs (event types) from first-person narratives. Our novel observation is that learning these compositional functions is greatly simplified in the case of first-person affect. People bear positive affect to themselves, so sentences with first-person elements, e.g. I/we/me, reduce the problem for an approach like A&R\u2019s to learning the polarity that results from composing the verb with only one of its arguments, i.e. only Rows 1, 2 in Table 1 need to be learned for first person subjects. Firstperson narratives are full of such sentences. See Table 2. We show that the learned patterns are often consonant with A&R\u2019s predictions, but are richer, including e.g. many private state descriptions (Wiebe et al., 2004; Wiebe, 1990).\nIn addition, we demonstrate that these lexicofunctional patterns improve the performance of several off-the-shelf sentiment analyzers. We show that Stanford sentiment (Socher et al., 2013) has a best performance of 0.67 macro F on our test set. We then supplement it with our learned patterns and demonstrate significant improvements.\nOur final ensemble achieves 0.75 F on the test set. We discuss related work in more detail in Sec. 5."}, {"heading": "2 Bootstrapping a First-Person Sentiment Corpus", "text": "We start with a set of first-person narratives (weblogs) drawn from the Spinn3r corpus, that cover a wide range of topics (Burton et al., 2009; Gordon and Swanson, 2009). To reduce noise, we restrict the blogs to those from well-known blogging sites (Ding and Riloff, 2016), and select 15,466 stories whose length ranges from 225 to 375 words.\nWe hand-annotate a set of 477 positive and 440 negative stories, and use these to bootstrap a larger set of 1,420 negative and 2,288 positive stories. To bootstrap, we apply AutoSlog-TS, a weakly supervised pattern learner that only requires training sets os stories labeled broadly as POSITIVE or NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003). AutoSlog uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Table 3 lists examples of AutoSlog patterns and the right-hand side illustrates a specific lexical-syntactic pattern that corresponds to each general pattern template, as instantiated in first-person stories.1 When bootstrapping a larger positive and negative story corpus, we use the whole story, not just the first person sentences.\n1The examples are shown as general expressions for readability, but the actual patterns must match the syntactic constraints associated with the pattern template.\nThe left-hand-side of Table 3 shows that the learned patterns can involve syntactic arguments of the verbal predicate, which means that these patterns are proxies for one column of verbal function tables like those in Table 1. However, they can also include verb-particle constructions, such as cheated on, or verb-head-of-preposition constructions. In each case though, because these patterns are localized to a verb and only one element, they allow us to learn highly specific patterns that could be incorporated into a dictionary such as +- Effect (Choi and Wiebe, 2014). AutoSlog simultaneously harvests both (syntactically constrained) MWE patterns and more compositionally regular verb-argument groups at the same time.\nAutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(POSITIVE | p) and P(NEGATIVE | p), along with the pattern\u2019s overall frequency. We define three parameters for each class: \u03b8f , the frequency with which a pattern occurs, \u03b8p, the probability with which a pattern is associated with the given class and \u03b8n, the number of patterns that must occur in the text for it to be labeled. These parameters are tuned on the dev set (Riloff, 1996; Oraby et al., 2015; Riloff and Wiebe, 2003).\nTo bootstrap a larger corpus, we want settings that have lower recall but very high precision. We select \u03b8p = 0.7, \u03b8f = 10 and \u03b8n = 3 for the positive class and \u03b8p = 0.85, \u03b8f = 10 and \u03b8n = 4 for the negative class for bootstrapping."}, {"heading": "3 Experimental Setup", "text": "Our experimental setup involves first creating a corpus of training and test sentences, then applying AutoSlog-TS a second time to learn linguistic patterns. We then set up methods for cascading classifiers to explore whether ensemble classifiers improve our results. Training Set: From the bootstrapped set of stories, we create a corpus of sentences. A critical simplifying assumption of our method is that a multi-sentence story can be labelled as a whole as positive or negative, and that each of its sentences inherit this polarity. This means we can learn the polarity of events in such narratives from their (noisy) inherited polarity without labelling individual sentences. Our training set consists of 46,255 positive and 25,069 negative sentences. Test Set: We create the test set by selecting 4k random first-person sentences. First-person sentences either contain an explicit first person marker, i.e. we andmy or start with either a progressive verb or\npleonastic it. To collect gold labels, we designed a qualifier and a HIT for Mechanical Turk, and put these out for annotation by 5 Turkers, who label each instance as positive, negative, or neutral. To ensure the high quality of the test set, we select sentences that were labelled consistently positive or negative by 4 or 5 Turkers. We collected 1,266 positive and 1,440 negative sentences.\nDev Set: We created the dev set using the same method as the test set, having Turkers annotate 2k random first-person sentences. We collected 498 positive and 754 negative sentences. The 4k test and dev sentences available for download at https://nlds.soe.ucsc.edu/first-person-sentiment.\nAutoSlog First-Person Sentence Classifier. In order to learn new affect functions, we develop a second sentence-level classifier using AutoSlogTS. We run AutoSlog over the training corpus, using the dev set to tune the parameters \u03b8f , \u03b8p and \u03b8n (Riloff, 1996), in order to maximize macro Fscore. Our best parameters on the dev set for positive is \u03b8f=18, \u03b8p=0.85 and \u03b8n=1 and for negative is \u03b8f=1, \u03b8p=0.5 and \u03b8n=1. We specify that if the sentence is in both classes we rename it as neutral. We will refer to this classifier as the AutoSlog classifier.\nBaseline First-Person Sentence Classifiers. Our goal is to see whether the knowledge we learn using AutoSlog-TS complements existing sentiment classifiers. We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sentiment classifier (Socher et al., 2013).\nRetrained Stanford. The Stanford Sentiment classifier is a based on Recursive Neural Networks, and trained on a compositional Sentiment Treebank, which includes fine-grained sentiment labels for 215,154 phrases from 11,855 sentences from movie reviews. It can accurately predict some compositional semantic effects and handle negation. However since it was trained on movie reviews, it is likely to be missing labelled data for some common phrases in our blogs. Thus we also retrained it (RETRAINED STANFORD) on high precision phrases from AutoSlog extracted from our training data of positive and negative blogs. This provides 67,710 additional phrases, including 58,972 positive phrases and 8,738 negative phrases. The retrained model includes both the labels from the original Sentiment Treebank and\nthe AutoSlog high precision phrases."}, {"heading": "4 Results and Analysis", "text": "We present our experimental results and analyze the results in terms of the lexico-functional linguistic patterns we learn.\nBaseline Classifiers. Rows 1-3 of Table 4 show the results for the three baselines, in terms of Fscore for each class and the macro F. Stanford outperforms both NRC and SVM, but misses many cases of positive sentiment.\nAutoSlog Classifier. Row 4 of Table 4 shows the results for the AutoSlog classifier. Although AutoSlog itself does not perform highly, the patterns that it learns represent a different type of knowledge than what is contained in many sentiment analysis tools. We therefore hypothesized that a cascading classifier, which supplements one of the baseline sentiment classifiers with the lexicofunctional patterns that AutoSlog learns might yield higher performance.\nRetrained Stanford. Row 5 of Table 4 shows the results for RETRAINED STANFORD. The F-scores for RETRAINED STANFORD are almost identical to the standard Stanford classifier. This may be because our data is a small percentage of the entire number of phrases used in training Stanford. Although RETRAINED STANFORD prioritizes our phrases, it would not make sense to remove the original training data.\nCascading Classifiers. We implement cascading classifiers to test our hypothesis. The cascade classifier has primary and secondary classifiers, and we invoke the secondary classifiers only if the primary assigns a prediction of neutral to a test instance, which reflects the lack of sentimentbearing lexical items. We also have a cascade classifier with a tertiary classifier, which is invoked in the same fashion as the secondary classifier after the primary and secondary classifiers have been run. The cascading classifiers are named in the order the classifier is employed, primary, secondary or primary, secondary, tertiary. For our cascading classifiers, we combine our baseline classifiers (NRC and Stanford), with our AutoSlog classifier. We do not use SVM as a primary classifier since it has no neutral label. The results for the cascading experiments are in Rows 6-9 of Table 4.\nCascading NRC and AutoSlog provides the best performance, improving both the positive and negative classes, for a macro F of 0.71. This shows that the learned implicit polarity information from\nAutoSlog improves NRC\u2019s performance.\nSince our best two-classifier cascade comes from combining NRC and AutoSlog, we also test a cascade that adds Stanford or SVM. We achieve our best macro F of 0.75 for the combination with SVM.\nAnalysis and Discussion. Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015).\nTables 5 and 6 illustrate several learned lexicofunctional patterns for positive events used in the AutoSlog classifier. The patterns shown in Table 5 are predicted by A&R\u2019s framework, some functions of which can be seen in Table 1. For example, we find a range of basic state descriptions (have party, have cancer) whose basic entailment category is either possessive or property state. Since Ehave is positive for a first-person subject only if the object is positive, and negative if the object is negative, we predict that parties are good to possess and that cancer is a bad property to have. In this way, we can recruit the existing function for have to induce new positive or negative things to \u201cpossess.\u201d In line with A&R\u2019s claims, many events are identified with their final results: headed for results in being at a desired location, while not coming home results in some-\nthing failing to be at a desired location. We find it a welcome result that our semi-supervised methods yield patterns that correspond to the A&R classes, thus validating our suspicion that first-person sentences furnish a simplifying test ground for discovering functional patterns in the wild.\nHowever, many patterns are not covered by A&R\u2019s general classes, see Table 6. Looking first at verbs, one major correlation is between positive classes and public events and negative classes and private states. Verbs extracted from the positive class tend to be eventive and agentive describing more dynamic activities and interactions, such as played, swim, enjoyed, and danced. Even many positive have uses are light verbs describing an activity such as have lunch.\nVerbs from the negative class are strikingly different. They are very often stative, where the author is the experiencer (cognitive subject) of that private state. While this state vs. event distinction is not one existing computational models of sentiment or affect discuss explicitly, it replicates a finding that consistently emerges in clinical psychology, one that is explicitly argued for in cognitive-behavioral accounts of the mood that particular activities evoke (Lewinsohn et al., 1985; MacPhillamy and Lewinsohn, 1982; Russo et al., 2015). In addition, Table 6 reveals several novel result state categories. The success, planning, and unmet desire frames are all ultimately about goalfulfillment (or lack thereof). While the success and unmet desire cases could be understood as having or lacking something, the planning cases indicate steps achieved toward a desired end-state. Previous work on learning affect from eventuality descriptions has largely focused on actions. Our results indicate that private state descriptions are another rich source of evidence."}, {"heading": "5 Related Work", "text": "Previous work learns phrasal markers of implicit polarity via bootstrapping from largescale text sources, e.g. Vu et al. (2014) learn emotion-specific event types by extracting emotion,event pairs on Twitter. Li et al. (2014) uses Twitter to bootstrap \u2018major life events\u2019 and typical replies to those events.\nDing and Riloff (2016) extract subj-verb-obj triples from blog posts. They then apply label propagation to spread polarity from sentences to events. However, the triples they learn do not focus on first-person experiencers. They also filter private states out of the verbs used to learn their triples, whereas we have found that verbs relating to private states such as need, want and realize are important indicators of first-person affect.\nBalahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions and extract sequences of subject-verb-object triples, which they annotate for basic emotions.\nRecent work has built on this idea, and developed methods to automatically expand Anand & Reschke\u2019s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015). Choi & Wiebe\u2019s work comes closest to ours in trying to induce (not annotate) lexical functions, but we attempt to infer these from stories directly, whereas they use a structured lexical resource."}, {"heading": "6 Conclusion", "text": "We show that we can learn lexico-functional linguistic patterns that reliably predict first-person affect. We constructed a dataset of positive and negative first-person experiencer sentences and used them to learn such patterns. We then showed that the performance of current sentiment classifiers can be enhanced by augmenting them with these patterns. By adding our AutoSlog classifier\u2019s results to existing classifiers we were able to improve from a baseline 0.67 to 0.75 Macro F with a cascading classifier of NRC, AutoSlog and SVM. In addition, we analyze the linguistic functions that indicate positivity and negativity for the first person experiencer, and show that they are very different. In first-person descriptions, positivity is often signaled by active participations in events, while negativity involves private states. In future\nwork, we plan to explore the integration of these observations into sentiment resources such as the +-Effect lexicon (Choi and Wiebe, 2014). We plan to apply these high precision first-person lexical patterns beyond blog data and with other personmarking."}], "references": [{"title": "Verb classes as evaluativity functor classes", "author": ["Pranav Anand", "Kevin Reschke."], "venue": "Proceedings of Verb pages 98\u2013103.", "citeRegEx": "Anand and Reschke.,? 2010", "shortCiteRegEx": "Anand and Reschke.", "year": 2010}, {"title": "Building and exploiting emotinet, a knowledge base for emotion detection based on the appraisal theory model", "author": ["Alexandra Balahur", "Jesus M. Hermida", "Andres Montoyo."], "venue": "IEEE Trans. Affect. Comput. 3(1):88\u2013101.", "citeRegEx": "Balahur et al\\.,? 2012", "shortCiteRegEx": "Balahur et al\\.", "year": 2012}, {"title": "Data-driven dialogue systems for social agents", "author": ["Kevin Bowden", "Shereen Oraby", "Amita Misra", "Jiaqu Wu", "Stephanie Lukin", "Marilyn Walker."], "venue": "International Workshop on Spoken Dialogue Systems.", "citeRegEx": "Bowden et al\\.,? 2017", "shortCiteRegEx": "Bowden et al\\.", "year": 2017}, {"title": "The icwsm 2009 spinn3r dataset", "author": ["Kevin Burton", "Akshay Java", "Ian Soboroff."], "venue": "Proceedings of the Annual Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Burton et al\\.,? 2009", "shortCiteRegEx": "Burton et al\\.", "year": 2009}, {"title": "+/effectwordnet: Sense-level lexicon acquisition for opinion inference", "author": ["Yoonjung Choi", "Janyce Wiebe."], "venue": "EMNLP.", "citeRegEx": "Choi and Wiebe.,? 2014", "shortCiteRegEx": "Choi and Wiebe.", "year": 2014}, {"title": "Benefactive/malefactive event and writer attitude annotation", "author": ["Lingjia Deng", "Yoonjung Choi", "Janyce Wiebe."], "venue": "ACL. pages 120\u2013125.", "citeRegEx": "Deng et al\\.,? 2013", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Sentiment propagation via implicature constraints", "author": ["Lingjia Deng", "Janyce Wiebe."], "venue": "EACL. pages 377\u2013385.", "citeRegEx": "Deng and Wiebe.,? 2014", "shortCiteRegEx": "Deng and Wiebe.", "year": 2014}, {"title": "Acquiring knowledge of affective events from blogs using label propagation", "author": ["Haibo Ding", "Ellen Riloff."], "venue": "AAAI.", "citeRegEx": "Ding and Riloff.,? 2016", "shortCiteRegEx": "Ding and Riloff.", "year": 2016}, {"title": "Identifying personal stories in millions of weblog entries", "author": ["Andrew Gordon", "Reid Swanson."], "venue": "Third International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA.", "citeRegEx": "Gordon and Swanson.,? 2009", "shortCiteRegEx": "Gordon and Swanson.", "year": 2009}, {"title": "Automatically producing plot unit representations for narrative text", "author": ["Amit Goyal", "Ellen Riloff", "Hal Daum\u00e9 III."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pages 77\u201386.", "citeRegEx": "Goyal et al\\.,? 2010", "shortCiteRegEx": "Goyal et al\\.", "year": 2010}, {"title": "The weka data mining software: An update", "author": ["M. Hall", "F. Eibe", "G. Holms", "B. Pfahringer", "P. Reutemann", "I. Witten."], "venue": "SIGKDD Explorations 11(1).", "citeRegEx": "Hall et al\\.,? 2005", "shortCiteRegEx": "Hall et al\\.", "year": 2005}, {"title": "Echoes from the past: how technology mediated reflection improves well-being", "author": ["Ellen Isaacs", "Artie Konrad", "Alan Walendowski", "Thomas Lennig", "Victoria Hollis", "Steve Whittaker."], "venue": "Proceedings of the SIGCHI Conference on Human", "citeRegEx": "Isaacs et al\\.,? 2013", "shortCiteRegEx": "Isaacs et al\\.", "year": 2013}, {"title": "Sentiment analysis of short informal texts", "author": ["Svetlana Kiritchenko", "Xiaodan Zhu", "Saif M. Mohammad."], "venue": "J. Artif. Int. Res. 50(1):723\u2013762.", "citeRegEx": "Kiritchenko et al\\.,? 2014", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "The unpleasant events schedule: A scale for the measurement of aversive events", "author": ["Peter M Lewinsohn", "Robin M Mermelstein", "Carolyn Alexander", "Douglas J MacPhillamy."], "venue": "Journal of Clinical Psychology 41(4):483\u2013498.", "citeRegEx": "Lewinsohn et al\\.,? 1985", "shortCiteRegEx": "Lewinsohn et al\\.", "year": 1985}, {"title": "What a nasty day: Exploring mood-weather relationship from twitter", "author": ["Jiwei Li", "Xun Wang", "Eduard Hovy."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, New", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "The pleasant events schedule: Studies on reliability, validity, and scale intercorrelation", "author": ["Douglas J MacPhillamy", "Peter M Lewinsohn."], "venue": "Journal of Consulting and Clinical Psychology 50(3):363.", "citeRegEx": "MacPhillamy and Lewinsohn.,? 1982", "shortCiteRegEx": "MacPhillamy and Lewinsohn.", "year": 1982}, {"title": "Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu."], "venue": "Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013). At-", "citeRegEx": "Mohammad et al\\.,? 2013", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "And thats a fact: Distinguishing factual and emotional argumentation in online dialogue", "author": ["Shereen Oraby", "Lena Reed", "Ryan Compton", "Ellen Riloff", "Marilyn Walker", "Steve Whittaker."], "venue": "Proceedings of the 2nd Workshop on Argumentation Mining.", "citeRegEx": "Oraby et al\\.,? 2015", "shortCiteRegEx": "Oraby et al\\.", "year": 2015}, {"title": "Learning emotion indicators from tweets: Hashtags, hashtag patterns, and phrases", "author": ["Ashequl Qadir", "Ellen Riloff."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational", "citeRegEx": "Qadir and Riloff.,? 2014", "shortCiteRegEx": "Qadir and Riloff.", "year": 2014}, {"title": "Connotation frames: A data-driven investigation", "author": ["Hannah Rashkin", "Sameer Singh", "Yejin Choi."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-", "citeRegEx": "Rashkin et al\\.,? 2016", "shortCiteRegEx": "Rashkin et al\\.", "year": 2016}, {"title": "teragram: Rule-based detection of sentiment phrases using sas sentiment analysis", "author": ["Hilke Reckman", "Cheyanne Baird", "Jean Crawford", "Richard Crowell", "LinneaMicciulla", "Saratendu Sethi", "Fruzsina Veress."], "venue": "SemEval@NAACL-HLT.", "citeRegEx": "Reckman et al\\.,? 2013", "shortCiteRegEx": "Reckman et al\\.", "year": 2013}, {"title": "Learning extraction patterns for subjective expressions", "author": ["E. Riloff", "J. Wiebe."], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics, pages 105\u2013112.", "citeRegEx": "Riloff and Wiebe.,? 2003", "shortCiteRegEx": "Riloff and Wiebe.", "year": 2003}, {"title": "Automatically generating extraction patterns from untagged text", "author": ["Ellen Riloff."], "venue": "Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2. pages 1044\u20131049.", "citeRegEx": "Riloff.,? 1996", "shortCiteRegEx": "Riloff.", "year": 1996}, {"title": "Extending effect annotation with lexical decomposition", "author": ["Josef Ruppenhofer", "Jasper Brandes."], "venue": "6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis Wass 2015. page 67.", "citeRegEx": "Ruppenhofer and Brandes.,? 2015", "shortCiteRegEx": "Ruppenhofer and Brandes.", "year": 2015}, {"title": "Semeval-2015 task 9: Clipeval implicit polarity of events", "author": ["Irene Russo", "Tommaso Caselli", "Carlo Strapparava."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval.", "citeRegEx": "Russo et al\\.,? 2015", "shortCiteRegEx": "Russo et al\\.", "year": 2015}, {"title": "Experiencing emotion : a crosscultural study", "author": ["Klaus R. Scherer", "Harald G. Wallbott", "Angela B. Summerfield."], "venue": "Cambridge University Press.", "citeRegEx": "Scherer et al\\.,? 1986", "shortCiteRegEx": "Scherer et al\\.", "year": 1986}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Conference on Empirical Methods in Nat-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Acquiring a dictionary of emotion-provokingevents", "author": ["Hoa Trong Vu", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."], "venue": "EACL 2014 page 128.", "citeRegEx": "Vu et al\\.,? 2014", "shortCiteRegEx": "Vu et al\\.", "year": 2014}, {"title": "Learning subjective language", "author": ["J. Wiebe", "T. Wilson", "R. Bruce", "M. Bell", "M. Martin."], "venue": "Computational linguistics 30(3):277\u2013308.", "citeRegEx": "Wiebe et al\\.,? 2004", "shortCiteRegEx": "Wiebe et al\\.", "year": 2004}, {"title": "Identifying subjective characters in narrative", "author": ["Janyce M Wiebe."], "venue": "Proceedings of the 13th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, pages 401\u2013 406.", "citeRegEx": "Wiebe.,? 1990", "shortCiteRegEx": "Wiebe.", "year": 1990}, {"title": "Opinionfinder: A system for subjectivity analysis", "author": ["T. Wilson", "P. Hoffmann", "S. Somasundaran", "J. Kessler", "J. Wiebe", "Y. Choi", "C. Cardie", "E. Riloff", "S. Patwardhan."], "venue": "Proceedings of HLT/EMNLP on Interactive Demonstrations. Association for Com-", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": "Being able to reliably predict the affect a person may feel towards events they encounter has a range of potential applications, including monitoring mood and mental health (Isaacs et al., 2013) and getting conversational assistants to respond appropriately (Bowden et al.", "startOffset": 173, "endOffset": 194}, {"referenceID": 2, "context": ", 2013) and getting conversational assistants to respond appropriately (Bowden et al., 2017).", "startOffset": 71, "endOffset": 92}, {"referenceID": 30, "context": "Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005).", "startOffset": 115, "endOffset": 136}, {"referenceID": 27, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 14, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 7, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 9, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 24, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 12, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 20, "context": "One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).", "startOffset": 139, "endOffset": 284}, {"referenceID": 0, "context": "Anand and Reschke (2010) (A&R) proposed that the affect a lexical predicate communicates should be modeled as an n-ary function, taking as inputs the affect that the speaker bears towards", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "Similarly, Rashkin et al. (2016) represent the typical affect communicated by particular predicates via connotation frames.", "startOffset": 11, "endOffset": 33}, {"referenceID": 28, "context": "many private state descriptions (Wiebe et al., 2004; Wiebe, 1990).", "startOffset": 32, "endOffset": 65}, {"referenceID": 29, "context": "many private state descriptions (Wiebe et al., 2004; Wiebe, 1990).", "startOffset": 32, "endOffset": 65}, {"referenceID": 26, "context": "We show that Stanford sentiment (Socher et al., 2013) has a best performance of 0.", "startOffset": 32, "endOffset": 53}, {"referenceID": 3, "context": "We start with a set of first-person narratives (weblogs) drawn from the Spinn3r corpus, that cover a wide range of topics (Burton et al., 2009; Gordon and Swanson, 2009).", "startOffset": 122, "endOffset": 169}, {"referenceID": 8, "context": "We start with a set of first-person narratives (weblogs) drawn from the Spinn3r corpus, that cover a wide range of topics (Burton et al., 2009; Gordon and Swanson, 2009).", "startOffset": 122, "endOffset": 169}, {"referenceID": 7, "context": "To reduce noise, we restrict the blogs to those from well-known blogging sites (Ding and Riloff, 2016), and select 15,466 stories whose length ranges from 225 to 375 words.", "startOffset": 79, "endOffset": 102}, {"referenceID": 22, "context": "To bootstrap, we apply AutoSlog-TS, a weakly supervised pattern learner that only requires training sets os stories labeled broadly as POSITIVE or NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003).", "startOffset": 156, "endOffset": 194}, {"referenceID": 21, "context": "To bootstrap, we apply AutoSlog-TS, a weakly supervised pattern learner that only requires training sets os stories labeled broadly as POSITIVE or NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003).", "startOffset": 156, "endOffset": 194}, {"referenceID": 4, "context": "In each case though, because these patterns are localized to a verb and only one element, they allow us to learn highly specific patterns that could be incorporated into a dictionary such as +Effect (Choi and Wiebe, 2014).", "startOffset": 199, "endOffset": 221}, {"referenceID": 22, "context": "These parameters are tuned on the dev set (Riloff, 1996; Oraby et al., 2015; Riloff and Wiebe, 2003).", "startOffset": 42, "endOffset": 100}, {"referenceID": 17, "context": "These parameters are tuned on the dev set (Riloff, 1996; Oraby et al., 2015; Riloff and Wiebe, 2003).", "startOffset": 42, "endOffset": 100}, {"referenceID": 21, "context": "These parameters are tuned on the dev set (Riloff, 1996; Oraby et al., 2015; Riloff and Wiebe, 2003).", "startOffset": 42, "endOffset": 100}, {"referenceID": 22, "context": "We run AutoSlog over the training corpus, using the dev set to tune the parameters \u03b8f , \u03b8p and \u03b8n (Riloff, 1996), in order to maximize macro Fscore.", "startOffset": 98, "endOffset": 112}, {"referenceID": 10, "context": "We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 16, "context": ", 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sen-", "startOffset": 58, "endOffset": 81}, {"referenceID": 10, "context": "We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sen-", "startOffset": 118, "endOffset": 254}, {"referenceID": 26, "context": "timent classifier (Socher et al., 2013).", "startOffset": 18, "endOffset": 39}, {"referenceID": 9, "context": "Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015).", "startOffset": 171, "endOffset": 266}, {"referenceID": 4, "context": "Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015).", "startOffset": 171, "endOffset": 266}, {"referenceID": 1, "context": "Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015).", "startOffset": 171, "endOffset": 266}, {"referenceID": 23, "context": "Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015).", "startOffset": 171, "endOffset": 266}, {"referenceID": 13, "context": "ical psychology, one that is explicitly argued for in cognitive-behavioral accounts of the mood that particular activities evoke (Lewinsohn et al., 1985; MacPhillamy and Lewinsohn, 1982; Russo et al., 2015).", "startOffset": 129, "endOffset": 206}, {"referenceID": 15, "context": "ical psychology, one that is explicitly argued for in cognitive-behavioral accounts of the mood that particular activities evoke (Lewinsohn et al., 1985; MacPhillamy and Lewinsohn, 1982; Russo et al., 2015).", "startOffset": 129, "endOffset": 206}, {"referenceID": 24, "context": "ical psychology, one that is explicitly argued for in cognitive-behavioral accounts of the mood that particular activities evoke (Lewinsohn et al., 1985; MacPhillamy and Lewinsohn, 1982; Russo et al., 2015).", "startOffset": 129, "endOffset": 206}, {"referenceID": 26, "context": "Vu et al. (2014) learn emotion-specific event types by extracting emotion,event pairs on Twitter.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "Li et al. (2014) uses Twitter to bootstrap \u2018major life events\u2019 and typical replies to those events.", "startOffset": 0, "endOffset": 17}, {"referenceID": 25, "context": "(2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions and extract sequences of subject-verb-object triples, which they annotate for basic emotions.", "startOffset": 62, "endOffset": 84}, {"referenceID": 1, "context": "Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke\u2019s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015).", "startOffset": 156, "endOffset": 272}, {"referenceID": 4, "context": "Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke\u2019s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015).", "startOffset": 156, "endOffset": 272}, {"referenceID": 5, "context": "Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke\u2019s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015).", "startOffset": 156, "endOffset": 272}, {"referenceID": 6, "context": "Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke\u2019s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015).", "startOffset": 156, "endOffset": 272}, {"referenceID": 23, "context": "Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke\u2019s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015).", "startOffset": 156, "endOffset": 272}, {"referenceID": 4, "context": "work, we plan to explore the integration of these observations into sentiment resources such as the +-Effect lexicon (Choi and Wiebe, 2014).", "startOffset": 117, "endOffset": 139}], "year": 2017, "abstractText": "Informal first-person narratives are a unique resource for computational models of everyday events and people\u2019s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate\u2019s arguments. We present a method to learn proxies for these functions from firstperson narratives. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F.", "creator": "LaTeX with hyperref package"}}}