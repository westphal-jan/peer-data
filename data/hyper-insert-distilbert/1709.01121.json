{"id": "1709.01121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Learning to parse from a semantic objective: It works. Is it syntax?", "abstract": "recent analytic work on reinforcement learning measures and other gradient estimators for latent tree learning has routinely made showed it possible best to directly train neural networks that learn accordingly to both parse precisely a sentence and simultaneously use potentially the resulting prediction parse problems to interpret completely the sentence, all automatically without exposure to ground - borne truth parse trees acting at sequential training recall time. second surprisingly, utilizing these models potentially often theoretically perform better at sentence trees understanding primary tasks than models employed that use parse trees from conventional parsers. and this paper aims instead to collectively investigate what these latent tree structural learning models supposedly learn. we replicate two discrete such derived models in a generally shared codebase and find that ( v i ) considers they do outperform global baselines on linear sentence dependency classification, but consider that ( ii ) their numerical parsing strategies are simply not geographically especially poorly consistent across random restarts, ( iii ) suggests the matching parses they initially produce tend to be geographically shallower than such ptb grammatical parses, and ( \uff08 iv ) these don't resemble those of older ptb examples or of any other recognizable primitive semantic models or functional syntactic grammar formalism.", "histories": [["v1", "Mon, 4 Sep 2017 19:05:39 GMT  (36kb)", "http://arxiv.org/abs/1709.01121v1", "13 pages, 6 figures, 4 tables, submitted to TACL"]], "COMMENTS": "13 pages, 6 figures, 4 tables, submitted to TACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adina williams", "andrew drozdov", "samuel r bowman"], "accepted": false, "id": "1709.01121"}, "pdf": {"name": "1709.01121.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Adina Williams", "Andrew Drozdov", "Samuel R. Bowman"], "emails": ["adinawilliams@nyu.edu", "andrew.drozdov@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n01 12\n1v 1\n[ cs\n.C L\n] 4\nS ep\nRecent work on reinforcement learning and other gradient estimators for latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) they do outperform baselines on sentence classification, but that (ii) their parsing strategies are not especially consistent across random restarts, (iii) the parses they produce tend to be shallower than PTB parses, and (iv) these do not resemble those of PTB or of any other recognizable semantic or syntactic grammar formalism."}, {"heading": "1 Introduction", "text": "Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)\u2014which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree\u2014have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016). Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce\n\u2217Now at eBay, Inc.\nparse trees that they then consume. Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification. These models are designed to learn grammars\u2014strategies for assigning trees to sentences\u2014that are suited to help solve the sentence understanding task at hand, rather than ones that approximate expert-designed grammars like that of the Penn Treebank (PTB; Marcus et al., 1999).\nLatent tree learning models have shown striking success at sentence understanding, reliably performing better on sentiment analysis and textual entailment than do comparable TreeRNN models which use parses assigned by conventional parsers, and setting the state of the art among sentence-encoding models for textual entailment. However, none of the\nwork in latent tree learning to date has included any substantial evaluation of the quality of the trees induced by these models, leaving open an important question which this paper aims to answer: Do these models owe their success to consistent, principled latent grammars? If they do, these grammars may be worthy objects of study for researchers in syntax and semantics. If they do not, understanding why the models succeed without syntax could lead to new insights into the use of TreeRNNs and into sentence understanding more broadly.\nWhile there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.\nThis is well illustrated by syntactically ambiguous sentences like the one below, repeated from Sag (1991) a.o.:\n1. (a) I [ saw the man ] [ with the telescope ]\n\u2192\u0592 I used the telescope to view the man\n(b) I saw the [ man [ with the telescope ] ]\n\u2192\u0592I saw the man who had a telescope\nUnder the constituency parse shown in 1a, with a telescope provides additional information about the action described by the constituent saw a man, and in 1b, with a telescope provides additional information about the individual described by man. If the same string of words can be assigned two different valid constituency structures, two different interpretations generally result. Constituency information can be straightforwardly expressed using an unlabeled parse tree like the ones used in TreeRNNs, and expressing constituency information is the generally the primary motivation for using trees in TreeRNNs.\nIn this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al. (2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four\nissues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.\nWe confirm that both types of model succeed at producing useful sentence representations, but find that only the stronger of the two models\u2014that of Choi et al. (2017)\u2014learns a nontrivial grammar or outperforms its baseline. We find that that grammar agrees with PTB grammar with roughly chance accuracy, and does not show any other consistent, linguistically plausible patterns. We do find, though, that the resulting grammar has some regularities, including a preference for shallow trees, and a preference to treat pairs of adjacent words at the edges of a sentence as constituents."}, {"heading": "2 Background", "text": "The work discussed in this paper is closely related to work on grammar induction, in which statistical learners attempt to solve the difficult problem of reconstructing the grammar that generated a corpus of text using only that corpus and, optionally, some heuristics about the nature of the expected grammar. Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here. In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model.\nThey find that the resulting SRL model is more effective than one built on a purely unsupervised grammar induction system, but that using a conventionally trained parser instead yields better SRL performance.\nThere is also a long history of work on artificial neural network models that build latent hierarchical structures without direct supervision when\nsolving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al. (2015).\nSocher et al. (2011) present the first neural network model that we are aware of that uses the same learned representations to both parse a sentence and make semantic classification decisions on that sentence and the resulting parse. Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work.\nWe are only aware of four prior works on of latent tree learning for sentence understanding with neural networks. All four jointly train two model components\u2014a parser based on distributed representations of words and phrases and a TreeRNN of some kind that uses those parses\u2014but differ in the parsing strategies, TreeRNN parameterizations, and training objective used.\nSocher et al. (2011) use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair. They train their model on a sentiment analysis objective, but rather than training the parsing component on that objective as well, they use a combination of an auxiliary autoencoding objective and a nonparametric scoring function to parse.\nWhile this work shows good results on sentiment analysis, it does not evaluate the proposed model\u2019s ability to induce trees. There is neither any direct analysis of the induced trees nor any comparison with a baseline that uses trees from a conventionallytrained parser.\nThe remaining three models all use TreeLSTMs (Tai et al., 2015), and all train and evaluate both components on a shared semantic objective. All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective. The models differ primarily in the ways in which they use this task objective to train their parsing components, and in the structures of those components.\nYogatama et al. (2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradi-\nents for the transition classification function, which produces discrete decisions and does not otherwise receive gradients through backpropagation. Surprisingly, and in contrast to Gormley et al., they find that a small 100D instance of this RL-SPINNmodel performs somewhat better on several text classification tasks than an otherwise-identical model which is explicitly trained to parse.\nMaillard et al. (2017) present a model which explicitly computes O(N2) possible tree nodes for N words, and uses a soft gating strategy to approximately select valid combinations of these nodes that form a tree. This model is trainable entirely using backpropagation, and a 100D instance of the model performs slightly better than RL-SPINN on SNLI.\nChoi et al. (2017) present a model (which we call ST-Gumbel) which uses a similar data structure and gating strategy to Maillard et al., but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016). This allows them to use a hard categorical gating function, so that their output vector sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al.. They report substantial gains in both speed and accuracy over Maillard et al. and Yogatama et al. on SNLI.\nSeveral models (Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using attention or related mechanisms, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2016) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth.\nOther past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights from this work include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation."}, {"heading": "3 Models and Methods", "text": "This paper investigates the behavior of two models: RL-SPINN and ST-Gumbel. Both have been shown to outperform similar models based on supervised parsing, and the two represent two substantially dif-\nferent approaches to latent tree learning.\nSPINN Variants All three of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016). Figure 2 shows and describes the architecture.\nIn the base SPINN model, all model components are used, and the transition classifier is trained on binarized Penn Treebank-style parses from the Stanford PCFGParser (Klein andManning, 2003), which are included with SNLI and MultiNLI. These binary-branching parse trees are converted to SHIFT/REDUCE sequences for use in the model through a simple reversible transformation.\nRL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss. Because this component produces discrete decisions, the REINFORCE algorithm is used (with the standard moving average reward baseline) to supply gradients for it.\nWe also evaluate two other variants of SPINN. In SPINN-NC (for No Connection from tracking to composition), the connection from the tracking LSTM to the composition function is severed. This weakens the model, but makes it exactly equivalent to a plain TreeLSTM\u2014it will produce the exact same vector that a TreeLSTM with the same composition function would have produced for the tree that the transition classifier implicitly produces. This model serves as a maximally comparable baseline for the ST-Gumbel model, which also performs composition using a standard TreeLSTM in forward-propagation.\nSPINN-PI-NT (for Parsed Input, No Tracking) removes the tracking LSTM as well as the two components that depend on it: the tracking-composition connection and the transition decision function. As such, it cannot produce its own parse trees, and must rely on trees from the input data. We include this in our comparison to understand the degree to which\ntraining a parser, rather than using a higher-quality off-the-shelf parser, impacts performance on our semantic task.\nST-Gumbel The ST-Gumbel model was developed by Choi et al. (2017) and is shown in Figure 3. The model takes a sequence of N \u2212 1 steps to build a tree over N words. At every step, every possible pair of adjacent words or phrase vectors in the partial tree is given to a TreeLSTM composition function to produce a new candidate phrase vector. A simple learned scoring function then selects the best of these candidates, which forms a constituent node in the tree and replaces its two children in the list of nodes that are available to compose. This repeats until only two nodes remain, at which point they are composed and the tree is complete. This exhaustive search increases the computational complexity of the model over (RL-)SPINN, but also allows the model to perform a form of easy-first parsing, making it easier for the model to explore the space of possible parsing strategies.\nThough the scoring function yields discrete decisions, the Straight-Through Gumbel-Softmax estimator of Jang et al. (2016) makes it possible to nonetheless efficiently compute an approximate gradient for the full model without the need for relatively brittle reinforcement learning techniques.\nData Our primary experiments use the MultiGenre Natural Language Inference Corpus (MultiNLI; Williams et al., 2017). MultiNLI is a 433k-example textual entailment dataset created in the style of SNLI, but with a more diverse range of source texts and longer and more complex sentences, which we expect will encourage the models to produce more consistent and interpretable trees than they otherwise might. Following Williams et al., we train on the combination of MultiNLI and SNLI in these experiments (yielding just under 1M training examples) and evaluate on MultiNLI (using the matched development and test sets).\nWe also evaluate trained models on the full Wall Street Journal section of the Penn Treebank, a seminal corpus of manually-constructed constituency parses which introduced the parsing standard used in this work. Because the models under study produce and consume binary-branching constituency trees without labels (and because such trees are already\nincluded with SNLI and MultiNLI), we use the Stanford Parser\u2019s CollapseUnaryTransformer and TreeBinarizer tools to convert these Penn Treebank Trees to this form.\nSentence Pair Classification Because our textual entailment task requires a model to classify pairs of sentences, but the models under study produce vectors for single sentences, we concatenate the two sentence vectors, their difference, and their elementwise product (following Mou et al., 2016), and feed the result into a 1024D ReLU layer to produce a representation for the sentence pair. This representation is fed into a three-way softmax classifier that selects one of the labels entailment, neutral, and contradiction for the pair.\nAdditional Details We implement all models in PyTorch 0.2. We closely follow the original Theano code for SPINN in our implementation, and we incorporate source code provided by Choi et al. for the core parsing data structures and sampling mechanism of the ST-Gumbel model. Our code, saved models, and model output are available on GitHub.1\nWe use GloVe vectors to represent words (standard 300D 840B word package, without fine tuning; Pennington et al., 2014), and feed them into a 2 \u00d7 300D bi-directional GRU RNN (based on the leaf LSTM of Choi et al.) to give the models access to local context information when making parsing decisions. To understand the impact of this component, we follow Choi et al. in also training each model with the leaf GRU replaced with a simpler context-sensitive input encoder that simply multiplies each GloVe vector by a matrix. We find that these models perform best when the temperature of the ST-Gumbel distribution is a trained parameter, rather than fixed at 1.0 as in Choi et al..\nWe use L2 regularization and apply dropout (Srivastava et al., 2014) to the input of the 1024D sentence pair combination layer. We train all models using the Adam optimizer (Kingma and Ba, 2015). For hyperparameters for which no obvious default value exists\u2014the L2 and dropout parameters, the relative weighting of the gradients from REINFORCE in RL-SPINN, the starting learning rate, and the size of the tracking LSTM state in SPINN\u2014we heuris-\n1 https://github.com/nyu-mll/spinn/tree/is-it-syntax-release\ntically select ranges in which usable values can be found (focusing on MultiNLI development set performance), and then randomly sample values from those ranges. We train each model five times using different samples from those ranges and different random initializations for model parameters. We use early stopping based on development set performance with all models."}, {"heading": "4 Does latent tree learning help sentence understanding?", "text": "Table 1 shows the accuracy of all models on two test sets: SNLI (training on SNLI only, for comparison with prior work), and MultiNLI (training on both datasets). Each figure represents the accuracy of the best run, selected using the development set, of five runs with different random initializations and hyperparameter values.\nOn both SNLI and MultiNLI, we reproduce the key result of Choi et al., showing that the STGumbel model, which receives no syntactic information at training time, outperforms SPINN-NC, which performs composition in an identical manner but is explicitly trained to parse. This suggests that the latent trees are helpful for constructing semantic representations for sentences, whether or not they resemble conventional parse trees.\nOur results with RL-SPINN are more equivocal. That model matches, but does not beat, the performance of the full SPINN model, which is equivalent except that it is trained to parse. However, our implementation of RL-SPINN outperforms Yogatama et al.\u2019s (lower-dimensional) implementation by a substantial margin. The impact of the leaf GRU is sometimes significant, but the direction of this effect is not consistent.\nOur results with SPINN-PI-NT are not substantially better than those with any other model, suggesting the relatively simple greedy parsing strategies used by the other models are not a major limiting factor in their performance. None of our models reach the state of the art on either task, but all are comparable in both absolute and relative performance to other published results, suggesting that we have trained reasonable examples of latent tree learning models and can draw informative conclusions by studying the behaviors of these models."}, {"heading": "5 Are these models consistent?", "text": "If it were the case that a latent tree learning model outperforms its baselines by identifying some specific grammar for English that is better than the one used in PTB and the Stanford Parser, then we would expect these models to identify roughly the same grammar across random restarts and minor configuration changes, and to use that grammar to pro-"}, {"heading": "300D SPINN 67.1 (1.0) 68.3 71.5", "text": "duce consistent task performance. Table 2 shows two measures of consistency for the four models that produce parses and a simple random baseline that produces parses by randomly merging pairs of adjacent words and phrases.\nWe first show the variation in accuracy on the MultiNLI development set across runs. While one outlier distorts these numbers for ST-Gumbel without the leaf GRU, these figures are roughly equivalent between the latent tree learning models and the baselines, suggesting that these models are not substantially more brittle or more hyperparameter sensitive in their task performance. The second metric shows the self F1 for each model: the unlabeled binary F1 between the parses produced by two runs of the same model for the MultiNLI development set, averaged out over all possible pairings of different runs. This measures the degree to which the models reliably converge on the same parses, and sheds some light on the behavior of the models. The baseline models show relatively high consistency, with self F1 above 65%. ST-Gumbel is substantially less consistent, with scores below 50% but above the 32.6% random baseline. RL-SPINN appears to be highly consistent, with the runs without the leaf GRU reaching 98.5% self F1, suggesting that it reliably converges to a specific grammar. However, as we will discuss in later sections, this grammar appears to be trivial."}, {"heading": "6 Do these models learn PTB grammar?", "text": "Given that our latent tree learning models are at least somewhat consistent in what they learn, it is reasonable to ask what it is that they learn. We investigate this first quantitatively, then, in the next section, more qualitatively.\nTable 3 shows parsing performance on the Wall Street Journal sections of PTB for models trained on SNLI and MultiNLI. The baseline models perform fairly poorly in absolute terms, as they are neither well tuned for parse quality nor trained on news text, but the latent tree learning models perform dramatically worse. The ST-Gumbel models perform at or slightly above chance (represented by the random trees results), while the RL-SPINNmodels perform consistently below chance. These results suggest that these models do not learn grammars that in any way resemble PTB grammar.\nNext, we turn to the MultiNLI development set for further investigation. Table 4 shows results on MultiNLI for a wider range of measures. The table shows F1 measured with respect to three different references: automatically generated trivial trees for the corpus that are either strictly left-branching or strictly right-branching, and the PTB-style trees produced by the Stanford Parser for the corpus. We see that the baseline models perform about as well on MultiNLI as on PTB, with scores above 65%, and that these models produce trees that tend toward right branching rather than left branching.\nThe ST-Gumbel models perform only at or\nslightly above chance on the parsed sentences, and show a similar use of both right- and left-branching structures, with only a slight preference for the more linguistically common right-branching structures. This suggests that they learn grammars that differ quite substantially from PTB grammars, but may share some minor properties.\nThe RL-SPINN results are unequivocally negative. All runs perform below chance on the parsed sentences, and all have F1 scores over 92% with respect to the left-branching structures, suggesting that they primarily learn to produce strictly leftbranching trees. This trivial strategy, which makes the model roughly equivalent to a sequential RNN, is very easy to learn. In a shift\u2013reduce model like SPINN, the model can simply learn to perform the REDUCE operation whenever it is possible to do so, regardless of the specific words and phrases being parsed. This can be done by setting a high bias value for this choice in the transition classifier.\nThe rightmost column shows another measure of what is learned: the average depth\u2014the length of the path from the root to any given word\u2014of the induced trees. For the baseline models, this value is slightly above the 5.7 value for the Stanford Parser trees. For the RL-SPINNmodels, this number is predictably much higher, reflecting the very deep and narrow left-branching trees that those models tend to produce. For the ST-Gumbel model, though, this\nmetric is informative: the models consistently produce shallow trees with depth under 5. This hints at a possible interpretation: While shallower trees may be less informative about the structure of the sentence than real PTB trees, they reduce the number of layers that a word needs to pass through to reach the final classifier, potentially making the it easier to learn an effective composition function that faithfully encodes the contents of a sentence. This interpretation is supported by the results of Munkhdalai and Yu (2017b), who show that it is possible to do well on SNLI using a TreeLSTM (with a leaf LSTM) over arbitrarily chosen balanced trees with low depths."}, {"heading": "7 Analyzing the Learned Trees", "text": "In the previous three sections, we have shown that latent tree learning models are able to perform as well or better than models that have access to linguistically principled parse trees at training or test time, but that the grammars that they learn are neither consistent across runs, nor meaningfully similar to PTB grammar. In this section, we investigate the trees produced by these learned grammars directly to identify whether they capture any recognizable syntactic or semantic phenomena.\nThe RL-SPINN models create overwhelmingly left-branching trees. We observe few deviations from this pattern, but these occur almost exclusively\non sentences of fewer than seven words.\nIn some preliminary tuning runs not shown above, we saw models that deviated from this pattern more often, and one that fixated on right-branching structures instead, but we find no grammatically interesting patterns in any of these deviant structures.\nThe ST-Gumbel models learned substantially more complex grammars, and we focus on these for the remainder of the section. We discuss three model behaviors which yield linguistically implausible constituents. The first two highlight settings where the ST-Gumbel model is consistent where it shouldn\u2019t be, and the third highlights a setting in which it is worryingly inconsistent. The models\u2019 treatment of these three phenomena and our observation of these models\u2019 behavior more broadly suggest that the models do not produce trees that follow any recognizable semantic or syntactic principles.\nInitial and Final Two-Word Constituents The shallow trees produced by the ST-Gumbel models generally contain more constituents comprised of two words (rather than a single word combined with a phrase) than appear in the reference parses, and this behavior is especially pronounced at the edges of sentences, where the models frequently treat the first two words and the last two words as constituents. Since this behavior does not correspond to any grammatical phenomenon known to these authors, it likely stems from some unknown bias within the model design.\nThese models parse the first two words of a sentence into a constituent at rates well above the 50% rate seen in random parses and the 27.7% rate seen with the SPINNmodels, with this strategy appearing in 77.4% of the model\u2019s parses with the leaf GRU and 64.1% without. This strategy was consistently discovered across all of our runs of ST-Gumbel models with the leaf GRU, but it was discovered less fre-\nquently across restarts for runs without, which do not have direct access to linear position information. The models combine the final two words in each sentence at similar rates.\nWhile merging the final two words of a sentence nearly always results in a meaningless constituent containing a period or punctuation mark, merging the first two words can produce reasonable parses. This strategy is reasonable, for example, in sentences that begin with a determiner and a noun (Figure 4, top left). However, combining the first two words in sentences that start with adverbials, proper names, bare plurals, or noun phrases with multiple modifiers will generally result in meaningless constituents like Kings frequently (Figure 4, bottom).\nCombining the first two words of a sentence also often results in more subtly unorthodox trees like the one in the top right of Figure 4 that combine a verb with its subject rather than its object. The object and the verb of a sentence are generally taken to form a constituent in mainstream syntactic theory (Adger 2003; Sportiche et al. 2013) for three reasons: (i) We can replace it with a new constituent of the same type without changing the surrounding sentence structure, as in he did so, (ii) it can stand alone as an answer to a question like what did he do?, and (iii) it can be omitted in otherwise-repetitive sentences like he shot his gun, but she didn\u2019t .\nNegation The ST-Gumbel models also tend to learn a systematic and superficially reasonable strategy for negation: they pair any negation word (e.g., not, n\u2019t, no, none) with the word that immediately follows it. Random parses only form these constituents in 34% of the sentences, but the ST-Gumbel models with the leaf GRU do so about 67.2% of the time and consistently across runs, while those without the leaf GRU do so less consistently, but over 90% of the time in some runs.\nThis strategy is effective when the negation word is meant to modify a single other word to its right, as in Figure 5, top sub-figures, but this is frequently not the case. In Figure 5, bottom left, not forms a constituent with the preposition at, which would yield the bizarre interpretation that the person being discussed is not located at the place all sure.\nFunction Words and Modifiers Finally, the STGumbel models are not consistent in their treatment\nof function words like determiners or prepositions, or in their treatment of modifiers like adverbs and adjectives. For example, the determiner the should form a constituent with the noun phrase Nazi angle in the top two of Figure 6. The resulting phrase, the Nazi angle, has a clear meaning, and it passes syntactic tests for constituency. For example, one can replace the it with the pronoun it without otherwise modifying the sentence.\nSimilarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013). For example, the question how did the students react? can be answered simply with with horror. ST-Gumbel models often instead pair prepositions with the verb phrases that precede them, as in Figure 6, lower left, where this results in in the constituent the students acted with. From the semantic perspective, anomalous constituents like this, discussed the (6, upper left) or we briefly (6, top left) cannot be given to coherent meanings.\nThe ST-Gumbel models outperform syntax-based models on MultiNLI and SNLI, and the trees that they assigns to sentences do not generally resemble those of PTB grammar. If we attempt to interpret these trees under the standard assumption that all the constituents in a sentence must be interpretable and must contribute to the meaning of the sentence, we\nforce ourselves to interpret implausible constituents like we briefly and reaching implausible sentencelevel interpretations, such as taking the sentence in Figure 6, top left, to mean that that those of us who are brief discussed the Nazi angle. It is clear that these models do not use constituency in the way that it is used in any accepted syntactic or semantic framework."}, {"heading": "8 Conclusion", "text": "The experiments and analysis presented in this paper show that the best available models for latent tree learning learn grammars that do not correspond to the structures of formal syntax and semantics in any recognizable way. In spite of this, these models perform as well or better on sentence understanding\u2014 as measured by MultiNLI performance\u2014as models with access to Penn Treebank-style parses.\nThis result leaves us with an immediate puzzle: What do these models\u2014especially those based on the ST-Gumbel technique\u2014learn that allows them to do so well? We have presented some observations, but we are left without a fully satisfying explanation. A thorough investigation of this problem will likely require a search of new architectures for sentence encoding that borrow various behaviors from the models trained in this work.\nThis result also opens farther-reaching questions about grammar and sentence understanding: Will the optimal grammars for sentence understanding problems like NLI\u2014were we to explore the full space of grammars to find them\u2014share any recognizable similarities with the structures seen in formal work on syntax and semantics? A priori, we should expect that they should. While it is unlikely that PTB grammar is strictly optimal for any task, the empirical motivations for many of its core constituent types\u2014the noun phrase, the prepositional phrase, and so forth\u2014are straightforward and compelling. However, our best latent tree learning models are not able to discover these structures.\nIf we accept that some form of principled constituent structure is necessary or desirable, then we are left with an engineering problem: How do we identify this structure? Making progress in this direction will likely involve both improvements to the TreeRNN models at the heart of latent tree learning\nsystems, to make sure that these models are able to perform composition effectively enough to be able to make full use of learned structures, and also improvements to the structure search methods that are used to explore possible grammars."}, {"heading": "Acknowledgments", "text": "This project has benefited from financial support to SB by Google and Tencent Holdings and from a Titan X Pascal GPU donated by the NVIDIA Corporation to AD. Jon Gauthier contributed to early discussions that motivated this work, and he, Nikita Nangia, Kelly Zhang, and Cipta Herwana and provided help and advice."}], "references": [{"title": "Core syntax: A minimalist approach", "author": ["David Adger."], "venue": "Oxford University Press.", "citeRegEx": "Adger.,? 2003", "shortCiteRegEx": "Adger.", "year": 2003}, {"title": "What do neural machine translation models learn about morphology", "author": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass"], "venue": "In Proceedings of the 55th Annual Meeting of the Association", "citeRegEx": "Belinkov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2017}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Bayesian grammar induction for language modeling", "author": ["Stanley F. Chen."], "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Chen.,? 1995", "shortCiteRegEx": "Chen.", "year": 1995}, {"title": "Unsupervised learning of task-specific tree structures with tree-lstms", "author": ["Jihun Choi", "Kang Min Yoo", "Sang-goo Lee."], "venue": "ArXiv preprint 1707.02786.", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Aspects of the Theory of Syntax", "author": ["Noam Chomsky."], "venue": "MIT press.", "citeRegEx": "Chomsky.,? 1965", "shortCiteRegEx": "Chomsky.", "year": 1965}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio."], "venue": "ArXiv preprint 1609.01704.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Unsupervised structure prediction with non-parallel multilingual guidance", "author": ["Shay B Cohen", "Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Sreerupa Das", "C. Lee Giles", "Guo-Zheng Sun."], "venue": "Proceedings of The Fourteenth Annual Conference of Cognitive", "citeRegEx": "Das et al\\.,? 1992", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Pattern classification", "author": ["Richard O Duda", "Peter E Hart", "David G Stork."], "venue": "Wiley, New York.", "citeRegEx": "Duda et al\\.,? 1973", "shortCiteRegEx": "Duda et al\\.", "year": 1973}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "\u00dcber sinn und bedeutung", "author": ["Gottlob Frege."], "venue": "Wittgenstein Studien 1(1).", "citeRegEx": "Frege.,? 1892", "shortCiteRegEx": "Frege.", "year": 1892}, {"title": "Introduction to syntactic pattern recognition", "author": ["King Sun Fu."], "venue": "Syntactic Pattern Recognition, Applications, Springer Verlag, Berlin, pages 1\u201331.", "citeRegEx": "Fu.,? 1977", "shortCiteRegEx": "Fu.", "year": 1977}, {"title": "Lowresource semantic role labeling", "author": ["Matthew R. Gormley", "Margaret Mitchell", "Benjamin Van Durme", "Mark Dredze."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Gormley et al\\.,? 2014", "shortCiteRegEx": "Gormley et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 1828\u20131836.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Semantics in generative grammar", "author": ["Irene Heim", "Angelika Kratzer."], "venue": "Blackwell.", "citeRegEx": "Heim and Kratzer.,? 1998", "shortCiteRegEx": "Heim and Kratzer.", "year": 1998}, {"title": "Identifiability and unmixing of latent parse trees", "author": ["Daniel J. Hsu", "ShamM. Kakade", "Percy S. Liang."], "venue": "Advances in neural information processing systems. pages 1511\u20131519.", "citeRegEx": "Hsu et al\\.,? 2012", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Jang et al\\.,? 2016", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 190\u2013198.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, Sapporo, Japan,", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "author": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:521\u2013535.", "citeRegEx": "Linzen et al\\.,? 2016", "shortCiteRegEx": "Linzen et al\\.", "year": 2016}, {"title": "Learning structured text representations", "author": ["Yang Liu", "Mirella Lapata."], "venue": "ArXiv preprint 1705.09207.", "citeRegEx": "Liu and Lapata.,? 2017", "shortCiteRegEx": "Liu and Lapata.", "year": 2017}, {"title": "Jointly learning sentence embeddings and syntax with unsupervised tree-lstms", "author": ["Jean Maillard", "Stephen Clark", "Dani Yogatama."], "venue": "ArXiv preprint 1705.09189.", "citeRegEx": "Maillard et al\\.,? 2017", "shortCiteRegEx": "Maillard et al\\.", "year": 2017}, {"title": "Linguistic theory and natu", "author": ["Ivan A. Sag"], "venue": null, "citeRegEx": "Sag.,? \\Q1991\\E", "shortCiteRegEx": "Sag.", "year": 1991}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Con-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "An introduction to syntactic analysis and theory", "author": ["Dominique Sportiche", "Hilda Koopman", "Edward Stabler."], "venue": "John Wiley & Sons.", "citeRegEx": "Sportiche et al\\.,? 2013", "shortCiteRegEx": "Sportiche et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research (JMLR) 15(1):1929\u2013", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "The neural network pushdown automaton: Model, stack and learning simulations", "author": ["G.Z. Sun", "C.L. Giles", "H.H. Chen", "Y.C. Lee."], "venue": "Technical Report UMIACS-TR-93-77/CSTR-3118, University of Maryland.", "citeRegEx": "Sun et al\\.,? 1993", "shortCiteRegEx": "Sun et al\\.", "year": 1993}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "author": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."], "venue": "ArXiv preprint 1704.05426.", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine learning 8(34):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to compose words into sentences with reinforcement learning", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Yogatama et al\\.,? 2017", "shortCiteRegEx": "Yogatama et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 27, "context": ", 2011)\u2014which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree\u2014have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al.", "startOffset": 223, "endOffset": 244}, {"referenceID": 3, "context": ", 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 11, "context": ", 2016), and translation (Eriguchi et al., 2016).", "startOffset": 25, "endOffset": 48}, {"referenceID": 3, "context": "Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce", "startOffset": 30, "endOffset": 72}, {"referenceID": 34, "context": "Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "startOffset": 36, "endOffset": 101}, {"referenceID": 25, "context": "Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "startOffset": 36, "endOffset": 101}, {"referenceID": 5, "context": "Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2017) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.", "startOffset": 36, "endOffset": 101}, {"referenceID": 6, "context": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.", "startOffset": 196, "endOffset": 211}, {"referenceID": 6, "context": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.", "startOffset": 196, "endOffset": 225}, {"referenceID": 6, "context": "While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has been clear since at least Chomsky (1965); Frege (1892); Heim and Kratzer (1998) that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units or constituents.", "startOffset": 196, "endOffset": 250}, {"referenceID": 26, "context": "This is well illustrated by syntactically ambiguous sentences like the one below, repeated from Sag (1991) a.", "startOffset": 96, "endOffset": 107}, {"referenceID": 2, "context": "(2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.", "startOffset": 134, "endOffset": 178}, {"referenceID": 32, "context": "(2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.", "startOffset": 134, "endOffset": 178}, {"referenceID": 29, "context": "In this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 3, "context": "(2017) and Choi et al. (2017) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 5, "context": "We confirm that both types of model succeed at producing useful sentence representations, but find that only the stronger of the two models\u2014that of Choi et al. (2017)\u2014learns a nontrivial grammar or outperforms its baseline.", "startOffset": 148, "endOffset": 167}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al.", "startOffset": 78, "endOffset": 128}, {"referenceID": 8, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al.", "startOffset": 78, "endOffset": 128}, {"referenceID": 17, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al.", "startOffset": 78, "endOffset": 128}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977).", "startOffset": 79, "endOffset": 231}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.", "startOffset": 79, "endOffset": 245}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.", "startOffset": 79, "endOffset": 296}, {"referenceID": 4, "context": "Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here. In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model.", "startOffset": 79, "endOffset": 492}, {"referenceID": 31, "context": "The remaining three models all use TreeLSTMs (Tai et al., 2015), and all train and evaluate both components on a shared semantic objective.", "startOffset": 45, "endOffset": 63}, {"referenceID": 2, "context": "All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective.", "startOffset": 76, "endOffset": 97}, {"referenceID": 33, "context": "(2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise receive gradients through backpropagation.", "startOffset": 122, "endOffset": 138}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al.", "startOffset": 40, "endOffset": 58}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al.", "startOffset": 40, "endOffset": 77}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al.", "startOffset": 40, "endOffset": 122}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al. (2015). Socher et al.", "startOffset": 40, "endOffset": 153}, {"referenceID": 7, "context": "solving algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently Joulin and Mikolov (2015) and Grefenstette et al. (2015). Socher et al. (2011) present the first neural network model that we are aware of that uses the same learned representations to both parse a sentence and make semantic classification decisions on that sentence and the resulting parse.", "startOffset": 40, "endOffset": 175}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work.", "startOffset": 0, "endOffset": 194}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work. We are only aware of four prior works on of latent tree learning for sentence understanding with neural networks. All four jointly train two model components\u2014a parser based on distributed representations of words and phrases and a TreeRNN of some kind that uses those parses\u2014but differ in the parsing strategies, TreeRNN parameterizations, and training objective used. Socher et al. (2011) use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair.", "startOffset": 0, "endOffset": 632}, {"referenceID": 2, "context": "Bowman et al. (2016) introduce an efficient batchable architecture for doing this\u2014the Shift-reduce Parser-Interpreter Neural Network (SPINN; Figure 2)\u2014which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work. We are only aware of four prior works on of latent tree learning for sentence understanding with neural networks. All four jointly train two model components\u2014a parser based on distributed representations of words and phrases and a TreeRNN of some kind that uses those parses\u2014but differ in the parsing strategies, TreeRNN parameterizations, and training objective used. Socher et al. (2011) use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair. They train their model on a sentiment analysis objective, but rather than training the parsing component on that objective as well, they use a combination of an auxiliary autoencoding objective and a nonparametric scoring function to parse. While this work shows good results on sentiment analysis, it does not evaluate the proposed model\u2019s ability to induce trees. There is neither any direct analysis of the induced trees nor any comparison with a baseline that uses trees from a conventionallytrained parser. The remaining three models all use TreeLSTMs (Tai et al., 2015), and all train and evaluate both components on a shared semantic objective. All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective. The models differ primarily in the ways in which they use this task objective to train their parsing components, and in the structures of those components. Yogatama et al. (2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise receive gradients through backpropagation.", "startOffset": 0, "endOffset": 1709}, {"referenceID": 18, "context": ", but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016).", "startOffset": 63, "endOffset": 82}, {"referenceID": 20, "context": "Several models (Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using attention or related mechanisms, but do not propagate information up the trees as in typical compositional models.", "startOffset": 15, "endOffset": 81}, {"referenceID": 24, "context": "Several models (Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using attention or related mechanisms, but do not propagate information up the trees as in typical compositional models.", "startOffset": 15, "endOffset": 81}, {"referenceID": 7, "context": "Other models like that of Chung et al. (2016) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth.", "startOffset": 26, "endOffset": 46}, {"referenceID": 22, "context": "Recent highlights from this work include Linzen et al. (2016) on language modeling and Belinkov et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 1, "context": "(2016) on language modeling and Belinkov et al. (2017) on translation.", "startOffset": 32, "endOffset": 55}, {"referenceID": 2, "context": "Figure 2: The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down, reproduced with permission from Bowman et al. (2016). The model parses a sentence by selecting a sequence of SHIFT and REDUCE transitions using the transition classifier (shown in blue) and simultaneously uses the resulting parse to build a vector representation of the sentence by using a TreeLSTM composition function (shown in green) during REDUCE transitions.", "startOffset": 143, "endOffset": 164}, {"referenceID": 5, "context": "Figure 3: The ST-Gumbel model in its first step of processing the sentence the cat sat down, based on a figure by Choi et al. (2017), used with permission.", "startOffset": 114, "endOffset": 133}, {"referenceID": 2, "context": "SPINN Variants All three of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016). Figure 2 shows and describes the architecture.", "startOffset": 128, "endOffset": 149}, {"referenceID": 34, "context": "RL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss.", "startOffset": 52, "endOffset": 75}, {"referenceID": 5, "context": "ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2017) and is shown in Figure 3.", "startOffset": 47, "endOffset": 66}, {"referenceID": 5, "context": "ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2017) and is shown in Figure 3. The model takes a sequence of N \u2212 1 steps to build a tree over N words. At every step, every possible pair of adjacent words or phrase vectors in the partial tree is given to a TreeLSTM composition function to produce a new candidate phrase vector. A simple learned scoring function then selects the best of these candidates, which forms a constituent node in the tree and replaces its two children in the list of nodes that are available to compose. This repeats until only two nodes remain, at which point they are composed and the tree is complete. This exhaustive search increases the computational complexity of the model over (RL-)SPINN, but also allows the model to perform a form of easy-first parsing, making it easier for the model to explore the space of possible parsing strategies. Though the scoring function yields discrete decisions, the Straight-Through Gumbel-Softmax estimator of Jang et al. (2016) makes it possible to nonetheless efficiently compute an approximate gradient for the full model without the need for relatively brittle reinforcement learning techniques.", "startOffset": 47, "endOffset": 1010}, {"referenceID": 32, "context": "Data Our primary experiments use the MultiGenre Natural Language Inference Corpus (MultiNLI; Williams et al., 2017).", "startOffset": 82, "endOffset": 115}, {"referenceID": 29, "context": "We use L2 regularization and apply dropout (Srivastava et al., 2014) to the input of the 1024D sentence pair combination layer.", "startOffset": 43, "endOffset": 68}, {"referenceID": 21, "context": "We train all models using the Adam optimizer (Kingma and Ba, 2015).", "startOffset": 45, "endOffset": 66}, {"referenceID": 13, "context": "This hints at a possible interpretation: While shallower trees may be less informative about the structure of the sentence than real PTB trees, they reduce the number of layers that a word needs to pass through to reach the final classifier, potentially making the it easier to learn an effective composition function that faithfully encodes the contents of a sentence. This interpretation is supported by the results of Munkhdalai and Yu (2017b), who show that it is possible to do well on SNLI using a TreeLSTM (with a leaf LSTM) over arbitrarily chosen balanced trees with low depths.", "startOffset": 309, "endOffset": 447}, {"referenceID": 28, "context": "The object and the verb of a sentence are generally taken to form a constituent in mainstream syntactic theory (Adger 2003; Sportiche et al. 2013) for three reasons: (i) We can replace it with a new constituent of the same type without changing the surrounding sentence structure, as in he did so, (ii) it can stand alone as an answer to a question like what did he do?, and (iii) it can be omitted in otherwise-repetitive sentences like he shot his gun, but she didn\u2019t .", "startOffset": 111, "endOffset": 146}, {"referenceID": 0, "context": "Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013).", "startOffset": 107, "endOffset": 144}, {"referenceID": 28, "context": "Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013).", "startOffset": 107, "endOffset": 144}], "year": 2017, "abstractText": "Recent work on reinforcement learning and other gradient estimators for latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) they do outperform baselines on sentence classification, but that (ii) their parsing strategies are not especially consistent across random restarts, (iii) the parses they produce tend to be shallower than PTB parses, and (iv) these do not resemble those of PTB or of any other recognizable semantic or syntactic grammar formalism.", "creator": "LaTeX with hyperref package"}}}