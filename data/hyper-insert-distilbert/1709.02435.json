{"id": "1709.02435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "An Analysis of ISO 26262: Using Machine Learning Safely in Automotive Software", "abstract": "machine learning ( called ml ) plays an ever - increasing evolving role in advanced hybrid automotive qualification functionality for driver assistance and automobile autonomous service operation ; however, its adequacy somewhat from the perspective of safety certification remains controversial. in this paper, we simply analyze the impacts indicating that evolving the specific use of ml as an implementation optimization approach has on most iso'26262 safety lifecycle scenarios and typically ask what procedures could correctly be done beforehand to address implementing them. periodically we then provide practically a set of engineering recommendations on how about to adapt globally the metric standard to accommodate advanced ml.", "histories": [["v1", "Thu, 7 Sep 2017 20:10:56 GMT  (725kb,D)", "http://arxiv.org/abs/1709.02435v1", "6 pages, 3 figures"]], "COMMENTS": "6 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.SE cs.SY", "authors": ["rick salay", "rodrigo queiroz", "krzysztof czarnecki"], "accepted": false, "id": "1709.02435"}, "pdf": {"name": "1709.02435.pdf", "metadata": {"source": "CRF", "title": "An Analysis of ISO 26262: Using Machine Learning Safely in Automotive Software", "authors": ["Rick Salay", "Rodrigo Queiroz", "Krzysztof Czarnecki"], "emails": ["rsalay@gsd.uwaterloo.ca", "rqueiroz@gsd.uwaterloo.ca", "kczarnec@gsd.uwaterloo.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe use of machine learning (ML) is on the rise in many sectors of software development, and automotive software development is no different. In particular, Advanced Driver Assistance Systems (ADAS) and Autonomous Vehicles (AV) are two areas where ML plays a significant role [1], [2]. In automotive development, safety is a critical objective, and the emergence of standards such as ISO 26262 [3] has helped focus industry practices to address safety in a systematic and consistent way. Unfortunately, ISO 26262 was not designed to accommodate technologies such as ML, and this has created a tension between the need to innovate and the need to improve safety.\nIn response to this issue, research has been active in several areas. Recently, the safety of ML approaches in general have been analyzed both from theoretical [4] and pragmatic perspectives [5]. However, most research is specifically about neural networks (NN). Work on supporting the verification & validation (V&V) of NNs emerged in the 1990\u2019s with a focus on making their internal structure easier to assess by extracting representations that are more understandable [6]. General V&V methodologies for NNs have also been proposed [7], [8]. More recently, with the popularity of deep neural networks (DNN), verification research has included more diverse topics such as generating explanations of DNN predictions [9], improving the stability of classification [10] and property checking of DNNs [11].\nDespite their challenges, NNs are already used in high assurance systems (see [12] for a survey), and safety certification of NNs has received some attention. Pullum et al. [13] give detailed guidance on V&V as well as other aspects of safety assessment such as hazard analysis with a focus on adaptive systems in the the aerospace domain. Bedford et al. [14] define general requirements for addressing NNs in any safety\nstandard. Kurd et al. [15] have established criteria for NNs to use in a safety case.\nThe recent surge of interest in AV has also been driving research in certification. Koopman and Wagner [2] identify some of the key challenges to certification, including ML. Martin et al. [16] analyze the adequacy of ISO 26262 for AV but focus on the impact of the increased complexity it creates rather than specifically the use of ML. Finally, Spanfelner et al. [1] assess ISO 26262 from the perspective of driver assistance systems.\nThe contribution of the current paper is complementary to the above research. We analyze the impact that the use of MLbased software has on various parts of ISO 26262. Specifically, we consider its impact in the areas of hazard analysis and in the phases of the software development process. In all, we identify five distinct problems that the use of ML creates and make recommendations on steps toward addressing these problems both through changes to the standard and through additional research.\nThe remainder of the paper is structured as follows. In Sec. II, we give the required background on ISO 26262 and ML. Sec. III contains the analysis of the ISO 26262 safety lifecycle with five subsections describing each impacted area and the corresponding recommendations. Finally, in Sec. IV, we summarize and give concluding remarks."}, {"heading": "II. BACKGROUND", "text": "A. ISO 26262\nISO 26262 is a standard that regulates functional safety of road vehicles. It recommends the use of a Hazard Analysis and Risk Assessment (HARA) method to identify hazardous events in the system and to specify safety goals that mitigate the hazards. The standard has 10 parts, but we focus on Part 6: \u201cproduct development at the software level\u201d. The standard follows the well-known V model for engineering shown in Fig. 1.\nAn Automotive Safety Integrity Level (ASIL) refers to a risk classification scheme defined in ISO 26262 for an item (e.g., subsystem) in an automotive system. The ASIL represents the degree of rigor required (e.g., testing techniques, types of documentation required, etc.) to reduce the risk of the item, where ASIL D represents the highest and ASIL A the lowest risk. If an element is assigned QM (Quality Management), it\nar X\niv :1\n70 9.\n02 43\n5v 1\n[ cs\n.A I]\n7 S\nep 2\n01 7\ndoes not require safety management. The ASIL assessed for a given hazard is first assigned to the safety goal set to address the hazard and is then inherited by the safety requirements derived from that goal.\nPart 6 of the standard specifies the compliance requirements for software development. For example, Fig. 2 shows the error handling mechanisms recommended for use as part of the architectural design. The degree of recommendation for a method depends on the ASIL and is categorized as follows: ++ indicates that the method is highly recommended for the ASIL; + indicates that the method is recommended for the ASIL; and o indicates that the method has no recommendation for or against its usage for the ASIL. For example, Graceful Degradation (1b) is the only highly recommended mechanism for an ASIL C item, while an ASIL D item would also require Independent Parallel Redundancy (1c)."}, {"heading": "1a Static recovery mechanism + + + +", "text": ""}, {"heading": "1d Correcting codes for data + + + +", "text": ""}, {"heading": "B. Machine learning", "text": "In this paper, we are concerned with software implementation using ML. We call a programmed component to be one that is implemented using a programming language, regardless of whether the programming was done manually or automatically (e.g., via code generation). In contrast, an ML component is one that is a trained model using a supervised, unsupervised or reinforcement learning (RL) approach.\nThere are several characteristics of ML that can impact safety or safety assessment.\nNon-transparency. All types of ML models contain knowledge in an encoded form, but this encoding is harder to interpret for humans in some types than others. For example, a Bayesian Network for weather prediction is easier to interpret since the nodes are random variables representing humandefined concepts such as \u201cprecipitation type\u201d, \u201ctemperature\u201d, etc. In contrast, NN models are considered non-transparent,\nand significant research effort has been devoted to making them more transparent (e.g., [6], [9]). Increasing ML model expressive power is typically at the expense of transparency but some research efforts focus on mitigating this [17]. Nontransparency is an obstacle to safety assurance because it is more difficult for an assessor to develop confidence that the model is operating as intended.\nError rate. An ML model typically does not operate perfectly and exhibits some error rate. Thus, \u201ccorrectness\u201d of an ML component, even with respect to test data, is seldom achieved and it must be assumed that it will periodically fail. Furthermore, although an estimate of the true error rate is an output of the ML development process, there is only a statistical guarantee about the reliability of this estimate. Finally, even if the estimate of the true error rate was accurate, it may not reflect the error rate the system actually experiences while in operation after a finite set of inputs because the true error is based on an infinite set of samples [4]. These characteristics must be considered when designing safe system using ML components.\nTraining-based. Supervised and unsupervised learning based ML models are trained using a subset of possible inputs that could be encountered operationally. Thus, the training set is necessarily incomplete and there is no guarantee that it is even representative of the space of possible inputs. In addition, learning may overfit a model by capturing details incidental to the training set rather than general to all inputs. RL suffers from similar limitations since it typically explores only a subset of possible behaviours during training. The uncertainty that this creates about how an ML component will behave is a threat to safety. Another factor is that, even if the training set is representative, it may under-represent the safety-critical cases because these are often rarer in the input space [4].\nInstability. More powerful ML models (e.g., DNN) are typically trained using local optimization algorithms, and there can be multiple optima. Thus, even when the training set remains the same, the training process may produce a different result. However, changing the training set also may change the optima. In general, different optima may be far apart structurally, even if they are similar behaviourally. This characteristic makes it difficult to debug models or reuse parts of previous safety assessments."}, {"heading": "III. ANALYSIS OF ISO 26262", "text": "In this section, we detail our analysis of ML impacts on ISO 26262. Since an ML component is a specialized type of software component, we define an area of the standard as impacted when it is relevant to software components and the treatment of an ML component should differ from the existing treatment of software components by the standard. Applying this criterion to the ten parts of the standard resulted in identifying five areas of impact in two parts: the hazard analysis from the concept phase (Part 3) and the software development phase (Part 6). We describe the five areas of impact with corresponding recommendations in the following subsections.\nA. Identifying hazards\nISO 26262 defines a hazard as \u201ca potential source of harm caused by malfunctioning behaviour of the item where harm is physical injury or damage to the health of persons\u201d [3, Part 1]. The use of ML can create new types of hazards. One type of such hazard is caused by the human operator becoming complacent because they think the automated driver assistance (often using ML) is smarter than it actually is [18]. For example, the driver stops monitoring steering in an automated steering function. On one level, this can be viewed as a case of \u201creasonably forseeable misuse\u201d by the operator, and such misuse is identified in ISO 26262 as requiring mitigation [3, Part 3]. However, this approach may be too simplistic. As ML creates opportunities for increasingly sophisticated driver assistance, the role of the human operator becomes increasingly critical to correct for malfunctions. But increasing automation can create behavioural changes in the operator, reducing their skill level and limiting their ability to respond when needed [19]. Such behavioural impacts can negatively impact safety even though there is no system malfunction or misuse.\nOther new types of hazards are due to the unique ways an ML component can fail. For RL, faults in the reward function can cause surprising failures. An RL-based component may negatively affect the environment in order to achieve its goal [5]. For example, an AV may break laws in order to reach a destination faster. Another possibility is that the RL component games the reward function [5]. For example, the AV figures out that it can avoid getting penalized for driving too close to other cars by exploiting certain sensor vulnerabilities so that it can\u2019t \u201csee\u201d how close it is getting. Although hazards such as these may be unique to ML components, they can be traced to faults, and thus they fit within the existing guidelines of ISO 26262.\nRecommendations for ISO 26262: The definition of hazard should be broadened to include harm potentially caused by complex behavioural interactions between humans and the vehicle that are not due to a system malfunction. The standard itself takes note that the current definition is \u201crestricted to the scope of ISO 26262; a more general definition is potential source of harm\u201d[3, Part 1]. The definition and methods for identifying such hazards should be informed by the research specifically on behavioural impacts of ADAS [20] as well as human-robot interaction (HRI)[21] more broadly. For example, van den Brule et al. [22] study how a robot\u2019s behavioural style can affect the trust of humans interacting with it."}, {"heading": "B. Faults and failure modes", "text": "ISO 26262 mandates the use of analyses such as Fault Mode Effects Analysis (FMEA) to identify how faults lead to failures that may cause harm (i.e., are hazards). We can ask whether there are types of faults and failures that are unique to ML and not found in programmed software. Specific fault types and failure modes have been catalogued for NNs (e.g., [13], [15]). Some of these are just \u201capparent\u201d ML specific faults. For example, a neuron that randomly changes its connection\nin an operational NN is not really about neurons but rather a conventional fault that can occur in the software on which the NN runs. Others are distinctly ML-specific such as faults in the network topology, learning algorithm or training set. This creates the opportunity to develop focused tools and techniques to help find faults independently of the domain for which the ML model is being trained.\nAlthough ML faults have some unique characteristics, this cannot be said about failure modes. All faults can do is to increase the error rate of the deployed component, and thus cause one particular type of failure \u2013 an incorrect output for some input. But since most software failures take the form of incorrect output for a given input, we may conclude that there is nothing different about the failure analysis of an ML component as compared to a programmed component, and existing ISO 26262 recommendations apply.\nRecommendations for ISO 26262: Require the use of fault detection tools and techniques that take into account the unique features of ML. For example, Chakarov et al. [23] describe a technique for debugging mis-classifications due to bad training in data, while Nushi et al. [24] propose an approach for troubleshooting faults due to complex interactions between linked ML components."}, {"heading": "C. The use of training sets", "text": "Spanfelner et al. [1] point out that there is an assumption in ISO 26262, given by the left side of the V model (Fig. 1), that component behaviour is fully specified and each refinement can be verified with respect to its specification. Note that this assumption is also made in other safety-critical domains such as aerospace [25]. This is important to ensure that a safety argument can trace the behaviour of the implementation to its design, safety requirements and ultimately, to the hazards that are mitigated.\nThis assumption is violated when a training set is used in place of a specification since such a set is necessarily incomplete, and it is not clear how to create assurance that the corresponding hazards are always mitigated. Thus, an ML component violates the assumption. Furthermore, the training process is not a verification process since the trained model will be \u201ccorrect by construction\u201d with respect to the training set, up to the limits of the model and the learning algorithm.\nA more careful analysis of the development lifecycle for an ML component shows that there are multiple levels of specification and implementation, some of which may satisfy the assumption in the standard. High-level requirements for the component, although abstract, can be expressed with completeness and traced to up to hazards. For example, the component may be required to \u201cidentify pedestrians\u201d that the AV should avoid harming. Detailed data requirements can be specified carefully to ensure that an appropriate training, validation and testing sets are obtained. Subsequently, the data gathered can be verified with respect to this specification. Completeness is still an issue but coverage can be used as a surrogate, as it is with the design of test sets for software testing.\nA deeper issue, discussed by Spanfelner et al. [1], is that many kinds of advanced functionality require perception of the environment, and this functionality may be inherently unspecifiable. For example, what is the specification for recognizing a pedestrian? We might observe that since a vehicle must move around in a human world, advanced functionality must involve perception of human categories (e.g., pedestrians). There is evidence that such categories can only partially be specified using rules (e.g., necessary and sufficient conditions) and also need examples [26]. This suggests that ML-based approaches may actually be required for implementing this type of functionality.\nRecommendations for ISO 26262: The approach to safe implementation should be geared to the type of functionality being implemented. If the functionality is fully specifiable, then conventional programming can be required. In other cases, such as advanced functionality requiring perception, ML-based approaches should be used, and the complete specification requirement must be relaxed. Partial specifications can be required, where possible. For example, if a pedestrian must be less than 9 feet tall, then this property can be used to filter out false positives. Such properties can be incorporated into the training process or checked on models after training (e.g., [11]).\nTraining set specifications and coverage metrics must be required to improve training set quality. Ensemble methods such as boosting and decision fusion can also be recommended to improve the error rate. However, when the ASIL level is high, it is unlikely that the error rate can ever be brought to an acceptably low level only through increasing or improving the training set (due to the \u201ccurse of dimensionality\u201d). Therefore, fault tolerance strategies for software must be required. For example, redundant pedestrian recognizers using different ML models and training sets can be used to detect potential recognition failures when there is disagreement. Another possibility is to define a \u201csafety envelope\u201d of possible known safe behaviours and limit the ML component to choose among them [27]. Some of these recommendations may be addressed in a forth-coming OMG standard relating to sensor and perception issues [28]."}, {"heading": "D. Level of ML usage", "text": "Fig. 1 identifies an architectural level and a unit (i.e., component) level of implementation. ISO 26262 defines a software architecture as consisting of components and their interactions in a hierarchical structure [3, Part 6]. This component decomposition is important for safety because it allows for easier comprehension of a complex system by human assessors and it permits the use of compositional formal analysis techniques.\nML could be used to implement an entire software system, including its architecture, using an end-to-end approach. For example, Bojarski et al. [29] train a DNN to make the appropriate steering commands directly from raw sensor data, side-stepping typical AV architectural components such as lane detection, path planning, etc.\nHere, we may assume that the unit level, in the conventional sense of a distinct component that can be developed independently of the architecture, no longer exists. This is the case, even if it is possible to extract and interpret the structure of the trained model as consisting of units with distinct functions, since this structure is emergent in the training process and unstable. If the model is re-trained with a slightly different training set, this structure can change arbitrarily. Note that a DNN does have an architecture in a different sense \u2013 the set of layers and their connections. However, since it is the training that actually \u201cimplements\u201d the required functionality, this architecture is more of an generic execution layer. Thus, an end-to-end approach deeply challenges the assumptions underlying ISO 26262.\nAnother challenge with an end-to-end approach is that, in some cases, the the size of the training set needs to be exponentially larger than when a programmed architecture is used [30]. This puts additional strain on the already challenging problem of obtaining an adequate training set for safetycritical contexts.\nFinally, note that issues with an end-to-end approach can also apply when ML is used at the component level, if components are too complex. For example, at one extreme, the architecture can consist of a single component. ISO 26262 specifically guards against this pitfall by mandating the use of modularity principles such as restricting the size of components and maximizing the cohesion within a component. However, the lack of transparency of ML components can hamper the ability to assess component complexity and therefore, to apply these principles. Fortunately, improving ML transparency is an active research area (e.g., [9], [6]).\nRecommendations for ISO 26262: Although using an end-to-end approach has shown some recent successes with autonomous driving (e.g., [29]), we recommend that an endto-end use of ML not be encouraged by ISO 26262, due to its incompatibility with the assumptions about stable hierarchical architectures of components."}, {"heading": "E. Required software techniques", "text": "Part 6 of ISO 26262 deals with product development at the software level and specifies 75 software development techniques, such as shown in Fig. 2, that are used in various phases of the development process in the V model (Fig. 1). Of these, 34 apply at the unit level, and the remaining at the architectural level. We performed an assessment of the software techniques to determine their applicability to ML components1. Based on our recommendation in Sec III-D, we assumed that ML was only used at the unit level and programming is used at the architecture level to connect components.\nThe charts in Fig. 3 show the results of the assessment for the techniques dealing with the unit level. We classified each technique into one of three categories based on the level of applicability to ML. Category Ok means the technique\n1The data is available at https://github.com/rsalay/safetyml\nis directly applicable without modification. Most of these cases are due to the fact that they are black box techniques (e.g., analysis of boundary values, error guessing, etc.) and thus, the method of component implementation is irrelevant. However, some white box techniques such as fault injection also apply. For example, faults can be injected into an NN by breaking links or randomly changing weights (e.g., [31]). Category Adapt says that the technique can be used for an ML component if it is adapted in some way. For example, the technique walk-through can\u2019t be used directly with an NN due to the non-transparency characteristic. Finally, category N/A indicates that the technique is fundamentally code-oriented and does not apply to an ML component. For example, no multiple use of variable names is meaningful for a program but has no corresponding notion in an ML model.\nThe results in Chart (a) are grouped by the degree to which the techniques are recommended. Recall from Sec. II that each technique is marked as highly recommended (++), recommended (+) or no recommendation (o) depending on the ASIL level. The bars in each category show the percentage of techniques that apply when considering all techniques (0,+,++), only the recommended techniques (+, ++), and only the highly recommended techniques (++). Since the degree of recommendation varies by ASIL, each percentage is an average value over all four ASILs with the standard deviation in parentheses. Note that the standard deviation is 0 for the \u201call\u201d group since every technique is present for each ASIL. Because of the high standard deviation for the highly recommended group, we have included Chart (b) which gives the actual data for each ASIL in this group.\nChart (a) shows that a significant part of the standard is still directly applicable (category Ok) and there is an emphasis on highly recommended techniques. However, the standard deviation is high and Chart (b) shows that most of these highly recommended techniques apply to the lower ASIL values \u2013 i.e. they are less relevant from a safety critical perspective. Chart (a) also shows that about 40% of the techniques do not apply at all (category N/A) regardless of the degree of recommendation. In general, techniques in the software part of the standard are clearly biased toward imperative programming languages (e.g., C, Java, etc.) [25]. In addition to precluding ML components, this bias makes it difficult to accept implementations in other mature programming paradigms such as functional programming, logic programming, etc.\nRecommendations for ISO 26262: One approach to addressing the gap in applicable techniques as well as the imperative language bias without compromising safety may be to specify the requirements for techniques based on their intent and maturity rather than on their specific details. For example, the intent of the no multiple use of variable names technique is to reduce the possibility for confusion that may prevent the detection of bugs. This helps humans understand the implementation better and increase their confidence in its correctness and safety. Thus, the standard can require the use of \u201caccepted clarity increasing\u201d techniques instead of the specific techniques."}, {"heading": "IV. SUMMARY AND CONCLUSION", "text": "Machine learning is increasingly seen as an effective software implementation technique for delivering advanced functionality; however, how to assure safety when ML is used in safety critical systems is still an open question. The ISO 26262 standard for functional safety of road vehicles provides a comprehensive set of requirements for assuring safety but does not address the unique characteristics of ML-based software. In this paper, we make a step towards addressing this gap by analyzing the places where ML can impact the standard and providing recommendations on how to accommodate this impact. Our results and recommendations are summarized as follows.\nIdentifying hazards. The use of ML can create new types of hazards that are not due to the malfunctioning of a component. In particular, the complex behavioural interactions possible between humans and advanced functionality implemented by ML can create hazardous situations that should be mitigated within the system design. We recommend that ISO 26262 expands their definition of hazard to address these kinds of situations.\nFault and failure modes. ML components have a development lifecycle that is different from other types of software. Analyzing the stages in the lifecycle reveals distinct types of faults they may have. We recommend that ISO 26262 be extended to explicitly address the ML lifecycle and require the use of fault detection tools and techniques that are customized to this lifecycle.\nThe use of training sets. Because ML components are trained from inherently incomplete data sets, they violate the assumption in V model-based processes that component functionality must be fully specified and that refinements are verifiable. Furthermore, it is possible that certain types of ad-\nvanced functionality (e.g., requiring perception) for which ML is well suited are unspecifiable in principle. As a result, ML components are designed with the knowledge that they have an error rate and that they will periodically fail. Rather than disqualifying this class of functionality, we recommend that ISO 26262 provide different safety requirements depending on whether the functionality is specifiable.\nThe level of ML usage. ML could be used broadly at the architectural level with a system by using an end-to-end approach or remain limited to use at the component level. The end-to-end approach challenges the assumption that a complex system is modeled as a stable hierarchical decomposition of components each with their own function. This limits the use of most techniques for system safety and we therefore recommend that ISO 26262 only allow the use of ML at the component level.\nRequired software techniques. ISO 26262 mandates the use of many specific techniques for various stages of the software development lifecycle. Our analysis shows that while some of these remain applicable to ML components and others could readily be adapted, many remain that are specifically biased toward the assumption that code is implemented using an imperative programming language. In order to remove this bias, we recommend that the requirements be expressed in terms of the intent and maturity of the techniques rather than their specific details."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Atri Sarkar, Michael Smart, Michal Antkiewicz, Marsha Chechik, Sahar Kokaly and Ramy Shahin for their insightful comments."}], "references": [{"title": "and C", "author": ["B. Spanfelner", "D. Richter", "S. Ebel", "U. Wilhelm", "W. Branz"], "venue": "Patz, \u201cChallenges in applying the ISO 26262 for driver assistance systems,\u201d Tagung Fahrerassistenz, M\u00fcnchen, vol. 15, no. 16, p. 2012", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Challenges in autonomous vehicle testing and validation,", "author": ["P. Koopman", "M. Wagner"], "venue": "SAE International Journal of Transportation Safety,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Engineering safety in machine learning,", "author": ["K.R. Varshney"], "venue": "arXiv preprint arXiv:1601.04126,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "and D", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman"], "venue": "Man\u00e9, \u201cConcrete problems in AI safety,\u201d arXiv preprint arXiv:1606.06565", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "and J", "author": ["A.B. Tickle", "R. Andrews", "M. Golea"], "venue": "Diederich, \u201cThe truth will come to light: Directions and challenges in extracting the knowledge embedded within trained artificial neural networks,\u201d IEEE Transactions on Neural Networks, vol. 9, no. 6, pp. 1057\u20131068", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Foundation for neural network verification and validation,", "author": ["G.E. Peterson"], "venue": "International Society for Optics and Photonics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Rodvold, \u201cA software development process model for artificial neural networks in critical applications,", "author": ["M. D"], "venue": "Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "and T", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele"], "venue": "Darrell, \u201cGenerating visual explanations,\u201d in European Conference on Computer Vision. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "and M", "author": ["X. Huang", "M. Kwiatkowska", "S. Wang"], "venue": "Wu, \u201cSafety verification of deep neural networks,\u201d arXiv preprint arXiv:1610.06940", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "and M", "author": ["G. Katz", "C. Barrett", "D. Dill", "K. Julian"], "venue": "Kochenderfer, \u201cReluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,\u201d arXiv preprint arXiv:1702.01135", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "and Y", "author": ["J. Schumann", "P. Gupta"], "venue": "Liu, \u201cApplication of neural networks in high assurance systems: A survey,\u201d in Applications of Neural Networks in High Assurance Systems. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Guidance for the Verification and Validation of Neural Networks", "author": ["L.L. Pullum", "B.J. Taylor", "M.A. Darrah"], "venue": "John Wiley & Sons", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "and J", "author": ["D. Bedford", "G. Morgan"], "venue": "Austin, \u201cRequirements for a standard certifying the use of artificial neural networks in safety critical applications,\u201d in Proceedings of the international conference on artificial neural networks", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "and J", "author": ["Z. Kurd", "T. Kelly"], "venue": "Austin, \u201cDeveloping artificial neural networks for safety critical systems,\u201d Neural Computing and Applications, vol. 16, no. 1, pp. 11\u201319", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Functional Safety of Automated Driving Systems: Does ISO 26262 Meet the Challenges?\u201d in Automated Driving", "author": ["H. Martin", "K. Tschabuschnig", "O. Bridal", "D. Watzenig"], "venue": "Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "and B", "author": ["M. Henzel", "H. Winner"], "venue": "Lattke, \u201cHerausforderungen in der Absicherung von Fahrerassistenzsystemen bei der Benutzung maschinell gelernter und lernenden Algorithmen,\u201d in Proceedings of 11th Workshop Fahrerassistenzsysteme und automatisiertes Fahren (FAS)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Humans and automation: Use", "author": ["R. Parasuraman", "V. Riley"], "venue": "misuse, disuse, abuse,\u201d Human Factors: The Journal of the Human Factors and Ergonomics Society, vol. 39, no. 2, pp. 230\u2013253", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "and W", "author": ["K.A. Brookhuis", "D. De Waard"], "venue": "H. Janssen, \u201cBehavioural impacts of advanced driver assistance systems\u2013an overview,\u201d EJTIR, vol. 1, no. 3, pp. 245\u2013253", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Literature Review of Behavioral Adaptations to Advanced Driver Assistance Systems", "author": ["J.M. Sullivan", "M.J. Flannagan", "A.K. Pradhan", "S. Bao"], "venue": "AAA Foundation for Traffic Safety", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-robot interaction: a survey,", "author": ["M.A. Goodrich", "A.C. Schultz"], "venue": "Foundations and Trends in Human-Computer Interaction,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "and D", "author": ["A. Chakarov", "A. Nori", "S. Rajamani", "S. Sen"], "venue": "Vijaykeerthy, \u201cDebugging machine learning tasks,\u201d arXiv preprint arXiv:1603.07292", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "and D", "author": ["B. Nushi", "E. Kamar", "E. Horvitz"], "venue": "Kossmann, \u201cOn Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems,\u201d arXiv preprint arXiv:1611.08309", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "and E", "author": ["S. Bhattacharyya", "D. Cofer", "D. Musliner", "J. Mueller"], "venue": "Engstrom, \u201cCertification considerations for adaptive systems,\u201d in Unmanned Aircraft Systems (ICUAS), 2015 International Conference on. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing exemplar and rule-based theories of categorization,", "author": ["J.N. Rouder", "R. Ratcliff"], "venue": "Current Directions in Psychological Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Lyapunov design for safe reinforcement learning,", "author": ["T.J. Perkins", "A.G. Barto"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "J", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller"], "venue": "Zhang et al., \u201cEnd to end learning for self-driving cars,\u201d arXiv preprint arXiv:1604.07316", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "On the sample complexity of end-to-end training vs", "author": ["S. Shalev-Shwartz", "A. Shashua"], "venue": "semantic abstraction training,\u201d arXiv preprint arXiv:1604.06915", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "and Y", "author": ["I. Takanami", "M. Sato"], "venue": "P. Yang, \u201cA fault-value injection approach for multiple-weight-fault tolerance of MNNs,\u201d in Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, vol. 3. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "In particular, Advanced Driver Assistance Systems (ADAS) and Autonomous Vehicles (AV) are two areas where ML plays a significant role [1], [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "In particular, Advanced Driver Assistance Systems (ADAS) and Autonomous Vehicles (AV) are two areas where ML plays a significant role [1], [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "Recently, the safety of ML approaches in general have been analyzed both from theoretical [4] and pragmatic perspectives [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Recently, the safety of ML approaches in general have been analyzed both from theoretical [4] and pragmatic perspectives [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Work on supporting the verification & validation (V&V) of NNs emerged in the 1990\u2019s with a focus on making their internal structure easier to assess by extracting representations that are more understandable [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "General V&V methodologies for NNs have also been proposed [7], [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "General V&V methodologies for NNs have also been proposed [7], [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "More recently, with the popularity of deep neural networks (DNN), verification research has included more diverse topics such as generating explanations of DNN predictions [9], improving the stability of classification [10] and property checking of DNNs [11].", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "More recently, with the popularity of deep neural networks (DNN), verification research has included more diverse topics such as generating explanations of DNN predictions [9], improving the stability of classification [10] and property checking of DNNs [11].", "startOffset": 219, "endOffset": 223}, {"referenceID": 9, "context": "More recently, with the popularity of deep neural networks (DNN), verification research has included more diverse topics such as generating explanations of DNN predictions [9], improving the stability of classification [10] and property checking of DNNs [11].", "startOffset": 254, "endOffset": 258}, {"referenceID": 10, "context": "Despite their challenges, NNs are already used in high assurance systems (see [12] for a survey), and safety certification of NNs has received some attention.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "[13] give detailed guidance on V&V as well as other aspects of safety assessment such as hazard analysis with a focus on adaptive", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] define general requirements for addressing NNs in any safety standard.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] have established criteria for NNs to use in a safety case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Koopman and Wagner [2] identify some of the key challenges to certification, including ML.", "startOffset": 19, "endOffset": 22}, {"referenceID": 14, "context": "[16] analyze the adequacy of ISO 26262 for AV but focus on the impact of the increased complexity it creates rather than specifically the use of ML.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] assess ISO 26262 from the perspective of driver assistance systems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": ", [6], [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [6], [9]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "Increasing ML model expressive power is typically at the expense of transparency but some research efforts focus on mitigating this [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Finally, even if the estimate of the true error rate was accurate, it may not reflect the error rate the system actually experiences while in operation after a finite set of inputs because the true error is based on an infinite set of samples [4].", "startOffset": 243, "endOffset": 246}, {"referenceID": 2, "context": "Another factor is that, even if the training set is representative, it may under-represent the safety-critical cases because these are often rarer in the input space [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 16, "context": "One type of such hazard is caused by the human operator becoming complacent because they think the automated driver assistance (often using ML) is smarter than it actually is [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "But increasing automation can create behavioural changes in the operator, reducing their skill level and limiting their ability to respond when needed [19].", "startOffset": 151, "endOffset": 155}, {"referenceID": 3, "context": "An RL-based component may negatively affect the environment in order to achieve its goal [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Another possibility is that the RL component games the reward function [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "The definition and methods for identifying such hazards should be informed by the research specifically on behavioural impacts of ADAS [20] as well as human-robot interaction (HRI)[21] more broadly.", "startOffset": 135, "endOffset": 139}, {"referenceID": 19, "context": "The definition and methods for identifying such hazards should be informed by the research specifically on behavioural impacts of ADAS [20] as well as human-robot interaction (HRI)[21] more broadly.", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": ", [13], [15]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": ", [13], [15]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "[23] describe a technique for debugging mis-classifications due to bad training in data, while Nushi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] propose an approach for troubleshooting faults due to complex interactions between linked ML components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] point out that there is an assumption in ISO 26262, given by the left side of the V model (Fig.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Note that this assumption is also made in other safety-critical domains such as aerospace [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "[1], is that", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": ", necessary and sufficient conditions) and also need examples [26].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": ", [11]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "Another possibility is to define a \u201csafety envelope\u201d of possible known safe behaviours and limit the ML component to choose among them [27].", "startOffset": 135, "endOffset": 139}, {"referenceID": 25, "context": "[29] train a DNN to make the", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Another challenge with an end-to-end approach is that, in some cases, the the size of the training set needs to be exponentially larger than when a programmed architecture is used [30].", "startOffset": 180, "endOffset": 184}, {"referenceID": 7, "context": ", [9], [6]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [9], [6]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 25, "context": ", [29]), we recommend that an endto-end use of ML not be encouraged by ISO 26262, due to its incompatibility with the assumptions about stable hierarchical architectures of components.", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [31]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": ") [25].", "startOffset": 2, "endOffset": 6}], "year": 2017, "abstractText": "Machine learning (ML) plays an ever-increasing role in advanced automotive functionality for driver assistance and autonomous operation; however, its adequacy from the perspective of safety certification remains controversial. In this paper, we analyze the impacts that the use of ML as an implementation approach has on ISO 26262 safety lifecycle and ask what could be done to address them. We then provide a set of recommendations on how to adapt the standard to accommodate ML.", "creator": "LaTeX with hyperref package"}}}