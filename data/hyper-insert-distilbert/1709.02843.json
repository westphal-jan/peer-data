{"id": "1709.02843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "CLaC at SemEval-2016 Task 11: Exploring linguistic and psycho-linguistic Features for Complex Word Identification", "abstract": "under this paper describes implementing the system deployed immediately by the ngo clac - edlk team 2015 to comprise the \" general semeval, 2016, complex symbolic word identification task \". and the goal of the task is to identify if a dialogue given one word in a given context is \" equally simple \" or \" complex \". our system relies mostly on linguistic intelligence features and cognitive model complexity. we got used already several professionally supervised descriptive models, however occasionally the true random forest forest model actually outperformed the others. overall our best configuration students achieved a g - score + of 68. 23 8 % achieved in the numerical task, successfully ranking our system 21 22nd out of 45.", "histories": [["v1", "Fri, 8 Sep 2017 19:34:02 GMT  (17kb)", "http://arxiv.org/abs/1709.02843v1", "In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2016), a workshop of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2016) pp 982-985. June 16-17, San Diego, California"]], "COMMENTS": "In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2016), a workshop of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2016) pp 982-985. June 16-17, San Diego, California", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["elnaz davoodi", "leila kosseim"], "accepted": false, "id": "1709.02843"}, "pdf": {"name": "1709.02843.pdf", "metadata": {"source": "CRF", "title": "CLaC at SemEval-2016 Task 11: Exploring linguistic and psycho-linguistic Features for Complex Word Identification", "authors": ["Elnaz Davoodi"], "emails": ["kosseim}@encs.concordia.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n02 84\n3v 1\n[ cs\n.C L\n] 8\nS ep\nThis paper describes the system deployed by the CLaC-EDLK team to the SemEval 2016, Complex Word Identification task. The goal of the task is to identify if a given word in a given context is simple or complex. Our system relies on linguistic features and cognitive complexity. We used several supervised models, however the Random Forest model outperformed the others. Overall our best configuration achieved a G-score of 68.8% in the task, ranking our system 21 out of 45."}, {"heading": "1 Introduction", "text": "Text simplification involves reducing the complexity of a text in order to make it more accessible to a larger audience or to a specific audience such as children, second language learners, the elderly, etc.\nResearch efforts on text simplification have mostly focused on either lexical (Devlin and Tait, 1998; Carroll et al., 1998; Biran et al., 2011) or syntactic simplification (Chandrasekar and Srinivas, 1997; Siddharthan, 2006; Kauchak, 2013). Lexical simplification involves replacing specific words in order to reduce lexical complexity, while still conveying the same information. Lexical simplification is still a challenging task as identifying and simplifying complex words in a given context is not straightforward. Complex word identification is the first step towards lexical simplification.\nFor the challenge, we experimented with a number of features that we suspected would be correlated with a word\u2019s complexity level and selected five that\nwere the most discriminating on the given training data set. Using a Random Forest classifier, we built two models that we deployed to the Sem-Eval 2016 Complex Word Identification Task. These models achieved better than average ranking compared to the other participating systems. We believe that the approach proposed in this paper can link linguistic features and cognitive complexity together in order to improve lexical simplification."}, {"heading": "2 Complex Word Identification Task Setup", "text": "The SemEval 2016 Complex Word Identification Task required participating systems to automatically classify words as simple or complex in a given context. For training, participants were given 2247 instances. Each instance consisted of a target word W , its offset in the sentence and a class label (0 if W was considered simple in the sentence, or 1 if W was complex in the sentence). For example, given the training instance 1 below, the target word happened is classified as simple; however in training instance 2, the same word is classified as complex.\nTraining instance 1: There are several stories about Mozart \u2019s final illness and death , and it is not easy to be sure what happened . happened 21 0\nTraining instance 2: Although anoxic events have not happened for millions of years , the geological record shows that they happened many times in the past . happened 5 1\nGiven the above, the task was to identify if a word is complex or simple in a test set of 88,221 instances."}, {"heading": "3 Methodology", "text": "We experimented with various supervised models for this binary classification task; however Random Forest outperformed the others. We also experimented with various features and retained the five features described below."}, {"heading": "3.1 Feature Set", "text": "In our final complex word identification system, we used as features: the frequency of the target word in the Web1T Google N-gram Corpus, its Part of Speech (POS) tag, the number of synonyms it has in WordNet, the inverse of its length and a psycholinguistic feature indicating its \u201cabstract\u201d level. These features were selected based on their discriminating power on the training data."}, {"heading": "3.1.1 Frequency in the Web1T Google N-gram Corpus", "text": "The Google N-gram corpus (Michel et al., 2011) is a collection of English one- to five-grams with their frequencies from different sources and from different years. This corpus contains approximately 1 trillion words from the Web. In order to focus on the more recent usage of the words and reduce the size of the corpus, we considered the frequency of the target words in sources which were indexed after year 2000. This way, we reduced the influence of frequent but obsolete words. We used this feature based on the assumption that simpler words are more frequently used."}, {"heading": "3.1.2 Part of Speech Tag", "text": "We suspected that a word\u2019s POS tag may influence its complexity level. As a word may be tagged with different parts-of-speech, we suspected that a particular usage may be more complex than another depending on how common that usage is. For example, the word highlight may be used as a noun or, less frequently, as a verb. Hence, highlight as noun may be more likely to be considered simple and highlight as verb may be more likely to be considered complex. As a feature, we used the POS tags of target words given in the training and test data sets. The words in each sentence are tagged using Stanford POS tagger (Toutanova et al., 2003)."}, {"heading": "3.1.3 Number of Synonyms in WordNet", "text": "Another linguistic feature that we suspected may be correlated with the complexity of a word is the number of synonyms it has. Based on our statistical analysis of the training set, complex words have fewer synonyms than simpler words. For instance, the probability that a complex word has less than 4 synonyms is 33.65% on the training set; while this number is 24.10% for simple words. This can be explained by the fact that complex words tend to denote specific entities or concepts and therefore tend to have less synonyms. Thus, we considered the number of synonyms of the target word as one of our features."}, {"heading": "3.1.4 Inverse of Word Length", "text": "Based on the work of traditional text complexity measures such as the Flesch index (Kincaid et al., 1975), we took into account the length of a word as a feature to determine its simplicity level."}, {"heading": "3.1.5 Psycholinguistic Feature", "text": "We suspected that the more abstract a word, the more complex it will be perceived. To investigate the correlation between the degree of abstractness of a word and its complexity, we used the MRC psycholinguistic database (Wilson, 1988). This electronic resource contains the score of 26 psycholinguistic features for 150,834 words. One such feature indicates the level of abstraction associated with the entity or concept denoted by the word. This concreteness feature is available for 8,228 words and is indicated by an integer value ranging from 100 (very abstract) to 700 (very concrete). For this psycholinguistic feature, if the target word had a concreteness value in the MRC, we used its value as a feature. However, using this feature has a drawback since it does not cover all the words. For now, we deal with this problem by considering the value of outof-database words as 0."}, {"heading": "3.2 Feature Selection", "text": "In order to evaluate the effectiveness of these five features, we used two feature selection methods: (1) information gain (a filter-based method) and (2) subset selection (a wrapper method). Table 1 shows the features ranked using information gain. For exam-\nple, WordNet synonyms are the most discriminating features; whereas word length is the least.\nOn the other hand, using the subset selection method, the best subset of features was determined to be the frequency in the Web1T Google N-gram Corpus + the number of synonyms in WordNet + the psycholinguistic feature. It is interesting to note that both methods identify the psycholinguistic feature and the number of synonyms inWordNet as two of the most discriminating features; and word length as less useful."}, {"heading": "4 Results and Discussion", "text": "We experimented with various learning models trained on the features described above. As a baseline, we used a Na\u0131\u0308ve Bayes classifier. Table 2 shows the weighted average precision, recall and F-measure on the training set evaluated using 10 fold cross-validation. As can be seen in Table 2, all learning models perform significantly better in classifying simple words rather than complex words. Based on the weighted average of the learning models, we submitted two Random Forest models: CLaC-EDLK-RF 0.6 and CLaC-EDLK-\nRF 0.5. The only difference between these submissions is the threshold for class assignment. In the first submission, we used the threshold of 0.5, and in the second we used 0.6. Our official ranking at the shared task ranked our CLaC-EDLK-RF 0.6 as 21st and CLaC-EDLK-RF 0.5 as 24th out of 45 systems using the G-score. However, based on the Fscore, CLaC-EDLK-RF 0.5 ranked 26th and CLaCEDLK-RF 0.6 ranked 30th."}, {"heading": "5 Future Work", "text": "For future work, we plan to investigate the use of other linguistic and psycholinguistic features. Because the MRC database contained a concreteness score for only 8,228 words, we plan to combine more psycholinguistic features from the same database. In addition, it would be interesting to investigate the influence of context on the readers\u2019 understanding of a target word. To do this, we could examine a window-based approach to consider the surrounding words which may affect the complexity of a word."}, {"heading": "Acknowledgement", "text": "The authors would like to thank the anonymous reviewers for their feedback on the paper. This work was financially supported by NSERC."}], "references": [{"title": "Putting it simply: A context-aware approach to lexical simplification", "author": ["Biran et al.2011] Or Biran", "Samuel Brody", "No\u00e9mie Elhadad"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:", "citeRegEx": "Biran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biran et al\\.", "year": 2011}, {"title": "Practical simplification of English newspaper text to assist aphasic readers", "author": ["Carroll et al.1998] John Carroll", "Guido Minnen", "Yvonne Canning", "Siobhan Devlin", "John Tait"], "venue": "In Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assis-", "citeRegEx": "Carroll et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Carroll et al\\.", "year": 1998}, {"title": "Automatic induction of rules for text simplification", "author": ["Chandrasekar", "Srinivas1997] Raman Chandrasekar", "Bangalore Srinivas"], "venue": "Knowledge-Based Systems,", "citeRegEx": "Chandrasekar et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chandrasekar et al\\.", "year": 1997}, {"title": "The use of a psycholinguistic database in the simplification of text for aphasic readers", "author": ["Devlin", "Tait1998] Siobhan Devlin", "John Tait"], "venue": "Linguistic Databases,", "citeRegEx": "Devlin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 1998}, {"title": "Improving text simplification language modeling using unsimplified text data", "author": ["David Kauchak"], "venue": "In Proceeding of ACL (Volume 1: Long Papers),", "citeRegEx": "Kauchak.,? \\Q2013\\E", "shortCiteRegEx": "Kauchak.", "year": 2013}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["Robert P Fishburne Jr.", "Richard L Rogers", "Brad S Chissom"], "venue": null, "citeRegEx": "Kincaid et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Quantitative analysis of culture using millions of digitized", "author": ["Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant"], "venue": null, "citeRegEx": "Michel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Michel et al\\.", "year": 2011}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Associ-", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "MRC psycholinguistic database: Machine-usable", "author": ["Michael Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q1988\\E", "shortCiteRegEx": "Wilson.", "year": 1988}], "referenceMentions": [{"referenceID": 1, "context": "Research efforts on text simplification have mostly focused on either lexical (Devlin and Tait, 1998; Carroll et al., 1998; Biran et al., 2011) or syntactic simplification (Chandrasekar and Srinivas, 1997;", "startOffset": 78, "endOffset": 143}, {"referenceID": 0, "context": "Research efforts on text simplification have mostly focused on either lexical (Devlin and Tait, 1998; Carroll et al., 1998; Biran et al., 2011) or syntactic simplification (Chandrasekar and Srinivas, 1997;", "startOffset": 78, "endOffset": 143}, {"referenceID": 6, "context": "The Google N-gram corpus (Michel et al., 2011) is a collection of English one- to five-grams with their frequencies from different sources and from different years.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "The words in each sentence are tagged using Stanford POS tagger (Toutanova et al., 2003).", "startOffset": 64, "endOffset": 88}, {"referenceID": 5, "context": "Based on the work of traditional text complexity measures such as the Flesch index (Kincaid et al., 1975), we took into account the length of a word as a feature to determine its simplicity level.", "startOffset": 83, "endOffset": 105}, {"referenceID": 8, "context": "To investigate the correlation between the degree of abstractness of a word and its complexity, we used the MRC psycholinguistic database (Wilson, 1988).", "startOffset": 138, "endOffset": 152}], "year": 2017, "abstractText": "This paper describes the system deployed by the CLaC-EDLK team to the SemEval 2016, Complex Word Identification task. The goal of the task is to identify if a given word in a given context is simple or complex. Our system relies on linguistic features and cognitive complexity. We used several supervised models, however the Random Forest model outperformed the others. Overall our best configuration achieved a G-score of 68.8% in the task, ranking our system 21 out of 45.", "creator": "LaTeX with hyperref package"}}}