{"id": "1709.02980", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2017", "title": "RDeepSense: Reliable Deep Mobile Computing Models with Uncertainty Estimations", "abstract": "incorporating recent data advances in deep learning have led various applications to unprecedented achievements, which could potentially bring higher intelligence to virtually a broad spectrum assortment of sensitive mobile instruments and ubiquitous applications. recent although few existing studies further have demonstrated rapidly the effectiveness and feasibility risks of already running deep knowledge neural network stability inference operations on mobile components and embedded devices, as they overlooked previously the reliability of reliable mobile computing models. reliability related measurements \u2014 such as introducing predictive uncertainty interval estimations that are key factors for improving reliability the dynamic decision capture accuracy system and user experience. in \" this work, \u2014 we propose rdeepsense, the first deep learning programming model that essentially provides comprehensive well - reliability calibrated generalized uncertainty estimations for multiple resource - constrained mobile processors and embedded devices. applying rdeepsense significantly enables exploring the artificial predictive uncertainty stability by adopting introducing a tunable automated proper scoring rule as the training criterion and weighted dropout as the efficient implicit bayesian approximation, although which theoretically proves significantly its behavioral correctness. to systematically reduce the raw computational complexity, improving rdeepsense employs efficient dropout and predictive distribution estimation instead of static model spectral ensemble arithmetic or sampling - based method for inference operations. \u201c we recently evaluate rdeepsense with four native mobile sensing applications extensively using ubiquitous intel at edison instrumentation devices. results collectively show that designing rdeepsense can reduce around 90 % flexibility of delivering the activation energy radiated consumption noise while producing environmentally superior uncertainty estimations and presumably preserving at least the same computational model accuracy compared with 10 other state - of - the - art sensor methods.", "histories": [["v1", "Sat, 9 Sep 2017 16:58:01 GMT  (2787kb,D)", "http://arxiv.org/abs/1709.02980v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NI", "authors": ["shuochao yao", "yiran zhao", "huajie shao", "aston zhang", "chao zhang", "shen li", "tarek abdelzaher"], "accepted": false, "id": "1709.02980"}, "pdf": {"name": "1709.02980.pdf", "metadata": {"source": "META", "title": "RDeepSense: Reliable Deep Mobile Computing Models with Uncertainty Estimations", "authors": ["SHUOCHAO YAO", "Urbana Champaign", "YIRAN ZHAO", "HUAJIE SHAO", "ASTON ZHANG", "CHAO ZHANG", "Shuochao Yao", "Yiran Zhao", "Huajie Shao"], "emails": [], "sections": [{"heading": null, "text": "1 RDeepSense: Reliable Deep Mobile Computing Models with Uncertainty Estimations\nSHUOCHAO YAO, University of Illinois Urbana Champaign YIRAN ZHAO, University of Illinois Urbana Champaign HUAJIE SHAO, University of Illinois Urbana Champaign ASTON ZHANG, University of Illinois Urbana Champaign CHAO ZHANG, University of Illinois Urbana Champaign SHEN LI, IBM Research TAREK ABDELZAHER, University of Illinois Urbana Champaign\nRecent advances in deep learning have led various applications to unprecedented achievements, which could potentially bring higher intelligence to a broad spectrum of mobile and ubiquitous applications. Although existing studies have demonstrated the e ectiveness and feasibility of running deep neural network inference operations on mobile and embedded devices, they overlooked the reliability of mobile computing models. Reliability measurements such as predictive uncertainty estimations are key factors for improving the decision accuracy and user experience. In this work, we propose RDeepSense, the rst deep learning model that provides well-calibrated uncertainty estimations for resource-constrained mobile and embedded devices. RDeepSense enables the predictive uncertainty by adopting a tunable proper scoring rule as the training criterion and dropout as the implicit Bayesian approximation, which theoretically proves its correctness. To reduce the computational complexity, RDeepSense employs e cient dropout and predictive distribution estimation instead of model ensemble or sampling-based method for inference operations. We evaluate RDeepSense with four mobile sensing applications using Intel Edison devices. Results show that RDeepSense can reduce around 90% of the energy consumption while producing superior uncertainty estimations and preserving at least the same model accuracy compared with other state-of-the-art methods.\nACM Reference format: Shuochao Yao, Yiran Zhao, Huajie Shao, Aston Zhang, Chao Zhang, Shen Li, and Tarek Abdelzaher. 2016. RDeepSense: Reliable Deep Mobile Computing Models with Uncertainty Estimations. 1, 1, Article 1 (January 2016), 25 pages. DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "Using embedded sensors to infer the surrounding physical states and context is one of the major tasks of mobile and ubiquitous computing. Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60]. An important component in these applications is a learning model that outputs target values given sensor inputs.\nRapid advancement in deep learning techniques has tempted researchers to employ deep neural networks as the learning models in the mobile applications. ese highly capable models are good at making sophisticated mappings between unstructured data such as sensor inputs and target quantities, which can hardly be achieved\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2016 ACM. XXXX-XXXX/2016/1-ART1 $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nar X\niv :1\n70 9.\n02 98\n0v 1\n[ cs\n.L G\n] 9\nS ep\n2 01\n7\nby traditional machine learning models. Speci c deep learning models have been designed to fuse multiple sensory modalities and extract temporal relationships along sensor inputs. ese speci cally designed models have shown signi cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi cation tasks [55].\nHowever, the inability of treating the deep learning model more than just an incomprehensible black box has become an important factor that hinders researchers from applying the model to mobile applications. e complexity and uninterpretability of such models mainly result from the deep and non-linear structures [37]. erefore researchers can hardly understand how deep neural networks derive their nal predictions. is leads to either the loss of trust in deep learning models or blind faith in deep learning models without being aware of predictive uncertainties and error bound.\nIn order to explicitly output the reliability measure of deep neural network model, we aim to provide the model with predictive uncertainties during inference. Predictive uncertainty is de ned as the probability of occurrence of the target variable conditioned on all available information. One particular approach to express predictive uncertainty is to treat the model predictions as random variables, i.e., in the form of probability distributions instead of point estimations [43]. In this paper, we center our discussion around this speci c representation of the uncertainty.\nOn one hand, although it is hard to directly interpret deep neural networks, predictive uncertainty can provide the quantitative con dence of prediction correctness, which boosts trust and faith in deep learning models. On the other hand, uncertainty estimation itself is crucial for scienti c measurements [19, 33]. Extensive investigations show that measurement uncertainties can impact user experiences [30, 36]. In order to monitor the uncertainties of mobile sensing applications, the rst important step is to obtain the predictive uncertainties of learning models used in the applications, which, in our case, are deep neural network models.\nHowever, enabling deep neural networks to provide high-quality and well-calibrated uncertainty estimations on mobile and embedded devices poses two major challenges. One challenge is to provide a mathematically grounded uncertainty estimations that require few changes on either the model or the optimization method. Although mathematically grounded methods such as Bayesian approaches serves as powerful tools to estimate predictive uncertainties [11], Bayesian neural networks are computationally expensive to train and inference even for brawny servers, let alone mobile and embedded devices [43]. erefore a mathematically grounded theory under minimal model modi cation requirements is a must for reliable uncertainty estimations. e other challenge is to reduce the computational burden of uncertainty estimations during inference. For mobile and ubiquitous computing applications, although we can train the deep neural networks on brawny servers with powerful GPUs, running inference on mobile and embedded devices is di cult due to limited energy supplies and computational resources on such devices [56]. Illuminating studies from the machine learning community try to provide mathematically grounded uncertainty estimations for deep neural networks, but these methods are based either on the sampling method [17] or the ensemble method [34]. ey require either running a single stochastic neural network for multiple times or training and running multiple deterministic neural networks. All these solutions are not resource-friendly to mobile and embedded devices. erefore, mobile applications call for a novel solution that theoretically guarantees the correctness of predictive uncertainties, and at the same time consumes much less resource.\nIn this work, we propose RDeepSense that enables predictive uncertainties with theoretically proven correctness for mobile and ubiquitous applications. RDeepSense signi cantly reduces the computational overhead and preserves at least the same model accuracy. To the best of our knowledge, this is the rst deep learning model that provides uncertainty estimations for resource-limited devices. e core of RDeepSense is the integration of the dropout training method that interprets neural networks as Gaussian process (GP) through Bayesian approximation [17, 45, 46] and proper scoring rules as training criterion that measure the quality of predictive uncertainty such as log-likelihood and the Brier score [20]. eir integration can be further interpreted as the\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nmixture distribution of a Gaussian or categorical distribution based on latent deep Gaussian process and a deep Gaussian process through Bayesian approximation. Firstly, RDeepSense uses a tunable proper scoring rule as the training criterion that signi cantly mitigates the problem of underestimating predictive uncertainties in deep neural networks [17]. Secondly, since dropout training can be interpreted as \u201cgeometric averaging\u201d over the ensemble of possible \u201cthinned\u201d subnetworks [2], RDeepSense applies dropout training instead of model ensemble. It greatly reduces the computation complexity of the nal neural network compared with model ensemble. erefore, our integrated method incurs only li le computational overhead, which makes it feasible on embedded devices for mobile applications.\nEvaluations of RDeepSense use the Intel Edison computing platform [1]. We conduct mobile and ubiquitous tasks that focus on human health and wellbeing, smart city transportation, environment monitoring, and activity recognition. Speci cally, our experiments include: 1) monitoring arterial blood pressure through photoplethysmogram (PPG) from ngertip [27], 2) NY city taxi commute time estimation [50], 3) gas mixture concentrations estimation through the chemical sensor array [15], 4) and heterogeneous human activity recognition through motion sensors [47].\nWe compare RDeepSense with the state-of-the-art Monte Carlo dropout method [17], ensemble method [34], and Gaussian process. e resource consumption of Intel Edison module and nal model performance such as the accuracy and the quality of uncertainty estimations are measured for all the algorithms. RDeepSense can reduce more than 90% of inference time and energy consumption, while obtaining the uncertainty estimations with be er quality compared with the other algorithms. e well-calibrated uncertainty estimations and resource e ciency make RDeepSense the rst choice to obtain uncertainty estimations of deep neural networks in mobile applications.\nIn summary, we propose a simple yet e ective and theoretically-grounded method, RDeepSense, which empowers neural networks with well-calibrated predictive uncertainty estimations. RDeepSense is also a resource-friendly algorithm for mobile and embedded devices that adds almost no computational overhead during model inference. e rest of paper is organized as follows. Section 2 introduces related works about uncertainty estimations and deep neural networks. We describe the technical details of RDeepSense in Section 3. e evaluation is presented in Section 4. Finally, we discuss the results in Section 5 and conclude in Section 6."}, {"heading": "2 RELATED WORK", "text": "On one hand, reliability and uncertainty estimation is one important issue of mobile and ubiquitous computing. A lot of works have been proposed to utilize uncertainty estimations for improving the decision accuracy and user experience. Baumann et al. [4] make next-place predictions based on the uncertainty estimation of classi ers. Kay et al. [29] propose a novel discrete representation of uncertainties for visualizing and user interaction. Boukhelifa et al. [7] propose design considerations for uncertainty-aware data analytics. On the other hand, the recent advances in deep learning techniques have motivated people to apply deep neural networks for solving mobile and ubiquitous computing tasks. Lane et al. [35] apply deep neural networks to solve audio sensing tasks. Castro et al. [13] predict daily activities from egocentric images using deep learning. Yao et al. [55] propose a deep learning structure that fuses multiple sensor inputs and extracts time dependencies. Guan et al. [25] apply ensembles of LSTM for activity recognition. However, uncertainty estimations of deep neural networks for mobile and ubiquitous computing tasks is an important topic that draws less a ention.\nRecently there are some illuminating works from the machine learning community that tries to provide deep neural networks with uncertainty estimations. Gal et al. [17] provide the rst theoretical proof of the linkage between dropout training with deep Gaussian process called MCDrop. However, the proposed method tends to underestimate the uncertainty due to the nature of variational inference. Lakshminarayanan et al. [34] propose\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\na solution SSP based on proper scoring rules and ensemble methods. However, the proposed method tends to overestimate the uncertainty on real datasets.\nSince these previous works do not consider the scenario of mobile and ubiquitous computing, all these proposed methods require the operations with high computational cost during model inference, i.e., sampling methods or ensemble methods. ese computationally intensive operations aggravate the time and energy consumption problems in the embedded devices, which is one of the key issues of mobile and ubiquitous computing [22, 40, 41].\nTo the best of our knowledge, RDeepSense is the rst work that provides a simple yet e ective solution to estimate the uncertainties of deep neural networks for mobile and ubiquitous computing applications. RDeepSense uses proper scoring rules to mitigate the underestimation e ect of MCDrop, and applies dropout training as implicit ensemble to avoid the computationally intensive ensemble method used in SSP.\nIn order to further illustrate the main di erence between RDeepSense and other two deep leaning uncertainty estimation algorithms, MCDrop and SSP, we show the designing components of these three algorithms in Table 1."}, {"heading": "3 RDEEPSENSE FRAMEWORK", "text": "is section elaborates on the technical details of the RDeepSense framework in three constituents. Section 3.1 introduces a simple yet e ective recipe to build a fully-connected neural network with predictive uncertainty estimations. In Section 3.2, we introduce preliminary knowledge and make the theoretical analysis of RDeepSense. We prove that RDeepSense is a mathematically grounded method to obtain predictive uncertainty estimations. In Section 3.3, we introduce an e ective and e cient approximation for RDeepSense to obtain predictive uncertainty estimations while running on the resource-constrained embedded devices.\nFor the rest of this paper, all vectors are denoted by bold lower-case le ers (e.g., x and y), and matrices and tensors are represented by bold upper-case le ers (e.g., X and Y). For a column vector x, the jth element is denoted by x[j]. For a tensor X, the t th matrix along the third axis is denoted by X\u00b7 \u00b7t , and the other slicing notations are de ned similarly. e superscript l in x(l ) and X(l ) denote the vector and tensor for the l th layer of the neural network. We use calligraphic le ers to denote sets (e.g., X and Y), where |X| denotes the cardinality of X."}, {"heading": "3.1 RDeepSense components", "text": "RDeepSense is a simple and e ective method that empowers fully-connected neural networks to output predictive uncertainty estimations. ere are only two steps to convert an arbitrary fully-connected neural networks into a neural network with uncertainty estimations:\n(1) Insert dropout operation to each fully-connected layer. (2) Adopt a proper scoring rule as the loss function, and emit a distribution estimation instead of a point\nestimation at the output layer. e following two subsections describe dropout training and proper scoring rules in detail. 3.1.1 Dropout training. Fully-connected neural networks can be formulated using the following equations:\ny(l ) = x(l )W(l ) + b(l ), x(l+1) = f (l ) ( y(l ) ) ,\n(1)\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nwhere the notation l = 1, \u00b7 \u00b7 \u00b7 ,L is the layer index in the fully-connected neural network. For any layer l , the weight matrix is denoted as W(l ) \u2208 Rd (l\u22121)\u00d7d (l ) ; the bias vector is denoted as b(l ) \u2208 Rd (l ) ; the input is denoted as x(l ) \u2208 Rd (l\u22121) ; and d (l ) is the dimension of the l th layer. In addition, f (l )(\u00b7) is a nonlinear activation function.\nHowever, such formulations could run into feature co-adapting and model over ing problems. To avoid these problems, researchers introduce the concept of dropout as a regularization method [46]. \u201cDropout\u201d originally refers to dropping out hidden and visible units in a neural network, which is mathematically equivalent to ignoring rows of the weight matrix W(l ). erefore, a fully-connected neural network with dropout can be represented as follows:\nz(l )[i] \u223c Bernoulli(p (l ) [i]), W\u0303(l ) = diag ( z(l ) ) W(l ),\ny(l ) = x(l )W\u0303(l ) + b(l ), x(l+1) = f (l ) ( y(l ) ) .\n(2)\nAs shown in (2), a vector of Bernoulli variables z(l ) \u2208 {0, 1}d (l\u22121) forms a diagonal matrix which acts as a mask to dropout the ith row of W\u0303(l ) with probability p(l )[i]. Intuitively, the dropout operations (2) convert a traditional (deterministic) neural network with parameters {W(l )} into a random Bayesian neural network with random variables {W\u0303(l )}, which equates a neural network with a statistical model without using the Bayesian approach explicitly. is conversion with dropout helps us to obtain predictive uncertainty estimations and avoid the computationally intensive operations used in Bayesian approaches. e detailed analysis about the equivalence will be discussed later.\n3.1.2 Proper scoring rules. Optimizing a deep neural network requires minimizing the loss function. erefore the loss function plays a crucial role in designing an e ective neural network. Many commonly used neural network loss functions are proper scoring rules, such as logistic loss and hinge loss.\nScoring rules, also known as score functions, measure the quality of predictive uncertainties [20]. Assume that p\u03b8 (y |x) is the probabilistic distribution represented by a deep neural network. e scoring rule S(p\u03b8 (y |x), (x,y)) assigns a numerical score for the quality of predictive distribution p\u03b8 (y |x) on event (x,y) \u223c q(x,y), where q(x,y) is the true distribution of data samples. e expected scoring rule is formulated as\nS(p\u03b8 (y |x),q(x,y)) = \u222b q(x,y)S(p\u03b8 (y |x), (x,y))dxdy. (3)\nFor a proper scoring rule, the equality in S(p\u03b8 (y |x),q(x,y)) \u2265 S(q(x,y)),q(x,y)) holds if and only if p\u03b8 (y |x) = q(x,y). Widely-adopted proper scoring rules include Log-likelihood logp\u03b8 (y |x) and Brier score \u2212 \u2211K k=1(1k (y) \u2212 p\u03b8 (y = k |x))2. RDeepSense employs a tunable function, the weighted sum of negative log-likelihood and mean square error (Brier score for classi cation problems), which is a proper scoring rule, as the loss functions for both regression and classi cation problems. is loss function tries to o set the e ect of overestimation and underestimation caused by negative log-likelihood and mean square error respectively, which will be analyzed and evaluated later.\nFor regression problems, in order to optimize the neural network with negative log-likelihood, we have to emit a distribution estimation instead of a point estimation at the output layer. erefore, we slightly change the structures of neural networks. e last output layer generates both the predictive mean \u00b5(y\u0302) and the predictive variance \u03c3 2(y\u0302). According to the notation in (2), the output layer is represented by xL+1 = [ \u00b5(y\u0302), \u03c3 2(y\u0302) ]\u1d40 =[\ny(L)[0] , so plus(y (L) [1] ) ]\u1d40 , where so plus function is log(1 + exp(\u00b7)) enforcing the positivity constraint on the , Vol. 1, No. 1, Article 1. Publication date: January 2016.\nvariance. Predictive mean \u00b5(y\u0302) and predictive variance \u03c3 2(y\u0302) compose a Gaussian distribution N(\u00b5(y\u0302),\u03c3 2(y\u0302)) as the output predictive distribution of the neural network. en the nal loss function of a regression problem, Lr , is the weighted sum of mean square error Lr e and negative log-likelihood Lr l ,\nLr e = N\u2211 n=1 ( y \u2212 \u00b5(y\u0302) )2 + \u03bbe L\u2211 l=1 \u2016W(l )\u201622 ,\nLr l = N\u2211 n=1 ( 1 2 log\u03c3 2(y\u0302) + 12\u03c3 2(y\u0302) ( y \u2212 \u00b5(y\u0302) )2) + \u03bbl L\u2211 l=1 \u2016W(l )\u201622 ,\nLr = (1 \u2212 \u03b1) \u00b7 Lr l + \u03b1 \u00b7 Lr e ,\n(4)\nwhere N is the number of training samples, the second term in the rst two equations are the L2 regularization, and \u03b1 is a hyper-parameter.\nAs we will discuss in Section 3.2.3 and evaluate in Section 4.6, a larger \u03b1 leads neural networks to focus more on estimating an accurate mean value, which may underestimate the true uncertainties, while a smaller \u03b1 leads neural networks to estimate a larger variance during the optimization process, which may overestimate the true uncertainties. erefore, \u03b1 is a hyper-parameter that makes the bias-variance tradeo and is tuned to generate a well-calibrated predictive uncertainty, i.e., neither underestimation nor overestimation.\nFor the classi cation problem, f (L)(\u00b7) is the so max function that generates predictive probabilities for each category. e nal loss function of a classi cation problem, Lc , is the weighted sum of mean square error Lce and negative log-likelihood Lcl ,\nLce = N\u2211 n=1 K\u2211 k=1 (1k (y) \u2212 p\u03b8 (y = k |x))2 + \u03bbe L\u2211 l=1 \u2016W(l )\u201622 ,\nLcl = N\u2211 n=1 \u2212 logpW(y\u0302 = y |x) + \u03bbl L\u2211 l=1 \u2016W(l )\u201622 ,\nLc = (1 \u2212 \u03b1) \u00b7 Lcl + \u03b1 \u00b7 Lce ,\n(5)\nwhere N is the number of training samples, K is the number of classes, the second term in the rst two equations are the L2 regularization, and \u03b1 is a hyper-parameter.\nIn summary, the whole neural network is optimized through a tunable proper scoring rule that maximizes the quality of predictive uncertainties. e detailed theoretical backup and proof of the equivalence between RDeepSense and a statistical model will be shown in Section 3.2."}, {"heading": "3.2 Theoretical analysis: the equivalence between RDeepSense and statistical models", "text": "Uncertainty estimations are usually inferred by a statistical model, such as a gaussian process [45] and a graphical model [32]. is section provides the theoretical bases for using RDeepSense to estimate predictive uncertainties by proving the equivalence between the RDeepSense model and a statistical model. To achieve this goal, we rst summarize the preliminary knowledge about the equivalence between dropout training with mean square error and a deep Gaussian process, which is proposed by Gal et al. [17] in Section 3.2.1. en we prove the equivalence between dropout with the proper scoring rule (log-likelihood) and a Gaussian or categorical distributions based on latent deep Gaussian process in Section 3.2.2. Finally, in Section 3.2.3, we generalize the analysis to another tunable proper scoring rule, weighted sum of log-likelihood and negative mean square error, which provides the theoretical foundation for the RDeepSense.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n3.2.1 Preliminary: Dropout with mean square error. Gaussian process is a powerful statistical tool that allows us to model distribution over functions [45]. Here we introduce the preliminary knowledge about Gaussian process and its basic relationship with dropout training through variational approximation, which is rst discussed and proven by Gal et al. [17].\nAssume that we have N pairs of training data, which can be formed into the input matrix X \u2208 RN\u00d7d (0) and the corresponding output matrix Y \u2208 RN\u00d7d (L) . For the regression problem, we place a joint Gaussian distribution over all function values\np(F|X) \u223c N(0,K(X,X)), p(Y|F) \u223c N(F ,\u03c4\u22121I).\n(6)\nwhere \u03c4 is the precision hyper-parameter and K(\u00b7, \u00b7) is the covariance function, encoding the prior function distribution of the Gaussian process. With a dataset of N samples, K(\u00b7, \u00b7) is a N \u00d7 N matrix.\nTo formulate a fully-connected neural network as a Gaussian process, for a single fully-connected layer in a Bayesian neural network, we can de ne the covariance function as\nK(x, x\u2032) = \u222b p(W(l ))f (l )(xW(l ) + b(l ))f (l )(x\u2032W(l ) + b(l ))dW(l ), (7)\nwhere p(W(l )) = N(0, l\u22122I) and f (l )(\u00b7) is the nonlinear activation function. For an L-layer fully-connected neural network, we can feed the output of one Gaussian process to the covariance of the next as a deep Gaussian process model [12]. en our nal target, predictive distribution estimation, can be formulated as\np(y|x,X,Y) = \u222b p(y|x,W)p(W|X,Y)dW, (8)\nwhere p(y|x,W) is the whole Bayesian neural network with random variablesW = {p(W(l ))}. However, calculating the predictive distribution estimation p(y|x,X,Y) requires the posterior distribution p(W|X,Y), and calculating the posterior distribution p(W|X,Y) further requires calculating the inverse of an N \u00d7N matrix, which is infeasible for a large-scale dataset used by a deep neural network. erefore, a variational distribution q(W) =\u220fLl=1 p(W\u0303(l )) is proposed to approximate the true posterior distribution, where W\u0303(l ) is the random variable used in dropout operations introduced in (2). en we minimize the KL divergence between the approximated posterior q(W) and the posterior of the deep Gaussian process over the variational parameters {W\u0303(l )}. e minimization objective is the negative log evidence lower bound derived from the likelihood,\nL\u0434p = \u2212 \u222b q(W) logp(Y|X,W)dW + KL(q(W)|p(W)). (9)\nWe can use Monte Carlo sampling to approximate the rst integral in (9), and (9) can be reduced to L\u0434p = N\u2211 n=1 (yn \u2212 y\u0302n)2 + pil 2 2\u03c4N L\u2211 l=1 \u2016W(l )\u201622 , (10)\nwhere pi is the dropout probability in (2), \u03c4 is the hyperparameter in (6), and l is the length-scale used to de ne the prior distribution p(W(l )).\nIf we compare (10) with the rst equations in (4) and (5), we can nd that optimizing a variational approximation of deep Gaussian process is equivalent to optimizing an dropout neural network based on mean square error as the loss function.\nHowever, mean square error is not a proper scoring rule for regression problems, which cannot generate a well calibrated uncertainty estimations. Besides, due to the mode matching nature of KL divergence, the variational approximating usually generates a highly underestimated predictive uncertainty [6], which is also veri ed in our\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nexperiments in Section 4.4. erefore we further discuss the case of dropout training with proper scoring rules in Section 3.2.2 and Section 3.2.3, which enables RDeepSense to provide a high quality uncertainty estimation.\n3.2.2 Dropout with negative log-likelihood. We have introduced the previous work that treats a neural network with dropout training based on mean square error loss function as a deep Gaussian process with variational approximation. We call this method MCDrop.\nHowever, there are two drawbacks for MCDrop. One is the underestimation of predictive distribution. Variational Bayesian used in MCDrop is known to provide underestimated posterior uncertainty, because optimizing the KL divergence will generate a low-variance estimation to a single mode of true posterior distribution [6]. In addition, the loss function of MCDrop is not a proper scoring rule that can help to mitigate the negative e ect of underestimation caused by the variational Bayesian method. Underestimation is not a desirable property for mobile and ubiquitous computing applications, because it means that the deep neural network will always be over-con dent about its prediction results. e other drawback of MCDrop is the high computational burden during uncertainty estimation. Since the output of MCDrop is a stochastic point estimation, Monte Carlo sampling method is required to estimate the predictive mean and variance. erefore we need to run the whole neural network for multiple times, i.e., running k times for k samples, to generate the predictive uncertainty. Since running time and energy consumption are two crucial problems for mobile and ubiquitous computing applications, MCDrop is not a suitable solution for applications running on embedded devices. erefore, we integrate proper scoring rules and dropout training in RDeepSense to solve the aforementioned two drawbacks. e proper scoring rules such as log-likelihood help to reduce or even erase the underestimation e ect of MCDrop, because proper scoring rule is a score function that gives higher quality uncertainty estimations more credits. In addition, since a neural network with proper score rule directly generates a predictive distribution estimation instead of a point estimation, we can e ciently obtain an approximated expectation of uncertainty estimation through dropout inference. At the same time, dropout as Bayesian approximation can provide a equivalence between the deep neural network and a statistical model, which guarantees RDeepSense to be a mathematically grounded uncertainty estimation method.\nIn this subsection, we show the equivalence between fully-connected neural networks with a proper scoring rule (log-likelihood) and the corresponding statistical model. We have already shown that the equivalence between dropout training and deep Gaussian process with variational approximation. In order to further formulate a fully-connected neural network with log-likelihood as a statistical model, we adds an additional generative step to deep Gaussian process that converts (6) into a new statistical model,\np(F|X) \u223c N(0,K(X,X)), p(Z|F) \u223c N(F ,\u03c4\u22121I), p(Y|Z) \u223c \u0434(Y;Z),\n(11)\nwhere \u0434(Y;Z) is a distribution that converts latent Gaussian process into predictive distribution that conforms the proper scoring rule, i.e., log-likelihood.\nFor regression problems, p(Y|Z) is the Gaussian distribution,\nZ = [Z\u00b5 ,Z\u03c3 2 ], p(Y|Z) \u223c N(Z\u00b5 ,Z\u03c3 2 ).\n(12)\nFor classi cation problems, p(Y|Z) is the composition of categorical distribution with so max function\np(Ynk |Zn \u00b7) \u223c exp(Znk )\u2211 k \u2032 exp(Znk \u2032) . (13)\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nerefore, the nal predictive distribution estimation is changed from (8) into p(y|x,X,Y) = \u222b p(y|z) ( \u222b p(z|x,W)p(W|X,Y)dW ) dz. (14)\nIn order to calculate the predictive probability (14), we still have to propose the same variational distribution q(W) =\u220fLl=1 p(W\u0303(l )) to approximate the posterior distribution p(W|X,Y), where W\u0303(l ) is the random variable used in dropout operations introduced in (2). en, in order to optimize over the variational distribution, the log evidence lower bound for the likelihood can be derived from the likelihood function,\nlogp(Y|X) = log \u222b p(Y|Z)p(Z|X,W)p(W)dWdZ\n= log \u222b\nq(W)p(Y|Z)p(Z|X,W)p(W) q(W)dWdZ\n\u2265 \u222b q(W)p(Z|X,W) log ( p(Y|Z)p(W)\nq(W)\n) dWdZ\n= \u222b q(W)p(Z|X,W) logp(Y|Z)dWdZ \u2212 KL ( q(W)||p(W) ) .\n(15)\nerefore we minimize the negative log evidence lower bound derived in (15) to optimize the variational parameters {W\u0303(l )},\nLl\u0434p = \u2212 N\u2211 n=1 \u222b p(zn |xn ,W) \u00b7 logp(yn |zn)dWdZ + KL ( q(W)||p(W) ) . (16)\ne rst integral in (16) can be approximated with Monte Carlo integration and the second term can be approximated according to MCDrop [17],\nLl\u0434pmc = \u2212 N\u2211 n=1 logp ( yn |z\u0302n(xn ,W\u0302) ) + pil 2 2\u03c4N L\u2211 l=1 \u2016W(l )\u201622 . (17)\nen it is trivial to verify that (17) is equivalent to the second equation in (4) and (5) for regression and classi cation problems respectively by substituting p(yn |z\u0302n(xn ,W\u0302)) with (12) or (13).\nNow, we have shown that training a fully-connected neural network with dropout and negative log-likelihood loss function is equivalent to a Gaussian or categorical distribution based on the latent deep Gaussian process.\n3.2.3 Dropout with weighted sum of negative log-likelihood and mean square error. Training a neural network with a proper scoring rule, log-likelihood loss, should generate predictive uncertainty estimations that faithfully re ect the probability that the prediction will happen. However, training a neural network will log-likelihood loss solely could converge to a local optima that overestimates the true uncertainty empirically, which will be shown in our evaluation Section 4.4. e intuitive explanation for this phenomenon is straight-forward. During the early phase of training a neural network with log-likelihood loss, it is relatively hard to generate an accurate estimation of predictive mean. en increasing the value of variance estimation can consistently decrease the negative log-likelihood loss with a high probability, since there is only a logarithm term that prevents variance from increasing as shown in (4). erefore, the predictive uncertainty tends to favor an estimation with large variance that overestimates the true uncertainty. As a result, although log-likelihood loss is a proper score rule that assigns more credits to predictive\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nuncertainties with higher quality, it usually fails to achieve a good bias-variance tradeo during training process in practice.\nIn order to achieve a well-calibrated uncertainty estimation, i.e., an estimation that neither underestimates nor overestimates, we design a tunable proper scoring rule as the training objective function of RDeepSense. It is a weighted sum of log-likelihood and negative mean square error controlled by a hyper-parameter \u03b1 ,\n(1 \u2212 \u03b1) \u00b7 logpW(y\u0302 = y |x) \u2212 \u03b1 \u00b7 (y\u0302 \u2212 y)2. (18) With the de nition in Section 3.1.2, we can easily see that (18) is a proper scoring rule. According to the analysis in the previous two subsections 3.2.1 and 3.2.2, we can see that RDeepSense, training fully-connected neural network by maximizing the weighted sum of log-likelihood and negative mean square error, is equivalent to the mixture distribution of a Gaussian or categorical distribution based on the latent deep Gaussian process and a deep Gaussian process.\nSince training solely with negative mean square error or log-likelihood tends to underestimate or overestimate the predictive uncertainties respectively, it is easy to ne-tune the hyper-parameter \u03b1 with the validation dataset. When the predictive uncertainty is underestimated, we decrease the value \u03b1 , and vice versa. e detailed analysis of the e ect of hyper-parameter \u03b1 will be illustrated in Section 4.6."}, {"heading": "3.3 RDeepSense uncertainty estimation", "text": "e previous sections prove that RDeepSense is a mathematically grounded method to estimate predictive uncertainties for fully-connected neural networks. In this section, we show that RDeepSense can e ciently estimate predictive uncertainties of fully-connected neural networks with only li le computational overhead.\nAccording to the analysis in Section 3.2.2, the approximated predictive distribution is q(y|x) = \u222b p(y|x,W)q(W)dW = Eq(W) [ p(y|x,W) ] , (19)\nwhereW = {W\u0303(l )} is the random variables generated by dropout operations at each layer.\nz(l )[i] \u223c Bernoulli(p (l ) [i]), W\u0303(l ) = diag ( z(l ) ) W(l ).\n(20)\nUsually Monte Carlo estimation is used to approximate the predictive distribution q(y|x) through sampling random variablesW,\nq(y|x) = 1 M M\u2211 m=1 p(y|x,Wm). (21)\nFor classi cation, (21) is the average of categorical distribution. For regression, (21) is an average of Gaussian distributions. If we assume that M Gaussian distributions are independent, the resulted average distribution can be approximated by a single Gaussian distribution according to the central limit theorem,\n1 M M\u2211 m=1 p(y|x,Wm) = M\u2211 m=1 N(\u00b5m(x),\u03c3 2m(x))\n= N(\u00b5\u0302(x), \u03c3\u0302 2(x)),\n\u00b5\u0302(x) = 1 M M\u2211 m=1 \u00b5m(x),\n\u03c3\u0302 2(x) = 1 M M\u2211 m=1 ( \u03c3 2m(x) + \u00b52m(x) ) \u2212 \u00b5\u03022(x).\n(22)\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\ne drawback of Monte Carlo estimation for embedded devices is its high energy and time consumptions. We have to run the whole neural network for M times to generate M samples, which is not suitable for embedded devices with limited resources.\nFortunately, there is a simple yet e ective recipe proposed by the dropout operation that can e ectively approximate the expected output value instead of using Monte Carlo estimation [46]. During test time, the dropout operation is changed from (2) into\nW\u0303(l ) = diag ( p(l ) ) W(l ),\ny(l ) = x(l )W\u0303(l ) + b(l ), x(l+1) = f (l ) ( y(l ) ) .\n(23)\nAlthough the approximation (23) is not theoretically equivalent to the Monte Carlo estimation (22) by assuming the zero variance of mean estimation, \u2211M m=1 \u00b5 2 m(x) \u2212 ( \u2211M m=1 \u00b5m(x))2 = 0, the proposed approximation (23) turns to be an e ective and e cient approximation during the evaluation in Section 4. In the evaluation section, we will empirically compare the biased approximation (23) with the unbiased Monte Carlo estimation (22). erefore, with the approximation (23), we can directly estimate the expected predictive mean and variance of a Gaussian distribution for regression problems and expected categorical probabilities for classi cation problems by just running the neural network for a single time. is makes RDeepSense a suitable candidate for deep neural networks with uncertainty estimations used in mobile and ubiquitous computing applications."}, {"heading": "4 EVALUATION", "text": "In this section, we evaluate RDeepSense on four mobile and ubiquitous computing tasks. We rst introduce the experimental setup for each task, including hardware, datasets, and baseline algorithms. We then evaluate the accuracy and the quality of uncertainty estimation. Next we evaluate the inference time and energy consumption of all algorithms on the testing hardware. At last we evaluate and analyze the e ect of hyper-parameter \u03b1 in the training objective function (18) on the model performance such as accuracy and quality of uncertainty estimation."}, {"heading": "4.1 Testing hardware", "text": "Our testing hardware is based on Intel Edison computing platform [1]. e Intel Edison computing platform is powered by the Intel Atom SoC dual-core CPU at 500 MHz and is equipped with 1GB memory and 4GB ash storage. For fairness, all neural network models are run solely on CPU during evaluation for inference time and energy consumption."}, {"heading": "4.2 Evaluation tasks", "text": "We conduct four experiments related to human health and wellbeing, smart city transportation, environment monitoring, and human activity recognition with RDeepSense and other two state-of-the-art deep learning uncertainty measuring methods as well as a statistical model. e experimental se ings of the tasks and datasets are introduced in this subsection.\ne detailed statistical information of four datasets is illustrated in Table 2 \u2022 BPEst: Cu less blood pressure monitoring through photoplethysmogram. e rst task is to monitor cu less\nblood pressure through photoplethysmogram from ngertip. e dataset is originally collected by patient monitors at various hospitals between 2001 and 2008. Waveform signals were sampled at the frequency of 125 Hz with at least 8 bit accuracy [21]. e photoplethysmogram from ngertip (PPG) and arterial blood pressure (ABP) signal (mmHg) is extracted by Mohamad et al. for the non-invasive cu less blood pressure monitoring task [27].1 e target of BPEst task is to infer the waveform of ABP based on the\n1h ps://archive.ics.uci.edu/ml/datasets/Cu -Less+Blood+Pressure+Estimation\n, Vol. 1, No. 1, Article 1. Publication date: January 2016."}, {"heading": "4.3 Baseline algorithms", "text": "We compare RDeepSense with other two state-of-the-algorithm deep learning uncertainty estimation algorithms, RDeepSense with Monte Carlo estimation, and Gaussian process. e algorithms with deep neural network, including RDeepSense, use the same neural network architecture. It is a 4-layer fully-connected neural network with 500 hidden dimension.\n2h p://www.nyc.gov/html/tlc/html/about/trip record data.shtml 3h ps://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+under+dynamic+gas+mixtures 4h ps://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n\u2022 MCDrop: is algorithm is based on the Monte Carlo dropout as described in Section 3.2.1 [17]. Compared with RDeepSense, the main di erence is that MCDrop is not optimized by a proper scoring rule. MCDrop requires running the neural network for multiple times to generate samples during uncertainty estimation. erefore we use MCDrop-k to represent MCDrop with k samples. Multiple samples, i.e., k > 1, are required to generate a predictive uncertainty estimation. During the evaluation, we let k to be 3, 5, 10, and 20 to evaluate the tradeo between the quality of uncertainty estimation and the resource consumption for MCDrop. \u2022 SSP: is algorithm trains the neural network with proper scoring methods and uses the ensemble\nmethod [34]. Compared with RDeepSense, the main di erence is that SSP uses the ensemble method instead of the dropout operation in each layer. SSP requires training multiple neural networks for ensemble. erefore we use SSP-k to represent SSP by ensemble k individual neural networks. During the evaluation, we let k to be 1, 3, 5, and 10 to evaluate the tradeo between the quality of uncertainty estimation and the resource consumption for SSP. \u2022 RDeepSense-MC: is algorithm is basically the proposed RDeepSense algorithm. e di erence is\nthat, during the inference, RDeepSense-MC uses Monte Carlo estimation (22) instead of the e cient approximation (23) for uncertainty estimation. erefore we use RDeepSense-MCk to present RDeepSenseMC with k samples. During the evaluation, we let k to be 3, 5, 10, and 20 to evaluate the e ectiveness and e ciency of RDeepSense inference approximation (23) compared with the Monte Carlo estimation (22). \u2022 GP: Gaussian process (GP) is the baseline algorithm used during the evaluation of accuracy and the\nquality of uncertainty estimations, but not for the evaluations of running time and energy consumption on Edison. e main reason is that the computation cost during model inference for GP is O(N 3), where N is the number of data instances. is cost can be prohibitive even for moderately sized datasets on embedded devices, such as Intel Edison. In additional, GP requires O(N 2) memory consumption during training. erefore we train the GP with only a proportion of dataset on a server with 128GB memory. Notice that GP is the baseline used to illustrate the quality of uncertainty estimations generated by a statistical model, so the size of training dataset is not the main concern."}, {"heading": "4.4 Accuracy of prediction and quality of uncertainty estimations", "text": "In this section, we discuss the accuracy and the uncertainty estimation quality of RDeepSense compared with the other baseline algorithms. RDeepSense is tuned with the validating dataset, and all algorithms in all experiments are tested on the testing dataset.\nFor three regression problems, two types of evaluation results will be illustrated and discussed. e rst type of evaluation is based on some basic measurements including mean absolute error and negative log-likelihood. e second type of evaluation is based on the calibration curves, also known as reliability diagrams. We compute the z% con dence interval for each testing data based on predictive mean and variance of each algorithm. en we measure the fraction of the testing data that falls into this con dence interval. For a well-calibrated uncertainty estimation, the fraction of testing data that falls into the con dence interval should be similar to z%. We compute the calibration curves with z = [10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 85%, 95%, 99%, 99.5%, 99.9%] for all three regression problems.\nFor the classi cation problem, the calibration curve is not available. erefore, we evaluate HHAR based on accuracy, f1 score, negative log-likelihood, and a new measurement called the mean entropy of false predictions. If the entropies of false predictions are higher, the learning algorithms show more uncertainties about the false predictions, which represents a be er quality of uncertainty estimations.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n4.4.1 BPEst. We rst compare RDeepSense with four baseline algorithms based on mean absolute error (MAE) and negative log-likelihood (NLL), which is illustrated in Table 3, where we highlight the results of RDeepSense and the best-performing one.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nFrom Table 3, we can see that, except for RDeepSense-MC20, RDeepSense is the best-performing and the second best-performing algorithm for NLL and MAE respectively, which means that RDeepSense can provide accurate estimation with high-quality predictive uncertainty. RDeepSense-MC20 only slightly beats RDeepSense on NLL, however RDeepSense-MC20 consumes around \u00d720 time and energy compared with RDeepSense. e performance of MCDrop-k increases when k increases. Larger k means that MCDrop algorithm generates more samples during model inference, which can provide higher-quality estimations but more resource consumptions. MCDrop-3 provides a relatively bad result for NLL, which means MCDrop does require a number of samples for uncertainty estimation with reasonable quality. e ensemble method used in SSP increases the prediction performance, but it is not consistent. SSP-10 observes the performance degradation compared with SSP-5. GP obtains a relatively large MAE. is is because GP cannot be scaled to train on the whole dataset. e calibration curves of BPEst task is illustrated in Figure 1. ese three gures show the quality of predictive uncertainty estimations. RDeepSense generates predictive uncertainties with the highest quality. RDeepSense even slightly out-performs the traditional statistical model, GP. As we mentioned in Section 3.2, MCDrop-k tends to underestimate the predictive uncertainty, while SSP-k tends to overestimate the predictive uncertainty. RDeepSense even generates predictive uncertainty with be er calibration compared with RDeepSense-MCk, which indicate the e ectiveness of approximation during inference. All MCDrop-k, SSP-k, and RDeepSense-MCk improve the quality of uncertainty estimations by increasing the value of k .\n4.4.2 NYCommute. en we compare RDeepSene with baseline algorithms for NYCommute task. e comparison based on Mean Absolute Error (MAE) and Negative Log-Likelihood (NLL) is shown in Table 4.\nIn this task, RDeepSense tends to nd a balance between MAE and NLL measurements. MCDrop-k shows low MAE and high NLL, while SSP-k shows high MAE and low NLL. MCDrop-k tries to minimize the mean square error, while SSP-k tries to minimize the negative log-likelihood. erefore, MCDrop-k focuses more on the mean of predictive distribution, and SSP-k focuses more on the overall likelihood. RDeepSense combines two objective functions, mean square error and negative log-likelihood, which tries to nd a balance point between these two. Still, due to the scalability problem, GP obtains a relatively larger MAE. Compared with RDeepSense-MCk, RDeepSense achieve a good performance on both MAE and NLL. Only RDeepSense-MC20 shows the same performance on the NLL measurement. e calibration curves of NYCommute task is illustrated in Figure 2. Both MCDrop-k and SSP-k fail to generate high-quality uncertainty estimations by either underestimating or overestimating the predictive uncertainties. However, RDeepSense can still provide uncertainty estimations with good quality, which outperforms GP with a signi cant margin. Compared with RDeepSense-MCk, RDeepSense shows similar performance on generating well-calibrated predictive uncertainties, which shows that the approximation (23) works well in practice.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n4.4.3 GasSen. Next we compare RDeepSense with other baseline algorithms for the GasSen task. Table 5 illustrates the performance of all these algorithms based on Mean Absolute Error (MAE) and Negative LogLikelihood (NLL). Except for RDeepSense-MC20, RDeepSense is the best-performing algorithm according to these two metrics. Similarly, MCDrop-k shows low MAE and NLL, while SSP-k shows high MAE and NLL. is is due to the objective of these two types of algorithms. MCDrop-k minimizes the mean square error, while SSP-k minimizes the negative log-likelihood. erefore, MCDrop-k focuses more on the mean of predictive distribution, and SSP-k focuses more on the overall likelihood. RDeepSense combines two objective function. erefore, RDeepSense is able to achieve the best performance in both cases. e usage of dropout that prevents feature co-adapting is the main reason why RDeepSense achieves be er NLL compared with SPP-k. e RDeepSense still achieves good performance compared with its Motel Carlo version. Only RDeepSense-MC20 slightly outperforms RDeepSense under the NLL measurement, which shows the e ectiveness of the approximation used in RDeepSense. e calibration curves of GasSen task is illustrated in Figure 3. e calibration curves of MCDrop-k highly underestimates the predictive distribution as shown in Figure 3a, while the calibration curves of SSP-k highly overestimates the predictive distribution as shown in Figure 3b. Although there exists a bit deviation for RDeepSense compared with the optimal calibration curve, RDeepSense greatly reduces the e ect of underestimation and overestimation, and slightly outperforms the traditional statistical model, GP. Compared with unbiased\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nRDeepSense-MCk, RDeepSense shows the similar performance. However, RDeepSense saves save a great amount of energy and time consumption as we will discuss in Section 4.5.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n4.4.4 HHAR. Last we compare RDeepSense with the other baseline algorithm for the HHAR task. Table 6 illustrates the performance metrics of all algorithms based on Accuracy (Acc), F1 score (F1 Score), Negative Log-Likelihood (NLL), and Mean Entropy of False Predictions (MEFP).\nExcept for RDeepSense-MC20, RDeepSense is the best-performing algorithm according to all measures, which means RDeepSense can provide both high prediction accuracy as well as high quality of uncertainty estimations. MCDrop-k algorithms are trained with log-likelihood. erefore they try to minimize the negative log-likelihood, but they are over-con dent about their prediction even when they make some wrong predictions according to the MEFP measure. SSP-k algorithms are trained with Brier score. erefore they fall short to achieve smaller NLL values. Compared with RDeepSense-MCk algorithms, RDeepSense still provides a good performance in all measurements. Only RDeepSense-MC20 shows a superior performance on F1 score and NLL measurements."}, {"heading": "4.5 Inference time and energy consumption", "text": "We compared the resource consumption of each algorithm including inference time and energy consumption of one-data-sample execution, which are two key issues for mobile and ubiquitous computing. All the experiments are conducted on Intel Edison with only CPU as the computing unit. No further optimization is made on any algorithms. e inference time and energy consumption of GP are not included. is is because the time complexity of GP is O(N 3), where N is the size of training dataset, which is infeasible for embedded devices such as Intel Edison. e results of four tasks, i.e., BPEst, NYCommute, GasSen, and HHAR, are illustrated in Figures 4, 5, 6, and 7 respectively.\nWe can clearly see that RDeepSense greatly reduces the inference time and energy consumption compared with the other deep learning uncertainty estimation algorithms. Compared with MCDrop algorithm, RDeepSense is trained according to the proper scoring rule, which can directly output the predictive distribution instead of using sampling methods. Compared with SSP algorithm, RDeepSense uses dropout regularization as an implicit ensemble method, which avoids running multiple deep learning models during model inference on embedded devices. Compared with RDeepSense-MC, RDeepSense use the approximation (23) to replace the computationally intensive Motel Carlo method (22) during the inference.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nWe further analyze the relationship between energy consumption and the quality of uncertainty estimation for each algorithms. For regression problems, we use the area between the calibration curve of an algorithm and the optimal calibration curve, called deviation area, as the quality measurement of uncertainty. e smaller deviation area is, the be er quality of uncertainty the algorithm estimates. When the calibration curve of an algorithm is optimal, the deviation area is 0. For classi cation problems, we use the negative mean entropy of false predictions as the quality measurement of uncertainty. Smaller negative mean entropy of false predictions means is that the algorithm is more uncertain about their false predictions. e result is shown in Figure 8. e point or line stay in the bo om-le corner of the graph represents a be er tradeo between energy and uncertainty quality, i.e., using less energy to obtain be er uncertainty estimations. erefore, RDeepSense is the best-performing algorithm that uses the least amount of energy to obtain the best uncertainty estimation quality. RDeepSense-MC can achieve similar uncertainty estimation quality as RDeepSense, however it requires much more energy consumption. e results show that RDeepSense is an e ective and e cient uncertainty estimation algorithm (23) compared with its Monte Carlo version (22). Other two baseline algorithms, MCDrop and SSP, usually su er a large deviation area or become over-con dence about their false predictions while using more energy for computation. Figure 8 shows that RDeepSense is the most suitable algorithm for generate predictive uncertainty estimations for mobile and ubiquitous computing application on embedded devices."}, {"heading": "4.6 E ect of hyper-parameter \u03b1 on model performance", "text": "e hyper-parameter \u03b1 controls the tradeo between optimization of mean and variance within the training objective function (18) that can help to obtain a well-calibrated uncertainty estimation. In this subsection, we evaluate the functionality of \u03b1 and also shed light on the way of tuning \u03b1 .\nFor each task, we train RDeepSense with \u03b1 = [0, 0.2, 0.4, 0.6, 0.8, 0.9]. When \u03b1 = 0.0, RDeepSense is trained by minimizing the negative log-likelihood. When we increase the value of \u03b1 , RDeepSense focuses more on the mean value estimation instead of the negative log-likelihood. In order to show the e ect of the choice of \u03b1 on the quality of predictive uncertainty estimation, we show the negative log-likelihood and devision area (the area between the calibration curve of an algorithm and the optimal calibration curve) for regression tasks and show the negative log-likelihood and Negative Mean Entropy (NME) of false predictions for the classi cation task in Figure 9.\nA good uncertainty estimation should faithfully re ect the probability that prediction will happen. erefore, RDeepSense targets on a well-calibrated uncertainty estimation, such as the prediction with low devision area, in stead of the prediction with low negative log-likelihood. From Figure 9a, 9b, and 9c, we can see that hyper-parameter \u03b1 controls the tradeo between optimization mean and variance within the training objective function (18). Smaller \u03b1 tends to reduce negative log-likelihood by increasing the predictive variance, which tends to result the overestimation of predictive uncertainties. Larger \u03b1 tends to reduce negative log-likelihood by predicting a be er mean value, which tends to result the underestimation of predictive uncertainties. When tuning the hyper-parameter \u03b1 , we can easily found a point that achieve the smallest devision area by grid searching \u03b1 from 0 to 1. At the same time, it is not surprising that increasing \u03b1 can slightly increase the negative log-likelihood, since \u03b1 = 0 represents regarding negative log-likelihood as the objective function. In addition, Figure 9d shows that increasing \u03b1 can consistently increase the negative mean entropy of false predictions."}, {"heading": "5 DISCUSSION", "text": "is paper focuses on empowering neural networks to generate high-quality predictive uncertainty estimations in a theoretically-grounded and energy-e cient manner for mobile and ubiquitous computing tasks. Currently, RDeepSense can only support fully-connected neural networks. It is possible to extend the two-step solution introduced in Section 3 to convolutional and recurrent neural networks by replacing the original dropout operation with convolutional dropout [16] and recurrent dropout [18]. ese two dropout operations can convert\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nconvolutional neural networks and recurrent neural networks into Bayesian neural networks [16, 18], but additional e orts are needed to 1) theoretically prove that the extended two-step solution can equate an arbitrary neural network with a statistical model, and 2) empirically show that the extended two-step solution can provide high-quality uncertainty estimations on the real datasets.\nAnother interesting extension could be empowering existing neural networks within mobile and ubiquitous computing applications to generate predictive uncertainty estimations without additional training. A lot of neural networks have already been trained with dropout operations. As shown by Gal et al. [17], although these models tend to underestimate the true uncertainties, they can provide uncertainty estimations during model inference. is can be a good solution for mobile and ubiquitous computing applications that want to obtain an indictor of predictive uncertainty instead of a high-quality predictive uncertainty estimation without retraining their neural networks. However, additional e orts are needed to bypass the Monte Carlo sampling method and provide an energy-e cient method for generating uncertainty estimations on embedded devices.\n, Vol. 1, No. 1, Article 1. Publication date: January 2016.\nIn addition, for classi cation problems, although traditional neural networks can also output predictive distribution on each class, which contains predictive uncertainties, RDeepSense provides a high-quality predictive distribution on each class and has been proved to be equivalent to a statistical model."}, {"heading": "6 CONCLUSION", "text": "We introduced RDeepSense, a simple yet e ective solution that empowers fully-connected neural networks to generate well-calibrated predictive uncertainty estimations during model inference. RDeepSense is a computationally e cient algorithm that can provide predictive uncertainty estimations in mobile and ubiquitous computing applications with almost no additional overhead. eoretical analysis also shows the equivalence between RDeepSense and a statistical model. We evaluated RDeepSense on four mobile and ubiquitous computing tasks, where RDeepSense outperformed the state-of-the-art baselines by signi cant margins on the quality of uncertainty estimations while still consuming the least amount of energy on embedded devices. In summary, RDeepSense is a simple, e ective, and e cient solution for mobile and ubiquitous applications to build reliable neural networks with uncertainty estimations."}], "references": [{"title": "Understanding dropout", "author": ["P. Baldi", "P.J. Sadowski"], "venue": "Advances in Neural Information Processing Systems, pages 2814\u20132822", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Shuteye: encouraging awareness of healthy sleep recommendations with a mobile", "author": ["J.S. Bauer", "S. Consolvo", "B. Greenstein", "J. Schooler", "E. Wu", "N.F. Watson", "J. Kientz"], "venue": "peripheral display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1401\u20131410. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "\u008bantifying the uncertainty of next-place predictions", "author": ["P. Baumann", "M. Langheinrich", "A. Dey", "S. Santini"], "venue": "Proceedings of the 8th EAI International Conference on Mobile Computing, Applications and Services, pages 74\u201385. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the stress of coordination: sharing travel time information between contacts on mobile phones", "author": ["F.R. Bentley", "Y.-Y. Chen", "C. Holz"], "venue": "Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 967\u2013970. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "and J", "author": ["D.M. Blei", "A. Kucukelbir"], "venue": "D. McAuli\u0082e. Variational inference: A review for statisticians. Journal of the American Statistical Association, (just-accepted)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "How data workers cope with uncertainty: A task characterisation study", "author": ["N. Boukhelifa", "M.-E. Perrin", "S. Huron", "J. Eagan"], "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pages 3645\u20133656. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Airlink: sharing \u0080les between multiple devices using in-air gestures", "author": ["K.-Y. Chen", "D. Ashbrook", "M. Goel", "S.-H. Lee", "S. Patel"], "venue": "Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 565\u2013569. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["T. Choudhury", "S. Consolvo", "B. Harrison", "J. Hightower", "A. LaMarca", "L. LeGrand", "A. Rahimi", "A. Rea", "G. Bordello", "B. Hemingway"], "venue": "\u008ce mobile sensing platform: An embedded activity recognition system. IEEE Pervasive Computing, 7(2)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Indoor location sensing using geo-magnetism", "author": ["J. Chung", "M. Donahoe", "C. Schmandt", "I.-J. Kim", "P. Razavai", "M. Wiseman"], "venue": "Proceedings of the 9th international conference on Mobile systems, applications, and services, pages 141\u2013154. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Model uncertainty", "author": ["M. Clyde", "E.I. George"], "venue": "Statistical science, pages 81\u201394", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep gaussian processes", "author": ["A. Damianou", "N. Lawrence"], "venue": "Arti\u0080cial Intelligence and Statistics, pages 207\u2013215", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Steven Hickson and I", "author": ["V.B.E.T.G.A.H.C. Daniel Castro"], "venue": "Essa. Predicting daily activities from egocentric images using deep learning. ISWC", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Behavioral activities collected through smartphones and the association with illness activity in bipolar disorder", "author": ["M. Faurholt-Jepsen", "M. Vinberg", "M. Frost", "S. Debel", "E. Margrethe Christensen", "J.E. Bardram", "L.V. Kessing"], "venue": "International journal of methods in psychiatric research, 25(4):309\u2013323", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring", "author": ["J. Fonollosa", "S. Sheik", "R. Huerta", "S. Marco"], "venue": "Sensors and Actuators B: Chemical, 215:618\u2013629", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian convolutional neural networks with bernoulli approximate variational inference", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1506.02158", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems, pages 1019\u20131027", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic machine learning and arti\u0080cial intelligence", "author": ["Z. Ghahramani"], "venue": "Nature, 521(7553):452\u2013459", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["T. Gneiting", "A.E. Ra\u0089ery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "J", "author": ["A.L. Goldberger", "L.A. Amaral", "L. Glass"], "venue": "M. Hausdor\u0082, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. Physiobank, physiotoolkit, and physionet. Circulation, 101(23):e215\u2013e220", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Energy-e\u0081cient activity recognition using prediction", "author": ["D. Gordon", "J. Czerny", "T. Miyaki", "M. Beigl"], "venue": "Wearable Computers (ISWC), 2012 16th International Symposium on, pages 29\u201336. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Health chair: implicitly sensing heart and respiratory rate", "author": ["E. Gri\u0081ths", "T.S. Saponas", "A. Brush"], "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "J", "author": ["T. Grosse-Puppendahl", "X. Dellangnol", "C. Hatzfeld", "B. Fu", "M. Kupnik", "A. Kuijper", "M.R. Hastall"], "venue": "Sco\u008a, and M. Gruteser. Platypus: Indoor localization and identi\u0080cation through sensing of electric potential changes in human bodies. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services, pages 17\u201330. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensembles of deep lstm learners for activity recognition using wearables", "author": ["Y. Guan", "T. Ploetz"], "venue": "arXiv preprint arXiv:1703.09370", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Ariel: Automatic wi-\u0080 based room \u0080ngerprinting for indoor localization", "author": ["Y. Jiang", "X. Pan", "K. Li", "Q. Lv", "R.P. Dick", "M. Hannigan", "L. Shang"], "venue": "Proceedings of the 2012 ACM Conference on Ubiquitous Computing, pages 441\u2013450. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Cu\u0082-less high-accuracy calibration-free blood pressure estimation using pulse transit time", "author": ["M. Kachuee", "M.M. Kiani", "H. Mohammadzade", "M. Shabany"], "venue": "Circuits and Systems (ISCAS), 2015 IEEE International Symposium on, pages 1006\u20131009. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Design and learnability of vortex whistles for managing chronic lung function via smartphones", "author": ["S. Kaiser", "A. Parks", "P. Leopard", "C. Albright", "J. Carlson", "M. Goel", "D. Nassehi", "E.C. Larson"], "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 569\u2013580. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "When (ish) is my bus?: User-centered visualizations of uncertainty in everyday", "author": ["M. Kay", "T. Kola", "J.R. Hullman", "S.A. Munson"], "venue": "mobile predictive systems. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 5092\u20135103. ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "How good is 85%?: A survey tool to connect classi\u0080er evaluation to acceptability of accuracy", "author": ["M. Kay", "S.N. Patel", "J.A. Kientz"], "venue": "Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 347\u2013356. ACM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["C. Koehler", "N. Banovic", "I. Oakley"], "venue": "Manko\u0082, and A. K. Dey. Indoor-alps: an adaptive indoor location prediction system. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 171\u2013181. ACM", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Points of signi\u0080cance: importance of being uncertain", "author": ["M. Krzywinski", "N. Altman"], "venue": "Nature methods, 10(9):809", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "author": ["B. Lakshminarayanan", "A. Pritzel", "C. Blundell"], "venue": "arXiv preprint arXiv:1612.01474", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepear: robust smartphone audio sensing in unconstrained acoustic environments using deep learning", "author": ["N.D. Lane", "P. Georgiev", "L. Qendro"], "venue": "Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 283\u2013294. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating intelligibility for uncertain context-aware applications", "author": ["B.Y. Lim", "A.K. Dey"], "venue": "Proceedings of the 13th international conference on Ubiquitous computing, pages 415\u2013424. ACM", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "\u008ce mythos of model interpretability", "author": ["Z.C. Lipton"], "venue": "arXiv preprint arXiv:1606.03490", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Activity recognition using a single accelerometer placed at the wrist or ankle", "author": ["A. Mannini", "S.S. Intille", "M. Rosenberger", "A.M. Sabatini", "W. Haskell"], "venue": "Medicine and science in sports and exercise, 45(11):2193", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Leveraging directional antenna capabilities for \u0080ne-grained gesture recognition", "author": ["P. Melgarejo", "X. Zhang", "P. Ramanathan", "D. Chu"], "venue": "Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 541\u2013551. ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "E-gesture: a collaborative architecture for energy-e\u0081cient gesture recognition with hand-worn sensor and mobile devices", "author": ["T. Park", "J. Lee", "I. Hwang", "C. Yoo", "L. Nachman", "J. Song"], "venue": "Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems, pages 260\u2013273. ACM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust", "author": ["G. Pirkl", "P. Lukowicz"], "venue": "low cost indoor positioning using magnetic resonant coupling. In Proceedings of the 2012 ACM Conference on Ubiquitous Computing, pages 431\u2013440. ACM", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Whole-home gesture recognition using wireless signals", "author": ["Q. Pu", "S. Gupta", "S. Gollakota", "S. Patel"], "venue": "Proceedings of the 19th annual international conference on Mobile computing & networking, pages 27\u201338. ACM", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating predictive uncertainty challenge", "author": ["J. \u008binonero-Candela", "C.E. Rasmussen", "F. Sinz", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classi\u0080cation, and Recognising Tectual Entailment,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2006}, {"title": "S", "author": ["V. Radu", "N.D. Lane"], "venue": "Bha\u008aacharya, C. Mascolo, M. K. Marina, and F. Kawsar. Towards multimodal deep learning for activity recognition on mobile devices. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct, pages 185\u2013188. ACM", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["A. Stisen", "H. Blunck"], "venue": "Bha\u008aacharya, T. S. Prentow, M. B. Kj\u00e6rgaard, A. Dey, T. Sonne, and M. M. Jensen. Smart devices are di\u0082erent: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, pages 127\u2013140. ACM", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Best intentions: health monitoring technology and children", "author": ["T. Toscos", "K. Connelly", "Y. Rogers"], "venue": "Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 1431\u20131440. ACM", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Hemaapp: noninvasive blood screening of hemoglobin using smartphone cameras", "author": ["E.J. Wang", "W. Li", "D. Hawkins", "T. Gernsheimer", "C. Norby-Slycord", "S.N. Patel"], "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 593\u2013604. ACM", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "A simple baseline for travel time estimation using large-scale trip data", "author": ["H. Wang", "Y.-H. Kuo", "D. Kifer", "Z. Li"], "venue": "Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, page 61. ACM", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Monitoring crowd condition in public spaces by tracking mobile consumer devices with wi\u0080 interface", "author": ["J. Weppner", "B. Bischke", "P. Lukowicz"], "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct, pages 1363\u20131371. ACM", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning from less for be\u008aer: semi-supervised activity recognition via shared structure discovery", "author": ["L. Yao", "F. Nie", "Q.Z. Sheng", "T. Gu", "X. Li", "S. Wang"], "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 13\u201324. ACM", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["S. Yao", "M.T. Amin", "L. Su", "S. Hu", "S. Li", "S. Wang", "Y. Zhao", "T. Abdelzaher", "L. Kaplan", "C. Aggarwal"], "venue": "Recursive ground truth estimator for social data streams. In Information Processing in Sensor Networks (IPSN), 2016 15th ACM/IEEE International Conference on, pages 1\u201312. IEEE", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "On source dependency models for reliable social sensing: Algorithms and fundamental error bounds", "author": ["S. Yao", "S. Hu", "S. Li", "Y. Zhao", "L. Su", "L. Kaplan", "A. Yener", "T. Abdelzaher"], "venue": "Distributed Computing Systems (ICDCS), 2016 IEEE 36th International Conference on, pages 467\u2013476. IEEE", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepsense: A uni\u0080ed deep learning framework for time-series mobile sensing data processing", "author": ["S. Yao", "S. Hu", "Y. Zhao", "A. Zhang", "T. Abdelzaher"], "venue": "Proceedings of the 26th International Conference on World Wide Web, pages 351\u2013360. International World Wide Web Conferences Steering Commi\u008aee", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2017}, {"title": "Deepiot: Compressing deep neural network structures for sensing systems with a compressor-critic framework", "author": ["S. Yao", "Y. Zhao", "A. Zhang", "L. Su", "T. Abdelzaher"], "venue": "arXiv preprint arXiv:1706.01215", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2017}, {"title": "and A", "author": ["H.-S. Yeo", "J. Lee", "A. Bianchi"], "venue": "\u008bigley. Watchmi: pressure touch, twist and pan gesture input on unmodi\u0080ed smartwatches. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services, pages 394\u2013399. ACM", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "T", "author": ["C. Zhang", "L. Liu", "D. Lei", "Q. Yuan", "H. Zhuang"], "venue": "Hanra\u008ay, and J. Han. Triovecevent: Embedding-based online local event detection in geo-tagged tweet streams. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 595\u2013604. ACM", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2017}, {"title": "T", "author": ["C. Zhang", "K. Zhang", "Q. Yuan", "H. Peng", "Y. Zheng"], "venue": "Hanra\u008ay, S. Wang, and J. Han. Regions, periods, activities: Uncovering urban dynamics via cross-modal representation learning. In Proceedings of the 26th International Conference on World Wide Web, pages 361\u2013370. International World Wide Web Conferences Steering Commi\u008aee", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2017}, {"title": "Eye tracking for public displays in the wild", "author": ["Y. Zhang", "M.K. Chong", "J. M\u00fcller", "A. Bulling", "H. Gellersen"], "venue": "Personal and Ubiquitous Computing, 19(5-6):967\u2013981", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 3, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 12, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 21, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 26, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 46, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 47, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 99, "endOffset": 125}, {"referenceID": 6, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 7, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 36, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 37, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 40, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 50, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 55, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 161, "endOffset": 187}, {"referenceID": 51, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 203, "endOffset": 219}, {"referenceID": 52, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 203, "endOffset": 219}, {"referenceID": 56, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 203, "endOffset": 219}, {"referenceID": 57, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 203, "endOffset": 219}, {"referenceID": 8, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 247, "endOffset": 271}, {"referenceID": 22, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 247, "endOffset": 271}, {"referenceID": 24, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 247, "endOffset": 271}, {"referenceID": 29, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 247, "endOffset": 271}, {"referenceID": 49, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 247, "endOffset": 271}, {"referenceID": 58, "context": "Numerous mobile applications have prospered in a wide range of areas, such as health and wellbeing [3, 5, 14, 23, 28, 48, 49], behavior and activity recognition [8, 9, 38, 39, 42, 52, 57], crowd sensing [53, 54, 58, 59], tracking and localization [10, 24, 26, 31, 51, 60].", "startOffset": 247, "endOffset": 271}, {"referenceID": 33, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 150, "endOffset": 166}, {"referenceID": 23, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 150, "endOffset": 166}, {"referenceID": 42, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 150, "endOffset": 166}, {"referenceID": 53, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 150, "endOffset": 166}, {"referenceID": 53, "context": "\u008cese speci\u0080cally designed models have shown signi\u0080cant improvements on audio sensing [35], tracking and localization [55], human activity recognition [13, 25, 44, 55], and user identi\u0080cation tasks [55].", "startOffset": 197, "endOffset": 201}, {"referenceID": 35, "context": "\u008ce complexity and uninterpretability of such models mainly result from the deep and non-linear structures [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 41, "context": ", in the form of probability distributions instead of point estimations [43].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "On the other hand, uncertainty estimation itself is crucial for scienti\u0080c measurements [19, 33].", "startOffset": 87, "endOffset": 95}, {"referenceID": 31, "context": "On the other hand, uncertainty estimation itself is crucial for scienti\u0080c measurements [19, 33].", "startOffset": 87, "endOffset": 95}, {"referenceID": 28, "context": "Extensive investigations show that measurement uncertainties can impact user experiences [30, 36].", "startOffset": 89, "endOffset": 97}, {"referenceID": 34, "context": "Extensive investigations show that measurement uncertainties can impact user experiences [30, 36].", "startOffset": 89, "endOffset": 97}, {"referenceID": 9, "context": "Although mathematically grounded methods such as Bayesian approaches serves as powerful tools to estimate predictive uncertainties [11], Bayesian neural networks are computationally expensive to train and inference even for brawny servers, let alone mobile and embedded devices [43].", "startOffset": 131, "endOffset": 135}, {"referenceID": 41, "context": "Although mathematically grounded methods such as Bayesian approaches serves as powerful tools to estimate predictive uncertainties [11], Bayesian neural networks are computationally expensive to train and inference even for brawny servers, let alone mobile and embedded devices [43].", "startOffset": 278, "endOffset": 282}, {"referenceID": 54, "context": "For mobile and ubiquitous computing applications, although we can train the deep neural networks on brawny servers with powerful GPUs, running inference on mobile and embedded devices is di\u0081cult due to limited energy supplies and computational resources on such devices [56].", "startOffset": 270, "endOffset": 274}, {"referenceID": 15, "context": "Illuminating studies from the machine learning community try to provide mathematically grounded uncertainty estimations for deep neural networks, but these methods are based either on the sampling method [17] or the ensemble method [34].", "startOffset": 204, "endOffset": 208}, {"referenceID": 32, "context": "Illuminating studies from the machine learning community try to provide mathematically grounded uncertainty estimations for deep neural networks, but these methods are based either on the sampling method [17] or the ensemble method [34].", "startOffset": 232, "endOffset": 236}, {"referenceID": 15, "context": "\u008ce core of RDeepSense is the integration of the dropout training method that interprets neural networks as Gaussian process (GP) through Bayesian approximation [17, 45, 46] and proper scoring rules as training criterion that measure the quality of predictive uncertainty such as log-likelihood and the Brier score [20].", "startOffset": 160, "endOffset": 172}, {"referenceID": 43, "context": "\u008ce core of RDeepSense is the integration of the dropout training method that interprets neural networks as Gaussian process (GP) through Bayesian approximation [17, 45, 46] and proper scoring rules as training criterion that measure the quality of predictive uncertainty such as log-likelihood and the Brier score [20].", "startOffset": 160, "endOffset": 172}, {"referenceID": 44, "context": "\u008ce core of RDeepSense is the integration of the dropout training method that interprets neural networks as Gaussian process (GP) through Bayesian approximation [17, 45, 46] and proper scoring rules as training criterion that measure the quality of predictive uncertainty such as log-likelihood and the Brier score [20].", "startOffset": 160, "endOffset": 172}, {"referenceID": 18, "context": "\u008ce core of RDeepSense is the integration of the dropout training method that interprets neural networks as Gaussian process (GP) through Bayesian approximation [17, 45, 46] and proper scoring rules as training criterion that measure the quality of predictive uncertainty such as log-likelihood and the Brier score [20].", "startOffset": 314, "endOffset": 318}, {"referenceID": 15, "context": "Firstly, RDeepSense uses a tunable proper scoring rule as the training criterion that signi\u0080cantly mitigates the problem of underestimating predictive uncertainties in deep neural networks [17].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "Secondly, since dropout training can be interpreted as \u201cgeometric averaging\u201d over the ensemble of possible \u201cthinned\u201d subnetworks [2], RDeepSense applies dropout training instead of model ensemble.", "startOffset": 129, "endOffset": 132}, {"referenceID": 25, "context": "Speci\u0080cally, our experiments include: 1) monitoring arterial blood pressure through photoplethysmogram (PPG) from \u0080ngertip [27], 2) NY city taxi commute time estimation [50], 3) gas mixture concentrations estimation through the chemical sensor array [15], 4) and heterogeneous human activity recognition through motion sensors [47].", "startOffset": 123, "endOffset": 127}, {"referenceID": 48, "context": "Speci\u0080cally, our experiments include: 1) monitoring arterial blood pressure through photoplethysmogram (PPG) from \u0080ngertip [27], 2) NY city taxi commute time estimation [50], 3) gas mixture concentrations estimation through the chemical sensor array [15], 4) and heterogeneous human activity recognition through motion sensors [47].", "startOffset": 169, "endOffset": 173}, {"referenceID": 13, "context": "Speci\u0080cally, our experiments include: 1) monitoring arterial blood pressure through photoplethysmogram (PPG) from \u0080ngertip [27], 2) NY city taxi commute time estimation [50], 3) gas mixture concentrations estimation through the chemical sensor array [15], 4) and heterogeneous human activity recognition through motion sensors [47].", "startOffset": 250, "endOffset": 254}, {"referenceID": 45, "context": "Speci\u0080cally, our experiments include: 1) monitoring arterial blood pressure through photoplethysmogram (PPG) from \u0080ngertip [27], 2) NY city taxi commute time estimation [50], 3) gas mixture concentrations estimation through the chemical sensor array [15], 4) and heterogeneous human activity recognition through motion sensors [47].", "startOffset": 327, "endOffset": 331}, {"referenceID": 15, "context": "We compare RDeepSense with the state-of-the-art Monte Carlo dropout method [17], ensemble method [34], and Gaussian process.", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": "We compare RDeepSense with the state-of-the-art Monte Carlo dropout method [17], ensemble method [34], and Gaussian process.", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "[4] make next-place predictions based on the uncertainty estimation of classi\u0080ers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[29] propose a novel discrete representation of uncertainties for visualizing and user interaction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] propose design considerations for uncertainty-aware data analytics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[35] apply deep neural networks to solve audio sensing tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] predict daily activities from egocentric images using deep learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[55] propose a deep learning structure that fuses multiple sensor inputs and extracts time dependencies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] apply ensembles of LSTM for activity recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] provide the \u0080rst theoretical proof of the linkage between dropout training with deep Gaussian process called MCDrop.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] propose", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "\u008cese computationally intensive operations aggravate the time and energy consumption problems in the embedded devices, which is one of the key issues of mobile and ubiquitous computing [22, 40, 41].", "startOffset": 184, "endOffset": 196}, {"referenceID": 38, "context": "\u008cese computationally intensive operations aggravate the time and energy consumption problems in the embedded devices, which is one of the key issues of mobile and ubiquitous computing [22, 40, 41].", "startOffset": 184, "endOffset": 196}, {"referenceID": 39, "context": "\u008cese computationally intensive operations aggravate the time and energy consumption problems in the embedded devices, which is one of the key issues of mobile and ubiquitous computing [22, 40, 41].", "startOffset": 184, "endOffset": 196}, {"referenceID": 44, "context": "To avoid these problems, researchers introduce the concept of dropout as a regularization method [46].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "Scoring rules, also known as score functions, measure the quality of predictive uncertainties [20].", "startOffset": 94, "endOffset": 98}, {"referenceID": 43, "context": "2 Theoretical analysis: the equivalence between RDeepSense and statistical models Uncertainty estimations are usually inferred by a statistical model, such as a gaussian process [45] and a graphical model [32].", "startOffset": 178, "endOffset": 182}, {"referenceID": 30, "context": "2 Theoretical analysis: the equivalence between RDeepSense and statistical models Uncertainty estimations are usually inferred by a statistical model, such as a gaussian process [45] and a graphical model [32].", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "[17] in Section 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "Gaussian process is a powerful statistical tool that allows us to model distribution over functions [45].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For an L-layer fully-connected neural network, we can feed the output of one Gaussian process to the covariance of the next as a deep Gaussian process model [12].", "startOffset": 157, "endOffset": 161}, {"referenceID": 4, "context": "Besides, due to the mode matching nature of KL divergence, the variational approximating usually generates a highly underestimated predictive uncertainty [6], which is also veri\u0080ed in our", "startOffset": 154, "endOffset": 157}, {"referenceID": 4, "context": "Variational Bayesian used in MCDrop is known to provide underestimated posterior uncertainty, because optimizing the KL divergence will generate a low-variance estimation to a single mode of true posterior distribution [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 15, "context": "\u008ce \u0080rst integral in (16) can be approximated with Monte Carlo integration and the second term can be approximated according to MCDrop [17],", "startOffset": 134, "endOffset": 138}, {"referenceID": 44, "context": "Fortunately, there is a simple yet e\u0082ective recipe proposed by the dropout operation that can e\u0082ectively approximate the expected output value instead of using Monte Carlo estimation [46].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "Waveform signals were sampled at the frequency of 125 Hz with at least 8 bit accuracy [21].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "for the non-invasive cu\u0082less blood pressure monitoring task [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "constructed the dataset by the continuous acquisition the signals of a sensor array with 16 chemical sensors for a duration of about 12 hours without interruption with the sampling frequency of 100 Hz [15].", "startOffset": 201, "endOffset": 205}, {"referenceID": 45, "context": "\u008ce dataset contains 9 users, 6 activities (biking, si\u008aing, standing, walking, climbStairup, and climbStair-down), and 6 types of mobile devices [47] 4.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "1 [17].", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "\u2022 SSP: \u008cis algorithm trains the neural network with proper scoring methods and uses the ensemble method [34].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "It is possible to extend the two-step solution introduced in Section 3 to convolutional and recurrent neural networks by replacing the original dropout operation with convolutional dropout [16] and recurrent dropout [18].", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "It is possible to extend the two-step solution introduced in Section 3 to convolutional and recurrent neural networks by replacing the original dropout operation with convolutional dropout [16] and recurrent dropout [18].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "convolutional neural networks and recurrent neural networks into Bayesian neural networks [16, 18], but additional e\u0082orts are needed to 1) theoretically prove that the extended two-step solution can equate an arbitrary neural network with a statistical model, and 2) empirically show that the extended two-step solution can provide high-quality uncertainty estimations on the real datasets.", "startOffset": 90, "endOffset": 98}, {"referenceID": 16, "context": "convolutional neural networks and recurrent neural networks into Bayesian neural networks [16, 18], but additional e\u0082orts are needed to 1) theoretically prove that the extended two-step solution can equate an arbitrary neural network with a statistical model, and 2) empirically show that the extended two-step solution can provide high-quality uncertainty estimations on the real datasets.", "startOffset": 90, "endOffset": 98}, {"referenceID": 15, "context": "[17], although these models tend to underestimate the true uncertainties, they can provide uncertainty estimations during model inference.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Recent advances in deep learning have led various applications to unprecedented achievements, which could potentially bring higher intelligence to a broad spectrum of mobile and ubiquitous applications. Although existing studies have demonstrated the e\u0082ectiveness and feasibility of running deep neural network inference operations on mobile and embedded devices, they overlooked the reliability of mobile computing models. Reliability measurements such as predictive uncertainty estimations are key factors for improving the decision accuracy and user experience. In this work, we propose RDeepSense, the \u0080rst deep learning model that provides well-calibrated uncertainty estimations for resource-constrained mobile and embedded devices. RDeepSense enables the predictive uncertainty by adopting a tunable proper scoring rule as the training criterion and dropout as the implicit Bayesian approximation, which theoretically proves its correctness. To reduce the computational complexity, RDeepSense employs e\u0081cient dropout and predictive distribution estimation instead of model ensemble or sampling-based method for inference operations. We evaluate RDeepSense with four mobile sensing applications using Intel Edison devices. Results show that RDeepSense can reduce around 90% of the energy consumption while producing superior uncertainty estimations and preserving at least the same model accuracy compared with other state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}