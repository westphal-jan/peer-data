{"id": "1709.05976", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Leveraging Distributional Semantics for Multi-Label Learning", "abstract": "we are present importantly a vastly novel and scalable label embedding framework for large - scale electronic multi - label learning a. k. ~ a exmlds ( comprising extreme mixed multi - label learning using formal distributional semantics ). accelerating our hybrid approach immediately draws inspiration from conceptual ideas ultimately rooted firmly in distributional sequential semantics, specifically the skip gram negative sampling ( sgns ) approach, widely widely used to learn discrete word sequence embeddings for natural language constraint processing tasks. learning such embeddings can be routinely reduced to a certain matrix factorization. assessing our approach is novel secondly in asserting that \" it highlights interesting connections between label embedding techniques methods which used for multi - screen label machine learning comprehension and abstract paragraph / box document embedding methods both commonly used for learning representations of text data. although the framework can also equally be easily adaptive extended significantly to incorporate auxiliary information systems such as deep label - label correlations ; this capability is amongst crucial means especially determining when why there are are a lot of often missing hidden labels defined in the training objective data. we both demonstrate the effectiveness benefit of our initial approach through considering an altogether extensive set of integrated experiments going on preparing a variety presented of specialized benchmark datasets, observe and show fact that the proposed learning methods methods perform favorably differently compared to several baselines and state - of - pure the - public art technique methods for large - population scale automated multi - label learning.", "histories": [["v1", "Mon, 18 Sep 2017 14:34:16 GMT  (26kb)", "http://arxiv.org/abs/1709.05976v1", "9 Pages, 0 Figures"]], "COMMENTS": "9 Pages, 0 Figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["rahul wadbude", "vivek gupta", "piyush rai", "nagarajan natarajan", "harish karnick"], "accepted": false, "id": "1709.05976"}, "pdf": {"name": "1709.05976.pdf", "metadata": {"source": "CRF", "title": "Leveraging Distributional Semantics for Multi-Label Learning", "authors": ["Rahul Wadbude", "Vivek Gupta", "Piyush Rai", "Nagarajan Natarajan", "Harish Karnick"], "emails": ["warahul@iitk.ac.in", "t-vigu@microsoft.com", "piyush@iitk.ac.in", "t-nanata@microsoft.com", "hk@iitk.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n05 97\n6v 1\n[ cs\n.L G\n] 1\n8 Se\np 20"}, {"heading": "Introduction", "text": "Modern data generated in various domains are increasingly \"multi-label\" in nature; images (e.g. Instagram) and documents (e.g. Wikipedia) are often identified with multiple tags, online advertisers often associate multiple search keywords with ads, and so on. Multi-label learning is the problem of learning to assign multiple labels to instances, and has received a great deal of attention over the last few years; especially so, in the context of learning with millions of labels, now popularly known as extreme multi-label learning (Jain, Prabhu, and Varma 2016; Bhatia et al. 2015; Babbar and Sch\u00f6lkopf 2017; Prabhu and Varma 2014). The key challenges in multi-label learning, especially when there are millions of labels, include a) the data may have a large fraction of labels missing, and b) the labels are often heavy-tailed (Bhatia et al. 2015; Jain, Prabhu, and Varma 2016) and predicting labels in the tail becomes significantly hard for lack of training data. For these reasons, and the sheer scale of\nSubmitted to Thirty-Second Association for the Advancement of Artificial Intelligence, AAAI 2018 (www.aaai.org). Do not distribute.\ndata, traditional multi-label classifiers are rendered impracticable. State-of-the-art approaches to extreme multi-label learning fall broadly under two classes: 1) embedding based methods, e.g. LEML (Yu et al. 2014), WSABIE (Weston, Bengio, and Usunier 2010), SLEEC (Bhatia et al. 2015), PD-SPARSE (Yen et al. 2016)), and 2) tree-based methods (Prabhu and Varma 2014; Jain, Prabhu, and Varma 2016). The first class of approaches are generally scalable and work by embedding the highdimensional label vectors to a lower-dimensional space and learning a regressor in that space. In most cases, these methods rely on a key assumption that the binary label matrix is low rank and consequently the label vectors can be embedded into a lower-dimensional space. At the time of prediction, a decompression matrix is used to retrieve the original label vector from the low-dimensional embeddings. As corroborated by recent empirical evidence (Bhatia et al. 2015; Jain, Prabhu, and Varma 2016), approaches based on standard structural assumptions such as low-rank label matrix fail and perform poorly on the tail. The second class of methods (tree-based) methods for multi-label learning try to move away from rigid structural assumptions (Prabhu and Varma 2014; Jain, Prabhu, and Varma 2016), and have been demonstrated to work very well especially on the tail labels.\nIn this work, we propose an embedding based approach, closely following the framework of SLEEC (Bhatia et al. 2015), that leverages a word vector embedding technique (Mikolov et al. 2013) which has found resounding success in natural language processing tasks. Unlike other embedding based methods, SLEEC has the ability to learn non-linear embeddings by aiming to preserve only local structures and example neighborhoods. We show that by learning rich word2vec style embedding for instances (and labels), we can a) achieve competitive multi-label prediction accuracies, and often improve over the performance of the state-of-the-art embedding approach SLEEC and b) cope with missing labels, by incorporating auxiliary information in the form of labellabel co-occurrences, which most of the state-of-the-art methods can not. Furthermore, our learning algorithm admits significantly faster implementation compared to other embedding based approaches. The distinguishing aspect of our work is that it draws inspiration from dis-\ntributional semantics approaches (Mikolov et al. 2013; Le and Mikolov 2014), widely used for learning non-linear representations of text data for natural language processing tasks such as understand word and document semantics, classifying documents, etc. Our main contributions are:\n1. We leverage an interesting connection between the problem of learning distributional semantics in text data analysis and the multi-label learning problem. To the best of our knowledge, this is a novel application.\n2. The proposed objectives for learning embeddings can be solved efficiently and scalably; the learning reduces to a certain matrix factorization problem.\n3. Unlike existing multi-label learning methods, our method can also leverage label co-occurrence information while learning the embeddings; this is especially appealing when a large fraction of labels are missing in the label matrix.\n4. We show improvement in training time as compared to state-of-art label embedding methods for extreme multilabel learning, while being competitive in terms of label prediction accuracies; we demonstrate scalability and prediction performance on several state-of-the-art moderateto-large scale multi-label benchmark datasets.\nThe outline of the paper is as follows. We begin by setting up notation, background and describing the problem formulation in Section . In Section , we present our training algorithms based on learning word embeddings for understanding word and document semantics. Here we propose two objectives, where we progressively incorporate auxiliary information viz. label correlations. We present comprehensive experimental evaluation in Section , and conclude."}, {"heading": "Problem Formulation and Background", "text": "In the standard multi-label learning formulation, the learning algorithm is given a set of training instances {x1, x2, . . . , xn}, where xi \u2208 Rd and the associated label vectors {y1, y2, . . . , yn}, where yi \u2208 {0, 1}\nL. In real-world multi-label learning data sets, one does not usually observe irrelevant labels; here yij = 1 indicates that the jth label is relevant for instance i but yij = 0 indicates that the label is missing or irrelevant. Let Y \u2208 {0, 1}n\u00d7L denote the matrix of label vectors. In addition, we may have access to labellabel co-occurrence information, denoted by C \u2208 ZL\u00d7L+ (e.g., number of times a pair of labels co-occur in some external source such as the Wikipedia corpus). The goal in multilabel learning is to learn a vector-valued function f : x 7\u2192 s, where s \u2208 RL scores the labels.\nEmbedding-based approaches typically model f as a com-\nposite function h(g(x)) where, g : Rd \u2192 Rd \u2032 and h : Rd \u2032\n\u2192 RL. For example, assuming both g and h as linear transformations, one obtains the formulation proposed by (Yu et al. 2014). The functions g and h can be learnt using training instances or label vectors, or both. More recently, non-linear embedding methods have been shown to help improve multi-label prediction accuracies\nsignificantly. In this work, we follow the framework of (Bhatia et al. 2015), where g is a linear transformation, but h is non-linear, and in particular, based on k-nearest neighbors in the embedded feature space.\nIn SLEEC, the function g : Rd \u2192 Rd \u2032 is given by\ng(x) = V x where V \u2208 Rd \u2032\u00d7d. The function h : Rd \u2032 \u2192 RL is defined as:\nh ( z; {zi, yi} n i=1 ) = 1\n|Nk|\n\u2211\ni\u2208Nk\nyi, (1)\nwhere zi = g(xi) andNk denotes the k\u2212nearest neighbor training instances of z in the embedded space. Our algorithm for predicting the labels of a new instance is identical to that of SLEEC and is presented for convenience in Algorithm 1. Note that, for speeding up predictions, the algorithm relies on clustering the training instances xi; for each cluster of instances Q\u03c4 , a different linear embedding g\u03c4 , denoted by V \u03c4 , is learnt.\nAlgorithm 1 Prediction Algorithm\nInput: Test point: x, no. of nearest neighbors k, no. of desired labels p. 1. Q\u03c4 : partition closest to x. 2. z \u2190 V \u03c4x. 3. Nk \u2190 k nearest neighbors of z in the embedded instances of Q\u03c4 . 4. s = h(z; {zi, yi}i\u2208Q\u03c4 ) where h is defined in (1). return top p scoring labels according to s.\nIn this work, we focus on learning algorithms for the functions g and h, inspired by their successes in natural language processing in the context of learning distributional semantics (Mikolov et al. 2013; Levy and Goldberg 2014). In particular, we use techniques for inferring word-vector embeddings for learning the function h using a) training label vectors yi, and b) label-label correlations C \u2208 R\nL\u00d7L. Word embeddings are desired in natural language processing in order to understand semantic relationships between words, classifying text documents, etc. Given a text corpus consisting of a collection of documents, the goal is to embed each word in some space such that words appearing in similar contexts (i.e. adjacenct words in documents) should be closer in the space, than those that do not. In particular, we use the word2vec embedding approach (Mikolov et al. 2013) to learn an embedding of instances, using their label vectors y1, y2, . . . , yn. SLEEC also uses nearest neighbors in the space of label vectors yi in order to learn the embeddings. However, we show in experiments that word2vec based embeddings are richer and help improve the prediction performance significantly, especially when there is a lot of missing labels. In the subsequent section, we discuss our algorithms for learning the embeddings and the training phase of multi-label learning.\nLearning Instance and Label Embeddings\nThere are multiple algorithms in the literature for learning word embeddings (Mikolov et al. 2013;\nPennington, Socher, and Manning 2014). In this work, we use the Skip Gram Negative Sampling (SGNS) technique, for two reasons a) it is shown to be competitive in natural language processing tasks, and more importantly b) it presents a unique advantage in terms of scalability, which we will address shortly after discussing the technique.\nSkip Gram Negative Sampling. In SGNS, the goal is to learn an embedding z \u2208 Rd \u2032\nfor each word w in the vocabulary. To do so, words are considered in the contexts in which they occur; context c is typically defined as a fixed size window of words around an occurrence of the word. The goal is to learn z such that the words in similar contexts are closer to each other in the embedded space. Let w\u2032 \u2208 c denote a word in the context c of word w. Then, the likelihood of observing the pair (w,w\u2032) in the data is modeled as a sigmoid of their inner product similarity:\nP (Observing (w,w\u2032)) = \u03c3(\u3008zw, zw\u2032\u3009) = 1\n1 + exp(\u3008\u2212zw, zw\u2032\u3009) .\nTo promote dissimilar words to be further apart, negative sampling is used, wherein randomly sampled negative examples (w,w\u2032\u2032) are used. Overall objective favors zw, zw\u2032 , zw\u2032\u2032 that maximize the log likelihood of observing (w.w\u2032), for w\u2032 \u2208 c, and the log likelihood of P (not observing (w,w\u2032\u2032)) = 1\u2212P (Observing (w,w\u2032\u2032)) for randomly sampled negative instances. Typically, n\u2212 negative examples are sampled per observed example, and the resulting SGNS objective is given by:\nmax z\n\u2211\nw\n( \u2211\nw\u2032:(w\u2032,w)\nlog ( \u03c3(\u3008zw, zw\u2032\u3009) ) +\nn\u2212\n#w\n\u2211\nw\u2032\u2032\nlog ( \u03c3(\u2212\u3008zw, zw\u2032\u2032\u3009) )) ,\n(2)\nwhere#w denotes the total number of words in the vocabulary, and the negative instances are sampled uniformly over the vocabulary."}, {"heading": "Embedding label vectors", "text": "We now derive the analogous embedding technique for multi-label learning. A simple model is to treat each instance as a \"word\"; define the \"context\" as k-nearest neighbors of a given instance in the space formed by the training label vectors yi, with cosine similarity as the metric. We then arrive at an objective identical to (2) for learning embeddings z1, z2, . . . , zn for instances x1, x2, . . . , xn respectively:\nmax z1,z2,...,zn\nn \u2211\ni=1\n(\n\u2211\nj:Nk(yi)\nlog ( \u03c3(\u3008zi, zj\u3009) ) +\nn\u2212\nn\n\u2211\nj\u2032\nlog ( \u03c3(\u2212\u3008zi, zj\u2032\u3009) )\n)\n,\n(3)\nNote that Nk(yi) denotes the k-nearest neighborhood of ith instance in the space of label vectors 1 or instance embedding. After learning label embeddings zi, we can learn the\n1Alternately, one can consider the neighborhood in the ddimensional feature space xi; however, we perform clustering in\nfunction g : x \u2192 z by regressing x onto z, as in SLEEC. Solving (3) for zi using standard word2vec implementations can be computationally expensive, as it requires training multiple-layer neural networks. Fortunately, the learning can be significantly sped up using the key observation by (Levy and Goldberg 2014). (Levy and Goldberg 2014) showed that solving SGNS objective is equivalent to matrix factorization of the shifted positive point-wise mutual information (SPPMI) matrix defined as follows. LetMij = \u3008yi, yj\u3009.\nPMIij(M) = log\n( Mij \u2217 |M |\u2211\nk M(i,k) \u2217 \u2211 k M(k,j)\n)\nSPPMIij(M) = max(PMIij(M)\u2212 log(k), 0) (4)\nHere,PMI is the point-wise mutual informationmatrix ofM and |M | denotes the sum of all elements in M . Solving the problem (3) reduces to factorizing the shifted PPMI matrix M . Finally, we use ADMM (Boyd et al. 2011) to learn the regressors V over the embedding space formed by zi. Overall training algorithm is presented in 2.\nAlgorithm 2 Learning embeddings via SPPMI factorization (EXMLDS1).\nInput. Training data (xi, yi), i = 1, 2, . . . , n. 1. Compute M\u0302 := SPPMI(M) in (4), where Mij = \u3008yi, yj\u3009. 2. Let U, S, V = svd(M\u0302), and preserve top d\u2032 singular values and singular vectors. 3. Compute the embedding matrix Z = US0.5, where Z \u2208 Rn\u00d7d \u2032\n, where ith row gives zi 4. Learn V s.t. XV T = Z using ADMM (Boyd et al. 2011), where X is the matrix with xi as rows. return V, Z\nWe refer to Algorithm 2 based on fast PPMI matrix factorization for learning label vector embeddings as EXMLDS1. We can also optimize the objective 3 using a neural network model (Mikolov et al. 2013); we refer to this word2vecmethod for learning embeddings in Algorithm 2 as EXMLDS2."}, {"heading": "Using label correlations", "text": "In various practical natural language processing applications, superior performance is obtained using joint models for learning embeddings of text documents as well as individual words in a corpus (Dai, Olah, and Le 2015). For example, in PV-DBoW (Dai, Olah, and Le 2015), the objective while learning embeddings is to maximize similarity between embedded documents and words that compose the documents. Negative sampling is also included, where the objective is to minimize the similarity between the document\nthis space for speed up, and therefore the label vectors are likely to preserve more discriminative information within clusters.\nembeddings and the embeddings of high frequency words. In multi-label learning, we want to learn the embeddings of labels as well as instances jointly. Here, we think of labels as individual words, whereas label vectors (or instances with the corresponding label vectors) as paragraphs or documents. As alluded to in the beginning of Section , in many real world problems, we may also have auxiliary label correlation information, such as label-label co-occurrence. We can easily incorporate such information in the joint modeling approach outlined above. To this end, we propose the following objective that incorporates information from both label vectors as well as label correlations matrix:\nmax z,z\u0304\nOz,z\u0304 = \u00b51O 1 z\u0304 + \u00b52O 2 z + \u00b53O 3 {z,z\u0304} (5)\nO 1 z\u0304 =\nL \u2211\ni=1\n(\n\u2211\nj:Nk(C(i,:))\nlog ( \u03c3(\u3008z\u0304i, z\u0304j\u3009) ) +\nn1\u2212\nL\n\u2211\nj\u2032\nlog ( \u03c3(\u2212\u3008z\u0304i, z\u0304j\u2032\u3009) )\n)\n,\n(6)\nO 2 z =\nn \u2211\ni=1\n(\n\u2211\nj:Nk(M(i,:))\nlog ( \u03c3(\u3008zi, zj\u3009) ) +\nn2\u2212\nn\n\u2211\nj\u2032\nlog ( \u03c3(\u2212\u3008zi, zj\u2032\u3009) )\n)\n,\n(7)\nO 3 {z,z\u0304} =\nL \u2211\ni=1\n(\n\u2211\nj:yij=1\nlog ( \u03c3(\u3008zi, z\u0304j\u3009) ) +\nn3\u2212\nL\n\u2211\nj\u2032\nlog ( \u03c3(\u2212\u3008zi, z\u0304j\u2032\u3009) )\n)\n(8)\nHere, zi, i = 1, 2, . . . , n denote embeddings of instances while z\u0304i, i = 1, 2, . . . , L denote embeddings of labels. Nk(M(i, :)) denotes the k-nearest neighborhood of ith instance in the space of label vectors.Nk(C(i, :)) denotes the k-nearest neighborhood of ith label in the space of labels. Here, M defines instance-instance correlation i.e. Mij = \u3008yi, yj\u3009 and C is the label-label correlation matrix. Clearly, (7) above is identical to (3). O1z\u0304 tries to embed labels z\u0304i in a vector space, where correlated labels are closer; O2z tries to embed instances zi in such a vector space, where correlated instances are closer; and finally,O3{z,\u0304z} tries to embed labels and instances in a common space where labels occurring in the ith instance are closer to the embedded instance. Overall the combined objective O{z,\u0304z} promotes learning a common embedding space where correlated labels, correlated instances and observed labels for a given instance occur closely. Here \u00b51,\u00b52 and \u00b53 are hyper-parameters to weight the contributions from each type of correlation. n1\u2212 negative examples are sampled per observed label, n2\u2212 negative examples are sampled per observed instance in context of labels and n3\u2212 negative examples are sampled per observed instance in context of instances. Hence, the proposed objective efficiently utilizes label-label correlations to help\nimprove embedding and, importantly, to cope with missing labels. The complete training procedure using SPPMI factorization is presented in Algorithm 3. Note that we can use the same arguments given by (Levy and Goldberg 2014) to show that the proposed combined objective (5) is solved by SPPMI factorization of the joint matrixA given in Step 1 of Algorithm 3.\nAlgorithm 3 Learning joint label and instance embeddings via SPPMI factorization (EXMLDS3).\nInput. Training data (xi, yi), i = 1, 2, . . . , n and C (labellabel correlation matrix) and objective weighting \u00b51,\u00b52 and \u00b53. 1. Compute A\u0302 := SPPMI(A) in (4); write\nA =\n( \u00b52M \u00b53Y\n\u00b53Y T \u00b51C\n) ,\nMij = \u3008yi, yj\u3009, Y is label matrix with yi as rows. 2. Let U, S, V = svd(A\u0302), and preserve top d\u2032 singular values and singular vectors. 3. Compute the embedding matrix Z = US0.5; write\nZ = ( Z1 Z2 ) ,\nwhere rows of Z1 \u2208 Rn\u00d7d \u2032 give instance embedding and rows of Z2 \u2208 RL\u00d7d \u2032\ngive label embedding. 4. Learn V s.t. XV T = Z1 using ADMM (Boyd et al. 2011), where X is the matrix with xi as rows. return V, Z\nAlgorithm 4 Prediction Algorithm with Label Correlations (EXMLDS3 prediction).\nInput: Test point: x, no. of nearest neighbors k, no. of desired labels p, V , embeddings Z1 and Z2. 1. Use Algorithm 1 (Step 3) with input Z1, k, p to get score s1. 3. Get score s2 = Z2V x 4. Get final score s = s1\u2016s1\u2016 + s2 \u2016s2\u2016 . return top p scoring labels according to s.\nAt test time, given a new data point we could use the Algorithm 1 to get top p labels. Alternately, we propose to use Algorithm 4 that also incorporates similarity with label embeddings Z2 along with Z1 during prediction, especially when there are very few training labels to learn from. In practice, we find this prediction approach useful. Note the zi corresponds to the ith row of Z1, and z\u0304j corresponds to the j th row of Z2. We refer the Algorithm 3 based on the combined learning objective (5) as EXMLDS3.\nExperiments\nWe conduct experiments on commonly used benchmark datasets from the extreme multi-label classification repos-\nitory provided by the authors of (Prabhu and Varma 2014; Bhatia et al. 2015) 2; these datasets are pre-processed, and have prescribed train-test splits. Statistics of the datasets used in experiments is shown in Table 1. We use the standard, practically relevant, precision at k (denoted by Prec@k) as the evaluationmetric of the prediction performance. Prec@k denotes the number of correct labels in the top k predictions. We run our code and all other baselines on a Linux machine with 40 cores and 128 GB RAM. We implemented our prediction Algorithms 1 and 4 in MATLAB. Learning Algorithms 2 and 3 are implemented parlty in Python and partly in MATLAB. The source code will be made available later. We evaluate three models (a) EXMLDS1 i.e. Algorithm 2 based on fast PPMI matrix factorization for learning label embeddings as described in Section , (b) EXMLDS2 based on optimizing the objective (3) as described in section , using neural network (Mikolov et al. 2013) (c) EXMLDS3 i.e. Algorithm 3 based on combined learning objective (5).\nCompared methods. We compare our algorithmswith the following baselines.\n1. SLEEC (Bhatia et al. 2015), which was shown to outperform all other embedding baselines on the benchmark datasets.\n2. LEML (Yu et al. 2014), an embedding based method. This method also facilitates incorporating label information (though not proposed in the original paper); we use the code given by the authors of LEML which uses item features3. We refer to the latter method that uses label correlations as LEML-IMC.\n3. FASTXML (Prabhu and Varma 2014), a tree-based method.\n4. PD-SPARSE (Yen et al. 2016), recently proposed embedding based method\n5. PFASTREXML (Jain, Prabhu, and Varma 2016) is an extension of FASTXML; it was shown to outperform all other tree-based baselines on benchmark datasets.\n6. DISMEC (Babbar and Sch\u00f6lkopf 2017) is recently proposed scalable implementation of the ONE-VS-ALL method.\n7. DXML (Zhang et al. 2017) is a recent deep learning solution for multi-label learning\n8. ONE-VS-ALL (Zhang et al. 2017) is traditional one vs all multi-label classifier\nWe report all baseline results from the the extreme classification repository. 4, where they have been curated; note that all the relevant research work use the same train-test split for benchmarking.\nHyperparameters. We use the same embedding dimensionality, preserve the same number of nearest neighbors for learning embeddings as well as at prediction\n2http://manikvarma.org/downloads/XC/XMLRepository.html 3https://goo.gl/jdGbDPl 4http://manikvarma.org/downloads/XC/XMLRepository.html\ntime, and the same number of data partitions used in SLEEC (Bhatia et al. 2015) for our method EXMLDS1and EXMLDS2. For small datasets, we fix negative sample size to 15 and number of iterations to 35 during neural network training, tuned based on a separate validation set. For large datasets (4 and 5 in Table 1), we fix negative sample size to 2 and number of iterations to 5, tuned on a validation set. In EXMLDS3, the parameters (negative sampling) are set identical to EXMLDS1. For baselines, we either report results from the respective publications or used the best hyperparameters reported by the authors in our experiments, as needed.\nPerformance evaluation. The performance of the compared methods are reported in Table 3. Performances of the proposed methods EXMLDS1 and EXMLDS2 are found to be similar in our experiments, as they optimize the same objective 3; so we include only the results of EXMLDS1 in the Table. We see that the proposed methods achieve competitive prediction performance among the state-of-the-art embedding and tree-based approaches. In particular, note that on Medialmill and Delicious-200K datasets our method achieves the best performance.\nTraining time. Objective 3 can be trained using a neural network, as described in (Mikolov et al. 2013). For training the neural network model, we give as input the k-nearest neighbor instance pairs for each training instance i, where the neighborhood is computed in the space of the label vectors yi. We use the Google word2vec code\n5 for training. We parallelize the training on 40 cores Linux machine for speed-up. Recall that we call this method EXMLDS2. We compare the training time with our method EXMLDS1, which uses a fast matrix factorization approach for learning embeddings. Algorithm 2 involves a single SVD as opposed to iterative SVP used by SLEEC and therefore it is significantly faster. We present training time measurements in Table 2. As anticipated, we observe that EXMLDS2 which uses neural networks is slower than EXMLDS1 (with 40 cores). Also, among the smaller datasets, EXMLDS1 trains 14x faster compared to SLEECon Bibtex dataset. In the large dataset, Delicious-200K, EXMLDS1 trains 5x faster than"}, {"heading": "SLEEC.", "text": "Coping with missing labels. In many real-world scenarios, data is plagued with lots of missing labels. A desirable property of multi-label learning methods is to cope with missing labels, and yield good prediction performance with very few training labels. In the dearth of training labels, auxiliary information such as label correlations can come in handy.As described in Section , our method EXMLDS3 can learn from additional information. The benchmark datasets, however, do not come with auxiliary information. To simulate this setting, we hide 80% non-zero entries of the training label matrix, and reveal the 20% training labels to learning algorithms. As a proxy for label correlations matrix C, we simply use the label-label co-occurrence from the 100% training data, i.e. C = Y TY where Y denotes the full training matrix. We give higher weight \u00b51 to O\n1 during training in Algorithm 3. For prediction, We use Algorithm 4 which\n5https://code.google.com/archive/p/word2vec/\ntakes missing labels into account. We compare the performance of EXMLDS3with SLEEC, LEML and LEML-IMCin Table 4. Note that while SLEEC and LEML methods do not incorporate such auxiliary information, LEML-IMC does. In particular, we use the spectral embedding based features i.e. SVD of Y Y T and take all the singular vectors corresponding to non-zero singular values as label features. It can be observed that on all three datasets, EXMLDS3 performs significantly better by huge margins. In particular, the lift over LEML-IMC is significant, even though both the methods use the same information. This serves to demonstrate the strength of our approach.\nConclusions and Future Work\nWe proposed a novel objective for learning label embeddings for multi-label classification, that leverages word2vec embedding technique; furthermore, the proposed formulation can be optimized efficiently by SPPMI matrix factorization. Through comprehensive experiments, we showed that the proposedmethod is competitive compared to state-of-the-art multi-label learning methods in terms of prediction accuracies. We proposed a novel objective that incorporates side information, that is particularly effective in handling missing labels. One promising extension of our objective is to do joint learning of embeddings Z and regressor V using gradient decent as shown in the Supplementary Material.\nReferences\nand Eckstein, J. 2011. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R\u00a9 in Machine Learning 3(1):1\u2013 122.\n[Dai, Olah, and Le 2015] Dai, A. M.; Olah, C.; and Le, Q. V. 2015. Document embedding with paragraph vectors. arXiv preprint arXiv:1507.07998.\n[Jain, Prabhu, and Varma 2016] Jain, H.; Prabhu, Y.; and Varma, M. 2016. Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 935\u2013944. ACM.\n[Katakis, Tsoumakas, and Vlahavas 2008] Katakis, I.; Tsoumakas, G.; and Vlahavas, I. 2008. Multilabel text classification for automated tag suggestion. ECML PKDD discovery challenge 75.\n[Le and Mikolov 2014] Le, Q. V., andMikolov, T. 2014. Distributed representations of sentences and documents. In ICML, volume 14, 1188\u20131196.\n[Levy and Goldberg 2014] Levy, O., and Goldberg, Y. 2014. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, 2177\u2013 2185.\n[Lewis et al. 2004] Lewis, D. D.; Yang, Y.; Rose, T. G.; and Li, F. 2004. Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research 5(Apr):361\u2013397.\n[Loza Menc\u00eda and F\u00fcrnkranz 2008] Loza Menc\u00eda, E., and F\u00fcrnkranz, J. 2008. Efficient pairwise multilabel classification for large-scale problems in the legal domain. Machine"}, {"heading": "Learning and Knowledge Discovery in Databases 50\u201365.", "text": "[Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111\u2013 3119.\n[Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP, volume 14, 1532\u20131543.\n[Prabhu and Varma 2014] Prabhu, Y., and Varma, M. 2014. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 263\u2013272. ACM.\n[Snoek et al. 2006] Snoek, C. G.; Worring, M.; Van Gemert, J. C.; Geusebroek, J.-M.; and Smeulders, A. W. 2006. The challenge problem for automated detection of 101 semantic concepts in multimedia. In Proceedings of the 14th ACM international conference on Multimedia, 421\u2013430. ACM.\n[Tsoumakas, Katakis, and Vlahavas 2008] Tsoumakas, G.; Katakis, I.; and Vlahavas, I. 2008. Effective and efficient multilabel classification in domains with large number of labels. In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMD\u201908), 30\u201344.\n[Weston, Bengio, and Usunier 2010] Weston, J.; Bengio, S.; and Usunier, N. 2010. Large scale image annotation: learning to rank with joint word-image embeddings. Machine learning 81(1):21\u201335.\n[Yen et al. 2016] Yen, I. E.-H.; Huang, X.; Ravikumar, P.; Zhong, K.; and Dhillon, I. 2016. Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification. In International Conference on Machine Learning, 3069\u20133077.\n[Yu et al. 2014] Yu, H.-F.; Jain, P.; Kar, P.; and Dhillon, I. 2014. Large-scale multi-label learning with missing labels. In International Conference on Machine Learning, 593\u2013601.\n[Zhang et al. 2017] Zhang, W.; Wang, L.; Yan, J.; Wang, X.; and Zha, H. 2017. Deep extreme multi-label learning. arXiv preprint arXiv:1704.03718."}, {"heading": "Leveraging Distributional Semantics for Multi-Label Learning", "text": ""}, {"heading": "A1. Joint Embedding and Regression", "text": "O t+1 i = O t i + \u03b7\u2207V Oi\nGradient of objective 3 w.r.t to V i.e.\u2207V Oi is describe in detail below : Given,\nKij = \u3008zizj\u3009 = \u3008z T i zj\u3009\nzi = V xi,where V \u2208 R d\u2032\u00d7d\nObjective 3 :\nmax z1,z2,...,zn\nn\u2211\ni=1\n( \u2211\nj:Nk(yi)\nlog ( \u03c3(\u3008zi, zj\u3009) ) +\nn\u2212\nn\n\u2211\nj\u2032\nlog ( \u03c3(\u2212\u3008zi, zj\u2032 \u3009) )) ,\n(9)\nrewriting with s.t.t V andKij , we obtained\nmax V\nn\u2211\ni=1\n( \u2211\nj:Nk(yi)\nlog ( \u03c3(\u3008V xi, V xj\u3009) ) +\nn\u2212\nn\n\u2211\nj\u2032\nlog ( \u03c3(\u2212\u3008V xi, V xj\u2032\u3009) )) ,\n(10)\nmax V\nn\u2211\ni=1\n( \u2211\nj:Nk(yi)\nlog ( \u03c3(Kij) ) + n\u2212\nn\n\u2211\nj\u2032\nlog ( \u03c3(\u2212Kij\u2032 ) )) ,\nrewriting for only ith instance, we have Oi = \u2211\nj:Nk(yi)\nlog ( \u03c3(Kij) ) + n\u2212\nn\n\u2211\nj\u2032\nlog ( \u03c3(\u2212Kij\u2032 ) ) ,\n\u2207V Oi = \u2211\nj:Nk(yi)\n\u03c3(\u2212Kij)\u2207V Kij\u2212 n\u2212\nn\n\u2211\nj\u2032\n\u03c3(Kij\u2032 )\u2207V Kij\u2032\nhere,\u2207V Kij can be obtain through,\n\u2207V Kij = V (xix T j + xjx T i ) = zi.x T j + zj .x T i\nSometime cosine similarity perform better then dot product because of scale invariant, in that case the gradient would modify to :\nKij = \u3008 zi\n\u2016zi\u2016\nzj\n\u2016zj\u2016 \u3009 =\n\u3008zTi zj\u3009\n\u2016zi\u2016\u2016zj\u2016 (11)\n\u2207V \u3008zizj\u3009 = \u2207V (V xi)zj +\u2207V (V xj)zi\n= \u3008zix T j \u3009+ \u3008zjx T i \u3009 = V (\u3008xix T j + xjx T i \u3009)\n(12)\n\u2207V 1\n\u2016zi\u2016 = \u2207V ziz\nT i\n\u22121 2 = \u22121\n2 ziz\nT i \u22123 2 \u2207V ziz T i =\n\u22121\n2 ziz\nT i \u22123 2 xiz T i\n(13)\n\u2207V 1\n\u2016zj\u2016 = \u2207V zjz\nT j\n\u22121 2 = \u22121\n2 zjz\nT j \u22123 2 \u2207V zjz T j =\n\u22121\n2 zjz\nT j \u22123 2 xjz T j\n(14)\nLet,\na = zTi zj , b = 1\n\u2016zi\u2016 , c =\n1\n\u2016zj\u2016\n\u2207V Kij = \u2212ab 3czi(xi) T \u2212 abc3zj(xj) T + bc(zix T j + zjx T i )\nGradient update after tth iteration for ith instance,\nO t+1 i = O t i + \u03b7\u2207V Oi"}, {"heading": "A2. SGNS Objective as Implicit SPPMI factorization", "text": "The SGNS (Mikolov et al. 2013) objective is as follows:\nOi = \u2211\nj\u2208Si\nlog(\u03c3(Kij)) +\nM\u2211\nk\u223cPD\nEk\u223cPD [log(\u03c3(\u2212Kik))]\nwhere, PD = (#k)0.75\n#D ,D is collection of all word-context\npairs and Kij represent dot-product similarity between the embeddings of a given word (i) and context (j). Here, #k represent total number of word-context pairs with context (k).\nO{i,j} = log(\u03c3(Kij)) +\nM |S|\u2211\nk\u223cPD\nEk\u223cPD [log(\u03c3(\u2212Kik))]\nEk\u223cPD [log(\u03c3(\u2212Kik))] = \u2211\nk\u223cPD\n(#k)0.75\n#D log(\u03c3(\u2212Kik))\nEk\u223cPD [log(\u03c3(\u2212Kik))] = (#j)0.75\n#D log(\u03c3(\u2212Kij))\n+ \u2211\nk\u223cPD&k 6=j\n(#k)0.75\n#D log(\u03c3(\u2212Kik))\nTherefore,\nEj\u223cPD [log(\u03c3(\u2212Kij))] = (#j)0.75\n#D log(\u03c3(\u2212Kij))\nO{i,j} = log(\u03c3(Kij)) + M\n|S|\n(#j)0.75\n#D log(\u03c3(\u2212Kij))\nLet \u03b3Kij = x, then\n\u2207xO{i,j} = \u03c3(\u2212x)\u2212 M\n|S|\n(#j)0.75\n#D \u03c3(x)\nequating\u2207xJ{i,j} to 0, we get :\ne2x \u2212\n  1\nM |S|\n(#j)0.75\n#D\n\u2212 1   ex \u2212   1\nM |S|\n(#j)0.75\n#D\n  = 0\nIf we define y = ex , this equation becomes a quadratic equation of y, which has two solutions, y =- 1 (which is invalid given the definition of y) and\ny = 1\nM |S|\n(#j)0.75\n#D\n= #D \u2217 |S|\nM \u2217 (#j)0.75\nSubstituting y with ex and x withKij reveals :\nKij = log\n( #D \u2217 |S|\nM \u2217 (#j)0.75\n)\nHere |S| = #(i, j) andM = \u00b5#(i) i.e. \u00b5 proportion of total number of times label vector (i) appear with others.\nKij = log\n( #(i, j)(#D)\n#(i)(#j)0.75\n) \u2212 log(\u00b5)\nKij = log\n( P (i, j)\nP (i)P (j)\n) \u2212 log(\u00b5)\nHere P(i,j),P(i) and P(j) represent probability of cooccurrences of {i, j} , occurrence of i and occurrence of j respectively, Therefore,\nKij = PMIij \u2212 log(\u00b5) = log(P (i|j))\u2212 log(\u00b5)\nNote that PMI+ is inconsistent, therefore we used the sparse and consistent positive PMI(PPMI) metric, in which all negative values and nan are replaced by 0:\nPPMIij = max(PMIij , 0)\nHere, PMI is point wise mutual information and PPMI is positive point wise mutual information. Similarity of two {i, j} is more influenced by the positive neighbor they share than by the negative neighbor they share as uninformative i.e. 0 value. Hence, SGNS objective can be cast into a weighted matrix factorization problem, seeking the optimal lower ddimensional factorization of the matrix SPPMI under a metric which pays more for deviations on frequent#(i, j) pairs than deviations on infrequent ones. Using a similar derivation, it can be shown that noisecontrastive estimation (NCE) which is alternative to (SGNS) can be cast as factorization of (shifted) log-conditionalprobability matrix\nKij = log\n( #(i, j)\n(#j)\n) \u2212 log(\u00b5)"}], "references": [{"title": "Dismec: Distributed sparse machines for extreme multi-label classification", "author": ["Babbar", "R. Sch\u00f6lkopf 2017] Babbar", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Babbar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Babbar et al\\.", "year": 2017}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Bhatia"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bhatia,? \\Q2015\\E", "shortCiteRegEx": "Bhatia", "year": 2015}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["J. Eckstein"], "venue": "Foundations and Trends R", "citeRegEx": "Eckstein,? \\Q2011\\E", "shortCiteRegEx": "Eckstein", "year": 2011}, {"title": "Document embedding with paragraph vectors", "author": ["Olah Dai", "A.M. Le 2015] Dai", "C. Olah", "Q.V. Le"], "venue": "arXiv preprint arXiv:1507.07998", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications", "author": ["Prabhu Jain", "H. Varma 2016] Jain", "Y. Prabhu", "M. Varma"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Tsoumakas Katakis", "I. Vlahavas 2008] Katakis", "G. Tsoumakas", "I. Vlahavas"], "venue": "ECML PKDD discovery challenge", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Q.V. Mikolov 2014] Le", "T. andMikolov"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "O. Goldberg 2014] Levy", "Y. Goldberg"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis"], "venue": "Journal of machine learning research 5(Apr):361\u2013397", "citeRegEx": "Lewis,? \\Q2004\\E", "shortCiteRegEx": "Lewis", "year": 2004}, {"title": "Efficient pairwise multilabel classification for large-scale problems in the legal domain. Machine Learning and Knowledge Discovery in Databases 50\u201365", "author": ["Loza Menc\u00eda", "E. F\u00fcrnkranz 2008] Loza Menc\u00eda", "J. F\u00fcrnkranz"], "venue": null, "citeRegEx": "Menc\u00eda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Menc\u00eda et al\\.", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Prabhu", "Y. Varma 2014] Prabhu", "M. Varma"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Prabhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prabhu et al\\.", "year": 2014}, {"title": "The challenge problem for automated detection of 101 semantic concepts in multimedia", "author": ["Snoek"], "venue": "In Proceedings of the 14th ACM international conference on Multimedia,", "citeRegEx": "Snoek,? \\Q2006\\E", "shortCiteRegEx": "Snoek", "year": 2006}, {"title": "Effective and efficient multilabel classification in domains with large number of labels", "author": ["Katakis Tsoumakas", "G. Vlahavas 2008] Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data", "citeRegEx": "Tsoumakas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2008}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings. Machine learning 81(1):21\u201335", "author": ["Bengio Weston", "J. Usunier 2010] Weston", "S. Bengio", "N. Usunier"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification", "author": ["Yen"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yen,? \\Q2016\\E", "shortCiteRegEx": "Yen", "year": 2016}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yu,? \\Q2014\\E", "shortCiteRegEx": "Yu", "year": 2014}, {"title": "Deep extreme multi-label learning", "author": ["Zhang"], "venue": "arXiv preprint arXiv:1704.03718", "citeRegEx": "Zhang,? \\Q2017\\E", "shortCiteRegEx": "Zhang", "year": 2017}, {"title": "SGNS Objective as Implicit SPPMI factorization The SGNS", "author": ["\u03b7\u2207V Oi A"], "venue": "(Mikolov et al", "citeRegEx": "A2.,? \\Q2013\\E", "shortCiteRegEx": "A2.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "We present a novel and scalable label embedding framework for large-scale multi-label learning a.k.a ExMLDS (Extreme Multi-Label Learning using Distributional Semantics). Our approach draws inspiration from ideas rooted in distributional semantics, specifically the Skip Gram Negative Sampling (SGNS) approach, widely used to learn word embeddings for natural language processing tasks. Learning such embeddings can be reduced to a certain matrix factorization. Our approach is novel in that it highlights interesting connections between label embedding methods used for multi-label learning and paragraph/document embedding methods commonly used for learning representations of text data. The framework can also be easily extended to incorporate auxiliary information such as label-label correlations; this is crucial especially when there are a lot of missing labels in the training data. We demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets, and show that the proposed learning methods perform favorably compared to several baselines and state-of-the-art methods for large-scale multi-label learning.", "creator": "LaTeX with hyperref package"}}}