{"id": "0710.0485", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2007", "title": "Prediction with expert advice for the Brier game", "abstract": "in this work study collaborators show that all brier phase of prediction is almost mixable and find successively optimal acceptance rate and substitution input for generators. these inputs are perfect, but the computations are surprisingly messy.", "histories": [["v1", "Tue, 2 Oct 2007 10:08:41 GMT  (9kb)", "http://arxiv.org/abs/0710.0485v1", "10 pages"], ["v2", "Fri, 27 Jun 2008 18:45:01 GMT  (940kb)", "http://arxiv.org/abs/0710.0485v2", "34 pages, 22 figures, 2 tables. The conference version (8 pages) is published in the ICML 2008 Proceedings"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir vovk", "fedor zhdanov"], "accepted": true, "id": "0710.0485"}, "pdf": {"name": "0710.0485.pdf", "metadata": {"source": "CRF", "title": "Prediction with expert advice for the Brier game", "authors": ["Vladimir Vovk"], "emails": ["vovk@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :0\n71 0.\n04 85\nv1 [\ncs .L\nG ]\n2 O\nIn this technical report I show that the Brier game of prediction is perfectly mixable and find the optimal learning rate and substitution function for it. These results are straightforward, but the computations are surprisingly messy.\n1 Loss bound\nA game of prediction consists of three components: the observation space \u2126, the decision space \u0393, and the loss function \u03bb : \u2126 \u00d7 \u0393 \u2192 R. In this note we are interested in the following Brier game [1]: \u2126 is a finite and non-empty set, \u0393 := P(\u2126) is the set of all probability measures on \u2126, and\n\u03bb(\u03c9, \u03b3) = \u2211\no\u2208\u2126\n(\u03b3{o} \u2212 \u03b4\u03c9{o}) 2 ,\nwhere \u03b4\u03c9 \u2208 P(\u2126) is the probability measure concentrated at \u03c9: \u03b4\u03c9{\u03c9} = 1 and \u03b4\u03c9{o} = 0 for o 6= \u03c9.\nThe Brier game is being played repeatedly by a learner having access to decisions made by a pool of experts, which leads to the following prediction protocol:\nPrediction with expert advice for the Brier game\nL0 := 0. Lk0 := 0, k = 1, . . . ,K. FOR N = 1, 2, . . . :\nExpert k announces \u03b3kN \u2208 \u0393, k = 1, . . . ,K. Learner announces \u03b3N \u2208 \u0393. Reality announces \u03c9N \u2208 \u2126. LN := LN\u22121 + \u03bb(\u03c9N , \u03b3N). LkN := L k N\u22121 + \u03bb(\u03c9N , \u03b3 k N).\nEND FOR.\nAt each step of the game Learner is given K experts\u2019 advice and is required to come up with his own decision; LN is his cumulative loss over the first N steps, and LkN is the kth expert\u2019s cumulative loss over the first N steps.\nTheorem 1 Learner has a strategy ensuring\nLN \u2264 min k=1,...,K\nLkN + lnK (1)\nfor all N = 1, 2, . . . and k \u2208 {1, . . . ,K}. If a < 1, Learner does not have a strategy ensuring\nLN \u2264 min k=1,...,K\nLkN + a lnK (2)\nfor all N = 1, 2, . . . and k \u2208 {1, . . . ,K}.\n2 Proof of Theorem 1\nA vector f \u2208 R\u2126 (understood to be a function f : \u2126 \u2192 R) is a superprediction if there is \u03b3 \u2208 \u0393 such that, for all \u03c9 \u2208 \u2126, \u03bb(\u03c9, \u03b3) \u2264 f(\u03c9); the set \u03a3 of all superpredictions is the superprediction set. For each learning rate \u03b7 > 0, let \u03a6\u03b7 : R \u2126 \u2192 (0,\u221e)\u2126 be the homeomorphism defined by\n\u03a6\u03b7(f) : \u03c9 \u2208 \u2126 7\u2192 e \u2212\u03b7f(\u03c9), f \u2208 R\u2126.\nThe image \u03a6\u03b7(\u03a3) of the superprediction set will be called the \u03b7-exponential superprediction set. It is known (see, e.g., [4], Appendix) that\nLN \u2264 min k=1,...,K\nLkN + lnK\n\u03b7\ncan be guaranteed if and only if the \u03b7-exponential superprediction set is convex. Comparing this with (1) and (2) we can see that we are required to prove that\n\u2022 \u03a6\u03b7(\u03a3) is convex when \u03b7 \u2264 1;\n\u2022 \u03a6\u03b7(\u03a3) is not convex when \u03b7 > 1.\nDefine the \u03b7-exponential superprediction surface to be the part of the boundary of the \u03b7-exponential superprediction set \u03a6\u03b7(\u03a3) lying inside (0,\u221e)\n\u2126. The idea of the proof is to check that, for all \u03b7 < 1, the Gauss\u2013Kronecker curvature of this surface is nowhere vanishing (we will need some basic notions of elementary differential geometry; all definitions can be found in, e.g., [2]). Even when this is done, however, there is still uncertainty as to in which direction the surface is bulging (towards the origin or away from it). The standard argument (as in [2], Chapter 12, Theorem 6) based on the continuity of the smallest principal curvature shows that the \u03b7-exponential superprediction set is bulging away from the origin for small enough \u03b7: indeed, this is obviously true at some point, and so is true everywhere on the surface. By the continuity in \u03b7 this is also true\nfor all \u03b7 < 1. Now, since the \u03b7-exponential superprediction set is convex for all \u03b7 < 1, it is also convex for \u03b7 = 1.\nLet us now check that the Gauss\u2013Kronecker curvature of the \u03b7-exponential superprediction surface is always positive when \u03b7 < 1 and is sometimes negative when \u03b7 > 1 (the rest of the proof, an elaboration of the above argument, will be easy). Set n := |\u2126|; without loss of generality we assume \u2126 = {1, . . . , n}.\nThe parametric representation of the \u03b7-exponential superprediction surface is\n\n     \nx1 x2\n... xn\u22121\nxn\n\n      =\n\n      \ne\u2212\u03b7((u 1 \u22121)2+(u2)2+\u00b7\u00b7\u00b7+(un\u22121)2+(un)2) e\u2212\u03b7((u 1)2+(u2\u22121)2+\u00b7\u00b7\u00b7+(un\u22121)2+(un)2)\n...\ne\u2212\u03b7((u 1)2+(u2)2+\u00b7\u00b7\u00b7+(un\u22121\u22121)2+(un)2) e\u2212\u03b7((u 1)2+(u2)2+\u00b7\u00b7\u00b7+(un\u22121)2+(un\u22121)2)\n\n       , (3)\nwhere u1, . . . , un\u22121 are the coordinates on the surface, u1, . . . , un\u22121 \u2208 (0, 1) subject to u1+ \u00b7 \u00b7 \u00b7un\u22121 < 1, and un is a shorthand for 1\u2212u1\u2212 \u00b7 \u00b7 \u00b7\u2212un\u22121. The derivative of (3) in u1 is\n\u2202\n\u2202u1\n\n     \nx1 x2\n... xn\u22121\nxn\n\n      = 2\u03b7\n\n      \n(un \u2212 u1 + 1)e\u2212\u03b7((u 1 \u22121)2+(u2)2+\u00b7\u00b7\u00b7+(un\u22121)2+(un)2)\n(un \u2212 u1)e\u2212\u03b7((u 1)2+(u2\u22121)2+\u00b7\u00b7\u00b7+(un\u22121)2+(un)2)\n...\n(un \u2212 u1)e\u2212\u03b7((u 1)2+(u2)2+\u00b7\u00b7\u00b7+(un\u22121\u22121)2+(un)2)\n(un \u2212 u1 \u2212 1)e\u2212\u03b7((u 1)2+(u2)2+\u00b7\u00b7\u00b7+(un\u22121)2+(un\u22121)2)\n\n      \n\u221d\n\n      \n(un \u2212 u1 + 1)e2\u03b7u 1\n(un \u2212 u1)e2\u03b7u 2\n...\n(un \u2212 u1)e2\u03b7u n\u22121\n(un \u2212 u1 \u2212 1)e2\u03b7u n\n\n       ,\nthe derivative in u2 is\n\u2202\n\u2202u2\n\n     \nx1 x2\n... xn\u22121\nxn\n\n      \u221d\n\n      \n(un \u2212 u2)e2\u03b7u 1\n(un \u2212 u2 + 1)e2\u03b7u 2\n...\n(un \u2212 u2)e2\u03b7u n\u22121\n(un \u2212 u2 \u2212 1)e2\u03b7u n\n\n       ,\nand so on, up to\n\u2202\n\u2202un\u22121\n\n     \nx1 x2\n... xn\u22121\nxn\n\n      \u221d\n\n      \n(un \u2212 un\u22121)e2\u03b7u 1 (un \u2212 un\u22121)e2\u03b7u 2\n...\n(un \u2212 un\u22121 + 1)e2\u03b7u n\u22121\n(un \u2212 un\u22121 \u2212 1)e2\u03b7u n\n\n       ,\nall coefficients of proportionality being equal and positive. A normal vector to the surface can be found as\nZ := \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 e1 e2 \u00b7 \u00b7 \u00b7 en\u22121 en (un \u2212 u1 + 1)e2\u03b7u 1 (un \u2212 u1)e2\u03b7u 2 \u00b7 \u00b7 \u00b7 (un \u2212 u1)e2\u03b7u n\u22121 (un \u2212 u1 \u2212 1)e2\u03b7u n (un \u2212 u2)e2\u03b7u 1 (un \u2212 u2 + 1)e2\u03b7u 2 \u00b7 \u00b7 \u00b7 (un \u2212 u2)e2\u03b7u n\u22121 (un \u2212 u2 \u2212 1)e2\u03b7u n ... ... . . . ... ...\n(un \u2212 un\u22121)e2\u03b7u 1\n(un \u2212 un\u22121)e2\u03b7u 2 \u00b7 \u00b7 \u00b7 (un \u2212 un\u22121 + 1)e2\u03b7u n\u22121 (un \u2212 un\u22121 \u2212 1)e2\u03b7u n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 .\nThe coefficient in front of e1 is the (n\u2212 1)\u00d7 (n\u2212 1) determinant\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 (un \u2212 u1)e2\u03b7u 2\n\u00b7 \u00b7 \u00b7 (un \u2212 u1)e2\u03b7u n\u22121\n(un \u2212 u1 \u2212 1)e2\u03b7u n\n(un \u2212 u2 + 1)e2\u03b7u 2 \u00b7 \u00b7 \u00b7 (un \u2212 u2)e2\u03b7u n\u22121\n(un \u2212 u2 \u2212 1)e2\u03b7u n\n... . . .\n... ...\n(un \u2212 un\u22121)e2\u03b7u 2 \u00b7 \u00b7 \u00b7 (un \u2212 un\u22121 + 1)e2\u03b7u n\u22121 (un \u2212 un\u22121 \u2212 1)e2\u03b7u n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u221d e\u22122\u03b7u 1 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 un \u2212 u1 \u00b7 \u00b7 \u00b7 un \u2212 u1 un \u2212 u1 \u2212 1 un \u2212 u2 + 1 \u00b7 \u00b7 \u00b7 un \u2212 u2 un \u2212 u2 \u2212 1 ... . . . ... ...\nun \u2212 un\u22121 \u00b7 \u00b7 \u00b7 un \u2212 un\u22121 + 1 un \u2212 un\u22121 \u2212 1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= e\u22122\u03b7u 1 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 1 \u00b7 \u00b7 \u00b7 1 un \u2212 u1 \u2212 1 2 1 \u00b7 \u00b7 \u00b7 1 un \u2212 u2 \u2212 1 1 2 \u00b7 \u00b7 \u00b7 1 un \u2212 u3 \u2212 1 ... ... . . . ... ...\n1 1 \u00b7 \u00b7 \u00b7 2 un \u2212 un\u22121 \u2212 1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= e\u22122\u03b7u 1 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 1 \u00b7 \u00b7 \u00b7 1 un \u2212 u1 \u2212 1 1 0 \u00b7 \u00b7 \u00b7 0 u1 \u2212 u2 0 1 \u00b7 \u00b7 \u00b7 0 u1 \u2212 u3 ... ... . . . ... ... 0 0 \u00b7 \u00b7 \u00b7 1 u1 \u2212 un\u22121 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= e\u22122\u03b7u 1( (\u22121)n(un \u2212 u1 \u2212 1) + (\u22121)n+1(u1 \u2212 u2) + (\u22121)n+1(u1 \u2212 u3) + \u00b7 \u00b7 \u00b7+ (\u22121)n+1(u1 \u2212 un\u22121) )\n= e\u22122\u03b7u 1 (\u22121)n ( (u2 + u3 + \u00b7 \u00b7 \u00b7+ un)\u2212 (n\u2212 1)u1 \u2212 1 )\n= \u2212e\u22122\u03b7u 1 (\u22121)nnu1 \u221d u1e\u22122\u03b7u 1 (4)\n(with a positive coefficient of proportionality, e2\u03b7, in the first \u221d; the third equality follows from the expansion of the determinant along the last column and then along the first row).\nSimilarly, the coefficient in front of ei is proportional (with the same co-\nefficient of proportionality) to uie\u22122\u03b7u i for i = 2, . . . , n \u2212 1; indeed, the\n(n \u2212 1) \u00d7 (n \u2212 1) determinant representing the coefficient in front of ei can be reduced to the form analogous to (4) by moving the ith row to the top.\nThe coefficient in front of en is proportional to\ne\u22122\u03b7u n \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 un \u2212 u1 + 1 un \u2212 u1 \u00b7 \u00b7 \u00b7 un \u2212 u1 un \u2212 u1 un \u2212 u2 un \u2212 u2 + 1 \u00b7 \u00b7 \u00b7 un \u2212 u2 un \u2212 u2 ... ... . . . ... ... un \u2212 un\u22122 un \u2212 un\u22122 \u00b7 \u00b7 \u00b7 un \u2212 un\u22122 + 1 un \u2212 un\u22122\nun \u2212 un\u22121 un \u2212 un\u22121 \u00b7 \u00b7 \u00b7 un \u2212 un\u22121 un \u2212 un\u22121 + 1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= e\u22122\u03b7u n \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 0 \u00b7 \u00b7 \u00b7 0 un \u2212 u1 0 1 \u00b7 \u00b7 \u00b7 0 un \u2212 u2 ... ... . . . ... ... 0 0 \u00b7 \u00b7 \u00b7 1 un \u2212 un\u22122\n\u22121 \u22121 \u00b7 \u00b7 \u00b7 \u22121 un \u2212 un\u22121 + 1\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= e\u22122\u03b7u n \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 0 \u00b7 \u00b7 \u00b7 0 un \u2212 u1 0 1 \u00b7 \u00b7 \u00b7 0 un \u2212 u2 ... ... . . . ... ... 0 0 \u00b7 \u00b7 \u00b7 1 un \u2212 un\u22122\n0 0 \u00b7 \u00b7 \u00b7 0 nun\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 = nune\u22122\u03b7u n\n(with the coefficient of proportionality e2\u03b7(\u22121)n\u22121). The Gauss\u2013Kronecker curvature at the point with coordinates (u1, . . . , un\u22121) is proportional (with a positive coefficient of proportionality, possibly depending on the point) to\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2202ZT \u2202u1 ... \u2202ZT \u2202un\u22121\nZT\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n([2], Chapter 12, Theorem 5, with T standing for transposition). A straightforward calculation allows us to rewrite this determinant (ignoring the positive coefficient ((\u22121)n\u22121ne2\u03b7)n) as\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 (1\u2212 2\u03b7u1)e\u22122\u03b7u 1\n0 \u00b7 \u00b7 \u00b7 0 (\u22121 + 2\u03b7un)e\u22122\u03b7u n\n0 (1\u2212 2\u03b7u2)e\u22122\u03b7u 2 \u00b7 \u00b7 \u00b7 0 (\u22121 + 2\u03b7un)e\u22122\u03b7u n ... ... . . . ... ... 0 0 \u00b7 \u00b7 \u00b7 (1\u2212 2\u03b7un\u22121)e\u22122\u03b7u n\u22121 (\u22121 + 2\u03b7un)e\u22122\u03b7u n\nu1e\u22122\u03b7u 1\nu2e\u22122\u03b7u 2\n\u00b7 \u00b7 \u00b7 un\u22121e\u22122\u03b7u n\u22121\nune\u22122\u03b7u n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u221d \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1\u2212 2\u03b7u1 0 \u00b7 \u00b7 \u00b7 0 \u22121 + 2\u03b7un 0 1\u2212 2\u03b7u2 \u00b7 \u00b7 \u00b7 0 \u22121 + 2\u03b7un ... ... . . . ... ... 0 0 \u00b7 \u00b7 \u00b7 1\u2212 2\u03b7un\u22121 \u22121 + 2\u03b7un\nu1 u2 \u00b7 \u00b7 \u00b7 un\u22121 un\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= u1(1\u2212 2\u03b7u2)(1\u2212 2\u03b7u3) \u00b7 \u00b7 \u00b7 (1\u2212 2\u03b7un) + u2(1\u2212 2\u03b7u1)(1\u2212 2\u03b7u3) \u00b7 \u00b7 \u00b7 (1\u2212 2\u03b7un)\n+ \u00b7 \u00b7 \u00b7+ un(1 \u2212 2\u03b7u1)(1 \u2212 2\u03b7u2) \u00b7 \u00b7 \u00b7 (1\u2212 2\u03b7un\u22121) (5)\n(with a positive coefficient of proportionality; to avoid calculation of the parities of various permutations, the reader might prefer to prove the last equality by induction in n, expanding the last determinant along the first column). Our goal is to show that the last expression in (5) is positive when \u03b7 < 1 but can be negative when \u03b7 > 1.\nIf \u03b7 > 1, set u1 = u2 := 1/2 and u3 = \u00b7 \u00b7 \u00b7 = un := 0. The last expression in (5) becomes negative. Therefore, the \u03b7-exponential superprediction set is not convex ([2], Chapter 13, Theorem 1).\nIt remains to consider the case \u03b7 < 1. Set ti := 1 \u2212 2\u03b7u i, i = 1, . . . , n; the\nconstraints on the ti are\n\u2212 1 < 1\u2212 2\u03b7 < ti < 1, t1 + \u00b7 \u00b7 \u00b7+ tn = n\u2212 2\u03b7 > n\u2212 2. (6)\nOur goal is to prove\n(1\u2212 t1)t2t3 \u00b7 \u00b7 \u00b7 tn + (1\u2212 t2)t1t3 \u00b7 \u00b7 \u00b7 tn + \u00b7 \u00b7 \u00b7+ (1\u2212 tn)t1t2 \u00b7 \u00b7 \u00b7 tn\u22121 > 0,\ni.e., t2t3 \u00b7 \u00b7 \u00b7 tn + t1t3 \u00b7 \u00b7 \u00b7 tn + \u00b7 \u00b7 \u00b7+ t1t2 \u00b7 \u00b7 \u00b7 tn\u22121 > nt1 \u00b7 \u00b7 \u00b7 tn. (7)\nThis reduces to 1\nt1 + \u00b7 \u00b7 \u00b7+\n1\ntn > n (8)\nif t1 \u00b7 \u00b7 \u00b7 tn > 0, and to 1\nt1 + \u00b7 \u00b7 \u00b7+\n1\ntn < n (9)\nif t1 \u00b7 \u00b7 \u00b7 tn < 0. The remaining case is where some of the ti are zero; for concreteness, let tn = 0. By (6) we have t1 + \u00b7 \u00b7 \u00b7 + tn\u22121 > n \u2212 2, and so all of t1, . . . , tn\u22121 are positive; this shows that (7) is indeed true.\nLet us prove (8). Since t1 \u00b7 \u00b7 \u00b7 tn > 0, all of t1, . . . , tn are positive (if two of them were negative, the sum t1 + \u00b7 \u00b7 \u00b7 + tn would be less than n \u2212 2; cf. (6)). Therefore,\n1 t1 + \u00b7 \u00b7 \u00b7+ 1 tn > 1 + \u00b7 \u00b7 \u00b7+ 1 \ufe38 \ufe37\ufe37 \ufe38\nn times\n= n.\nTo establish (7) it remains to prove (9). Suppose, without loss of generality, that t1 > 0, t2 > 0, . . . , tn\u22121 > 0, tn < 0. Since the function t \u2208 (0, 1] 7\u2192 1/t\nis convex, we can also assume, without loss of generality, t1 = \u00b7 \u00b7 \u00b7 = tn\u22122 = 1. Then tn\u22121 + tn > 0, and so\n1\ntn\u22121 +\n1\ntn < 0;\ntherefore, 1\nt1 + \u00b7 \u00b7 \u00b7+\n1\ntn\u22122 +\n1\ntn\u22121 +\n1\ntn < n\u2212 2 < n.\nFinally, let us check that the positivity of the Gauss\u2013Kronecker curvature implies the convexity of the \u03b7-exponential superprediction set, for \u03b7 \u2264 1. Because of the continuity of the \u03b7-exponential superprediction surface in \u03b7 we can and will assume, without loss of generality, that \u03b7 < 1. The \u03b7-exponential superprediction surface will be oriented by choosing the normal vector field directed towards the origin; this can be done since\n\n \nx1\n... xn\n\n  \u221d\n\n \ne2\u03b7u 1\n... e2\u03b7u n\n\n  , Z \u221d\n\n \n\u2212u1e\u22122\u03b7u 1 ... \u2212une\u22122\u03b7u n\n\n  , (10)\nwith the first coefficient of proportionality positive (cf. (3) and the bottom row of the first determinant in (5)), and the scalar product of the two vectors in (10) is always negative.\nLet us first check that the smallest principal curvature\nk1 = k1(u 1, . . . , un\u22121, \u03b7)\nof the \u03b7-exponential superprediction surface is always positive (among the arguments of k1 we list not only the coordinates u\n1, . . . , un\u22121 of a point on the surface (3) but also the learning rate \u03b7 \u2208 (0, 1)). At least at some (u1, . . . , un\u22121, \u03b7) the value of k1(u\n1, . . . , un\u22121, \u03b7) is positive: take a sufficiently small \u03b7 and the point on the surface (3) at which the maximum of x1 + \u00b7 \u00b7 \u00b7+ xn is attained (the point of the \u03b7-exponential superprediction set at which the maximum is attained will lie on the surface since the maximum is attained at (x1, . . . , xn) = (1, . . . , 1) when \u03b7 = 0). Therefore, for all (u1, . . . , un\u22121, \u03b7) the value of k1(u\n1, . . . , un\u22121, \u03b7) is positive: if k1 had different signs at two points in the set\n{ (u1, . . . , un\u22121, \u03b7) |u1 \u2208 (0, 1), . . . , un\u22121 \u2208 (0, 1),\nu1 + \u00b7 \u00b7 \u00b7+ un\u22121 < 1, \u03b7 \u2208 (0, 1) } , (11)\nwe could connect these points by a continuous curve lying completely inside (11); at some point on the curve, k1 would be zero, in contradiction to the positivity of the Gauss\u2013Kronecker curvature k1 \u00b7 \u00b7 \u00b7 kn\u22121.\nNow it is easy to show that the \u03b7-exponential superprediction set is convex. Suppose there are two points A and B on the \u03b7-exponential superprediction surface such that the interval [A,B] contains points outside the \u03b7-exponential\nsuperprediction set. The intersection of the plane OAB, where O is the origin, with the \u03b7-exponential superprediction surface is a planar curve; the curvature of this curve at the point between A and B closest to the origin will be negative (with the curve oriented by directing the normal vector field towards the origin), contradicting the positivity of k1 at that point and Meusnier\u2019s theorem (cf. (10)).\n3 Prediction algorithm\nTo achieve the loss bound (1) in Theorem 1 Learner can use the Aggregating Algorithm (AA, as described in, e.g., [3], Section 2.1, (15)) with \u03b7 = 1. In this section we will find a substitution function for the AA for the Brier game with \u03b7 \u2264 1, which is the only component of the AA not described explicitly in [3]. Our substitution function will not require that its input, the generalized prediction, should be computed from the normalized distribution on the experts; this is a valuable feature for generalizations to an infinite number of experts (as demonstrated in, e.g., [3], Appendix A.1).\nSuppose that we are given a generalized prediction (l1, . . . , ln) T computed by the Aggregating Pseudo-Algorithm (APA) from a normalized distribution on the experts. Since (l1, . . . , ln)\nT is a superprediction (remember that we are assuming \u03b7 \u2264 1), we are only required to find a permitted prediction\n\n     \n\u03bb1 \u03bb2 ...\n\u03bbn\u22121 \u03bbn\n\n      =\n\n     \n(u1 \u2212 1)2 + (u2)2 + \u00b7 \u00b7 \u00b7+ (un\u22121)2 + (un)2 (u1)2 + (u2 \u2212 1)2 + \u00b7 \u00b7 \u00b7+ (un\u22121)2 + (un)2 ... (u1)2 + (u2)2 + \u00b7 \u00b7 \u00b7+ (un\u22121 \u2212 1)2 + (un)2 (u1)2 + (u2)2 + \u00b7 \u00b7 \u00b7+ (un\u22121)2 + (un \u2212 1)2\n\n     \n(12)\n(cf. (3)) satisfying \u03bb1 \u2264 l1, . . . , \u03bbn \u2264 ln. (13)\nNow suppose we are given a generalized prediction (L1, . . . , Ln) T computed by the APA from an unnormalized distribution on the experts; in other words, we are given\n\n  L1 ... Ln\n\n  =\n\n  l1 + c ...\nln + c\n\n \nfor some c \u2208 R. To find (12) satisfying (13) we can first find the largest t \u2208 R such that (L1 \u2212 t, . . . , Ln \u2212 t)\nT is still a superprediction and then find (12) satisfying\n\u03bb1 \u2264 L1 \u2212 t, . . . , \u03bbn \u2264 Ln \u2212 t. (14)\nSince t \u2265 c, it is clear that (\u03bb1, . . . , \u03bbn) T will also satisfy the required (13).\nProposition 1 Define s \u2208 R by the requirement\nn\u2211\ni=1\n(s\u2212 Li) + = 2. (15)\nThe unique solution to the optimization problem t \u2192 max under the constraints (14) with \u03bb1, . . . , \u03bbn as in (12) will be\nui = (s\u2212 Li)\n+\n2 , i = 1, . . . , n, (16)\nt = s\u2212 1\u2212 (u1)2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 (un)2. (17)\nThere exists a unique s satisfying (15) since the left-hand side of (15) is a continuous, increasing (strictly increasing when positive) and unbounded above function of s. The substitution function is given by (16).\nProof of Proposition 1 Let us denote the ui and t defined by (16) and (17) as ui and t, respectively. To see that they satisfy the constraints (14), notice that the ith constraint can be spelt out as\n(u1)2 + \u00b7 \u00b7 \u00b7+ (un)2 \u2212 2ui + 1 \u2264 Li \u2212 t,\nwhich immediately follows from (16) and (17). As a by-product, we can see that the inequality becomes an equality, i.e.,\nt = Li \u2212 1 + 2u i \u2212 (u1)2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 (un)2, (18)\nfor all i with ui > 0. We can rewrite (14) as\n \n\nt \u2264 L1 \u2212 1 + 2u 1 \u2212 (u1)2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 (un)2, ... t \u2264 Ln \u2212 1 + 2u n \u2212 (u1)2 \u2212 \u00b7 \u00b7 \u00b7 \u2212 (un)2,\n(19)\nand our goal is to prove that these inequalities imply t < t (unless u1 = u1, . . . , un = un). Choose ui (necessarily ui > 0 unless u1 = u1, . . . , un = un; in the latter case, however, we can, and will, also choose ui > 0) for which \u01ebi := u i \u2212 ui is maximal. Then every value of t satisfying (19) will also satisfy\nt \u2264 Li \u2212 1 + 2u i \u2212\nn\u2211\nj=1\n(uj)2\n= Li \u2212 1 + 2u i \u2212 2\u01ebi \u2212\nn\u2211\nj=1\n(uj)2 + 2 n\u2211\nj=1\n\u01ebju j \u2212\nn\u2211\nj=1\n\u01eb2j\n\u2264 Li \u2212 1 + 2u i \u2212\nn\u2211\nj=1\n(uj)2 \u2212\nn\u2211\nj=1\n\u01eb2j \u2264 t,\nwith the last \u2264 following from (18) and becoming < when not all uj coincide with uj .\nAcknowledgments\nThis work was partially supported by EPSRC (grant EP/F002998/1), MRC (grant G0301107), and the Cyprus Research Promotion Foundation.\nReferences\n[1] Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1\u20133, 1950.\n[2] John A. Thorpe. Elementary Topics in Differential Geometry. Springer, New York, 1979.\n[3] Vladimir Vovk. Competitive on-line statistics. International Statistical Review, 69:213\u2013248, 2001.\n[4] Vladimir Vovk. Defensive forecasting for optimal prediction with expert advice. Technical Report arXiv:0708.1503 [cs.LG], arXiv.org e-Print archive, August 2007."}], "references": [{"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W. Brier"], "venue": "Monthly Weather Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1950}, {"title": "Elementary Topics in Differential Geometry", "author": ["John A. Thorpe"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1979}, {"title": "Competitive on-line statistics", "author": ["Vladimir Vovk"], "venue": "International Statistical Review,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Defensive forecasting for optimal prediction with expert advice", "author": ["Vladimir Vovk"], "venue": "Technical Report arXiv:0708.1503 [cs.LG], arXiv.org e-Print archive,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "In this note we are interested in the following Brier game [1]: \u03a9 is a finite and non-empty set, \u0393 := P(\u03a9) is the set of all probability measures on \u03a9, and", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": ", [4], Appendix) that", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": ", [2]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "The standard argument (as in [2], Chapter 12, Theorem 6) based on the continuity of the smallest principal curvature shows that the \u03b7-exponential superprediction set is bulging away from the origin for small enough \u03b7: indeed, this is obviously true at some point, and so is true everywhere on the surface.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "([2], Chapter 12, Theorem 5, with T standing for transposition).", "startOffset": 1, "endOffset": 4}, {"referenceID": 1, "context": "Therefore, the \u03b7-exponential superprediction set is not convex ([2], Chapter 13, Theorem 1).", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": ", [3], Section 2.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "In this section we will find a substitution function for the AA for the Brier game with \u03b7 \u2264 1, which is the only component of the AA not described explicitly in [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": ", [3], Appendix A.", "startOffset": 2, "endOffset": 5}], "year": 2017, "abstractText": "In this technical report I show that the Brier game of prediction is perfectly mixable and find the optimal learning rate and substitution function for it. These results are straightforward, but the computations are surprisingly messy.", "creator": "LaTeX with hyperref package"}}}