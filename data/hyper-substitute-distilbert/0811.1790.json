{"id": "0811.1790", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2008", "title": "Robust Regression and Lasso", "abstract": "lasso, symbol $ \\ ell ^ 1 $ regularized product cost, has been explored extensively for predicting expressive noise properties. clear is shown in this work that algebraic solution to lasso, beside addition to impressive sparsity, has robustness inherent : it has noisy solution comprising a complex optimization argument. this ignores two interesting consequences. broadly, computation provides a connection to the regularizer to a physical property, therefore, production from noise. performance enables a principled upper class generalized regularizer, from in particular, classification of lasso codes also yield convex optimization is efficiently obtained by applying standard uncertainty scenarios.", "histories": [["v1", "Tue, 11 Nov 2008 22:46:10 GMT  (91kb)", "http://arxiv.org/abs/0811.1790v1", null]], "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT", "authors": ["huan xu", "constantine caramanis", "shie mannor"], "accepted": true, "id": "0811.1790"}, "pdf": {"name": "0811.1790.pdf", "metadata": {"source": "CRF", "title": "Robust Regression and Lasso", "authors": ["Huan Xu", "Constantine Caramanis"], "emails": ["(xuhuan@cim.mcgill.ca;", "shie.mannor@mcgill.ca)"], "sections": [{"heading": null, "text": "ar X\niv :0\n81 1.\n17 90\nv1 [\ncs .I\nT ]\n1 1\nN ov\n2 00\n8 1\nSecondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.\nIndex Terms\nStatistical Learning, Regression, Regularization, Kernel density estimator, Lasso, Robustness, Sparsity, Stability.\nI. INTRODUCTION\nIn this paper we consider linear regression problems with least-square error. The problem is to find a vector x so that the \u21132 norm of the residual b \u2212 Ax is minimized, for a given matrix A \u2208 Rn\u00d7m and vector b \u2208 Rn. From a learning/regression perspective, each row of A can be regarded as a training sample, and the corresponding element of b as the target value of this observed sample. Each column of A corresponds to a feature, and the objective is to find a set of weights so that the weighted sum of the feature values approximates the target value.\nIt is well known that minimizing the least squared error can lead to sensitive solutions [1]\u2013 [4]. Many regularization methods have been proposed to decrease this sensitivity. Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms. These methods minimize a weighted sum of the residual norm and a certain regularization term, \u2016x\u20162 for Tikhonov regularization and \u2016x\u20161 for Lasso. In addition to providing regularity, Lasso is also known for the tendency to select sparse solutions. Recently this has attracted much attention for its ability to reconstruct sparse solutions when sampling occurs far below the Nyquist rate, and\nA preliminary version of this paper was presented at Twenty-Second Annual Conference on Neural Information Processing Systems.\nH. Xu and S. Mannor are with the Department of Electrical and Computer Engineering, McGill University, Montre\u0301al, H3A2A7, Canada email: (xuhuan@cim.mcgill.ca; shie.mannor@mcgill.ca)\nC. Caramanis is with the Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712 USA email: (cmcaram@ece.utexas.edu).\nNovember 11, 2008 DRAFT\n2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]\u2013[12] and references therein).\nThe first result of this paper is that the solution to Lasso has robustness properties: it is the solution to a robust optimization problem. In itself, this interpretation of Lasso as the solution to a robust least squares problem is a development in line with the results of [13]. There, the authors propose an alternative approach of reducing sensitivity of linear regression by considering a robust version of the regression problem, i.e., minimizing the worst-case residual for the observations under some unknown but bounded disturbance. Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].\nNone of these robust optimization approaches produces a solution that has sparsity properties (in particular, the solution to Lasso does not solve any of these previously formulated robust optimization problems). In contrast, we investigate the robust regression problem where the uncertainty set is defined by feature-wise constraints. Such a noise model is of interest when values of features are obtained with some noisy pre-processing steps, and the magnitudes of such noises are known or bounded. Another situation of interest is where features are meaningfully coupled. We define coupled and uncoupled disturbances and uncertainty sets precisely in Section II-A below. Intuitively, a disturbance is feature-wise coupled if the variation or disturbance across features satisfy joint constraints, and uncoupled otherwise.\nConsidering the solution to Lasso as the solution of a robust least squares problem has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows more principled selection of the regularizer, and in particular, considering different uncertainty sets, we construct generalizations of Lasso that also yield convex optimization problems.\nSecondly, and perhaps most significantly, robustness is a strong property that can itself be used as an avenue to investigating different properties of the solution. We show that robustness of the solution can explain why the solution is sparse. The analysis as well as the specific results we obtain differ from standard sparsity results, providing different geometric intuition, and extending beyond the least-squares setting. Sparsity results obtained for Lasso ultimately depend on the fact that introducing additional features incurs larger \u21131-penalty than the least squares error reduction. In contrast, we exploit the fact that a robust solution is, by definition, the optimal solution under a worst-case perturbation. Our results show that, essentially, a coefficient of the solution is nonzero if the corresponding feature is relevant under all allowable perturbations. In addition to sparsity, we also use robustness directly to prove consistency of Lasso.\nWe briefly list the main contributions as well as the organization of this paper. \u2022 In Section II, we formulate the robust regression problem with feature-wise independent\ndisturbances, and show that this formulation is equivalent to a least-square problem with a weighted \u21131 norm regularization term. Hence, we provide an interpretation of Lasso from a robustness perspective. \u2022 We generalize the robust regression formulation to loss functions of arbitrary norm in Section III. We also consider uncertainty sets that require disturbances of different features to satisfy joint conditions. This can be used to mitigate the conservativeness of the robust solution and to obtain solutions with additional properties. \u2022 In Section IV, we present new sparsity results for the robust regression problem with feature-wise independent disturbances. This provides a new robustness-based explanation\nNovember 11, 2008 DRAFT\n3 to the sparsity of Lasso. Our approach gives new analysis and also geometric intuition, and furthermore allows one to obtain sparsity results for more general loss functions, beyond the squared loss. \u2022 Next, we relate Lasso to kernel density estimation in Section V. This allows us to re-prove consistency in a statistical learning setup, using the new robustness tools and formulation we introduce. Along with our results on sparsity, this illustrates the power of robustness in explaining and also exploring different properties of the solution. \u2022 Finally, we prove in Section VI a \u201cno-free-lunch\u201d theorem, stating that an algorithm that encourages sparsity cannot be stable.\nNotation. We use capital letters to represent matrices, and boldface letters to represent column vectors. Row vectors are represented as the transpose of column vectors. For a vector z, zi denotes its ith element. Throughout the paper, ai and r\u22a4j are used to denote the i\nth column and the jth row of the observation matrix A, respectively. We use aij to denote the ij element of A, hence it is the jth element of ri, and ith element of aj . For a convex function f(\u00b7), \u2202f(z) represents any of its sub-gradients evaluated at z. A vector with length n and each element equals 1 is denoted as 1n.\nII. ROBUST REGRESSION WITH FEATURE-WISE DISTURBANCE\nIn this section, we show that our robust regression formulation recovers Lasso as a special case. We also derive probabilistic bounds that guide in the construction of the uncertainty set.\nThe regression formulation we consider differs from the standard Lasso formulation, as we minimize the norm of the error, rather than the squared norm. It is known that these two coincide up to a change of the regularization coefficient. Yet as we discuss above, our results lead to more flexible and potentially powerful robust formulations, and give new insight into known results."}, {"heading": "A. Formulation", "text": "Robust linear regression considers the case where the observed matrix is corrupted by some potentially malicious disturbance. The objective is to find the optimal solution in the worst case sense. This is usually formulated as the following min-max problem,\nRobust Linear Regression:\nmin x\u2208Rm\n{\nmax \u2206A\u2208U\n\u2016b\u2212 (A+\u2206A)x\u20162 } , (1)\nwhere U is called the uncertainty set, or the set of admissible disturbances of the matrix A. In this section, we consider the class of uncertainty sets that bound the norm of the disturbance to each feature, without placing any joint requirements across feature disturbances. That is, we consider the class of uncertainty sets:\nU , { (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) \u2223 \u2223 \u2223 \u2016\u03b4i\u20162 \u2264 ci, i = 1, \u00b7 \u00b7 \u00b7 , m } , (2)\nfor given ci \u2265 0. We call these uncertainty sets feature-wise uncoupled, in contrast to coupled uncertainty sets that require disturbances of different features to satisfy some joint constraints (we discuss these extensively below, and their significance). While the inner maximization problem of (1) is nonconvex, we show in the next theorem that uncoupled norm-bounded uncertainty sets lead to an easily solvable optimization problem.\nNovember 11, 2008 DRAFT\n4 Theorem 1: The robust regression problem (1) with uncertainty set of the form (2) is equivalent to the following \u21131 regularized regression problem:\nmin x\u2208Rm\n{ \u2016b\u2212 Ax\u20162 + m \u2211\ni=1\nci|xi| } . (3)\nProof: Fix x\u2217. We prove that max\u2206A\u2208U \u2016b\u2212 (A+\u2206A)x\u2217\u20162 = \u2016b\u2212Ax\u2217\u20162 + \u2211m i=1 ci|x\u2217i |. The left hand side can be written as\nmax \u2206A\u2208U\n\u2016b\u2212 (A +\u2206A)x\u2217\u20162\n= max (\u03b41,\u00b7\u00b7\u00b7 ,\u03b4m)|\u2016\u03b4i\u20162\u2264ci\n\u2225 \u2225 \u2225 b \u2212 ( A+ (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) ) x\u2217 \u2225 \u2225 \u2225\n2\n= max (\u03b41,\u00b7\u00b7\u00b7 ,\u03b4m)|\u2016\u03b4i\u20162\u2264ci\n\u2016b\u2212 Ax\u2217 \u2212 m \u2211\ni=1\nx\u2217i\u03b4i\u20162\n\u2264 max (\u03b41,\u00b7\u00b7\u00b7 ,\u03b4m)|\u2016\u03b4i\u20162\u2264ci\n\u2225 \u2225\n\u2225 b \u2212Ax\u2217\n\u2225 \u2225 \u2225\n2 +\nm \u2211\ni=1\n\u2016x\u2217i \u03b4i\u20162\n\u2264\u2016b\u2212 Ax\u2217\u20162 + m \u2211\ni=1\n|x\u2217i |ci.\n(4)\nNow, let\nu ,\n{ b\u2212Ax\u2217 \u2016b\u2212Ax\u2217\u20162 if Ax \u2217 6= b, any vector with unit \u21132 norm otherwise;\nand let \u03b4 \u2217 i , \u2212cisgn(x\u2217i )u.\nObserve that \u2016\u03b4\u2217i \u20162 \u2264 ci, hence \u2206A\u2217 , (\u03b4\u22171, \u00b7 \u00b7 \u00b7 , \u03b4\u2217m) \u2208 U . Notice that max \u2206A\u2208U\n\u2016b\u2212 (A+\u2206A)x\u2217\u20162 \u2265\u2016b \u2212 (A+\u2206A\u2217)x\u2217\u20162 = \u2225 \u2225 \u2225 b\u2212 ( A + (\u03b4\u22171, \u00b7 \u00b7 \u00b7 , \u03b4\u2217m) ) x\u2217 \u2225 \u2225 \u2225\n2\n= \u2225 \u2225 \u2225 (b\u2212 Ax\u2217)\u2212\nm \u2211\ni=1\n( \u2212 x\u2217i cisgn(x\u2217i )u )\n\u2225 \u2225 \u2225\n2\n= \u2225 \u2225 \u2225 (b\u2212 Ax\u2217) + (\nm \u2211\ni=1\nci|x\u2217i |)u \u2225 \u2225 \u2225\n2\n=\u2016b \u2212Ax\u2217\u20162 + m \u2211\ni=1\nci|x\u2217i |.\n(5)\nThe last equation holds from the definition of u. Combining Inequalities (4) and (5), establishes the equality max\u2206A\u2208U \u2016b\u2212 (A+\u2206A)x\u2217\u20162 = \u2016b\u2212Ax\u2217\u20162 + \u2211m\ni=1 ci|x\u2217i | for any x\u2217. Minimizing over x on both sides proves the theorem. Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].\nNovember 11, 2008 DRAFT\n5"}, {"heading": "B. Uncertainty Set Construction", "text": "The selection of an uncertainty set U in Robust Optimization is of fundamental importance. One way this can be done is as an approximation of so-called chance constraints, where a deterministic constraint is replaced by the requirement that a constraint is satisfied with at least some probability. These can be formulated when we know the distribution exactly, or when we have only partial information of the uncertainty, such as, e.g., first and second moments. This chance-constraint formulation is particularly important when the distribution has large support, rendering the naive robust optimization formulation overly pessimistic.\nFor confidence level \u03b7, the chance constraint formulation becomes:\nminimize: t\nSubject to: Pr(\u2016b \u2212 (A+\u2206A)x\u20162 \u2264 t) \u2265 1\u2212 \u03b7. Here, x and t are the decision variables.\nConstructing the uncertainty set for feature i can be done quickly via line search and bisection, as long as we can evaluate Pr(\u2016ai\u20162 \u2265 c). If we know the distribution exactly (i.e., if we have complete probabilistic information), this can be quickly done via sampling. Another setting of interest is when we have access only to some moments of the distribution of the uncertainty, e.g., the mean and variance. In this setting, the uncertainty sets are constructed via a bisection procedure which evaluates the worst-case probability over all distributions with given mean and variance. We do this using a tight bound on the probability of an event, given the first two moments.\nIn the scalar case, the Markov Inequality provides such a bound. The next theorem is a generalization of the Markov inequality to Rn, which bounds the probability where the disturbance on a given feature is more than ci, if only the first and second moment of the random variable are known. We postpone the proof to the appendix, and refer the reader to [15] for similar results using semi-definite optimization.\nTheorem 2: Consider a random vector v \u2208 Rn, such that E(v) = a, and E(vv\u22a4) = \u03a3, \u03a3 0. Then we have\nPr{\u2016v\u20162 \u2265 ci} \u2264\n\n     \n      \nminP,q,r,\u03bb Trace(\u03a3P ) + 2q \u22a4a + r\nsubject to:\n(\nP q q\u22a4 r\n)\n0 (\nI(m) 0 0\u22a4 \u2212c2i\n)\n\u03bb (\nP q q\u22a4 r \u2212 1\n)\n\u03bb \u2265 0.\n(6)\nThe optimization problem (6) is a semi-definite programming, which is known be solved in polynomial time. Furthermore, if we replace E(vv\u22a4) = \u03a3 by an inequality E(vv\u22a4) \u2264 \u03a3, the uniform bound still holds. Thus, even if our estimation to the variance is not precise, we are still able to bound the probability of having \u201clarge\u201d disturbance.\nIII. GENERAL UNCERTAINTY SETS\nOne reason the robust optimization formulation is powerful, is that having provided the connection to Lasso, it then allows the opportunity to generalize to efficient \u201cLasso-like\u201d regularization algorithms.\nNovember 11, 2008 DRAFT\n6 In this section, we make several generalizations of the robust formulation (1) and derive counterparts of Theorem 1. We generalize the robust formulation in two ways: (a) to the case of arbitrary norm; and (b) to the case of coupled uncertainty sets.\nWe first consider the case of an arbitrary norm \u2016 \u00b7 \u2016a of Rn as a cost function rather than the squared loss. The proof of the next theorem is identical to that of Theorem 1, with only the \u21132 norm changed to \u2016 \u00b7 \u2016a. Theorem 3: The robust regression problem\nmin x\u2208Rm\n{\nmax \u2206A\u2208Ua\n\u2016b \u2212 (A+\u2206A)x\u2016a } ; Ua , { (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) \u2223 \u2223 \u2223 \u2016\u03b4i\u2016a \u2264 ci, i = 1, \u00b7 \u00b7 \u00b7 , m } ;\nis equivalent to the following regularized regression problem\nmin x\u2208Rm\n{ \u2016b \u2212Ax\u2016a + m \u2211\ni=1\nci|xi| } .\nWe next remove the assumption that the disturbances are feature-wise uncoupled. Allowing coupled uncertainty sets is useful when we have some additional information about potential noise in the problem, and we want to limit the conservativeness of the worst-case formulation. Consider the following uncertainty set:\nU \u2032 , { (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) \u2223 \u2223fj(\u2016\u03b41\u2016a, \u00b7 \u00b7 \u00b7 , \u2016\u03b4m\u2016a) \u2264 0; j = 1, \u00b7 \u00b7 \u00b7 , k } ,\nwhere fj(\u00b7) are convex functions. Notice that, both k and fj can be arbitrary, hence this is a very general formulation, and provides us with significant flexibility in designing uncertainty sets and equivalently new regression algorithms (see for example Corollary 1 and 2). The following theorem converts this formulation to tractable optimization problems. The proof is postponed to the appendix.\nTheorem 4: Assume that the set\nZ , {z \u2208 Rm|fj(z) \u2264 0, j = 1, \u00b7 \u00b7 \u00b7 , k; z \u2265 0} has non-empty relative interior. Then the robust regression problem\nmin x\u2208Rm\n{\nmax \u2206A\u2208U \u2032\n\u2016b\u2212 (A+\u2206A)x\u2016a }\nis equivalent to the following regularized regression problem\nmin \u03bb\u2208Rk\n+ ,\u03ba\u2208Rm + ,x\u2208Rm\n{ \u2016b\u2212 Ax\u2016a + v(\u03bb,\u03ba,x) } ;\nwhere: v(\u03bb,\u03ba,x) , max c\u2208Rm\n[ (\u03ba + |x|)\u22a4c \u2212 k \u2211\nj=1\n\u03bbjfj(c) ]\n(7)\nRemark: Problem (7) is efficiently solvable. Denote zc(\u03bb,\u03ba,x) , [ (\u03ba+|x|)\u22a4c\u2212 \u2211k j=1 \u03bbjfj(c) ] .\nThis is a convex function of (\u03bb,\u03ba,x), and the sub-gradient of zc(\u00b7) can be computed easily for any c. The function v(\u03bb,\u03ba,x) is the maximum of a set of convex functions, zc(\u00b7) , hence is convex, and satisfies\n\u2202v(\u03bb\u2217,\u03ba\u2217,x\u2217) = \u2202zc0(\u03bb\u2217,\u03ba\u2217,x\u2217),\nNovember 11, 2008 DRAFT\n7 where c0 maximizes [ (\u03ba\u2217 + |x|\u2217)\u22a4c \u2212 \u2211k j=1 \u03bb \u2217 jfj(c) ] . We can efficiently evaluate c0 due to convexity of fj(\u00b7), and hence we can efficiently evaluate the sub-gradient of v(\u00b7). The next two corollaries are a direct application of Theorem 4. Corollary 1: Suppose U \u2032 = { (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) \u2223 \u2223 \u2223 \u2225 \u2225\u2016\u03b41\u2016a, \u00b7 \u00b7 \u00b7 , \u2016\u03b4m\u2016a \u2225 \u2225\ns \u2264 l;\n}\nfor a symmetric norm\n\u2016 \u00b7 \u2016s, then the resulting regularized regression problem is\nmin x\u2208Rm\n{ \u2016b\u2212Ax\u2016a + l\u2016x\u2016\u2217s } ; where \u2016 \u00b7 \u2016\u2217s is the dual norm of \u2016 \u00b7 \u2016s.\nThis corollary interprets arbitrary norm-based regularizers from a robust regression perspective. For example, it is straightforward to show that if we take both \u2016 \u00b7 \u2016\u03b1 and \u2016 \u00b7 \u2016s as the Euclidean norm, then U \u2032 is the set of matrices with their Frobenious norms bounded, and Corollary 1 reduces to the robust formulation introduced by [13].\nCorollary 2: Suppose U \u2032 = { (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) \u2223 \u2223 \u2223 \u2203c \u2265 0 : Tc \u2264 s; \u2016\u03b4j\u2016a \u2264 cj; }\n, then the resulting regularized regression problem is\nMinimize: \u2016b \u2212Ax\u2016a + s\u22a4\u03bb Subject to: x \u2264 T\u22a4\u03bb\n\u2212 x \u2264 T\u22a4\u03bb \u03bb \u2265 0.\nUnlike previous results, this corollary considers general polytope uncertainty sets. Advantages of such sets include the linearity of the final formulation. Moreover, the modeling power is considerable, as many interesting disturbances can be modeled in this way.\nWe briefly mention some further examples meant to illustrate the power and flexibility of the robust formulation. We refer the interested reader to [16] for full details.\nAs the results above indicate, the robust formulation can model a broad class of uncertainties, and yield computationally tractable (i.e., convex) problems. In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.\nAnother avenue one may take using robustness, and which is also possible to solve easily, is the case where the uncertainty set allows independent perturbation of the columns and the rows of the matrix A. The resulting formulation resembles the elastic-net formulation [18], where there is a combination of \u21132 and \u21131 regularization.\nIV. SPARSITY\nIn this section, we investigate the sparsity properties of robust regression (1), and equivalently Lasso. Lasso\u2019s ability to recover sparse solutions has been extensively studied and discussed (cf [8]\u2013[11]). There are generally two approaches. The first approach investigates the problem from a statistical perspective. That is, it assumes that the observations are generated by a (sparse) linear combination of the features, and investigates the asymptotic or probabilistic conditions required for Lasso to correctly recover the generative model. The second approach treats the problem from an optimization perspective, and studies under what conditions a pair (A, b) defines a problem with sparse solutions (e.g., [19]).\nNovember 11, 2008 DRAFT\n8 We follow the second approach and do not assume a generative model. Instead, we consider the conditions that lead to a feature receiving zero weight. Our first result paves the way for the remainder of this section. We show in Theorem 5 that, essentially, a feature receives no weight (namely, x\u2217i = 0) if there exists an allowable perturbation of that feature which makes it irrelevant. This result holds for general norm loss functions, but in the \u21132 case, we obtain further geometric results. For instance, using Theorem 5, we show, among other results, that \u201cnearly\u201d orthogonal features get zero weight (Theorem 6). Using similar tools, we provide additional results in [16]. There, we show, among other results, that the sparsity pattern of any optimal solution must satisfy certain angular separation conditions between the residual and the relevant features, and that \u201cnearly\u201d linearly dependent features get zero weight.\nSubstantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others). In particular, similar results as in point (a), that rely on an incoherence property, have been established in, e.g., [19], and are used as standard tools in investigating sparsity of Lasso from the statistical perspective. However, a proof exploiting robustness and properties of the uncertainty is novel. Indeed, such a proof shows a fundamental connection between robustness and sparsity, and implies that robustifying w.r.t. a feature-wise independent uncertainty set might be a plausible way to achieve sparsity for other problems.\nTo state the main theorem of this section, from which the other results derive, we introduce some notation to facilitate the discussion. Given a feature-wise uncoupled uncertainty set, U , an index subset I \u2286 {1, . . . , n}, and any \u2206A \u2208 U , let \u2206AI denote the element of U that equals \u2206A on each feature indexed by i \u2208 I , and is zero elsewhere. Then, we can write any element \u2206A \u2208 U as \u2206AI +\u2206AIc (where Ic = {1, . . . , n} \\ I). Then we have the following theorem. We note that the result holds for any norm loss function, but we state and prove it for the \u21132 norm, since the proof for other norms is identical.\nTheorem 5: The robust regression problem\nmin x\u2208Rm\n{\nmax \u2206A\u2208U\n\u2016b\u2212 (A+\u2206A)x\u20162 } ,\nhas a solution supported on an index set I if there exists some perturbation \u2206A\u0303I c \u2208 U of the features in Ic, such that the robust regression problem\nmin x\u2208Rm\n{\nmax \u2206A\u0303I\u2208UI\n\u2016b\u2212 (A+\u2206A\u0303Ic +\u2206A\u0303I)x\u20162 } ,\nhas a solution supported on the set I . Thus, a robust regression has an optimal solution supported on a set I , if any perturbation of the features corresponding to the complement of I makes them irrelevant. Theorem 5 is a special case of the following theorem with cj = 0 for all j 6\u2208 I:\nTheorem 5\u2019. Let x\u2217 be an optimal solution of the robust regression problem:\nmin x\u2208Rm\n{\nmax \u2206A\u2208U\n\u2016b\u2212 (A+\u2206A)x\u20162 } ,\nand let I \u2286 {1, \u00b7 \u00b7 \u00b7 , m} be such that x\u2217j = 0 \u2200 j 6\u2208 I . Let\nU\u0303 , { (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) \u2223 \u2223 \u2223 \u2016\u03b4i\u20162 \u2264 ci, i \u2208 I; \u2016\u03b4j\u20162 \u2264 cj + lj , j 6\u2208 I } .\nNovember 11, 2008 DRAFT\n9 Then, x\u2217 is an optimal solution of\nmin x\u2208Rm\n{\nmax \u2206A\u2208U\u0303\n\u2016b\u2212 (A\u0303+\u2206A)x\u20162 } ,\nfor any A\u0303 that satisfies \u2016a\u0303j \u2212 aj\u2016 \u2264 lj for j 6\u2208 I , and a\u0303i = ai for i \u2208 I . Proof: Notice that\nmax \u2206A\u2208U\u0303\n\u2225 \u2225\n\u2225 b\u2212 (A+\u2206A)x\u2217\n\u2225 \u2225 \u2225\n2\n= max \u2206A\u2208U\n\u2225 \u2225\n\u2225 b\u2212 (A+\u2206A)x\u2217\n\u2225 \u2225 \u2225\n2\n= max \u2206A\u2208U\n\u2225 \u2225\n\u2225 b\u2212 (A\u0303+\u2206A)x\u2217\n\u2225 \u2225 \u2225\n2 .\nThese equalities hold because for j 6\u2208 I , x\u2217j = 0, hence the jth column of both A\u0303 and \u2206A has no effect on the residual.\nFor an arbitrary x\u2032, we have\nmax \u2206A\u2208U\u0303\n\u2225 \u2225\n\u2225 b\u2212 (A+\u2206A)x\u2032\n\u2225 \u2225 \u2225\n2\n\u2265 max \u2206A\u2208U\n\u2225 \u2225\n\u2225 b\u2212 (A\u0303+\u2206A)x\u2032\n\u2225 \u2225 \u2225\n2 .\nThis is because, \u2016aj \u2212 a\u0303j\u2016 \u2264 lj for j 6\u2208 I , and ai = a\u0303i for i \u2208 I . Hence, we have {\nA+\u2206A \u2223 \u2223\u2206A \u2208 U } \u2286 { A\u0303+\u2206A \u2223 \u2223\u2206A \u2208 U\u0303 } .\nFinally, notice that\nmax \u2206A\u2208U\n\u2225 \u2225\n\u2225 b \u2212 (A+\u2206A)x\u2217\n\u2225 \u2225 \u2225\n2 \u2264 max \u2206A\u2208U\n\u2225 \u2225\n\u2225 b \u2212 (A +\u2206A)x\u2032\n\u2225 \u2225 \u2225\n2 .\nTherefore we have\nmax \u2206A\u2208U\u0303\n\u2225 \u2225\n\u2225 b \u2212 (A\u0303+\u2206A)x\u2217\n\u2225 \u2225 \u2225\n2 \u2264 max \u2206A\u2208U\u0303\n\u2225 \u2225\n\u2225 b \u2212 (A\u0303 +\u2206A)x\u2032\n\u2225 \u2225 \u2225\n2 .\nSince this holds for arbitrary x\u2032, we establish the theorem. We can interpret the result of this theorem by considering a generative model1 b = \u2211\ni\u2208I wiai+\n\u03be\u0303 where I \u2286 {1 \u00b7 \u00b7 \u00b7 , m} and \u03be\u0303 is a random variable, i.e., b is generated by features belonging to I . In this case, for a feature j 6\u2208 I , Lasso would assign zero weight as long as there exists a perturbed value of this feature, such that the optimal regression assigned it zero weight.\nWhen we consider \u21132 loss, we can translate the condition of a feature being \u201cirrelevant\u201d into a geometric condition, namely, orthogonality. We now use the result of Theorem 5 to show that robust regression has a sparse solution as long as an incoherence-type property is satisfied. This result is more in line with the traditional sparsity results, but we note that the geometric reasoning is different, and ours is based on robustness. Indeed, we show that a feature receives zero weight, if it is \u201cnearly\u201d (i.e., within an allowable perturbation) orthogonal to the signal, and\n1While we are not assuming generative models to establish the results, it is still interesting to see how these results can help in a generative model setup.\nNovember 11, 2008 DRAFT\n10\nall relevant features. Theorem 6: Let ci = c for all i and consider \u21132 loss. If there exists I \u2282 {1, \u00b7 \u00b7 \u00b7 , m} such that for all v \u2208 span ( {ai, i \u2208 I} \u22c3 {b} )\n, \u2016v\u2016 = 1, we have v\u22a4aj \u2264 c, \u2200j 6\u2208 I , then any optimal solution x\u2217 satisfies x\u2217j = 0, \u2200j 6\u2208 I .\nProof: For j 6\u2208 I , let a=j denote the projection of aj onto the span of {ai, i \u2208 I} \u22c3 {b}, and let a+j , aj \u2212 a=j . Thus, we have \u2016a=j \u2016 \u2264 c. Let A\u0302 be such that\na\u0302i =\n{\nai i \u2208 I; a+i i 6\u2208 I.\nNow let U\u0302 , {(\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m)|\u2016\u03b4i\u20162 \u2264 c, i \u2208 I; \u2016\u03b4j\u20162 = 0, j 6\u2208 I}.\nConsider the robust regression problem minx\u0302 { max\u2206A\u2208U\u0302 \u2225 \u2225b\u2212 (A\u0302+\u2206A)x\u0302 \u2225 \u2225\n2\n}\n, which is equiv-\nalent to minx\u0302 { \u2225 \u2225b \u2212 A\u0302x\u0302 \u2225 \u2225\n2 +\n\u2211 i\u2208I c|x\u0302i| } . Note that the a\u0302j are orthogonal to the span of\n{a\u0302i, i \u2208 I} \u22c3 {b}. Hence for any given x\u0302, by changing x\u0302j to zero for all j 6\u2208 I , the minimizing objective does not increase.\nSince \u2016a\u0302 \u2212 a\u0302j\u2016 = \u2016a=j \u2016 \u2264 c \u2200j 6\u2208 I , (and recall that U = {(\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m)|\u2016\u03b4i\u20162 \u2264 c, \u2200i}) applying Theorem 5 concludes the proof.\nV. DENSITY ESTIMATION AND CONSISTENCY\nIn this section, we investigate the robust linear regression formulation from a statistical perspective and rederive using only robustness properties that Lasso is asymptotically consistent. The basic idea of the consistency proof is as follows. We show that the robust optimization formulation can be seen to be the maximum error w.r.t. a class of probability measures. This class includes a kernel density estimator, and using this, we show that Lasso is consistent."}, {"heading": "A. Robust Optimization, Worst-case Expected Utility and Kernel Density Estimator", "text": "In this subsection, we present some notions and intermediate results. In particular, we link a robust optimization formulation with a worst expected utility (w.r.t. a class of probability measures); we then briefly recall the definition of a kernel density estimator. Such results will be used in establishing the consistency of Lasso, as well as providing some additional insights on robust optimization. Proofs are postponed to the appendix.\nWe first establish a general result on the equivalence between a robust optimization formulation and a worst-case expected utility:\nProposition 1: Given a function g : Rm+1 \u2192 R and Borel sets Z1, \u00b7 \u00b7 \u00b7 ,Zn \u2286 Rm+1, let\nPn , {\u00b5 \u2208 P|\u2200S \u2286 {1, \u00b7 \u00b7 \u00b7 , n} : \u00b5( \u22c3\ni\u2208S Zi) \u2265 |S|/n}.\nThe following holds\n1\nn\nn \u2211\ni=1\nsup (ri,bi)\u2208Zi h(ri, bi) = sup \u00b5\u2208Pn\n\u222b\nRm+1\nh(r, b)d\u00b5(r, b).\nThis leads to the following corollary for Lasso, which states that for a given x, the robust regression loss over the training data is equal to the worst-case expected generalization error.\nNovember 11, 2008 DRAFT\n11\nCorollary 3: Given b \u2208 Rn, A \u2208 Rn\u00d7m, the following equation holds for any x \u2208 Rm,\n\u2016b\u2212 Ax\u20162 + \u221a ncn\u2016x\u20161 + \u221a ncn = sup\n\u00b5\u2208P\u0302(n)\n\u221a\nn\n\u222b\nRm+1\n(b\u2032 \u2212 r\u2032\u22a4x)2d\u00b5(r\u2032, b\u2032). (8)\nHere2,\nP\u0302(n) , \u22c3\n\u2016\u03c3\u20162\u2264 \u221a ncn;\u2200i:\u2016\u03b4i\u20162\u2264 \u221a ncn\nPn(A,\u2206,b,\u03c3);\nPn(A,\u2206,b,\u03c3) , {\u00b5 \u2208 P|Zi = [bi \u2212 \u03c3i, bi + \u03c3i]\u00d7 m \u220f\nj=1\n[aij \u2212 \u03b4ij , aij + \u03b4ij];\n\u2200S \u2286 {1, \u00b7 \u00b7 \u00b7 , n} : \u00b5( \u22c3\ni\u2208S Zi) \u2265 |S|/n}.\nRemark 1: We briefly explain Corollary 3 to avoid possible confusions. Equation (8) is a non-probabilistic equality. That is, it holds without any assumption (e.g., i.i.d. or generated by certain distributions) on b and A. And it does not involve any probabilistic operation such as taking expectation on the left-hand-side, instead, it is an equivalence relationship which hold for an arbitrary set of samples. Notice that, the right-hand-side also depends on the samples since P\u0302(n) is defined through A and b. Indeed, P\u0302(n) represents the union of classes of distributions Pn(A,\u2206,b,\u03c3) such that the norm of each column of \u2206 is bounded, where Pn(A,\u2206,b,\u03c3) is the set of distributions corresponds to (see Proposition 1) disturbance in hyper-rectangle Borel sets Z1, \u00b7 \u00b7 \u00b7 ,Zn centered at (bi, r\u22a4i ) with lengths (2\u03c3i, 2\u03b4i1, \u00b7 \u00b7 \u00b7 , 2\u03b4im).\nWe will later show that P\u0302n consists a kernel density estimator. Hence we recall here its definition. The kernel density estimator for a density h\u0302 in Rd, originally proposed in [24], [25], is defined by\nhn(x) = (nc d n)\n\u22121 n \u2211\ni=1\nK\n(\nx \u2212 x\u0302i cn\n)\n,\nwhere {cn} is a sequence of positive numbers, x\u0302i are i.i.d. samples generated according to f\u0302 , and K is a Borel measurable function (kernel) satisfying K \u2265 0, \u222b\nK = 1. See [26], [27] and the reference therein for detailed discussions. Figure 1 illustrates a kernel density estimator using Gaussian kernel for a randomly generated sample-set. A celebrated property of a kernel density estimator is that it converges in L1 to h\u0302 when cn \u2193 0 and ncdn \u2191 \u221e [26]."}, {"heading": "B. Consistency of Lasso", "text": "We restrict our discussion to the case where the magnitude of the allowable uncertainty for all features equals c, (i.e., the standard Lasso) and establish the statistical consistency of Lasso from a distributional robustness argument. Generalization to the non-uniform case is straightforward. Throughout, we use cn to represent c where there are n samples (we take cn to zero).\nRecall the standard generative model in statistical learning: let P be a probability measure with bounded support that generates i.i.d samples (bi, ri), and has a density f \u2217(\u00b7). Denote the\n2Recall that aij is the jth element of ri\nNovember 11, 2008 DRAFT\n12\nset of the first n samples by Sn. Define\nx(cn,Sn) , argmin x\n{\n\u221a \u221a \u221a \u221a 1\nn\nn \u2211\ni=1\n(bi \u2212 r\u22a4i x)2 + cn\u2016x\u20161 }\n= argmin x\n{ \u221a n\nn\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n(bi \u2212 r\u22a4i x)2 + cn\u2016x\u20161 } ;\nx(P) , argmin x\n{\n\u221a\n\u222b\nb,r\n(b\u2212 r\u22a4x)2dP(b, r) } .\nIn words, x(cn,Sn) is the solution to Lasso with the tradeoff parameter set to cn \u221a n, and x(P) is the \u201ctrue\u201d optimal solution. We have the following consistency result. The theorem itself is a wellknown result. However, the proof technique is novel. This technique is of interest because the standard techniques to establish consistency in statistical learning including Vapnik-Chervonenkis (VC) dimension (e.g., [28]) and algorithmic stability (e.g., [29]) often work for a limited range of algorithms, e.g., the k-Nearest Neighbor is known to have infinite VC dimension, and we show in Section VI that Lasso is not stable. In contrast, a much wider range of algorithms have robustness interpretations, allowing a unified approach to prove their consistency.\nTheorem 7: Let {cn} be such that cn \u2193 0 and limn\u2192\u221e n(cn)m+1 = \u221e. Suppose there exists a constant H such that \u2016x(cn,Sn)\u20162 \u2264 H . Then,\nlim n\u2192\u221e\n\u221a\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2dP(b, r) = \u221a \u222b\nb,r\n(b\u2212 r\u22a4x(P))2dP(b, r),\nalmost surely. Proof: Step 1: We show that the right hand side of Equation (8) includes a kernel density estimator for the true (unknown) distribution. Consider the following kernel estimator given\nNovember 11, 2008 DRAFT\n13\nsamples Sn = (bi, ri)ni=1 and tradeoff parameter cn,\nfn(b, r) , (nc m+1 n )\n\u22121 n \u2211\ni=1\nK\n(\nb\u2212 bi, r\u2212 ri cn\n)\n,\nwhere: K(x) , I[\u22121,+1]m+1(x)/2 m+1.\n(9)\nLet \u00b5\u0302n denote the distribution given by the density function fn(b, r). Easy to check that \u00b5\u0302n belongs to Pn(A, (cn1n, \u00b7 \u00b7 \u00b7 , cn1n),b, cn1n) and hence belongs to P\u0302(n) by definition.\nStep 2: Using the L1 convergence property of the kernel density estimator, we prove the consistency of robust regression and equivalently Lasso.\nFirst notice that, \u2016x(cn,Sn)\u20162 \u2264 H and P has a bounded support implies that there exists a universal constant C such that\nmax b,r\n(b\u2212 r\u22a4w(cn,Sn))2 \u2264 C.\nBy Corollary 3 and \u00b5\u0302n \u2208 P\u0302(n) we have \u221a\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5\u0302n(b, r)\n\u2264 sup \u00b5\u2208P\u0302(n)\n\u221a\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5(b, r)\n=\n\u221a n\nn\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n(bi \u2212 r\u22a4i x(cn,Sn))2 + cn\u2016x(cn,Sn)\u20161 + cn\n\u2264 \u221a n\nn\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n(bi \u2212 r\u22a4i x(P))2 + cn\u2016x(P)\u20161 + cn,\nthe last inequality holds by definition of x(cn,Sn). Taking the square of both sides, we have\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5\u0302n(b, r)\n\u22641 n\nn \u2211\ni=1\n(bi \u2212 r\u22a4i x(P))2 + c2n(1 + \u2016x(P)\u20161)2\n+ 2cn(1 + \u2016x(P)\u20161)\n\u221a \u221a \u221a \u221a 1\nn\nn \u2211\ni=1\n(bi \u2212 r\u22a4i x(P))2.\nNotice that, the right-hand side converges to \u222b b,r (b \u2212 r\u22a4x(P))2dP(b, r) as n \u2191 \u221e and cn \u2193 0\nNovember 11, 2008 DRAFT\n14\nalmost surely. Furthermore, we have \u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2dP(b, r)\n\u2264 \u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5\u0302n(b, r)\n+ [\nmax b,r\n(b\u2212 r\u22a4x(cn,Sn))2 ]\n\u222b\nb,r\n|fn(b, r)\u2212 f \u2217(b, r)|d(b, r)\n\u2264 \u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5\u0302n(b, r) + C \u222b\nb,r\n|fn(b, r)\u2212 f \u2217(b, r)|d(b, r),\nwhere the last inequality follows from the definition of C. Notice that \u222b b,r |fn(b, r)\u2212f \u2217(b, r)|d(b, r) goes to zero almost surely when cn \u2193 0 and ncm+1n \u2191 \u221e since fn(\u00b7) is a kernel density estimation of f \u2217(\u00b7) (see e.g. Theorem 3.1 of [26]). Hence the theorem follows.\nWe can remove the assumption that \u2016x(cn,Sn)\u20162 \u2264 H , and as in Theorem 7, the proof technique rather than the result itself is of interest.\nTheorem 8: Let {cn} converge to zero sufficiently slowly. Then\nlim n\u2192\u221e\n\u221a\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2dP(b, r) = \u221a \u222b\nb,r\n(b\u2212 r\u22a4x(P))2dP(b, r),\nalmost surely. Proof: To prove the theorem, we need to consider a set of distributions belonging to P\u0302(n). Hence we establish the following lemma first. Lemma 1: Partition the support of P as V1, \u00b7 \u00b7 \u00b7 , VT such the \u2113\u221e radius of each set is less than cn. If a distribution \u00b5 satisfies\n\u00b5(Vt) = \u2223 \u2223 \u2223 { i|(bi, ri) \u2208 Vt } \u2223 \u2223 \u2223 /n; t = 1, \u00b7 \u00b7 \u00b7 , T, (10)\nthen \u00b5 \u2208 P\u0302(n). Proof: Let Zi = [bi \u2212 cn, bi + cn]\u00d7 \u220fm\nj=1[aij \u2212 cn, aij + cn]; recall that aij the jth element of ri. Notice Vt has \u2113\u221e norm less than cn we have\n(bi, ri \u2208 Vt) \u21d2 Vt \u2286 Zi. Therefore, for any S \u2286 {1, \u00b7 \u00b7 \u00b7 , n}, the following holds\n\u00b5( \u22c3\ni\u2208S Zi) \u2265 \u00b5(\n\u22c3\nVt|\u2203i \u2208 S : bi, ri \u2208 Vt)\n= \u2211\nt|\u2203i\u2208S:bi,ri\u2208Vt\n\u00b5(Vt) = \u2211\nt|\u2203i\u2208S:bi,ri\u2208Vt\n# ( (bi, ri) \u2208 Vt ) /n \u2265 |S|/n.\nHence \u00b5 \u2208 Pn(A,\u2206, b, cn) where each element of \u2206 is cn, which leads to \u00b5 \u2208 P\u0302(n). Now we proceed to prove the theorem. Partition the support of P into T subsets such that \u2113\u221e radius of each one is smaller than cn. Denote P\u0303(n) as the set of probability measures satisfying Equation (10). Hence P\u0303(n) \u2286 P\u0302(n) by Lemma 1. Further notice that there exists a universal constant K such that \u2016x(cn,Sn)\u20162 \u2264 K/cn due to the fact that the square loss of the solution\nNovember 11, 2008 DRAFT\n15\nx = 0 is bounded by a constant only depends on the support of P. Thus, there exists a constant C such that maxb,r(b\u2212 r\u22a4x(cn,Sn))2 \u2264 C/c2n.\nFollow a similar argument as the proof of Theorem 7, we have\nsup \u00b5n\u2208P\u0303(n)\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5n(b, r)\n\u22641 n\nn \u2211\ni=1\n(bi \u2212 r\u22a4i x(P))2 + c2n(1 + \u2016x(P)\u20161)2\n+ 2cn(1 + \u2016x(P)\u20161)\n\u221a \u221a \u221a \u221a 1\nn\nn \u2211\ni=1\n(bi \u2212 r\u22a4i x(P))2,\n(11)\nand \u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2dP(b, r)\n\u2264 inf \u00b5n\u2208P\u0303(n)\n{\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5n(b, r)\n+ max b,r\n(b\u2212 r\u22a4x(cn,Sn))2 \u222b\nb,r\n|f\u00b5n(b, r)\u2212 f(b, r)|d(b, r)\n\u2264 sup \u00b5n\u2208P\u0303(n)\n\u222b\nb,r\n(b\u2212 r\u22a4x(cn,Sn))2d\u00b5n(b, r)\n+ 2C/c2n inf \u00b5\u2032n\u2208P\u0303(n)\n{\n\u222b\nb,r\n|f\u00b5\u2032n(b, r)\u2212 f(b, r)|d(b, r) } ,\nhere f\u00b5 stands for the density function of a measure \u00b5. Notice that P\u0303n is the set of distributions satisfying Equation (10), hence inf\u00b5\u2032n\u2208P\u0303(n) \u222b b,r |f\u00b5\u2032n(b, r) \u2212 f(b, r)|d(b, r) is upper-bounded by \u2211T t=1 |P(Vt) \u2212 #(bi, ri \u2208 Vt)|/n, which goes to zero as n increases for any fixed cn (see for example Proposition A6.6 of [30]). Therefore,\n2C/c2n inf \u00b5\u2032n\u2208P\u0303(n)\n{\n\u222b\nb,r\n|f\u00b5\u2032n(b, r)\u2212 f(b, r)|d(b, r) } \u2192 0,\nif cn \u2193 0 sufficiently slow. Combining this with Inequality (11) proves the theorem.\nVI. STABILITY\nKnowing that the robust regression problem (1) and in particular Lasso encourage sparsity, it is of interest to investigate another desirable characteristic of a learning algorithm, namely, stability. We show in this section that Lasso is not stable. This is a special case of a more general result we prove in [31], where we show that this is a common property for all algorithms that encourage sparsity. That is, if a learning algorithm achieves certain sparsity condition, then it cannot have a non-trivial stability bound.\nWe recall the definition of uniform stability [29] first. We let Z denote the space of points and labels (typically this will be a compact subset of Rn+1) so that S \u2208 Zm denotes a collection of m labelled training points. We let L denote a learning algorithm, and for S \u2208 Zm, we let LS\nNovember 11, 2008 DRAFT\n16\ndenote the output of the learning algorithm (i.e., the regression function it has learned from the training data). Then given a loss function l, and a labeled point s = (z, b) \u2208 Z , we let l(LS, s) denote the loss of the algorithm that has been trained on the set S, on the data point s. Thus for squared loss, we would have l(LS, s) = \u2016LS(z)\u2212 b\u20162.\nDefinition 1: An algorithm L has uniform stability bound of \u03b2m with respect to the loss function l if the following holds\n\u2200S \u2208 Zm, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , m}, \u2016l(LS, \u00b7)\u2212 l(LS\\i, \u00b7)\u2016\u221e \u2264 \u03b2m. Here LS\\i stands for the learned solution with the i\nth sample removed from S. At first glance, this definition may seem too stringent for any reasonable algorithm to exhibit good stability properties. However, as shown in [29], Tikhonov-regularized regression has stability that scales as 1/m. Stability that scales at least as fast as o( 1\u221a\nm ) can be used to establish strong PAC\nbounds (see [29]). In this section we show that not only is the stability (in the sense defined above) of Lasso much worse than the stability of \u21132-regularized regression, but in fact Lasso\u2019s stability is, in the following sense, as bad as it gets. To this end, we define the notion of the trivial bound, which is the worst possible error a training algorithm can have for arbitrary training set and testing sample labelled by zero.\nDefinition 2: Given a subset from which we can draw m labelled points, Z \u2286 Rn\u00d7(m+1) and a subset for one unlabelled point, X \u2286 Rm, a trivial bound for a learning algorithm L w.r.t. Z and X is\nb(L,Z,X ) , max S\u2208Z,z\u2208X l ( LS, (z, 0) ) .\nAs above, l(\u00b7, \u00b7) is a given loss function. Notice that the trivial bound does not diminish as the number of samples increases, since by repeatedly choosing the worst sample, the algorithm will yield the same solution.\nNow we show that the uniform stability bound of Lasso can be no better than its trivial bound with the number of features halved.\nTheorem 9: Given Z\u0302 \u2286 Rn\u00d7(2m+1) be the domain of sample set and X\u0302 \u2286 R2m be the domain of new observation, such that\n(b, A) \u2208 Z =\u21d2 (b, A, A) \u2208 X\u0302 , (z\u22a4) \u2208 X =\u21d2 (z\u22a4, z\u22a4) \u2208 X\u0302 .\nThen the uniform stability bound of Lasso is lower bounded by b(Lasso,Z,X ). Proof: Let (b\u2217, A\u2217) and (0, z\u2217\u22a4) be the sample set and the new observation such that they jointly achieve b(Lasso,Z,X ), and let x\u2217 be the optimal solution to Lasso w.r.t (b\u2217, A\u2217). Consider the following sample set\n( b\u2217 A\u2217 A\u2217\n0 0\u22a4 z\u2217\u22a4\n)\n.\nObserve that (x\u22a4, 0\u22a4)\u22a4 is an optimal solution of Lasso w.r.t to this sample set. Now remove the last sample from the sample set. Notice that (0\u22a4,x\u22a4)\u22a4 is an optimal solution for this new sample set. Using the last sample as a testing observation, the solution w.r.t the full sample set has zero cost, while the solution of the leave-one-out sample set has a cost b(Lasso,Z,X ). And hence we prove the theorem.\nNovember 11, 2008 DRAFT\n17\nVII. CONCLUSION\nIn this paper, we considered robust regression with a least-square-error loss. In contrast to previous work on robust regression, we considered the case where the perturbations of the observations are in the features. We show that this formulation is equivalent to a weighted \u21131 norm regularized regression problem if no correlation of disturbances among different features is allowed, and hence provide an interpretation of the widely used Lasso algorithm from a robustness perspective. We also formulated tractable robust regression problems for disturbance coupled among different features and hence generalize Lasso to a wider class of regularization schemes.\nThe sparsity and consistency of Lasso are also investigated based on its robustness interpretation. In particular we present a \u201cno-free-lunch\u201d theorem saying that sparsity and algorithmic stability contradict each other. This result shows, although sparsity and algorithmic stability are both regarded as desirable properties of regression algorithms, it is not possible to achieve them simultaneously, and we have to tradeoff these two properties in designing a regression algorithm.\nThe main thrust of this work is to treat the widely used regularized regression scheme from a robust optimization perspective, and extend the result of [13] (i.e., Tikhonov regularization is equivalent to a robust formulation for Frobenius norm bounded disturbance set) to a broader range of disturbance set and hence regularization scheme. This provides us not only with new insight of why regularization schemes work, but also offer solid motivations for selecting regularization parameter for existing regularization scheme and facilitate designing new regularizing schemes.\nREFERENCES\n[1] L. Elden. Perturbation theory for the least-square problem with linear equality constraints. BIT, 24:472\u2013476, 1985. [2] G. Golub and C. Van Loan. Matrix Computation. John Hopkins University Press, Baltimore, 1989. [3] D. Higham and N. Higham. Backward error and condition of structured linear systems. SIAM Journal on Matrix Analysis\nand Applications, 13:162\u2013175, 1992. [4] R. Fierro and J. Bunch. Collinearity and total least squares. SIAM Journal on Matrix Analysis and Applications, 15:1167\u2013\n1181, 1994. [5] A. Tikhonov and V. Arsenin. Solution for Ill-Posed Problems. Wiley, New York, 1977. [6] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B,\n58(1):267\u2013288, 1996. [7] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407\u2013499, 2004. [8] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing,\n20(1):33\u201361, 1998. [9] A. Feuer and A. Nemirovski. On sparse representation in pairs of bases. IEEE Transactions on Information Theory,\n49(6):1579\u20131581, 2003. [10] E. Cande\u0300s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2):489\u2013509, 2006. [11] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50(10):2231\u20132242, 2004. [12] M. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of spar-\nsity using \u21131-constrained quadratic programming. Technical Report Available from: http://http://www.stat.berkeley.edu/tech-reports/709.pdf, Department of Statistics, UC Berkeley, 2006.\n[13] L. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data. SIAM Journal on Matrix Analysis and Applications, 18:1035\u20131064, 1997. [14] P. Shivaswamy, C. Bhattacharyya, and A. Smola. Second order cone programming approaches for handling missing and uncertain data. Journal of Machine Learning Research, 7:1283\u20131314, July 2006. [15] D. Bertsimas and I. Popescu. Optimal inequalities in probability theory: A convex optimization approach. SIAM Journal of Optimization, 15(3):780\u2013800, 2004. [16] H. Xu, C. Caramanis, and S. Mannor. Robust regression and Lasso. Technical report, Gerad, Available from http://www.cim.mcgill.ca/\u223cxuhuan/LassoGerad.pdf, 2008.\nNovember 11, 2008 DRAFT\n18\n[17] D. Bertsimas and M. Sim. The price of robustness. Operations Research, 52(1):35\u201353, January 2004. [18] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal Of The Royal Statistical Society Series B, 67(2):301\u2013320, 2005. [19] J. Tropp. Just relax: Convex programming methods for identifying sparse signals. IEEE Transactions on Information Theory, 51(3):1030\u20131051, 2006. [20] F. Girosi. An equivalence between sparse approximation and support vector machines. Neural Computation, 10(6):1445\u2013 1480, 1998. [21] R. R. Coifman and M. V. Wickerhauser. Entropy-based algorithms for best-basis selection. IEEE Transactions on Information Theory, 38(2):713\u2013718, 1992. [22] S. Mallat and Z. Zhang. Matching Pursuits with time-frequence dictionaries. IEEE Transactions on Signal Processing, 41(12):3397\u20133415, 1993. [23] D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306, 2006. [24] M. Rosenblatt. Remarks on some nonparametric estimates of a density function. Annals of Mathematical Statistics, 27:832\u2013837, 1956. [25] E. Parzen. On the estimation of a probability density function and the mode. Annals of Mathematical Statistics, 33:1065\u2013 1076, 1962. [26] L. Devroye and L. Gyo\u0308rfi. Nonparametric Density Estimation: the l1 View. John Wiley & Sons, 1985. [27] D. Scott. Multivariate Density Estimation: Theory, Practice and Visualization. John Wiley & Sons, 1992. [28] V. Vapnik and A. Chervonenkis. The necessary and sufficient conditions for consistency in the empirical risk minimization method. Pattern Recognition and Image Analysis, 1(3):260\u2013284, 1991. [29] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499\u2013526, 2002. [30] A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag, New York, 2000. [31] H. Xu, C. Caramanis, and S. Mannor. Sparse algorithms are not stable: A no-free-lunch theorem. In Proceedings of\nForty-Sixth Allerton Conference on Communication, Control, and Computing, 2008.\nAPPENDIX A PROOF OF THEOREM 2\nTheorem 2. Consider a random vector v \u2208 Rn, such that E(v) = a, and E(vv\u22a4) = \u03a3, \u03a3 0. Then we have\nPr{\u2016v\u20162 \u2265 ci} \u2264\n\n      \n     \nminP,q,r,\u03bb Trace(\u03a3P ) + 2q \u22a4a + r\nsubject to:\n(\nP q q\u22a4 r\n)\n0 (\nI(m) 0 0\u22a4 \u2212c2i\n)\n\u03bb (\nP q q\u22a4 r \u2212 1\n)\n\u03bb \u2265 0.\n(12)\nProof: Consider a function f(\u00b7) parameterized by P,q, r defined as f(v) = v\u22a4Pv+2q\u22a4v+ r. Notice E ( f(v) )\n= Trace(\u03a3P ) + 2q\u22a4a+ r. Now we show that f(v) \u2265 1\u2016v\u2016\u2265ci for all P,q, r satisfying the constraints in (12).\nTo show f(v) \u2265 1\u2016v\u20162\u2265ci , we need to establish (i) f(v) \u2265 0 for all v, and (ii) f(v) \u2265 1 when \u2016v\u20162 \u2265 ci. Notice that\nf(v) =\n(\nv\n1 )\u22a4 ( P q q\u22a4 r )( v 1 ) ,\nhence (i) holds because (\nP q q\u22a4 r\n)\n0.\nTo establish condition (ii), it suffices to show v\u22a4v \u2265 c2i implies v\u22a4Pv+2q\u22a4v+r \u2265 1, which is equivalent to show { v \u2223 \u2223v\u22a4Pv + 2q\u22a4v + r \u2212 1 \u2264 0 } \u2286 { v \u2223\n\u2223v\u22a4v \u2264 c2i }\n. Noticing this is an ellipsoid-containment condition, by S-Procedure, we see that is equivalent to the condition that\nNovember 11, 2008 DRAFT\n19\nthere exists a \u03bb \u2265 0 such that (\nI(m) 0 0\u22a4 \u2212c2i\n)\n\u03bb (\nP q q\u22a4 r \u2212 1\n)\n.\nHence we have f(v) \u2265 1\u2016v\u20162\u2265ci , taking expectation over both side that notice that the expectation of a indicator function is the probability, we establish the theorem.\nAPPENDIX B PROOF OF THEOREM 4\nTheorem 4. Assume that the set\nZ , {z \u2208 Rm|fj(z) \u2264 0, j = 1, \u00b7 \u00b7 \u00b7 , k; z \u2265 0} has non-empty relative interior. Then the robust regression problem\nmin x\u2208Rm\n{\nmax \u2206A\u2208U \u2032\n\u2016b\u2212 (A+\u2206A)x\u2016a }\nis equivalent to the following regularized regression problem\nmin \u03bb\u2208Rk\n+ ,\u03ba\u2208Rm + ,x\u2208Rm\n{ \u2016b\u2212 Ax\u2016a + v(\u03bb,\u03ba,x) } ;\nwhere: v(\u03bb,\u03ba,x) , max c\u2208Rm\n[ (\u03ba + |x|)\u22a4c\u2212 k \u2211\nj=1\n\u03bbjfj(c) ]\nProof: Fix a solution x\u2217. Notice that,\nU \u2032 = {(\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m)|c \u2208 Z; \u2016\u03b4i\u2016a \u2264 ci, i = 1, \u00b7 \u00b7 \u00b7 , m}. Hence we have:\nmax \u2206A\u2208U \u2032\n\u2016b \u2212 (A+\u2206A)x\u2217\u2016a\n=max c\u2208Z\n{\nmax \u2016\u03b4i\u2016a\u2264ci, i=1,\u00b7\u00b7\u00b7 ,m\n\u2016b\u2212 ( A+ (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m) ) x\u2217\u2016a }\n=max c\u2208Z\n{ \u2016b\u2212Ax\u2217\u2016a + m \u2211\ni=1\nci|x\u2217i | }\n=\u2016b \u2212Ax\u2217\u2016a +max c\u2208Z\n{ |x\u2217|\u22a4c } .\n(13)\nThe second equation follows from Theorem 3. Now we need to evaluate maxc\u2208Z{|x\u2217|\u22a4c}, which equals to \u2212minc\u2208Z{\u2212|x\u2217|\u22a4c}. Hence we are minimizing a linear function over a set of convex constraints. Furthermore, by assumption the Slater\u2019s condition holds. Hence the duality gap of minc\u2208Z{\u2212|x\u2217|\u22a4c} is zero. A standard duality analysis shows that\nmax c\u2208Z\n{ |x\u2217|\u22a4c }\n= min \u03bb\u2208Rk\n+ ,\u03ba\u2208Rm +\nv(\u03bb,\u03ba,x\u2217). (14)\nNovember 11, 2008 DRAFT\n20\nWe establish the theorem by substituting Equation (14) back into Equation (13) and taking minimum over x on both sides.\nAPPENDIX C PROOF OF PROPOSITION 1\nProposition 1. Given a function g : Rm+1 \u2192 R and Borel sets Z1, \u00b7 \u00b7 \u00b7 ,Zn \u2286 Rm+1, let\nPn , {\u00b5 \u2208 P|\u2200S \u2286 {1, \u00b7 \u00b7 \u00b7 , n} : \u00b5( \u22c3\ni\u2208S Zi) \u2265 |S|/n}.\nThe following holds\n1\nn\nn \u2211\ni=1\nsup (ri,bi)\u2208Zi h(ri, bi) = sup \u00b5\u2208Pn\n\u222b\nRm+1\nh(r, b)d\u00b5(r, b).\nProof: To prove Proposition 1, we first establish the following lemma. Lemma 2: Given a function f : Rm+1 \u2192 R, and a Borel set Z \u2286 Rm+1, the following holds:\nsup x\u2032\u2208Z f(x\u2032) = sup \u00b5\u2208P|\u00b5(Z)=1\n\u222b\nRm+1\nf(x)d\u00b5(x).\nProof: Let x\u0302 be a \u01eb\u2212optimal solution to the left hand side, consider the probability measure \u00b5\u2032 that put mass 1 on x\u0302, which satisfy \u00b5\u2032(Z) = 1. Hence, we have\nsup x\u2032\u2208Z f(x\u2032)\u2212 \u01eb \u2264 sup \u00b5\u2208P|\u00b5(Z)=1\n\u222b\nRm+1\nf(x)d\u00b5(x),\nsince \u01eb can be arbitrarily small, this leads to\nsup x\u2032\u2208Z f(x\u2032) \u2264 sup \u00b5\u2208P|\u00b5(Z)=1\n\u222b\nRm+1\nf(x)d\u00b5(x). (15)\nNext construct function f\u0302 : Rm+1 \u2192 R as\nf\u0302(x) ,\n{\nf(x\u0302) x \u2208 Z; f(x) otherwise.\nBy definition of x\u0302 we have f(x) \u2264 f\u0302(x) + \u01eb for all x \u2208 Rm+1. Hence, for any probability measure \u00b5 such that \u00b5(Z) = 1, the following holds\n\u222b\nRm+1\nf(x)d\u00b5(x) \u2264 \u222b\nRm+1 f\u0302(x)d\u00b5(x) + \u01eb = f(x\u0302) + \u01eb \u2264 sup x\u2032\u2208Z f(x\u2032) + \u01eb.\nThis leads to sup\n\u00b5\u2208P|\u00b5(Z)=1\n\u222b\nRm+1 f(x)d\u00b5(x) \u2264 sup x\u2032\u2208Z f(x\u2032) + \u01eb.\nNotice \u01eb can be arbitrarily small, we have\nsup \u00b5\u2208P|\u00b5(Z)=1\n\u222b\nRm+1 f(x)d\u00b5(x) \u2264 sup x\u2032\u2208Z f(x\u2032) (16)\nCombining (15) and (16), we prove the lemma.\nNovember 11, 2008 DRAFT\n21\nNow we proceed to prove the proposition. Let x\u0302i be an \u01eb\u2212optimal solution to supxi\u2208Zi f(xi). Observe that the empirical distribution for (x\u03021, \u00b7 \u00b7 \u00b7 , x\u0302n) belongs to Pn, since \u01eb can be arbitrarily close to zero, we have\n1\nn\nn \u2211\ni=1\nsup xi\u2208Zi f(xi) \u2264 sup \u00b5\u2208Pn\n\u222b\nRm+1\nf(x)d\u00b5(x). (17)\nWithout loss of generality, assume\nf(x\u03021) \u2264 f(x\u03022) \u2264 \u00b7 \u00b7 \u00b7 \u2264 f(x\u0302n). (18) Now construct the following function\nf\u0302(x) ,\n{\nmini|x\u2208Zi f(x\u0302i) x \u2208 \u22c3n j=1Zj ; f(x) otherwise.\n(19)\nObserve that f(x) \u2264 f\u0302(x) + \u01eb for all x. Furthermore, given \u00b5 \u2208 Pn, we have\n\u222b\nRm+1\nf(x)d\u00b5(x)\u2212 \u01eb\n=\n\u222b\nRm+1\nf\u0302(x)d\u00b5(x)\n=\nn \u2211\nk=1\nf(x\u0302k) [ \u00b5(\nk \u22c3\ni=1\nZi)\u2212 \u00b5( k\u22121 \u22c3\ni=1\nZi) ]\nDenote \u03b1k , [ \u00b5( \u22c3k i=1 Zi)\u2212 \u00b5( \u22c3k\u22121 i=1 Zi) ] , we have\nn \u2211\nk=1\n\u03b1k = 1, t \u2211\nk=1\n\u03b1k \u2265 t/n.\nHence by Equation (18) we have n\n\u2211\nk=1\n\u03b1kf(x\u0302k) \u2264 1\nn\nn \u2211\nk=1\nf(x\u0302k).\nThus we have for any \u00b5 \u2208 Pn, \u222b\nRm+1\nf(x)d\u00b5(x)\u2212 \u01eb \u2264 1 n\nn \u2211\nk=1\nf(x\u0302k).\nTherefore,\nsup \u00b5\u2208Pn\n\u222b\nRm+1 f(x)d\u00b5(x)\u2212 \u01eb \u2264 sup xi\u2208Zi\n1\nn\nn \u2211\nk=1\nf(xk).\nNotice \u01eb can be arbitrarily close to 0, we proved the proposition by combining with (17).\nNovember 11, 2008 DRAFT\n22\nAPPENDIX D PROOF OF COROLLARY 3\nCorollary 3. Given b \u2208 Rn, A \u2208 Rn\u00d7m, the following equation holds for any x \u2208 Rm,\n\u2016b\u2212 Ax\u20162 + \u221a ncn\u2016x\u20161 + \u221a ncn = sup\n\u00b5\u2208P\u0302(n)\n\u221a\nn\n\u222b\nRm+1\n(b\u2032 \u2212 r\u2032\u22a4x)2d\u00b5(r\u2032, b\u2032). (20)\nHere,\nP\u0302(n) , \u22c3\n\u2016\u03c3\u20162\u2264 \u221a ncn;\u2200i:\u2016\u03b4i\u20162\u2264 \u221a ncn\nPn(A,\u2206,b,\u03c3);\nPn(A,\u2206,b,\u03c3) , {\u00b5 \u2208 P|Zi = [bi \u2212 \u03c3i, bi + \u03c3i]\u00d7 m \u220f\nj=1\n[aij \u2212 \u03b4ij , aij + \u03b4ij];\n\u2200S \u2286 {1, \u00b7 \u00b7 \u00b7 , n} : \u00b5( \u22c3\ni\u2208S Zi) \u2265 |S|/n}.\nProof: The right-hand-side of Equation (20) equals\nsup \u2016\u03c3\u20162\u2264 \u221a ncn;\u2200i:\u2016\u03b4i\u20162\u2264 \u221a ncn\n{\nsup \u00b5\u2208Pn(A,\u2206,b,\u03c3)\n\u221a\nn\n\u222b\nRm+1\n(b\u2032 \u2212 r\u2032\u22a4x)2d\u00b5(r\u2032, b\u2032) } .\nNotice by the equivalence to robust formulation, the left-hand-side equals to\nmax \u2016\u03c3\u20162\u2264 \u221a ncn;\u2200i:\u2016\u03b4i\u20162\u2264 \u221a ncn\n\u2225 \u2225 \u2225 b+ \u03c3 \u2212 ( A+ [\u03b41, \u00b7 \u00b7 \u00b7 , \u03b4m] ) x \u2225 \u2225 \u2225\n2\n= sup \u2016\u03c3\u20162\u2264 \u221a ncn; \u2200i:\u2016\u03b4i\u20162\u2264 \u221a ncn\n\n\n sup (b\u0302i,r\u0302i)\u2208[bi\u2212\u03c3i,bi+\u03c3i]\u00d7 Qm j=1[aij\u2212\u03b4ij ,aij+\u03b4ij ]\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n(b\u0302i \u2212 r\u0302\u22a4i x)2  \n\n= sup \u2016\u03c3\u20162\u2264 \u221a ncn; \u2200i:\u2016\u03b4i\u20162\u2264 \u221a ncn\n\u221a \u221a \u221a \u221a n \u2211\ni=1 sup (b\u0302i,r\u0302i)\u2208[bi\u2212\u03c3i,bi+\u03c3i]\u00d7 Qm j=1[aij\u2212\u03b4ij ,aij+\u03b4ij ]\n(b\u0302i \u2212 r\u0302\u22a4i x)2,\nfurthermore, applying Proposition 1 yields \u221a\n\u221a \u221a \u221a n \u2211\ni=1 sup (b\u0302i,r\u0302i)\u2208[bi\u2212\u03c3i,bi+\u03c3i]\u00d7 Qm j=1[aij\u2212\u03b4ij ,aij+\u03b4ij ]\n(b\u0302i \u2212 r\u0302\u22a4i x)2\n=\n\u221a\nsup \u00b5\u2208Pn(A,\u2206,b,\u03c3) n\n\u222b\nRm+1\n(b\u2032 \u2212 r\u2032\u22a4x)2d\u00b5(r\u2032, b\u2032)\n= sup \u00b5\u2208Pn(A,\u2206,b,\u03c3)\n\u221a\nn\n\u222b\nRm+1\n(b\u2032 \u2212 r\u2032\u22a4x)2d\u00b5(r\u2032, b\u2032),\nwhich proves the corollary.\nNovember 11, 2008 DRAFT"}], "references": [{"title": "Perturbation theory for the least-square problem with linear equality constraints", "author": ["L. Elden"], "venue": "BIT, 24:472\u2013476,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Matrix Computation", "author": ["G. Golub", "C. Van Loan"], "venue": "John Hopkins University Press, Baltimore,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Backward error and condition of structured linear systems", "author": ["D. Higham", "N. Higham"], "venue": "SIAM Journal on Matrix Analysis and Applications, 13:162\u2013175,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Collinearity and total least squares", "author": ["R. Fierro", "J. Bunch"], "venue": "SIAM Journal on Matrix Analysis and Applications, 15:1167\u2013 1181,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Solution for Ill-Posed Problems", "author": ["A. Tikhonov", "V. Arsenin"], "venue": "Wiley, New York,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1977}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B, 58(1):267\u2013288,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, 32(2):407\u2013499,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Atomic decomposition by basis pursuit", "author": ["S. Chen", "D. Donoho", "M. Saunders"], "venue": "SIAM Journal on Scientific Computing, 20(1):33\u201361,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "On sparse representation in pairs of bases", "author": ["A. Feuer", "A. Nemirovski"], "venue": "IEEE Transactions on Information Theory, 49(6):1579\u20131581,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory, 52(2):489\u2013509,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J. Tropp"], "venue": "IEEE Transactions on Information Theory, 50(10):2231\u20132242,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Sharp thresholds for noisy and high-dimensional recovery of sparsity using l1-constrained quadratic programming", "author": ["M. Wainwright"], "venue": " Technical Report Available from: http://http://www.stat.berkeley.edu/tech-reports/709.pdf, Department of Statistics, UC Berkeley,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust solutions to least-squares problems with uncertain data", "author": ["L. El Ghaoui", "H. Lebret"], "venue": "SIAM Journal on Matrix Analysis and Applications, 18:1035\u20131064,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Second order cone programming approaches for handling missing and uncertain data", "author": ["P. Shivaswamy", "C. Bhattacharyya", "A. Smola"], "venue": "Journal of Machine Learning Research, 7:1283\u20131314, July", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal inequalities in probability theory: A convex optimization approach", "author": ["D. Bertsimas", "I. Popescu"], "venue": "SIAM Journal of Optimization, 15(3):780\u2013800,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust regression and Lasso", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Technical report, Gerad, Available from http://www.cim.mcgill.ca/\u223cxuhuan/LassoGerad.pdf,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "The price of robustness", "author": ["D. Bertsimas", "M. Sim"], "venue": "Operations Research, 52(1):35\u201353, January", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal Of The Royal Statistical Society Series B, 67(2):301\u2013320,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Just relax: Convex programming methods for identifying sparse signals", "author": ["J. Tropp"], "venue": "IEEE Transactions on Information Theory, 51(3):1030\u20131051,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "An equivalence between sparse approximation and support vector machines", "author": ["F. Girosi"], "venue": "Neural Computation, 10(6):1445\u2013 1480,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Entropy-based algorithms for best-basis selection", "author": ["R.R. Coifman", "M.V. Wickerhauser"], "venue": "IEEE Transactions on Information Theory, 38(2):713\u2013718,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Matching Pursuits with time-frequence dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, 41(12):3397\u20133415,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, 52(4):1289\u20131306,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "Annals of Mathematical Statistics, 27:832\u2013837,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1956}, {"title": "On the estimation of a probability density function and the mode", "author": ["E. Parzen"], "venue": "Annals of Mathematical Statistics, 33:1065\u2013 1076,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1962}, {"title": "Nonparametric Density Estimation: the l1 View", "author": ["L. Devroye", "L. Gy\u00f6rfi"], "venue": "John Wiley & Sons,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "Multivariate Density Estimation: Theory, Practice and Visualization", "author": ["D. Scott"], "venue": "John Wiley & Sons,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "The necessary and sufficient conditions for consistency in the empirical risk minimization method", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Pattern Recognition and Image Analysis, 1(3):260\u2013284,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1991}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research, 2:499\u2013526,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Weak Convergence and Empirical Processes", "author": ["A. van der Vaart", "J. Wellner"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Sparse algorithms are not stable: A no-free-lunch theorem", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Proceedings of Forty-Sixth Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "It is well known that minimizing the least squared error can lead to sensitive solutions [1]\u2013 [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "It is well known that minimizing the least squared error can lead to sensitive solutions [1]\u2013 [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]\u2013[12] and references therein).", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]\u2013[12] and references therein).", "startOffset": 222, "endOffset": 226}, {"referenceID": 12, "context": "In itself, this interpretation of Lasso as the solution to a robust least squares problem is a development in line with the results of [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "We postpone the proof to the appendix, and refer the reader to [15] for similar results using semi-definite optimization.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "For example, it is straightforward to show that if we take both \u2016 \u00b7 \u2016\u03b1 and \u2016 \u00b7 \u2016s as the Euclidean norm, then U \u2032 is the set of matrices with their Frobenious norms bounded, and Corollary 1 reduces to the robust formulation introduced by [13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 15, "context": "We refer the interested reader to [16] for full details.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "The resulting formulation resembles the elastic-net formulation [18], where there is a combination of l and l regularization.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "Lasso\u2019s ability to recover sparse solutions has been extensively studied and discussed (cf [8]\u2013[11]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Lasso\u2019s ability to recover sparse solutions has been extensively studied and discussed (cf [8]\u2013[11]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Using similar tools, we provide additional results in [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": ", [19], and are used as standard tools in investigating sparsity of Lasso from the statistical perspective.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "The kernel density estimator for a density \u0125 in R, originally proposed in [24], [25], is defined by hn(x) = (nc d n) \u22121 n \u2211", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "The kernel density estimator for a density \u0125 in R, originally proposed in [24], [25], is defined by hn(x) = (nc d n) \u22121 n \u2211", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "See [26], [27] and the reference therein for detailed discussions.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "See [26], [27] and the reference therein for detailed discussions.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "A celebrated property of a kernel density estimator is that it converges in L1 to \u0125 when cn \u2193 0 and ncn \u2191 \u221e [26].", "startOffset": 108, "endOffset": 112}, {"referenceID": 27, "context": ", [28]) and algorithmic stability (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": ", [29]) often work for a limited range of algorithms, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "1 of [26]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "6 of [30]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "This is a special case of a more general result we prove in [31], where we show that this is a common property for all algorithms that encourage sparsity.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "We recall the definition of uniform stability [29] first.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "However, as shown in [29], Tikhonov-regularized regression has stability that scales as 1/m.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "Stability that scales at least as fast as o( 1 \u221a m ) can be used to establish strong PAC bounds (see [29]).", "startOffset": 101, "endOffset": 105}], "year": 2008, "abstractText": "Lasso, or l regularized least squares, has been explored extensively for its remarkable sparsity properties. It is shown in this paper that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets. Secondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.", "creator": "LaTeX with hyperref package"}}}