{"id": "0903.2851", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2009", "title": "A Parameter-free Hedging Algorithm", "abstract": "in this paper, to utilize maximal decision - theoretic framework for online software ( dtol ) proposed since freund and schapire \\ security { fs97 }. previous algorithms for learning in this framework have a tunable learning rate parameter. designing the learning rate requires prior assumptions about sampling sequence and severely fails experimental accuracy inside his algorithm. whilst preliminary progress has be made in such whole decade for adaptively tracking the learning rate, implementation of comparable methods help ultimately frown concerning constraint functional implementations. more propose a maximum parameter - free discipline enabling courses in this framework. we continue theoretically fitting our algorithm has weak regret bound serving as performance best bounds achieved by developing algorithms with optimally - tuned averaging rates. we also present any few comments relating the predictions of the algorithm with that of other routines for the courses.", "histories": [["v1", "Mon, 16 Mar 2009 20:48:33 GMT  (70kb)", "http://arxiv.org/abs/0903.2851v1", null], ["v2", "Mon, 18 Jan 2010 23:58:51 GMT  (29kb)", "http://arxiv.org/abs/0903.2851v2", "Updated Version"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["kamalika chaudhuri", "yoav freund", "daniel j hsu"], "accepted": true, "id": "0903.2851"}, "pdf": {"name": "0903.2851.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kamalika@soe.ucsd.edu", "yfreund@ucsd.edu", "djhsu@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 3.\n28 51\nv1 [\ncs .L\nG ]\n1 6\nM ar\n2 00\nWe propose a completely parameter-free algorithm for learning in this framework. We show theoretically that our algorithm has a regret bound similar to the best bounds achieved by previous algorithms with optimally-tuned learning rates. We also present a few experiments comparing the performance of the algorithm with that of other algorithms for various tunings."}, {"heading": "1 Introduction", "text": "In this paper we consider the decision-theoretic framework for online learning (DTOL) proposed by Freund and Schapire [FS97]. DTOL is a variant of the framework of prediction with expert advice introduced by Littlestone and Warmuth [LW94] and Vovk [Vov98]. In this setting, a forecaster repeatedly assigns probabilities to a fixed set of actions. After each assignment, the actual loss associated with each action is revealed. The losses are restricted to the range [0, 1]. The forecaster\u2019s loss on each round is the average loss of actions for that round, where the average is computed according to the forecaster\u2019s current probability assignment. The goal of the forecaster is to achieve, on any sequence of losses, a cumulative loss close to the lowest cumulative loss among all single actions. We call this action the best action; and we call regret the difference between the cumulative loss achieved by the forecaster on a fixed loss sequence, and the cumulative loss of the best action.\nFreund and Schapire [FS97, FS99] use the Hedge algorithm, which assigns to the ith action a probability proportional to exp(\u2212\u03b7Xi) where Xi is the cumulative loss of action i and \u03b7 > 0 is a parameter called the learning rate. By\nappropriately tuning \u03b7, Hedge can achieve a regret bounded by O( \u221a T lnN), where T is the number of iterations and N is the number of actions. A matching lower bound of \u2126( \u221a T lnN) is proven in [FS99]. A disadvantage of the upper bound is that it holds only for particular values of T and N because the learning rate \u03b7 is set as a function of these parameters. It is clear that setting T in advance is a serious limitation; we would like an algorithm that performs close to optimally for every sequence length. It is less obvious why fixing the number of actions ahead of time is a limitation. To appreciate this, suppose that the actions consist of two equalsized sets where all of the actions in each set have an identical sequence of losses. While the number of actions N might be very large, the effective number of actions is two. Without prior information about the loss sequences we cannot set N correctly and therefore our choice of the learning rate \u03b7 will be suboptimal. In order to quantify the effect of this phenomenon, we introduce a new notion of regret. We order the cumulative losses of all actions from lowest to highest and define the regret of the forecaster to the top \u01eb-quantile to be the difference between the cumulative loss of the forecaster and the \u230a\u01ebN\u230b-th element in the sorted list.\nIn this paper we present a new hedging algorithm for the DTOL problem. This new algorithm has no parameters. We show an upper bound on the regret to the top \u01eb-quantile of the form\nO\n(\n\u221a\nT ln 1\n\u01eb + ln3 N\n)\n,\nwhich holds simultaneously for all T and \u01eb. If we set \u01eb = 1/N we get a bound on the regret to the best action of the form O ( \u221a T lnN + ln3 N ) which is just slight worse than\nthe bound achieved by Hedge with optimally-tuned parameters, and which holds only for a specific N and T .\nGoing back to the illustrative example, while N might be very large, our algorithm still gets a bound of the form O( \u221a\nT ln 2 + ln3 N) on the regret of the algorithm to the 1/2-quantile (or, in other words, the median).\nAnother useful property of our algorithm is that it assigns zero probability to any action whose cumulative loss is larger than the cumulative loss of the algorithm itself. In other words, non-zero weights are assigned only to actions which perform better than the algorithm. In most applications of DTOL we expect a small set of the actions to perform significantly better than most of the actions. As the regret of\nthe hedging algorithm is guaranteed to be small, this means that the algorithm will perform better than most of the actions and will therefore assign them zero probability. This can significantly reduce the computational cost when hedging over a large set of actions.\nThe rest of the paper is organized as follows. In Section 2, we review the DTOL setting and present our parameterfree algorithm called Normal-Hedge. Section 3 gives an account of how Normal-Hedge was derived in terms of drifting games [Sch01, FO02]. We review other related work in Section 4. Section 5 describes experiments that illustrate the need for adaptivity with respect to the number of actions N . Finally, the formal regret bounds and analysis are given in Sections 6 and 7."}, {"heading": "2 Algorithm", "text": ""}, {"heading": "2.1 Setting", "text": "We consider the decision-theoretic framework for online learning. In this setting, the learner is given access to a set of N actions, where N \u2265 2. In round t, the learner chooses a weight distribution pt = (p1,t, . . . , pN,t) over the actions 1, 2, . . . , N . Each action i incurs a loss li,t, and the learner incurs the expected loss\nlA,t = N \u2211\ni=1\npi,tli,t.\nThe learner\u2019s instantaneous regret with respect to an action i in round t is ri,t = lA,t \u2212 li,t, and the regret with respect to an action i in each of the first t rounds is\nRi,t = t \u2211\n\u03c4=1\nri,\u03c4 .\nWe assume that the range of the losses li,t is an interval of length 1 (e.g. [0, 1] or [\u22121/2, 1/2]; the sign of the loss does not matter).\nThe goal of the learner is to minimize this cumulative regret Ri,t for any value of t. The algorithm we present guarantees a bound on the regret to the best action\ni = arg max i\u2032=1,...,N Ri\u2032,t ."}, {"heading": "2.2 Normal-Hedge", "text": "Our algorithm, Normal-Hedge, is based on a potential function reminiscent of the half-normal distribution, specifically\n\u03c6(x, c) = exp\n(\n([x]+) 2\n2c\n)\nfor x \u2208 R, c > 0 (1)\nwhere [x]+ denotes max{0, x}. It is easy to check that this function is separately convex in x and c, differentiable, and twice-differentiable except at x = 0. Figure 4 catalogues the derivatives.\nIn addition to tracking the cumulative regretsRi,t to each action i after each round t, the algorithm also maintains a scale parameter ct. This is chosen so that the average of the potential, over all actions i, evaluated at Ri,t and ct, remains constant at e:\n1\nN\nN \u2211\ni=1\nexp\n(\n([Ri,t]+) 2\n2ct\n)\n= e. (2)\nWe observe that since \u03c6(x, c) is convex in c > 0, we can determine the value of ct with a line search.\nThe weight assigned to i in round t is set proportional to the first-derivative of the potential, evaluated at Ri,t\u22121 and ct\u22121:\npi,t \u221d \u2202\n\u2202x \u03c6(x, c)\n\u2223 \u2223 \u2223 \u2223\nx=Ri,t\u22121,c=ct\u22121\n= [Ri,t\u22121]+\nct\u22121 exp\n(\n([Ri,t\u22121]+) 2\n2ct\u22121\n)\n.\nNotice that the actions for which Ri,t\u22121 \u2264 0 receive zero weight in round t.\nWe summarize the learning algorithm in Figure 1."}, {"heading": "3 Derivation of the algorithm", "text": "Before we present the rigorous analysis of the algorithm, it might be useful to describe the ideas that led us to its design. This work is based on the binomial weights (BW) algorithm [CBFHW96] and on its analysis using drifting games [Sch01, FO02]. Due to space constraints we focus on the differences between Normal-Hedge and the BW algorithm.\nWe first remark on differences in the respective frameworks. The BW algorithm receives as input a bound k on the total number of mistakes that the best action (or expert); this input is not given to Normal-Hedge. In addition, the payoffs in BW are restricted to be either 0 or \u22121; in Normal-Hedge, they can be any number from an interval of length 1.\nThere are two ideas in common between BW and NormalHedge. The first is a potential function whose average value does not increase between iterations of the game. The second is a simple randomized strategy for the adversary of the hedging algorithm. The adversarial strategy is to select each payoff to be either \u22121 or 0 with equal probability. The potential function is a function of total payoff and time which is defined by recursion backwards in time. The potential associated with payoff x at time t is the expected potential at time t+ 1 if the adversary uses the randomized strategy.\nThe BW algorithm corresponds to a drifting game with a finite horizon where the potential at the end of the game is one if total payoff is larger than \u2212k and zero otherwise (and this value k is given to the algorithm). Using this potential\nfunction, one can derive a weighting scheme that guarantees a small upper bound on the number of mistakes of the algorithm; this weighting scheme is exactly the BW algorithm.\nNormal-Hedge is based on a drifting game with an unbounded horizon. The potential function depends on the regret relative to the action, rather than on the total payoff of the action. The potential function is designed in such a way that it has the same functional form in all iterations. Intuitively, we want a potential function that increases as fast as possible as a function of the regret, because a faster increasing potential function yields smaller bounds on the regret. On the other hand, if the potential function increases too fast then it will change its functional form between iterations. We therefore search for the fastest increasing potential function under the restriction that it maintains its functional form. Because the payoffs of the adversary are random with mean zero, the total payoff of the optimal algorithm is always zero and the regrets with respect to each action is equal to the total gain of that action. The distribution of the total payoffs is binomial and by the central limit theorem closely approximated by the normal distribution. A potential function that is the reciprocal of the probability distribution of the regrets will maintain its functional form. This yields the potential function defined in Equation (1).\nAnother difference between Normal-Hedge and BW is that payoffs for the actions hedged over by Normal-Hedge can take any value in the range [\u22121, 0], rather than just the two endpoints. Extending the drifting games analysis to this case can be done by using the continuous time limit described in [FO02]. However, doing this requires changing the setup of the problem to that of prediction in continuous time.1 In this paper, we avoid this change prove our results in the standard discrete time framework."}, {"heading": "4 Related work", "text": "A significant portion of the online learning literature is devoted to improving the adaptivity of the learning algorithm. Many of the works are variations of the exponential weights algorithm (e.g. Hedge), which was originally based on Littlestone and Warmuth\u2019s Weighted Majority algorithm [LW94]. In exponential weights, the weight assigned to action i in round t is proportional to e\u03b7Ri,t\u22121 . The parameter \u03b7 is the learning rate that controls the algorithm\u2019s sensitivity to the advantage of one action over another. Here we highlight some of these variations of exponential weights, as well as a few other algorithms, as they relate to Normal-Hedge.\nThe issue of time adaptivity\u2014i.e. not knowing the time horizon in advance\u2014was originally addressed in [CBFH+97] with a doubling trick: guess the horizon (and tune the algorithm accordingly), and double the guess if it turns out incorrect. Time-varying learning rates were considered in [ACBG02] that improve on the doubling trick. Specifically, they show that if \u03b7t = \u221a\n8 ln(N)/t, then using exponential weights with \u03b7 = \u03b7t in round t guarantees regret bounds of\u221a 2T lnN + O( \u221a lnN) for any T . This bound compares favorably to ours, but the algorithm requires tuning with respect to the number of actions N .\n1A different extension of the BW algorithm to the case of continuous outcomes was done by Mukherjee and Schapire [MS08].\nMore recent work in [YEYS04, CBMS07, HK08] give adaptive algorithms with significantly improved bounds when the total loss of the best action or the total variation in the losses is small. That is, the regret bounds of O( \u221a\nL\u2217T lnN)\nor O( \u221a V \u2217T lnN) only depend implicitly on T throughL \u2217 T = mini \u2211T t=1 li,t or V \u2217 T = maxi \u2211T t=1(li,t \u2212 \u2211T \u03c4=1 li,\u03c4/T ) 2 (the notion of variation in [CBMS07] is slightly different, but the algorithm avoids the need for doubling tricks). One of the techniques in [CBMS07] of setting \u03b7t according to the total loss variation is similar to ours, and thus we believe some of their analysis can be adapted for our algorithm. We stress, however, that these previous algorithms still require learning rates that depend on N .\nBesides exponential weights, time-adaptive methods can be derived from other families of online learning algorithms. The polynomial weights algorithms studied in [GLS01, Gen03, CBL03] are inherently time-adaptive but depend crucially on the number of actions N . The weight assigned to action i in round t is proportional to ([Ri,t\u22121]+)p for some p > 1; setting p = 2 lnN yields regret bounds of the form \u221a\n2eT (lnN \u2212 0.5) for any T . Note that our algorithm and polynomial weights share the feature that zero weight is given to actions that are performing worse than the algorithm, although the degree of this weight sparsity depends on the performance of the algorithm. Finally, [HP05] derive a timeadaptive variation of the follow-the-(perturbed) leader algorithm [Han57, KV05] by scaling the perturbations by a learning rate that depends on both t and N ."}, {"heading": "5 Experiments", "text": "We report a few simple synthetic experiments that illustrate the effect of tuning with respect to the number of actions N . In these experiments, the instantaneous losses of the actions are given by a matrix AN based on the Hadamard matrix (deleting the all-ones row, stacking the result on top of its negation, and then repeating each row infinitely). The loss of action i in round t is the (i, t)-th entry in the matrix AN . We show A6 for concreteness, but it easily generalizes to any N = 2d+1 \u2212 2 using the 2d \u00d7 2d Hadamard matrix:\nA6 =\n\n    \n \u22121/2 +1 \u22121 +1 \u22121 +1 \u22121 +1 . . . \u22121/2 \u22121 +1 +1 \u22121 \u22121 +1 +1 . . . \u22121/2 +1 +1 \u22121 \u22121 +1 +1 \u22121 . . . +1/2 \u22121 +1 \u22121 +1 \u22121 +1 \u22121 . . . +1/2 +1 \u22121 \u22121 +1 +1 \u22121 \u22121 . . . +1/2 \u22121 \u22121 +1 +1 \u22121 \u22121 +1 . . .\n(the first column is halved for convenience). Thus, the action(s) with smallest total losses changes in every round. Most learning algorithms will tends towards assigning all actions the same weight.\nWe actually use a slightly different loss matrix A\u03b5,KN , which is the same as AN except that \u03b5 is subtracted from every entry in the first K rows. Here, a successful algorithm will recognize that the first K actions perform better than the remaining N \u2212K , and then assign these K actions equal weights.\nWe compare the performance of Normal-Hedge to two other algorithms: \u201cExp\u201d, a time/variation-adaptive exponential weights algorithm due to [CBMS07]; and \u201cPoly\u201d, polynomial weights with p = 2 lnN [Gen03].\nFigure 2 shows the regrets to the best action versus time. In the plots marked with diamonds, Exp and Poly are given the correct values of N . In the plots marked with circles and squares, the algorithms are told the number of actions is 16N and 1024N , respectively. This simulates the effect of replicating each action 16 or 1024 times. We see that tuning Exp and Poly with respect to the number of actions (including replications) can result in suboptimal behavior: when K = 32 or K = 8, the algorithms are too sensitive to the changes in total loss, so they are slow to stabilize the weights over the best set of actions. When K = 1, Exp and Poly actually perform better when told an inflated value of N , as this causes the slight advantage of the single best action to be magnified. However, this is a fortuitous exception and not the rule. In Figure 3, we plot the regret after t = 32768 rounds versus the replication factor. Even when K = 2, performance degrades as the replication factor increases."}, {"heading": "6 Analysis", "text": ""}, {"heading": "6.1 Main results", "text": "Our main result is the following theorem.\nTheorem 1. If Normal-Hedge has access to N actions, then for all action loss sequences, for all t, for all 0 < \u01eb \u2264 1 and for all 0 < \u03b4 \u2264 1/2, the regret of the algorithm to the top \u01eb-quantile of the actions is at most \u221a\n(1 + ln(1/\u01eb))\n( 3(1 + 50\u03b4)t+ 16 ln2 N \u03b4 ( 10.2 \u03b42 + lnN) ) .\nIn particular, by setting \u01eb = 1/N we get that the regret to the best action is at most \u221a\n(1 + lnN)\n( 3(1 + 50\u03b4)t+ 16 ln2 N\n\u03b4 ( 10.2 \u03b42 + lnN)\n)\n.\nThe following corollary illustrates the performance of our algorithm for large values of t.\nCorollary 2. If Normal-Hedge has access toN actions, then, as t \u2192 \u221e, the regret of Normal-Hedge to the top \u01eb-quantile of actions can be made to approach an upper bound of\n\u221a\n3t(1 + ln(1/\u01eb)) + o(t) .\nIn particular, the regret of Normal-Hedge to the best action can be made to approach an upper bound of of\n\u221a\n3t(1 + lnN) + o(t) .\nThe proof of Theorem 1 follows from a combination of Lemmas 3, 4, and 5, and is presented in detail at the end of the current section."}, {"heading": "6.2 Regret bounds from the potential equation", "text": "The following lemma relates the performance of the algorithm at time t to the scale ct.\nLemma 3. At any time t, the regret to the best action can be bounded as:\nmax i\nRi,t \u2264 \u221a 2ct(lnN + 1)\nMoreover, for any 0 \u2264 \u01eb \u2264 1 and any t, the regret to the top \u01eb-quantile of actions is at most\n\u221a\n2ct(ln(1/\u01eb) + 1).\nProof. We use Et to denote the actions that have non-zero weight on iteration t. The first part of the lemma follows from the fact that, for any action i \u2208 Et,\nexp\n(\n(Ri,t) 2\n2ct\n)\n= exp\n(\n([Ri,t]+) 2\n2ct\n)\n\u2264 N \u2211\ni\u2032=1\nexp\n(\n([Ri\u2032,t]+) 2\n2ct\n)\n\u2264 Ne\nwhich implies Ri,t \u2264 \u221a\n2ct(lnN + 1). For the second part of the lemma, let Ri,t denote the regret of our algorithm to the action with the \u01ebN -th highest regret. Then, the total potential of the actions with regrets greater than or equal to Ri,t is at least:\n\u01ebN exp\n(\n([Ri,t]+) 2\n2ct\n)\n\u2264 Ne\nfrom which the second part of the lemma follows."}, {"heading": "6.3 Bounds on the scale ct and proof of Theorem 1", "text": "In Lemmas 4 and 5, we bound the growth of the scale ct as a function of the time t.\nThe main outline of the proof of Theorem 1 is as follows. As ct increases monotonically with t, we can divide the rounds t into two phases, t < t0 and t \u2265 t0, where t0 is the first time such that\nct0 \u2265 4 ln2 N\n\u03b4 +\n16 lnN\n\u03b43 ,\nfor some fixed \u03b4 \u2208 (0, 1/2). We then show bounds on the growth of ct for each phase separately. Lemma 4 shows that ct is not too large at the end of the first phase, while Lemma 5 bounds the per-round growth of ct in the second phase.\nLemma 4. For any time t,\nct+1 \u2264 2ct(1 + lnN) + 3 .\nLemma 5. Suppose that at some time t0, ct0 \u2265 4 ln 2 N \u03b4 + 16 lnN \u03b43 , where 0 \u2264 \u03b4 \u2264 12 is a constant. Then, for any time t \u2265 t0, ct+1 \u2212 ct \u2264 3\n2 (1 + 49.19\u03b4) .\nWe now combine Lemmas 4 and 5 together with Lemma 3 to prove the main theorem.\nProof of Theorem 1. Let t0 be the first time at which ct0 \u2265 4 ln2 N\n\u03b4 + 16 lnN \u03b43 . Then, from Lemma 4,\nct0 \u2264 2ct0\u22121(1 + lnN) + 3, which is at most:\n8 ln3 N\n\u03b4 +\n34 ln2 N\n\u03b43 +\n32 lnN \u03b43 + 3 \u2264 8 ln\n3 N\n\u03b4 +\n81 ln2 N\n\u03b43 .\nThe last inequality follows because N \u2265 2 and \u03b4 \u2264 1/2. By Lemma 5, we have that for any t \u2265 t0,\nct \u2264 3\n2 (1 + 49.19\u03b4)(t\u2212 t0) + ct0 .\nCombining these last two inequalities yields\nct \u2264 3\n2 (1 + 49.19\u03b4)t+\n8 ln3 N\n\u03b4 +\n81 ln2 N\n\u03b43 .\nNow the theorem follows by applying Lemma 3."}, {"heading": "7 Remaining proofs", "text": ""}, {"heading": "7.1 Proof of Lemma 4", "text": "Proof of Lemma 4. To show Lemma 4, we first show that, for any t,\n1\nN\n\u2211\ni\n\u03c6(Ri,t+1, 2ct(1 + lnN) + 3) \u2264 e. (3)\nFor any i, Ri,t+1 \u2264 Ri,t + 1, the left hand side of the above inequality is at most\n1\nN\n\u2211\ni\nexp\n(\n(Ri,t + 1) 2\n4ct(1 + lnN) + 6\n)\n.\nThis, in turn, can be upper bounded by\n1\nN\n\u2211\ni\nexp\n(\nR2i,t 4ct(1 + lnN) + 6\n)\n(4)\n\u00b7 exp ( 2Ri,t 4ct(1 + lnN) + 6 ) \u00b7 exp (\n1\n4ct(1 + lnN) + 6\n)\n.\nWe now bound each term in this equation. First, we note that using Lemma 3, the first term can be bounded as\nexp\n(\nR2i,t 4ct(1 + lnN) + 6\n)\n\u2264 exp ( 2ct(1 + lnN)\n4ct(1 + lnN)\n)\n\u2264 e1/2.\nThe second term can be written as:\nexp\n(\nRi,t 4ct(1 + lnN) + 6\n) \u2264 exp ( \u221a 2ct(1 + lnN)\n4ct(1 + lnN) + 6\n)\n.\nNow, we observe that for any real number a \u2265 0, 12 \u221a a + 6\u221a a \u2265 4 \u221a 3. This holds, because, for any a \u2265 0, the function 1 2 \u221a a+ 6\u221a a is convex, and has a minima at a = 3. Therefore,\n\u221a\n2ct(1 + lnN)\n4ct(1 + lnN) + 6\n\u2264 1 2 \u221a 2ct(1 + lnN) + 6 \u221a 2ct(1 + lnN) \u2264 1 4 \u221a 3\nso\nexp\n(\n\u221a\n2ct(1 + lnN)\n4ct(1 + lnN) + 6\n)\n\u2264 e1/4.\nFinally, the third term is trivially bounded as\nexp\n(\n1\n4ct(1 + lnN) + 6\n)\n\u2264 e1/6.\nNow combining the bounds for the three terms in (4) gives 1\nN\n\u2211\ni\n\u03c6(Ri,t+1, 2ct(1 + lnN) + 3) \u2264 e.\nSince the quantity \u2211\ni \u03c6(Ri,t+1, c) is always increasing with c, Equation (3) implies that ct+1 \u2264 2ct(1 + lnN) + 3. The lemma follows."}, {"heading": "7.2 A bootstrap for Lemma 5", "text": "Before we can prove Lemma 5, we first show a somewhat weaker bound on the growth of ct with t (Lemma 6); this bound is used in the proof of Lemma 5 which concludes with the tighter bound on ct+1 \u2212 ct. Lemma 6. Suppose that at some time t0, ct0 \u2265 16 lnN\u03b42 , where 0 \u2264 \u03b4 \u2264 1/2 is a constant. Then, for any time t \u2265 t0,\nct+1 \u2212 ct \u2264 e\u03b4\n(\n3 2 + \u03b4 + lnN\n)\n1\u2212 \u03b42e\u03b4 .\nThe main idea behind the proof of Lemma 6 is as follows. First, we use Lemma 7 to show that ct is monotonic in t, and to get an expression for ct+1 \u2212 ct as a ratio of some derivatives and double derivatives of the potential function \u03c6. Next, we use Lemma 8 and Corollary 10 to bound the numerator and denominator of this ratio. Combining these bounds gives us a proof of Lemma 6.\nWe denote by Et,t+1 . = Et\u222aEt+1 the actions relevant to the change of potential between iterations t and t+1 (recall, Et are the actions with non-zero weight on iteration t).\nLemma 7. At any time t, ct+1 \u2212 ct \u2265 0. Moreover, ct+1 \u2212 ct is at most: \u2211\ni\u2208Et,t+1 (ri,t+1)\n2\n2\n(\n1 ct + \u03c12i,t c2t\n) exp ( \u03c12i,t 2ct )\n\u2211 i\u2208Et+1 (Ri,t+1)2\n\u03b52 t+1\nexp ( (Ri,t+1)2\n2\u03b5t+1\n)\nwhere \u03b5t+1 lies in between ct and ct+1 and for each i, \u03c1i,t lies between Ri,t and Ri,t+1.\nProof. We consider the change in average potential due to the regrets changing from Ri,t to Ri,t+1 (with the scale fixed at ct), and then the change due to the scale changing from ct to ct+1:\n0 =\nN \u2211\ni=1\n\u03c6(Ri,t+1, ct+1)\u2212 N \u2211\ni=1\n\u03c6(Ri,t, ct)\n=\nN \u2211\ni=1\n\u03c6(Ri,t+1, ct)\u2212 \u03c6(Ri,t, ct) (5)\n+ N \u2211\ni=1\n\u03c6(Ri,t+1, ct+1)\u2212 \u03c6(Ri,t+1, ct). (6)\nIt is clear that the sum in (5) can be restricted to i \u2208 Et,t+1, and that the sum in (6) can be restricted to i \u2208 Et+1. We now will express (6) in terms of ct+1 \u2212 ct and employ upper and lower bounds on (5).\nFirst, we derive bounds on (5). Let\u03c8(x) = exp(x2/(2ct)). Then f(x) = \u03c6(x, ct) can be written as\nf(x) =\n{\n\u03c8(x) if x \u2265 0 \u03c8(0) if x < 0.\nThe function f satisfies the preconditions of Lemma 11 (deferred to the end of the section), so we have f(Ri,t+1)\u2212 f(Ri,t) \u2265 f \u2032(Ri,t)ri,t+1 (7) and\nf(Ri,t+1)\u2212f(Ri,t) \u2264 f \u2032(Ri,t)ri,t+1+ \u03c8\u2032\u2032(\u03c1i,t)\n2 r2i,t+1 (8)\nwhere min{Ri,t, Ri,t+1} \u2264 \u03c1i,t \u2264 max{Ri,t, Ri,t+1} and ri,t+1 = Ri,t+1 \u2212 Ri,t. Now we sum both the lower and upper bounds (Eqs. (7) and (8)) over i \u2208 Et,t+1 and apply the fact\n\u2211\ni\u2208Et,t+1\nf \u2032(Ri,t)ri,t+1 =\nN \u2211\ni=1\nf \u2032(Ri,t)ri,t+1 = 0\nwhich follows easily from the fact that the weight assigned to an action i in trial t+ 1 is proportional to f \u2032(Ri,t). Thus,\n0 \u2264 \u2211\ni\u2208Et,t+1\n\u03c6(Ri,t+1, ct)\u2212 \u03c6(Ri,t, ct) (9)\n\u2264 \u2211\ni\u2208Et,t+1\n(\n1 ct + \u03c12i,t c2t\n)\nexp\n(\n\u03c12i,t 2ct\n)\n\u00b7(ri,t+1)2. (10) To deal with (6), we view it as a function of ct+1 and then equated via Taylor\u2019s theorem to a first-order expansion around ct\n\u2212(ct+1 \u2212 ct) \u00b7 \u2211\ni\u2208Et+1\n(Ri,t+1) 2\n2\u03b52t+1 exp\n(\n(Ri,t+1) 2\n2\u03b5t+1\n)\nfor some \u03b5t+1 between ct and ct+1. Substituting this back into (6), we have\n\u2211\ni\u2208Et,t+1\n\u03c6(Ri,t+1, ct)\u2212 \u03c6(Ri,t, ct)\n= \u2211\ni\u2208Et+1\n(Ri,t+1) 2\n2\u03b52t+1 exp\n(\n(Ri,t+1) 2\n2\u03b5t+1\n)\n\u00b7(ct+1 \u2212 ct). (11)\nThe summation on the right-hand side is non-negative, as is the summation on the left-hand side (recall the lower bound (9)), so ct+1 \u2212 ct is non-negative as well. This shows the first part of the lemma. The second part follows from rearranging Eq. (11) and applying the upper bound (10).\nLemma 8. Let, for some t = t0, ct0 \u2265 16 lnN\u03b42 + 1\u03b4 , for some 0 \u2264 \u03b4 \u2264 1. Then, for any t \u2265 t0,\n\u2211\ni\u2208Et,t+1\nexp\n(\n\u03c12i,t 2ct\n)\n\u2264 e\u03b4Ne\nand also \u2211\ni\u2208Et,t+1\n\u03c12i,t 2ct exp\n(\n\u03c12i,t 2ct\n)\n\u2264 e\u03b4(\u03b4 + 1 + lnN)Ne where the \u03c1i,t are the values introduced in Lemma 7.\nProof. Pick any i \u2208 Et,t+1. If Ri,t \u2265 0, then |\u03c1i,t| \u2264 Ri,t + 1 = [Ri,t]+ + 1. Otherwise Ri,t+1 \u2265 0 > Ri,t. But since |ri,t+1| = |Ri,t+1 \u2212 Ri,t| \u2264 1, it must be that |\u03c1i,t| \u2264 1 = [Ri,t]+ + 1. Therefore \u03c12i,t \u2264 ([Ri,t]+ + 1)2, which in turn implies\nexp\n(\n\u03c12i,t 2ct\n)\n\u2264 exp ( ([Ri,t]+ + 1) 2\n2ct\n)\n= exp\n(\n([Ri,t]+) 2\n2ct\n)\n\u00b7 exp ( 2[Ri,t]+ 2ct ) \u00b7 exp ( 1 2ct ) .\nTo prove the first claim, it suffices to show that each of the two exponentials in the final product is bounded by e\u03b4/2. Since ct \u2265 ct0 = (16 lnN)/\u03b42+1/\u03b4, we have exp(1/(2ct)) \u2264 e\u03b4/2. Also, Lemma 3 imply\nexp\n(\n[Ri,t]+ ct\n) \u2264 exp ( \u221a\n2(1 + lnN)\u221a ct\n)\n\u2264 exp (\n2\u03b4 \u221a lnN\n4 \u221a lnN\n)\n\u2264 e\u03b4/2,\nso the first claim follows. To prove the second claim, we use the first claim to derive the fact\nmax i\u2032\u2208Et,t+1 \u03c12i\u2032,t 2ct \u2264 ln \u2211\ni\u2032\u2208Et,t+1\nexp\n(\n\u03c12i\u2032,t 2ct\n)\n\u2264 \u03b4 + 1 + lnN which in turn is combined again with the first claim to arrive at\n\u2211\ni\u2208Et,t+1\n\u03c12i,t 2ct exp\n(\n\u03c12i,t 2ct\n)\n\u2264 (\nmax i\u2032\u2208Et,t+1 \u03c12i\u2032,t 2ct\n)\n\u2211\ni\u2208Et,t+1\nexp\n(\n\u03c12i,t 2ct\n)\n\u2264 (\u03b4 + 1 + lnN)e\u03b4Ne,\ncompleting the proof.\nLemma 9. Let B \u2265 1. If \u2211Ni=1 exi \u2265 BN for some x \u2265 0, then f(x) =\n\u2211N i=1 xie xi \u2265 BN lnB.\nProof. We consider minimizing f under the constraint given by the hypothesis. Define the Lagrangian functionL(x, \u03bb) = \u2211N\ni=1 xie xi +\u03bb(BN \u2212 \u2211N i=1 e xi). Then (\u2202/\u2202xi)L(x, \u03bb) = (xi + 1 \u2212 \u03bb)exi , which is 0 when xi = \u03bb \u2212 1. Let g(\u03bb) = L(x\u2217, \u03bb) be the dual function, where x\u2217i = \u03bb \u2212 1. Then g is maximized when \u03bb = 1 + lnB. By weak duality, sup\u03bb g(\u03bb) \u2264 infx f(x) (with the constraints on x), so f(x) \u2265 g(1 + lnB) = BN lnB.\nThe lemma above leads to the following corollary.\nCorollary 10. For any t,\n\u2211\ni\u2208Et+1\n(Ri,t+1) 2\n2ct+1 exp\n(\n(Ri,t+1) 2\n2ct+1\n)\n\u2265 Ne.\nProof. Let xi = (Ri,t+1)2/(2ct+1), so\n\u2211\ni\u2208Et+1\nexi = \u2211\ni\u2208Et+1\nexp\n(\n(Ri,t+1) 2\n2ct+1\n)\n=\nN \u2211\ni=1\nexp\n(\n([Ri,t+1]+) 2\n2ct+1\n)\n\u2265 Ne.\nApplying Lemma 9 completes the proof.\nNow we prove Lemma 6.\nProof of Lemma 6. From Lemma 7, ct+1 \u2265 ct, and therefore, \u03b5t+1 \u2265 ct. By Lemma 7 and the fact |ri,t+1| \u2264 1,\nct+1 \u2212 ct\n\u2264 \u2211 i\u2208Et,t+1( 1 2ct + \u03c12i,t 2c2t ) exp ( \u03c12i,t 2ct )\n\u2211 i\u2208Et+1 (Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n) .\nCombining this with Lemma 8, Corollary 10, we have\nct+1 \u2212 ct \u2264 ct+1 ct\n\u00b7 e \u03b4 ( 1 2 + \u03b4 + 1 + lnN ) Ne\nNe\n=\n(\n1 + ct+1 \u2212 ct\nct\n) \u00b7 e\u03b4 ( 1 + 1\n2 + \u03b4 + lnN\n)\n.\nRe-arranging, and using the fact that ct \u2265 ct0 \u2265 (16 lnN)/\u03b42, we get that,\nct+1 \u2212 ct \u2264 e\u03b4\n( 1 + 12 + \u03b4 + lnN )\n1\u2212 e \u03b4(1+ 12+\u03b4+lnN)\nct0\n\u2264 e \u03b4(32 + \u03b4 + lnN)\n1\u2212 \u03b42e\u03b4( 116 lnN + 1 2 +\u03b4 16 lnN + 1 16 )\n.\nThe rest of the lemma follows by plugging in the fact that N \u2265 2, and \u03b4 \u2264 1/2."}, {"heading": "7.3 Proof of Lemma 5", "text": "Finally, we are ready to prove Lemma 5. As in the proof of Lemma 6, here too, we start with an upper bound on ct+1 \u2212 ct, obtained from Lemma 7. We then use this upper bound, and the bound in Lemma 6 to get a finer bound on the quantity ct+1 \u2212 ct.\nProof of Lemma 5. We divide the actions into two sets:\nS1 = {i \u2208 Et,t+1 : [Ri,t+1]+ + 1 \u2264 \u221a\n2ct\u03b4} S2 = {i \u2208 Et,t+1 : [Ri,t+1]+ + 1 > \u221a\n2ct\u03b4}. Using the fact that |ri,t+1| \u2264 1, the bound from Lemma 7 can be written as\nct+1 \u2212 ct\n\u2264 1 2ct\n\u2211 i\u2208Et,t+1 exp ( \u03c12i,t 2ct )\n\u2211 i\u2208Et+1 (Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n)\n+\n\u2211 i\u2208S1 \u03c12i,t 2c2t exp ( \u03c12i,t 2c2t )\n\u2211\ni\u2208Et+1\nR2 i,t+1 2c2 t+1 exp ( ([Ri,t+1]+)2 2c2 t+1 )\n+\n\u2211 i\u2208S2 \u03c12i,t 2c2t exp ( \u03c12i,t 2ct )\n\u2211 i\u2208Et+1 (Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n) .\nWe will upper-bound each of these three terms separately. The first term is bounded by (ct+1/ct)e\u03b4/2 using Lemma 8 and Corollary 10. To bound the second term, the definition of S1 implies\n\u2211\ni\u2208S1\n([Ri,t+1]+ + 1) 2\n2ct exp\n(\n([Ri,t+1]+ + 1) 2\n2ct\n)\n\u2264 N 2ct\u03b4 2ct exp\n(\n2ct\u03b4\n2ct\n)\n\u2264 \u03b4e\u03b4N.\nNow Corollary 10 implies a bound of (ct+1/ct)(\u03b4e\u03b4/e). Now we bound the third term. Note that since \u221a 2ct\u03b4 \u2265 \u221a\n2ct0\u03b4 > 1, we have that each i \u2208 S2 is also in Et+1. So the third term is bounded above by the largest ratio\n\u03c12i,t 2c2t exp ( \u03c12i,t 2ct )\n(Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n)\nover all i \u2208 Et+1. Since |\u03c1i,t| \u2264 Ri,t+1 + 1, each such ratio is at most\nc2t+1 c2t \u00b7 ( Ri,t+1 + 1 Ri,t+1\n)2\n\u00b7 exp ( 1\n2ct + Ri,t+1 ct\n)\n\u00b7 exp ( (Ri,t+1) 2(ct+1 \u2212 ct)\n2ctct+1\n)\n.\nWe bound each factor in this product (deferring c2t+1/c 2 t until later).\n\u2022 As Ri,t+1+1 \u2265 \u221a 2ct\u03b4 and ct \u2265 ct0 \u2265 10/\u03b43, we have\u221a\n2ct\u03b4 \u2265 1/\u03b4 and (\nRi,t+1 + 1\nRi,t+1\n)2\n\u2264 1 (1\u2212 \u03b4)2 .\n\u2022 By Lemma 3, we have\nRi,t+1 \u2264 Ri,t + 1 \u2264 \u221a 2ct(1 + lnN) + 1,\nso\nexp\n(\n1\n2ct + Ri,t+1 ct\n) \u2264 exp ( 3\n2ct + Ri,t ct\n)\n\u2264 exp\n\n\n3\n2ct +\n\u221a\n2(1 + lnN)\nct\n\n \u2264 e3\u03b42/20+3\u03b4/5\nsince ct \u2265 ct0 \u2265 (16 lnN)/\u03b42 \u2265 10/\u03b42. \u2022 We use Lemma 6 with ct0 \u2265 (16 lnN)/\u03b42, and \u03b4 \u2264 1/2 to obtain the crude bound\nct+1 \u2212 ct \u2264 e\u03b4(4 + 2 lnN). (12) Now using this bound along with Lemma 3 and \u03b4 \u2264 1/2 gives\n(Ri,t+1) 2(ct+1 \u2212 ct)\n2ctct+1 \u2264 e\n\u03b4(4 + 2 lnN)(1 + lnN)\nct\n\u2264 6.2 + 4.7 lnN + 3.1 ln 2 N\nct0\nwhich is at most \u03b4 since ct0 \u2265 (16 lnN)/\u03b42+(4 ln2 N)/\u03b4. Therefore, the third term is bounded by\nc2t+1 c2t \u00b7 exp(1.6\u03b4 + 0.15\u03b4 2) (1\u2212 \u03b4)2 \u2264 c2t+1 c2t \u00b7 e 2\u03b4 (1\u2212 \u03b4)2 .\nCollecting the three terms in the bound for ct+1 \u2212 ct,\nct+1 \u2212 ct \u2264 ct+1 ct \u00b7 e \u03b4 2 + ct+1 ct \u00b7 \u03b4e \u03b4 e + c2t+1 c2t \u00b7 e 2\u03b4 (1\u2212 \u03b4)2 .\nWe bound the ratio ct+1/ct as\nct+1 ct \u2264 1 + ct+1 \u2212 ct ct \u2264 1 + e \u03b4(4 + 2 lnN) ct\n\u2264 1 + 4\u03b4 2\n10 +\n\u03b42\n8 = 1 +\n21\u03b4e\u03b4\n40 \u2264 1 + \u03b4\nwhere we have used the bound in (12), ct0 \u2265 (16 lnN)/\u03b42, and \u03b4 \u2264 1/2. Therefore, we have\nct+1 \u2212 ct \u2264 1 2 \u00b7 e\u03b4(1 + \u03b4) + 1 \u00b7 e\n2\u03b4(1 + \u03b4)2 (1\u2212 \u03b4)2 + \u03b4e\u03b4(1 + \u03b4) e\nTo finish the proof, we use the fact that \u03b4 \u2264 12 . Using this condition, and a Taylor series expansion, when 0 \u2264 \u03b4 \u2264 12 , e\u03b4 \u2264 1 +\u221ae\u03b4 \u2264 1 + 1.65\u03b4. Using this fact,\n1 2 e\u03b4(1 + \u03b4) + \u03b4 e e\u03b4(1 + \u03b4)\nis at most 1\n2 + 3.02\u03b4 + 2.63\u03b42 + 0.61\u03b43\nwhich in turn is at most 0.5 + 3.49\u03b4. Moreover,\ne2\u03b4(1 + \u03b4)2\n(1 \u2212 \u03b4)2 \u2264 e 2\u03b4(1 + 4\u03b4)2\nwhich again is at most\n(1 + 3.3\u03b4)(1 + 4\u03b4)2\nUsing the fact that \u03b4 \u2264 12 , this is at most 1 + 45.7\u03b4. The lemma follows by combining this with the bound in the previous paragraph.\nLemma 11. Let \u03c8 : R \u2192 R be any continuous, twicedifferentiable, convex function such that for some a \u2208 R, \u03c8 is non-decreasing on [a,\u221e) and \u03c8\u2032(a) = 0. Define f : R \u2192 R by\nf(x) =\n{\n\u03c8(x) if x \u2265 a \u03c8(a) if x < a,\nThen for any x0, x \u2208 R, f \u2032(x0)(x \u2212 x0) \u2264 f(x)\u2212 f(x0)\n\u2264 f \u2032(x0)(x \u2212 x0) + \u03c8\u2032\u2032(\u03be)\n2 (x\u2212 x0)2\nfor some min{x0, x} \u2264 \u03be \u2264 max{x0, x}. Proof. The lower bound follows from the convexity of f , which is inherited from the convexity of \u03c8. For the upper bound, we first consider the case x0 < a and x \u2265 a. Then, for some \u03be \u2208 [a, x],\nf(x)\u2212 f(x0) = \u03c8(x) \u2212 \u03c8(a)\n= \u03c8\u2032(a)(x \u2212 a) + \u03c8 \u2032\u2032(\u03be)\n2 (x\u2212 a)2 (13)\n\u2264 \u03c8\u2032(a)(x \u2212 x0) + \u03c8\u2032\u2032(\u03be)\n2 (x\u2212 x0)2 (14)\n= f \u2032(x0)(x\u2212 x0) + \u03c8\u2032\u2032(\u03be)\n2 (x\u2212 x0)2\nwhere (13) follows by Taylor\u2019s theorem and (14) follows since x0 \u2264 a < x, \u03c8\u2032(a) \u2265 0, and \u03c8\u2032\u2032(\u03be) \u2265 0. The case x0 \u2265 a and x < a is analogous, and the remaining cases are immediate using Taylor\u2019s theorem."}], "references": [{"title": "Adaptive and self-confident on-line learning algorithms", "author": ["P. Auer", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Hembold", "R.E. Schapire", "M. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "On-line prediction and conversion strategies", "author": ["N. Cesa-Bianchi", "Y. Freund", "D.P. Hembold", "M. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1996}, {"title": "Potentialbased algorithms in on-line prediction and game theory", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2003\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2003}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Drifting games and Brownian motion", "author": ["Y. Freund", "M. Opper"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Opper.,? \\Q2002\\E", "shortCiteRegEx": "Freund and Opper.", "year": 2002}, {"title": "A decisiontheoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "The robustness of p-norm algorithms", "author": ["C. Gentile"], "venue": "Machine Learning,", "citeRegEx": "Gentile.,? \\Q2003\\E", "shortCiteRegEx": "Gentile.", "year": 2003}, {"title": "General convergence results for linear discriminant updates", "author": ["A.J. Grove", "N. Littlestone", "D. Schuurmans"], "venue": "Machine Learning,", "citeRegEx": "Grove et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Grove et al\\.", "year": 2001}, {"title": "Approximation to bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["E. Hazan", "S. Kale"], "venue": "In COLT,", "citeRegEx": "Hazan and Kale.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2008}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["M. Hutter", "J. Poland"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "Efficient algorithms for the online optimization", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Learning with continuous experts using drifting games", "author": ["Indraneel Mukherjee", "Robert E. Schapire"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Mukherjee and Schapire.,? \\Q2008\\E", "shortCiteRegEx": "Mukherjee and Schapire.", "year": 2008}, {"title": "A game of prediction witih expert advice", "author": ["V. Vovk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vovk.,? \\Q1998\\E", "shortCiteRegEx": "Vovk.", "year": 1998}, {"title": "How to better use expert advice", "author": ["R. Yaroshinsky", "R. El-Yaniv", "S. Seiden"], "venue": "Machine Learning,", "citeRegEx": "Yaroshinsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yaroshinsky et al\\.", "year": 2004}], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we consider the decision-theoretic framework for online learning (DTOL) proposed by Freund and Schapire [FS97]. Previous algorithms for learning in this framework have a tunable learning rate parameter. Tuning the learning rate requires prior knowledge about the sequence and severely limits the practicality of the algorithm. While much progress has been made in the past decade for adaptively tuning the learning rate, all of these methods still ultimately rely on some prior information. We propose a completely parameter-free algorithm for learning in this framework. We show theoretically that our algorithm has a regret bound similar to the best bounds achieved by previous algorithms with optimally-tuned learning rates. We also present a few experiments comparing the performance of the algorithm with that of other algorithms for various tunings.", "creator": "LaTeX with hyperref package"}}}