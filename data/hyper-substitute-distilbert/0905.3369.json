{"id": "0905.3369", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2009", "title": "Learning nonlinear dynamic models", "abstract": "now present a confusing approach for learning nonlinear dynamic models, but leads to a generic set of tools because of calculating problems many are otherwise intuitive. we provide theory therefore this new approach is consistent for models with long differential convergence, adds another topological mechanics allowing motion capture and high - dimensional algebraic programs, provides results superior to standard alternatives.", "histories": [["v1", "Wed, 20 May 2009 18:08:18 GMT  (223kb)", "http://arxiv.org/abs/0905.3369v1", null], ["v2", "Wed, 3 Jun 2009 20:29:16 GMT  (223kb)", "http://arxiv.org/abs/0905.3369v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["john langford", "ruslan salakhutdinov", "tong zhang 0001"], "accepted": true, "id": "0905.3369"}, "pdf": {"name": "0905.3369.pdf", "metadata": {"source": "CRF", "title": "Learning Nonlinear Dynamic Models", "authors": ["John Langford"], "emails": ["jl@yahoo-inc.com", "rsalakhu@cs.toronto.edu", "tongz@rci.rutgers.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 5.\n33 69\nv1 [\ncs .A\nI] 2\n0 M"}, {"heading": "1. Introduction", "text": "The notion of hidden states appears in many nonstationary models of the world such as Hidden Markov Models (HMMs), which have discrete states, and Kalman filters, which have continuous states. Figure 1 shows a general dynamic model with observation xt and unobserved hidden state yt. The system is characterized by a state transition probability P (yt+1|yt), and a state to observation probability P (xt|yt).\nThe method for predicting future events under such a dynamic model is to maintain a posterior distribution over the hidden state yt+1, based on all observations X1:t = {x1, . . . ,xt} up to time t. The posterior can be updated using the formula:\nP (yt+1|X1:t)\n\u221d \u2211\nyt\nP (yt|X1:t\u22121)P (xt|yt)P (yt+1|yt). (1)\nThe prediction of future events xt+1, . . . ,xt+k, k > 0,\nAppearing in Proceedings of the 26 th International Conference on Machine Learning, Montreal, Canada, 2009. Copyright 2009 by the author(s)/owner(s).\n.....y1 y2 yn\nx1 x2 xn\nFigure 1. Dynamic Model with observation vector xt and hidden state vector yt.\nconditioned on X1:t is through the posterior over yt:\nP (xt+1, . . . ,xt+k|x1:t)\n\u221d P (yt+1|X1:t)P (xt+1, . . . ,xt+k|yt+1). (2)\nHidden state based dynamic models have a wide range of applications, such as time series forecasting, finance, control, robotics, video and speech processing. Some detailed dynamic models and application examples can be found in (West & Harrison, 1997).\nFrom Eq. 2, it is clear that the benefit of using a hidden state dynamic model is that the information contained in the observation X1:t can be captured by a relatively small hidden state yt+1. Therefore in order to predict the future, we do not have to use all previous observations X1:t but only its state representation yt+1. In principle, yt+1 may contain a finite history of length k + 1, such as xt,xt\u22121, . . . ,xt\u2212k. Although the notation only considers first order dependency, it incorporates higher order dependency by considering a representation of the form Yt = [y \u2032 t,y \u2032 t\u22121, . . . ,y \u2032\nt\u2212k], which is a standard trick.\nIn an HMM or Kalman filter, both transition and observation functions are linear maps. There are reasonable algorithms that can learn these linear dynamic models. For example, in addition to the classical EM approach, it was recently shown that global learning\nof certain hidden Markov models can be achieved in polynomial time (Hsu et al., 2008). Moreover, for linear models, the posterior update rule is quite simple. Therefore, once the model parameters are estimated, such models can be readily applied for prediction.\nHowever in many real problems, the system dynamics cannot be approximated linearly. For such problems, it is often necessary to incorporate nonlinearity into the dynamic model. The standard approach to this problem is through nonlinear probability modeling, where prior knowledge is required to define a sensible state representation, together with parametric forms of transition and observation probabilities. The model parameters are learned by using probabilistic methods such as the EM (Wilson & Bobick, 1999; Roweis & Ghahramani, 2001). When the learned model is applied for prediction purposes, it is necessary to maintain the posterior P (yt|X1:t) using the update formula in Eq. 1. Unfortunately, for nonlinear systems, maintaining P (yt|x1:t) is generally difficult because the posterior can become exponentially more complex (e.g., exponentially many mixture components in a mixture model) as t increases.\nThis computational difficulty is a significant obstacle to applying nonlinear dynamic systems to practical problems. The traditional approach to address the computational difficulty is through approximation methods. For example, in the particle filtering approach (Gordon et al., 1993; Arulampalam et al., 2002), one uses a finite number of samples to represent the posterior distribution and the samples are then updated as observations arrive. Another approach is to maintain a mixture of Gaussians to approximate the posterior, P (yt|X1:t), which may also be regarded as a mixture of Kalman filters (Chen & Liu, 2000). Although an exponential in t number of mixture components are needed to accurately represent the posterior, in practice, one has to use a fixed number of mixture components to approximate the distribution. This leads to the following question: even if the posterior can be well-approximated by a computationally tractable approximation family (such as finite mixtures of Gaussians), how can one design a good approximate inference method that is guaranteed to find a good quality approximation? The use of complex techniques required to design reasonable approximation schemes makes it non-trivial to apply nonlinear dynamic models for many practical problems.\nThis paper introduces an alternative approach, where we start with a different representation of a linear dynamic model which we call the sufficient posterior representation. It is shown that one can recover\nthe underlying state representation by using prediction methods that are not necessarily probabilistic. This allows us to model nonlinear dynamic behaviors with many available nonlinear supervised learning algorithms such as neural networks, boosting, and support vector machines in a simple and unified fashion. Compared to the traditional approach, it has several distinct advantages:\n\u2022 It does not require us to design any explicit state representation and probability model using prior knowledge. Instead, the representation is implicitly embedded in the representational choice of the underlying supervised learning algorithm, which may be regarded as a black box with the power to learn an arbitrary representation. The prior knowledge can be simply encoded as input features to the learning algorithms, which significantly simplifies the modeling aspect.\n\u2022 It does not require us to come up with any specific representation of the posterior and the corresponding approximate Bayesian inference schemes for posterior updates. Instead, this issue is addressed by incorporating the posterior update as part of the learning process. Again, the posterior representation is implicitly embedded in the representational choice of the underlying supervised learning algorithm. In this sense, our scheme learns the optimal representation for posterior approximation and the corresponding update rules within the representational power of the underlying supervised algorithm1.\n\u2022 It is possible to obtain performance guarantees for our algorithm in terms of the learning performance of the underlying supervised algorithm. The performance of the latter has been heavily investigated in the statistical and learning theory literature. Such results can thus be applied to obtain theoretical results on our methods for learning nonlinear dynamic models."}, {"heading": "2. Sufficient Posterior Representation", "text": "Instead of starting with a probability model, our approach directly attacks the problem of predicting yt+k based on X1:t. Clearly the prediction depends only on the posterior distribution P (yt+1|X1:t). Therefore we can solve the prediction problem as long as we can estimate, and update this posterior distribution.\n1Many modern supervised learning algorithms are universal, in the sense that they can learn an arbitrary representation in the large sample limit.\nIn our approach, it is assumed that the posterior P (yt+1|X1:t) can be approximated by a family of distributions parameterized by st+1 \u2208 S: P (yt+1|X1:t) \u2248 P (yt+1|st+1) for some deterministic parameter st+1 that depends on X1:t. That is, st+1 is a sufficient statistic for the posterior P (yt+1|X1:t), and updating the posterior is equivalent to updating the sufficient statistic st+1. The augmented model that incorporates the (approximate) sufficient statistics st \u2208 S is shown in Fig. 2. In this model, yt can be integrated out, which leaves a model containing only st and xt.\nAccording to the posterior update of Eq. 1, there exists a deterministic function B such that:\nst+1 = B(xt, st).\nFor simplicity, we can give an arbitrary value for the initial state s1, and let:\ns2 = A(x1) = B(x1, s1).\nMoreover, according to Eq. 2, given an arbitrary vector function f of the future events Xt+1:\u221e = {xt+1,xt+2, \u00b7 \u00b7 \u00b7 }, there exists a deterministic function Cf (k > 0) such that:\nEXt+1:\u221e [f(Xt+1:\u221e)|X1:t] = C f (st+1).\nTherefore the dynamics of the model in Fig. 1 is determined by the posterior initialization rule A and posterior update rule B. Moreover, the prediction of the system is completely determined by the function Cf .\nThe key observation of our approach is that the functions A, B, and C are deterministic, which does not require any probability assumption. It fully captures the correct dynamics of the underlying probabilistic dynamic model. However, by removing the probability assumption, we obtain a more general and flexible model. In particular, we are not required to start with specific forms of the transition model P (yt+1|yt), the observation model P (xt|yt), or the posterior sufficient statistic model P (yt+1|x1:t) \u2248 P (yt+1|st+1), as\nrequired in the standard approach. Instead, we may embed the forms of such models into the functional approximation forms in standard learning algorithms, such as neural networks, kernel machines, or tree ensembles. These are universal learning machines that are well studied in the learning theory literature.\nOur approach essentially replaces a stochastic hidden state representation through the actual state Y by a deterministic representation through the posterior sufficient statistic S. Although the corresponding representation may become more complex (which is why in the traditional approach, yt is always explicitly included in the model), this is not a problem in our approach, because we do not have to know the explicit representation. Instead, the complexity is incorporated into the underlying learning algorithm \u2014 this allows us to take advantage of sophisticated modern supervised learning algorithms that can handle complex functional representations. Moreover, unlike the traditional approach, in which one designs a specific form of P (yt|st) by hand, and then derives an approximate update rule B by hand using Bayesian inference methods, here, we simply use learning to come up with the best possible representation and update (assuming the underlying learning algorithm is sufficiently powerful). We believe this approach is also more robust because it is less sensitive to model mis-specifications or non-optimal approximate inference algorithms that commonly occur in practice.\nBy changing the standard probabilistic dynamic model in Fig. 1 to its sufficient posterior representation in Fig. 2 (where we assume yt is integrated out, and thus can be ignored), we can define the goal of our learning problem. Since yt is removed from the formulation, in the following, we shall refer to the sufficient posterior statistic st simply as state.\nWe can now introduce the following definition of Sufficient Posterior Representation of Dynamic Model, which we refer to SPR-DM.\nDefinition 2.1. (SPR-DM) A sufficient posterior representation of a dynamic model is given by an observed sequence {xt} and unobserved hidden state {st}, characterized by state initialization map s2 = A(x1), state update map st+1 = B(xt, st), and state prediction maps:\nEXt+1:\u221e [f(Xt+1:\u221e)|X1:t] = C f (st+1)\nfor any pre-determined vector function Cf .\nOur goal in this model is to learn the model dynamics characterized by A and B, as well as Cf for any given vector function of interest."}, {"heading": "3. Learning SPR-DM", "text": "The essential idea of our algorithm is to use a bottlenecking approach to construct an implicit definition of state, along with state space evolution and projection operators to answer various natural questions we might pose."}, {"heading": "3.1. Training", "text": "There are two parts to understanding the training process. The first is the architecture trained, and the second is the exact method of training this architecture. Note that our architecture is essentially functional rather than representational."}, {"heading": "3.1.1. Architecture", "text": "Graphically, in order to recover the system dynamics, we solve two distinct kinds of prediction problems. To understand these graphs it is essential to understand that the arrows do not represent graphical models. Instead, they are a depiction of which information is used to predict which other information. We distinguish observations and hidden \u201cstate\u201d as double circles and circles respectively, to make clear what is observed and what is not.\nThe first prediction problem solved in Fig. 3, left panel, provides our initial definition of state. Essentially, state is \u201cthat information which summarizes the first observation in predicting the second observation\u201d. Compared to a conventional dynamic model, the quantity s2 may be a sufficient statistic of the state posterior after integrating x1, the posterior after integrating x1 and evolving one step or some intermediate mixture. This ambiguity is fundamental, but inessential.\nThe second prediction problem is state evolution, shown in Fig. 3, middle panel. Here, we use a state and an observation to predict the next state, reusing the prediction of state from observation from the first\nstep. Note that even though there are two sources of information for predicting st, only one prediction problem (using both sources) is solved. Operator B is what is used to integrate new information into the state of an online system.\nWithout loss of generality, in the notation of Fig. 3 we consider f0(Xt+1:\u221e) = E[xt+1|X1:t], and denote C f0 by C. An alternative interpretation of C, which we do not distinguish in this paper, is to learn the probability distribution over xt+1. It should be understood that our algorithm can be applied with other choices of f0.\nThe above two learning diagrams are used to obtain the system dynamics (A and B). One can then use the learned system dynamics to learn prediction rules Cf with any function f of interest. Here, we consider the problem of predicting xt+k at different ranges of k = 2j. This gives a state projection operator Dj : st \u2192 st+2j , without observing the future sequence xt+1,xt+2, \u00b7 \u00b7 \u00b7 . The learning of state projection is presented in Fig. 3, right panel. The idea in state projection is that we want to build a predictor of the observation far in the future. To do this, we\u2019ll chain together several projection operators from the current state. To make the system computationally more efficient, we learn \u230alog2 T \u230b operators, each specialized to cover different timespans. Note that state evaluation provides an efficient way to learn xt+k based on st simultaneously for multiple k through combination of projection operators. If computation is not an issue, one may also learn xt+k based on st separately for each k."}, {"heading": "3.1.2. Method", "text": "Training of A is straightforward. Training of C is complicated by the fact that samples appear at multiple timesteps, but otherwise straightforward given the other components. To deal with multiple timesteps, it is important for our correctness proof in section 4.2\nthat the observation xt include the timestep t. The training of D is also straightforward given everything else (and again, we\u2019ll require the timestep be a part of the update for the correctness proofs).\nThe most difficult thing to train is B, since an alteration to B can cascade over multiple timesteps. The method we chose takes advantage of both local and global information to provide a fast near-optimal solution.\n1. Initialization: Learn Bt, Ct starting from timestep t = 1 and conditioning on the previous learned value. Multitask learning or initialization with prior solutions may be applied to improve convergence here. In our experiments, we initialize Bt, Ct to the average parameter values of previous timesteps and use stochastic gradient descent techniques for learning.\n2. Conditional Training Learn an alteration B\u2032\nwhich optimizes performance given that the existing Bt are used at every other time step. Since computational performance is an issue, we use a \u201cbackprop through time\u201d gradient descent style algorithm. For each timestep t, we compute the change in squared loss for all future observations using the chain rule, and update according to the negative gradient.\n3. Iteration: Update B using stochastic mixing according to Bi = \u03b1B\n\u2032+(1\u2212\u03b1)Bi\u22121 where \u03b1 is the stochastic mixing parameter. The precise method of stochastic mixing used in the experiments is equivalent to applying the derivative update with probability \u03b1 and not update with probability 1 \u2212 \u03b1, which is a computational and representational improvement over Searn (Daume et al., 2009).\nWe prove (below) that the method in step (1) alone is consistent. Steps (2) and (3) are used to force convergence to a single B and C while retaining the performance gained in step (1). The intuition behind step (3) is that when \u03b1 = o( 1\nT ), with high probability B\u2032 is\nexecuted only once, implying that B\u2032 need only perform well with respect to the learning problem induced by the rest of the system to improve the overall system. This approach was first described in Conservative Policy Iteration (Kakade & Langford, 2002)."}, {"heading": "3.2. Testing", "text": "We imagine testing the algorithm by asking questions like: what is the probability of observation xt\u2032 given\nwhat is known up to time t for t\u2032 > t? This is done by using A(x1) to get s2, then using B(xi, si) to evolve the state to st. Then the time interval from t\n\u2032 \u2212 t is broken down into factors of 2, and the corresponding state projection operators Di are applied to the state resulting in a prediction for st\u2032\u22121. This is transformed into a prediction for xt\u2032 using operator C."}, {"heading": "4. Analysis", "text": ""}, {"heading": "4.1. Computation", "text": "The computational requirements depend on the exact training method used. For the initialization step, training of A, Bt, and Ct requires just O(nT ) examples. Training Di can be done with just O(nT log2 T ) examples. For the iterative methods, an extra factor of T is generally required per iteration for learning B."}, {"heading": "4.2. Consistency", "text": "We now show that under appropriate assumptions, the SPR-DM model can be learned in the infinite sample limit using our algorithm. Due to the space limitation, we only consider the non-agnostic situation, where the SPR-DM model is exact. That is, the functions A, B, C used in our learning algorithm contains the correct functions. The agnostic setting, where the SPR-DM model is only approximately correct, can be analyzed using perturbation techniques (e.g., for linear systems, this is done in (Hsu et al., 2008)). Although such analysis is useful, the fundamental insight is identical to the non-agnostic analysis considered here.\nWe consider the following constraints in the SPR-DM model. We assume that the model is invertible: The distribution over xt (more generally, the definition can be extended to other vector functions \u03c60(xt, . . . , )) is a sufficient statistic for the state st that generates xt. This is a nontrivial limitation of state based dynamic models which retains the ability to capture long range dependencies.\nDefinition 4.1. (Invertible SPR-DM) The SPR-DM in Definition 2.1 is invertible if there exist a function E such that for all t, E(Cf (st)) = st.\nInvertibility is a natural assumption, but it\u2019s important to understand that invertible dynamic systems are a subset of dynamic systems as shown by the following hidden Markov model example:\nExample 4.1 A hidden Markov model which is not invertible: Suppose there are two observations, 0 and 1 where the first observation is uniform random, the second given the first is always 0, and the third is the same as the first. Under this setting, the two valid\nsequences are 000 and 101. There is a hidden Markov model which is not invertible that can express this sequence. In particular, suppose state s1 is (0, 1) or (1, 1) and state s2 is (0, 2) or (1, 2), with a conditional observation that is P (0|\u2217, 1) = 1 and P (0|0, 2) = 1 and P (0|1, 2) = 0. However, no invertible hidden Markov model can induce a distribution over these sequences because the distribution on x2 is always 0, implying that a specification of state is impossible due to lack of information.\nAlthough Invertible SPR-DMs form a limited subset of SPR-DMs, they are still nontrivial as the following example shows.\nExample 4.2An Invertible hidden Markov model with long range dependencies: Suppose there are two observations 0 and 1 and two states s1 and s2. Let the first observation always be 0 and the first state be uniform random P (s1|0) = P (s2|0) = 0.5. Let the states only self-transition according to P (s1|s1) = 1 and P (s2|s2) = 1. Let the observations be according to the following distribution: P (0|s1) = 0.75, P (0|s2) = 0.25. Given only one observation, the probability of state s1 is 0.75 or 0.25 for observations 0 or 1 respectively. Given T observations, the probability of state s1 converges to 0 or 1 exponentially fast in T using Bayes Law and the Chernoff bound.\nThe above two examples illustrate the intuition behind invertibility. One can extend the concept by incorporating look aheads: that is, instead of taking C as the probability of xt given st, we may let C be the probability of Xt:t+k given st. This broadens the class of invertible models. In this notation, invertibility means that if two states st and s \u2032 t induce the same short range behavior Xt:tk , then they are identical in the sense they induce the same behavior for all future observations: Xt+1:\u221e. Generally speaking, non-invertible models are those that cannot be efficiently learned by any algorithm because we do not have sufficient information to recover states that have different long range dynamics but identical behavior in short ranges. In fact, there are well-known hardness results for learning such models in the theoretical analysis of hidden Markov models. There are no known efficient methods to capture non-trivial long-range effects. This implies that our restriction is not only necessary, but also not a significant limitation in comparison to any other known efficient learning algorithms.\nNext we prove that our algorithm can recover any invertible hidden Markov model given sufficiently powerful prediction with infinitely many samples. This is analogous to similar infinite-sample consistency results for supervised learning.\nTheorem 4.1. (Consistency) For all Invertible SPRDMs, if all prediction problems are solved perfectly, then for all i, p(xi|x1, ...,xi\u22121) is given by: C\u0302(B\u0302(xi\u22121, B\u0302(xi\u22122, ..., A\u0302(x1)...))).\nA similar theorem statement holds for projections.\nProof. The proof is by induction.\nThe base case is C(A(x1)) = C\u03022(A\u0302(x1)) which holds under the assumption that the prediction problem is solved perfectly. In the inductive case, define: s2 = A(x1), s\u03022 = A\u0302(x1), si = B(xi\u22121, si\u22121), s\u0302i = B\u0302i(xi\u22121, s\u0302i\u22121) and assume C(si) = C\u0302i(s\u0302i). Invertibility and the inductive assumption implies there exists E such that: si = E(C\u0302i(s\u0302i)). Consequently, there exists C\u0302i+1 = C and B\u0302i+1(xi, s\u0302i) = B(xi, E(C\u0302(s\u0302i))) such that:\nC(B(xi, si)) = C\u0302i+1(B\u0302i+1(xi, s\u0302i)\nproving the inductive case."}, {"heading": "5. Experiments", "text": "In this section we present experimental results on two datasets that involve high-dimensional, highlystructured sequence data. The first dataset is the motion capture data that comes from CMU Graphics Lab Motion Capture Database. The second dataset is the Weizmann dataset2, which contains video sequences of nine human subjects performing various actions."}, {"heading": "5.1. Details of Training", "text": "While the introduced framework allows us to use many available nonlinear supervised learning algorithms, in our experiments we use the following parametric forms for our operators:\ns2 = A(x1) = \u03c3 ( A\u22a4x1 + b ) , st = B(xt\u22121, st\u22121) = \u03c3 ( B\u22a41 xt\u22121 +B \u22a4 2 st\u22121 + b ) , x\u0302t = C(st) = C \u22a4st + a,\nst+2j = Dj(st) = D \u22a4 j st + d,\nwhere \u03c3(y) = 1/(1 + exp(\u2212y)) is the logistic function, applied componentwise, {C,B,A,Dij , a,b,d} are the model parameters with a,b and d representing the bias terms.\nFor both datasets, during the initialization step, the values of {Bt, Ct} are initialized to the average parameter values of previous timesteps3. Learning of\n2Available at http://www.wisdom.weizmann.ac.il/ \u223cvision/SpaceTimeActions.html.\n3The values of A,B1, C1 were initialized with small random values sampled from a zero-mean normal distribution with standard deviation of 0.01.\n{Bt, Ct} then proceeds by minimizing the squared loss using stochastic gradient descent. For each time step, we use 500 parameter updates, with learning rate of 0.001. We then used 500 iterations of stochastic mixing, using gradients obtained by backpropagation through time. The stochastic mixing rate \u03b1 was set to 0.9 and was gradually annealed towards zero. We experimented with various values for the learning rate and various annealing schedules for the mixing rate \u03b1. Our results are fairly robust to variations in these parameters. In all experiments we were conditioning on the two previous time steps to predict the next."}, {"heading": "5.2. Motion Capture Data", "text": "The human motion capture data consists of sequences of 3D joint angles plus body orientation and translation. The dataset was preprocessed to be invariant to isometries (Taylor et al., 2006), and contains various walking styles, including normal, drunk, graceful, gangly, sexy, dinosaur, chicken, and strong. We split at random the data into 30 training and 8 test sequences, each of length 50. The training data was further split at random into the 25 training and 5 validation sequences. Each time step was represented by a vector of 58 real-valued numbers. The dataset was also normalized to have zero mean was scaled by a single number, so that the variance across each dimension was on average equal to 1. The dimensionality of the hidden state was set to 20.\nFigure 4 shows the average test prediction errors using squared loss, where the prediction horizon ranges over 1,2,4,8,10,16, and 25. The nonlinear model was compared to two simple autoregressive linear models that operate directly in the input space. The first linear model, LINEAR-2, makes predictions x\u0302t+k via the\nlinear combination of the two previous time steps:\nx\u0302t+k = L 1\u22a4xt + L 2\u22a4xt\u22121 + l. (3)\nThe model parameters {L1, L2, l} were fit by ridge regression. The second model, LINEAR-5, makes predictions by conditioning on the previous five time steps. We note that the number of the model parameters for these simple autoregressive linear models grows linearly with the input information. Hence when faced with high-dimensional sequence data, learning linear operators directly in the input space is unlikely to perform well.\nIt is interesting to observe that autoregressive linear models perform quite well in terms of making shortrange predictions. This is probably due to the fact that locally, motion capture data is linear. However, the nonlinear model performs considerably better compared to both linear models when making long-range predictions. Figure 4 (right panel) further shows that the proposed nonlinear model performs considerably better than 20 and 100-state HMM\u2019s. Both HMM\u2019s use Gaussian distribution as their observation model. It is obvious that a simple HMM model is unable to cope with complex nonlinear dynamics. Even a 100- state HMM is unable to generalize."}, {"heading": "5.3. Modeling Video", "text": "Results on the motion capture dataset show that a nonlinear model can outperform linear and HMM models, when making long-range predictions. In this section we present results on the Weizmann dataset, which is considerably more difficult than the motion capture dataset.\nThe Weizmann dataset contains video sequences of\nnine human subjects performing various actions, including waving one hand, waving two hands, jumping, and bending. Each video sequence was preprocessed by placing a bounding box around a person performing an action. The dataset was then downsampled to 29\u00d7 16 images, hence each time step was represented by a vector of 464 real-valued numbers. We split at random the data into into 36 training (30 training and 6 validation), and 10 test sequences, each of length 50. The dataset was also normalized to have zero mean and variance 1. The dimension of the hidden state was set to 50.\nFigure 5 shows that the nonlinear model consistently outperforms both linear autoregressive and HMM models, particularly when making long-range predictions. It is interesting to observe that on this dataset, the nonlinear model outperforms the autoregressive model even when making short-range predictions."}, {"heading": "6. Conclusions", "text": "In this paper we introduced a new approach to learning nonlinear dynamical systems and showed that it performs well on rather hard high-dimensional time series datasets compared to standard models such as HMMs or linear predictors. We believe that the presented framework opens up an entirely new set of devices for nonlinear dynamic modeling. It removes several obstacles in the traditional approach that requires heavy human design, and allows well-established supervised learning algorithms to be used automatically for nonlinear dynamic models."}, {"heading": "Acknowledgments", "text": "This work was done when Ruslan Salakhutdinov visited Yahoo. Tong Zhang is partially supported by NSF\ngrant DMS-0706805."}], "references": [{"title": "A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking", "author": ["M.S. Arulampalam", "S. Maskell", "N. Gordon", "T. Clapp"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Arulampalam et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Arulampalam et al\\.", "year": 2002}, {"title": "Mixture Kalman filters", "author": ["R. Chen", "J.S. Liu"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Chen and Liu,? \\Q2000\\E", "shortCiteRegEx": "Chen and Liu", "year": 2000}, {"title": "Searchbased structured prediction", "author": ["H. Daume", "J. Langford", "D. Marcu"], "venue": "Machine Learning Journal", "citeRegEx": "Daume et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daume et al\\.", "year": 2009}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N.J. Gordon", "D.J. Salmond", "A. Smith"], "venue": "IEE Proceedings Part F. (pp. 107\u2013113)", "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "A spectral algorithm for learning hidden markov models. http://arxiv.org/abs/0811.4413", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2008}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["S. Kakade", "J. Langford"], "venue": "Int. Conference on Machine Learning (pp. 267\u2013274)", "citeRegEx": "Kakade and Langford,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Langford", "year": 2002}, {"title": "Learning nonlinear dynamical systems using the em algorithm", "author": ["S. Roweis", "Z. Ghahramani"], "venue": "In S. Haykin (Ed.), Kalman filtering and neural networks,", "citeRegEx": "Roweis and Ghahramani,? \\Q2001\\E", "shortCiteRegEx": "Roweis and Ghahramani", "year": 2001}, {"title": "Modeling human motion using binary latent variables. Advances in Neural Information Processing Systems (pp. 1345\u20131352)", "author": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"], "venue": null, "citeRegEx": "Taylor et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2006}, {"title": "Bayesian forecasting and dynamic models (2nd ed.)", "author": ["M. West", "J. Harrison"], "venue": null, "citeRegEx": "West and Harrison,? \\Q1997\\E", "shortCiteRegEx": "West and Harrison", "year": 1997}, {"title": "Parametric hidden markov models for gesture recognition", "author": ["A.D. Wilson", "A.F. Bobick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wilson and Bobick,? \\Q1999\\E", "shortCiteRegEx": "Wilson and Bobick", "year": 1999}], "referenceMentions": [{"referenceID": 4, "context": "of certain hidden Markov models can be achieved in polynomial time (Hsu et al., 2008).", "startOffset": 67, "endOffset": 85}, {"referenceID": 3, "context": "For example, in the particle filtering approach (Gordon et al., 1993; Arulampalam et al., 2002), one uses a finite number of samples to represent the posterior distribution and the samples are then updated as observations arrive.", "startOffset": 48, "endOffset": 95}, {"referenceID": 0, "context": "For example, in the particle filtering approach (Gordon et al., 1993; Arulampalam et al., 2002), one uses a finite number of samples to represent the posterior distribution and the samples are then updated as observations arrive.", "startOffset": 48, "endOffset": 95}, {"referenceID": 2, "context": "The precise method of stochastic mixing used in the experiments is equivalent to applying the derivative update with probability \u03b1 and not update with probability 1 \u2212 \u03b1, which is a computational and representational improvement over Searn (Daume et al., 2009).", "startOffset": 239, "endOffset": 259}, {"referenceID": 4, "context": ", for linear systems, this is done in (Hsu et al., 2008)).", "startOffset": 38, "endOffset": 56}, {"referenceID": 7, "context": "The dataset was preprocessed to be invariant to isometries (Taylor et al., 2006), and contains various walking styles, including normal, drunk, graceful, gangly, sexy, dinosaur, chicken, and strong.", "startOffset": 59, "endOffset": 80}], "year": 2017, "abstractText": "We present a novel approach for learning nonlinear dynamic models, which leads to a new set of tools capable of solving problems that are otherwise difficult. We provide theory showing this new approach is consistent for models with long range structure, and apply the approach to motion capture and highdimensional video data, yielding results superior to standard alternatives.", "creator": null}}}