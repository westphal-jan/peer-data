{"id": "0906.4779", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2009", "title": "Minimum Probability Flow Learning", "abstract": "learning in probabilistic models were often found hampered by problematic abstract concept of the normalization factor and secondary consequences. here theories need a comprehensive numerical technique that obviates the choice to eliminate an intractable normalization factor or sample by the equilibrium distribution of the model. they helps achieved by establishing dynamics that would partition the observed mass distribution into the model distribution, and precisely setting as appropriate outcome the minimization of appropriate initial flow of knowledge away until typically accepted distribution. score matching, minimum state learning, and certain forms of normal divergence are shown both contain special cases requiring such learning task. generally take the application toward infinite probability flow titled continuous error estimation or ising models, static belief networks, compact gaussian distributions form a continuous model covering certain highly general energy estimate recognized as a power output. in his ising model case, minimum directed flow learning outperforms current state flow and art techniques exceeding approximately five orders of magnitude in added time, with catastrophic error rendering the recovered trajectory. it is given hope showing both criterion will alleviate existing relies on the classes of simulated models whereas are recommended not use.", "histories": [["v1", "Thu, 25 Jun 2009 19:15:44 GMT  (774kb,D)", "http://arxiv.org/abs/0906.4779v1", "10 pages, 7 figures"], ["v2", "Wed, 16 Sep 2009 02:20:21 GMT  (1291kb,D)", "http://arxiv.org/abs/0906.4779v2", "12 pages, 7 figures; added notes on KL divergence and convexity, comparison to CD, minor textual changes"], ["v3", "Mon, 7 Jun 2010 07:03:16 GMT  (2239kb,D)", "http://arxiv.org/abs/0906.4779v3", "12 pages, 7 figures; now motivates in terms of KL divergence"], ["v4", "Sun, 25 Sep 2011 01:33:51 GMT  (2392kb,DS)", "http://arxiv.org/abs/0906.4779v4", "Updated to match ICML conference proceedings"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG physics.data-an stat.ML", "authors": ["jascha sohl-dickstein", "peter battaglino", "michael r deweese"], "accepted": true, "id": "0906.4779"}, "pdf": {"name": "0906.4779.pdf", "metadata": {"source": "CRF", "title": "Minimum Probability Flow Learning", "authors": ["Jascha Sohl-Dicksteinad", "Peter Battaglinobd", "Michael R. DeWeese"], "emails": ["jascha@berkeley.edu,", "pbb@berkeley.edu,", "deweese@berkeley.edu,"], "sections": [{"heading": null, "text": "Learning in probabilistic models is often severely hampered by the general intractability of the normalization factor and its derivatives. Here we propose a new learning technique that obviates the need to compute an intractable normalization factor or sample from the equilibrium distribution of the model. This is achieved by establishing dynamics that would transform the observed data distribution into the model distribution, and then setting as the objective the minimization of the initial flow of probability away from the data distribution. Score matching, minimum velocity learning, and certain forms of contrastive divergence are shown to be special cases of this learning technique. We demonstrate the application of minimum probability flow learning to parameter estimation in Ising models, deep belief networks, multivariate Gaussian distributions and a continuous model with a highly general energy function defined as a power series. In the Ising model case, minimum probability flow learning outperforms current state of the art techniques by approximately two orders of magnitude in learning time, with comparable error in the recovered parameters. It is our hope that this technique will alleviate existing restrictions on the classes of probabilistic models that are practical for use."}, {"heading": "1 Introduction", "text": "Estimating parameters for probabilistic models is a fundamental problem in many scientific and engineering disciplines. Unfortunately, most probabilistic learning algorithms require calculating the normalization factor, or partition function, of the probabilistic model in question, or at least calculating its gradient. For the overwhelming majority of models there are no known analytic solutions, confining us to the highly restrictive subset of probabilistic models that can be analytically solved, or those that can be made tractable using known approximate learning techniques. Thus, development of new techniques for parameter estimation in currently intractable probabilistic models has the potential to be of great benefit, lifting near ubiquitous restrictions on how we are able to model the world.\nMany approaches exist for approximate learning, including mean field theory and its expansions, variational Bayes techniques and a plethora of sampling or numerical integration based methods [21, 10, 9, 5]. Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpin\u0303a\u0301n [22, 4], Hyva\u0308rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].\nContrastive divergence [22, 4, 23] is a variation on steepest gradient descent of the maximum (log) likelihood (ML) objective function. The gradient of the log likelihood for observed data under a model can be calculated by comparing a gradient averaged over the data distribution, to a gradient\nar X\niv :0\n90 6.\n47 79\nv1 [\ncs .L\nG ]\n2 5\nJu n\naveraged over the model distribution1. Everywhere the model has higher probability than the data, the model term dominates and the probability assigned under the model is reduced. Everywhere the data distribution has greater probability than the model distribution, the data term dominates and the probability under the model is increased. The term averaged over the model is commonly approximated by sampling. Unfortunately, sampling can be extremely expensive, costing in general an amount of time exponential in the dimensionality of the system.\nUnder contrastive divergence [22, 4], averaging over the model is performed using a Markov Chain Monte Carlo (MCMC) [14] sampling algorithm, initialized at the data distribution. Rather than running to equilibrium however, the Markov Chain is terminated after only one (or a few) steps, avoiding exponential sampling time. The intuition behind this early termination is that, since the sampling algorithm is initialized at the data, any divergence of the sampled distribution from the data distribution will tend to be in the direction of the model distribution. Contrasting the samples to the data distribution will thus tend to pull the model in the correct direction even before the Markov Chain has reached equilibrium.\nScore matching, developed by Aapo Hyva\u0308rinen [7], is a method that learns parameters in a probabilistic model using only derivatives of the energy function evaluated over the data distribution (see Equation (12)). This obviates the need to explicitly sample or integrate over the model distribution. In score matching one minimizes the expected square distance of the score function with respect to spatial coordinates given by the data distribution from the similar score function given by the model distribution. It can be seen as an integration of the contrastive divergence gradient for infinitesimal Langevin dynamics [8], as the limit of approximating the model distribution by patching together cutouts of the model distribution around each data point [20] and finally as equivalent to minimum velocity learning [12].\nMinimum velocity learning is an approach recently proposed by Movellan [12] that recasts a number of the ideas behind CD, treating the minimization of the initial dynamics away from the data distribution as the goal itself rather than a surrogate for it.\nFrequently when performing learning, one begins at the data distribution, and then evolves it into the model distribution via some sampling dynamics that have the model as their equilibrium distribution. Movellan\u2019s proposal is that rather than directly minimize the difference between the data and the model, one introduces system dynamics leading to the equilibrium distribution and minimizes the initial flow of probability away from the data under those dynamics. If the model already looks exactly like the data there will be no initial flow of probability. This can be thought of as an analogy for how we learn in the world. As Movellan has pointed out [12], a child learning to ride a bike does not need to fall repeatedly all the way to the ground to learn how to keep her balance, she just needs to feel the bike starting to deviate from its expected position to update her model for how it responds.\nMovellan applies this intuition to a specific class of distributions and a single choice of system dynamics. Namely that of distributions over continuous state spaces evolving via diffusion dynamics. The velocity in minimum velocity learning is the difference in average drift velocities between particles diffusing under the model distribution and particles diffusing under the data distribution.\nHere we provide a framework, applicable to any parametric model, of which minimum velocity, certain forms of CD, and SM are all special cases, and which is in many situations more powerful than any of these algorithms. This framework extends the ideas behind minimum velocity learning to arbitrary state spaces and a far broader class of dynamics. We show that learning under this framework is effective and fast in a number of cases: Ising models, deep belief networks (DBN),\n1 Specifically, a gradient in terms of the energy function. The update step for model parameters \u03b8 is proportional to the gradient of the log likelihood of the data distribution\n\u2206\u03b8 \u221d \u2202\nhP i p (0) i log p (\u221e) i (\u03b8) i \u2202\u03b8 = \u2212 X\ni\n\u2202Ei (\u03b8)\n\u2202\u03b8 p (0) i + X i \u2202Ei (\u03b8) \u2202\u03b8 p (\u221e) i (\u03b8) ,\nwhere p(0) and p(\u221e) (\u03b8) represent the data distribution and model distribution, respectively, E (\u03b8) is the energy function associated with the model distribution and i indexes the states of the system (see Section 2.1). The second term in this gradient can be extremely difficult to compute.\nmultidimensional Gaussian distributions, and a complicated two-dimensional continuous distribution."}, {"heading": "2 Minimum probability flow", "text": "Our goal is to find the parameters that cause a probabilistic model to best agree with a set of (assumed iid) observations of the state of a system. We will do this by proposing dynamics that guarantee the transformation of the data distribution into the model distribution, and then minimizing the magnitude of the initial flow of probability away from the data distribution."}, {"heading": "2.1 Distributions", "text": "The data distribution is represented by a vector p(0), with p(0)i the probability of observing the system in a state i. The superscript (0) represents time t = 0 under the system dynamics, as will be described in more detail in Section 2.2. If the observations were of a two variable binary system, then p(0) would have four entries representing the probabilities of observing states 00, 01, 10 and 11.\nOur goal is to find the parameters \u03b8 that cause a model distribution p(\u221e) (\u03b8) to best match the data distribution p(0). Without loss of generality, we assume the model distribution to be of the form\np (\u221e) i (\u03b8) = exp (\u2212Ei (\u03b8)) Z (\u03b8) , (1)\nwhere E (\u03b8) is referred to as the energy function, and the normalizing factor Z (\u03b8) is called the partition function,\nZ (\u03b8) = \u2211\ni\nexp (\u2212Ei (\u03b8)) (2)\n(here we have set the \u201ctemperature\u201d of the system to 1). The superscript (\u221e) indicates that this is the equilibrium distribution reached after running the dynamics for infinite time."}, {"heading": "2.2 Dynamics", "text": "We wish to generalize Movellan\u2019s diffusion dynamics to arbitrary state spaces. To accomplish this, we observe that diffusion dynamics are a special case of dynamics governed by a master equation that enforces conservation of probability [15]:\np\u0307 (t) i =\n\u2211 j 6=i \u0393ij(\u03b8) p (t) j \u2212 \u2211 j 6=i \u0393ji(\u03b8) p (t) i , (3)\nwhere p\u0307(t)i = \u2202p\n(t) i\n\u2202t is the rate of change of probability with time. Transition rates \u0393ij(\u03b8) give the rate at which probability will flow from a state j into a state i. The first term of Equation (3) represents flow of probability out of other states j into the state i, and the second represents flow out of i into other states j. The dependence on \u03b8 results from the requirement that the dynamics we choose cause p(t) to flow to the equilibrium distribution p(\u221e)(\u03b8). For readability, explicit dependence on \u03b8 will be dropped except where specifically relevant. If we choose the diagonal of \u0393 to obey \u0393ii = \u2212 \u2211 j 6=i \u0393ji, then we can write the dynamics as\np\u0307(t) = \u0393p(t) (4) (see Figure 1). The unique solution for p(t) is2\np(t) = exp (\u0393t) p(0). (5)"}, {"heading": "2.3 Detailed Balance", "text": "\u0393 must be chosen such that the dynamics in Equation (4) converge to the model distribution. One way to guarantee this is by choosing \u0393 such that it satisfies detailed balance for the model distribution p(\u221e), and such that there is a path through \u0393 allowing mixing between any two states. Note that there is no need to restrict the dynamics defined by \u0393 to those of any real physical process, such as diffusion. Detailed balance requires that at equilibrium the probability flow from state i into state j equals the probability flow from j into i,\n\u0393jip (\u221e) i (\u03b8) = \u0393ijp (\u221e) j (\u03b8) , (6)\nwhich can be rewritten as \u0393ij \u0393ji = p (\u221e) i (\u03b8)\np (\u221e) j (\u03b8)\n= exp [Ej (\u03b8)\u2212 Ei (\u03b8)] . (7)\n\u0393 is underconstrained by the above equation. Motivated by symmetry and aesthetics, we choose as the form for the (non-zero, non-diagonal) entries in \u0393\n\u0393ij = exp [\n1 2\n(Ej (\u03b8)\u2212 Ei (\u03b8)) ]\n(i 6= j) . (8)\nThe choice \u0393ij = \u0393ji = 0 also satisfies Equation (6), allowing a sparse population of \u0393 for purposes of computational tractability. Theoretically, to guarantee convergence to the model distribution, the non-zero elements of \u0393 must be chosen such that, given sufficient time, probability can flow between any pair of states. In practice, we will only need to consider a small fraction of the non-zero elements in \u0393 (see Section 2.5)."}, {"heading": "2.4 Objective Function", "text": "The goal is to minimize the initial flow of probability away from the data distribution (Figure 2). Although other objective functions are possible for a minimum probability flow approach, we have found the L1 norm to be particularly effective:\n\u03b8\u0302 = arg min \u03b8 K (\u03b8) , (9)\nK (\u03b8) = \u2223\u2223\u2223 \u2223\u2223\u2223p\u0307(0) (\u03b8) \u2223\u2223\u2223 \u2223\u2223\u2223 1 = \u2211\ni\n\u2223\u2223\u2223p\u0307(0)i (\u03b8) \u2223\u2223\u2223 . (10)\nThis objective function is uniquely zero when p(0) and p(\u221e) (\u03b8) are exactly equal (although \u03b8\u0302 may not be sensible for models that are unable to fit the data perfectly). Some algebra gives the learning gradient with respect to \u03b8:\n\u2202K\n\u2202\u03b8 = 1 2\n\u2211\ni,j\np (0) j \u0393ij (\u03b8) [ \u2202Ej (\u03b8) \u2202\u03b8 \u2212 \u2202Ei (\u03b8) \u2202\u03b8 ] [ sgn ( p\u0307 (0) i (\u03b8) ) \u2212 sgn ( p\u0307 (0) j (\u03b8) )] . (11)\nNote that Equations (9) through (11) do not depend on the partition function Z (\u03b8) or its derivatives. 2 The form chosen for \u0393 in Equation (4), coupled with the satisfaction of detailed balance and ergodicity introduced in section 2.3, guarantees that there is a unique eigenvector p(\u221e) of \u0393 with eigenvalue zero, and that all other eigenvalues of \u0393 have negative real parts."}, {"heading": "2.5 Tractability", "text": "The vector p(0) is typically huge, as is \u0393 (e.g., 2N and 2N \u00d7 2N , respectively, for an N -bit binary system). Na\u0131\u0308vely, this would seem to prohibit evaluation and minimization of the objective function. Fortunately, all the elements in p(0) not corresponding to observations are zero. Since our objective function is only evaluated at time t = 0 this allows us to ignore all those \u0393ij for which no data point exists at state j. Additionally, there is a great deal of flexibility as far as which elements of \u0393 can be set to zero. By populating \u0393 so as to connect each state to a fixed number of additional states, the cost of the algorithm in both memory and time is O(M), where M is the number of observed data points, and does not depend on the number of system states."}, {"heading": "2.6 Continuous Systems", "text": "Although we have motivated this technique using systems with a large, but finite, number of states, it generalizes in a straightforward manner to continuous systems. The flow matrix \u0393 and distribution vectors p(t) transition from being very large to being infinite in size. \u0393 can still be chosen to connect each state to a small, finite, number of additional states however, and only outgoing probability flow from states with data contributes to the objective function, so the cost of learning remains largely unchanged.\nIn addition, for a particular pattern of connectivity in \u0393 this objective function, like Movellan\u2019s [12], reduces to score matching [7] (other connectivity patterns reduce to alternate forms). Taking the limit of connections between all states within a small distance of each other, and then Taylor expanding in , one can show that, up to an overall constant and scaling factor\nK \u2248 KSM = \u2211\ni\u2208{samples}\n[ 1 2 \u2207E(xi) \u00b7 \u2207E(xi)\u2212\u22072E(xi) ] . (12)\nThis reproduces the link discovered by Movellan [12] between diffusion dynamics over continuous spaces and score matching."}, {"heading": "3 Experimental Results", "text": "Matlab code implementing minimum probability flow learning for each of the following cases is available upon request. A public toolkit is under construction.\nAll minimization was performed using Mark Schmidt\u2019s remarkably effective minFunc [16]."}, {"heading": "3.1 Ising model", "text": "The Ising model has a long and storied history in physics [3] and machine learning [1] and it has recently been found to be a surprisingly useful model for networks of neurons in the retina [17, 19]. The ability to fit Ising models to the activity of large groups of simultaneously recorded neurons is\nof current interest given the increasing number of these types of data sets from the retina, cortex and other brain structures.\nWe fit an Ising model (fully visible Boltzmann machine) of the form\np(\u221e)(x; J) = 1\nZ(J) exp\n \u2212 \u2211\ni,j\nJijxixj\n  (13)\nto a set of N d-element iid data samples { x(i)|i = 1...N } generated via Gibbs sampling from an Ising model as described below, where each of the d elements of x is either 0 or 1. Because each xi \u2208 {0, 1}, x2i = xi, we can write the energy function as\nE(x; J) = \u2211\ni,j 6=i Jijxixj +\n\u2211\ni\nJiixi. (14)\nThe probability flow matrix \u0393 has 2N \u00d7 2N elements, but we allow only elements corresponding to transitions into states a single bit-flip away to be non-zero.\nFigure 3 shows the average error in predicted correlations as a function of learning time for 20, 000 samples from a 40 unit, fully connected Ising model. The Jij used were graciously provided by Broderick and coauthors, and were identical to those used for synthetic data generation in the 2008 paper \u201cFaster solutions of the inverse pairwise Ising problem\u201d [2]. Training was performed on 20, 000 samples so as to match the number of samples used in section III.A. of Broderick et al. Note that given sufficient samples, the minimum probability flow algorithm would converge exactly to the right answer. On an 8 core 2.33 GHz Intel Zeon, the learning converges in about 15 seconds. Broderick et al. perform a similar learning task on a 100-CPU grid computing cluster, with a convergence time of approximately 200 seconds.\nSimilar learning was performed for 100, 000 samples from a 100 unit, fully connected, Ising model. A coupling matrix was chosen with elements randomly drawn from a Gaussian with mean 0 and variance 0.04. Using the minimum probability flow learning technique, learning took approximately 1 minute, compared to roughly 12 hours for a 100 unit nearest neighbor coupling model of retinal data [18] (personal communication, J. Shlens). Figure 4 demonstrates the recovery of the coupling and correlation matrices for our fully connected Ising model, while Figure 3 shows the time course for learning."}, {"heading": "3.2 Deep Belief Network", "text": "As a demonstration of learning on a more complex discrete valued model, we trained a 4 layer deep belief network (DBN) [6] on MNIST handwritten digits. A DBN consists of stacked restricted Boltzmann machines (RBMs), such that the hidden layer of one RBM forms the visible layer of the\nnext. Each RBM has the form:\np(\u221e)(xvis,xhid; W) = 1\nZ(W) exp\n \u2212 \u2211\ni,j\nWijxvis,ixhid,j\n  , (15)\np(\u221e)(xvis; W) = 1\nZ(W) exp\n \u2211\nj\nlog ( 1 + exp [ \u2212 \u2211\ni\nWijxvis,i\n])  . (16)\nNote that sampling-free application of the minimum probability flow algorithm requires analytically marginalizing over the hidden units. RBMs were trained in sequence, starting at the bottom layer, on 10,000 samples from the MNIST postal hand written digits data set. As in the Ising case, the probability flow matrix \u0393 was populated so as to connect every state to all states which differed by only a single bit flip.\nConfabulations were performed by Gibbs sampling from the top layer RBM, then propagating each sample back down to the pixel layer by way of the conditional distribution p(\u221e)(xvis|xhid; Wk) for each of the intermediary RBMs, where k indexes the layer in the stack. As shown in Figure 5, minimum probability flow learned a good model of handwritten digits."}, {"heading": "3.3 Gaussian", "text": "As an example of minimum probability flow learning applied to continuous models, we fit a multivariate Gaussian distribution to synthetic data. The model distribution has the form\np(\u221e)(x; \u03a3\u22121) = 1\nZ (\u03a3\u22121) exp\n[ \u22121\n2 xT\u03a3\u22121x\n] , (17)\nwith vector x and coupling matrix \u03a3\u22121. We fit to 10, 000 iid samples from a 10-dimensional Gaussian distribution. The probability flow matrix \u0393 was populated so as to connect every state to 20 additional states, chosen from a Gaussian distribution with variance 0.01 centered on the state. Results are shown in Figure 6."}, {"heading": "3.4 Power Series Energy Function", "text": "To demonstrate minimum probability flow\u2019s effectiveness in an extremely flexible, difficult to normalize, model, we learned parameters \u03b8 for a two-dimensional continuous distribution of the form\np(\u221e)(x, y; \u03b8) = 1\nZ (\u03b8) exp\n[ \u2212 M\u2211\nm,n=0\n\u03b8mnLm(x)Ln(y) ] , (18)\nexp\n(\n\u2212\n\u2211128\nm=0\n\u2211128\nn=0 \u03b8mnLm (x)Ln (y)\n)\nand Lm (x) is the mth Legendre polynomial in x.\nwhere Lm(x) is the mth order Legendre polynomial in x, (x, y) \u2208 [\u22121, 1]2, M is the maximum polynomial order, and Z (\u03b8) is the normalization factor. We fit an M = 128 distribution using consecutive line searches on batches of 100, 000 iid samples from the distribution shown on the left of Figure 7. The probability flow matrix \u0393 was populated so as to connect every state with 20 other states, chosen from a uniform distribution in the range [\u22121, 1]2. Figure 7 shows a histogram of the data distribution p(0)(x, y; \u03b8) compared to a histogram of the learned Legendre function expansion p(\u221e)(x, y; \u03b8)."}, {"heading": "4 Future Directions", "text": "There are many exciting possibilities for further exploration. We find the following particularly interesting:\n1. Learning in models with hidden units, likely by a technique that closely resembles expectation maximization (EM).\n2. Accelerating learning, by changing the number and location of nonzero \u0393 entries associated with each data point (possibly on a schedule during learning) and by implementing an online rather than batch version of the learning algorithm.\n3. Exploring connections between the minimum probability flow objective function and more traditional objective functions, such as average log likelihood.\n4. Adapting this technique to causal temporal models, by matching the dynamics inherent to the learning algorithm to the data dynamics."}, {"heading": "5 Summary", "text": "We have presented a novel framework for efficient learning in the context of any parametric model. This method was inspired by the minimum velocity approach developed by Movellan, and it reduces to that technique as well as to score matching and some forms of contrastive divergence under suitable choices for the dynamics and state space. By decoupling the dynamics from any specific physical process, such as diffusion, and focusing on the initial flow of probability from the data to a subset of other states chosen in part for their utility and convenience, we have arrived at a framework that is not only more general than previous approaches, but also potentially much more powerful.\nWe expect that this framework will render some previously intractible models more amenable to estimation."}, {"heading": "Acknowledgments", "text": "We would like to thank Javier Movellan for sharing a work in progress; Tamara Broderick, Miroslav Dud\u0131\u0301k, Gas\u030cper Tkac\u030cik, Robert E. Schapire and William Bialek for use of their Ising model coupling parameters; Jonathon Shlens for useful discussion and ground truth for his Ising model convergence times; Bruno Olshausen, Christopher Hillar, Charles Cadieu, Kilian Koepsell and the rest of the Redwood Center for many useful discussions and for comments on earlier versions of the manuscript; and the Canadian Institute for Advanced Research - Neural Computation and Perception Program for their financial support (JSD)."}], "references": [{"title": "A learning algorithm for Boltzmann machines", "author": ["D H Ackley", "G E Hinton", "T J Sejnowski"], "venue": "Cognitive Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Faster solutions of the inverse pairwise Ising problem", "author": ["T Broderick", "M Dud\u0131\u0301k", "G Tka\u010dik", "R Schapire", "W Bialek"], "venue": "E-print arXiv,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "History of the Lenz-Ising model", "author": ["S G Brush"], "venue": "Reviews of Modern Physics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1967}, {"title": "On contrastive divergence (CD) learning", "author": ["M A Carreira-Perpi\u00f1\u00e1n", "G E Hinton"], "venue": "Technical report, Dept. of Computere Science, University of Toronto,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Neural networks and learning machines; 3rd edition", "author": ["S Haykin"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Estimation of non-normalized statistical models using score matching", "author": ["A Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables", "author": ["A Hyv\u00e4rinen"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A variational approach to Bayesian logistic regression models and their extensions", "author": ["T Jaakkola", "M Jordan"], "venue": "Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Rodr\u0131\u0301guez. Mean field approach to learning in Boltzmann machines", "author": ["F H Kappen"], "venue": "Pattern Recognition Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Contrastive divergence in Gaussian diffusions", "author": ["J R Movellan"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A minimum velocity approach to learning", "author": ["J R Movellan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Learning continuous probability distributions with symmetric diffusion networks", "author": ["J R Movellan", "J L McClelland"], "venue": "Cognitive Science,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Probabilistic inference using Markov chain Monte Carlo methods", "author": ["D R Neal"], "venue": "Technical Report CRG-TR-93-1,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Statistical Mechanics", "author": ["R Pathria"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1972}, {"title": "Weak pairwise correlations imply strongly correlated network states in a neural population", "author": ["E Schneidman", "M J Berry 2nd", "R Segev", "W Bialek"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The structure of large-scale synchronized firing in primate retina", "author": ["J Shlens", "G D Field", "J L Gauthier", "M Greschner", "A Sher", "A M Litke", "E J Chichilnisky"], "venue": "Journal of Neuroscience,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "The structure of multi-neuron firing patterns in primate retina", "author": ["J Shlens", "G D Field", "J L Gauthier", "M I Grivich", "D Petrusca", "A Sher", "A M Litke", "E J Chichilnisky"], "venue": "J. Neurosci.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "A spatial derivation of score matching", "author": ["J Sohl-Dickstein", "B Olshausen"], "venue": "Redwood Center Technical Report,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Mean-field theory of Boltzmann machine learning", "author": ["T Tanaka"], "venue": "Physical Review Letters E,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "A new learning algorithm for mean field Boltzmann machines", "author": ["M Welling", "G Hinton"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "The convergence of contrastive divergences", "author": ["A Yuille"], "venue": "Department of Statistics, UCLA. Department of Statistics Papers.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "Many approaches exist for approximate learning, including mean field theory and its expansions, variational Bayes techniques and a plethora of sampling or numerical integration based methods [21, 10, 9, 5].", "startOffset": 191, "endOffset": 205}, {"referenceID": 9, "context": "Many approaches exist for approximate learning, including mean field theory and its expansions, variational Bayes techniques and a plethora of sampling or numerical integration based methods [21, 10, 9, 5].", "startOffset": 191, "endOffset": 205}, {"referenceID": 8, "context": "Many approaches exist for approximate learning, including mean field theory and its expansions, variational Bayes techniques and a plethora of sampling or numerical integration based methods [21, 10, 9, 5].", "startOffset": 191, "endOffset": 205}, {"referenceID": 4, "context": "Many approaches exist for approximate learning, including mean field theory and its expansions, variational Bayes techniques and a plethora of sampling or numerical integration based methods [21, 10, 9, 5].", "startOffset": 191, "endOffset": 205}, {"referenceID": 20, "context": "Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpi\u00f1\u00e1n [22, 4], Hyv\u00e4rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].", "startOffset": 107, "endOffset": 114}, {"referenceID": 3, "context": "Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpi\u00f1\u00e1n [22, 4], Hyv\u00e4rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].", "startOffset": 107, "endOffset": 114}, {"referenceID": 6, "context": "Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpi\u00f1\u00e1n [22, 4], Hyv\u00e4rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].", "startOffset": 148, "endOffset": 151}, {"referenceID": 11, "context": "Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpi\u00f1\u00e1n [22, 4], Hyv\u00e4rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].", "startOffset": 218, "endOffset": 230}, {"referenceID": 10, "context": "Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpi\u00f1\u00e1n [22, 4], Hyv\u00e4rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].", "startOffset": 218, "endOffset": 230}, {"referenceID": 12, "context": "Of particular interest are contrastive divergence (CD), developed by Welling, Hinton and Carreira-Perpi\u00f1\u00e1n [22, 4], Hyv\u00e4rinen\u2019s score matching (SM) [7], and the minimum velocity learning framework proposed by Movellan [12, 11, 13].", "startOffset": 218, "endOffset": 230}, {"referenceID": 20, "context": "Contrastive divergence [22, 4, 23] is a variation on steepest gradient descent of the maximum (log) likelihood (ML) objective function.", "startOffset": 23, "endOffset": 34}, {"referenceID": 3, "context": "Contrastive divergence [22, 4, 23] is a variation on steepest gradient descent of the maximum (log) likelihood (ML) objective function.", "startOffset": 23, "endOffset": 34}, {"referenceID": 21, "context": "Contrastive divergence [22, 4, 23] is a variation on steepest gradient descent of the maximum (log) likelihood (ML) objective function.", "startOffset": 23, "endOffset": 34}, {"referenceID": 20, "context": "Under contrastive divergence [22, 4], averaging over the model is performed using a Markov Chain Monte Carlo (MCMC) [14] sampling algorithm, initialized at the data distribution.", "startOffset": 29, "endOffset": 36}, {"referenceID": 3, "context": "Under contrastive divergence [22, 4], averaging over the model is performed using a Markov Chain Monte Carlo (MCMC) [14] sampling algorithm, initialized at the data distribution.", "startOffset": 29, "endOffset": 36}, {"referenceID": 13, "context": "Under contrastive divergence [22, 4], averaging over the model is performed using a Markov Chain Monte Carlo (MCMC) [14] sampling algorithm, initialized at the data distribution.", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "Score matching, developed by Aapo Hyv\u00e4rinen [7], is a method that learns parameters in a probabilistic model using only derivatives of the energy function evaluated over the data distribution (see Equation (12)).", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "It can be seen as an integration of the contrastive divergence gradient for infinitesimal Langevin dynamics [8], as the limit of approximating the model distribution by patching together cutouts of the model distribution around each data point [20] and finally as equivalent to minimum velocity learning [12].", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": "It can be seen as an integration of the contrastive divergence gradient for infinitesimal Langevin dynamics [8], as the limit of approximating the model distribution by patching together cutouts of the model distribution around each data point [20] and finally as equivalent to minimum velocity learning [12].", "startOffset": 244, "endOffset": 248}, {"referenceID": 11, "context": "It can be seen as an integration of the contrastive divergence gradient for infinitesimal Langevin dynamics [8], as the limit of approximating the model distribution by patching together cutouts of the model distribution around each data point [20] and finally as equivalent to minimum velocity learning [12].", "startOffset": 304, "endOffset": 308}, {"referenceID": 11, "context": "Minimum velocity learning is an approach recently proposed by Movellan [12] that recasts a number of the ideas behind CD, treating the minimization of the initial dynamics away from the data distribution as the goal itself rather than a surrogate for it.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "As Movellan has pointed out [12], a child learning to ride a bike does not need to fall repeatedly all the way to the ground to learn how to keep her balance, she just needs to feel the bike starting to deviate from its expected position to update her model for how it responds.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "To accomplish this, we observe that diffusion dynamics are a special case of dynamics governed by a master equation that enforces conservation of probability [15]: \u1e57 (t) i = \u2211", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "In addition, for a particular pattern of connectivity in \u0393 this objective function, like Movellan\u2019s [12], reduces to score matching [7] (other connectivity patterns reduce to alternate forms).", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "In addition, for a particular pattern of connectivity in \u0393 this objective function, like Movellan\u2019s [12], reduces to score matching [7] (other connectivity patterns reduce to alternate forms).", "startOffset": 132, "endOffset": 135}, {"referenceID": 11, "context": "This reproduces the link discovered by Movellan [12] between diffusion dynamics over continuous spaces and score matching.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "The Ising model has a long and storied history in physics [3] and machine learning [1] and it has recently been found to be a surprisingly useful model for networks of neurons in the retina [17, 19].", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "The Ising model has a long and storied history in physics [3] and machine learning [1] and it has recently been found to be a surprisingly useful model for networks of neurons in the retina [17, 19].", "startOffset": 83, "endOffset": 86}, {"referenceID": 15, "context": "The Ising model has a long and storied history in physics [3] and machine learning [1] and it has recently been found to be a surprisingly useful model for networks of neurons in the retina [17, 19].", "startOffset": 190, "endOffset": 198}, {"referenceID": 17, "context": "The Ising model has a long and storied history in physics [3] and machine learning [1] and it has recently been found to be a surprisingly useful model for networks of neurons in the retina [17, 19].", "startOffset": 190, "endOffset": 198}, {"referenceID": 1, "context": "The Jij used were graciously provided by Broderick and coauthors, and were identical to those used for synthetic data generation in the 2008 paper \u201cFaster solutions of the inverse pairwise Ising problem\u201d [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 16, "context": "Using the minimum probability flow learning technique, learning took approximately 1 minute, compared to roughly 12 hours for a 100 unit nearest neighbor coupling model of retinal data [18] (personal communication, J.", "startOffset": 185, "endOffset": 189}, {"referenceID": 5, "context": "As a demonstration of learning on a more complex discrete valued model, we trained a 4 layer deep belief network (DBN) [6] on MNIST handwritten digits.", "startOffset": 119, "endOffset": 122}], "year": 2017, "abstractText": "Learning in probabilistic models is often severely hampered by the general intractability of the normalization factor and its derivatives. Here we propose a new learning technique that obviates the need to compute an intractable normalization factor or sample from the equilibrium distribution of the model. This is achieved by establishing dynamics that would transform the observed data distribution into the model distribution, and then setting as the objective the minimization of the initial flow of probability away from the data distribution. Score matching, minimum velocity learning, and certain forms of contrastive divergence are shown to be special cases of this learning technique. We demonstrate the application of minimum probability flow learning to parameter estimation in Ising models, deep belief networks, multivariate Gaussian distributions and a continuous model with a highly general energy function defined as a power series. In the Ising model case, minimum probability flow learning outperforms current state of the art techniques by approximately two orders of magnitude in learning time, with comparable error in the recovered parameters. It is our hope that this technique will alleviate existing restrictions on the classes of probabilistic models that are practical for use.", "creator": "LaTeX with hyperref package"}}}