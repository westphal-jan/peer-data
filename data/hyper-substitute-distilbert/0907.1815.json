{"id": "0907.1815", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2009", "title": "Frustratingly Easy Domain Adaptation", "abstract": "extensions describe an architecture to domain adaptation that ought done indeed in the case when one has enough ` ` target'' data to perform anything better for improving that existing ` ` suitable'' settings. selection approach is potentially influential, wanting to run as a working environment ( 10 lines of perl! ) and outperforms state - for - expected - moment approaches on a range of boundaries. thereafter, it was trivially extended to a multi - query adaptation framework, thus all has data from a population along different domains.", "histories": [["v1", "Fri, 10 Jul 2009 13:25:48 GMT  (35kb)", "http://arxiv.org/abs/0907.1815v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["hal daum\u00e9 iii"], "accepted": true, "id": "0907.1815"}, "pdf": {"name": "0907.1815.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["me@hal3.name"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 7.\n18 15\nv1 [\ncs .L\nG ]\n1 0\nJu l 2"}, {"heading": "1 Introduction", "text": "The task of domain adaptation is to develop learning algorithms that can be easily ported from one domain to another\u2014say, from newswire to biomedical documents. This problem is particularly interesting in NLP because we are often in the situation that we have a large collection of labeled data in one \u201csource\u201d domain (say, newswire) but truly desire a model that performs well in a second \u201ctarget\u201d domain. The approach we present in this paper is based on the idea of transforming the domain adaptation learning problem into a standard supervised learning problem to which any standard algorithm may be applied (eg., maxent, SVMs, etc.). Our transformation is incredibly simple: we augment the feature space of both the source and target data and use the result as input to a standard learning algorithm.\nThere are roughly two varieties of the domain adaptation problem that have been addressed in the literature: the fully supervised case and the semi-\nsupervised case. The fully supervised case models the following scenario. We have access to a large, annotated corpus of data from a source domain. In addition, we spend a little money to annotate a small corpus in the target domain. We want to leverage both annotated datasets to obtain a model that performs well on the target domain. The semisupervised case is similar, but instead of having a small annotated target corpus, we have a large but unannotated target corpus. In this paper, we focus exclusively on the fully supervised case.\nOne particularly nice property of our approach is that it is incredibly easy to implement: the Appendix provides a 10 line, 194 character Perl script for performing the complete transformation (available at http://hal3.name/easyadapt.pl.gz). In addition to this simplicity, our algorithm performs as well as (or, in some cases, better than) current state of the art techniques."}, {"heading": "2 Problem Formalization and Prior Work", "text": "To facilitate discussion, we first introduce some notation. Denote by X the input space (typically either a real vector or a binary vector), and by Y the output space. We will write Ds to denote the distribution over source examples and Dt to denote the distribution over target examples. We assume access to a samples Ds \u223c Ds of source examples from the source domain, and samples Dt \u223c Dt of target examples from the target domain. We will assume that Ds is a collection of N examples and Dt is a collection of M examples (where, typically, N \u226b M ). Our goal is to learn a function h : X \u2192 Y with low expected loss with respect to the target domain.\nFor the purposes of discussion, we will suppose that X = RF and that Y = {\u22121,+1}. However, most of the techniques described in this section (as well as our own technique) are more general.\nThere are several \u201cobvious\u201d ways to attack the domain adaptation problem without developing new algorithms. Many of these are presented and evaluated by Daume\u0301 III and Marcu (2006).\nThe SRCONLY baseline ignores the target data and trains a single model, only on the source data.\nThe TGTONLY baseline trains a single model only on the target data.\nThe ALL baseline simply trains a standard learning algorithm on the union of the two datasets.\nA potential problem with the ALL baseline is that if N \u226b M , then Ds may \u201cwash out\u201d any affect Dt might have. We will discuss this problem in more detail later, but one potential solution is to re-weight examples from Ds. For instance, if N = 10\u00d7M , we may weight each example from the source domain by 0.1. The next baseline, WEIGHTED, is exactly this approach, with the weight chosen by cross-validation.\nThe PRED baseline is based on the idea of using the output of the source classifier as a feature in the target classifier. Specifically, we first train a SRCONLY model. Then we run the SRCONLY model on the target data (training, development and test). We use the predictions made by the SRCONLY model as additional features and train a second model on the target data, augmented with this new feature.\nIn the LININT baseline, we linearly interpolate the predictions of the SRCONLY and the TGTONLY models. The interpolation parameter is adjusted based on target development data.\nThese baselines are actually surprisingly difficult to beat. To date, there are two models that have successfully defeated them on a handful of datasets. The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). The idea of this model is to use the SRCONLY model as a prior on the weights for a second model, trained on the target data. Chelba and Acero (2004) describe this approach within the context of a maximum entropy\nclassifier, but the idea is more general. In particular, for many learning algorithms (maxent, SVMs, averaged perceptron, naive Bayes, etc.), one regularizes the weight vector toward zero. In other words, all of these algorithms contain a regularization term on the weights w of the form \u03bb ||w||22. In the generalized PRIOR model, we simply replace this regularization term with \u03bb ||w \u2212 ws||22, where w\ns is the weight vector learned in the SRCONLY model.1 In this way, the model trained on the target data \u201cprefers\u201d to have weights that are similar to the weights from the SRCONLY model, unless the data demands otherwise. Daume\u0301 III and Marcu (2006) provide empirical evidence on four datasets that the PRIOR model outperforms the baseline approaches.\nMore recently, Daume\u0301 III and Marcu (2006) presented an algorithm for domain adaptation for maximum entropy classifiers. The key idea of their approach is to learn three separate models. One model captures \u201csource specific\u201d information, one captures \u201ctarget specific\u201d information and one captures \u201cgeneral\u201d information. The distinction between these three sorts of information is made on a per-example basis. In this way, each source example is considered either source specific or general, while each target example is considered either target specific or general. Daume\u0301 III and Marcu (2006) present an EM algorithm for training their model. This model consistently outperformed all the baseline approaches as well as the PRIOR model. Unfortunately, despite the empirical success of this algorithm, it is quite complex to implement and is roughly 10 to 15 times slower than training the PRIOR model."}, {"heading": "3 Adaptation by Feature Augmentation", "text": "In this section, we describe our approach to the domain adaptation problem. Essentially, all we are going to do is take each feature in the original problem and make three versions of it: a general version, a source-specific version and a target-specific version. The augmented source data will contain only general\n1For the maximum entropy, SVM and naive Bayes learning algorithms, modifying the regularization term is simple because it appears explicitly. For the perceptron algorithm, one can obtain an equivalent regularization by performing standard perceptron updates, but using (w + ws)\u22a4x for making predictions rather than simply w\u22a4x.\nand source-specific versions. The augmented target data contains general and target-specific versions.\nTo state this more formally, first recall the notation from Section 2: X and Y are the input and output spaces, respectively; Ds is the source domain data set and Dt is the target domain data set. Suppose for simplicity that X = RF for some F > 0. We will define our augmented input space by X\u0306 = R3F . Then, define mappings \u03a6s,\u03a6t : X \u2192 X\u0306 for mapping the source and target data respectively. These are defined by Eq (1), where 0 = \u30080, 0, . . . , 0\u3009 \u2208 RF is the zero vector.\n\u03a6s(x) = \u3008x,x,0\u3009, \u03a6t(x) = \u3008x,0,x\u3009 (1)\nBefore we proceed with a formal analysis of this transformation, let us consider why it might be expected to work. Suppose our task is part of speech tagging, our source domain is the Wall Street Journal and our target domain is a collection of reviews of computer hardware. Here, a word like \u201cthe\u201d should be tagged as a determiner in both cases. However, a word like \u201cmonitor\u201d is more likely to be a verb in the WSJ and more likely to be a noun in the hardware corpus. Consider a simple case where X = R2, where x1 indicates if the word is \u201cthe\u201d and x2 indicates if the word is \u201cmonitor.\u201d Then, in X\u0306 , x\u03061 and x\u03062 will be \u201cgeneral\u201d versions of the two indicator functions, x\u03063 and x\u03064 will be source-specific versions, and x\u03065 and x\u03066 will be target-specific versions.\nNow, consider what a learning algorithm could do to capture the fact that the appropriate tag for \u201cthe\u201d remains constant across the domains, and the tag for \u201cmonitor\u201d changes. In this case, the model can set the \u201cdeterminer\u201d weight vector to something like \u30081, 0, 0, 0, 0, 0\u3009. This places high weight on the common version of \u201cthe\u201d and indicates that \u201cthe\u201d is most likely a determiner, regardless of the domain. On the other hand, the weight vector for \u201cnoun\u201d might look something like \u30080, 0, 0, 0, 0, 1\u3009, indicating that the word \u201cmonitor\u201d is a noun only in the target domain. Similar, the weight vector for \u201cverb\u201d might look like \u30080, 0, 0, 1, 0, 0\u3009, indicating the \u201cmonitor\u201d is a verb only in the source domain.\nNote that this expansion is actually redundant. We could equally well use \u03a6s(x) = \u3008x,x\u3009 and \u03a6t(x) = \u3008x,0\u3009. However, it turns out that it is easier to analyze the first case, so we will stick with\nthat. Moreover, the first case has the nice property that it is straightforward to generalize it to the multidomain adaptation problem: when there are more than two domains. In general, for K domains, the augmented feature space will consist of K+1 copies of the original feature space."}, {"heading": "3.1 A Kernelized Version", "text": "It is straightforward to derive a kernelized version of the above approach. We do not exploit this property in our experiments\u2014all are conducted with a simple linear kernel. However, by deriving the kernelized version, we gain some insight into the method. For this reason, we sketch the derivation here.\nSuppose that the data points x are drawn from a reproducing kernel Hilbert space X with kernel K : X \u00d7 X \u2192 R, with K positive semi-definite. Then, K can be written as the dot product (in X ) of two (perhaps infinite-dimensional) vectors: K(x, x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009X . Define \u03a6s and \u03a6t in terms of \u03a6, as:\n\u03a6s(x) = \u3008\u03a6(x),\u03a6(x),0\u3009 (2)\n\u03a6t(x) = \u3008\u03a6(x),0,\u03a6(x)\u3009\nNow, we can compute the kernel product between \u03a6s and \u03a6t in the expanded RKHS by making use of the original kernel K . We denote the expanded kernel by K\u0306(x, x\u2032). It is simplest to first describe K\u0306(x, x\u2032) when x and x\u2032 are from the same domain, then analyze the case when the domain differs. When the domain is the same, we get: K\u0306(x, x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009X + \u3008\u03a6(x),\u03a6(x\n\u2032)\u3009X = 2K(x, x\u2032). When they are from different domains, we get: K\u0306(x, x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009X = K(x, x\u2032). Putting this together, we have:\nK\u0306(x, x\u2032) =\n{\n2K(x, x\u2032) same domain K(x, x\u2032) diff. domain\n(3)\nThis is an intuitively pleasing result. What it says is that\u2014considering the kernel as a measure of similarity\u2014data points from the same domain are \u201cby default\u201d twice as similar as those from different domains. Loosely speaking, this means that data points from the target domain have twice as much influence as source points when making predictions about test target data."}, {"heading": "3.2 Analysis", "text": "We first note an obvious property of the featureaugmentation approach. Namely, it does not make learning harder, in a minimum Bayes error sense. A more interesting statement would be that it makes learning easier, along the lines of the result of (Ben-David et al., 2006) \u2014 note, however, that their results are for the \u201csemi-supervised\u201d domain adaptation problem and so do not apply directly. As yet, we do not know a proper formalism in which to analyze the fully supervised case.\nIt turns out that the feature-augmentation method is remarkably similar to the PRIOR model2. Suppose we learn feature-augmented weights in a classifier regularized by an \u21132 norm (eg., SVMs, maximum entropy). We can denote by ws the sum of the \u201csource\u201d and \u201cgeneral\u201d components of the learned weight vector, and by wt the sum of the \u201ctarget\u201d and \u201cgeneral\u201d components, so that ws and wt are the predictive weights for each task. Then, the regularization condition on the entire weight vector is approximately ||wg|| 2 + ||ws \u2212 wg|| 2 + ||wt \u2212wg||\n2, with free parameter wg which can be chosen to minimize this sum. This leads to a regularizer proportional to ||ws \u2212wt||\n2, akin to the PRIOR model. Given this similarity between the featureaugmentation method and the PRIOR model, one might wonder why we expect our approach to do better. Our belief is that this occurs because we optimize ws and wt jointly, not sequentially. First, this means that we do not need to cross-validate to estimate good hyperparameters for each task (though in our experiments, we do not use any hyperparameters). Second, and more importantly, this means that the single supervised learning algorithm that is run is allowed to regulate the trade-off between source/target and general weights. In the PRIOR model, we are forced to use the prior variance on in the target learning scenario to do this ourselves."}, {"heading": "3.3 Multi-domain adaptation", "text": "Our formulation is agnostic to the number of \u201csource\u201d domains. In particular, it may be the case that the source data actually falls into a variety of more specific domains. This is simple to account for in our model. In the two-domain case, we ex-\n2Thanks an anonymous reviewer for pointing this out!\npanded the feature space from RF to R3F . For a K-domain problem, we simply expand the feature space to R(K+1)F in the obvious way (the \u201c+1\u201d corresponds to the \u201cgeneral domain\u201d while each of the other 1 . . . K correspond to a single task)."}, {"heading": "4 Results", "text": "In this section we describe experimental results on a wide variety of domains. First we describe the tasks, then we present experimental results, and finally we look more closely at a few of the experiments."}, {"heading": "4.1 Tasks", "text": "All tasks we consider are sequence labeling tasks (either named-entity recognition, shallow parsing or part-of-speech tagging) on the following datasets:\nACE-NER. We use data from the 2005 Automatic Content Extraction task, restricting ourselves to the named-entity recognition task. The 2005 ACE data comes from 5 domains: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Weblog (wl), Usenet (un) and Converstaional Telephone Speech (cts).\nCoNLL-NE. Similar to ACE-NER, a named-entity recognition task. The difference is: we use the 2006 ACE data as the source domain and the CoNLL 2003 NER data as the target domain.\nPubMed-POS. A part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. (2006). There are two domains: the source domain is the WSJ portion of the Penn Treebank and the target domain is PubMed.\nCNN-Recap. This is a recapitalization task introduced by Chelba and Acero (2004) and also used by Daume\u0301 III and Marcu (2006). The source domain is newswire and the target domain is the output of an ASR system.\nTreebank-Chunk. This is a shallow parsing task based on the data from the Penn Treebank. This data comes from a variety of domains: the standard WSJ domain (we use the same data as for CoNLL 2000), the ATIS switchboard domain, and the Brown corpus (which is, itself, assembled from six subdomains).\nTreebank-Brown. This is identical to the TreebankChunk task, except that we consider all of the Brown corpus to be a single domain.\nIn all cases (except for CNN-Recap), we use roughly the same feature set, which has become somewhat standardized: lexical information (words, stems, capitalization, prefixes and suffixes), membership on gazetteers, etc. For the CNN-Recap task, we use identical feature to those used by both Chelba and Acero (2004) and Daume\u0301 III and Marcu (2006): the current, previous and next word, and 1-3 letter prefixes and suffixes.\nStatistics on the tasks and datasets are in Table 1. In all cases, we use the SEARN algorithm for solving the sequence labeling problem (Daume\u0301 III et al., 2007) with an underlying averaged perceptron classifier; implementation due to (Daume\u0301 III, 2004). For structural features, we make a second-order Markov assumption and only place a bias feature on the transitions. For simplicity, we optimize and report only on label accuracy (but require that our outputs be parsimonious: we do not allow \u201cI-NP\u201d to follow \u201cB-PP,\u201d for instance). We do this for three reasons. First, our focus in this work is on building better learning algorithms\nand introducing a more complicated measure only serves to mask these effects. Second, it is arguable that a measure like F1 is inappropriate for chunking tasks (Manning, 2006). Third, we can easily compute statistical significance over accuracies using McNemar\u2019s test."}, {"heading": "4.2 Experimental Results", "text": "The full\u2014somewhat daunting\u2014table of results is presented in Table 2. The first two columns specify the task and domain. For the tasks with only a single source and target, we simply report results on the target. For the multi-domain adaptation tasks, we report results for each setting of the target (where all other data-sets are used as different \u201csource\u201d domains). The next set of eight columns are the error rates for the task, using one of the different techniques (\u201cAUGMENT\u201d is our proposed technique). For each row, the error rate of the best performing technique is bolded (as are all techniques whose performance is not statistically significantly different at the 95% level). The \u201cT<S\u201d column is contains a \u201c+\u201d whenever TGTONLY outperforms SRCONLY (this will become important shortly). The final column indicates when AUGMENT comes in first.3\nThere are several trends to note in the results. Excluding for a moment the \u201cbr-*\u201d domains on the Treebank-Chunk task, our technique always performs best. Still excluding \u201cbr-*\u201d, the clear secondplace contestant is the PRIOR model, a finding consistent with prior research. When we repeat the Treebank-Chunk task, but lumping all of the \u201cbr-*\u201d data together into a single \u201cbrown\u201d domain, the story reverts to what we expected before: our algorithm performs best, followed by the PRIOR method.\nImportantly, this simple story breaks down on the Treebank-Chunk task for the eight sections of the Brown corpus. For these, our AUGMENT technique performs rather poorly. Moreover, there is no clear winning approach on this task. Our hypothesis is that the common feature of these examples is that these are exactly the tasks for which SRCONLY outperforms TGTONLY (with one exception: CoNLL). This seems like a plausible explanation, since it im-\n3One advantage of using the averaged perceptron for all experiments is that the only tunable hyperparameter is the number of iterations. In all cases, we run 20 iterations and choose the one with the lowest error on development data.\nplies that the source and target domains may not be that different. If the domains are so similar that a large amount of source data outperforms a small amount of target data, then it is unlikely that blowing up the feature space will help.\nWe additionally ran the MEGAM model (Daume\u0301 III and Marcu, 2006) on these data (though not in the multi-conditional case; for this, we considered the single source as the union of all sources). The results are not displayed in Table 2 to save space. For the majority of results, MEGAM performed roughly comparably to the best of the systems in the table. In particular, it was not statistically significantly different that AUGMENT on: ACE-NER, CoNLL, PubMed, Treebank-chunk-wsj, Treebank-chunk-swbd3, CNN and Treebank-brown. It did outperform AUGMENT on the Treebank-chunk on the Treebank-chunk-br-* data sets, but only outperformed the best other model on these data sets for br-cg, br-cm and br-cp. However, despite its advantages on these data sets, it was quite significantly slower to train: a single run required about ten times longer than any of the other models (including AUGMENT), and also required five-to-ten iterations of cross-validation to tune its hyperparameters so as to achieve these results."}, {"heading": "4.3 Model Introspection", "text": "One explanation of our model\u2019s improved performance is simply that by augmenting the feature space, we are creating a more powerful model. While this may be a partial explanation, here we show that what the model learns about the various domains actually makes some plausible sense.\nWe perform this analysis only on the ACE-NER data by looking specifically at the learned weights. That is, for any given feature f , there will be seven versions of f : one corresponding to the \u201ccrossdomain\u201d f and seven corresponding to each domain. We visualize these weights, using Hinton diagrams, to see how the weights vary across domains.\nFor example, consider the feature \u201ccurrent word has an initial capital letter and is then followed by\none or more lower-case letters.\u201d This feature is presumably useless for data that lacks capitalization information, but potentially quite useful for other domains. In Figure 1 we shown a Hinton diagram for this figure. Each column in this figure correspond to a domain (the top row is the \u201cgeneral domain\u201d). Each row corresponds to a class.4 Black boxes correspond to negative weights and white boxes correspond to positive weights. The size of the box depicts the absolute value of the weight.\nAs we can see from Figure 1, the /Aa+/ feature is a very good indicator of entity-hood (it\u2019s value is strongly positive for all four entity classes), regardless of domain (i.e., for the \u201c*\u201d domain). The lack of boxes in the \u201cbn\u201d column means that, beyond the settings in \u201c*\u201d, the broadcast news is agnostic with respect to this feature. This makes sense: there is no capitalization in broadcast news domain, so there would be no sense is setting these weights to anything by zero. The usenet column is filled with negative weights. While this may seem strange, it is due to the fact that many email addresses and URLs match this pattern, but are not entities.\nFigure 2 depicts a similar figure for the feature \u201cword is \u2019bush\u2019 at the current position\u201d (this figure is case sensitive).5 These weights are somewhat harder to interpret. What is happening is that \u201cby default\u201d the word \u201cbush\u201d is going to be a person\u2014this is because it rarely appears referring to a plant and so even in the capitalized domains like broadcast con-\n4Technically there are many more classes than are shown here. We do not depict the smallest classes, and have merged the \u201cBegin-*\u201d and \u201cIn-*\u201d weights for each entity type.\n5The scale of weights across features is not comparable, so do not try to compare Figure 1 with Figure 2.\nversations, if it appears at all, it is a person. The exception is that in the conversations data, people do actually talk about bushes as plants, and so the weights are set accordingly. The weights are high in the usenet domain because people tend to talk about the president without capitalizing his name.\nFigure 3 presents the Hinton diagram for the feature \u201cword at the current position is \u2019the\u201d\u2019 (again, case-sensitive). In general, it appears, \u201cthe\u201d is a common word in entities in all domain except for broadcast news and conversations. The exceptions are broadcast news and conversations. These exceptions crop up because of the capitalization issue.\nIn Figure 4, we show the diagram for the feature \u201cprevious word is \u2019the\u2019.\u201d The only domain for which this is a good feature of entity-hood is broadcast conversations (to a much lesser extent, newswire). This occurs because of four phrases very common in the broadcast conversations and rare elsewhere: \u201cthe Iraqi people\u201d (\u201cIraqi\u201d is a GPE), \u201cthe Pentagon\u201d (an ORG), \u201cthe Bush (cabinet|advisors|. . . )\u201d (PER), and \u201cthe South\u201d (LOC).\nFinally, Figure 5 shows the Hinton diagram for the feature \u201cthe current word is on a list of common names\u201d (this feature is case-insensitive). All around, this is a good feature for picking out people and nothing else. The two exceptions are: it is also a good feature for other entity types for broadcast news and it is not quite so good for people in usenet. The first is easily explained: in broadcast news, it is very common to refer to countries and organizations by the name of their respective leaders. This is essentially a metonymy issue, but as the data is annotated, these are marked by their true referent. For usenet, it is because the list of names comes from news data, but usenet names are more diverse.\nIn general, the weights depicte for these features make some intuitive sense (in as much as weights for any learned algorithm make intuitive sense). It is particularly interesting to note that while there are some regularities to the patterns in the five diagrams, it is definitely not the case that there are, eg., two domains that behave identically across all features. This supports the hypothesis that the reason our algorithm works so well on this data is because the domains are actually quite well separated."}, {"heading": "5 Discussion", "text": "In this paper we have described an incredibly simple approach to domain adaptation that\u2014under a common and easy-to-verify condition\u2014outperforms previous approaches. While it is somewhat frustrating that something so simple does so well, it is perhaps not surprising. By augmenting the feature space, we are essentially forcing the learning algorithm to do the adaptation for us. Good supervised learning algorithms have been developed over\ndecades, and so we are essentially just leveraging all that previous work. Our hope is that this approach is so simple that it can be used for many more realworld tasks than we have presented here with little effort. Finally, it is very interesting to note that using our method, shallow parsing error rate on the CoNLL section of the treebank improves from 5.35 to 5.11. While this improvement is small, it is real, and may carry over to full parsing. The most important avenue of future work is to develop a formal framework under which we can analyze this (and other supervised domain adaptation models) theoretically. Currently our results only state that this augmentation procedure doesn\u2019t make the learning harder \u2014 we would like to know that it actually makes it easier. An additional future direction is to explore the kernelization interpretation further: why should we use 2 as the \u201csimilarity\u201d between domains\u2014we could introduce a hyperparamter \u03b1 that indicates the similarity between domains and could be tuned via cross-validation.\nAcknowledgments. We thank the three anonymous reviewers, as well as Ryan McDonald and John Blitzer for very helpful comments and insights."}], "references": [{"title": "Analysis of representations for domain adaptation", "author": ["John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Ben.David et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2006}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Adaptation of maximum entropy classifier: Little data can help a lot", "author": ["Chelba", "Acero2004] Ciprian Chelba", "Alex Acero"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chelba et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2004}, {"title": "Domain adaptation for statistical classifiers", "author": ["III Daum\u00e9", "III Marcu2006] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2006}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning Journal (submitted)", "citeRegEx": "III et al\\.,? \\Q2007\\E", "shortCiteRegEx": "III et al\\.", "year": 2007}, {"title": "Doing named entity recognition? Don\u2019t optimize for F1", "author": ["Christopher Manning"], "venue": "Post on the NLPers Blog,", "citeRegEx": "Manning.,? \\Q2006\\E", "shortCiteRegEx": "Manning.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "A more interesting statement would be that it makes learning easier, along the lines of the result of (Ben-David et al., 2006) \u2014 note, however, that their results are for the \u201csemi-supervised\u201d domain adaptation problem and so do not apply directly.", "startOffset": 102, "endOffset": 126}, {"referenceID": 1, "context": "A part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. (2006). There are two domains: the source domain is the WSJ portion of the Penn Treebank and the target domain is PubMed.", "startOffset": 67, "endOffset": 89}, {"referenceID": 5, "context": "Second, it is arguable that a measure like F1 is inappropriate for chunking tasks (Manning, 2006).", "startOffset": 82, "endOffset": 97}], "year": 2009, "abstractText": "We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough \u201ctarget\u201d data to do slightly better than just using only \u201csource\u201d data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains.", "creator": "LaTeX with hyperref package"}}}